{
    "id": "SOLR-822",
    "title": "CharFilter - normalize characters before tokenizer",
    "details": {
        "affect_versions": "1.3",
        "status": "Closed",
        "fix_versions": [
            "1.4"
        ],
        "components": [
            "Schema and Analysis"
        ],
        "type": "New Feature",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "A new plugin which can be placed in front of <tokenizer/>.\n\n\n<fieldType name=\"textCharNorm\" class=\"solr.TextField\" positionIncrementGap=\"100\" >\n  <analyzer>\n    <charFilter class=\"solr.MappingCharFilterFactory\" mapping=\"mapping_ja.txt\" />\n    <tokenizer class=\"solr.MappingCJKTokenizerFactory\"/>\n    <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\"/>\n    <filter class=\"solr.LowerCaseFilterFactory\"/>\n  </analyzer>\n</fieldType>\n\n\n\n<charFilter/> can be multiple (chained). I'll post a JPEG file to show character normalization sample soon.\n\nMOTIVATION:\nIn Japan, there are two types of tokenizers \u2013 N-gram (CJKTokenizer) and Morphological Analyzer.\nWhen we use morphological analyzer, because the analyzer uses Japanese dictionary to detect terms,\nwe need to normalize characters before tokenization.\n\nI'll post a patch soon, too.",
    "attachments": {
        "SOLR-822.patch": "https://issues.apache.org/jira/secure/attachment/12392641/SOLR-822.patch",
        "SOLR-822-renameMethod.patch": "https://issues.apache.org/jira/secure/attachment/12402551/SOLR-822-renameMethod.patch",
        "character-normalization.JPG": "https://issues.apache.org/jira/secure/attachment/12392639/character-normalization.JPG",
        "japanese-h-to-k-mapping.txt": "https://issues.apache.org/jira/secure/attachment/12408724/japanese-h-to-k-mapping.txt",
        "SOLR-822-for-1.3.patch": "https://issues.apache.org/jira/secure/attachment/12394228/SOLR-822-for-1.3.patch",
        "sample_mapping_ja.txt": "https://issues.apache.org/jira/secure/attachment/12392733/sample_mapping_ja.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12641758",
            "date": "2008-10-22T10:21:16+0000",
            "content": "forgive me if I'm wrong something with German and Chinese language.  "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12641763",
            "date": "2008-10-22T10:32:30+0000",
            "content": "patch attached. it includes MappingCharFilter and its Factory as a sample charFilter.\n\nKnown bug:\nanalysis.jsp has not been supported yet in this patch. This can be fixed. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12642169",
            "date": "2008-10-23T14:31:06+0000",
            "content": "\nKnown bug:\nanalysis.jsp has not been supported yet in this patch. This can be fixed.\n\nNow the patch fixes the bug. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12642178",
            "date": "2008-10-23T15:03:05+0000",
            "content": "sample_mapping_ja.txt file is attached. To use it, open schema.xml by editor and define textCharNorm fieldType and text_cjk field as follows:\n\n\n<fieldType name=\"textCharNorm\" class=\"solr.TextField\" positionIncrementGap=\"100\" >\n  <analyzer>\n    <charFilter class=\"solr.MappingCharFilterFactory\" mapping=\"sample_mapping_ja.txt\"/>\n    <tokenizer class=\"solr.MappingCJKTokenizerFactory\"/>\n    <filter class=\"solr.LowerCaseFilterFactory\"/>\n  </analyzer>\n</fieldType>\n\n\n\n\n<field name=\"text_cjk\" type=\"textCharNorm\" indexed=\"true\" stored=\"true\"/>\n\n\n\nthen start solr and access analysis.jsp. "
        },
        {
            "author": "Todd Feak",
            "id": "comment-12642181",
            "date": "2008-10-23T15:13:43+0000",
            "content": "Seems like a very flexible way to solve the issue, as well as SOLR-814 and SOLR-815. It should also work for existing filters like LowerCase. Seems like it has the potential to be faster then the filters, as it doesn't have to perform the same replacement multiple times if a particular character is replicated into multiple tokens, like in NGramTokenizer or CJKTokenizer.\n\nI didn't look in depth at the patch (good size patch to look through), but I wanted to verify at least 2 things. First, I assume that this only affects indexing and searching, not the actual document field contents? Second, is it easy to create a MappingCharFilter subclass with a hardcoded map built in? I don't think users should all have to recreate the same mapping files if we can just embed them.\n\nHowever, what about Lucene? Is this something that should exist in Lucene first, then be expanded to Solr? Are Lucene users in need of a similar functionality? "
        },
        {
            "author": "Todd Feak",
            "id": "comment-12642183",
            "date": "2008-10-23T15:16:05+0000",
            "content": "Oh, and another thought. Can it support characters written as \"\\uff01\" format in the mapping file? "
        },
        {
            "author": "Walter Underwood",
            "id": "comment-12642188",
            "date": "2008-10-23T15:31:37+0000",
            "content": "Yes, it should be in Lucene. LIke this: http://webui.sourcelabs.com/lucene/issues/1343\n\nThere are (at least) four kinds of character mapping:\n\nUnicode normalization from decomposed to composed forms (always safe).\n\nUnicode normalization from compatability forms to standard forms (may change the look, like fullwidth to halfwidth Latin).\n\nLanguage-specific normalization, like \"oe\" to \"\u00f6\" (German-only).\n\nMappings that improve search but are linguistically dodgy, like stripping accents and mapping katakana to hirigana.\n\nwunder "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12642210",
            "date": "2008-10-23T17:33:40+0000",
            "content": "\nFirst, I assume that this only affects indexing and searching, not the actual document field contents?\n\nRight. This only affects indexing and searching.\n\n\nSecond, is it easy to create a MappingCharFilter subclass with a hardcoded map built in?\n\nI didn't expect such use case, but it is must have.\n\n\nCan it support characters written as \"\\uff01\" format in the mapping file?\n\nThe patch doesn't support this format, but must have, too. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12642631",
            "date": "2008-10-25T16:10:27+0000",
            "content": "Koji:\n\n1) the patch is a little hard to read ... there seems to be a ton of unrelated whitespace changes (some in files that don't seem like they need modified for this functionality at all)\n\n2) the motivation of adding a new type of plugin that has direct access to the \"stream of characters\" in the Reader before the tokenizer gets access to it seems like a great idea, but i'm a little unclear as to the specific reason for some of the new apis: CharReader, CharFilter, CharStream.  What value do these add beyond something like...\n\n\npublic abstract class ReaderWrapperFactory {\n  public void init(Map<String,String> args) { ... }\n  public Map<String,String> getArgs() { ... }\n  public Reader create(Reader input) { \n     return input;\n  }\n}\n\n\n\n? "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12642835",
            "date": "2008-10-27T02:07:30+0000",
            "content": "Hoss,\n\nSorry about the unrelated whitespaces in the patch. I'll remove them in the next patch.\n\nAbout CharStream, CharReader and CharFilter classes, I created CharFilterFactory:\n\n\npublic interface CharFilterFactory {\n  public void init(Map<String,String> args);\n  public Map<String,String> getArgs();\n  public CharStream create(CharStream input);\n}\n\n\n\ninstead of ReaderWrapperFactory mentioned by Hoss. CharFilterFactory is a factory of CharFilter which reads CharStream and outputs CharStream. CharStream is a Reader but has correctPosition method:\n\n\npublic abstract class CharStream extends Reader {\n  public abstract int correctPosition( int currentPos );\n}\n\n\n\nThe method will be called by CharFilters and Tokenizer(in this case, Tokenizer should be CharStream \"aware\") to correct start/end offsets of tokens, because CharFilters may convert 1 char to 2 chars or the other way around. The following is a sample implementation of the method:\n\nMappingCharFilter.java\nprivate List<PosCorrectMap> pcmList;\n\npublic int correctPosition( int currentPos ){\n  currentPos = input.correctPosition( currentPos );\n  if( pcmList.isEmpty() ) return currentPos;\n  for( int i = pcmList.size() - 1; i >= 0; i-- ){\n    if( currentPos >= pcmList.get( i ).pos )\n      return currentPos + pcmList.get( i ).cumulativeDiff;\n  }\n  return currentPos;\n}\n\nstatic class PosCorrectMap {\n  int pos;\n  int cumulativeDiff;\n  public PosCorrectMap( int pos, int cumulativeDiff ){\n    this.pos = pos;\n    this.cumulativeDiff = cumulativeDiff;\n  }\n}\n\n\n\nThere is another CharStream class, CharReader. It is a Reader wrapper and necessary to get Reader and outputs CharStream. CharReader is a concrete class and instanciated in TokenizerChain.\n\nDoes that make sense to you? "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12642913",
            "date": "2008-10-27T13:22:36+0000",
            "content": "I think I found a bug... the correctPosition() returns incorrect position.  I'm working on that... "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12643443",
            "date": "2008-10-29T09:04:14+0000",
            "content": "I think I found a bug... the correctPosition() returns incorrect position.  I'm working on that...\n\nAttached patch fixes the problem. It also includes:\n\n\n\tsome unit tests\n\tJavadoc for CharStream, CharReader and CharFilter\n\trename correctPosition() to correctOffset() and make it final in CharFilter:\n\n\n\n\npublic final int correctOffset(int currentOff) {\n  return input.correctOffset( correctPosition( currentOff ) );\n}\n\nprotected int correctPosition( int pos ){\n  return pos;\n}\n\n\n\nthen correctOffset() calls correctPosition(). correctPosition() can be override to correct position in subclass of CharFilter.\n\n\trename MappingCJKTokenizer to CharStreamAwareCJKTokenizer\n\n\n\nTODO:\n\n\tsupport \\uNNNN style in mapping.txt\n\tadd StopCharFilter\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12643608",
            "date": "2008-10-29T19:03:32+0000",
            "content": "Does that make sense to you?\n\nyes, definitely ... but still a few questions:\n\n1) if i understand correctly: another use case beyond character normalization could be refactoring the existing HTMLStrip___Tokenizers so that instead people would use an HTMLStripCharFilter and then whatever tokenizer they like, correct?\n\n2) based on your explanation, shouldn't CharFilterFactory be renamed CharStreamFactory ? ... there's no requirement that implementations produce a CharFilter, as long as they produce a ChaStream, correct?\n\n3) should CharStream extend FilterReader?\n\n\u2014\n\nOne thing that worries me is the interaction of CharStreams with their corrected positions and Tokenizers that may not know about CharStream at all.  Oviously that could just be an unsupported case (ie; if you want to use some CharStreamFactories, you better use a TokenizerFactory that can handle it) but i still suspect some people could easily be bitten by this.\n\ni wonder if we could protect people from this.  perhaps a new CharStreamTokenizerFactory interface that must be implemented by any TokenizerFactory that knows about CharStreams (with a single \"public TokenStream create(CharStream input)\")  if a fieldType uses any CharStreamFactory it's an initialize error unless the TokenizerFactory is also a CharStreamTokenizerFactory.\n\nSomething else to consider: it seems like a lot of future headache could be simplied if the CharStream API was committed in lucene-java so that the Tokenizer API and all of the existing OOTB Tokenizers could know about it. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12644955",
            "date": "2008-11-04T13:18:06+0000",
            "content": "Hoss, sorry for the late reply.\n\n\n1) if i understand correctly: another use case beyond character normalization could be refactoring the existing HTMLStrip___Tokenizers so that instead people would use an HTMLStripCharFilter and then whatever tokenizer they like, correct?\n\nCorrect.\n\n\n3) should CharStream extend FilterReader?\n\nI think we need all these classes to construct the CharFilter framework - CharStream, CharReader and CharFilter. CharReader and CharFilter are the subclass of CharStream. CharStream has an abstract method correctOffset():\n\n\npublic abstract class CharStream extends Reader {\n  /**\n   * called by CharFilter(s) and Tokenizer to correct token offset.\n   *\n   * @param currentOff current offset\n   * @return corrected token offset\n   */\n  public abstract int correctOffset( int currentOff );\n}\n\n\n\nCharStream extends Reader instead of FilterReader because FilterReader has a Reader member but I don't need it. Instead, CharReader has a Reader and CharFilter has CharStream. The role of CharReader is that it wraps Reader and makes it CharStream.\n\n\npublic final class CharReader extends CharStream {\n  protected Reader input;\n  public CharReader( Reader in ){\n    input = in;\n  }\n  @Override\n  public int correctOffset(int currentOff) {\n    return currentOff;\n  }\n  :\n}\n\n\n\nThen CharReader is placed at the beginning of char-filter-chain. Now we get CharStream, CharFilters can be used to organize\na filter chain. I made the correctOffset() to final in CharFilter.\n\n\npublic abstract class CharFilter extends CharStream {\n  protected CharStream input;\n  protected CharFilter( CharStream in ){\n    input = in;\n  }\n  protected int correctPosition( int pos ){\n    return pos;\n  }\n  @Override\n  public final int correctOffset(int currentOff) {\n    return input.correctOffset( correctPosition( currentOff ) );\n  }\n  :\n}\n\n\n\nSubclass of CharFilter can override correctPosition() method to correct current position.\n\n\n2) based on your explanation, shouldn't CharFilterFactory be renamed CharStreamFactory ? ... there's no requirement that implementations produce a CharFilter, as long as they produce a ChaStream, correct?\n\nYes, CharFilterFactory creates CharStream but I like CharFilterFactory because 1) the factory will instanciate CharFilter (not CharStream) and 2) the return type of TokenFilterFactory.create() is TokenStream although it instantiates TokenFilter.\n\n\nSomething else to consider: it seems like a lot of future headache could be simplied if the CharStream API was committed in lucene-java so that the Tokenizer API and all of the existing OOTB Tokenizers could know about it.\n\nAgreed. I'll open a ticket in Lucene. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12645654",
            "date": "2008-11-07T01:24:15+0000",
            "content": "Agreed. I'll open a ticket in Lucene.\n\nBefore opening a ticket, I'm seeking comments:\nhttp://www.nabble.com/Proposal-for-introducing-CharFilter-to20327007.html "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12646309",
            "date": "2008-11-10T17:37:19+0000",
            "content": "The patch includes:\n\n\t'\\uNNNN' style supported in mapping.txt\n\tmapping-ISOLatin1Accent.txt\n\tCharStreamAwareWhitespaceTokenizer\n\t<charFilter/> in example/solr/conf/schema.xml\n\n<!-- charFilter + WhitespaceTokenizer  -->\n<fieldType name=\"textCharNorm\" class=\"solr.TextField\" positionIncrementGap=\"100\" >\n  <analyzer>\n    <charFilter class=\"solr.MappingCharFilterFactory\" mapping=\"mapping-ISOLatin1Accent.txt\"/>\n    <tokenizer class=\"solr.CharStreamAwareWhitespaceTokenizerFactory\"/>\n  </analyzer>\n</fieldType>\n\n\n\n "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12646588",
            "date": "2008-11-11T16:31:51+0000",
            "content": "added:\n\n\tsupport multiple mapping files (SOLR-663)\n\tan abstract base class - BaseCharFilter and moved PosCorrectMap to the base class\n\tmore test code\nI'll commit in a few days if there is no objections.\n\n "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12647497",
            "date": "2008-11-14T01:57:17+0000",
            "content": "Committed revision 713902. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12648929",
            "date": "2008-11-19T08:28:20+0000",
            "content": "patch file for Solr 1.3.0 users. "
        },
        {
            "author": "Peter Wolanin",
            "id": "comment-12677627",
            "date": "2009-02-28T02:29:55+0000",
            "content": "Is there an issue for CharStream API  in lucene?  The e-mail thread looks like people were generally in support. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12677641",
            "date": "2009-02-28T04:09:32+0000",
            "content": "Is there an issue for CharStream API in lucene? The e-mail thread looks like people were generally in support. \n\nOops. The pointer of the ticket for Lucene is missing. That is LUCENE-1466 . "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12683345",
            "date": "2009-03-19T06:58:29+0000",
            "content": "Reopening because currentPosition() method in CharFilter class is not for token position, but for token offset. It should be renamed before releasing Solr 1.4. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12683347",
            "date": "2009-03-19T07:06:51+0000",
            "content": "I plan to commit shortly. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12683443",
            "date": "2009-03-19T11:52:52+0000",
            "content": "Committed revision 755945. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12702434",
            "date": "2009-04-24T16:49:23+0000",
            "content": "Todd's comment from Oct 23, 2008 caught my attention:\n\n\nIt should also work for existing filters like LowerCase. Seems like it has the potential to be faster then the filters, as it doesn't have to perform the same replacement multiple times if a particular character is replicated into multiple tokens, like in NGramTokenizer or CJKTokenizer. \n\nCouldn't we replace LowerCaseFilter then?  Or does LCF still have some unique value?  Ah, it does - it makes it possible to put it after something like WordDelimiterFilterFactory.  Lowercasing at the very beginning would make it impossible for WDFF to do its job.  Never mind.  Leaving for posterity. "
        },
        {
            "author": "Mark Bennett",
            "id": "comment-12711717",
            "date": "2009-05-21T17:37:55+0000",
            "content": "In SOLR-814 it was suggested that some systems might want to normalizes all Hiragana characters to their Katakana counterpart.\n\nAlthough this is not universally agreed to, if somebody wanted to do it, I believe this attached mapping file would peform that task when used with this 822 patch.  I don't speak Japanese and don't have test content yet, so I'm not 100% it works, but wanted to upload it as a start. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12769700",
            "date": "2009-10-24T19:21:53+0000",
            "content": "Please update the Wiki for this feature.\n\nhttp://wiki.apache.org/solr/AnalyzersTokenizersTokenFilters?highlight=char+filters\n "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12769741",
            "date": "2009-10-25T00:26:28+0000",
            "content": "Please update the Wiki for this feature. \n\nDone.  "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12775559",
            "date": "2009-11-10T15:51:49+0000",
            "content": "Bulk close for Solr 1.4 "
        },
        {
            "author": "Victor Yap",
            "id": "comment-13867832",
            "date": "2014-01-10T14:26:55+0000",
            "content": "An old comment's link has been moved.\n\nOriginally: http://webui.sourcelabs.com/lucene/issues/1343\nMoved to: https://issues.apache.org/jira/browse/LUCENE-1343 "
        }
    ]
}