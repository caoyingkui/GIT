{
    "id": "SOLR-1715",
    "title": "Externally Generated Filters",
    "details": {
        "affect_versions": "None",
        "status": "Open",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "components": [
            "search"
        ],
        "type": "New Feature",
        "priority": "Minor",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "I'd like to have a way of loading document filters from externally generated sources.  For instance, I may have run a clustering task over my corpus and the results may be stored in HDFS (Hadoop).  A given cluster will have a centroid and a bunch of points that are in the cluster.  The points are labeled by doc id (the Solr doc id) from the index.  \n\nAnother example is I may have an external security system that can provide info on which documents are available to a user.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Ryan McKinley",
            "id": "comment-12798797",
            "date": "2010-01-11T19:18:59+0000",
            "content": "this would also be a good way to 'punt' complex geographic queries to some other system.   "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12872537",
            "date": "2010-05-27T22:07:24+0000",
            "content": "Bulk updating 240 Solr issues to set the Fix Version to \"next\" per the process outlined in this email...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E\n\nSelection criteria was \"Unresolved\" with a Fix Version of 1.5, 1.6, 3.1, or 4.0.  email notifications were suppressed.\n\nA unique token for finding these 240 issues in the future: hossversioncleanup20100527 "
        },
        {
            "author": "Tom Burton-West",
            "id": "comment-12920786",
            "date": "2010-10-13T22:23:11+0000",
            "content": "\n+1 for work on this feature.\n\nOther use cases include :\n  1) creating custom sub-collections of documents (Example that won't scale without this issue being addressed :  http://babel.hathitrust.org/cgi/mb?a=listcs)\n2) tagging documents \n3) anything that might need querying a relational database (i.e. complex joins) \n4) sort of an alternative to implementation of incremental field update in lucene/solr (i.e. updated fields stored outside of lucene/solr)\n\nTom Burton-West "
        },
        {
            "author": "Roman Chyla",
            "id": "comment-13039816",
            "date": "2011-05-26T17:43:03+0000",
            "content": "I noticed this question at the Lucene conference, and it seems suspiciously similar to what we solved as well while developing the intgration between our system and Solr - if you are still at LR, you can come to see my talk about embedding Python in Solr \u2013 http://lucenerevolution.com/2011/sessions-day-2  at 15:45\n\nI know, this will not be considered a 'kosher' solution , but it could help \u2013 in fact, we can embed not only Python, but also (almost any) C/C++ app inside Solr.\n\nCheck out the source at:\n\nhttps://github.com/romanchyla/montysolr "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043823",
            "date": "2011-06-03T16:47:15+0000",
            "content": "Bulk move 3.2 -> 3.3 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13106407",
            "date": "2011-09-16T14:50:56+0000",
            "content": "3.4 -> 3.5 "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13234704",
            "date": "2012-03-21T18:08:57+0000",
            "content": "Bulk of fixVersion=3.6 -> fixVersion=4.0 for issues that have no assignee and have not been updated recently.\n\nemail notification suppressed to prevent mass-spam\npsuedo-unique token identifying these issues: hoss20120321nofix36 "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13717358",
            "date": "2013-07-23T18:47:58+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13971241",
            "date": "2014-04-16T12:57:32+0000",
            "content": "Move issue to Solr 4.9. "
        },
        {
            "author": "Toke Eskildsen",
            "id": "comment-14309706",
            "date": "2015-02-06T19:27:30+0000",
            "content": "We need something like this for defining subsets of our net archive index (billions of documents), so I am considering writing a patch. My idea for a solution is as follows:\n\nIn a request, a user can state a key and a URL. These two values are stored for the lifetime of the Solr JVM (or until updated/deleted). When a user requests a filter for the given key, it is evaluated to a filter docset, which is stored until the index is updated.\n\nEvaluation of dhe content from the URL is processed in a streaming manner, without any size limit. Each line is treated as a Solr query and the result ing docIDs are enabled in the docset. Treating each line as a query means that \"everything\" can be expressed, from corpus spanning : through large TermsQueries to a billion ID:xxxxxx statements. The downside is size (and as part of that speed), so other types of streams (bitmap with the content for the docset, list of ID-values, etc) might be worth adding at a later point. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14309790",
            "date": "2015-02-06T20:09:07+0000",
            "content": "Toke Eskildsen can be the same achieved by updating docvalues, and then caching filter on-demand via fq=\n{!frange ...}\nval ? wdyt? "
        },
        {
            "author": "Toke Eskildsen",
            "id": "comment-14310932",
            "date": "2015-02-07T21:09:24+0000",
            "content": "Mikhail Khludnev I think that would also be a viable solution, especially since it does not require new code. It might be preferable for many setups. It would require a bit of bookkeeping when multiple subsets are used, with a limitation of 64 subsets per DV-long-field, but persistent subsets requires a bookkeeping system anyway.\n\nI our case we would very much like to avoid changing the shards as it triggers a backup round. But it might be that large shards with a semi-hard requirement of immutability are rare. "
        }
    ]
}