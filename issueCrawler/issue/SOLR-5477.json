{
    "id": "SOLR-5477",
    "title": "Async execution of OverseerCollectionProcessor tasks",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.8",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Sub-task",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Typical collection admin commands are long running and it is very common to have the requests get timed out.  It is more of a problem if the cluster is very large.Add an option to run these commands asynchronously\n\nadd an extra param async=true for all collection commands\n\nthe task is written to ZK and the caller is returned a task id. \nas separate collection admin command will be added to poll the status of the task\n\ncommand=status&id=7657668909\n\nif id is not passed all running async tasks should be listed\n\nA separate queue is created to store in-process tasks . After the tasks are completed the queue entry is removed. OverSeerColectionProcessor will perform these tasks in multiple threads",
    "attachments": {
        "SOLR-5477.patch": "https://issues.apache.org/jira/secure/attachment/12622808/SOLR-5477.patch",
        "SOLR-5477-updated.patch": "https://issues.apache.org/jira/secure/attachment/12628111/SOLR-5477-updated.patch",
        "SOLR-5477-CoreAdminStatus.patch": "https://issues.apache.org/jira/secure/attachment/12621573/SOLR-5477-CoreAdminStatus.patch",
        "SOLR-5477.urlschemefix.patch": "https://issues.apache.org/jira/secure/attachment/12634901/SOLR-5477.urlschemefix.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Yago Riveiro",
            "id": "comment-13827764",
            "date": "2013-11-20T15:33:35+0000",
            "content": "Related with this feature we can add a notification panel in the UI. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13836658",
            "date": "2013-12-02T16:28:35+0000",
            "content": "Here's what I'd recommend. \n\nHave 3 queues in the first phase of implementation. One each for submitted, running, completed. The completed queue only keeps the top-X tasks (by recency of completion). The completion queue is important for people to figure out details about a completed task e.g. completion time, running time etc.\n\nI've started working on it and would recommend that we have a ThreadPool for the running tasks. This can be capped at a config setting.\n\nI am still debating about when to accept tasks (or perhaps accept everything and fail them when they run). Here's a sample case on that. Firing a Shard split for collection1/shard1 would lead to an inactive shard1. If we continue to accept tasks until this completes, we may accept actions that involve shard1. We may need to take a call on that.\n\nFor now, I am not looking at truly multi-threading my implementation (but certainly doing that before having this particular JIRA as resolved). Once I get to it, I'd perhaps still just run only one request per collection at a time, until we have a more complex decision making capability.\n\nOnce a task is submitted, the OverseerCollectionProcessor peeks and processes tasks which are in the submitted queue and moves them to in-process. We'll have to synchronize this task on the queue/collection.\n\nUpon completion, again the task is moved from the in-progress queue to the completed queue.\n\nCleaning up of the completed queue could also be tricky and we may need a failed tasks queue or have a way to perhaps retain failed tasks in the completed queue longer. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13836659",
            "date": "2013-12-02T16:29:07+0000",
            "content": "+1 for Yago Riveiro "
        },
        {
            "author": "Jessica Cheng Mallet",
            "id": "comment-13846868",
            "date": "2013-12-12T23:03:17+0000",
            "content": "Would you please comment more about how failures are handled? I'm interested especially in how the Overseer may find out if individual subcommand to individual core admin API failed vs. timed out (the same sort of problem on the overall collection task). Also, if it fails, if and how states are cleaned up so that when the client re-issue the command it has a chance of succeeding. (For example, if a split shard command fails for some unknown reason, it might have left the shardX_0 and shardX_1 created, and the next split command might fail because it tries to create those two new target shards but they already exist. Note that this is just an example for explanation's sake--I don't know if it actually will complain this way.) Thanks! "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13847135",
            "date": "2013-12-13T03:59:57+0000",
            "content": "I am not really planning at adding any new checks for the moment but any exception in the response can be checked for. In case the response contains an exception, the task would be removed from the work queue and put into the failure queue.\nRetries would work for all commands that handle retries gracefully (split shard for instance does).\n\nGoing forward, we can always enhance this to be smarter as far as handling failures is concerned. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13847162",
            "date": "2013-12-13T04:40:29+0000",
            "content": "auto-retry is not a good feature , I would say. The fact that, a command  failed to execute means that there was something keeping it from running . ( A timeout would not happen in  a background operation).So it might fail again. It is wise to capture the error in the 'failed' queue and present the information to the admin user . The user can rectify the problem and re-issue a command "
        },
        {
            "author": "Jessica Cheng Mallet",
            "id": "comment-13847857",
            "date": "2013-12-13T20:03:19+0000",
            "content": "I agree that auto-retry is not the right thing to do.\n\nHowever, a timeout can possibly happen on the Overseer to node admin requests (if these requests have no timeouts, it might be dangerous because a connection can be sometimes be sunk and the client will never find out--we've actually seen this happen on the apache httpclient through solrj). What I'm getting at is that for the same reason that we're changing this client to Overseer request to being async, maybe the Overseer to node admin request should be async too. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13849250",
            "date": "2013-12-16T15:59:32+0000",
            "content": "Thanks for the input Jessica.\n\nI would like to believe that most of the internal timeouts (from the Overseer to Core Admin) would already be taken care of. If that's not the case, I'll handle it internally and add timeouts. Having said that, I wouldn't propose to self-retry or anything even in that case.\nI would instead want to let it timeout, report the same back to the user and let him reconfigure the timeout (would add a param that let's a user override the default timeouts for the particular request).\nThis way, we enable the user to make the 'intelligent' decisions and overcome timeout (and other) issues.\n\nWhen you poll for the status of a request, the response would contain the failure information (in case of failure) which can be used by the admin user to take a call. "
        },
        {
            "author": "Jessica Cheng Mallet",
            "id": "comment-13849428",
            "date": "2013-12-16T18:24:12+0000",
            "content": "Again, I'm not advocating an auto-retry and agree that it is not the right thing to do.\n\nAll I'm saying is that as an admin user, I would like to have the definitive answer of \"success\", \"failure\", and \"in progress\", and if a request times out anywhere down in the pipeline, the answer is \"don't know\" because it can either be \"failure\" or \"in progress\". Without a way to separately/asynchronously poll the status of any individual subtask, the overall collection request cannot offer this definitive answer, and I as an admin user will not be able to make the call of whether or not to re-issue a request. That is, if my failure status is \"time out\", I have no idea what actually went wrong, what to fix, or even if actually my timed out request is still in progress and will finish in another 10 seconds. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13849581",
            "date": "2013-12-16T18:49:15+0000",
            "content": "On a related note, I think our current timeouts are much too low at one minute. If nothing comes up to make us think the call is still not in progress, we should be willing to wait much longer I think. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13849649",
            "date": "2013-12-16T19:47:42+0000",
            "content": "I think most calls (or perhaps all) are idempotent/safe. If something times-out, we can safely put it as a failed task with a message specifying an internal timeout for now and have the admin user reissue the call with increased timeout param.\n\nAs far as I remember most of the calls are built to check and cleanup an older call that failed.\n\nAlso, I agree that our time-out values right now are too low for a lot of operations to fail (specially at larger scales). "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13852058",
            "date": "2013-12-18T19:26:45+0000",
            "content": "Having thought about it, here's another solution that also makes CoreAdmin calls async.\n\nAdding a CoreAdmin API that also works on similar lines as that of Collection API for fetching back the request status. This will enable async Core Admin calls and avoid potential timeouts between the overseer and the cores.\n\nHere's what all of the solution will look like:\n\n\tAsync CollectionAPI calls for the end user which uses zk.\n\tRequest status API for Collection level.\n\tCoreAdmin API to return request id as part of the response (if run in async mode?).\n\tCoreAdmin API to get the status of a submitted task (should solve the problem that you highlighted in particular).\n\n\n\nI'm still debating between having even the CoreAdmin to use zk  (which means it'd only work in SolrCloud mode) or just have a local map of running taks. I'm anyways moving on with making the calls 'to' overseer async. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13862862",
            "date": "2014-01-06T09:10:28+0000",
            "content": "First raw patch with a test that starts tracking completed and in-progress tasks at the CoreAdmin level.\nIt's nowhere close to being complete on this front and have a few TODOs in there.\n\nI have another patch (will upload in a while) for the CollectionAPI that uses zk to track information at collection level.\n\nOnce I have both of them, will have them work together. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13863798",
            "date": "2014-01-07T01:50:43+0000",
            "content": "A few small suggestions from someone who hasn't through much of this but has done similar async setups in other systems in another lifetime...\n\n1) on where the (core task) queues should live...\n\nI'm still debating between having even the CoreAdmin to use zk (which means it'd only work in SolrCloud mode) or just have a local map of running taks. \n\nI think it would be wise to keep them in ZK \u2013 if for no other reason then because the primary usecase you expect is for the async core calls to be made by the async overseer calls; and by keeping the async core queues in zk, the overseer can watch those queues directly for \"completed\" instead of needing ot wake up, poll every replica, go back to sleep.\n\nHowever, a secondary concern (i think) is what should happen if/when a node gets rebooted \u2013 if the core admin tasks queues are in RAM then you could easily get in a situation where the overseer asks 10 replicas to do something, replicaA succeeds or fails quickly and then reboots, the overseer checks back once all replicas are done and finds that replicaA can't say one way or another whether it succeeded or failed \u2013 it's queues are totally empty.\n\n2) on generating the task/request IDs.\n\nin my experience, when implementing an async callback API like this, it can be handy to require the client to specify the magical id that you use to keep track of things \u2013 you just ensure it's unique among the existing async jobs you know about (either in the queue, or in the recently completed/failed queues).  Sometimes single threaded (or centrally manged) client apps can generate a unique id easier then your distributed system, and/or they may already have a one-to-one mapping between some id they've already got and the task they are asking you to do, and re-using that id makes the client's life easier for debuging/audit-logs.\n\nin the case of async collection commands -> async core commands, it would also mean the overseer could reuse whatever id the client passed in for the collection commands when talking to each of the replicas. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13863988",
            "date": "2014-01-07T07:42:12+0000",
            "content": "Thanks for the inputs Hoss.\n\nHaving them in zk certainly makes it easier to track completed tasks but I was just trying to\n1. Keep it simpler for now\n2. Have no dependency on zk for CoreAdmin tasks.\n\nPt. 2 that I mentioned however takes a back seat if we don't intend to use it for purely core admin calls from other clients.\n\nAbout generating request IDs at client-side, I'll ping you on IRC and get more clarity on that. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13864855",
            "date": "2014-01-07T23:26:47+0000",
            "content": "in my experience, when implementing an async callback API like this, it can be handy to require the client to specify the magical...\n\nConsidering that we have a 1-n relationship between calls made by the client to the OCP and OCP to Cores, we can't really use the client generated id. We would anyways need multiple ids be generated at the OCP-Core call level. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13870476",
            "date": "2014-01-14T07:20:28+0000",
            "content": "Patch with more meat. Here's what this includes:\n\n\tRequest Status API based on ZK for Collection level calls.\n\tRequest Status API for CoreAdmin level (still memory based in this patch).\n\tAsync mode. Specifying 'async=requestid' let's run splitshard and createcollection commands for now in async mode. This request id can then be used to track the progress of the request. If run in async mode, the call returns almost immediately, only pushing the task to the zk queue.\n\tBasic tests for the above.\n\n\n\nHere's what's lying semi baked on my machine, will continue pushing more of this stuff over the next few days:\n\n\tCheck and dedup the request id as specified by the user.\n\tCoreAdmin calls to be async from OCP in case the original call was initiated with 'async' option.\n\tReturn more than just the status of the request as part of the status request API. Will be returning (atleast) the entire Response instead.\n\tMake CoreAdmin calls optionally async.\n\n "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13870532",
            "date": "2014-01-14T08:52:06+0000",
            "content": "More todo:\n\n\tCleanup the task queues. Another API call? Timed cleanup? Size limiting?\n\n "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13870549",
            "date": "2014-01-14T09:08:03+0000",
            "content": "One change that I'm almost done with is 'not managing the request id generation'. The client is supposed to generate/manage the id.\nThe overseer would use the id sent to it as a prefix for the ids it generates for the CoreAdmin calls. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13872573",
            "date": "2014-01-15T20:49:29+0000",
            "content": "I have a few questions regrading my approach for making the CoreAdmin calls async:\n\nApproach #1:\n\n\tCoreAdmin requests get submitted to zk.\n\tCore watches it's zk node for submitted tasks. Request object is the data in the node (when submitted).\n\tOn completion, the core deletes the submitted task and puts a new node with the response and other metadata into zk.\n\tCollection API watches the node when it submits a task, waits for it to complete.\n\tOn completion of the Collection API call, delete all related core admin request nodes in zk that were generated.\n\n\n\n\n\tCleaning up of request nodes in zk happens through an explicit API call.\n\tHaving something on the following lines in zk would be helpful:\n\n\n\n/tasks\n./collections/collection1/task1\n./cores/core1/collection1/task1/coretask1\n./_\n\nThis would help us delete the entire group of tasks associated to a core/collection/core task/collection task.\n\nQuestions:\n\n\tThis move would mean having a lot more clients talk to and write to zk. Does this approach make sense as far as the intended direction of SolrCloud is concerned?\n\tAny suggestions/concerns about scalability of zk as far as having multiple updates coming into zk is concerned.\n\n\n\nApproach #2:\nContinue accepting the request like right now, but just :\n\n\tGet the call to return immediately\n\tUse zk to only track/store the status (persistence). The request status calls still comes to the core and the status is fetched from zk by the core instead of the client being intelligent and talking directly to zk.\n\n\n\nThis approach is certainly less intrusive but then also doesn't come with the benefit of having the client just watch over a particular zk node for task state change etc.\n\n\nApproach #3 (Not the best option, and more like the option if zk has scalability issues with everyone writing/watching):\n\n\tNot have CoreAdmin calls as async but instead introduce a tracking mode. Once the task is submitted [with async = \"taskid\"], track this request using an in-memory data structure. Even if the request times out, the client can go back and query about the task status.\n\n "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13872621",
            "date": "2014-01-15T21:30:10+0000",
            "content": "Also, SOLR-5519 suggests that \"Creating ZK nodes should be done at overseer (as much as possible).\".\nNoble Paul , any suggestions on that? "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13872626",
            "date": "2014-01-15T21:31:52+0000",
            "content": "Also, SOLR-5519 suggests that \"Creating ZK nodes should be done at overseer (as much as possible).\".\nNoble Paul , any suggestions on that? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13873554",
            "date": "2014-01-16T16:14:37+0000",
            "content": "\nQuestions:\nThis move would mean having a lot more clients talk to and write to zk. Does this approach make sense as far as the intended direction of SolrCloud is concerned?\nAny suggestions/concerns about scalability of zk as far as having multiple updates coming into zk is concerned.\n\n\n\tI don't think we need to use a ZooKeeper queue for communication. The only reason where that is required is when we need fail over. That is desired for the overseer actions but not required for core admin actions. For example, I don't think you should be able to submit a core admin action when the target node is not up.\n\tThis approach only works with a cloud aware client such as SolrJ i.e. you can't submit an async core admin action with HTTP. This in itself is a strong reason to warrant a -1 from me.\n\tThis a very intrusive change compared to other approaches. I think the benefits you have outlined can be achieved in simpler ways (see my comments on approach #2 below)\n\n\n\n\nApproach #2:\nContinue accepting the request like right now, but just :\nGet the call to return immediately\nUse zk to only track/store the status (persistence). The request status calls still comes to the core and the status is fetched from zk by the core instead of the client being intelligent and talking directly to zk.\nThis is much more acceptable to me. Clients should not have to worry about ZK to submit a request.\n\n\nThis approach is certainly less intrusive but then also doesn't come with the benefit of having the client just watch over a particular zk node for task state change etc.\nIn my mind, the proposal to have the client watch nodes for task state change is orthogonal to the way the task is invoked. There is no reason why the request can't be made via HTTP and the response from core container can't contain the request id (is this the complete zookeeper node name?) which can be used by a client to setup a watch directly if it so desires. In any case a HTTP based status/polling API must exist. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13873583",
            "date": "2014-01-16T16:43:06+0000",
            "content": "Other than Overseer no other nodes watch queues. A core watching a queue will be a huge scalability issue  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13873843",
            "date": "2014-01-16T19:53:44+0000",
            "content": "Given the numbers published for ZooKeeper, even with 10,000 cores all watching, I highly doubt it would be a huge scalability issue to have them all watching a queue.\n\nApproach #2 still looks like the right approach though. Nice and simple. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13874003",
            "date": "2014-01-16T21:52:49+0000",
            "content": "Thanks all of you. I'll go with approach#2 in that case.\n\nI think more than zk being a scalability bottleneck, not wanting to delegate all of the communication etc. to zk could be a stronger reason to not take approach#1. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13876232",
            "date": "2014-01-20T07:54:08+0000",
            "content": "Some comments on the last patch:\n\n\n\tWhy have two different sets of parameter names; 'requeststatus' with 'id' for collection handler and 'reqstatus' with 'requestid' for OverseerCollectionProcessor? Let's make it the same. Maybe just call it 'status' with a request_id?\n\tIn CollectionHandler.handleSplitShardAction, the following can throw NPE if ASYNC is not present. Use Boolean.parseBoolean instead:\n\nif (req.getParams().get(ASYNC).equals(\"true\")) {\n      props.put(ASYNC, \"true\");\n    }\n\n\n\tShouldn't the CollectionHandler should add support for async in all commands and not just create collection and split?\n\tCollectionHandler.handleResponse should actually check for non-null value of \"async\" Just checking if the key exists is not enough.\n\tI guess the client creating/managing the id part is not implemented in the last patch.\n\tIn DistributedQueue.createData \u2013 there is debug logging that should be marked with nocommit and removed\n\tDistributedQueue.getNumericId is an unused method\n\tDistributedQueue.containsValue creates a new String from byte[] with the default charset. Similar issues with String.getBytes. Run ant check-forbidden-apis from inside the solr folder for a full list.\n\tThe completed queue and the failure queue are actually for OverseerCollectionProcessor so they should be named appropriately i.e. /overseer/collection-queue-completed and /overseer/collection-queue-failed\n\tWhy are items being put in the completedQueue in Overseer? The async and status support is only for collection commands not overseer commands.\n\tThe OverseerCollectionProcessor.processMessage catches Throwables with this patch. It should be changed back to catching Exception only.\n\tProcessing OverseerCollectionProcessor tasks in multiple threads is a key requirement because these tasks can be long running. Just making them asynchronous is not enough. This hasn't been addressed in this patch yet.\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13877554",
            "date": "2014-01-21T15:56:11+0000",
            "content": "This is Anshum's patch brought in sync with latest trunk. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13879795",
            "date": "2014-01-23T09:40:56+0000",
            "content": "Still a lot of stuff to be added/fixed but here's another patch so that if someone is interested, he/she could get some idea on the direction in which I'm moving.\n\nHere's what's been fixed/changed in this patch:\n\n\tStandardized the request parameters for CoreAdmin and Collection level request status. Parameters now are : action=REQUESTSTATUS&requestid=XX\n\tAdded async option for all other calls (not just splitshard and createcollection).\n\tFixed the encoding related stuff and the potential NPEs where ever I could spot those.\n\tRemoved unused code and cleaned up some debugging stuff.\n\tUsed a ThreadPool in case of coreadmin in a rather raw manner i.e. static (and without the mode check etc). Have added todo's to add those checks and to also call shutdown for the threadpool.\n\tChanged names of the queues to the ones suggested by Shalin.\n\tItems are no longer being put in the completed queue in Overseer.\n\tRan ant check-forbidden-apis to fix reported issues.\n\n\n\nHere's what still lies with me incomplete/not-working:\n\n\tMake shard requests async (pass the param to coreadmin requests and poll for completion/failure) from the OverseerCollectionProcessor.\n\tHave a threadpool for OverseerCollectionProcessor so that the long running tasks don't block everything else.\n\tImprove current tests and add more tests.\n\n "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13880845",
            "date": "2014-01-24T09:27:08+0000",
            "content": "Another patch that has collection creation and split shard as async from OCP.\nWorking on making other calls also async (which I think should be trivial and merely about calling the same methods).\n\nWorking on storing/responding with better status for the status request.\nRight now, the status tracking is limited to : running/completed/failed/notfound.\n\nAlso, trying to get another patch that saves information in zk instead of in-memory for CoreAdmin  request state. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13881093",
            "date": "2014-01-24T16:07:20+0000",
            "content": "I had a discussion with Noble (offline) and we thought that holding this stuff for persistence in zk didn't make much sense. Here are the reasons:\n\n\tEvery request would need to de-duplicate against the submitted/completed/failed tasks and zk queues aren't fit for that. Every dedup would translate to fetching all children and running a compare or something. With all cores trying to use the same zk setup, not sure if it's even required. In memory hashes would work better at handling this use case.\n\tWe may not be interested in persistence of results over a longer duration i.e. if the node goes down, we should be fine with losing the request information as in general, the time taken to bring the same node back up would be more than someone else doing the job in the meanwhile (fair assumption?).\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13881153",
            "date": "2014-01-24T17:10:13+0000",
            "content": "I don't understand why we are using DistributedQueues for holding status in the first place. It is just not required. We should just have a zk node constructed with a prefix and user specified hash for holding status. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13881166",
            "date": "2014-01-24T17:18:21+0000",
            "content": "Just to clarify, I'm not against storing status for core admin tasks in memory but just wanted to point out that problem of deduping by getting all children is a problem that we're creating ourselves by (ab)using DQ. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13881271",
            "date": "2014-01-24T18:33:32+0000",
            "content": "Changed to using another implementation of storing data in zk. Not using DistributedQueue for anything but checking on the workQueue.\n\nThe new implementation is a plain requestid based node map in zk and never really needs to do a getChildren or anything of that sort.\n\nThanks for pointing that out Shalin. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13881292",
            "date": "2014-01-24T18:44:02+0000",
            "content": "Seems like something got screwed up during 'svn diff'. Reposting the latest patch. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13881414",
            "date": "2014-01-24T20:43:36+0000",
            "content": "Added checking for existing tasks with the same id.\n\nHave a lot of coming up in the logs. Trying to debug that.\n\n\n414075 [Overseer-91131269805768705-10.0.0.3:7574_solr-n_0000000001] WARN  org.apache.solr.cloud.OverseerCollectionProcessor  \u2013 Overseer cannot talk to ZK\n414075 [Thread-16] ERROR org.apache.solr.cloud.Overseer  \u2013 Exception in Overseer main queue loop\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /overseer/queue...\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:99)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1468)\n\tat org.apache.solr.common.cloud.SolrZkClient$6.execute(SolrZkClient.java:256)\n\tat org.apache.solr.common.cloud.SolrZkClient$6.execute(SolrZkClient.java:253)\n\tat org.apache.solr.common.cloud.ZkCmdExecutor.retryOperation(ZkCmdExecutor.java:73) "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13881427",
            "date": "2014-01-24T21:06:19+0000",
            "content": "Cleaned up patch.\n\nWorking on:\n\n\tLimiting the length of tracking data structures.\n\tReturning more information for request status.\n\tTests.\n\n "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13881639",
            "date": "2014-01-25T03:36:37+0000",
            "content": "\n\tLimited the size of tracking data structures.\n\tShutdown the ThreadPoolExectutor.\n\tFixed tests\n\tOther changes - Naming, indentation, thread safety etc.\n\n\n\nIt's almost there.\n\nTODO:\n\n\tMore tests\n\tHandle CoreAdmin failures at OCP end a little better.\n\n "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13882480",
            "date": "2014-01-26T23:38:47+0000",
            "content": "Fixed an issue with task tracking.\nThe patch has failing tests as the CoreAdminHandler.shutdown() is called at the wrong place/time.\nWorking on correcting it. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13882565",
            "date": "2014-01-27T04:42:29+0000",
            "content": "Patch without the ThreadPool.\nFixed other issues - Exceptions in OCP and CoreAdminHandler.\nResponse for CoreAdmin status request now contains:\n\n\tresponseHeader if successful.\n\tException message if failure.\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13882621",
            "date": "2014-01-27T07:58:00+0000",
            "content": "Thanks Anshum.\n\n\n\tWhy is it called a taskQueue in CoreAdminHandler? There is no queueing happening here.\n\tWhy is the taskQueue defined as a Map<String, Map<String, TaskObject>>? It can simply be a Map<String, TaskObject>. The task object itself can contain a volatile status flag to indicate running/completed/failure.\n\tThe CoreAdminHandler.addTask with limit=true just removes a random (first?) entry if the limit is reached. It should remove the oldest entry instead.\n\tOverseerCollectionProcessor.requestStatus returns response with \u201csuccess\u201d even if requestid is found in \u201crunning\u201d or \u201cfailure\u201d map\n\tThe \u2018migrate\u2019 api doesn\u2019t use async core admin requests\n\tIn all places where synchronous calls have been replaced with waitForAsyncCallsToComplete calls, we need to ensure that the correct response messages are returned on failures. Right now, the waitForAsyncCallToComplete method returns silently on detecting failure.\n\tAlthough there is a provision to clear the overseer status maps by passing requestid=1, it is never actually called. When do you intend to call this api?\n\tI don\u2019t understand why we need three different maps for running/completed/failure for overseer collection processor. My comment #2 applies here too. We can store the status in the value bytes instead of keeping three different maps and moving the key around. What do you think?\n\n "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13882627",
            "date": "2014-01-27T08:19:07+0000",
            "content": "Why is it called a taskQueue in CoreAdminHandler? There is no queueing happening here.\nChanged it. Had that change on my machine before you mentioned \n\nWhy is the taskQueue defined as a Map<String, Map<String, TaskObject>>? It can simply be a Map<String, TaskObject>.\n I don\u2019t understand why we need three different maps for running/completed/failure for overseer collection processor. My comment #2 applies here too. We can store the status in the value bytes instead of keeping three different maps and moving the key around. What do you think? \nIt takes away the ability (or atleast makes it too complicated) to limit number of tasks in a particular state e.g. limiting storage of 50 completed tasks only.\n\nThe CoreAdminHandler.addTask with limit=true just removes a random (first?) entry if the limit is reached.\nIt removes the first element. Its a synchronized LinkedHashMap so the iterator preserves order and returns the first element.\n\nOverseerCollectionProcessor.requestStatus returns response with \u201csuccess\u201d even if requestid is found in \u201crunning\u201d or \u201cfailure\u201d map\nSuccess was supposed to mean that the task was found in a status map. It might actually make sense to change it. Thanks for the suggestion.\n\nAlthough there is a provision to clear the overseer status maps by passing requestid=1, it is never actually called. \nThe intention is for the user to explicitly call the API. There's no concept of a map/queue in zk that maintains insertion state.\nyou'd have to check it, order it and then delete the apt one every time the numChildren exceeds the limit. I thought it was best left to the user.\n\nWill upload a patch with the following:\n\n\tMigrate API to also use the ASYNC CoreAdmin requests.\n\tStore the failed tasks information from CoreAdmin async calls in case of Collection API requests.\n\tTests for\n\t\n\t\tmigratekey (and other calls) in ASYNC mode.\n\t\tFailing Collection API calls.\n\t\n\t\n\n "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13882641",
            "date": "2014-01-27T08:52:47+0000",
            "content": "Fixed the following:\n\n\tChanged the var name from Queue to Map.\n\tResponse structure from OCP async calls changed. Now it's:\n\n<status>\n  <state>running|failed|completed|notfound</state>\n  <msg>apt message</msg>\n</status>\n\n\n\n "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13884097",
            "date": "2014-01-28T13:04:05+0000",
            "content": "Have added the async functionality to migrate key but it seems to be running into issues.\n\nAdded test for async migrate key and fixed a few more things that I found while working on it. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13884191",
            "date": "2014-01-28T14:35:46+0000",
            "content": "\n\tMigrate key partially works. There are a couple of spots where the CoreAdmin async calls still fails as it doesn't find the async request at the Core. Have commented that single call out.\n\tAdded back Threadpoolexecutor as an instance var into CAH. Switched to using that and got tests to pass.\n\n "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13884442",
            "date": "2014-01-28T19:04:09+0000",
            "content": "Added more tests:\n\n\ttest for duplicate requestid in CollectionAPI call.\n\ttest for failed request (duplicate collection creation attempt) using async.\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13884478",
            "date": "2014-01-28T19:32:43+0000",
            "content": "Migrate key partially works. There are a couple of spots where the CoreAdmin async calls still fails as it doesn't find the async request at the Core. Have commented that single call out.\n\nI found the problem. Creation of the temp collection's leader on the target node in async mode fails because it is missing a collectShardResponse statement. It works fine in sync mode because we call collectShardResponses after the WaitForState call. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13884485",
            "date": "2014-01-28T19:37:02+0000",
            "content": "Also, the WaitForState call in migrate has not been made asynchronous yet. This is a long running call and a perfect candidate for the async feature. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13884505",
            "date": "2014-01-28T19:46:18+0000",
            "content": "Refactored OCP to use setupAsyncRequest  and completeAsyncRequest everywhere possible. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13884520",
            "date": "2014-01-28T19:56:03+0000",
            "content": "Fixed migrateKey async mode.\n\nThanks to Shalin for having a look at it.\nWill try and get WaitForState call to also be async next.\n\nThe latest code has :\n\n\tAsync mode at OCP and CoreAdmin level (using ThreadPool).\n\tRequest status APIs for both collection and CoreAdmin levels.\n\tPassing tests\n\n "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13885094",
            "date": "2014-01-29T07:16:22+0000",
            "content": "Got WaitForState to also be Async during migrateKey call. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13885162",
            "date": "2014-01-29T09:31:23+0000",
            "content": "Considering that the OCP itself might have long running tasks and it's currently single threaded, we don't want it to be blocked.\n\nI've introduced multi threading in the OCP using a local TPE. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13885303",
            "date": "2014-01-29T12:48:20+0000",
            "content": "I would like to have a separate issue for multi threading the OCP as it's pretty much a big task in itself. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13887175",
            "date": "2014-01-30T22:25:00+0000",
            "content": "Making OverseerCollectionProcessor multi-threaded. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13894839",
            "date": "2014-02-07T18:50:53+0000",
            "content": "I'd like to commit this feature to trunk next week if there are no objections. The multi-threading aspect will be handled by SOLR-5681. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13897306",
            "date": "2014-02-10T23:58:26+0000",
            "content": "Updated patch. There were quite a few conflicts and I think I fixed them.\nIt'd be good if whoever commits this has a final look at it. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13900152",
            "date": "2014-02-13T09:23:26+0000",
            "content": "I updated Anshum's latest patch.\n\n\tFixed javadoc errors reported by ant precommit\n\tAlso removed a TODO in CoreAdminHandler:811 because we now return full responses instead of just a status\n\n "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13920149",
            "date": "2014-03-04T22:49:18+0000",
            "content": "Patch updated to trunk (again). I'll re-run some tests and if all goes well, will commit it tomorrow. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13921363",
            "date": "2014-03-05T20:30:06+0000",
            "content": "Hey Anshum, this looks pretty good!\n\nI did a quick code review and dumped my comments below:\n\n\n\tDoesn't seem like this should be logged info level:\nlog.info(\"REQUESTSTATUS action invoked: \" + req.getParamString());\n\n\n\n\n\tWe should pull requestid into a constant - there already is one in CoreAdminParams:\nr.add(\"requestid\", (String) m.get(ASYNC));\n\n\n\n\n\tWe should clean this up:\n//    parallelHandlerThread.run();\n\n\n\n\n\tWe should clean this up?\n\n    } catch (Exception e) {\n      throw e;\n    }\n\n\n\n\n\n\n\tShouldn't we at least keep doing this in non cloud mode?\nrsp.setHttpCaching(false);\n\n\n\n\n\tWe should use constants here:\n\n  /**\n   * Handle \"REQUESTSTATUS\" action\n   */\n  protected void handleRequestActionStatus(SolrQueryRequest req, SolrQueryResponse rsp) {\n    SolrParams params = req.getParams();\n    String requestId = params.get(CoreAdminParams.REQUESTID);\n    log.info(\"Checking request status for : \" + requestId);\n\n    if (mapContainsTask(\"running\", requestId)) {\n      rsp.add(\"STATUS\", \"running\");\n    } else if(mapContainsTask(\"completed\", requestId)) {\n      rsp.add(\"STATUS\", \"completed\");\n      rsp.add(\"Response\", getMap(\"completed\").get(requestId).getRspObject());\n    } else if(mapContainsTask(\"failed\", requestId)) {\n      rsp.add(\"STATUS\", \"failed\");\n      rsp.add(\"Response\", getMap(\"failed\").get(requestId).getRspObject());\n    } else {\n      rsp.add(\"STATUS\", \"notfound\");\n      rsp.add(\"msg\", \"No task found in running, completed or failed tasks\");\n    }\n  }\n\n\n\n\n\n\n\tThe patch hits CoreAdminRequest but makes no real change.\n\n\n\n\n\tShould consider using ExecUtil.shutdown:\n\n  public void shutdown() {\n    if (parallelExecutor != null && !parallelExecutor.isShutdown())\n      parallelExecutor.shutdown();\n  }\n\n\n\n\n\n\n\tThe following should continue CoreContainer shutdown even if it throws an exception:\ncoreAdminHandler.shutdown();\n\n\n\n\n\tDoes this work nicely with the collections api solrj classes?\n\n\n\n\n\tIn TestRequestStatusCollectionAPI, doesn't following mean the test can pass in bad cases and miss asserts?\n\n    } catch (SolrServerException e) {\n      e.printStackTrace();\n    } catch (IOException e) {\n      e.printStackTrace();\n    }\n\n\n\n\n\n\n\tIs there any stress testing? I wonder about some of the promises in terms of request id's and the status api for example - can two clients not race creating the same id? Are there any tests that try and fire off a bunch of these async commands in parralel?\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13921372",
            "date": "2014-03-05T20:44:37+0000",
            "content": "On the following:\n\n* Shouldn't we at least keep doing this in non cloud mode? \nbq. rsp.setHttpCaching(false); \n\n\n\nWas that a mistake - even in cloud mode, shouldn't we continue to do this? "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13921988",
            "date": "2014-03-06T04:16:48+0000",
            "content": "Thanks for looking at it Mark.\nHere's a patch that addresses most of the stuff.\n\nrsp.setHttpCaching(false); \nI'd moved it but it makes sense to move it back (done).\n\nAbout 2 clients racing to create the same id, I don't think that would happen (if you're talking about the OCP creating conflicting id) as it's currently single threaded and at a given time only one task would get processed.\nI'll however look at it again to see if I'm missing something. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13922994",
            "date": "2014-03-06T20:13:20+0000",
            "content": "Mark Miller I haven't added anything for SolrJ so for now, it doesn't really support async calls. I am assuming that by collection API SolrJ calls you mean methods like \"CollectionAdminRequest.createCollection()\".\n\nAlso, I'm working on adding some stress tests i.e. something that fires multiple async requests. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13923489",
            "date": "2014-03-07T03:30:04+0000",
            "content": "I haven't added anything for SolrJ so for now, it doesn't really support async calls.\n\nLet's open a new issue for this. It'd be nice to add support in SolrJ sooner rather than later. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13923942",
            "date": "2014-03-07T15:07:19+0000",
            "content": "SolrJ calls you mean methods like \"CollectionAdminRequest.createCollection()\".\n\nRight - it can def come in a second issue, but it seems like just at least adding the async param is pretty low hanging fruit.  "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13923960",
            "date": "2014-03-07T15:34:08+0000",
            "content": "Mark Miller Sure, I'll add that and put up another patch. It's just that I wanted to get it into trunk sooner than later considering that the patch touches a reasonable points in the code, which makes it tricky to forward port every time. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13931503",
            "date": "2014-03-12T08:03:33+0000",
            "content": "Patch with SolrJ support and tests. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13932043",
            "date": "2014-03-12T17:27:36+0000",
            "content": "+1 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13934729",
            "date": "2014-03-14T08:01:20+0000",
            "content": "Commit 1577444 from Anshum Gupta in branch 'dev/trunk'\n[ https://svn.apache.org/r1577444 ]\n\nSOLR-5477: Async execution of OverseerCollectionProcessor tasks "
        },
        {
            "author": "Steve Davids",
            "id": "comment-13936004",
            "date": "2014-03-15T05:40:12+0000",
            "content": "This code manipulates the URL scheme in the OverSeerCollectionProcessor, this is not necessary and may cause issues for clients that want to run in ssl mode. You may want to consider dropping it:\n\n\nIndex: solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java\n===================================================================\n--- solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java\t(revision 1577773)\n+++ solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java\t(working copy)\n@@ -1826,8 +1826,6 @@\n           params.set(CoreAdminParams.COLLECTION, collectionName);\n           params.set(CoreAdminParams.SHARD, sliceName);\n           params.set(ZkStateReader.NUM_SHARDS_PROP, numSlices);\n-          String replica = zkStateReader.getBaseUrlForNodeName(nodeName);\n-          if (replica.startsWith(\"http://\")) replica = replica.substring(7);\n \n           setupAsyncRequest(async, requestMap, params, nodeName);\n \n@@ -2139,7 +2137,6 @@\n       params.set(\"qt\", adminPath);\n       sreq.purpose = 1;\n       String replica = zkStateReader.getBaseUrlForNodeName(nodeName);\n-      if (replica.startsWith(\"http://\")) replica = replica.substring(7);\n       sreq.shards = new String[] {replica};\n       sreq.actualShards = sreq.shards;\n       sreq.params = params;\n\n "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13936061",
            "date": "2014-03-15T07:33:39+0000",
            "content": "Thanks for pointing that out Steve.\nThis must have gotten in when I started working on this one i.e. before SOLR-3854 went in and just stayed as a result of a bad merge.\n\nI'll fix this up. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13936080",
            "date": "2014-03-15T08:01:18+0000",
            "content": "Fix for not modifying url scheme. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13936085",
            "date": "2014-03-15T08:09:33+0000",
            "content": "Commit 1577801 from Anshum Gupta in branch 'dev/trunk'\n[ https://svn.apache.org/r1577801 ]\n\nSOLR-5477: Fix URL scheme modification from an earlier commit for SOLR-5477. "
        },
        {
            "author": "Steve Davids",
            "id": "comment-13936377",
            "date": "2014-03-16T02:02:24+0000",
            "content": "You should drop the unnecessary assignment:\n\nString replica = zkStateReader.getBaseUrlForNodeName(nodeName);\n\n\n\non line 1829, making an unnecessary call out to zk for a value that isn't being used. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13936380",
            "date": "2014-03-16T02:27:50+0000",
            "content": "Commit 1577965 from Anshum Gupta in branch 'dev/trunk'\n[ https://svn.apache.org/r1577965 ]\n\nSOLR-5477: Removing an unwanted call to zk "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13938413",
            "date": "2014-03-17T21:29:04+0000",
            "content": "Commit 1578598 from Anshum Gupta in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1578598 ]\n\nSOLR-5477: Async execution of OverseerCollectionProcessor tasks (merged trunk r1577444, r1577801, r1577965) "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13939466",
            "date": "2014-03-18T16:53:41+0000",
            "content": "Line 222 OverseerCollectionProcessor \n\nfinal String asyncId = (message.containsKey(ASYNC) && message.get(ASYNC) != null) ? (String) message.get(ASYNC) : null;\n\nwe should use message.getStr() instead of typecasting to String \n\nLine 237\nThis code stores data in java serialization format. Can we do json serialization?\n\nhead.setBytes(SolrResponse.serializable(response)); "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13949485",
            "date": "2014-03-27T16:04:09+0000",
            "content": "Noble Paul This one doesn't really belong to this JIRA i.e. it wasn't added/changed in this patch but yes, we could look at changing this as a part of another issue. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13949488",
            "date": "2014-03-27T16:07:21+0000",
            "content": "OK, the diff somehow showed it as a part of the check in. If you are storing the data in json it should be good "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13949524",
            "date": "2014-03-27T16:28:43+0000",
            "content": "Noble Paul for now it's more of a string. SOLR-5886 would require more information to be stored. I'll use your suggestion on that one. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13982547",
            "date": "2014-04-27T23:25:41+0000",
            "content": "Close issue after release of 4.8.0 "
        }
    ]
}