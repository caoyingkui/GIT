{
    "id": "SOLR-5209",
    "title": "last replica removal cascades to remove shard from clusterstate",
    "details": {
        "affect_versions": "4.4",
        "status": "Resolved",
        "fix_versions": [
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Blocker",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "The problem we saw was that unloading of an only replica of a shard deleted that shard's info from the clusterstate. Once it was gone then there was no easy way to re-create the shard (other than dropping and re-creating the whole collection's state).\n\nThis seems like a bug?\n\nOverseer.java around line 600 has a comment and commented out code:\n// TODO TODO TODO!!! if there are no replicas left for the slice, and the slice has no hash range, remove it\n// if (newReplicas.size() == 0 && slice.getRange() == null) {\n// if there are no replicas left for the slice remove it",
    "attachments": {
        "SOLR-5209.patch": "https://issues.apache.org/jira/secure/attachment/12601077/SOLR-5209.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Christine Poerschke",
            "id": "comment-13756189",
            "date": "2013-09-02T17:43:05+0000",
            "content": "original clusterstate (extract)\n\nshards : {\n  \"shard1\":{ \"range\":\"...\", \"replicas\":{ { \"core\":\"collection1_shard1\" } } },\n  \"shard2\":{ \"range\":\"...\", \"replicas\":{ { \"core\":\"collection1_shard2\" } } }\n}\n\n\n\nactual clusterstate after UNLOAD of collection1_shard1\n\nshards : {\n  \"shard2\":{ \"range\":\"...\", \"replicas\":{ { \"core\":\"collection1_shard2\" } } }\n}\n\n\n\nexpected clusterstate after UNLOAD of collection1_shard1\n\nshards : {\n  \"shard1\":{ \"range\":\"...\", \"replicas\":{} },\n  \"shard2\":{ \"range\":\"...\", \"replicas\":{ { \"core\":\"collection1_shard2\" } } }\n}\n\n "
        },
        {
            "author": "Daniel Collins",
            "id": "comment-13756446",
            "date": "2013-09-03T08:25:19+0000",
            "content": "When I had a look at that code, I was confused why removeCore was even attempting to remove \"all empty pre allocated slices\"?  And it also tries to clean up collections, given we have the collections API to do that, why is a simple core unload going any further?  I can see that its \"nice\" and saves the user having to run the collections API to remove the collection, but isn't it potentially dangerous (like in this case) if we are second guessing what the user will do next?\n\nSay they are just removing all the replicas, and re-assigning them to different hosts?  They may want to leave the collection/shard ranges \"empty\" and then move the data to new hosts, and restart the cores to re-register them.  Yes, we could/should start the new ones before removing the old, but that's not enforced? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13756714",
            "date": "2013-09-03T15:51:24+0000",
            "content": "I think this deserves another look. We have the deleteshard API now which can be used to completely remove a slice from the cluster state. We should remove this trappy behaviour. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13756786",
            "date": "2013-09-03T17:00:06+0000",
            "content": "given we have the collections API to do that\n\nWe don't actually have the collections API to do that - it's simply a thin candy wrapper around SolrCore admin calls. Everything is driven by SolrCores being added or removed. There is work being done to migrate towards something where the collections API is actually large and in charge, but currently it's just a sugar wrapper. "
        },
        {
            "author": "Daniel Collins",
            "id": "comment-13756827",
            "date": "2013-09-03T17:46:14+0000",
            "content": "Ok, my bad, I wasn't clear enough.  At the user-level there is collections API and core API, and yes one is just a wrapper around the other.  But at the Overseer level, we seem to have various different sub-commands (not sure what the correct terminology for them is!): create_shard, removeshard, createcollection, removecollection, deletecore, etc.  I appreciate this is probably historical code, but since we have these other methods, it felt like deletecore was overstepping its bounds  now \n\nCould submit an extra patch, but wasn't sure of the historical nature of this code, hence just a comment first to get an opinion/discussion. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13756861",
            "date": "2013-09-03T18:16:51+0000",
            "content": "Right, but the sub commands are just the wrapper calls - except the shard commands - those are new. Th delete core one is mostly about cleanup ini remember right. \n\nThe problem is, the overseer and zk do not own the state. The individual cores do basically. Mostly that's due to historical stuff. We intend to change that, but it's no small feat. Until that is done, I think this is much trickier to get right than it looks.  "
        },
        {
            "author": "Christine Poerschke",
            "id": "comment-13867720",
            "date": "2014-01-10T11:09:32+0000",
            "content": "Hello. Just wanted to follow-up if we could proceed with this patch? To either reduce or completely remove the cascading behaviour UNLOAD currently has, between them DELETESHARD and DELETECOLLECTION should allow for shard and collection removal if that was to be required after UNLOAD-ing. \n\noriginal clusterstate\n\n\"collection1\" : {\n  shards : {\n    \"shard1\":{ \"range\":\"...\", \"state\" : \"active\", \"replicas\":{ { \"core\":\"collection1_shard1\" } } },\n    \"shard2\":{ \"range\":\"...\", \"state\" : \"active\", \"replicas\":{ { \"core\":\"collection1_shard2\" } } }\n  }, ...\n},\n\"collection2\" : {\n  ...\n}\n\n\n\ncurrent clusterstate after UNLOAD of collection1 shard1\n(The UNLOAD of the last shard1 replica cascade-triggered the removal shard1 itself.)\n\n\"collection1\" : {\n  shards : {\n    \"shard2\":{ \"range\":\"...\", \"state\" : \"active\", \"replicas\":{ { \"core\":\"collection1_shard2\" } } }\n  }, ...\n},\n\"collection2\" : {\n  ...\n}\n\n\n\nexpected clusterstate after UNLOAD of collection1 shard1\n(There are now no replica in shard1. If one wanted to get rid of the shard then DELETESHARD could be modified to allow removal of active shards without replicas (currently DELETESHARD only removes shards that are state=inactive or state=recovery or range=null).)\n\n\"collection1\" : {\n  shards : {\n    \"shard1\":{ \"range\":\"...\", \"state\" : \"active\", \"replicas\":{} },\n    \"shard2\":{ \"range\":\"...\", \"state\" : \"active\", \"replicas\":{ { \"core\":\"collection1_shard2\" } } }\n  }, ...\n},\n\"collection2\" : {\n  ...\n}\n\n\n\ncurrent clusterstate after UNLOAD of collection1 shard1 and UNLOAD of collection1 shard2\n(The UNLOAD of the last shard2 replica cascade-triggered the removal shard2 itself, and then the removal of the last shard cascade-triggered the removal of collection1 itself.)\n\n\"collection2\" : {\n  ...\n}\n\n\n\nexpected clusterstate after UNLOAD of collection1 shard1 and UNLOAD of collection1 shard2\n(There are now no replica in shard1 and shard2. If one wanted to get rid of shard1 and shard2 then DELETESHARD could be modified and used as per above. If one wanted to get rid of collection1 itself then DELETECOLLECTION could be used.)\n\n\"collection1\" : {\n  shards : {\n    \"shard1\":{ \"range\":\"...\", \"state\" : \"active\", \"replicas\":{} },\n    \"shard2\":{ \"range\":\"...\", \"state\" : \"active\", \"replicas\":{} }\n  }, ...\n},\n\"collection2\" : {\n  ...\n}\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13867762",
            "date": "2014-01-10T12:30:11+0000",
            "content": "What is the resolution on this?\n\n+1 to not remove the slice when the last replica is gone\n\nI can take this up if this is the proposed resolution "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13867790",
            "date": "2014-01-10T13:37:44+0000",
            "content": "This would need to be done in the new \"zk is truth\" mode, not in the legacy cloud mode. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13867797",
            "date": "2014-01-10T13:44:11+0000",
            "content": "I guess this should not matter because it would not break anything that people are doing. I would consider this a bug. Because creating a lost shard would be extremely hard because the 'range' info would be missing and you will end up with  a 'broken' cluster "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13867805",
            "date": "2014-01-10T14:02:08+0000",
            "content": "It's not a bug, the behavior was added explicitly and has been around for a long time. If you want to remove it, its got to be in the new mode. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13867810",
            "date": "2014-01-10T14:06:19+0000",
            "content": "Just a note: The new zk=truth mode is the future. The old mode is never going to be great. It's not possible. It was built in a halfway world, and it will always seems halfway funny. We need to embrace the zk=truth mode to make things nice. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13867840",
            "date": "2014-01-10T14:32:25+0000",
            "content": "Of course, if we really need to, we can remove this, add lots of warnings about the break, notes about the new apis that allow the same thing, etc. Because there is now an alternate way to get this behavior, that is not an out of this world idea.\n\nPersonally though, I'd so much rather see that energy put into the new zk=truth mode, it's required for so many things to work as we want. "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-13868054",
            "date": "2014-01-10T18:04:32+0000",
            "content": "FWIW, newer API actually won't allow you to do exactly what this does. The shard delete API (sensibly) explicitly checks if we have a null range or if we are not an active shard. This potentially lops out an active shard with an assigned range leaving the cluster state broken. I actually don't know if there's a way to get the cluster state back to a sensible state without modifying it by hand or recreating it altogether by nuking ZK state \u2013 when we tried bringing up the replica after this, it got assigned to a shard with an empty range..\n\nI guess if we plan to get to the ZK truth mode sometime soon, its fine to drop this patch. We mainly meant for this as an interim step to ensure someone doesn't accidentally burn their fingers.. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13868062",
            "date": "2014-01-10T18:11:34+0000",
            "content": "This behavior existed long before hash ranges, so I'd say that issue was not complete in this area.\n\nThe zk=truth mode can be started quite simply - its just a new config param we add and we can say some behaviors will change over time as we complete it. I think Shalin has already created an issue for it. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13868632",
            "date": "2014-01-11T03:08:54+0000",
            "content": "SOLR-5610 adds a the necessary property. I'm committing i right away "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14237820",
            "date": "2014-12-08T12:24:57+0000",
            "content": "This seems to be still relevant (in SliceMutator though rather than Overseer after the refactoring), 5.0 might be a good time to revisit this behaviour since the concerns above were more of back-compat.. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14237883",
            "date": "2014-12-08T14:11:36+0000",
            "content": "Yup, now is a good time to change this.  "
        },
        {
            "author": "ASF GitHub Bot",
            "id": "comment-14258367",
            "date": "2014-12-24T16:44:04+0000",
            "content": "GitHub user cpoerschke opened a pull request:\n\n    https://github.com/apache/lucene-solr/pull/118\n\n    SOLR-5209: last replica removal no longer cascades to remove shard from clusterstate\n\n    https://issues.apache.org/jira/i#browse/SOLR-5209\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/bloomberg/lucene-solr trunk-solr-5209\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/lucene-solr/pull/118.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #118\n\n\ncommit 90b1a394b50d4f456bfabab756cc64f28733e1a5\nAuthor: Christine Poerschke <cpoerschke@bloomberg.net>\nDate:   2014-12-24T12:07:56Z\n\n    SOLR-5209: last replica removal no longer cascades to remove shard from clusterstate\n\n    old behaviour:\n\n\tlast replica removal cascades to remove shard from clusterstate\n\tlast shard removal cascades to remove collection from clusterstate\n\n\n\n    new behaviour:\n\n\tlast replica removal preserves shard within clusterstate\n\tOverseerTest.testRemovalOfLastReplica added for replica removal logic\n\n\n\n    also:\n\n\tstrings such as \"collection1\" and \"state\" in OverseerTest replaced with variable or enum equvivalent\n\n\n\n "
        },
        {
            "author": "Christine Poerschke",
            "id": "comment-14258368",
            "date": "2014-12-24T16:46:44+0000",
            "content": "Replaced attached patch with Solr trunk pull request which also includes OverseerTest test case logic. "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14261246",
            "date": "2014-12-30T16:58:24+0000",
            "content": "I think this might fail BasicDistributedZkTest.testFailedCoreCreateCleansUp.. If we call core create, it happens to be the first core of the collection, and the core creation fails (due to say, a config issue) \u2013 the test currently verifies if the rollback happens by checking for the absence of the collection. We could obviously change the test, but in such a case, should we be removing the collection as well? (or disallow creation of a core for a non-existent collection \u2013 though that might be a bigger, disruptive end user change and not necessarily good).. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14261249",
            "date": "2014-12-30T17:03:55+0000",
            "content": "I think as part of making zk the truth we actually do want to prevent creation of cores that are not part of a zk collection.  "
        },
        {
            "author": "Christine Poerschke",
            "id": "comment-14277175",
            "date": "2015-01-14T16:33:47+0000",
            "content": "https://github.com/apache/lucene-solr/pull/118 now updated to remove the BasicDistributedZkTest.testFailedCoreCreateCleansUp test and adjust UnloadDistributedZkTest.testUnloadShardAndCollection and OverseerTest.testOverseerFailure tests.\n\nWould agree that longer-term core creation should not auto-create a collection. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-14284062",
            "date": "2015-01-20T17:25:47+0000",
            "content": "Mark Miller is this a blocker? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14284159",
            "date": "2015-01-20T18:22:34+0000",
            "content": "Not fully. It would just stink to have to wait until 6 to remove this ugly back compat behavior. I thought I'd have time to get to it last week or someone else might pick it up, but no such luck, so perhaps we just push it out.  "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-14743672",
            "date": "2015-09-14T15:30:40+0000",
            "content": "On IRC today:\n\n\n09:14 < yriveiro> Hi, I unloaded by accident the las replica of a shard in a\n                  collection\n09:14 < yriveiro> How can I recreate the shard?\n\n "
        },
        {
            "author": "Christine Poerschke",
            "id": "comment-15025453",
            "date": "2015-11-24T21:29:31+0000",
            "content": "Am in the process of updating/rebasing the patch for this (SOLR-5209) ticket here. SOLR-8338 is towards that, just replacing magic strings so that the actual test changes required for SOLR-5209 will then be simpler and clearer. "
        },
        {
            "author": "Christine Poerschke",
            "id": "comment-15063929",
            "date": "2015-12-18T13:05:19+0000",
            "content": "Attaching updated patch against trunk. "
        },
        {
            "author": "Christine Poerschke",
            "id": "comment-15069846",
            "date": "2015-12-23T16:40:12+0000",
            "content": "Mark Miller - if you have no objections then I will re-assign this ticket to myself with a view towards committing it in the first half of January or so, to trunk (and not branch_5x) in good time for the 6.0.0 release hopefully, after reviews of course.\n\nEveryone - the latest SOLR-5209.patch attachment contains the (small) change itself, a new OverseerTest.testRemovalOfLastReplica() test plus adjustments to existing tests. Reviews, comments, suggestions etc. welcome. Thank you. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15092375",
            "date": "2016-01-11T17:52:49+0000",
            "content": "Commit 1724098 from Christine Poerschke in branch 'dev/trunk'\n[ https://svn.apache.org/r1724098 ]\n\nSOLR-5209: Unloading or deleting the last replica of a shard now no longer cascades to remove the shard from the clusterstate. "
        },
        {
            "author": "Christine Poerschke",
            "id": "comment-15093696",
            "date": "2016-01-12T10:28:57+0000",
            "content": "Committed to trunk only (for 6.x releases) and deliberately not committed to branch_5x since this changes existing behaviour. "
        },
        {
            "author": "ASF GitHub Bot",
            "id": "comment-15094456",
            "date": "2016-01-12T18:42:01+0000",
            "content": "Github user cpoerschke commented on the pull request:\n\n    https://github.com/apache/lucene-solr/pull/118#issuecomment-171007162\n\n    SOLR-5209 committed to trunk yesterday (https://svn.apache.org/r1724098). "
        },
        {
            "author": "ASF GitHub Bot",
            "id": "comment-15094457",
            "date": "2016-01-12T18:42:02+0000",
            "content": "Github user cpoerschke closed the pull request at:\n\n    https://github.com/apache/lucene-solr/pull/118 "
        },
        {
            "author": "Noble Paul",
            "id": "comment-15145531",
            "date": "2016-02-12T23:17:52+0000",
            "content": "I guess we should get this into 5.5 . This looks like a serious enough problem that backward incompatibility should not be a problem "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15146134",
            "date": "2016-02-13T19:00:21+0000",
            "content": "I don't agree for the same reasons we have already discussed.\n\nWe reserved some leeway for 5x to fix things in this vein, but we didn't do any of it. To just make this large change in behavior out of the blue in a 5.5 release doesn't make a lot of sense to me. Best done in the 6 release. "
        }
    ]
}