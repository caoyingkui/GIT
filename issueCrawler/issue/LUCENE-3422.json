{
    "id": "LUCENE-3422",
    "title": "IndeIndexWriter.optimize() throws FileNotFoundException and IOException",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Incomplete",
        "status": "Resolved"
    },
    "description": "I am using lucene 3.0.2 search APIs for my application. \nIndexed data is about 350MB and time taken for indexing is 25 hrs. Search indexing and Optimization runs in two different threads. Optimization runs for every 1 hour and it doesn't run while indexing is going on and vice versa. When optimization is going on using IndexWriter.optimize(), FileNotFoundException and IOException are seen in my log and the index file is getting corrupted, log says\n\n\n1. java.io.IOException: No sub-file with id _5r8.fdt found \n[The file name in this message changes over time (_5r8.fdt, _6fa.fdt, _6uh.fdt, ..., _emv.fdt) ]\n\n2. java.io.FileNotFoundException: /local/groups/necim/index_5.3/index/_bdx.cfs (No such file or directory)  \n\n3. java.io.FileNotFoundException: /local/groups/necim/index_5.3/index/_hkq.cfs (No such file or directory)\n\tStack trace: java.io.IOException: background merge hit exception: _hkp:c100->_hkp _hkq:c100->_hkp _hkr:c100->_hkr _hks:c100->_hkr _hxb:c5500 _hx5:c1000 _hxc:c198\n84 into _hxd [optimize] [mergeDocStores]\n       at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:2359)\n       at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:2298)\n       at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:2268)\n       at com.telelogic.cs.search.SearchIndex.doOptimize(SearchIndex.java:130)\n       at com.telelogic.cs.search.SearchIndexerThread$1.run(SearchIndexerThread.java:337)\n       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n       at java.lang.Thread.run(Thread.java:662)\nCaused by: java.io.FileNotFoundException: /local/groups/necim/index_5.3/index/_hkq.cfs (No such file or directory)\n       at java.io.RandomAccessFile.open(Native Method)\n       at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)\n       at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput$Descriptor.<init>(SimpleFSDirectory.java:76)\n       at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.<init>(SimpleFSDirectory.java:97)\n       at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.<init>(NIOFSDirectory.java:87)\n       at org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:67)\n       at org.apache.lucene.index.CompoundFileReader.<init>(CompoundFileReader.java:67)\n       at org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:114)\n       at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:590)\n       at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:616)\n       at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4309)\n       at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3965)\n       at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:231)\n       at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:288)",
    "attachments": {
        "delete.log": "https://issues.apache.org/jira/secure/attachment/12593248/delete.log"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2011-09-08T14:48:30+0000",
            "content": "Are you using NoLockFactory in your Directory?  Is this all on a local filesystem?\n\nCan you turn on IndexWriter's infoStream and post the resulting output?\n\nCan you try with Lucene 3.0.3 (just in case you're hitting an already fixed bug)? ",
            "author": "Michael McCandless",
            "id": "comment-13100363"
        },
        {
            "date": "2013-05-12T14:25:51+0000",
            "content": "I have the similar problems in 3.1.0, likewise here and LUCENE-1638. I believe there's a bug in lucene synchronization/multi threading execution model during merge.\n\nI use the Hibernate Search with exclusive index usage enabled, what means that the same IndexWriter is used constantly per index dir and only closed for some reasons. On the other hand the IndexWriter uses ConcurrentMergeScheduler by default. I occasionally have FileNotFoundException telling me that some segment files are not found (FNFE).\n\nWhen I look into the implementation I can see in merge:\n\nvoid merge() {\n  mergeInit(merge);\n  mergeMiddle(merge);\n  mergeSuccess(merge);\n  // is the problem here (?)\n  mergeFinish(merge);\n}\n\n\n\nThe merge() method is not synchronized at all. I believe that in first three lines (before \"is the problem here?\" line) the merge is started and scheduled, and can do some job (like remove the segment files). However, the segmentInfo update is only done in mergeFinish() method (where it waits for all merges and updates segmentInfo-s then). If in the \"is the problem here?\" line other thread invokes doFlush() (which is synchronized) we have the IndexWriter in the partial or completely done merge (some files are removed), but it has still outdated segmentsInfo, which will be updated for a while in mergeFinish().\n\nThis might be a wrong interpretation because I didn't thoroughly review the code, but the error is there. It might occasionally produce FNFE on doFlush() because of outated segmentInfos regarding the current directory state in filesystem (this has been verified). ",
            "author": "l0co",
            "id": "comment-13655545"
        },
        {
            "date": "2013-05-12T19:58:43+0000",
            "content": "An ongoing merge won't remove files from other segments (eg a flushed segment) so I don't think that alone can lead to FNFE.\n\nCan you give more details about how you're hitting FNFEs? ",
            "author": "Michael McCandless",
            "id": "comment-13655626"
        },
        {
            "date": "2013-05-12T22:04:47+0000",
            "content": "Sure, this can happen because of other reasons (might be a bug in Hibernate Search?). This works like this:\n\n 1. I have IndexWriter writing to the index in Hibernate Search exclusive mode (it creates Workspace with single IndexWriter which is not closed/re-created on each usage, but constantly opened) with RAM flush threshold=2MB.\n 2. IndexWriter has concurrent merge scheduler by default.\n 3. I'm writing to the index using application UI and on the other window I'm observing the index directory.\n 4. After each write (entity save) new bunch of XXX.* files is created (_13., _14. etc)\n 5. After some time these files dissapears from the directory, for example I have 13,14,15,16,17,18,19 files and after the merge  process I have only 18,19 and the rest dissapears.\n 6. 5. effect happens during the IndexWriter usage - when I save entitiy to the database again.\n 7. Sometimes in this scenario I have FNFE.\n 8. I caught the error with breakpoint and I see that during FNFE the IndexWriter has segmentInfos corresponded to the files that already dissapeared from the index directory in current index writer usage (ie. in the directory there are 18,19 files but the segmentInfos shows all 13,14,15,16,17,18,19).\n 9. So, I suppose that when the writer has been invoked, the merge thread has removed these files, but the (another, concurrent) write thread still sees them.\n 10. This didn't happen (by now) when I switched to serial merge scheduler. ",
            "author": "l0co",
            "id": "comment-13655651"
        },
        {
            "date": "2013-05-15T12:04:36+0000",
            "content": "If you are keeping an IndexWriter open across multiple changes, then you should not see a new segment (set of _N.* files) after each entity save, i.e. IndexWriter instead should be buffering 2MB worth of changes and only then flushing a new segment.\n\nWhen a merge completes, it's only after updating the SegmentInfos that it goes and deletes files from the directory.\n\nIs it possible you accidentally have two IndexWriters open on the same directory?  Are you changing the LockFactory used by the Directory (the purpose of locking is to prevent two IndexWriters on one directory). ",
            "author": "Michael McCandless",
            "id": "comment-13658277"
        },
        {
            "date": "2013-05-15T12:08:08+0000",
            "content": "is this on a nfs filesystem? ",
            "author": "Robert Muir",
            "id": "comment-13658280"
        },
        {
            "date": "2013-05-16T09:00:35+0000",
            "content": "ext3, I'm gonna check if there possible to have more IndexWriter-s to the same dir opened ",
            "author": "l0co",
            "id": "comment-13659366"
        },
        {
            "date": "2013-05-16T10:18:10+0000",
            "content": "1. Files are immediately created in index directory, because the HS Workspace makes commit after each work.\n2. There's only one IndexWriter using given index directory (for both update and merge)\n\nNow, I've made some concurrent tests from which it looks that the concurrent model of merge is fine. This is probably the other problem on my side. ",
            "author": "l0co",
            "id": "comment-13659413"
        },
        {
            "date": "2013-07-19T20:12:34+0000",
            "content": "Unfortunately I currently have the same problem with SerialMergeScheduler, but I've done a lot of changes to the Hibernate Search because it didn't fit my requirements (but the Reporter probably didn't). Within few days I need to debug this, so I'm gonna put the feedback here. I currently have the detailed log of operations. Everything behaves like I said before, after few index updates some files dissapear and then I always have FNFE. I'm putting the log into attachments. ",
            "author": "l0co",
            "id": "comment-13714025"
        },
        {
            "date": "2013-07-19T20:48:57+0000",
            "content": "There's a very unusual thing that I wouldn't be probably able to debug. Maybe someone who knows something more about lucene will give me a clue...\n\nThe behavior in simple steps is following (I'm repeating this to be precise for this comment).\n\n 1. I have new empty index\n 2. I'm writing the document to the index, I have some files (_1.*)\n 3. I'm updating the same document to the index, I have some more files (_1., _2.)\n 4. After repating few times step 3 (eg. I have _1,_2,_3,_4,_5,_6 files), it decides to do something on the commit and I have only eg. _7.* files in the index after this.\n\nA. First the question: is the \"something\" from the 4th step a merge operation?\n\nNow - why I wouldn't be probably able to find easily the problem. I was working almost whole day on my computer, after 8 hrs of work I started to get the problem. There was only one thing required to do to trigger this situation.\n\n 1a. Step 1\n 2a. Step 2\n 3a. Step 3 \n 4a. Step 3 again <- always error (index files _1.?,_2.? after two steps were merged into _3.? in 3a and I always had the error in 4a).\n\nThe computer was of course heavily exhausted after these hours of working, memory fragmented etc. a lot of compile/start/stop tomcat steps were perfomed. In the meantime I needed few times to make a hard kill of tomcat process.\n\nThen I had a break for few hours and came now in the evening and I'm testing what's going on. The system is clean and freshly started. Now I can perform:\n\n 1b. Step 1\n 2b. Step 2\n 3b. Step 3\n 4b-10b. Step 3\n ~11b. Step 3 - after I have a lot of files (eg. _1.?,_2.?,_3.?,_4.?,_5.?,_6.?,_7.?,_8.?) it only now decides to \"merge\" everything into a single bunch of files\n 12b. Step 3 - no FNFE error \n\nAnd here the other questions:\n\nB. What can be the difference before morning and evening execution? Why previously it decided to run merge just on second update, and now it makes it after 8-10 update?\nC. Any clue why I don't have the FNFE now?\n\nPlus what I checked:\n\n1. There's no way of having the other tomcat working in the background during the tests, because tomcat port would be blocked.\n2. I double checked and I always have only a single instance of IndexWriter, which is reused across all testing requests. Moreover it always uses the same sequential task executor (all write operations are serializable and performed in a separate thread one by one).\n\nSorry for long story, but this looks amazing. ",
            "author": "l0co",
            "id": "comment-13714053"
        },
        {
            "date": "2013-07-19T21:12:52+0000",
            "content": "Oh, god, I've found it. This is really funny \n\nThis has been done by LUKE !!!\n\nI was checking what happens in the index in the meanwhile in LUKE and I didn't realize that this app not only READS the index, but also WRITES it - eg. does the merge.\n\nSorry for bothering  But after today magic in my soft I've really been frustrated.\n\nFinally, there's no any bug in lucene or in my code. The Reported probably has either the other process having access to the index, or other IndexWriter. ",
            "author": "l0co",
            "id": "comment-13714079"
        }
    ]
}