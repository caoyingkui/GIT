{
    "id": "LUCENE-7371",
    "title": "BKDReader could compress values better",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [
            "6.2",
            "7.0"
        ],
        "priority": "Minor",
        "status": "Closed",
        "type": "Bug"
    },
    "description": "For compressing values, BKDReader only relies on shared prefixes in a block. We could probably easily do better. For instance there are only 256 possible values for the first byte of the dimension that the values are sorted by, yet we use a block size of 1024. So by using something simple like run-length compression we could save 6 bits per value on average.",
    "attachments": {
        "LUCENE-7371.patch": "https://issues.apache.org/jira/secure/attachment/12816426/LUCENE-7371.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15364291",
            "author": "Adrien Grand",
            "date": "2016-07-06T13:21:17+0000",
            "content": "Here is a patch. On the IndexAndSearchOpenStreetMaps benchmark, it saves 9.7% storage (577MB -> 521B). The \"distance\" and \"poly 5\" benchmarks report the same response times but the \"box\" benchmark is about 4% slower. "
        },
        {
            "id": "comment-15372527",
            "author": "Adrien Grand",
            "date": "2016-07-12T08:31:29+0000",
            "content": "I first thought the issue was with inlining since the methods have many arguments and I had made them bigger, but it turned out that the main issue was branch misprediction due the use of vints for encoding the run length since runs are almost alternatively less/greater than 127 (the boundary for 1-2 bytes with vints). So I capped the run length to 256 in order to be able to use one byte for run lengths all the time and things are now faster with compression (about 75.1 QPS on master and 78.2 QPS with the patch, a 4% improvement). Disk savings are similar to what they were with the previous iteration of the patch since the index is now 522MB on disk vs. 521MB with the previous iteration of the patch. "
        },
        {
            "id": "comment-15372803",
            "author": "Michael McCandless",
            "date": "2016-07-12T12:34:03+0000",
            "content": "This is a nice optimization!  Patch looks good!\n\nThe BKDWriter change to pick which dimension to apply the run-length coding to is best effort right?  Because, you could have a dim with fewer unique leading suffix bytes, but a larger delta between first and last values?  But it would take quite a bit more work at indexing time to figure it out ... maybe add a comment explaining this tradeoff?  It seems likely the \"min delta\" approach should work well in practice, but have you tried with the slow-but-correct approach to verify?\n\nAlso, I noticed TestBackwardsCompatibility seems not to test points!  I'll go fix that ... "
        },
        {
            "id": "comment-15372880",
            "author": "Adrien Grand",
            "date": "2016-07-12T13:33:24+0000",
            "content": "Good point, here is an updated patch. I am getting the following indexing times, which suggests that it does not hurt:\n\n278.064423505 sec to index part 0 // master\n270.492947789 sec to index part 0 // patch\n\n\n\nThe index size is also unchanged, which was expected since the previous heuristic should be equivalent with dense data.\n\nAlso, I noticed TestBackwardsCompatibility seems not to test points! I'll go fix that ...\n\nOuch, good catch. Thanks! "
        },
        {
            "id": "comment-15372981",
            "author": "Michael McCandless",
            "date": "2016-07-12T14:43:47+0000",
            "content": "+1, great! "
        },
        {
            "id": "comment-15373129",
            "author": "ASF subversion and git services",
            "date": "2016-07-12T15:58:54+0000",
            "content": "Commit 866398bea67607bcd54331a48736e6bdb94a703d in lucene-solr's branch refs/heads/master from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=866398b ]\n\nLUCENE-7371: Better compression of values in Lucene60PointsFormat. "
        },
        {
            "id": "comment-15373137",
            "author": "ASF subversion and git services",
            "date": "2016-07-12T16:03:19+0000",
            "content": "Commit 1f446872aa9346c22643d0fb753ec42942b5a4d2 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1f44687 ]\n\nLUCENE-7371: Better compression of values in Lucene60PointsFormat. "
        },
        {
            "id": "comment-15373139",
            "author": "ASF subversion and git services",
            "date": "2016-07-12T16:04:53+0000",
            "content": "Commit 1a6df249f91ca9f4dab792c48f5965f3388f1776 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1a6df24 ]\n\nLUCENE-7371: Fix CHANGES entry. "
        },
        {
            "id": "comment-15373140",
            "author": "ASF subversion and git services",
            "date": "2016-07-12T16:04:54+0000",
            "content": "Commit b54d46722b36f107edd59a8d843b93f5727a9058 in lucene-solr's branch refs/heads/master from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b54d467 ]\n\nLUCENE-7371: Fix CHANGES entry. "
        },
        {
            "id": "comment-15373141",
            "author": "Adrien Grand",
            "date": "2016-07-12T16:05:16+0000",
            "content": "Thanks Mike! "
        },
        {
            "id": "comment-15382404",
            "author": "Adrien Grand",
            "date": "2016-07-18T15:00:54+0000",
            "content": "The benchmarks are reporting interesting changes, some seem to perform slightly faster now, like IntNRQ (http://people.apache.org/~mikemccand/lucenebench/IntNRQ.html) or the geo3d distance filter (http://people.apache.org/~mikemccand/geobench.html#search-distance) but some others seem to perform a bit slower like the 10-gon filter (http://people.apache.org/~mikemccand/geobench.html#search-poly_10) or the 10 nearest points (http://people.apache.org/~mikemccand/geobench.html#search-nearest_10). The fact that it is not consistently slower or faster is due to the distribution of points in the blocks that need to be read I think (the more unique leading bytes, the more expensive the read). Given that the slow down is not general to all benchmarks and that the size reduction is significant I don't think this should be reverted, but let me know if you think otherwise. (For the record many benchmarks look slower on July 17th but I don't think this is related to this change, for instance even phrases got slower http://people.apache.org/~mikemccand/lucenebench/Phrase.html) "
        },
        {
            "id": "comment-15382452",
            "author": "Robert Muir",
            "date": "2016-07-18T15:23:46+0000",
            "content": "I think Michael McCandless may have upgraded his operating system. "
        },
        {
            "id": "comment-15382455",
            "author": "Michael McCandless",
            "date": "2016-07-18T15:25:46+0000",
            "content": "Oh sorry, I upgraded the Linux kernel from 4.4 -> 4.6.4 on 7/17!  I'll add an annotation. "
        },
        {
            "id": "comment-15439058",
            "author": "Michael McCandless",
            "date": "2016-08-26T14:00:51+0000",
            "content": "Bulk close resolved issues after 6.2.0 release. "
        }
    ]
}