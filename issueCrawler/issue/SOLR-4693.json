{
    "id": "SOLR-4693",
    "title": "Create a collections API to delete/cleanup a shard",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.4",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "A collections API that unloads all replicas of a given shard and then removes it from the cluster state.\n\nIt will remove only those shards which are INACTIVE or have no range (created for custom sharding)\n\nAmong other places, this would be useful post the shard split call to manage the parent/original shard.",
    "attachments": {
        "SOLR-4693.patch": "https://issues.apache.org/jira/secure/attachment/12578705/SOLR-4693.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Anshum Gupta",
            "id": "comment-13631607",
            "date": "2013-04-15T09:36:48+0000",
            "content": "Working patch, minus the test.\nWill add the tests ASAP. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13631608",
            "date": "2013-04-15T09:38:45+0000",
            "content": "Also, here's how the API call looks like:\nhttp://<host>/solr/admin/collections?action=DELETESHARD&shard=<shard_name>&name=<collection_name>\n\nThis only allows for deletion of inactive slices to maintain integrity.\nHave added a todo to perhaps allow deletion of active slices as long as the HashRange for that slice is serviced by another active slice. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13631752",
            "date": "2013-04-15T14:33:15+0000",
            "content": "Have added a todo to perhaps allow deletion of active slices as long as the HashRange for that slice is serviced by another active slice.\n\nAnd we should allow deletion if the slice has no hash range (i.e. custom sharding)? "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13631764",
            "date": "2013-04-15T14:54:04+0000",
            "content": "And we should allow deletion if the slice has no hash range (i.e. custom sharding)?\n\nMakes sense, will add that as a todo. But for now, (primarily for shard split cleanup) we'll just let a user delete a Slice as long as the Slice is inactive. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13668362",
            "date": "2013-05-28T15:17:55+0000",
            "content": "Patch with the test.\nAs of now the API allows for the deletion of:\n1. INACTIVE Slices\n2. Slices which have no Range (Custom Hashing).\n\nWe should cleanup the shard<->slice confusion and stick to something for all of SolrCloud. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13670552",
            "date": "2013-05-30T18:12:34+0000",
            "content": "Thanks Anshum.\n\nA few comments:\n\n\tCan we use \"collection\" instead of \"name\" just like we use in splitshard?\n\tThe following code will throw an exception for a shard with no range (custom hashing use-case). Also it allows deletion of slices in construction state going against the error message.\n\n    // For now, only allow for deletions of Inactive slices or custom hashes (range==null).\n    // TODO: Add check for range gaps on Slice deletion\n    if (!slice.getState().equals(Slice.INACTIVE) && slice.getRange() != null) {\n      throw new SolrException(ErrorCode.BAD_REQUEST,\n          \"The slice: \" + slice.getName() + \" is not currently \"\n          + slice.getState() + \". Only inactive (or custom-hashed) slices can be deleted.\");\n    }\n\n\n\tThe \"deletecore\" call to overseer is redundant because it is also made by the CoreAdmin UNLOAD action.\n\tCan we re-use the code between \"deletecollection\" and \"deleteshard\"? The collectionCmd code checks for \"live\" state as well.\n\tIn DeleteSliceTest, after setSliceAsInactive(), we should poll the slice state until it becomes inactive or until a timeout value instead of just waiting for 5000ms\n\tDeleteSliceTest.waitAndConfirmSliceDeletion is wrong. It does not actually use the counter variable. Also, cloudClient.getZkStateReader().getClusterState() doesn't actually force refresh the cluster state\n\tWe should fail with appropriate error message if there were nodes which could not be unloaded. Perhaps a separate \"deletecore\" call is appropriate here?\n\tDo we know what would happen if such a \"zombie\" node comes back up? We need to make sure it cleans up properly.\n\n "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13694566",
            "date": "2013-06-27T08:40:42+0000",
            "content": "Thanks for the feedback Shalin Shekhar Mangar.\nThe comments are w.r.t a patch that's almost complete, shall put it up in a few hours hopefully.\n\nCan we use \"collection\" instead of \"name\" just like we use in splitshard?\nSure, changed. Actually, we could look at moving all APIs to use \"collection\" with 5.0 perhaps.\n\nFixed the exception to have an OR (which short circuits and never ends up being an NPE)\n\nCan we re-use the code between \"deletecollection\" and \"deleteshard\"? The collectionCmd code checks for \"live\" state as well.\nWe 'can' but I feel we shouldn't as shard deletion is specific to a Slice in a collection, the collection deletes all Slices. I think it'd be an overkill.\n\nDeleteSliceTest.waitAndConfirmSliceDeletion is wrong. It does not actually use the counter variable. Also, cloudClient.getZkStateReader().getClusterState() doesn't actually force refresh the cluster state\nThe counter variable is incremented. The force refresh bit is fixed.\n\nIn DeleteSliceTest, after setSliceAsInactive(), we should poll the slice state until it becomes inactive or until a timeout value instead of just waiting for 5000ms\nDone.\n\nWe should fail with appropriate error message if there were nodes which could not be unloaded. Perhaps a separate \"deletecore\" call is appropriate here?\nIsn't this opposite of what you say in #3?\n\nDo we know what would happen if such a \"zombie\" node comes back up? We need to make sure it cleans up properly.\nNot sure of that behaviour. Wouldn't that be as good as a random shard coming up with an invalid/non-existent state? Do we currently know what would happen in that case, leaving the delete API aside?  Will test it out anyways. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13695270",
            "date": "2013-06-28T05:40:05+0000",
            "content": "We 'can' but I feel we shouldn't as shard deletion is specific to a Slice in a collection, the collection deletes all Slices. I think it'd be an overkill.\n\nI was thinking of a method:\n\nprivate void sliceCmd(ClusterState clusterState, ModifiableSolrParams params, String stateMatcher, Slice slice)\n\n\nwhich can be refactored out of collectionCmd and used for delete shard as well.\n\nbq. We should fail with appropriate error message if there were nodes which could not be unloaded. Perhaps a separate \"deletecore\" call is appropriate here?\nIsn't this opposite of what you say in #3?\n\nWell, no. Look at what delete collection does in OverseerCollectionProcessor. It sends a collectionCmd to unload cores and then to make sure that a deleted collection does not ever come back, it makes a call to Overseer to remove the collection from ZK. We can do the same thing (and introduce a delete slice in overseer) or we could track the hosts on which the core unload failed and call Overseer's deletecore api directly. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13697915",
            "date": "2013-07-02T16:07:20+0000",
            "content": "I guess I've integrated all changes that seemed fine to me. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13699067",
            "date": "2013-07-03T15:09:59+0000",
            "content": "\n\tI changed Overseer action and methods to deleteShard instead of deleteSlice, sliceCmd to shardCmd and DeleteSliceTest to DeleteShardTest (and related changes inside the test as well). I know it is confusing but we already have createshard and updateshardstate in Overseer and I didn't want to add more inconsistency.\n\tIn CollectionHandler.handleDeleteShardAction, I removed the name == null check because we used params.required().get which ensures that name can never be null. The createcollection api also makes the same mistake \n\tThe Overseer.deleteSlice/deleteShard was not atomic so I changed the following:\n\nMap<String, Slice> newSlices = coll.getSlicesMap();\n\n\nto\n\nMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>(coll.getSlicesMap());\n\n\n\tAdded a wait loop to OCP.deleteShard like the one in delete collection\n\tFixed shardCount and sliceCount in DeleteShardTest constructor\n\tAdded a break to the wait loop inside DeleteShardTest.setSliceAsInactive and added a force update cluster state. Also an exception in if (!transition) is created but never thrown.\n\tRemoved redundant assert in DeleteShardTest.doTest because it can never be triggerred (because an exception is thrown from the wait loop in setSliceAsInactive)\n\nassertEquals(\"Shard1 is not inactive yet.\", Slice.INACTIVE, slice1.getState());\n\n\n\tAdded a connection and read timeout to HttpSolrServer created in DeleteShardTest.deleteShard\n\tI also took this opportunity to remove the timeout hack I had added to CollectionHandler for splitshard and had forgotten to remove it\n\tMoved zkStateReader.updateClusterState(true) inside the wait loop of DeleteShardTest.confirmShardDeletion\n\tRemoved extra assert in DeleteShardTest.confirmShardDeletion for shard2 to be not null.\n\n\n\nI'll commit this tomorrow if there are no objections. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13699081",
            "date": "2013-07-03T15:31:09+0000",
            "content": "I'll just go through the patch tonight. Also, here are some related JIRAs I created:\nMake the use of Slice and Shard consistent across the code and document base https://issues.apache.org/jira/browse/SOLR-4998 "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13699742",
            "date": "2013-07-04T03:52:22+0000",
            "content": "Looks good to me other than a small suggestion.\nYou may want to have an overloaded handleResponse function that also accepts TIMEOUT and let all the default not bother to pass it. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13699757",
            "date": "2013-07-04T04:27:54+0000",
            "content": "Patch with an overloaded CollectionHandler.handleResponse method which accepts timeout.\n\nThanks Anshum. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13699772",
            "date": "2013-07-04T05:04:23+0000",
            "content": "Commit 1499655 from shalin@apache.org\n[ https://svn.apache.org/r1499655 ]\n\nSOLR-4693: A \"deleteshard\" collections API that unloads all replicas of a given shard and then removes it from the cluster state. It will remove only those shards which are INACTIVE or have no range (created for custom sharding). "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13699774",
            "date": "2013-07-04T05:05:47+0000",
            "content": "Commit 1499656 from shalin@apache.org\n[ https://svn.apache.org/r1499656 ]\n\nSOLR-4693: A 'deleteshard' collections API that unloads all replicas of a given shard and then removes it from the cluster state. It will remove only those shards which are INACTIVE or have no range (created for custom sharding). "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13699776",
            "date": "2013-07-04T05:10:01+0000",
            "content": "Thanks Anshum! "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13700325",
            "date": "2013-07-04T19:48:54+0000",
            "content": "Shalin Shekhar Mangar looks like you missed a few changes that you put up in your list above.\nRenaming of Slice->Shard stuff primarily.\n\nI'll fix those as a part of SOLR-4998. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13700437",
            "date": "2013-07-05T03:42:13+0000",
            "content": "Yes, there is sliceCmd in Overseer and some local variables but those were there in the patch that I had posted.\n\nThanks for opening SOLR-4998 "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13716851",
            "date": "2013-07-23T18:38:46+0000",
            "content": "Bulk close resolved 4.4 issues "
        }
    ]
}