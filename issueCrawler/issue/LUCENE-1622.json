{
    "id": "LUCENE-1622",
    "title": "Multi-word synonym filter (synonym expansion at indexing time).",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "New Feature",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Duplicate",
        "status": "Resolved"
    },
    "description": "It would be useful to have a filter that provides support for indexing-time synonym expansion, especially for multi-word synonyms (with multi-word matching for original tokens).\n\nThe problem is not trivial, as observed on the mailing list. The problems I was able to identify (mentioned in the unit tests as well):\n\n\n\tif multi-word synonyms are indexed together with the original token stream (at overlapping positions), then a query for a partial synonym sequence (e.g., \"big\" in the synonym \"big apple\" for \"new york city\") causes the document to match;\n\n\n\n\n\tthere are problems with highlighting the original document when synonym is matched (see unit tests for an example),\n\n\n\n\n\tif the synonym is of different length than the original sequence of tokens to be matched, then phrase queries spanning the synonym and the original sequence boundary won't be found. Example \"big apple\" synonym for \"new york city\". A phrase query \"big apple restaurants\" won't match \"new york city restaurants\".\n\n\n\nI am posting the patch that implements phrase synonyms as a token filter. This is not necessarily intended for immediate inclusion, but may provide a basis for many people to experiment and adjust to their own scenarios.",
    "attachments": {
        "synonyms.patch": "https://issues.apache.org/jira/secure/attachment/12406674/synonyms.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-04-28T18:30:16+0000",
            "content": "Token filter implementing synonyms. Java 1.5 is required to compile it (I left generics for clarity; if folks really need 1.4 compatibility they can be easily removed of course). ",
            "author": "Dawid Weiss",
            "id": "comment-12703782"
        },
        {
            "date": "2009-04-28T18:49:40+0000",
            "content": "I'll shortly cite my experiences mentioned on the list.\n\n\n\tInjecting \"synonym group id\" token instead of all tokens for all synonyms in group is a big win with index size and saves you from matching for \"big\". It also plays better with highlighting (still had to rewrite it to handle all corner cases).\n\tProperly handling multiword synonyms only on index-side is impossible, you have to dabble in query rewriting (even then low-probability corner cases exist, and you might find extra docs).\n\tQuery expansion is the only absolutely clear way to have multiword synonyms with current Lucene, but it is impractical on any adequate synonym dictionary.\n\tThere is a possible change to the way Lucene indexes tokens+positions to enable fully proper multiword synonyms (with index+query rewrite approach) - adding a notion of 'length' or 'span' to a token, this length should play together with positionIncrement when calculating distance between tokens in phrase/spannear queries.\n\n ",
            "author": "Earwin Burrfoot",
            "id": "comment-12703790"
        },
        {
            "date": "2010-05-24T12:34:52+0000",
            "content": "Here's the dev thread that lead to this issue, for context:\n\n  http://www.lucidimagination.com/search/document/fde6d4b979481398/synonym_filter_with_support_for_phrases\n\nI think the syn filter here takes generally the same approach as\nSolr's (now moved to modules/analyzer in trunk) SynonymFilter, ie\noverlapping words as the expanded synonyms unwind?  Are there salient\ndifferences between the two?  Maybe we can merge them and get best of\nboth worlds?\n\nThere are tricky tradeoffs of index time vs search time \u2013 index time\nis less flexible (you must re-index on changing them) but better\nsearch perf (OR in a TermQuery instead of expanding to many\nPhraseQuerys); index time is better scoring (the IDF is \"true\" if the\nsyn is a term in the index, vs PhraseQuery which necessarily\napproximates, possibly badly).\n\nThere is also the controversial question of whether using manually\ndefined synonyms even helps relevance  As Robert points out, doing\nan iteration of feedback (take the top N docs, that match user's\nquery, extract their salient terms, and do a 2nd search expanded w/\nthose salient terms), sort of accomplishes something similar (and\nperhaps better since it's not just synonyms but also uncovers\n\"relationships\" like Barack Obama is a US president), but w/o the\nmanual effort of creating the synonyms.  And it's been shown to\nimprove relevance.\n\nStill, I think Lucene should make index and query time expansion\nfeasible.  At the plumbing level we don't have a horse in that race \n\nIf you do index syns at index time, you really should just inject a\nsingle syn token, representing any occurrence of a term/phrase that\nthis synonym accepts (and do the matching thing @ query time).  But,\nthen, as Earwin pointed out, Lucene is missing the notion of \"span\"\nsaying how many positions this term took up (we only encode the pos\nincr, reflecting where this token begins relative to the last token's\nbeginning).\n\nEG if \"food place\" is a syn for \"restaurant\", and you have a doc\n\"... a great food place in boston ...\", and so you inject RESTAURANT (syn\ngroup) \"over\" the phrase \"food place\", then an exact phrase query\nwon't work right \u2013 you can't have \"a great RESTAURANT in boston\"\nmatch.\n\nOne simple way to express this during analysis is as a new SpanAttr\n(say), which expresses how many positions the token takes up.  We\ncould then index this, doing so efficiently for the default case\n(span==1), and then in addition to getting the .nextPosition() you\ncould then also ask for .span() from DocsAndPositionsEnum.\n\nBut, generalizing this a bit, really we are indexing a graph, where\nthe nodes are positions and the edges are tokens connecting them.\nWith only posIncr & span, you restrict the nodes to be a single linear\nchain; but if we generalize it, then nodes can be part of side\nbranches; eg the node in the middle of \"food place\" need not be a\n\"real\" position if it were injected into a document / query containing\nrestaurant.  Hard boundaries (eg b/w sentences) would be more cleanly\nrepresented here \u2013 there would not even be an edge between the nodes.\n\nWe'd then need an AutomatonWordQuery \u2013 the same idea as\nAutomatonQuery, except at the word level not at the character level.\nMultiPhraseQuery would then be a special case of AutomatonWordQuery.\n\nThen analysis becomes the serializing of this graph... analysis would\nhave to flatten out the nodes into a single linear chain, and then\nexpress the edges using position & span.  I think position would no\nlonger be a hard relative position.  EG when injecting \"food place\" (=\n2 tokens) into the tokens that contain restaurant, both food and\nrestaurant would have the same start position, but food would have\nspan 1 and restaurant would have span 2.\n\n(Sorry for the rambling... this is a complex topic!!).\n ",
            "author": "Michael McCandless",
            "id": "comment-12870588"
        },
        {
            "date": "2010-05-24T12:47:29+0000",
            "content": "There are tricky tradeoffs of index time vs search time\n\nThe worst tradeoff at all, is that users can't make it.\n\nFor other reasons, including this, we should start thinking about removing QueryParser's split-on-whitespace. ",
            "author": "Robert Muir",
            "id": "comment-12870593"
        },
        {
            "date": "2010-05-24T14:46:15+0000",
            "content": "For other reasons, including this, we should start thinking about removing QueryParser's split-on-whitespace.\n\nI think we should remove it!\n\nThe fact that QP does this whitespace pre-split means a SynFilter\n(that applies to multiple words) is unusable with QP since the\nanalyzer sees only one word at a time from QP.\n\nAnd, QP should be as language neutral as possible... ",
            "author": "Michael McCandless",
            "id": "comment-12870619"
        },
        {
            "date": "2010-05-24T15:05:55+0000",
            "content": "In my opinion, we should also have a very simply and user-friendly QP like Google: no syntax at all. Just tokenize Text with Analyzer and create a TermQuery for each token. The only params to this QP are field name and default Occur enum.\n\nPeople should create always ranges and so on programatically. Having this in a query parser is stupid. XMLQueryParser is good for this, or maybe we also get a JSON query parser (I have plans to create one similar to XML Query Parser, maybe using the saem builders). Mark Miller was talking about this for solr, too. ",
            "author": "Uwe Schindler",
            "id": "comment-12870629"
        },
        {
            "date": "2010-05-24T15:10:43+0000",
            "content": "\nWe'd then need an AutomatonWordQuery - the same idea as\nAutomatonQuery, except at the word level not at the character level.\n\nThis is a cool idea, and on the analysis side a word-level automaton is really the datastructure I think we want for actually doing the multi-word synonym match efficiently (with minimal lookahead etc) ",
            "author": "Robert Muir",
            "id": "comment-12870633"
        },
        {
            "date": "2017-01-03T10:51:21+0000",
            "content": "This is finally fixed with LUCENE-6664 and LUCENE-7603. ",
            "author": "Michael McCandless",
            "id": "comment-15794766"
        }
    ]
}