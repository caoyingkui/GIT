{
    "id": "LUCENE-5905",
    "title": "Different behaviour of JapaneseAnalyzer at indexing time vs. at search time results in no matches for some words.",
    "details": {
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved",
        "components": [
            "modules/analysis"
        ],
        "affect_versions": "3.6.2,                                            4.9,                                            5.2.1,                                            6.6",
        "status": "Open",
        "fix_versions": []
    },
    "description": "A document with the word \u79cb\u8449\u539f in the body, when analysed by the JapaneseAnalyzer (AKA Kuromoji), cannot be found when searching for the same text as a phrase query.\n\nTwo programs are provided to reproduce the issue. Both programs print out the term docs and positions and then the result of parsing the phrase query.\n\nAs shown by the output, at analysis time, there is a lone Japanese term \"\u79cb\u8449\u539f\". At query parsing time, there are three such terms - \"\u79cb\u8449\" and \"\u79cb\u8449\u539f\" at position 0 and \"\u539f\" at position 1. Because all terms must be present for a phrase query to be a match, the query never matches, which is quite a serious issue for us.\n\nAny workarounds, no matter how hacky, would be extremely helpful at this point.\n\nMy guess is that this is a quirk with the analyser. If it happened with StandardAnalyzer, surely someone would have discovered it before I did.\n\nLucene 5.2.1 reproduction:\n\n\nimport org.apache.lucene.analysis.Analyzer;\nimport org.apache.lucene.analysis.ja.JapaneseAnalyzer;\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.document.Field;\nimport org.apache.lucene.document.TextField;\nimport org.apache.lucene.index.DirectoryReader;\nimport org.apache.lucene.index.IndexReader;\nimport org.apache.lucene.index.IndexWriter;\nimport org.apache.lucene.index.IndexWriterConfig;\nimport org.apache.lucene.index.LeafReader;\nimport org.apache.lucene.index.LeafReaderContext;\nimport org.apache.lucene.index.MultiFields;\nimport org.apache.lucene.index.PostingsEnum;\nimport org.apache.lucene.index.Terms;\nimport org.apache.lucene.index.TermsEnum;\nimport org.apache.lucene.queryparser.flexible.standard.StandardQueryParser;\nimport org.apache.lucene.queryparser.flexible.standard.config.StandardQueryConfigHandler;\nimport org.apache.lucene.search.DocIdSetIterator;\nimport org.apache.lucene.search.IndexSearcher;\nimport org.apache.lucene.search.Query;\nimport org.apache.lucene.search.TopDocs;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.RAMDirectory;\nimport org.apache.lucene.util.Bits;\nimport org.apache.lucene.util.BytesRef;\n\npublic class LuceneMissingTerms {\n    public static void main(String[] args) throws Exception {\n        try (Directory directory = new RAMDirectory()) {\n            Analyzer analyser = new JapaneseAnalyzer();\n\n            try (IndexWriter writer = new IndexWriter(directory, new IndexWriterConfig(analyser))) {\n                Document document = new Document();\n                document.add(new TextField(\"content\", \"blah blah commercial blah blah \\u79CB\\u8449\\u539F blah blah\", Field.Store.NO));\n                writer.addDocument(document);\n            }\n\n            try (IndexReader multiReader = DirectoryReader.open(directory)) {\n                for (LeafReaderContext leaf : multiReader.leaves()) {\n                    LeafReader reader = leaf.reader();\n\n                    Terms terms = MultiFields.getFields(reader).terms(\"content\");\n                    TermsEnum termsEnum = terms.iterator();\n                    BytesRef text;\n                    //noinspection NestedAssignment\n                    while ((text = termsEnum.next()) != null) {\n                        System.out.println(\"term: \" + text.utf8ToString());\n\n                        Bits liveDocs = reader.getLiveDocs();\n                        PostingsEnum postingsEnum = termsEnum.postings(liveDocs, null, PostingsEnum.POSITIONS);\n                        int doc;\n                        //noinspection NestedAssignment\n                        while ((doc = postingsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                            System.out.println(\"  doc: \" + doc);\n\n                            int freq = postingsEnum.freq();\n                            for (int i = 0; i < freq; i++) {\n                                int pos = postingsEnum.nextPosition();\n                                System.out.println(\"    pos: \" + pos);\n                            }\n                        }\n                    }\n                }\n\n                StandardQueryParser queryParser = new StandardQueryParser(analyser);\n                queryParser.setDefaultOperator(StandardQueryConfigHandler.Operator.AND);\n                // quoted to work around strange behaviour of StandardQueryParser treating this as a boolean query.\n                Query query = queryParser.parse(\"\\\"\\u79CB\\u8449\\u539F\\\"\", \"content\");\n                System.out.println(query);\n\n                TopDocs topDocs = new IndexSearcher(multiReader).search(query, 10);\n                System.out.println(topDocs.totalHits);\n            }\n        }\n    }\n}\n\n\n\nLucene 4.9 reproduction:\n\n\nimport org.apache.lucene.analysis.Analyzer;\n    import org.apache.lucene.analysis.ja.JapaneseAnalyzer;\n    import org.apache.lucene.document.Document;\n    import org.apache.lucene.document.Field;\n    import org.apache.lucene.document.TextField;\n    import org.apache.lucene.index.AtomicReader;\n    import org.apache.lucene.index.AtomicReaderContext;\n    import org.apache.lucene.index.DirectoryReader;\n    import org.apache.lucene.index.DocsAndPositionsEnum;\n    import org.apache.lucene.index.IndexReader;\n    import org.apache.lucene.index.IndexWriter;\n    import org.apache.lucene.index.IndexWriterConfig;\n    import org.apache.lucene.index.MultiFields;\n    import org.apache.lucene.index.Terms;\n    import org.apache.lucene.index.TermsEnum;\n    import org.apache.lucene.queryparser.flexible.standard.StandardQueryParser;\n    import org.apache.lucene.queryparser.flexible.standard.config.StandardQueryConfigHandler;\n    import org.apache.lucene.search.DocIdSetIterator;\n    import org.apache.lucene.search.IndexSearcher;\n    import org.apache.lucene.search.Query;\n    import org.apache.lucene.search.TopDocs;\n    import org.apache.lucene.store.Directory;\n    import org.apache.lucene.store.RAMDirectory;\n    import org.apache.lucene.util.Bits;\n    import org.apache.lucene.util.BytesRef;\n    import org.apache.lucene.util.Version;\n\n    public class LuceneMissingTerms {\n        public static void main(String[] args) throws Exception {\n            try (Directory directory = new RAMDirectory()) {\n                Analyzer analyser = new JapaneseAnalyzer(Version.LUCENE_4_9);\n\n                try (IndexWriter writer = new IndexWriter(directory,\nnew IndexWriterConfig(Version.LUCENE_4_9, analyser))) {\n                    Document document = new Document();\n                    document.add(new TextField(\"content\", \"blah blah\ncommercial blah blah \\u79CB\\u8449\\u539F blah blah\", Field.Store.NO));\n                    writer.addDocument(document);\n                }\n\n                try (IndexReader multiReader =\nDirectoryReader.open(directory)) {\n                    for (AtomicReaderContext atomicReaderContext :\nmultiReader.leaves()) {\n                        AtomicReader reader = atomicReaderContext.reader();\n\n                        Terms terms =\nMultiFields.getFields(reader).terms(\"content\");\n                        TermsEnum termsEnum = terms.iterator(null);\n                        BytesRef text;\n                        //noinspection NestedAssignment\n                        while ((text = termsEnum.next()) != null) {\n                            System.out.println(\"term: \" + text.utf8ToString());\n\n                            Bits liveDocs = reader.getLiveDocs();\n                            DocsAndPositionsEnum docsAndPositionsEnum\n= termsEnum.docsAndPositions(liveDocs, null);\n                            int doc;\n                            //noinspection NestedAssignment\n                            while ((doc =\ndocsAndPositionsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n                                System.out.println(\"  doc: \" + doc);\n\n                                int freq = docsAndPositionsEnum.freq();\n                                for (int i = 0; i < freq; i++) {\n                                    int pos =\ndocsAndPositionsEnum.nextPosition();\n                                    System.out.println(\"    pos: \" + pos);\n                                }\n                            }\n                        }\n                    }\n\n                    StandardQueryParser queryParser = new\nStandardQueryParser(analyser);\n\nqueryParser.setDefaultOperator(StandardQueryConfigHandler.Operator.AND);\n                    // quoted to work around strange behaviour of\nStandardQueryParser treating this as a boolean query.\n                    Query query =\nqueryParser.parse(\"\\\"\\u79CB\\u8449\\u539F\\\"\", \"content\");\n                    System.out.println(query);\n\n                    TopDocs topDocs = new\nIndexSearcher(multiReader).search(query, 10);\n                    System.out.println(topDocs.totalHits);\n                }\n            }\n        }\n    }\n\n\n\nLucene 3.6.2 reproduction:\n\n\n    import org.apache.lucene.analysis.Analyzer;\n    import org.apache.lucene.analysis.ja.JapaneseAnalyzer;\n    import org.apache.lucene.document.Document;\n    import org.apache.lucene.document.Field;\n    import org.apache.lucene.index.IndexReader;\n    import org.apache.lucene.index.IndexWriter;\n    import org.apache.lucene.index.IndexWriterConfig;\n    import org.apache.lucene.index.Term;\n    import org.apache.lucene.index.TermEnum;\n    import org.apache.lucene.index.TermPositions;\n    import org.apache.lucene.queryParser.standard.StandardQueryParser;\n    import org.apache.lucene.queryParser.standard.config.StandardQueryConfigHandler;\n    import org.apache.lucene.search.IndexSearcher;\n    import org.apache.lucene.search.Query;\n    import org.apache.lucene.search.TopDocs;\n    import org.apache.lucene.store.Directory;\n    import org.apache.lucene.store.RAMDirectory;\n    import org.apache.lucene.util.Version;\n    import org.junit.Test;\n\n    import static org.hamcrest.Matchers.*;\n    import static org.junit.Assert.*;\n\n    public class TestJapaneseAnalysis {\n       @Test\n        public void testJapaneseAnalysis() throws Exception {\n            try (Directory directory = new RAMDirectory()) {\n                Analyzer analyser = new JapaneseAnalyzer(Version.LUCENE_36);\n\n                try (IndexWriter writer = new IndexWriter(directory,\nnew IndexWriterConfig(Version.LUCENE_36, analyser))) {\n                    Document document = new Document();\n                    document.add(new Field(\"content\", \"blah blah\ncommercial blah blah \\u79CB\\u8449\\u539F blah blah\", Field.Store.NO,\nField.Index.ANALYZED));\n                    writer.addDocument(document);\n                }\n\n                try (IndexReader reader = IndexReader.open(directory);\n                     TermEnum terms = reader.terms(new Term(\"content\", \"\"));\n                     TermPositions termPositions = reader.termPositions()) {\n                    do {\n                        Term term = terms.term();\n                        if (term.field() != \"content\") {\n                            break;\n                        }\n\n                        System.out.println(term);\n                        termPositions.seek(terms);\n\n                        while (termPositions.next()) {\n                            System.out.println(\"  \" + termPositions.doc());\n                            int freq = termPositions.freq();\n                            for (int i = 0; i < freq; i++) {\n                                System.out.println(\"    \" +\ntermPositions.nextPosition());\n                            }\n                        }\n                    }\n                    while (terms.next());\n\n                    StandardQueryParser queryParser = new\nStandardQueryParser(analyser);\n\nqueryParser.setDefaultOperator(StandardQueryConfigHandler.Operator.AND);\n                    // quoted to work around strange behaviour of\nStandardQueryParser treating this as a boolean query.\n                    Query query =\nqueryParser.parse(\"\\\"\\u79CB\\u8449\\u539F\\\"\", \"content\");\n                    System.out.println(query);\n\n                    TopDocs topDocs = new\nIndexSearcher(reader).search(query, 10);\n                    assertThat(topDocs.totalHits, is(1));\n                }\n            }\n        }\n    }",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15082221",
            "author": "Trejkaz",
            "content": "Still occurs in Lucene 5.2.1. Updated the reproduction. ",
            "date": "2016-01-05T02:01:39+0000"
        },
        {
            "id": "comment-15093824",
            "author": "Masaru Hasegawa",
            "content": "Hacky workaround would be to set outputCompounds in JapaneseTokenizer to false using reflection or something.\n(Or simply use extended or normal mode instead of search mode)\n\nBut root cause looks to be the way JapaneseTokenizer determines whether it emits 2nd best segmentation.\n\nIt looks maxCost and calculated cost differ depending on following token. Because of that, depending token after \u79cb\u8449\u539f, it emits 2nd best segmentation.\nBut I think behavior shouldn't change depending on next token. (we always want to have consistent output for the same token, don't we?)\n\nIf I understand it correctly, if it doesn't take connection cost to next token, it should produce consistent result.\n\nThoughts? ",
            "date": "2016-01-12T12:37:46+0000"
        },
        {
            "id": "comment-16233550",
            "author": "Trejkaz",
            "content": "Adding one more case we found which seems to behave the same in Normal/Extended modes as well, but where adding a space at the end somehow changes the tokenisation to break the word 1/2 instead of 2/1.\n\n\n\"\u52a0\u85e4\u6728\" (no space) => \n\t\u52a0\u85e4\t\u540d\u8a5e,\u56fa\u6709\u540d\u8a5e,\u4eba\u540d,\u59d3\t\u52a0\u85e4\t\u30ab\u30c8\u30a6\t\u30ab\u30c8\u30fc\n\t\u6728\t\u540d\u8a5e,\u4e00\u822c,*,*\t\u6728\t\u30ad\t\u30ad\n\n\"\u52a0\u85e4\u6728 \" (one space at the end) => \n\t\u52a0\t\u540d\u8a5e,\u56fa\u6709\u540d\u8a5e,\u5730\u57df,\u56fd\t\u52a0\t\u30ab\t\u30ab\n\t\u85e4\u6728\t\u540d\u8a5e,\u56fa\u6709\u540d\u8a5e,\u5730\u57df,\u4e00\u822c\t\u85e4\u6728\t\u30d5\u30b8\u30ad\t\u30d5\u30b8\u30ad\n\t\u8a18\u53f7,\u7a7a\u767d,*,*\t\t?\t?\n\n ",
            "date": "2017-11-01T01:20:14+0000"
        }
    ]
}