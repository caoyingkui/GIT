{
    "id": "LUCENE-5267",
    "title": "java.lang.ArrayIndexOutOfBoundsException on reading data",
    "details": {
        "components": [
            "core/index"
        ],
        "fix_versions": [],
        "affect_versions": "4.4",
        "priority": "Major",
        "labels": "",
        "type": "Bug",
        "resolution": "Not A Problem",
        "status": "Resolved"
    },
    "description": "java.lang.ArrayIndexOutOfBoundsException\n\tat org.apache.lucene.codecs.compressing.LZ4.decompress(LZ4.java:132)\n\tat org.apache.lucene.codecs.compressing.CompressionMode$4.decompress(CompressionMode.java:135)\n\tat org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.visitDocument(CompressingStoredFieldsReader.java:336)\n\tat org.apache.lucene.index.SegmentReader.document(SegmentReader.java:133)\n\tat org.apache.lucene.index.BaseCompositeReader.document(BaseCompositeReader.java:110)\n\tat org.apache.lucene.index.SlowCompositeReaderWrapper.document(SlowCompositeReaderWrapper.java:212)\n\tat org.apache.lucene.index.FilterAtomicReader.document(FilterAtomicReader.java:365)\n\tat org.apache.lucene.index.BaseCompositeReader.document(BaseCompositeReader.java:110)\n\tat org.apache.lucene.index.IndexReader.document(IndexReader.java:447)\n\tat org.apache.lucene.search.IndexSearcher.doc(IndexSearcher.java:204)",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2013-10-09T01:24:44+0000",
            "content": "lucene 4.5 also failed.\n\norg.apache.lucene.index.CheckIndex /ssd/bad_lz4_index\n===========================================\negments file=segments_1r7 numSegments=1 version=4.4 format= userData={$SEQ$VAL$=0}\n  1 of 1: name=_1xt docCount=1490908\n    codec=hybaseStd42x\n    compound=false\n    numFiles=13\n    size (MB)=4,895.955\n    diagnostics = \n{timestamp=1378489485190, os=Linux, os.version=2.6.32-220.el6.x86_64, mergeFactor=18, source=merge, lucene.version=4.4.0 1504776 - sarowe - 2013-07-19 02:49:47, os.arch=amd64, mergeMaxNumSegments=1, java.version=1.7.0_25, java.vendor=Oracle Corporation}\n    no deletions\n    test: open reader.........OK\n    test: fields..............OK [58 fields]\n    test: field norms.........OK [11 fields]\n    test: terms, freq, prox...OK [23042711 terms; 1058472153 terms/docs pairs; 935028031 tokens]\n    test: stored fields.......ERROR [null]\njava.lang.ArrayIndexOutOfBoundsException\n\tat org.apache.lucene.codecs.compressing.LZ4.decompress(LZ4.java:132)\n\tat org.apache.lucene.codecs.compressing.CompressionMode$4.decompress(CompressionMode.java:135)\n\tat org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.visitDocument(CompressingStoredFieldsReader.java:336)\n\tat org.apache.lucene.index.SegmentReader.document(SegmentReader.java:133)\n\tat org.apache.lucene.index.IndexReader.document(IndexReader.java:436)\n\tat org.apache.lucene.index.CheckIndex.testStoredFields(CheckIndex.java:1268)\n\tat org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:626)\n\tat org.apache.lucene.index.CheckIndex.main(CheckIndex.java:1903)\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [12 docvalues fields; 7 BINARY; 3 NUMERIC; 2 SORTED; 0 SORTED_SET]\nFAILED\n    WARNING: fixIndex() would remove reference to this segment; full exception:\njava.lang.RuntimeException: Stored Field test failed\n\tat org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:640)\n\tat org.apache.lucene.index.CheckIndex.main(CheckIndex.java:1903)\n\nWARNING: 1 broken segments (containing 1490908 documents) detected\nWARNING: would write new segments file, and 1490908 documents would be lost, if -fix were specified ",
            "author": "Littlestar",
            "id": "comment-13789918"
        },
        {
            "date": "2013-10-09T02:41:14+0000",
            "content": "\npublic static int decompress(DataInput compressed, int decompressedLen, byte[] dest, int dOff) throws IOException {\n    final int destEnd = dest.length;\n\n    do {\n      ..........................\n\n      // copying a multiple of 8 bytes can make decompression from 5% to 10% faster\n      final int fastLen = (matchLen + 7) & 0xFFFFFFF8;\n      if (matchDec < matchLen || dOff + fastLen > destEnd) {\n        // overlap -> naive incremental copy\n        for (int ref = dOff - matchDec, end = dOff + matchLen; dOff < end; ++ref, ++dOff) {\n          dest[dOff] = dest[ref];\n        }\n      } else {\n        // no overlap -> arraycopy\n        try {\n            System.arraycopy(dest, dOff - matchDec, dest, dOff, fastLen);\n        }catch(Throwable e) {\n            System.out.println(\"dest.length=\" + dest.length + \",dOff=\" + dOff + \",matchDec=\" + matchDec + \",matchLen=\" + matchLen + \",fastLen=\" + fastLen);\n        }\n        dOff += matchLen;\n      }\n    } while (dOff < decompressedLen);\n\n    return dOff;\n  }\n\n ",
            "author": "Littlestar",
            "id": "comment-13789958"
        },
        {
            "date": "2013-10-09T07:10:02+0000",
            "content": "Thanks for the report. Can you check if there are disk-related issues in your system logs and share the .fdx and .fdt files of the broken segment? ",
            "author": "Adrien Grand",
            "id": "comment-13790108"
        },
        {
            "date": "2013-10-09T07:11:18+0000",
            "content": "Can you also confirm that you are using Lucene42StoredFieldsFormat in your hybaseStd42x codec (and not eg. a customized CompressingStoredFieldsFormat)? ",
            "author": "Adrien Grand",
            "id": "comment-13790110"
        },
        {
            "date": "2013-10-09T07:23:14+0000",
            "content": "dOff - matchDec <0, so throws java.lang.ArrayIndexOutOfBoundsException\ndest.length=33288,dOff=3184,matchDec=34510,matchLen=15,fastLen=16\ndest.length=33288,dOff=3213,matchDec=34724,matchLen=9,fastLen=16\ndest.length=33288,dOff=3229,matchDec=45058,matchLen=12,fastLen=16\ndest.length=33288,dOff=3255,matchDec=20482,matchLen=9,fastLen=16\ndest.length=33288,dOff=3275,matchDec=26122,matchLen=12,fastLen=16\ndest.length=33288,dOff=3570,matchDec=35228,matchLen=6,fastLen=8\n ",
            "author": "Littlestar",
            "id": "comment-13790117"
        },
        {
            "date": "2013-10-09T07:37:14+0000",
            "content": "// Lucene42Codec + LZ4\npublic final class Hybase42StandardCodec extends FilterCodec {\n\tpublic Hybase42StandardCodec() \n{\n\t\tsuper(\"hybaseStd42x\", new Lucene42Codec());\n\t}\n}\n\n>>>disk-related issues in your system logs and share the .fdx and .fdt files of the broken segment\ntoo big(5G).... ",
            "author": "Littlestar",
            "id": "comment-13790131"
        },
        {
            "date": "2013-10-09T07:41:50+0000",
            "content": "when ArrayIndexOutOfBoundsException omit....\nERROR [Invalid vLong detected (negative values disallowed)]\njava.lang.RuntimeException: Invalid vLong detected (negative values disallowed)\n\tat org.apache.lucene.store.ByteArrayDataInput.readVLong(ByteArrayDataInput.java:152)\n\tat org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.visitDocument(CompressingStoredFieldsReader.java:342)\n\tat org.apache.lucene.index.SegmentReader.document(SegmentReader.java:133)\n\tat org.apache.lucene.index.IndexReader.document(IndexReader.java:436)\n\tat org.apache.lucene.index.CheckIndex.testStoredFields(CheckIndex.java:1268)\n\tat org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:626)\n\tat org.apache.lucene.index.CheckIndex.main(CheckIndex.java:1903)\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [12 docvalues fields; 7 BINARY; 3 NUMERIC; 2 SORTED; 0 SORTED_SET] ",
            "author": "Littlestar",
            "id": "comment-13790134"
        },
        {
            "date": "2013-10-09T08:09:34+0000",
            "content": "dOff - matchDec <0, so throws java.lang.ArrayIndexOutOfBoundsException\ndest.length=33288,dOff=3184,matchDec=34510,matchLen=15,fastLen=16\n\nIndeed, all the lines you pasted make no sense since matchDec should be lower than dOff. To me this really looks like your index got corrupted somehow. It could be a single corrupt byte that makes LZ4 read a length on 2 bytes instead of 1 and this shift makes LZ4 try to decompress bytes that make no sense at all, explaining why all matchDecs are all higher than dOff.\n\nThere are likely only a few chunks that are broken so if you want to try to get back as many documents as possible from the corrupt segment, the following piece of code may help https://gist.github.com/jpountz/6461246 ",
            "author": "Adrien Grand",
            "id": "comment-13790152"
        },
        {
            "date": "2013-10-09T10:51:43+0000",
            "content": "Thanks, most of records recoverd.\nBut why index got corrupted? mybe compress or writer has bug ... ",
            "author": "Littlestar",
            "id": "comment-13790236"
        },
        {
            "date": "2013-10-09T11:02:37+0000",
            "content": "Good question. I've had this issue myself once and the dmesg of the system was full with disk-related errors so something really bad probably happened with the disk. I am actually thinking of adding some basic checksuming to the future stored fields format (4 bytes per chunk, this wouldn't hurt the compression ratio much) in order to be able to distinguish easily index corruptions from bugs in the stored fields format (and especially the compression layer). ",
            "author": "Adrien Grand",
            "id": "comment-13790248"
        }
    ]
}