{
    "id": "SOLR-6969",
    "title": "When opening an HDFSTransactionLog for append we must first attempt to recover it's lease to prevent data loss.",
    "details": {
        "components": [
            "hdfs"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.0",
            "6.0"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Critical"
    },
    "description": "This can happen after a hard crash and restart. The current workaround is to stop and wait it out and start again. We should retry and wait a given amount of time as we do when we detect safe mode though.",
    "attachments": {
        "SOLR-6969.patch": "https://issues.apache.org/jira/secure/attachment/12695119/SOLR-6969.patch",
        "SOLR-6969-4.10.4-backport.patch": "https://issues.apache.org/jira/secure/attachment/12701415/SOLR-6969-4.10.4-backport.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-01-12T19:48:01+0000",
            "author": "Mark Miller",
            "content": "\nERROR - 2015-01-12 17:49:43.992; org.apache.solr.common.SolrException; Failure to open existing log file (non fatal) hdfs://localhost:8020/solr_test/collection1/core_node1/data/tlog/tlog.0000000000000000000:org.apache.solr.common.SolrException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.RecoveryInProgressException): Failed to close file /solr_test/collection1/core_node1/data/tlog/tlog.0000000000000000000. Lease recovery is in progress. Try again later.\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:2626)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInternal(FSNamesystem.java:2462)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInt(FSNamesystem.java:2700)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFile(FSNamesystem.java:2663)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append(NameNodeRpcServer.java:559)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.append(ClientNamenodeProtocolServerSideTranslatorPB.java:388)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)\n\n\tat org.apache.solr.update.HdfsTransactionLog.<init>(HdfsTransactionLog.java:121)\n\tat org.apache.solr.update.HdfsUpdateLog.init(HdfsUpdateLog.java:190)\n\tat org.apache.solr.update.UpdateHandler.<init>(UpdateHandler.java:134)\n\tat org.apache.solr.update.UpdateHandler.<init>(UpdateHandler.java:94)\n\n ",
            "id": "comment-14274046"
        },
        {
            "date": "2015-01-12T22:00:38+0000",
            "author": "Mark Miller",
            "content": "Praneeth also mentions seeing AlreadyBeingCreatedException in SOLR-6367 - we should deal with that as well. ",
            "id": "comment-14274244"
        },
        {
            "date": "2015-01-14T00:48:37+0000",
            "author": "Mike Drob",
            "content": "Ignoring the error itself for a moment, the error message is also really confusing. It starts with \"Failure to open existing log file\" and then later in the same line is \"Failed to close file.\"\n\nFor SafeMode it looks like we retry indefinitely. Do we want to exhibit the same behaviour here? ",
            "id": "comment-14276292"
        },
        {
            "date": "2015-01-14T00:54:45+0000",
            "author": "Mark Miller",
            "content": "Safe mode can be turned on and off by the user, so we retry without limit. It seems this should be a bound wait though. ",
            "id": "comment-14276299"
        },
        {
            "date": "2015-01-20T17:03:06+0000",
            "author": "Anshum Gupta",
            "content": "Mark Miller do you intend to get this in for 5.0? I'm asking as this has been marked as critical for this release. ",
            "id": "comment-14284016"
        },
        {
            "date": "2015-01-21T22:59:08+0000",
            "author": "Mike Drob",
            "content": "Is retrying always going to be safe? That works fine after we've lost a server and started a new one (albeit too quickly) but what about the case where two servers both think they are responsible for that tlog? This can happen if the original server partially dies, but still has some threads that are doing work and haven't been cleaned up.\n\nLooking at how other projects handle similar issues - HBase moves the entire directory[1] to break any existing leases and ensure any other processes gets kicked out. Maybe a retry is a good stop-gap, but is it going to be a full solution?\n\n[1]: https://github.com/apache/hbase/blob/master/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterFileSystem.java#L310 ",
            "id": "comment-14286485"
        },
        {
            "date": "2015-01-22T03:21:08+0000",
            "author": "Mark Miller",
            "content": "You can't have two Solr severs trying to access the same files when configured properly. The index lock will prevent this before you get to tlog files. \n\nI do think a long term / full solution requires some discussion with some hdfs guys. I talked to Colin a little about it today. Sounds like it's kind of an ugly situation.  ",
            "id": "comment-14286892"
        },
        {
            "date": "2015-01-22T03:22:23+0000",
            "author": "Mark Miller",
            "content": "Part of the problem is that we don't want to diverge from how things work with local file system as much as we can. ",
            "id": "comment-14286894"
        },
        {
            "date": "2015-01-22T03:36:57+0000",
            "author": "Anshum Gupta",
            "content": "So it's certainly not for 5.0, right? ",
            "id": "comment-14286910"
        },
        {
            "date": "2015-01-28T15:43:55+0000",
            "author": "Mark Miller",
            "content": "Colin pointed us to https://github.com/apache/hbase/blob/833feefbf977a8208f8f71f5f3bd9b027d54961f/hbase-server/src/main/java/org/apache/hadoop/hbase/util/FSHDFSUtils.java ",
            "id": "comment-14295307"
        },
        {
            "date": "2015-01-28T23:27:11+0000",
            "author": "Mark Miller",
            "content": "Here is a first patch. ",
            "id": "comment-14296043"
        },
        {
            "date": "2015-01-28T23:48:01+0000",
            "author": "Mark Miller",
            "content": "Ignoring the error itself for a moment, the error message is also really confusing. It starts with \"Failure to open existing log file\" and then later in the same line is \"Failed to close file.\"\n\nIt's two different things. The high level log item says that there was a \"Failure to open existing log file\". It shows the cause of this as RecoveryInProgressException and that has a message that let's you know that the problem was \"Failed to close file.\". Which is what happened. If the file had been properly closed previously (no -9, bug, OOM, etc), you would not have an outstanding lease that causes this issue. ",
            "id": "comment-14296076"
        },
        {
            "date": "2015-01-29T05:11:01+0000",
            "author": "Mark Miller",
            "content": "Patch adds a basic test to hit the lease recovery path. ",
            "id": "comment-14296382"
        },
        {
            "date": "2015-01-29T17:22:44+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1655754 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1655754 ]\n\nSOLR-6969: When opening an HDFSTransactionLog for append we must first attempt to recover  it's lease to prevent data loss. ",
            "id": "comment-14297162"
        },
        {
            "date": "2015-01-29T17:23:28+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1655756 from Mark Miller in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1655756 ]\n\nSOLR-6969: When opening an HDFSTransactionLog for append we must first attempt to recover  it's lease to prevent data loss. ",
            "id": "comment-14297166"
        },
        {
            "date": "2015-01-29T17:25:36+0000",
            "author": "Mark Miller",
            "content": "The testing could be improved, but I have added a basic test to hit the code path and did a bunch of manual testing so that we can get this into 5.0. ",
            "id": "comment-14297175"
        },
        {
            "date": "2015-01-29T17:33:31+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1655761 from Mark Miller in branch 'dev/branches/lucene_solr_5_0'\n[ https://svn.apache.org/r1655761 ]\n\nSOLR-6969: When opening an HDFSTransactionLog for append we must first attempt to recover  it's lease to prevent data loss. ",
            "id": "comment-14297193"
        },
        {
            "date": "2015-02-07T00:40:49+0000",
            "author": "Mark Miller",
            "content": "The lease renewer here can end up running through other tests because we don't shut it down. ",
            "id": "comment-14310311"
        },
        {
            "date": "2015-02-09T18:56:55+0000",
            "author": "Mark Miller",
            "content": "I filed SOLR-7092 for that issue. ",
            "id": "comment-14312631"
        },
        {
            "date": "2015-02-23T05:00:52+0000",
            "author": "Anshum Gupta",
            "content": "Bulk close after 5.0 release. ",
            "id": "comment-14332596"
        },
        {
            "date": "2015-02-27T19:02:16+0000",
            "author": "Anshum Gupta",
            "content": "Reopening for backporting to 4.10.4. ",
            "id": "comment-14340615"
        },
        {
            "date": "2015-02-27T19:02:59+0000",
            "author": "Anshum Gupta",
            "content": "4.10.4 backport patch. ",
            "id": "comment-14340620"
        },
        {
            "date": "2015-02-27T19:07:48+0000",
            "author": "Anshum Gupta",
            "content": "This adds a dependency for solr/core so I'm not sure if we should be backporting this to a bug fix release: \n\n<dependency org=\"commons-collections\" name=\"commons-collections\" rev=\"${/commons-collections/commons-collections}\" conf=\"compile.hadoop\"/>\n\n\n\nWhat's the general practice? ",
            "id": "comment-14340632"
        },
        {
            "date": "2015-03-02T04:46:15+0000",
            "author": "Anshum Gupta",
            "content": "I'm opting to not backport this to 4.10.x branch due to the added dependency.\nIn case someone wants to use it, feel free to use the patch though. ",
            "id": "comment-14342732"
        }
    ]
}