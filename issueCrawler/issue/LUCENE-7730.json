{
    "id": "LUCENE-7730",
    "title": "Better encode length normalization in similarities",
    "details": {
        "labels": "",
        "priority": "Major",
        "resolution": "Fixed",
        "affect_versions": "None",
        "status": "Resolved",
        "type": "Task",
        "components": [],
        "fix_versions": [
            "7.0"
        ]
    },
    "description": "Now that index-time boosts are gone (LUCENE-6819) and that indices record the version that was used to create them (for backward compatibility, LUCENE-7703), we can look into storing the length normalization factor more efficiently.",
    "attachments": {
        "LUCENE-7730.patch": "https://issues.apache.org/jira/secure/attachment/12855916/LUCENE-7730.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15894073",
            "date": "2017-03-03T10:12:21+0000",
            "content": "Should we try and separate norms encoding from the different Similarity implementations?  It seems a bit odd to me that you have to set a Similarity on IndexWriterConfig, but then you can use a completely different Sim at search time and it will all still work, providing that it encodes norms in the same way.  Perhaps instead we should abstract out a NormsEncoding interface, which gets passed both to IWC and to Similarity? ",
            "author": "Alan Woodward"
        },
        {
            "id": "comment-15894629",
            "date": "2017-03-03T16:08:24+0000",
            "content": "I'm not sure we should do this, the fact that it is built into the similarity allows to make trade-offs that are directly tight to the scoring formula. It's true that we use the same encoding in our main similarities for convenience, but I can imagine expert users would want to encode things in a special way that reduces accuracy loss of the scoring formula? ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-15894676",
            "date": "2017-03-03T16:34:38+0000",
            "content": "Here's a patch that does the following:\n\n\tadds LeafReader.getIndexInfos() which bundles the index created version and the index sort in order to keep the number of methods on LeafReader contained. This way similarities can decide how to decode norms based on the created version.\n\tadds indexCreatedVersion to FieldInvertedState so that similarities can decide how to encode norms based on the created version\n\tGiven that readers now know about their created version, I improved IndexWriter.addIndexes(CodecReader...) to fail when a reader what was created with a different version is added.\n\tSimilarityBase and BM25Similarity now encode directly the length (rather than 1/sqrt(length)) in a way that preserves 4 significant bits across the whole integer range and is accurate up to 40. ClassicSimilarity is left unmodified however.\n\n\n\nHere is a table of the encoded lengths for every possible byte. Everything works as if the lengths were rounded to the value in this table that is immediately lesser.\n\n\n\n Byte & 0xff \n Length \n\n\n0\n0\n\n\n1\n1\n\n\n2\n2\n\n\n3\n3\n\n\n4\n4\n\n\n5\n5\n\n\n6\n6\n\n\n7\n7\n\n\n8\n8\n\n\n9\n9\n\n\n10\n10\n\n\n11\n11\n\n\n12\n12\n\n\n13\n13\n\n\n14\n14\n\n\n15\n15\n\n\n16\n16\n\n\n17\n17\n\n\n18\n18\n\n\n19\n19\n\n\n20\n20\n\n\n21\n21\n\n\n22\n22\n\n\n23\n23\n\n\n24\n24\n\n\n25\n25\n\n\n26\n26\n\n\n27\n27\n\n\n28\n28\n\n\n29\n29\n\n\n30\n30\n\n\n31\n31\n\n\n32\n32\n\n\n33\n33\n\n\n34\n34\n\n\n35\n35\n\n\n36\n36\n\n\n37\n37\n\n\n38\n38\n\n\n39\n39\n\n\n40\n40\n\n\n41\n42\n\n\n42\n44\n\n\n43\n46\n\n\n44\n48\n\n\n45\n50\n\n\n46\n52\n\n\n47\n54\n\n\n48\n56\n\n\n49\n60\n\n\n50\n64\n\n\n51\n68\n\n\n52\n72\n\n\n53\n76\n\n\n54\n80\n\n\n55\n84\n\n\n56\n88\n\n\n57\n96\n\n\n58\n104\n\n\n59\n112\n\n\n60\n120\n\n\n61\n128\n\n\n62\n136\n\n\n63\n144\n\n\n64\n152\n\n\n65\n168\n\n\n66\n184\n\n\n67\n200\n\n\n68\n216\n\n\n69\n232\n\n\n70\n248\n\n\n71\n264\n\n\n72\n280\n\n\n73\n312\n\n\n74\n344\n\n\n75\n376\n\n\n76\n408\n\n\n77\n440\n\n\n78\n472\n\n\n79\n504\n\n\n80\n536\n\n\n81\n600\n\n\n82\n664\n\n\n83\n728\n\n\n84\n792\n\n\n85\n856\n\n\n86\n920\n\n\n87\n984\n\n\n88\n1048\n\n\n89\n1176\n\n\n90\n1304\n\n\n91\n1432\n\n\n92\n1560\n\n\n93\n1688\n\n\n94\n1816\n\n\n95\n1944\n\n\n96\n2072\n\n\n97\n2328\n\n\n98\n2584\n\n\n99\n2840\n\n\n100\n3096\n\n\n101\n3352\n\n\n102\n3608\n\n\n103\n3864\n\n\n104\n4120\n\n\n105\n4632\n\n\n106\n5144\n\n\n107\n5656\n\n\n108\n6168\n\n\n109\n6680\n\n\n110\n7192\n\n\n111\n7704\n\n\n112\n8216\n\n\n113\n9240\n\n\n114\n10264\n\n\n115\n11288\n\n\n116\n12312\n\n\n117\n13336\n\n\n118\n14360\n\n\n119\n15384\n\n\n120\n16408\n\n\n121\n18456\n\n\n122\n20504\n\n\n123\n22552\n\n\n124\n24600\n\n\n125\n26648\n\n\n126\n28696\n\n\n127\n30744\n\n\n128\n32792\n\n\n129\n36888\n\n\n130\n40984\n\n\n131\n45080\n\n\n132\n49176\n\n\n133\n53272\n\n\n134\n57368\n\n\n135\n61464\n\n\n136\n65560\n\n\n137\n73752\n\n\n138\n81944\n\n\n139\n90136\n\n\n140\n98328\n\n\n141\n106520\n\n\n142\n114712\n\n\n143\n122904\n\n\n144\n131096\n\n\n145\n147480\n\n\n146\n163864\n\n\n147\n180248\n\n\n148\n196632\n\n\n149\n213016\n\n\n150\n229400\n\n\n151\n245784\n\n\n152\n262168\n\n\n153\n294936\n\n\n154\n327704\n\n\n155\n360472\n\n\n156\n393240\n\n\n157\n426008\n\n\n158\n458776\n\n\n159\n491544\n\n\n160\n524312\n\n\n161\n589848\n\n\n162\n655384\n\n\n163\n720920\n\n\n164\n786456\n\n\n165\n851992\n\n\n166\n917528\n\n\n167\n983064\n\n\n168\n1048600\n\n\n169\n1179672\n\n\n170\n1310744\n\n\n171\n1441816\n\n\n172\n1572888\n\n\n173\n1703960\n\n\n174\n1835032\n\n\n175\n1966104\n\n\n176\n2097176\n\n\n177\n2359320\n\n\n178\n2621464\n\n\n179\n2883608\n\n\n180\n3145752\n\n\n181\n3407896\n\n\n182\n3670040\n\n\n183\n3932184\n\n\n184\n4194328\n\n\n185\n4718616\n\n\n186\n5242904\n\n\n187\n5767192\n\n\n188\n6291480\n\n\n189\n6815768\n\n\n190\n7340056\n\n\n191\n7864344\n\n\n192\n8388632\n\n\n193\n9437208\n\n\n194\n10485784\n\n\n195\n11534360\n\n\n196\n12582936\n\n\n197\n13631512\n\n\n198\n14680088\n\n\n199\n15728664\n\n\n200\n16777240\n\n\n201\n18874392\n\n\n202\n20971544\n\n\n203\n23068696\n\n\n204\n25165848\n\n\n205\n27263000\n\n\n206\n29360152\n\n\n207\n31457304\n\n\n208\n33554456\n\n\n209\n37748760\n\n\n210\n41943064\n\n\n211\n46137368\n\n\n212\n50331672\n\n\n213\n54525976\n\n\n214\n58720280\n\n\n215\n62914584\n\n\n216\n67108888\n\n\n217\n75497496\n\n\n218\n83886104\n\n\n219\n92274712\n\n\n220\n100663320\n\n\n221\n109051928\n\n\n222\n117440536\n\n\n223\n125829144\n\n\n224\n134217752\n\n\n225\n150994968\n\n\n226\n167772184\n\n\n227\n184549400\n\n\n228\n201326616\n\n\n229\n218103832\n\n\n230\n234881048\n\n\n231\n251658264\n\n\n232\n268435480\n\n\n233\n301989912\n\n\n234\n335544344\n\n\n235\n369098776\n\n\n236\n402653208\n\n\n237\n436207640\n\n\n238\n469762072\n\n\n239\n503316504\n\n\n240\n536870936\n\n\n241\n603979800\n\n\n242\n671088664\n\n\n243\n738197528\n\n\n244\n805306392\n\n\n245\n872415256\n\n\n246\n939524120\n\n\n247\n1006632984\n\n\n248\n1073741848\n\n\n249\n1207959576\n\n\n250\n1342177304\n\n\n251\n1476395032\n\n\n252\n1610612760\n\n\n253\n1744830488\n\n\n254\n1879048216\n\n\n255\n2013265944\n\n\n\n\n\nIt is still a work-in-progress, some tests that rely on the way accuracy was lost are not passing for instance. Feedback about eg. better ways that we could propagate the index created version or encode the norm is welcome. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-15955043",
            "date": "2017-04-04T12:01:40+0000",
            "content": "Here is a new patch that builds upon LUCENE-7756. It is not 100% ready as some tests still don't pass due to the fact that I did not switch ClassicSimilarity to a new encoding but ready for review if anyone wants to have a look. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16012655",
            "date": "2017-05-16T16:14:35+0000",
            "content": "New patch. It is not really possible to switch ClassicSimilarity to the new encoding given how it is built on the assumption that it encodes the normalization factor directly while the new encoding I have been working on encodes the length. So I ended up doing the following:\n\n\tClassicSimilarity will still encode norms the same way in 7.0 as it did before, it means it is no longer index-time compatible with, say, BM25Similarity\n\tClassicSimilarity docs have been updated to advise using BM25Similarity instead\n\tClassicSimilarity has been moved out of similarity randomization in the test framework\n\n\n\nI'd like to get it in 7.0 as this change can only be done in a major release (it uses the index creation major to know which encoding to use) so please speak up if you have concerns. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16012832",
            "date": "2017-05-16T18:07:21+0000",
            "content": "I think there is a bit of a trap wrt classicsimilarity, maybe can ClassicSimilarity be moved out of core since it is the only one no longer compatible with the others? e.g. maybe into misc/ or somewhere else.  ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16013125",
            "date": "2017-05-16T21:14:39+0000",
            "content": "This is a good idea, I'll move it to misc. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16013263",
            "date": "2017-05-16T23:33:17+0000",
            "content": "+1\n\nThis solves a hairy problem in a non-intrusive way and is a much better tradeoff to users. I ran some basic relevance tests and it all checks out including 6x back compat. I see the typical 1% difference in this corpus that i would see vs using e.g. a 32 bit integer. But for e.g. very small docs users will be much happier and less likely to compalin about the quantization to a single byte.\n\nI think it is fine to move TFIDFSimilarity/ClassicSimilarity to misc/. Another option is to fold them into one class and clean up the abstractions, fix them to use this encoding too. TFIDFSimilarity was really just a migration thing (its the pre-4.x Similarity api basically). It is kinda like a rotting abstraction/tech debt since it has fallen behind. But I think these days for a custom TF/IDF-like scoring, you'd just use Similarity or SimilarityBase so that you have all the index statistics and so on? Worth a thought.\n\nWhen can the old tables and backwards compatibility logic be removed from e.g. BM25Similarity? I think that part is important. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16015598",
            "date": "2017-05-18T11:17:11+0000",
            "content": "Here is an updated patch. I tried moving TFIDFSimilarity and ClassicSimilarity to misc yesterday but gave up due to how MLT and some values sources depend on it. This new patch removes the ability to customize the norms encoding in TFIDFSimilarity and makes it compatible again with other similarities (so I added it back to Similarity randomization). ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16015599",
            "date": "2017-05-18T11:18:32+0000",
            "content": "When can the old tables and backwards compatibility logic be removed from e.g. BM25Similarity? I think that part is important.\n\nWell we are supposed to only support one major version back, so hopefull we can remove them in 8.0. However that means we need to add a check to refuse to load indices that have been created with version N-2 if we don't want it to be trappy and silently decode length norm factors as if they were 7.0 lengths. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16015666",
            "date": "2017-05-18T12:23:48+0000",
            "content": "I opened LUCENE-7837. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16015678",
            "date": "2017-05-18T12:30:15+0000",
            "content": "+1 ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16015830",
            "date": "2017-05-18T14:28:46+0000",
            "content": "Commit 06a6034d9bc8f06ea567c0110b954b35515c2ea0 in lucene-solr's branch refs/heads/master from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=06a6034 ]\n\nLUCENE-7730: Better accuracy for the length normalization factor. ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-16015834",
            "date": "2017-05-18T14:30:32+0000",
            "content": "Thanks fo having a look, Robert! ",
            "author": "Adrien Grand"
        }
    ]
}