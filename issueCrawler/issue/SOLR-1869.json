{
    "id": "SOLR-1869",
    "title": "RemoveDuplicatesTokenFilter doest have expected behaviour",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [],
        "components": [
            "Schema and Analysis"
        ],
        "type": "New Feature",
        "priority": "Minor",
        "labels": "",
        "resolution": "Not A Problem"
    },
    "description": "the RemoveDuplicatesTokenFilter seems broken as it initializes its map and attributes at the class level and not within its constructor\nin addition i would think the expected behaviour would be to remove identical terms with the same offset positions, instead it looks like it removes duplicates based on position increment which wont work when using it after something like the edgengram filter. when i posted this to the mailing list even erik hatcher seemed to think thats what this filter was supposed to do...\n\nattaching a patch that has the expected behaviour and initializes variables in constructor",
    "attachments": {
        "RemoveDupOffsetTokenFilter.java": "https://issues.apache.org/jira/secure/attachment/12441166/RemoveDupOffsetTokenFilter.java",
        "RemoveDupOffsetTokenFilterFactory.java": "https://issues.apache.org/jira/secure/attachment/12441167/RemoveDupOffsetTokenFilterFactory.java",
        "SOLR-1869.patch": "https://issues.apache.org/jira/secure/attachment/12441053/SOLR-1869.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Robert Muir",
            "id": "comment-12854676",
            "date": "2010-04-07T20:15:37+0000",
            "content": "Joe, the initialization is the same. I simply prefer to do this right where the attribute is declared, rather than doing it in the ctor (its the same in java!). So this is no problem.\n\nas far as the behavior, the filter is currently correct:\n\nA TokenFilter which filters out Tokens at the same position and Term text as the previous token in the stream.\n\n\n\nif you want to instead create a filter that removes duplicates across an entire field, this is really a completely different filter, but it sounds like a useful completely different filter!\n\nCan you instead create a patch for a separate filter with a different name?\n\nI think you can start with this patch, but there are a number of issues with this patch though:\n\n\tthe map/set is never cleared, so it won't work across reusable tokenstreams. The map/set should be cleared in reset()\n\ti would use chararrayset instead of this map, like the current RemoveDuplicatesTokenFilter\n\n "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12854700",
            "date": "2010-04-07T20:46:55+0000",
            "content": "the RemoveDuplicatesTokenFilter seems broken as it initializes its map and attributes at the class level and not within its constructor\n\nThe filter is correct. That are final instance fields and the autogenerated ctor by javac does the same, so there is no need to move them to ctor. In Lucene/Solr all TokenStreams are done this way, thats our code style for TokenStreams.\n\nThe CharArrayMap is more performant in lookup, but you are right, we may need posincr. In general the Map should really be simply a CharArraySet or HashSet<String> and the check should use contains.\n\nBut I dont understand the rest of the patch. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12854712",
            "date": "2010-04-07T21:21:13+0000",
            "content": "The CharArrayMap is more performant in lookup, but you are right, we may need posincr.\n\nwe don't need it for the current implementation, as we clear() the chararrayset when we encounter a term of posincr > 0.\nso the set is only a set of \"seen terms\" at some position. "
        },
        {
            "author": "Joe Calderon",
            "id": "comment-12854972",
            "date": "2010-04-08T16:13:04+0000",
            "content": "\"at the same position and Term text as the previous token \" is ambiguous, i assumed position to mean same start and end offsets, hence i assumed there was a bug.\n\ni changed the filter to use CharArraySet, there was already a call to previous.clear() in reset(). Since the filter name is different i attached its accompanying factory.\n\nthis all started because the highlighter was highlighting a term at the same offsets twice, for example if i had a word with a synonym [ex-con,0,6] and [excon,0,5]  then ran it through edgengram filter i would end up with two tokens [ex, 0,2] with different position increments, the highlighted snippet was then \"<em>ex</em><em>ex</em>-con\", i posted this on the mailing list and  RemoveDuplicatesTokenFilter was suggested. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12854983",
            "date": "2010-04-08T16:34:50+0000",
            "content": "this all started because the highlighter was highlighting a term at the same offsets twice,\n\nPerhaps we should fix this directly in DefaultSolrHighlighter? It already has this TokenStream-sorting filter thats intended to do the following:\n\n/** Orders Tokens in a window first by their startOffset ascending.\n * endOffset is currently ignored.\n * This is meant to work around fickleness in the highlighter only.  It\n * can mess up token positions and should not be used for indexing or querying.\n */\n\n\n\nMaybe the deduplication logic should occur here after it sorts on startOffset?  "
        },
        {
            "author": "Joe Calderon",
            "id": "comment-12855144",
            "date": "2010-04-08T20:55:14+0000",
            "content": "yes, i think modifying the highlighter would be a better solution here... "
        },
        {
            "author": "Joe Calderon",
            "id": "comment-12860015",
            "date": "2010-04-22T21:12:06+0000",
            "content": "im closing this since there is no bug in RemoveDuplicates filter, will post another patch in another ticker if i ever get around to patching the highlighter "
        }
    ]
}