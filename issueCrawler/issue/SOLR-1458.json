{
    "id": "SOLR-1458",
    "title": "Java Replication error: NullPointerException SEVERE: SnapPull failed on 2009-09-22 nightly",
    "details": {
        "affect_versions": "1.4",
        "status": "Closed",
        "fix_versions": [
            "1.4"
        ],
        "components": [
            "replication (java)"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "After finally figuring out the new Java based replication, we have started both the slave and the master and issued optimize to all master Solr instances. This triggered some replication to go through just fine, but it looks like some of it is failing.\n\nHere's what I'm getting in the slave logs, repeatedly for each shard:\n\n\n \nSEVERE: SnapPull failed \njava.lang.NullPointerException\n        at org.apache.solr.handler.SnapPuller.fetchLatestIndex(SnapPuller.java:271)\n        at org.apache.solr.handler.ReplicationHandler.doFetch(ReplicationHandler.java:258)\n        at org.apache.solr.handler.SnapPuller$1.run(SnapPuller.java:159)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\n\n \n\nIf I issue an optimize again on the master to one of the shards, it then triggers a replication and replicates OK. I have a feeling that these SnapPull failures appear later on but right now I don't have enough to form a pattern.\n\nHere's replication.properties on one of the failed slave instances.\n\ncat data/replication.properties \n#Replication details\n#Wed Sep 23 19:35:30 PDT 2009\nreplicationFailedAtList=1253759730020,1253759700018,1253759670019,1253759640018,1253759610018,1253759580022,1253759550019,1253759520016,1253759490026,1253759460016\npreviousCycleTimeInSeconds=0\ntimesFailed=113\nindexReplicatedAtList=1253759730020,1253759700018,1253759670019,1253759640018,1253759610018,1253759580022,1253759550019,1253759520016,1253759490026,1253759460016\nindexReplicatedAt=1253759730020\nreplicationFailedAt=1253759730020\nlastCycleBytesDownloaded=0\ntimesIndexReplicated=113\n\n\n\nand another\n\ncat data/replication.properties \n#Replication details\n#Wed Sep 23 18:42:01 PDT 2009\nreplicationFailedAtList=1253756490034,1253756460169\npreviousCycleTimeInSeconds=1\ntimesFailed=2\nindexReplicatedAtList=1253756521284,1253756490034,1253756460169\nindexReplicatedAt=1253756521284\nreplicationFailedAt=1253756490034\nlastCycleBytesDownloaded=22932293\ntimesIndexReplicated=3\n\n\n\n\nSome relevant configs:\nIn solrconfig.xml:\n\n<!-- For docs see http://wiki.apache.org/solr/SolrReplication -->\n  <requestHandler name=\"/replication\" class=\"solr.ReplicationHandler\" >\n    <lst name=\"master\">\n        <str name=\"enable\">${enable.master:false}</str>\n        <str name=\"replicateAfter\">optimize</str>\n        <str name=\"backupAfter\">optimize</str>\n        <str name=\"commitReserveDuration\">00:00:20</str>\n    </lst>\n    <lst name=\"slave\">\n        <str name=\"enable\">${enable.slave:false}</str>\n\n        <!-- url of master, from properties file -->\n        <str name=\"masterUrl\">${master.url}</str>\n\n        <!-- how often to check master -->\n        <str name=\"pollInterval\">00:00:30</str>\n    </lst>\n  </requestHandler>\n\n\n\nThe slave then has this in solrcore.properties:\n\nenable.slave=true\nmaster.url=URLOFMASTER/replication\n\n\n\nand the master has\n\nenable.master=true\n\n\n\nI'd be glad to provide more details but I'm not sure what else I can do.  SOLR-926 may be relevant.\n\nThanks.",
    "attachments": {
        "SOLR-1458.patch": "https://issues.apache.org/jira/secure/attachment/12420433/SOLR-1458.patch",
        "SolrDeletionPolicy.patch": "https://issues.apache.org/jira/secure/attachment/12420595/SolrDeletionPolicy.patch",
        "reserve.patch": "https://issues.apache.org/jira/secure/attachment/12421268/reserve.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Noble Paul",
            "id": "comment-12758998",
            "date": "2009-09-24T03:50:41+0000",
            "content": "can you hit the master with the filelist command  and see the output. \n\nhttp://wiki.apache.org/solr/SolrReplication#line-155 "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12758999",
            "date": "2009-09-24T03:56:00+0000",
            "content": "log the error if no file list available "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12759001",
            "date": "2009-09-24T04:08:52+0000",
            "content": "Why would there not be a filelist? Any idea what the underlying error is? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12759003",
            "date": "2009-09-24T04:17:24+0000",
            "content": "isn't it possible that by the time filelist is invoked the indexcommit of the version is gone ? In that case no files would be available "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12759004",
            "date": "2009-09-24T04:21:01+0000",
            "content": "and we don't reserve the commit after an indexversion command . should we reserve the commitpoint if after an indexversion command? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12759021",
            "date": "2009-09-24T05:50:34+0000",
            "content": "reserve commits in indexversion command "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12759179",
            "date": "2009-09-24T17:12:17+0000",
            "content": "The best way to eliminate some of these race conditions would seem to be combine it into a single command.\n\"what is your current index version?  if greater than 5, please give me the list of new files since then and please reserve them for x milliseconds\"\n\nBut at this point (close to 1.4) I guess your patch is the most straightforward fix. "
        },
        {
            "author": "Artem Russakovskii",
            "id": "comment-12759247",
            "date": "2009-09-24T20:49:15+0000",
            "content": "Re: hit the master with the filelist command.\n\nFirst of all, this may have been a really late night for the person who wrote the wiki:\n\"Get list of lucene files present in the index: http://host:port/solr/replication?command=filelist&indexversion=<index-version-number> . The version number can be obtained using the indexversion calmmand\"\nThe last word there ;-]\n\nNow, I hit the master with the following: MASTER/replication/?command=indexversion, get back\n\n<response> \n<lst name=\"responseHeader\"><int name=\"status\">0</int><int name=\"QTime\">0</int></lst><long name=\"indexversion\">1253136035158</long>\n<long name=\"generation\">4447</long> \n</response> \n\n\n\nThen I use this in the following query: MASTER/replication/?command=filelist&indexversion=1253136035158\n\nbut I get back\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response>\n<lst name=\"responseHeader\"><int name=\"status\">0</int><int name=\"QTime\">0</int></lst><str name=\"status\">invalid indexversion</str>\n</response>\n\n\n\nI tried the same against an another instance that doesn't have the NullPointerException replication problem and still get the same error, then I tried another one, which had no documents indexed into yet, and it returned\n\n<response> \n<lst name=\"responseHeader\"><int name=\"status\">0</int><int name=\"QTime\">0</int></lst><arr name=\"filelist\"><lst><str name=\"name\">segments_1</str><long name=\"lastmodified\">1253136032000</long><long name=\"size\">32</long></lst></arr> \n</response> \n\n\n\nHere's what I see in the logs.\n\nOn the master:\n\nSep 24, 2009 1:56:30 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/events_2009_08 path=/replication params={command=indexversion&wt=javabin} status=0 QTime=0 \nSep 24, 2009 1:56:30 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/events_2009_04 path=/replication params={command=indexversion&wt=javabin} status=0 QTime=0 \nSep 24, 2009 1:56:30 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/events_2009_10 path=/replication params={command=indexversion&wt=javabin} status=0 QTime=0 \nSep 24, 2009 1:56:30 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/events_2009_02 path=/replication params={command=indexversion&wt=javabin} status=0 QTime=0 \nSep 24, 2009 1:56:30 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/events_2009_12 path=/replication params={command=indexversion&wt=javabin} status=0 QTime=0 \nSep 24, 2009 1:56:30 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/events_2009_06 path=/replication params={command=indexversion&wt=javabin} status=0 QTime=0 \nSep 24, 2009 1:56:30 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/events_2009_08 path=/replication params={indexversion=1253136074767&command=filelist&wt=javabin} status=0 QTime=0 \nSep 24, 2009 1:56:30 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/events_2009_06 path=/replication params={indexversion=1253136077032&command=filelist&wt=javabin} status=0 QTime=0 \n\n\n\nOn the slave:\n\nSep 24, 2009 2:01:00 PM org.apache.solr.handler.SnapPuller fetchLatestIndex\nINFO: Slave in sync with master.\nSep 24, 2009 2:01:00 PM org.apache.solr.handler.SnapPuller fetchLatestIndex\nINFO: Slave in sync with master.\nSep 24, 2009 2:01:00 PM org.apache.solr.handler.SnapPuller fetchLatestIndex\nINFO: Master's version: 1253136074767, generation: 40983\nSep 24, 2009 2:01:00 PM org.apache.solr.handler.SnapPuller fetchLatestIndex\nINFO: Master's version: 1253136077032, generation: 42291\nSep 24, 2009 2:01:00 PM org.apache.solr.handler.SnapPuller fetchLatestIndex\nINFO: Slave's version: 1253136076722, generation: 41981\nSep 24, 2009 2:01:00 PM org.apache.solr.handler.SnapPuller fetchLatestIndex\nINFO: Starting replication process\nSep 24, 2009 2:01:00 PM org.apache.solr.handler.SnapPuller fetchLatestIndex\nINFO: Slave in sync with master.\nSep 24, 2009 2:01:00 PM org.apache.solr.handler.SnapPuller fetchLatestIndex\nINFO: Slave's version: 1253136074452, generation: 40668\nSep 24, 2009 2:01:00 PM org.apache.solr.handler.SnapPuller fetchLatestIndex\nINFO: Starting replication process\nSep 24, 2009 2:01:00 PM org.apache.solr.handler.SnapPuller fetchLatestIndex\nINFO: Slave in sync with master.\nSep 24, 2009 2:01:00 PM org.apache.solr.handler.ReplicationHandler doFetch\nSEVERE: SnapPull failed \njava.lang.NullPointerException\n        at org.apache.solr.handler.SnapPuller.fetchLatestIndex(SnapPuller.java:271)\n        at org.apache.solr.handler.ReplicationHandler.doFetch(ReplicationHandler.java:258)\n        at org.apache.solr.handler.SnapPuller$1.run(SnapPuller.java:159)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\nSep 24, 2009 2:01:00 PM org.apache.solr.handler.ReplicationHandler doFetch\nSEVERE: SnapPull failed \njava.lang.NullPointerException\n        at org.apache.solr.handler.SnapPuller.fetchLatestIndex(SnapPuller.java:271)\n        at org.apache.solr.handler.ReplicationHandler.doFetch(ReplicationHandler.java:258)\n        at org.apache.solr.handler.SnapPuller$1.run(SnapPuller.java:159)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)\n        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:619)\n\n\n\nSuggestions? "
        },
        {
            "author": "Artem Russakovskii",
            "id": "comment-12759290",
            "date": "2009-09-24T22:26:00+0000",
            "content": "Also, after an optimize is issued to the master, the problem goes away. For a while, and then starts again. I do optimizes every hour, and commits are ongoing every minute.\n\nFor instance, after an optimize on the master:\n\n<response> \n<lst name=\"responseHeader\"><int name=\"status\">0</int><int name=\"QTime\">0</int></lst><arr name=\"filelist\"><lst><str name=\"name\">_1bx2.frq</str>\n<long name=\"lastmodified\">1253826921000</long><long name=\"size\">67855561</long></lst><lst><str name=\"name\">_1bx2.nrm</str><long \nname=\"lastmodified\">1253826921000</long><long name=\"size\">2515184</long></lst><lst><str name=\"name\">_1bx2.tii</str><long \nname=\"lastmodified\">1253826921000</long><long name=\"size\">581824</long></lst><lst><str name=\"name\">_1bx2.fnm</str><long \nname=\"lastmodified\">1253826906000</long><long name=\"size\">132</long></lst><lst><str name=\"name\">_1bx2.fdt</str><long \nname=\"lastmodified\">1253826906000</long><long name=\"size\">7805294</long></lst><lst><str name=\"name\">_1bx2.tis</str><long \nname=\"lastmodified\">1253826921000</long><long name=\"size\">43326001</long></lst><lst><str name=\"name\">_1bx2.fdx</str><long \nname=\"lastmodified\">1253826906000</long><long name=\"size\">4024292</long></lst><lst><str name=\"name\">_1bx2.prx</str><long \nname=\"lastmodified\">1253826921000</long><long name=\"size\">47213429</long></lst><lst><str name=\"name\">segments_19jy</str><long \nname=\"lastmodified\">1253826922000</long><long name=\"size\">287</long></lst></arr> \n</response> \n\n\n\nAnd another thing worth mentioning, I updated the solr.war from 8/25/09 nightly to 9/22/09 nightly but I hope that doesn't cause problems. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12759361",
            "date": "2009-09-25T03:35:35+0000",
            "content": "Thanks, Artem.\nYou have been quite accommodating w/ my requests. I am already looking into it. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12759366",
            "date": "2009-09-25T04:10:50+0000",
            "content": "diagnosis: commits keep happening and the optimized commits were getting removed by lucene because there were normal commits after the optimized commits\n\nimmediete fix (probable): \n\nyou may add the following snippet to your solrconfig.xml \n\n\n  <mainIndex>\n<deletionPolicy class=\"solr.SolrDeletionPolicy\">\t\n\t<str name=\"keepOptimizedOnly\">true</str>\n        <str name=\"maxCommitsToKeep\">3</str>\t\t\n\t</deletionPolicy>\n</mainIndex>\n\n\n\nreal fix:\n\nif only replicateAfter optimize is present, ReplicationHandler should ensure that the optimized commit does not get deleted (even after normal commits happen). either by reserving it or by changing the policy\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12759367",
            "date": "2009-09-25T04:27:04+0000",
            "content": "for any IndexCommit kept with the ReplicationHandler , reserve it for infinity. ensure that it gets cleaned up after ReplicationHandler no longer refers to it.\n\nyonik : Please review .  "
        },
        {
            "author": "Artem Russakovskii",
            "id": "comment-12759392",
            "date": "2009-09-25T05:50:24+0000",
            "content": "Paul - I'm just glad you guys are so fast to respond and eager to fix. Love OSS :-] "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12759405",
            "date": "2009-09-25T06:43:34+0000",
            "content": "added 2 new methods to  IndexDeletionPolicyWrapper. \n\npublic synchronized void reserveCommitPoint(Long indexCommitVersion)\n\n  public synchronized void releaseCommmitPoint(Long indexCommitVersion)\n\n\n\nevery commit point held by ReplicationHandler should be reserved for ever till it is  released "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12759546",
            "date": "2009-09-25T14:41:48+0000",
            "content": "Shouldn't there be some kind of option on the deletion policy... say \"keepLastOptimized\"?\nThen the ReplicationHandler would only have to flip it on (if it weren't already on).  It doesn't seem like the ReplicationHandler should be the one to pick which commit points to \"reserve forever\". "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12759671",
            "date": "2009-09-25T18:16:25+0000",
            "content": "Looking into this more, I think this should be the deletion policy that keeps around the last optimized commit point if necessary.\nAlso, in checking out SolrDeletionPolicy again, it doesn't seem like the maxCommitsToKeep logic will work if keepOptimizedOnly is true.\nI'm going to take a whack at rewriting updateCommits() "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12759698",
            "date": "2009-09-25T18:58:16+0000",
            "content": "Here's a partial patch - only to SolrDeletionPolicy that rewrites that logic so that all of the options hopefully work correctly.  Now if keepOptimizedOnly==true and maxCommitsToKeep==1, then there should always be one optimized index commit point.\n\nShould we just document that keepOptimizedOnly needs to be true if you're doing replication on optimized commit points only?\n\nAlternately, the replication handler could set parameters on the deletion policy - the problem being that the current parameters don't lend themselves to being manipulated.  For example if the policy has keepOptimizedOnly=false and maxCommitsToKeep==5.... then if the replication handler changed keepOptimizedOnly to true, we would end up keeping 5 optimized commit points!  It would be more flexible to be able to specify a separate count for optimized commit points. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12759721",
            "date": "2009-09-25T20:04:16+0000",
            "content": "Updated patch that implements a maxOptimizedCommitsToKeep parameter - this would allow the replication handler to raise it to 1 if necessary. "
        },
        {
            "author": "Artem Russakovskii",
            "id": "comment-12759784",
            "date": "2009-09-25T22:24:16+0000",
            "content": "I haven't changed any configs yet, and this probably doesn't come as a shock to you guys, but the master just ran out of space. Upon inspection, I found 30+ snapshot dirs sitting around in /data.\n\nPaul, adding your deletionPolicy fix didn't delete the files, even after optimize. Is that expected?\n\n\ndrwxrwxr-x  2 bla bla  4096 Sep 23 18:42 snapshot.20090923064214\ndrwxrwxr-x  2 bla bla  4096 Sep 23 19:15 snapshot.20090923071530\ndrwxrwxr-x  2 bla bla  4096 Sep 23 19:45 snapshot.20090923074535\ndrwxrwxr-x  2 bla bla  4096 Sep 23 20:15 snapshot.20090923081531\ndrwxrwxr-x  2 bla bla  4096 Sep 23 21:15 snapshot.20090923091531\ndrwxrwxr-x  2 bla bla  4096 Sep 23 22:15 snapshot.20090923101532\ndrwxrwxr-x  2 bla bla  4096 Sep 23 23:15 snapshot.20090923111533\ndrwxrwxr-x  2 bla bla  4096 Sep 24 01:15 snapshot.20090924011501\ndrwxrwxr-x  2 bla bla  4096 Sep 24 13:15 snapshot.20090924011535\ndrwxrwxr-x  2 bla bla  4096 Sep 24 02:15 snapshot.20090924021501\ndrwxrwxr-x  2 bla bla  4096 Sep 24 14:15 snapshot.20090924021534\ndrwxrwxr-x  2 bla bla  4096 Sep 24 15:15 snapshot.20090924031501\ndrwxrwxr-x  2 bla bla  4096 Sep 24 03:15 snapshot.20090924031502\ndrwxrwxr-x  2 bla bla  4096 Sep 24 04:15 snapshot.20090924041501\ndrwxrwxr-x  2 bla bla  4096 Sep 24 16:15 snapshot.20090924041536\ndrwxrwxr-x  2 bla bla  4096 Sep 24 05:15 snapshot.20090924051501\ndrwxrwxr-x  2 bla bla  4096 Sep 24 17:15 snapshot.20090924051537\ndrwxrwxr-x  2 bla bla  4096 Sep 24 06:15 snapshot.20090924061501\ndrwxrwxr-x  2 bla bla  4096 Sep 24 18:15 snapshot.20090924061534\ndrwxrwxr-x  2 bla bla  4096 Sep 24 07:15 snapshot.20090924071501\ndrwxrwxr-x  2 bla bla  4096 Sep 24 19:15 snapshot.20090924071533\ndrwxrwxr-x  2 bla bla  4096 Sep 24 08:15 snapshot.20090924081534\ndrwxrwxr-x  2 bla bla  4096 Sep 24 20:15 snapshot.20090924081535\ndrwxrwxr-x  2 bla bla  4096 Sep 24 09:15 snapshot.20090924091501\ndrwxrwxr-x  2 bla bla  4096 Sep 24 21:15 snapshot.20090924091532\ndrwxrwxr-x  2 bla bla  4096 Sep 24 10:15 snapshot.20090924101501\ndrwxrwxr-x  2 bla bla  4096 Sep 24 22:15 snapshot.20090924101533\ndrwxrwxr-x  2 bla bla  4096 Sep 24 11:15 snapshot.20090924111501\ndrwxrwxr-x  2 bla bla  4096 Sep 24 23:15 snapshot.20090924111532\ndrwxrwxr-x  2 bla bla  4096 Sep 24 12:15 snapshot.20090924121532\ndrwxrwxr-x  2 bla bla  4096 Sep 24 00:15 snapshot.20090924121533\ndrwxrwxr-x  2 bla bla  4096 Sep 25 01:15 snapshot.20090925011533\ndrwxrwxr-x  2 bla bla  4096 Sep 25 13:15 snapshot.20090925011540\ndrwxrwxr-x  2 bla bla  4096 Sep 25 02:15 snapshot.20090925021534\ndrwxrwxr-x  2 bla bla  4096 Sep 25 14:15 snapshot.20090925021540\ndrwxrwxr-x  2 bla bla  4096 Sep 25 03:15 snapshot.20090925031535\ndrwxrwxr-x  2 bla bla  4096 Sep 25 15:15 snapshot.20090925031540\ndrwxrwxr-x  2 bla bla  4096 Sep 25 15:29 snapshot.20090925032931\ndrwxrwxr-x  2 bla bla  4096 Sep 25 04:15 snapshot.20090925041535\ndrwxrwxr-x  2 bla bla  4096 Sep 25 05:15 snapshot.20090925051539\ndrwxrwxr-x  2 bla bla  4096 Sep 25 06:15 snapshot.20090925061538\ndrwxrwxr-x  2 bla bla  4096 Sep 25 07:15 snapshot.20090925071539\ndrwxrwxr-x  2 bla bla  4096 Sep 25 08:15 snapshot.20090925081539\ndrwxrwxr-x  2 bla bla  4096 Sep 25 09:15 snapshot.20090925091538\ndrwxrwxr-x  2 bla bla  4096 Sep 25 09:52 snapshot.20090925095213\ndrwxrwxr-x  2 bla bla  4096 Sep 25 10:15 snapshot.20090925101540\ndrwxrwxr-x  2 bla bla  4096 Sep 25 11:15 snapshot.20090925111538\ndrwxrwxr-x  2 bla bla  4096 Sep 25 00:15 snapshot.20090925121534\ndrwxrwxr-x  2 bla bla  4096 Sep 25 12:15 snapshot.20090925121538\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12759825",
            "date": "2009-09-26T00:12:34+0000",
            "content": "Hmmm... just happened onto this bit of odd code:\n\n\n  void refreshCommitpoint() {\n    IndexCommit commitPoint = core.getDeletionPolicy().getLatestCommit();\n    if(replicateOnCommit && !commitPoint.isOptimized()){\n      indexCommitPoint = commitPoint;\n    }\n    if(replicateOnOptimize && commitPoint.isOptimized()){\n      indexCommitPoint = commitPoint;\n    }\n  }\n\n\n\nedit: Looks like a bug... refreshCommitPoint isn't set for optimized indexes if only replicateOnCommit is set.\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12759849",
            "date": "2009-09-26T02:12:22+0000",
            "content": "Here's a draft of a full patch.... but\nI'm getting some relatively non-reproducible test failures in a bunch of places - not sure what's up yet. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12759851",
            "date": "2009-09-26T02:37:47+0000",
            "content": "I reported SOLR-1383 a few weeks ago. It is one edge case of what you're all working on.  \n\nShort version: running \"add 1 document/commit/replicate\" continuously is a reliable way to cause the deletion policy to misfire.\n\nTry the detailed test scenario. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12759859",
            "date": "2009-09-26T06:08:04+0000",
            "content": "I haven't changed any configs yet, and this probably doesn't come as a shock to you guys, but the master just ran out of space. Upon inspection, I found 30+ snapshot dirs sitting around in /data.\n\nArtem, the Java replication does not make use of snapshot directories. They are generated if you have \"backupAfter\" in your configuration. That feature is only there for people who were using the script replication's snapshot directories for backup purposes. If you don't need it, just remove \"backupAfter\". "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12759898",
            "date": "2009-09-26T14:01:06+0000",
            "content": "Strange stuff - the last error I just saw was a corrupted index exception from the spellchecker - couldn't load the segments_n file.\nBut the spellcheck building code is Lucene code - Solr's deletion policy should have no effect... weird. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12759900",
            "date": "2009-09-26T14:08:35+0000",
            "content": "bq. I think this should be the deletion policy that keeps around the last optimized commit point if necessary.\n\nYonik, shouldn't ReplicationHandler be the one to reserve commit points? Also, if we go down this way (having SolrDeletionPolicy decide these things), would a custom deletion policy play nicely with Solr? "
        },
        {
            "author": "Artem Russakovskii",
            "id": "comment-12759927",
            "date": "2009-09-26T17:54:38+0000",
            "content": "Shalin, I've taken the backupAfter line directly from the SolrReplication wiki which talks about Java based replication: http://wiki.apache.org/solr/SolrReplication. I realize now it says in the comment above that line it's for backup only but why is it there in the first place? It threw me off a bit. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12759937",
            "date": "2009-09-26T19:30:03+0000",
            "content": "Yonik, shouldn't ReplicationHandler be the one to reserve commit points? \n\nIt does... that part doesn't change, but replication handler requests short term reservations based on use.\nThe existing SolrDeletionPolicy already had the functionality of always keeping an optimized commit point around, and so this actually isn't new. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12759952",
            "date": "2009-09-26T21:01:16+0000",
            "content": "Testing Update: I rebooted my ubuntu box, did a clean solr checkout, re-applied the patch, and got a much higher rate of test passes.  Looks like it was Gremlins.\n\nLast night I set it up to build continuously in a loop - and got about a 25% failure rate.  Problem is, I didn't have it copy out failed tests for inspection,\n so I don't know why it failed, and it may be as simple as a loss of internet connectivity or DNS service, or apache going down, etc (yes we have tests\n that rely on external networks - that's a pain).\n\nI'm re-running tests now, with a stop on a test failure so I can figure out if anything is actually related to this proposed patch! "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12759957",
            "date": "2009-09-26T21:47:13+0000",
            "content": "Update: OK... my failures in DirectUpdateHandlerTest turned out to be testExpungeDeletes().  I've committed simpler test code previously attached to SOLR-1275.\nI was initially thrown off by seeing exceptions in building the spell check index... but the actual test failure was caused by testExpungeDeletes.\n\nSo - is there a really bug lurking in the spellchecker component? I'm at a loss of how the old testExpungeDeletes code could trigger these exceptions (or of they did/do).  It's also possible that these spellcheck exceptions spuriously happened before but they don't cause the test to fail. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12760051",
            "date": "2009-09-27T16:42:08+0000",
            "content": "I've committed this patch.  I'm occasionally getting errors in TestReplicationHandler... but I also get occasional errors there w/o this patch!\n\nArtem, it would be great if you could test trunk and see if your problems are fixed. "
        },
        {
            "author": "Artem Russakovskii",
            "id": "comment-12760325",
            "date": "2009-09-28T18:35:41+0000",
            "content": "Is the fix included in the latest nightly? 9/28/09 one. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12760354",
            "date": "2009-09-28T19:21:43+0000",
            "content": "Is the fix included in the latest nightly? 9/28/09 one. \n\nYep. "
        },
        {
            "author": "Artem Russakovskii",
            "id": "comment-12760799",
            "date": "2009-09-30T01:12:52+0000",
            "content": "Yonik, everything has been running for a day+ now and replication works as expected.\n\nOn a side note, I did think that the replication notes are not very clear on the what replicateAfter on the master and pollInterval on the slave are for and what each does. I now understand what each is for but I think they could be explained more clearly. Just a suggestion. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12760802",
            "date": "2009-09-30T01:25:51+0000",
            "content": "Yonik, everything has been running for a day+ now and replication works as expected.\n\nCool!  Resolving this issue.\n\nOn a side note, I did think that the replication notes are not very clear on the what replicateAfter on the master and pollInterval on the slave are for and what each does. \n\nTo be honest, I've not looked at the actual docs on the wiki \nConcrete suggestions for changes are very welcome though. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12760891",
            "date": "2009-09-30T11:58:13+0000",
            "content": "isn't this a wrong fix.? It removes an existing functionality (one can't set a custom deletion policy when replicateAfterCOmmit is set)\n\nWhat was wrong with my latest patch? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12761288",
            "date": "2009-10-01T18:06:24+0000",
            "content": "Since SolrDeletionPolicy already had the needed functionallity, it seemed like the most straightforward fix.\n\nIt removes an existing functionality (one can't set a custom deletion policy when replicateAfterCOmmit is set) \n\nTrue - I hadn't considered that.  Of course I was confused why we allowed custom deletion policies in the first place.\nIt's a dangerous place to mess around, and there haven't been any identifiable use cases, right? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12761916",
            "date": "2009-10-03T17:12:52+0000",
            "content": "Seems like if you want to replicate only optimized indexes, we should recommend the user configure SolrDeletionPolicy to always keep an optimized commit point around.\nIf you rely on the replication handler to reserve it, it won't work across a reboot, right?\n\nAlthough I see no reason for custom delete policies, I'm not really against adding support for that in the replication handler as long as people are confident the changes don't introduce any new bugs.\nRegardless, I think the separate count for optimized commit points that I added to SolrDeletionPolicy should remain (esp since it fixed other bugs too). "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12762127",
            "date": "2009-10-05T09:37:41+0000",
            "content": "added methods to reserve and unreserve to IndexDeletionPolicyWrapper "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12762129",
            "date": "2009-10-05T09:44:08+0000",
            "content": "we need to ensure that any component which is using a commit point should reserve it till it is done. After that it should be unreserved.\n\nIf you rely on the replication handler to reserve it, it won't work across a reboot, right?\n\nThere is a problem with the current configuration \n\nexample\n\nset\nreplicateAfter=optimize\n\nThis means that if the master is restarted soon after an optimize , the slaves will not get the new commit point\nset\nreplicateAfter=optimize\nreplicateAfter=startup\n\nThis means that the replication will pick up the latest commit point after a master restart. not necessarily an optimized one.\n\nSo the solution should be , if replicateAfter=commit is absent,  then pickup the latest optimized commitpoint \n\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12762145",
            "date": "2009-10-05T10:39:06+0000",
            "content": "I guess this should work. We need a testcase for only replicateAfter=optimize and replicateAfter=startup "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12762524",
            "date": "2009-10-06T06:10:59+0000",
            "content": "I plan to commit this shortly,\nPlease comment  if there is any concern "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12764147",
            "date": "2009-10-09T18:36:34+0000",
            "content": "\n\tadded the logic to actually check the saved commits in delete()... wouldn't have worked w/o that.\n\tadded protection against a NPE if no searcher had been opened in inform()\n\tchanged reservedCommits to savedCommits since we already use \"reserve\" to describe short term reservations\n\tadded getUserData() to the commit point wrapper\n\tadded @Override to commit point wrapper to more easily detect future changes\n\ta little javadoc for the new methods\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12775887",
            "date": "2009-11-10T15:52:16+0000",
            "content": "Bulk close for Solr 1.4 "
        }
    ]
}