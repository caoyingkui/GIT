{
    "id": "LUCENE-6863",
    "title": "Store sparse doc values more efficiently",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [
            "5.4",
            "6.0"
        ],
        "priority": "Major",
        "status": "Closed",
        "type": "Improvement"
    },
    "description": "For both NUMERIC fields and ordinals of SORTED fields, we store data in a dense way. As a consequence, if you have only 1000 documents out of 1B that have a value, and 8 bits are required to store those 1000 numbers, we will not require 1KB of storage, but 1GB.\n\nI suspect this mostly happens in abuse cases, but still it's a pity that we explode storage requirements. We could try to detect sparsity and compress accordingly.",
    "attachments": {
        "LUCENE-6863.patch": "https://issues.apache.org/jira/secure/attachment/12769426/LUCENE-6863.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14979373",
            "author": "Adrien Grand",
            "date": "2015-10-28T22:25:48+0000",
            "content": "Here is a patch. It detects sparsity by checking if less than 10% documents have a value. When this happens, it stores doc IDs that have a value using a DirectMonotonicWriter and non-missing values as a regular numeric field (so values will still be compressed with table/gcd compression if applicable).\n\nI first wanted to update the format javadocs but noticed that the low-level javadocs were very outdated (still documenting that sorted doc values were stored in a FST) so I decided to remove the low-level description in favour of the high-level one, which is much more interesting in my opinion.\n\nThis is just a first iteration, tests pass but maybe the heuristic needs to be better thought. I will also do some benchmarking. "
        },
        {
            "id": "comment-14979462",
            "author": "Yonik Seeley",
            "date": "2015-10-28T23:24:24+0000",
            "content": "+1 for sparse support!  Sparse fields are a fact of life for a lot of users.  Sometimes they can encode the info into a common field, but it's often not straight-forward to do so. "
        },
        {
            "id": "comment-14979524",
            "author": "Robert Muir",
            "date": "2015-10-29T00:04:41+0000",
            "content": "\nHere is a patch. It detects sparsity by checking if less than 10% documents have a value.\n\nPlease, put a minimum count like 1024 where this starts to make sense, so types arent flipping back and forth during ordinary NRT search. And where did 10% come from? It seems far too aggressive. In previous work we determined much lower thresholds to work better (e.g. 3%). "
        },
        {
            "id": "comment-14979533",
            "author": "Robert Muir",
            "date": "2015-10-29T00:08:55+0000",
            "content": "I think this needs to be a very low number, if its going to be \"automatic\", like 3%, pure abuse case detection, just like how sparse norms worked.\n\nOtherwise, it needs to be a separate format in codecs/ that is explicit opt-in by the user. "
        },
        {
            "id": "comment-14979559",
            "author": "Adrien Grand",
            "date": "2015-10-29T00:32:44+0000",
            "content": "And where did 10% come from? It seems far too aggressive.\n\nIt was just a start so that I could easily trigger usage of this compression in tests. I agree this is very likely too aggressive, this is why I said the heuristic needs to be better thought. I will also do some benchmarking, if this ends up being much slower than regular (delta) compression, we might want to have an even lower threshold. "
        },
        {
            "id": "comment-14979601",
            "author": "Yonik Seeley",
            "date": "2015-10-29T01:00:31+0000",
            "content": "It was just a start so that I could easily trigger usage of this compression in tests.\n\nAdding a small constant always helps with test coverage... (n*0.05 + 5) for example. "
        },
        {
            "id": "comment-14979871",
            "author": "Varun Thacker",
            "date": "2015-10-29T05:50:27+0000",
            "content": "Hi Adrien,\n\nGreat idea to use heuristics and use it automatically. Looking forward to the benchmarks which I never got to in LUCENE-5688 \n\nIs it okay if we mark LUCENE-5688 and LUCENE-4921 as duplicate of this Jira or could there still be plans on having a specialized doc value format? "
        },
        {
            "id": "comment-14980418",
            "author": "Adrien Grand",
            "date": "2015-10-29T13:30:09+0000",
            "content": "Is it okay if we mark LUCENE-5688 and LUCENE-4921 as duplicate of this Jira or could there still be plans on having a specialized doc value format?\n\nI think it would be a bit premature to say the other jiras are superseded by this one, it's not clear yet whether the proposed approach here is actually a better idea and/or could make it to the default codec. I suggest that we wait to see how happy benchmarks are with this patch first. "
        },
        {
            "id": "comment-14980687",
            "author": "Adrien Grand",
            "date": "2015-10-29T16:06:14+0000",
            "content": "I ran some benchmarks with the geoname dataset which has a few sparse fields:\n\n\tcc2: 3.2% of documents have this field, which has 573 unique values\n\tadmin4: 4.3% of documents have this field, which has 102950 unique values\n\tadmin3: 10.2% of documents have this field, which has 73120 unique values\n\tadmin2: 45.3% of documents have this field, which has 30603 unique values\n\n\n\nFirst I enabled sparse compression on all fields, regardless of density to see how this compares to the delta compression that we use by default, and then ran two kinds of queries:\n\n\tqueries on a random partition of the index, which I guess would be the case when you have true sparse fields\n\ta query only on documents that have a value, which I guess would be more realistic if you store several types of data in the same index that don't have the same fields\n\n\n\n\n\n\nField\ndisk usage for ordinals\nmemory usage with sparse compression enabled\nsort performance on a MatchAllDocsQuery\nsort performance on a term query that matches 10% of docs\nsort performance on a term query that matches 1% of docs\nsort performance on a term query that matches docs that have the field\n\n\ncc2 \n -88%\n1680 bytes\n-27%\n+25%\n+58%\n+208%\n\n\nadmin4\n-86%\n568 bytes\n-20%\n+7%\n-20%\n+214%\n\n\nadmin3\n-67%\n1312 bytes\n+11%\n+57%\n+42%\n+236%\n\n\nadmin2 \n+17%\n2904 bytes\n+132%\n+275%\n+331%\n+221%\n\n\n\n\n\nThe reduction in disk usage is significant, but so is the slowdown, especially when running a query that only matches docs that have a value. However memory usage looks acceptable to me for 10M docs.\n\nI couldn't test with 3% as even the rarest field is contained by 3.2% of documents, but I updated the heuristic to require at least 1024 docs in the segment (like Robert suggested) and that less than 5% of docs have a value:\n\n\n\n\nField\nmemory usage due to sparse compression\nsort performance on a MatchAllDocsQuery\nsort performance on a term query that matches 10% of docs\nsort performance on a term query that matches 1% of docs\nsort performance on a term query that matches docs that have the field\n\n\ncc2 \n 1680 bytes\n-10%\n+34%\n+62%\n+214%\n\n\nadmin4\n568 bytes\n-7%\n+20%\n-14%\n+241%\n\n\nadmin3\n576 bytes\n+9%\n+7%\n+11%\n+10%\n\n\nadmin2 \n1008 bytes\n+1%\n+8%\n+9%\n+11%\n\n\n\n\n\nTo my surprise, admin2 and admin3 were still using sparse compression on some segments. The reason is that documents with sparse values are not uniform in the dataset but rather clustered: I suspect this partially explains of the slowdown for admin2/admin3, maybe there is also hotspot not liking having more impls to deal with. "
        },
        {
            "id": "comment-14980764",
            "author": "Adrien Grand",
            "date": "2015-10-29T16:47:30+0000",
            "content": "Here is an updated patch that makes the sparse impl a bit more efficient when consumed in sequential order by keeping track of the upper bound of the current window. This is what has been used in the above benchmark.\n\nI also updated the heuristic to require 1024 docs in the segment and that less than 1% of docs have a value in order to be on the safe side and to only slow down abuse/exceptionnal cases. Even if/when this gets used for some fields, I think the slowdown is acceptable insofar as it would only slow down fast queries: if you look at the above benchmarks, when the query matches many docs (such as a MatchAllDocsQuery) this encoding is actually faster than regular delta encoding. Only queries that match a small partition of the index (so that most dv lookups will require a binary search) would become slower.\n\nOpinions? "
        },
        {
            "id": "comment-14982596",
            "author": "David Smiley",
            "date": "2015-10-30T14:07:24+0000",
            "content": "Did you consider a hash lookup instead of binary-search, as was done in LUCENE-5688?  I just read the comments there and it seems promising for very sparse data.  \n\nRegarding the performance trade-off in your table \u2013 I find it hard to evaluate to consider if it's worth it.  Does +214% mean the whole query, search & top-10 doc retrieval took over twice as long?  Or is this measurement isolated to... somehow just the sort part?  How fast was this query any way?  If we're making a 3ms query take 9ms then it wouldn't bother me so much as a 300ms query taking 900ms.  Of course it depends on the amount of data. "
        },
        {
            "id": "comment-14982832",
            "author": "Adrien Grand",
            "date": "2015-10-30T16:36:56+0000",
            "content": "Did you consider a hash lookup instead of binary-search, as was done in LUCENE-5688? I just read the comments there and it seems promising for very sparse data. \n\nI took this approach because I saw a couple of setups that had lots of fields, many of those being sparse and the fact that sparse fields require as much resources as the dense ones is a bit frustrating. The issue description focuses on disk usage, but I think memory usage is even more important. Obviously I had to increase memory usage (since numerics don't require any memory at all today) but I wanted to keep it very low. For instance, if you look at the cc2 field, about 320k documents have a value for this field and yet it only takes 1680 bytes of memory for the entire index, so about 0.005 byte per document. With a hastable, you would either put data into memory and you could hardly avoid using one or more bytes per documents, or on disk but then the random-access pattern could be a problem if not everything fits into your filesystem cache. In contrast, the patch keeps memory usage very low and keeps the access pattern sequential by keeping track of the previous/next documents that have a value and using exponential search (a variant of binary search) to seek forward.\n\nDoes +214% mean the whole query, search & top-10 doc retrieval took over twice as long?\n\nI computed \n\n(${new response time} - ${old response time}) / ${old response time}\n\n so it means more than 3x slower actually. However this does not include loading stored fields, just computing the top document by calling IndexSearcher.search(query, 1, sort).\n\nHow fast was this query any way?\n\nHere are the times it take to run these queries on trunk (in milliseconds).\n\n\n\n\nField\nsort time on a MatchAllDocsQuery\nsort time on a term query that matches 10% of docs\nsort time on a term query that matches 1% of docs\nsort time on a term query that matches docs that have the field\n\n\ncc2 \n128\n20\n2\n6\n\n\nadmin4\n122\n19\n3\n7\n\n\nadmin3\n116\n18\n3\n17\n\n\nadmin2 \n121\n19\n3\n78\n\n\n\n\n\nI've put the response times that we are making significantly slower in red and those that we are making faster in green. So as you can see, the queries that we are speeding up are among the slower ones, and those that we are slowing down are among the faster ones. "
        },
        {
            "id": "comment-14982914",
            "author": "David Smiley",
            "date": "2015-10-30T17:21:37+0000",
            "content": "For instance, if you look at the cc2 field, about 320k documents have a value for this field and yet it only takes 1680 bytes of memory for the entire index, so about 0.005 byte per document. With a hastable, you would either put data into memory and you could hardly avoid using one or more bytes per documents, or on disk but then the random-access pattern could be a problem if not everything fits into your filesystem cache.\n\nIf it's sparse (thus small) and the user has chosen to use docValues because it's going to be sorted/faceted/etc (thus it's likely 'hot'  i.e. used) then I think it's reasonable to expect it'll be in the filesystem cache?  \n\nNonetheless what you've done here is good.  I think that an in-memory hash would be a nice alternative for those that elect for the trade-off.  On a semi-related note, I wonder how well a 4-byte int FST -> long would perform.\n\nHere are the times it take to run these queries on trunk (in milliseconds).\n\nAh; looks like a fair trade-off to me.  Thanks for these details. "
        },
        {
            "id": "comment-14983027",
            "author": "Erick Erickson",
            "date": "2015-10-30T18:32:21+0000",
            "content": "Bypassing whether one should supercede the other, but linking so they can be found more easily "
        },
        {
            "id": "comment-14985604",
            "author": "Adrien Grand",
            "date": "2015-11-02T17:45:42+0000",
            "content": "Updated patch that:\n\n\tmakes the code a bit more readable and adds comments\n\tavoids loading a slice for values when only docs with field are requested\n\tsaves some monotonic lookups\n\n\n\nHere is an updated result of the benchmark (still with a threshold of 5% for benchmarking purposes, even though the patch still has a threshold of 1%), computed exactly the same way as above. It makes the slowdown a bit more contained. Times are in ms.\n\n\n\n\nField\nsort performance on a MatchAllDocsQuery\nsort performance on a term query that matches 10% of docs\nsort performance on a term query that matches 1% of docs\nsort performance on a term query that matches docs that have the field\n\n\ncc2 \n128\u219299 (-23%)\n21.8\u219223.8 (+9%)\n2.92\u21924.33 (+48%)\n6.84\u219213.0 (+90%)\n\n\nadmin4\n121\u219298 (-19%)\n21.4\u219221.1 (-1%)\n 3.65\u21922.81 (-23%)\n8.36\u219216.6 (+98%)\n\n\nadmin3\n116\u2192125 (+1%)\n20.6\u219220.0 (-3%)\n3.20\u21923.24 (+1%)\n18.9\u219219.4 (+8%)\n\n\nadmin2 \n124\u2192132 (+6%)\n21.5\u219220.6 (-4%)\n3.30\u21923.49 (+6%)\n8.58\u21928.64 (+1%)\n\n\n\n\n\nI think the change is good to go, but I know this can be controversial. Please let me know if you have concerns, otherwise I plan to commit it by the end of the week. "
        },
        {
            "id": "comment-14993625",
            "author": "ASF subversion and git services",
            "date": "2015-11-06T13:04:38+0000",
            "content": "Commit 1712957 from Adrien Grand in branch 'dev/trunk'\n[ https://svn.apache.org/r1712957 ]\n\nLUCENE-6863: Optimized storage requirements of doc values fields when less than 1% of documents have a value. "
        },
        {
            "id": "comment-14993774",
            "author": "ASF subversion and git services",
            "date": "2015-11-06T14:58:56+0000",
            "content": "Commit 1712973 from Adrien Grand in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1712973 ]\n\nLUCENE-6863: Optimized storage requirements of doc values fields when less than 1% of documents have a value. "
        }
    ]
}