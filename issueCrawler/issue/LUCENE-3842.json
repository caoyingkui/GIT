{
    "id": "LUCENE-3842",
    "title": "Analyzing Suggester",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/spellchecker"
        ],
        "type": "New Feature",
        "fix_versions": [
            "4.1",
            "6.0"
        ],
        "affect_versions": "3.6,                                            4.0-ALPHA",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Since we added shortest-path wFSA search in LUCENE-3714, and generified the comparator in LUCENE-3801,\nI think we should look at implementing suggesters that have more capabilities than just basic prefix matching.\n\nIn particular I think the most flexible approach is to integrate with Analyzer at both build and query time,\nsuch that we build a wFST with:\ninput: analyzed text such as ghost0christmas0past <-- byte 0 here is an optional token separator\noutput: surface form such as \"the ghost of christmas past\"\nweight: the weight of the suggestion\n\nwe make an FST with PairOutputs<weight,output>, but only do the shortest path operation on the weight side (like\nthe test in LUCENE-3801), at the same time accumulating the output (surface form), which will be the actual suggestion.\n\nThis allows a lot of flexibility:\n\n\tUsing even standardanalyzer means you can offer suggestions that ignore stopwords, e.g. if you type in \"ghost of chr...\",\n  it will suggest \"the ghost of christmas past\"\n\twe can add support for synonyms/wdf/etc at both index and query time (there are tradeoffs here, and this is not implemented!)\n\tthis is a basis for more complicated suggesters such as Japanese suggesters, where the analyzed form is in fact the reading,\n  so we would add a TokenFilter that copies ReadingAttribute into term text to support that...\n\tother general things like offering suggestions that are more \"fuzzy\" like using a plural stemmer or ignoring accents or whatever.\n\n\n\nAccording to my benchmarks, suggestions are still very fast with the prototype (e.g. ~ 100,000 QPS), and the FST size does not\nexplode (its short of twice that of a regular wFST, but this is still far smaller than TST or JaSpell, etc).",
    "attachments": {
        "LUCENE-3842-TokenStream_to_Automaton.patch": "https://issues.apache.org/jira/secure/attachment/12517000/LUCENE-3842-TokenStream_to_Automaton.patch",
        "LUCENE-3842.patch": "https://issues.apache.org/jira/secure/attachment/12516958/LUCENE-3842.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-03-03T20:52:57+0000",
            "content": "messy,dirty,drinking beer on a saturday-afternoon style patch with tons of nocommits.\n\nparticularly:\n\n\twe should make the separator optional, i dont think e.g. for the japanese case we care so much about segmentation but readings, but for the english case we do.\n\tboth index-time and query-time should respect position increments (if you dont want that for e.g. stopfilter, set your analyzer correctly). But i dont even want to go this route until after LUCENE-3767 has landed, it adds PositionLengthAttribute, and there is no sense in dealing with a \"lossy\" tokenstream since we want the real graph, not one thats been sausaged into the existing tokenstream model.\n\tduplicates on the input side dont matter, of course two suggestions could analyze to the same thing. Though this is not particularly desirable (as they will likely result in the same query results), I dont think we should just drop duplicates for the highest cost? instead we can add bogus bytes to the end of the FST input side to uniquify them, since it wont matter anyway.\n\n ",
            "author": "Robert Muir",
            "id": "comment-13221690"
        },
        {
            "date": "2012-03-03T20:54:27+0000",
            "content": "also just so i dont forget, we should allow separate specification of \"index-time\" and \"query-time\" analyzers...\nbecause in cases where you are adding synonyms/wdf/etc there is a tradeoff (bigger FST, versus slower query-time perf) ",
            "author": "Robert Muir",
            "id": "comment-13221691"
        },
        {
            "date": "2012-03-03T20:58:27+0000",
            "content": "Linking to SOLR-2479 since i think this is the most obvious way to support infix suggestions... we already have the analyzer splitting the terms into \"words\" so its just a matter of indexing tokenstream suffixes if the user wants.\n\nHowever, I think anything that supports infixing should look at this as word-level edit distance... we should apply some cost penalty to these since they are somewhat of a \"reach\" for a query suggestion.\n\nI think we want to allow several parameters for that though:\n\n\tmaximum infix length (we should only infix suffixes up to some N)\n\tinfix penalty (I think when indexing infixed suffixes we should simply bake a penalty into the FST)\n\n ",
            "author": "Robert Muir",
            "id": "comment-13221693"
        },
        {
            "date": "2012-03-04T00:21:04+0000",
            "content": "VERY cool  ",
            "author": "Michael McCandless",
            "id": "comment-13221748"
        },
        {
            "date": "2012-03-04T10:39:19+0000",
            "content": "Once posLength is in, I think a very simple way to handle multiple paths at query time is to open up the TopNSearcher class in oal.fst.Util.\n\nCurrently the API only allows you to pass in a single starting FST node, but we can easily improve this by adding eg a addStartNode(FST.Arc<T>, int startingCost) instead.  This way the app could create a TopNSearcher, add any number of start nodes with the initial path cost, then call .search() to get the best completions.\n\nThe only limitation of this is that all differences must be pre-computed as an initial path cost that's \"consistent\" with how the path costs are accumulated (with the Outputs.add) during searching; I'm not sure if that'd be overly restrictive here? ",
            "author": "Michael McCandless",
            "id": "comment-13221844"
        },
        {
            "date": "2012-03-04T14:31:17+0000",
            "content": "Patch with a static utility method to translate a TokenStream to a byte-by-byte automaton.\n\nIt respects posInc and posLength but there are lots of nocommits still!\n\nI think we can use this at index time and maybe query time to graph analyzers... but we still need the \"enumerate all paths from this automaton\" method... ",
            "author": "Michael McCandless",
            "id": "comment-13221902"
        },
        {
            "date": "2012-03-04T15:00:28+0000",
            "content": "Wow, thats awesome Mike... its so small too!\n\nI think we can integrate these two patches into one, with your patch and your addStartNode idea\nthe AnalyzingSuggester should be able to support both index-time and query-time synonyms/wdf/whatever\ncrazy stuff we throw at it \n\n\nbut we still need the \"enumerate all paths from this automaton\" method...\n\nBrics has some code for this (puts all the accepted strings into a set). we should be able to do\nsomething similar, to create a set of bytesref? \n\nBut really, i'm not sure we need a 'general' method for this. i think we should just have an enumerator\nfor finite automata (e.g. tokenstream) as we can probably make this a 'real' enum rather than creating\na massive list/set, we dont need the set deduplication at all either, because its finite. ",
            "author": "Robert Muir",
            "id": "comment-13221910"
        },
        {
            "date": "2012-03-04T16:38:16+0000",
            "content": "Patch with a static utility method to translate a TokenStream to a byte-by-byte automaton.\n\nI looked at the patch but I don't fully get what it does. Looks like a combination of state sequence unions, am I right?\n\nBrics has some code for this (puts all the accepted strings into a set). \n\nIt's probably a naive walk with an acceptor. I've always wanted to see what Brics returns from that method for an automaton equivalent to .*  ",
            "author": "Dawid Weiss",
            "id": "comment-13221929"
        },
        {
            "date": "2012-03-04T16:42:39+0000",
            "content": "\nI've always wanted to see what Brics returns from that method for an automaton equivalent to .* \n\nOh, so it is only for finite automata, so it returns null in that case:\n\nhttp://www.brics.dk/automaton/doc/dk/brics/automaton/SpecialOperations.html#getFiniteStrings%28dk.brics.automaton.Automaton%29 ",
            "author": "Robert Muir",
            "id": "comment-13221930"
        },
        {
            "date": "2012-03-04T16:48:23+0000",
            "content": "\nI looked at the patch but I don't fully get what it does. Looks like a combination of state sequence unions, am I right?\n\nWell the conversion should ultimately be more useful for the suggester to intersect with the FST than a tokenstream, because a tokenstream is like a word-level automaton, if dogs is a synonym for dog, then we have:\nsmelly dog|dogs(positionIncrement=0). \n\nSo for our intersection, we would prefer it to be a deterministic at 'character' (byte) level instead. So the conversion should produce an automaton of: smelly dog(s?) in regex notation... this is easier to work with.\n\nat index time its useful too, because in the FST we only care about all the possible byte strings, so this should be easier to enumerate than a tokenstream (especially if you consider multiword synonyms, decompounded terms etc where some span across many). ",
            "author": "Robert Muir",
            "id": "comment-13221932"
        },
        {
            "date": "2012-03-04T16:57:49+0000",
            "content": "Ok, get it, thanks. I wonder if it's always possible, but I bet you can write a random test to ensure that  ",
            "author": "Dawid Weiss",
            "id": "comment-13221934"
        },
        {
            "date": "2012-03-04T17:01:50+0000",
            "content": "It also occurred to me that it would be interesting to have a naive minimalization technique for Brics Automata which would (for automata with a finite language):\n\n\tenumarate the language\n\tsort\n\tbuild the minimal automaton\n\n\n\nWhile it may seem like an idea with crazy overhead it may actually be a viable alternative to minimization algorithms in Brics for very large automata. Interesting. ",
            "author": "Dawid Weiss",
            "id": "comment-13221935"
        },
        {
            "date": "2012-03-04T17:04:57+0000",
            "content": "Dawid: hmm, well the transitions in brics are ranges in sorted order, so, if its finite, couldn't we just enumerate the language in sorted order while building the minimal automaton incrementally in parallel?\n\nOr am i missing something... its sunday...  ",
            "author": "Robert Muir",
            "id": "comment-13221936"
        },
        {
            "date": "2012-03-04T17:20:04+0000",
            "content": "Yep, sure you could (I admit I haven't looked at Brics in a long time so I don't remember the details, but I do remember the overhead was significant on optimization; this was a while ago - maybe it's improved). ",
            "author": "Dawid Weiss",
            "id": "comment-13221940"
        },
        {
            "date": "2012-03-14T20:24:54+0000",
            "content": "updated patch, tying in Mike's patch too.\n\nCurrently my silly test fails because it trips mike's assert. it starts with a stopword  ",
            "author": "Robert Muir",
            "id": "comment-13229581"
        },
        {
            "date": "2012-03-14T20:28:33+0000",
            "content": "I also don't think we really need this generic getFiniteStrings. its just to get it off the ground.\nwe can just write the possibilities on the fly i think and it will be simpler... ",
            "author": "Robert Muir",
            "id": "comment-13229584"
        },
        {
            "date": "2012-05-10T14:57:32+0000",
            "content": "merged the patch up to trunk. But it still trips the assert in tokenStreamToAutomaton because the test begins with a stopword. ",
            "author": "Robert Muir",
            "id": "comment-13272395"
        },
        {
            "date": "2012-05-10T15:17:40+0000",
            "content": "i see the problem. it actually happens on the second term (we have ghost/2 christmas/2).\nThe problem is it tries to find the last state to connect the new node, but it uses\na hashmap based on position for that... so if there are holes this returns null.\n\nI think for this code we would add nodes for holes (text=POS_SEP) to simplify the logic? ",
            "author": "Robert Muir",
            "id": "comment-13272413"
        },
        {
            "date": "2012-05-11T12:39:07+0000",
            "content": "updated patch: i fixed the bug in tokenStreamToAutomaton (just use lastEndPos instead) ",
            "author": "Robert Muir",
            "id": "comment-13273204"
        },
        {
            "date": "2012-05-11T16:26:07+0000",
            "content": "Patch, fixing TS2A to insert holes ... this is causing the AnalyzingCompletionTest.testStandard to fail... we have to fix its query-time to insert holes too... ",
            "author": "Michael McCandless",
            "id": "comment-13273399"
        },
        {
            "date": "2012-05-11T17:05:04+0000",
            "content": "testStandard is also bogus: it has 2 asserts. the first one should pass, but the second one should\nreally only work if you disable positionincrements in the (mock) stopfilter. ",
            "author": "Robert Muir",
            "id": "comment-13273419"
        },
        {
            "date": "2012-05-12T20:17:44+0000",
            "content": "New patch, getting us closer ... I opened up Util.shortestPaths so that you can make a TopNSearcher and seed multiple start nodes into its queue... and I created an intersectPaths method to intersect an automaton with an FST, gathering the end nodes and the accumulated outputs.  Then I fixed lookup to use these two to enumerate and complete the paths.\n\nThe first test in testStandard now passes, but not the 2nd one (I haven't tried disabling posincs in the StopFilter yet). ",
            "author": "Michael McCandless",
            "id": "comment-13274065"
        },
        {
            "date": "2012-05-12T20:36:30+0000",
            "content": "OK, when I pass false for enablePositionIncrements to MockAnalyzer in testStandard then both cases pass... and I added a 3rd case testing for \"ghost chris\" and it also passes... cool! ",
            "author": "Michael McCandless",
            "id": "comment-13274068"
        },
        {
            "date": "2012-05-15T15:42:18+0000",
            "content": "New patch, fixing some nocommits, and a separate bug where WFSTSuggester can return a dup suggestion.  Still more to do... ",
            "author": "Michael McCandless",
            "id": "comment-13275888"
        },
        {
            "date": "2012-05-15T15:48:59+0000",
            "content": "Nice catch on the exactFirst dup problem! ",
            "author": "Robert Muir",
            "id": "comment-13275895"
        },
        {
            "date": "2012-05-15T23:14:26+0000",
            "content": "When running the benchmark (LookupBenchmarkTest) i noticed the FST size has increased since the original patch. I wonder why this is? the benchmark uses KeywordAnalyzer... it could be (likely even) that the original patch had a bug and now its correct, but maybe its worth investigation... ",
            "author": "Robert Muir",
            "id": "comment-13276326"
        },
        {
            "date": "2012-05-17T19:52:56+0000",
            "content": "I was not able to apply the latest patch cleanly\n\n\nsmg@dev21:~/lucene_trunk$ patch -p0 < ~/LUCENE-3842.patch \npatching file lucene/test-framework/src/java/org/apache/lucene/util/RollingBuffer.java\npatching file lucene/core/src/java/org/apache/lucene/util/automaton/SpecialOperations.java\npatching file lucene/core/src/java/org/apache/lucene/util/RollingBuffer.java\nHunk #1 FAILED at 109.\n1 out of 1 hunk FAILED \u2013 saving rejects to file lucene/core/src/java/org/apache/lucene/util/RollingBuffer.java.rej\npatching file lucene/core/src/java/org/apache/lucene/util/fst/Util.java\npatching file lucene/core/src/java/org/apache/lucene/analysis/TokenStreamToAutomaton.java\npatching file lucene/core/src/test/org/apache/lucene/analysis/TestGraphTokenizers.java\npatching file lucene/core/src/test/org/apache/lucene/util/automaton/TestSpecialOperations.java\npatching file lucene/suggest/src/test/org/apache/lucene/search/suggest/analyzing/AnalyzingCompletionTest.java\npatching file lucene/suggest/src/test/org/apache/lucene/search/suggest/LookupBenchmarkTest.java\npatching file lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingCompletionLookup.java\npatching file lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/FSTUtil.java\n\nI needed to copy RollingBuffer.java to from test-framework to core for the patch to apply cleanly.\n\n\ncp lucene/test-framework/src/java/org/apache/lucene/util/RollingBuffer.java lucene/core/src/java/org/apache/lucene/util/\n\n ",
            "author": "Sudarshan Gaikaiwari",
            "id": "comment-13278166"
        },
        {
            "date": "2012-05-17T21:05:30+0000",
            "content": "Hi Sudarshan, sorry that was my bad: I had svn mv'd RollingBuffer but when I created the patch I failed to pass --show-copies-as-adds to svn ... so you have to do that mv yourself before applying the patch... ",
            "author": "Michael McCandless",
            "id": "comment-13278267"
        },
        {
            "date": "2012-05-31T01:26:11+0000",
            "content": "Hi Michael,\n\nThanks a lot for opening up Util.shortestPaths, now that I can seed the queue with the intial nodes using addStartPaths the performance of the GeoSpatialSuggest that I presented at Lucene Revolution has been improved by 2x.\n\nWhile migrating my code to use this patch, I noticed that I would hit the following assertion in addIfCompetitive.\n\n\n          path.input.length--;\n          assert cmp != 0;\n          if (cmp < 0) {\n\n\n\nThis assert fires when it is not possible to differentiate between the path that we are trying to add to the queue and the bottom. This happens because the different paths that lead to FST nodes during the automata FST intersection are not stored. So the inputpath used to differentiate path contains only the characters that have been consumed from one of the initial FST nodes.\n\nFrom your comments for the addStartPaths method I think that you have foreseen this problem.\n\n\n    // nocommit this should also take the starting\n    // weight...?\n\n    /** Adds all leaving arcs, including 'finished' arc, if\n     *  the node is final, from this node into the queue.  */\n    public void addStartPaths(FST.Arc<T> node, T startOutput, boolean allowEmptyString) throws IOException {\n\n\n\nHere is a unit test that causes the assert to be triggered.\n\n\n  public void testInputPathRequired() throws Exception {\n    TermFreq keys[] = new TermFreq[] {\n        new TermFreq(\"fast ghost\", 50),\n        new TermFreq(\"quick gazelle\", 50),\n        new TermFreq(\"fast ghoul\", 50),\n        new TermFreq(\"fast gizzard\", 50),\n    };\n\n    SynonymMap.Builder b = new SynonymMap.Builder(false);\n    b.add(new CharsRef(\"fast\"), new CharsRef(\"quick\"), true);\n    final SynonymMap map = b.build();\n\n    final Analyzer analyzer = new Analyzer() {\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MockTokenizer(reader, MockTokenizer.SIMPLE, true);\n        TokenStream stream = new SynonymFilter(tokenizer, map, true);\n        return new TokenStreamComponents(tokenizer, new RemoveDuplicatesTokenFilter(stream));\n      }\n    };\n    AnalyzingCompletionLookup suggester = new AnalyzingCompletionLookup(analyzer);\n    suggester.build(new TermFreqArrayIterator(keys));\n    List<LookupResult> results = suggester.lookup(\"fast g\", false, 2);\n  }\n\n\n\nPlease let me know if the above analysis looks correct to you and I will start trying to fix this by storing paths during the FST automata intersection. ",
            "author": "Sudarshan Gaikaiwari",
            "id": "comment-13286249"
        },
        {
            "date": "2012-06-01T12:53:45+0000",
            "content": "Hi Sudarshan, thanks for raising this ... I'll have a look... ",
            "author": "Michael McCandless",
            "id": "comment-13287369"
        },
        {
            "date": "2012-06-02T11:32:21+0000",
            "content": "Please let me know if the above analysis looks correct to you \n\nThat sounds right to me: when we do the initial intersection of the suggest FST with A (from the analyzer), it looks like we must keep the full input path (BytesRef) and carry that over when we pass the node to addStartPaths.\n\nEither that, or ... we change how we break ties in the scores?  EG maybe we can tie-break instead by the surface form? ",
            "author": "Michael McCandless",
            "id": "comment-13287887"
        },
        {
            "date": "2012-06-28T12:03:19+0000",
            "content": "no changes: just syncing up with trunk ",
            "author": "Robert Muir",
            "id": "comment-13403042"
        },
        {
            "date": "2012-06-30T16:19:14+0000",
            "content": "maybe we can tie-break instead by the surface form?\n\nThe FST construction guarantees that the input paths leading to different nodes are unique, while I don't think we have such a guarantee about the surface form.\n\nI have attached a patch that modifies the intersectPrefixPaths method to keep track of the input paths as the automaton and FST are intersected. Please let me know if that look ok to you? ",
            "author": "Sudarshan Gaikaiwari",
            "id": "comment-13404537"
        },
        {
            "date": "2012-09-15T16:51:30+0000",
            "content": "Sudarshan those changes look great!  You now also record the input for every Path coming back from intersectPrefixPaths, and pass that to addStartPaths.  Sorry it took so long to have a look!\n\nI started from your patch, got up to trunk again (there was one compilation error I think), added some comments, downgraded a nocommit.\n\nI think we are close here ... I'll make a branch so we can iterate. ",
            "author": "Michael McCandless",
            "id": "comment-13456442"
        },
        {
            "date": "2012-09-15T16:59:33+0000",
            "content": "OK I created branch: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3842 ",
            "author": "Michael McCandless",
            "id": "comment-13456443"
        },
        {
            "date": "2012-09-15T18:54:29+0000",
            "content": "Thanks for resurrecting this from the dead! I had forgotten just how fun this issue was  ",
            "author": "Robert Muir",
            "id": "comment-13456468"
        },
        {
            "date": "2012-09-15T18:56:51+0000",
            "content": "Patch, addressing a few nocommits and getting PRESERVE_HOLES and PRESERVE_SEPS working.  I did this by adding options to AnalyzingCompletionLookup, and then post-processing the returned automaton from TokenStreamToAutomaton.\n\nI also added a couple new nocommits .... ",
            "author": "Michael McCandless",
            "id": "comment-13456469"
        },
        {
            "date": "2012-09-15T19:09:12+0000",
            "content": "New patch, removing \"preserve holes\" option from AnalyzingCompletionLookup: you can simply tell your StopFilter whether or not holes are meaningful. ",
            "author": "Michael McCandless",
            "id": "comment-13456476"
        },
        {
            "date": "2012-09-15T19:12:11+0000",
            "content": "+1 for that. lets keep this as simple as possible and leave the responsibility to the analyzer as much as possible.\n\nMy main concern for the PRESERVE_SEPS was for the japanese use case: we don't much care what the actual tokenization\nof the japanese words was, only the concatenated reading string. If the tokenization is a little off but the concatenation\nof all the readings is still correct, then we are ok. So it makes it more robust against tokenization differences,\nespecially considering its partial inputs going into this thing (not whole words) ",
            "author": "Robert Muir",
            "id": "comment-13456477"
        },
        {
            "date": "2012-09-16T12:53:10+0000",
            "content": "Patch, getting \"duplicates\" (different surface forms analyzing to same analyzed bytes) fully working, with exactFirst.\n\nI did this by having the FST handle multiple outputs for a single input (this is something it can do in general but we don't often use it)...\n\nBut ... I don't really like the approach: it makes the path/results handling more complex because now a single \"path\" can correspond to multiple results.\n\nI think it would be better/cleaner to append unique (disambiguating) bytes to the end of the analyzed bytes (this was Robert's original idea): then each path is a single result.  The only downside I can think of is we will have to reserve a byte (0xFF?), ie we'd append 0xFF 0x00, then 0xFF 0x01 to the next duplicate, ... but since these input BytesRefs are \"typically\" UTF-8 ... this seems not so bad?  Then can of course in general be arbitrary bytes since they are produced by the analysis process... ",
            "author": "Michael McCandless",
            "id": "comment-13456569"
        },
        {
            "date": "2012-09-17T01:46:06+0000",
            "content": "A common Suggester use case is to boost the results by closest (auto suggest the whole USA but boost the results in the suggester by geodistance). WOuld love to get faster response with that. At the Lucene Revolution 2012 in Boston a speaker did discuss using WFST to do this, but I have yet to figure out how to do it). ",
            "author": "Bill Bell",
            "id": "comment-13456728"
        },
        {
            "date": "2012-09-17T01:56:55+0000",
            "content": "Bill can you start a different thread for that. Its unrelated to this issue. ",
            "author": "Robert Muir",
            "id": "comment-13456732"
        },
        {
            "date": "2012-09-18T02:25:21+0000",
            "content": "\nI think it would be better/cleaner to append unique (disambiguating) bytes to the end of the analyzed bytes (this was Robert's original idea): then each path is a single result. The only downside I can think of is we will have to reserve a byte (0xFF?), ie we'd append 0xFF 0x00, then 0xFF 0x01 to the next duplicate, ... but since these input BytesRefs are \"typically\" UTF-8 ... this seems not so bad? Then can of course in general be arbitrary bytes since they are produced by the analysis process...\n\nI don't understand why we have to reserve any bytes. We can append arbitrary bytes of any sort to the end of the input side, this will have no effect on the actual surface form that we suggest. ",
            "author": "Robert Muir",
            "id": "comment-13457552"
        },
        {
            "date": "2012-09-18T02:31:55+0000",
            "content": "and as far as exactFirst: lets just keep it simple and have it as a surface form comparison?\n\nThis is really what I think most people will expect anyway: in the case of \"duplicates\" and exactFirst,\nnobody really cares nor sees the underlying analyzed form.\n\nSo I don't think we should have multiple outputs for the FST here. ",
            "author": "Robert Muir",
            "id": "comment-13457556"
        },
        {
            "date": "2012-09-18T02:44:54+0000",
            "content": "Sorry mike, I'm still catching up and sadly just creating a lot of noise and thinking out loud. \n\nI think you actually have a point with reserving a byte when there are duplicates \n\nBut at the same time i still think the surface form is valuable for this option too... when we reserve a byte can we also sort the \nduplicate outputs up front, in such a way that we can start traversing the output side to look for an exactly-matching surface form?\n\nSo its like within that 'subtree' of the FST (the duplicates for the exact input), we can binary search?\n\nOtherwise exactFirst would be inefficient in some cases (as we have to do a special 'walk' here). Christian Moen\nshowed me some scary stuff on his Japanese phone as far as readings->kanji forms ... i think if we can it might be \ngood to be cautious and keep this fast... ",
            "author": "Robert Muir",
            "id": "comment-13457562"
        },
        {
            "date": "2012-09-18T12:31:00+0000",
            "content": "New patch, going back to deduping on the input side... it's not done yet: I think we need to escape the bytes we steal. ",
            "author": "Michael McCandless",
            "id": "comment-13457778"
        },
        {
            "date": "2012-09-19T12:00:51+0000",
            "content": "New patch, with escaping working, and exactFirst now means exact surface-form.  I also made ctor parameters maxSurfaceFormsPerAnalyzedForm and maxGraphExpansions.\n\nI did the simple linear scan for now ... I think that's fine to commit (\"N\" is bounded).  We may be able to optimize later with something similar to getByOutput...\n\nI think this patch is ready to commit to the branch ... and I think we are close overall to landing ... still a number of nocommits to resolve though. ",
            "author": "Michael McCandless",
            "id": "comment-13458611"
        },
        {
            "date": "2012-09-28T13:55:52+0000",
            "content": "Applyable patch from the branch (you just have to remove the 0-byte RollingBuffer.java in test-framework after applying it).\n\nI think it's ready ... ",
            "author": "Michael McCandless",
            "id": "comment-13465612"
        },
        {
            "date": "2012-09-28T17:32:24+0000",
            "content": "in TStoA:\n\nif (pos == -1 && posInc == 0) {\n  // TODO: hmm are TS's still allowed to do this...?\n  posInc = 1;\n}\n\n\n\nNO they are not! \n\nAs far as the limitations, i feel like if the last token's endOffset != length of input\nthat might be pretty safe in general (e.g. standardtokenizer) because of how unicode\nworks... i have to think about it.\n\nstrange the FST size increased so much? If i run the benchmark:\n\n[junit4:junit4]   2> -- RAM consumption\n[junit4:junit4]   2> JaspellLookup   size[B]:    9,815,152\n[junit4:junit4]   2> TSTLookup       size[B]:    9,858,792\n[junit4:junit4]   2> FSTCompletionLookup size[B]:      466,520\n[junit4:junit4]   2> WFSTCompletionLookup size[B]:      507,640\n[junit4:junit4]   2> AnalyzingCompletionLookup size[B]:    1,832,952\n\n\n\nI don't know if we should worry about that, but it seems somewhat large\nfor just using KeywordTokenizer.\n\n\n * <b>NOTE</b>: Although the {@link TermFreqIterator} API specifies\n * floating point weights\n\n\n\nThats obselete. See WFSTSuggester in trunk where I fixed this already.\n ",
            "author": "Robert Muir",
            "id": "comment-13465767"
        },
        {
            "date": "2012-09-28T17:45:49+0000",
            "content": "Thanks Rob, good feedback ... I'll post new patch changing that posInc check to an assert, and removing that obsolete NOTE.\n\n\nAs far as the limitations, i feel like if the last token's endOffset != length of input\nthat might be pretty safe in general (e.g. standardtokenizer) because of how unicode\nworks... i have to think about it.\n\nI think we should try that!  This way the suggester can \"guess\" whether the input text is still inside the last token.\n\nBut this won't help the StopFilter case, ie if user types 'a' then StopFilter will still delete it even though the token isn't \"done\" (ie maybe user intends to type 'apple').\n\nStill it's progress so I think we should try it ...\n\nI'm not sure why FST is so much larger ... the outputs should share very well with KeywordTokenizer ... hmm what weights do we use for the benchmark? ",
            "author": "Michael McCandless",
            "id": "comment-13465778"
        },
        {
            "date": "2012-09-28T19:30:47+0000",
            "content": "New patch, folding in Rob's suggestions above (thanks!).\n\nOK the super-large FST size was a false alarm: we were using\nRAMUsageEstimator, which then went and included the RAM usage of\nMockAnalyzer; I changed LookupBenchmarkTest to use FST.sizeInBytes\ninstead:\n\n\n.-- RAM consumption\nJaspellLookup   size[B]:    9,815,152\nTSTLookup       size[B]:    9,858,792\nFSTCompletionLookup size[B]:      466,520\nWFSTCompletionLookup size[B]:      507,640\nAnalyzingCompletionLookup size[B]:      889,138\n\n\n\nSo we are still larger ... but not insanely so.  I do think we could\nshrink the FST if we didn't add 2 bytes in the non-dup case ... I put\na TODO to do this, but it'd make the exactFirst logic even hairier ...\n\nI also put a TODO to use the end offset as a heuristic to \"guess\"\nwhether final token was a partial token or not ... ",
            "author": "Michael McCandless",
            "id": "comment-13465847"
        },
        {
            "date": "2012-09-28T20:05:48+0000",
            "content": "Can we split Analyzer into indexAnalyzer and queryAnalyzer?\n\nCan we also add 1 or 2 sugar ctors that use default values?\n\nI'm thinking:\n\nctor(Analyzer analyzer) {\n  this(analyzer, analyzer);\n}\n\nctor(Analyzer index, Analyzer query) {\n  this(index, query, default, default, default);\n}\n\nctor(Analyzer index, Analyzer query, int option, int option, int option) {\n  // this is the full ctor!\n}\n\n ",
            "author": "Robert Muir",
            "id": "comment-13465865"
        },
        {
            "date": "2012-09-28T21:45:26+0000",
            "content": "Good ideas!  New patch, separating indexAnalyzer and queryAnalyzer, w/ the sugar ctors.\n\nI also renamed to AnalyzingSuggester. ",
            "author": "Michael McCandless",
            "id": "comment-13465929"
        },
        {
            "date": "2012-09-28T21:54:01+0000",
            "content": "+1 thanks Mike. lets get it in! ",
            "author": "Robert Muir",
            "id": "comment-13465934"
        },
        {
            "date": "2012-10-04T19:08:56+0000",
            "content": "+1. This is awesome. It would be great to get this in trunk. ",
            "author": "Sudarshan Gaikaiwari",
            "id": "comment-13469608"
        },
        {
            "date": "2012-10-04T19:13:31+0000",
            "content": "Thanks Sudarshan!  It's actually already committed (will be in 4.1) ... I just forgot to resolve ... ",
            "author": "Michael McCandless",
            "id": "comment-13469612"
        },
        {
            "date": "2012-11-10T23:12:57+0000",
            "content": "That TokenStreamToAutomaton is cool Mike!  I can put that to use in my FST text tagger work. ",
            "author": "David Smiley",
            "id": "comment-13494793"
        },
        {
            "date": "2013-03-22T16:30:11+0000",
            "content": "[branch_4x commit] Michael McCandless\nhttp://svn.apache.org/viewvc?view=revision&revision=1391704\n\nLUCENE-3842: refactor: don't make spooky State methods public ",
            "author": "Commit Tag Bot",
            "id": "comment-13610728"
        },
        {
            "date": "2013-03-22T16:30:22+0000",
            "content": "[branch_4x commit] Michael McCandless\nhttp://svn.apache.org/viewvc?view=revision&revision=1391686\n\nLUCENE-3842: add AnalyzingSuggester ",
            "author": "Commit Tag Bot",
            "id": "comment-13610730"
        },
        {
            "date": "2013-05-01T08:58:31+0000",
            "content": "I tried building the analyzing suggester model from an external file containing 1mln short phrases taken from Wikipedia titles.\n2Gb of memory seems not enough, it runs forever and dies with OOM.\nWhat is the expected dictionary size? What is the benchmark behavior?\n\nThanks! ",
            "author": "Alexey Kudinov",
            "id": "comment-13646468"
        },
        {
            "date": "2013-05-01T11:40:01+0000",
            "content": "The building process is unfortunately RAM intensive, but there are settings/knobs in the FST Builder API to tradeoff RAM required during building vs how small the resulting FST is.  Maybe we need to expose control for these in AnalyzingSuggester ...\n\nCan you share those 1M short phrases?  What is the total number of characters across them? ",
            "author": "Michael McCandless",
            "id": "comment-13646511"
        },
        {
            "date": "2013-05-01T12:49:23+0000",
            "content": "Setting maxGraphExpansions to some value > 0 (say, 30) ends with null reference exception. paths is null here:\nmaxAnalyzedPathsForOneInput = Math.max(maxAnalyzedPathsForOneInput, paths.size());\nFixing this, the model loads after a while.\nWith maxGraphExpansions < 0 it doesn't load regardless the dictionary size.\nI'm using the wordnet synonyms, so I guess this causes a lot of paths, I suspect loops.\nThe total dictionary file size is about 20Mb, but this doesn't really matter as I get similar behavior for even smaller one (2Mb).\nThe dataset is from here: http://wiki.dbpedia.org/Downloads32, Titles in english. I took the values only and tried different sizes (10mln-1mln-0.1mln).\n ",
            "author": "Alexey Kudinov",
            "id": "comment-13646543"
        },
        {
            "date": "2013-05-01T13:13:36+0000",
            "content": "I'm using the wordnet synonyms, so I guess this causes a lot of paths, I suspect loops.\n\nAhhhh   Yes this will cause lots of expansions / RAM used.\n\nBut NPE because paths is null sounds like a real bug.\n\nOK I see why it's happening ... when we try to enumerate all finite strings from the expanded graph, if it exceeds the limit (maxGraphExpansions), SpecialOperations.getFiniteStrings returns null but the code assumes it will return the N finite strings it had found \"so far\".  Can you open a new issue for this?  We should separately fix it. ",
            "author": "Michael McCandless",
            "id": "comment-13646559"
        },
        {
            "date": "2013-05-01T13:36:58+0000",
            "content": "I opened an issue for NPE - LUCENE-4971 ",
            "author": "Alexey Kudinov",
            "id": "comment-13646574"
        },
        {
            "date": "2013-05-01T14:15:11+0000",
            "content": "Thank you Alexey! ",
            "author": "Michael McCandless",
            "id": "comment-13646610"
        },
        {
            "date": "2013-05-10T10:34:02+0000",
            "content": "Closed after release. ",
            "author": "Uwe Schindler",
            "id": "comment-13654106"
        }
    ]
}