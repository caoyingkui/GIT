{
    "id": "SOLR-1283",
    "title": "Mark Invalid error on indexing",
    "details": {
        "affect_versions": "1.3",
        "status": "Closed",
        "fix_versions": [
            "3.1",
            "4.0-ALPHA"
        ],
        "components": [],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "When indexing large (1 megabyte) documents I get a lot of exceptions with stack traces like the below.  It happens both in the Solr 1.3 release and in the July 9 1.4 nightly.  I believe this to NOT be the same issue as SOLR-42.  I found some further discussion on solr-user: http://www.nabble.com/IOException:-Mark-invalid-while-analyzing-HTML-td17052153.html \n\nIn that discussion, Grant asked the original poster to open a Jira issue, but I didn't see one so I'm opening one; please feel free to merge or close if it's redundant. \n\nMy stack trace follows.\n\nJul 15, 2009 8:36:42 AM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/solr path=/update params={} status=500 QTime=3 \nJul 15, 2009 8:36:42 AM org.apache.solr.common.SolrException log\nSEVERE: java.io.IOException: Mark invalid\n        at java.io.BufferedReader.reset(BufferedReader.java:485)\n        at org.apache.solr.analysis.HTMLStripReader.restoreState(HTMLStripReader.java:171)\n        at org.apache.solr.analysis.HTMLStripReader.read(HTMLStripReader.java:728)\n        at org.apache.solr.analysis.HTMLStripReader.read(HTMLStripReader.java:742)\n        at java.io.Reader.read(Reader.java:123)\n        at org.apache.lucene.analysis.CharTokenizer.next(CharTokenizer.java:108)\n        at org.apache.lucene.analysis.StopFilter.next(StopFilter.java:178)\n        at org.apache.lucene.analysis.standard.StandardFilter.next(StandardFilter.java:84)\n        at org.apache.lucene.analysis.LowerCaseFilter.next(LowerCaseFilter.java:53)\n        at org.apache.solr.analysis.WordDelimiterFilter.next(WordDelimiterFilter.java:347)\n        at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:159)\n        at org.apache.lucene.index.DocFieldConsumersPerField.processFields(DocFieldConsumersPerField.java:36)\n        at org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:234)\n        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:765)\n        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:748)\n\tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:2512)\n\tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:2484)\n\tat org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:240)\n\tat org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:61)\n\tat org.apache.solr.handler.XMLLoader.processUpdate(XMLLoader.java:140)\n\tat org.apache.solr.handler.XMLLoader.load(XMLLoader.java:69)\n\tat org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:54)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1292)\n\tat org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:338)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:241)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1089)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:365)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:712)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:405)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:211)\n\tat org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:139)\n\tat org.mortbay.jetty.Server.handle(Server.java:285)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:502)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:835)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:641)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:208)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:378)\n\tat org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:226)\n\tat org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:442)\n\nThanks.",
    "attachments": {
        "SOLR-1283.patch": "https://issues.apache.org/jira/secure/attachment/12431432/SOLR-1283.patch",
        "SOLR-1283.modules.patch": "https://issues.apache.org/jira/secure/attachment/12448610/SOLR-1283.modules.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Grant Ingersoll",
            "id": "comment-12732126",
            "date": "2009-07-16T20:10:05+0000",
            "content": "We should make the buffer size configurable, I guess.  However, there's always the potential to go past it or use up a lot of memory in the meantime (if one is expecting really large files) "
        },
        {
            "author": "solrize",
            "id": "comment-12732206",
            "date": "2009-07-16T22:20:29+0000",
            "content": "Right now I'm getting a ton of these errors.  It doesn't seem strictly dependent on the doc size.  If I can crank up the buffer size enough that the error happens only occasionally instead of frequently, that would be a big improvement over the present situation.  Thanks! "
        },
        {
            "author": "solrize",
            "id": "comment-12734207",
            "date": "2009-07-22T17:33:47+0000",
            "content": "Is the buffer size the parameter DEFAULT_READ_AHEAD (set to 8192) in HTMLStripReader.java?\n\nShould I set it to be the same as maxFieldLength from solrconfig.xml?  That would let it hold the entire document.  I currently have that config parameter set to 10000000 (10 MB).\n\nThanks "
        },
        {
            "author": "solrize",
            "id": "comment-12734815",
            "date": "2009-07-23T22:14:54+0000",
            "content": "I now have a workaround.  The documents I'm indexing don't actually have html in them, but the schema was set up to use HTMLStripReader anyway.  I switched to the standard analyzer and the problem went away, and indexing also seems to be running faster than before.  I do still think the issue needs fixing since I'm sure some people use solr to index large web pages which do need html stripping.  Anyway, thanks to Erik H. for advice about this. "
        },
        {
            "author": "David Bowen",
            "id": "comment-12770586",
            "date": "2009-10-27T17:29:52+0000",
            "content": "It seems to me that the code should bail out and just assume that a \"<\" did not begin an HTML tag if it still isn't sure after reading the DEFAULT_READ_AHEAD (8,192) characters.  It looks like the code was intended to do that (see the checks against safeReadAheadLimit) but must be missing some case.\n "
        },
        {
            "author": "Julien Coloos",
            "id": "comment-12805083",
            "date": "2010-01-26T16:37:27+0000",
            "content": "The issue is also happening in current trunk (revision 903234), with the class HTMLStripCharFilter (replacing deprecated HTMLStripReader it seems).\n\nExample of stacktrace:\n\n26 janv. 2010 16:02:56 org.apache.solr.common.SolrException log\nGRAVE: java.io.IOException: Mark invalid\n        at java.io.BufferedReader.reset(BufferedReader.java:485)\n        at org.apache.lucene.analysis.CharReader.reset(CharReader.java:63)\n        at org.apache.solr.analysis.HTMLStripCharFilter.restoreState(HTMLStripCharFilter.java:172)\n        at org.apache.solr.analysis.HTMLStripCharFilter.read(HTMLStripCharFilter.java:734)\n        at org.apache.solr.analysis.HTMLStripCharFilter.read(HTMLStripCharFilter.java:748)\n        at java.io.Reader.read(Reader.java:122)\n        at org.apache.lucene.analysis.CharTokenizer.incrementToken(CharTokenizer.java:77)\n        at org.apache.lucene.analysis.ISOLatin1AccentFilter.incrementToken(ISOLatin1AccentFilter.java:43)\n        at org.apache.lucene.analysis.TokenStream.next(TokenStream.java:383)\n        at org.apache.lucene.analysis.ISOLatin1AccentFilter.next(ISOLatin1AccentFilter.java:64)\n        at org.apache.solr.analysis.WordDelimiterFilter.next(WordDelimiterFilter.java:379)\n        at org.apache.lucene.analysis.TokenStream.incrementToken(TokenStream.java:318)\n        at org.apache.lucene.analysis.StopFilter.incrementToken(StopFilter.java:225)\n        at org.apache.lucene.analysis.LowerCaseFilter.incrementToken(LowerCaseFilter.java:38)\n        at org.apache.solr.analysis.SnowballPorterFilter.incrementToken(SnowballPorterFilterFactory.java:116)\n        at org.apache.lucene.analysis.TokenStream.next(TokenStream.java:406)\n        at org.apache.solr.analysis.BufferedTokenStream.read(BufferedTokenStream.java:97)\n        at org.apache.solr.analysis.BufferedTokenStream.next(BufferedTokenStream.java:83)\n        at org.apache.lucene.analysis.TokenStream.incrementToken(TokenStream.java:321)\n        at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:138)\n        at org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:244)\n        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:781)\n        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:764)\n        at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:2630)\n        at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:2602)\n        at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:241)\n        at org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:61)\n        at org.apache.solr.handler.XMLLoader.processUpdate(XMLLoader.java:139)\n        at org.apache.solr.handler.XMLLoader.load(XMLLoader.java:69)\n        at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:54)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1317)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:341)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:244)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1089)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:365)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:712)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:405)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:211)\n        at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:139)\n        at org.mortbay.jetty.Server.handle(Server.java:285)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:502)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:835)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:723)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:208)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:378)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:226)\n        at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:442)\n\n\n\nAfter a quick code review, it seems this one is due to the peek function which can read a byte from the input stream, while not incrementing the numRead variable (as done in the next function): functions checking whether read ahead limit was reached rely on numRead.\nThe exception can then be triggered when reading exceeds the read ahead limit, as for example with a big document containing a malformed processing instruction like\n\n<?>   ?????\n... (anything except  '?>')\n\n\nNote: the issue is triggered here because readProcessingInstruction calls peek whenever the character '?' was found (to check whether it is followed by '>').\n\n\nYou will find attached a patch to fix the issue, as well as an updated JUnit test (which actually only checks for the malformed processing instruction, maybe you will find a more general test to perform on the next/peek functions).\n\n\nRegards "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12884860",
            "date": "2010-07-02T23:41:25+0000",
            "content": "Updates patch to trunk (where the charfilter stuff has been refactored into the new top level \"modules\" directory)\n\nI'm not familiar with the HTMLStripCharFilter stuff, so i can't say whether the \"fix\" is correct (no idea if \"peek\" should be incrementing that counter \u2013 that's why even private methods should have javadocs), but the test certainly looks valid to me "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12884862",
            "date": "2010-07-02T23:45:47+0000",
            "content": "we have a patch that seems to work, so we should dfinitely try to get this into the next release ... i'm hoping someone more familiar with the code can sanity check it soon. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12884886",
            "date": "2010-07-03T01:40:44+0000",
            "content": "From IRC:\n\nI wonder if the issue isn't that in next()\n[21:35] gsingers: if it gets something off the stack (pushed) it doesn't increment numRead\n[21:37] gsingers: but, I guess one could argue that numRead should track exactly what is read off the InputStream\n[21:38] gsingers: and in that case, peek is still doing a read\n[21:38] gsingers: so it should inc. it\n[21:38] gsingers: I suppose the only harm in more aggressively incrementing it is that you don't hold as much in buffer as you could otherwise "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12885658",
            "date": "2010-07-06T19:53:32+0000",
            "content": "As i mentioned in IRC (prior to Grant's previously posted comments) the core issue is: what is the intended purpose of the \"numRead\" counter?\n\n\n\tIf it's suppose to count the number of times \"input.read()\" is called (ie: \"num read from inner stream\"), then \"peek\" has a bug by not incrementing.\n\tIf it's  suppose to count the number of times \"next()\" returns a char (ie: \"num read from outer stream\"), then as grant mentioned \"next\" has a bug by not incrementing when using the stack.\n\n\n\nThe patch currently assumes the former and seems to fix the bug, i haven't tried the same test case with an approach to the later, but i suspect that may also work.  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12987253",
            "date": "2011-01-26T22:05:50+0000",
            "content": "Since it looks like the primary use of numRead is in relation to mark() and reset() on the underlying stream, it does look like #1 is the correct interpretation (i.e. the patch looks correct) "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12987307",
            "date": "2011-01-26T23:58:03+0000",
            "content": "Committed to 3x and trunk. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-13013102",
            "date": "2011-03-30T15:45:32+0000",
            "content": "Bulk close for 3.1.0 release "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13190605",
            "date": "2012-01-22T04:54:33+0000",
            "content": "The below-listed exception, which appears to be the same as that in other reports on this issue, is triggered when processing with HTMLStripCharFilter the ClueWeb09 documents with TREC-IDs clueweb09-en0000-00-14171, clueweb09-en0000-00-14228, clueweb09-en0000-00-14235, clueweb09-en0000-00-14240, clueweb09-en0000-00-14248, and clueweb09-en0000-00-14265:\n\n\njava.io.IOException: Mark invalid\n        at java.io.BufferedReader.reset(BufferedReader.java:485)\n        at org.apache.lucene.analysis.CharReader.reset(CharReader.java:69)\n        at org.apache.lucene.analysis.charfilter.HTMLStripCharFilter.restoreState(HTMLStripCharFilter.java:171)\n        at org.apache.lucene.analysis.charfilter.HTMLStripCharFilter.read(HTMLStripCharFilter.java:734)\n\n\n\nOnce LUCENE-3690 has been committed, this will only affect the (deprecated) old implementation, which will be renamed to LegacyHTMLStripCharFilter. "
        }
    ]
}