{
    "id": "SOLR-9731",
    "title": "Add jvm-wide JMX statistics for Solr",
    "details": {
        "components": [],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Resolved",
        "resolution": "Implemented",
        "priority": "Major"
    },
    "description": "The statistics that can currently be gathered via JMX tend to be core-specific, making monitoring \"how is the Solr node doing\" harder than it needs to be. This JIRA is about exploring what it would take for instance-wide statistics to be JMX-enabled.\n\nI'm imagining cumulative stats like:\n> How many Solr<->Solr communications errors have there been?\n> How many Solr<->ZK communication errors have there been\n> How many full synchronizations have happened across all replicas?\n> Operations people, fill in your favorite health monitoring bit here.\n\nWhat do people think? Is JMX even the right thing? We have an admin end-point for gathering information, but that's not as \"operations friendly\".\n\nI'm open to any suggestions for how/where to implement this, whether there are any huge \"gotchas\", bottleneck concerns, etc.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2016-11-04T20:04:38+0000",
            "author": "Shawn Heisey",
            "content": "Sounds like a really good idea.  Any indicators of health that aren't horribly disruptive should be tracked and made available. Codahale metrics is already a dependency, and it can give us percentiles on any stats where they make sense.\n\nThinking out loud (and this might be per-core, not JVM-wide, but I don't have anywhere else to discuss it right now):\n\nI wonder if there's any way to detect when and how much actual disk I/O is required to satisfy a query.  I suspect that this information is not readily available to Java, and even if it its, that it would need to be tracked down in the Lucene layer and made available via public getters that Solr could query.\n\nLucene might be able to track statistics about how many nanoseconds it takes for reading X bytes from MMap, and that information could ultimately be interpreted by a user to indicate whether or not their disk caching is effective.  One problem with that idea: Lucene's core functionality has no dependencies, so that feature would probably have to be written using native classes/methods included with the JVM, not an external dependency like the metrics package.  It would be really awesome if we could see median and percentile info about how long the MMap accesses are taking.  We'd be able to use that info to determine whether a performance issue is due to insufficient disk cache. ",
            "id": "comment-15637520"
        },
        {
            "date": "2017-03-03T08:13:38+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "This was implemented by SOLR-4735 and related improvements. JVM wide stats are exposed with both JMX and the /admin/metrics API. Please open issues for specific improvements around metrics. ",
            "id": "comment-15893897"
        }
    ]
}