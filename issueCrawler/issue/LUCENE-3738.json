{
    "id": "LUCENE-3738",
    "title": "Be consistent about negative vInt/vLong",
    "details": {
        "labels": "",
        "priority": "Blocker",
        "components": [],
        "type": "Bug",
        "fix_versions": [
            "3.6",
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Today, write/readVInt \"allows\" a negative int, in that it will encode and decode correctly, just horribly inefficiently (5 bytes).\n\nHowever, read/writeVLong fails (trips an assert).\n\nI'd prefer that both vInt/vLong trip an assert if you ever try to write a negative number... it's badly trappy today.  But, unfortunately, we sometimes rely on this... had we had this assert in 'since the beginning' we could have avoided that.\n\nSo, if we can't add that assert in today, I think we should at least fix readVLong to handle negative longs... but then you quietly spend 9 bytes (even more trappy!).",
    "attachments": {
        "LUCENE-3738.patch": "https://issues.apache.org/jira/secure/attachment/12512570/LUCENE-3738.patch",
        "LUCENE-3738-improvement.patch": "https://issues.apache.org/jira/secure/attachment/12520780/LUCENE-3738-improvement.patch",
        "ByteArrayDataInput.java.patch": "https://issues.apache.org/jira/secure/attachment/12520155/ByteArrayDataInput.java.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-01-31T13:51:51+0000",
            "content": "That was my fault when the \"Schindler VM\" instead the \"Hotspot VM\" unrolled the loop (Schindler loop optimizer bug). I unrolled maximum of 9 bytes not 10 (which is wasteful, too).\n\nWe had no negative VLongs until now, so that was no issue  ",
            "author": "Uwe Schindler",
            "id": "comment-13196915"
        },
        {
            "date": "2012-01-31T13:55:43+0000",
            "content": "So, if we can't add that assert in today, I think we should at least fix readVLong to handle negative longs... but then you quietly spend 9 bytes (even more trappy!).\n\nAs the code is written by a loop, we would write 10 bytes, of course the last one only with 1 bit. If we wont to spare that and optimize the long case to interpret the continuation bit in the last byte different (as part of data), the writer must also do that. Ideally we would unroll both loops. ",
            "author": "Uwe Schindler",
            "id": "comment-13196916"
        },
        {
            "date": "2012-01-31T14:00:32+0000",
            "content": "\nAs the code is written by a loop, we would write 10 bytes, of course the last one only with 1 bit.\n\nI think this is ok: otherwise its not a variable-length integer but something else \n(with a special case where 9th byte high bit means sign bit instead of continuation bit).\n\nEither way we should either fix it to assert value >=0 in the writer, or make it work.\nIdeally we would do that for both vint and vlong, but the problem is some things like term vectors\nsometimes write negative vints (since it does startOffset - lastEndOffset, if you have any synonyms\nyou get tons of huge 5-byte vints in your term vectors)\n\nBut currently its inconsistent: negatives don't trip any assert for either vint/vlong at write-time,\nbut at read-time for vlong only. ",
            "author": "Robert Muir",
            "id": "comment-13196918"
        },
        {
            "date": "2012-01-31T14:02:38+0000",
            "content": "\nWe had no negative VLongs until now, so that was no issue\n\nI think we should still avoid this!\n\nI think the javadocs should still say: negatives are unsupported.\nMaybe we can fix lucene 4's term vectors format to never write negatives, \nand in version 5 when 3.x indexes no longer need to be read, we can assert >= 0 at write-time? ",
            "author": "Robert Muir",
            "id": "comment-13196919"
        },
        {
            "date": "2012-01-31T14:11:43+0000",
            "content": "The correct fix for longs would be to add one more unrolled loop iteration (for 10th byte). Then it would work with negative numbers as vInts. But very wasterful.\n\nAbout negative vInts: We have them unfortunately in pre-4.0 formats with version numbers. I think e.g. stored fields reader reads the first vInt from file, if its >=0 its a pre-very-ancient format and is some offset/count/foo/bar (no idea). If its negative, its a version number.\n\nWe should fix the unrolled vLong reader in 3.x and trunk to have one more loop so its consisten with writer (the assert stays -> ist an assert for the continuation bit not set on last byte).\n\nWe should try to never write negative numbers for post 4.0 formats, maybe add conditional assert (like for utf8 strings), so preflex can write using negative vints and dont trip assert. ",
            "author": "Uwe Schindler",
            "id": "comment-13196929"
        },
        {
            "date": "2012-01-31T14:20:08+0000",
            "content": "\nWe should try to never write negative numbers for post 4.0 formats, maybe add conditional assert (like for utf8 strings), so preflex can write using negative vints and dont trip assert.\n\nWe still have this problem then with negatives in 4.0 formats (like stored fields).\n\nI think we should fix them all to use codec header, and fix term vectors writer.\n\nwe could have a conditionalized assert for this in mockdirectorywrapper or something like that: it could be disabled when preflex is used for now. ",
            "author": "Robert Muir",
            "id": "comment-13196941"
        },
        {
            "date": "2012-01-31T14:30:51+0000",
            "content": "Here the patch that simply fixes the \"Schindler VM unroll bug\". ",
            "author": "Uwe Schindler",
            "id": "comment-13196948"
        },
        {
            "date": "2012-01-31T14:35:33+0000",
            "content": "Just to conclude: The bug with reading negative vLongs affects only DataInputs that dont override readVLong. So e.g. BufferedIndexInput is not affected. So when you read index from MMap or ByteArrayDataInput or InputStreamDataInput you will hit the bug. ",
            "author": "Uwe Schindler",
            "id": "comment-13196951"
        },
        {
            "date": "2012-01-31T14:47:06+0000",
            "content": "The BufferedIndexInput one has also this bug, only affecting reads at the boundaries (if the 10 bytes of a full int are no longer in buffer), in that case it throws AIOOBE:\n\n\npublic long readVLong() throws IOException {\n  if (9 <= bufferLength-bufferPosition) {\n    byte b = buffer[bufferPosition++];\n    long i = b & 0x7F;\n    for (int shift = 7; (b & 0x80) != 0; shift += 7) {\n      b = buffer[bufferPosition++];\n      i |= (b & 0x7FL) << shift;\n    }\n    return i;\n  } else {\n    return super.readVLong();\n  }\n}\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-13196955"
        },
        {
            "date": "2012-01-31T14:49:42+0000",
            "content": "Patch fixing both to be consistent/not-buggy. ",
            "author": "Uwe Schindler",
            "id": "comment-13196957"
        },
        {
            "date": "2012-01-31T15:15:06+0000",
            "content": "I think we should disallow (assert) writing negative vLong, and, ideally disallow negative vInt also, by fixing all the places that rely on this (but, carefully... preflexrw will need a backdoor)...\n\nSeparately: can't we strengthen the last assert in writeVInt to verify the top 4 bits are 0, not just the top bit?  We have 36 bits at that point right?  So top 4 should be unused (assert (b & 0xf0) == 0) ",
            "author": "Michael McCandless",
            "id": "comment-13196968"
        },
        {
            "date": "2012-01-31T15:20:14+0000",
            "content": "If we disallow, it should be a hard check (no assert), as the data is coming from a file (and somebody could used a hex editor). The reader will crash later... ",
            "author": "Uwe Schindler",
            "id": "comment-13196972"
        },
        {
            "date": "2012-01-31T23:35:30+0000",
            "content": "after investigating: its difficult to prevent negative offsets, even after fixing term vectors writer (LUCENE-3739)\n\nAt first i tried a simple assert in BaseTokenStreamTestCase:\n\n        assertTrue(\"offsets must not go backwards\", offsetAtt.startOffset() >= lastStartOffset);\n        lastStartOffset = offsetAtt.startOffset();\n\n\n\nThen these analyzers failed:\n\n\tMockCharFilter itself had a bug, but thats easy to fix (LUCENE-3741)\n\tsynonymsfilter failed sometimes (LUCENE-3742) because it wrote zeros for offsets in situations like \"a -> b c\"\n\t(edge)ngramtokenizers failed, because ngrams(1,2) of \"ABCD\" are not A, AB, B, BC, C, CD, D but instead A, B, C, D, AB, BC, CD, ...\n\t(edge)ngramfilters failed for similar reasons.\n\tworddelimiterfilter failed, because it doesnt break \"AB\" into A, AB, B but instead A, B, AB\n\ttrimfilter failed when 'offsets changing' is enabled, because if you have \" rob\", \"robert\" as synonyms then it trims the first, and the second offsets \"go backwards\"\n\n\n\nThese are all bugs.\n\nIn general I think offsets after being set should not be changed, because filters don't have access to any charfilters\noffset correction (correctOffset()) anyway, so they shouldnt be mucking offsets.\n\nSo really: only the creator of tokens should make the offsets. And if thats a filter, it should be a standard way, \nonly inherited from existing offsets and not 'offset mathematics' and not A, AB, B in some places and A, B, AB in others.\n\nReally i think we need to step it up if we want highlighting to be first-class citizen in lucene, nothing checks the offsets anyhwere at all,\neven to check/assert if they are negative, and there are little tests... all we have is some newish stuff in basetokenstreamtestcase and\na few trivial test cases.\n\nOn the other hand, for example, position increment's impl actually throws exception if you give it something like a negative number... ",
            "author": "Robert Muir",
            "id": "comment-13197383"
        },
        {
            "date": "2012-03-16T14:19:52+0000",
            "content": "LUCENE-3876/LUCENE-3879 reveal more situations where we must write negatives with the current encodings,\nbecause we steal bits from things like positions (payloads) and docids too (at least in skip data?)\n\nSo sometimes its possible these are encoded as negatives.\n\nI think Uwe should commit his fixes? ",
            "author": "Robert Muir",
            "id": "comment-13231222"
        },
        {
            "date": "2012-03-16T15:33:14+0000",
            "content": "Hmm... I think we should think about it more.\n\nIe, we apparently never write a negative vLong today... and I'm not sure we should start allowing it...? ",
            "author": "Michael McCandless",
            "id": "comment-13231295"
        },
        {
            "date": "2012-03-16T15:36:22+0000",
            "content": "Well that differs with the title of the issue (consistency with vInt).\n\nI don't see how we can avoid negative vints. I think its ok to be inconsistent with vLong,\nbut it should not be something we assert only at read-time. It should be asserted on write\nso that problems are found immediately. ",
            "author": "Robert Muir",
            "id": "comment-13231297"
        },
        {
            "date": "2012-03-16T15:48:22+0000",
            "content": "\n don't see how we can avoid negative vints. I think its ok to be inconsistent with vLong,\n but it should not be something we assert only at read-time. It should be asserted on write\n so that problems are found immediately.\n\n+1\n\nI think we are stuck with negative vInts, as trappy as they are (5 bytes!!).\n\nLet's not make it worse by allowing negative vLongs.  But let's assert that at write time (and read time)...\n\nI think inconsistency here is the lesser evil. ",
            "author": "Michael McCandless",
            "id": "comment-13231309"
        },
        {
            "date": "2012-03-16T16:18:58+0000",
            "content": "Patch, just adding assert in writeVLong that i >=0, and also strengthening existing assert in readVInt to check that top 4 (not just top 1) bits are 0. ",
            "author": "Michael McCandless",
            "id": "comment-13231329"
        },
        {
            "date": "2012-03-16T17:17:42+0000",
            "content": "I just repeat myself:\nIf we disallow, it should be a hard check (no assert), as the data is coming from a file (and somebody could used a hex editor). The reader will crash later...\n\nMike: If you fix the unrolled loops, please also add the checks to the other implementations in Buffered* and so on. My original patch fixed those. Please include that patch. ",
            "author": "Uwe Schindler",
            "id": "comment-13231401"
        },
        {
            "date": "2012-03-16T18:10:55+0000",
            "content": "If we disallow, it should be a hard check (no assert), as the data is coming from a file (and somebody could used a hex editor). The reader will crash later...\n\nHmm, I don't think we should do that.\n\nIf you go and edit your index with a hex editor... there are no guarantees on what may ensue!\n\nMike: If you fix the unrolled loops, please also add the checks to the other implementations in Buffered* and so on. \n\nI don't think the unrolled loops or other impls of write/readVLong are wrong?  The javadocs state clearly that negatives are not supported.  All we're doing here is added an assert to backup that javadoc statement.\n ",
            "author": "Michael McCandless",
            "id": "comment-13231470"
        },
        {
            "date": "2012-03-16T18:30:21+0000",
            "content": "Hmm, I don't think we should do that.\n\nIt costs nothing, as the standard vInt will be read only some 1 or 2 bytes, if you really read until the last byte, you have so big vInts that it might even be better not to use vInts at all.- And: The not-unrolled loops do the check always.\n\nIf you go and edit your index with a hex editor... there are no guarantees on what may ensue!\n\nDisk IO can produce wrong data. We must check this if we can and it costs nothing, which is the case here (see above).\n\nI was already talking with Robert, there are other asserts in the index readiung code at places completely outside any loops, executed only once when index is opened. Its first priority to do consistency checks of the read bytes. Otherwise you can even produce endless loops at some places. - Of course not when you have tight loops, but things like checking that the document count is in line with e.g. some other value from liveDocs is essential. I will open an issue for that, the older Lucene formats are much besster secured, but trunk is horrible, just because some people here seem to want to prevent any check, which is also a security issue when you e.g. download indexes through network connections and a man in the middle modifies the stream.  ",
            "author": "Uwe Schindler",
            "id": "comment-13231492"
        },
        {
            "date": "2012-03-16T18:42:33+0000",
            "content": "\nOtherwise you can even produce endless loops at some places. - Of course not when you have tight loops, but things like checking that the document count is in line with e.g. some other value from liveDocs is essential.\n\nI agree there a ton of places (essentially all metadata) where we should be using real checks (not asserts).\n\nI see Mike's point though: readVLong() is very general, so someone could be using it where performance is important.\nIt just so happens its mostly only used today for metadata type things (except maybe terms dictionary stats and a few other places). ",
            "author": "Robert Muir",
            "id": "comment-13231506"
        },
        {
            "date": "2012-03-16T19:18:11+0000",
            "content": "You misunderstood my comment:\n\nI see Mike's point though: readVLong() is very general, so someone could be using it where performance is important.\n\nThe check is only ommitted in the unrolled loop, the for-loop still contains the check. In that case it also handles maybe too-long vints correctly, which the unrolled code will never do. The unrolled code also has a bug, that it handles negative longs wrong, but that should be prevented (in my opinion also for ints).\n\nThe current assert for both long and int is completely harmless, as it will only be executed, if the vInt/vLong has the maximum number of bytes, which is very unlikely. And as said before the check is done in the loop-based code, too. And comparison in perf showed that the speed of the unrolled loop and the standard loop are identical, so about what are you talking?\n\nThe good thing here is that we can (in the unrolled loops) harden the check for negative vInts, as because of the unrolled loop we have a separate cocde branch already, so we can modify the check (which was always done before my unrolling) to do the better check.\n\nI had the idea at that times to unroll that loop because of Java bugs, so the bug is caused by me and I want to fix it the correct way, definitely without loosing anything. Is this so hard to understand? ",
            "author": "Uwe Schindler",
            "id": "comment-13231543"
        },
        {
            "date": "2012-03-17T13:27:39+0000",
            "content": "The check is only ommitted in the unrolled loop, the for-loop still contains the check.\n\nI'm confused... I don't see how/where BufferedIndexInput.readVLong is\nchecking for negative result now...?  Are you proposing adding an if\ninto that method?  That's what I don't want to do... eg, readVLong is\ncalled 3 times per term we decode (Lucene40 codec); it's a very low\nlevel API... other codecs may very well call it more often.  I don't\nthink we should add an if inside BII.readVLong.\n\nOr.... maybe you are saying you just want the unrolled code to handle\nthe negative vLong case (ie, unroll the currently missing 10th cycle),\nand not add an if to BufferedIndexInput.readVLong?  And then \"for\nfree\" we can add a real if (not assert) if that 10th cycle is hit?\n(ie, if we get to that 10th byte, throw an exception).  I think that\nmakes sense!\n\nthere are other asserts in the index readiung code at places completely outside any loops, executed only once when index is opened. \n\n+1 to make those real checks, as long as the cost is vanishingly\nsmall.\n\nwhich is also a security issue when you e.g. download indexes through network connections and a man in the middle modifies the stream.\n\nI don't think it's our job to protect against / detect that.\n\nDisk IO can produce wrong data.\n\nTrue, but all bets are off if that happens: you're gonna get all sorts\nof crazy exceptions out of Lucene.  We are not a filesystem. ",
            "author": "Michael McCandless",
            "id": "comment-13231962"
        },
        {
            "date": "2012-03-17T14:08:53+0000",
            "content": "\nThe check is only ommitted in the unrolled loop, the for-loop still contains the check.\n\nI'm confused... I don't see how/where BufferedIndexInput.readVLong is\nchecking for negative result now...?\n\nI mean that the actual \"if (b & mask != 0)\" is also in the original while loop. The original while loop then simply proceeds with reading bytes util the highest bit is null. The unrolled loop behaves different (and thats a real bug), because it will silently not read those remaining bytes, so the file pointer is on a different byte after the call. This also affects readVInt!!!\n\nIn my opinion, we should unroll all readVInt/readVLong loops so all behave 100% identical! And in the case of the last byte read (where the current assert is), throw exception. If we don't unroll all readVInts we have to somehow also make the loop exit after too many bytes are read, which would be an costly extra check in the loop - thats the reason why I want to unroll all loops to fail after 5 or 9 bytes. ",
            "author": "Uwe Schindler",
            "id": "comment-13231978"
        },
        {
            "date": "2012-03-18T13:32:31+0000",
            "content": "In my opinion, we should unroll all readVInt/readVLong loops so all behave 100% identical! \n\n+1 ",
            "author": "Michael McCandless",
            "id": "comment-13232272"
        },
        {
            "date": "2012-03-18T19:07:53+0000",
            "content": "Patch that fixes all DataInput and DataOutput subclasses to:\n\n\ton writeVLong add Mike's assert for positive longs\n\tunrolled all readVInts and readVLongs\n\tthe readV* methods are now more straightforward implemented at the end of method and do the same branching and fail at the end with an IOException (last statement)\n\tI made all read methods in BufferedIndexInput final, as they should be never overridden (only readInternal/seekInternal). This could improve performance as hotspot knows finalness better -> inlining.\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-13232332"
        },
        {
            "date": "2012-03-18T19:20:30+0000",
            "content": "+1\n\nLooks awesome Uwe! ",
            "author": "Michael McCandless",
            "id": "comment-13232336"
        },
        {
            "date": "2012-03-18T22:23:24+0000",
            "content": "I added a test, also checking ByteArrayDataInput and the exceptions.\n\nI will commit this now and then backport. ",
            "author": "Uwe Schindler",
            "id": "comment-13232391"
        },
        {
            "date": "2012-03-18T22:27:40+0000",
            "content": "Committed trunk revision: 1302238 ",
            "author": "Uwe Schindler",
            "id": "comment-13232392"
        },
        {
            "date": "2012-03-18T22:42:03+0000",
            "content": "Committed 3.x revision: 1302242 ",
            "author": "Uwe Schindler",
            "id": "comment-13232396"
        },
        {
            "date": "2012-03-27T16:48:17+0000",
            "content": "Regarding unrolling... it hasn't always proved faster in the past, esp wrt vint.\n\nMy first try was in 2005: http://www.lucidimagination.com/search/document/6d2efedb4dde07d#2a896a9a9adc3f2d\nAnd again in 2006: https://issues.apache.org/jira/browse/LUCENE-639 ",
            "author": "Yonik Seeley",
            "id": "comment-13239618"
        },
        {
            "date": "2012-03-27T16:51:40+0000",
            "content": "Yonik, the unrolling was added because of a recent Java 6 hotspot bug (who unrolled the loop itsself - but wrongly). The thing that Mike has seen was a strange thing that the already unrolled code (since 3.1) behaves different before/after a slight code change done in this issue. ",
            "author": "Uwe Schindler",
            "id": "comment-13239621"
        },
        {
            "date": "2012-03-27T16:54:14+0000",
            "content": "The original unrolling here was to dodge a JVM bug, possible\nin all java versions from ... java6u20 until java6u29 or so?\n\nI don't know if there's another solution other than unrolling\nto work around that loop bug.\n\nI don't like the workaround but it does seem realistic at this\npoint to prevent index corruption since these versions of\njava are really recent. ",
            "author": "Robert Muir",
            "id": "comment-13239628"
        },
        {
            "date": "2012-03-27T17:21:49+0000",
            "content": "The problem might come from the asserts (they are not completely non-existent -> classloader does not remove them; the JVM relies on hotspot removing them when hotspot sees \"dead\" code -> \"final static boolean $assertionsDisabled\" in every class). The method in this class got too complex by that. We should better remove the assert and not use it in small methods (because they prevent inlining): http://goo.gl/KjrXe ",
            "author": "Uwe Schindler",
            "id": "comment-13239662"
        },
        {
            "date": "2012-03-27T17:27:01+0000",
            "content": "+1 to remove those asserts... let's see if this fixes the slowdown the nightly builds hit on 3/18: http://people.apache.org/~mikemccand/lucenebench/IntNRQ.html\n ",
            "author": "Michael McCandless",
            "id": "comment-13239667"
        },
        {
            "date": "2012-03-27T17:30:03+0000",
            "content": "Committed the assert removal in revision: trunk 1305909, 3.x 1305911\n\nIf this does not help, we can revert again. But the checks are in my opinion not really useful and too risky. ",
            "author": "Uwe Schindler",
            "id": "comment-13239672"
        },
        {
            "date": "2012-03-30T17:08:54+0000",
            "content": "Removing the asserts apparently didn't change the perf...\n\nI can reproduce the slowdown in a separate test (before/after this commit):\n\n\n                Task    QPS base StdDev base    QPS vInt StdDev vInt      Pct diff\n              IntNRQ        7.11        0.89        6.73        0.58  -23% -   17%\n             Prefix3       16.07        0.96       15.65        0.72  -12% -    8%\n            Wildcard       20.14        0.91       19.67        0.77  -10% -    6%\n            PKLookup      154.62        5.08      151.11        2.82   -7% -    2%\n              Fuzzy1       85.24        1.53       83.87        1.18   -4% -    1%\n              Fuzzy2       44.11        1.03       43.96        0.44   -3% -    3%\n            SpanNear        3.23        0.11        3.22        0.07   -5% -    5%\n      TermBGroup1M1P       42.35        0.49       42.43        1.43   -4% -    4%\n             Respell       65.11        1.91       65.27        1.27   -4% -    5%\n          AndHighMed       54.18        4.04       54.50        2.27  -10% -   13%\n         TermGroup1M       31.27        0.35       31.46        0.63   -2% -    3%\n        TermBGroup1M       45.01        0.33       45.37        1.42   -3% -    4%\n         AndHighHigh       13.35        0.71       13.46        0.50   -7% -   10%\n                Term       82.71        3.12       83.56        2.33   -5% -    7%\n           OrHighMed       10.66        0.67       10.78        0.44   -8% -   12%\n          OrHighHigh        7.08        0.42        7.19        0.26   -7% -   11%\n        SloppyPhrase        5.11        0.24        5.20        0.31   -8% -   13%\n              Phrase       11.14        0.75       11.40        0.50   -8% -   14%\n\n\n\nBut then Uwe made a patch (I'll attach) reducing the byte code for the\nunrolled methods:\n\n\n                Task    QPS base StdDev base    QPS vInt StdDev vInt      Pct diff\n            SpanNear        3.24        0.13        3.18        0.07   -7% -    4%\n              Phrase       11.34        0.68       11.13        0.38  -10% -    7%\n        SloppyPhrase        5.17        0.23        5.08        0.18   -9% -    6%\n      TermBGroup1M1P       41.92        0.80       41.57        0.94   -4% -    3%\n         TermGroup1M       30.74        0.68       30.81        0.96   -5% -    5%\n                Term       80.87        3.52       81.29        2.05   -6% -    7%\n        TermBGroup1M       43.94        0.93       44.17        1.32   -4% -    5%\n          AndHighMed       53.71        2.62       54.21        1.97   -7% -    9%\n         AndHighHigh       13.20        0.42       13.41        0.41   -4% -    8%\n             Respell       65.37        2.70       66.53        3.29   -7% -   11%\n              Fuzzy1       84.29        2.11       86.44        3.36   -3% -    9%\n            PKLookup      149.81        4.20      153.87        9.46   -6% -   12%\n          OrHighHigh        7.19        0.28        7.40        0.48   -7% -   13%\n           OrHighMed       10.82        0.43       11.16        0.73   -7% -   14%\n              Fuzzy2       43.72        0.96       45.24        2.03   -3% -   10%\n            Wildcard       18.96        1.00       20.05        0.39   -1% -   13%\n             Prefix3       14.96        0.83       15.89        0.27   -1% -   14%\n              IntNRQ        5.89        0.58        6.95        0.17    4% -   34%\n\n\n\nSo... I think we should commit it! ",
            "author": "Michael McCandless",
            "id": "comment-13242550"
        },
        {
            "date": "2012-03-30T17:11:24+0000",
            "content": "Uwe's patch to reduce bytecode for the readVInt/Long methods... ",
            "author": "Michael McCandless",
            "id": "comment-13242552"
        },
        {
            "date": "2012-03-30T17:13:57+0000",
            "content": "I an commit that, OK?\n\nWe should also do this in 3.x, Robert are you fine? Otherwise this issue is only half committed to 3.x  Its no risk. ",
            "author": "Uwe Schindler",
            "id": "comment-13242554"
        },
        {
            "date": "2012-03-30T17:22:08+0000",
            "content": "I'm going with your instinct on this one. It would be bad to have a slowdown for 3.6,\nbut I want the negative vlong checks, too. ",
            "author": "Robert Muir",
            "id": "comment-13242564"
        },
        {
            "date": "2012-03-31T11:13:32+0000",
            "content": "After looking a while on the code, I have a further minor improvement. The most common case (int < 128) now exits directly after reading the byte without any & or variable assignment operations.\n\nMike: Can you look at it and maybe do a quick test? I would like to commit this this evening to both branches. ",
            "author": "Uwe Schindler",
            "id": "comment-13243113"
        },
        {
            "date": "2012-03-31T12:22:27+0000",
            "content": "Thanks Uwe, I'll test! ",
            "author": "Michael McCandless",
            "id": "comment-13243125"
        },
        {
            "date": "2012-03-31T13:22:22+0000",
            "content": "Alas, the results are now all over the place!  And I went back to the prior patch and tried to reproduce the above results... and the results are still all over the place.  I think we are chasing Java ghosts at this point... ",
            "author": "Michael McCandless",
            "id": "comment-13243138"
        },
        {
            "date": "2012-03-31T13:42:20+0000",
            "content": "What does your comment mean? Good or bad? ",
            "author": "Uwe Schindler",
            "id": "comment-13243143"
        },
        {
            "date": "2012-03-31T19:33:30+0000",
            "content": "Mike, I was away from home and did not understand your comment, now its clear: You cannot reproduce the speedup from last patch neither can you see a difference with current patch.\n\nI would suggest that I commit this now to trunk, we test a few nights and then commit it to 3.x (Robert needs to backport Ivy to 3.6, so we have some time).\n\nI will commit this later before going to sleep, so we see results tomorrow. ",
            "author": "Uwe Schindler",
            "id": "comment-13243265"
        },
        {
            "date": "2012-03-31T21:22:29+0000",
            "content": "Sorry Uwe, that was exactly it: I don't know what to conclude from the perf runs anymore.\n\nBut +1 for your new patch: it ought to be better since the code is simpler. ",
            "author": "Michael McCandless",
            "id": "comment-13243293"
        },
        {
            "date": "2012-03-31T21:34:06+0000",
            "content": "Committed trunk revision: 1307910\n\nI will keep this issue open for merging to 3.x the next days. ",
            "author": "Uwe Schindler",
            "id": "comment-13243295"
        },
        {
            "date": "2012-04-01T12:04:16+0000",
            "content": "since there is no performance regression, let's do trunk only. ",
            "author": "Robert Muir",
            "id": "comment-13243704"
        },
        {
            "date": "2012-04-01T13:58:56+0000",
            "content": "I added a random test for vints and vlogs in TestIndexInput. I wanted to especially test the long case, which looked broken in DataOutput (but it was correct - but only because of the way how java handles negative values when casting to long - I just made it clear what happens).\n\nRobert: The tests last night showed no change, so I have no preference. ",
            "author": "Uwe Schindler",
            "id": "comment-13243729"
        },
        {
            "date": "2012-04-02T18:12:00+0000",
            "content": "Uwe, is this one good to go now? Can we mark it resolved? ",
            "author": "Robert Muir",
            "id": "comment-13244399"
        },
        {
            "date": "2012-04-02T22:15:00+0000",
            "content": "Commit patch or not? I have no preference. Perf is sometimes slightly better (in microbenchmark on my slow-io system always). ",
            "author": "Uwe Schindler",
            "id": "comment-13244729"
        },
        {
            "date": "2012-04-02T22:18:38+0000",
            "content": "If there is no real measurable performance regression, can we just commit it to trunk?\n\nI really am afraid of last minute .store optimizations. ",
            "author": "Robert Muir",
            "id": "comment-13244738"
        },
        {
            "date": "2012-04-02T22:21:55+0000",
            "content": "Part of the optimization is already committed to 3.x since 4 days, this is just another round of optimization.  ",
            "author": "Uwe Schindler",
            "id": "comment-13244743"
        },
        {
            "date": "2012-04-02T22:23:06+0000",
            "content": "I don't care, it's committed to trunk. The new random test I created yesterday was committed to 3.x, but not the latest code change/opto. You are the release manager, do what you prefer  ",
            "author": "Uwe Schindler",
            "id": "comment-13244748"
        },
        {
            "date": "2012-04-02T22:38:42+0000",
            "content": "Lets resolve the issue. ",
            "author": "Robert Muir",
            "id": "comment-13244770"
        },
        {
            "date": "2012-04-02T22:41:18+0000",
            "content": "Latest code change not backported. ",
            "author": "Uwe Schindler",
            "id": "comment-13244774"
        }
    ]
}