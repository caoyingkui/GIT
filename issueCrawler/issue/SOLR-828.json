{
    "id": "SOLR-828",
    "title": "A RequestProcessor to support updates",
    "details": {
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [],
        "components": [],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Won't Fix"
    },
    "description": "This is same as SOLR-139. A new issue is opened so that the UpdateProcessor approach is highlighted and we can easily focus on that solution.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Noble Paul",
            "id": "comment-12651850",
            "date": "2008-11-30T18:26:44+0000",
            "content": "The old approach is more work compared to the DB approach. It was not good for very fast updates/commits "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12652442",
            "date": "2008-12-02T18:09:50+0000",
            "content": "The new UpdateProcessor called (UpdateableIndexProcessor) must be inserted before RunUpdateProcessor. \n\n\n\tThe UpdateProcessor must add an update method.\n\tthe AddUpdateCommand has a new boolean field append. If append= true multivalued fields will be appended else old ones are removed and new ones are added\n\tThe schema must have a <uniqueKey>\n\tUpdateableIndexProcessor registers postCommit/postOptimize listeners.\n\n\n\nImplementation\nUpdateableIndexProcessor uses a DB (JDBC / Berkley DB java?) to store the data. Each document will be a row in the DB . The uniqueKey of the document will be used as the primary key. The data will be written as a BLOB into a DB column . The format will be javabin serialized format. The javabin format in the current form is inefficient  but it is possible to enhance it (SOLR-810)\n\nThe schema of the table would be\nID : VARCHAR The primarykey of the document as string\nDATA : LONGVARBINARY : A javabin Serialized SolrInputDocument\nSTATUS:ENUM (COMITTED = 0,UNCOMMITTED = 1,UNCOMMITTED_MARKED_FOR_DELETE = 2,COMMITTED_MARKED_FOR_DELETE = 3)\nBOOST:DOUBLE\nFIELD_BOOSTS:VARBINARY A javabin serialized data with boosts of each fields\n\nImplementation of various methods\n\nprocessAdd()\nUpdateableIndexProcessor writes the serialized document to the DB (COMMITTED=false) . Call next UpdateProcessor#add()\n\nprocessDelete()\nUpdateableIndexProcessor gets the Searcher from a core query and find the documents which matches the query and delete from the data table . If it is a delete by id delete the document with that id from data table. Call next UpdateProcessor\n\nprocessCommit()\nCall next UpdateProcessor\n\non postCommit/postOmptimize\nUpdateableIndexProcessor gets all the documents from the data table which is committed =false. If the document is present in the main index it is marked as COMMITTED=true, else it is deleted because a deletebyquery would have deleted it .\n\nprocessUpdate()\nUpdateableIndexProcessor check the document first in data table. If it is present read the document . If it is not present , read all the missing fields from there, and the backup document is prepared\n\nThe single valued fields are used from the incoming document (if present) others are filled from backup doc . If append=true all the multivalues values from backup document are added to the incoming document else the values from backup document is not used if they are present in incoming document also.\n\nprocessAdd() is called on the next UpdateProcessor\n\nnew BackupIndexRequestHandler registered automatically at /backup\nThis exposes the data present in the backup indexes. The user must be able to get any document by id by invoking /backup?id=<value> (multiple id values can be sent eg:id=1&id=2&id=4). This helps the user to query the backup index and construct the new doc if he wishes to do so. \n\nNext steps\nThe datastore can be optimized by not storing the stored fields in the DB. This means on postCommit/postOptimize we must read back the data and remove the already stored fields and store it back. That can be another iteration\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12652828",
            "date": "2008-12-03T15:22:49+0000",
            "content": "A lot of useful comments on the mail thread \n\nhttp://markmail.org/message/57dpsbz3z6dam7q7 "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12670776",
            "date": "2009-02-05T15:10:10+0000",
            "content": "Marking for 1.5 "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12872579",
            "date": "2010-05-27T22:08:12+0000",
            "content": "Bulk updating 240 Solr issues to set the Fix Version to \"next\" per the process outlined in this email...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E\n\nSelection criteria was \"Unresolved\" with a Fix Version of 1.5, 1.6, 3.1, or 4.0.  email notifications were suppressed.\n\nA unique token for finding these 240 issues in the future: hossversioncleanup20100527 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043681",
            "date": "2011-06-03T16:46:30+0000",
            "content": "Bulk move 3.2 -> 3.3 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13106406",
            "date": "2011-09-16T14:50:56+0000",
            "content": "3.4 -> 3.5 "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13234619",
            "date": "2012-03-21T18:08:37+0000",
            "content": "Bulk of fixVersion=3.6 -> fixVersion=4.0 for issues that have no assignee and have not been updated recently.\n\nemail notification suppressed to prevent mass-spam\npsuedo-unique token identifying these issues: hoss20120321nofix36 "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13717392",
            "date": "2013-07-23T18:48:06+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13937294",
            "date": "2014-03-16T19:29:46+0000",
            "content": "I think this is redundant now that we have atomic updates via stored fields and transaction logs. "
        }
    ]
}