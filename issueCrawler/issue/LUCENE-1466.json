{
    "id": "LUCENE-1466",
    "title": "CharFilter - normalize characters before tokenizer",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "New Feature",
        "fix_versions": [
            "2.9"
        ],
        "affect_versions": "2.4",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "This proposes to import CharFilter that has been introduced in Solr 1.4.\n\nPlease see for the details:\n\n\tSOLR-822\n\thttp://www.nabble.com/Proposal-for-introducing-CharFilter-to20327007.html",
    "attachments": {
        "LUCENE-1466.patch": "https://issues.apache.org/jira/secure/attachment/12394512/LUCENE-1466.patch",
        "LUCENE-1466-back.patch": "https://issues.apache.org/jira/secure/attachment/12411327/LUCENE-1466-back.patch",
        "LUCENE-1466-TestCharFilter.patch": "https://issues.apache.org/jira/secure/attachment/12411598/LUCENE-1466-TestCharFilter.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-11-23T16:48:21+0000",
            "content": "a patch attached. ",
            "author": "Koji Sekiguchi",
            "id": "comment-12650034"
        },
        {
            "date": "2009-03-19T11:34:30+0000",
            "content": "renamed correctPosition() to correct() because it is for correcting token offset, not for token position. ",
            "author": "Koji Sekiguchi",
            "id": "comment-12683436"
        },
        {
            "date": "2009-06-11T02:20:38+0000",
            "content": "Anyone want to step up for this one or should we push it off to 3.0? ",
            "author": "Mark Miller",
            "id": "comment-12718277"
        },
        {
            "date": "2009-06-11T03:49:06+0000",
            "content": "just as an alternative, i have a different mechanism as part of lucene-1488 patch I am working on. But maybe its good to have options, as it depends on the ICU library.\n\nbelow is excerpt from javadoc.\n\nA TokenFilter that transforms text with ICU.\n\nICU provides text-transformation functionality via its Transliteration API.\nAlthough script conversion is its most common use, a transliterator can actually perform a more general class of tasks. \n...\nSome useful transformations for search are built-in:\n\n\tConversion from Traditional to Simplified Chinese characters\n\tConversion from Hiragana to Katakana\n\tConversion from Fullwidth to Halfwidth forms.\n...\nExample usage:\n\tstream = new ICUTransformFilter(stream, Transliterator.getInstance(\"Traditional-Simplified\"));\n\n ",
            "author": "Robert Muir",
            "id": "comment-12718291"
        },
        {
            "date": "2009-06-11T16:19:25+0000",
            "content": "If I can vote for it, I want it to be part of 2.9. I know several (at least five) companies use this feature in production. We use it via Solr (SOLR-822), but we hope it to be part of Lucene core. ",
            "author": "Koji Sekiguchi",
            "id": "comment-12718539"
        },
        {
            "date": "2009-06-11T18:32:38+0000",
            "content": "I'd like to get this in for 2.9. ",
            "author": "Michael McCandless",
            "id": "comment-12718579"
        },
        {
            "date": "2009-06-19T00:43:04+0000",
            "content": "updated patch attached.\n\n\tsync trunk (smart chinese analyzer(LUCENE-1629), etc.)\n\tadded a useful idiom to get CharStream and make private CharReader constructor\n\n ",
            "author": "Koji Sekiguchi",
            "id": "comment-12721588"
        },
        {
            "date": "2009-06-19T12:25:17+0000",
            "content": "Thanks for the update Koji!\n\nThe patch looks good.  Some questions:\n\n\n\tCan you add a CHANGES entry describing this new feature, as well\n    as the change in type of Tokenizer.input?\n\n\n\n\n\tCan we rename NormalizeMap -> NormalizeCharMap?\n\n\n\n\n\tCould you add some javadocs to NormalizeCharMap,\n    MappingCharFilter, BaseCharFilter?\n\n\n\n\n\tThe BaseCharFilter correct method looks spookily costly (has a for\n    loop, going backwards for all added mappings).  It seems like in\n    practice it should not be costly, because typically one corrects\n    the offset only for the \"current\" token?  And, one could always\n    build their own CharFilter (eg using arrays of ints or something)\n    if they needed a more efficient mapping.\n\n\n\n\n\tMappingCharFilter's match method is recursive.  But I think the\n    depth of that recursion equals the length of character sequence\n    that's being mapped, right?  So risk of stack overlflow should be\n    basically zero, unless someone does some insanely long character\n    string mappings?\n\n\n\n\nI have some back-compat concerns. First, I see these 2 failures in\n\"ant test-tag\":\n\n\n[junit] Testcase: testExclusiveLowerNull(org.apache.lucene.search.TestRangeQuery):\tCaused an ERROR\n[junit] input\n[junit] java.lang.NoSuchFieldError: input\n[junit] \tat org.apache.lucene.search.TestRangeQuery$SingleCharAnalyzer$SingleCharTokenizer.incrementToken(TestRangeQuery.java:247)\n[junit] \tat org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:160)\n[junit] \tat org.apache.lucene.index.DocFieldConsumersPerField.processFields(DocFieldConsumersPerField.java:36)\n[junit] \tat org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:234)\n[junit] \tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:773)\n[junit] \tat org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:751)\n[junit] \tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2354)\n[junit] \tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2328)\n[junit] \tat org.apache.lucene.search.TestRangeQuery.insertDoc(TestRangeQuery.java:306)\n[junit] \tat org.apache.lucene.search.TestRangeQuery.initializeIndex(TestRangeQuery.java:289)\n[junit] \tat org.apache.lucene.search.TestRangeQuery.testExclusiveLowerNull(TestRangeQuery.java:317)\n[junit] \n[junit] \n[junit] Testcase: testInclusiveLowerNull(org.apache.lucene.search.TestRangeQuery):\tCaused an ERROR\n[junit] input\n[junit] java.lang.NoSuchFieldError: input\n[junit] \tat org.apache.lucene.search.TestRangeQuery$SingleCharAnalyzer$SingleCharTokenizer.incrementToken(TestRangeQuery.java:247)\n[junit] \tat org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:160)\n[junit] \tat org.apache.lucene.index.DocFieldConsumersPerField.processFields(DocFieldConsumersPerField.java:36)\n[junit] \tat org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:234)\n[junit] \tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:773)\n[junit] \tat org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:751)\n[junit] \tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2354)\n[junit] \tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2328)\n[junit] \tat org.apache.lucene.search.TestRangeQuery.insertDoc(TestRangeQuery.java:306)\n[junit] \tat org.apache.lucene.search.TestRangeQuery.initializeIndex(TestRangeQuery.java:289)\n[junit] \tat org.apache.lucene.search.TestRangeQuery.testInclusiveLowerNull(TestRangeQuery.java:351)\n\n\n\nThese are JAR drop-inability failures, because the type of\nTokenizer.input changed from Reader to CharStream.  Since CharStream\nsubclasses Reader, references to Tokenizer.input would be fixed w/ a\nsimple recompile.\n\nHowever, assignments to \"input\" by external subclasses of Tokenizer\nwill result in compilation error.  You have to replace such\nassignments with this.input = CharReader.get(input).  Since input\nis protected, any subclass can up and assign to it.  The good news is\nthis'd be a catastrophic compilation error (vs something silent at\nruntime); the bad news is that's [unfortunately] against our\nback-compat policies.\n\nAny ideas how we can fix this to \"migrate\" to CharStream without\nbreaking back compat? ",
            "author": "Michael McCandless",
            "id": "comment-12721761"
        },
        {
            "date": "2009-06-21T00:15:03+0000",
            "content": "I think we should make an exception to back-compat here, and simply\nchange TokenStream.input from Reader to CharStream (subclasses\nReader).  Properly respecting back-compat will be alot of work, and,\nif external subclasses are directly assigning to input, they really\nought to be using reaset(Reader) instead.\n\nI updated the patch with the above issues, fixed some whitespace\nissues, added Tokenizer.reset(CharStream) and patched back-compat. ",
            "author": "Michael McCandless",
            "id": "comment-12722281"
        },
        {
            "date": "2009-06-21T00:42:24+0000",
            "content": "Oops. Thanks for the updated patch, Mike!\n\n\n\tCan you add a CHANGES entry describing this new feature, as well\n      as the change in type of Tokenizer.input?\n\tCan we rename NormalizeMap -> NormalizeCharMap?\n\tCould you add some javadocs to NormalizeCharMap,\n      MappingCharFilter, BaseCharFilter?\n\n\nYour patch looks good!\n\n\n\tThe BaseCharFilter correct method looks spookily costly (has a for\n      loop, going backwards for all added mappings). It seems like in\n      practice it should not be costly, because typically one corrects\n      the offset only for the \"current\" token? And, one could always\n      build their own CharFilter (eg using arrays of ints or something)\n      if they needed a more efficient mapping.\n\n\nYes, users can create their own CharFilter if they needed a more efficient mapping.\n\n\n\tMappingCharFilter's match method is recursive. But I think the\n      depth of that recursion equals the length of character sequence\n      that's being mapped, right? So risk of stack overlflow should be\n      basically zero, unless someone does some insanely long character\n      string mappings?\n\n\nYou are correct.\n\n\nI think we should make an exception to back-compat here, and simply\nchange TokenStream.input from Reader to CharStream (subclasses\nReader). Properly respecting back-compat will be alot of work, and,\nif external subclasses are directly assigning to input, they really\nought to be using reaset(Reader) instead. \nI agree with you, Mike. ",
            "author": "Koji Sekiguchi",
            "id": "comment-12722283"
        },
        {
            "date": "2009-06-21T09:51:41+0000",
            "content": "OK thanks Koji.  I'll add a bit more to the javadocs of BaseCharFilter about the performance caveats.\n\nI plan to commit in a day or too.\n\nThanks for persisting Koji!\n\nSolr has already committed CharFilter, and had to workaround it not being in Lucene with classes like CharStreamAwareTokenizer, etc.  Koji, are you planning to work out a patch for Solr to switch to Lucene's impl? ",
            "author": "Michael McCandless",
            "id": "comment-12722329"
        },
        {
            "date": "2009-06-21T22:48:49+0000",
            "content": "Solr has already committed CharFilter, and had to workaround it not being in Lucene with classes like CharStreamAwareTokenizer, etc. Koji, are you planning to work out a patch for Solr to switch to Lucene's impl?\n\nYeah, why not!  ",
            "author": "Koji Sekiguchi",
            "id": "comment-12722435"
        },
        {
            "date": "2009-06-22T15:12:20+0000",
            "content": "Added TestMappingCharFilter test case (copied from Solr). ",
            "author": "Koji Sekiguchi",
            "id": "comment-12722628"
        },
        {
            "date": "2009-06-23T19:16:41+0000",
            "content": "OK I just committed this.  Thanks Koji!  Can you open a Solr issue & work out a patch so Solr can cutover to this?  Thanks. ",
            "author": "Michael McCandless",
            "id": "comment-12723256"
        },
        {
            "date": "2009-06-23T22:40:30+0000",
            "content": "Thank you Mike for committing this! I'll open a ticket for Solr soon. BTW, I cannot see TestMappingCharFilter that is in the latest patch I attached. Is there a problem in the test or just slipped over? ",
            "author": "Koji Sekiguchi",
            "id": "comment-12723334"
        },
        {
            "date": "2009-06-24T00:12:40+0000",
            "content": "Woops, sorry, I did indeed see your new patch and applied it but then failed to svn add.  OK I just committed them.  Thanks! ",
            "author": "Michael McCandless",
            "id": "comment-12723374"
        },
        {
            "date": "2009-06-24T02:06:39+0000",
            "content": "an additional test for CharFilter that I forgot to move from Solr... Mike, can you commit this? Thank you.  ",
            "author": "Koji Sekiguchi",
            "id": "comment-12723398"
        },
        {
            "date": "2009-06-24T08:55:09+0000",
            "content": "an additional test for CharFilter that I forgot to move from Solr... Mike, can you commit this? Thank you.\n\nDone... thanks! ",
            "author": "Michael McCandless",
            "id": "comment-12723500"
        }
    ]
}