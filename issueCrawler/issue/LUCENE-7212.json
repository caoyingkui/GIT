{
    "id": "LUCENE-7212",
    "title": "Add Geo3DPoint equivalents of LatLonPointDistanceComparator and LatLonPointSortField",
    "details": {
        "resolution": "Unresolved",
        "affect_versions": "6.0",
        "components": [],
        "labels": "",
        "fix_versions": [],
        "priority": "Major",
        "status": "Open",
        "type": "Improvement"
    },
    "description": "Geo3D has a number of distance measurements and a generic way of computing interior distance.  It would be great to take advantage of that for queries that return results ordered by interior distance.",
    "attachments": {
        "LUCENE-7212.patch": "https://issues.apache.org/jira/secure/attachment/12803644/LUCENE-7212.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15239109",
            "author": "Karl Wright",
            "date": "2016-04-13T11:50:02+0000",
            "content": "I looked at ...\nclass LatLonPointDistanceComparator extends FieldComparator<Double> implements LeafFieldComparator {\n\n... briefly.  This implementation is clearly not general at all for search shape.  But I think one could write a geo3d equivalent that would work for any GeoDistance implementation with a provided DistanceStyle.  This would also serve to find objects within the implied Membership implementation, because the contract for GeoDistance calls for it to return MAX_VALUE when the point is outside the shape, so it would work beautifully.\n\nThe big problem is that there's not much performance gain possible because there's currently no geo3d general means of going from a GeoDistance object and distance measurement to a bounding XYZSolid.  That is, there's no equivalent to Rectangle.fromPointDistance().  Solving that is a harder problem and would require extensions to all the shape objects that implement GeoDistance.  But I think that would clearly be step 2 anyway.\n\nGeo3DPointDistanceComparator would be constructed, therefore, with a GeoDistance object and a DistanceStyle object, and would call GeoDistance.computeDistance() using these parameters when it needed to find a distance.  The decoding/encoding of XYZ values is most of the class. "
        },
        {
            "id": "comment-15239199",
            "author": "Michael McCandless",
            "date": "2016-04-13T13:06:36+0000",
            "content": "Maybe we could start with the equivalent of LatLonPoint.newDistanceSort?  This will sort all hits according to their earth-as-sphere-surface-distance (haversine) from the provided point?\n\nIt's really cool if geo3d can also get you distance from any shape, so maybe it could have two newDistanceSort methods, one taking lat/lon, another taking a GeoShape?\n\nIdeally we wouldn't need to expose things like GeoDistance and DistanceStyle to the user?  Can these somehow be under-the-hood implementation details? "
        },
        {
            "id": "comment-15239219",
            "author": "Karl Wright",
            "date": "2016-04-13T13:23:33+0000",
            "content": "Ideally we wouldn't need to expose things like GeoDistance and DistanceStyle to the user? Can these somehow be under-the-hood implementation details?\n\nThe classes that correspond to LatLonPointDistanceComparator and LatLonPointSortField will need to have a GeoDistance and DistanceStyle passed to them.\n\nThe newDistanceSort method would need to be equivalent to the LatLonPoint API, so it would construct a GeoCircle that occupies the whole world with the specified center, and use a standard arc distance DistanceStyle.  But the problem is that such a query is completely unconstrained without the ability to go from shape/distance to XYZSolid bound, so it would not perform reasonably in that form.  If you introduced an upper bound to the circle radius as an additional argument, then it would work fine without additional geo3d code.\n\nI would in general think that all \"newXXXQuery\" methods would have a corresponding \"newXXXSort\" method, if the underlying shape implements GeoDistance.  So there would be \"newDistanceSort\" and \"newPathSort\", but not \"newBoxSort\" or \"newPolygonSort\".  The arguments would have to correspond exactly, though, to \"newXXXQuery\", including distance bounds, for the geo3d API. "
        },
        {
            "id": "comment-15239338",
            "author": "Karl Wright",
            "date": "2016-04-13T14:32:11+0000",
            "content": "FWIW, the way I'd add bounds computation to shapes by distance would be as follows:\n\n(1) Add a method to the GeoDistance interface:\n\nvoid getDistanceBounds(DistanceStyle ds, Bounds xxx, double distanceValue);\n\n\nThe purpose of this method would be to compute the bounds for the current shape where the chosen distance metric was less than the specified distanceValue.\n(2) Add this method to the following classes:\n\nGeoBaseDistanceShape\nGeoDegeneratePoint\nGeoStandardCircle\nGeoStandardPath\n\nThe base class implementation would check if distanceValue was equal to MAX_VALUE, and if so would simply compute the bounds for the current shape.  If less than that value, it would call an abstract method that would do the job.  For GeoStandardCircle and GeoStandardPath, the approach taken for doing the actual computation would involve newly constructing a constrained GeoStandardCircle or GeoStandardPath, and then computing the bounds from that.  There would likely be new constructors, therefore, which would hopefully repurpose planes etc. if possible from the starting GeoStandardCircle or GeoStandardPath.  It may be easier code-wise to just build the new shapes from scratch, though.\n\nCreating a new complex object is expensive to do both mathematically and in terms of object creation.  It really cannot be easily avoided, however.  That means that when you call getDistanceBounds() you should expect it to be expensive enough that you wouldn't want to do it on every document addition, or even every 10. "
        },
        {
            "id": "comment-15239365",
            "author": "Robert Muir",
            "date": "2016-04-13T14:54:11+0000",
            "content": "How expensive are geo3d's distance formulas though?\n\nThe 2D distance comparator examines the least-competitive element of the priority queue when it changes, to create a bounding box that can quickly reject hits that won't compete anyway in a cheaper way than invoking haversin distance.\n\nIn the worst case, this will happen for every document, so it has a guard in the code to start doing it less often (only every Nth update) if it gets called too many times.\n\nThe optimization helps for two reasons in 2D:\n1. haversin distance computation is expensive\n2. the bounding box check can be done in \"encoded\" integer space so it saves any decoding costs.\n\nFor 3D the tradeoffs may be different. "
        },
        {
            "id": "comment-15239384",
            "author": "Karl Wright",
            "date": "2016-04-13T15:05:31+0000",
            "content": "How expensive are geo3d's distance formulas though?\n\nIt varies \u2013 that's one of the reasons there are different DistanceStyle implementations.  The cheapest (squared linear distance) is quite cheap; the most expensive (arc distance) requires several Math.acos invocations (for a path). "
        },
        {
            "id": "comment-15239417",
            "author": "Robert Muir",
            "date": "2016-04-13T15:24:30+0000",
            "content": "Also the encoding plays a role: to support sorting at all geo3d needs to figure out how to encode it in docvalues.\n\nDoes geo3d also need two-phase query support? If so this also relates to how to encode in docvalues (and whether its necessary or maybe should be optional or any of that). We don't benchmark this at all, but it seems to be almost necessary for 2D to meet user expectations, so 2D requires it. In other words a distance or poly query may be slow, but if they restrict the dataset with other filters then its less slower as it defers per-hit computations until its truly needed. If this is not needed for 3D, it might simplify how to support sorting as any encoding need only be geared at that. "
        },
        {
            "id": "comment-15239430",
            "author": "Karl Wright",
            "date": "2016-04-13T15:30:24+0000",
            "content": "Does geo3d also need two-phase query support?\n\nProbably.  The same arguments apply here for 3D vs 2D.  But I don't know what it entails. "
        },
        {
            "id": "comment-15239449",
            "author": "Robert Muir",
            "date": "2016-04-13T15:37:35+0000",
            "content": "\nIt varies \u2013 that's one of the reasons there are different DistanceStyle implementations. The cheapest (squared linear distance) is quite cheap; the most expensive (arc distance) requires several Math.acos invocations (for a path).\n\nI would start simple as mike suggests and not try to make a generic solution: maybe just support one of them as a start. Sorting can be a real hotspot: look at all the craziness the 2D one does to speed up/avoid haversin computations. The best solution is very much tied to both the formula in question and the underlying encoding. "
        },
        {
            "id": "comment-15239460",
            "author": "Karl Wright",
            "date": "2016-04-13T15:43:00+0000",
            "content": "Also the encoding plays a role: to support sorting at all geo3d needs to figure out how to encode it in docvalues.\n\nRight, some six months ago I attempted to code a packing of (X,Y,Z) into a single integer, using knowledge of the IEEE floating point spec to make this fast.  I'll have to find or reconstruct this.  What characteristics does this packing need to have in order to function properly in a sorted environment? In a two-phase environment? "
        },
        {
            "id": "comment-15239470",
            "author": "Robert Muir",
            "date": "2016-04-13T15:49:11+0000",
            "content": "This is why i asked if we need two-phase. Maybe we need to extend the benchmark first to understand if its really necessary? \n\nWhen i look at what geo3d computes per-doc for a distance query, it seems to look much less expensive (e.g. just some multiplication) and I think this is also why its faster at distances. For polygons we need to investigate too (these get slower, linearly, as polygon complexity increases with 2D today). \n\nIf geo3d doesn't have these problems, and doesn't really need two-phase, then we simplify the problem: and what goes in there need not be 100% consistent with the query (ok, it could cause some confusion if its not, but it wont break things). This means the encoding could be different, more lossy, etc. "
        },
        {
            "id": "comment-15239494",
            "author": "Karl Wright",
            "date": "2016-04-13T15:57:41+0000",
            "content": "This is why i asked if we need two-phase. Maybe we need to extend the benchmark first to understand if its really necessary?\n\n+1 to that.  I'm operating in the dark at the moment. I presume in order to do that we have to have a naive implementation available?\n\nWhen i look at what geo3d computes per-doc for a distance query, it seems to look much less expensive (e.g. just some multiplication) and I think this is also why its faster at distances. For polygons we need to investigate too (these get slower, linearly, as polygon complexity increases with 2D today).  If geo3d doesn't have these problems, and doesn't really need two-phase, then we simplify the problem:\n\ngeo3d polygon performance will still degrade linearly with the number of sides, but your assessment is accurate for the cheapness of evaluating any one of those sides, yes, and of the cheapness of evaluating circle membership.\n\nand what goes in there need not be 100% consistent with the query (ok, it could cause some confusion if its not, but it wont break things). This means the encoding could be different, more lossy, etc.\n\nOk, you've lost me here, but that's my problem.  "
        },
        {
            "id": "comment-15239509",
            "author": "Robert Muir",
            "date": "2016-04-13T16:09:28+0000",
            "content": "\n+1 to that. I'm operating in the dark at the moment. I presume in order to do that we have to have a naive implementation available?\n\nWe just need to issue queries in the benchmark that are e.g. AND'd with a filter that only matches X% of the documents. This is a good thing to benchmark anyway, its far more realistic to expect there is more to the query than just a spatial component (e.g. \"pizza near my house\"). \n\nLemme try to dig into this... "
        },
        {
            "id": "comment-15239526",
            "author": "Karl Wright",
            "date": "2016-04-13T16:17:58+0000",
            "content": "As for encoding, we basically get 20 bits per x, y, or z value.  If proximity is a requirement, then 3D morton encoding can be used:\n\nhttp://stackoverflow.com/questions/1024754/how-to-compute-a-3d-morton-number-interleave-the-bits-of-3-ints\n\n... but maybe that's not needed.\nThe twenty bits I would extract from the IEEE double, after scaling by the planetmodel max distance in the corresponding dimension.  So we'd know that the max value would be 1.0, and the min value would be -1.0, and thus we can select the most significant mantissa bits.  Or maybe it's easier to just multiply and Math.floor.  We'll see.  But 2^20 ~= 10^6, so precision is not great, but probably enough. "
        },
        {
            "id": "comment-15239819",
            "author": "Robert Muir",
            "date": "2016-04-13T18:50:38+0000",
            "content": "I ran some rough benchmarks. its tough because the filter we use is itself costly  Also i use an ancient jvm and hardware and my numbers are in general very different from what mike reports.\n\n\n2D distance with 100% filter: 16.0 QPS\n2D distance with 10% filter: 24.1 QPS\n2D distance with 1% filter: 42.7 QPS\n2D distance with 0.1% filter: 51.1 QPS\n2D distance with 0.0001% filter: 55.7 QPS \n\n3D distance with 100% filter: 21.7 QPS\n3D distance with 10% filter: 31.9 QPS\n3D distance with 1% filter: 36.6 QPS\n3D distance with 0.1% filter: 38.2 QPS\n3D distance with 0.0001% filter: 42.3 QPS\n\n\n\nFor distance: I'm not sure geo3d needs two-phase support: i don't think its per-doc checks are so expensive? For 2D we should reinvestigate it too: maybe it has outgrown the need for a \"two-phase crutch\". Maybe its really overkill after LUCENE-7147 since there are no longer tons of per-doc boundary cases being evaluated. And there is truly some cost for \"throwing away\" the data points that BKD sends us and then suffering docvalues reads later to fetch them from a different storage.\n\nFor polygons: its hard for me to see. Both 2D and 3D get really slow as polygon complexity increases. For 2D the per-doc check is O(#vertices). But 2D also has an expensive up-front cost (grid construction) that you can see bounds the performance of these complex queries regardless of how any hits they have. For 3D i'm not sure where the heavy cost lies with these. It might not be related to per-doc checks at all.\n\nPolygon (n=500)\n\n2D poly with 100% filter: 7.2 QPS\n2D poly with 10% filter: 16.1 QPS\n2D poly with 1% filter: 25.9 QPS\n2D poly with 0.1% filter: 28.6 QPS\n2D poly with 0.0001% filter: 29.8 QPS\n\n3D poly with 100% filter: 1.9 QPS\n...\n3D poly with 0.0001% filter: 2.0 QPS\n\n\n\nThis is my commit: https://github.com/mikemccand/luceneutil/commit/4e22c6abeedb00bf6de66d4c3fbefbd95cb26b98\n\nAnyway take the numbers with a grain of salt. It might be worthwhile to investigate what is happening with polygons with geo3d though, i think something is not right there. Lots more experiments needed... "
        },
        {
            "id": "comment-15239846",
            "author": "Karl Wright",
            "date": "2016-04-13T19:00:29+0000",
            "content": "It might be worthwhile to investigate what is happening with polygons with geo3d though, i think something is not right there.\n\nSomething is definitely funny.  Did you try this with prebuilt queries, or without?  Query construction is likely to be slow, and this is known \u2013 it's an O(N^2) algorithm in places.\n\nMichael McCandless: any idea why these numbers would be so different from the ones you reported earlier? "
        },
        {
            "id": "comment-15239853",
            "author": "Robert Muir",
            "date": "2016-04-13T19:02:53+0000",
            "content": "Well i dont think we should do \"prebuilt queries\". If its expensive to do up-front stuff this should be part of the cost. 2D has a prebuilding too at the moment, it happens in createWeight though. But to an end user, its still latency. "
        },
        {
            "id": "comment-15239855",
            "author": "Michael McCandless",
            "date": "2016-04-13T19:04:29+0000",
            "content": "Michael McCandless: any idea why these numbers would be so different from the ones you reported earlier?\n\nI think I tested 10-gons in my previous tests?  Seems like 500-gons add quite a bit more cost? "
        },
        {
            "id": "comment-15239908",
            "author": "Karl Wright",
            "date": "2016-04-13T19:33:57+0000",
            "content": "Well i dont think we should do \"prebuilt queries\". If its expensive to do up-front stuff this should be part of the cost. 2D has a prebuilding too at the moment, it happens in createWeight though. But to an end user, its still latency.\n\nIt's latency that can be controlled, though - the typical use case for very large polygons is for them to be associated with countries etc., and there are not many of those, and they can be prebuilt.  So I think it is worth considering the two costs separately.\n\nI can make polygon building much faster if I know that you are handing me well-formed convex polygons (or concave ones).  The time comes from tiling irregular polygons in a way that doesn't violate convexity/concavity constraints.  But that changes the API quite a bit then.  "
        },
        {
            "id": "comment-15239912",
            "author": "Robert Muir",
            "date": "2016-04-13T19:37:27+0000",
            "content": "No need to change the API. detecting these things can be done in linear or n log n time. I continue to insist that users do not need to concern themselves with this stuff. "
        },
        {
            "id": "comment-15239915",
            "author": "Karl Wright",
            "date": "2016-04-13T19:40:25+0000",
            "content": "I think I tested 10-gons in my previous tests? Seems like 500-gons add quite a bit more cost?\n\nOf course they do.  If order N then it would be roughly 50x as much time to build a query.  Since this is O(N^2) its up to 2500x worse, approximately.\n\nI still maintain that anyone who is constructing a 500-point polygon for each query in real life needs to have their head examined.  \n "
        },
        {
            "id": "comment-15239916",
            "author": "Karl Wright",
            "date": "2016-04-13T19:41:29+0000",
            "content": "Well, go ahead, have a look at that code then.  It's all in GeoPolygonFactory.  I'm open to suggestions.\n "
        },
        {
            "id": "comment-15239937",
            "author": "Karl Wright",
            "date": "2016-04-13T20:00:19+0000",
            "content": "detecting these things can be done in linear or n log n time.\n\nOk, this code snippet from GeoConvexPolygon does nothing more than confirm that a polygon is in fact convex in the 3D world.  Can you point me at a resource that would describe how to do this in O(n log n) time?  The actual checks that are done in GeoPolygonFactory are a good deal more complex than this but one algorithm would likely solve both problems.\n\nIn a nutshell, the problem in the 3D planar world is that relationships are not transitive.  If you show that a point is within an edge, and you construct a new edge with it, and then you find a subsequent point, there's no guarantee that the newest point will be within the oldest edge.\n\nAs a practical matter, this needs to be done only once (in GeoPolygonFactory), and this check can go away from GeoConvexPolygon since it's now package private.  But that does not stop the need to perform a check of this kind and break up the original polygon accordingly.\n\n\n\n    // In order to naively confirm that the polygon is convex, I would need to\n    // check every edge, and verify that every point (other than the edge endpoints)\n    // is within the edge's sided plane.  This is an order n^2 operation.  That's still\n    // not wrong, though, because everything else about polygons has a similar cost.\n    for (int edgeIndex = 0; edgeIndex < edges.length; edgeIndex++) {\n      final SidedPlane edge = edges[edgeIndex];\n      for (int pointIndex = 0; pointIndex < points.size(); pointIndex++) {\n        if (pointIndex != edgeIndex && pointIndex != legalIndex(edgeIndex + 1)) {\n          if (!edge.isWithin(points.get(pointIndex)))\n            throw new IllegalArgumentException(\"Polygon is not convex: Point \" + points.get(pointIndex) + \" Edge \" + edge);\n        }\n      }\n    }\n\n "
        },
        {
            "id": "comment-15240212",
            "author": "Karl Wright",
            "date": "2016-04-13T23:11:56+0000",
            "content": "I've moved the polygon construction cost discussion to LUCENE-7216. "
        },
        {
            "id": "comment-15240230",
            "author": "Robert Muir",
            "date": "2016-04-13T23:24:34+0000",
            "content": "This is the one i was investigating for 2D: https://en.wikibooks.org/wiki/Algorithm_Implementation/Geometry/Convex_hull/Monotone_chain\n\nBut there are some steps/cleanups before this can be done/used in a reasonable fashion "
        },
        {
            "id": "comment-15240243",
            "author": "Karl Wright",
            "date": "2016-04-13T23:37:00+0000",
            "content": "For 2D, there's quite a bit to choose from, e.g. the O( N ) cross-product solution described here: http://stackoverflow.com/questions/471962/how-do-determine-if-a-polygon-is-complex-convex-nonconvex\n\nFor 3D it's more challenging.\nI'll keep thinking about it, of course... "
        },
        {
            "id": "comment-15240261",
            "author": "Robert Muir",
            "date": "2016-04-13T23:44:06+0000",
            "content": "Yeah, for this issue, it would be great to just confirm the cost is really an up-front issue for polygons. As long as we are confident in that (and it looks that way to me), then we can continue here unblocked.\n\nI have the feeling thats all it is, and that Geo3D doesn't really need two-phase iteration. (Hopefully 2D won't need it either, if it can clean up its polygon act to not be trappy).\n\nSo I think this issue can just focus on docvalues for distance? \n\nAs far as LatLonPoint goes, if we can remove the need for it to use docvalues in polygon queries, I think its better to have a separate *Field for going into docvalues. \n\nIt means that the docvalues become optional, just like they are for the other fields in lucene. You add it to your doc too if you want sorting, faceting, expressions, etc, but you don't need it if you are just doing searches, and vice-versa: you don't need the multi-dimensional ports index if you are just doing sorting/faceting/expressions. so the DocValues field really is a completely unrelated thing. We might want to do it the same way for 3D. "
        },
        {
            "id": "comment-15240275",
            "author": "Karl Wright",
            "date": "2016-04-13T23:55:12+0000",
            "content": "So I think this issue can just focus on docvalues for distance?\n\n+1\n\nWe might want to do it the same way for 3D.\n\nI'm happy to follow your lead.  I was going to model the 3D Comparator/Sort code on LatLon's.  If you want to make structural changes first, let me know and I'll hold off until you're ready. "
        },
        {
            "id": "comment-15240287",
            "author": "Robert Muir",
            "date": "2016-04-14T00:04:06+0000",
            "content": "I don't think the SortField/Comparator would change at all. Its just if we can make polygon queries not need a \"two-phase crutch\" then docvalues are no longer needed for queries. So we'd add something like LatLonDocValuesField (ugh, will mull on the name) and move the newSortField() method to it, and turn off the docvalues in LatLonPoint: its no longer needed for queries.\n\nnewSortField() and any other docvalues-oriented methods (e.g. getting a LUCENE-5325 \"stream\" of latitudes or longitudes for expressions integration) would be on that LatLonDocValuesField: it just concerns itself with docvalues, and the LatLonPoint just concerns itself with the points stuff. \n\nJust like the separation of DoublePoint vs DoubleDocValuesField in core/. "
        },
        {
            "id": "comment-15242911",
            "author": "Karl Wright",
            "date": "2016-04-15T12:55:34+0000",
            "content": "I'm still holding off on doing anything here for two reasons:\n(1) I need to finish figuring out the math of producing a bound that is constrained by distance for geo3d, and\n(2) I want to wait until the docvalues field is pulled out of the corresponding LatLon classes before introducing geo3d classes.\n\nIf I'm not very much mistaken, (1) is crucial for this functionality to be a performance win, so I need to make sure the math works first.  I whacked away at it for a good chunk of my non-meeting hours yesterday before getting distracted by general large polygon performance concerns, but I ran into trouble with expression complexity related to the ellipsoid nature of WGS84.  I probably need to transform the problem in some way to make it tractable, and I tried one way already that looked like it would work, but it wound up still making too messy an expression to deal with.  Working more on this this weekend... "
        },
        {
            "id": "comment-15243105",
            "author": "Robert Muir",
            "date": "2016-04-15T15:29:19+0000",
            "content": "ok but #2 should take some time. i sorta see it as really important \"sandiness\" to fix though . in the meantime if you want to proceed with 3D, and come up with e.g. a 64-bit encoding, I can help get the docvalues field together or whatever you need.\n\nAs far as if #1 is crucial, i think you have to do some experiments to find out? In 2D we know the bounding box check is a big win. This optimization is a very tricky balance because computing that is itself expensive, and users do crazy things (sort with enormous top N's or somehow data could be in adversarial sort order causing shittons of priority queue updates). The situation for 3D may be different. "
        },
        {
            "id": "comment-15244060",
            "author": "Karl Wright",
            "date": "2016-04-16T07:12:06+0000",
            "content": "Ok, I have the math worked out for one metric: arc distance.  But now additional unrelated problems have landed on my plate... looking at those first.  If I get done with them before you're ready with #2, I'll get started. "
        },
        {
            "id": "comment-15247822",
            "author": "ASF subversion and git services",
            "date": "2016-04-19T14:20:36+0000",
            "content": "Commit 23422908165f62581c271524955af2ab0e6e069f in lucene-solr's branch refs/heads/master from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=2342290 ]\n\nLUCENE-7212: Structural changes necessary to support distance-limited bounds. "
        },
        {
            "id": "comment-15247826",
            "author": "ASF subversion and git services",
            "date": "2016-04-19T14:22:13+0000",
            "content": "Commit 4dd7c0c9191b0314e6de464f3998ae09ed859f54 in lucene-solr's branch refs/heads/branch_6x from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=4dd7c0c ]\n\nLUCENE-7212: Structural changes necessary to support distance-limited bounds. "
        },
        {
            "id": "comment-15259819",
            "author": "Michael McCandless",
            "date": "2016-04-27T09:03:14+0000",
            "content": "So the 2D world has now separated out doc values (LatLonDocValuesField), used if you need distance sorting, from points (LatLonPoint), used if you need filtering by shape.\n\nIt sounds like 3D can likely do the same?  I.e., we don't need two-phase support for geo3d shapes since the per-hit check is fast?\n\nIn which case ... we can do a more lossy (x/y/z -> 64 bits) encoding, solely for distance sorting?  Will the geo3d distance checking APIs be upset about the heavy quantization (21 bits per value, instead of 32 bits per value points is using)?  I know we've had issues with \"the point is not on the earth's surface\" already with 32 bits per x/y/z.\n\nAlternatively, we could maybe use BINARY dvs, and keep 32 bits per component ... but this gets messy with multi-valued fields.  geo2d (LatLonDocValuesField) does support multi-valued fields but I wonder how necessary that really is ... "
        },
        {
            "id": "comment-15259973",
            "author": "Karl Wright",
            "date": "2016-04-27T11:04:51+0000",
            "content": "Michael McCandless, the resolution is quite important for determination if in-set/out-of-set for shapes like small circles.  However, for distance measurement/ordering, I can't see any need to be that precise, so if the docvalues is only used for distance computation then 64 bits should be OK. "
        },
        {
            "id": "comment-15281523",
            "author": "Karl Wright",
            "date": "2016-05-12T13:40:32+0000",
            "content": "Here's a geo3d doc values implementation. "
        },
        {
            "id": "comment-15284382",
            "author": "ASF subversion and git services",
            "date": "2016-05-16T10:53:18+0000",
            "content": "Commit 351878223ddbb31722ee00882a214bf252378c65 in lucene-solr's branch refs/heads/master from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=3518782 ]\n\nLUCENE-7212: Add geo3d doc values field "
        },
        {
            "id": "comment-15284384",
            "author": "ASF subversion and git services",
            "date": "2016-05-16T10:56:07+0000",
            "content": "Commit 0e9ec477d76da308e2a2d16e4aab5e42ae5ad820 in lucene-solr's branch refs/heads/branch_6x from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0e9ec47 ]\n\nLUCENE-7212: Add geo3d doc values field "
        },
        {
            "id": "comment-15286553",
            "author": "Karl Wright",
            "date": "2016-05-17T12:53:35+0000",
            "content": "Michael McCandless: Here's a further patch reorganizing Geo3D a bit and adding sort fields for paths and circles. "
        },
        {
            "id": "comment-15287594",
            "author": "ASF subversion and git services",
            "date": "2016-05-17T21:21:01+0000",
            "content": "Commit 07af00d8e7bc4ce2820973e2ab511bfe536654c6 in lucene-solr's branch refs/heads/master from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=07af00d ]\n\nLUCENE-7212: Add Geo3D sorted document fields. "
        },
        {
            "id": "comment-15287617",
            "author": "ASF subversion and git services",
            "date": "2016-05-17T21:30:28+0000",
            "content": "Commit 8a407f0399c6575d6f4bb087f1d9fdc7d112e5d2 in lucene-solr's branch refs/heads/branch_6x from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=8a407f0 ]\n\nLUCENE-7212: Add Geo3D sorted document fields. "
        },
        {
            "id": "comment-15288846",
            "author": "ASF subversion and git services",
            "date": "2016-05-18T11:58:49+0000",
            "content": "Commit 417c37279e91954c105f6d70e0f863cd28bf0682 in lucene-solr's branch refs/heads/master from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=417c372 ]\n\nLUCENE-7212: Add tests for encoding/decoding. "
        },
        {
            "id": "comment-15288849",
            "author": "ASF subversion and git services",
            "date": "2016-05-18T12:00:49+0000",
            "content": "Commit ace032ea2645e3e25f38aa3631c6e2ef0422e801 in lucene-solr's branch refs/heads/branch_6x from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ace032e ]\n\nLUCENE-7212: Add tests for encoding/decoding. "
        },
        {
            "id": "comment-15288884",
            "author": "ASF subversion and git services",
            "date": "2016-05-18T12:35:04+0000",
            "content": "Commit 2a810938bad25438b7c3474e1ad2c5d3500cdb31 in lucene-solr's branch refs/heads/master from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=2a81093 ]\n\nLUCENE-7212: Rename the public method for path sort fields. "
        },
        {
            "id": "comment-15288886",
            "author": "ASF subversion and git services",
            "date": "2016-05-18T12:35:47+0000",
            "content": "Commit a13428e0082912b27ecf0642c30e8d441b2ea8f6 in lucene-solr's branch refs/heads/branch_6x from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a13428e ]\n\nLUCENE-7212: Rename the public method for path sort fields. "
        },
        {
            "id": "comment-15288911",
            "author": "ASF subversion and git services",
            "date": "2016-05-18T13:04:03+0000",
            "content": "Commit cc263413457c826914c3f3c1c26f885330a931aa in lucene-solr's branch refs/heads/master from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=cc26341 ]\n\nLUCENE-7212: Add outside distance classes and methods. "
        },
        {
            "id": "comment-15288918",
            "author": "ASF subversion and git services",
            "date": "2016-05-18T13:06:18+0000",
            "content": "Commit aa339a2f0756a487293c271a673e60e12722d661 in lucene-solr's branch refs/heads/branch_6x from Karl Wright\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=aa339a2 ]\n\nLUCENE-7212: Add outside distance classes and methods. "
        },
        {
            "id": "comment-15289112",
            "author": "Karl Wright",
            "date": "2016-05-18T15:01:32+0000",
            "content": "What's still needed for this ticket:\n\n\n\ttests\n\tgetDistanceBounds() implementation for GeoStandardPath and GeoStandardCircle\n\tperformance tests\n\n "
        }
    ]
}