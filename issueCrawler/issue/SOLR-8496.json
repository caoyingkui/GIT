{
    "id": "SOLR-8496",
    "title": "Facet search count numbers are falsified by older document versions when multi-select is used",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.5",
            "6.0"
        ],
        "affect_versions": "5.4",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "Our setup is based on multiple cores. In One core we have a multi-filed with integer values. and some other unimportant fields. We're using multi-faceting for this field.\n\nWe're querying a test scenario with:\n\n\nhttp://localhost:8983/solr/core-name/select?q=dummyask: (true) AND manufacturer: false AND id: (15039 16882 10850 20781)&fq={!tag=professions}professions: (59)&fl=id&wt=json&indent=true&facet=true&facet.field={!ex=professions}professions\n\n\n\n\n\tQuery: (numDocs:48545, maxDoc:48545)\n\n<response>\n<lst name=\"responseHeader\">\n<int name=\"status\">0</int>\n<int name=\"QTime\">1</int>\n</lst>\n<result name=\"response\" numFound=\"4\" start=\"0\">\n<doc>\n<int name=\"id\">10850</int>\n</doc>\n<doc>\n<int name=\"id\">16882</int>\n</doc>\n<doc>\n<int name=\"id\">15039</int>\n</doc>\n<doc>\n<int name=\"id\">20781</int>\n</doc>\n</result>\n<lst name=\"facet_counts\">\n<lst name=\"facet_queries\"/>\n<lst name=\"facet_fields\">\n<lst name=\"professions\">\n<int name=\"59\">4</int>\n</lst>\n</lst>\n<lst name=\"facet_dates\"/>\n<lst name=\"facet_ranges\"/>\n<lst name=\"facet_intervals\"/>\n<lst name=\"facet_heatmaps\"/>\n</lst>\n</response>\n\n\n\tThen we update one document and change some fields (numDocs:48545, maxDoc:48546) The number of maxDocs is increased\n\n<response>\n<lst name=\"responseHeader\">\n<int name=\"status\">0</int>\n<int name=\"QTime\">1</int>\n</lst>\n<result name=\"response\" numFound=\"4\" start=\"0\">\n<doc>\n<int name=\"id\">10850</int>\n</doc>\n<doc>\n<int name=\"id\">16882</int>\n</doc>\n<doc>\n<int name=\"id\">15039</int>\n</doc>\n<doc>\n<int name=\"id\">20781</int>\n</doc>\n</result>\n<lst name=\"facet_counts\">\n<lst name=\"facet_queries\"/>\n<lst name=\"facet_fields\">\n<lst name=\"professions\">\n<int name=\"59\">5</int>\n</lst>\n</lst>\n<lst name=\"facet_dates\"/>\n<lst name=\"facet_ranges\"/>\n<lst name=\"facet_intervals\"/>\n<lst name=\"facet_heatmaps\"/>\n</lst>\n</response>\n\n\n\n\n\nThe Problem:\nIn the first query, we're getting a facet count of 4, which is correct. After updating one document, we're getting 5 as a result wich is not correct.",
    "attachments": {
        "SOLR-8496.patch": "https://issues.apache.org/jira/secure/attachment/12782726/SOLR-8496.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-01-13T15:44:52+0000",
            "author": "Shawn Heisey",
            "content": "This should really be handled on the mailing list.  I see that you asked on IRC, but you had already been gone from the IRC channel for 45 minutes before I got reconnected to my IRC session (at 8:45 AM in my timezone).\n\nWhat exactly does \"based on multiple cores\" mean? ",
            "id": "comment-15096369"
        },
        {
            "date": "2016-01-14T14:33:21+0000",
            "author": "Shawn Heisey",
            "content": "SOLR-8540 is the same problem.  They said they tried docValues and it did not fix the problem.\n\nRelated to your attempts to replicate the problem with simple tests ... if all of the documents in a segment are deleted, the entire segment will be deleted, and the problem will disappear.  You'll need to run a test where you index several documents at once (so they end up in the same segment), then replace only some of those documents.  The segment will continue to exist as long as at least one document is not replaced. ",
            "id": "comment-15098172"
        },
        {
            "date": "2016-01-14T16:12:51+0000",
            "author": "Shawn Heisey",
            "content": "One workaround is optimizing the index, but this is not a good workaround for most people, especially when changes are small and very frequent. ",
            "id": "comment-15098324"
        },
        {
            "date": "2016-01-14T16:19:38+0000",
            "author": "Shawn Heisey",
            "content": "Summary of the fundamental symptoms for a committer or contributor that knows the facet code:  In recent versions, facets are no longer excluding deleted documents from the counts.  It looks like this is the case whether facets are using docValues or not. ",
            "id": "comment-15098335"
        },
        {
            "date": "2016-01-14T17:29:12+0000",
            "author": "Hoss Man",
            "content": "can we get some more details about your configs/schema? ... i'm trying to figure out enough details to be able to reproduce this.\n\nUsing a trivial test with the techproducts example, i can't seem to reproduce...\n\n\nhossman@tray:~/lucene/5x_dev/solr$ bin/solr -e techproducts\n...\nhossman@tray:~/lucene/5x_dev/solr$ curl 'http://localhost:8983/solr/techproducts/query?facet=true&facet.field=inStock&q=solr&omitHeader=true&rows=0'\n{\n  \"response\":{\"numFound\":1,\"start\":0,\"docs\":[]\n  },\n  \"facet_counts\":{\n    \"facet_queries\":{},\n    \"facet_fields\":{\n      \"inStock\":[\n        \"true\",1,\n        \"false\",0]},\n    \"facet_dates\":{},\n    \"facet_ranges\":{},\n    \"facet_intervals\":{},\n    \"facet_heatmaps\":{}}}\n...\nhossman@tray:~/lucene/5x_dev/solr$ bin/post -c techproducts example/exampledocs/solr.xml \n...\nhossman@tray:~/lucene/5x_dev/solr$ curl 'http://localhost:8983/solr/techproducts/query?facet=true&facet.field=inStock&q=solr&omitHeader=true&rows=0'\n{\n  \"response\":{\"numFound\":1,\"start\":0,\"docs\":[]\n  },\n  \"facet_counts\":{\n    \"facet_queries\":{},\n    \"facet_fields\":{\n      \"inStock\":[\n        \"true\",1,\n        \"false\",0]},\n    \"facet_dates\":{},\n    \"facet_ranges\":{},\n    \"facet_intervals\":{},\n    \"facet_heatmaps\":{}}}\nhossman@tray:~/lucene/5x_dev/solr$ curl -sS 'http://localhost:8983/solr/techproducts/admin/luke?wt=json&indent=true' | egrep \"maxDoc|numDoc\"\n    \"numDocs\":32,\n    \"maxDoc\":33,\n\n ",
            "id": "comment-15098463"
        },
        {
            "date": "2016-01-14T20:22:30+0000",
            "author": "Erick Erickson",
            "content": "I couldn't reproduce with a test case either in a JUnit test (non Cloud, one core). ",
            "id": "comment-15098779"
        },
        {
            "date": "2016-01-14T21:36:38+0000",
            "author": "Hoss Man",
            "content": "Also: does this reproduce for you when indexing from scratch, or is this an index you originally built with an older version of Solr and then upgraded to 5.4? (trying to figure out if there are older segments and maybe the bug is specific to 5.4 reading deleted docs from those older segments)\n\ncan you also run CheckIndex (command line) and provide all of that output? ",
            "id": "comment-15098922"
        },
        {
            "date": "2016-01-15T12:28:34+0000",
            "author": "Vasiliy Bout",
            "content": "You do not use multi-select faceting in your simple test. Multi select local parameters are necessary to reproduce this issue. You can see SOLR-8540, there is a complete description of when this issue occurs. ",
            "id": "comment-15101708"
        },
        {
            "date": "2016-01-15T13:02:37+0000",
            "author": "Vasiliy Bout",
            "content": "When I tried to reproduce this issue on a new empty test core, I did the following:\n\n1. Fill the core with a number of documents\n2. Overwrite some documents in the core, i.e. add new documents with the same id as were added before.\n\nAfter that you can see, that in \"Schema Browser\" when you select your field and press \"Load Term Info\" counts for your field are incorrect (they take into account also old versions of the overwritten documents). And you can see that normal faceting gives correct results but multi select faceting gives incorrect results (the same you saw in \"Load Term Info\" counts). ",
            "id": "comment-15101737"
        },
        {
            "date": "2016-01-15T14:17:41+0000",
            "author": "Andreas M\u00fcller",
            "content": "We did a complete new index from scratch. There 48545 docs in the index. The effect only occurred if there are 10k docs in the index. In the following our solr configuration and scheme and the output of CacheIndex\n\nsolrconfig.xml\n<config>\n  <luceneMatchVersion>4.5</luceneMatchVersion>\n  <!--  The DirectoryFactory to use for indexes.\n        solr.StandardDirectoryFactory, the default, is filesystem based.\n        solr.RAMDirectoryFactory is memory based, not persistent, and doesn't work with replication. -->\n  <directoryFactory name=\"DirectoryFactory\" class=\"${solr.directoryFactory:solr.StandardDirectoryFactory}\"/>\n\n  <updateHandler class=\"solr.DirectUpdateHandler2\">\n   <autoSoftCommit>\n        <maxTime>1000</maxTime>\n    </autoSoftCommit>\n    <autoCommit>\n        <maxTime>60000</maxTime> \n        <openSearcher>false</openSearcher>\n    </autoCommit>\n  </updateHandler>\n\n\n  <requestDispatcher handleSelect=\"true\" >\n    <requestParsers enableRemoteStreaming=\"false\" multipartUploadLimitInKB=\"2048\" />\n  </requestDispatcher>\n  \n  <requestHandler name=\"standard\" class=\"solr.StandardRequestHandler\" default=\"true\" />\n  <requestHandler name=\"/update\" class=\"solr.UpdateRequestHandler\" />\n  <requestHandler name=\"/admin/\" class=\"org.apache.solr.handler.admin.AdminHandlers\" />\n      \n  <!-- config for the admin interface --> \n  <admin>\n    <defaultQuery>solr</defaultQuery>\n  </admin>\n\n</config>\n\n\nschema.xml\n<schema name=\"company comptest3\" version=\"1.1\">\n\n    <types>\n        <fieldType name=\"string\" class=\"solr.StrField\" sortMissingLast=\"true\" omitNorms=\"true\"/>\n\n        <!-- boolean type: \"true\" or \"false\" -->\n        <fieldType name=\"boolean\" class=\"solr.BoolField\" sortMissingLast=\"true\" omitNorms=\"true\"/>\n\n        <!-- Default numeric field types. For faster range queries, consider the tint/tfloat/tlong/tdouble types. -->\n        <fieldType name=\"int\" class=\"solr.TrieIntField\" precisionStep=\"0\" omitNorms=\"true\" positionIncrementGap=\"0\"/>\n        <fieldType name=\"date\" class=\"solr.TrieDateField\" omitNorms=\"true\" precisionStep=\"0\" positionIncrementGap=\"0\"/>\n        <fieldType name=\"long\" class=\"solr.TrieLongField\" precisionStep=\"0\" omitNorms=\"true\" positionIncrementGap=\"0\"/>\n\n        <!-- lat long fields -->\n        <fieldType name=\"double\" class=\"solr.TrieDoubleField\" precisionStep=\"0\" omitNorms=\"true\" positionIncrementGap=\"0\"/>\n\n        <!-- A Trie based date field for faster date range queries and date faceting. -->\n        <fieldType name=\"tdate\" class=\"solr.TrieDateField\" omitNorms=\"true\" precisionStep=\"6\" positionIncrementGap=\"0\"/>\n\n        <!-- A text field that only splits on whitespace for exact matching of words -->\n        <fieldType name=\"text_ws\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n            <analyzer>\n                <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n            </analyzer>\n        </fieldType>\n\n        <fieldType name=\"text\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n            <analyzer type=\"index\">\n                <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n                <filter class=\"solr.WordDelimiterFilterFactory\" generateWordParts=\"1\" generateNumberParts=\"1\" catenateWords=\"1\" catenateNumbers=\"1\" catenateAll=\"0\" splitOnCaseChange=\"1\"/>\n                <filter class=\"solr.LowerCaseFilterFactory\"/>\n            </analyzer>\n            <analyzer type=\"query\">\n                <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n                <filter class=\"solr.WordDelimiterFilterFactory\" generateWordParts=\"1\" generateNumberParts=\"1\" catenateWords=\"0\" catenateNumbers=\"0\" catenateAll=\"0\" splitOnCaseChange=\"1\"/>\n                <filter class=\"solr.LowerCaseFilterFactory\"/>\n            </analyzer>\n        </fieldType>\n\n        <fieldType name=\"text_rev\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n\n            <analyzer type=\"index\">\n                <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n                <filter class=\"solr.WordDelimiterFilterFactory\" generateWordParts=\"1\" generateNumberParts=\"1\" catenateWords=\"1\" catenateNumbers=\"1\" catenateAll=\"0\" splitOnCaseChange=\"0\"/>\n                <filter class=\"solr.LowerCaseFilterFactory\"/>\n                <filter class=\"solr.ReversedWildcardFilterFactory\" withOriginal=\"true\" maxPosAsterisk=\"3\" maxPosQuestion=\"2\" maxFractionAsterisk=\"0.33\"/>\n            </analyzer>\n            <analyzer type=\"query\">\n                <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n                <filter class=\"solr.WordDelimiterFilterFactory\" generateWordParts=\"1\" generateNumberParts=\"1\" catenateWords=\"0\" catenateNumbers=\"0\" catenateAll=\"0\" splitOnCaseChange=\"0\"/>\n                <filter class=\"solr.LowerCaseFilterFactory\"/>\n            </analyzer>\n\n        </fieldType>\n\n        <fieldtype name=\"phonetic\" stored=\"true\" indexed=\"true\" class=\"solr.TextField\" >\n            <analyzer>\n                <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n                <filter class=\"solr.DoubleMetaphoneFilterFactory\" inject=\"false\"/>\n            </analyzer>\n        </fieldtype>\n\n        <!-- lowercases the entire field value, keeping it as a single token.   -->\n        <fieldType name=\"lowercase\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n            <analyzer>\n                <tokenizer class=\"solr.KeywordTokenizerFactory\"/>\n                <filter class=\"solr.LowerCaseFilterFactory\" />\n            </analyzer>\n        </fieldType>\n\n        <fieldType name=\"location\" class=\"solr.LatLonType\" subFieldSuffix=\"_coordinate\"/>\n\n    </types>\n\n    <fields>\n        <!-- general -->\n        <field name=\"id\"                    type=\"int\"           indexed=\"true\" stored=\"true\" multiValued=\"false\" required=\"true\"/>\n        <field name=\"dummyask\"              type=\"boolean\"       indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n        <field name=\"disabled\"              type=\"boolean\"       indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n        <field name=\"closed\"                type=\"boolean\"       indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n        <field name=\"show\"                  type=\"boolean\"       indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n        <field name=\"pagecalls\"             type=\"int\"           indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n        <field name=\"publicated\"            type=\"tdate\"         indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n\n        <field name=\"name\"                  type=\"text_rev\"      indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n        <field name=\"name_filtered\"         type=\"lowercase\"     indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n        <field name=\"name_phonetic\"         type=\"lowercase\"     indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n        <field name=\"manufacturer\"          type=\"boolean\"       indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n        <field name=\"fulltext\"              type=\"text_rev\"      indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n        <field name=\"owner\"                 type=\"text_rev\"      indexed=\"true\" stored=\"true\" multiValued=\"true\" />\n        <field name=\"member\"                type=\"boolean\"       indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n        <field name=\"professions\"           type=\"long\"          indexed=\"true\" stored=\"true\" multiValued=\"true\" />\n        <field name=\"founding\"              type=\"tdate\"         indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n        <field name=\"employee_number\"       type=\"int\"           indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n        <field name=\"jobs\"                  type=\"boolean\"       indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n        <field name=\"image\"                 type=\"text\"          indexed=\"false\" stored=\"true\" multiValued=\"false\" />\n\n        <!-- geografic options -->\n        <field name=\"ort\"                   type=\"lowercase\"     indexed=\"true\" stored=\"true\" multiValued=\"true\" />\n        <field name=\"plz\"                   type=\"lowercase\"     indexed=\"true\" stored=\"true\" multiValued=\"true\" />\n        <field name=\"land\"                  type=\"lowercase\"     indexed=\"true\" stored=\"true\" multiValued=\"true\" />\n        <field name=\"bundesland\"            type=\"lowercase\"     indexed=\"true\" stored=\"true\" multiValued=\"true\" />\n        <field name=\"lat\"                   type=\"double\"        indexed=\"true\" stored=\"true\" multiValued=\"true\" />\n        <field name=\"lon\"                   type=\"double\"        indexed=\"true\" stored=\"true\" multiValued=\"true\" />\n        <field name=\"geo\"                   type=\"location\"      indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n        <field name=\"geo_0_coordinate\"      type=\"double\"        indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n        <field name=\"geo_1_coordinate\"      type=\"double\"        indexed=\"true\" stored=\"true\" multiValued=\"false\" />\n\n        <!-- display fields -->\n        <field name=\"profession_display\"    type=\"text\"          indexed=\"false\" stored=\"true\" multiValued=\"true\" />\n        <field name=\"address_display\"       type=\"text_rev\"      indexed=\"true\"  stored=\"true\" multiValued=\"true\" />\n\n        <!-- realized projects -->\n        <field name=\"done_projects\"         type=\"lowercase\"     indexed=\"true\"  stored=\"true\" multiValued=\"true\"/>\n\n        <!-- projects in planing / projects in construction -->\n        <field name=\"projects\"              type=\"long\"          indexed=\"true\"  stored=\"true\" multiValued=\"true\"/>\n\n        <!-- references -->\n        <field name=\"references\"            type=\"lowercase\"     indexed=\"true\" stored=\"true\" multiValued=\"true\"/>\n\n        <field name=\"reference_info\"        type=\"text\"          indexed=\"false\" stored=\"true\" multiValued=\"false\"/>\n        <field name=\"relevance\"             type=\"int\"           indexed=\"true\" stored=\"true\" multiValued=\"false\"/>\n\n        <field name=\"_version_\"             type=\"long\"          indexed=\"true\" stored=\"true\"/>\n\n    </fields>\n\n    <!-- field to use to determine and enforce document uniqueness. -->\n    <uniqueKey>id</uniqueKey>\n\n    <!-- field for the QueryParser to use when an explicit fieldname is absent -->\n    <defaultSearchField>name</defaultSearchField>\n\n    <!-- SolrQueryParser configuration: defaultOperator=\"AND|OR\" -->\n    <solrQueryParser defaultOperator=\"OR\"/>\n</schema>\n\n\n\njava -cp ../server/solr-webapp/webapp/WEB-INF/lib/lucene-core-5.4.0.jar -ea:org.apache.lucene... org.apache.lucene.index.CheckIndex ../server/solr/companies/data/index\nOpening index @ ../server/solr/companies/data/index\n\nSegments file=segments_4t numSegments=10 version=5.4.0 id=8b82erk4sdq7dvuluswzgthh5 format= userData={commitTimeMSec=1452862011769}\n  1 of 10: name=_3fw maxDoc=44624\n    version=5.4.0\n    id=8b82erk4sdq7dvuluswzgth5m\n    codec=Lucene54\n    compound=false\n    numFiles=11\n    size (MB)=54.18\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_66-internal, java.vm.version=25.66-b17, lucene.version=5.4.0, mergeMaxNumSegments=-1, os.arch=amd64, java.runtime.version=1.8.0_66-internal-b17, source=merge, mergeFactor=10, os.version=3.11-2-amd64, timestamp=1452857890899}\n    has deletions [delGen=5]\n    test: open reader.........OK [took 1.244 sec]\n    test: check integrity.....OK [took 0.148 sec]\n    test: check live docs.....OK [500 deleted docs] [took 0.011 sec]\n    test: field infos.........OK [35 fields] [took 0.001 sec]\n    test: field norms.........OK [12 fields] [took 0.048 sec]\n    test: terms, freq, prox...OK [1119766 terms; 5522980 terms/docs pairs; 6617236 tokens] [took 5.664 sec]\n    test: stored fields.......OK [1786969 total field count; avg 40.5 fields per doc] [took 2.215 sec]\n    test: term vectors........OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.001 sec]\n    test: docvalues...........OK [0 docvalues fields; 0 BINARY; 0 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.001 sec]\n\n  2 of 10: name=_3pc maxDoc=1476\n    version=5.4.0\n    id=8b82erk4sdq7dvuluswzgthf8\n    codec=Lucene54\n    compound=true\n    numFiles=3\n    size (MB)=1.988\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_66-internal, java.vm.version=25.66-b17, lucene.version=5.4.0, mergeMaxNumSegments=-1, os.arch=amd64, java.runtime.version=1.8.0_66-internal-b17, source=merge, mergeFactor=10, os.version=3.11-2-amd64, timestamp=1452861829493}\n    no deletions\n    test: open reader.........OK [took 0.034 sec]\n    test: check integrity.....OK [took 0.006 sec]\n    test: check live docs.....OK [took 0.000 sec]\n    test: field infos.........OK [35 fields] [took 0.000 sec]\n    test: field norms.........OK [12 fields] [took 0.001 sec]\n    test: terms, freq, prox...OK [67708 terms; 174468 terms/docs pairs; 204435 tokens] [took 0.938 sec]\n    test: stored fields.......OK [59440 total field count; avg 40.3 fields per doc] [took 0.052 sec]\n    test: term vectors........OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.000 sec]\n    test: docvalues...........OK [0 docvalues fields; 0 BINARY; 0 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.001 sec]\n\n  3 of 10: name=_3pw maxDoc=1426\n    version=5.4.0\n    id=8b82erk4sdq7dvuluswzgthfs\n    codec=Lucene54\n    compound=true\n    numFiles=3\n    size (MB)=2.08\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_66-internal, java.vm.version=25.66-b17, lucene.version=5.4.0, mergeMaxNumSegments=-1, os.arch=amd64, java.runtime.version=1.8.0_66-internal-b17, source=merge, mergeFactor=10, os.version=3.11-2-amd64, timestamp=1452861864304}\n    no deletions\n    test: open reader.........OK [took 0.019 sec]\n    test: check integrity.....OK [took 0.015 sec]\n    test: check live docs.....OK [took 0.000 sec]\n    test: field infos.........OK [35 fields] [took 0.000 sec]\n    test: field norms.........OK [12 fields] [took 0.001 sec]\n    test: terms, freq, prox...OK [67794 terms; 175792 terms/docs pairs; 216683 tokens] [took 0.836 sec]\n    test: stored fields.......OK [62036 total field count; avg 43.5 fields per doc] [took 0.056 sec]\n    test: term vectors........OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.000 sec]\n    test: docvalues...........OK [0 docvalues fields; 0 BINARY; 0 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.000 sec]\n\n  4 of 10: name=_3pm maxDoc=1398\n    version=5.4.0\n    id=8b82erk4sdq7dvuluswzgthfi\n    codec=Lucene54\n    compound=true\n    numFiles=3\n    size (MB)=2.035\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_66-internal, java.vm.version=25.66-b17, lucene.version=5.4.0, mergeMaxNumSegments=-1, os.arch=amd64, java.runtime.version=1.8.0_66-internal-b17, source=merge, mergeFactor=10, os.version=3.11-2-amd64, timestamp=1452861844413}\n    no deletions\n    test: open reader.........OK [took 0.016 sec]\n    test: check integrity.....OK [took 0.017 sec]\n    test: check live docs.....OK [took 0.000 sec]\n    test: field infos.........OK [35 fields] [took 0.000 sec]\n    test: field norms.........OK [12 fields] [took 0.001 sec]\n    test: terms, freq, prox...OK [67878 terms; 173372 terms/docs pairs; 213758 tokens] [took 0.162 sec]\n    test: stored fields.......OK [59498 total field count; avg 42.6 fields per doc] [took 0.048 sec]\n    test: term vectors........OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.000 sec]\n    test: docvalues...........OK [0 docvalues fields; 0 BINARY; 0 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.000 sec]\n\n  5 of 10: name=_3r1 maxDoc=114\n    version=5.4.0\n    id=8b82erk4sdq7dvuluswzgthgy\n    codec=Lucene54\n    compound=true\n    numFiles=3\n    size (MB)=0.658\n    diagnostics = {os=Linux, java.vendor=Oracle Corporation, java.version=1.8.0_66-internal, java.vm.version=25.66-b17, lucene.version=5.4.0, mergeMaxNumSegments=-1, os.arch=amd64, java.runtime.version=1.8.0_66-internal-b17, source=merge, mergeFactor=10, os.version=3.11-2-amd64, timestamp=1452861925974}\n    no deletions\n    test: open reader.........OK [took 0.008 sec]\n    test: check integrity.....OK [took 0.002 sec]\n    test: check live docs.....OK [took 0.000 sec]\n    test: field infos.........OK [35 fields] [took 0.000 sec]\n    test: field norms.........OK [12 fields] [took 0.000 sec]\n    test: terms, freq, prox...OK [18002 terms; 41857 terms/docs pairs; 64375 tokens] [took 0.061 sec]\n    test: stored fields.......OK [14505 total field count; avg 127.2 fields per doc] [took 0.018 sec]\n    test: term vectors........OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.000 sec]\n    test: docvalues...........OK [0 docvalues fields; 0 BINARY; 0 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.000 sec]\n\n  6 of 10: name=_3r2 maxDoc=1\n    version=5.4.0\n    id=8b82erk4sdq7dvuluswzgthgz\n    codec=Lucene54\n    compound=false\n    numFiles=10\n    size (MB)=0.026\n    diagnostics = {java.runtime.version=1.8.0_66-internal-b17, java.vendor=Oracle Corporation, java.version=1.8.0_66-internal, java.vm.version=25.66-b17, lucene.version=5.4.0, os=Linux, os.arch=amd64, os.version=3.11-2-amd64, source=flush, timestamp=1452861930569}\n    no deletions\n    test: open reader.........OK [took 0.017 sec]\n    test: check integrity.....OK [took 0.007 sec]\n    test: check live docs.....OK [took 0.000 sec]\n    test: field infos.........OK [34 fields] [took 0.000 sec]\n    test: field norms.........OK [12 fields] [took 0.000 sec]\n    test: terms, freq, prox...OK [809 terms; 809 terms/docs pairs; 1374 tokens] [took 0.010 sec]\n    test: stored fields.......OK [324 total field count; avg 324.0 fields per doc] [took 0.001 sec]\n    test: term vectors........OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.000 sec]\n    test: docvalues...........OK [0 docvalues fields; 0 BINARY; 0 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.001 sec]\n\n  7 of 10: name=_3r3 maxDoc=1\n    version=5.4.0\n    id=8b82erk4sdq7dvuluswzgthh0\n    codec=Lucene54\n    compound=false\n    numFiles=10\n    size (MB)=0.046\n    diagnostics = {java.runtime.version=1.8.0_66-internal-b17, java.vendor=Oracle Corporation, java.version=1.8.0_66-internal, java.vm.version=25.66-b17, lucene.version=5.4.0, os=Linux, os.arch=amd64, os.version=3.11-2-amd64, source=flush, timestamp=1452861931845}\n    no deletions\n    test: open reader.........OK [took 0.022 sec]\n    test: check integrity.....OK [took 0.000 sec]\n    test: check live docs.....OK [took 0.000 sec]\n    test: field infos.........OK [35 fields] [took 0.000 sec]\n    test: field norms.........OK [12 fields] [took 0.000 sec]\n    test: terms, freq, prox...OK [1611 terms; 1611 terms/docs pairs; 2890 tokens] [took 0.008 sec]\n    test: stored fields.......OK [805 total field count; avg 805.0 fields per doc] [took 0.001 sec]\n    test: term vectors........OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.000 sec]\n    test: docvalues...........OK [0 docvalues fields; 0 BINARY; 0 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.000 sec]\n\n  8 of 10: name=_3r4 maxDoc=2\n    version=5.4.0\n    id=8b82erk4sdq7dvuluswzgthh1\n    codec=Lucene54\n    compound=false\n    numFiles=10\n    size (MB)=0.097\n    diagnostics = {java.runtime.version=1.8.0_66-internal-b17, java.vendor=Oracle Corporation, java.version=1.8.0_66-internal, java.vm.version=25.66-b17, lucene.version=5.4.0, os=Linux, os.arch=amd64, os.version=3.11-2-amd64, source=flush, timestamp=1452861933112}\n    no deletions\n    test: open reader.........OK [took 0.024 sec]\n    test: check integrity.....OK [took 0.001 sec]\n    test: check live docs.....OK [took 0.000 sec]\n    test: field infos.........OK [35 fields] [took 0.000 sec]\n    test: field norms.........OK [12 fields] [took 0.000 sec]\n    test: terms, freq, prox...OK [3333 terms; 3742 terms/docs pairs; 8204 tokens] [took 0.010 sec]\n    test: stored fields.......OK [1176 total field count; avg 588.0 fields per doc] [took 0.005 sec]\n    test: term vectors........OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.000 sec]\n    test: docvalues...........OK [0 docvalues fields; 0 BINARY; 0 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.000 sec]\n\n  9 of 10: name=_3r5 maxDoc=2\n    version=5.4.0\n    id=8b82erk4sdq7dvuluswzgthh2\n    codec=Lucene54\n    compound=false\n    numFiles=10\n    size (MB)=0.07\n    diagnostics = {java.runtime.version=1.8.0_66-internal-b17, java.vendor=Oracle Corporation, java.version=1.8.0_66-internal, java.vm.version=25.66-b17, lucene.version=5.4.0, os=Linux, os.arch=amd64, os.version=3.11-2-amd64, source=flush, timestamp=1452861935365}\n    no deletions\n    test: open reader.........OK [took 0.010 sec]\n    test: check integrity.....OK [took 0.001 sec]\n    test: check live docs.....OK [took 0.000 sec]\n    test: field infos.........OK [35 fields] [took 0.000 sec]\n    test: field norms.........OK [12 fields] [took 0.001 sec]\n    test: terms, freq, prox...OK [2346 terms; 2583 terms/docs pairs; 4660 tokens] [took 0.010 sec]\n    test: stored fields.......OK [1051 total field count; avg 525.5 fields per doc] [took 0.002 sec]\n    test: term vectors........OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.000 sec]\n    test: docvalues...........OK [0 docvalues fields; 0 BINARY; 0 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.000 sec]\n\n  10 of 10: name=_3r6 maxDoc=1\n    version=5.4.0\n    id=8b82erk4sdq7dvuluswzgthh4\n    codec=Lucene54\n    compound=false\n    numFiles=10\n    size (MB)=0.073\n    diagnostics = {java.runtime.version=1.8.0_66-internal-b17, java.vendor=Oracle Corporation, java.version=1.8.0_66-internal, java.vm.version=25.66-b17, lucene.version=5.4.0, os=Linux, os.arch=amd64, os.version=3.11-2-amd64, source=flush, timestamp=1452861952782}\n    no deletions\n    test: open reader.........OK [took 0.008 sec]\n    test: check integrity.....OK [took 0.001 sec]\n    test: check live docs.....OK [took 0.000 sec]\n    test: field infos.........OK [35 fields] [took 0.000 sec]\n    test: field norms.........OK [12 fields] [took 0.000 sec]\n    test: terms, freq, prox...OK [2581 terms; 2581 terms/docs pairs; 5101 tokens] [took 0.011 sec]\n    test: stored fields.......OK [1241 total field count; avg 1241.0 fields per doc] [took 0.001 sec]\n    test: term vectors........OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.003 sec]\n    test: docvalues...........OK [0 docvalues fields; 0 BINARY; 0 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET] [took 0.000 sec]\n\nNo problems were detected with this index.\n\n ",
            "id": "comment-15101815"
        },
        {
            "date": "2016-01-15T15:25:32+0000",
            "author": "Shawn Heisey",
            "content": "My apologies, Hoss Man.  My summary of the issue was incomplete and did not mention multi-select, which it should have. ",
            "id": "comment-15101910"
        },
        {
            "date": "2016-01-15T16:10:37+0000",
            "author": "Yonik Seeley",
            "content": "I can confirm this bug. Looking into it... ",
            "id": "comment-15101985"
        },
        {
            "date": "2016-01-15T16:22:03+0000",
            "author": "Vasiliy Bout",
            "content": "I developed a small example on how to reproduce this problem with the completely new core with a very simple schema and about 20 documents in the core.\n\nFirst of all, I created a new core with the following schema.xml:\n\n<?xml version=\"1.0\" ?>\n<schema name=\"basic\" version=\"1.1\">\n    <types>\n        <fieldType name=\"string\" class=\"solr.StrField\" omitNorms=\"true\" indexed=\"true\" stored=\"true\"/>\n        <fieldType name=\"int\" class=\"solr.TrieIntField\" precisionStep=\"0\" positionIncrementGap=\"0\" indexed=\"true\" stored=\"true\"/>\n    </types>\n    <fields>\n        <field name=\"id\" type=\"string\" required=\"true\"/>\n        <field name=\"foo_s\" type=\"string\"/>\n        <field name=\"bar_s\" type=\"string\" docValues=\"true\"/>\n        <field name=\"foo_i\" type=\"int\"/>\n        <field name=\"bar_i\" type=\"int\" docValues=\"true\"/>\n    </fields>\n    <uniqueKey>id</uniqueKey>\n    <solrQueryParser defaultOperator=\"OR\"/>\n</schema>\n\n\n\nAfter that, I generated a set of documents to fill the core with. I launched python interpreter in the terminal and typed the following oneliner:\n\n[ {\"id\":i,\"foo_i\":i,\"bar_i\":i,\"foo_s\":i,\"bar_s\":i} for i in range(1, 21) ]\n\n\n\nIt gave me a set of 20 documents. This is the same set but slightly formatted to be human readable:\n\n[\n    {'bar_s': 1, 'foo_i': 1, 'bar_i': 1, 'foo_s': 1, 'id': 1},\n    {'bar_s': 2, 'foo_i': 2, 'bar_i': 2, 'foo_s': 2, 'id': 2},\n    {'bar_s': 3, 'foo_i': 3, 'bar_i': 3, 'foo_s': 3, 'id': 3},\n    {'bar_s': 4, 'foo_i': 4, 'bar_i': 4, 'foo_s': 4, 'id': 4},\n    {'bar_s': 5, 'foo_i': 5, 'bar_i': 5, 'foo_s': 5, 'id': 5},\n    {'bar_s': 6, 'foo_i': 6, 'bar_i': 6, 'foo_s': 6, 'id': 6},\n    {'bar_s': 7, 'foo_i': 7, 'bar_i': 7, 'foo_s': 7, 'id': 7},\n    {'bar_s': 8, 'foo_i': 8, 'bar_i': 8, 'foo_s': 8, 'id': 8},\n    {'bar_s': 9, 'foo_i': 9, 'bar_i': 9, 'foo_s': 9, 'id': 9},\n    {'bar_s': 10, 'foo_i': 10, 'bar_i': 10, 'foo_s': 10, 'id': 10},\n    {'bar_s': 11, 'foo_i': 11, 'bar_i': 11, 'foo_s': 11, 'id': 11},\n    {'bar_s': 12, 'foo_i': 12, 'bar_i': 12, 'foo_s': 12, 'id': 12},\n    {'bar_s': 13, 'foo_i': 13, 'bar_i': 13, 'foo_s': 13, 'id': 13},\n    {'bar_s': 14, 'foo_i': 14, 'bar_i': 14, 'foo_s': 14, 'id': 14},\n    {'bar_s': 15, 'foo_i': 15, 'bar_i': 15, 'foo_s': 15, 'id': 15},\n    {'bar_s': 16, 'foo_i': 16, 'bar_i': 16, 'foo_s': 16, 'id': 16},\n    {'bar_s': 17, 'foo_i': 17, 'bar_i': 17, 'foo_s': 17, 'id': 17},\n    {'bar_s': 18, 'foo_i': 18, 'bar_i': 18, 'foo_s': 18, 'id': 18},\n    {'bar_s': 19, 'foo_i': 19, 'bar_i': 19, 'foo_s': 19, 'id': 19},\n    {'bar_s': 20, 'foo_i': 20, 'bar_i': 20, 'foo_s': 20, 'id': 20}\n]\n\n\n\nAfter that I opened Solr Admin page in my browser, went to the \"Documents\" tab of my core and filled the core with the set of documents above. I selected the following parameters:\n\n\tRequest-Handler (qt): /update/json;\n\tDocument Type: Solr Command (raw XML or JSON);\n\tDocuments set to the above JSON generate in python interpreter.\n\n\n\nAfter the Solr core is filled with documents, I add a single document once again, so this document overwrites the previous one:\n\n{'bar_s': 2, 'foo_i': 2, 'bar_i': 2, 'foo_s': 2, 'id': 2}\n\n\n\nNow when I look at the \"Overview\" tab I see the following statistics:\n\nLast Modified: less than a minute ago\nNum Docs: 20\nMax Doc: 21\nHeap Memory Usage: -1\nDeleted Docs: 1\nVersion: 7\nSegment Count: 2\n\n\n\nAnd at this stage all multi select facet queries give incorrect results. Since all the documents in the core have unique values for all fields, all facet queries should give count 1 for all values for all fields. Simple facet queries return correct results:\n\nquery is q=*:*&rows=0&facet=true&facet.limit=1&facet.field=foo_s&facet.field=foo_i&facet.field=bar_s&facet.field=bar_i\nresponse is\n\n{\n  \"responseHeader\":{\"status\":0,\"QTime\":1},\n  \"response\":{\"numFound\":20,\"start\":0,\"docs\":[]},\n  \"facet_counts\":{\n    \"facet_queries\":{},\n    \"facet_fields\":{\n      \"foo_s\":[\"1\",1],\n      \"foo_i\":[\"1\",1],\n      \"bar_s\":[\"1\",1],\n      \"bar_i\":[\"1\",1]\n    },\n    \"facet_dates\":{},\n    \"facet_ranges\":{},\n    \"facet_intervals\":{},\n    \"facet_heatmaps\":{}}}\n\n\n\nAnd this is what we get for multi select facet query:\n\nquery is q=*:*&fq={!tag=a}id:*&rows=0&facet=true&facet.limit=1&facet.field={!ex=a}foo_s&facet.field={!ex=a}foo_i&facet.field={!ex=a}bar_s&facet.field={!ex=a}bar_i\nresponse is\n\n{\n  \"responseHeader\":{\"status\":0,\"QTime\":2},\n  \"response\":{\"numFound\":20,\"start\":0,\"docs\":[]},\n  \"facet_counts\":{\n    \"facet_queries\":{},\n    \"facet_fields\":{\n      \"foo_s\":[\"2\",2],\n      \"foo_i\":[\"2\",2],\n      \"bar_s\":[\"2\",2],\n      \"bar_i\":[\"2\",2]},\n    \"facet_dates\":{},\n    \"facet_ranges\":{},\n    \"facet_intervals\":{},\n    \"facet_heatmaps\":{}}}\n\n\n\nSo we get count 2 for value \"2\", i.e. replaced (old) version of the document with id=2 is taken into account when using multi selection facets. ",
            "id": "comment-15102003"
        },
        {
            "date": "2016-01-15T18:30:38+0000",
            "author": "Hoss Man",
            "content": "...My summary of the issue was incomplete and did not mention multi-select, which it should have.\n\nInteresting \u2013 i noticed the taged/excluded filters in the original example and definitely tried that when i was trying to reproduce, but I didn't see any change in the results so i didn't include it in my \"can't reproduce\" example ... i must have either made a mistake somewhere, or tickled an excluded filter code path that doesn't have this bug.\n ",
            "id": "comment-15102257"
        },
        {
            "date": "2016-01-16T15:22:31+0000",
            "author": "Yonik Seeley",
            "content": "Patch attached, running complete tests now. ",
            "id": "comment-15103227"
        },
        {
            "date": "2016-01-16T15:24:11+0000",
            "author": "Yonik Seeley",
            "content": "i noticed the taged/excluded filters in the original example and definitely tried that when i was trying to reproduce\n\nTry uncached... i.e. \n\nq={!cache=false}*:*&...\n\n ",
            "id": "comment-15103228"
        },
        {
            "date": "2016-01-16T16:48:24+0000",
            "author": "Adrien Grand",
            "content": "Should we only have the bugfix here that applies deleted docs in getDocSet and open another issue to discuss the hasDeletedDocs optimization? ",
            "id": "comment-15103250"
        },
        {
            "date": "2016-01-16T16:56:39+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1725005 from Yonik Seeley in branch 'dev/trunk'\n[ https://svn.apache.org/r1725005 ]\n\nSOLR-8496: multi-select faceting and getDocSet(List<Query>) can match deleted docs ",
            "id": "comment-15103253"
        },
        {
            "date": "2016-01-16T17:06:19+0000",
            "author": "Yonik Seeley",
            "content": "Crossed messages - I had finished testing and committed by the time I saw this.\nAnyway, I didn't see it as an optimization - I simply wrote it how I would have originally by checking deleted docs in just the case where it was missing. ",
            "id": "comment-15103258"
        },
        {
            "date": "2016-01-16T17:22:09+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1725008 from Yonik Seeley in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1725008 ]\n\nSOLR-8496: multi-select faceting and getDocSet(List<Query>) can match deleted docs ",
            "id": "comment-15103266"
        },
        {
            "date": "2016-01-16T17:34:13+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1725010 from Yonik Seeley in branch 'dev/branches/lucene_solr_5_4'\n[ https://svn.apache.org/r1725010 ]\n\nSOLR-8496: multi-select faceting and getDocSet(List<Query>) can match deleted docs ",
            "id": "comment-15103271"
        },
        {
            "date": "2016-01-16T17:47:14+0000",
            "author": "Adrien Grand",
            "content": "I have concerns that this part of the code is playing with live docs a bit too much: there are several ways that live docs might get applied, which multiplies the risks to have other livedocs-related bugs in the future. I would have rather liked something that just applies live docs all the time in getDocSet or that does if (answer == null) answer = getLiveDocs(); instead of the current pf.hasDeletedDocs = (answer == null). ",
            "id": "comment-15103284"
        },
        {
            "date": "2016-01-16T18:07:47+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1725012 from Yonik Seeley in branch 'dev/branches/lucene_solr_5_3'\n[ https://svn.apache.org/r1725012 ]\n\nSOLR-8496: multi-select faceting and getDocSet(List<Query>) can match deleted docs ",
            "id": "comment-15103296"
        },
        {
            "date": "2016-01-16T18:29:09+0000",
            "author": "Yonik Seeley",
            "content": "The original code was missing the case when liveDocs should be consulted, so I added a patch that consulted liveDocs only in the case when it was needed (when all clauses are uncached).\nIf we want to investigate further code cleanups, I think that can be done in another issue what won't hold up the releases. ",
            "id": "comment-15103306"
        },
        {
            "date": "2016-01-16T18:32:15+0000",
            "author": "Joel Bernstein",
            "content": "Yonik Seeley, can you provide a quick summary of the issue. I see lot's of symptoms in the ticket but I don't see the details of the inner workings of the bug.\n\nI'm concerned this bug may be hitting us in many different places besides facets, such as field collapsing, and exporting. ",
            "id": "comment-15103309"
        },
        {
            "date": "2016-01-16T18:40:05+0000",
            "author": "Joel Bernstein",
            "content": "One of the things that is mentioned in this ticket is that the doc counts in the schema browser were also effected. I'm wondering how far this bug reaches. ",
            "id": "comment-15103317"
        },
        {
            "date": "2016-01-16T19:05:08+0000",
            "author": "Yonik Seeley",
            "content": "I'm concerned this bug may be hitting us in many different places besides facets, such as field collapsing, and exporting.\n\nIndeed.  We may still be vulnerable , but not due to this bug in particular.\n\nThe change in general was LUCENE-6553, and that may yet cause bugs (like this one) in different areas.\nDeleted docs are now only screened out before hitting the Collector.  So any place that does something lower level, like Weight.scorer(), is vulnerable if used in a context was was expecting only live docs.\n\nThis specific bug:\nThe DocSet returned from SolrIndexSearcher.getDocSet(List<Query>) could contain deleted documents (and that breaks our current invariants that DocSets never contain deleted docs).\nLUCENE-6553 changed (among many others) this line: \nhttps://github.com/apache/lucene-solr/blob/trunk/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java#L2473\nWe used to pass liveDocs at that point, but that method signature was removed.\nSo now, if all clauses to be intersected are uncached, then Weight.scorer() is used for all of them and the intersection can thus still contain deleted docs.  If even one clause is a normal DocSet, we're good since they do reflect liveDocs.\n\nSo the fix was, detect the case where all clauses are uncached (i.e. will use Weight.scorer) and check liveDocs in that specific case. ",
            "id": "comment-15103376"
        },
        {
            "date": "2016-01-16T19:13:23+0000",
            "author": "Yonik Seeley",
            "content": "I'm concerned this bug may be hitting us in many different places besides facets, such as field collapsing, and exporting.\n\nIf you're wondering about post filters, I think it depends on how they get their docs.\nIf it's through the normal mechanism (search(query, collector)) then we're good.  Lucene filters out deleted docs before they hit the collector.\nIf you ever feed a collector yourself, you need to ensure that it isn't fed deleted docs. ",
            "id": "comment-15103384"
        },
        {
            "date": "2016-01-16T19:23:18+0000",
            "author": "Joel Bernstein",
            "content": "One candidate I can think of right off is the HashQParserPlugin which handles shuffling for the streaming API. I'll review that code. The schema browser bug reported in this issue is also important to track down.  ",
            "id": "comment-15103397"
        },
        {
            "date": "2016-01-16T19:35:39+0000",
            "author": "Yonik Seeley",
            "content": "After that you can see, that in \"Schema Browser\" when you select your field and press \"Load Term Info\" counts for your field are incorrect (they take into account also old versions of the overwritten documents).\n\nEither behavior could be correct, so we should first verify that this behavior has changed (i.e. did 5.2 take into account deleted documents?)\nNot that for tull-text scoring, term statistics like ttf, idf, etc, do not take deletions into account.  ",
            "id": "comment-15103412"
        },
        {
            "date": "2016-02-08T22:35:49+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 7b42653a274962f50661f4bed52dad298f7064d5 in lucene-solr's branch refs/heads/branch_5_4 from Yonik Seeley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7b42653 ]\n\nSOLR-8496: multi-select faceting and getDocSet(List<Query>) can match deleted docs\n\ngit-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene_solr_5_4@1725010 13f79535-47bb-0310-9956-ffa450edef68 ",
            "id": "comment-15137883"
        }
    ]
}