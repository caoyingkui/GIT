{
    "id": "LUCENE-2376",
    "title": "java.lang.OutOfMemoryError:Java heap space",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "2.9.1",
        "resolution": "Invalid",
        "status": "Closed"
    },
    "description": "I see an OutOfMemory error in our product and it is happening when we have some data objects on which we built the index. I see the following OutOfmemory error, this is happening after we call Indexwriter.optimize():\n\n\n4/06/10 02:03:42.160 PM PDT [ERROR] Lucene Merge Thread #12  In thread Lucene Merge Thread #12 and the message is org.apache.lucene.index.MergePolicy$MergeException: java.lang.OutOfMemoryError: Java heap space\n4/06/10 02:03:42.207 PM PDT [VERBOSE] Lucene Merge Thread #12 [Manager] Uncaught Exception in thread Lucene Merge Thread #12\norg.apache.lucene.index.MergePolicy$MergeException: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:351)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:315)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.HashMap.resize(HashMap.java:462)\n\tat java.util.HashMap.addEntry(HashMap.java:755)\n\tat java.util.HashMap.put(HashMap.java:385)\n\tat org.apache.lucene.index.FieldInfos.addInternal(FieldInfos.java:256)\n\tat org.apache.lucene.index.FieldInfos.read(FieldInfos.java:366)\n\tat org.apache.lucene.index.FieldInfos.<init>(FieldInfos.java:71)\n\tat org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:116)\n\tat org.apache.lucene.index.SegmentReader.get(SegmentReader.java:638)\n\tat org.apache.lucene.index.SegmentReader.get(SegmentReader.java:608)\n\tat org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:686)\n\tat org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4979)\n\tat org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:4614)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:235)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:291)\n4/06/10 02:03:42.895 PM PDT [ERROR]  this writer hit an OutOfMemoryError; cannot complete optimize",
    "attachments": {
        "CheckIndex_JavaHeapOOM.txt": "https://issues.apache.org/jira/secure/attachment/12441266/CheckIndex_JavaHeapOOM.txt",
        "InfoStreamOutput.txt": "https://issues.apache.org/jira/secure/attachment/12440986/InfoStreamOutput.txt",
        "CheckIndex_PermGenSpaceOOM.txt": "https://issues.apache.org/jira/secure/attachment/12441267/CheckIndex_PermGenSpaceOOM.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-04-07T02:55:02+0000",
            "content": "I attached the info stream output from the IndexWriter.\n\nBasically I copied the output from the log file. The actual log contains more \"PurgeFiled\" statements than what I copied here. ",
            "author": "Shivender Devarakonda",
            "id": "comment-12854307"
        },
        {
            "date": "2010-04-07T09:01:47+0000",
            "content": "That's the duplicate of LUCENE-2361.\nIt seems to me you guys have a segment with insane amount of fields. Well, or your index is corrupt and this segment just parses as one having insane amount of fields. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12854393"
        },
        {
            "date": "2010-04-07T09:19:16+0000",
            "content": "You mean insane amount of fields with norms...? ",
            "author": "Uwe Schindler",
            "id": "comment-12854396"
        },
        {
            "date": "2010-04-07T09:24:41+0000",
            "content": "Is this the same issue as LUCENE-2361?  Or is something different here?  Are you using the same settings in IndexWriter?\n\nThe infoStream output is now much more complete \u2013 eg I see where the OOME happens at the end.\n\nIt does look like the cause here is an insane number of fields:\n\n\n  purge field=Cor414D5120514D475231202020202020204A94834B21C22E6B\n  purge field=Cor414D5120514D475231202020202020204A94834B21C22E6C\n  purge field=Cor414D5120514D475231202020202020204A94834B21C22E6D\n  purge field=Cor414D5120514D475231202020202020204A94834B21C22E6E\n  purge field=Cor414D5120514D475231202020202020204A94834B21C22E70\n  purge field=Cor414D5120514D475231202020202020204A94834B21C22E72\n\n\n\nI count 43K purge fields lines in your log (and that's an undercount since you've pruned some).  Are you disabling norms for these fields (if not, this will certainly lead to OOME)?  In your design, is there any limit to the number of fields?  Or will each document have a new field name?\n\nCan you run CheckIndex on the index and post the output? ",
            "author": "Michael McCandless",
            "id": "comment-12854398"
        },
        {
            "date": "2010-04-08T00:25:44+0000",
            "content": "\nThanks for looking into this.\n\nThis issue is different from LUCENE-2361. The LUCENE-2361 is about the OOM on permgenspace and this issue is about OOM due to Java heapspace. \n\n1 - LUCENE-2361 is happenining when we start ourproduct with the index directory that was already created. After few minutes, the product is throwing OOM-Permgenspace(As I described in the 2361)\n\n2 - This issue happens when we start the product without any index directory. The product has the functionality to rebuild the complete index if index directory does not exist. You can see the infostream output in the attached file, this comes when it tries to index the Objects and we have huge data on which we are indexing.\n\n1 - Can you tell me why do we see \"purge field\" entries in infostream output?\n\n2 - We always have limited number of field entries to Document but there is a case where we can get different number of fields for different documents, do you think that will cause the problem?\n\n\n ",
            "author": "Shivender Devarakonda",
            "id": "comment-12854749"
        },
        {
            "date": "2010-04-08T09:27:07+0000",
            "content": "OK but I suspect the root cause is the same here \u2013 your index seems to have a truly massive number of fields.  Can you post the CheckIndex output?\n\nIW re-uses per-field objects internally, so that many docs with the same field can be indexed more efficiently.  However, when IW sweeps to free up RAM, if it notices an allocated field object hasn't been used recently, because that field name has not occurred in recently added docs, it frees up that memory and logs that \"purge field\".  So from this output I can see you have at least 43K unique field names.\n\nIf you have not disabled norms on these fields you'll certainly run out of memory.  Even if you disable norms, Lucene is in general not optimized for a tremendous number of unique fields and you'll likely hit other issues. ",
            "author": "Michael McCandless",
            "id": "comment-12854876"
        },
        {
            "date": "2010-04-09T05:42:40+0000",
            "content": "CheckIndex output for JavaHeapOOM error. As I specified earlier, We saw OOM when it is indexing the data. I ran the checkIndex on the partially generated index folder.\n\n ",
            "author": "Shivender Devarakonda",
            "id": "comment-12855278"
        },
        {
            "date": "2010-04-09T05:48:44+0000",
            "content": "If we start our product with already generated index content  then we see and permgenspace OOM. I generated the CheckIndex on this index folder.\n\nPlease let me know your thoughts on these output files. ",
            "author": "Shivender Devarakonda",
            "id": "comment-12855280"
        },
        {
            "date": "2010-04-09T07:32:12+0000",
            "content": "Please find the attached CheckIndex output when it was run against the  index directorries  for both permgent and heapOOM issues ",
            "author": "Shivender Devarakonda",
            "id": "comment-12855306"
        },
        {
            "date": "2010-04-09T10:17:02+0000",
            "content": "Hmm indeed you have a great many unique fields.  A number of your segments have 100K-200K unique fields.  And it doesn't help that these field names are looong.\n\nI suspect you're going to have to change your design to not create such a huge number of fields.\n\nHow does your app use these fields? ",
            "author": "Michael McCandless",
            "id": "comment-12855336"
        },
        {
            "date": "2010-04-09T17:55:50+0000",
            "content": "I have a question on this, if we have two documents in the index: \nDocument entrry 1 contains \"field1\" , \"field2\" \"field3\" \n\nDocument entry 2 contains \"field1\" , \"field2\", field4\" \n\nShall I assume that the total unique fields in the index are 4? \n\n\nWe have a set of key and value pairs that we add them as fields to the document. The set of keys must be same in most cases but might be different in some scenarios. I need to see what actually our data contains? I will do some research on that. \n\nThanks, \nShivender ",
            "author": "Shivender Devarakonda",
            "id": "comment-12855496"
        },
        {
            "date": "2010-04-10T12:20:02+0000",
            "content": "Yes total unique fields are 4 in that case.  I suspect it's the fields like Cor414D5120514D475231202020202020204A94834B21C22E6E that are causing your problems. ",
            "author": "Michael McCandless",
            "id": "comment-12855548"
        },
        {
            "date": "2010-04-10T17:24:46+0000",
            "content": "Thanks for your inpput on this.\n\nCould you please assist me in understanding more on this issue?:\n\nFor every object on which we index will have one or more CorXXXXXXX entry(XXXXX is a uniquenumber), Example:\n\nObject 1's document will have \" field A\" \"Cor12345\" \"Cor2345\"\n\nObject 2's documebt will hava \"field A\" \"Cor4567\" \"Cor8902\"\n\nand so on...\n\n\nThese COrXXX are unique fields for each document so does that mean each document will contain empty holders or holes for the other fields that exists in other object's document but not in it's document?\n\nHow is that becoming reason for OOM? \n\nThanks,\nShivender ",
            "author": "Shivender Devarakonda",
            "id": "comment-12855606"
        },
        {
            "date": "2010-04-10T17:50:01+0000",
            "content": "A field is basically an index in itself. As such, it has a bunch of bookkeeping data associated with it, which is always loaded into memory, unlike field values, which are mostly kept to disk. Also, like I explained in LUCENE-2361, field names are interned as they take part in string comparisons very often internally.\n\nSo creating an insane amount of unique fields is like begging for either OOM or OOPermGen. Can you elaborate, why do you need to store unique number in field name rather then field value? ",
            "author": "Earwin Burrfoot",
            "id": "comment-12855615"
        },
        {
            "date": "2010-04-16T22:53:27+0000",
            "content": "Hi,\nI reached to the development team that is actually creating these unique fields. they may revisit their redesign. I have a question what is the entry in the \"CheckIndex\" output refers to Unique index fields?\n\nThanks,\nShivender  ",
            "author": "Shivender Devarakonda",
            "id": "comment-12858047"
        },
        {
            "date": "2015-08-18T07:48:51+0000",
            "content": "Closing old inactive issue which seemed to be a usage problem.\u00b4\nPlease re-open if you feel there is a bug in Lucene that needs fixing  ",
            "author": "Jan H\u00f8ydahl",
            "id": "comment-14700869"
        }
    ]
}