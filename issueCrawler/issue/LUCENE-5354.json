{
    "id": "LUCENE-5354",
    "title": "Blended score in AnalyzingInfixSuggester",
    "details": {
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed",
        "components": [
            "modules/spellchecker"
        ],
        "affect_versions": "4.4",
        "status": "Closed",
        "fix_versions": [
            "4.7",
            "6.0"
        ]
    },
    "description": "I'm working on a custom suggester derived from the AnalyzingInfix. I require what is called a \"blended score\" (//TODO ln.399 in AnalyzingInfixSuggester) to transform the suggestion weights depending on the position of the searched term(s) in the text.\n\nRight now, I'm using an easy solution :\nIf I want 10 suggestions, then I search against the current ordered index for the 100 first results and transform the weight :\na) by using the term position in the text (found with TermVector and DocsAndPositionsEnum)\nor\nb) by multiplying the weight by the score of a SpanQuery that I add when searching\nand return the updated 10 most weighted suggestions.\n\nSince we usually don't need to suggest so many things, the bigger search + rescoring overhead is not so significant but I agree that this is not the most elegant solution.\nWe could include this factor (here the position of the term) directly into the index.\n\nSo, I can contribute to this if you think it's worth adding it.\n\nDo you think I should tweak AnalyzingInfixSuggester, subclass it or create a dedicated class ?",
    "attachments": {
        "LUCENE-5354_3.patch": "https://issues.apache.org/jira/secure/attachment/12622204/LUCENE-5354_3.patch",
        "LUCENE-5354.patch": "https://issues.apache.org/jira/secure/attachment/12618904/LUCENE-5354.patch",
        "LUCENE-5354_4.patch": "https://issues.apache.org/jira/secure/attachment/12622839/LUCENE-5354_4.patch",
        "LUCENE-5354_2.patch": "https://issues.apache.org/jira/secure/attachment/12619372/LUCENE-5354_2.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-13836569",
            "author": "Michael McCandless",
            "content": "This sounds very useful!\n\nI think a subclass could work well, if we open up the necessary methods (which Query to run, how to do the search / resort the results)?\n\nWe could make the index-time sorting optional as well?  This way you'd build an \"ordinary\" index, run an \"ordinary\" query, so you have full flexibility (but at more search-time cost). ",
            "date": "2013-12-02T14:55:49+0000"
        },
        {
            "id": "comment-13849158",
            "author": "Remi Melisson",
            "content": "I attached a first patch which allows blended score based on the position of the search term. It only provides strategy (a) with two options :\n\n\tLinear (-10% for each position) : blended_weight = weight * (1-0.10*position)\n\tReciprocal : blended_weight = weight/(1+position)\n\n\n\nI would also like to add the second strategy (b) by directly using the score, but here is a first attempt.\nAny advice/remarks welcome! ",
            "date": "2013-12-16T13:39:22+0000"
        },
        {
            "id": "comment-13850713",
            "author": "Michael McCandless",
            "content": "Thanks Remi, patch looks great!\n\nCan you move that boolean finished inside the if (lastToken != null)?  (If there was no lastToken then we should not be calling offsetEnd.endOffset).\n\nCan we leave AnalyzingInfixSuggester with DOCS_ONLY?  I.e., open up a method (maybe getTextFieldType?) that the subclass would override and set to DOCS_AND_FREQS_AND_POSITIONS.\n\nIn createCoefficient, instead of splitting the incoming key on space, I think you should ask the analyzer to do so?  In fact, since the lookup (in super) already did that (break into tokens, figure out if last token is a \"prefix\" or not), maybe we can just pass that down to createResult?\n\nIf the query has more than one term, it looks like you only use the first?  Maybe instead we should visit all the terms and record which one has the lowest position?\n\nHave you done any performance testing?  Visiting term vectors for each hit can be costly.  It should be more performant to pull a DocsAndPositionsEnum up front and then do .advance to each (sorted) docID to get the position ... but this is likely more complex (it inverts the \"stride\", so you'd do term by term on the outer loop, then\ndocs on the inner loop, vs the opposite that you have now).\n\nkey.toString() can be pulled out of the while loop and done once up front.\n\nWhy do you use key.toString().contains(docTerm) for the finished case? Won't that result in false positives, e.g. if key is \"foobar\" and docTerm is \"oba\"?\n\nCan you rewrite the embedded ternary operator in the LookUpComparator to just use simple if statements?  I think that's more readable... ",
            "date": "2013-12-17T17:44:51+0000"
        },
        {
            "id": "comment-13851978",
            "author": "Remi Melisson",
            "content": "Hey Michael, thanks for the in-depth code review!\nI attached another patch which makes things simpler and fixes what you suggested.\n\nThe remaining things are :\nHave you done any performance testing?\nNot really, I've seen that you did some for the infix suggester, but I couldn't find the code. Is there something already or should I test the performance my way ?\n\n\nVisiting term vectors for each hit can be costly. It should be more performant to pull a DocsAndPositionsEnum up front and then do .advance to each (sorted) docID to get the position ... but this is likely more complex (it inverts the \"stride\", so you'd do term by term on the outer loop, then docs on the inner loop, vs the opposite that you have now).\nFor now, the only way I know to access the DocsAndPositionsEnum is by getting it from the TermsEnum which implies iterating over the term vector (the doc says \"Get DocsAndPositionsEnum for the current term\"). ",
            "date": "2013-12-18T18:12:27+0000"
        },
        {
            "id": "comment-13864328",
            "author": "Remi Melisson",
            "content": "Hi, any news about this feature ?\nCould I do anything else ? ",
            "date": "2014-01-07T15:24:45+0000"
        },
        {
            "id": "comment-13864683",
            "author": "Michael McCandless",
            "content": "Woops, sorry, this fell below the event horizon of my TODO list.  I'll look at your new patch soon.\n\nThere is an existing performance test, LookupBenchmarkTest, but it's a bit tricky to run.  See the comment on LUCENE-5030: https://issues.apache.org/jira/browse/LUCENE-5030?focusedCommentId=13689155&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13689155 ",
            "date": "2014-01-07T21:00:27+0000"
        },
        {
            "id": "comment-13865659",
            "author": "Michael McCandless",
            "content": "New patch looks great, thanks Remi!\n\nI'm worried about how costly iterating over term vectors is going to be ... are you planning to run the performance test?  If not, I can.\n\nIt might be better to open up a protected method to convert the smallest position to the coefficient?  The default impl can do the switch based on the BlenderType enum... but apps may want to control how the score is \"boosted\" by position. ",
            "date": "2014-01-08T17:25:05+0000"
        },
        {
            "id": "comment-13866760",
            "author": "Remi Melisson",
            "content": "Hi!\nHere is new patch including your comment for the coefficient calculation (I guess a Lambda function would be perfect here!).\n\nI ran the performance test on my laptop, here are the results compared to the AnalyzingInfixSuggester : \n\u2013 construction time\nAnalyzingInfixSuggester input: 50001, time[ms]: 1780 [+- 367.58]\nBlendedInfixSuggester input: 50001, time[ms]: 6507 [+- 2106.52]\n\u2013 prefixes: 2-4, num: 7, onlyMorePopular: false\nAnalyzingInfixSuggester queries: 50001, time[ms]: 6804 [+- 1403.13], ~kQPS: 7\nBlendedInfixSuggester queries: 50001, time[ms]: 26503 [+- 2624.41], ~kQPS: 2\n\u2013 prefixes: 6-9, num: 7, onlyMorePopular: false\nAnalyzingInfixSuggester queries: 50001, time[ms]: 3995 [+- 551.20], ~kQPS: 13\nBlendedInfixSuggester queries: 50001, time[ms]: 5355 [+- 1295.41], ~kQPS: 9\n\u2013 prefixes: 100-200, num: 7, onlyMorePopular: false\nAnalyzingInfixSuggester queries: 50001, time[ms]: 2626 [+- 588.43], ~kQPS: 19\nBlendedInfixSuggester queries: 50001, time[ms]: 1980 [+- 574.16], ~kQPS: 25\n\u2013 RAM consumption\nAnalyzingInfixSuggester size[B]:    1,430,920\nBlendedInfixSuggester size[B]:    1,630,488\n\nIf you have any idea on how we could improve the performance, let me know (see above my comment for your previous suggestion to avoid visiting term vectors). ",
            "date": "2014-01-09T16:32:04+0000"
        },
        {
            "id": "comment-13866989",
            "author": "Michael McCandless",
            "content": "Thanks Remi, the performance seems fine?  But I realized this is not the best benchmark, since all suggestions are just a single token.\n\nNew patch looks great; I think we should commit this approach, and performance improvements can come later if necessary.\n\nsee above my comment for your previous suggestion to avoid visiting term vectors\n\nOh, the idea I had was to not use term vectors at all: you can get a TermsEnum for the normal inverted index, and then visit each term from the query, and then .advance to each doc from the top N results.  But we can do this later ... I'll commit this patch (I'll make some small code style improvements, e.g. adding { } around all ifs). ",
            "date": "2014-01-09T20:11:21+0000"
        },
        {
            "id": "comment-13867072",
            "author": "Michael McCandless",
            "content": "Thanks Remi!\n\nI committed with the wrong issue LUCENE-5345 by accident... ",
            "date": "2014-01-09T21:17:36+0000"
        },
        {
            "id": "comment-13867698",
            "author": "Remi Melisson",
            "content": "Great, glad to contribute!\nIn term of performance, I'm using it on my laptop with 30K terms and the mean time for lookup is 5ms for 5 results and 45ms for 50 results (with a factor 10, ie. I retrieve 50 / 500 items then reduce to 5 / 50). I'm not following a proper testing methodology so it's just roughly what I observed. \nI will do more extensive testing performance-wise and yeah, we can tackle that later on. ",
            "date": "2014-01-10T10:57:05+0000"
        },
        {
            "id": "comment-13870656",
            "author": "Remi Melisson",
            "content": "Woops, I introduced a bug when refactoring the comparator.\nI submitted another patch to fix this. I also updated the test case accordingly. ",
            "date": "2014-01-14T11:56:52+0000"
        },
        {
            "id": "comment-13870701",
            "author": "Michael McCandless",
            "content": "OK, no problem, I'll have a look!  Thanks Remi. ",
            "date": "2014-01-14T13:07:50+0000"
        },
        {
            "id": "comment-13870867",
            "author": "ASF subversion and git services",
            "content": "Commit 1558100 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1558100 ]\n\nLUCENE-5354: BlendedInfixSuggester: fix wrong return (0 instead of -1) from the LookupResult comparator ",
            "date": "2014-01-14T16:32:23+0000"
        },
        {
            "id": "comment-13870870",
            "author": "ASF subversion and git services",
            "content": "Commit 1558102 from Michael McCandless in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1558102 ]\n\nLUCENE-5354: BlendedInfixSuggester: fix wrong return (0 instead of -1) from the LookupResult comparator ",
            "date": "2014-01-14T16:33:24+0000"
        }
    ]
}