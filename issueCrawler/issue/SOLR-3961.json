{
    "id": "SOLR-3961",
    "title": "LimitTokenCountFilterFactory config parsing is totally broken",
    "details": {
        "affect_versions": "4.0",
        "status": "Closed",
        "fix_versions": [
            "4.1",
            "6.0"
        ],
        "components": [],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "As noted on the mailing list, LimitTokenCountFilterFactory throws a NumberFormatException because it tries to use the value of it's config param as a key to look up another param that it parses as an integer ... totally ridiculous.",
    "attachments": {
        "SOLR-3961.patch": "https://issues.apache.org/jira/secure/attachment/12549562/SOLR-3961.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Hoss Man",
            "id": "comment-13478256",
            "date": "2012-10-17T19:08:32+0000",
            "content": "From the list...\n\n\nI read the following in the example solrconfig:\n\n <!-- maxFieldLength was removed in 4.0. To get similar behavior, include a\n         LimitTokenCountFilterFactory in your fieldType definition. E.g.\n     <filter class=\"solr.LimitTokenCountFilterFactory\" maxTokenCount=\"10000\"/>\n    -->\n\nI tried that as follows:\n\n...\n<fieldType name=\"textgen\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer type=\"index\">\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LimitTokenCountFilterFactory\"\nmaxTokenCount=\"100000\"/>\n        <filter class=\"solr.WordDelimiterFilterFactory\"\ngenerateWordParts=\"1\" generateNumberParts=\"1\" catenateWords=\"0\"\ncatenateNumbers=\"0\" catenateAll=\"0\" splitOnCaseChange=\"0\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.SnowballPorterFilterFactory\" language=\"German\"\n/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\"\nwords=\"stopwords.txt\" enablePositionIncrements=\"true\" />\n        <filter class=\"solr.RemoveDuplicatesTokenFilterFactory\" />\n      </analyzer>\n...\n\nThe LimitTokenCountFilterFactory configured like that crashes the startup\nof the corresponding core with the following exception (without the Factory\nthe core startup works):\n\n\n17.10.2012 17:44:19 org.apache.solr.common.SolrException log\nSCHWERWIEGEND: null:org.apache.solr.common.SolrException: Plugin init\nfailure for [schema.xml] fieldType \"textgen\": Plugin init failure for\n[schema.xml] analyze\nr/filter: null\n        at\norg.apache.solr.util.plugin.AbstractPluginLoader.load(AbstractPluginLoader.java:177)\n        at\n...\n\n\n\nAnd as Jack noted...\n\n\nAnybody want to guess what's wrong with this code:\n\nString maxTokenCountArg = args.get(\"maxTokenCount\");\nif (maxTokenCountArg == null) {\n throw new IllegalArgumentException(\"maxTokenCount is mandatory.\");\n}\nmaxTokenCount = Integer.parseInt(args.get(maxTokenCountArg));\n\nHmmm... try this \"workaround\":\n\n    <filter class=\"solr.LimitTokenCountFilterFactory\" maxTokenCount=\"foo\" foo=\"10000\"/>\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13478306",
            "date": "2012-10-17T19:49:29+0000",
            "content": "\ni modified one of the test schemas to include LImitTOkenCountFilterFactory to quickly demonstrate the init failure, then banged out a quick fix to the factory.  for added measure, i added a unit test of the factory itself, and seem to have uncovered another bug somewhere \u2013 although i'm not yet clear if it's in the Filter (violating the TokenStream contract) or in BaseTokenStreamTestCase (trying to enforce the contract)...\n\n\n[junit4:junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestLimitTokenCountFilterFactory -Dtests.method=test -Dtests.seed=D40B43DE6627A1AB -Dtests.slow=true -Dtests.locale=mk -Dtests.timezone=Singapore -Dtests.file.encoding=US-ASCII\n[junit4:junit4] FAILURE 0.13s | TestLimitTokenCountFilterFactory.test <<<\n[junit4:junit4]    > Throwable #1: java.lang.AssertionError: end() called before incrementToken() returned false!\n[junit4:junit4]    > \tat __randomizedtesting.SeedInfo.seed([D40B43DE6627A1AB:5C5F7C04C8DBCC53]:0)\n[junit4:junit4]    > \tat org.apache.lucene.analysis.MockTokenizer.end(MockTokenizer.java:243)\n[junit4:junit4]    > \tat org.apache.lucene.analysis.TokenFilter.end(TokenFilter.java:46)\n[junit4:junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:239)\n[junit4:junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:250)\n[junit4:junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:262)\n[junit4:junit4]    > \tat org.apache.lucene.analysis.miscellaneous.TestLimitTokenCountFilterFactory.test(TestLimitTokenCountFilterFactory.java:37)\n\n\n\nI'm going to need another set of eyeballs on this. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13478318",
            "date": "2012-10-17T20:04:44+0000",
            "content": "For testing LimitTokenCountFilter, you need to set http://lucene.apache.org/core/4_0_0/test-framework/org/apache/lucene/analysis/MockTokenizer.html#setEnableChecks%28boolean%29\n\nThats because LimitTokenCountFilter violates the workflow of the standard tokenstream API (http://lucene.apache.org/core/4_0_0/core/org/apache/lucene/analysis/TokenStream.html)... it cuts off its consumer and calls end() even while that consumer still has more tokens: this could easily cause unexpected side effects and bugs.\n\nBut we knew about this anyway: existing tests for LimitTokenCountFilter already do setEnableChecks(false) for this exact reason... this is just an explanation of why. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13478456",
            "date": "2012-10-17T22:27:12+0000",
            "content": "Thanks rmuir, setEnableChecks is the piece i was missing and using it makes the test pass, but i'm still a little confused by the rest of your comment and what i'm seeing in the tests...\n\nBut we knew about this anyway: existing tests for LimitTokenCountFilter already do setEnableChecks(false) for this exact reason... this is just an explanation of why.\n\n1)  HunspellStemFilterTest is the only lucene/analysis test i see using setEnableChecks (although there do seem to be some highlighter tests that use it, and TestIndexWriterExceptions uses it to ignore secondary problems since it's going out of it's way to force exceptions)\n\n2) i don't see any existing tests for LimitTokenCountFilter .. were they deleted by mistake?\n\n3) the closest thing i see to a test of LimitTokenCountFilter is TestLimitTokenCountAnalyzer - i realize now the reason it's testLimitTokenCountAnalyzer doesn't get the same failure is because it's wrapping WhitespaceAnalyzer, StandardAnalyzer - should those be changed to use MockTokenizer?\n\n4) TestLimitTokenCountAnalyzer also has a testLimitTokenCountIndexWriter that uses MockAnalyzer w/o calling setEnableChecks(false) which seems like it should trigger the same failure i got since it uses MockTokenizer, but in general that test looks suspicious, as it seems to add the exact number of tokens that the limit is configured for, and then asserts that the last token is in the index - but never actaully triggers the limiting logic since exactly the allowed umber of tokens are used. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13478487",
            "date": "2012-10-17T23:13:26+0000",
            "content": "\nHunspellStemFilterTest is the only lucene/analysis test i see using setEnableChecks\n\nIt sets it to true, which is dead code (true is the default!).\n\n\nalthough there do seem to be some highlighter tests that use it\n\nHighlighter has a built-in limiter, that limits not based on tokencount, but accumulated # of analyzed chars. \nSo it disables it for the same reason as LimitTokenCount does or should\n\n\n2) i don't see any existing tests for LimitTokenCountFilter .. were they deleted by mistake?\n\nI think these are in TestLimitTokenCountAnalyzer? For lucene users this is the way you use this (just wrap your analyzer).\n\n\n3) the closest thing i see to a test of LimitTokenCountFilter is TestLimitTokenCountAnalyzer - i realize now the reason it's testLimitTokenCountAnalyzer doesn't get the same failure is because it's wrapping WhitespaceAnalyzer, StandardAnalyzer - should those be changed to use MockTokenizer?\n\nI think we should always do this!\n\n\n4) TestLimitTokenCountAnalyzer also has a testLimitTokenCountIndexWriter that uses MockAnalyzer w/o calling setEnableChecks(false) which seems like it should trigger the same failure i got since it uses MockTokenizer, but in general that test looks suspicious, as it seems to add the exact number of tokens that the limit is configured for, and then asserts that the last token is in the index - but never actaully triggers the limiting logic since exactly the allowed umber of tokens are used.\n\nThen thats fine, because when LimitTokenCountFilter consumes the whole stream, its a \"good consumer\". its only when it actually truncates that it breaks the tokenstream contract.\n\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13478491",
            "date": "2012-10-17T23:30:47+0000",
            "content": "I spun off LUCENE-4489 for some of the remaining questions about testingLimitTokenCountFilter since it's really only tangentially related to this issue \u2013 In the meantime i want to commit a fix for this factory bug.\n\nCommitted revision 1399474. - trunk\nCommitted revision 1399482. - branch_4x\n\n...i'm a little unclear as to how we're dealing with possible 4.0.1 fixes, so leaving this issue open for now until i figure it out.\n\n(BTW: Thanks for figuring out this problem Jack!) "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13554286",
            "date": "2013-01-15T20:24:52+0000",
            "content": "Hoss, I don't think there will be a 4.0.1 release - can this be resolved as fixed? "
        },
        {
            "author": "Commit Tag Bot",
            "id": "comment-13610667",
            "date": "2013-03-22T16:25:37+0000",
            "content": "[branch_4x commit] Chris M. Hostetter\nhttp://svn.apache.org/viewvc?view=revision&revision=1399482\n\nSOLR-3961: Fixed error using LimitTokenCountFilterFactory (merge r1399474) "
        }
    ]
}