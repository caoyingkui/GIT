{
    "id": "SOLR-4655",
    "title": "The Overseer should assign node names by default.",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.4",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Currently we make a unique node name by using the host address as part of the name. This means that if you want a node with a new address to take over, the node name is misleading. It's best if you set custom names for each node before starting your cluster. This is cumbersome though, and cannot currently be done with the collections API. Instead, the overseer could assign a more generic name such as nodeN by default. Then you can easily swap in another node with no pre planning and no confusion in the name.",
    "attachments": {
        "SOLR-4655.patch": "https://issues.apache.org/jira/secure/attachment/12577479/SOLR-4655.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-13625065",
            "date": "2013-04-08T01:00:46+0000",
            "content": "For back compat, I have added a new config option at the core container level called genericNodeNames - it will default to true in the next release, but it's absence will be equal to false. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13625555",
            "date": "2013-04-08T17:13:04+0000",
            "content": "Mark, I noticed this issue after I committed SOLR-3755. We assign names to sub-shard nodes in OverseerCollectionProcessor. Is the change as simple as using Assign.assignNode() or is there something more I need to take care of? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13625606",
            "date": "2013-04-08T18:03:36+0000",
            "content": "Is the change as simple as using Assign.assignNode() \n\nI think it's a bit more complicated than that - its a 62 kb patch. I think I've simplified things overall though. The way you could override the node name or get the address based name previously was a bit ugly. This creates one place for corenodename to be set, rather than having various places knowing and using the default when the core node name is null or something. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13625635",
            "date": "2013-04-08T18:24:24+0000",
            "content": "Patch to trunk. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13625713",
            "date": "2013-04-08T19:37:22+0000",
            "content": "Is the change as simple as using Assign.assignNode() or is there something more I need to take care of?\n\nOh, do you mean in terms of keeping the names sub shard names consistent with the super shard names? I didn't catch your meaning on the first read.\n\nThe way the node names are assigned is super simple - just Assign.assignNode to get a new name. I'm not positive that it will just play nice with sub shards - hope to look at that stuff closer soon.\n\nHow do you currently handle the case where a user specifies a custom arbitrary shard name? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13625718",
            "date": "2013-04-08T19:42:49+0000",
            "content": "Sub-slice names are created with just appending _N to the parent shard name. For example, shard1 gets split into shard1_0 and shard1_1 etc.\n\nThe node names are created as collection_shard1_0_replica1, collection_shard1_0_replica2 etc. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13626000",
            "date": "2013-04-08T23:57:33+0000",
            "content": "Checkpoint - I'm having trouble with the new ShardSplitTest - not sure what the issue is yet, but it passes about 50% of the time, while currently a std checkout is passing consistently for me. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13626141",
            "date": "2013-04-09T02:30:48+0000",
            "content": "Hey Mark, I just looked at the patch and it looks like you've removed the update/createshard APIs. Just wanted to bring it to your notice. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13626160",
            "date": "2013-04-09T02:53:38+0000",
            "content": "Hmm, must have lost them in the merge up somehow. Hopefully thats the reason that test is failing - oddly I've seen all tests pass a couple times though. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13630757",
            "date": "2013-04-12T23:32:07+0000",
            "content": "node\n{n} is not the right name here - going with core_node{n}\n - this is at the coreNodeName level.\n\nUnfortunately, the above code i mistakenly took out was not the only problem, that test is still failing. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13639216",
            "date": "2013-04-23T16:46:55+0000",
            "content": "Patch to trunk and with the above change. Tests seem to be passing now.\n\nI'd like to commit this soon. Shalin Shekhar Mangar or Anshum Gupta, are you able to sync up shard splitting with this change? "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13639222",
            "date": "2013-04-23T16:50:59+0000",
            "content": "[~hakeber] Sure. I'll have a look at it sometime tomorrow (My time, IST).\nHope that's fine. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13641932",
            "date": "2013-04-25T16:31:23+0000",
            "content": "To trunk. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13642960",
            "date": "2013-04-26T15:35:17+0000",
            "content": "Any progress with this Anshum Gupta ? "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13643432",
            "date": "2013-04-27T00:48:33+0000",
            "content": "Mark, sorry I couldn't find time. Was travelling from India to CA for Lucene Revolution.\nJust landed, will try and close it before the conference though. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13646950",
            "date": "2013-05-01T21:20:34+0000",
            "content": "I just tested the patch and this seems to work just about fine out of the box (other than a few minor hiccups while patching) even for the split shards. We don't seem to be using the hostname in the SplitShard code.\nHere's how the clusterstate looks like post a shard split call using genericNames.\n\n  {\"collection1\":{\n      \"shards\":{\n        \"shard1\":{\n          \"range\":\"80000000-ffffffff\",\n          \"state\":\"active\",\n          \"replicas\":{\"core_node1\":\nUnknown macro: {              \"state\"} \n}},\n        \"shard2\":{\n          \"range\":\"0-7fffffff\",\n          \"state\":\"active\",\n          \"replicas\":{\"core_node2\":\nUnknown macro: {              \"state\"} \n}},\n        \"shard1_0\":{\n          \"state\":\"active\",\n          \"replicas\":{\"core_node3\":\nUnknown macro: {              \"state\"} \n}},\n        \"shard1_1\":{\n          \"state\":\"active\",\n          \"replicas\":{\"core_node4\":{\n              \"state\":\"active\",\n              \"core\":\"collection1_shard1_1_replica1\",\n              \"node_name\":\"192.168.2.2:8983_solr\",\n              \"base_url\":\"http://192.168.2.2:8983/solr\",\n              \"leader\":\"true\"}}}},\n      \"router\":\"compositeId\"}} "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13646959",
            "date": "2013-05-01T21:28:58+0000",
            "content": "Ok, seems like something did break. Don't commit it just yet.\nNeed to double check and validate. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13651311",
            "date": "2013-05-07T21:05:03+0000",
            "content": "Mark, can you please reupload the latest patch? I'm having issues using the last patch you uploaded. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13651507",
            "date": "2013-05-08T00:52:20+0000",
            "content": "Yup, I'll try and merge up and upload a new patch tonight or tomorrow. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13651589",
            "date": "2013-05-08T03:30:41+0000",
            "content": "Just updated this trunk. Got a clean test pass, but I'll check it some more in the morning. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13651934",
            "date": "2013-05-08T14:30:47+0000",
            "content": "I'm a little confused how the tests are currently passing - makes me think something is off.\n\nOverseerCollectionProcesser will try and wait for the coreNodeName of: cmd.setCoreNodeName(nodeName + \"_\" + subShardName);\n\nBut according to your output above, the core node name will will be core_node\n{n}\n\nI'm not sure how that wait ends up working right. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13651951",
            "date": "2013-05-08T14:53:43+0000",
            "content": "I'll just patch up and have a look at it later in the day. I guess I know what's going on there. i.e. it waits (for the wrong coreNodeName) and times out but this isn't getting caught up.\nOnce I confirm that, I'll also create a JIRA to fix that. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13651967",
            "date": "2013-05-08T15:01:18+0000",
            "content": "What's odd is that I have another branch I'm working on where it does get caught up over that...\n\nAnother problem is that in ZkController#waitForCoreNodeName, it must use getSlicesMap rather than getActiveSlicesMap, otherwise these subshards don't find there set coreNodeNames. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13651970",
            "date": "2013-05-08T15:03:13+0000",
            "content": "Re: the wrong coreNodeName\n\nBecause the coreNodeName is set in the call to the coreadminhandler, there is no realy way to find out what it's set to without polling the clusterstate I think...I'm going to look into that real quick. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13653046",
            "date": "2013-05-09T16:28:22+0000",
            "content": "That's right. I'm trying to figure if it could be pre-specified as a param while making the call.\nAlso, seems like there are some other issues with the patch too which happened while integrating your patch with the trunk changes all through this while. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13653054",
            "date": "2013-05-09T16:33:39+0000",
            "content": "I think I've got this all working relatively okay in my dev branch - i just have to get it ported over to trunk before I put a patch up. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13653065",
            "date": "2013-05-09T16:42:42+0000",
            "content": "Ok, coz there are are few things like using shardState and shardRange only once in the preRegister command and resetting it that seems to have flipped over on the patch (order reversed).\n\nI could look at it once you put up the patch and integrate if there are some changes that I've made that you may have skipped. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13653098",
            "date": "2013-05-09T19:47:17+0000",
            "content": "Yeah, I'm currently working on another branch that has a fix for SOLR-4745, so that stuff has been moved in what I'm mainly looking at.\n\nUnfortunetly, we don't seem to have a test that will catch that yet (at least on my dev machine) - unless a more rare chaos monkey test can trigger it?\n\nAs a side note since I'm reminded: It also seemed like the shard splitting tests could occasionaly pass even though no split had been ordered - you noted some time ago above I was missing some critical code in a merge up, but I still got some passing runs at the time. It did fail commonly, but it seems like no split should be an easy all the time fail. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13653225",
            "date": "2013-05-09T21:17:30+0000",
            "content": "shard splitting tests could occasionaly pass even though no split had been ordered\n\nDo you intend to say complete and not 'ordered'? So the thing is, the shard split code instead of returning an error and exiting out, continues in case of a timeout while waiting for a core to come up. The shard gets created incorrectly. We'd need to stop that from happening and error out instead.\n\nP.S: Yes, we need to add tests that check for things other than just the final clusterstate. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13653296",
            "date": "2013-05-09T22:05:42+0000",
            "content": "Do you intend to say complete and not 'ordered'?\n\nI guess I mean 'started'? I think the missing code prevented a shard split to even be 'started' or requested. I didn't look super closely or anything, but it seemed I could still have succesful test runs in that state, so it seemed like with the right timing, the test did not assert a split had happened. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13653302",
            "date": "2013-05-09T22:09:21+0000",
            "content": "P.S: Yes, we need to add tests that check for things other than just the final clusterstate.\n\nI'm just surprised that it wouldn't trip the chaosmonkey test if this order was important, so I was wondering if I just didn't happen to see a fail that a jenkins chaosmonkey fail brought up previously - I seem to remember Shalin adding some of that stuff after the first commit and I didn't know if that was in response to a jenkins chaos fail that perhaps my fast dev machine is just randomly not easily hitting. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13653312",
            "date": "2013-05-09T22:18:09+0000",
            "content": "There are asserts for stuff other than the final cluster state just that there still are cases which could be a false positive as we may not have checked for everything that there is.\n\nI don't think the test can ever pass without a split actually happening (shards being created). I'll still have a look at it again though. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13653316",
            "date": "2013-05-09T22:20:47+0000",
            "content": "I don't think the test can ever pass without a split actually happening\n\nThat's what really surprised me - that i could get any pass based on that missing code - but I didn't look closely enough. I can probably remove those lines again by looking at the patch you originally commented on and see if I can reproduce it. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13664303",
            "date": "2013-05-22T17:35:25+0000",
            "content": "Okay, this is fairly out of date again - costing me too much to maintain, so I'm going to focus on getting this committed now. New patch coming soon. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13664475",
            "date": "2013-05-22T19:59:57+0000",
            "content": "I'd be more than happy to get the changes related to the shard split bit in place in 2 days after you put the new patch up. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13664509",
            "date": "2013-05-22T20:50:33+0000",
            "content": "This is the patch updated to trunk and with some of my changes from another branch that has other open JIRA fixes. On the branch tests were passing, but for some reason the shard splitting tests are not passing with my merge up - I have not been able to spot the root cause yet, but it looks like for some reason the sub shards do not end up with ranges in clusterstate.json. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13667862",
            "date": "2013-05-27T17:04:43+0000",
            "content": "I'll just start working on this. The email for this completely skipped my eyes. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13667889",
            "date": "2013-05-27T18:11:30+0000",
            "content": "I integrated the above patch and have the following tests failing on the trunk. Mark Miller Can you confirm that all of these tests fail for you as well?\n\n[junit4:junit4]   - org.apache.solr.cloud.ShardSplitTest.testDistribSearch\n[junit4:junit4]   - org.apache.solr.cloud.ChaosMonkeyShardSplitTest.testDistribSearch\n[junit4:junit4]   - org.apache.solr.cloud.ClusterStateUpdateTest.testCoreRegistration\n[junit4:junit4]   - org.apache.solr.cloud.BasicDistributedZkTest.testDistribSearch\n[junit4:junit4]   - org.apache.solr.cloud.BasicDistributedZkTest (suite) "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13667974",
            "date": "2013-05-27T21:47:53+0000",
            "content": "The last patch for me only had the shard split tests failing - ill try and update to trunk tomorrow.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13671576",
            "date": "2013-05-31T15:30:03+0000",
            "content": "Update to trunk.\n\nFor me the only fails are still in the shard splitting - i assume for the same reason - they are not getting a range somehow (the subshards). I have another branch with some other fixes/changes, and tests pass there, but I can't figure out the difference yet. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13671577",
            "date": "2013-05-31T15:30:26+0000",
            "content": "\n[junit4:junit4] Tests with failures:\n[junit4:junit4]   - org.apache.solr.cloud.ShardSplitTest.testDistribSearch\n[junit4:junit4]   - org.apache.solr.cloud.ChaosMonkeyShardSplitTest.testDistribSearch\n\n "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13673025",
            "date": "2013-06-03T11:25:31+0000",
            "content": "I'm looking at it but here's what fails for me:\n\n\n[junit4:junit4] Tests with failures:\n[junit4:junit4]   - org.apache.solr.cloud.ShardSplitTest.testDistribSearch\n[junit4:junit4]   - org.apache.solr.cloud.BasicDistributedZkTest.testDistribSearch\n[junit4:junit4]   - org.apache.solr.cloud.BasicDistributedZkTest (suite)\n\nOn similar lines but not exactly the same tests. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13673157",
            "date": "2013-06-03T14:30:29+0000",
            "content": "Looks like no slice related info is getting passed on while creating the sub-slices. Range and State are both missing.\nThe range is completely missing as we don't default it whereas the state is defaulting to 'ACTIVE'. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13673293",
            "date": "2013-06-03T16:37:18+0000",
            "content": "Strangely, I see non generic name for the core come up randomly on the test. Weird but have noticed that not too often but twice. Did you also see any such things happen? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13673321",
            "date": "2013-06-03T17:05:55+0000",
            "content": "I think all of the tests randomly switch between - to cover both paths - you can use the same seed or temp hard code to force one or the other. I'm traveling this week, so have less time than normal, but hopefully I can help look into this stuff as well a bit. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13680840",
            "date": "2013-06-12T00:06:19+0000",
            "content": "Okay, I think something changed from 4.3 to 4.x that affects this - my other branch that had tests passing against 4.3 also has shard tests failing after updating to trunk. Don't know what it is yet, but some new information. \n\nI'll dig in and try and figure it out - this patch is pretty useful for SOLR-4916. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13681370",
            "date": "2013-06-12T16:35:40+0000",
            "content": "I now have all tests passing for me with this integrated into SOLR-4916 "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13697463",
            "date": "2013-07-02T03:27:34+0000",
            "content": "SOLR-4916 is in, and this went in with it. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13697465",
            "date": "2013-07-02T03:31:02+0000",
            "content": "Commit 1498767 from Mark Miller\n[ https://svn.apache.org/r1498767 ]\n\nSOLR-4655: Add CHANGES entry "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13697468",
            "date": "2013-07-02T03:32:21+0000",
            "content": "Commit 1498768 from Mark Miller\n[ https://svn.apache.org/r1498768 ]\n\nSOLR-4655: Add CHANGES entry "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13716840",
            "date": "2013-07-23T18:38:44+0000",
            "content": "Bulk close resolved 4.4 issues "
        }
    ]
}