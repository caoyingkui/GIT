{
    "id": "LUCENE-6685",
    "title": "GeoPointInBBox/Distance queries should have safeguards",
    "details": {
        "resolution": "Won't Fix",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [],
        "priority": "Major",
        "status": "Closed",
        "type": "Improvement"
    },
    "description": "These queries build a big list of term ranges, where the size of the list is in proportion to how many cells of the space filling curve are \"crossed\" by the perimeter of the query (I think?).\n\nThis can easily be 100s of MBs for a big enough query ... not to mention slow to enumerate (we still do this again for each segment).\n\nI think the queries should have safeguards, much like we have maxDeterminizedStates for Automaton based queries, to prevent accidental OOMEs.\n\nBut I think longer term we should either change the ranges to be enumerated on-demand and never stored in entirety (like NumericRangeTermsEnum), or change the query so it has a fixed budget of how many cells it's allowed to visit and then within a crossing cell it uses doc values to post-filter.",
    "attachments": {
        "LUCENE-6685.patch": "https://issues.apache.org/jira/secure/attachment/12746262/LUCENE-6685.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14634504",
            "author": "Nicholas Knize",
            "date": "2015-07-21T04:16:49+0000",
            "content": "I put together a visualization of the ranges that were being created (will add the link to the video when I post it). This revealed some interesting issues. At precision_step 6 and detailLevel 16 the number of ranges for the worst case boundary condition were nearly 2 million. 100 iteration beast tests would take just over an hour.  Reducing that precisionStep to 3 and the detailLevel to 12 reduced the number of ranges to just over 10K.  The 100 iteration beast test was reduced from over an hour to just over 8 minutes. There was also a bug in the pointDistance query that added unnecessary high resolution ranges that fell within the bounding box but outside the actual pointRadius.  Patch included "
        },
        {
            "id": "comment-14635858",
            "author": "Nicholas Knize",
            "date": "2015-07-21T21:40:55+0000",
            "content": "Updated patch iincludes the following improvements:\n\n\n\tDynamically compute detail level based on query size (includes min/max bounds on detail level)\n\tRemove unnecessary ranges from PointDistanceQuery\n\tUpdated javadocs\n\n "
        },
        {
            "id": "comment-14637152",
            "author": "Nicholas Knize",
            "date": "2015-07-22T16:36:58+0000",
            "content": "In the spirit of Michael McCandless demonstration video for BKD I have posted a youtube video demonstrating GeoPointField's generation of morton numeric ranges computed from a GeoPointDistanceQuery using the most recent patch.  The detail level is computed dynamically based on query size. This trades memory consumption for potentially visiting more detail terms.  I plan on adding an enhancement making this level user configurable giving more control over the query performance (not to be confused with accuracy). "
        },
        {
            "id": "comment-14641926",
            "author": "Michael McCandless",
            "date": "2015-07-26T11:48:15+0000",
            "content": "Ooooh that video is WAY COOL!  Thanks Nicholas Knize!\n\nI'll look at the patch... "
        },
        {
            "id": "comment-14642094",
            "author": "Michael McCandless",
            "date": "2015-07-26T21:11:42+0000",
            "content": "There was also a bug in the pointDistance query that added unnecessary high resolution ranges that fell within the bounding box but outside the actual pointRadius.\n\nThis was a performance bug but not a correctness bug?  Is there some\nway we could get a test case that exposes it?\n\nDynamically compute detail level based on query size (includes min/max bounds on detail level)\n\nHmm the javadocs refer to detailLevel as a parameter, but I think\nyou cannot change it right?  It's always 5% of the smallest lat or lon\nrange?  Or did you mean to make it configurable via the API?\n\nThis means the query is actually \"pixelated\" on the boundary?  So,\npoints outside of the shape but inside of a pixel crossing the shape's\nboundary, will also be included?  Maybe improve the javadocs to\nclarify this?\n\nAlso I found this confusing in the javadocs:\n\n\nThe higher the resolution the greater the number of ranges along the query boundary. This results in visiting fewer terms\nin the terms dictionary for the price of memory usage. \n\nShouldn't it be more not fewer?  I.e., higher resolution means better\naccuracy (less pixelation), but slower CPU, more memory?\n\nHmm this (3) is a very low prec-step, i.e. index size will be much\nlarger (~ 22 terms per lat/lon I think, vs ~11 terms now):\n\n\n public final class GeoPointField extends Field {\n-  public static final int PRECISION_STEP = 6;\n+  public static final int PRECISION_STEP = 3;\n\n\n\nLooks like this just added a typo?:\n\n\n-// TODO: remove this?  Just absorb into its base class\n+\n+// TODO: remove this?  Just absorb into its base class5\n\n\n\nCan you try to add { } around single-line-in-body if statements\n(matches Lucene's coding style)?\n\n\n@@ -436,9 +437,8 @@ public class TestGeoPointQuery extends LuceneTestCase {\n                 verifyHits = new VerifyHits() {\n                     @Override\n                     protected Boolean shouldMatch(double pointLat, double pointLon) {\n-                      if (Double.isNaN(pointLat) || Double.isNaN(pointLon)) {\n+                      if (Double.isNaN(pointLat) || Double.isNaN(pointLon))\n                         return null;\n-                      }\n                       if (radiusQueryCanBeWrong(centerLat, centerLon, pointLon, pointLat, radius)) {\n                         return null;\n                       } else {\n\n "
        },
        {
            "id": "comment-14698479",
            "author": "Michael McCandless",
            "date": "2015-08-15T23:08:35+0000",
            "content": "Nicholas Knize is this no longer relevant, since we cut over to doc values for edge filtering? "
        },
        {
            "id": "comment-14713358",
            "author": "Shalin Shekhar Mangar",
            "date": "2015-08-26T13:13:15+0000",
            "content": "Bulk move to 5.4 after 5.3 release. "
        },
        {
            "id": "comment-15215964",
            "author": "Nicholas Knize",
            "date": "2016-03-29T12:52:02+0000",
            "content": "No longer relevant "
        }
    ]
}