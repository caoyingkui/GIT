{
    "id": "SOLR-139",
    "title": "Support updateable/modifiable documents",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.0"
        ],
        "components": [
            "update"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "It would be nice to be able to update some fields on a document without having to insert the entire document.\n\nGiven the way lucene is structured, (for now) one can only modify stored fields.\n\nWhile we are at it, we can support incrementing an existing value - I think this only makes sense for numbers.\n\nfor background, see:\nhttp://www.nabble.com/loading-many-documents-by-ID-tf3145666.html#a8722293",
    "attachments": {
        "SOLR-139-IndexDocumentCommand.patch": "https://issues.apache.org/jira/secure/attachment/12350297/SOLR-139-IndexDocumentCommand.patch",
        "SOLR-139.patch": "https://issues.apache.org/jira/secure/attachment/12525736/SOLR-139.patch",
        "SOLR-139-ModifyInputDocuments.patch": "https://issues.apache.org/jira/secure/attachment/12361094/SOLR-139-ModifyInputDocuments.patch",
        "SOLR-269+139-ModifiableDocumentUpdateProcessor.patch": "https://issues.apache.org/jira/secure/attachment/12361755/SOLR-269%2B139-ModifiableDocumentUpdateProcessor.patch",
        "getStoredFields.patch": "https://issues.apache.org/jira/secure/attachment/12362079/getStoredFields.patch",
        "SOLR-139_createIfNotExist.patch": "https://issues.apache.org/jira/secure/attachment/12536373/SOLR-139_createIfNotExist.patch",
        "SOLR-139-XmlUpdater.patch": "https://issues.apache.org/jira/secure/attachment/12350299/SOLR-139-XmlUpdater.patch",
        "Eriks-ModifiableDocument.patch": "https://issues.apache.org/jira/secure/attachment/12364891/Eriks-ModifiableDocument.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Ryan McKinley",
            "id": "comment-12470035",
            "date": "2007-02-04T08:59:41+0000",
            "content": "SOLR-139-IndexDocumentCommand.patch adds a new command to UpdateHandler and deprecates 'AddUpdateCommand'\n\nThis patch is only concerned with adding updateability to the UpdateHandler, it does not deal with how request handlers specify what should happen with each field.\n\nI added:\n\npublic class IndexDocumentCommand \n{\n  public enum MODE \n{\n    OVERWRITE, // overwrite existing values with the new one. (the default behavior)\n    APPEND,    // add the new value to existing value\n    DISTINCT,  // same as APPEND, but make sure each value is distinct\n    INCREMENT, // increment existing value.  Must be a number!\n    REMOVE     // remove the previous value.\n  }\n;\n\n  public boolean overwrite = true;\n  public SolrDocument doc;\n  public Map<SchemaField,MODE> mode; // What to do for each field.  null is the default\n  public int commitMaxTime = -1; // make sure the document is commited within this much time\n}\n\nRequestHandlers will need to fill up the 'mode' map if they want to support updateability.  Setting the mode.put( null, APPEND ) sets the default mode.\n\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12470043",
            "date": "2007-02-04T11:42:19+0000",
            "content": "I just attached a modified XmlUpdateRequestHandler that uses the new IndexDocumentCommand .  I left this out originally because I think the discussion around syntax and functionality should be separated...  BUT without some example, it is tough to get a sense how this would work, so i added this example.\n\nCheck the new file:\nmonitor-modifier.xml\n\nIt starts with:\n<add mode=\"cat=DISTINCT,features=APPEND,price=INCREMENT,sku=REMOVE,OVERWRITE\">\n<doc>\n  <field name=\"id\">3007WFP</field>\n  ...\n\nIf you run  ./post.sh  monitor-modifier.xml multiple times and check: http://localhost:8983/solr/select?q=id:3007WFP you should notice\n1) the price increments by 5 each time\n2) there is an additional 'feature' line each time\n3) the categories are distinct even if the input is not\n\nsku=REMOVE is required because sku is a stored field that is written to with copyField.  \n\nAlthough I think this syntax is reasonable, this is just an example intended to spark discussion.  Other things to consider:\n\n\n\trather then 'field=mode,' we could do 'field:mode,' this may look less like HTTP request parameter syntax\n\n\n\n\n\tThe update handler could skip any stored field that is the target of a 'copyField' automatically.  This is the most normal case, so it may be the most reasonable thing to do.\n\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12470061",
            "date": "2007-02-04T15:35:40+0000",
            "content": "Haven't had a chance to check out any code, but a few quick comments:\n\nIf the field modes were parameters, they could be reused for other update handlers like SQL or CSV\nPerhaps something like:\n/update/xml?modify=true&f.price.mode=increment,f.features.mode=append\n\n> sku=REMOVE is required because sku is a stored field that is written to with copyField.\nI'm not sure I quite grok what REMOVE means yet, and how it fixes the copyField problem.\n\nAnother way to work around copyField is to only collect stored fields that aren't copyField targets.  Then you run the copyField logic while indexing the document again (as you should anyway).\n\nI think I'll have a tagging usecase that requires removing a specific field value from a multivalued field.  Removing based on a regex might be nice too. though.\n\nf.tag.mode=remove\nf.tag.mode=removeMatching or removeRegex\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12470064",
            "date": "2007-02-04T16:11:54+0000",
            "content": "I browsed the code really quick, looking for the tricky part... It's here:\n+      openSearcher();\n+      Term t = new Term( uniqueKey.getName(), uniqueKey.getType().toInternal( id.toString() ) );\n+      int docID = searcher.getFirstMatch( t );\n\nWhen you overwrite a document, it is really just adds another instance... so the index contains multiple copies.  When we \"commit\", deletes of the older versions are performed.  So you really want the last doc matching a term, not the first.\n\nAlso, to need to make sure that the searcher you are using can actually \"see\" the last document (once a searcher is opened, it sees documents that were added since the last IndexWriter close().\n\nSo a quick fix would be to do a commit() first that would close the writer, and then delete any old copies of docments.\nOpening and closing readers and writers is very expensive though.\nYou can get slightly more sophisticated by checking the pset (set of pending documents), and skip the commit() if the doc you are updating isn't in there (so you know an older searcher will still have the freshest doc for that id).\n\nWe might be able to get more efficient yet in the future by leveraging NewIndexModifier: SOLR-124 "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12470065",
            "date": "2007-02-04T16:13:49+0000",
            "content": "Oh, so we obviously need some tests that modify the same document multiple times w/o a commit in between. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12470076",
            "date": "2007-02-04T19:21:29+0000",
            "content": "> \n> If the field modes were parameters, they could be reused for other update handlers like SQL or CSV\n> Perhaps something like:\n> /update/xml?modify=true&f.price.mode=increment,f.features.mode=append\n> \n\nYes.  I think we should decide a standard 'modify modes' syntax that can be used across handlers.  In this example, I am using the string:\n\nmode=cat=DISTINCT,features=APPEND,price=INCREMENT,sku=REMOVE,OVERWRITE\n\nand passing it to 'parseFieldModes' in XmlUpdateRequestHandler.\n\nPersonally, I think all the modes should be specified in a single param rather then a list of them.  I vote for a syntax like:\n\n<lst name=\"params\">\n <str name=\"mode\">cat=DISTINCT,features=APPEND,price=INCREMENT,sku=REMOVE,OVERWRITE</str>\n</lst>\n or:\n<lst name=\"params\">\n <str name=\"mode\">cat:DISTINCT,features:APPEND,price:INCREMENT,sku:REMOVE,OVERWRITE</str>\n</lst>\n\nrather then:\n\n<lst name=\"params\">\n <str name=\"f.cat.mode\">DISTINCT</str>\n <str name=\"f.features.mode\">APPEND</str>\n <str name=\"f.price.mode\">INCREMENT</str>\n <str name=\"f.sku.mode\">REMOVE</str>\n <str name=\"modify.default.mode\">OVERWRITE</str>\n</lst>\n\n\n\n>> sku=REMOVE is required because sku is a stored field that is written to with copyField.\n> I'm not sure I quite grok what REMOVE means yet, and how it fixes the copyField problem.\n> \n\nI'm using 'REMOVE' to say \"remove the previous value of this field before doing anything.\"  Essentially, this makes sure you new document does not start with a value for 'sku'.\n\n\n> Another way to work around copyField is to only collect stored fields that aren't copyField targets.  \n\nI just implemented this.  It is the most normal case, so it should be the default.  It can be overridden by setting the mode for a copyField explicitly.\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12470080",
            "date": "2007-02-04T19:44:47+0000",
            "content": "If modes are in a single param, a \":\" syntax might be nicer because you could cut-n-paste it into a URL w/o escaping.\n\n> I'm using 'REMOVE' to say \"remove the previous value of this field before doing anything.\" Essentially, this makes sure you new \n> document does not start with a value for 'sku'.\n\nThat wouldn't work for multi-valued fields though, right?\nIf we keep this option, perhaps we should find a better name... to me \"remove\" suggests that the given value should be removed.  Think adding / removing tag values from a document. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12470081",
            "date": "2007-02-04T19:49:59+0000",
            "content": "Is this what you are suggesting?\n\nI added a second searcher to DirectUpdatehandler2 that is only closed when you call commit();  \n\n\n  // Check if the document has not been commited yet\n  Integer cnt = pset.get( id.toString() );\n  if( cnt != null && cnt > 0 ) \n{\n    commit( new CommitUpdateCommand(false) );\n  }\n  if( committedSearcher == null ) \n{\n    committedSearcher = core.newSearcher(\"DirectUpdateHandler2.committed\");\n  }\n  Term t = new Term( uniqueKey.getName(), uniqueKey.getType().toInternal( id.toString() ) );\n  int docID = committedSearcher.getFirstMatch( t );\n  if( docID >= 0 ) \n{\n    Document luceneDoc = committedSearcher.doc( docID );\n    // set the new doc as the existingDoc + our changes\n    DocumentBuilder builder = new DocumentBuilder( schema );\n    add.doc = builder.bulid( \n       this.modifyDocument( luceneDoc, cmd.doc, cmd.mode ) );\n  }\n\n\n\n\t- - - - - -\n\n\n\nThis passes a new test that adds the same doc multiple times.  BUT it does commit each time.\n\nAnother alternative would be to keep a Map<String,Document> of the pending documents in memory.  Then we would not have to commit each time something has changed.\n\n\n\n\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12470082",
            "date": "2007-02-04T19:57:15+0000",
            "content": "\n> \n> That wouldn't work for multi-valued fields though, right?\n\n'REMOVE' on a mult-valued field would clear the old fields before adding the new ones.  It is essentially the same as OVERWRITE, but you may or may not pass in a new value on top of the old one.\n\n> If we keep this option, perhaps we should find a better name... \n\nhow about 'IGNORE' or 'CLEAR'  It is awkward because it refers to what was in in the document before, not what you are passing in.\n\nThe more i think about it, I think we should drop the 'REMOVE' option.  You can get the same effect using 'OVERWRITE' and passing in a null value. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12470084",
            "date": "2007-02-04T20:07:27+0000",
            "content": "> I added a second searcher to DirectUpdatehandler2 that is only closed when you call commit();\n\nThat's OK for right now, but longer term we should re-use the other searcher.\nPreviously, the searcher was only used for deletions, hence we always had to close it before we opened a writer.\nIf it's going to be used for doc retrieval now, we should change closeSearcher() to flushDeletes() and only really close the searcher if there had been deletes pending (from our current deleteByQuery  implementation). "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12470109",
            "date": "2007-02-05T01:51:44+0000",
            "content": "I added a new version of SOLR-139-IndexDocumentCommand.patch that:\n\n\tgets rid of 'REMOVE' option\n\tuses a separate searcher to search for existing docs\n\tincludes the XmlUpdateRequestHandler\n\tmoves general code from XmlUpdateRequestHandler to SolrPluginUtils\n\tadds a few more tests\n\n\n\nCan someone with a better lucene understanding look into re-useint the existing searcher as Yonik suggests above - I don't quite understand the other DUH2 implications.\n\nI moved the part that parses (and validates) field mode parsing into SolrPluginUtils.  This could be used by other RequestHandlers to parse the mode map.\n\nThe XmlUpdateRequestHandler in this patch should support all legacy calls except cases where overwritePending != overwriteCommitted.  There are no existing tests with this case, so it is not a problem from the testing standpoint.  I don't know if anyone is using this (mis?) feature.\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12471497",
            "date": "2007-02-08T21:51:45+0000",
            "content": "This is a minor change that keeps the OVERWRITE property if it is specified.  The previous version ignored the update mode if everthing was OVERWRITE. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12473863",
            "date": "2007-02-17T01:39:58+0000",
            "content": "added missing files.  this should is ready to check over if anyone has some time "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12478307",
            "date": "2007-03-06T08:16:54+0000",
            "content": "An updated and cleaned up versoin.  The big change is to the SolrDocument interface - rather then expose Maps directly, it hides them in an interface:\n\nhttp://solrstuff.org/svn/solrj/src/org/apache/solr/util/SolrDocument.java\n\nStill needs some test cases "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12482361",
            "date": "2007-03-20T10:43:06+0000",
            "content": "depending on SOLR-193 "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12492613",
            "date": "2007-04-30T00:31:33+0000",
            "content": "applies (almost cleanly) with trunk + SOLR-193 "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12505316",
            "date": "2007-06-15T17:14:53+0000",
            "content": "updated with trunk "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12505611",
            "date": "2007-06-17T19:36:54+0000",
            "content": "\n\tUpdated to work with trunk.\n\tmoved MODE enum to common\n\n\n\nThis could be added without affecting any call to the XmlUpdateHandler \u2013 (for now) this only works with the StaxUpdateHandler.  Adding it will help clarify some of the DocumentBuilder needs/issues... "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12509498",
            "date": "2007-07-02T06:59:55+0000",
            "content": "\n\tUpdated for new SolrDocument implementation.\n\tTesting via solrj.\n\n\n\nI think this should be committed soon.  It should only affect performance if you specify a 'mode' in the add command. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12509608",
            "date": "2007-07-02T16:25:23+0000",
            "content": "Another use case to keep in mind... someone might use an UpdateRequestProcessor\nto add new fields to a document (something like copyField, but more complex).  Some of this logic might be based on the value of other fields themselves.\n\nexample1:  a userTag field that represents tags on objects of the form user#tagstring.\nIf user==member, then add tagstring to the indexed-only ownerTags field, else\nadd the tagstring to the socialTags field.\n\nexample2: an UpdateRequestProcessor is used to encode the value of a field with rot13... this should obviously only be done for new field values, and not values that are just being re-stored, so the UpdateRequestProcessor needs to be able to distinguish between the two.\n\nexample3: some field values are pulled from a database when missing rather than being stored values.\n\nI think we need to try to find the right UpdateRequestProcessor interface to support all these with UpdateableDocuments. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12509615",
            "date": "2007-07-02T16:51:01+0000",
            "content": "\nSo you are suggesting pulling this out of the UpdateHandler and managing the document merging in the UpdateRequestProcessor?  (this might makes sense - It was not an option when the patch started in feb)  \n\nHow can the UpdateHandler get access to pending documents?  should it just use req.getSearcher()? \n\n\n> example1:  a userTag field that represents tags on objects of the form user#tagstring.\n> If user==member, then add tagstring to the indexed-only ownerTags field, else\n> add the tagstring to the socialTags field.\n> \n> example2: an UpdateRequestProcessor is used to encode the value of a field with rot13... this should obviously only be done for new field values, and not values that are just being re-stored, so the UpdateRequestProcessor needs to be able to distinguish between the two.\n> \n\n1 & 2 seem pretty straightforwad\n\n> example3: some field values are pulled from a database when missing rather than being stored values.\n> \n\nDo you mean as input or output?  The UpdateRequestProcessor could not affect if a field is stored or not, it could augment a document with more fields before it is indexed.  To add fields from a database rather then storing them, we would need a hook at the end. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12509622",
            "date": "2007-07-02T17:28:46+0000",
            "content": "> So you are suggesting [...]\n\nI don't have a concrete implementation idea, I'm just going over all the things I know people will want to do (and many of these I have an immediate use for).\n\n> Do you mean as input or output?\n\nInput, for index-only fields.  Normally field values need to be stored for an \"update\" to work, but we could also allow the user to get these field values from an external source.\n\n> we would need a hook at the end.\n\nYes, it might make sense to have more than one callback method per UpdateRequestProcessor\n\nOf course now that I finally look at the code, UpdateRequestProcessor isn't quite what I expected.\nI was originally thinking more along the lines of DocumentMutator(s) that manipulate a document, not that actually initiate the add/delete/udpate calls.  But there is a certain greater power to what you are exposing/allowing too (as long as you don't need multiple of them).\n\nIn UpdateRequestProcessor , instead of \n  protected final NamedList<Object> response;\nWhy not just expose SolrQueryRequest, SolrQueryResponse?\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12509631",
            "date": "2007-07-02T17:43:21+0000",
            "content": "> How can the UpdateHandler get access to pending documents? should it just use req.getSearcher()? \n\nI don't think so... the document may have been added more recently than the last searcher was opened.\nWe need to use the IndexReader from the UpdateHandler.  The reader and writer can both remain open as long as the reader is not used for deletions (which is a write operation and hence exclusive with IndexWriter).\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12509635",
            "date": "2007-07-02T18:18:07+0000",
            "content": "> (as long as you don't need multiple of them). \n\nIf you had a bunch of independent things you wanted to do, having a single processor forces you to squish them together (say you even added another document type to your index and then wanted to add another mutation operation).\n\nWhat if we had some sort of hybrid between the two approaches. Had a list of processors, and the last would have primary responsibility for getting the documents in the index? "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12510040",
            "date": "2007-07-04T01:41:43+0000",
            "content": "implementing modifiable documents in UpdateRequestProcessor.\n\nThis adds two example docs:\nmodify1.xml and modify2.xml\n\n<add mode=\"OVERWRITE,price:INCREMENT,cat:DISTINCT\">\n <doc>\n  <field name=\"id\">CSC</field>\n  <field name=\"name\">Campbell's Soup Can</field>\n  <field name=\"manu\">Andy Warhol</field>\n  <field name=\"price\">23.00</field>\n  <field name=\"popularity\">100</field>\n  <field name=\"cat\">category1</field>\n </doc>\n</add>\n\nwill increment the price by 23 each time it is run. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12512388",
            "date": "2007-07-13T07:44:52+0000",
            "content": "implements modifiable documents in the SOLR-269 update processor chain.\n\nIf the request does not have a 'mode' string, the ModifyDocumentProcessorFactory does not add a processor to the chain.\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12512553",
            "date": "2007-07-13T17:57:08+0000",
            "content": "Some general issues w/ update processors and modifiable documents, and keeping this stuff out of the update handler is that the udpate handler knows much more about the index than we do outside, and it constrains implementation (and performance optimizations).\n\nFor example, if modifiable documents were implemented in the update handler, and the old version of the document hasn't been committed yet, the update handler could buffer the complete modify command to be done at a later time (the much slower alternative is closing the writer and opening the reader to get the latest stored fields), then closing the reader and re-opening the writer. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12512555",
            "date": "2007-07-13T17:58:45+0000",
            "content": "Updated patch to work with SOLR-269 UpdateRequestProcessors.\n\nOne thing I think is weird about this is that it uses parameters to say the mode rather then the add command.  That is, to modify a documetn you have to send:\n\n/update?mode=OVERWRITE,count:INCREMENT\n<add>\n <doc>\n  <field name=\"id\">1</field>\n  <field name=\"count\">5</field>\n </doc>\n</add>\n\nrather then:\n<add mode=\"OVERWRITE,count:INCREMENT\">\n <doc>\n  <field name=\"id\">1</field>\n  <field name=\"count\">5</field>\n </doc>\n</add>\n\nThis is fine, but it makes it hard to have an example 'modify' xml document. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12512558",
            "date": "2007-07-13T18:09:58+0000",
            "content": "> the udpate handler knows much more about the index than we do outside\n\nYes.  The patch i just attached only deals with documents that are already commited.  It uses req.getSearcher() to find existing documents.  \n\nBeyond finding commited or non-commited Documents, is there anything else that it can do better?  \n\nIs it enought to add something to UpdateHandler to ask for a pending or commited document by uniqueId?\n\nI like having the the actual document manipulation happening in the Processor because it is an easy place to put in other things like grabbing stuff from a SQL database.   "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12512602",
            "date": "2007-07-13T20:08:19+0000",
            "content": "> I like having the the actual document manipulation happening in the Processor because it is an easy \n> place to put in other things like grabbing stuff from a SQL database.\n\nThe update handler could call the processor when it was time to do the manipulation too.\n\nConsider the case of future support for ParallelReader, where some fields are in one sub-index and some fields are in another sub-index.  I'm not sure if our current processor interfaces will be able to handle that scenario well (but I'm not sure if we should worry about it too much right now either).\n\nThere is another way to perhaps mitigate the problem of a few frequently modified documents causing thrashing: a document cache in the update handler. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12512613",
            "date": "2007-07-13T20:43:17+0000",
            "content": "> \n> The update handler could call the processor when it was time to do the manipulation too.\n> \n\nWhat are you thinking?  Adding the processor as a parameter to AddUpdateCommand?\n\n> ... ParallelReader, where some fields are in one sub-index  ...\n\nthe processor would ask the updateHandler for the existing document - the updateHandler deals with getting it to/from the right place.\n\nwe could add something like:\n  Document getDocumentFromPendingOrCommited( String indexId )\nto UpdateHandler and then that is taken care of.\n\nOther then extracting the old document, what needs to be done that cant be done in the processor?   "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12512617",
            "date": "2007-07-13T20:52:42+0000",
            "content": ">> ... ParallelReader, where some fields are in one sub-index ...\n> the processor would ask the updateHandler for the existing document - the updateHandler deals with\n> getting it to/from the right place.\n\nThe big reason you would use ParallelReader is to avoid touching the less-modified/bigger fields in one index when changing some of the other fields in the other index.\n\n> What are you thinking? Adding the processor as a parameter to AddUpdateCommand?\n\nI didn't have a clear alternative... I was just pointing out the future pitfalls of assuming too much implementation knowledge.\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12512628",
            "date": "2007-07-13T21:25:59+0000",
            "content": "> \n> ... avoid touching the less-modified/bigger fields ...\n> \n\naaah, perhaps a future updateHandler getDocument() function could take a list of fields it should extract.  Still problems with what to do when you add it.. maybe it checks if anything has changed in the less-modified index?  I see your point.\n\n>> What are you thinking? Adding the processor as a parameter to AddUpdateCommand?\n> \n> I didn't have a clear alternative... I was just pointing out the future pitfalls of assuming too much implementation knowledge.\n> \n\nI am fine either way \u2013 in the UpdateHandler or the Processors.  \n\nRequest plumbing-wise, it feels the most natural in a processor.  But if we rework the AddUpdateCommand it could fit there too.  I don't know if it is an advantage or disadvantage to have the 'modify' parameters tied to the command or the parameters.  either way has its +-, with no real winner (or loser) IMO\n\nIn the end, I want to make sure that I never need a custom UpdateHandler (80% is greek to me), but can easily change the 'modify' logic. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12513355",
            "date": "2007-07-17T20:59:02+0000",
            "content": "FYI, I'm working on a loadStoredFields() for UpdateHandler now. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12513654",
            "date": "2007-07-18T17:55:35+0000",
            "content": "Here's a patch that adds getStoredFields to updateHandler.\nChanges include leaving the searcher open as long as possible (and in conjunction with the writer, if the reader has no unflushed deletions).  This would allow reuse of a single index searcher when modifying multiple documents that are not in the pending set.\n\nNo tests for getStoredFields() yet... I plan on a comprehensive update handler test that uses multiple threads to try and flush out any possible concurrency bugs.  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12513681",
            "date": "2007-07-18T18:45:37+0000",
            "content": "updated getStoredFields.patch to use synchronization instead of the write lock since you can't upgrade a read lock to a write lock.  I could have started out with the write lock and downgraded it to a read lock, but that would probably lead to much worse concurrency since it couldn't proceed in parallel with other operations such as adds.\n\nIt would be good if someone could review this for threading issues. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12513685",
            "date": "2007-07-18T19:00:50+0000",
            "content": "Found another bug just by looking at it a little longer... pset needs to be synchronized if all we have is the read lock, since addDoc() actually changes the pset while only holding the readLock (but it also synchronizes).  I expanded the sync block to encompass the pset access and merged the two sync blocks.  Hopefully this one should be correct. "
        },
        {
            "author": "Mike Klaas",
            "id": "comment-12514099",
            "date": "2007-07-20T05:24:28+0000",
            "content": "It is my fault that the DUH2 locking is so hairy to begin with, so I should at least review changes to it \n\nWith your last change, the locking looks sound.  However, I noticed a few things:\n\nThis comment is now inaccurate:\n+    // need to start off with the write lock because we can't aquire\n+    // the write lock if we need to.\n\nShould openSearcher() call closeSearcher() instead of doing it manually?  It looks like searcherHasChanges is not being reset to false.\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12514234",
            "date": "2007-07-20T16:20:43+0000",
            "content": "Thanks Mike, I've made those changes to my local copy. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12514772",
            "date": "2007-07-23T20:24:47+0000",
            "content": "Here's the latest patch, with the beginnings of a test, but I've run into some issues.\n\n1) I'm getting occasional errors about the index writer already being closed (with lucene trunk), or null pointer exception with the lucene version we have bundled.  It's the same issue I believe... somehow the writer gets closed when someone else is trying to use it.  If I comment out verifyLatest() in the test (that calls getStoredFields), it no longer happens.\n\n2) When I comment out getStoredFields, things seem to run correctly, but memory grows without bound and i eventually run out.  100 threads each limiting themselves to manipulating 16 possible documents each should not be able to cause this.\n\nAt this point, I think #2 is the most important to investigate.  It may be unrelated to this latest patch, and could be related to other peoples reports of running out of memory while indexing. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12515154",
            "date": "2007-07-25T04:23:46+0000",
            "content": "Weird... I put a profiler on it to try and figure out the OOM issue, and I never saw heap usage growing over time (stayed between 20M and 30M), right up until I got an OOM (it never registered on the profiler). "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12515306",
            "date": "2007-07-25T14:27:36+0000",
            "content": "OOM still happens from the command line also after lucene updates to 2.2.\nLooks like it's time for old-school instrumentation (printfs, etc). "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12515366",
            "date": "2007-07-25T17:58:53+0000",
            "content": "I disabled logging on all of \"org.apache.solr\" via a filter, and voila, OOM problems are gone.\nPerhaps the logger could not keep up with the number of records and they piled up over time time (does any component of the logging framework use another thread that might be getting starved?)\n\nAnyway, it doesn't look like Solr has a memory leak.\nOn to the next issue. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12515406",
            "date": "2007-07-25T20:14:17+0000",
            "content": "The locking logic for getStoredFields() is indeed flawed.\nclosing the writer inside the sync block of getStoredFields() doesn't project callers of addDoc() from concurrently using that writer.  The commit lock aquire will be needed after all... no getting around it I think. "
        },
        {
            "author": "Mike Klaas",
            "id": "comment-12515424",
            "date": "2007-07-25T21:17:54+0000",
            "content": "Darn, you're right: writer.addDocument() is outside of the synchronized block.\n\nWe could do as you suggested, downgrading to a read lock from commit.  It should only reduce concurrently when the document is in pending state. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12516470",
            "date": "2007-07-30T18:36:31+0000",
            "content": "Attaching a patch for getStoredFields that appears to work. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12516473",
            "date": "2007-07-30T18:42:35+0000",
            "content": "So the big issue now is that I don't think we can use getStoredFields() and do document modification outside the update handler.  The biggest reason is that I think we need to be able to update documents atomically (in the sense that updates should not be lost).\n\nConsider the usecase of adding a new tag to a multi-valued field:  if two different clients tag a document at the same time, it doesn't seem acceptable that one of the tags could be lost.  So I think that we need a modifyDocument() call on updateHandler, and perhaps a ModifyUpdateCommand to go along with it.\n\nI'm not sure yet what this means for request processors.  Perhaps another method that handles the reloaded storedFields? "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12516598",
            "date": "2007-07-31T02:25:56+0000",
            "content": "\n> \n> So I think that we need a modifyDocument() call on updateHandler, and perhaps a ModifyUpdateCommand to go along with it.\n> \n> I'm not sure yet what this means for request processors.  Perhaps another method that handles the reloaded storedFields?\n>\n\nAnother option might be some sort of transaction or locking model.  Could it block other requests while there is an open transaction/lock?\n\nConsider the case where we need the same atomic protection for fields loaded from non-stored fields loaded from a SQL database.  In this case, it may be nice to have locking/blocking happen at the processor level.\n\nI don't know synchronized well enough to know if this works or is a bad idea, but what about something like:\n\nclass ModifyExistingDocumentProcessor {\n  void processAdd(AddUpdateCommand cmd) {\n    String id = cmd.getIndexedId(schema);\n    synchronized( updateHandler ) \n{\n       SolrDocument existing = updateHandler.loadStoredFields( id );\n       myCustomHelper.loadTagsFromDB( existing );\n       cmd.solrDoc = ModifyDocumentUtils.modifyDocument( ... );\n       \n       // This eventually calls: updateHandler.addDoc(cmd);\n       super.processAdd(cmd);\n    }\n  }\n}\n\nThis type of approach would need to make sure everyone modifying fields was locking on the same updateHandler.\n\n\n\t- - -\n\n\n\nI'm not against adding a ModifyUpdateCommand, I just like having the modify logic sit outside the UpdateHandler. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12517746",
            "date": "2007-08-05T06:16:51+0000",
            "content": "Updated to use Yonik's 'getStoredFields'.  The one change to that is to have UpdateHandler addFields() load fields as the Object (not external string) and to skip copy fields:\n\nvoid addFields(Document luceneDoc, SolrDocument solrDoc) {\n    for (Fieldable f : (List<Fieldable>)luceneDoc.getFields()) {\n      SchemaField sf = schema.getField( f.name() );\n      if( !schema.isCopyFieldTarget( sf ) ) \n{\n        Object externalVal = sf.getType().toObject(f);\n        solrDoc.addField(f.name(), externalVal);  \n      }\n    }\n  }\n\n\n\t- - -\n\n\n\nThis still implements modifiable documents as a RequestProcessor. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12519741",
            "date": "2007-08-14T18:14:16+0000",
            "content": "applies with trunk "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12520554",
            "date": "2007-08-17T13:24:24+0000",
            "content": "I'm experimenting with this patch with tagging.   I'm modeling the fields in this way, beyond general document metadata fields:\n\n   <username>_tags\n  usernames\n\nAnd copyFielding *_tags into \"tags\". \n\nusernames field allows seeing all users who have tagged documents.\n\nUsers are allowed to \"uncollect\" an object, which would remove the <username>_tags field and remove their name from the usernames field.   Removing the <username>_tags field use case is covered with the <username>_tags:OVERWRITE mode.  But removing a username from the multiValued and non-duplicating usernames field is not.\n\nAn example (from some conversations with Ryan): \n\n id: 10\n usernames: ryan, erik\n\nYou want to be able to remove 'ryan' but keep 'erik'.\n\nPerhaps we need to add a 'REMOVE' mode to remove the first (all?)\nmatching values\n /update?mode=OVERWRITE,username=REMOVE\n <doc>\n  id=10,\n  usernames=ryan\n </doc>\n\nand make the output:\n\n id: 10\n usernames: erik\n\nBut what about duplicate values?   "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12521900",
            "date": "2007-08-22T19:23:15+0000",
            "content": "One thing to note about overwrite and copyFields is that to keep a purely copyFielded field in sync you must basically remove it (overwrite without providing a value).   \n\nFor example, my schema:\n\n    <dynamicField name=\"*_tag\" type=\"string\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/>\n    <field name=\"tag\" type=\"string\" indexed=\"true\" stored=\"true\" multiValued=\"true\"/>\n\nand then this:\n  <copyField source=\"*_tag\" dest=\"tag\"/>\n\nThe client never provides a value for \"tag\" only ever <username>_tag values.   I was seeing old values in the tag field after doing overwrites of <username>_tag expecting \"tag\" to get rewritten entirely.   Saying mode=tag:OVERWRITE does the trick.    This is understandable, but confusing, as the client then needs to know about purely copyFielded fields that it never sends directly.  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12521903",
            "date": "2007-08-22T19:32:51+0000",
            "content": "IMO, copyField targets should always be re-generated... so no, it doesn't seem like you should have to say anything about tag if you update erik_tag.\n\nOn a side note, I'm not sure how scalable the field-per-user strategy is.  There are some places in Lucene (segment merging for one) that go over each field, merging the properties.  Low thousands would be OK, but millions would not be OK. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12521906",
            "date": "2007-08-22T19:44:19+0000",
            "content": "I agree in the gut feel to how copyFields and overwrite should work, but what about the situation where the client is sending in values for that field also?   If it got completely regenerated during a modify operation, data would be lost.  No?\n\nThanks for the note about field-per-user strategy.  In our case, even thousands of users is on down the road.   The simplicity of having fields per user is mighty alluring and is working nicely in the prototype for now.  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12521913",
            "date": "2007-08-22T19:56:21+0000",
            "content": "I think there are a number of restrictions for using this feature:\n1) all source fields (not copyField targets) need to be stored, or included in modify commands\n2) copyField targets should either be unstored, or if stored, should never be explicitly set by the user\n\nWhat tags would you send in that aren't user tags?  If they are some sort of global tags, then you could do global_tag=foo (reusing your dynamic user_tag field), or create a new globalTags field and an additional copyField to the tag field. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12521928",
            "date": "2007-08-22T20:59:16+0000",
            "content": "A mistake I had was my copyField target (\"tag\") was stored.  Setting it to be unstored alleviated the need to overwrite it - thanks!\n\nOne thing I noticed is that all fields sent in the update must be stored, but that doesn't really need to be the case with fields being overwritten -  perhaps that restriction should be lifted and only applies when the stored data is needed.\n\nAs for sending in a field that was the target of a copyField - I'm not doing this nor can I really envision this case, but it seemed like it might be a case to consider here.  Perhaps a \"text\" field that could be optionally set by the client, and is also the destination of copyField's of title, author, etc.  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12521932",
            "date": "2007-08-22T21:19:48+0000",
            "content": "> One thing I noticed is that all fields sent in the update must be stored, but that doesn't really need to be the case with fields being overwritten - perhaps that restriction should be lifted and only applies when the stored data is needed.\n\n+1\n\n> Perhaps a \"text\" field that could be optionally set by the client, and is also the destination of copyField's of title, author, etc.\n\nSeems like things of this form can always be refactored by adding a field \"settableText\" and adding another copyField from that to \"text\" "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12522310",
            "date": "2007-08-23T23:10:47+0000",
            "content": "There is a bug in the last patch that allows an update to a non-existent document to create a new document. \n\nI've corrected this by adding this else clause in ModifyExistingDocumentProcessor:\n\n      if( existing != null ) \n{\n        cmd.solrDoc = ModifyDocumentUtils.modifyDocument(existing, cmd.solrDoc, modes, schema );\n      }\n else \n{\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST,\n            \"Cannot update non-existent document: \" + id);\n      }\n\nI can't generate a patch, yet, that is clean addition of just that bit of code along with the other changes. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12522320",
            "date": "2007-08-24T00:02:31+0000",
            "content": "Some thoughts on the update request, how about using <update> instead of <add>?   And put the modes as attributes to <update>...\n\n   <update overwrite=\"title\" distinct=\"cat\">\n      <field name=\"id\">ID</field>\n      <field name=\"title\">new title</field>\n      <field name=\"cat\">NewCategory</field>\n    </update>\n\nUsing <add> has the wrong implication for this operation.   Thoughts? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12522333",
            "date": "2007-08-24T00:41:29+0000",
            "content": "> how about using <update> instead of <add>\n\nWe had previously talked about making this distinction (and configuration for each field) in the URL:\nhttp://localhost:8983/solr/update?mode=title:overwrite,cat:distinct\nThis makes it usable and consistent for different update handlers and formats (CSV, future SQL, future JSON, etc)\n\nbut perhaps if we allowed the <add> tag to optionally be called something more neutral like <docs>?\n\nwrt patches, I think the functionality needs refactoring so that modify document logic is in the update handler.  It seems like it's the only clean way from a locking perspective, and it also leaves open future optimizations (like using different indices depending on the fieldname and using a parallel reader across them). "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12522920",
            "date": "2007-08-27T01:32:12+0000",
            "content": "Added DELETE support in ModifyDocumentUtils like this:\n\n      case DELETE:\n        if( field != null ) {\n          Collection<Object> collection = existing.getFieldValues(name);\n          if (collection != null) {\n            collection.remove(field.getValue());\n            if (collection.isEmpty()) \n{\n              existing.removeField(name);\n            }\n else \n{\n              existing.setField(name, collection, field.getBoost());\n            }\n          }\n        }\n        // TODO: if field is null, should the field be deleted?\n        break; "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12524089",
            "date": "2007-08-31T14:24:58+0000",
            "content": "For posterity, here's the patch I'm running in Collex production right now, and its working fine thus far.\n\nSorry, I know this issue has a convoluted set of patches to follow at the moment.  I trust that when Ryan is back in action we'll get this tidied up and somehow get Yonik-satisfying refactorings  "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12525951",
            "date": "2007-09-08T20:30:09+0000",
            "content": "Updated Erik's patch to /trunk and added back the solrj modifiable document tests. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12525955",
            "date": "2007-09-08T20:52:11+0000",
            "content": "I'm back and have some time to focus on this...  Internally, I need to move the document modification to the update handler, but before I get going it would be nice to agree what we want the external interface to look like.\n\n\n\t- -\n\n\n\nShould we deprecate the AddUpdateCommand and replace it with something else? Do we want one command to do Add/Update/Modify?  Two?\n\n\n\t- -\n\n\n\nwhat should happen if you \"modify\" a non-existent document?\na. error \u2013 makes sense in a 'tagging' context\nb. treat it as an add \u2013 makes sense in a \"keep these fields up to date\" context (i don't want to check if the document already exists or not)\n\nI happen to be working with context b, but I suspect 'a' makes more sense.\n\n\n\t- -\n\n\n\nShould we have different xml syntax for document modification vs add?  Erik suggested:\n\n<update overwrite=\"title\" distinct=\"cat\">\n ...\n</update>\n\nsince 'update' is already used in a few ways, maybe <modify>?\n\nShould we put the id as an xml attribute?  this would make it possible to change the unique key.\n  <modify id=\"ID\">\n   <field name=\"id\">new id</field>\n  </modify>\nThat may look weird if the uniqueKeyField is not called \"id\"\n\nAssuming we put the modes as attributes, I guess multiple fields would be comma delimited?\n <modify distinct=\"cat,keyword\">\n\nDo you like the default mode called \"default\" or \"mode\"?\n<modify id=\"ID\" default=\"overwrite\">?\n<modify id=\"ID\" mode=\"overwrite\">?\n\n\n\n\n\n\n\n\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12526272",
            "date": "2007-09-10T21:25:38+0000",
            "content": "applies to /trunk "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12529710",
            "date": "2007-09-22T22:43:35+0000",
            "content": "applies with trunk... "
        },
        {
            "author": "J\u00f6rg Kiegeland",
            "id": "comment-12544856",
            "date": "2007-11-22T17:53:59+0000",
            "content": "A useful feature would be \"update based on query\", so that documents matching the query condition will all be modified in the same way on the given update fields. \nThis feature would correspond to SQL's UPDATE command, so that Solr would now cover all the basic commands SQL provides.  (While this is a theoretic motivation, I just missed this feature for my Solr projects..) "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12564884",
            "date": "2008-02-01T19:09:48+0000",
            "content": "updated to work with trunk.  no real changes.\n\nThe final version of this will need to move updating logic out of the processor into the UpdateHandler "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12564896",
            "date": "2008-02-01T19:25:24+0000",
            "content": "I'm having second thoughts if this is a good enough approach to really put in core Solr.\nRequiring that all fields be stored is a really large drawback, esp for large indicies with really large documents. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12564910",
            "date": "2008-02-01T19:52:30+0000",
            "content": "that is part of why i thought having it in an update request processor makes sense \u2013 it can easily be subclassed to pull the existing fields from whereever it needs.  Even if it is directly in the UpdateHandler, there could be some interface to loadExistingFields( id ) or something similar. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12573906",
            "date": "2008-02-29T19:10:18+0000",
            "content": "I'm looking at this issue now.  I used to think \"sure, this will work fine\", but I'm looking at a 1B doc index (split over N servers) and am suddenly very scared of having to store more than necessary in the index.  In other words, writing custom field value loaders from external storage sounds like the right thing to do.  Perhaps one such loader could simply load from the index itself.  "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12582401",
            "date": "2008-03-26T19:08:38+0000",
            "content": "If I may comment?  \n\nIs it ok if a first implementation requires all fields to stored, and then a later iteration supports non-stored fields?  This seems to be a complex problem, and you might decide later that the first design is completely bogus.\n\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12586372",
            "date": "2008-04-07T13:03:32+0000",
            "content": "updated patch to work with trunk "
        },
        {
            "author": "David Smiley",
            "id": "comment-12591317",
            "date": "2008-04-22T14:49:36+0000",
            "content": "I agree Lance.  I don't know what the \"first design\" is that might be bogus you're talking about... but we definitely eventually want to handle non-stored fields.  I have an index that isn't huge and I'm salivating at the prospects of doing an update.  For non-stored fields... it seems that if an update always over-writes such fields with new data then we should be able to support that now easily because we don't care what the old data was.\n\nFor non-stored fields that need to be retained (Otis & Yoniks concern)... I wonder what Lucene exposes about the indexed data for a non-stored field.  We'd just want to copy this low-level data over to a new document, basically. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12591323",
            "date": "2008-04-22T15:07:06+0000",
            "content": "\nFor non-stored fields that need to be retained (Otis & Yoniks concern)... I wonder what Lucene exposes about the indexed data for a non-stored field. We'd just want to copy this low-level data over to a new document, basically.\n\nLucene maintains an inverted index, so the \"indexed\" part is spread over the entire index (terms point to documents).  Copying an indexed field would require looping over every indexed term (all documents with that field).  It would be very slow once an index got large. "
        },
        {
            "author": "Bill Au",
            "id": "comment-12601408",
            "date": "2008-05-31T19:36:37+0000",
            "content": "I noticed that this bug is no longer included in the 1.3 release.  Are there any outstanding issues if all the fields are stored?  Requiring that all fields are stored for a document to be update-able seems like reasonable to me.  This feature will simplify things for Solr users who are doing a query to get all the fields following by an add when they only want to update a very small number of fields. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12601614",
            "date": "2008-06-02T12:27:22+0000",
            "content": "the biggest reason this patch won't work is that with SOLR-559, the DirectUpdateHandler2 does not keep track of pending updates \u2013 to get this to work again, it will need to maintain a list somewhere.\n\nAlso, we need to make sure the API lets you grab stored fields from somewhere else \u2013 as is, it forces you to store all fields for all documents. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12616399",
            "date": "2008-07-24T09:05:26+0000",
            "content": "How about maintaining a separate index at store.index and write all the documents to that index also. \n\nIn the store.index , all the fields must be stored and none will be indexed. This index will not have any copy fields . It will blindly dump the data as it is.\n\nUpdating can be done by reading data from this.  Deletion must also be done on both the indices\n\nThis way the original index will be small and the users do not have to make all fields stored "
        },
        {
            "author": "David Smiley",
            "id": "comment-12616467",
            "date": "2008-07-24T12:38:48+0000",
            "content": "It's unclear to me how your suggestion, Paul, is better.  If at the end of the day, all the fields must be stored on disk somewhere (and that is the case), then why complicate matters and split out the storage of the fields into a separate index?  In my case, nearly all of my fields are stored so this would be redundant.  AFAIK, having your \"main\" index \"small\" only matters as far as index storage, not stored-field storage.  So I don't get the point in this. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12616564",
            "date": "2008-07-24T18:20:22+0000",
            "content": "There are many approaches to make this work \u2013 i don't think there will be a one-size-fits all approach though.  Storing all fields in the lucene index may be fine.  Perhaps we want to write the xml to disk when it is indexed, then reload it when the file is 'updated', perhaps the content should be stored in a SQL db.\n\nDavid - storing all data in the search index can be a problem because it can get BIG.  Imagine if nutch stored the raw content in the lucene index?  (I may be wrong on this) even with Lazy loading, there is a query time cost to having stored fields.\n\nIn general, I think we just need an API that will allow for a variety of storage mechanisms. "
        },
        {
            "author": "David Smiley",
            "id": "comment-12616570",
            "date": "2008-07-24T18:35:57+0000",
            "content": "Ryan, I know of course the index can get big because one needs to store all the data for re-indexing; but due to Lucene's fundamental limitations, we can't get around that fact.  Moving the data off to another place (a DB of some sort or whatever) doesn't change the fundamental problem.  If one is unwilling to store a copy somewhere convenient due to data scalability issues then we simply cannot support the feature because Lucene doesn't have an underlying update capability.\n\nIf the schema's field isn't stored, then it may be useful to provide an API that can fetch un-stored fields for a given document.  I don't think it'd be common to use the API and definitely wouldn't be worth providing a default implementation. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12616725",
            "date": "2008-07-25T00:38:22+0000",
            "content": "It's possible to recover unstored fields, if the purpose of such recovery is to make a copy of the document and update other fields. The process is time-consuming, because you need to traverse all postings for all terms, so it might be impractical for larger indexes. Furthermore, such recovered content may be incomplete - tokens may have been changed or skipped/added by analyzers, positionIncrement gaps may have been introduced, etc, etc.\n\nMost of this functionality is implemented in Luke \"Restore & Edit\" function. Perhaps it's possible to implement a new low-level Lucene API to do it more efficiently. "
        },
        {
            "author": "Mike Klaas",
            "id": "comment-12616729",
            "date": "2008-07-25T01:01:57+0000",
            "content": "[quote]David - storing all data in the search index can be a problem because it can get BIG. Imagine if nutch stored the raw content in the lucene index? (I may be wrong on this) even with Lazy loading, there is a query time cost to having stored fields.[/quote]\n\nSplitting it out into another store is much better at scale.  A distinct lucene index works relatively well.\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12616762",
            "date": "2008-07-25T04:33:58+0000",
            "content": "\n\tIf your index is really big most likely you may have a master/slave deployment. In that case only master needs to store the data copy. The slaves do not have to pay the 'update tax'\n\n\n\nPerhaps we want to write the xml to disk when it is indexed, then reload it when the file is 'updated', perhaps the content should be stored in a SQL db. \n\nWriting the xml may not be an option if I use DIH for indexing (there is no xml). And what if I use CSV. That means multiple storage formats. RDDBMS storage is a problem because of incompatibility of Lecene/RDBMS data structures. Creating a schema will be extremely hard because of dynamic fields\n\nI guess we can have multiple solutions. I can provide a simple DuplicateIndexUpdateProcessor (for lack of a better name) which can store all the data in a duplicate index. Let the user decide what he wants\n\nIt's possible to recover unstored fields, if the purpose of such recovery is to make a copy of the document and update other fields.\nIt is not wise to invest our time to do 'very clever' things because it is error prone. Unless Lucene gives us a clean API to do so "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12623383",
            "date": "2008-08-18T15:42:58+0000",
            "content": "Shall I open another issue on my idea keeping another index for just stored fields (in an UpdateRequestProcessor). \n\nIs it a good idea to have multiple approaches for the same feature? \nOr should I post the patch in this issue only? "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12623385",
            "date": "2008-08-18T15:50:37+0000",
            "content": "to me the key is getting an interface that would allow for the existing fields to be stored a number of ways:\n\n\twithin the index itself\n\twithin an independent index (as you suggest)\n\twithin SQL\n\ton the file system\n\t...\n\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12623416",
            "date": "2008-08-18T17:44:28+0000",
            "content": "there are pros and cons w/ each approaches (am I discovering a universal truth here  )\n\nMany approaches can to confuse users . I can propose something like \n\n<modifiable>true</modifiable> in the manIndex .\nAnd say \n<modificationStrategy>solr.SepareteIndexStrategy</modificationStrategy>\nor\n<modificationStrategy>solr.SameIndexStrategy</modificationStrategy>\n\n(I did not mention  the other two because of personal preferences   ) "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12670783",
            "date": "2009-02-05T15:10:11+0000",
            "content": "Marking for 1.5 "
        },
        {
            "author": "Marcus Herou",
            "id": "comment-12702778",
            "date": "2009-04-25T20:49:12+0000",
            "content": "It would make sense of adding ParallelReader functionality so a core can read from several index-dirs. \nGuess it complicates things a little since you would need to have support for adding data as well to more than one index.\n\nSuggestion:\n/update/coreX/index1 - Uses schema1.xml\n/update/coreX/index2 - Uses schema2.xml\n/select/coreX - Uses all schemas e.g. A ParallelReader.\n\nSeing quite a lot questions on the mailinglist about users that want to be able to update a single field while maintaining the rest of the index intact (not reindex).\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12702883",
            "date": "2009-04-26T13:23:42+0000",
            "content": "ParallelReader assumes you have two indexes that \"line up\" so the internal docids match.  Maintaining something like that would currently be pretty hard or impractical. "
        },
        {
            "author": "Mark Diggory",
            "id": "comment-12787072",
            "date": "2009-12-07T19:40:53+0000",
            "content": "I notice this is a very long lived issue and that it is marked for 1.5.  Are there outstanding issues or problems with its usage if I apply it to my 1.4 source? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12872416",
            "date": "2010-05-27T22:04:49+0000",
            "content": "Bulk updating 240 Solr issues to set the Fix Version to \"next\" per the process outlined in this email...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E\n\nSelection criteria was \"Unresolved\" with a Fix Version of 1.5, 1.6, 3.1, or 4.0.  email notifications were suppressed.\n\nA unique token for finding these 240 issues in the future: hossversioncleanup20100527 "
        },
        {
            "author": "Ravish Bhagdev",
            "id": "comment-12987837",
            "date": "2011-01-27T23:55:49+0000",
            "content": "This seems like a very old issue as someone suggested.  Is there any update on whether it will ever be resolved?  It is quite important feature and causes big problems when you have a huge index and only need to update one column. "
        },
        {
            "author": "Gerd Bremer",
            "id": "comment-13021007",
            "date": "2011-04-18T12:43:23+0000",
            "content": "I need this feature. How much do I have to pay in order to get this issue fixed? Can I pass around a piggy bank? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13269007",
            "date": "2012-05-05T16:45:09+0000",
            "content": "Here's a patch for updateable docs that reuses the infrastructure we put in place around versioning and realtime-get.\n\nOnly the JSON parser is currently implemented.\n\nRecall that a normal field value in JSON is of the form:\n\n\"myfield\":10\n\n\n\nThis patch extends the JSON parser to support extended values as a Map.  For example, to add an additional value to a multi-valued field, you would use\n\n\"myfield\":{\"add\":10}\n\n\n\nUsing an existing data structure (a Map) should allow us to pass this through javabin format w/o any additional changes.\n\nThis patch depends on optimistic locking (it's currently included in this patch) and updating documents fully supports optimistic locking (i.e. you can conditionally update a document based on it's version)\n\nRight now only \"add\" and \"set\" are supported as mutators (and setting a null value is like \"remove\"), but I figured it would be best to do a slice first from start to finish.\n\nComments? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13269042",
            "date": "2012-05-05T19:38:10+0000",
            "content": "I'm working in getting an XML syntax going.  The easiest/least disruptive seems to be an attribute:\n\nCurrent:\n\n  <field name=\"foo\" boost=\"2.5\">100</foo>\n\n\n\nProposed:\n\n  <field name=\"foo\" boost=\"2.5\" update=\"add\">100</foo>\n\n "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13269078",
            "date": "2012-05-05T22:55:57+0000",
            "content": "Cool. Any plans for supporting modification of existing value? Most useful would be add, subtract (for numeric) and append text for textual. (In FAST ESP we had this as part of the partial update APIs) "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13269081",
            "date": "2012-05-05T23:07:11+0000",
            "content": "Cool. Any plans for supporting modification of existing value?\n\nDefinitely!\n\n\tincrement or inc (add is taken for adding additional field values).  decrement not needed (just use a negative increment)\n\tappend/prepend (and maybe allow \"add\" to mean \"append\" if a text/string field is multi-valued)\n\tset operations for multi-valued fields (union, intersection, remove, etc)\n\twe could get into conditionals, but at some point we should just punt that to a script-updator (i.e. update this document using the given script)\n\n "
        },
        {
            "author": "David Smiley",
            "id": "comment-13269101",
            "date": "2012-05-06T01:41:23+0000",
            "content": "Yonik; I don't see your patch.  Will this support the ability to replace a text field that is indexed?  If so, what is the essence of the implementation?  The most common use-case I need for my employer involves essentially a complete re-indexing of all docs but only for a few particular fields. I tend to think, at least for my use-case, that it is within reach if I had enough time to work on it.  The implementation concept I have involves building parallel segments with aligned docids.  Segment merging would clean out older versions of a field. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13269104",
            "date": "2012-05-06T01:59:59+0000",
            "content": "Yonik; I don't see your patch. \n\nTry sorting by date, or click on the \"All\" tab and you can see where I added it.\nhttps://issues.apache.org/jira/secure/attachment/12525736/SOLR-139.patch\n\nwhat is the essence of the implementation?\n\nThis is the simplest form.  Original fields need to be stored.  The document is retrieved and then re-indexed after modification. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13269191",
            "date": "2012-05-06T11:41:51+0000",
            "content": "David, please see LUCENE-3837 for a low-level partial update of inverted fields without re-indexing other fields. That is very much work in progress, and it's more complex. This issue provides a shortcut to a \"retrieve stored fields, modify, delete original doc, add modified doc\" sequence that users would have to execute manually. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13269683",
            "date": "2012-05-07T15:14:22+0000",
            "content": "Here's an updated patch with XML support and cloud tests.\n\nThe underlying mechanism for updating a field always change in the future (and depending on the field type), so the really important part of this is the API.\n\nI plan on committing soon unless someone comes does come up with some API improvements. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13270612",
            "date": "2012-05-08T17:03:13+0000",
            "content": "Committed (5 years after the issue was opened!)\n\nI'll keep this issue open and we can add follow-on patches to implement increment and other set operations. "
        },
        {
            "author": "Seshagiri Nuthalapati",
            "id": "comment-13282202",
            "date": "2012-05-24T05:29:31+0000",
            "content": "yonik,  I see a small issue with this.\n\n\n\tI have added one of the example document from exampledocs folder.  The schema has \"price\" and \"price_c\" (copy field from price)\n\n\n\n1.When i add the document, it looks as follows:\n\n<str name=\"id\">TWINX2048-3200PRO</str>\n<str name=\"name\">CORSAIR  XMS 2GB (2 x 1GB) 184-Pin DDR SDRAM Unbuffered DDR\n                    400 (PC 3200) Dual Channel Kit System Memory - Retail\n</str>\n<str name=\"manu\">Corsair Microsystems Inc.</str>\n<float name=\"price\">185.0</float>\n<arr name=\"price_c\">\n   <str>185,USD</str>\n</arr>\n\n2. Now I want to set price field with 100 , so I sent a json for it.\n[{\"id\":\"TWINX2048-3200PRO\",\"price\":{\"set\":100}}]\n\nNow the document looks as follows:\n<str name=\"id\">TWINX2048-3200PRO</str>\n<str name=\"name\">CORSAIR  XMS 2GB (2 x 1GB) 184-Pin DDR SDRAM Unbuffered DDR\n                    400 (PC 3200) Dual Channel Kit System Memory - Retail\n</str>\n<str name=\"manu\">Corsair Microsystems Inc.</str>\n\n<float name=\"price\">100.0</float>\n<arr name=\"price_c\">\n      <str>100,USD</str>\n      <str>185,USD</str>\n</arr>\n\n\nas you can see, the old price value still there in the \"price_c\". Is there a workaround/patch we can do for this? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13282532",
            "date": "2012-05-24T14:43:26+0000",
            "content": "The schema has \"price\" and \"price_c\" (copy field from price)\n\nFor this feature to work correctly, source fields (the ones you normally send in) should be stored, and copyField targets (like price_c) should be un-stored. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13397739",
            "date": "2012-06-20T18:30:45+0000",
            "content": "Yonik,\n\nit's hard to follow for me. Can't you clarify what's actually can be updated stored/indexed field, field cache? Where update will be searchable? \n\nThanks "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13413687",
            "date": "2012-07-13T12:29:46+0000",
            "content": "Per the discussion on the mailing list, here's a patch that creates the document being updated if it doesn't exist already.  The standard optimistic concurrency mechanism can be used to specify that a document must exist if desired. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13414324",
            "date": "2012-07-14T09:06:35+0000",
            "content": "The create-if-not-exist patch was committed to both trunk and 4x branch.\nhttp://svn.apache.org/viewvc?rev=1361301&view=rev "
        },
        {
            "author": "Christopher Ball",
            "id": "comment-13442472",
            "date": "2012-08-27T15:34:31+0000",
            "content": "Yonik,\n\nDo you have an example with the XML syntax? I have been trying to test this in 4.0-Beta, but am obviously not grokking the right syntax =(\nAlso, have you tried to use this with a join query? I can think of some interesting use cases =)\n\nRegards "
        },
        {
            "author": "Dilip Maddi",
            "id": "comment-13443446",
            "date": "2012-08-28T19:40:26+0000",
            "content": "Christopher, \nHere is how I am able to update a document by posting an XML\n<add>\n  <doc>\n    <field name=\"id\">VA902B</field>\n    <field name=\"price\" update=\"set\">300</field>\n  </doc>\n</add> "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13456391",
            "date": "2012-09-15T12:49:26+0000",
            "content": "Unassigned issues -> 4.1 "
        },
        {
            "author": "Sean Timm",
            "id": "comment-13457326",
            "date": "2012-09-17T21:07:36+0000",
            "content": "It appears that SolrJ does not yet (as of 4.0 Alpha) support updating fields in a document.  Is there a separate Jira ticket for this? "
        },
        {
            "author": "Mike Lissner",
            "id": "comment-13483495",
            "date": "2012-10-24T19:20:07+0000",
            "content": "Can we get this on the Wiki somewhere? I've been looking around, haven't been able to find it. Not sure where to put it... "
        },
        {
            "author": "Matt Altermatt",
            "id": "comment-13483496",
            "date": "2012-10-24T19:22:11+0000",
            "content": "\ufeff\nI will be out of the office until the 29th of October.\n\nIf you need immediate assistance, please contact IT Integration (itintegration@paml.com) or my manager Jon Tolley (jtolley@paml.com).\n\nThanks.\n\nPAML EMAIL DISCLAIMER:\nInformation contained in this message may be privileged and confidential. \nIf the reader of this message is not the intended recipient, be notified \nthat any dissemination, distribution or copying of this communication is \nstrictly prohibited. If this communication is received in error, please \nnotify the sender immediately by replying to the message and deleting \nfrom your computer. Thank you "
        },
        {
            "author": "Joel Nothman",
            "id": "comment-13485544",
            "date": "2012-10-28T01:36:53+0000",
            "content": "Hi,\n\nI'm a fan of the feature, but not really a fan of the syntax, for the following reasons:\n\n\n\tIt is extremely verbose for batch update operations, e.g. setting a new field on all documents in the index. Surely the update modes should be specified outside of each individual record (either as URL query parameters, or in some content header/wrapper). The current approach is entirely inappropriate for extending to CSV, which might otherwise be an obvious choice of format when  adding a single field to each of a set of objects.\n\tThe distinction between an \"insert\" and an \"update\" operation (in SQL terms) is implicit, only indicated by the presence of an object in a JSON value, or by the presence of update in any one of the specified fields. Since insert and update operations are quite distinct on the server, it should select between these on a per-request basis, not per-record.\n\tThe JSON syntax would appear as if one could extend \n{\"set\":100}\n to \n{\"set\":100,\"inc\":2}\n on the same field, which is nonsense. It uses JSON a object for inappropriate semantics, where what one actually means is \n{\"op\":\"set\",\"val\":100}\n, or even \n{\"name\":\"price\",\"op\":\"set\",\"val\":100}\n.\n\tIt may be worth reserving JSON-object-as-value for something more literal in the future.\n\n "
        },
        {
            "author": "Jack Krupansky",
            "id": "comment-13503003",
            "date": "2012-11-23T03:44:22+0000",
            "content": "Thanks! "
        },
        {
            "author": "Lukas Graf",
            "id": "comment-13504540",
            "date": "2012-11-27T11:10:25+0000",
            "content": "This feature doesn't work as advertised in Solr 4.0.0 (final).\n\nSince it's not documented, I used the information in these blog posts (yonik.com, solr.pl) and this ticket to try to get it working, and asked in the #solr IRC channel, to no avail.\n\nWhenever I use the 'set' command in an update message, it mangles the value to something like \n\n<str name=\"Title\">{set=My new title}</str>\n\n , and drops all other fields.\n\nI tried the JSON as well as the XML Syntax for the update message, and I tried it with both a manually defined 'version' field and without.\n\nRelevant parts from my schema.xml:\n\n\n<schema name=\"solr-instance\" version=\"1.4\">\n\n    <fields>\n      <field name=\"Creator\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"false\" multiValued=\"false\" />\n      <!-- ... -->\n      <field name=\"Title\" type=\"text\" indexed=\"true\" stored=\"true\" required=\"false\" multiValued=\"false\" />\n      <field name=\"UID\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" />\n      <field name=\"_version_\" type=\"long\" indexed=\"true\" stored=\"true\" required=\"false\" multiValued=\"false\" />\n    </fields>\n\n    <!-- ... -->\n\n    <uniqueKey>UID</uniqueKey>\n\n</schema>\n\n\n\nI initially created some content like this:\n\n\n$ curl 'localhost:8983/solr/update?commit=true' -H 'Content-type:application/json' -d '[{\"UID\":\"7cb8a43c\",\"Title\":\"My original Title\", \"Creator\": \"John Doe\"}]'\n\n\n\nWhich resulted in this document:\n\n\n<doc>\n    <str name=\"UID\">7cb8a43c</str>\n    <str name=\"Title\">My original Title</str>\n    <str name=\"Creator\">John Doe</str>\n</doc>\n\n\n\nThen I tried to update that document with this statement:\n\n\n$ curl 'localhost:8983/solr/update?commit=true' -H 'Content-type:application/json' -d '[{\"UID\":\"7cb8a43c\",\"Title\":{\"set\":\"My new title\"}}]'\n\n\n\nWhich resulted in this mangled document:\n\n\n<doc>\n    <str name=\"UID\">7cb8a43c</str>\n    <str name=\"Title\">{set=My new title}</str>\n</doc>\n\n\n\n(I would have expected the document to still have the value 'John Doe' for the 'Creator' field,\nand have the value of its 'Title' field update to 'My new title')\n\nI tried using the XML format for the update message as well:\n\n\n<add>\n   <doc>\n       <field name=\"UID\">7cb8a43c</field>\n       <field name=\"Title\" update=\"set\">My new title</field>\n   </doc>\n</add>\n\n\n\nSame result as above. "
        },
        {
            "author": "Jack Krupansky",
            "id": "comment-13504724",
            "date": "2012-11-27T16:20:04+0000",
            "content": "I just tried it and the feature does work as advertised. If there is a bug, that should be filed as a separate issue. If there is a question or difficulty using the feature, that should be pursued on the Solr user list.\n\nFor reference, I took a fresh, stock copy of the Solr 4.0 example, no changes to schema or config, and added one document:\n\n\ncurl 'localhost:8983/solr/update?commit=true' -H 'Content-type:application/json' -d '\n[{\"id\":\"id-123\",\"title\":\"My original Title\", \"content\": \"Initial content\"}]'\n\n\n\nI queried it and it looked fine.\n\nI then modified only the title field:\n\n\ncurl 'localhost:8983/solr/update?commit=true' -H 'Content-type:application/json' -d '\n[{\"id\":\"id-123\",\"title\":{\"set\":\"My new title\"}}]'\n\n\n\nI tried the XML equivalents and that worked fine as well, with the original content field preserved. "
        },
        {
            "author": "Lukas Graf",
            "id": "comment-13505035",
            "date": "2012-11-27T22:39:20+0000",
            "content": "Thanks for your response. I thought I had the issue reduced to a simple enough test case, but apparently not. I will try again with a clean stock Solr 4.0, and file a seperate issue if necessary, or look for support on the mailing list. My choice of words ('doesn't work as advertised') might have been influenced by frustration about the lack of documentation, sorry for the noise. "
        },
        {
            "author": "Jack Krupansky",
            "id": "comment-13505040",
            "date": "2012-11-27T22:50:09+0000",
            "content": "No apology necessary for the noise. I mean, none of us was able to offer a prompt response to earlier inquiries and this got me focused on actually trying the feature for the first time. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13505054",
            "date": "2012-11-27T23:21:58+0000",
            "content": "about the lack of documentation\n\nSince you are eating some of this pain, perhaps you could give a hand when you have it figured out and contribute to our wiki? http://wiki.apache.org/solr/ "
        },
        {
            "author": "Lukas Graf",
            "id": "comment-13506298",
            "date": "2012-11-29T07:47:28+0000",
            "content": "Certainly, to the extent that I can. "
        },
        {
            "author": "Lukas Graf",
            "id": "comment-13506392",
            "date": "2012-11-29T10:58:14+0000",
            "content": "Ok, I finally figured it out by diffing every single difference from my test case to the stock Solr 4.0 example using git bisect.\n\nThe culprit was a missing <updateLog /> directive in solrconfig.xml. As soon as I configured a transaction log, atomic updates worked as expected. I added a note about this at http://wiki.apache.org/solr/UpdateXmlMessages#Optional_attributes_for_.22field.22 . "
        },
        {
            "author": "Jack Krupansky",
            "id": "comment-13506520",
            "date": "2012-11-29T15:06:38+0000",
            "content": "Oh, yeah, that. I actually was going to mention it, but I wanted to focus on running with the stock Solr example first. Actually, we need to look a little closer as to why/whether the <updateLog> directive is really always needed for partial document update. That should probably be a separate Jira issue. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13506533",
            "date": "2012-11-29T15:28:30+0000",
            "content": "we need to look a little closer as to why/whether the <updateLog> directive is really always needed for partial document update.\n\nI believe yonik chose to implement it by using updateLog features. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13506735",
            "date": "2012-11-29T19:56:59+0000",
            "content": "I believe yonik chose to implement it by using updateLog features.\n\ni think it has to be - the \"real time get\" support provided by the updateLog is the only way to garuntee that the document will be available to atomicly update it.\n\nLukas: if the atomic update code path isn't throwing a big fat error if you try to use it w/o updateLog configured then that sounds to me like a bug \u2013 can you please file a Jira for that "
        },
        {
            "author": "Lukas Graf",
            "id": "comment-13506942",
            "date": "2012-11-29T23:35:08+0000",
            "content": "Filed SOLR-4127: Atomic updates used w/o updateLog should throw an error "
        },
        {
            "author": "Abhinav Shah",
            "id": "comment-13547021",
            "date": "2013-01-08T17:11:58+0000",
            "content": "I am using apache-solr 4.0.\nI am trying to post the following document - \n\ncurl http://irvis016:8983/solr/collection1/update?commit=true -H \"Content-Type: text/xml\" --data-binary '<add commitWithin=\"5000\"><doc boost=\"1.0\"><field name=\"accessionNumber\" update=\"set\">3165297</field><field name=\"status\" update=\"set\">ORDERED</field><field name=\"account.accountName\" update=\"set\">US LABS DEMO ACCOUNT</field><field name=\"account.addresses.address1\" update=\"set\">2601 Campus Drive</field><field name=\"account.addresses.city\" update=\"set\">Irvine</field><field name=\"account.addresses.state\" update=\"set\">CA</field><field name=\"account.addresses.zip\" update=\"set\">92622</field><field name=\"account.externalIds.sourceSystem\" update=\"set\">10442</field><field name=\"orderingPhysician.lcProviderNumber\" update=\"set\">60086</field><field name=\"patient.lpid\" update=\"set\">5571351625769103</field><field name=\"patient.patientName.lastName\" update=\"set\">test</field><field name=\"patient.patientName.firstName\" update=\"set\">test123</field><field name=\"patient.patientSSN\" update=\"set\">643522342</field><field name=\"patient.patientDOB\" update=\"set\">1979-11-11T08:00:00.000Z</field><field name=\"patient.mrNs.mrn\" update=\"set\">5423</field><field name=\"specimens.specimenType\" update=\"set\">Bone Marrow</field><field name=\"specimens.specimenType\" update=\"set\">Nerve tissue</field><field name=\"UID\">3165297USLABS2012</field></doc></add>'\n\n\n\nThis document gets successfully posted. However, the multi-valued field 'specimens.specimenType', gets stored as following in SOLR -\n\n<arr name=\"specimens.specimenType\">\n<str>{set=Bone Marrow}</str>\n<str>{set=Nerve tissue}</str>\n</arr>\n\n\n\nI did not expect \"\n{set=\" to be stored along with the text \"Bone Marror\".\n\nMy Solr schema xml definition for the field specimens.SpecimenType is - \n{code}\n<field indexed=\"true\" multiValued=\"true\" name=\"specimens.specimenType\" omitNorms=\"false\" omitPositions=\"true\" omitTermFreqAndPositions=\"true\" stored=\"true\" termVectors=\"false\" type=\"text_en\"/>\n\n\n\n\nCan someone help? "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13653969",
            "date": "2013-05-10T10:33:35+0000",
            "content": "Closed after release. "
        }
    ]
}