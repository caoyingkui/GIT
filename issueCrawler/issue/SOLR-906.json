{
    "id": "SOLR-906",
    "title": "Buffered / Streaming SolrServer implementaion",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "1.4"
        ],
        "components": [
            "clients - java"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "While indexing lots of documents, the CommonsHttpSolrServer add( SolrInputDocument ) is less then optimal.  This makes a new request for each document.\n\nWith a \"StreamingHttpSolrServer\", documents are buffered and then written to a single open Http connection.\n\nFor related discussion see:\nhttp://www.nabble.com/solr-performance-tt9055437.html#a20833680",
    "attachments": {
        "StreamingHttpSolrServer.java": "https://issues.apache.org/jira/secure/attachment/12395748/StreamingHttpSolrServer.java",
        "SOLR-906-StreamingHttpSolrServer.patch": "https://issues.apache.org/jira/secure/attachment/12395760/SOLR-906-StreamingHttpSolrServer.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Ryan McKinley",
            "id": "comment-12655298",
            "date": "2008-12-10T18:14:15+0000",
            "content": "One basic problem with calling add( SolrInputDocument) with the CommonsHttpSolrServer is that it logs a request for each document.  This can be a substantial impact.  For example while indexing 40K docs on my machine, it takes ~3 1/2 mins.  If I turn logging off the time drops to ! 2 1/2 mins.  With the streaming approach, the time drops to 20sec!   Some of that is obviously because it limits the logging:\n\nINFO: {add=[id1,id2,id3,id4, ...(38293 more)]} 0 20714\n\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12655299",
            "date": "2008-12-10T18:17:53+0000",
            "content": "This implementation buffers documents in a BlockingQueue<SolrInputDocument>\nwhen a document is added, it makes sure a Thread is working on sending the queue to solr.\n\nWhile this implementation only starts one thread, it would be easy to extent this so that multiple threads are writing to solr simultaneously and draining the same Queue. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12655302",
            "date": "2008-12-10T18:22:58+0000",
            "content": "The error handling behavior is less then ideal....  (as it is for <add> with multiple documents already)\n\nAs is, it will break the connection when an error occures, but will not know which one.  However, it will continue indexing the Queue when an error occures. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12655357",
            "date": "2008-12-10T21:08:23+0000",
            "content": "Here is an updated version that lets you specify how many threads should work on emptying the Queue.  It also adds tests to make sure it passes all the same tests that CommonsHttpSolrServer and EmbeddedSolrServer already pass.   That is, it is a drop in replacement and passes all existing tests.\n\nOne big change is that calling commit or optimize with waitSearcher=true:\n1. blocks adding new docs to the Queue\n2. waits for the Queue to empty (send all docs)\n3. waits for <commit waitSearcher=true /> to return\n4. unblocks everything\n5. finally continues execution.\n\nMy threading chops are not great, so I may be doing something really strange.  It would be good to get some more eyes on this! "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12655363",
            "date": "2008-12-10T21:13:11+0000",
            "content": "also, using the recent patch with a Queue size = 20 and thread count=3 (on a dual core machine), the indexing time dropped from 30 secs -> 20 secs.  In sum:  with the data I am working with, switch from CommonsHttpSolrServer => StreamingHttpSolrServer changes the index time from 3.5 min => 20 sec, or ~10x faster   "
        },
        {
            "author": "Ian Holsman",
            "id": "comment-12655427",
            "date": "2008-12-10T23:33:17+0000",
            "content": "how much of the 3.5minutes -> 30seconds is due to the logging?\nwould it be simpler to change the log message to 'DEBUG' instead of 'INFO' and see how the performance of the regular commons server behaves then? "
        },
        {
            "author": "Ian Holsman",
            "id": "comment-12655431",
            "date": "2008-12-10T23:39:02+0000",
            "content": "\nalso..\n\nThe way I've always thought commonsHttpServer was designed to work was to batch up the documents in groups and use the add(List<SolrInputDocument>) function instead of the individual add(SolrInputDocument) function. \n\nis adding documents in batches (say 100 or 1,000 at a time) also as slow as 3.5m ? or wont this work as you won't get the feedback on which document failed to add if there was an error. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12655438",
            "date": "2008-12-11T00:25:42+0000",
            "content": "> how much of the 3.5minutes -> 30seconds is due to the logging?\n\n~1 min.  When I turn off logging completely, the time is ~2.5 mins  (also, note that with 3 threads, it is down to 20sec)\n\nRE: calling add( doc ) vs add( List<doc> )...  \nyes, things are much better if you call add( List<doc> ) however, it is not the most convenient api if you are running though tons of things.\n\nI would expect (but have not tried) adding 40K docs in one call to add( List<doc> ) would have the same time as this StreamingHttpSolrServer.  It is probably also similar if you buffer 100? 1,000? at a time, but I have not tried.\n\nThe StreamingHttpSolrServer essentially handles the buffering for you.  It keeps an http connection open as long as the Queue has docs to send.  It can start multiple threads and drain the same Queue simultaneously.\n\nEssentially, this just offers an easier interface to get the best possible performance.  The trade off (for now) is that there is no good error reporting.\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12655442",
            "date": "2008-12-11T00:37:05+0000",
            "content": "The other aspect no note is that StreamingHttpSolrServer sends the request in the background so after you call: add( doc ) or add( list ) that thread is free to keep working.  With the off the shelf CommonsHttpSolrServer the client needs to wait for the server to parse the request and index the data before it can continue.\n\nThis switches where the client gets blocked.\n\n\twith CommonsHttpSolrServer it blocks while waiting for solr to write the response\n\twith StreamingHttpSolrServer it blocks while waiting for solr to read the request\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12655917",
            "date": "2008-12-12T08:19:28+0000",
            "content": "Ryan, I'm seeing compile errors related to @Override with interface methods (that's a Java 6 feature). Also, new IOException( e ) is not defined (also Java 6, I guess). "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12656063",
            "date": "2008-12-12T16:57:41+0000",
            "content": "removes @Override from interfaces\n\nI guess:\n  <property name=\"java.compat.version\" value=\"1.5\" />\n\ndoes not take check everything! "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12657102",
            "date": "2008-12-16T18:37:54+0000",
            "content": "I've started taking a look at this. A couple of points:\n\n\n\tInstantiating the lock in blockUntilFinished and nulling it can cause a race condition. A thread in the 'add' method can find that the lock is not null, another thread can null it and the first thread proceeds to lock on it leading to NPE. In the same way, creation of multiple locks is possible in the blockUntilFinished method.\n\tThe run method calling itself recursively looks suspicious. We may be in danger of overflowing the stack.\n\tThe SolrExampleTest cannot be used directly because it depends on the order of the commands being executed. We must clearly document that clients should not depend on the order of commands being executed in the same order as they are given.\n\tThe if (req.getCommitWithin() < 0) should be > 0, right?\n\tThe add calls do not come to the process method. Due to this some add calls may still get in before the commit acquires the lock (assuming multiple producers). Is this class strictly meant for a single document producer use-case?\n\tThe wait loop in blockUntilFinished is very CPU intensive. It can probably be optimized.\n\n\n\nI'm experimenting with a slightly different implementation. Still trying to tie the loose ends. I hope to have a patch soon. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12657107",
            "date": "2008-12-16T18:46:24+0000",
            "content": "Thanks for looking at this!\n\n\n\n\n The if (req.getCommitWithin() < 0) should be > 0, right?\n\n\n\n\n\nno \u2013 if a commit within time is specified, we can not use the open request.  It needs to start a new request so that a new <add ...> command could be sent.  I experimeted with sending everything over the open connection, but we would need to add a new parent tag to the xml format.   That might not be a bad idea.  Then we could send:\n<stream>\n  <add>\n     <doc...>\n  </add>\n  <add commitWithin=\"\">\n  ...\n <commit />\n <add ...>\n\nand finally\n</stream> "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12657108",
            "date": "2008-12-16T18:48:54+0000",
            "content": "\n\n\n The SolrExampleTest cannot be used directly\n\n\n\n\n\nwhy not?  All the tests pass for me... \n\nsending a commit() (with waitSearcher=true) should wait for all the docs to get added, then issue the commit, then return.\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12657110",
            "date": "2008-12-16T18:53:38+0000",
            "content": "\n\n\n The add calls do not come to the process method. Due to this some add calls may still get in before the commit acquires the lock (assuming multiple producers). Is this class strictly meant for a single document producer use-case?\n\n\n\n\n\nI don't totally follow... but if possible, it would be good if multiple threads could fill the same queue.  This would let the StreamingHttpSolrServer  manage all solr communication "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12657115",
            "date": "2008-12-16T19:02:36+0000",
            "content": "why not? All the tests pass for me... \nThere are multiple places where SolrExampleTest calls commit without waitSearcher=true and proceeds to query and assert on results. The failure happens intermittently. Try varying the number of threads and you may be able to reproduce the failure.\n\nThe add calls do not come to the process method.\nI meant the request method. Sorry about that. The SolrServer.add() calls the request method but this implementation does not. If there are multiple threads using this class, new documents may get added to the queue before we acquire the lock inside blockUntilFinished due to the call to commit. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12657278",
            "date": "2008-12-17T04:00:50+0000",
            "content": "another observation :\nwhy do we need a ScheduledExecutorService we only need a ThreadPoolExecutorService\nThe name of the class is somewhat misleading. We must document that this may be exclusively used for updates\n\nHow about renaming this to StreamingUpdateSolrServer "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12657849",
            "date": "2008-12-18T18:42:46+0000",
            "content": "Here is an updated patch that bufferes UpdateRequests rather then SolrInputDocuments \u2013 this is good because then everything is handled in request( final SolrRequest request ) so blocking can be easier.\n\nAlso this lets up submit submit <add commands with the commitWithin syntax.\n\nAll /update commands are streamed to server unless waitSearcher==true\n\nShalin \u2013 can you check this over for threading issues or general improvements? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12657868",
            "date": "2008-12-18T19:41:21+0000",
            "content": "I haven't looked at the code, but can someone outline what the API looks like (i.e. what would typical pseudo code that used the API look like). "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12657871",
            "date": "2008-12-18T19:47:15+0000",
            "content": "API is identical to SolrServer\n\nrather then instantiating with CommonsHttpSolrServer, you just use StreamingSolrServer.\n\nThe constructor args are:\n  StreamingHttpSolrServer(String solrServerUrl, int queueSize, int threadCount ) \n\nqueueSize is how big you want the buffering queue to be\nthreadCount is the maximum number of threads that will get used to empty the queue "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12657877",
            "date": "2008-12-18T20:06:00+0000",
            "content": "Isn't there a need for a polling API to check for errors?\nI think something like the following client code would be easiest for people to deal with:\n\nwhile not done:\n   myDoc = [...] // make the doc\n   solr.addDoc(myDoc)\n   List<Error> errors = solr.getErrors();\n   if (errors != null) handleErrors(errors)\n\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12657882",
            "date": "2008-12-18T20:21:15+0000",
            "content": "I suppose...  though I don't see it as a strict requirement \u2013 if you need full error handling, use a different SolrServer implementation.\n\nI think a more reasonable error API would be a callback function rather then polling \u2013 the error could occur outside you loop (assuming you break at some point).  That callback could easily be converted to a polling api if desired.\n\nThe big thing to note with this API is that calling:\n solr.add( doc )\njust adds it to the queue processes it in the background.  It is a BlockingQueue, so after it hits the max size the client will block before it can add \u2013 but that should be transparent to the client.\n\nthe error caused by adding that doc may happen much later in time.\n\nI'll go ahead and add that callback... "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12657888",
            "date": "2008-12-18T20:36:15+0000",
            "content": "updated version that includes the callback function:\n\n  public void handleError( Throwable ex )\n  {\n    log.error( \"error\", ex );\n  }\n\n\n\nthis gets called whenever an error occurs.  We could have that keep a list of any errors or whatever.  I'm not sure what the default error behavior should be.  Collecting everything in a list?  just logging? "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12657889",
            "date": "2008-12-18T20:39:06+0000",
            "content": "This can be use like so:\n\nSolrServer solr = new StreamingHttpSolrServer( url, 2, 5 ) {\n  @Override\n  public void handleError(Throwable ex) {\n     // do somethign...\n  }\n};\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12657893",
            "date": "2008-12-18T20:44:41+0000",
            "content": "IMO callbacks can be tougher to deal with and require client code to be multi-threaded (like signal handling in C that caused tons of trouble for people because they weren't thinking about the fact that it could be called at any time concurrently with other code, esp back in the days when C library calls were not re-entrant).\n\nBut I guess as you say, polling could be added on top of a callback API if needed later. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12657896",
            "date": "2008-12-18T20:49:58+0000",
            "content": "public void handleError(Throwable ex)\n\nIdeally, we could give the user back\n\n\tthe actual error response (http error code, and the body?)\n\tthe InputDocument that caused the failure, if there was one\n\twhat else?\n\n\n\nSo it seems like we should add these parameters to handleError, and just document that they currently return null if we can't get that info yet.  Or perhaps more extensible, a SolrError class that has these fields? "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12657911",
            "date": "2008-12-18T21:23:44+0000",
            "content": "\n\n\n the actual error response (http error code, and the body?)\nThat is / can be encoded in the Throwable implementation no?  As is, it adds the same Exception that you get when running the standard one.\n\n\n\n\n\n\n\n\n the InputDocument that caused the failure, if there was one\n\n\n\n\n\nI don't know if there is a good way to do this.  The scope that catches exceptions is way outside of the context where we knew what was written.  This is just like hitting an error somewhere in the add( List<Doc> ) \u2013 the response has no way to know where it broke.  Ideally the error that Solr returns is enough information, but the current exception behavior is to barf or not.\n\n\n\n\n what else?\n\n\n\n\n\nMy thought was we could add any of the specialized parameters in finer grained Throwable implementations.  When we have a real 'error' response, this could be parsed and passed as an exception. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12659750",
            "date": "2008-12-30T06:53:45+0000",
            "content": "Shalin, did you get a change to look at this version? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12660457",
            "date": "2009-01-03T10:06:20+0000",
            "content": "\n\tOne problem with the current implementation is that it writes everything to a local buffer and then uploads the whole content in one go. So essentially we are wasting time till your 40K docs are written into this huge XML. Another issue is that this XML has to fit in memory. We need to fix the comonsHttpSolrServer first. It must stream the docs .\n\tWe can enhance the SolrServer API by adding a method SolrServer#add(Iterator<SolrInputDocs> docs) . So CommonsHttpSolrServer can start writing the documents as and when you are producing your documents . We also have the advantage of not storing the huge list of docs in memory.\n\n\n\n\n\tAnother enhancement is using a different format (SOLR-865). It uses javabin format and it can be extremely fast compared to XML and the payload can be reduced substantially.\n\n\n\nProbably we can overcome the perf problems to a certain extent with these two fixes. \n\n\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12660496",
            "date": "2009-01-03T18:11:56+0000",
            "content": "\n\n\n One problem with the current implementation is that it writes everything to a local buffer and then uploads the whole content in one go. So essentially we are wasting time till your 40K docs are written into this huge XML. Another issue is that this XML has to fit in memory. We need to fix the comonsHttpSolrServer first. It must stream the docs .\n\n\n\n\n\nReally?!\n\nAre you saying that the RequestEntity.html#getContentLength() does not behave as advertised?\n\nThis implementation returns -1 for the content length, and that tells the connection use chunk encoding to transmit the request entity.  \n\nWhere do you get the 40K number?  Is it from the log?  If so, that is the expected behavior \u2013 the server continually processes documents until it reaches the end of the stream.  That may be 1 document that may be 1M docs...\n\nIf you are filling up a Collection<SolrInputDocument> with 40K docs, then sending it of course it is going to hold on to 40K docs at once.\n\n\n\n\n\n We can enhance the SolrServer API by adding a method SolrServer#add(Iterator<SolrInputDocs> docs) . So CommonsHttpSolrServer can start writing the documents as and when you are producing your documents . We also have the advantage of not storing the huge list of docs in memory.\n\n\n\n\n\nI'm not following...  with the StreamingHttpSolrServer, you can send documents one at a time and each documents starts sending as soon as it can.  There is a BlockingQueue<UpdateRequest> that holds all UpdateRequests that come through the 'request' method.  BlockingQueue's only hold a fixed number of items and will block before adding something beyond the limit.\n\n\n\n\n Another enhancement is using a different format (SOLR-865). It uses javabin format and it can be extremely fast compared to XML and the payload can be reduced substantially.\n\n\n\n\n\nThat is a different issue altogether.  That relates to having something different running on the server.  Once that is in, then this should be able to leverage that as well...\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12660572",
            "date": "2009-01-04T14:00:22+0000",
            "content": "Please ignore the number 40K docs. I just took it from your perf test numbers. I thought you were writing docs as a list\n\nI am referring to the client code .The method in UpdateRequest\n\npublic Collection<ContentStream> getContentStreams() throws IOException {\n    return ClientUtils.toContentStreams( getXML(), ClientUtils.TEXT_XML );\n}\n\n\n\nThis means that the getXML() method actually constructs a huge String which is the entire xml. It is not very good if we are writing out very large no:of docs\n\nI am suggesting that ComonsHttpSolrServer has scope for improvement. Instead of building that String in memory  we can just start streaming it to the server. So the OutputStream can be passed on to UpdateRequest so that it can write the xml right into the stream. So there is streaming effectively on both ends\n\nThis is valid where users do bulk updates. Not when they write one doc at a time. \n\nThe new method SolrServer#add(Iterator<SolrInputDocs> docs) can start writing the docs immedietly and the docs can be uploaded as and when they are being produced. It is not related to these issue exactly, But the intend of this issue is to make upload faster.\n\n\nSOLR-865 is not very related to this issue. StreamingHttpSolrServer can use javabin format as well.\n\nwith the StreamingHttpSolrServer, you can send documents one at a time and each documents starts sending as soon as it can\nOne drawback of a StreamingHttpSolrServer is that it ends up opening  multiple connections for uploading the documents\n\nAnother enhancement . We can add one (or more ) extra thread in the server to do the call updaterequestprocessor.processAdd() .  "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12660607",
            "date": "2009-01-04T17:57:53+0000",
            "content": "Are you looking at the patch or just brainstorming how this could be implemented?\n\n\nI am referring to the client code .The method in UpdateRequest\n\npublic Collection<ContentStream> getContentStreams() throws IOException {\n    return ClientUtils.toContentStreams( getXML(), ClientUtils.TEXT_XML );\n}\n\nThis means that the getXML() method actually constructs a huge String which is the entire xml. It is not very good if we are writing out very large no:of docs\n\n\nThis is not how the patch works...  for starters, it never calls getContentStreams() for UpdateRequest.  It opens a single connection and continually dumps the xml for each request.  Rather then call getXML() the patch adds a function writeXml( Writer ) that writes directly to the open buffer.\n\n\nI am suggesting that ComonsHttpSolrServer has scope for improvement. Instead of building that String in memory we can just start streaming it to the server. So the OutputStream can be passed on to UpdateRequest so that it can write the xml right into the stream. So there is streaming effectively on both ends\n\nThe ComonsHttpSolrServer is fine, but you are right that each UpdateRequest may want to write the content directly to the open stream.  The ContentStream interface gives us all that control.  One thing to note is that if you do not specify the length, the HttpCommons server will use chunked encoding.\n\nBut I think adding the StreammingUpdateSolrServer resolves that for everyone.  Uses have either option.\n\n\n One drawback of a StreamingHttpSolrServer is that it ends up opening multiple connections for uploading the documents\n\nNonsense \u2013 that is exactly what this avoids.  It opens a single connection and writes everything to it.  You can configure how many threads you want emptying the queue; each one will open a connection.\n\n\nAnother enhancement . We can add one (or more ) extra thread in the server to do the call updaterequestprocessor.processAdd() . \n\nThat opens a whole can of worms...  perhaps better discussed on java-dev.  For now I think sticking to the 1 thread/prequest is a good model.  If you want multiple threads running on the server use multiple connections (it is even an argument in the StreammingHttpSolrServer) "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12660610",
            "date": "2009-01-04T17:59:38+0000",
            "content": "I would like to go ahead and commit this patch soon.  Shalin - did the changes in the latest patch resolve the issues you referred to? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12660662",
            "date": "2009-01-05T04:38:35+0000",
            "content": "Hi Ryan,\nYou got me wrong. I was trying to say how to make CommonsHttpSolrServer efficient by streaming docs as StreamingHttpSolrServer does when I add docs in bulk using \n\nSolrServer.add(List<SolrInputDocument> docs)\n\n\n\nYes , StreamingHttpSolrServer uses only one connection per thread and it closes the connection after waiting for 250ms for a new document. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12660784",
            "date": "2009-01-05T15:17:52+0000",
            "content": "Sorry Ryan for stalling this. The tests run fine. But we need to change the global lock to be final.\n\nWhat does the \"<stream>\" tag do?\n\nPlease go ahead and take charge. It's all yours. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12661285",
            "date": "2009-01-06T19:57:12+0000",
            "content": "\n\n\n But we need to change the global lock to be final.\n\n\n\n\n\nwhy?  The global lock is used to block all the threads \u2013 each worker checks if it is null to see if it should block or not.\n\nThe only place that sets the lock is within the same synchronized block:\n\n\n  public synchronized void blockUntilFinished()\n  {\n    if( lock == null ) {\n      lock = new ReentrantLock();\n    }\n    lock.lock();\n\n    ...\n\n    lock.unlock();\n    lock = null;\n  }\n\n\n\nsince nothing else changes lock, i think it is ok.\n\n\n\n\n\n What does the \"<stream>\" tag do?\n\n\n\n\n\nThat is just there so that multiple <add> commands can be in the same XML document.  it is just an arbitrary parent tag.  The parser on the other end only validates once it hits a known cmd tag. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12661434",
            "date": "2009-01-07T04:12:52+0000",
            "content": "That is just there so that multiple <add> commands can be in the same XML document.\n\nIt may not be necessary to have the <strream> tag , if we modify the XMLLoader to accept multiple <add> we can do away with the extra <stream> tag.\n\nThe XML format is a public interface and we should be conservative in adding stuff into that .\n\nIt would be nice to document the public changes in the JIRA itself so that other committers can see the changes which are going to come in w/o going through the patch itself "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12661440",
            "date": "2009-01-07T04:33:35+0000",
            "content": "\n\n\n if we modify the XMLLoader to accept multiple <add>\n\n\n\n\n\nThe XMLLoader already accepts multiple <add> commands without any changes.\n\nThe <stream> tag (or <whatever> tag) is added so that the xml is valid \u2013 you can not have multiple roots in an xml document.\n\nAgain \u2013 nothing has changed with the parser.  "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12775603",
            "date": "2009-11-10T15:51:52+0000",
            "content": "Bulk close for Solr 1.4 "
        }
    ]
}