{
    "id": "SOLR-12798",
    "title": "Structural changes in SolrJ since version 7.0.0 have effectively disabled multipart post",
    "details": {
        "type": "Improvement",
        "status": "Open",
        "labels": "",
        "fix_versions": [],
        "components": [
            "SolrJ"
        ],
        "priority": "Major",
        "resolution": "Unresolved",
        "affect_versions": "7.4"
    },
    "description": "Project ManifoldCF uses SolrJ to post documents to Solr.  When upgrading from SolrJ 7.0.x to SolrJ 7.4, we encountered significant structural changes to SolrJ's HttpSolrClient class that seemingly disable any use of multipart post.  This is critical because ManifoldCF's documents often contain metadata in excess of 4K that therefore cannot be stuffed into a URL.\n\nThe changes in question seem to have been performed by Paul Noble on 10/31/2017, with the introduction of the RequestWriter mechanism.  Basically, if a request has a RequestWriter, it is used exclusively to write the request, and that overrides the stream mechanism completely.  I haven't chased it back to a specific ticket.\n\nManifoldCF's usage of SolrJ involves the creation of ContentStreamUpdateRequests for all posts meant for Solr Cell, and the creation of UpdateRequests for posts not meant for Solr Cell (as well as for delete and commit requests).  For our release cycle that is taking place right now, we're shipping a modified version of HttpSolrClient that ignores the RequestWriter when dealing with ContentStreamUpdateRequests.  We apparently cannot use multipart for all requests because on the Solr side we get \"pfountz Should not get here!\" errors on the Solr side when we do, which generate HTTP error code 500 responses.  That should not happen either, in my opinion.",
    "attachments": {
        "HOT Balloon Trip_Ultra HD.jpg": "https://issues.apache.org/jira/secure/attachment/12941420/HOT%20Balloon%20Trip_Ultra%20HD.jpg",
        "SOLR-12798-workaround.patch": "https://issues.apache.org/jira/secure/attachment/12941645/SOLR-12798-workaround.patch",
        "SOLR-12798-approach.patch": "https://issues.apache.org/jira/secure/attachment/12941493/SOLR-12798-approach.patch",
        "solr-update-request.txt": "https://issues.apache.org/jira/secure/attachment/12941419/solr-update-request.txt",
        "SOLR-12798-reproducer.patch": "https://issues.apache.org/jira/secure/attachment/12941599/SOLR-12798-reproducer.patch",
        "SOLR-12798.patch": "https://issues.apache.org/jira/secure/attachment/12941838/SOLR-12798.patch",
        "no params in url.png": "https://issues.apache.org/jira/secure/attachment/12941596/no%20params%20in%20url.png"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16625544",
            "content": "Noble Paul, any help would be welcome.  We're in a ManifoldCF release cycle now, and SolrJ issues are blocking it.\n ",
            "author": "Karl Wright",
            "date": "2018-09-24T08:55:05+0000"
        },
        {
            "id": "comment-16625809",
            "content": "The status is as follows:\n\n(1) I've confirmed that the RequestWriter override only permits multipart form requests for the \"commit\" request.  \"Update\" or \"Delete\" both do not allow this pathway at all.\n\n(2) If I change the logic for all POST and PUT requests to disable the contentWriter clause, POST requests of documents work properly, but delete document requests fail, with the following exception:\n\n\njava.lang.RuntimeException: This Should not happen\n        at org.apache.solr.client.solrj.impl.BinaryRequestWriter.getContentStreams(BinaryRequestWriter.java:67) ~[?:?]\n        at org.apache.manifoldcf.agents.output.solr.ModifiedHttpSolrClient.createMethod(ModifiedHttpSolrClient.java:175) ~[?:?]\n\n\n\n(4) Conditionally disabling contentWriter when the request is of class ContentStreamUpdateRequest allows things to work partly.  Text documents that are indexed via standard UpdateRequest do not use multipart post, however.  So we need a better solution.\n ",
            "author": "Karl Wright",
            "date": "2018-09-24T13:16:34+0000"
        },
        {
            "id": "comment-16625869",
            "content": "Can you test with the latest SolrJ release 7.5 and confirm if this problem still exists. I shall take a look after that ",
            "author": "Noble Paul",
            "date": "2018-09-24T14:11:13+0000"
        },
        {
            "id": "comment-16626079",
            "content": "Noble Paul, I have verified that the problem still exists on Solr 7.5. ",
            "author": "Karl Wright",
            "date": "2018-09-24T16:15:40+0000"
        },
        {
            "id": "comment-16626970",
            "content": "You have custom impl of HttpSolrClient. Is it not possible for you to start using the {{ public ContentWriter getContentWriter(SolrRequest req) }} method ?\n\nThe getContentStreams method is not the appropriate method to use ",
            "author": "Noble Paul",
            "date": "2018-09-25T08:06:12+0000"
        },
        {
            "id": "comment-16627010",
            "content": "I'm looking for workarounds \u2013 initially, at least.\n\nWhat I've tried is adding the following code in the POST/PUT section of the HttpSolrClient code:\n\n\n      // UpdateRequest uses PUT now, and ContentStreamUpdateHandler uses POST.\n      // We must override PUT with POST if multipart is on.\n      // If useMultipart is on, we fall back to getting streams directly from the request.\n      final boolean mustUseMultipart;\n      final SolrRequest.METHOD methodToUse;\n      if (this.useMultiPartPost) {\n        final Collection<ContentStream> requestStreams = request.getContentStreams();\n        mustUseMultipart = requestStreams != null && requestStreams.size() > 0;\n        if (mustUseMultipart) {\n          System.out.println(\"Overriding with multipart post\");\n          streams = requestStreams;\n          methodToUse = SolrRequest.METHOD.POST;\n        } else {\n          methodToUse = request.getMethod();\n        }\n      } else {\n        mustUseMultipart = false;\n        methodToUse = request.getMethod();\n      }\n      \n      //System.out.println(\"Post or put\");\n      String url = basePath + path;\n      /*\n      boolean hasNullStreamName = false;\n      if (streams != null) {\n        for (ContentStream cs : streams) {\n          if (cs.getName() == null) {\n            hasNullStreamName = true;\n            break;\n          }\n        }\n      }\n      */\n      \n      /*\n      final boolean isMultipart = ((this.useMultiPartPost && SolrRequest.METHOD.POST == methodToUse)\n          || (streams != null && streams.size() > 1)) && !hasNullStreamName;\n      */\n      final boolean isMultipart = this.useMultiPartPost && SolrRequest.METHOD.POST == methodToUse &&\n          (streams != null && streams.size() >= 1);\n          \n      System.out.println(\"isMultipart = \"+isMultipart);\n\n\n\nThe problem is that when multipart post is used for document delete requests, they fail because the stream is empty.  And the code above doesn't distinguish between UpdateRequests that include real documents and UpdateRequests that are delete requests.  Any ideas?\n ",
            "author": "Karl Wright",
            "date": "2018-09-25T08:42:27+0000"
        },
        {
            "id": "comment-16627028",
            "content": "Noble Paul We have a custom implementation because SolrJ and indeed HttpComponents/HttpClient have problems we're forced to work around.  These have been raised before but so far not taken too seriously apparently.  The need to workaround things has gotten even more significant with the latest release.\n\nModifiedHttpSolrClient is a derivation of HttpSolrClient.  The method overridden, createMethod(), is a direct copy of HttpSolrClient.createMethod() with certain very specific changes.  These are apparently all still necessary.  I've included the method code below. \n\nIf I disable this custom method, and use standard code, I never get multipart form posts at all.  That is unacceptable in this application.\n\nWith the current modifications included below, I get multipart posts for everything, including for deletions, which breaks because Solr doesn't like that.\n\nI'm asking for advice as to how to get multipart posts only for documents, either ones transmitted by ContentStreamUpdateHandler or UpdateHandler.add(SolrInputDocument).\n\n\n  @Override\n  protected HttpRequestBase createMethod(SolrRequest request, String collection) throws IOException, SolrServerException {\n    if (request instanceof V2RequestSupport) {\n      request = ((V2RequestSupport) request).getV2Request();\n    }\n    SolrParams params = request.getParams();\n    RequestWriter.ContentWriter contentWriter = requestWriter.getContentWriter(request);\n    Collection<ContentStream> streams = contentWriter == null ? requestWriter.getContentStreams(request) : null;\n    String path = requestWriter.getPath(request);\n    if (path == null || !path.startsWith(\"/\")) {\n      path = DEFAULT_PATH;\n    }\n    \n    ResponseParser parser = request.getResponseParser();\n    if (parser == null) {\n      parser = this.parser;\n    }\n    \n    // The parser 'wt=' and 'version=' params are used instead of the original\n    // params\n    ModifiableSolrParams wparams = new ModifiableSolrParams(params);\n    if (parser != null) {\n      wparams.set(CommonParams.WT, parser.getWriterType());\n      wparams.set(CommonParams.VERSION, parser.getVersion());\n    }\n    if (invariantParams != null) {\n      wparams.add(invariantParams);\n    }\n\n    String basePath = baseUrl;\n    if (collection != null)\n      basePath += \"/\" + collection;\n\n    if (request instanceof V2Request) {\n      if (System.getProperty(\"solr.v2RealPath\") == null) {\n        basePath = baseUrl.replace(\"/solr\", \"/api\");\n      } else {\n        basePath = baseUrl + \"/____v2\";\n      }\n    }\n\n    if (SolrRequest.METHOD.GET == request.getMethod()) {\n      if (streams != null || contentWriter != null) {\n        throw new SolrException(SolrException.ErrorCode.BAD_REQUEST, \"GET can't send streams!\");\n      }\n\n      return new HttpGet(basePath + path + toQueryString(wparams, false));\n    }\n\n    if (SolrRequest.METHOD.DELETE == request.getMethod()) {\n      return new HttpDelete(basePath + path + toQueryString(wparams, false));\n    }\n\n    if (SolrRequest.METHOD.POST == request.getMethod() || SolrRequest.METHOD.PUT == request.getMethod()) {\n\n      // UpdateRequest uses PUT now, and ContentStreamUpdateHandler uses POST.\n      // We must override PUT with POST if multipart is on.\n      // If useMultipart is on, we fall back to getting streams directly from the request.\n      final boolean mustUseMultipart;\n      final SolrRequest.METHOD methodToUse;\n      if (this.useMultiPartPost) {\n        final Collection<ContentStream> requestStreams = request.getContentStreams();\n        mustUseMultipart = requestStreams != null && requestStreams.size() > 0;\n        if (mustUseMultipart) {\n          System.out.println(\"Overriding with multipart post\");\n          streams = requestStreams;\n          methodToUse = SolrRequest.METHOD.POST;\n        } else {\n          methodToUse = request.getMethod();\n        }\n      } else {\n        mustUseMultipart = false;\n        methodToUse = request.getMethod();\n      }\n      \n      //System.out.println(\"Post or put\");\n      String url = basePath + path;\n      /*\n      boolean hasNullStreamName = false;\n      if (streams != null) {\n        for (ContentStream cs : streams) {\n          if (cs.getName() == null) {\n            hasNullStreamName = true;\n            break;\n          }\n        }\n      }\n      */\n      \n      /*\n      final boolean isMultipart = ((this.useMultiPartPost && SolrRequest.METHOD.POST == methodToUse)\n          || (streams != null && streams.size() > 1)) && !hasNullStreamName;\n      */\n      final boolean isMultipart = this.useMultiPartPost && SolrRequest.METHOD.POST == methodToUse &&\n          (streams != null && streams.size() >= 1);\n          \n      System.out.println(\"isMultipart = \"+isMultipart);\n      \n      LinkedList<NameValuePair> postOrPutParams = new LinkedList<>();\n\n      if(contentWriter != null && !isMultipart) {\n        //System.out.println(\" using contentwriter\");\n        String fullQueryUrl = url + toQueryString(wparams, false);\n        HttpEntityEnclosingRequestBase postOrPut = SolrRequest.METHOD.POST == methodToUse ?\n            new HttpPost(fullQueryUrl) : new HttpPut(fullQueryUrl);\n        postOrPut.addHeader(\"Content-Type\",\n            contentWriter.getContentType());\n        postOrPut.setEntity(new BasicHttpEntity(){\n          @Override\n          public boolean isStreaming() {\n            return true;\n          }\n\n          @Override\n          public void writeTo(OutputStream outstream) throws IOException {\n            contentWriter.write(outstream);\n          }\n        });\n        return postOrPut;\n\n      } else if (streams == null || isMultipart) {\n        // send server list and request list as query string params\n        ModifiableSolrParams queryParams = calculateQueryParams(getQueryParams(), wparams);\n        queryParams.add(calculateQueryParams(request.getQueryParams(), wparams));\n        String fullQueryUrl = url + toQueryString(queryParams, false);\n        HttpEntityEnclosingRequestBase postOrPut = fillContentStream(methodToUse, streams, wparams, isMultipart, postOrPutParams, fullQueryUrl);\n        return postOrPut;\n      }\n      // It is has one stream, it is the post body, put the params in the URL\n      else {\n        String fullQueryUrl = url + toQueryString(wparams, false);\n        HttpEntityEnclosingRequestBase postOrPut = SolrRequest.METHOD.POST == methodToUse ?\n            new HttpPost(fullQueryUrl) : new HttpPut(fullQueryUrl);\n        fillSingleContentStream(streams, postOrPut);\n\n        return postOrPut;\n      }\n    }\n\n    throw new SolrServerException(\"Unsupported method: \" + request.getMethod());\n\n  }\n\n  private void fillSingleContentStream(Collection<ContentStream> streams, HttpEntityEnclosingRequestBase postOrPut) throws IOException {\n    // Single stream as body\n    // Using a loop just to get the first one\n    final ContentStream[] contentStream = new ContentStream[1];\n    for (ContentStream content : streams) {\n      contentStream[0] = content;\n      break;\n    }\n    Long size = contentStream[0].getSize();\n    postOrPut.setEntity(new InputStreamEntity(contentStream[0].getStream(), size == null ? -1 : size) {\n      @Override\n      public Header getContentType() {\n        return new BasicHeader(\"Content-Type\", contentStream[0].getContentType());\n      }\n\n      @Override\n      public boolean isRepeatable() {\n        return false;\n      }\n    });\n\n  }\n\n  private HttpEntityEnclosingRequestBase fillContentStream(SolrRequest.METHOD methodToUse, Collection<ContentStream> streams, ModifiableSolrParams wparams, boolean isMultipart, LinkedList<NameValuePair> postOrPutParams, String fullQueryUrl) throws IOException {\n    HttpEntityEnclosingRequestBase postOrPut = SolrRequest.METHOD.POST == methodToUse ?\n        new HttpPost(fullQueryUrl) : new HttpPut(fullQueryUrl);\n\n    if (!isMultipart) {\n      postOrPut.addHeader(\"Content-Type\",\n          \"application/x-www-form-urlencoded; charset=UTF-8\");\n    }\n\n    List<FormBodyPart> parts = new LinkedList<>();\n    Iterator<String> iter = wparams.getParameterNamesIterator();\n    while (iter.hasNext()) {\n      String p = iter.next();\n      String[] vals = wparams.getParams(p);\n      if (vals != null) {\n        for (String v : vals) {\n          if (isMultipart) {\n            parts.add(new FormBodyPart(p, new StringBody(v, StandardCharsets.UTF_8)));\n          } else {\n            postOrPutParams.add(new BasicNameValuePair(p, v));\n          }\n        }\n      }\n    }\n\n    // TODO: remove deprecated - first simple attempt failed, see {@link MultipartEntityBuilder}\n    if (isMultipart && streams != null) {\n      for (ContentStream content : streams) {\n        String contentType = content.getContentType();\n        if (contentType == null) {\n          contentType = BinaryResponseParser.BINARY_CONTENT_TYPE; // default\n        }\n        String name = content.getName();\n        if (name == null) {\n          name = \"\";\n        }\n        parts.add(new FormBodyPart(encodeForHeader(name),\n            new InputStreamBody(\n                content.getStream(),\n                ContentType.parse(contentType),\n                encodeForHeader(content.getName()))));\n      }\n    }\n\n    System.out.println(\"Using multipart post!\");\n    if (parts.size() > 0) {\n      ModifiedMultipartEntity entity = new ModifiedMultipartEntity(HttpMultipartMode.STRICT, null, StandardCharsets.UTF_8);\n      //MultipartEntity entity = new MultipartEntity(HttpMultipartMode.STRICT);\n      for (FormBodyPart p : parts) {\n        entity.addPart(p);\n      }\n      postOrPut.setEntity(entity);\n    } else {\n      //not using multipart\n      postOrPut.setEntity(new UrlEncodedFormEntity(postOrPutParams, StandardCharsets.UTF_8));\n    }\n    return postOrPut;\n  }\n\n\n ",
            "author": "Karl Wright",
            "date": "2018-09-25T08:54:44+0000"
        },
        {
            "id": "comment-16627038",
            "content": "Pardon me, \nI'm still wondering what is the real reason why you must use a multi part request. What is stopping you from using a standard update request with all these operations instead of using a multipart requests.  ",
            "author": "Noble Paul",
            "date": "2018-09-25T09:09:27+0000"
        },
        {
            "id": "comment-16627098",
            "content": "Hi Noble Paul, as I explained before, we have document metadata in excess of the maximum URL length quite often.  In fact, it's the typical case.  That is why we must use multipart post in this application.  My rough estimate of the percentage of ManifoldCF users who fall into this category is greater than 90%.\n ",
            "author": "Karl Wright",
            "date": "2018-09-25T10:06:44+0000"
        },
        {
            "id": "comment-16627138",
            "content": "It looks like the only implementer of ContentWriter is StringPayloadContentWriter, which just furnishes a string for output, correct?\n\nIn order to work within that framework, ContentStreamUpdateHandler would need a streaming ContentWriter implementation that pulls from the input and writes to the output.  That seems to be missing.  And then this has nothing whatsoever to do with how the content is actually transmitted \u2013 it seems that the assumption is that the new ContentWriter stuff all goes via PUT with metadata in the URL.  That's not good for two reasons: first, the URL length problems I've already mentioned, and second \u2013 Solr Cell uses the \"name\" part of the multipart post to inject its own bit of metadata into the document, and there would be no way to transmit that anymore.  Logic is still therefore going to be needed to use multipart forms under specific circumstances.  Maybe there needs to be a useMultipart() method in all Requests, and HttpSolrClient should look at that to decide whether to use multipart or standard PUT?\n\n ",
            "author": "Karl Wright",
            "date": "2018-09-25T10:29:10+0000"
        },
        {
            "id": "comment-16627408",
            "content": "ContentWriter is mostly implemented by anonymous classes. StringPayloadWriter is just a helper class.\n\nI'm still thinking this is an XY problem. We are assuming your usecase can only be implemented using a multipart request. Can we see what do you send in the request parameters? ",
            "author": "Noble Paul",
            "date": "2018-09-25T13:58:53+0000"
        },
        {
            "id": "comment-16627547",
            "content": "Noble Paul 'We are assuming your usecase can only be implemented using a multipart request. Can we see what do you send in the request parameters?'\n\nThat's kind of a silly question if you don't mind me saying so.  MCF is a framework with dozens of connectors for accessing different kinds of document repositories.  A \"document\" in ManifoldCF consists of:\n\n\n\tA content stream of infinite length\n\tUnlimited metadata, in the form of name/valuelist pairs\n\n\n\nDocuments that have large amounts of metadata are common.  The details vary considerably by source repository.  For only one example, we have one client who seemingly specializes in indexing image content.  The images are run through Tika, which takes these images and produces a zero-length text file and sometimes 100K bytes of metadata text, in multiple metadata fields.\n\nI hope that's enough to demonstrate why it is impossible to expect all the metadata for a document to fit in the URL. ",
            "author": "Karl Wright",
            "date": "2018-09-25T15:46:53+0000"
        },
        {
            "id": "comment-16627589",
            "content": "As far as I understand, we do advertise multipart upload in several places:\n\n\thttps://lucene.apache.org/solr/guide/7_5/content-streams.html#content-stream-sources\n\tmultipartUploadLimitInKB parameter in solrconfig.xml https://lucene.apache.org/solr/guide/7_5/requestdispatcher-in-solrconfig.html#requestparsers-element\n\n\n\nIf the current issue changes those expectations without explicitly discussing them and including the new user-visible limitation in the migration guide for 7.5, we have a clear case of critical regression on our hands. I cannot see any tests in the codebases for this, but - if my assessment is correct - perhaps one should exist.\n\n\u00a0 ",
            "author": "Alexandre Rafalovitch",
            "date": "2018-09-25T16:24:16+0000"
        },
        {
            "id": "comment-16627674",
            "content": "Alexandre Rafalovitch\u00a0I believe this issue is about SolrJ (client-side), not the Solr server.\n\nKarl Wright\u00a0why must ManifoldCF rely on HTTP Multipart in particular \u2013 can't it compose a SolrInputDocument and just send it like basically all Solr clients I've ever seen? \u00a0Is the issue about the \"infinite length\" content stream, which I presume maps to some sort of body text? \u00a0Note the existence of UpdateRequest.setDocIterator(Iterator<SolrInputDocument>) which can be helpful in streaming and materializing documents on the fly. ",
            "author": "David Smiley",
            "date": "2018-09-25T17:20:40+0000"
        },
        {
            "id": "comment-16627698",
            "content": "David Smiley, there are two problems with using UpdateRequest.  First, as you point out, the entire document has to hit memory.  This is problematic because sometimes these documents are massive and nevertheless Tika needs all of them to extract stuff from them.  So we allow two modes of operation:\n\n(1) Via Solr Cell, in which case we use ContentStreamUpdateRequest, which embeds a stream and forms the request without having the entire document hit memory, and\n(2) Via UpdateRequest, and SolrinputDocument, but only after Tika has been invoked, and with a length limit.  Even then we have problems with people running out of memory unless they are very careful, given that there are sometimes dozens of indexing requests active at any one time.\n\nThis information, by the way, has nothing to do with length limits on the URL, since those are determined solely by metadata, which can be large and is independent of the main content stream.  URL limits get in the way just as readily when we use mode (2) as when we use mode (1).\n\n\nNote the existence of UpdateRequest.setDocIterator(Iterator<SolrInputDocument>) which can be helpful in streaming and materializing documents on the fly.\n\nYes, of course it can, but the way SolrJ is constructed it makes no use of this.  In fact, it currently doesn't use multipart post at all, unless I override much functionality in order to force it to do so. ",
            "author": "Karl Wright",
            "date": "2018-09-25T17:37:05+0000"
        },
        {
            "id": "comment-16627718",
            "content": "Okay I appreciate your points. \u00a0It'd be nice if SolrJ could be enhanced to support multi-part to avoid long URL construction. \u00a0Please help make this a first class supported feature if it matters to you/ManifoldCF.\n\nI don't think this is a bug though, and thus not a regression. \u00a0Before 7.5 by your account you really had to go out of your way to make multi-part work with SolrJ. \u00a0The internals changed which thwarted your efforts (a shame) but doesn't represent a bug. \u00a0I appreciate it's a frustrating unexpected turn of events, nonetheless. ",
            "author": "David Smiley",
            "date": "2018-09-25T17:50:18+0000"
        },
        {
            "id": "comment-16627826",
            "content": "David Smiley, whereas it doesn't seem to have been appreciated, SolrJ did have reasonable support for multipart post some few major version ago but I appreciate the fact that this is no longer a priority.  I'm happy to help get this back to a point that MCF needs. ",
            "author": "Karl Wright",
            "date": "2018-09-25T19:22:10+0000"
        },
        {
            "id": "comment-16628265",
            "content": "here are two problems with using UpdateRequest. First, as you point out, the entire document has to hit memory. \n\nthis no longer is the case. The reason why I changed the interface is to ensure that we don't write everything to memory .You can provide a request that creates documents on the fly and the memory consumption is trivial.\n\nYes, of course it can, but the way SolrJ is constructed it makes no use of this. In fact, it currently doesn't use multipart post at all, unless I override much functionality in order to force it to do so.\n\nI have fixed this problem in the current SolrJ ",
            "author": "Noble Paul",
            "date": "2018-09-26T05:28:27+0000"
        },
        {
            "id": "comment-16628307",
            "content": "\nthis no longer is the case\n\nThat's good news; I can change things in ManifoldCF accordingly, since we no longer have to enforce a maximum document size limit in that case then.\n\n\nI have fixed this problem in the current SolrJ\n\nSo there's a fix for multipart post usage?  Is this committed to master?  How do you turn it on, or does it do this automatically?\n\nOnce that's there, it would be straightforward to add my other fixes; I'm a Lucene/Solr committer now as well, so I can ticket and propose them and they will get done this time. ",
            "author": "Karl Wright",
            "date": "2018-09-26T06:44:24+0000"
        },
        {
            "id": "comment-16628338",
            "content": "So there's a fix for multipart post usage? Is this committed to master? How do you turn it on, or does it do this automatically?\n\nI never bothered with multipart post. I wanted to ensure that we don't write the docs to memory before we post to the server. That's the fix. As long as you can generate docs in a streaming fashion there is no limit to the no:of docs that we can write in a single request in the client   ",
            "author": "Noble Paul",
            "date": "2018-09-26T07:14:28+0000"
        },
        {
            "id": "comment-16628354",
            "content": "Ok, thanks for the clarification.\n\nI will propose SolrJ changes to allow multipart form transport as a first-class citizen, using the ContentWriter construct, and attach those as a patch to this ticket.  The other fixes I will propose separately.  Or, if you want to tackle this, I'd be happy to hand it to you.  Please let me know. ",
            "author": "Karl Wright",
            "date": "2018-09-26T07:32:28+0000"
        },
        {
            "id": "comment-16628586",
            "content": "Karl Wright - One thing that wasn't very clear to me reading through the issue description and comments is what's the metadata for and why is it supposed to go through the request URL? I'd appreciate if you can give an example of the metadata for my understanding. Thanks! ",
            "author": "Shalin Shekhar Mangar",
            "date": "2018-09-26T10:55:19+0000"
        },
        {
            "id": "comment-16628630",
            "content": "Shalin Shekhar Mangar, there's no general answer to that question, because there's no one definitive example of metadata.\n\nI refer you to the project page for ManifoldCF here:\n\nhttps://manifoldcf.apache.org/en_US/index.html#What+Is+Apache+ManifoldCF%3F\n\nJust for fun, I dug up a ManifoldCF ticket related to this issue, involving the email connector:\n\nhttps://issues.apache.org/jira/browse/CONNECTORS-1408\n ",
            "author": "Karl Wright",
            "date": "2018-09-26T11:43:30+0000"
        },
        {
            "id": "comment-16628813",
            "content": "Karl Wright I am with Shalin on this. While I appreciate that MCF (which we do refer people to from Solr) is very general framework, I think it would be very useful to have a concrete sample example that shows what kind of information actually goes to the wire.\n\nSpecifically the example that generates meaningful metadata and body (multipart) both of which are ending-up used in Solr. This would really help us to visualize the kind of use-cases, that are very obvious to your project. The link example was about forcing multipart, so was not quite representative. Similarly, Tika generates one part with all parameters. An example that has 2 (3?) meaningful parts would be most helpful I feel. And maybe even something that could go into a Solr test (so does not need to be very long, just truly multipart). ",
            "author": "Alexandre Rafalovitch",
            "date": "2018-09-26T13:58:18+0000"
        },
        {
            "id": "comment-16628827",
            "content": "there's no general answer to that question, because there's no one definitive example of metadata.\n\nThe data may be generic, but it has to be fed into Solr in one of the accepted parameters. This reason why we insist on an example is because we want to know which parameters are sent as part of query string. We also want to find out if you are using it wrong ",
            "author": "Noble Paul",
            "date": "2018-09-26T14:09:51+0000"
        },
        {
            "id": "comment-16628879",
            "content": "\nThe data may be generic, but it has to be fed into Solr in one of the accepted parameters.\n\nUm, this stuff has been working for more than a decade.  Yes, we're using accepted parameters.\n\n\nThis reason why we insist on an example is because we want to know which parameters are sent as part of query string.\n\nOk, if that's what you need, I will put out an all points bulletin on the ManifoldCF user list for a Solr INFO message that contains an example of long metadata.  How many examples do you need to convince yourselves that we're not making this up?\n\n\n ",
            "author": "Karl Wright",
            "date": "2018-09-26T14:45:33+0000"
        },
        {
            "id": "comment-16628903",
            "content": "Karl, we totally believe you that it is happening. We just don't have enough knowledge about your use cases to easily visualize our side of it. I think one or two simple examples would be sufficient, no need to do an all-point. Clearly, even though your use-case was working for a long time, we somehow missed it in our tests/reasoning. So, this discussion is explicitly trying to do better on it than the last time. ",
            "author": "Alexandre Rafalovitch",
            "date": "2018-09-26T15:01:24+0000"
        },
        {
            "id": "comment-16628912",
            "content": "How many examples do you need to convince yourselves that we're not making this up?\n\nLooks like you don't understand the objectives here. We design SolrJ client and server with certain usecases in mind. While doing that we assume that we meet the needs of most/all users. The fact that you had to implement a custom client suggests that either we have failed in that or you have failed in understanding how SolrJ works . I'm sure you wouldn't open a ticket to waste our time. We have also come across so many cases were users are \"holding it wrong\" . That is why a specific example is useful. If we realize that there is a genuine use case that cannot be satisfied by the state-of-the-art SolrJ client, we will work towards improving our code so that you don't have to do the dirty work. The objective of Solr is not to support multipart form posts . It is designed to send in docs/commands and get out query results. The multipart mechanism is just a means to an end. Imagine, Solr working on a non HTTP standard. In that case we still need to support all these use cases. So, please be patient if we are trying to get details ",
            "author": "Noble Paul",
            "date": "2018-09-26T15:07:05+0000"
        },
        {
            "id": "comment-16629146",
            "content": "Hi Noble Paul, Alexandre Rafalovitch, Karl Wright\n\nI am a ManifoldCF user/committer and you will find\u00a0as attached files\u00a0an example of an update request that is sent to Solr after being analyzed by Tika (solr-update-request.txt) and the\u00a0corresponding original file.\nI also have an entity extractor that produce a lot of metadata on files that exceed the URL limits. ",
            "author": "Julien Massiera",
            "date": "2018-09-26T17:08:30+0000"
        },
        {
            "id": "comment-16629853",
            "content": "\nSpecifically the example that generates meaningful metadata and body (multipart) both of which are ending-up used in Solr. \n\nThe data has now been provided, and the Solr [INFO] log line for it as well.  Are you still asking for the multipart request that should be generated by SolrJ for that request?  As I've stated, we have had to modify chunks of SolrJ in order to generate that multipart request; with some work we can probably capture it in an HttpClient wire log, but it is some work. ",
            "author": "Karl Wright",
            "date": "2018-09-27T07:20:16+0000"
        },
        {
            "id": "comment-16629860",
            "content": "I should also note that other prime examples of this issue cannot be added to this ticket for security reasons.  Most of ManifoldCF's clients are integrators; they don't generally have permission to include company content without obtaining specific company permission.   Luckily FranceLabs has a few examples hanging around or it would be a real challenge to put together a real-world example for you guys, since I don't have licensed and operating copies of the worst offending proprietary repositories available to me anymore. ",
            "author": "Karl Wright",
            "date": "2018-09-27T07:25:44+0000"
        },
        {
            "id": "comment-16629864",
            "content": "I've attached a patch, not meant to be applied, which shows the general approach I'd like to explore for a fix.\nThe biggest problems I've had in making this stuff work is figuring out when multipart ought to be used in the HttpSolrClient code.  I therefore propose that there be an explicit METHOD type created for multipart post, and that HttpSolrClient pay attention to that when assembling its payload.  The payload would be assembled solely using the ContentWriter mechanism, but the metadata would go into multipart form fields rather than the URL.  The patch does not contain the modifications to HttpSolrClient yet; I just wanted to initiate the discussion.  Does anyone see a problem with this? ",
            "author": "Karl Wright",
            "date": "2018-09-27T07:31:41+0000"
        },
        {
            "id": "comment-16629971",
            "content": "If I understand correctly, you now have a choice in MCF whether to\n\n\tStream\u00a0the original binary document to Solr's extracting request handler and use Solr's built-in Tika to parse it.\u00a0\n In this case there will NOT be a problem since you won't have much metadata as request params, just the few you would have configured statically\n\tLet MCF do the Tika conversion using Tika Content Extractor (https://manifoldcf.apache.org/release/release-1.10/en_US/end-user-documentation.html#tikaextractor)\nIn this case MCF will have all the various metadata parsed from the docs, that it may want to send to Solr, alongside the plain-text parsed version of the document.\n\n\n\nFor 1) you don't have an issue, as you send the binary stream to /extract endpoint.\n\nFor 2) I wonder why you use /extract\u00a0at all, since Tika has already been invoked on the MCF side. This seems like an anti-pattern. The best way would be to construct a SolrInputDocument on where each literal.xyz params becomes a separate xyz\u00a0field, and where the text body is put into a content field (configurable) and everything is sent to /update as opposed to /extract. In the case of jpg files the body text would of course be empty as there is only metadata to be indexed. ",
            "author": "Jan H\u00f8ydahl",
            "date": "2018-09-27T09:00:06+0000"
        },
        {
            "id": "comment-16630027",
            "content": "Hi Jan H\u00f8ydahl,\n\nThe /extract update handler is misleading in the case of the log I shared with you as it is not the original update/extract of Solr but a custom one that is NOT using Tika. Because, like you said, the document has already been parsed by the Tika of MCF.\u00a0\n\nThe 1) that you mentioned is definitely not a recommended solution in a production environment cause, till now, I experienced a lot of OOM when Tika has to deal with exotic files. As we use Solr as the main search engine, we cannot afford to have an interruption of service\u00a0at the indexing phase, and thus, this proposal is not an option.\u00a0\n\n\u00a0 ",
            "author": "Julien Massiera",
            "date": "2018-09-27T09:17:40+0000"
        },
        {
            "id": "comment-16630133",
            "content": "Hi Jan H\u00f8ydahl, typically for case (2) the /update handler is used, not the /update/extract handler. ",
            "author": "Karl Wright",
            "date": "2018-09-27T10:25:05+0000"
        },
        {
            "id": "comment-16630151",
            "content": "typically for case (2) the /update handler is used, not the /update/extract handler.\nBut the /update handler does not support the literal.xxx parameters, so that makes no sense, does it? ",
            "author": "Jan H\u00f8ydahl",
            "date": "2018-09-27T10:35:29+0000"
        },
        {
            "id": "comment-16630203",
            "content": "Jan H\u00f8ydahl, the example we provided is using type (1) output configuration, as Julien noted.  Do you want a type (2) example?  It will not change the need for multipart post. ",
            "author": "Karl Wright",
            "date": "2018-09-27T11:01:19+0000"
        },
        {
            "id": "comment-16630406",
            "content": "Karl Wright, Jan H\u00f8ydahl,\n\nactually the provided example IS of type 2), as I mentioned, the handler used on Solr side is a modified /update handler, not an /extract, the name is misleading\u00a0I would have renammed it as /update/no-tika and here is its declaration in the solrconfig.xml file :\n\n<requestHandler class=\"com.francelabs.datafari.handler.parsed.ParsedRequestHandler\" name=\"/update/no-tika\" startup=\"lazy\">\n  <lst name=\"defaults\">\n    <str name=\"lowernames\">true</str>\n    <str name=\"fmap.language\">ignored_</str>\n    <str name=\"fmap.source\">ignored_</str>\n    <str name=\"uprefix\">ignored_</str>\n    <str name=\"update.chain\">datafari</str>\n  </lst>\n</requestHandler>\n\n\nIt is not using Tika and understands literal.xxx parameters, so, from my point of view, no need to discuss about this...\n\u00a0 ",
            "author": "Julien Massiera",
            "date": "2018-09-27T13:24:14+0000"
        },
        {
            "id": "comment-16630508",
            "content": "Ok, let's keep the discussion about standard handlers. Then when MCF is not going to stream a huge binary file to Solr but rather send one Solr document with one potentially huge plain-text content field and several other metadata fields. This looks to me like a plain Solr document post to /update handler, in whatever format you'd like? If you can take adavantage of Noble Paul's enhancements to stream the content this can still be a plain document not needing multipart, and no need\u00a0sending data in http params?\n\nHowever, if you have a use case where you both need to post some binary blob to Solr Cell and also need to pass huge metadata in literal params, then things would be different. But I have not seen such a usecase yet? ",
            "author": "Jan H\u00f8ydahl",
            "date": "2018-09-27T14:19:55+0000"
        },
        {
            "id": "comment-16630531",
            "content": "\nThis looks to me like a plain Solr document post to /update handler, in whatever format you'd like? If you can take adavantage of Noble Paul's enhancements to stream the content this can still be a plain document not needing multipart, and no need sending data in http params?\n\nThe streaming part is great.  But if you look at the current master implementation of HttpSolrClient, you will note that all parameters and metadata are folded into the URL for the ContentWriter transmission mechanism.  This fails for us because the URL size can easily exceed 8192 bytes.  That is why we need the multipart post handling even for UpdateRequest/SolrInputDocument requests. ",
            "author": "Karl Wright",
            "date": "2018-09-27T14:34:43+0000"
        },
        {
            "id": "comment-16630547",
            "content": "you will note that all parameters and metadata are folded into the URL for the ContentWriter transmission mechanism\nI don't get it. What parameters and metadata are we talking about here, that you wish to send to Solr's standard /update handler? All the document fields and metadata would go in the POST body, not? Please give an\u00a0example of this type 2) request. Does not need to be an example with a large request, just any request using\u00a0MCF's Tika component and then how things look like when attempting to POST that content to Solr's /update endpoint. ",
            "author": "Jan H\u00f8ydahl",
            "date": "2018-09-27T14:48:37+0000"
        },
        {
            "id": "comment-16630605",
            "content": "Jan H\u00f8ydahl, considering the discussion thread, I don't think that having us send you what we do will convince you that we do it the proper way. I think it would be more helpful for us if you show us the SolrJ code that you envision in order to create a Solr document with some content and some metadata, and stream it to Solr via POST method.\u00a0 ",
            "author": "Julien Massiera",
            "date": "2018-09-27T15:19:02+0000"
        },
        {
            "id": "comment-16630625",
            "content": "I'm trying to understand what's problem. Giving that the challenge is to send a huge file in body and long param. I took the test:\u00a0\n\nhttps://github.com/apache/lucene-solr/blob/c587410f99375005c680ece5e24a4dfd40d8d3eb/solr/solrj/src/test/org/apache/solr/client/solrj/SolrExampleTests.java#L675\n\nadded long param into:\n\nup.setParam(CommonParams.HEADER_ECHO_PARAMS, CommonParams.EchoParamStyle.ALL.toString());\n{{ { // added long param}}\n{{ StringBuilder sb = new StringBuilder();}}\n{{ for(int i=0; i<10000000; i++) {}}\n{{ sb.append((char)('a'+((char)(i%26))));}}\n{{ }}}\n{{ String longparam = sb.toString();}}\n{{ //System.out.println(longparam.length());}}\n{{ up.setParam(\"b\", longparam);}}\n{{ }}}\n{{ up.setAction(AbstractUpdateRequest.ACTION.COMMIT, true, true);}}\n\n\u00a0\n\nThen I run\u00a0SolrExampleJettyTest and it passed. Is it possible if Manifold request by the same way?\u00a0 ",
            "author": "Mikhail Khludnev",
            "date": "2018-09-27T15:38:02+0000"
        },
        {
            "id": "comment-16630851",
            "content": "Please examine the following code from master HttpSolrClient.java:\n\n\n      if(contentWriter != null) {\n        String fullQueryUrl = url + wparams.toQueryString();\n        HttpEntityEnclosingRequestBase postOrPut = SolrRequest.METHOD.POST == request.getMethod() ?new HttpPost(fullQueryUrl) : new HttpPut(fullQueryUrl);\n        postOrPut.addHeader(\"Content-Type\",\n            contentWriter.getContentType());\n        postOrPut.setEntity(new BasicHttpEntity(){\n          @Override\n          public boolean isStreaming() {\n            return true;\n          }\n\n          @Override\n          public void writeTo(OutputStream outstream) throws IOException {\n            contentWriter.write(outstream);\n          }\n        });\n        return postOrPut;\n\n      } else if (streams == null || isMultipart) {\n\n\n\nThe request is formed by taking all the parameters in wparams (which include the metadata fields AFAICT) and putting them into the URL:\n\n\n        HttpEntityEnclosingRequestBase postOrPut = SolrRequest.METHOD.POST == request.getMethod() ?new HttpPost(fullQueryUrl) : new HttpPut(fullQueryUrl);\n\n\n\nThere is no other way in the SolrJ request handling code for PUT and POST requests to transmit metadata to Solr.  \n\nIndeed, right now, both documents added to an UpdateRequest, as well as documents that are specified via ContentStreamUpdateRequest, go by this route.  We did verify that using the 7.5.0 version of SolrJ and completely removing all ManifoldCF custom code led to documents that would exceed the maximum URL length if their metadata was long enough. ",
            "author": "Karl Wright",
            "date": "2018-09-27T18:15:48+0000"
        },
        {
            "id": "comment-16630999",
            "content": "If by \"metadata\" you mean the &literal.myfield=value\u00a0http parameters that the ExtractingRequestHandler expects, then why would you send those on a normal update request containing a SolrInputDocument with all fields embedded?\n\nI.e. instead of this (which does not even make sense since JSON update handler does not support literal param)\n\ncurl -XPOST http://localhost:8983/solr/foo/update?literal.id=1&literal.author=George&literal.title=Hello\n\nyou post all metadata as fields in the body:\n\ncurl -XPOST http://localhost:8983/solr/foo/update -H \"Content-type: application/json\" -d '[{\"id\":1\", \"author\":\"George\", \"title\":\"Hello\"}]'\n ",
            "author": "Jan H\u00f8ydahl",
            "date": "2018-09-27T20:29:04+0000"
        },
        {
            "id": "comment-16631041",
            "content": "Karl, fwiw\u00a0SolrExampleTests.testMultiContentStreamRequest() bypasses the code path you pointed me on. I still not fully understand, but why don't pass all it needs via ContentStreamUpdateRequest.addFile() and .setParam() instead of ContentWriter? I've checked that long wparams\u00a0encoded and passed as a separate part keeping URL short. \n  ",
            "author": "Mikhail Khludnev",
            "date": "2018-09-27T20:58:21+0000"
        },
        {
            "id": "comment-16631057",
            "content": "Mikhail Khludnev, your walkthrough in the code is fine but (a) when we use ContentStreamUpdateHandler in the manner you describe to the update/extract handler, we still wind up going through the contentWriter clause above where you stop, and (b) when we use UpdateHandler in the manner you describe we also go through that same path.  In fact I could find no way to send the content through any other path with the code as it exists in master right now, because in our usage there's always a contentWriter and the check for its presence excludes all else that happens after that.  So I don't understand where the disconnect is.  Perhaps if you attach the exact code you are testing we can resolve this.\n ",
            "author": "Karl Wright",
            "date": "2018-09-27T21:12:24+0000"
        },
        {
            "id": "comment-16631059",
            "content": "Jan H\u00f8ydahl, so your suggestion is to use JSON format for the body, and put the metadata into that.  How do you suggest we handle binary data that is meant for SolrCell?  Encoding the binary in a JSON document is possible but in practice this is quite verbose, yielding 3 or 4 bytes to one.  Is that nevertheless your official suggestion?\n\nAlso, how do you force SolrJ to transmit the right mime type to Solr, as well as the document name field (which SolrCell cares about), if you use JSON encoding?  I assume that you have to signal this somehow?  The code seems to get the mime type from the Request, but it's not set anywhere by the user, so I presume this is either set by default or there is some way to set it? ",
            "author": "Karl Wright",
            "date": "2018-09-27T21:15:17+0000"
        },
        {
            "id": "comment-16631063",
            "content": "This is somewhat scary interesting. see SOLR-12798-reproducer.patch. If we sent just one file it disables multipart and huge params might go to URL. After that, test hangs fails with the log message\n\n390718 WARN  (qtp1324113116-23) [    x:collection1] o.e.j.h.HttpParser URI is too large >8192\n\nHttpSolrClient$RemoteSolrException: Error from server at http://127.0.0.1:55150/solr/collection1: Expected mime type application/octet-stream but got text/html. <h1>Bad Message 414</h1><pre>reason: URI Too Long</pre> ",
            "author": "Mikhail Khludnev",
            "date": "2018-09-27T21:27:28+0000"
        },
        {
            "id": "comment-16631066",
            "content": "so your suggestion is to use JSON format\nNot at all. My cURL examples were just to discus what \"metadata\" might mean in this context. In a pure type-2) case where Tika runs in MCF one would\u00a0construct documents with all metadata as fields in those documents. So I still don't understand why/how you'd get those long URLs at all in this scenario, since all the content goes into the streamed body. But I have not tested this streaming fashion use of SolrJ myself, I have just compiled in-memory SolrInputDocuments as usual and understand that you want to be memory efficient here and stream those docs as far as possible. ",
            "author": "Jan H\u00f8ydahl",
            "date": "2018-09-27T21:29:27+0000"
        },
        {
            "id": "comment-16631084",
            "content": "Jan H\u00f8ydahl, if you didn't mean that the metadata and content should be sent in the content body, then I'm completely missing what your suggestion is.\n\n\nMy cURL examples were just to discus what \"metadata\" might mean in this context.\n\nRepositories that are crawled by ManifoldCF have documents that are represented as follows:\n\n\tA long content stream, binary\n\tN pairs of name/value data, called metadata, which is fielded data associated with the document\n\n\n\nIf the metadata is extracted in a ManifoldCF pipeline from the content stream, it's done via Tika, from a binary stream, which changes the binary content stream to a simple text stream, and also supplies more metadata generated as a result of the extraction.  In other words, your JSON example is not like anything we do at all at this time.\n\nIf you want this translated into CURL, you can do it one of two ways:\n(1) Put the metadata onto the URL as & parameters, e.g. name1=value1&name2=value2 etc, or\n(2) Send the metadata as sections in a multipart post.  This too can be set up in CURL if you want me to propose an example.  Each section in a multipart post has a name, and you can thus transmit a section for every metadata name/value pair, as well as one for the content part (which has its own name, that is in fact used by SolrCell for metadata of its own.)\n\nHope this helps. ",
            "author": "Karl Wright",
            "date": "2018-09-27T21:43:20+0000"
        },
        {
            "id": "comment-16631107",
            "content": "How do you suggest we handle binary data that is meant for SolrCell?\nThat would be for case 1) where \u00a0you don't do Tika stuff on the MCF side but want Solr to handle the binary stream. In this case there should be\u00a0no problem with huge metadata request params. And I agree that SolrJ should support this case (ContentStreamUpdateRequest?). I got confused by your other use case where you parse the file with Tika on the MCF side and still sent the text to /extract.\n\nAs I understand, this Jira issue is really\u00a0mainly about the classic use case where you do NOT invoke Tika on client side but stream binary content to SolrCell and still need some Url parameters, and doing this in SolrJ is broken somehow. In this case there will NOT be huge metadata to pass as URL parameters, right? ",
            "author": "Jan H\u00f8ydahl",
            "date": "2018-09-27T22:17:19+0000"
        },
        {
            "id": "comment-16631127",
            "content": "Jan H\u00f8ydahl,\n\nYour proposal could resolve the long URL problem , but how would you create a Solr document (XML, JSON or CSV cause if I am not wrong, these are the only three formats that the update handler of Solr can manage) based on some metadata and a content file (which\u00a0in my case is\u00a0pure text) without having to entirely read the content file to inject it to the Solr document ? \nI think it will have hudge performance impact when one have to crawl millions of documents if not billions ....\nThe Solr Output connector of MCF is currently just constructing a simple POST request with document metadata as parameters and\u00a0the\u00a0content file as stream. Your solution will add a\u00a0significant step before sending the document. Am I wrong ?\u00a0 ",
            "author": "Julien Massiera",
            "date": "2018-09-27T22:31:54+0000"
        },
        {
            "id": "comment-16631149",
            "content": "Jan H\u00f8ydahl:\n\n\nThat would be for case 1) where  you don't do Tika stuff on the MCF side but want Solr to handle the binary stream. In this case there should be no problem with huge metadata request params. And I agree that SolrJ should support this case (ContentStreamUpdateRequest?).\n\nOk.  At the moment that sort of request seems to be transmitted with standard POST with metadata stuffed into the URL.  So a fix is needed for that.\n\n\nI got confused by your other use case where you parse the file with Tika on the MCF side and still sent the text to /extract\n\n\n\nWhile Julien has a custom Solr handler, that's not what we typically do, and we recommend that already-Tika-extracted content and metadata be sent to the /update handler.  In that case, we build a SolrInputDocument from the content stream, and add it into an UpdateRequest.  This mode of usage also seems to use standard POST or even PUT, and it puts all the metadata parameters on the URL.  This is transmitted to the /update handler.  Do you want to support the case where the metadata parameters are sizable enough that the URL exceeds 8192 bytes?\n\n\n\n ",
            "author": "Karl Wright",
            "date": "2018-09-27T22:54:12+0000"
        },
        {
            "id": "comment-16631211",
            "content": "I wanted to chime in here because we have run into the problem of the body of large POST requests getting encoded in the URL in a different scenario and it would be nice if there was a solution for this. To work around the problem we have had to\u00a0copy and modify Solr classes.\n\nOur use case is not a common one: we sometimes make query requests to a custom handler with a very large number of integer values encoded into a RoaringBitMap. On the client side it is not a big problem, we\u00a0created a\u00a0subclass of\u00a0HttpSolrClient.Builder\u00a0that\u00a0set\u00a0UseMultiPartPost to true. This is passed in to the LBHttpSolrClient which\u00a0in turn is passed into CloudSolrClient.\n\nThe problem that was harder to solve was in the HttpShardHandler\u00a0on the Solr nodes, which ends up encoding the parameters in the URL. The work around we came up with was to duplicate and modify HttpShardHandler so we could again set\u00a0UseMultiPartPost to true. We also had to\u00a0subclass\u00a0HttpShardHandlerFactory\u00a0and HttpSolrClient.Builder.\n\nIt would be great if there\u00a0was a way to force the request both on the Solrj client side and in the requests made between the nodes to use multipart requests.\n\n\u00a0 ",
            "author": "Michael Schumann",
            "date": "2018-09-28T00:27:01+0000"
        },
        {
            "id": "comment-16631337",
            "content": "\nAn ideal solution would be\n\n\tBe able to construct a SolrInputDocument with a binary payload + metadata parameters for that doc\n\tWhen this is sent to Solr, SolrJ should sent the payload+parameters in the body\n\tThis ensures that the query string length is always constant\n\tThis also helps in inter-node communication where the documents are sent between replicas\n\n\n\nI'm not sure if we can achieve this without some changes at the server side too. Meanwhile we may need a custom HttpSolrClient implementation that can do a multipart request ",
            "author": "Noble Paul",
            "date": "2018-09-28T04:33:15+0000"
        },
        {
            "id": "comment-16631430",
            "content": "Ok. turns out to trigger passing params as a part of multipart request one needs to pass at least two named streams. Here SOLR-12798-workaround.patch. Karl Wright would you mind to evaluate a quick workaround, after binary payload is added as a content stream, can it add a named add-nothing stream as well like in the patch below? \n\n   up.addContentStream(new ContentStreamBase.StringStream(\"<add></add>\") {\n     {\n       setName(\"multipart trigger. SOLR-12798\");\n     }\n   });\n\n\n\nRegarding the more or less appropriate fix: should we pass params as multipart with POST always? or try to estimate  their size, and put so only long one?   ",
            "author": "Mikhail Khludnev",
            "date": "2018-09-28T06:58:14+0000"
        },
        {
            "id": "comment-16631502",
            "content": "The ideal fix is to avoid multipart altogether because we control both ends of the communication.  ",
            "author": "Noble Paul",
            "date": "2018-09-28T08:04:18+0000"
        },
        {
            "id": "comment-16631514",
            "content": "Noble Paul not sure I follow. How we can avoid multipart when we have one big chunk as a content stream and one chunk with huge params? ",
            "author": "Mikhail Khludnev",
            "date": "2018-09-28T08:14:53+0000"
        },
        {
            "id": "comment-16631520",
            "content": "How we can avoid multipart when we have one big chunk as a content stream and one chunk with huge params?\nI have still not seen the usecase for this.\u00a0Why would there be huge params when you post a binary content stream to SolrCell? The params would come from the metadata inside the binary docs, which are unpacked on the Solr server side? You could of course have large metadata about a PDF sitting in a database on the client and want to post that with the binary doc but as I understand the usecase for MCF, the huge metadata is parsed from the binary doc by Tika on the server side? ",
            "author": "Jan H\u00f8ydahl",
            "date": "2018-09-28T08:26:25+0000"
        },
        {
            "id": "comment-16631548",
            "content": "Jan H\u00f8ydahl, here you can see how ManifoldCF accomplish content stream blob with long params. https://github.com/apache/manifoldcf/blob/11f8021c22c7fc141d237970b713b197992b5921/connectors/solr/connector/src/main/java/org/apache/manifoldcf/agents/output/solr/HttpPoster.java#L1224\nYou can see particular params attached. \nI just replying your question literally, regardless of my (lack of) understanding nor opinion regarding this design.  ",
            "author": "Mikhail Khludnev",
            "date": "2018-09-28T08:50:21+0000"
        },
        {
            "id": "comment-16631565",
            "content": "Mikhail Khludnev \n we will post the data as follows\n\n{\n\"docs\" :[\n{\"params:{ \"a\":\"b\",\"c\":\"d\"},\n\"payload\" : \"<binary-payload-doc1>\"\n},\n{\"params:{ \"p\":\"q\",\"r:\"s\"},\n\"payload\" : \"<binary-payload-doc2>\"\n}\n]\n}\n\n\n\nOn the serverside, we unmarshal the params first and then read the pay load stream ",
            "author": "Noble Paul",
            "date": "2018-09-28T08:58:40+0000"
        },
        {
            "id": "comment-16631581",
            "content": "here you can see how ManifoldCF accomplish content stream blob with long params\nThis code is for posting to ExtractingHandler, and contains a limited amount of literal metadata, unless the ACLs are huge, which I suppose they may very well be. And that would warrant the multi-part requirement in itself. ",
            "author": "Jan H\u00f8ydahl",
            "date": "2018-09-28T09:06:48+0000"
        },
        {
            "id": "comment-16631608",
            "content": "Assuming I have a PDF file which contains an image that can be \"OCRized\". I have a\u00a0process that sends the PDF to a Tika server that will extract the metadata of the PDF file + the text extracted from the image thanks to Tesseract. At the end of the Tika job, the process\u00a0retrieve two elements : a list of metadata as an arraylist and a file containing the text extracted from the image inside the PDF file. Now, to the metadata list I add the ACLs of the PDF file (which are hudge) and I need\u00a0the metadata and the file\u00a0to be sent as one document to Solr for indexation.\nWhat are you recommendations in term of code to do this in\u00a0the most\u00a0efficient way (in term of memory consumption and performances of course), using SolrJ ?\u00a0 And which handler would you use on Solr side ?\nI will test it and see if I experience the URL limit issue ",
            "author": "Julien Massiera",
            "date": "2018-09-28T09:35:33+0000"
        },
        {
            "id": "comment-16631644",
            "content": "Noble Paul\nI'm not sure why we need to revamp handlers which expect content stream to manage them to read doc fields. The other concern is that now the request with single content stream works like a mine field, when one adds too long params it blows surprisingly. Always stripping params from POST urls make it way more predictable for users.    ",
            "author": "Mikhail Khludnev",
            "date": "2018-09-28T10:03:58+0000"
        },
        {
            "id": "comment-16631860",
            "content": "How do you suggest we handle binary data that is meant for SolrCell?\n\nI would suggest that you don't do this.  At all.  Tika is prone to OOM and JVM crashes, as Julien Massiera already noted.  When this happens in SolrCell, Solr goes down too.  So it's strongly recommended for all users to never use SolrCell in production, which in my opinion means that MCF should not be using SolrCell.  Tika should be separate, so if it explodes, the Solr server keeps running.\n\nThat said... I think support for multi-part POST should be first class in SolrJ, and I would even say that sending separate parts for parameters and the actual body should be what SolrJ always does when it's asked to do POST, so URL limits aren't exceeded no matter what gets thrown at it.  And we need to make sure that multi-part handling on the server side is rock-solid.  (I'm not suggesting there's any problems there ... but if any are found, they need attention)\n\nIt's probably a good idea to support multiple data streams as well in SolrJ.  This would probably require some changes on the server side, and a separate Jira issue.\n\nIf MCF creates SolrInputDocument objects, it can put everything there.  MCF wouldn't need to be concerned about format (the JSON mentioned earlier), only one POST part is required, URL parameters are not needed, and the standard /update handler can be used, even without a change for this issue. ",
            "author": "Shawn Heisey",
            "date": "2018-09-28T13:45:33+0000"
        },
        {
            "id": "comment-16632114",
            "content": "should be what SolrJ always does when it's asked to do POST, so URL limits aren't exceeded no matter what gets thrown at it.\n\nSounds like a good thing to secure robustness. Also the /admin/metrics bug that recently surfaced would benefit if we prefer POST over GET in general more places ",
            "author": "Jan H\u00f8ydahl",
            "date": "2018-09-28T16:41:43+0000"
        },
        {
            "id": "comment-16633065",
            "content": "Attaching SOLR-12798.patch it has raw test reproducing ManifoldCF problem, sure tests need to be improved before commit.\n\n\tContentStreamUpdateRequest is switched to use content stream always instead of content writer, which in its' order doesn't work along side with sending huge params as a separate part. That contradicts with SOLR-11380.\n\tHttpSolrClient is switched to prefer multipart when it's possible.\n\n\n\nAlso, we might think about at least warning or even throwing exception from SolrParams.toQueryString() when it turns to be very long. Opinions?  ",
            "author": "Mikhail Khludnev",
            "date": "2018-09-29T17:20:56+0000"
        },
        {
            "id": "comment-16633241",
            "content": "After thinking some time, I'd agree with Noble Paul. Params are supposed to be meta info, which is supposed to be short and more static. Whereas payload is big and changes everytime. Current manifold's approach (even we fix multiparts) doesn.t let to pass many docs with params attached to each other. Imho it proves design flaw. \n Karl Wright can you go ahead with ugly workaround and lately migrate to passing meta via fields? ",
            "author": "Mikhail Khludnev",
            "date": "2018-09-30T05:14:08+0000"
        },
        {
            "id": "comment-16633252",
            "content": "Mikhail Khludnev Ugly hack has been voted on and shipped.  Hopefully by next round (December) there's a better way though. ",
            "author": "Karl Wright",
            "date": "2018-09-30T05:52:52+0000"
        },
        {
            "id": "comment-16633256",
            "content": "Shawn Heisey\n\n\nI would suggest that you don't do this. At all. Tika is prone to OOM and JVM crashes, as Julien Massiera already noted.\n\nIt's not a very good citizen running inside ManifoldCF either.  We have ability to use the external service version but really that just offshores the problem.  But I agree it's better to keep user-facing services alive if one can.\n\nFor backwards compatibility reasons, we will need to continue to support this mode of operation, but we'll recommend against it, and consider changing our defaults accordingly as well.  FWIW, we've been steadily pushing tickets into the Tika queue and issues are getting addressed.  That's really the best long-term solution. ",
            "author": "Karl Wright",
            "date": "2018-09-30T06:01:14+0000"
        },
        {
            "id": "comment-16634537",
            "content": "Is there Solr Cell ticket to move from those fancy literal.foo param to solr docs/fields format?  ",
            "author": "Mikhail Khludnev",
            "date": "2018-10-01T19:52:53+0000"
        },
        {
            "id": "comment-16635135",
            "content": "Noble Paul, how do you propose to pass binary payloads in json? I've found one SO thread discussing it, which looks not so promising.  ",
            "author": "Mikhail Khludnev",
            "date": "2018-10-02T08:16:57+0000"
        },
        {
            "id": "comment-16639934",
            "content": "untested patch. This lets you post multiple payloads+params in the same request. I plan to add tests later ",
            "author": "Noble Paul",
            "date": "2018-10-05T15:06:32+0000"
        },
        {
            "id": "comment-16640511",
            "content": "We never supported JSON payloads from solrJ. For this format, we only support binary ",
            "author": "Noble Paul",
            "date": "2018-10-06T01:15:43+0000"
        },
        {
            "id": "comment-16640660",
            "content": "with a test case. Using the same contentstream to post multiple file types ",
            "author": "Noble Paul",
            "date": "2018-10-06T10:10:35+0000"
        },
        {
            "id": "comment-16642871",
            "content": "I've created a separate issue to track this SOLR-12843 ",
            "author": "Noble Paul",
            "date": "2018-10-09T06:38:11+0000"
        }
    ]
}