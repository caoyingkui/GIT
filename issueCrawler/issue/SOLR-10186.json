{
    "id": "SOLR-10186",
    "title": "Allow CharTokenizer-derived tokenizers and KeywordTokenizer to configure the max token length",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Duplicate",
        "status": "Resolved"
    },
    "description": "Is there a good reason that we hard-code a 256 character limit for the CharTokenizer? In order to change this limit it requires that people copy/paste the incrementToken into some new class since incrementToken is final.\n\nKeywordTokenizer can easily change the default (which is also 256 bytes), but to do so requires code rather than being able to configure it in the schema.\n\nFor KeywordTokenizer, this is Solr-only. For the CharTokenizer classes (WhitespaceTokenizer, UnicodeWhitespaceTokenizer and LetterTokenizer) (Factories) it would take adding a c'tor to the base class in Lucene and using it in the factory.\n\nAny objections?",
    "attachments": {
        "SOLR-10186.patch": "https://issues.apache.org/jira/secure/attachment/12854035/SOLR-10186.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-02-22T06:01:14+0000",
            "content": "Why is this filed in Solr?  KeywordTokenizer is in Lucene. ",
            "author": "David Smiley",
            "id": "comment-15877536"
        },
        {
            "date": "2017-02-22T15:19:34+0000",
            "content": "Because the only change we'd need to make for KeywordTokenizer is in KeywordTokenizerFactory where we'd have to be sensitive to the presence of a new parameter and use it in the KeywordTokenizer c'tor. The CharacterTokenizer based factories would require changes in both the factory methods and the tokenizers themselves and it seems unnecessary to have two separate JIRAs as all the changes would be pretty trivial. ",
            "author": "Erick Erickson",
            "id": "comment-15878455"
        },
        {
            "date": "2017-02-22T16:41:16+0000",
            "content": "The Factory is also in Lucene: https://github.com/apache/lucene-solr/blob/branch_6x/lucene/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizerFactory.java ",
            "author": "David Smiley",
            "id": "comment-15878686"
        },
        {
            "date": "2017-02-22T19:00:40+0000",
            "content": "Erick,\n\nFirst draft, SOLR-10186.patch, is uploaded which allows CharTokenizer-derived tokenizers and KeywordTokenizer to configure the max token length in their definition in schema.\n\n\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizerFactory.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizer.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizerFactory.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/UnicodeWhitespaceTokenizer.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizer.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizerFactory.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java\n\n\n\nCurrently finishing up relevant comments for the new arguments, modified and new constructors in respective classes and thorough tests.\n\nAs all the classes/tokenizers are part of lucene core, I agree with Mr Smiley of opening JIRA under Lucene project and probably link this JIRA there. \n\nLet me know your thoughts. ",
            "author": "Amrit Sarkar",
            "id": "comment-15878984"
        },
        {
            "date": "2017-02-22T20:10:50+0000",
            "content": "Gah, any class found in IntelliJ by the cmd-o key sequence MUST be in Solr, right? My mistake.\n\nYes, let's open the JIRA in LUCENE if for no other reason than have the Lucene guys notice and comment if they don't like the idea.\n\nErick\n\nP.S. On a quick glance I notice these lines still in the code:\n    if (!args.isEmpty()) \n{\n       throw new IllegalArgumentException(\"Unknown parameters: \" + args);\n     }\n\nSo I think if you specify a tag in the schema file it'll throw an error here. It'd be good to have a test here I should think. ",
            "author": "Erick Erickson",
            "id": "comment-15879134"
        },
        {
            "date": "2017-02-23T06:35:18+0000",
            "content": "Erick,\n\nIf we specify the correct tag in the schema, get(..) and getInt(..) will remove the desired tuple from the arguments and the (!args.isEmpty()) check is for the unknown parameters only.\n\n\n   maxCharLen = getInt(args, \"maxCharLen\", KeywordTokenizer.DEFAULT_BUFFER_SIZE);\n\n   protected final int getInt(Map<String,String> args, String name, int defaultVal) {\n    String s = args.remove(name);\n    return s == null ? defaultVal : Integer.parseInt(s);\n  }\n\n\n\nI will write tests for this too. Opening JIRA under Lucene, and let me know where to have the discussion from the either two. ",
            "author": "Amrit Sarkar",
            "id": "comment-15879971"
        },
        {
            "date": "2017-02-23T11:45:34+0000",
            "content": "SOLR-10186.patch updated with LowerCaseTokenizer included and minor fixes in error information. ",
            "author": "Amrit Sarkar",
            "id": "comment-15880299"
        },
        {
            "date": "2017-02-23T12:42:41+0000",
            "content": "SOLR-10186.patch updated with relevant and thorough test-cases, comments, minor adjustments in code, error messages and indentation:\n\nChanges:\n\n\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/KeywordTokenizerFactory.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizer.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LetterTokenizerFactory.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizer.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/LowerCaseTokenizerFactory.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/UnicodeWhitespaceTokenizer.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizer.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/core/WhitespaceTokenizerFactory.java\nmodified:   lucene/analysis/common/src/java/org/apache/lucene/analysis/util/CharTokenizer.java\nmodified:   lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestUnicodeWhitespaceTokenizer.java\nmodified:   lucene/analysis/common/src/test/org/apache/lucene/analysis/util/TestCharTokenizers.java\nnew file:  lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestKeywordTokenizer.java\n\n\n\nKindly review and will appreciate feedback on this. Thank you for the guidance. ",
            "author": "Amrit Sarkar",
            "id": "comment-15880361"
        },
        {
            "date": "2017-03-06T18:12:24+0000",
            "content": "this is really LUCENE-7705, see that JIRA for status. ",
            "author": "Erick Erickson",
            "id": "comment-15897748"
        }
    ]
}