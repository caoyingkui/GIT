{
    "id": "SOLR-7867",
    "title": "implicit sharded, facet grouping problem with multivalued string field starting with digits",
    "details": {
        "components": [
            "faceting",
            "SolrCloud"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "5.2",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "related parts @ schema.xml:\n\n<field name=\"keyword_ss\" type=\"string\" indexed=\"true\" stored=\"true\" docValues=\"true\" multiValued=\"true\"/>\n<field name=\"author_s\" type=\"string\" indexed=\"true\" stored=\"true\" docValues=\"true\"/>\n\n\nevery document has valid author_s and keyword_ss fields;\n\nwe can make successful facet group queries on single node, single collection, solr-4.9.0 server\n\nq: *:* fq: keyword_ss:3m\nfacet=true&facet.field=keyword_ss&group=true&group.field=author_s&group.facet=true\n\n\nwhen querying on solr-5.2.0 server with implicit sharded environment with:\n\n<!-- router.field -->\n<field name=\"shard_name\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\"/>\n\nwith example shard names; affinity1 affinity2 affinity3 affinity4\n\nthe same query with same documents gets:\n\nERROR - 2015-08-04 08:15:15.222; [document affinity3 core_node32 document_affinity3_replica2] org.apache.solr.common.SolrException; org.apache.solr.common.SolrException: Exception during facet.field: keyword_ss\n        at org.apache.solr.request.SimpleFacets$3.call(SimpleFacets.java:632)\n        at org.apache.solr.request.SimpleFacets$3.call(SimpleFacets.java:617)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at org.apache.solr.request.SimpleFacets$2.execute(SimpleFacets.java:571)\n        at org.apache.solr.request.SimpleFacets.getFacetFieldCounts(SimpleFacets.java:642)\n...\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ArrayIndexOutOfBoundsException\n        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesProducer$CompressedBinaryDocValues$CompressedBinaryTermsEnum.readTerm(Lucene50DocValuesProducer.java:1008)\n        at org.apache.lucene.codecs.lucene50.Lucene50DocValuesProducer$CompressedBinaryDocValues$CompressedBinaryTermsEnum.next(Lucene50DocValuesProducer.java:1026)\n        at org.apache.lucene.search.grouping.term.TermGroupFacetCollector$MV$SegmentResult.nextTerm(TermGroupFacetCollector.java:373)\n        at org.apache.lucene.search.grouping.AbstractGroupFacetCollector.mergeSegmentResults(AbstractGroupFacetCollector.java:91)\n        at org.apache.solr.request.SimpleFacets.getGroupedCounts(SimpleFacets.java:541)\n        at org.apache.solr.request.SimpleFacets.getTermCounts(SimpleFacets.java:463)\n        at org.apache.solr.request.SimpleFacets.getTermCounts(SimpleFacets.java:386)\n        at org.apache.solr.request.SimpleFacets$3.call(SimpleFacets.java:626)\n        ... 33 more\n\n\n\nall the problematic queries are caused by strings starting with digits; (\"3m\", \"8 saniye\", \"2 broke girls\", \"1v1y\")\nthere are some strings that the query works like (\"24\", \"90+\", \"45 dakika\")\n\nwe do not observe the problem when querying with \n-keyword_ss:(0-9)*\n\nupdating the problematic documents (a small subset of keyword_ss:(0-9)*), fixes the query, \nbut we cannot find an easy solution to find the problematic documents\nthere is around 400m docs; seperated at 28 shards; \n-keyword_ss:(0-9)* matches %97 of documents",
    "attachments": {
        "ErrorReadingDocValues.PNG": "https://issues.apache.org/jira/secure/attachment/12748955/ErrorReadingDocValues.PNG",
        "DocValuesException.PNG": "https://issues.apache.org/jira/secure/attachment/12748956/DocValuesException.PNG"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-08-05T23:59:46+0000",
            "author": "Jonathan Gonzalez",
            "content": "The problem rely on the docValues attribute, for some reason reading the dvd file fails after several incremental feedings (at least in my case),  I'm able to reproduce this problem either on SolrCloud and Standalone instance, the query has to have &group.facet=true and the facet field definition docValues=true.\n\nA short-term fix: disable the docValues attribute (docValues=false).\n\nFields definition:\n\n<field name=\"fieldForGrouping\" type=\"int\" indexed=\"true\" stored=\"false\" multiValued=\"false\" omitNorms=\"true\" termVectors=\"false\" termPositions=\"false\" docValues=\"false\"/>\n<field name=\"fieldForFacet\" type=\"string\" indexed=\"true\" stored=\"true\" multiValued=\"true\" omitNorms=\"true\" termVectors=\"false\" termPositions=\"false\" docValues=\"true\"/>\n\n\n\nQuery:\nThe query is using &group.field=<fieldForGrouping>&group.facet=true and a simple facet like:\n\n&facet.field={!key=FacetKey_12345678%20facet.prefix=12345678}fieldForFacet\n\n\n\nThe following image, shows Solr reading the index file of type dvd (Per-Document Values .dvd, .dvm - Encodes additional scoring factors or other per-document information. https://lucene.apache.org/core/5_2_0/core/org/apache/lucene/codecs/lucene50/Lucene50DocValuesFormat.html), enabled by the docValues=true. (https://cwiki.apache.org/confluence/display/solr/DocValues)\n\n\nThen trying to read the facet.prefix value from this dvd file, there is an attempt to read more than the current buffer size causing this issue:\n\n\nChecking the index integrity it seems to be ok, so probably is something in the code reading the document values for numbers.\n\n\nOpening index @ .........\\\\data\\\\index\\\\\n\nSegments file=segments_6j numSegments=1 version=5.1.0 id=9bsp5504j7u6jjf6gxl6zw0oo format= userData={commitTimeMSec=1438876474785}\n  1 of 1: name=_dz maxDoc=801607\n    version=5.1.0\n    id=9bsp5504j7u6jjf6gxl6zw0on\n    codec=Lucene50\n    compound=false\n    numFiles=10\n    size (MB)=601.626\n    diagnostics = {java.vendor=Oracle Corporation, java.version=1.7.0_67, lucene.version=5.1.0, mergeFactor=27, mergeMaxNumSegments=1, os=Windows 8.1, os.arch=amd64, os.version=6.3, source=merge, timestamp=1438876452570}\n    no deletions\n    test: open reader.........OK [took 0.104 sec]\n    test: check integrity.....OK [took 0.800 sec]\n    test: check live docs.....OK [took 0.000 sec]\n    test: field infos.........OK [103 fields] [took 0.000 sec]\n    test: field norms.........OK [0 fields] [took 0.000 sec]\n    test: terms, freq, prox...OK [3335590 terms; 188733439 terms/docs pairs; 139381743 tokens] [took 7.999 sec]\n    test: stored fields.......OK [84670070 total field count; avg 105.6 fields per doc] [took 11.457 sec]\n    test: term vectors........OK [0 total term vector count; avg 0.0 term/freq vector fields per doc] [took 0.000 sec]\n    test: docvalues...........OK [28 docvalues fields; 0 BINARY; 15 NUMERIC; 6 SORTED; 0 SORTED_NUMERIC; 7 SORTED_SET] [took 1.176 sec]\n\nNo problems were detected with this index.\n\nTook 21.662 sec total.\n\n\n ",
            "id": "comment-14659224"
        },
        {
            "date": "2015-08-12T12:08:24+0000",
            "author": "G\u00fcrkan Vural",
            "content": "I can confirm that such a bug exists. Some specific positioned documents in the index are causing this error. If you filter the group/facet query to return only this document the error still exists. For my specific document in the readTerm function start and suffix are computed as 32 and 9 respectively. However term.bytes array has length only 37. If you update the document with the same values the problem disappears. I assume this is because the position in the index is changing. ",
            "id": "comment-14693384"
        },
        {
            "date": "2015-08-25T13:58:26+0000",
            "author": "VIshvendra Singh",
            "content": "Till this bug is solved we can use group.truncate=true instead of group.sort=true.\ngroup.truncate=true counts one document per group for calculating facet count ",
            "id": "comment-14711303"
        },
        {
            "date": "2015-08-25T14:16:36+0000",
            "author": "Umut Erogul",
            "content": "just adding group.truncate=true to the query did not solve the problem on my test.\ndid it work, on your test; or was it an idea to try out ? ",
            "id": "comment-14711344"
        },
        {
            "date": "2015-08-26T05:25:26+0000",
            "author": "VIshvendra Singh",
            "content": "It solved my problem.\nI think solr 5.2.1 do have problem with using group.facet=true\ni used group.truncate\nit counts facet based on most relevant doc of a group.till problem is not solved try this ",
            "id": "comment-14712524"
        },
        {
            "date": "2015-11-24T09:05:07+0000",
            "author": "G\u00fcrkan Vural",
            "content": "Any updates to this bug? ",
            "id": "comment-15024046"
        },
        {
            "date": "2015-12-17T17:39:10+0000",
            "author": "Vishnu Mishra",
            "content": "We are using Solr 5.3.1 and facing the same issue with group.facet. Any progress? ",
            "id": "comment-15062403"
        },
        {
            "date": "2017-03-13T10:52:03+0000",
            "author": "Gianpaolo Lopresti",
            "content": "Hi,\nthis issue is still present on Solr 6.2.1, and we can't use group.truncate because we need the facet grouping just on one field (and group.truncate cannot be defined on a per-field basis). Does anyone have some update on this?\nThanks. ",
            "id": "comment-15907159"
        },
        {
            "date": "2018-04-18T07:17:49+0000",
            "author": "Jay",
            "content": "I am seeing similar error in Solr 5.3 & 6.6.3. In my case, the\u00a0error happens intermittently and\u00a0all the values are alphanumeric (don't start with digit as reported above). Reindexing the document seems to address the issue. Have not been able to verify if it happens after reindexing it. ",
            "id": "comment-16442022"
        }
    ]
}