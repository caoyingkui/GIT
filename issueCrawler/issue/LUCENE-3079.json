{
    "id": "LUCENE-3079",
    "title": "Faceting module",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/facet"
        ],
        "type": "Improvement",
        "fix_versions": [
            "3.4",
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Faceting is a hugely important feature, available in Solr today but\nnot [easily] usable by Lucene-only apps.\n\nWe should fix this, by creating a shared faceting module.\n\nIdeally, we factor out Solr's faceting impl, and maybe poach/merge\nfrom other impls (eg Bobo browse).\n\nHoss describes some important challenges we'll face in doing this\n(http://markmail.org/message/5w35c2fr4zkiwsz6), copied here:\n\n\nTo look at \"faceting\" as a concrete example, there are big the reasons \nfaceting works so well in Solr: Solr has total control over the \nindex, knows exactly when the index has changed to rebuild caches, has a \nstrict schema so it can make sense of field types and \npick faceting algos accordingly, has multi-phase distributed search \napproach to get exact counts efficiently across multiple shards, etc...\n(and there are still a lot of additional enhancements and improvements \nthat can be made to take even more advantage of knowledge solr has because \nit \"owns\" the index that we no one has had time to tackle)\n\n\n\nThis is a great list of the things we face in refactoring.  It's also\nimportant because, if Solr needed to be so deeply intertwined with\ncaching, schema, etc., other apps that want to facet will have the\nsame \"needs\" and so we really have to address them in creating the\nshared module.\n\nI think we should get a basic faceting module started, but should not\ncut Solr over at first.  We should iterate on the module, fold in\nimprovements, etc., and then, once we can fully verify that cutting\nover doesn't hurt Solr (ie lose functionality or performance) we can\nlater cutover.",
    "attachments": {
        "TestPerformanceHack.java": "https://issues.apache.org/jira/secure/attachment/12484466/TestPerformanceHack.java",
        "LUCENE-3079_4x_broken.patch": "https://issues.apache.org/jira/secure/attachment/12484663/LUCENE-3079_4x_broken.patch",
        "LUCENE-3079.patch": "https://issues.apache.org/jira/secure/attachment/12483461/LUCENE-3079.patch",
        "LUCENE-3079_4x.patch": "https://issues.apache.org/jira/secure/attachment/12484672/LUCENE-3079_4x.patch",
        "facet-userguide.pdf": "https://issues.apache.org/jira/secure/attachment/12484629/facet-userguide.pdf",
        "LUCENE-3079-dev-tools.patch": "https://issues.apache.org/jira/secure/attachment/12483959/LUCENE-3079-dev-tools.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2011-06-22T17:33:07+0000",
            "content": "This patch was generated by git and tested to apply with\npatch -p0 -i LUCENE-3079.patch --dry-run\nBe patient if anything went wrong.\n\nReview starting points may be\n\n\tFacetSearcherTest.testSimpleFacetWithIndexSearcher() or\n\tFacetSearcher.facetCollectSearch()\n\n\n\nFunctions.java may be  dismissed in favor of Guava.\nIf you are willing to keep it I'll strip it down to the required parts.\n\n----------------------------------------------------------------------\n\nThe implementation relies on field cache only, no index scheme, no \ncached filters etc. It supports\n\n\tsingle valued facets (Facet.java)\n\tmulti valued facets (Facet.MultiValued.java)\n\tfacet filters (see FacetSearcher.java)\n\tevaluation of facet values that would dismiss due to other facet\nfilters (Yonik says Solr calls this \"multi-select faceting\").\n(realized by FacetSearcher.fillFacetsForGuiMode())\n\n\n\nLet me explain the last point: For the user a facet query\n\u00a0 (color==green) AND (shape==circle OR shape==square)\nmay look like\n\nFacet color\n[ ] (3) red\n[x] (5) green\n[ ] (7) blue\n\nFacet shape\n[x] (9) circle\n[ ] (4) line\n[x] (2) square\n\nThe red/blue/line facet values will display even though the \ncorresponding documents are not in the result set. Also there is \nsupport for filtered facet values with zero results, so users \nunderstand why they do not get results. ",
            "author": "Stefan Trcek",
            "id": "comment-13053361"
        },
        {
            "date": "2011-06-23T01:08:30+0000",
            "content": "if Solr needed to be so deeply intertwined with caching, schema, etc., other apps that want to facet will have the same \"needs\"\n\nSort of an aside, but not really.... specific applications are much easier.  A lot more indirection is required in Solr and a schema is needed for pretty much everything.  Without the schema, a client would specify \"sort=foo desc\" and Solr would have no idea how to do that.  A specific application just does it because they have the knowledge of what all the fields are.  It's why people have gotten along just fine without a schema in Lucene thus far.  If you're building another Solr... yes, you need something like a schema.\n ",
            "author": "Yonik Seeley",
            "id": "comment-13053580"
        },
        {
            "date": "2011-06-23T01:19:22+0000",
            "content": "Schemas should probably be a module that makes use of embedding the field types per-segment, this is something the faceting module could/should use.  I think is what LUCENE-2308 is aiming for?  Though I thought there was another Jira issue created by Simon for this as well. ",
            "author": "Jason Rutherglen",
            "id": "comment-13053581"
        },
        {
            "date": "2011-06-23T01:24:29+0000",
            "content": "I don't think any Facet module needs to be concerned with Schemas.  Instead the module can expose an API which asks for the information it needs to make the best choices.  Solr can then provide that information based on its Schema, pure Lucene users can do it however they want.  ",
            "author": "Chris Male",
            "id": "comment-13053587"
        },
        {
            "date": "2011-06-23T01:32:07+0000",
            "content": "I don't think any Facet module needs to be concerned with Schemas\n\nRight, they should be field type aware. ",
            "author": "Jason Rutherglen",
            "id": "comment-13053591"
        },
        {
            "date": "2011-06-23T13:36:09+0000",
            "content": "I would like to contribute IBM's faceted search package (been wanting to do that for quite a while). The package supports the following features/capabilities (at a high-level):\n\n\n\tTaxonomy index \u2013 manages trees of 'categories'. You can view example of taxonomies at e.g. the Open Directory Project.\n\t\n\t\tIt's a Lucene index managed alongside the content index.\n\t\tBuilds the taxonomy on-the-fly (i.e. as categories are discovered).\n\t\tIn general it maps a category hierarchy to ordinals (integers). For example, the category /date/2011/06/24 will create the following entry in the taxonomy index:\n\t\t\n\t\t\t/date, ordinal=1\n\t\t\t/date/2011, ordinal=2\n\t\t\t/date/2011/06, ordinal=3\n\t\t\t/date/2011/06/24, ordinal=4\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tFacetsDocumentBuilder which receives a list of categories that are associated w/ the document (can be of several dimensions) and:\n\t\n\t\tFetches the ordinals of the category components from the taxonomy index (adding them to it on-the-fly).\n\t\tIndexes them in a (compressed) payload for the document (so for the above category example, 4 payloads will be indexed for the document).\n\t\tFDB can be used to augment a Document with other fields for indexing (it adds its own Field objects).\n\t\n\t\n\n\n\n\n\tFacetsCollector receives a handle to the taxonomy, a list of facet 'roots' to count and returns the top-K categories for each requested facet:\n\t\n\t\tThe root can denote any node in the category tree (e.g., 'count all facets under /date/2011')\n\t\ttop-K can be returned for the top most K immediate children of root, or any top-K in the sub-tree of root.\n\t\n\t\n\n\n\n\n\tCounting algorithm (at a high-level):\n\t\n\t\tFetch the payload for every matching document.\n\t\tIncrement by 1 the count of every ordinal that was encountered (even for facets that were not requested by the user)\n\t\tAfter all ordinals are counted, compute the top-K on the ones the user requested\n\t\tLabel the result facets\n\t\n\t\n\n\n\n\n\tMiscellaneous features:\n\t\n\t\tSampling algorithm allows for more efficient facets counting/accumulation, while still returning exact counts for the top-K facets.\n\t\tComplements algorithm allows for more efficient facets counting/accumulation, when the number of results is > 50% of the docs in the index (we keep a total count of facets, count facets on the docs that did not match the query and subtract).\n\t\t\n\t\t\tComplements can be used to count facets that do not appear in any of the matching documents (of this result set). This does not exist in the package though ... yet.\n\t\t\n\t\t\n\t\tFacets partitioning \u2013 if the taxonomy is huge (i.e. millions of categories), it is better to partition them at indexing time, so that search time is faster and consumes less memory. Note that this is required because of the approach of counting all (allocating a count array) and then keeping only the results of interest.\n\t\tCategory enhancements allow storing 'metadata' with categories in the index, so that more than simple counting can be implemented:\n\t\t\n\t\t\tweighted facets (built on top of enhancements) allows associating a weight w/ each category, and use smarter counting techniques at runtime. For example, if facets are generated by an analytics component, the confidence level can be set as the category's weight. If tags are indexed as facets (for e.g. generating a tag cloud for the result set), the number of times the document was tagged by the tag can be set as the tag's weight.\n\t\t\n\t\t\n\t\tThat that facets are indexed in the payloads of documents allows managing very large taxonomies and indexes, without blowing up the RAM at runtime (but incur some search performance overhead). However, the payloads can be loaded up to RAM (like in FieldCache) in which case runtime becomes much faster.\n\t\t\n\t\t\tHowever facets are stored is abstracted though by a CountingList API, so we can definitely explore other means of storing the facet ordinals. Actually, the CountingList API allows us to read the ordinals from disk or RAM w/o affecting the rest of the algorithm at all.\n\t\t\n\t\t\n\t\tI did not want to dive too deep on the API here, but the runtime API is very extensible and allows one to use FacetsCollector for the simple cases, or lower-level API to get more control on the process. You can look at: FacetRequest, FacetSearchParams, FacetResult, FacetResultNode, FacetsCollector, FacetsAccumulator, FacetsAggregator for a more extensive set of API to use.\n\t\n\t\n\n\n\n\n\tThe package comes with example code which shows how to use the different features I've mentioned. There are also unit tests for ensuring the example code works .\n\n\n\n\n\tThe package comes with a very extensive tests suite and is in use by many of our products for a long time, so I can state that it's very stable.\n\n\n\n\n\tSome rough performance numbers:\n\t\n\t\tCollection of 1M documents, few hierarchical facets per document (we call it the 'Amazon' case)\n\t\tQuery matches 49.8% of the docs (~500K)\n\t\tNo sampling / complements were used.\n\t\tFacets read from disk\n\t\tTakes 0.4 seconds to execute the query and count facets.\n\t\n\t\n\n\n\nIt will take me a few days to prepare a patch (lots of code) - will upload it as soon as it's ready. ",
            "author": "Shai Erera",
            "id": "comment-13053847"
        },
        {
            "date": "2011-06-23T14:33:37+0000",
            "content": "Bravo Shai & IBM! ",
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13053886"
        },
        {
            "date": "2011-06-23T16:00:48+0000",
            "content": "Bravo Shai & IBM!\n\n+1!  This sounds awesome, and I hope will prove how modules will help lucene and solr ",
            "author": "Ryan McKinley",
            "id": "comment-13053940"
        },
        {
            "date": "2011-06-27T08:51:35+0000",
            "content": "This is quite another design than the quarter-baked one I've proposed with SOLR-2412 (which is really just a thin wrapper around LUCENE-2369). While maintaining a sidecar index makes the workflow more complicated, I would expect that it is beneficial for re-open speed and scalability.\n\nTechnical note: For hierarchical faceting, I find that it is possible to avoid storing all levels in the hierarchy. By maintaining two numbers for each tag, denoting the tag-level and the level for the previous tag that matches, only the relevant tags needs to be indexed (full explanation at https://sbdevel.wordpress.com/2010/10/05/fast-hierarchical-faceting/).\n\nKudos for contributing solid code. I am looking forward to seeing the patch. ",
            "author": "Toke Eskildsen",
            "id": "comment-13055402"
        },
        {
            "date": "2011-06-27T10:00:34+0000",
            "content": "Thanks Toke for the pointer. I think it's very interesting. We've actually explored in the past storing just the category/leaf, instead of the entire hierarchy, in the document. The search response time was much slower than what I reported above (nearly 2x slowdown). While storing the entire hierarchy indeed consumes more space, it is more performing at search time, and we figure that space today is cheap, and usually search apps are more interested in faster search response times and are willing to spend some more time at indexing and analysis stages.\n\nNevertheless, the link you provided proposes an interesting way to manage the hierarchy, and I think it's worth exploring at some point. Could be that it will perform better than how we managed it when we indexed just the leaf category for each document. We'd also need to see how to update the taxonomy on the go. For example, it describes that for A/B/C you know that its level is 3 (that's easy) and that the previous category/tag that matches (P) is A. But what if at some point A/B is added to a document? What happens to the data indexed for the doc w/ A/B/C, which now its previous matching category is A/B? It's not clear to me, but could be that I've missed the description in the proposal.\n\nI am very close to uploading the patch. Hopefully I'll upload it by the end of my day. ",
            "author": "Shai Erera",
            "id": "comment-13055443"
        },
        {
            "date": "2011-06-27T11:39:02+0000",
            "content": "SOLR-2412/LUCENE-2369 were created with the trade-offs (relatively) long startup, low memory, high performance: When the index is (re)opened, the hierarchy is analyzed by iterating the terms (it could be offloaded to index-time, but it is still iterate-the-entire-term-list after each change). This does not play well with real-time, but should be a nice fit for large indexes with low update rate.\n\nAs for speed, my theory is that the sparser hierarchy (only the concrete paths) wins due to less counting, but without another solution to compare against it has so far remained a theory. There are some measurements at https://sbdevel.wordpress.com/2010/10/11/hierarchical-faceting/ but I find that for hierarchical faceting, small changes to test-setups can easily have vast implications on performance, so they are not comparable to your million-document test. ",
            "author": "Toke Eskildsen",
            "id": "comment-13055480"
        },
        {
            "date": "2011-06-27T13:02:56+0000",
            "content": "Attached patch includes the faceted search module. It currently compiles \nagainst 3x, so I've put it under lucene/contrib, but after we port it to \ntrunk, it should be under modules/.\n\nThere isn't a short way to describe the content of the patch (as you can \nsee, it's huge), so instead I'll give a brief overview of some key \npackages: \n\n\n\tsrc/examples: contains code of different capabilities of the facets module. I'd start w/ examples/simple.\n\tsrc/test: contains many tests, great place to start too\n\to.a.l.facet.taxonomy contains the taxonomy index management code. There are two interfaces TaxonomyWriter/Reader with a LuceneTW/TR impl\n\to.a.l.facet.index contains the indexing code of the different capabilities (e.g. simpl, enhancements etc.)\n\to.a.l.facet.search contains the respective search code\n\n\n\nFew points:\n\n\tI've put the ASL on all files.\n\tMarked all code as @lucene.experimental.\n\tAfter you apply the patch you can run 'ant eclipse' and it will build the facets code in eclipse (no maven integration yet - will need to look into it)\n\tUnder o.a.l and o.a.l.util there are several utility classes that are not specific to facetted search, however the facets code uses them. I've kept them there so that we can review and decide whether we want to move them to lucene-core at some point.\n\n\n\nTODOs:\n\n\tAfter it's on trunk, I think we should explore replacing the payloads w/ DocValues.\n\tLeverage Lucene's superb random testing framework !\n\tThere are few TODOs in the code which I think can be addressed following this issue.\n\n\n\nI will open follow-on issues for those.\n\nGiven the amount of code, I am wondering if perhaps we should commit it \nas-is, and do more thorough reviews afterwards. The code does not modify \nany existing code (aside from a tiny change to LuceneTestCase), so I \nthink there's no risk in doing so. I am also not sure that it's sane to \nreview that amount of code (nearly 40K lines) in patch form. What do you \nthink? ",
            "author": "Shai Erera",
            "id": "comment-13055517"
        },
        {
            "date": "2011-06-27T13:10:43+0000",
            "content": "Great contribution Shai.  What about putting it into a branch? I think it really does need a thorough review before we put it into trunk. ",
            "author": "Chris Male",
            "id": "comment-13055526"
        },
        {
            "date": "2011-06-27T13:17:53+0000",
            "content": "We can put it in a branch for trunk, in case we plan to refactor the code right away (at first I just thought to get it to compile against trunk). I thought that at first people would like to get hands on experience with it, before we discuss changes and refactoring. I mean, this code can really be released with Lucene's next 3x release. And since everything is @lucene.experimental, and is in its own separate contrib/module, I don't think a branch will ease off the review or refactoring process?\n\nI guess what I'm aiming for is for our users to get this feature soon. And I'm afraid that putting it in a branch will only delay it. ",
            "author": "Shai Erera",
            "id": "comment-13055532"
        },
        {
            "date": "2011-06-27T14:14:43+0000",
            "content": "just some trivial test modifications so the tests work with an unmodified LuceneTestCase:\n\n\n\tin some cases, if an assertion failed it would print the seed... but LTC does this.\n\tin other tests, the test wanted to repeat a random sequence, but instead of exposing LTC internals, the test just grabs random.nextLong, makes a new Random from this, and then resets it with .setSeed.\n\n ",
            "author": "Robert Muir",
            "id": "comment-13055568"
        },
        {
            "date": "2011-06-27T14:17:27+0000",
            "content": "\nI guess what I'm aiming for is for our users to get this feature soon. And I'm afraid that putting it in a branch will only delay it.\n\n+1\n\nMy suggestion:\n\n\tcommit to branch 3.x with @experimental.\n\tnext, do a \"fast\" port to trunk, this doesnt mean heavy refactoring to take advantage of things like docvalues, just get it working correctly on trunk's APIs.\n\tfinally, close this issue and do improvements as normal, backporting whichever ones are easy and make sense, like any other issue.\n\n ",
            "author": "Robert Muir",
            "id": "comment-13055570"
        },
        {
            "date": "2011-06-27T14:43:00+0000",
            "content": "Thanks Robert for the fix. This indeed looks better than patching LTC !\n\nPatch for dev-tools only, this time w/ Maven \nsupport too. I hope it works well . ",
            "author": "Shai Erera",
            "id": "comment-13055583"
        },
        {
            "date": "2011-06-27T14:45:29+0000",
            "content": "\nMy suggestion:\n\n1. commit to branch 3.x with @experimental.\n2. next, do a \"fast\" port to trunk, this doesnt mean heavy refactoring to take advantage of things like docvalues, just get it working correctly on trunk's APIs.\n3. finally, close this issue and do improvements as normal, backporting whichever ones are easy and make sense, like any other issue.\n\nI agree. I'll give it a day or two before I commit, unless everyone agree it can be committed today, in which case I'll happily press the button . ",
            "author": "Shai Erera",
            "id": "comment-13055584"
        },
        {
            "date": "2011-06-28T08:11:54+0000",
            "content": "The patch compiles neatly against a 3x stable checkout and the examples worked fine. Unfortunately I could not locate the million document test you mentioned above. Is it part of the patch? ",
            "author": "Toke Eskildsen",
            "id": "comment-13056377"
        },
        {
            "date": "2011-06-28T08:43:18+0000",
            "content": "Unfortunately I could not locate the million document test you mentioned above. Is it part of the patch?\n\nIt's not included. We wrote a proprietary (not extending Benchmark) test and would like to extend Lucene's benchmark for benchmarking facets. I think it will be interesting to define some facets on the Wikipedia collection, and benchmark it. ",
            "author": "Shai Erera",
            "id": "comment-13056390"
        },
        {
            "date": "2011-06-28T10:08:53+0000",
            "content": "Shai, MASSIVE PATCH! phew I don't want to review this entire thing in a patch really but at a first glance this looks very good. Lots of tests, documentation etc. and obviously this has been used in production so its seen some cpu cycles  I think roberts proposed way is good!\n\n\n+1\n\nMy suggestion:\n\ncommit to branch 3.x with @experimental.\nnext, do a \"fast\" port to trunk, this doesnt mean heavy refactoring to take advantage of things like docvalues, just get it working correctly on trunk's APIs.\nfinally, close this issue and do improvements as normal, backporting whichever ones are easy and make sense, like any other issue.\n\nhere is my +1 ",
            "author": "Simon Willnauer",
            "id": "comment-13056427"
        },
        {
            "date": "2011-06-28T13:54:47+0000",
            "content": "Some preliminary performance testing: I hacked together a test where 1M documents were created with an average of 3.5 paths, down to a depth of 4 (resulting in 1.4M unique paths). A search that hit every other document was issued and the top 5 facets/tags was requested. Hopefully this is somewhat similar to your test.\n\n\n\n\n\u00a0\n LUCENE-3079 \n LUCENE-2369 \n\n\n Index build time \n  52 s \n  23 s \n\n\n Memory required for indexing \n 192 MB \n 48 MB \n\n\n First facet request \n 432 ms \n 12,000 ms \n\n\n Best of 5 requests \n 228 ms \n 159 ms \n\n\n Memory usage after faceting (after gc()) \n 21 MB \n 22 MB \n\n\n\n\n\nUpping the ante to 5M documents, 6.8 paths/docs, max depth 6 (22M unique paths) resulted in\n\n\n\n\n\u00a0\n LUCENE-3079 \n LUCENE-2369 \n\n\n Index build time \n  752 s \n  238 s \n\n\n Memory required for indexing \n 2500 MB \n 128 MB \n\n\n First facet request \n 3370 ms \n 147,000 ms \n\n\n Best of 5 requests \n 2400 ms \n 2673 ms \n\n\n Memory usage after faceting \n 435 MB \n 294 MB \n\n\n\n\n\nScaling down to 100K documents, 1.6 paths/doc, max depth 4 (63K unique paths) resulted in\n\n\n\n\u00a0\n LUCENE-3079 \n LUCENE-2369 \n\n\n Index build time \n  5317 ms \n  2563 ms \n\n\n Memory required for indexing \n 48 MB \n 32 MB \n\n\n First facet request \n 245 ms \n 1425 ms \n\n\n Best of 5 requests \n 15 ms \n 8 ms \n\n\n Memory usage after faceting \n 1 MB \n 2 MB \n\n\n\n\n\nSome observations: It seems clear that some trade-offs are very different for the two solutions. LUCENE-3079 has brilliant startup time and slows analyzing time a bit through the whole indexing process. LUCENE-2369 is dog slow at startup but does not impact indexing. They seem similar with regards to search-time speed and memory usage.\n\nNow, LUCENE-2369 patches some semi-random Lucene-4, so this is not a fair test. Likewise, the tests were just quick hacks; the disk cache was not flushed, the laptop was used for browsing etc. When LUCENE-3079 patches trunk, a proper comparison can be made.\n\nI am a bit worried about the observed memory usage for index build. It seems that LUCENE-3079 uses a lot of heap there? I created the documents one at a time just before adding them to the index, so the memory usage is for the writers and a quick profile told me that it was mainly used for int-arrays. Does that sound right? ",
            "author": "Toke Eskildsen",
            "id": "comment-13056517"
        },
        {
            "date": "2011-06-28T14:58:17+0000",
            "content": "You write LUCENE-3097 which is about \"post group faceting\", while this issue is LUCENE-3079. I assume you meant the latter, but want to confirm . You also write LUCENE-2309 which is about decoupling IW from Analyzers. Are you perhaps referring to a Solr issue, or a different Lucene issue? If so can you please let me know which one?\n\nThis is a great test, and it matches more or less the test we've been running. Is it in 'benchmark' form? Can you post it on this issue so I can try the same?\n\nWhat do you mean by \"top 5 facets/tags\"? If I were to speak of dimensions, where a dimensions is like \"tags\", \"authors\", \"date\", then do you mean you've requested to count 5 dimensions, or you indexed just one dimension (i.e. one \"root\") and requested to fetch the top-5 results for it? I assume it's the latter, but again, confirming my understanding.\n\nSo assuming I understood correctly the terminology and test setup, you execute one query which matches 50% of the documents and ask to count the top-5 facets under a single \"root\"/\"dimension\", and record the time as 'first facet request'. And then you execute it 4-5 additional times, and record 'best of 5 requests'. Do I understand it correctly?\n\nOne difference between the two approaches, assuming you're referring to a faceting approach that uses the FieldCache is that by default, the faceting approach here reads everything from disk. So it would be interesting to run w/ the facets-in-memory feature.\n\nI don't know how to relate to the memory usage \u2013 on the last test it consumed 50% less than the other approach, on the first it consumed nearly the same and on the second test it consumed 150% more. This is odd. Do you trust this measurement?\n\nThe 'first facet request' result is not surprising, because it takes time to warm up the FieldCache (assuming that's what you use).\n\nI am interested in the memory observed for indexing because that too seems fluctuating? I.e., in the second test the difference is nearly x20 more, which is weird.\n\nAlso, the difference in indexing time is interesting too, as it too is not very consistent. And I find the x2 factor suspicious - would like to understand it better. Since trunk reports to improve indexing speed by a large factor (nearly 200%), I think it will be wise if we wait with this comparison until I bring the patch up w/ trunk.\n\nI like it that you test the default behavior. I think it's very important that we have the greatest out-of-the-box experience. Since the two approaches read from disk/memory, I first would like to test the in-memory facets using this approach, so we can at least compare the same thing. I know that trunk plays some role here (definitely at indexing time), so we can focus on search time for now.\n\nThis is great stuff Toke ! ",
            "author": "Shai Erera",
            "id": "comment-13056553"
        },
        {
            "date": "2011-06-28T15:06:15+0000",
            "content": "Edited the issue title (it had an extra 'i' in 'faceting'), as well as added fix versions etc. I intend to commit the patch to the 3x branch either later today or tomorrow. ",
            "author": "Shai Erera",
            "id": "comment-13056560"
        },
        {
            "date": "2011-06-28T15:07:57+0000",
            "content": "Created component modules/facet ",
            "author": "Shai Erera",
            "id": "comment-13056561"
        },
        {
            "date": "2011-06-28T15:36:54+0000",
            "content": "Shai: I completely messed up the JIRA-numbers, clearly I need to go home and cool my brain. It is fixed now. Sorry for the inconvenience.\n\nYes, I only created a single root (one dimension) and requested the top-5 facets. You understood the timing measurements correctly. I am sorry that my table was confusing with regards to memory. The first numbers was the Xmx required for index build (binary search until I got bored), while the second if what the JVM reported after the faceting calls were finished. For LUCENE-2369 in the middle test, the faceted search required more memory (aka higher Xmx) than index build (which could probably have gotten by with even less).\n\nI do not use field cance for LUCENE-2369. It holds a compressed list of ordinals for tags for the documents in memory, with a few levels of indirections to handle doublettes. The startup time is basically due to doublette elimination.\n\nRegarding the memory difference, LUCENE-2369 does not operate at index-time. This means that is it plain Lucene indexing of terms like hierarchy:a/b/c/d. Actually I am surprised that it took 128MB for the larger test and I should probably re-run that with a lower allocation.\n\nMy guesstimage, based purely on observation, is that LUCENE-3079 requires heap relative to the taxonomy size at indexing time. At least with the (assumedly default) settings I used. Thus the 22M unique values in test #2 is the cause for the large memory requirement. Looking at the number of unique tags vs. index memory requirements for case #1 and #2, the factor seems nearly linear. It seems to fit your recommendation of splitting on large taxonomies?\n\nI'll upload my test class for LUCENE-3079 now. I apologize for its hackish nature - this was just meant as explorative work. ",
            "author": "Toke Eskildsen",
            "id": "comment-13056585"
        },
        {
            "date": "2011-06-28T15:38:23+0000",
            "content": "A quick hack to get an idea of performance characteristics for hierarchical faceting. ",
            "author": "Toke Eskildsen",
            "id": "comment-13056586"
        },
        {
            "date": "2011-06-28T17:50:01+0000",
            "content": "resulting in 1.4M unique paths\n\nAND\n\nLUCENE-3079 requires heap relative to the taxonomy size at indexing time\n\nOk now I see what's happening. The taxonomy index maintains a category cache, which maps a category to its ordinal. It's maintained at both indexing and search time. At indexing time, it's used to quickly determine if an incoming category already exists, and if so returns its ordinal. At search time its used to quickly label ordinals, as well as determining hierarchy information.\n\nThe default settings of LuceneTaxonomyWriter uses Cl2oTaxonomyWriterCache, which maintains a mapping of all categories to their ordinal, although the mapping is maintained compact. There is another cache LruTaxonomyWriterCache which as its javadocs states \"good choice for huge taxonomies\".\n\nThe taxonomy you create is HUGE by all standards (and I'm not speaking on the 22M case ). The largest 'normal' taxonomy we've seen was in the order of few 100K nodes (when people names were maintained, and social tags), while the largest 'abnormal' taxonomy we've seen contained ~5M nodes, and that is considered a very extreme case for taxonomies.\n\nJust to be clear, I'm not trying to make excuses. Your perf test is still great in that it tests an extreme case. But I'd expect an extreme case to require some mods to the defaults, and using LruTWC is one of them (w/ a cacheSize of let's say 100/200K). Another thing I've mentioned this package can do is the partitions \u2013 because at runtime we allocate an array the size of the taxonomy, in your case (1.4M nodes), we'll create an array that is ~6MB size for every query. While if you use partitions, and partition the categories into 10/100K-categories buckets, you'd allocate less RAM, but might incur some search performance overhead.\n\nOut of curiosity, w/ 1.4M unique paths, and 1M docs, how many categories are assigned to each document and how many documents are associated w/ each category? When we ran our test, we used a Zipf distribution for categories. If in this test we end up associating only a couple of documents per category, then this is not a too realistic scenario. And while the package can handle it, by not using defaults, perhaps we should define a scenario that makes sense (a common one that is) and run with it?\n\nI don't think there can be one \"right\" faceted search solution, but rather a collection of tools that can match different scenarios. And if it turns out that for one case one implementation is better than another, then our job will be to create a faceted search layer which allows the user to choose what's best for him, and leave the rest of his app code unmodified.\n\nWhat do you think? Perhaps we should open a separate issue, let's call it \"facet benchmarking\", where we define some scenarios, work on extending the benchmark package (we have done some preliminary work there) and then compare few approaches? ",
            "author": "Shai Erera",
            "id": "comment-13056673"
        },
        {
            "date": "2011-06-28T18:11:46+0000",
            "content": "About the TaxonomyWriterCache, looking at its API now, I think that an FST-based TWC might be a good fit here? FST is known for its performance and low RAM consumption, and TWC maps from a CategoryPath (or String) to an integer, which sounds like a typical usage for FST. So we can have both the keep-all-in-RAM-TWC and LRU use FST and consume less memory.\n\nI'll open an issue for that.\n\nOne small correction - TWC is used only at indexing time, mapping from category->ordinal. For labeling ordinals you use TaxonomyReader which maintains its own int->String cache (LRU). Can FST aid in that case as well? I assume it will consume less space than an Integer->String hash map.\n\n-------------\n\nBack to performance \u2013 Toke, did you verify that you get the same top-5 categories from both implementations? Also, can you try running the test asking for top-5 categories of a node below the root? I.e., if the paths are in the form /a/b/c/d and you request to count \"/a\", then I'm interested in how this performs if you ask to count \"/a/b\". ",
            "author": "Shai Erera",
            "id": "comment-13056687"
        },
        {
            "date": "2011-06-28T18:26:13+0000",
            "content": "Oh Toke ... I just reviewed your test and there is a problem in it:\n\n\n    CountFacetRequest facetRequest = new CountFacetRequest(\n        new CategoryPath(HIERARCHICAL), num);\n    facetRequest.setDepth(5);\n\n\n\nYou create a CountFacetRequest, requesting to count HIERARCHICAL (which is the root), and fetch the top <num>, which is 5. BUT, you set the depth of the request to 5, which means it will compute the top-5 categories at each level !\n\nThis is a nice feature of the package, which lets you get not only the top-N child nodes of the immediate \"root\", but also the top-N of their child nodes and it's applied recursively until 'depth'. This is a nice feature, but not very performance friendly .\n\nCan you please rerun the test then, commenting out that line? I can run it on my laptop, but I don't have the env. setup w/ the patch from LUCENE-2369, so I cannot compare.\n\nIn general, this looks like a very useful test. I think we can commit it too, but rename it so that it doesn't run regularly w/ our tests, but rather selectively. ",
            "author": "Shai Erera",
            "id": "comment-13056696"
        },
        {
            "date": "2011-06-28T21:07:51+0000",
            "content": "I must admit that my choices of tests were more aimed at probing edge cases than simulating real taxonomies. Nevertheless, it is good to hear that LUCENE-3079 can be tweaked to handle it. I fully support the idea of a facet benchmarking issue -  perhaps with an associated wiki page?\n\nAs for the 1M test case, the number of tags/documents were random with the average being the stated 3.5 tags/document. Seen from the other side, there were an average of 1M*3.5/1.4M ~= 2.5 documents/tag.\n\nI did verify the facet-results and they did fit my expectations. I will try requesting from further down the tree later - hopefully tomorrow. I am a bit confused about your protest on depth=5, but I suspect that we have different ideas of what is relevant when issuing a hierarchical request. The API states that specifying a depth of 5 will count all sub-tags until depth 5. I used the number 5 to effectively count all the way to the bottom (whoops! It should be 6 for the second case. That might explain why LUCENE-3079 was faster than LUCENE-2369 in that one as LUCENE-2369 counted to the bottom). The reason for the complete counting was that I implicitly found this to be the \"correct\" behavior, internally visioning a taxonomy of species or something similar, with the wish to get the number of unique elements at the finest level.\n\nThinking about this, I now have a better understanding of the duplication of data by indexing all levels of the paths. This speeds up shallow counting tremendously.\n\nAll this confusion supports the need for at coordinated effort to get some test cases with clear goals and realistic data. ",
            "author": "Toke Eskildsen",
            "id": "comment-13056785"
        },
        {
            "date": "2011-06-29T03:22:52+0000",
            "content": "I fully support the idea of a facet benchmarking issue - perhaps with an associated wiki page?\n\nYes. And on the Wiki page you should clearly describe the scenario that is tested, along with results for 'default config' + 'optimized config'. That way, a user coming to the page can pick the scenario that best matches his app and config the facets package (whatever we end up with) accordingly.\n\nSeen from the other side, there were an average of 1M*3.5/1.4M ~= 2.5 documents/tag\n\nThat's indeed an extreme case. We've seen it when an analytics module extracted facets automatically from documents (such as places, people etc.), and in that case the taxonomy was very 'flat' and wide.\n\nI am a bit confused about your protest on depth=5\n\nI did not protest . Actually, the common scenario is to count the immediate children and fetch the top-K. And I thought that that's what you do in LUCENE-2369. But counting all the way down is a valid scenario - and shows another reason why we should have a benchmark page with clear description.\n\nThinking about this, I now have a better understanding of the duplication of data by indexing all levels of the paths. This speeds up shallow counting tremendously.\n\nIt actually speeds up counting overall. If you think about it, when we encounter category ordinals, we just increment the count by 1 in the respective location in the count array. No need to ask whether this is an ordinal the user asked to count at all. Later when we compute the top-K, we know more efficiently while root ordinal the user requested to count, and its children, so it's just a matter of putting everything into a heap and returning the top-K.\n\nAll this confusion supports the need for at coordinated effort to get some test cases with clear goals and realistic data.\n\nIndeed. And we shouldn't pursue only 'realistic data', but edge cases too. As long as everything is clearly documented, it should be easy to interpret results.\n\nI think that setting up facet benchmarking is more important than working on improving any implementation. Mostly because it will allow measuring the how much the improvements really improved. I'll open an issue for that. ",
            "author": "Shai Erera",
            "id": "comment-13056972"
        },
        {
            "date": "2011-06-29T10:54:28+0000",
            "content": "I re-ran the edge case 5M documents, 6.8 paths/docs, max depth 6 (22M unique paths) to drill down to level 6 (d6) and to level 1 (d1). A test for the path 'root/a/b' as starting point for the facet request was added. I also checked that indexing does indeed run with 48MB for LUCENE-2369, but again this is just plain Lucene indexing.\n\n\n\n\n\u00a0\n LUCENE-3079 (d6) \n LUCENE-3079 (d1) \n LUCENE-3079 (d1+'deep/a/b') \n LUCENE-2369 (d6) \n\n\n Index build time             \n  752 s            \n   771 s           \n\n\t\n\n\n     347 s         \n\n\n Memory required for indexing \n 2500 MB           \n 2500 MB           \n   2500 MB           \n      48 MB        \n\n\n First facet request          \n 3840 ms           \n 1929 ms           \n   1963 ms           \n 147,000 ms        \n\n\n Best of 5 requests           \n 2688 ms           \n 1172 ms           \n   1246 ms           \n    2673 ms        \n\n\n Memory usage after faceting  \n  435 MB           \n  435 MB           \n   435 MB           \n     294 MB        \n\n\n\n\n\nGoing to depth 1 helped a lot. I would have expected that requesting from 'deep/a/b' was even faster, but I guess I'll have to dig into the code to understand why it was not.\n\nIt actually speeds up counting overall. If you think about it, when we encounter category ordinals, we just increment the count by 1 in the respective location in the count array. No need to ask whether this is an ordinal the user asked to count at all. Later when we compute the top-K, we know more efficiently while root ordinal the user requested to count, and its children, so it's just a matter of putting everything into a heap and returning the top-K.\n\nLUCENE-2369 uses exactly the same counting strategy (brute counting of everything). Not having explicit duplicates speeds this phase up and saves memory, but the numbers for d6 and d1 very clearly shows that it is faster overall to skip the drill-down in the extraction phase. At least for this test (and then we're back to creating a proper test suite).\n\nJust for kicks, I tried guesstimating the layout of a corpus with addresses by upping the docs and lowering the paths: 100M documents, 0.84 paths/doc, max depth 4 (2.5M unique paths)\n\n\n\n\u00a0\n LUCENE-3079 (d4) \n LUCENE-3079 (d1) \n LUCENE-2369 (d6) \n\n\n Index build time             \n   28 min          \n\n\t\n\n\n     17 min        \n\n\n Memory required for indexing \n    ? MB           \n    ? MB           \n      ? MB         \n\n\n First facet request          \n13933 ms           \n13367 ms           \n 46,000 ms         \n\n\n Best of 5 requests           \n11718 ms           \n11036 ms           \n   2989 ms         \n\n\n Memory usage after faceting  \n  240 MB           \n  240 MB           \n    475 MB         \n\n\n\n ",
            "author": "Toke Eskildsen",
            "id": "comment-13057148"
        },
        {
            "date": "2011-06-29T12:13:52+0000",
            "content": "Committed revision 1141060 (3x). Will start the port to trunk.\n\nI'm also attaching a userguide we wrote which should help newcomers get up to speed w/ the package. It is not meant to be an end-to-end cover of all the functionality and API, but rather as a complementary asset to the Javadocs, example code and source code itself.\n\nI think it will be good if we check it in with the source, e.g. under contrib/facet/docs or something, in ODT (+PDF?) format, and that it will be included in the release binaries (i.e. along with the .jar).\n\nWhat do you think? ",
            "author": "Shai Erera",
            "id": "comment-13057190"
        },
        {
            "date": "2011-06-29T16:41:06+0000",
            "content": "here's a hack patch, tried to quickly move this thing to trunk apis... 3 out of the 250 tests fail though, I'm 99% positive i jacked something up in the taxonomywriter (this is the most complicated one to convert), so maybe that one should just be started over.\n\nbut maybe some of the patch (except taxonomywriter, again i think its broken) would be useful in getting it ported to 4.x ",
            "author": "Robert Muir",
            "id": "comment-13057338"
        },
        {
            "date": "2011-06-29T18:34:28+0000",
            "content": "updated patch, one of the fails was caused by my use of seekExact, i think at least for MultiTermsEnum, if you seekExact, and even if it finds your term, its not positioned correctly, so if you then call next() its unsafe.\n\nanother one of the fails, is calling getPayload() before hasPayload() will not work with SepCodec.\n\nthanks to mike for helping track some of these down. \n\ni also added to build.xml the logic to depend on the analyzers module, now all tests pass (some of the time).\n\nbut i have at least one random fail:NOTE: reproduce with: ant test -Dtestcase=FacetsPayloadProcessorProviderTest -Dtestmethod=testTaxonomyMergeUtils -Dtests.seed=3732021887561370529:1102439953879128238 ",
            "author": "Robert Muir",
            "id": "comment-13057386"
        },
        {
            "date": "2011-06-29T18:56:23+0000",
            "content": "the previous fail is somehow a bug in memorycodec (the seed randomly selected it):\nant test -Dtestcase=FacetsPayloadProcessorProviderTest -Dtests.codec=Memory ",
            "author": "Robert Muir",
            "id": "comment-13057399"
        },
        {
            "date": "2011-06-29T19:46:49+0000",
            "content": "the previous fail is somehow a bug in memorycodec (the seed randomly selected it)\n\nI just committed a fix for this; it was because .getPayload() in MemoryCodec was (incorrectly) assuming caller did not change the .bytes of the returned BytesRef between calls. ",
            "author": "Michael McCandless",
            "id": "comment-13057428"
        },
        {
            "date": "2011-06-29T20:08:02+0000",
            "content": "Thanks guys for doing this port so quickly. The patch looks good. I suggest that we change the 'nocommit' to TODO (Facet): and commit it (under modules/). Then we can iterate on the TODOs and resolve them one by one, in followup issues. Makes sense?\n\nRobert, would you like to do the honors?  ",
            "author": "Shai Erera",
            "id": "comment-13057445"
        },
        {
            "date": "2011-06-29T20:37:02+0000",
            "content": "updated patch: all tests pass.\n\nI changed the nocommits to TODO (Facet):'s, and added verbage for the reason to each one.\n\nwe also have two TODOs for two bugs (The MTE.seekExact and SepCodec hasPayload bug) that we should fix, but currently we have workarounds in place (when we fix these bugs we can then remove the workarounds).\n\nI'll svn move to modules, and doublecheck things like javadoc warnings, and commit later today. ",
            "author": "Robert Muir",
            "id": "comment-13057457"
        },
        {
            "date": "2011-06-29T21:03:29+0000",
            "content": "Committed revision 1141246.\n\nI think we should close this issue soon, and open followup issues?\nMaybe just start with a separate issue for the documentation guide? ",
            "author": "Robert Muir",
            "id": "comment-13057468"
        },
        {
            "date": "2011-06-30T03:25:49+0000",
            "content": "I opened LUCENE-3261 and LUCENE-3262 to track userguide + benchmark issues. Will update more as we go along.\n\nI think we should close this issue\n\n+1.\n\nWe can now say Lucene has a faceting module ! Perhaps we should advertise it on the user-list?\n\nGreat job at porting to trunk Robert ! ",
            "author": "Shai Erera",
            "id": "comment-13057599"
        },
        {
            "date": "2011-06-30T09:07:29+0000",
            "content": "Faceting module in 3.x and trunk, tests pass, opened follow up issues. I think we can close this.\n\nThanks for everyone for helping get this in so quickly ! ",
            "author": "Shai Erera",
            "id": "comment-13057703"
        },
        {
            "date": "2013-03-09T05:53:02+0000",
            "content": "Has this been integrated into SOLR? Even if we just add it to SOLR as an option with hooks would be good. Or as an override in the lower levels? ",
            "author": "Bill Bell",
            "id": "comment-13597853"
        },
        {
            "date": "2013-03-09T20:50:28+0000",
            "content": "Has this been integrated into SOLR?\n\nHi Bill,\n\nNo, not yet ... the separate taxonomy index makes it tricky.  Although, the new facet method based on SortedSetDocValues is being used in Solr and is a patch (LUCENE-4795) to add to the faceting module, so there's a small overlap there... ",
            "author": "Michael McCandless",
            "id": "comment-13598066"
        }
    ]
}