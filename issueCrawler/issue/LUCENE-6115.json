{
    "id": "LUCENE-6115",
    "title": "Add getMergeInstance to CompressingStoredFieldsReader",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [
            "5.0",
            "6.0"
        ],
        "priority": "Minor",
        "status": "Closed",
        "type": "Improvement"
    },
    "description": "CompressingStoredFieldsReader is currently terrible at merging with different codecs or wrapped readers since it does not keep state. So if you want to get 5 documents that come from the same block, it means that you will have to decode the block header and decompress 5 times. It has some optimizations so that if you want to get the 2nd doc of the block then it will stop decompressing soon after the 2nd document, but it doesn't help much with merging since we want all documents.\n\nWe should implement getMergeInstance and have a different behaviour when merging by decompressing everything up-front and then reusing for all documents of the block.",
    "attachments": {
        "LUCENE-6115.patch": "https://issues.apache.org/jira/secure/attachment/12687472/LUCENE-6115.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14248096",
            "author": "Robert Muir",
            "date": "2014-12-16T10:38:16+0000",
            "content": "+1, Can we make a \"Buffering\" instance or something similar? Maybe it would be simple and reusable. "
        },
        {
            "id": "comment-14248125",
            "author": "Adrien Grand",
            "date": "2014-12-16T10:47:44+0000",
            "content": "Here is a patch. Here are the differences with today:\n\n\twhen doing random access, the header is always completely decoded (while we previously sometimes stopped eagerly)\n\twhen merging, we make sure to only decompress a given block of documents a single time\n\tchecksums are verified up-front instead of on-the-fly (like most other formats actually)\n\tI made the logic for large blocks more robust. Up to now, if you were storing a document that was so large that it would grow larger than 2x the chunk size, then it would be splitted into 16KB (exact this time) slices in order to make decompressing a bit more memory-efficient. The reader used to duplicate the writer logic (if block_len >= 2 * chunk_size) but this info is now encoded in the stream.\n\n\n\nI did some benchmarking and there were no significant differences in terms of indexing speed or read speed, so having to read the whole header every time does not seem to hurt (since the bottleneck is likely decompressing documents). I tried to remove the specialized merging to see if it was still needed and unfortunately it seems so, I got merging times that were about 20% slower without specialized merging. (In that case specialized merging still decompresses and recompresses all the time, it only saves some decoding and reuses directly the serialized bytes of each document.) "
        },
        {
            "id": "comment-14248133",
            "author": "Robert Muir",
            "date": "2014-12-16T10:54:25+0000",
            "content": "At a glance the patch looks great. About testing, does/can our BaseXXXStoredFieldsTest exercise merging in the different ways? \n\nStored fields are easy to write without flushing and merging. we should be able to do things like call addIndexes(bogusWrapper) for now and really exercise the different cases. "
        },
        {
            "id": "comment-14248222",
            "author": "Adrien Grand",
            "date": "2014-12-16T13:17:01+0000",
            "content": "We already have testWriteReadMerge which tests merging across codecs, I'll add another test with a filter reader. "
        },
        {
            "id": "comment-14248273",
            "author": "Adrien Grand",
            "date": "2014-12-16T14:17:32+0000",
            "content": "New patch which tests addIndexes. "
        },
        {
            "id": "comment-14248696",
            "author": "Michael McCandless",
            "date": "2014-12-16T19:02:42+0000",
            "content": "TestDemoParallelLeafReader should get faster with this!  (It iterates all stored docs in a segment, to create a parallel index). "
        },
        {
            "id": "comment-14248718",
            "author": "Robert Muir",
            "date": "2014-12-16T19:10:41+0000",
            "content": "We need to make it clear how bad the problem is in trunk: if you use a leafreader or are upgrading/changing codec today, with my test data merging is 7x slower merging with LZ4, and 64x slower merging with deflate. \n\nThis means indexing data that ordinarily takes a minute takes an hour instead.\n\nUnfortunately, this patch won't solve the performance issues with FilterReaders at all.\nThe problem is: nobody will ever call getMergeInstance in that case. See LUCENE-6065 for some details (I am not happy with any solution yet, i need help).\n\nHowever, the patch will work in the case of a user upgrading or changing codec, just as long as IW is passed a segmentreader. So its a great step. It is separate from LeafReader's brokenness. "
        },
        {
            "id": "comment-14251410",
            "author": "ASF subversion and git services",
            "date": "2014-12-18T09:13:22+0000",
            "content": "Commit 1646413 from Adrien Grand in branch 'dev/trunk'\n[ https://svn.apache.org/r1646413 ]\n\nLUCENE-6115: Add getMergeInstance to CompressingStoredFieldsReader. "
        },
        {
            "id": "comment-14251416",
            "author": "ASF subversion and git services",
            "date": "2014-12-18T09:25:38+0000",
            "content": "Commit 1646415 from Adrien Grand in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1646415 ]\n\nLUCENE-6115: Add getMergeInstance to CompressingStoredFieldsReader. "
        },
        {
            "id": "comment-14332684",
            "author": "Anshum Gupta",
            "date": "2015-02-23T05:01:19+0000",
            "content": "Bulk close after 5.0 release. "
        }
    ]
}