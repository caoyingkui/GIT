{
    "id": "SOLR-2700",
    "title": "transaction logging",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.0",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "A transaction log is needed for durability of updates, for a more performant realtime-get, and for replaying updates to recovering peers.",
    "attachments": {
        "SOLR-2700.patch": "https://issues.apache.org/jira/secure/attachment/12489573/SOLR-2700.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Yonik Seeley",
            "id": "comment-13080419",
            "date": "2011-08-06T16:14:30+0000",
            "content": "Here's a draft patch.\nThere is a tlog.<number> file created for each commit.  The javabin format is used to serialize SolrInputDocuments.\nAn in-memory map of pointers into the log is kept for documents not yet soft-committed, and the realtime-get component checks that first before using SolrCore.getNewestSearcher().\n\nSeems to work for getting documents not in the newest searcher so far.\n\nTons of stuff left to do\n\n\tthe tlog files are currently in the CWD\n\tneed to handle deletes\n\tneed to handle flushes in a performant way\n\tneed to implement optional fsync for durability on power-failure\n\twould be nice to make some of this multi-threaded for better performance\n\tneed to implement durability (apply updates from logs on startup)\n\tneed to implement some form of cleanup for transaction logs\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13080609",
            "date": "2011-08-07T17:44:16+0000",
            "content": "Here's an update that handles delete-by-id and also makes lookups concurrent (no synchronization on the file reads so multiple can proceed at once). "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13083504",
            "date": "2011-08-11T20:36:59+0000",
            "content": "Here's the latest prototype patch. I've hit a bit of an oddity with locking though that causes TestRealTimeGet to hang.\n\nI put a ReentrantLock around the commit in the update hander.  The test hangs with one or more of the writer threads blocked on the .lock().\n\n\n\t.unlock is called in a finally block - so it should always get called\n\tI added a counter that is incremented after the lock and decremented after the unlock.  it shows \"0\" in the debugger after the hang, meaning that we unlocked as many times as we locked.\n\tthe only place that touches that lock is DUH2.commit()\n\tif I look into the Sync object inside the ReentrantLock, the state is 1 (meaning locked I think).  The exclusiveOwnerThread is \"main\" for some reason.\n\tI think what I am seeing is that unlock() seems to normally fail to take effect.  The normal course is that cleanIndex() causes the main thread to do a deleteByQuery + commit, and even though the print says the lock was released, main retains it and no one else can ever acquire it.\n\n\n\nI can see the output via intellij, but not from the command line (since output seems to be buffered until the end of the test). "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13083828",
            "date": "2011-08-12T00:03:02+0000",
            "content": "Ah, silly mistake.  When I moved the commitLock.lock() further up in the file, I failed to remove the original lock() and thus locked it twice. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13085611",
            "date": "2011-08-16T09:40:36+0000",
            "content": "A couple of comments\n\n\n\tShould we not have a base class/interface for Transaction Log ?  What if I wish to have an alternate implementation\n\tWhat is the application of realtime get ?  is it for the cloud?\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13085664",
            "date": "2011-08-16T12:40:55+0000",
            "content": "Should we not have a base class/interface for Transaction Log ? What if I wish to have an alternate implementation\n\nYeah, eventually.  I started that way, but it was premature.  Before committing we should have a no-op implementation.\n\nWhat is the application of realtime get ?\n\nIt's a step in the direction that allows an application to use Solr as a data store and not just an index. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13085722",
            "date": "2011-08-16T14:15:46+0000",
            "content": "+1 for abstract class. E.g. one interesting mechanism for tlog persistence and fanout to slaves could be Kafka (http://incubator.apache.org/projects/kafka.html). "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13086207",
            "date": "2011-08-17T09:02:39+0000",
            "content": "Realtime get can be a separate issue altogether. These are two distinct features "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13086280",
            "date": "2011-08-17T12:53:49+0000",
            "content": "Realtime get can be a separate issue altogether.\n\nRealtime get is a separate JIRA issue - see SOLR-2656. However, it obviously goes very much with this issue if you do a little back reading - you don't want to have to reopen a reader each time for realtime get - you want to pull from the transaction log when you can instead. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13090438",
            "date": "2011-08-24T19:51:11+0000",
            "content": "Here's an update that among other things uses the \"tlog\" directory under the data directory. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13090546",
            "date": "2011-08-24T22:08:17+0000",
            "content": "Patch that updates to trunk and comments out the prints (those were actually causing test failures for some reason...)\n\n\n    [junit] Testsuite: org.apache.solr.update.Batch-With-Multiple-Tests\n    [junit] Testcase: org.apache.solr.update.Batch-With-Multiple-Tests:testDistribSearch:       Caused an ERROR\n    [junit] Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.\n    [junit] junit.framework.AssertionFailedError: Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.\n    [junit]     at java.lang.Thread.run(Thread.java:680)\n\n  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13090615",
            "date": "2011-08-24T23:41:05+0000",
            "content": "Just to get a rough idea of performance, I uploaded one of my CSV test files (765MB, 100M docs, 7 small string fields per doc).\nTime to complete indexing was 42% longer, and the transaction log grew to 1.8GB.  The lucene index was 1.2GB.  The log was on the same device, so the main impact may have been disk IO. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-13090722",
            "date": "2011-08-25T02:41:35+0000",
            "content": "Typically a transaction log configured to be written to a different hard drive than the indexes / database. "
        },
        {
            "author": "Simon Willnauer",
            "id": "comment-13090886",
            "date": "2011-08-25T09:29:20+0000",
            "content": "\nJust to get a rough idea of performance, I uploaded one of my CSV test files (765MB, 100M docs, 7 small string fields per doc).\nTime to complete indexing was 42% longer, and the transaction log grew to 1.8GB. The lucene index was 1.2GB. The log was on the same device, so the main impact may have been disk IO.\n\nI think this is far from what we can really do here. I didn't look too close at the code yet but it seems you are doing blocking writes which might not be ideal here at all. I think what you can do here is to allocate the space you need per record and write concurrently on a Channel (see FileChannel#write(ByteBuffer src, long position)), the same is true for reads (FileChannel#read(ByteBuffer dst, long position)). What we need to store in main memory is the offset and the length to do the realtime get here.\n\nTo take that one step further it might be good keep around the already serialized data if possible so if binary update is used can we piggyback the bytes in the SolrInputDocument somehow? If not I think we should use a faster hand written serialization instead of java serialization which is proven to be freaking slow.\n\nAnother totally different idea for the RT get is to spend more time on a RAM Reader that is capable of doing exactSeeks on the anyway used BytesRefHash. I don't thinks this would be too far away since the biggest problem here is to provide an efficiently sorted dictionary. maybe this should be a long term goal for the RT Get feature. \n\nSince we are already doing Write Behind here we could also try to use some compression especially if the source data is large, not sure if that will pay off though since we are not keeping the logs around forever. \n\nEventually I think this should be a feature that lives outside of solr since many Lucene applications could make use of it. ElasticSearch for instance uses pretty similar features which could be adopted to something like a DurableIndexWriter wrapper. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13091034",
            "date": "2011-08-25T14:20:04+0000",
            "content": "it seems you are doing blocking writes which might not be ideal here at all.\n\nYeah, I had eventually planned on concurrent writes... we're already doing concurrent reads, which seemed more important.\n\nWhat we need to store in main memory is the offset and the length to do the realtime get here.\n\nRight, that's what we're currently storing (just the offset).\n\nIf not I think we should use a faster hand written serialization instead of java serialization which is proven to be freaking slow.\n\n\"javabin\" is misleading - it does not use Java serialization (and is both much faster and more compact).\n\nAnother totally different idea for the RT get is to spend more time on a RAM Reader that is capable of doing exactSeeks on the anyway used BytesRefHash.\n\nYeah, but it seems like we'd still need the transaction log stuff anyway (for durability and to helping a peer recover.) "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13091046",
            "date": "2011-08-25T14:46:32+0000",
            "content": "Here's an update that uses a fixed set of external strings in the javabin codec to ret and avoid repeating all of the field names in the logs.  This drops the indexing penalty to 28% slower in this specific test, and decreases the transaction log size to 974M. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13096255",
            "date": "2011-09-02T20:02:42+0000",
            "content": "OK, I think we're getting close to committing now.\nAmong other things, this latest version adds the abstract UpdateLog class with a NullUpdateLog and FSUpdateLog subclasses, adds an updateHandler/updateLog section solrconfig.xml, and allows one to specify the log directory.\n\nCurrently the default is NullUpdateLog. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13097593",
            "date": "2011-09-05T22:09:22+0000",
            "content": "OK, I think we're getting close to committing now.\n\nUrggg - scratch that.  At some point in the past, some of the asserts were commented out to aid in debugging and I never re-enabled them.  The realtime-get test now fails, so I need to dig into that again. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-13099138",
            "date": "2011-09-07T17:44:01+0000",
            "content": "I'm not sure how this feature makes any sense, the documents are already being serialized to disk, eg, to the docstore by StoredFieldsWriter.  Now the system will be serializing the exact same documents twice, that is extremely redundant.   "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13099187",
            "date": "2011-09-07T18:37:59+0000",
            "content": "I'm not sure how this feature makes any sense, the documents are already being serialized to disk, eg, to the docstore by StoredFieldsWriter.\n\nIf the only use for a transaction log were realtime-get, I would agree.  But we have many more uses planned, so this is just a stepping stone (and a nice little feature along the way) to where we are going. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-13099239",
            "date": "2011-09-07T19:22:28+0000",
            "content": "This is going to best be amazing, I wonder if other projects have already implemented these features years ago? "
        },
        {
            "author": "Mike Anderson",
            "id": "comment-13143211",
            "date": "2011-11-03T14:45:19+0000",
            "content": "Will the transaction log be available via API? It would be very useful for application debugging if it were possible to query a record's transaction log and see a history of updates.  "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13146936",
            "date": "2011-11-09T10:56:43+0000",
            "content": "The transaction logging as it is done currently does not provide durability. \nif the server crashed in between , will it be able to recover from the transaction log ?\nI don't see the headers being written (globalStrings) during add().  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13147038",
            "date": "2011-11-09T13:31:17+0000",
            "content": "The transaction logging as it is done currently does not provide durability. \n\nCorrect - that's up next (I actually have it in a local copy).\n\nI don't see the headers being written (globalStrings) during add().\n\nThanks!  That would make the read-side difficult  "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13147606",
            "date": "2011-11-10T11:06:41+0000",
            "content": "What about rollbacks? Are they going to be supported too? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13147705",
            "date": "2011-11-10T14:25:17+0000",
            "content": "What about rollbacks? Are they going to be supported too?\n\nvia Lucene's IW.rollback? I don't think so... Rolling back assumes one client has complete control over the index and the commits. "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13147732",
            "date": "2011-11-10T14:57:26+0000",
            "content": "I meant this: http://wiki.apache.org/solr/UpdateXmlMessages#A.22rollback.22  I also thought things might start to get hairy if rollbacks are to be supported when multiple nodes are involved. Now it looks like realtime get returns docs even if they are not going to end up being indexed (rolled back) until next commit is issued. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13147743",
            "date": "2011-11-10T15:05:35+0000",
            "content": "I meant this: http://wiki.apache.org/solr/UpdateXmlMessages#A.22rollback.22\n\nRight - that's what I mean too (since all that really does is call Lucene's IW.rollback).\nIt's a low-level operation I'm not particularly fond of - to most clients, when a commit happened should be relatively arbitrary (esp when going into the world of NRT). "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13147747",
            "date": "2011-11-10T15:11:46+0000",
            "content": "Now it looks like realtime get returns docs even if they are not going to end up being indexed (rolled back) until next commit is issued.\n\nYup - NRT does the same. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13151124",
            "date": "2011-11-16T10:26:30+0000",
            "content": "Serialize the strings to a meta file (.tlm = transaction log meta) before the add is complete "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13205211",
            "date": "2012-02-10T04:11:21+0000",
            "content": "I think this one just needs a changes entry to be resolved. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13456394",
            "date": "2012-09-15T12:49:26+0000",
            "content": "Unassigned issues -> 4.1 "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-13501196",
            "date": "2012-11-20T14:05:17+0000",
            "content": "Mark Miller & Yonik Seeley - I think this is closable? "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13653971",
            "date": "2013-05-10T10:33:36+0000",
            "content": "Closed after release. "
        }
    ]
}