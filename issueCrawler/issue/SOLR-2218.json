{
    "id": "SOLR-2218",
    "title": "Performance of start= and rows= parameters are exponentially slow with large data sets",
    "details": {
        "affect_versions": "1.4.1",
        "status": "Resolved",
        "fix_versions": [],
        "components": [
            "Build"
        ],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Duplicate"
    },
    "description": "With large data sets, > 10M rows.\n\nSetting start=<large number> and rows=<large numbers> is slow, and gets slower the farther you get from start=0 with a complex query. Random also makes this slower.\n\nWould like to somehow make this performance faster for looping through large data sets. It would be nice if we could pass a pointer to the result set to loop, or support very large rows=<number>.\n\nSomething like:\nrows=1000\nstart=0\nspointer=string_my_query_1\n\nThen within interval (like 5 mins) I can reference this loop:\nSomething like:\nrows=1000\nstart=1000\nspointer=string_my_query_1\n\nWhat do you think? Since the data is too great the cache is not helping.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Lance Norskog",
            "id": "comment-12928457",
            "date": "2010-11-05T03:02:25+0000",
            "content": "There is a workaround for this called docid. \n\nhttp://www.lucidimagination.com/search/?q=_docid_#/p:solr "
        },
        {
            "author": "Peter Karich",
            "id": "comment-12928536",
            "date": "2010-11-05T08:14:23+0000",
            "content": "Lance, would you mind explaining this a bit in detail  ?\n\nThe idea is to grab all/alot documents from solr even if the dataset is very large, if I haven't misunderstood what Bill was requesting. This is very useful IMHO. "
        },
        {
            "author": "Bill Bell",
            "id": "comment-12928674",
            "date": "2010-11-05T16:47:08+0000",
            "content": "Lance,\n\nCan you point me directly to the document on Lucid's website? That search returns a Luke handler, that is not what I am asking.\n\n1. I have a query that returns thousands of results.\n2. I want to return fl=id, start=1000, rows=1000 and af I move start farther from 0, the results slow down substantially. \n3. Need the results to come back quickly even when start=10000 if I am looping across all the results. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12928895",
            "date": "2010-11-06T00:24:44+0000",
            "content": "The search returns many things, including a Solr issue with this title: \"Enable sort by docid\".\n "
        },
        {
            "author": "Bill Bell",
            "id": "comment-12928913",
            "date": "2010-11-06T01:58:02+0000",
            "content": "Lance, \n\nI know how to do that. That is not the issue. Let me explain again.\n\nThis is a performance issue.\n\nWhen you loop through results \"deeply\" the performance of the results get SLOWER and SLOWER.\n\n1. http://hostname/solr/select?fl=id&start=0&rows=1000&q=*:*\n<int name=\"QTime\">2</int>\n\n2. http://hostname/solr/select?fl=id&start=10000&rows=1000&q=*:*\n<int name=\"QTime\">8</int>\n\n3. http://hostname/solr/select?fl=id&start=20000&rows=1000&q=*:*\n<int name=\"QTime\">38</int>\n\nIt keeps getting slower!!\n\nWe need it to be consistently fast at QTIME=2.\n\nAny solutions?\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12930723",
            "date": "2010-11-10T19:21:36+0000",
            "content": "The performance gets slower as the start increases because in order to give you rows N...M sorted by score solr must collect the the top M documents (in sorted order) Lance's point is that if you use \"sort=docid+asc\" this collection of top ranking documents in sorted order doesn't have to happen.\n\nIf you have to use sorting, keep in mind that the decrease in performance as the \"start\" param increases w/o bounds is primarily driven by the amount of documents that have to be collected/compared on the sort field \u2013 something thta wouldn't change if yo have a named cursor (you would just be paying that cost up front instead of per request).\n\nYou should be able to get equivalent functionality by reducing the number of collected documents \u2013 instead of increasing the start param, add a filter on the sort field indicating that you only want documents with a field value higher (or lower if using \"desc\" sort) then the last document so far encountered.  (if you are sorting on score this becomes tricker, but should be possible using the \"frange\" parser wit the \"query\" function) "
        },
        {
            "author": "Bill Bell",
            "id": "comment-12976606",
            "date": "2011-01-03T04:33:34+0000",
            "content": "Hoss,\n\nSo what you are saying is instead of:\n\n1. http://hostname/solr/select?fl=id&start=20000&rows=1000&q=*:*&sort=id asc\n\nI should use:\n\nLAST_ID=20000\n1. http://hostname/solr/select?fl=id&rows=1000&q=*:*&sort=id asc&fq=id:[<LAST_ID> TO *]\n\nThis should definately be faster. Unfortunately, I need the results by highest score. Does fq support score?\n\nSCORE=5.6\n1. http://hostname/solr/select?fl=id,score&rows=1000&q=*:*&sort=score desc&fq=score:[0 TO <SCORE>]\n\nThoughts?\n\nI get an error when using fq=score:...\n\nHTTP ERROR 400\nProblem accessing /solr/provs/select. Reason: \n\n    undefined field score\n\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12978958",
            "date": "2011-01-07T20:47:18+0000",
            "content": "Unfortunately, I need the results by highest score. Does fq support score?\n\nAs i mentioned..\n\nif you are sorting on score this becomes tricker, but should be possible using the \"frange\" parser wit the \"query\" function)\n\nI think something like..\n\n\nLAST_SCORE=5.6\n...?q=...&fq={!frange u=5.6}query($q)&sort=score+desc\n\n\n\n...should work (but you have the issue of docs with identical scores to worry about \u2013 something that's not a problem with uniqueIds) "
        },
        {
            "author": "jess canabou",
            "id": "comment-13049020",
            "date": "2011-06-14T07:03:49+0000",
            "content": "Hi all\n\nI'm a bit confused by this thread, but think I have the same or almost same issue. I'm searching on a document with over 7000000 entries. I'm using the start and rows parameters (querying 30000 recs at a time), and notice the query times getting increasingly large, the further into the document I get. Unlike Bill, I do not care about scores or relevancy, and am having difficulty understanding whether the docid is a suitable solution to my problem. Is there something I can simply tack onto the end of my query to help speed up these query times? From what I understand, it's not necessary for me to be sorting all the rows before the chunk of data I'm querying on\nMy query looks as below.\nhttp://hostname/solr/select/?q=blablabla&version=2.2&start=4000000rows=30000&indent=on&fl=<bunch of fields>\n\nAny help would be greatly appreciated  "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-13121958",
            "date": "2011-10-06T13:54:54+0000",
            "content": "Dup of SOLR-1726 "
        }
    ]
}