{
    "id": "SOLR-829",
    "title": "replication Compression",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "1.4"
        ],
        "components": [
            "replication (java)"
        ],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "From a discussion on the mailing list solr-user, it would be useful to have an option to compress the files sent between servers for replication purposes.\n\nFiles sent across between indexes can be compressed by a large margin allowing for easier replication between sites.\n\n...Noted by Noble Paul \n\nwe will use a gzip on both ends of the pipe . On the slave side you can say <str name=\"zip\">true<str> as an extra option to compress and send data from server \n\n\nOther thoughts on issue: \n\nDo keep in mind that compression is a CPU intensive process so it is a trade off between CPU utilization and network bandwidth.  I have see cases where compressing the data before a network transfer ended up being slower than without compression because the cost of compression and un-compression was more than the gain in network transfer.\n\nWhy invent something when compression is standard in HTTP? --wunder",
    "attachments": {
        "solr-829.patch": "https://issues.apache.org/jira/secure/attachment/12393685/solr-829.patch",
        "email discussion.txt": "https://issues.apache.org/jira/secure/attachment/12392991/email%20discussion.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Simon Collins",
            "id": "comment-12643508",
            "date": "2008-10-29T14:58:57+0000",
            "content": "email thread discussing the issue "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12643541",
            "date": "2008-10-29T17:38:17+0000",
            "content": "email thread http://markmail.org/message/rmxywrgdlnz4vbwe "
        },
        {
            "author": "Akshay K. Ukey",
            "id": "comment-12646476",
            "date": "2008-11-11T05:35:02+0000",
            "content": "Patch with following changes:\nZip configuration parameter in replicationhandler (on slave):\n\n\n<str name=\"zip\">true</str>\n\n\n\nHave tested it with replication across two data centres with an index size of 1.1G.\nTime taken for replicating with gzipping is 1012 seconds (17 mins) compared to 1250 seconds (21 mins) with replication without gzipping. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12646478",
            "date": "2008-11-11T05:40:56+0000",
            "content": "Thanks Akshay\n\nHoss, do you know of a GzipServlet which we can borrow? Until that is in place, we can probably go ahead with this patch. Anyway, the configuration will not change, only the internal implementation needs to change (client sending Accept-Encoding in place of zip=true) "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12646486",
            "date": "2008-11-11T06:04:08+0000",
            "content": "Do we really need a zipping for any other response? Assuming that Solr clients mostly make the requests from the same LAN using CPU instead of Bandwidth looks like an overkill to me. (Probably we need a perf comparison). If the wt==javabin we may achieve very little compression.  "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12646657",
            "date": "2008-11-11T20:24:44+0000",
            "content": "I have used the GzipFilter from ehcache for a few years and never had any troubles...\n\nThere may be something smaller out there though.\n\nRe \"Do we really need a zipping for any other response?\"...  The point of using standards based approach is that the client can decide.  Essentially we could enable gzip for the whole web application and let each request say if the response should be gzipped. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12648079",
            "date": "2008-11-17T04:40:09+0000",
            "content": "The server(Replicationhandler is agnostic of compression. The client (SnapPuller) sets appropriate header  before sending the request . Use appropriate filter or front-end the master w/ an apache to handle compression "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12648149",
            "date": "2008-11-17T12:18:27+0000",
            "content": "After a lot of discussions on SOLR-856 I realize that it is not straight forward to provide a 'container independent' means to provide compression. We have to document different ways for different containers to ensure that it works properly. \n\n\n\tHow important is it to use HTTP standards to achieve this? Consider the fact that nothing else in the whole solution is complying with any standard\n\tFor this feature , compression is a critical  . It can mean huge differences in replication time\n\tI am not very comfortable with complex configuration documentation saying do this if you use jetty or do this if you use resin and this for glassfish etc etc.\n\tHow about giving both the options to users and let them choose what they want. This also gives them the flexibility of doing compression only for replication\n\tPower users can use their own favorite configuration to do the compression.\nSomething like\n\n<lst name=\"slave\">\n  <!-- values can be internal|external . --> \n  <str name=\"compression\">internal</str>\n</lst>\n\n\n\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12648507",
            "date": "2008-11-18T06:21:46+0000",
            "content": "Let's keep this issue focused on one thing: making it possible to configure a \"slave\" solr instance so that it indicates it can \"Accept-Encoding\" compressed responses during replication (discussion of what the \"master\" does with that information are a separate matter)\n\nFrom my (naive) reading of the current patch, a few things jump out at me...\n\n1) the \"FastOutputStream\" changes in ReplicationHandler looks like an unintentional part of the patch.\n2) why does setting the ZIP option to true disable checksums?  i'm not sure when/how checksums are currently computed/compared, but if it can be done with a raw i/o streams right now, it can be done with a GZIP i/o streams if the response is compressed.\n3) the behavior of checkCompressed doesn't seem right. A Content-Encoding header is used to indicate that the orriginal content has been compressed in order to transfer over HTTP, but the Content-Type header is used to identify the true type of the payload.  we shouldn't silently uncompress files just because they happen to have a mime type of \"application/x-gzip-compressed\".  we might be able to get away with it in dealing with replication, but we shouldn't need it (and unless i'm severaly mistaken, this will break in the event that gzip content is sent with additional gzip Content-Encoding. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12648517",
            "date": "2008-11-18T07:12:03+0000",
            "content": "\n\tYes, the constructor call is moved to a different line, that's all.\n\tWe disable checksum because if GZIP does checksums internally, so we do not need to do it again. However, deflate does not use checksums and when we use the InflaterInputStream, we should do checksums. This is not in the patch right now.\n\tThat code is exactly copied from CommonsHttpSolrServer. In this case, if we are getting a compressed stream from the master, it should be decompressed and written to the filesystem as it is. We do not need to worry about the type of the response. This patch is only for this particular use-case.\n\n\n\nI don't think this patch is in sync with Noble's latest proposal. A new one will be needed. "
        },
        {
            "author": "Akshay K. Ukey",
            "id": "comment-12649435",
            "date": "2008-11-20T18:10:50+0000",
            "content": "Patch with additional configuration in replicationhandler as suggested by Noble.\n\n<lst name=\"slave\">\n  <!-- values can be internal|external. --> \n  <str name=\"compression\">internal</str>\n</lst>\n\n\n\nIf internal compression is used InflaterInputStream and DeflaterOutputStream Java apis are used for data transfer from master to slaves. \n\nIf external compression is used Accept-Encoding header value is set to \"gzip,deflate\" before making request to the master. And the container has to be configured with appropriate setting. E.g. In case of Tomcat, following settings are to be done in the Connector section to use Tomcat's compression mechanism:\n\n<Connector .... compression=on \ncompressableMimeType=\"application/octet-stream,text/html,text/xml,text/plain\" \ncompressionMinSize=\"somePreferredValue\"/>\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12649622",
            "date": "2008-11-21T07:53:01+0000",
            "content": "Changes:\n\n\tFixes a possible connection leak in FileFetcher#getStream method.\n\tA single HttpClient is created with MultiThreadedHttpConnectionManager in SnapPuller and is re-used for every operation\n\tIdle connections are closed in SnapPull#destroy method\n\tRelease connection and stream closing is not done in a separate thread anymore\n\tReplicationHandler does a snappull command in a new thread so that an API call for this command is not kept waiting until the operation completes. The admin jsp which used to make a call to this method in another thread is also changed to remove the thread creation.\n\n\n\nI'll commit this in a day or two if there are no problems. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12650574",
            "date": "2008-11-25T14:19:49+0000",
            "content": "Committed revision 720502.\n\nThanks Simon, Noble, Hoss and Akshay! "
        }
    ]
}