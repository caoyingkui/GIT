{
    "id": "LUCENE-1317",
    "title": "Add InstantiatedIndexWriter.addIndexes(IndexReader[] readers)",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/other"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Won't Fix",
        "status": "Closed"
    },
    "description": "Enable InstantiatedIndexWriter to have IndexReaders passed in like IndexWriter and merged into the index.  \n\nKarl mentioned:\nbq: It's doable. The simplest solution I can think of is to reconstruct all the documents in one single enumeration of the source index and then add them to the writer. I'm however not certain this is the best way nor if InstantiatedIndexWriter is the place for the code.\n\nHow would the documents be reconstructed without creating a lot of overhead?  It seems like InstantiatedIndexWriter is the right place, given it is presumably more efficient to recreate all the IndexReaders and then commit?",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2008-06-26T16:32:43+0000",
            "content": "Looks like a modified version of addDocument will work, that operates on TokenStreams and Documents manufactured from the IndexReaders.  Can use the org.apache.lucene.search.highlight.TokenSources for the TokenStreams.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12608486"
        },
        {
            "date": "2008-06-26T17:34:38+0000",
            "content": "Can use the org.apache.lucene.search.highlight.TokenSources for the TokenStreams.\n\nTokenSources only does one document at the time. It is much more efficient to create all documents in a single enumeration of the source reader. \n\nI'm thinking something like this:\n\n\tLoad all term vector offsets in a Map</*document number/ Integer, Map<Term, int[]>>.\n\tCreate  a Document[]  with all doucments from the source reader.\n\tEnumerate all terms and document positions and fill up some sort of token stream factory per field and document. Map</*doc/Integer, Map</*field/String, Map</*pos/ Integer, List<Token>>>>. It would be really nice if Tokens that equals (text, offsets, payload, et c) was reused, but the cost of equality should probably be benchmarked first.\n\tAdd all documents to an InstantiatedIndexWriter.\n\n ",
            "author": "Karl Wettin",
            "id": "comment-12608513"
        },
        {
            "date": "2008-06-26T17:49:20+0000",
            "content": "The problem with this is, if the fields is only indexed without vector offsets and not stored.  Is there a way to handle these types of fields?  The Token equals you are mentioning is handled in the DocumentsWriter code, however without payloads.  There may be a better way to do this reusing some of the SegmentMerger code.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12608524"
        },
        {
            "date": "2008-06-26T19:07:16+0000",
            "content": "The problem with this is, if the fields is only indexed without vector offsets and not stored. \n\nOnly use the vectors for the offsets, nothing else. Extract everything else (token text, posincr, payload, et c) from the inverted index using TermEnum and TermPositions. ",
            "author": "Karl Wettin",
            "id": "comment-12608550"
        },
        {
            "date": "2011-01-24T21:16:44+0000",
            "content": "Won't be working on these and they're old ",
            "author": "Jason Rutherglen",
            "id": "comment-12986020"
        }
    ]
}