{
    "id": "SOLR-10086",
    "title": "Add Streaming Expression for Kafka Streams",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "SolrJ"
        ],
        "type": "New Feature",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "This is being asked to have SolrCloud pull data from Kafka topic periodically using DataImport Handler. \n\nAdding streaming expression support to pull data from Kafka would be good feature to have.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2017-02-01T20:51:12+0000",
            "content": "So I'm not 100% sure that adding the Kafka dependencies to Solr would be the right approach. I was chatting with Joel Bernstein earlier and was wondering if its possible to load custom streaming expressions. They can be registered (see SOLR-9103) in solrconfig.xml as long as the jar is on the classpath. \n\nAnother option that I just tried out was registering jars via the blob store and then registering the custom streaming expression. I put together an example of this here: https://github.com/risdenk/solr_custom_streaming_expressions\n\nThis requires a change to the StreamHandler class currently. See SOLR-10087 ",
            "author": "Kevin Risden",
            "id": "comment-15848909"
        },
        {
            "date": "2017-02-02T01:10:12+0000",
            "content": "Here is an example of what I was referring to using the blob store and registering a custom Kafka streaming expression.\n\nhttps://github.com/risdenk/solr_custom_streaming_expressions/tree/kafka\n\nThe streaming expression is at least a start. It hard codes the configs for Kafka but those would be easy to add the customization in the streaming expression. ",
            "author": "Kevin Risden",
            "id": "comment-15849209"
        },
        {
            "date": "2017-02-02T21:14:04+0000",
            "content": "Thanks, Kevin for putting the example and steps together (SOLR-10087).  Are we looking to put these custom streaming expressions code i.e. especially these common one like kafka etc.  under Solr repo (or contrib etc.) or its upto the users to maintain it.  ",
            "author": "Susheel Kumar",
            "id": "comment-15850511"
        },
        {
            "date": "2017-02-16T13:08:53+0000",
            "content": "I don't think that has been decided yet. Right now I'm personally leaning towards users maintaining. It would be a pain to include dependencies for any streaming expression in Solr. There is a related JIRA about solr replication that uses Kafka and if that dependency should be in Solr. ",
            "author": "Kevin Risden",
            "id": "comment-15869890"
        },
        {
            "date": "2017-03-04T04:48:51+0000",
            "content": "For anyone interested, I've created https://github.com/dennisgove/streaming-expressions which is setup as a suite of projects that will have various streaming components which shouldn't be added to solr core. ",
            "author": "Dennis Gove",
            "id": "comment-15895515"
        },
        {
            "date": "2017-03-05T01:53:15+0000",
            "content": "I'm developing this under a different project, so I dunno if the conversation should occur here or elsewhere, but to keep everyone in the loop, here's what I'm thinking about for the expression for a Kafka Producer. \n\n\nkafkaProducer(\n  <incoming stream>, \n  topic=<evaluator>, \n  bootstrapServers=<kafka servers>,\n  key=<evaluator>, // optional, if not provided then no key used when sending record\n  keyType=[string,long,double,boolean], // only required if using key and can't figure out from key param (knowing long or double is tricky)\n  value=<evaluator> // optional, if not provided then whole tuple as json string is used\n  valueType=[string,long,double,boolean], // only required if can't figure out from value param (knowing long or double is tricky)\n  partition=<evaluator>, // optional, if not provided then no partition used when sending record\n)\n\n\n\nAnything that is <evaluator> is allowed to be any valid StreamEvaluator, such as\n\nraw(\"foo\") // raw value \"foo\"\n\"foo\" // value of field foo\nadd(foo, bar) // value of foo + value of bar\n\n\n\nThere are other parameters allowed with a Kafka v0.10.x producer, such as \n\n props.put(\"acks\", \"all\");\n props.put(\"retries\", 0);\n props.put(\"batch.size\", 16384);\n props.put(\"linger.ms\", 1);\n props.put(\"buffer.memory\", 33554432);\n\n\n\nThese could be handled like the parameters in a Search Stream, where any parameters not explicitly known are just passed along. We could do that here so people can set whatever parameters they want on the publisher and we don't have to keep updating our supported list as the Kafka lib version changes. ",
            "author": "Dennis Gove",
            "id": "comment-15896023"
        },
        {
            "date": "2017-03-05T17:25:35+0000",
            "content": "Dennis Gove - Plan looks good. Its great to see this get a little more attention. Only question I have is when you talk about other parameters can they be <key>=<evaluator> or will they have to just be plain objects/strings.\n\nThere are a few other related stream ideas (SOLR-8706, SOLR-8763, SOLR-8679) that could fall into this category of useful, but have too many dependencies to be in the Solr project. FYI Joel Bernstein ",
            "author": "Kevin Risden",
            "id": "comment-15896447"
        },
        {
            "date": "2017-03-05T17:37:17+0000",
            "content": "For the other parameters, I'm thinking of just key/value strings. The reason being, the <evaluator> usage above allows the values to be calculated out of each tuple, like \n\ntopic=if(gt(a,b),raw(\"topic1\"),raw(\"topic2\"))\n\n would allow a tuple to be sent to topic1 if a > b, else go to topic2. This type of thing doesn't really apply for the publisher options because those are determined at the beginning and don't change from tuple to tuple. ",
            "author": "Dennis Gove",
            "id": "comment-15896451"
        },
        {
            "date": "2017-03-05T18:06:26+0000",
            "content": "Yea that makes sense thanks for the explanation. ",
            "author": "Kevin Risden",
            "id": "comment-15896464"
        },
        {
            "date": "2017-07-30T14:59:10+0000",
            "content": "I've got a working -SNAPSHOT version of a Kafka Consumer available at https://oss.sonatype.org/content/repositories/snapshots/com/dennisgove/streaming-expressions-kafka/\nand some initial documentation at https://dennisgove.github.io/streaming-expressions/kafka_overview.html ",
            "author": "Dennis Gove",
            "id": "comment-16106528"
        }
    ]
}