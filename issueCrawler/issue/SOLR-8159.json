{
    "id": "SOLR-8159",
    "title": "Tokenizing Chinese strings using lucene Chinese analyzer",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Minor"
    },
    "description": "The text that is indexed: \u6821\u51c6\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\nQuery string: \u5361\u5c14\u66fc\u6ee4\u6ce2\n\nThe exact query string is present in an indexed document on SOLR. But it doesn't return this document. \n\nSOLR analysis shows on index: \n\u7684\u5361 \n\u5c14 \n\u66fc \n\u6ee4\u6ce2\u5668 \n\nbut the queried terms show: \n\u5361 \n\u5c14 \n\u66fc \n\u6ee4\u6ce2 \n\nThe other characters appear to be influencing how \u5361\u5c14\u66fc\u6ee4\u6ce2 is tokenized. \n\nIs this an expected behavior??\n\nHere are the things I have tried\n1) I tried a couple of different tokenizers and the behavior is the same. \n\n2) I tried to explore the option of dictionary but I found this:\nhttps://issues.apache.org/jira/browse/LUCENE-1817\n\n3) I tried using the following with text_zh for chinese documents. \na) solr.KeywordMarkerFilterFactory\nb) solr.StemmerOverrideFilterFactory\nc) Adding to synonyms.txt \nAll these seem to work only with text_en and have no effect for text_zh\n\nAre there any options I can try to make sure that the query returns this document?",
    "attachments": {},
    "issue_links": {},
    "comments": []
}