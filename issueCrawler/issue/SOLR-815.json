{
    "id": "SOLR-815",
    "title": "Add new Japanese half-width/full-width normalizaton Filter and Factory",
    "details": {
        "affect_versions": "1.3",
        "status": "Resolved",
        "fix_versions": [],
        "components": [
            "Schema and Analysis"
        ],
        "type": "New Feature",
        "priority": "Minor",
        "labels": "",
        "resolution": "Won't Fix"
    },
    "description": "Japanese Katakana and  Latin alphabet characters exist as both a \"half-width\" and \"full-width\" version. This new Filter normalizes to the full-width version to allow searching and indexing using both.",
    "attachments": {
        "SOLR-815.patch": "https://issues.apache.org/jira/secure/attachment/12392369/SOLR-815.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Todd Feak",
            "id": "comment-12640606",
            "date": "2008-10-17T18:15:11+0000",
            "content": "Added patch containing Filter, Factory, and Unit Tests for both. "
        },
        {
            "author": "Walter Underwood",
            "id": "comment-12640609",
            "date": "2008-10-17T18:16:52+0000",
            "content": "If I remember correctly, Latin characters should normalize to half-width, not full-width. "
        },
        {
            "author": "Todd Feak",
            "id": "comment-12640618",
            "date": "2008-10-17T18:36:44+0000",
            "content": "It's a hidden storage format in the index. As long as index and search do it the same way, it's a coin toss.\n\nFor this particular case, Full-Width was chosen as the underlying format, as the majority of Japanese text and searches that we are seeing actually uses the Full-Width versions of both the Katakana and Latin characters. This is probably due to the platform we are on. This means less conversion occurs. Admittedly, it's a minor performance choice, but this is what we have. \n\nI'm not stuck on it being one way or the other and change should be easy. "
        },
        {
            "author": "Walter Underwood",
            "id": "comment-12641071",
            "date": "2008-10-20T17:05:45+0000",
            "content": "I looked it up, and even found a reason to do it the right way.\n\nLatin should be normalized to halfwidth (in the Latin-1 character space).\n\nKana should be normalized to fullwidth.\n\nNormalizing Latin characters to fullwidth would mean you could not use the existing accent-stripping filters or probably any other filter that expected Latin-1, like synonyms. Normalizing to halfwidth makes the rest of Solr and Lucene work as expected.\n\nSee section 12.5: http://www.unicode.org/versions/Unicode5.0.0/ch12.pdf\n\nThe compatability forms (the ones we normalize away from) are int the Unicode range U+FF00 to U+FFEF.\nThe correct mappings from those forms are in this doc: http://www.unicode.org/charts/PDF/UFF00.pdf\n\nOther charts are here: http://www.unicode.org/charts/ "
        },
        {
            "author": "Todd Feak",
            "id": "comment-12641079",
            "date": "2008-10-20T17:42:58+0000",
            "content": "That's a good reason to switch the Latin mappings. I reversed them and updated the Javadoc and tests as well. New patch attached. "
        },
        {
            "author": "Todd Feak",
            "id": "comment-12641100",
            "date": "2008-10-20T18:26:27+0000",
            "content": "For our purposes, the Japanese and Latin characters were all we were interested in, and that's what has been contributed.\n\nFor future growth, the entire set of characters that have mappings in this space could be included.  At that point, maybe the Filter and Factory name would change to be non-Japanese specific. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12641757",
            "date": "2008-10-22T10:15:40+0000",
            "content": "Thank you for your great patch on this ticket (and SOLR-814) and I'm sorry for late reply. There is definitely such requirements in Japan.\n\nI think this type of normalization can be done in Reader, not TokenFilter. And in my project, I'm using extended Tokenizer which\nreads chars from \"MappingReader\", the MappingReader solves this type of character mapping (and mapping rule can be read from mapping.txt etc).\n\nI've been thinking my methods (hard coded) can be more general. I'll open a new ticket for it soon. "
        },
        {
            "author": "Todd Feak",
            "id": "comment-12641953",
            "date": "2008-10-22T20:18:13+0000",
            "content": "I think there is value in covering the entire HalfWidth/FullWidth normalization issue in a single class. So, I renamed it to something more appropriate and added the additional mappings.\n\nIt should now work just as well with Korean as Japanese.\n\nHere's a new version for your consideration. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12642027",
            "date": "2008-10-23T01:02:11+0000",
            "content": "Todd, I've opened SOLR-822. I think it can solve SOLR-814 and SOLR-815. Please see and let me know if it can help you.  "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12662770",
            "date": "2009-01-11T12:56:45+0000",
            "content": "Koji, now that SOLR-822 has been committed, can this and SOLR-814 be closed? "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12662775",
            "date": "2009-01-11T13:15:28+0000",
            "content": "Using CharFilter can solve this problem in a flexible manner. "
        }
    ]
}