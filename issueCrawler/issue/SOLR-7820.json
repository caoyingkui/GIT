{
    "id": "SOLR-7820",
    "title": "IndexFetcher should calculate ahead of time how much space is needed for full snapshot based recovery and cleanly abort instead of trying and running out of space on a node",
    "details": {
        "components": [
            "replication (java)"
        ],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "When a replica is trying to recover and it's IndexFetcher decides it needs to pull the full index from a peer (isFullCopyNeeded == true), then the existing index directory should be deleted before the full copy is started to free up disk to pull a fresh index, otherwise the server will potentially need 2x the disk space (old + incoming new). Currently, the IndexFetcher removes the index directory after the new is downloaded; however, once the fetcher decides a full copy is needed, what is the value of the existing index? It's clearly out-of-date and should not serve queries. Since we're deleting data preemptively, maybe this should be an advanced configuration property, only to be used by those that are disk-space constrained (which I'm seeing more and more with people deploying high-end SSDs - they typically don't have 2x the disk capacity required by an index).",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2015-07-22T16:28:26+0000",
            "author": "Mark Miller",
            "content": "they typically don't have 2x the disk capacity required by an index\n\nScary spot to be in because natural merging can eat up 2x-3x+ space \n\nWill their be races around the first searcher trying to do something with it's files while waiting for the download and new searcher to be ready? ",
            "id": "comment-14637145"
        },
        {
            "date": "2015-07-24T09:30:46+0000",
            "author": "Ramkumar Aiyengar",
            "content": "-0. As Mark mentions, its a scary place to be in already. However advanced you make this, it's still going to be one more \"optimize\" like feature. Ooh, saves space, let's turn this on! Yes, it will, but it carries a risk which is not easily quantifiable. We still have cases where the entire cloud can go to recovery. They are rare with all our fixes over time, but they still exist. If you are serious/demanding enough about your search index to procure SSDs, you don't want to wake up finding your index gone just because you were feeling wise with your dollars one day in the past. I am still -0 with this only because I don't foresee any technical issues with this, but still doesn't sound like a feature we want to have. ",
            "id": "comment-14640204"
        },
        {
            "date": "2015-07-24T15:15:55+0000",
            "author": "Timothy Potter",
            "content": "Thanks for the feedback ... this actually came up in a production installation I worked on ... they had 1.4TB of indexes (oversharded on a node) and that node went down. When it came back, Solr decided all shards had to be fully copied over because they were too far out-of-date with the leader. The node could never recover because they didn't have another 1.4TB of SSD allocated on that node. Granted this is an extreme case. The interesting thing here is that node wasn't offline for very long, so I was surprised to see it need a full copy.\n\nPart of this is bad design in that they shouldn't have oversharded the nodes as much given their space limitations.\n\nI'm wondering if we can compute the necessary space needed for an incoming full-index for a shard and if that isn't available, then don't do it. Of course that's harder to do when oversharding. But to me that's better than running the disk out of space just to keep failing to recover.\n\nI also want to put some more energy into trying to avoid a full copy because in my case, the node that went down wasn't out of sync with the leader by more than a couple thousand docs per shard, so the fact that Solr wanted to do a full copy of 1.4TB of indexes because a few thousand docs were missing sounds like the real culprit in my case. ",
            "id": "comment-14640575"
        },
        {
            "date": "2015-07-25T11:13:07+0000",
            "author": "Ramkumar Aiyengar",
            "content": "I agree there are a few issues here, just that the deleting the current index just brushes them all under the carpet and adds risk.\n\n\n\tThe current default of 100 updates for UpdateLog is often insufficient for many cases. I made that number configurable, if it's a few thousand updates, just tweaking it might work. But UpdateLog has scaling limitations I think, so YMMV. I thought CdcrUpdateLog came about to overcome this scaling limitation \u2013 but I haven't looked at it enough to know if it can replace UpdateLog, perhaps Erick Erickson or Yonik Seeley know..\n\tThe other thing which could vastly improve this situation, even if a full recovery was needed, was synchronizing commits across replicas, since recovery skips segments already present in the current index. I believe Varun Thacker was looking at this, but I can't find the issue now.\n\tRegardless, I agree that it would be a good enhancement to calculate ahead of time how much space is needed for recovery and cleanly abort instead of trying and running out of space.\n\n ",
            "id": "comment-14641543"
        },
        {
            "date": "2015-07-25T14:30:13+0000",
            "author": "Varun Thacker",
            "content": "was synchronizing commits across replicas, since recovery skips segments already present in the current index. \n\nSOLR-6606 is the Jira. ",
            "id": "comment-14641637"
        },
        {
            "date": "2015-07-27T16:41:05+0000",
            "author": "Timothy Potter",
            "content": "Ramkumar Aiyengar thanks for the thoughtful feedback here. Varun Thacker let's try to get SOLR-6606 committed soon!\n\nI agree that having a larger threshold would have helped my specific situation, although I've heard having too large of a threshold has its own set of issues. I'll research the CdcrUpdateLog and revisit the concerns around larger thresholds. But this definitely seems like a good area to have better instructions / guidance around esp. for addressing brief outages with very large indexes.\n\nwould be a good enhancement to calculate ahead of time how much space is needed for recovery and cleanly abort instead of trying and running out of space.\n\nI'll change the title of this ticket to address that enhancement. ",
            "id": "comment-14642992"
        },
        {
            "date": "2015-07-27T20:13:26+0000",
            "author": "Mark Miller",
            "content": "although I've heard having too large of a threshold has its own set of issues. \n\nHopefully that has been heavily mitigated by other improvements (like streaming updates). I never remember the full issue though - Yonik Seeley always has to remind me. I think RAM usage was perhaps the other major concern, but that's all about doc size and the RAM you have - more a default issue than anything. ",
            "id": "comment-14643311"
        },
        {
            "date": "2015-07-27T20:29:26+0000",
            "author": "Ramkumar Aiyengar",
            "content": "In Yonik's words..\n\nhttps://issues.apache.org/jira/browse/SOLR-6460?focusedCommentId=14117807&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14117807 ",
            "id": "comment-14643329"
        },
        {
            "date": "2016-06-01T22:20:42+0000",
            "author": "Lanny Ripple",
            "content": "Experiencing this right now since as a startup pinching penny's isn't optional.  We're about 70% allocated on disk with 60 or so shards over a dozen or two collections.  If any couple of replicas throw a hissy it's not a big deal for Solr to recover.  If a node goes down, or in one case the AWS instance starts being flaky, then we fill disk and get to spend a lot of time baby sitting the recovery.\n\nIf Solr sequencing recovery to avoid blowing disk isn't a good idea then please at least expose tooling to make it easier for a human to do the same thing.  Even a way to start Solr without immediately trying to sync would be a win.  When Solr goes all-in to recover then the collections API times out on DELETEREPLICA. ",
            "id": "comment-15311244"
        }
    ]
}