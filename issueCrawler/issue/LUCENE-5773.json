{
    "id": "LUCENE-5773",
    "title": "Test SegmentReader.ramBytesUsed",
    "details": {
        "type": "Test",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed",
        "components": [],
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.9",
            "6.0"
        ]
    },
    "description": "There have been cases where the memory reported by this API was larger than the JVM heap size in the past so we should try to add some basic tests to it.",
    "attachments": {
        "LUCENE-5773-2.patch": "https://issues.apache.org/jira/secure/attachment/12651285/LUCENE-5773-2.patch",
        "LUCENE-5773.patch": "https://issues.apache.org/jira/secure/attachment/12650916/LUCENE-5773.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14034534",
            "author": "Robert Muir",
            "content": "+1, it would be great long term too if somehow codecs (since most code is package private but tests have access) could test the relevant producers and so on directly. \n\nThis way there is actually unit tests rather than trying to debug a complex random codec configuration and figuring out where the bugs are.\n\nbut as a step, we should at least have even one test, of any sort. ",
            "date": "2014-06-17T22:50:16+0000"
        },
        {
            "id": "comment-14034551",
            "author": "Adrien Grand",
            "content": "Here is a patch. It compares the output of SegmentReader.ramBytesUsed against RamUsageTester for various codecs. In order to be successful the error needs to be either under 10% (relative) or 500 bytes (absolute) on an index on 100k documents with random small fields. The absolute value is needed for things that consume very little memory like Lucene's 4.9 norms with constant compression or stored fields. Otherwise it would very easily fail due to the constant overhead of the objects that we maintain to make SegmentReader work.\n\nI had to refactor RamUsageTester a bit to make it work. In particular, I needed to make sure that pointers to other segments and to directory objects are not followed. Otherwise this would count eg. the buffers of the NIO directory's buffers.\n\nIt found a couple of interesting bugs although the default codec had pretty accurate estimations. Quick overview of things that have been fixed and/or are surprising:\n\n\tPagedBytes.Reader assumed all pages had the same size. However with trim=true the last page is trimmed so the estimation could be quite far from accurate with large page sizes. It now returns the exact memory usage (as reported by RamUsageTester).\n\tThe various FSTs that we use in codecs sometimes have massive cached root arcs, MemoryPostingsFormat in particular but that was also the case for BlockTreeTermsReader (or maybe is it due to the test data?).\n\tOther bugs were mostly about forgotten references, of things counted twice (eg. a paged bytes and a reader to the same pages).\n\n ",
            "date": "2014-06-17T22:59:35+0000"
        },
        {
            "id": "comment-14034554",
            "author": "Adrien Grand",
            "content": "it would be great long term too if somehow codecs (since most code is package private but tests have access) could test the relevant producers and so on directly. \n\nSince the current patch works on BaseIndexFileFormatTestCase that is close to it.  Eg. stored fields tests only create documents with stored fields. So SegmentReader.ramBytesUsed is close to StoredFieldsProducer.ramBytesUsed. One exception to that are norms since they need an indexed field. ",
            "date": "2014-06-17T23:01:46+0000"
        },
        {
            "id": "comment-14034571",
            "author": "Robert Muir",
            "content": "+1, its been totally bugging me how this stuff was completely untested. Thank you! ",
            "date": "2014-06-17T23:09:48+0000"
        },
        {
            "id": "comment-14035603",
            "author": "Adrien Grand",
            "content": "Here is an updated patch that fixed estimations on SimpleText and fixes a few more bugs. I had to reduce the number of index documents so that the test isn't too slow eg. when a random per-field PF uses SimpleText, and raised the maximum error to 20% (estimations are less accurate when there are fewer documents). I'll commit soon. ",
            "date": "2014-06-18T11:40:38+0000"
        },
        {
            "id": "comment-14035639",
            "author": "ASF subversion and git services",
            "content": "Commit 1603430 from Adrien Grand in branch 'dev/trunk'\n[ https://svn.apache.org/r1603430 ]\n\nLUCENE-5773: Test SegmentReader.ramBytesUsed. ",
            "date": "2014-06-18T12:24:18+0000"
        },
        {
            "id": "comment-14035706",
            "author": "ASF subversion and git services",
            "content": "Commit 1603455 from Adrien Grand in branch 'dev/trunk'\n[ https://svn.apache.org/r1603455 ]\n\nLUCENE-5773: Don't account for FST.Outputs.getNoOutput which is a singleton.\n\nFound while merging... ",
            "date": "2014-06-18T14:00:38+0000"
        },
        {
            "id": "comment-14035821",
            "author": "ASF subversion and git services",
            "content": "Commit 1603484 from Adrien Grand in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1603484 ]\n\nLUCENE-5773: Test SegmentReader.ramBytesUsed. ",
            "date": "2014-06-18T15:29:56+0000"
        },
        {
            "id": "comment-14036286",
            "author": "ASF subversion and git services",
            "content": "Commit 1603618 from Adrien Grand in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1603618 ]\n\nLUCENE-5773: Disable test temporarily. ",
            "date": "2014-06-18T20:07:13+0000"
        },
        {
            "id": "comment-14036291",
            "author": "Adrien Grand",
            "content": "Tests fail on Lucene 3.x because there is a lot of small objects that take more memory than the codec. We could increase the number of indexed documents, but this would also slow down the test. Instead I think we can create a big an a large segment and only compare the differences in terms of RAM usage. This will avoid problems that are due to constant overhead and we will be able to reduce the error margin that we have today. So this will probably be a better test? I'll work on a patch to see if that works. ",
            "date": "2014-06-18T20:09:25+0000"
        },
        {
            "id": "comment-14036460",
            "author": "ASF subversion and git services",
            "content": "Commit 1603647 from Adrien Grand in branch 'dev/trunk'\n[ https://svn.apache.org/r1603647 ]\n\nLUCENE-5773: Fix BlockTermsReader ram usage estimation. ",
            "date": "2014-06-18T21:55:01+0000"
        },
        {
            "id": "comment-14036576",
            "author": "Adrien Grand",
            "content": "Here is a patch. I had to disable MockRandomPF since the test relies on the fact that both segment readers will use the same codec. ",
            "date": "2014-06-18T22:39:48+0000"
        },
        {
            "id": "comment-14036579",
            "author": "Adrien Grand",
            "content": "(Note: the patch applies on 4.x) ",
            "date": "2014-06-18T22:41:55+0000"
        },
        {
            "id": "comment-14036641",
            "author": "ASF subversion and git services",
            "content": "Commit 1603667 from Adrien Grand in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1603667 ]\n\nLUCENE-5773: Improve test by measuring deltas instead of absolute values. ",
            "date": "2014-06-18T23:24:26+0000"
        },
        {
            "id": "comment-14036669",
            "author": "ASF subversion and git services",
            "content": "Commit 1603676 from Adrien Grand in branch 'dev/trunk'\n[ https://svn.apache.org/r1603676 ]\n\nLUCENE-5773: Improve test by measuring deltas instead of absolute values. ",
            "date": "2014-06-18T23:43:20+0000"
        },
        {
            "id": "comment-14036693",
            "author": "ASF subversion and git services",
            "content": "Commit 1603682 from Adrien Grand in branch 'dev/branches/lucene_solr_4_9'\n[ https://svn.apache.org/r1603682 ]\n\nLUCENE-5773: Test SegmentReader.ramBytesUsed. ",
            "date": "2014-06-19T00:05:33+0000"
        },
        {
            "id": "comment-14036695",
            "author": "Adrien Grand",
            "content": "It should be good now. I'll open another issue if there are more failures, sorry for the noise... ",
            "date": "2014-06-19T00:06:13+0000"
        },
        {
            "id": "comment-14037166",
            "author": "ASF subversion and git services",
            "content": "Commit 1603777 from Adrien Grand in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1603777 ]\n\nLUCENE-5773: Fix ram usage estimation on PositiveIntOutputs. ",
            "date": "2014-06-19T09:29:24+0000"
        },
        {
            "id": "comment-14037167",
            "author": "ASF subversion and git services",
            "content": "Commit 1603780 from Adrien Grand in branch 'dev/branches/lucene_solr_4_9'\n[ https://svn.apache.org/r1603780 ]\n\nLUCENE-5773: Fix ram usage estimation on PositiveIntOutputs. ",
            "date": "2014-06-19T09:30:59+0000"
        },
        {
            "id": "comment-14037168",
            "author": "ASF subversion and git services",
            "content": "Commit 1603782 from Adrien Grand in branch 'dev/trunk'\n[ https://svn.apache.org/r1603782 ]\n\nLUCENE-5773: Fix ram usage estimation on PositiveIntOutputs. ",
            "date": "2014-06-19T09:31:50+0000"
        }
    ]
}