{
    "id": "SOLR-5146",
    "title": "Figure out what it would take for lazily-loaded cores to play nice with SolrCloud",
    "details": {
        "affect_versions": "4.5,                                            6.0",
        "status": "Open",
        "fix_versions": [
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "The whole lazy-load core thing was implemented with non-SolrCloud use-cases in mind. There are several user-list threads that ask about using lazy cores with SolrCloud, especially in multi-tenant use-cases.\n\nThis is a marker JIRA to investigate what it would take to make lazy-load cores play nice with SolrCloud. It's especially interesting how this all works with shards, replicas, leader election, recovery, etc.\n\nNOTE: This is pretty much totally unexplored territory. It may be that a few trivial modifications are all that's needed. OTOH, It may be that we'd have to rip apart SolrCloud to handle this case. Until someone dives into the code, we don't know.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13892080",
            "date": "2014-02-05T13:16:04+0000",
            "content": "This is next. Exciting times ahead  "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13892221",
            "date": "2014-02-05T15:22:11+0000",
            "content": "Shalin:\n\nLet me know how I can help. The last two months I've been pretty much out of touch, culminating in a cross-country move. But we're at our destination now so I have some bandwidth, maybe....\n\nBest,\nErick "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13897763",
            "date": "2014-02-11T11:49:52+0000",
            "content": "Sure Erick. I've been studying the code for transient cores to understand the potential issues and bottlenecks with using this feature with SolrCloud.\n\nI think we can break it down to four major features:\n\n\tMake loadOnStartup=true work with SolrCloud shards - slices are marked with loadOnStartup=false. All nodes are woken up on a request.\n\tMake transient replicas work with SolrCloud replication - Leader is always active, replicas are down. Leader can send 'requestrecovery' to replicas based on maxDocs/maxTime parameters to make them sync. Maybe we can make peersync and buffer doc counts configurable.\n\tMake transient leaders work with SolrCloud - Down leaders are okay. Replicas may still be up but we won't force leader election. Leaders are woken up only on a write request.\n\tOptimize leader election for transient shards - We probably don't want to force leader election each time a shard wakes up. Instead clusterstate can remain the truth and leaders can go down. If a shard is woken up again, it can use the same leader until it goes down. This is far away. We shall focus on optimization later.\n\n\n\nI think an easy win here would be to translate loadOnStartup=false to complete shards (leader+replicas). I'm going to start building a prototype and see how easy it turns out to be  "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13897965",
            "date": "2014-02-11T16:06:15+0000",
            "content": "The tantalizing bit is it could \"just work\". Anything that uses \"getCore\" (as I remember)\nwill autoload the core and carry out the request. That includes updates as well as \nqueries. The trap here would be the time involved. Say some replicas had to be loaded,\nwould a request timeout? In any lightly-loaded system, the chance that some of the\nreplicas are down increases of course..\n\nI haven't really thought it out, but the process most true the the notion of a single machine\nwith lots of cores seems to be a cluster-wide sense of what should be loaded. In fact,\nI might think of it as collection rather than shard being transient. I doubt one could use\nZK for this as it would require that every request to every node get some info from ZK.\n\nHmmm, and my ignorance of ZK is showing, but is it possible for ZK to raise \"load/unload\nyourself\" events to the cluster? Mostly spinning half-baked ideas here, you know the \nZK code far, far better than I do...\n\nWhat fun! "
        },
        {
            "author": "Scott Blum",
            "id": "comment-14628765",
            "date": "2015-07-15T21:22:18+0000",
            "content": "I would just like to add a thought here, maybe we could redefine what it means for a core to be loaded/unloaded/transient.  Hear me out for a second... the real operational problem with having tons of cores all loaded at once is the enormous amount of heap required, since all cores maintain their own caches.  If we could simply have \"inactive\" cores dump all their caches (maybe the whole searcher?) but continue to participate in solrcloud/zk, wouldn't that make the problem a lot simpler? "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-14628782",
            "date": "2015-07-15T21:34:07+0000",
            "content": "Scott:\n\nInteresting. Indexing and querying are such different beasts in SolrCloud that I'm afraid that things would get out of hand. The point is, though, that I really have no clue except this would have to be scoped out pretty thoroughly in order to flush out all the crannies before starting the work, it could go down a rat-hole really quickly.\n\nUnloading the caches would probably be viable for the indexing side. The problem being that when an update goes out, all the replicas must get it thus would need to be loaded. But if it was just a case of the caches being absent, that might be OK.\n\nQuerying is a bit more problematical. Assuming replicas then just because one query went to one replica in a shard doesn't mean the next one would. I can see this leading to perf issues. Not to mention long-loading cores (even if it's just reloading the caches) going past ZK timeouts, the leader marking the node as down and forcing it into recovery etc.\n\nAnd the scale is interesting. I agree that the bulk of the memory is in the various caches. That said, though, people are talking 10,000+ cores. The overhead adds up there.\n\nIn SolrCloud mode, having this many cores really is about having this many collections I think. The first thing that would be needed to work out is whether this even makes sense in SolrCloud.\n\nThe model \"lots of cores\" was designed for is, say, e-mail searching where each user has her own core. The assumption here is that the pattern is userA logs on, searches for a few minutes then goes away. Not sure that model even translates well into SolrCloud.\n "
        },
        {
            "author": "Scott Blum",
            "id": "comment-14628827",
            "date": "2015-07-15T21:58:20+0000",
            "content": "We have a multi-tenant model ourselves (collection per tenant), but our indexes are large enough to warrant sharding and replication for performance and durability.  The problem we have is that once a core has been active and filled its cache, that memory is tied up forever.  It's sort of a tragedy-of-the-commons with no one overseeing global caching / memory usage.  Over time, the heap low water mark rises indefinitely until VM death.\n\nOur current best idea, absent a real solution, is to periodically reload all cores, just to force all the caches to empty out.  This resets the heap low water mark back to a reasonable level.  So anything that would make things even incrementally better would be a win for us. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-14628863",
            "date": "2015-07-15T22:22:54+0000",
            "content": "Right, I can see where that would happen. There are a couple of tickets out there for handling lots and lots of collections. I'm a little blinded when I think of lazily-loaded cores by knowing the code. Maybe I'm getting it now....\n\nHmmmm, I haven't thought this through at all, but picking up on your initial comment today instead of conflating lazy loading with this issue maybe we could use the LRU core lists maintained by the lazy core load to periodically reopen searchers on the oldest N cores. I can't say I have the bandwidth to work on this, but it seems like what you were thinking.\n\nI'm guessing that in this situation you don't have much autowarming going on, and that the cache sizes for all your collections is pretty small. If those assumptions are false lowering them might help.\n\nInteresting use-case though, hadn't thought about that very carefully. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-14628874",
            "date": "2015-07-15T22:33:42+0000",
            "content": "I suspect that the coding challenges associated with getting SolrCloud working with transient cores are, in a relative sense, not all that hard for someone who has a good grasp on Solr internals.  It might take some time and experimentation to work through the issues, but I would not expect it to be extremely challenging.\n\nI think the really nasty problems are operational and exist because SolrCloud is geared towards a setup where the entire cluster is fully online and very responsive at all times.  The process of loading a single core of moderate size is typically pretty fast, only taking a few seconds, but that can slow down substantially if the server is very busy, which it would be if it is frequently unloading and loading large transient cores.  In that situation, response time can quickly grow, which can cause zkClientTimeout problems, sometimes leading to a performance death spiral of index recoveries and exceeded timeouts.  The solution to SolrCloud death-spiral problems is usually more resources (memory, CPU, or possibly additional servers), but that defeats one of the most important reasons that transient cores exist \u2013 keeping hardware costs down. "
        },
        {
            "author": "Jerome Yang",
            "id": "comment-15393148",
            "date": "2016-07-26T03:34:39+0000",
            "content": "Any news on this issue? "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-15393266",
            "date": "2016-07-26T05:42:05+0000",
            "content": "In a word, \"no\". This is one of those special-purpose cases that I don't believe generalizes well to SolrCloud without some significant design work. "
        }
    ]
}