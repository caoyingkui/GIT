{
    "id": "SOLR-1169",
    "title": "SortedIntDocSet",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "1.4"
        ],
        "components": [],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "A DocSet type that can skip to support SOLR-1165",
    "attachments": {
        "SOLR-1169.patch": "https://issues.apache.org/jira/secure/attachment/12408217/SOLR-1169.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Yonik Seeley",
            "id": "comment-12709605",
            "date": "2009-05-14T21:51:49+0000",
            "content": "We need skipping in our DocSets in order to be able to pass them as filters and improve search performance.\nOne could Sort a HashDocSet every time it's used.... but that's not desirable.\n\nThe way Lucene now uses filters, random access performance is no longer important, but being able to efficiently skip is.\nIntersection performance is very important to Solr of course, so if we can get SortedIntDocSet performance fast enough then it would make sense for it to replace HashDocSet.\n\nI've already started working on this, and the results look promising.  I'll post a draft soon. "
        },
        {
            "author": "Mike Klaas",
            "id": "comment-12709645",
            "date": "2009-05-15T00:23:11+0000",
            "content": "sweet.  intersecting sorted int dicts should be faster in the general case.  HashSet will of course win when one set is very small, but I expect this to still be pretty fast anyway. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12709692",
            "date": "2009-05-15T03:54:19+0000",
            "content": "Draft patch attached.  It's not \"hooked in\" to Solr yet... just the performance tests, which I'm doing now. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12709713",
            "date": "2009-05-15T05:33:05+0000",
            "content": "The first results are in, and it's looking good.\n\nAlgorithm:\nFor intersectionSize (and intersection), if the sets are near in size, we do a linear scan.  A little micro-optimization there got us an extra 13% speedup... just for rearranging the order of comparisons base on the fact that one was more likely to be true than the other (based on the set sizes).  When the set sizes differ by a factor of 8 or more, we use a modified binary search.  The first modification keeps track of the lower bound rather than resetting to 0... a big win (an obvious optimization... I didn't measure the benefit).  Then second modification probes closer to the lower bound (rather than using the midpoint) based on the relative size difference of the sets, and leads to a 40% performance increase.\n\nPerformance results of intersectionSize:\n\n\n\n\nDocSet sizes\nAdvantage\nPercent\ncomments\n\n\n1-200 x 1-200                            \nInt \n53%\n random sized DocSets from 1-200 in size intersected with other DocSets of size 1-200\n\n\n1-3000 x 1-3000                       \n    Int \n33% \n3000 is the current default upper bound of HashDocSet\n\n\n1-3000 x 1-3000                       \n   Int \n130% \n-client instead of -server\n\n\n2000-3000 x 2000-3000         \n  Int \n60% \n only big sets\n\n\n20000-30000 x 20000-30000\n  Int \n54% \n  only bigger sets\n\n\n100-200 x 1000-2000              \n  Hash \n87% \n small sets with big sets\n\n\n1-10000 x 20000-30000         \n  Int  \n74% \n smaller sets intersected with BitDocSets\n\n\n1-30000 x 1-30000                  \n  Int \n80% \n docsets over maxDoc/64 are BitDocSets (maxDoc=1M)\n\n\n\n\n\nSo to sum up, only small sets intersected with big sets are slower.... but given that big sets intersected with big sets take a majority of the time, we get a nice net win.  It gets more dramatic when intersecting a small set with a BitDocSet... these affects are probably due to nicer treatment of the memory subsystem when accessing memory in order.  I think these intersections tend to be bound by memory bandwidth.\n\nThe improvements will also allow us to bump up the max size of the \"small set\" implementation.  From a memory consumption point of view, the break-even point is maxDoc/32.  When I tested using SortedIntDocSets with maxDoc/64, there was always a net speedup over maxDoc/32 and maxDoc/100, so this seems to be a good balance between performance and saving memory.\n\nMemory savings: SortedIntDocSet is more efficient than HashDocSet at storing the same amount of data, and it can be used at larger sizes (relative to maxDoc) before performance decreases (another memory win).\n\nOther savings: Faster set creation - Lucene currently delivers docs in order, hence so sorting step is needed after collection.\n\nOther notes: I tried a cool partitioning algorithm that I thought would be superior - take the middle element of the big set and use it to partition the small set.  Say you have set sizes of 100 and 10000... you do a single binary search on the small set, and now for all 100 elements you have half the big set size to search.  Recurse on the corresponding lower and upper partitions until they are small enough to use a different method such as a modified binary search.  This approach worked, but it wasn't able to beat the modified binary search alone once I put in all the optimizations... shrugs "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12711205",
            "date": "2009-05-20T16:08:10+0000",
            "content": "committed. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12775744",
            "date": "2009-11-10T15:52:05+0000",
            "content": "Bulk close for Solr 1.4 "
        }
    ]
}