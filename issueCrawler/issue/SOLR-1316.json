{
    "id": "SOLR-1316",
    "title": "Create autosuggest component",
    "details": {
        "affect_versions": "1.4",
        "status": "Closed",
        "fix_versions": [
            "3.1"
        ],
        "components": [
            "search"
        ],
        "type": "New Feature",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Autosuggest is a common search function that can be integrated\ninto Solr as a SearchComponent. Our first implementation will\nuse the TernaryTree found in Lucene contrib. \n\n\n\tEnable creation of the dictionary from the index or via Solr's\nRPC mechanism\n\n\n\n\n\tWhat types of parameters and settings are desirable?\n\n\n\n\n\tHopefully in the future we can include user click through\nrates to boost those terms/phrases higher",
    "attachments": {
        "SOLR-1316.patch": "https://issues.apache.org/jira/secure/attachment/12445593/SOLR-1316.patch",
        "TST.zip": "https://issues.apache.org/jira/secure/attachment/12420146/TST.zip",
        "SOLR-1316_3x-2.patch": "https://issues.apache.org/jira/secure/attachment/12453958/SOLR-1316_3x-2.patch",
        "suggest.patch": "https://issues.apache.org/jira/secure/attachment/12420129/suggest.patch",
        "SOLR-1316_3x.patch": "https://issues.apache.org/jira/secure/attachment/12453625/SOLR-1316_3x.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Jason Rutherglen",
            "id": "comment-12736905",
            "date": "2009-07-29T23:22:25+0000",
            "content": "An alternative to the TernaryTree which does not offer a\ntraverse method is a Patricia Trie which conveniently has been\nApache licensed and implemented at:\nhttp://code.google.com/p/patricia-trie/\n\nFurther links about Patricia Tries:\n\n\n\thttp://en.wikipedia.org/wiki/Radix_tree\n\n\n\n\n\thttp://www.csse.monash.edu.au/~lloyd/tildeAlgDS/Tree/PATRICIA\n\n\n\n\n\thttp://www.imperialviolet.org/binary/critbit.pdf\n\n "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12737308",
            "date": "2009-07-30T21:46:44+0000",
            "content": "Patch coming... "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12754404",
            "date": "2009-09-11T22:57:16+0000",
            "content": "Jason, did you make any progress on this? I'm interested in this functionality.. I'm not sure tries are the best choice, unless heavily pruned they occupy a lot of RAM space. I had some moderate success using ngram based method (I reused the spellchecker, with slight modifications) - the method is fast and reuses the existing spellchecker index, but precision of lookups is not ideal. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12754440",
            "date": "2009-09-12T00:21:06+0000",
            "content": "Andrzej,\n\nThere's the ternary tree which is supposed to be better?  There are other algorithms for compressed dictionaries that could be used (off hand I can't think of them).   "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12754443",
            "date": "2009-09-12T00:27:22+0000",
            "content": "Basically we need an algorithm that does suffix compression as well? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12754482",
            "date": "2009-09-12T05:23:18+0000",
            "content": "This is a duplicate of SOLR-706.\n\nSince we comments on both, which one should I close?  "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12754505",
            "date": "2009-09-12T10:47:10+0000",
            "content": "Hi Shalin sir,\nMe and my team have successfully benchmarked Ternary Tree and Trie for autocomplete, and Ternary Tree gives the best insertion and search time. We have started working on creating a patch that can be integrated with Solr as a search component. The patch is expected to come soon.  "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12754623",
            "date": "2009-09-12T21:59:57+0000",
            "content": "Ankul, sounds good, feel free to post your ternary tree implementation.\n\nThere's some other algorithms to think about:\n\"Incremental Construction of Minimal Acyclic Finite-State Automata\"\nhttp://arxiv.org/PS_cache/cs/pdf/0007/0007009v1.pdf\n\n\"Directed acyclic word graph\"\nhttp://en.wikipedia.org/wiki/Directed_acyclic_word_graph\n\n\"worlds fastest scrabble program\"\nhttp://www1.cs.columbia.edu/~kathy/cs4701/documents/aj.pdf\n\nThese enable suffix compression and create much smaller word graphs. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12754624",
            "date": "2009-09-12T22:04:43+0000",
            "content": "And a video:\nLecture 25\nhttp://see.stanford.edu/see/lecturelist.aspx?coll=11f4f422-5670-4b4c-889c-008262e09e4e "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12754782",
            "date": "2009-09-13T23:02:09+0000",
            "content": "Hi Jason,\nMy TST implementation is here. The zip contains 4 benchmarking results too : TST1.txt , TST2.txt etc.\n\nThe 4 datasets were as follows :\nAll words are real life words extracted from dbpedia dump.\n1. The first dataset contains 1,00,000 tokens consisting of single words, phrases of two words and phrases of three words.\n2. The second dataset contains 5,00,000 tokens consisting of single words, phrases of two words and phrases of three words.\n3. The third dataset contains 10,00,000 tokens consisting of single words, phrases of two words and phrases of three words.\n4. The fourth dataset contains 50,00,000 tokens consisting of single words, phrases of two words and phrases of three words.\n\nThese were the environment details while benchmarking :\nPlatfrom : Linux\njava version \"1.6.0_16\"\nJava(TM) SE Runtime Environment (build 1.6.0_16-b01)\nJava HotSpot(TM) 64-Bit Server VM (build 14.2-b01, mixed mode)\nRAM : 16GiB\nJava HeapSize : default\n\nIs there any other way to balance the tree? Also, what's your progress? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12755030",
            "date": "2009-09-14T15:47:07+0000",
            "content": "Note, not sure how it compares, but Lucene has a TST implementation in it already. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12755039",
            "date": "2009-09-14T15:58:19+0000",
            "content": "Note, not sure how it compares, but Lucene has a TST implementation in it already. \n\nYes at org.apache.lucene.analysis.compound.hyphenation.TernaryTree but it uses char type as a pointer thus limiting it to around 65K nodes. That will not be enough. "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12755047",
            "date": "2009-09-14T16:08:01+0000",
            "content": "Also, Lucene's TST implementation doesn't has any method for autocompletion.\nI had problems understanding TST and the Lucene's TST implementation, so I\nfelt its better to code it myself.\n\n "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12756050",
            "date": "2009-09-16T14:31:40+0000",
            "content": "These enable suffix compression and create much smaller word graphs.\n\nDAWGs are problematic, because they are essentially immutable once created (the cost of insert / delete is very high). So I propose to stick to TSTs for now.\n\nAlso, I think that populating TST from the index would have to be discriminative, perhaps based on a threshold (so that it only adds terms with large enough docFreq), and it would be good to adjust the content of the tree based on actual queries that return some results (poor man's auto-learning), gradually removing least frequent strings to save space.. We could also use as a source a field with 1-3 word shingles (no tf, unstored, to save space in the source index, with a similar thresholding mechanism).\n\nAnkul, I'm not sure what's the behavior of your implementation when dynamically adding / removing keys? Does it still remain balanced?\n\nI also found a MIT-licensed  impl. of radix tree here: http://code.google.com/p/radixtree, which looks good too, one spelling mistake in the API notwithstanding  "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12756072",
            "date": "2009-09-16T15:04:01+0000",
            "content": "Removing keys shall not affect the balancing of the tree as it can be easily\ndone by making the boolean end at the leaf as false. Adding keys dynamically\nwont really keep the tree balanced in my implementation, as in my\nimplementation the tree is balanced by ordered insertion of keys. So while\nadding more keys, the TST will have to be rebuilt to make it balanced. Will\nthat be problematic?\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12756094",
            "date": "2009-09-16T16:11:43+0000",
            "content": "DAWGs are problematic, because they are essentially immutable once created (the cost of insert / delete is very high)\n\nAndrej, why would immutability be a problem? Wouldn't we have to re-build the TST if the source index changes?\n\nAlso, I think that populating TST from the index would have to be discriminative, perhaps based on a threshold\n\nI think the building of the data structure can be done in a way similar to what SpellCheckComponent does. We can re-use the HighFrequencyDictionary which can give tokens above a certain threshold frequency. The field names to use for building the data structure and the analysis can also be done like SCC. The response format for this component can also be similar to SCC. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12756149",
            "date": "2009-09-16T18:15:20+0000",
            "content": "Andrej, why would immutability be a problem? Wouldn't we have to re-build the TST if the source index changes?\n\nWell, the use case I have in mind is a TST that improves itself over time based on the observed query log. I.e. you would bootstrap a TST from the index (and here indeed you can do this on every searcher refresh), but it's often claimed that real query logs provide a far better source of autocomplete than the index terms. My idea was to start with what you have - in the absence of query logs - and then improve upon it by adding successful queries (and removing least-used terms to keep the tree at a more or less constant size).\n\nAlternatively we could provide an option to bootstrap it from a real query log data.\n\nThis use case requires mutability, hence my negative opinion about DAGWs (besides, we are lacking an implementation, don't we, whereas we already have a few suitable TST implementations). Perhaps this doesn't have to be an either/or, if we come up with a pluggable interface for this type of component?\n\nI think the building of the data structure can be done in a way similar to what SpellCheckComponent does. [..]\n\n+1 "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12756605",
            "date": "2009-09-17T16:03:27+0000",
            "content": "I started working on a skeleton component for this, so that we can test various ideas and implementations. Patch is coming shortly. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12756785",
            "date": "2009-09-17T21:02:03+0000",
            "content": "Andrzej,\n\nIs it necessary to create a new abstraction layer?  It looks like the SolrSpellChecker abstraction will work?\n "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12756819",
            "date": "2009-09-17T22:24:50+0000",
            "content": "Yes, it should work for now. In fact I started writing a new component, but it had to replicate most of the spellchecker  so I will just add bits to the existing spellchecker. I'm worried though that we abuse the semantics of the API, and it will be more difficult to fit both functions in a single API as the functionality evolves. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12757592",
            "date": "2009-09-19T01:22:24+0000",
            "content": "The DAWG seems like a potential fit as a replacement for the\nLucene term dictionary. It would provide the extra benefit of\nfaster prefix etc lookups. I believe it could be stored on disk\nby writing file pointers to the locations of the letters. I\nfound the Stanford lecture on them interesting, though the\npapers seem to overcomplicate them. I coauld not find an existing\nJava implementation. \n\nAs a generic library I think it could be useful for a variety of\nLucene based use cases (i.e. storing terms in a compact form\nthat allows fast lookups, prefix and otherwise).  "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12757687",
            "date": "2009-09-19T18:40:48+0000",
            "content": "This is a very much work in progress, to get review before proceeding.\n\nHighlights of this patch:\n\n\n\tcreated a set of interfaces in o.a.s.spelling.suggest to hide implementation details of various autocomplete mechanisms.\n\n\n\n\n\timported sources of RadixTree, Jaspell TST and Ankul's TST. Wrapped each implementation so that it works with the same interface. (Ankul: I couldn't figure out how to actually retrieve suggested keys from your TST?)\n\n\n\n\n\textended HighFrequencyDictionary to return TermFreqIterator, which gives not only words but also their frequencies. Implemented a similar iterator for file-based term-freq lists.\n\n "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12757688",
            "date": "2009-09-19T19:01:17+0000",
            "content": "For the purpose of benchmarking alone, I employed DFS just to find the\nnumber of hits for each autocomplete. But to retrieve complete keys, just\ncreate a string key variable in the TST node. In the insert function, where\nend boolean has been declared to be true (the termination condition), key\ncan be assigned the complete string. I will modify the code and post it\nsoon. May be you can also do the changes as described above.\n\nOn Sun, Sep 20, 2009 at 12:11 AM, Andrzej Bialecki (JIRA)\n "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12757758",
            "date": "2009-09-20T19:21:59+0000",
            "content": "Modified the code for returning a list of suggest keys. Andrez kindly update the same in your patch. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-12757774",
            "date": "2009-09-20T23:07:36+0000",
            "content": "For an extremely fast in-memory lookup table, I saw a TrieMap used in one of my projects. In a Trie Map, the nodes of a Hash Map are internally arranged like a Trie. The following implementation is very space efficient:\nhttp://airhead-research.googlecode.com/svn/trunk/sspace/src/edu/ucla/sspace/util/\n\nAlso, I have a fast memory mapped file based on disk TST implementation. If someone things it would be good, I can submit a patch.  "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12758589",
            "date": "2009-09-23T05:28:19+0000",
            "content": "Ishan,\n\nFeel free to post your disk TST implementation, it sounds interesting. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12777042",
            "date": "2009-11-12T16:19:20+0000",
            "content": "Updated patch that includes the new TST sources. Tests on a 100k-words dictionary yield very similar results for the TST and Jaspell implementations, i.e. the initial build time is around 600ms, and then the lookup time is around 4-7ms for prefixes that yield more than 100 results.\n\nTo test it put this in your solrconfig.xml:\n\n\n  <searchComponent name=\"spellcheck\" class=\"solr.SpellCheckComponent\">\n    <lst name=\"spellchecker\">\n      <str name=\"name\">suggest</str>\n      <str name=\"classname\">org.apache.solr.spelling.suggest.Suggester</str>\n      <str name=\"lookupImpl\">org.apache.solr.spelling.suggest.jaspell.JaspellLookup</str>\n      <str name=\"field\">text</str>\n      <str name=\"sourceLocation\">american-english</str>\n    </lst>\n  </searchComponent>\n\n...\n\n\n\n\n\nAnd then use e.g. the following parameters:\n\n\nspellcheck=true&spellcheck.build=true&spellcheck.dictionary=suggest& \\\nspellcheck.extendedResults=true&spellcheck.count=100&q=test\n\n "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12777045",
            "date": "2009-11-12T16:29:48+0000",
            "content": "Forgot to add - the RadixTree implementation doesn't work for now - it needs further refactoring to return the completed keys, and not just the values stored in nodes ... "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12778167",
            "date": "2009-11-15T20:10:47+0000",
            "content": "Nice work Andrzej!!! There's a little problem with insertion of tokens in build function of TSTLookup class. Strings in HighFrequencyDictionary must be in sorted order and simply iterating over the dict and adding strings in sorted order in TST will make the tree highly unbalanced. An ordered insertion of strings in the same way as one does a binary search over a sorted list will make the tree balanced. The function balancedTree of TSTAutocomplete class takes care of that. "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12778252",
            "date": "2009-11-16T08:18:03+0000",
            "content": "Andrzej, how are you creating the new patch? The Solr svn server seems to be down!!! Tell me asap, got to update the patch. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12778280",
            "date": "2009-11-16T10:11:46+0000",
            "content": "Re: the tree creation - well, this is the current limitation of the Dictionary API that provides only an Iterator. So in general case it's not possible to start from the middle of the iterator so that the tree is well-balanced. Is it possible to re-balance the tree on the fly?\n\nRe: svn - it works for me ... "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12778289",
            "date": "2009-11-16T10:33:48+0000",
            "content": "Re: I think we can first add the terms and frequency in separate ArrayLists using the iterator and then start from the middle? "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12778295",
            "date": "2009-11-16T10:59:26+0000",
            "content": "Well, this is kind of ugly, because it increases the memory footprint of the build phase - that was the whole point of using Iterator in the Dictionary, so that you don't have to cache all dictionary data in memory - dictionaries could be large, and they are not guaranteed to be sorted and with unique keys.\n\nBut if there are no better options for now, then yes we could do this just in TSTLookup. Is there really no way to rebalance the tree? "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12778297",
            "date": "2009-11-16T11:11:31+0000",
            "content": "I couldn't find any way to balance the tree dynamically at each insertion. But am trying to figure out some possible way (may be the way binary trees are balanced by dynamically modifying the root of the tree). Till then we can balance it by adding terms to a List and then inserting as mentioned above. Or in case the Dictionary is not sorted and is randomly ordered, then a random insertion of strings will also give roughly a balanced tree. We can benchmark it both ways. What do you say? "
        },
        {
            "author": "Mike Anderson",
            "id": "comment-12780435",
            "date": "2009-11-20T04:59:03+0000",
            "content": "Two questions, and apologies if they are addressed in the patches themselves:\n\n1) Will this be supported on distributed setups?\n\n2) (maybe this is a new ticket) Is it possible to store the field name in the spellcheck index. The use case for this is: I create a dictionary from the 'person' field and the 'title' field, when using autosuggest it would be nice to separate suggestions into 'person' suggestions and 'title' suggestions. "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12780485",
            "date": "2009-11-20T08:50:25+0000",
            "content": "Re: Mike\nAm answering your 2nd query. Yes, it is possible to auto-suggest separately for separate fields. Create separate NamedList configurations in the solrconfig.xml file specifying the fieldname(s) for each configuration. Now, words from the solr index will be extracted only from the specified fieldname(s) for each configuration and also separate search trees will be created for each configuration. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12780530",
            "date": "2009-11-20T11:56:20+0000",
            "content": "Re: question 1 - currently this component doesn't support populating the dictionary from a distributed index. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12788701",
            "date": "2009-12-10T12:25:09+0000",
            "content": "I've started looking into the patch. \n\n\n\tWhy do we concatenate all the tokens into one before calling Lookup#lookup? It seems we should be getting suggestions for each token just as SpellCheckComponent does.\n\tRelated to #1, the Lookup#lookup method should return something more fine grained rather than a SpellingResult\n\tHas anyone done any benchmarking to figure out the data structure we want to go ahead with?\n\n\n\nI love that we are (ab)using the SpellCheckComponent. The good part is that if we go this route, this auto-suggest pseudo-component will automatically work with distributed setups. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12788913",
            "date": "2009-12-10T20:51:16+0000",
            "content": "Thanks for the review!\n\nWhy do we concatenate all the tokens into one before calling Lookup#lookup? It seems we should be getting suggestions for each token just as SpellCheckComponent does.\n\nYeah, it's disputable, and we could change it to use single tokens ... My thinking was that the usual scenario is that you submit autosuggest queries soon after user starts typing the query, and the highest perceived value of such functionality is when it can suggest complete meaningful phrases and not just individual terms. I.e. when you start typing \"token sug\" it won't suggest \"token sugar\" but instead it will suggest \"token suggestions\".\n\nRelated to #1, the Lookup#lookup method should return something more fine grained rather than a SpellingResult\n\nSuch as? What you put there is what you get  so the fact that we are getting complete phrases as suggestions is the consequence of the choice above - the trie in this case is populated with phrases. If we populate it with tokens, then we can return per-token suggestions, again - losing the added value I mentioned above.\n\nHas anyone done any benchmarking to figure out the data structure we want to go ahead with?\n\nFor now I'm sure that we do NOT want to use the impl. of RadixTree in this patch, because it doesn't support our use case - I'll prepare a patch that removes this impl. Other implementations seem comparable wrt. to the speed, based on casual tests using /usr/share/dict/words, but I didn't run any exact benchmarks yet. "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12789418",
            "date": "2009-12-11T17:47:28+0000",
            "content": "Shouldn't we be creating a separate AutoSuggestComponent like the SpellCheckComponent havings its own prepare, process and inform functions? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12789797",
            "date": "2009-12-12T21:36:47+0000",
            "content": "My thinking was that the usual scenario is that you submit autosuggest queries soon after user starts typing the query, and the highest perceived value of such functionality is when it can suggest complete meaningful phrases and not just individual terms. I.e. when you start typing \"token sug\" it won't suggest \"token sugar\" but instead it will suggest \"token suggestions\".\n\nYes but the decision of selecting the complete phrase or an individual term should be up to the user. This is controlled by the \"queryAnalyzerFieldType\" in SpellCheckComponent. We will index tokens returned by that analyzer so the user can configure whichever behavior he wants. For example, if it is KeywordAnalyzer, we will index/suggest phrases and if it is a WhitespaceAnalyzer we will index/suggest individual terms.\n\nSuch as? What you put there is what you get so the fact that we are getting complete phrases as suggestions is the consequence of the choice above - the trie in this case is populated with phrases. If we populate it with tokens, then we can return per-token suggestions, again - losing the added value I mentioned above.\n\nMy point was that SpellingResult is too coarse. It is a complete result (for all tokens given by \"queryAnalyzerFieldType\"). If that analyzer gives us multiple tokens then we must get suggestions for each. In that case returning a SpellingResult for each token is not right. Instead the Suggestor should combine suggestions for all tokens into a SpellingResult object. I don't have a suggestion on an alternative. Looks like we may need to invent a custom type which represents the (suggestion, frequency) pair.\n\n\nFor now I'm sure that we do NOT want to use the impl. of RadixTree in this patch, because it doesn't support our use case - I'll prepare a patch that removes this impl. Other implementations seem comparable wrt. to the speed, based on casual tests using /usr/share/dict/words, but I didn't run any exact benchmarks yet.\n\nOK. Go ahead with the patch and I'll try to find some time to compare the two methods. What about DAWGs? Are we still considering them?\n\n\nShouldn't we be creating a separate AutoSuggestComponent like the SpellCheckComponent havings its own prepare, process and inform functions?\n\nWe could do that but as Andrej noted, we'd end up re-implementing a lot of its functionality. I'm not sure if it is worth it. I agree that it'd be odd using parameters prefixed with \"spellcheck\" for auto-suggest and it'd have been easier if it were vice-versa. Does anybody have a suggestion? "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12791161",
            "date": "2009-12-16T03:30:29+0000",
            "content": "Updated patch:\n\n\n\tremoved the broken RadixTree,\n\tchanged Suggester and Lookup API so that they don't join the tokens - instead they will use whatever tokens are produced by the analyzer. For now results are merged into a single SpellingResult.\n\n "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12791164",
            "date": "2009-12-16T03:33:46+0000",
            "content": "What about DAWGs? Are we still considering them?\n\nI would be happy to include DAWGs if someone were to implement them ...  "
        },
        {
            "author": "Brad Giaccio",
            "id": "comment-12796182",
            "date": "2010-01-04T14:13:42+0000",
            "content": "We could do that but as Andrej noted, we'd end up re-implementing a lot of its functionality. I'm not sure if it is worth it. I agree that it'd be odd using parameters prefixed with \"spellcheck\" for auto-suggest and it'd have been easier if it were vice-versa. Does anybody have a suggestion?\n\nCouldn't you just extend the SpellCheckComponent, and make use of something like COMPONENT_NAME or PARAM_PREFIX in the param calls instead of the static string in SpellingParams?  That way the autosuggestcomponent would have COMPONENT_NAME=autoSuggest and the spelling would have it set to spelling then let all the common code just live in the base class.\n\nJust a thought "
        },
        {
            "author": "David Smiley",
            "id": "comment-12804325",
            "date": "2010-01-24T23:11:28+0000",
            "content": "For auto-complete, I use Solr's faceting via facet.prefix as described in the Solr book which I authored.  How does this approach differ from that?  I suspect that the use of radix-trees and what-not as described here will result in less memory (RAM) requirements.   It would be interesting to see rough RAM requirements for the facet prefix approach based on the number of distinct terms... though only a fraction of the term space ends up being auto-completed. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12831490",
            "date": "2010-02-09T15:27:27+0000",
            "content": "Where are we on this - do people feel it's ready to commit?\nWe probably want to add some unit tests too, and some documentation on the wiki at some point.\n\nAFAIK, we're limited to one spellcheck component per request handler - that should be OK though, since presumably this is meant to be used on it's own, right?  What is the recommended/default configuration?  We should probably add it as a /autocomplete handler in the example server.\n\nDoes this currently work with phrases? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12831544",
            "date": "2010-02-09T17:10:32+0000",
            "content": "Where are we on this - do people feel it's ready to commit?\n\nIt has been some time since I looked at it but I don't feel it is ready. Using it through spellcheck works but specifying spell check params feels odd. Also, I don't know how well it compares to regular TermsComponent or facet.prefix searches in terms of memory and cpu cost. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-12833566",
            "date": "2010-02-14T13:21:18+0000",
            "content": "Found this excellent article on usability of auto suggest: http://blog.twigkit.com/search-suggestions-part-1/\n\nA bit more talking about use cases could help us assert that we're solving the right problem \n\nOne complex use case could be:\n\n\tFirst display at most 0-1 suggestion if some terms are mis-spelled.\n\tThen display 0-3 suggestions from the categories field, ordered by most docs in that category\n\tThen display 0-10 generic suggestions based on fields \"title,keywords\" ordered by relevancy and tf/idf\n\n\n\nYet another use case (for shopping sites) is:\n\n\tFirst display 0-3 actual products, if there is an exact match on product name. These suggestsion are of type \"instant result\", and must return to frontend all data necessary to display a preview of the instant result. Clicking this suggestion takes the user directly to product page.\n\tThen display 0-10 suggestions from an index (separate Solr core?) containing actual user queries, with offensive content filtered out, sort by relevancy, boost by frequency\n\n\n\nOne way to back the suggest from user query log is through a full-blown solr core, where you in addition to the terms also index meta data such as usage frequency. Your auto suggest query could then benefit from all of Solr's power in ranking etc.\n\nDo complex scenarios like this belong within one request to one instance of autosuggest component? Would be very powerful. Or is it better to require users to build a proxy between frontend and Solr which issues multiple requests to multiple autosuggest URLs and/or queries to ordinary indices? "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12833786",
            "date": "2010-02-15T12:31:16+0000",
            "content": "I would lean towards the latter - complex do-it-all components often suffer from creeping featuritis and insufficient testing/maintenance, because there are few users that use all their features, and few developers that understand how they work. I subscribe to the Unix philosophy - do one thing, and do it right, so I think that if we can implement autosuggest that works well from the technical POV, then it will become a reliable component that you can combine in many creative ways to satisfy different scenarios, of which there are likely many more than what you described ... "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12844083",
            "date": "2010-03-11T14:52:15+0000",
            "content": "What's the status on this? "
        },
        {
            "author": "jonas stock",
            "id": "comment-12848773",
            "date": "2010-03-23T15:59:22+0000",
            "content": "hello. \ni have some trouble to apply this patch. \ncan anyone send me a little how to for windows ?  "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12871986",
            "date": "2010-05-26T22:33:45+0000",
            "content": "Updated patch. This version of the patch is relative to trunk/. It supports onlyMorePopular, and includes a unit test with a small benchmark.\n\nExample results for 100,000 keys in the dictionary - size is in bytes, times are totals for all 100k lookups.\n\n\nJaspellLookup:\tsize=81078997\tbuildTime=1347\tlookupTime=1431\nTSTLookup:\tsize=53453696\tbuildTime=316\tlookupTime=1453\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12872225",
            "date": "2010-05-27T14:07:49+0000",
            "content": "And what are the units for the time?  I assume ms.  Do you have any tests for the \"average\" case?  i.e. lookup of a top 5-10 for a given prefix? "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12872263",
            "date": "2010-05-27T15:53:29+0000",
            "content": "Yes, sorry, it's [ms] of elapsed time for adding 100k strings or looking up 100k strings, though I admit I was lazy and used the same strings for lookup as I did for the build phase ... I'll change this so that it properly looks up prefixes and I'll post new results. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12872634",
            "date": "2010-05-27T22:09:37+0000",
            "content": "Updated patch - previous version had a missing \"else\" in TSTLookup.\n\nImproved benchmark - the other results are useless, the running time was too short. Also, this time the lookup keys are strict prefixes between 1/3 and 1/2 length of the key.  Result counts are cross-validated between JaSpell and TST implementations.\n\nHere are the new timings (times in [ms] for 100k items):\n\n\nJaspellLookup:\tbuildTime=448\tlookupTime=1316\nJaspellLookup:\tbuildTime=379\tlookupTime=1073\nJaspellLookup:\tbuildTime=399\tlookupTime=709\nJaspellLookup:\tbuildTime=405\tlookupTime=698\nJaspellLookup:\tbuildTime=454\tlookupTime=758\nJaspellLookup:\tbuildTime=451\tlookupTime=746\nJaspellLookup:\tbuildTime=436\tlookupTime=886\nJaspellLookup:\tbuildTime=424\tlookupTime=696\nJaspellLookup:\tbuildTime=402\tlookupTime=697\nJaspellLookup:\tbuildTime=415\tlookupTime=1156\nJaspellLookup:\tbuildTime=413\tlookupTime=693\nJaspellLookup:\tbuildTime=429\tlookupTime=698\nJaspellLookup:\tbuildTime=411\tlookupTime=885\nJaspellLookup:\tbuildTime=402\tlookupTime=688\nJaspellLookup:\tbuildTime=398\tlookupTime=691\nJaspellLookup:\tbuildTime=405\tlookupTime=1152\nJaspellLookup:\tbuildTime=405\tlookupTime=695\nJaspellLookup:\tbuildTime=410\tlookupTime=1009\nJaspellLookup:\tbuildTime=409\tlookupTime=891\nJaspellLookup:\tbuildTime=400\tlookupTime=685\nTSTLookup:\tbuildTime=185\tlookupTime=289\nTSTLookup:\tbuildTime=161\tlookupTime=427\nTSTLookup:\tbuildTime=173\tlookupTime=311\nTSTLookup:\tbuildTime=183\tlookupTime=304\nTSTLookup:\tbuildTime=177\tlookupTime=311\nTSTLookup:\tbuildTime=175\tlookupTime=287\nTSTLookup:\tbuildTime=173\tlookupTime=431\nTSTLookup:\tbuildTime=161\tlookupTime=278\nTSTLookup:\tbuildTime=161\tlookupTime=282\nTSTLookup:\tbuildTime=177\tlookupTime=453\nTSTLookup:\tbuildTime=157\tlookupTime=286\nTSTLookup:\tbuildTime=160\tlookupTime=432\nTSTLookup:\tbuildTime=161\tlookupTime=281\nTSTLookup:\tbuildTime=160\tlookupTime=275\nTSTLookup:\tbuildTime=160\tlookupTime=454\nTSTLookup:\tbuildTime=178\tlookupTime=298\nTSTLookup:\tbuildTime=181\tlookupTime=289\nTSTLookup:\tbuildTime=159\tlookupTime=432\nTSTLookup:\tbuildTime=164\tlookupTime=285\nTSTLookup:\tbuildTime=159\tlookupTime=480\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12872642",
            "date": "2010-05-27T22:09:47+0000",
            "content": "Bulk updating 240 Solr issues to set the Fix Version to \"next\" per the process outlined in this email...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E\n\nSelection criteria was \"Unresolved\" with a Fix Version of 1.5, 1.6, 3.1, or 4.0.  email notifications were suppressed.\n\nA unique token for finding these 240 issues in the future: hossversioncleanup20100527 "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12873560",
            "date": "2010-05-31T06:47:23+0000",
            "content": "Are we currently using the UnsortedTermFreqIteratorWrapper to build a randomly balanced ternary search tree? How about building a balanced tree using SortedIterator in manner similar to the balancedTree( ) function that I have implemented in TSTAutocomplete. That may further improve the results. What say? "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12873583",
            "date": "2010-05-31T08:30:37+0000",
            "content": "Are we currently using the UnsortedTermFreqIteratorWrapper to build a randomly balanced ternary search tree?\n\nYes.\n\nHow about building a balanced tree using SortedIterator in manner similar to the balancedTree( ) function that I have implemented in TSTAutocomplete. That may further improve the results. What say?\n\nIndeed  UnsortedTFIW buffers all input strings anyway, so we might as well do it this way. I'll post results shortly. "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12873586",
            "date": "2010-05-31T08:56:14+0000",
            "content": "Here are the results comparing TSTLookup for UnsortedTFIT and SortedTFIT:\n\nFor UnsortedTFIT\n\nTSTLookup:\tbuildTime=384\tlookupTime=418\nTSTLookup:\tbuildTime=272\tlookupTime=414\nTSTLookup:\tbuildTime=402\tlookupTime=611\nTSTLookup:\tbuildTime=383\tlookupTime=372\nTSTLookup:\tbuildTime=421\tlookupTime=569\nTSTLookup:\tbuildTime=317\tlookupTime=519\nTSTLookup:\tbuildTime=388\tlookupTime=588\nTSTLookup:\tbuildTime=269\tlookupTime=508\nTSTLookup:\tbuildTime=267\tlookupTime=454\nTSTLookup:\tbuildTime=380\tlookupTime=613\nTSTLookup:\tbuildTime=312\tlookupTime=360\nTSTLookup:\tbuildTime=279\tlookupTime=536\nTSTLookup:\tbuildTime=337\tlookupTime=368\nTSTLookup:\tbuildTime=356\tlookupTime=368\nTSTLookup:\tbuildTime=343\tlookupTime=517\nTSTLookup:\tbuildTime=383\tlookupTime=541\nTSTLookup:\tbuildTime=277\tlookupTime=590\nTSTLookup:\tbuildTime=300\tlookupTime=720\nTSTLookup:\tbuildTime=334\tlookupTime=464\nTSTLookup:\tbuildTime=381\tlookupTime=613\n\nFor SortedTFIT\nTSTLookup:\tbuildTime=532\tlookupTime=382\nTSTLookup:\tbuildTime=507\tlookupTime=367\nTSTLookup:\tbuildTime=535\tlookupTime=433\nTSTLookup:\tbuildTime=377\tlookupTime=365\nTSTLookup:\tbuildTime=513\tlookupTime=364\nTSTLookup:\tbuildTime=355\tlookupTime=419\nTSTLookup:\tbuildTime=359\tlookupTime=418\nTSTLookup:\tbuildTime=371\tlookupTime=362\nTSTLookup:\tbuildTime=475\tlookupTime=461\nTSTLookup:\tbuildTime=371\tlookupTime=422\nTSTLookup:\tbuildTime=360\tlookupTime=362\nTSTLookup:\tbuildTime=357\tlookupTime=421\nTSTLookup:\tbuildTime=511\tlookupTime=419\nTSTLookup:\tbuildTime=482\tlookupTime=360\nTSTLookup:\tbuildTime=532\tlookupTime=430\nTSTLookup:\tbuildTime=465\tlookupTime=458\nTSTLookup:\tbuildTime=534\tlookupTime=359\nTSTLookup:\tbuildTime=358\tlookupTime=420\nTSTLookup:\tbuildTime=359\tlookupTime=418\nTSTLookup:\tbuildTime=356\tlookupTime=361\n\nThe lookup time in second case seems to have further reduced with a slight overhead for build time.\n "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12873590",
            "date": "2010-05-31T09:12:49+0000",
            "content": "Andrzej, waiting for your results. After that we can finalize the more appropriate method and plan out the work that needs to be done further (if any). "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12873599",
            "date": "2010-05-31T09:58:33+0000",
            "content": "I modified the benchmark slightly - it takes much longer to execute, but results are more stable across runs.\n\n\nTSTLookup - size=53453696\nJaspellLookup - size=81078997\n* Running 100 iterations for JaspellLookup ...\n  - warm-up 10 iterations...\n  - main iterations: 10 20 30 40 50 60 70 80 90\nJaspellLookup: buildTime[ms]=412 lookupTime[ms]=1030\n* Running 100 iterations for TSTLookup ...\n  - warm-up 10 iterations...\n  - main iterations: 10 20 30 40 50 60 70 80 90\nTSTLookup: buildTime[ms]=405 lookupTime[ms]=288\n\n\n\nAt this point I think we are making only minor incremental improvements, so if people  feel this is a useful component ni its current shape then  I propose to commit it. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12873603",
            "date": "2010-05-31T10:02:35+0000",
            "content": "Just to clarify: the buildTime above is how long it takes to build a trie with 100k elements. The lookupTime is how long it takes to perform 100k lookups using short keys (i.e. returning usually many more than 1 completion). "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12873623",
            "date": "2010-05-31T11:38:52+0000",
            "content": "Minor changes - clean up imports, improve the Suggester.reload(). "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12873638",
            "date": "2010-05-31T12:48:30+0000",
            "content": "Another minor change in TSTLookup. Its better to assign root as null, so that on building the tree, the splitchar of root won't be null as it is the case currently. "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12873639",
            "date": "2010-05-31T12:55:06+0000",
            "content": "Updated TSTAutocomplete balancedTree() too. Its fine now. "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12873640",
            "date": "2010-05-31T13:01:01+0000",
            "content": "How about committing the component? Comments awaited! "
        },
        {
            "author": "David Smiley",
            "id": "comment-12874026",
            "date": "2010-06-01T14:24:36+0000",
            "content": "(re-post)\nFor auto-complete, I use Solr's faceting via facet.prefix as described in the Solr book which I authored. How does this approach differ from that? I suspect that the use of radix-trees and what-not as described here will result in less memory (RAM) requirements. It would be interesting to see rough RAM requirements for the facet prefix approach based on the number of distinct terms... though only a fraction of the term space ends up being auto-completed. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12874084",
            "date": "2010-06-01T15:56:32+0000",
            "content": "I would expect the facet.prefix method to consume less additional RAM dedicated to this functionality (because field caches are shared among different components), but I doubt it could beat the performance of ternary tries.\n\nAlso, in many situations it makes sense to populate an auto-complete component from a query log, and not from the index. If you want to offer phrase-based autocomplete then using facet.prefix method you would have to create an additional field populated with shingles, and then the RAM cost would grow tremendously, whereas with TST this increase would be moderate.\n\nRegarding specific numbers ... if you have a test setup for facet.prefix feel free to apply this patch and test it, we'd love to see your results - see also the numbers above for 100k random strings. "
        },
        {
            "author": "Stefan Seidel",
            "id": "comment-12879799",
            "date": "2010-06-17T15:06:18+0000",
            "content": "Hi. First of all, thanks for making this effort for a usable auto-suggest feature.\n\nI have a question which is (hopefully) relevant to all this: we tried using the facet method for autosuggest as presented in David's book about solr. It works well and usually fast, however, there are two issues: (a) initializing the faceting takes time (>300 sec. for 8.3 million docs here), which would be ok if not (b) after any commit (even without new documents added) the faceting needs exactly this time again. On a heavy load server with commits every 60 seconds, this doesn't work at all. Also, it makes solr use >1G of RAM, and I can get it into HeapSpace errors when multiple queries are running and the faceting hasn't been initialized.\n\nQuestion: has this scenario (frequent commits) been considered a use case when implementing these new components? Will the tree get rebuilt at every commit as well? (Judging from the previous numbers this should take about 10 sec. in our scenario?) I think this is a crucial question for the whole feature. "
        },
        {
            "author": "David Smiley",
            "id": "comment-12879820",
            "date": "2010-06-17T15:52:55+0000",
            "content": "Stefan, just checking, are you using Solr 1.4?  It contains efficiency improvements in this regard that are very important for this use case.  Also, I recommend that your firstSearcher queries contain the faceting you need so that users never see this delay.  I don't think this needs to go in newSearcher but I could be wrong. "
        },
        {
            "author": "muneeb ali",
            "id": "comment-12879847",
            "date": "2010-06-17T17:17:03+0000",
            "content": "Hi, I have a very relevant question with regards to this feature. I am a beginner to solr so bare with my limited knowledge. I am following David's book to implement the auto complete feature, which uses facet.prefix to return the suggestions. I have two questions, \n\n1- To make use of solr-auto-complete, would I need to query solr everytime a letter is typed in the search box? \n2 - How does the above component differ from david's method in terms of implementation (a quick demo of schema/solrconfig would be appreciated). \n\nIf there is any documentation on this, could anyone guide me to that please?  \n\nThanks in advance. "
        },
        {
            "author": "Andy",
            "id": "comment-12880589",
            "date": "2010-06-20T05:15:37+0000",
            "content": "Does this handle non-prefix matches?\n\nFor example, if user types \"guit\", I want to suggest both \"guitar\" and \"electric guitar\".\n\nWould this patch do that? "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12880751",
            "date": "2010-06-21T08:58:25+0000",
            "content": "Stefan, yes the tree needs to be re-built at each commit. But, I think the problem can be overcome. This can be done by implementing the ternary tree to be self-balancing. Thus, inserting and deleting strings won't affect the balancing of the tree. But, does the commit operation gives us a new set of tokens altogether, or can we have separate sets of newly added tokens and deleted tokens? The latter can help the trick come into action. "
        },
        {
            "author": "Stefan Seidel",
            "id": "comment-12881149",
            "date": "2010-06-22T10:04:56+0000",
            "content": "@David, yes we'\u00aee using Solr1.4. And of course, putting the facet query in firstSearcher only alleviates the problem of several searches trashing the solr server, because they all need to wait for the autowarming, but it still takes ~240 seconds to initialize faceting, and this is not usable with commits every 60 seconds.\n\n@Ankul, thanks for the analysis. It would be great if the commit changes could be merged on the fly, but what about an option to not rebuild the tree at every commit? Like the spellchecker, the autosuggest can usually drift from the main index until an optimize is done (e.g. during night time). Then we wouldn't get the most up-to-date suggestions, but this is usually not really needed. "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12881177",
            "date": "2010-06-22T11:45:55+0000",
            "content": "@Stefan, for not re-building the tree at each commit, we need to have a separate List of newly added tokens. Is that possible? My idea here to have a separate list of newly added tokens is that each time a commit is performed, we would be having a list of tokens which have been newly added and they can be inserted in the tree still keeping the tree balanced. This can be done by the implementation of Self-balancing ternary search tree. But, first I need to confirm whether we can have any such list of newly added tokens or not? "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12882527",
            "date": "2010-06-25T10:50:06+0000",
            "content": "Newly added terms will come from the latest segments, if the tree is populated from an IndexReader. Also, Solr uses IndexReader.reopen() to catch up with new data in new segments. From this it follows that as long as the IndexReader instance remains the same (i.e. it comes from reopen) if we keep track of the previous maxDoc then we can also catch up with new terms by processing only the latest segments (using IndexReader.getSequentialSubReaders() and IndexReader.getSubReaderDocBase()).\n\nSo, the short answer to your question is \"yes\"  "
        },
        {
            "author": "Ankul Garg",
            "id": "comment-12882838",
            "date": "2010-06-26T13:26:23+0000",
            "content": "@Andrzej, am working on the implementation of self-balancing TST. The idea is similar to height balancing as used in AVL trees. Hope to get back soon with the implementation  "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12889134",
            "date": "2010-07-16T11:24:44+0000",
            "content": "Play the catch-up game with the trunk ... This patch includes a test case + resources, and fixes an error in conversion from priority queue to lookup results.\n\nI would appreciate a review - if there are no objections I'd like to commit this soon. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12889139",
            "date": "2010-07-16T11:56:20+0000",
            "content": "Previous patch had some unrelated changes. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12889154",
            "date": "2010-07-16T12:46:13+0000",
            "content": "Hi Andrzej, i have a concern about unicode support here:\n\nin JaspellTernarySearchTrie.compareCharsAlphabetically there is some code like:\n\nif (cCompare2 >= 65) {\n     if (cCompare2 < 89) {\n        cCompare = (2 * cCompare2) - 65;\n      } else if (cCompare2 < 97) {\n        cCompare = cCompare2 + 24;\n      } else if (cCompare2 < 121) {\n        cCompare = (2 * cCompare2) - 128;\n      } else cCompare = cCompare2;\n    } else cCompare = cCompare2;\n...\n\n\n\nCould we consider a more unicode-friendly approach, such as simply comparing Character.toLowerCase(cCompare2) with Character.toLowerCase(cRef) ? "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12889172",
            "date": "2010-07-16T13:34:00+0000",
            "content": "Yeah, this code comes from Jaspell... if we change it then it needs a unit test IMHO. I'll see what I can do. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12889195",
            "date": "2010-07-16T14:34:57+0000",
            "content": "Andrzej, it might not be a problem, looking at how the result is used (it seems only the sign is significant)? \n\nBut i have trouble understanding what the point of all the complexity in this method is... if its just as its documented, it seems it could be much simpler: (eg b-a). So I feel like I am missing something.\n\n\n\n\nchar A\nchar B\nresult\n\n\nA\nA\n0\n\n\na\na\n0\n\n\nA\na\n-1\n\n\na\nA\n1\n\n\n\u00c3\n\u00e3\n-32\n\n\n\u00e3\n\u00c3\n32\n\n\n\u043b\n\u041b\n32\n\n\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12899114",
            "date": "2010-08-16T21:54:56+0000",
            "content": "is it the case that I have build the TST (i.e. issue spellcheck.build=true) every time I start up Solr? "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12899186",
            "date": "2010-08-16T23:31:25+0000",
            "content": "Unfortunately yes, that's the way it works at the moment. I added hooks for storing/loading tries but they are dummy placeholders for now. Contributions are welcome  "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12899454",
            "date": "2010-08-17T16:31:18+0000",
            "content": "I think we should at least open an issue for it and link to this one when this one gets committed, as it takes a while to build.  "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12900275",
            "date": "2010-08-19T13:17:45+0000",
            "content": "Robert,\nBut i have trouble understanding what the point of all the complexity in this method is... if its just as its documented, it seems it could be much simpler: (eg b-a). So I feel like I am missing something.\n\nI don't get it either... it's a borrowed code after all  Anyway, I replaced this method with the following:\n\n\n  private static int compareCharsAlphabetically(char cCompare2, char cRef) {\n    return Character.toLowerCase(cCompare2) - Character.toLowerCase(cRef);\n  }\n\n\n\nand all tests pass, including those that test for correctness of returned suggestions and consistency between Jaspell and TST. I also ran testBenchmark() and differences in timings are negligible.\n\nGrant,\nI think we should at least open an issue for it and link to this one when this one gets committed, as it takes a while to build.\nYes, I'll open an issue when this gets committed.\n\nIf there are no further objections I'd like to commit this. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12900279",
            "date": "2010-08-19T13:31:31+0000",
            "content": "Updated patch. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12900282",
            "date": "2010-08-19T14:01:20+0000",
            "content": "\nand all tests pass, including those that test for correctness of returned suggestions and consistency between Jaspell and TST. I also ran testBenchmark() and differences in timings are negligible.\n\nThanks Andrzej! "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12901386",
            "date": "2010-08-23T13:13:44+0000",
            "content": "Yet another attempt to catch the peleton leaders... This is the version to be committed now. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12901404",
            "date": "2010-08-23T13:58:42+0000",
            "content": "Committed to trunk in rev. 988120. Thanks to all who contributed to this issue! "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12902693",
            "date": "2010-08-25T22:58:38+0000",
            "content": "Would it make sense to add this to 3.x? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12902989",
            "date": "2010-08-26T19:07:51+0000",
            "content": "Here's a patch to the tests:\n\n\tadds ASL header\n\tremoves @Override from interface methods\n\tconverts to Junit4\n\tgreatly simplifies things by using standard test utility methods and going through the front door (i.e. there's usually no reason to manually look up a request handler).\n\tremoves the testReload altogether.. it didn't seem to test anything that wasn't tested by the first test method (i.e. that a commit will rebuild).\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12902993",
            "date": "2010-08-26T19:13:58+0000",
            "content": "removes @Override from interface methods\n\nwhys that when we are on java 6 now? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12902996",
            "date": "2010-08-26T19:19:19+0000",
            "content": "Makes it easier to backport to 3x later if desired.  Not a big deal - it's test code after all.\nAnd from what Uwe says... even though Java6 allows it, it's still \"incorrect\"  "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12905223",
            "date": "2010-09-01T21:00:09+0000",
            "content": "Backports to 3.x (includes Andrzej's final patch, plus Yonik's update).  Weird thing, though, the SuggesterTest no longer passes.  It suggests accidentally instead of the ones given.  The only thing that is different is the HighFreqDictionary implementation of freq().\n\nWeird.  Maybe I'm missing something in terms of diffs between term enumeration between the 3.x and trunk. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12906562",
            "date": "2010-09-06T18:35:28+0000",
            "content": "Grant, you missed changes to HighFrequencyDictionary#HighFrequencyIterator. Here's the patch that adds these changes, all tests pass now.\n\nIf there are no objections I'll commit this shortly. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12906611",
            "date": "2010-09-06T22:33:33+0000",
            "content": "If there are no objections I'll commit this shortly.\n\n+1. BTW, some of newly added files in the patch are missing Apache License header. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12906812",
            "date": "2010-09-07T14:20:19+0000",
            "content": "I added license headers and committed the patch in rev.  993367 - thank you! "
        },
        {
            "author": "John Beck",
            "id": "comment-12910777",
            "date": "2010-09-17T20:40:28+0000",
            "content": "If you wish to use the comment text you have typed (shown below), please copy it now. This text will be lost when you leave this screen.\nHey guys, really nice work on this, it is extremely fast! In production right now we're seeing 200-700ms latency, and with this I typically get between 1ms and 10ms. \n\nI did find one issue though. I'm going to use this with a dictionary of 150k medical terms, and it's works, except for when my query happens to be a popular starting word. \n\nIf I use this as a dictionary, \n\n \nHepatitis B Viruses, Duck \nHepatitis B e Antigens \nHepatitis B virus \nHepatitis B, Chronic \nHepatitis Be Antigens \nHepatitis C \nHepatitis C Antibodies \nHepatitis C Antigen \n\n \n\nAnd then search for Hepatitis C, \n\n \ncurl \"http://localhost:8982/solr/suggest/?spellcheck=true&spellcheck.dictionary=suggest&spellcheck.extendedResults=true&spellcheck.count=5&q=Hepatitis%20C\" \n<response> \n<lst name=\"responseHeader\"><int name=\"status\">0</int><int name=\"QTime\">1</int></lst><str name=\"command\">build</str><lst name=\"spellcheck\">\n<lst name=\"suggestions\"><lst name=\"Hepatitis\"><int name=\"numFound\">5</int><int name=\"startOffset\">0</int><int name=\"endOffset\">9</int>\n<arr name=\"suggestion\"><str>hepatitis b e antigens</str><str>hepatitis b virus</str><str>hepatitis b viruses, duck</str><str>hepatitis b, chronic</str>\n<str>hepatitis be antigens</str></arr></lst></lst></lst> \n</response> \n\n \n\nYou can see it never makes it to Hepatitis C since it's #6 in that dictionary, and I'm limiting the results to 5. \n\nWhen I bump spellcheck.count=6, then I get the very first Hepatitis C result but not the rest. \n\nSo there are about 2500 terms that start with \"Receptor\" and I don't want to have to bump it to 3000 results. Is there anything else that can be done? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12915363",
            "date": "2010-09-27T17:08:49+0000",
            "content": "Is there any documentation for this autosuggest component?  If so, I can't seem to find it. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12915393",
            "date": "2010-09-27T17:54:52+0000",
            "content": "yeah, with 32 watchers and 10 votes, some doc might be appreciated  Tough to find out how to set this up by looking through all these jira comments. We need some tips/warnings about how this works too - how much RAM it might take on a large index (eg warn that it could be a lot), how to reduce that if you need to (threshold), and how the autocomplete index is not preserved over core reloads, etc. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12915449",
            "date": "2010-09-27T19:53:40+0000",
            "content": "Thanks for prodding  I'll create a wiki page with the docs. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12915454",
            "date": "2010-09-27T20:05:20+0000",
            "content": "FWIW, much of the docs are the same as the SpellCheckComponent.  In fact, I'm planning on implementing a Related Search capability, and it occurs to me that Related Search, Spelling and AutoSuggest are all just variants of the same thing (given a Query, give me suggestions for pertinent things around that query), so I'm going to refactor things a little bit to reflect that.  There shouldn't be any functionality change.  The only thing I was contemplating changing was in the config the <spellcheck> tag be renamed as <suggestion> (it would still accept the old, too).  Code-wise, the gist of what I have in mind is that there will be a SuggestionComponent and SpellCheckComp will just be a empty, deprecated shell on that.   Maybe would take SolrSpellChecker and rename that (or pull it up a level) as SolrSuggester.  Everything should still work fine, but the nomenclature is better, I think.\n\nGiven the above, I think the docs could then be reworked to show the various \"suggestion\" paths one might take: auto, spelling, related, other and how they can be implemented and reuse the same input params and output responses. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12915530",
            "date": "2010-09-27T22:48:33+0000",
            "content": "Sounds like a good plan. Let's track this in a separate issue - this one is bloated enough as it is. FWIW, I created http://wiki.apache.org/solr/Suggester to cover the current functionality, we can reuse it later too for the merged docs. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12917262",
            "date": "2010-10-02T23:14:25+0000",
            "content": "Thanks for the docs Andrzej!  I was able to get it working using the configuration on that wiki page, after\nchanging threshold to a float:\n      <float name=\"threshold\">0.005</float>\nand changing the \"field\" to something in the example data:\n      <str name=\"field\">name</str>\n\nAutosuggest is a desirable enough feature for people that we should consider adding some config for it to the example schema,\nand adding a \"QuickStart\" to the wiki page so that people can immediately be successful with it.\n\nRandom Q: do we have a capability that can mimic google's suggest yet (i.e. both single and multi-word suggestions at the same time)?  I assume that maybe adding a field-type that shingles may be enough?  If so, we should add that to the example server (hopefully on some field that won't negatively impact performance). "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12917366",
            "date": "2010-10-03T17:15:52+0000",
            "content": "It's baked into the VelocityRespWriter example at /solr/browse (although I think it currently uses browse).\n\nFWIW, I think we should make /solr/browse the default example/tutorial.  It is really great for getting up and going and showing people a real search user interface. "
        },
        {
            "author": "Silvan Zurbruegg",
            "id": "comment-12924305",
            "date": "2010-10-24T10:33:42+0000",
            "content": "Is there a possibility to have minimal query-logic when looking up suggestions with the suggester?\nAfaik, all syntax like 'AND' or 'OR' is stripped by the QueryConverter. From playing around with the suggester i figured out that looking up multiple terms is possible, resulting in a 'OR'-like list for each term. Is it possible to have support for an 'AND' logic as well? For example looking up the terms '+css' and '+web' would yield in suggestions from documents that contain both terms. The first thing coming to mind are TermVectors, but since term vectors are document based and probably not related to the suggester-dictionary this could get complicated and is probably out of the scope of the suggester component.  "
        },
        {
            "author": "Abhay Dabholkar",
            "id": "comment-12924542",
            "date": "2010-10-25T12:05:45+0000",
            "content": "I have a file which has words, phrases in it.\n\nI was wondering how to make following possible.\n\nfile has\n-------------\nrebate form\nform\n\nwhen i look for \"form\" or even \"for\" i would like to have rebate form to be included too.\nI tried using\n      <str name=\"lookupImpl\">org.apache.solr.spelling.suggest.jaspell.JaspellLookup</str>\nbut no luck, wiki suggests some one liner change to get fuzzy suggestions.\n\nhow would i configure this? and if changes need to be made to JaspellLookup what that would be? "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12924630",
            "date": "2010-10-25T16:50:08+0000",
            "content": "This issue is closed. Please discuss the functionality on the mailing lists, or if there are bugs / missing features then please create a new JIRA issue. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12988590",
            "date": "2011-01-30T14:04:17+0000",
            "content": "Andrzej, could you review the files added by this commit, and see if any need Apache 2 license headers?\nFrom the comments, it seems some of it comes from jaspell, so I am hesitant to apply any headers...\n\n\n[rat:report]   C:/Users/rmuir/workspace/lucene/solr/src/java/org/apache/solr/spelling/suggest/BufferingTermFreqIteratorWrapper.java\n[rat:report]   C:/Users/rmuir/workspace/lucene/solr/src/java/org/apache/solr/spelling/suggest/Lookup.java\n[rat:report]   C:/Users/rmuir/workspace/lucene/solr/src/java/org/apache/solr/spelling/suggest/SortedTermFreqIteratorWrapper.java\n[rat:report]   C:/Users/rmuir/workspace/lucene/solr/src/java/org/apache/solr/spelling/suggest/UnsortedTermFreqIteratorWrapper.java\n[rat:report]   C:/Users/rmuir/workspace/lucene/solr/src/java/org/apache/solr/spelling/suggest/jaspell/JaspellLookup.java\n[rat:report]   C:/Users/rmuir/workspace/lucene/solr/src/java/org/apache/solr/spelling/suggest/jaspell/JaspellTernarySearchTrie.java\n[rat:report]   C:/Users/rmuir/workspace/lucene/solr/src/java/org/apache/solr/spelling/suggest/tst/TSTAutocomplete.java\n[rat:report]   C:/Users/rmuir/workspace/lucene/solr/src/java/org/apache/solr/spelling/suggest/tst/TSTLookup.java\n[rat:report]   C:/Users/rmuir/workspace/lucene/solr/src/java/org/apache/solr/spelling/suggest/tst/TernaryTreeNode.java\n\n "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12988591",
            "date": "2011-01-30T14:06:36+0000",
            "content": "Also, some files in solr.util, too:\n\n[rat:report]   C:/Users/rmuir/workspace/lucene/solr/src/java/org/apache/solr/util/SortedIterator.java\n[rat:report]   C:/Users/rmuir/workspace/lucene/solr/src/java/org/apache/solr/util/TermFreqIterator.java\n\n "
        },
        {
            "author": "David Smiley",
            "id": "comment-13031223",
            "date": "2011-05-10T14:49:56+0000",
            "content": "Hoss, you inadvertently renamed the title from \"Create autosuggest component\" to the number 3.  Please put it back. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13157881",
            "date": "2011-11-27T12:38:58+0000",
            "content": "Bulk close after release of 3.1 "
        }
    ]
}