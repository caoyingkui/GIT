{
    "id": "LUCENE-6879",
    "title": "Allow to define custom CharTokenizer using Java 8 Lambdas/Method references",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "6.0",
        "components": [
            "modules/analysis"
        ],
        "labels": "",
        "fix_versions": [
            "6.0"
        ],
        "priority": "Major",
        "status": "Resolved",
        "type": "Improvement"
    },
    "description": "As a followup from LUCENE-6874, I thought about how to generate custom CharTokenizers wthout subclassing. I had this quite often and I was a bit annoyed, that you had to create a subclass every time.\n\nThis issue is using the pattern like ThreadLocal or many collection methods in Java 8: You have the (abstract) base class and you define a factory method named fromXxxPredicate (like ThreadLocal.withInitial(() -> value).\n\n\npublic static CharTokenizer fromTokenCharPredicate(java.util.function.IntPredicate predicate)\n\n\n\nThis would allow to define a new CharTokenizer with a single line statement using any predicate:\n\n\n// long variant with lambda:\nTokenizer tok = CharTokenizer.fromTokenCharPredicate(c -> !UCharacter.isUWhiteSpace(c));\n\n// method reference for separator char predicate + normalization by uppercasing:\nTokenizer tok = CharTokenizer.fromSeparatorCharPredicate(UCharacter::isUWhiteSpace, Character::toUpperCase);\n\n// method reference to custom function:\nprivate boolean myTestFunction(int c) {\n return (cracy condition);\n}\nTokenizer tok = CharTokenizer.fromTokenCharPredicate(this::myTestFunction);\n\n\n\nI know this would not help Solr users that want to define the Tokenizer in a config file, but for real Lucene users this Java 8-like way would be easy and elegant to use. It is fast as hell, as it is just a reference to a method and Java 8 is optimized for that.\n\nThe inverted factories fromSeparatorCharPredicate() are provided to allow quick definition without lambdas using method references. In lots of cases, like WhitespaceTokenizer, predicates are on the separator chars (isWhitespace(int), so using the 2nd set of factories you can define them without the counter-intuitive negation. Internally it just uses Predicate#negate().\n\nThe factories also allow to give the normalization function, e.g. to Lowercase, you may just give Character::toLowerCase as IntUnaryOperator reference.",
    "attachments": {
        "LUCENE-6879.patch": "https://issues.apache.org/jira/secure/attachment/12770178/LUCENE-6879.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14986186",
            "author": "Uwe Schindler",
            "date": "2015-11-02T22:35:56+0000",
            "content": "Patch using Java 8s new functional APIs. Very cool and simple to define a new Tokenizer.\n\nI only don't like that CharTokenizer is in oal.analysis.util package. Maybe we should move the factories to a separate class in the oal.analysis,core pkg.\n\nThe patch also has some tests showing how you would use them. "
        },
        {
            "id": "comment-14986198",
            "author": "Robert Muir",
            "date": "2015-11-02T22:43:17+0000",
            "content": "I think the tests are nice examples and like the separator vs tokenchar methods (it can be hard to think about opposites).\n\nGood improvement for java 8 on trunk. "
        },
        {
            "id": "comment-14986222",
            "author": "Uwe Schindler",
            "date": "2015-11-02T23:03:42+0000",
            "content": "We can improve the Javadocs by adding the examples. I just wanted to quickly write the patch to demonstrate how it could look like. We can also discuss about method names. The pattern follows convention used for all functional interfaces in Java 8 (method naming), but we can make it more readable. I am open to suggestions.\n\nIn Lucene trunk we can also remove all the separate implementations like LetterTokenizer and just allow them to be produced by factories. This would be a slight break, but we could still provide the Solr/CustomAnalyzer factories as usual. The Tokenizer for ICU in LUCENE-6874 could also be a one-liner just provided by the Solr factory, but no actual instance \n\nWe could also provide a one-for all Solr/CustomAnalyzer factory using a Enum of predicate/normalizer functions to be choosen by string parameter. "
        },
        {
            "id": "comment-14986861",
            "author": "Dawid Weiss",
            "date": "2015-11-03T07:53:01+0000",
            "content": "Pretty cool, Uwe!\n\nIt is fast as hell\n\nI always thought hell was about slow and endless suffering?  "
        },
        {
            "id": "comment-14986883",
            "author": "Uwe Schindler",
            "date": "2015-11-03T08:17:19+0000",
            "content": "I always thought hell was about slow and endless suffering?\n\n\u00c4hm, yes \n\nBut this video tells you different: https://www.youtube.com/watch?v=Uqa8MFSXZHM\nIf you need to burn fat, fast as hell: http://www.amazon.com/ULTIMATE-CUTS-SECRETS-English-Edition-ebook/dp/B00HMQS8TA "
        },
        {
            "id": "comment-14987491",
            "author": "David Smiley",
            "date": "2015-11-03T15:45:59+0000",
            "content": "+1 Nice Uwe. "
        },
        {
            "id": "comment-14990607",
            "author": "Uwe Schindler",
            "date": "2015-11-04T22:43:08+0000",
            "content": "New patch with improved Javadocs. Will commit this soon. "
        },
        {
            "id": "comment-14990610",
            "author": "ASF subversion and git services",
            "date": "2015-11-04T22:44:19+0000",
            "content": "Commit 1712682 from Uwe Schindler in branch 'dev/trunk'\n[ https://svn.apache.org/r1712682 ]\n\nLUCENE-6879: Allow to define custom CharTokenizer instances without subclassing using Java 8 lambdas or method references "
        },
        {
            "id": "comment-14990611",
            "author": "Uwe Schindler",
            "date": "2015-11-04T22:44:36+0000",
            "content": "Thanks for review! "
        },
        {
            "id": "comment-14990718",
            "author": "Uwe Schindler",
            "date": "2015-11-04T23:32:36+0000",
            "content": "Just FYI: I did some quick microbenchmark like this:\n\n\n// init & warmup\nString text = \"Tokenizer(Test)FooBar\";\nString[] result = new String[] { \"tokenizer\", \"test\", \"foobar\" };\nfinal Tokenizer tokenizer1 = CharTokenizer.fromTokenCharPredicate(Character::isLetter, Character::toLowerCase);\nfor (int i = 0; i < 10000; i++) {\n  tokenizer1.setReader(new StringReader(text));\n  assertTokenStreamContents(tokenizer1, result);\n}\nfinal Tokenizer tokenizer2 = new LowerCaseTokenizer();\nfor (int i = 0; i < 10000; i++) {\n  tokenizer2.setReader(new StringReader(text));\n  assertTokenStreamContents(tokenizer2, result);\n}\n\n// speed test\nlong [] lens1 = new long[100], lens2 = new long[100]; \nfor (int j = 0; j < 100; j++) {\n  System.out.println(\"Run: \" + j);\n  long start1 = System.currentTimeMillis();\n  for (int i = 0; i < 1000000; i++) {\n    tokenizer1.setReader(new StringReader(text));\n    assertTokenStreamContents(tokenizer1, result);\n  }\n  lens1[j] = System.currentTimeMillis() - start1;\n  \n  long start2 = System.currentTimeMillis();\n  for (int i = 0; i < 1000000; i++) {\n    tokenizer2.setReader(new StringReader(text));\n    assertTokenStreamContents(tokenizer2, result);\n  }\n  lens2[j] = System.currentTimeMillis() - start2;\n}\n\nSystem.out.println(\"Time Lambda: \" + Arrays.stream(lens1).summaryStatistics());\nSystem.out.println(\"Time Old: \" + Arrays.stream(lens2).summaryStatistics());\n\n\n\nI was not able to find any speed difference after warmup:\n\n\tTime Lambda: LongSummaryStatistics\n{count=100, sum=58267, min=562, average=582.670000, max=871}\n\tTime Old: LongSummaryStatistics\n{count=100, sum=61489, min=600, average=614.890000, max=721}\n\n\n "
        },
        {
            "id": "comment-14995137",
            "author": "ASF subversion and git services",
            "date": "2015-11-07T10:06:36+0000",
            "content": "Commit 1713098 from Uwe Schindler in branch 'dev/trunk'\n[ https://svn.apache.org/r1713098 ]\n\nLUCENE-6879: Add missing null checks for parameters "
        },
        {
            "id": "comment-14995140",
            "author": "ASF subversion and git services",
            "date": "2015-11-07T10:08:20+0000",
            "content": "Commit 1713099 from Uwe Schindler in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1713099 ]\n\nMerge additional null check from LUCENE-6879 "
        }
    ]
}