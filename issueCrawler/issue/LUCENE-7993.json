{
    "id": "LUCENE-7993",
    "title": "Speed up phrase queries when total hit count is not needed",
    "details": {
        "labels": "",
        "priority": "Minor",
        "resolution": "Fixed",
        "affect_versions": "None",
        "status": "Resolved",
        "type": "Improvement",
        "components": [],
        "fix_versions": [
            "master (8.0)"
        ]
    },
    "description": "Follow-up of LUCENE-4100: When thinking about the API that we needed to introduce to support MAXSCORE, I wondered whether the same API could support other optimizations. The idea is that when running phrase queries, before we start reading positions, we already have access to the term frequency of each term. And the frequency of the phrase is bounded by the minimum term frequency of the involved terms. So if the score for that minimum term frequency is not competitive then it means that the score for the phrase is not competitive either if we can assume that the score increases (or stagnates) when the term freq increases, which sounds like an ok requirement for a sane Similarity?",
    "attachments": {
        "LUCENE-7993.patch": "https://issues.apache.org/jira/secure/attachment/12892003/LUCENE-7993.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16203175",
            "date": "2017-10-13T08:01:41+0000",
            "content": "Here is a patch that applies on top of LUCENE-4100 to show the idea. Luceneutil confirms it brings interesting gains on wikimedium10m:\n\n\n                    TaskQPS baseline      StdDev   QPS patch      StdDev                Pct diff\n            OrHighNotLow       88.30      (4.4%)       72.67      (2.4%)  -17.7% ( -23% -  -11%)\n            OrHighNotMed       93.18      (3.3%)       86.58      (1.9%)   -7.1% ( -11% -   -1%)\n            OrNotHighLow     1386.80      (4.0%)     1289.38      (3.3%)   -7.0% ( -13% -    0%)\n           OrHighNotHigh       49.84      (3.2%)       47.59      (1.7%)   -4.5% (  -9% -    0%)\n                  Fuzzy2      196.79     (16.6%)      188.44      (7.7%)   -4.2% ( -24% -   24%)\n            HighSpanNear       58.01      (2.2%)       56.18      (2.4%)   -3.2% (  -7% -    1%)\n            OrNotHighMed      184.60      (1.7%)      178.77      (2.4%)   -3.2% (  -7% -    0%)\n              AndHighMed      224.60      (1.9%)      217.95      (2.3%)   -3.0% (  -7% -    1%)\n             LowSpanNear      143.79      (2.4%)      139.98      (2.4%)   -2.7% (  -7% -    2%)\n                  IntNRQ       19.47      (4.2%)       19.13      (5.0%)   -1.8% ( -10% -    7%)\n                 MedTerm      248.95      (2.3%)      244.80      (1.9%)   -1.7% (  -5% -    2%)\n                 LowTerm      766.37      (3.6%)      758.11      (3.9%)   -1.1% (  -8% -    6%)\n                HighTerm      131.14      (2.5%)      129.74      (2.6%)   -1.1% (  -5% -    4%)\n             AndHighHigh       30.70      (2.4%)       30.40      (1.5%)   -1.0% (  -4% -    3%)\n           OrNotHighHigh       55.99      (2.7%)       55.50      (1.7%)   -0.9% (  -5% -    3%)\n                 Prefix3      105.33      (4.8%)      104.60      (3.6%)   -0.7% (  -8% -    8%)\n             MedSpanNear       13.38      (2.3%)       13.30      (2.1%)   -0.6% (  -4% -    3%)\n                Wildcard       84.93      (4.8%)       84.59      (3.7%)   -0.4% (  -8% -    8%)\n              AndHighLow     1419.89      (3.3%)     1432.43      (2.8%)    0.9% (  -4% -    7%)\n         LowSloppyPhrase       38.50      (3.0%)       39.02      (1.7%)    1.3% (  -3% -    6%)\n        HighSloppyPhrase       15.85      (4.2%)       16.10      (2.4%)    1.6% (  -4% -    8%)\n         MedSloppyPhrase      118.20      (3.8%)      120.36      (2.4%)    1.8% (  -4% -    8%)\n                 Respell      272.44      (6.5%)      279.22      (3.5%)    2.5% (  -7% -   13%)\n       HighTermMonthSort      226.59      (9.1%)      233.94      (9.1%)    3.2% ( -13% -   23%)\n                  Fuzzy1      163.36     (10.6%)      171.95      (8.7%)    5.3% ( -12% -   27%)\n               LowPhrase      195.93      (2.2%)      222.77      (2.2%)   13.7% (   9% -   18%)\n              OrHighHigh       34.58      (6.4%)       45.87      (6.8%)   32.6% (  18% -   49%)\n   HighTermDayOfYearSort       65.42      (6.6%)       87.68     (12.5%)   34.0% (  14% -   56%)\n               MedPhrase       40.05      (2.0%)       59.16      (2.3%)   47.7% (  42% -   53%)\n               OrHighMed       41.35      (6.0%)       64.85      (7.3%)   56.8% (  41% -   74%)\n              HighPhrase       22.51      (3.8%)       39.33      (4.0%)   74.8% (  64% -   85%)\n               OrHighLow       61.15      (3.2%)      629.98     (41.3%)  930.3% ( 858% - 1007%)\n\n\n\nChanges to the performance of disjunctions are thanks to MAXSCORE, however we can see that LowPhrase (+13.7%), MedPhrase (+47.7%) and HighPhrase (+74.8%) have good speedups too. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16204639",
            "date": "2017-10-14T13:41:21+0000",
            "content": "\nif we can assume that the score increases (or stagnates) when the term freq increases, which sounds like an ok requirement for a sane Similarity?\n\nBut what about length normalization? ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16204641",
            "date": "2017-10-14T13:54:10+0000",
            "content": "ok, i get it, the optimization is ok since we actually call score for the doc with the theoretically maximum possible TF (taking its norm into account), just before reading positions.\n\nNote that this optimization is definitely unsafe for some broken similarities (specifically the ones documented to be broken in this way, such as DFR model P), and probably also for certain parameters to e.g. DFR NormalizationXXX. Additionally some similarities (such as AxiomaticXYZ) are not in our random test framework, so its unknown there. We could use some explicit tests rather than relying on the test suite in that way, too.\n\nBut the requirement is 100% reasonable, we can't let some fundamentally broken formulas get in our way here  I would go a step further and say that maybe these broken ones should move to the sandbox? ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16204642",
            "date": "2017-10-14T13:55:15+0000",
            "content": "feel free to open separate issue for the sim testing/cleanup, i can try to assist with that stuff. I have done battle with many of these before over this same stuff. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16208508",
            "date": "2017-10-17T22:21:51+0000",
            "content": "Thanks for looking! I opened LUCENE-7997. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16211856",
            "date": "2017-10-19T22:26:56+0000",
            "content": "Do we think the opto can be extended to sloppy phrase queries as well? E.G. assuming we had a similar requirement on sloppyFreq such that \"more slop only makes scores worse\" and so on? I feel like its possible. Just thinking thru how we should exactly state the requirements on the Similarity here. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16213772",
            "date": "2017-10-21T07:05:11+0000",
            "content": "Yes, I think it would be possible. I started with exact phrases which are easier to reason about but we should definitely think about sloppy phrases too. I think it just needs a bit more thinking given that a term can count twice in the frequency of sloppy freqs, eg. if you search for \"a b\"~3 and your document contains \"a b a\" (two matches in spite of a freq of 1 for b)? ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16213858",
            "date": "2017-10-21T11:43:54+0000",
            "content": "I see. I wonder if we could try a simple \"degraded\" form of the optimization at first, where we look at maximum tf value versus the minimum one. In other words, if freq(a)=4 and freq(b)=2, we'd test score(4) for sloppyPhrase instead of score(2) like we do for exactPhrase.\n\nI realize this is not very good and really makes the optimization significantly less potent, but perhaps still avoids reading a lot of positions, safe and easy as a start? ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16216607",
            "date": "2017-10-24T09:31:50+0000",
            "content": "where we look at maximum tf value versus the minimum one\n\nI think it is a bit more complicated than that? For instance \"a b a b\" has 3 matches for \"a b\"~2? ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16220743",
            "date": "2017-10-26T16:41:14+0000",
            "content": "Benchmarks on wikibig this time, which is more appropriate since artificially truncated documents defeat the purpose of this optimization. HighPrase is now 3x faster.\n\n\n                    TaskQPS baseline      StdDev   QPS patch      StdDev                Pct diff\n              OrHighHigh       97.15      (3.7%)       85.83      (3.6%)  -11.7% ( -18% -   -4%)\n                  Fuzzy2      142.85      (8.7%)      131.63     (11.0%)   -7.9% ( -25% -   12%)\n                  Fuzzy1      216.22      (9.6%)      200.10      (8.1%)   -7.5% ( -22% -   11%)\n         MedSloppyPhrase        8.02      (7.4%)        7.78     (10.1%)   -3.0% ( -19% -   15%)\n        HighSloppyPhrase       31.23      (5.7%)       30.59      (7.7%)   -2.0% ( -14% -   12%)\n             MedSpanNear      124.68      (4.7%)      122.26      (4.7%)   -1.9% ( -10% -    7%)\n             LowSpanNear       34.39      (8.2%)       33.90      (8.0%)   -1.4% ( -16% -   16%)\n         LowSloppyPhrase       27.55      (5.1%)       27.28      (6.8%)   -1.0% ( -12% -   11%)\n                  IntNRQ      164.57      (7.2%)      163.10      (8.5%)   -0.9% ( -15% -   16%)\n            HighSpanNear       48.43      (4.5%)       48.03      (4.2%)   -0.8% (  -9% -    8%)\n                 Respell      226.20      (3.1%)      225.11      (4.7%)   -0.5% (  -8% -    7%)\n              AndHighLow     1211.79      (3.9%)     1211.37      (3.1%)   -0.0% (  -6% -    7%)\n              AndHighMed      130.59      (2.0%)      130.71      (1.8%)    0.1% (  -3% -    3%)\n       HighTermMonthSort      307.88      (7.8%)      308.47      (8.4%)    0.2% ( -14% -   17%)\n                 MedTerm      361.52      (2.9%)      362.23      (2.8%)    0.2% (  -5% -    6%)\n             AndHighHigh      114.80      (1.9%)      115.38      (1.8%)    0.5% (  -3% -    4%)\n                 Prefix3      248.47      (5.0%)      249.86      (5.3%)    0.6% (  -9% -   11%)\n                HighTerm      201.95      (2.9%)      203.53      (2.9%)    0.8% (  -4% -    6%)\n                Wildcard      224.17      (4.4%)      226.12      (3.9%)    0.9% (  -7% -    9%)\n                 LowTerm     1862.62      (3.6%)     1903.87      (4.2%)    2.2% (  -5% -   10%)\n               OrHighMed      106.09      (4.6%)      145.10      (5.5%)   36.8% (  25% -   49%)\n               LowPhrase       81.86      (5.9%)      112.43      (3.5%)   37.4% (  26% -   49%)\n   HighTermDayOfYearSort      227.00      (7.3%)      312.89     (10.6%)   37.8% (  18% -   60%)\n               MedPhrase       17.95     (14.2%)       43.93     (15.1%)  144.7% ( 101% -  202%)\n              HighPhrase       29.28      (7.5%)       87.43      (8.6%)  198.6% ( 169% -  231%)\n               OrHighLow      110.21      (3.9%)      835.01     (34.0%)  657.6% ( 596% -  723%)\n\n ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16281853",
            "date": "2017-12-07T13:36:26+0000",
            "content": "Updated patch to include a test. This should be safe to commit now that we check that similarity scores increase with freq. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16281873",
            "date": "2017-12-07T13:57:38+0000",
            "content": "Nice work! ",
            "author": "David Smiley"
        },
        {
            "id": "comment-16306125",
            "date": "2017-12-29T09:06:58+0000",
            "content": "Commit c95dc6d95743f4a9a1ffe9baa04c3a9d1e3acdf9 in lucene-solr's branch refs/heads/master from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c95dc6d ]\n\nLUCENE-7993: Faster phrases if total hit counts are not required. ",
            "author": "ASF subversion and git services"
        }
    ]
}