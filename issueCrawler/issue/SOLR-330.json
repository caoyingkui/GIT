{
    "id": "SOLR-330",
    "title": "Use new Lucene Token APIs (reuse and char[] buff)",
    "details": {
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [],
        "components": [],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Lucene is getting new Token APIs for better performance.\n\n\ttoken reuse\n\tchar[] offset + len instead of String\nRequires a new version of lucene.",
    "attachments": {
        "SOLR-330.patch": "https://issues.apache.org/jira/secure/attachment/12374564/SOLR-330.patch",
        "token_filter.patch": "https://issues.apache.org/jira/secure/attachment/12379937/token_filter.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Grant Ingersoll",
            "id": "comment-12546762",
            "date": "2007-11-29T15:22:04+0000",
            "content": "Solr may also benefit from the new TeeTokenFilter and SinkTokenizer.  Intelligent use of these by the copy field mechanism MAY speed up things by significantly reducing the amount of redundant analysis work. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12564816",
            "date": "2008-02-01T16:18:20+0000",
            "content": "First draft of a patch that updates the various TokenFilters, etc. in Solr to use the new Lucene reuse API.  Notes on implementation below:\n\nAlso, cleans up some of the javadocs in various files\n\nAdded Test for the Porter stemmer.\n\nCleaned up some string literals to be constants so that they can be safely referred to in the tests.\n\nIn the PatternTokenFilter, it would be cool if there was a way to just operate on the char array, but I don't see that the Pattern/Matcher API supports it.\n\nSame goes for PhoneticTokenFilter\n\nI'm not sure yet if the BufferedTokenStream can take advantage of reuse, so I have left them alone for now, other than some minor doc fixes.  I will think about this some more.\n\nIn RemoveDuplicatesTF, I only converted to using termBuffer, not Token reuse.   I removed the \"IN\" and \"OUT\" loop labels, as I don't see what functionality they provide.\n\nAdded ArraysUtils class and test to provide a bit more functionality than Arrays.java offers in terms of comparing two char arrays.  This could be expanded at some point to cover other primitive comparisons.\n\nMy understanding of the new reusableTokenStream means we can't use it in the SolrAnalyzer\n\nOn the TrimFilter, it is not clear to me that there would be a token that is ever all whitespace.  However, since the test handles it, I wonder why the a Token of \"        \", when update offsets are on, reports the offsets as the end and not the start.  Just a minor nit, but it seems like the start/end offsets should be 0, not the end of the token.\n\nI'm not totally sure on the WordDelimiterFilter, as there is a fair amount of new token creation,  Also, I think, the newTok() method doesn't set the position increment based on the original position increment, so I added that.\n\n I'm also not completely sure how to handle FieldType DefaultAnalyzer.next().  It seems like it could reuse the token\n\nAlso not sure why the duplicate code for the MultiValueTokenStream in HighlighterUtils and SolrHighlighter, so I left the highlighter TokenStreams alone. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12566610",
            "date": "2008-02-07T14:20:11+0000",
            "content": "Note, this patch also includes SOLR-468 "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12570024",
            "date": "2008-02-18T22:26:43+0000",
            "content": "WordDelimiterFilter, as there is a fair amount of new token creation, Also, I think, the newTok() method doesn't set the position increment based on the original position increment, so I added that.\n\nCareful, that might introduce a bug... the position increments in WDF are generally set elsewhere, and setting it in newTok might cause more than one token to get that increment. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12583369",
            "date": "2008-03-29T21:35:15+0000",
            "content": "OK, I removed the WDF setPositionIncrement addition, otherwise patch is as before minus the Capitalization stuff that has already been committed.\n\nI plan to commit tomorrow or Monday. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12584197",
            "date": "2008-04-01T16:09:41+0000",
            "content": "Committed revision 643465. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12588040",
            "date": "2008-04-11T17:27:05+0000",
            "content": "Attaching token_filter.patch, minor update to synonym and WFD to prevent extra token creation. "
        }
    ]
}