{
    "id": "SOLR-12087",
    "title": "Deleting replicas sometimes fails and causes the replicas to exist in the down state",
    "details": {
        "labels": "",
        "priority": "Critical",
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "fix_versions": [
            "7.3.1"
        ],
        "affect_versions": "7.2",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Sometimes when deleting\u00a0replicas, the replica fails to be\u00a0removed from the cluster state. This occurs especially when\u00a0deleting replicas en mass; the resulting cause is that the data is deleted but the replicas aren't removed from the cluster state. Attempting to delete the downed replicas causes failures because the core does not exist anymore.\n\nThis also occurs when trying\u00a0to move replicas, since that move is an add and delete.\n\nSome more information regarding this issue; when the MOVEREPLICA command is issued, the new replica is created successfully but the replica to be deleted fails to be\u00a0removed from state.json (the core is deleted though) and we see two logs spammed.\n\n\tThe node containing the leader\u00a0replica\u00a0continually (read every second) attempts to initiate recovery on the replica and fails to do so because the core does not exist. As a result it continually publishes a\u00a0down state for the replica to zookeeper.\n\tThe deleted replica node spams that it cannot locate the core because it's been deleted.\n\n\n\nDuring this period of time, we see an increase in ZK network connectivity overall, until the\u00a0replica is finally deleted (spamming\u00a0DELETEREPLICA on the shard until its removed from the state)\n\nMy guess is there's two issues at hand here:\n\n\tThe\u00a0leader continually attempts to recover a downed replica that is unrecoverable because the core does not exist.\n\tThe replica to be deleted is having trouble being deleted from state.json in ZK.\n\n\n\nThis is mostly consistent for my use case. I'm running 7.2.1 with 66 nodes.",
    "attachments": {
        "SOLR-12087.test.patch": "https://issues.apache.org/jira/secure/attachment/12915513/SOLR-12087.test.patch",
        "Screen Shot 2018-03-16 at 11.50.32 AM.png": "https://issues.apache.org/jira/secure/attachment/12914933/Screen%20Shot%202018-03-16%20at%2011.50.32%20AM.png",
        "SOLR-12087.patch": "https://issues.apache.org/jira/secure/attachment/12915518/SOLR-12087.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2018-03-13T23:44:34+0000",
            "content": "Deletes are done via DELETE Command.\n\nSteps:\n\n1. Create a replica in new node.\n2. Delete the existing replica in previous node.\n\nand Deletes fail (sometimes) leaving the zombie replicas in a down state.\n\n\u00a0\n\n\u00a0 ",
            "author": "Sathiya N Sundararajan",
            "id": "comment-16397864"
        },
        {
            "date": "2018-03-14T03:47:25+0000",
            "content": "What's the error from the logs during the delete call?\u00a0 ",
            "author": "Varun Thacker",
            "id": "comment-16398058"
        },
        {
            "date": "2018-03-16T19:25:28+0000",
            "content": "\n// 2018-03-14 22:11:29.965 ERROR (qtp959447386-273280) [ ] o.a.s.h.RequestHandlerBase org.apache.solr.common.SolrException: Unable to locate core subreddits_shard2_replica_n189 at org.apache.solr.handler.admin.CoreAdminOperation.lambda$static$5(CoreAdminOperation.java:150) at org.apache.solr.handler.admin.CoreAdminOperation.execute(CoreAdminOperation.java:384) at org.apache.solr.handler.admin.CoreAdminHandler$CallInfo.call(CoreAdminHandler.java:389) at org.apache.solr.handler.admin.CoreAdminHandler.handleRequestBody(CoreAdminHandler.java:174) at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:177) at org.apache.solr.servlet.HttpSolrCall.handleAdmin(HttpSolrCall.java:735) at org.apache.solr.servlet.HttpSolrCall.handleAdminRequest(HttpSolrCall.java:716) at org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:497) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:382) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:326) at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1751) at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143) at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548) at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180) at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512) at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213) at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134) at org.eclipse.jetty.rewrite.handler.RewriteHandler.handle(RewriteHandler.java:335) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134) at org.eclipse.jetty.server.Server.handle(Server.java:534) at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320) at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251) at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93) at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303) at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148) at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671) at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589) at java.lang.Thread.run(Thread.java:748)\n\n\n\u00a0 ",
            "author": "Sathiya N Sundararajan",
            "id": "comment-16402376"
        },
        {
            "date": "2018-03-16T21:19:28+0000",
            "content": "Adding some more potentially relevant information:\n\nWe're constantly updating Solr\u00a0collections via live\u00a0streaming updates. I noticed that moving shards that don't have live indexing is much easier than those that do. Also heavy indexing seems to be a factor in whether or not zombie shards exist.\n\nEDIT: It seems that collections with indexing/querying consistently have zombie shards vs those that dont. ",
            "author": "Jerry Bao",
            "id": "comment-16402548"
        },
        {
            "date": "2018-03-21T04:24:22+0000",
            "content": "I'm not sure but this can be a race condition between delete replica and updates\n\n\tMany updates\u00a0are coming to the leader\n\tThe leader forward these updates to replicaA\n\tDeleteReplica is called for\u00a0replicaA\n\tThere are several updates sent to replicaA failed ( because replicaA is closed )\u00a0\n\tEntry of replicaA is removed from states.json\n\tThe leader put replicaA into LIR by publishing replicaA state to DOWN which adds back the entry of replicaA to states.json\n\n\n\nJerry Bao If this is your case there must be some log like this\n\n\tOn replica node\u00a0on time t :\u00a0log.info(logid+\" CLOSING SolrCore \" + this);\n\tOn leader node on time t+delta : log.warn(\"Leader is publishing core={} coreNodeName ={} state={} on behalf of un-reachable replica {}\",\nreplicaCoreName, replicaCoreNodeName, Replica.State.DOWN.toString(), replicaUrl);\n\n\n\nYou wrote that\nAttempting to delete the downed replicas causes failures because the core does not exist anymore.\nBut if the case that I described above is correct, you will still be able to delete the replica from clusterstate even when the replica does not exist in the node. Make sure that on the 2nd call of DeleteReplica (for removing zombie replica), parameters are correct because the name of the replica may get changed, ie: from core_node3 to core_node4.\n\nSathiya N Sundararajan\u00a0Your log seems relate to REQUEST_RECOVERY call, not DELETE call, right?\n\u00a0\n\u00a0\nBTW: In theory, SOLR-11702 will be able to solve this problem ( by not calling old LIR code ). Unfortunately that because of the backward compatibility we\u00a0still run through old LIR cause we cannot distinguish whether the replica is removed or not registered its term (had not upgraded to 7.3). ",
            "author": "Cao Manh Dat",
            "id": "comment-16407442"
        },
        {
            "date": "2018-03-21T15:58:07+0000",
            "content": "I uploaded a test patch which simulates the above case and the test failed 50% in my pc (both in master and branch_7_2) ",
            "author": "Cao Manh Dat",
            "id": "comment-16408143"
        },
        {
            "date": "2018-03-21T16:22:31+0000",
            "content": "Cao Manh Dat\u00a0That sounds exactly like the case I'm running into. I can't verify that the logs you say I should see I saw but I did definitely see the leader logs you were mentioning.\nYou wrote that\n\nAttempting to delete the downed replicas causes failures because the core does not exist anymore.\nSorry I should have been more clear here: It causes failures but not failures that block the\u00a0deletion of the replica; the replica does eventually get deleted.\u00a0\nMake sure that on the 2nd call of DeleteReplica (for removing zombie replica), parameters are correct because the name of the replica may get changed, ie: from core_node3 to core_node4.\nI wrote a small script to find all downed\u00a0replicas and\u00a0issue a delete command against them, which does take into account the name change. ",
            "author": "Jerry Bao",
            "id": "comment-16408177"
        },
        {
            "date": "2018-03-21T16:26:00+0000",
            "content": "Uploaded a draft patch for fixing the problem. The idea here is when the leader publish state of a replica as DOWN in old LIR, if the replica cannot be found in current states.json, do nothing.\u00a0 ",
            "author": "Cao Manh Dat",
            "id": "comment-16408182"
        },
        {
            "date": "2018-03-21T21:28:57+0000",
            "content": "Hi Dat,\n\nGreat catch!\n\n\u00a0\n\nA couple of minor comments about the patch:\n\n\tThe log.warn in\u00a0ReplicaMutator , should we just remove it? Like to a user going through the logs he will be confused when he sees it and there is no action to be taken anyways. Maybe it could be a DEBUG log entry?\n\tIn\u00a0DeleteReplicaTest\u00a0 can we change e.printStackTrace(); to be written out with the logger ?\n\tJust curious as to why is there a Thread.sleep(2000); wait in the test code?\u00a0\n\n\n\n\u00a0\n\n\u00a0\n\nFor reference, I ran the test patch on master ( which has the new LIR code ) and all 10 runs passed\n\nA few things worth noting were these log lines\n\n7471 INFO (qtp1388245618-49) [n:127.0.0.1:56141_solr ] o.a.s.s.HttpSolrCall [admin] webapp=null path=/admin/cores params={deleteInstanceDir=true&core=deleteReplicaOnIndexing_shard1_replica_n1&qt=/admin/cores&deleteDataDir=true&action=UNLOAD&wt=javabin&version=2&deleteIndex=true} status=0 QTime=85\n7559 INFO (qtp1216387063-400) [n:127.0.0.1:56142_solr c:deleteReplicaOnIndexing s:shard1 r:core_node4 x:deleteReplicaOnIndexing_shard1_replica_n2] o.a.s.c.ZkShardTerms Successful update of terms at /collections/deleteReplicaOnIndexing/terms/shard1 to Terms{values={core_node4=1}, version=3}\n7559 INFO (qtp1216387063-289) [n:127.0.0.1:56142_solr c:deleteReplicaOnIndexing s:shard1 r:core_node4 x:deleteReplicaOnIndexing_shard1_replica_n2] o.a.s.c.ZkShardTerms Failed to save terms, version is not a match, retrying\n7559 INFO (qtp1216387063-57) [n:127.0.0.1:56142_solr c:deleteReplicaOnIndexing s:shard1 r:core_node4 x:deleteReplicaOnIndexing_shard1_replica_n2] o.a.s.c.ZkShardTerms Failed to save terms, version is not a match, retrying\n7560 INFO (qtp1216387063-319) [n:127.0.0.1:56142_solr c:deleteReplicaOnIndexing s:shard1 r:core_node4 x:deleteReplicaOnIndexing_shard1_replica_n2] o.a.s.c.ZkShardTerms Failed to save terms, version is not a match, retrying\n7560 INFO (qtp1216387063-476) [n:127.0.0.1:56142_solr c:deleteReplicaOnIndexing s:shard1 r:core_node4 x:deleteReplicaOnIndexing_shard1_replica_n2] o.a.s.c.ZkShardTerms Failed to save terms, version is not a match, retrying\n7559 INFO (qtp1216387063-423) [n:127.0.0.1:56142_solr c:deleteReplicaOnIndexing s:shard1 r:core_node4 x:deleteReplicaOnIndexing_shard1_replica_n2] o.a.s.c.ZkShardTerms Failed to save terms, version is not a match, retrying\n7561 INFO (qtp1216387063-444) [n:127.0.0.1:56142_solr c:deleteReplicaOnIndexing s:shard1 r:core_node4 x:deleteReplicaOnIndexing_shard1_replica_n2] o.a.s.c.ZkShardTerms Failed to save terms, version is not a match, retrying\n7560 INFO (qtp1216387063-325) [n:127.0.0.1:56142_solr c:deleteReplicaOnIndexing s:shard1 r:core_node4 x:deleteReplicaOnIndexing_shard1_replica_n2] o.a.s.c.ZkShardTerms Failed to save terms, version is not a match, retrying\n7563 INFO (qtp1216387063-324) [n:127.0.0.1:56142_solr c:deleteReplicaOnIndexing s:shard1 r:core_node4 x:deleteReplicaOnIndexing_shard1_replica_n2] o.a.s.c.ZkShardTerms Failed to save terms, version is not a match, retrying\n7561 INFO (qtp1216387063-321) [n:127.0.0.1:56142_solr c:deleteReplicaOnIndexing s:shard1 r:core_node4 x:deleteReplicaOnIndexing_shard1_replica_n2] o.a.s.c.ZkShardTerms Failed to save terms, version is not a match, retrying\n\n....\n\n7705 ERROR (updateExecutor-16-thread-94-processing-https:////127.0.0.1:56141//solr//deleteReplicaOnIndexing_shard1_replica_n1 x:deleteReplicaOnIndexing_shard1_replica_n2 r:core_node4 n:127.0.0.1:56142_solr s:shard1 c:deleteReplicaOnIndexing) [n:127.0.0.1:56142_solr c:deleteReplicaOnIndexing s:shard1 r:core_node4 x:deleteReplicaOnIndexing_shard1_replica_n2] o.a.s.u.ErrorReportingConcurrentUpdateSolrClient error\norg.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at https://127.0.0.1:56141/solr/deleteReplicaOnIndexing_shard1_replica_n1: Can not find: /solr/deleteReplicaOnIndexing_shard1_replica_n1/update\n\n\n\nrequest: https://127.0.0.1:56141/solr/deleteReplicaOnIndexing_shard1_replica_n1/update?update.distrib=FROMLEADER&distrib.from=https%3A%2F%2F127.0.0.1%3A56142%2Fsolr%2FdeleteReplicaOnIndexing_shard1_replica_n2%2F&wt=javabin&version=2\n\n...\n\n7751 WARN\u00a0 (qtp1216387063-326) [n:127.0.0.1:56142_solr c:deleteReplicaOnIndexing s:shard1 r:core_node4 x:deleteReplicaOnIndexing_shard1_replica_n2] o.a.s.u.p.DistributedUpdateProcessor Core core_node4 belonging to deleteReplicaOnIndexing shard1, does not have error'd node https://127.0.0.1:56141/solr/deleteReplicaOnIndexing_shard1_replica_n1/ as a replica. No request recovery command will be sent!\n ",
            "author": "Varun Thacker",
            "id": "comment-16408606"
        },
        {
            "date": "2018-03-22T02:34:22+0000",
            "content": "Hi Varun, all your points are valid. Thanks!\n\nSeveral points from the log you are mentioned are worth to concern\n\n\tMultiple\u00a0log ZkShardTerms failed to save terms, version is not a match, retrying, this is a ZK performance problem, all thread will try to fetch the latest term from ZK -> SOLR-12135\n\tDistributedUpdateProcessor Core core_node4 belonging to deleteReplicaOnIndexing shard1, does not have error'd node -> SOLR-12073\n\n\n\n\u00a0 ",
            "author": "Cao Manh Dat",
            "id": "comment-16408948"
        },
        {
            "date": "2018-03-22T02:44:28+0000",
            "content": "Uploaded the patch that\u00a0\n\n\tFollow Varun's hints above.\n\tIf the replica does not exist, LIR process will be skipped.\n\tIn case of LIRThread already created a LIR znode\u00a0(lagging in update clusterstate of leader), remove that the node\u00a0on final.\n\n ",
            "author": "Cao Manh Dat",
            "id": "comment-16408962"
        },
        {
            "date": "2018-03-22T09:11:59+0000",
            "content": "Commit 92f1cdebfaff79867a6a65f9419fc0bba274c62b in lucene-solr's branch refs/heads/master from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=92f1cde ]\n\nSOLR-12087: Deleting replicas sometimes fails and causes the replicas to exist in the down state ",
            "author": "ASF subversion and git services",
            "id": "comment-16409259"
        },
        {
            "date": "2018-03-22T09:12:35+0000",
            "content": "Commit 137dc1df2e9d5b3c6053f0c98a7e1a7ea6206b00 in lucene-solr's branch refs/heads/branch_7x from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=137dc1d ]\n\nSOLR-12087: Deleting replicas sometimes fails and causes the replicas to exist in the down state ",
            "author": "ASF subversion and git services",
            "id": "comment-16409260"
        },
        {
            "date": "2018-03-22T23:50:27+0000",
            "content": "\n\n\n  -1 overall \n\n\n\n\n\n\n\n\n\n Vote \n Subsystem \n Runtime \n Comment \n\n\n -1 \n  patch  \n   0m  5s \n  SOLR-12087 does not apply to master. Rebase required? Wrong Branch? See https://wiki.apache.org/solr/HowToContribute#Creating_the_patch_file for help.  \n\n\n\n\n\n\n\n\n\n Subsystem \n Report/Notes \n\n\n JIRA Issue \n SOLR-12087 \n\n\n JIRA Patch URL \n https://issues.apache.org/jira/secure/attachment/12915636/SOLR-12087.patch \n\n\n Console output \n https://builds.apache.org/job/PreCommit-SOLR-Build/12/console \n\n\n Powered by \n Apache Yetus 0.7.0   http://yetus.apache.org \n\n\n\n\n\n\nThis message was automatically generated.\n ",
            "author": "Lucene/Solr QA",
            "id": "comment-16410578"
        },
        {
            "date": "2018-03-26T11:29:06+0000",
            "content": "Commit b0f677c3834992ac87d98bc3a50bef2a2b019634 in lucene-solr's branch refs/heads/master from Shalin Shekhar Mangar\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b0f677c ]\n\nSOLR-12087: Fixed bug in TimeOut.waitFor that can cause an infinite loop and added javadocs ",
            "author": "ASF subversion and git services",
            "id": "comment-16413707"
        },
        {
            "date": "2018-03-26T11:29:50+0000",
            "content": "Commit 983a443b2fb72dfe08d1a0919ed7ac31ceb8a89f in lucene-solr's branch refs/heads/branch_7x from Shalin Shekhar Mangar\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=983a443 ]\n\nSOLR-12087: Fixed bug in TimeOut.waitFor that can cause an infinite loop and added javadocs\n\n(cherry picked from commit b0f677c) ",
            "author": "ASF subversion and git services",
            "id": "comment-16413709"
        },
        {
            "date": "2018-03-29T07:16:48+0000",
            "content": "I see frequent failures on jenkins (and locally) for DeleteReplicaTest.deleteReplicaOnIndexing added in this issue.\n\nhttps://builds.apache.org/job/Lucene-Solr-Tests-7.x/533/\nhttps://jenkins.thetaphi.de/job/Lucene-Solr-master-Solaris/1762/\n\nThe first failure was on 2 days ago which is when I committed the fix to TimeOut.waitFor so they are probably related. Cao Manh Dat - can you please take a look? ",
            "author": "Shalin Shekhar Mangar",
            "id": "comment-16418534"
        },
        {
            "date": "2018-03-29T11:38:27+0000",
            "content": "Commit d7197b2565de4388d584065c8af5c622cb3a1c4f in lucene-solr's branch refs/heads/master from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d7197b2 ]\n\nSOLR-12087: Check for exception message of request recovery to remove LIR node ",
            "author": "ASF subversion and git services",
            "id": "comment-16418814"
        },
        {
            "date": "2018-03-29T11:39:24+0000",
            "content": "Commit 751adad5b16ba2cd343427556459427517a41aed in lucene-solr's branch refs/heads/branch_7x from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=751adad ]\n\nSOLR-12087: Check for exception message of request recovery to remove LIR node ",
            "author": "ASF subversion and git services",
            "id": "comment-16418816"
        },
        {
            "date": "2018-03-29T11:39:43+0000",
            "content": "Thank Shalin Shekhar Mangar , I committed the fix. ",
            "author": "Cao Manh Dat",
            "id": "comment-16418817"
        },
        {
            "date": "2018-03-29T17:42:37+0000",
            "content": "Can we get this fix backported to 7.3 and have a 7.3.1? ",
            "author": "Jerry Bao",
            "id": "comment-16419464"
        },
        {
            "date": "2018-04-19T08:00:43+0000",
            "content": "Commit ec9ccb5cd07f54eacba2cf071281cc9c37b766c1 in lucene-solr's branch refs/heads/branch_7_3 from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ec9ccb5 ]\n\nSOLR-12087: Deleting replicas sometimes fails and causes the replicas to exist in the down state ",
            "author": "ASF subversion and git services",
            "id": "comment-16443682"
        }
    ]
}