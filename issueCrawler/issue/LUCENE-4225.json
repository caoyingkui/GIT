{
    "id": "LUCENE-4225",
    "title": "New FixedPostingsFormat for less overhead than SepPostingsFormat",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "I've worked out the start at a new postings format that should have\nless overhead for fixed-int[] encoders (For,PFor)... using ideas from\nthe old bulk branch, and new ideas from Robert.\n\nIt's only a start: there's no payloads support yet, and I haven't run\nLucene's tests with it, except for one new test I added that tries to\nbe a thorough PostingsFormat tester (to make it easier to create new\npostings formats).  It does pass luceneutil's performance test, so\nit's at least able to run those queries correctly...\n\nLike Lucene40, it uses two files (though once we add payloads it may\nbe 3).  The .doc file interleaves doc delta and freq blocks, and .pos\nhas position delta blocks.  Unlike sep, blocks are NOT shared across\nterms; instead, it uses block encoding if there are enough ints to\nencode, else the same Lucene40 vInt format.  This means low-freq terms\n(< 128 = current default block size) are always vInts, and high-freq\nterms will have some number of blocks, with a vInt final block.\n\nSkip points are only recorded at block starts.",
    "attachments": {
        "LUCENE-4225-on-rev-1362013.patch": "https://issues.apache.org/jira/secure/attachment/12536804/LUCENE-4225-on-rev-1362013.patch",
        "LUCENE-4225.patch": "https://issues.apache.org/jira/secure/attachment/12536626/LUCENE-4225.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-07-16T13:06:31+0000",
            "content": "Initial patch, lots of nocommits ...\n\nPatch is against the LUCENE-3892 branch. ",
            "author": "Michael McCandless",
            "id": "comment-13415079"
        },
        {
            "date": "2012-07-16T13:09:05+0000",
            "content": "Initial results are compelling!  On the 10M doc Wikipedia test,\nSep(For) vs Fixed(For):\n\n\n                Task    QPS base StdDev base     QPS for  StdDev for      Pct diff\n              IntNRQ        8.40        0.83        8.33        0.38  -13% -   15%\n         TermGroup1M       46.67        1.51       48.95        0.21    1% -    8%\n        TermBGroup1M       79.97        1.96       85.05        0.52    3% -    9%\n             Prefix3       68.82        2.62       73.96        2.27    0% -   15%\n              Fuzzy2       69.54        2.69       75.55        2.29    1% -   16%\n      TermBGroup1M1P       42.67        1.07       46.38        0.86    4% -   13%\n              Fuzzy1       85.07        3.34       93.16        2.20    2% -   16%\n             Respell       67.30        2.20       74.69        3.87    1% -   20%\n                Term      156.81        8.62      180.38        6.83    4% -   26%\n            Wildcard       42.55        1.13       50.97        0.87   14% -   25%\n          OrHighHigh        8.66        0.77       10.46        0.59    4% -   40%\n           OrHighMed       15.62        1.54       18.93        1.05    4% -   41%\n          AndHighMed       45.80        1.69       57.18        0.80   18% -   31%\n            SpanNear        7.59        0.32        9.95        0.14   23% -   38%\n         AndHighHigh       11.09        0.32       14.68        0.15   27% -   37%\n            PKLookup      143.83        2.80      195.40        4.13   30% -   41%\n              Phrase       15.53        1.15       21.34        0.18   26% -   49%\n        SloppyPhrase        5.94        0.49        8.74        0.24   32% -   64%\n\n\n\nAnd Fixed(For) vs Lucene40:\n\n\n                Task    QPS base StdDev base     QPS for  StdDev for      Pct diff\n          AndHighMed       60.07        1.69       44.20        1.17  -30% -  -22%\n              Phrase       11.97        0.60        9.61        0.20  -25% -  -13%\n              IntNRQ        9.77        0.46        8.93        0.38  -16% -    0%\n              Fuzzy2       49.08        1.33       48.72        1.08   -5% -    4%\n             Respell       61.33        1.52       60.90        1.41   -5% -    4%\n            SpanNear        7.72        0.20        7.74        0.07   -3% -    3%\n            PKLookup      194.64        3.03      197.83        3.27   -1% -    4%\n        SloppyPhrase        4.76        0.19        4.93        0.11   -2% -   10%\n              Fuzzy1       63.49        1.07       66.57        1.53    0% -    9%\n         TermGroup1M       53.91        1.40       58.24        1.27    3% -   13%\n             Prefix3       61.02        1.72       66.14        2.11    2% -   15%\n            Wildcard       51.27        1.40       56.26        1.78    3% -   16%\n      TermBGroup1M1P       29.65        0.98       32.77        0.79    4% -   17%\n        TermBGroup1M       34.37        1.16       38.07        1.14    3% -   18%\n                Term       24.98        1.32       28.13        3.31   -5% -   32%\n         AndHighHigh       17.08        0.69       19.42        0.52    6% -   21%\n          OrHighHigh       10.68        0.40       12.52        0.94    4% -   30%\n           OrHighMed       13.66        0.52       16.65        1.34    7% -   36%\n\n\n\nSo we are still slower than Lucene40 in some cases, but a lot closer\nthan with Sep!\n\nBut these are early results ... and the PF doesn't pass tests yet ... so!\n ",
            "author": "Michael McCandless",
            "id": "comment-13415083"
        },
        {
            "date": "2012-07-16T13:54:58+0000",
            "content": "Looks good Mike. I think the slower cases are all explained: the skip interval is crazy, and lazy-loading the freq blocks should fix IntNRQ. (Though, i dont know how you get away with AndHighHigh currently).\n\nStill the second benchmark could be confusing: we are mixing concerns benchmarking FOR vs Vint and also different index layouts \nMaybe we can we benchmark this layout with bulkvint vs Lucene40 to get a better idea of just how the index layout is doing?\n\nI like how clean it is without the payloads crap: I still think we probably need to know up-front if the consumer is going to consume a payload off the enum for positional queries, without that its going to make things like this really hairy and messy.\n\nDo you think its worth it that even for \"big terms\" we write the last partial block as vints the way we do? \nSince these terms are going to be biggish anyway (at least enough to fill a block), this seems not worth the trouble?\n\nInstead if we only did this for low-freq terms, the code might even be clearer/faster, but I guess there would be a downside of\nnot being able to reuse these enums as much that would hurt e.g. NIOFSDirectory?\n\nThanks for bringing all this back to life... and the new test looks awesome! I think it will really make our lives a lot easier... ",
            "author": "Robert Muir",
            "id": "comment-13415126"
        },
        {
            "date": "2012-07-16T13:57:44+0000",
            "content": "By the way: I also really like how clean the code is. Lets see if we can keep that, its really nice!\n\nWe should seriously balance this against any little optimizations we can do. ",
            "author": "Robert Muir",
            "id": "comment-13415130"
        },
        {
            "date": "2012-07-16T14:06:59+0000",
            "content": "Some more ideas for payloads:\n\nI don't like how we double every position in the payloads case to record if there is one there, and we shouldnt also\nhave a condition to indicate if the length changed. I think practically its typically \"all or none\", e.g. the analysis\nprocess marks a payload like POS or it doesnt, and a fixed length across the whole term or not. So I don't think we \nshould waste time with this for block encoders, nor should we put this in skipdata. I think we should just do something\nsimpler, like if payloads are present, we have a block of lengths. Its a 0 if there is no payload. If all the payloads\nfor the entire term are the same, mark that length in the term dictionary and omit the lengths blocks.\n\nWe could consider the same approach for offset length. ",
            "author": "Robert Muir",
            "id": "comment-13415142"
        },
        {
            "date": "2012-07-17T00:17:08+0000",
            "content": "I think the slower cases are all explained: the skip interval is crazy, and lazy-loading the freq blocks should fix IntNRQ. (Though, i dont know how you get away with AndHighHigh currently).\n\nMaybe AndHighHigh isn't doing much actual skipping... ie the distance\nb/w each doc is probably around the blockSize?\n\nI wonder even how much skipping AndMedHigh queries are really\ndoing... but I agree we need to have a smaller skipInterval since our\n\"base\" skipInterval is so high.\n\nAnd try a smaller block size...\n\n\nStill the second benchmark could be confusing: we are mixing concerns benchmarking FOR vs Vint and also different index layouts \n Maybe we can we benchmark this layout with bulkvint vs Lucene40 to get a better idea of just how the index layout is doing?\n\nOh yeah!  OK I cutover BulkVInt to fixed postings format and compared\nit (base) to FOR:\n\n\n                Task    QPS base StdDev base     QPS for  StdDev for      Pct diff\n        SloppyPhrase        6.90        0.18        6.88        0.17   -5% -    4%\n            PKLookup      196.92        4.41      197.38        4.55   -4% -    4%\n             Respell       65.25        2.09       65.55        0.80   -3% -    5%\n         TermGroup1M       39.07        0.78       39.34        0.94   -3% -    5%\n            SpanNear        5.42        0.14        5.48        0.12   -3% -    6%\n        TermBGroup1M       44.91        0.44       45.45        0.51    0% -    3%\n      TermBGroup1M1P       40.42        0.68       40.95        0.76   -2% -    4%\n              Fuzzy2       63.85        1.14       65.01        0.66    0% -    4%\n              Phrase       10.23        0.27       10.46        0.33   -3% -    8%\n              Fuzzy1       61.89        1.06       63.60        0.61    0% -    5%\n              IntNRQ        8.77        0.23        9.02        0.36   -3% -    9%\n            Wildcard       29.22        0.40       30.18        0.84    0% -    7%\n         AndHighHigh        9.13        0.15        9.49        0.18    0% -    7%\n                Term      126.40        0.41      132.48        5.62    0% -    9%\n             Prefix3       30.54        0.69       32.21        1.06    0% -   11%\n          OrHighHigh        8.69        0.38        9.21        0.37   -2% -   15%\n           OrHighMed       28.00        1.15       29.67        1.05   -1% -   14%\n          AndHighMed       32.28        0.67       34.29        0.56    2% -   10%\n\n\n\nLooks like some small gain over BulkVInt but not much...\n\nI like how clean it is without the payloads crap: I still think we probably need to know up-front if the consumer is going to consume a payload off the enum for positional queries, without that its going to make things like this really hairy and messy.\n\nI agree!  Not looking forward to getting payloads working \n\n\nDo you think its worth it that even for \"big terms\" we write the last partial block as vints the way we do? \n Since these terms are going to be biggish anyway (at least enough to fill a block), this seems not worth the trouble?\n\nWe could try just leaving partial blocks at the end ... that made me\nnervous   I think there are a lot of terms in the 128 - 256 docFreq\nrange!  But we should try it.\n\n\nInstead if we only did this for low-freq terms, the code might even be clearer/faster, but I guess there would be a downside of\n not being able to reuse these enums as much that would hurt e.g. NIOFSDirectory?\n\nHmm true.  We'd need to pair up low and high freq enums?  (Like Pulsing).\n\nThanks for bringing all this back to life... and the new test looks awesome! I think it will really make our lives a lot easier...\n\nI really want this test to be thorough, so that if it passes on your\nnew PF, all other tests should too!  I know that's overly ambitious\n... but when it misses something we should go back and add it.\nBecause debugging a PF bug when you're in a deep scary stack trace\ninvolving Span*Query is a slow process ... it's too hard to make a new\nPF now.\n\n\nI don't like how we double every position in the payloads case to record if there is one there, and we shouldnt also\nhave a condition to indicate if the length changed. I think practically its typically \"all or none\", e.g. the analysis\nprocess marks a payload like POS or it doesnt, and a fixed length across the whole term or not. So I don't think we \nshould waste time with this for block encoders, nor should we put this in skipdata. I think we should just do something\nsimpler, like if payloads are present, we have a block of lengths. Its a 0 if there is no payload. If all the payloads\nfor the entire term are the same, mark that length in the term dictionary and omit the lengths blocks.\n\nWe could consider the same approach for offset length.\n\nThat sounds good! ",
            "author": "Michael McCandless",
            "id": "comment-13415810"
        },
        {
            "date": "2012-07-17T09:57:04+0000",
            "content": "\nLooks like some small gain over BulkVInt but not much...\n\nWell, thats good enough to see which one is faster. Now lets nuke the abstraction and just\nmake this FORPostingsFormat that uses PackedInts?\n\nI think its finally clear that its the abstractions here in the codec, not in the search API, that\nare slowing down bulk decompression  ",
            "author": "Robert Muir",
            "id": "comment-13416049"
        },
        {
            "date": "2012-07-17T11:50:45+0000",
            "content": "The initial patch doesn't pass compilation  on branch codes, after svn up -r 1362013.\n\nSo I made some change to pass ant compile, however, still some test fails: http://pastebin.com/jdFecZm5 ",
            "author": "Han Jiang",
            "id": "comment-13416113"
        },
        {
            "date": "2012-07-17T12:28:38+0000",
            "content": "Oh...OK, those fails are all related to payload.  ",
            "author": "Han Jiang",
            "id": "comment-13416136"
        },
        {
            "date": "2012-07-17T15:45:07+0000",
            "content": "Woops, thanks Billy, I'll merge w/ my patch.\n\nSorry, test failures are expected until I get payloads working ... I'll do that next but will take some time.  Payloads always get tricky  ",
            "author": "Michael McCandless",
            "id": "comment-13416308"
        },
        {
            "date": "2012-07-17T15:49:38+0000",
            "content": "\nWell, thats good enough to see which one is faster. Now lets nuke the abstraction and just\nmake this FORPostingsFormat that uses PackedInts?\n\nOK I'll do that! ",
            "author": "Michael McCandless",
            "id": "comment-13416314"
        },
        {
            "date": "2012-07-17T21:22:17+0000",
            "content": "New patch, moving fixed -> block and hardwiring it to For int block encoding.\n\nStill need to do payloads... ",
            "author": "Michael McCandless",
            "id": "comment-13416617"
        },
        {
            "date": "2012-07-17T23:15:23+0000",
            "content": "New patch, adding Block PF to META-INF services, and fixing a bug in skipping. ",
            "author": "Michael McCandless",
            "id": "comment-13416702"
        },
        {
            "date": "2012-07-18T17:14:58+0000",
            "content": "Quite curious why index size is reduced in Block PF. Here is a comparison base on the 1M wikipedia data: \n\n                  SepPF+For  BlockPF     \nskip_data_size    36M        n/a\ntotal_index_size  598M       540M\n\n\n\nSince in BlockPF, skip data is inlined into .doc files, it is interesting that considering this part of size, BlockPF will still get better compression rate.\n\nAlso, as BlockPF uses different formats to store information for each term, we try to see how the data is actually stored. Here, we sum docFreq%128 for all terms to get the vInt encoded ints, and remaining ints will all be encoded as Block Format.\n\n\nBlock encoded 88,326,528 ints \nVInt encoded  39,929,349 ints\n\n ",
            "author": "Han Jiang",
            "id": "comment-13417262"
        },
        {
            "date": "2012-07-19T10:14:07+0000",
            "content": "New patch w/ payloads & offsets working ... I think it's ready! ",
            "author": "Michael McCandless",
            "id": "comment-13418198"
        },
        {
            "date": "2012-07-19T12:47:52+0000",
            "content": "Those are interesting numbers: I'm surprised so many postings end up block encoded.\n\nBlock PF has far far less skip data (skipInterval=128 vs 16 for Sep), and since it only skips to doc/freq block starts it saves two bytes per skip point. ",
            "author": "Michael McCandless",
            "id": "comment-13418257"
        },
        {
            "date": "2012-07-19T16:54:28+0000",
            "content": "OK I committed this ... it has lots of nocommits still but we can iterate on the branch. ",
            "author": "Michael McCandless",
            "id": "comment-13418438"
        },
        {
            "date": "2012-08-02T13:22:20+0000",
            "content": "Just hit an error on BlockPostingsFormat, this should reproduce in latest branch\n\n\nant test-core -Dtestcase=TestGraphTokenizers -Dtests.method=testDoubleMockGraphTokenFilterRandom -Dtests.seed=1FD78436D5E26B9A -Dtests.postingsformat=Block\n\n ",
            "author": "Han Jiang",
            "id": "comment-13427304"
        },
        {
            "date": "2012-08-02T16:15:37+0000",
            "content": "Thanks Billy, I'll dig... ",
            "author": "Michael McCandless",
            "id": "comment-13427413"
        },
        {
            "date": "2012-08-02T20:00:04+0000",
            "content": "OK I committed the fix: Block/PackedPF was incorrectly encoding offsets as startOffset - lastEndOffset.  It must instead be startOffset - lastStartOffset because it is possible (though rare) for startOffset - lastEndOffset to be negative.\n\nI also separately committed a fix for NPEs that tests were hitting when the index didn't index payloads nor offsets.  Tests should now pass for BlockPF and BlockPackedPF... ",
            "author": "Michael McCandless",
            "id": "comment-13427562"
        },
        {
            "date": "2012-08-03T02:34:20+0000",
            "content": "OK, thanks Mike! ",
            "author": "Han Jiang",
            "id": "comment-13427811"
        }
    ]
}