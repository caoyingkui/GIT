{
    "id": "SOLR-5593",
    "title": "shard leader loss due to ZK session expiry",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.7",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "The problem we saw was that the shard leader ceased to be shard leader (in our case due to its zookeeper session expiring). The followers thus rejected update requests (DistributedUpdateProcessor setupRequest's call to ZkStateReader getLeaderRetry) and the leader asked them to recover (DistributedUpdateProcessor doFinish). The followers published themselves as recovering (CoreAdminHandler handleRequestRecoveryAction) and the shard leader loss triggered an election in which none of the followers became the leader due to their recovering state (ShardLeaderElectionContext shouldIBeLeader). The former shard leader also did not become shard leader because its new seq number placed it after the existing replicas (LeaderElector checkIfIamLeader seq <= intSeqs.get(0)).",
    "attachments": {
        "CoreAdminHandler.patch": "https://issues.apache.org/jira/secure/attachment/12620943/CoreAdminHandler.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Christine Poerschke",
            "id": "comment-13859546",
            "date": "2013-12-31T16:08:26+0000",
            "content": "Attaching one potential solution (we are investigating others):\n\nAs part of the recovery process state=recovering publishing already happens (RecoveryStrategy doRecovery) but only after a shard leader to recover from has been found. If the CoreAdminHandler handleRequestRecoveryAction publish had not happened then one of the followers should have been elected shard leader. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13859557",
            "date": "2013-12-31T16:44:30+0000",
            "content": "Great find!\n\nThe former shard leader also did not become shard leader because its new seq number placed it after the existing replicas (LeaderElector checkIfIamLeader seq <= intSeqs.get(0)).\n\nThis I'm surprised to see - when someone cannot be the leader, for instance if they published a non active state last, they should get back in line - the original leader should have his chance again... "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13859562",
            "date": "2013-12-31T16:48:38+0000",
            "content": "I think your change is probably good in general. Let's see what else we can do here.\n\nOne thing that seems kind of silly is that those replicas reject the updates at all. It seems like perhaps we should relax things a bit so that they would be accepted. "
        },
        {
            "author": "Christine Poerschke",
            "id": "comment-13859600",
            "date": "2013-12-31T18:14:57+0000",
            "content": "One thing that seems kind of silly is that those replicas reject the updates at all. It seems like perhaps we should relax things a bit so that they would be accepted.\n\nYes, we are working on changes to DistributedUpdateProcessor to relax the requirement for the getLeaderRetry to succeed within setupRequest (if phase is DistribPhase.FROMLEADER and the shard state shows it could not be subShardLeader then getLeaderRetry success should be optional). "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13859652",
            "date": "2013-12-31T20:05:14+0000",
            "content": "I didn't mean to reassign - I was typing in another window, accidentally hit the mouse button with my elbow, which focused the browser window with this issue up, and then I guess JIRA interpreted my random typing as keyboard shortcuts .... "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13859683",
            "date": "2013-12-31T20:44:00+0000",
            "content": "Yes, we are working on changes to DistributedUpdateProcessor to relax the requirement for the getLeaderRetry to succeed within setupRequest (if phase is DistribPhase.FROMLEADER and the shard state shows it could not be subShardLeader then getLeaderRetry success should be optional).\n\nYeah, on some thought, this is the right approach I think. Removing the publish is actually probably not a good idea. It actually protects us from losing data - we don't want a replica that was asked to recover to become the leader - that could mean updates were accepted that it is expected to have. If the previous leader died before one of the replicas became a leader, that leader might have been ahead. In this case, we don't choose a new leader, because you should really reboot the whole shard with all the replicas you can to avoid any possible data lose. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13859684",
            "date": "2013-12-31T20:44:58+0000",
            "content": "bq within setupRequest \n\nThe tough case seems to be nailing delete by query - I've been peaking a bit at it. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13888944",
            "date": "2014-02-02T14:59:20+0000",
            "content": "Yes, we are working on changes to DistributedUpdateProcessor to relax\n\nAny progress? I'll likely look at attacking this soon. "
        },
        {
            "author": "Christine Poerschke",
            "id": "comment-13889416",
            "date": "2014-02-03T11:49:16+0000",
            "content": "Uploaded https://github.com/apache/lucene-solr/pull/27 which rather than relaxing the error handling for the getLeaderRetry call actually tries to completely avoid it in the first place (if circumstances seem to permit it i.e. the request said it came from the leader and we don't think we are leader and we could not be sub-shard leader). "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13892720",
            "date": "2014-02-05T22:57:48+0000",
            "content": "Great, thanks Christine! Patch looks good on first glance. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13892937",
            "date": "2014-02-06T02:38:01+0000",
            "content": "Commit 1565049 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1565049 ]\n\nSOLR-5593: Replicas should accept the last updates from a leader that has just  lost it's connection to ZooKeeper. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13892941",
            "date": "2014-02-06T02:41:57+0000",
            "content": "Commit 1565053 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1565053 ]\n\nSOLR-5593: Replicas should accept the last updates from a leader that has just  lost it's connection to ZooKeeper. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13892942",
            "date": "2014-02-06T02:42:31+0000",
            "content": "Thanks Christine! "
        },
        {
            "author": "ASF GitHub Bot",
            "id": "comment-15094561",
            "date": "2016-01-12T19:13:46+0000",
            "content": "Github user cpoerschke closed the pull request at:\n\n    https://github.com/apache/lucene-solr/pull/27 "
        }
    ]
}