{
    "id": "LUCENE-2766",
    "title": "ParallelReader should support getSequentialSubReaders if possible",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "Applications that need to use ParallelReader can't currently use per-segment optimizations because getSequentialSubReaders returns null.\n\nConsidering the strict requirements on input indexes that ParallelReader already enforces it's usually the case that the additional indexes are built with the knowledge of the primary index, in order to keep the docId-s synchronized. If that's the case then it's conceivable that these indexes could be created with the same number of segments, which in turn would mean that their docId-s are synchronized on a per-segment level. ParallelReader should detect such cases, and in getSequentialSubReader it should return an array of ParallelReader-s created from corresponding segments of input indexes.",
    "attachments": {
        "LUCENE-2766.patch": "https://issues.apache.org/jira/secure/attachment/12467435/LUCENE-2766.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-11-16T03:48:32+0000",
            "content": "If you forgot about detection to start, and put it on the user to declare they will keep segments in sync, then its pretty simple isn't it? Something like:\n\n\n  public IndexReader[] getSequentialSubReaders() {\n    if (!synchedSubReaders) {\n      return null;\n    } else {\n      int numReaders = readers.size();\n      IndexReader firstReader = readers.get(0);\n      IndexReader[] firstReaderSubReaders = firstReader\n          .getSequentialSubReaders();\n      IndexReader[] seqSubReaders;\n      if (firstReaderSubReaders != null) {\n        int segCnt = firstReaderSubReaders.length;\n        seqSubReaders = new IndexReader[segCnt];\n        try {\n          for (int j = 0; j < segCnt; j++) {\n            ParallelReader pr = new ParallelReader();\n            seqSubReaders[j] = pr;\n            for (int i = 0; i < numReaders; i++) {\n              IndexReader reader = readers.get(i);\n              IndexReader[] subs = reader.getSequentialSubReaders();\n              if (subs == null) {\n                return null;\n              }\n              pr.add(subs[j]);\n            }\n          }\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n        return seqSubReaders;\n      }\n      return null;\n    }\n  }\n\n ",
            "author": "Mark Miller",
            "id": "comment-12932326"
        },
        {
            "date": "2010-11-16T05:00:24+0000",
            "content": "And if you don't necessarily need to descend into a deep/non standard reader graph - but one step at a time. ",
            "author": "Mark Miller",
            "id": "comment-12932336"
        },
        {
            "date": "2010-11-16T14:06:23+0000",
            "content": "Also, the process of creating secondary indexes needs to use the same merge policy, so that it arrives at segments with exactly the same count and same sequence of docIds... ",
            "author": "Andrzej Bialecki",
            "id": "comment-12932465"
        },
        {
            "date": "2010-11-16T14:11:01+0000",
            "content": "Same merge policy would normally end up giving different results since the data is different.\nIf you have the primary index, and are building an aux index, what you want is a policy that won't merge at all, but that you can manually flush at the end of every segment in the primary index. ",
            "author": "Yonik Seeley",
            "id": "comment-12932469"
        },
        {
            "date": "2010-11-16T14:11:32+0000",
            "content": "Thats the other side of the coin though (the harder part it would seem). Doesn't seem too difficult to add support to ParallelReader for getSequentialSubReaders for the right cases - the hard part is keeping synched up segments in your indexes. But this issue seemed to assume that part separately. ",
            "author": "Mark Miller",
            "id": "comment-12932471"
        },
        {
            "date": "2011-01-04T16:45:18+0000",
            "content": "Patch and unit test that implements getSequentialSubReaders. The other part (a suitable MergePolicy) is left as an exercise for the reader for now  ",
            "author": "Andrzej Bialecki",
            "id": "comment-12977338"
        },
        {
            "date": "2011-01-04T17:56:31+0000",
            "content": "I'm gonna hold off on LUCENE-2771 until we figure this one out... because it would make your getSequentialSubReaders in the synced=true case quite heavy (without modifications).\n\nThis is because in that issue the norms caching is removed from the non-atomic readers\n(Dir/MultiReader) and pushed onto SlowMultiReaderWrapper/ParallelReader.\n\nSo one idea is to fix parallelreader to not 'sometimes' return getSequentialSubReaders,\nbut instead have two supported approaches, one that supports the 'synced' case properly with\nper-segment search (and a suitable mergepolicy to go with it), another (deprecated) one\nto support the synced=false case?\n\nOr reworded: is there any reason we shouldn't actually only support this way going forwards\nin the future? ",
            "author": "Robert Muir",
            "id": "comment-12977376"
        },
        {
            "date": "2011-01-04T18:03:22+0000",
            "content": "Or reworded: is there any reason we shouldn't actually only support this way going forwards in the future?\n\n+1 to requiring that PR only handle the sync'd case.\n\nIn fact... I think PR should only support atomic readers, and we can have sugar (static method) somewhere that can take N sync'd composite readers, get their subs, assert that they are \"sync'd\", and make a MultiReader of all the ParallelReaders against the aligned subs (basically the same as the patch here).\n\nIe, once we only support the sync'd case, I don't see why PR should also be an MR.  We should just re-use MR for that and not duplicate code? ",
            "author": "Michael McCandless",
            "id": "comment-12977379"
        },
        {
            "date": "2011-01-04T22:58:54+0000",
            "content": "I'm gonna hold off on LUCENE-2771 until we figure this one out... because it would make your getSequentialSubReaders in the synced=true case quite heavy (without modifications).\n\nSorry, I was wrong on this... I totally forgot the norms cache is lazy-loaded always in that patch. I'll commit LUCENE-2771 it shouldnt affect this! ",
            "author": "Robert Muir",
            "id": "comment-12977519"
        }
    ]
}