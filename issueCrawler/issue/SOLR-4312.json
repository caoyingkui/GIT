{
    "id": "SOLR-4312",
    "title": "SolrCloud upgrade path",
    "details": {
        "affect_versions": "4.0,                                            4.1",
        "status": "Open",
        "fix_versions": [],
        "components": [
            "SolrCloud"
        ],
        "type": "Task",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "Upgrading from one SolrCloud version to another needs to be figured out and documented.  \n\nMark Miller wrote on the 4.1 VOTE email on dev@l.a.o:\n\n\nOne issue that is probably still a problem is that you can't easily upgrade form a 4.0 to 4.1 SolrCloud setup in some cases - at least to my knowledge. I don't know all the details, but at a minimum, we should probably add an entry to changes about what you should do. It may require blowing away your own clusterstate.json and re doing your numShards settings, or starting over, or\u2026I don't really know. I don't think anyone has tested.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-13556485",
            "date": "2013-01-17T19:11:10+0000",
            "content": "I'm going to try and do a little testing on this today. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13556765",
            "date": "2013-01-17T23:45:21+0000",
            "content": "I tried from 4.0 to 4.1 with mixed results.\n\n\nI0=/opt/code/lusolr40\nI1=/opt/code/lusolr\n\ncd $I0/solr\nant clean example\nrm -rf example/solr/zoo_data\nrm -rf example/solr/collection1/data\ncp -rp example e1\ncp -rp example e2\n\ncd e1\njava -Dbootstrap_confdir=./solr/collection1/conf -Dcollection.configName=myconf -DzkRun -DnumShards=2 -jar start.jar\n\ncd e2\njava -Djetty.port=7574 -DzkHost=localhost:9983 -jar start.jar\n\ncd example/exampledocs\n./post.sh *xml\n\n#kill e1\n#kill e2\n\ncd  $I1/solr\nant clean example\nrm -rf example/solr/zoo_data\nrm -rf example/solr/collection1/data\ncp -rp example e1\ncp -rp example e2\n\n# copy complete solr home\nrm -rf e1/solr\nrm -rf e2/solr\ncp -rp $I0/solr/e1/solr ./e1/solr/\ncp -rp $I0/solr/e2/solr ./e2/solr/\n\ncd e1\njava -DzkRun -jar start.jar\n\ncd e2\njava -Djetty.port=7574 -DzkHost=localhost:9983 -jar start.jar\n\n\n\nSo on bringing up the 4.1 servers with the 4.0 solr homes, I was greeted with:\n\n\nJan 17, 2013 5:45:31 PM org.apache.solr.cloud.ShardLeaderElectionContext waitForReplicasToComeUp\nINFO: Waiting until we see more replicas up: total=2 found=1 timeoutin=119655\n[...]\nINFO: Waiting until we see more replicas up: total=2 found=1 timeoutin=19090\nJan 17, 2013 5:47:31 PM org.apache.solr.cloud.ShardLeaderElectionContext waitForReplicasToComeUp\nINFO: Was waiting for replicas to come up, but they are taking too long - assuming they won't come back till later\nJan 17, 2013 5:47:31 PM org.apache.solr.cloud.ShardLeaderElectionContext runLeaderProcess\nINFO: I may be the new leader - try and sync\n\n\n\nEssentially, nothing would respond for 2 minutes because they were waiting for the old replicas to come up.  They did not recognize themselves as the previous replicas I think because the node naming changed (at least for OS-X).  See clusterstate.json below:\n\n\n{\"collection1\":{\"shards\":{\n      \"shard1\":{\n        \"range\":\"80000000-ffffffff\",\n        \"replicas\":{\n          \"Rogue:8983_solr_collection1\":{\n            \"shard\":\"shard1\",\n            \"roles\":null,\n            \"state\":\"active\",\n            \"core\":\"collection1\",\n            \"collection\":\"collection1\",\n            \"node_name\":\"Rogue:8983_solr\",\n            \"base_url\":\"http://Rogue:8983/solr\"},\n          \"192.168.1.109:8983_solr_collection1\":{\n            \"shard\":\"shard1\",\n            \"roles\":null,\n            \"state\":\"active\",\n            \"core\":\"collection1\",\n            \"collection\":\"collection1\",\n            \"node_name\":\"192.168.1.109:8983_solr\",\n            \"base_url\":\"http://192.168.1.109:8983/solr\",\n            \"leader\":\"true\"}}},\n      \"shard2\":{\n        \"range\":\"0-7fffffff\",\n        \"replicas\":{\n          \"Rogue:7574_solr_collection1\":{\n            \"shard\":\"shard2\",\n            \"roles\":null,\n            \"state\":\"active\",\n            \"core\":\"collection1\",\n            \"collection\":\"collection1\",\n            \"node_name\":\"Rogue:7574_solr\",\n            \"base_url\":\"http://Rogue:7574/solr\"},\n          \"192.168.1.109:7574_solr_collection1\":{\n            \"shard\":\"shard2\",\n            \"roles\":null,\n            \"state\":\"active\",\n            \"core\":\"collection1\",\n            \"collection\":\"collection1\",\n            \"node_name\":\"192.168.1.109:7574_solr\",\n            \"base_url\":\"http://192.168.1.109:7574/solr\",\n            \"leader\":\"true\"}}}}}}\n\n\n\nNote that the clusterstate.json format was updated to add the extra level for collection properties.  There is no router defined, so the default is the back compatible \"plain\".  I verified it worked by doing another post.sh *xml and verified all the docs went to the correct shards.\n\nThe bad part: I think it's probably luck that the indexes ended up assigned to the right shards, probably based on the order that I brought them up.  They didn't have any other context to make the decision since we don't store that info locally yet, and they couldn't match up their info in zookeeper due to the node naming change. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13557348",
            "date": "2013-01-18T17:02:40+0000",
            "content": "Looking into this a little more, I guess the node naming change was needed for SOLR-4088 "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13557361",
            "date": "2013-01-18T17:15:52+0000",
            "content": "I think that points out a change we should probably make - we should not re guess the address on every startup - we should guess it once and keep using that unless someone then overrides it? Or sets a flag to force a re guess?  "
        }
    ]
}