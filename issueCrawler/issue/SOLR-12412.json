{
    "id": "SOLR-12412",
    "title": "Leader should give up leadership when IndexWriter.tragedy occur",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [
            "7.5",
            "master (8.0)"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "When a leader meets some kind of unrecoverable exception (ie: CorruptedIndexException). The shard will go into the readable state and human has to intervene. \n\nIn that case, if there are another active replica in the same shard, the leader should give up its leadership.",
    "attachments": {
        "SOLR-12412.patch": "https://issues.apache.org/jira/secure/attachment/12930485/SOLR-12412.patch",
        "jenkins-failure-2325.log": "https://issues.apache.org/jira/secure/attachment/12931849/jenkins-failure-2325.log"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2018-05-29T07:24:06+0000",
            "content": "Does any come up with other critical exceptions beside CorruptedIndexException? ",
            "author": "Cao Manh Dat",
            "id": "comment-16493174"
        },
        {
            "date": "2018-06-04T23:22:47+0000",
            "content": "Just a thought here, but what about anything inside IndexWriter.tragedy? ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16501019"
        },
        {
            "date": "2018-07-06T03:57:02+0000",
            "content": "Thanks Tom\u00e1s Fern\u00e1ndez L\u00f6bbe, that is a good idea.\n\nI attached a patch for this ticket. Whenever a request gets failed we will check the IndexWriter.tragedy if there is another active replica in the same shard and the current replica is the leader (no need for handle the case when the replica is not leader since the leader will bring the replica into recovery state). It will enqueue a DELETE and ADD operation to the Overseer. ",
            "author": "Cao Manh Dat",
            "id": "comment-16534414"
        },
        {
            "date": "2018-07-09T01:24:34+0000",
            "content": "Final patch with another test and precommit fix. I will commit it soon. ",
            "author": "Cao Manh Dat",
            "id": "comment-16536476"
        },
        {
            "date": "2018-07-09T02:14:39+0000",
            "content": "Commit 119717611094c755b271db6e7a8614fe9406bb5e in lucene-solr's branch refs/heads/master from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1197176 ]\n\nSOLR-12412: Leader should give up leadership when IndexWriter.tragedy occur ",
            "author": "ASF subversion and git services",
            "id": "comment-16536490"
        },
        {
            "date": "2018-07-09T02:15:22+0000",
            "content": "Commit fddf35cfebd3f612a5e5089e76aa02b105209e6d in lucene-solr's branch refs/heads/branch_7x from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=fddf35c ]\n\nSOLR-12412: Leader should give up leadership when IndexWriter.tragedy occur ",
            "author": "ASF subversion and git services",
            "id": "comment-16536491"
        },
        {
            "date": "2018-07-09T17:20:52+0000",
            "content": "Thanks for working on this Cao Manh Dat! I'm wondering if there can be a way to give up leadership that's more light weight than adding/removing replicas while still being safe. Maybe something that ends up doing a core reload? ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16537291"
        },
        {
            "date": "2018-07-09T19:54:12+0000",
            "content": "Policeman Jenkins found a reproducing seed https://jenkins.thetaphi.de/job/Lucene-Solr-7.x-MacOSX/734/ for test failures that git bisect blames on commit fddf35c on this issue:\n\n\nChecking out Revision 80eb5da7393dd25c8cb566194eb9158de212bfb2 (refs/remotes/origin/branch_7x)\n[...]\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestPullReplica -Dtests.method=testKillLeader -Dtests.seed=89003455250E12D2 -Dtests.slow=true -Dtests.locale=lg -Dtests.timezone=America/Rainy_River -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] FAILURE 60.4s J1 | TestPullReplica.testKillLeader <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: Replica core_node4 not up to date after 10 seconds expected:<1> but was:<0>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([89003455250E12D2:C016C0E147B58684]:0)\n   [junit4]    > \tat org.apache.solr.cloud.TestPullReplica.waitForNumDocsInAllReplicas(TestPullReplica.java:542)\n   [junit4]    > \tat org.apache.solr.cloud.TestPullReplica.doTestNoLeader(TestPullReplica.java:490)\n   [junit4]    > \tat org.apache.solr.cloud.TestPullReplica.testKillLeader(TestPullReplica.java:309)\n   [junit4]    > \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n   [junit4]    > \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n   [junit4]    > \tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n   [junit4]    > \tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n   [junit4]    > \tat java.base/java.lang.Thread.run(Thread.java:844)\n[...]\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestPullReplica -Dtests.method=testRemoveAllWriterReplicas -Dtests.seed=89003455250E12D2 -Dtests.slow=true -Dtests.locale=lg -Dtests.timezone=America/Rainy_River -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] FAILURE 24.6s J1 | TestPullReplica.testRemoveAllWriterReplicas <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: Replica core_node4 not up to date after 10 seconds expected:<1> but was:<0>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([89003455250E12D2:1A0EA86E31F0FB7B]:0)\n   [junit4]    > \tat org.apache.solr.cloud.TestPullReplica.waitForNumDocsInAllReplicas(TestPullReplica.java:542)\n   [junit4]    > \tat org.apache.solr.cloud.TestPullReplica.doTestNoLeader(TestPullReplica.java:490)\n   [junit4]    > \tat org.apache.solr.cloud.TestPullReplica.testRemoveAllWriterReplicas(TestPullReplica.java:303)\n   [junit4]    > \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n   [junit4]    > \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n   [junit4]    > \tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n   [junit4]    > \tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n   [junit4]    > \tat java.base/java.lang.Thread.run(Thread.java:844)\n[...]\n   [junit4]   2> NOTE: test params are: codec=HighCompressionCompressingStoredFields(storedFieldsFormat=CompressingStoredFieldsFormat(compressionMode=HIGH_COMPRESSION, chunkSize=8218, maxDocsPerChunk=6, blockSize=10), termVectorsFormat=CompressingTermVectorsFormat(compressionMode=HIGH_COMPRESSION, chunkSize=8218, blockSize=10)), sim=RandomSimilarity(queryNorm=true): {}, locale=lg, timezone=America/Rainy_River\n   [junit4]   2> NOTE: Mac OS X 10.11.6 x86_64/Oracle Corporation 9 (64-bit)/cpus=3,threads=1,free=262884464,total=536870912\n\n ",
            "author": "Steve Rowe",
            "id": "comment-16537479"
        },
        {
            "date": "2018-07-10T01:51:46+0000",
            "content": "Thanks Steve Rowe, I will take a look at the failure.\n\nTom\u00e1s Fern\u00e1ndez L\u00f6bbe I tried to do that, but it will be quite complex, the process will be (not mention the race condition we can meet)\n\n\tThe core publish itself as DOWN\n\tThe core cancel it election context\n\tThe core delete its index dir\n\t...\n\n\n\nGiven that tragic exception is not a frequent event and using Overseer will bring us some benefits like\n\n\tThe update request that met the exception does not get blocked (async)\n\tMuch cleaner and well-tested approach\n\tWe can easily improve the solution to make it more robust. Ex: when delete replica failed because the node went down, Overseer can remove the replica from clusterstate (therefore even when the node come back, it will be automatically removed) then, Overseer can add a new replica in another node.\n\n ",
            "author": "Cao Manh Dat",
            "id": "comment-16537875"
        },
        {
            "date": "2018-07-10T02:25:54+0000",
            "content": "Commit cd08c7ef13613ceb88c1caf7b25e793ed51d47af in lucene-solr's branch refs/heads/master from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=cd08c7e ]\n\nSOLR-12412: release IndexWriter after getting tragic exception ",
            "author": "ASF subversion and git services",
            "id": "comment-16537901"
        },
        {
            "date": "2018-07-10T02:26:29+0000",
            "content": "Commit 0dc6ef996eab378bdd8329153bdecddbf89af9ee in lucene-solr's branch refs/heads/branch_7x from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0dc6ef9 ]\n\nSOLR-12412: release IndexWriter after getting tragic exception ",
            "author": "ASF subversion and git services",
            "id": "comment-16537903"
        },
        {
            "date": "2018-07-10T21:52:53+0000",
            "content": "Hi Cao Manh Dat ,\n\n\u00a0\n\nMaybe we could do something like this here as well to avoid this in the future ? https://issues.apache.org/jira/browse/SOLR-11616?focusedCommentId=16477719&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16477719\u00a0\n\ncc David Smiley ",
            "author": "Varun Thacker",
            "id": "comment-16539268"
        },
        {
            "date": "2018-07-14T02:45:25+0000",
            "content": "Yes, +1 to adding a withIndexWriter(lambda) method similar to this guy:\u00a0org.apache.solr.core.SolrCore#withSearcher\n\nThis ref-counted business is error-rpone.\n\n~ David ",
            "author": "David Smiley",
            "id": "comment-16543966"
        },
        {
            "date": "2018-07-16T22:43:40+0000",
            "content": "Jenkins is reporting quite a few failures for this test. I'm attaching one such run.\u00a0\n\nI ran the seed a couple of times locally but was not able to reproduce it , so it's timing related most likely.\u00a0 ",
            "author": "Varun Thacker",
            "id": "comment-16545823"
        },
        {
            "date": "2018-07-16T22:49:05+0000",
            "content": "Dat: in the past 7 days, LeaderTragicEventTest.testOtherReplicasAreNotActive has failed 36.33% (222 / 611) of all jenkins runs, and LeaderTragicEventTest.test has failed 21.28% (130 / 611).\n\nIn just the past 24 hours, we've seen a failure rate of 29.09% (16 / 55) for both methods.\n\nIt seems that even after your most recent commit, these tests need significant hardening to run even remotely close to reliably? ",
            "author": "Hoss Man",
            "id": "comment-16545831"
        },
        {
            "date": "2018-07-16T23:28:24+0000",
            "content": "This test class has two methods\n\n\ttest()\u00a0\n\ttestOtherReplicasAreNotActive()\n\n\n\nBoth try creating a collection \"collection1\" .\u00a0 We should probably put the delete collection in a finally block. This would avoid the following error\n\n[junit4] 2> 13586 INFO (TEST-LeaderTragicEventTest.test-seed#[7146D51E1F1D9F1A]) [ ] o.a.s.SolrTestCaseJ4 ###Starting test\n[junit4] 2> 13588 INFO (qtp1687913357-34) [n:127.0.0.1:36827_solr ] o.a.s.h.a.CollectionsHandler Invoked Collection Action :create with params collection.configName=config&name=collection1&nrtReplicas=2&action=CREATE&numShards=1&wt=javabin&version=2 and sendToOCPQueue=true\n[junit4] 2> 13590 INFO (OverseerThreadFactory-38-thread-1) [ ] o.a.s.c.a.c.CreateCollectionCmd Create collection collection1\n[junit4] 2> 13591 ERROR (OverseerThreadFactory-38-thread-1) [ ] o.a.s.c.a.c.OverseerCollectionMessageHandler Collection: collection1 operation: create failed:org.apache.solr.common.SolrException: collection already exists: collection1\n[junit4] 2> \u00a0\u00a0\u00a0at org.apache.solr.cloud.api.collections.CreateCollectionCmd.call(CreateCollectionCmd.java:106)\n[junit4] 2> \u00a0\u00a0\u00a0at org.apache.solr.cloud.api.collections.OverseerCollectionMessageHandler.processMessage(OverseerCollectionMessageHandler.java:255)\n[junit4] 2> \u00a0\u00a0\u00a0at org.apache.solr.cloud.OverseerTaskProcessor$Runner.run(OverseerTaskProcessor.java:469)\n[junit4] 2> \u00a0\u00a0\u00a0at org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor.lambda$execute$0(ExecutorUtil.java:209)\n[junit4] 2> \u00a0\u00a0\u00a0at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n[junit4] 2> \u00a0\u00a0\u00a0at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n[junit4] 2> \u00a0\u00a0\u00a0at java.lang.Thread.run(Thread.java:748)\n\nSince\u00a0testOtherReplicasAreNotActive() failed with an error , it didn't delete the collection1. test() was run after that and hit the above error.\u00a0 test() still passed even if the create collection failed ( which means there was already a corrupted index ) . Sounds fishy?\n\n\u00a0\n\nWe could replace this the following line?\u00a0\n\n- int numReplicas = random().nextInt(2) + 1;\n+ int numReplicas = TestUtil.nextInt(random(), 1, 2);\n\n\u00a0\n\ntestOtherReplicasAreNotActive() -> When there are two replicas , where are we actually checking if it becomes active or not after it has been started again? i.e after this statement should we be checking if it becomes active and fail the test?\n\nif (otherReplicaJetty != null) {\n  // won't be able to do anything here, since this replica can't recovery from the leader\n  otherReplicaJetty.start();\n}\n\n\u00a0\n\ntestOtherReplicasAreNotActive() ->\u00a0 when the test selects one replica , what are we testing exactly ? From what I can understand we are corrupting the leader of a single sharded collection and then validating if it's still the leader ?\u00a0\n\nI'm trying to understand the\u00a0corruptLeader() method : Why are we trying to delete segment files after every add ?\u00a0 What if we just add the 100 docs and then delete the segments_N file ?\u00a0\n\nHappy to pitch in just wanted to understand the test better before diving in ",
            "author": "Varun Thacker",
            "id": "comment-16545867"
        },
        {
            "date": "2018-07-16T23:45:21+0000",
            "content": "With regards to the actual failure , I think we are shutting down the wrong Jetty?\n\n\u00a0\n\nFrom the seed we have numReplicas=2.\u00a0 Which means we want to shutdown the non-leader shard but from the logs it's shutting down the leader jetty?\u00a0\n\nAnd then when we go to corrupt the leader jetty , it's actually closed ?\n\n[junit4] 2> 13526 INFO (TEST-LeaderTragicEventTest.testOtherReplicasAreNotActive-seed#[7146D51E1F1D9F1A]) [ ] o.a.s.c.ZkController Remove node as live in ZooKeeper:/live_nodes/127.0.0.1:35477_solr\n[junit4] 2> 13526 INFO (TEST-LeaderTragicEventTest.testOtherReplicasAreNotActive-seed#[7146D51E1F1D9F1A]) [ ] o.a.s.m.SolrMetricManager Closing metric reporters for registry=solr.cluster, tag=null\n[junit4] 2> 13526 INFO (zkCallback-17-thread-1) [ ] o.a.s.c.c.ZkStateReader Updated live nodes from ZooKeeper... (2) -> (1)\n....\n\n[junit4] 2> 13543 INFO (coreCloseExecutor-33-thread-1) [n:127.0.0.1:35477_solr c:collection1 s:shard1 r:core_node3 x:collection1_shard1_replica_n1] o.a.s.m.SolrMetricManager Closing metric reporters for registry=solr.collection.collection1.shard1.leader, tag=f37433\n...\n[junit4] 2> 13554 INFO (OverseerStateUpdate-72132540686336006-127.0.0.1:35477_solr-n_0000000000) [ ] o.a.s.c.Overseer Overseer Loop exiting : 127.0.0.1:35477_solr\n[junit4] 2> 13554 WARN (OverseerAutoScalingTriggerThread-72132540686336006-127.0.0.1:35477_solr-n_0000000000) [ ] o.a.s.c.a.OverseerTriggerThread OverseerTriggerThread woken up but we are closed, exiting.\n[junit4] 2> 13562 INFO (zkCallback-17-thread-1) [ ] o.a.s.c.OverseerElectionContext I am going to be the leader 127.0.0.1:36827_solr\n[junit4] 2> 13562 INFO (zkCallback-17-thread-1) [ ] o.a.s.c.Overseer Overseer (id=72132540686336005-127.0.0.1:36827_solr-n_0000000001) starting\n...\n[junit4] 2> 13575 INFO (TEST-LeaderTragicEventTest.testOtherReplicasAreNotActive-seed#[7146D51E1F1D9F1A]) [ ] o.a.s.SolrTestCaseJ4 ###Ending testOtherReplicasAreNotActive\n[junit4] 2> NOTE: reproduce with: ant test -Dtestcase=LeaderTragicEventTest -Dtests.method=testOtherReplicasAreNotActive -Dtests.seed=7146D51E1F1D9F1A -Dtests.multiplier=3 -Dtests.slow=true -Dtests.badapples=true -Dtests.locale=es-CL -Dtests.timezone=Pacific/Niue -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1\n[junit4] ERROR 5.96s J2 | LeaderTragicEventTest.testOtherReplicasAreNotActive <<<\n[junit4] > Throwable #1: java.lang.IllegalStateException: Jetty Connector is not open: -2\n[junit4] > \u00a0\u00a0\u00a0at __randomizedtesting.SeedInfo.seed([7146D51E1F1D9F1A:F4F2F96923E22682]:0)\n[junit4] > \u00a0\u00a0\u00a0at org.apache.solr.client.solrj.embedded.JettySolrRunner.getBaseUrl(JettySolrRunner.java:499)\n[junit4] > \u00a0\u00a0\u00a0at org.apache.solr.cloud.MiniSolrCloudCluster.getReplicaJetty(MiniSolrCloudCluster.java:539)\n[junit4] > \u00a0\u00a0\u00a0at org.apache.solr.cloud.LeaderTragicEventTest.corruptLeader(LeaderTragicEventTest.java:100)\n[junit4] > \u00a0\u00a0\u00a0at org.apache.solr.cloud.LeaderTragicEventTest.testOtherReplicasAreNotActive(LeaderTragicEventTest.java:150)\n[junit4] > \u00a0\u00a0\u00a0at java.lang.Thread.run(Thread.java:748)\n\n\u00a0 ",
            "author": "Varun Thacker",
            "id": "comment-16545876"
        },
        {
            "date": "2018-07-16T23:53:03+0000",
            "content": "Sorry about the failure, I will take a look today. ",
            "author": "Cao Manh Dat",
            "id": "comment-16545877"
        },
        {
            "date": "2018-07-17T11:43:15+0000",
            "content": "Commit 705e6f76a44fc774693c36e598022466e0cb1a95 in lucene-solr's branch refs/heads/master from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=705e6f7 ]\n\nSOLR-12412: Fix test failure ",
            "author": "ASF subversion and git services",
            "id": "comment-16546464"
        },
        {
            "date": "2018-07-17T11:45:31+0000",
            "content": "Commit eed5e7bb1c28fc99982a8d13b33c68425e99e21c in lucene-solr's branch refs/heads/branch_7x from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eed5e7b ]\n\nSOLR-12412: Fix test failure ",
            "author": "ASF subversion and git services",
            "id": "comment-16546467"
        },
        {
            "date": "2018-07-17T18:29:15+0000",
            "content": "Hi Dat,\n\nThe Jira description\u00a0reads\u00a0 \"When a leader meets some kind of unrecoverable exception (ie: CorruptedIndexException). The shard will go into the readable state and human has to intervene. In that case, it will be the best if the leader gives up its leadership and let other replicas become the leader.\"\n\n\u00a0\n\nBut in the test we are asserting this?\n\nassertEquals(leader.getName(), oldLeader.getName());\n\n\u00a0\n\nI had a question that I posted yesterday , reposting it for reference :\n\ntestOtherReplicasAreNotActive() -> When there are two replicas , where are we actually checking if it becomes active or not after it has been started again? i.e after this statement should we be checking if it becomes active and fail the test?\n\nif (otherReplicaJetty != null) {\n// won't be able to do anything here, since this replica can't recovery from the leader\notherReplicaJetty.start();\n}\n ",
            "author": "Varun Thacker",
            "id": "comment-16546974"
        },
        {
            "date": "2018-07-18T01:42:59+0000",
            "content": "Varun Thacker The leader will only give up its leadership only in the case there are another active replica in the same shard. In the testOtherReplicasAreNotActive, we randomly 2 cases:\n\n\tA shard with only 1 replica\n\tA shard with 2 replica but the non leader replica is DOWN\n\n\n\nIn both case, the leader should not give up its leadership. ",
            "author": "Cao Manh Dat",
            "id": "comment-16547278"
        },
        {
            "date": "2018-07-18T01:46:40+0000",
            "content": "Got it!\n\n//TODO better way to test this\nThread.sleep(5000);\nReplica leader = getCollectionState(collection).getSlice(\"shard1\").getLeader();\nassertEquals(leader.getName(), oldLeader.getName());\n\nif (otherReplicaJetty != null) {\n  // won't be able to do anything here, since this replica can't recovery from the leader\n  otherReplicaJetty.start();\n}\n\nShould we start the\u00a0otherReplicaJetty and then check if the leader doesn't change in the test i.e reverse the order here ?\u00a0\n\nAlso maybe we can add the\u00a0explanation\u00a0as comments to the test code ? To someone new it would make it a lot easier to understand what this test is trying to do. ",
            "author": "Varun Thacker",
            "id": "comment-16547281"
        },
        {
            "date": "2018-07-26T17:14:33+0000",
            "content": "ASF Jenkins found a reproducing seed for a LeaderTragicEventTest failure https://builds.apache.org/job/Lucene-Solr-NightlyTests-7.x/271/:\n\n\nChecking out Revision 950b7b6b1b92849721eaed50ecad9711199180e8 (refs/remotes/origin/branch_7x)\n[...]\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=LeaderTragicEventTest -Dtests.seed=14F869F052BC897B -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/jenkins-slave/workspace/Lucene-Solr-NightlyTests-7.x/test-data/enwiki.random.lines.txt -Dtests.locale=de-DE -Dtests.timezone=US/Michigan -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] ERROR   0.00s J1 | LeaderTragicEventTest (suite) <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: ObjectTracker found 1 object(s) that were not released!!! [TransactionLog]\n   [junit4]    > org.apache.solr.common.util.ObjectReleaseTracker$ObjectTrackerException: org.apache.solr.update.TransactionLog\n   [junit4]    > \tat org.apache.solr.common.util.ObjectReleaseTracker.track(ObjectReleaseTracker.java:42)\n   [junit4]    > \tat org.apache.solr.update.TransactionLog.<init>(TransactionLog.java:188)\n   [junit4]    > \tat org.apache.solr.update.UpdateLog.newTransactionLog(UpdateLog.java:467)\n   [junit4]    > \tat org.apache.solr.update.UpdateLog.ensureLog(UpdateLog.java:1323)\n   [junit4]    > \tat org.apache.solr.update.UpdateLog.add(UpdateLog.java:571)\n   [junit4]    > \tat org.apache.solr.update.UpdateLog.add(UpdateLog.java:551)\n   [junit4]    > \tat org.apache.solr.update.DirectUpdateHandler2.doNormalUpdate(DirectUpdateHandler2.java:345)\n   [junit4]    > \tat org.apache.solr.update.DirectUpdateHandler2.addDoc0(DirectUpdateHandler2.java:283)\n   [junit4]    > \tat org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:233)\n   [junit4]    > \tat org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:67)\n   [junit4]    > \tat org.apache.solr.update.processor.UpdateRequestProcessor.processAdd(UpdateRequestProcessor.java:55)\n   [junit4]    > \tat org.apache.solr.update.processor.DistributedUpdateProcessor.doLocalAdd(DistributedUpdateProcessor.java:951)\n   [junit4]    > \tat org.apache.solr.update.processor.DistributedUpdateProcessor.versionAdd(DistributedUpdateProcessor.java:1167)\n   [junit4]    > \tat org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:634)\n   [junit4]    > \tat org.apache.solr.update.processor.LogUpdateProcessorFactory$LogUpdateProcessor.processAdd(LogUpdateProcessorFactory.java:103)\n   [junit4]    > \tat org.apache.solr.handler.loader.JavabinLoader$1.update(JavabinLoader.java:98)\n   [junit4]    > \tat org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readOuterMostDocIterator(JavaBinUpdateRequestCodec.java:188)\n   [junit4]    > \tat org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readIterator(JavaBinUpdateRequestCodec.java:144)\n   [junit4]    > \tat org.apache.solr.common.util.JavaBinCodec.readObject(JavaBinCodec.java:311)\n   [junit4]    > \tat org.apache.solr.common.util.JavaBinCodec.readVal(JavaBinCodec.java:256)\n   [junit4]    > \tat org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readNamedList(JavaBinUpdateRequestCodec.java:130)\n   [junit4]    > \tat org.apache.solr.common.util.JavaBinCodec.readObject(JavaBinCodec.java:276)\n   [junit4]    > \tat org.apache.solr.common.util.JavaBinCodec.readVal(JavaBinCodec.java:256)\n   [junit4]    > \tat org.apache.solr.common.util.JavaBinCodec.unmarshal(JavaBinCodec.java:178)\n   [junit4]    > \tat org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec.unmarshal(JavaBinUpdateRequestCodec.java:195)\n   [junit4]    > \tat org.apache.solr.handler.loader.JavabinLoader.parseAndLoadDocs(JavabinLoader.java:109)\n   [junit4]    > \tat org.apache.solr.handler.loader.JavabinLoader.load(JavabinLoader.java:55)\n   [junit4]    > \tat org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:97)\n   [junit4]    > \tat org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:68)\n   [junit4]    > \tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:199)\n   [junit4]    > \tat org.apache.solr.core.SolrCore.execute(SolrCore.java:2541)\n   [junit4]    > \tat org.apache.solr.servlet.HttpSolrCall.execute(HttpSolrCall.java:709)\n   [junit4]    > \tat org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:515)\n   [junit4]    > \tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:377)\n   [junit4]    > \tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:323)\n   [junit4]    > \tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\n   [junit4]    > \tat org.apache.solr.client.solrj.embedded.JettySolrRunner$DebugFilter.doFilter(JettySolrRunner.java:139)\n   [junit4]    > \tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1642)\n   [junit4]    > \tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\n   [junit4]    > \tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\n   [junit4]    > \tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\n   [junit4]    > \tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255)\n   [junit4]    > \tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1317)\n   [junit4]    > \tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\n   [junit4]    > \tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\n   [junit4]    > \tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\n   [junit4]    > \tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\n   [junit4]    > \tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1219)\n   [junit4]    > \tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\n   [junit4]    > \tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:674)\n   [junit4]    > \tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\n   [junit4]    > \tat org.eclipse.jetty.server.Server.handle(Server.java:531)\n   [junit4]    > \tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:352)\n   [junit4]    > \tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:260)\n   [junit4]    > \tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:281)\n   [junit4]    > \tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102)\n   [junit4]    > \tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:291)\n   [junit4]    > \tat org.eclipse.jetty.io.ssl.SslConnection$3.succeeded(SslConnection.java:151)\n   [junit4]    > \tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102)\n   [junit4]    > \tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:118)\n   [junit4]    > \tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\n   [junit4]    > \tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\n   [junit4]    > \tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\n   [junit4]    > \tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\n   [junit4]    > \tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\n   [junit4]    > \tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:762)\n   [junit4]    > \tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:680)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:748)\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([14F869F052BC897B]:0)\n   [junit4]    > \tat org.apache.solr.SolrTestCaseJ4.teardownTestCases(SolrTestCaseJ4.java:304)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:748)\n   [junit4] Completed [125/825 (1!)] on J1 in 55.18s, 2 tests, 1 failure <<< FAILURES!\n\n ",
            "author": "Steve Rowe",
            "id": "comment-16558626"
        },
        {
            "date": "2018-07-27T22:46:11+0000",
            "content": "Hi Dat,\n\nChecking again with the doubts that I had regarding this test\n\nFirst question\nI'm trying to understand the\u00a0corruptLeader() method : Why are we trying to delete segment files after every add ?\u00a0 What if we just add the 100 docs and then delete the segments_N file ?\u00a0\nSecond question\nShould we start the\u00a0otherReplicaJetty and then check if the leader doesn't change in the test i.e reverse the order here ?\u00a0\nThe reason I ask this again is to me what's the point of starting the jetty and putting a code comment that the shard won't be able to do anything without actually validating it?\n\nif (otherReplicaJetty != null) {\n  // won't be able to do anything here, since this replica can't recovery from the leader\n  otherReplicaJetty.start();\n}\n ",
            "author": "Varun Thacker",
            "id": "comment-16560438"
        },
        {
            "date": "2018-07-28T02:51:24+0000",
            "content": "Hi Varun Thacker ,\u00a0\n\n1st question: this is the only way to avoid the cache of the directory and trigger merge reliable -> tragic event will reliably occur. You can try to change the code to your strategy and valid that tragic event, in that case, won't occur reliably.\n\n2nd: Yeah, I plan to do that, but too busy with other stuff and it only makes the test less clear. not affect the case I want to test. That comment mean, we won't be able to recover shard to come active, since the leader is already corrupted hence the replica won't be able to do recovery. ",
            "author": "Cao Manh Dat",
            "id": "comment-16560568"
        },
        {
            "date": "2018-08-01T00:13:54+0000",
            "content": "Commit eada799f576a2a1cb6dd16179a34ef283cdb4101 in lucene-solr's branch refs/heads/master from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=eada799 ]\n\nSOLR-12412: Leak transaction log on tragic event ",
            "author": "ASF subversion and git services",
            "id": "comment-16564531"
        },
        {
            "date": "2018-08-01T00:14:54+0000",
            "content": "Commit 0dc124aa78e2a1c121a9634e69f84c8b1f6be331 in lucene-solr's branch refs/heads/master from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0dc124a ]\n\nSOLR-12412: Fix precommit ",
            "author": "ASF subversion and git services",
            "id": "comment-16564533"
        },
        {
            "date": "2018-08-01T00:15:39+0000",
            "content": "Commit 41028dc989bc53717878123c0ea3effbbd7351ae in lucene-solr's branch refs/heads/branch_7x from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=41028dc ]\n\nSOLR-12412: Leak transaction log on tragic event ",
            "author": "ASF subversion and git services",
            "id": "comment-16564536"
        },
        {
            "date": "2018-08-01T00:15:41+0000",
            "content": "Commit a9f129190f9065c8775a628df181fb53248db488 in lucene-solr's branch refs/heads/branch_7x from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a9f1291 ]\n\nSOLR-12412: Fix precommit ",
            "author": "ASF subversion and git services",
            "id": "comment-16564537"
        }
    ]
}