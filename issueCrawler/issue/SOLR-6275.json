{
    "id": "SOLR-6275",
    "title": "Improve accuracy of QTime reporting",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "5.1"
        ],
        "components": [
            "search"
        ],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Currently, QTime uses currentTimeMillis instead of nano Time and hence is not suitable for time measurements. Further, it is really started after all the dispatch logic in SolrDispatchFilter (same with the top level timing reported by debug=timing) which may or may not be expensive, and hence may not fully represent the time taken by the search. This is to remedy both cases.",
    "attachments": {
        "SOLR-6275.patch": "https://issues.apache.org/jira/secure/attachment/12702280/SOLR-6275.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "ASF GitHub Bot",
            "id": "comment-14073392",
            "date": "2014-07-24T17:16:10+0000",
            "content": "GitHub user andyetitmoves opened a pull request:\n\n    https://github.com/apache/lucene-solr/pull/70\n\n    solr: Start RTimer for SearchHandler from right when the request starts\n\n    Patch for SOLR-6275\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/bloomberg/lucene-solr trunk-rtimer-qtime\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/lucene-solr/pull/70.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #70\n\n\ncommit 4dbe0b2660331de10944d8b0290a4f7fcae0f1ea\nAuthor: Ramkumar Aiyengar <raiyengar@bloomberg.net>\nDate:   2014-07-03T18:55:37Z\n\n    solr: Start RTimer for SearchHandler from right when the request starts\n\n "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-14075756",
            "date": "2014-07-27T21:08:26+0000",
            "content": "Hmmm, I'm not sure I agree. QTime is useful for knowing how long the lower level stuff took.\nThus it's useful as it stands. \n\nHaving the dispatch time reported too seems valuable, but I don't think folding it into\nQTime is a good idea. Reporting both, however, gives me more information to work \nwith and a much better idea of what to look for for explaining query slowness than\nif the dispatch time was folded into QTime. A better alternative would be to include \nthis time as a separate component in the return timings block?\n\n-1 on switching to nanoTime for reporting\n\nI mean we're talking human-time here. When I'm asking how long a query took,\nmy frail human system is not capable of noticing a difference of a millisecond or two.\nnanoTime is not necessarily all that accurate either. Sure, it gives a lot or precision,\nbut that's different from accuracy.\n\n\"This method provides nanosecond precision, but not necessarily nanosecond accuracy.\nNo guarantees are made about how frequently values change\"\n\nFrankly, this doesn't seem worth the change to me. And especially if we return the full\nnanoTime string which  would render any current tools for reporting performance\nmetrics invalid by a factor 100,000, all for extremely questionable utility. "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14075884",
            "date": "2014-07-28T04:27:15+0000",
            "content": "Hmmm, I'm not sure I agree. QTime is useful for knowing how long the lower level stuff took. Thus it's useful as it stands.\n\nCan you elaborate what you mean by \"lower level stuff\"? If you are intending QTime to reflect time taken by Lucene, i.e. roughly the time taken by the top-level collector to collect the results needed, QTime covers way more than that already, including time taken for query parsing and finishing up just in the non-distributed case. In the distributed case, it also covers time taken waiting on the shard handler factory pool, network latency, any servlet container pooling time at the processing shards, time taken waiting for the federating node to take all responses, and time taken to merge all responses.\n\nQTime as it stands is currently defined from the time the SolrQueryRequest object is created till the response is rendered, which is hard to associate any semantic meaning to, and it roughly is all steps except the logic required for resolving the core to send the request to. All I am doing is adding that as well, to logically mean (again roughly, but less roughly) \"time taken by the servlet to service the request\".\n\nI mean we're talking human-time here. When I'm asking how long a query took, my frail human system is not capable of noticing a difference of a millisecond or two. nanoTime is not necessarily all that accurate either. Sure, it gives a lot or precision, but that's different from accuracy.\n\nThe problem is not because of millisecond vs. nanosecond accuracy. currentTimeMillis represents the wall clock of the system and is subject to issues like clock skew. For example, if NTP resets the time or if the sys admin changes the time for some reason or if some other action changes the wall time, the difference between two such measurements can be totally incorrect (including being negative). Which is why nanoTime is preferred for timing measurements as it's guaranteed to be monotonic (where the OS supports it, i.e. everywhere except some older versions of Windows). See SOLR-5734 for some context where we changed all references of this thing, QTime was left out.\n\nFrankly, this doesn't seem worth the change to me. And especially if we return the full nanoTime string which would render any current tools for reporting performance metrics invalid by a factor 100,000, all for extremely questionable utility.\n\nWe are still reporting as milliseconds, no change in resolution here. The RTimer utility class used for this purpose already does the conversion. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14076289",
            "date": "2014-07-28T15:12:31+0000",
            "content": "hence is not suitable for time measurements.\n\n+1, we should probably fix it. \n\nThis is where the performance issue that I've read about with concurrent requests to nano time scare me though. This happens per request. If that happened to be a decent measurable hit, it may be better that this method returns bad information .0001% of the time. "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14076581",
            "date": "2014-07-28T18:54:24+0000",
            "content": "From what I have seen from googling, yes, nanoTime is way slower (only on Windows, there are claims its actually faster on Linux \u2013 not that I buy that), currentTimeMillis takes a few nanoseconds and nanoTime a microsecond or two. But with what we are dealing with, I doubt it matters, esp. once per request. I didn't se anything about concurrency though,  you have a link?\n\nTheoretically, we could add a flag to RTimer which falls back to currentTimeMillis on windows alone (ugh), but I doubt the ugliness is warranted. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14076749",
            "date": "2014-07-28T20:17:37+0000",
            "content": "This one perhaps? http://shipilev.net/blog/2014/nanotrusting-nanotime/\n\nThere were 2 good posts on nanotime floating around a couple/few months ago, so perhaps the other one. In any case, easy enough to spot check with a simple benchmark I think. Though there are then the variations between OS's and such, but I'm sure that would come to our attention and wouldn't be very likely if it's not really noticeable on one setup. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14076750",
            "date": "2014-07-28T20:18:18+0000",
            "content": "But yeah, I'm not really worried about the single cost call difference. I just have a faint memory reading that it's the concurrent calls to nanotime that will kill you. "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14077745",
            "date": "2014-07-29T14:16:09+0000",
            "content": "I added a rudimentary test for System.nanoTime performance, there's some perf degradation when you increase the number of threads but it's not alarming. And actually beyond a certain number of threads, I think some kind of caching kicks in and the average time dips considerably. I have updated the pull request with the test.\n\nThe following output was from a Linux machine with 16 cores..\n\n\n718 T11 oasu.TestUtils.testNanoTimeSpeed testNanoTime: maxNumThreads = 100, numIters = 1000\n723 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 1, time_per_call = 423ns\n724 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 2, time_per_call = 295ns\n725 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 3, time_per_call = 109ns\n726 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 4, time_per_call = 491ns\n727 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 5, time_per_call = 747ns\n728 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 6, time_per_call = 851ns\n729 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 7, time_per_call = 1031ns\n731 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 8, time_per_call = 453ns\n731 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 9, time_per_call = 42ns\n732 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 10, time_per_call = 77ns\n733 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 11, time_per_call = 78ns\n733 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 12, time_per_call = 44ns\n734 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 13, time_per_call = 74ns\n736 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 14, time_per_call = 47ns\n737 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 15, time_per_call = 46ns\n738 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 16, time_per_call = 68ns\n738 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 17, time_per_call = 45ns\n738 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 18, time_per_call = 46ns\n739 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 19, time_per_call = 46ns\n739 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 20, time_per_call = 47ns\n740 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 21, time_per_call = 47ns\n741 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 22, time_per_call = 65ns\n741 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 23, time_per_call = 48ns\n742 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 24, time_per_call = 47ns\n...\n795 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 96, time_per_call = 48ns\n796 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 97, time_per_call = 47ns\n796 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 98, time_per_call = 47ns\n799 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 99, time_per_call = 67ns\n800 T11 oasu.TestUtils.testNanoTimeSpeed numThreads = 100, time_per_call = 55ns\n\n "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14082267",
            "date": "2014-08-01T14:10:36+0000",
            "content": "Mark Miller, do those numbers sound reasonable to you? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14082268",
            "date": "2014-08-01T14:11:25+0000",
            "content": "Yeah, I dont see a real issue with it. "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14108591",
            "date": "2014-08-24T21:21:01+0000",
            "content": "Hey Mark, could this go in? Happy to help if there's anything else needed here.. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14227221",
            "date": "2014-11-27T03:41:24+0000",
            "content": "Sorry - been inactive for a while. Could I ask you to bring this one up to date as well  "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14227735",
            "date": "2014-11-27T15:17:43+0000",
            "content": "Done! "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14281409",
            "date": "2015-01-17T15:21:52+0000",
            "content": "Brought this up to date again, could this go in? "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14345915",
            "date": "2015-03-03T22:44:53+0000",
            "content": "Updated patch, passes tests and precommit. "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14345916",
            "date": "2015-03-03T22:45:34+0000",
            "content": "Mark, let me know if this attached patch looks good and I can commit it.. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14345961",
            "date": "2015-03-03T23:07:27+0000",
            "content": "+1, looks okay to me. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14346002",
            "date": "2015-03-03T23:27:23+0000",
            "content": "Commit 1663829 from andyetitmoves@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1663829 ]\n\nSOLR-6275: Improve accuracy of QTime reporting "
        },
        {
            "author": "ASF GitHub Bot",
            "id": "comment-14346597",
            "date": "2015-03-04T08:31:24+0000",
            "content": "Github user andyetitmoves closed the pull request at:\n\n    https://github.com/apache/lucene-solr/pull/70 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14346598",
            "date": "2015-03-04T08:32:19+0000",
            "content": "Commit 1663886 from andyetitmoves@apache.org in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1663886 ]\n\nSOLR-6275: Improve accuracy of QTime reporting "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14352030",
            "date": "2015-03-08T12:55:49+0000",
            "content": "Reopening this to resolve some Jenkins failures with the MacOSX..\n\nUwe Schindler pointed me to http://mail.openjdk.java.net/pipermail/hotspot-dev/2013-May/009496.html which talks about MacOSX using wall time instead (gettimeofday) \u2013 which would certainly fail this test (and is in some sense validation of why this issue exists!  )\n\nWe have two approaches here..\n\n\n\tDisable the test for nanoTime altogether\n\tDisable it just for MacOSX\n\n\n\nI haven't included reverting the change as it doesn't make things any better, the test failure is on a platform which is a no-op as far as this change is concerned anyway, and there haven't been other failures.. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-14352139",
            "date": "2015-03-08T17:03:21+0000",
            "content": "+1 for disabling on MacOSX. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-14352623",
            "date": "2015-03-09T07:13:41+0000",
            "content": "+1 to disable this test completely. As I said, its not testing Solr, it checks some stuff in the JVM - in a way thats likely to break on busy hardware (not only on OSX). "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14362055",
            "date": "2015-03-14T21:58:28+0000",
            "content": "Commit 1666754 from Ramkumar Aiyengar in branch 'dev/trunk'\n[ https://svn.apache.org/r1666754 ]\n\nSOLR-6275: Remove nanoTime speed test "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14362056",
            "date": "2015-03-14T22:00:23+0000",
            "content": "Commit 1666755 from Ramkumar Aiyengar in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1666755 ]\n\nSOLR-6275: Remove nanoTime speed test "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14362060",
            "date": "2015-03-14T22:06:44+0000",
            "content": "I have removed the full test, mainly because it has grinded through for a while now, and only MacOSX seems to be failing mostly.\n\nAt this point, this seems to be mostly testing (and failing) on OS scheduling due to the Jenkins machine running at full CPU usage.\n\nFWIW, the slow/non-monotonic nanoTime issue on MacOSX is now resolved.. https://bugs.openjdk.java.net/browse/JDK-8040140, and I have seen these failures with the fix applied (on Java 8) and without (on Java 7) \u2013 so it seems mainly due to scheduling. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-14495220",
            "date": "2015-04-15T00:30:07+0000",
            "content": "Bulk close after 5.1 release "
        }
    ]
}