{
    "id": "LUCENE-1545",
    "title": "Standard analyzer does not correctly tokenize combining character U+0364 COMBINING LATIN SMALL LETTRE E",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Bug",
        "fix_versions": [
            "3.1",
            "4.0-ALPHA"
        ],
        "affect_versions": "2.4",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Standard analyzer does not correctly tokenize combining character U+0364 COMBINING LATIN SMALL LETTRE E.\nThe word \"mo\u0364chte\" is incorrectly tokenized into \"mo\" \"chte\", the combining character is lost.\nExpected result is only on token \"mo\u0364chte\".",
    "attachments": {
        "AnalyzerTest.java": "https://issues.apache.org/jira/secure/attachment/12400612/AnalyzerTest.java"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-02-20T15:24:18+0000",
            "content": "$ java -Dfile.encoding=UTF-8 -cp lib/lucene-core-2.4-20090219.021329-1.jar:. AnalyzerTest    \n(mo,0,2,type=<ALPHANUM>)\n(chte,3,7,type=<ALPHANUM>)\n(m,8,9,type=<ALPHANUM>)\n(mo,10,12,type=<ALPHANUM>)\n(chte,13,17,type=<ALPHANUM>)\n$locale\nLANG=de_DE.UTF-8\nLC_CTYPE=\"de_DE.UTF-8\"\nLC_NUMERIC=\"de_DE.UTF-8\"\nLC_TIME=\"de_DE.UTF-8\"\nLC_COLLATE=de_DE.UTF-8\nLC_MONETARY=\"de_DE.UTF-8\"\nLC_MESSAGES=de_DE.UTF-8\nLC_PAPER=\"de_DE.UTF-8\"\nLC_NAME=\"de_DE.UTF-8\"\nLC_ADDRESS=\"de_DE.UTF-8\"\nLC_TELEPHONE=\"de_DE.UTF-8\"\nLC_MEASUREMENT=\"de_DE.UTF-8\"\nLC_IDENTIFICATION=\"de_DE.UTF-8\"\nLC_ALL= ",
            "author": "Andreas Hauser",
            "id": "comment-12675380"
        },
        {
            "date": "2009-02-20T15:25:06+0000",
            "content": "this is an example of why i started messing with LUCENE-1488 ",
            "author": "Robert Muir",
            "id": "comment-12675381"
        },
        {
            "date": "2009-06-11T02:46:05+0000",
            "content": "Feel free to switch back, but for now I'm going to mark this as part of LUCENE-1488, as offhand, that looks like the best solution for this issue. As that issue is not marked 2.9 at the moment, I'm pushing this off to 3.0. ",
            "author": "Mark Miller",
            "id": "comment-12718282"
        },
        {
            "date": "2009-06-11T05:19:26+0000",
            "content": "if you are looking for a more short-term solution (since i think 1488 will take quite a bit more time), it would be possible to make StandardAnalyzer more 'unicode-friendly'.\n\nits not possible to make it 'correct', and adding additional unicode friendliness would make backwards compat a much more complex issue (different unicode versions across JVM  versions, etc).\n\nbut if you want, i'm willing to come up with some minor grammar changes for StandardAnalyzer that could help things like this. ",
            "author": "Robert Muir",
            "id": "comment-12718300"
        },
        {
            "date": "2009-06-11T09:33:56+0000",
            "content": "Mark, when we push, we should push to 3.1 not 3.0 (I just added a 3.1 version to Jira for Lucene)... because 3.0 will come quickly after 2.9 and will \"only\" remove deprecations, etc. ",
            "author": "Michael McCandless",
            "id": "comment-12718370"
        },
        {
            "date": "2009-06-12T11:08:40+0000",
            "content": "but if you want, i'm willing to come up with some minor grammar changes for StandardAnalyzer that could help things like this.\n\nIs it possible to conditionalize, at runtime, certain parts of a JFlex grammar?  Ie, with matchVersion (LUCENE-1684) we could preserve back-compat on this issue, but I'm not sure how to cleanly push that matchVersion (provided @ runtime to StandardAnalyzer's ctor) \"down\" into the grammar so that eg we're not force to make a new full copy of the grammar for each fix.  (Though perhaps that's an OK solution since it'd make it easy to strongly guarantee back compat...). ",
            "author": "Michael McCandless",
            "id": "comment-12718795"
        },
        {
            "date": "2009-06-12T13:03:31+0000",
            "content": "michael, I don't see a way from the manual to do it.\n\nits not just the rules, but the JRE used to compile the rules (and its underlying unicode defs) so you might need separate standardtokenizerimpl's to really control the thing... ",
            "author": "Robert Muir",
            "id": "comment-12718825"
        },
        {
            "date": "2010-09-29T05:51:11+0000",
            "content": "I updated AnalyzerTest.java:\n\n\nimport java.io.FileOutputStream;\nimport java.io.OutputStreamWriter;\nimport java.io.StringReader;\nimport org.apache.lucene.analysis.TokenStream;\nimport org.apache.lucene.analysis.standard.StandardAnalyzer;\nimport org.apache.lucene.util.Version;\n\npublic class AnalyzerTest {\n  public static void test() throws Exception {\n    StandardAnalyzer a = new StandardAnalyzer(Version.LUCENE_31);\n    TokenStream ts = a.tokenStream(\"\", new StringReader(\"mo\u0364chte m mo\\u0364chte \"));\n    OutputStreamWriter writer = new OutputStreamWriter(new FileOutputStream(\"output.txt\"), \"UTF-8\");\n    while (ts.incrementToken()) {\n      writer.append(ts.toString()).append(System.getProperty(\"line.separator\"));\n    }\n    writer.flush();\n    writer.close();\n  }\n\n  public static void main(String[] argv) throws Exception {\n    test();\n  }\n}\n\n\n\nHere's what goes into output.txt when I compile AnalyzerTest.java with javac -encoding UTF-8 -cp lucene/dev/branches/branch_3x/lucene/build/lucene-core-3.1-SNAPSHOT.jar\" AnalyzerTest:\n\n(mo\u0364chte,startOffset=0,endOffset=7,positionIncrement=1,type=<ALPHANUM>)\n(m,startOffset=8,endOffset=9,positionIncrement=1,type=<ALPHANUM>)\n(mo\u0364chte,startOffset=10,endOffset=17,positionIncrement=1,type=<ALPHANUM>)\n\n\n\nWith LUCENE-2167 committed on the 3.X branch and on trunk, I think this issue is resolved.  Please reopen if you see different behavior. ",
            "author": "Steve Rowe",
            "id": "comment-12916033"
        },
        {
            "date": "2010-09-29T05:53:25+0000",
            "content": "fixed in LUCENE-2167 ",
            "author": "Robert Muir",
            "id": "comment-12916037"
        },
        {
            "date": "2011-03-30T15:50:14+0000",
            "content": "Bulk close for 3.1 ",
            "author": "Grant Ingersoll",
            "id": "comment-13013410"
        }
    ]
}