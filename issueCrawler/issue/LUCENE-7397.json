{
    "id": "LUCENE-7397",
    "title": "Inefficient FVhighlighting when set many HighlightedField.",
    "details": {
        "resolution": "Unresolved",
        "affect_versions": "None",
        "components": [
            "modules/highlighter"
        ],
        "labels": "",
        "fix_versions": [],
        "priority": "Minor",
        "status": "Open",
        "type": "Improvement"
    },
    "description": "when highlighting, search result org.apache.lucene.search.vectorhighlight.FastVectorHighlighter.java\ngetBestFragment method ~ FieldTermStack.java read whole doc's termvector every highlighted field.\nIt causes slow query when many highlight field",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15394825",
            "author": "donghyun Kim",
            "date": "2016-07-27T00:14:31+0000",
            "content": "\n\nwhen each searchDoc's highlightPhase, It calls highlighter's(In this case, FVH) highlight(highlighterContext).\nIn FastVectorHighlighter.java, loop for each requested 'HighlightedField' and getBestFragments.\ngetBestFragments method receive parameter[2] hitContext.docId() that uses for getting termVector of doc.\nIt finally reach org.apache.lucene.search.vectorhighlight.FieldTermStack.java and get termVector of doc.\n\nThe problem is \n'every highlightedField's getBestFragments method' call\n final Fields vectors = reader.getTermVectors(docId); \n\nand it seems Inefficiently slow when search highlight result include (big document && many highlightedField). read whole doc's termvector with every highlightedField.\n\nmy testing machine:\nquad 1.87 ghz,\n8Gb memory,\nspinning disk.\nES-1.5.2 (relevent code not changed, when I saw)\n\nExample,\nmy query :\n`\n{\n\"from\" : 0,\n\"size\" : 20,\n\n\"query\" : \n{\n\"query about highlighted field and more\"}\n,\n\"explain\" : false,\n\"fields\" : [ \"highlight\", \"fileRevision\", \"ownerNameUnigram\", \"ownerName\", \"ownerId\", \"timeLastModified\", \"size\" ],\n\"sort\" : [ {\n\"_score\" : { }\n} ],\n\"highlight\" : {\n\"pre_tags\" : [ \"\" ],\n\"post_tags\" : [ \"\" ],\n\"order\" : \"score\",\n\"fragment_size\" : 128,\n\"number_of_fragments\" : 10,\n\"require_field_match\" : true,\n\"type\" : \"fvh\",\n\"fields\" : [ {\n\"ownerName\" : { }\n}, {\n\"fileName_ko\" : { }\n}, {\n\"fileName_en\" : { }\n}, {\n\"fileName_id\" : { }\n}, {\n\"fileName_es\" : { }\n}, {\n\"fileName_zh\" : { }\n}, {\n\"fileName_ja\" : { }\n}, {\n\"fileName_it\" : { }\n}, {\n\"fileName_ru\" : { }\n}, {\n\"fileName_pt\" : { }\n}, {\n\"fileName_hi\" : { }\n}, {\n\"fileName_etc\" : { }\n}, {\n\"contents_ko\" : { }\n}, {\n\"contents_ko.ngram\" : { }\n}, {\n\"contents_en\" : { }\n}, {\n\"contents_en.ngram\" : { }\n}, {\n\"contents_id\" : { }\n}, {\n\"contents_id.ngram\" : { }\n}, {\n\"contents_es\" : { }\n}, {\n\"contents_es.ngram\" : { }\n}, {\n\"contents_zh\" : { }\n}, {\n\"contents_zh.ngram\" : { }\n}, {\n\"contents_ja\" : { }\n}, {\n\"contents_ja.ngram\" : { }\n}, {\n\"contents_it\" : { }\n}, {\n\"contents_it.ngram\" : { }\n}, {\n\"contents_ru\" : { }\n}, {\n\"contents_ru.ngram\" : { }\n}, {\n\"contents_pt\" : { }\n}, {\n\"contents_pt.ngram\" : { }\n}, {\n\"contents_hi\" : { }\n}, {\n\"contents_hi.ngram\" : { }\n}, {\n\"contents_etc\" : { }\n}, {\n\"contents_etc.ngram\" : { }\n} ]\n}\n}\n`\n\nTest\n[tookTime in millis, getBestFragments]\nmy doc 12538's every field getBestFragments took about 20ms. \nand total highlight phase tooks 705 ms.\n\nI have a sparse mapping field. that means\n'doc 12538' field fileName_*\n[fileName_id , fileName_es, fileName_zh, fileName_ja, fileName_it, fileName_ru, fileName_pt, fileName_hi, fileName_etc]\nonly one field is filled with data among this array.\nIt's same to contents_* field.\n\ndangerous doc 12538 - \nCONVOCADOS_TALLER_SALUDMENTAL_JULIO2014.xlsx.txt\n\n[2016-07-26 16:57:04,043][INFO ][root ] [4][FastVectorHighlighter.highlight] [22], filedName : fileName_id, docId : 12538\n[23], filedName : fileName_id, docId : 12538\n[20], filedName : fileName_es, docId : 12538\n[21], filedName : fileName_zh, docId : 12538\n[21], filedName : fileName_ja, docId : 12538\n[28], filedName : fileName_it, docId : 12538\n[26], filedName : fileName_ru, docId : 12538\n[24], filedName : fileName_pt, docId : 12538\n[22], filedName : fileName_hi, docId : 12538\n[22], filedName : fileName_etc, docId : 12538\n[22], filedName : contents_ko, docId : 12538\n[20], filedName : contents_ko.ngram, docId : 12538\n[19], filedName : contents_en, docId : 12538\n[19], filedName : contents_en.ngram, docId : 12538\n[20], filedName : contents_id, docId : 12538\n[20], filedName : contents_id.ngram, docId : 12538\n[19], filedName : contents_es, docId : 12538\n[18], filedName : contents_es.ngram, docId : 12538\n[19], filedName : contents_zh, docId : 12538\n[19], filedName : contents_zh.ngram, docId : 12538\n[19], filedName : contents_ja, docId : 12538\n[19], filedName : contents_ja.ngram, docId : 12538\n[18], filedName : contents_it, docId : 12538\n[18], filedName : contents_it.ngram, docId : 12538\n[18], filedName : contents_ru, docId : 12538\n[18], filedName : contents_ru.ngram, docId : 12538\n[20], filedName : contents_pt.ngram, docId : 12538\n[18], filedName : contents_hi, docId : 12538\n[18], filedName : contents_hi.ngram, docId : 12538\n[18], filedName : contents_etc, docId : 12538\n[19], filedName : contents_etc.ngram, docId : 12538\n[2016-07-26 16:57:04,654][INFO ][root ] highlight tooks : 705, docId : 12538\n\nand...\nreader.getTermVectors(docId) tooks.\nI didn't log sync with getBestFragments took. but i can see rough sequence and how it tooks.\nI think heavy analyzed doc (have big termvectors) impact my query. (around 20ms sequence.)\n\nlong tTime = System.currentTimeMillis();\nfinal Fields vectors = reader.getTermVectors(docId);\ntermVectorTimeLogging(\"tVectorTime : \"+(System.currentTimeMillis() - tTime));\n\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 24\ntVectorTime : 24\ntVectorTime : 23\ntVectorTime : 21\ntVectorTime : 20\ntVectorTime : 19\ntVectorTime : 19\ntVectorTime : 48\ntVectorTime : 19\ntVectorTime : 18\ntVectorTime : 18\ntVectorTime : 18\ntVectorTime : 18\ntVectorTime : 18\ntVectorTime : 18\ntVectorTime : 18\ntVectorTime : 19\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 20\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 0\ntVectorTime : 1\ntVectorTime : 0\ntVectorTime : 1\ntVectorTime : 1\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 1\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 0\ntVectorTime : 1 "
        },
        {
            "id": "comment-15396156",
            "author": "David Smiley",
            "date": "2016-07-27T18:57:21+0000",
            "content": "Sorry I'm not clear what specifically you propose.  \n\nFYI, perhaps related: the Lucene default term vector implementation/codec will load term vectors for all fields, even if you are actually only interested in fewer.  It's easy to accidentally re-decode all term vectors when code asks for just one.  I don't know if the FVH has this problem or not.  I know the default highlighter can benefit from term vectors and it does not suffer from this potential trap. "
        },
        {
            "id": "comment-15396812",
            "author": "donghyun Kim",
            "date": "2016-07-28T02:40:37+0000",
            "content": "I think it's better to reuse read termVector when each document x many highlight field search.\n\neg) \n1. each document indexed as many fields.\n2. search for document.\n3. I want get highlight fragments for many docs each. \nfor each doc that searched\n    for each field that I want to get highlight fragment\n        4. I may call [getBestFragments] method that takes IndexReader, docId.\n        5. execute [final Fields vectors = reader.getTermVectors(docId);]. and I think It's possibly slow depends on size of termVector\n\nwe may support read termvector once per doc highlight process outer elsewhere and pass (Fields Object) as param I think.\noverloading the method possibly can solve my problem.\n\nmy scenario is :\nfor each doc that searched\n    execute [final Fields vectors = reader.getTermVectors(docId);].\n    for each field that I want to get highlight fragment\n          I may call [getBestFragments] method that takes IndexReader, docId,... (+Fields vectors).\n\nAny reason to reader.getTermVectors(docId) must located inside each getBestFragment? "
        },
        {
            "id": "comment-15398638",
            "author": "donghyun Kim",
            "date": "2016-07-29T03:04:56+0000",
            "content": "I test in local, editted lucene-4.10.4 and Elasticsearch-1.5.2\nthis way It quicken my query about approximately x4 at specific query that include large doc in result.\nmoreover I expect the other queries that include many, more large doc(large termvector) get more quicken. "
        }
    ]
}