{
    "id": "LUCENE-8361",
    "title": "Make TestRandomChains check that filters preserve positions",
    "details": {
        "components": [],
        "status": "Open",
        "resolution": "Unresolved",
        "fix_versions": [],
        "affect_versions": "None",
        "labels": "",
        "priority": "Minor",
        "type": "Test"
    },
    "description": "Follow-up of LUCENE-8360: it is a bit disappointing that we only found this issue because of a newly introduced token filter. I'm wondering that we might be able to make TestRandomChains detect more bugs by verifying that the sum of position increments is preserved through the whole analysis chain.",
    "attachments": {
        "LUCENE-8361.patch": "https://issues.apache.org/jira/secure/attachment/12928196/LUCENE-8361.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16515679",
            "author": "Alan Woodward",
            "content": "Here's a patch that adds a step to BaseTokenStreamTestCase.checkAnalysisConsistency.\n\nI've already found a failure in MinHashTokenFilter, which raises the question of whether we still expect end() to report the total number of original tokens for filters that summarise the entire stream - the same will apply to FingerprintFilter and ConcatenateGraphFilter.  I'm minded to just exclude filter chains that contain any of these from the test. ",
            "date": "2018-06-18T13:13:34+0000"
        },
        {
            "id": "comment-16519382",
            "author": "Alan Woodward",
            "content": "TestRandomChains found another failure due to this, in LimitTokenOffsetFilter. ",
            "date": "2018-06-21T13:47:38+0000"
        },
        {
            "id": "comment-16519406",
            "author": "Robert Muir",
            "content": "Both those limit-filters are broken and buggy by default, they won't consume all the tokens unless you pass some special boolean. ",
            "date": "2018-06-21T14:08:42+0000"
        },
        {
            "id": "comment-16527484",
            "author": "Alan Woodward",
            "content": "Both those limit-filters are broken and buggy by default\n\nThey're designed to short-cut tokenization (mainly for highlighting, I think) - do we have a non-buggy way of not consuming all tokens?  Because I can see that it's a valid thing to do in some circumstances. ",
            "date": "2018-06-29T11:23:03+0000"
        },
        {
            "id": "comment-16527562",
            "author": "Adrien Grand",
            "content": "We should disable some checks when the analysis chain includes one of these token filters that truncate the token stream. If we made them correct so that they consumed the wrapped token stream to eg. have access to the end offset, it would defeat their purpose a bit, which is to reduce the amount of work to do. ",
            "date": "2018-06-29T12:35:10+0000"
        },
        {
            "id": "comment-16527605",
            "author": "Robert Muir",
            "content": "Strongly disagree with making our tests wimpy because of these buggy filters. \n\nThese filters have the ability to be correct or incorrect, the problem is that they choose to default to incorrect behavior. \nThey should default to correct instead, and the ctor with the boolean option should be blacklisted in the test (since its known to be buggy).\nThey will still achieve their purpose, but yeah it means the user should pay more attention to where they sit in the analysis chain and so on, that's all.\n\n ",
            "date": "2018-06-29T13:08:44+0000"
        },
        {
            "id": "comment-16527606",
            "author": "Robert Muir",
            "content": "It is also bad they behave this way by default as I imagine it only causes problems for highlighting fields with multiple values and so on if they are used. ",
            "date": "2018-06-29T13:09:45+0000"
        },
        {
            "id": "comment-16527615",
            "author": "Robert Muir",
            "content": "\nThey're designed to short-cut tokenization (mainly for highlighting, I think) - do we have a non-buggy way of not consuming all tokens? Because I can see that it's a valid thing to do in some circumstances.\n\nYes, these filters have a boolean option to do this correctly. Its just not the default. This is really too bad, since somehow these bugs (which are like implementation details of how particular highlighters worked) made their way into the analysis module in such a way that its easy to put wrong offsets into your index. ",
            "date": "2018-06-29T13:18:53+0000"
        },
        {
            "id": "comment-16527623",
            "author": "Adrien Grand",
            "content": "Only blacklisting the bad constructor works for me too. ",
            "date": "2018-06-29T13:21:01+0000"
        },
        {
            "id": "comment-16527631",
            "author": "Robert Muir",
            "content": "Yes, the issue is that currently, all the constructors are bad  ",
            "date": "2018-06-29T13:25:13+0000"
        },
        {
            "id": "comment-16527636",
            "author": "Robert Muir",
            "content": "Even if we fix the default ctor to behave as correct=true, I think these filters will still have problems because they don't do anything with position increments? So they can easily stop at the wrong place (eg middle of a graph), cause wrong posLengths, etc. ",
            "date": "2018-06-29T13:30:38+0000"
        }
    ]
}