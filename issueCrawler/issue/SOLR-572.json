{
    "id": "SOLR-572",
    "title": "Spell Checker as a Search Component",
    "details": {
        "affect_versions": "1.3",
        "status": "Closed",
        "fix_versions": [
            "1.3"
        ],
        "components": [
            "spellchecker"
        ],
        "type": "New Feature",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "http://wiki.apache.org/solr/SpellCheckComponent\n\nExpose the Lucene contrib SpellChecker as a Search Component. Provide the following features:\n\n\tAllow creating a spell index on a given field and make it possible to have multiple spell indices \u2013 one for each field\n\tGive suggestions on a per-field basis\n\tGiven a multi-word query, give only one consistent suggestion\n\tProcess the query with the same analyzer specified for the source field and process each token separately\n\tAllow the user to specify minimum length for a token (optional)\n\n\n\nConsistency criteria for a multi-word query can consist of the following:\n\n\tPreserve the correct words in the original query as it is\n\tNever give duplicate words in a suggestion",
    "attachments": {
        "SOLR-572.patch": "https://issues.apache.org/jira/secure/attachment/12382129/SOLR-572.patch",
        "solr-572.patch": "https://issues.apache.org/jira/secure/attachment/12385212/solr-572.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12597203",
            "date": "2008-05-15T17:22:16+0000",
            "content": "Linked to SOLR-507 - Spell Checking Improvements. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12597207",
            "date": "2008-05-15T17:44:56+0000",
            "content": "A first cut for this issue. Please consider this as work in progress. I've posted this to get feedback on the approach and syntax.\n\nThe contains the following:\n\n\tSpellCheckComponent is an implementation of SearchComponent\n\tThe configuration is specified in solrconfig.xml with multiple \"dictionary\" nodes. Each dictionary must have a name and a type. The name must be specified during query time. The type is needed to allow for more than one way of loading data into the spell index (solr field or file). For example:\n\n<searchComponent name=\"spellcheck\" class=\"org.apache.solr.handler.component.SpellCheckComponent\">\n\t<lst name=\"dictionary\">\n\t\t<str name=\"name\">default</str>\n\t\t<str name=\"type\">solr</str>\n\t\t<str name=\"field\">word</str>\n\t\t<str name=\"indexDir\">c:/temp/spellindex</str>\n\t</lst>\n\t<lst name=\"dictionary\">\n\t\t<str name=\"name\">external</str>\n\t\t<str name=\"type\">file</str>\n\t\t<str name=\"path\">spellings.txt</str>\n\t</lst>\n</searchComponent>\n\n\n\tIf indexDir is not present in the dictionary's configuration then a RAMDirectory is used, otherwise a FSDirectory is used.\n\tThis patch supports dictionaries loaded from Solr fields.\n\tA separate Lucene SpellChecker is created for each configured dictionary\n\tSample query syntax is as follows:\n\t\n\t\t/select/?q=aura&version=2.2&start=0&rows=10&indent=on&spellcheck=true&spellcheck.dictionary=default&spellcheck.count=10\n\t\t/select/?q=toyata&version=2.2&start=0&rows=10&indent=on&spellcheck=true&spellcheck.dictionary=default\n\t\n\t\n\tThe value for \"q\" is analyzed with the Solr field's query analyzer. Suggestions for each token are fetched separately.\n\tOnly one suggestion for a query is given by default. This should be used for multi-token queries.\n\tIf spellcheck.count is specified then the response has a number of suggestions <= spellcheck.count for each token separately.\n\tOnly unique words are returned in the suggestions.\n\n\n\nThings to be done:\n\n\tAdd JUnit tests\n\tReloading dictionaries. Currently the dictionary is loaded only once during the first request.\n\tMake things more configurable like SpellCheckerRequestHandler\n\tAdd support for onlyMorePopular flag as in SpellCheckerRequestHandler\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12597345",
            "date": "2008-05-16T04:28:32+0000",
            "content": "\n\tthe spellcheck.dictionary=default must be optional in query. The user must be able to name a dictionary as 'default' and that can be used as the default if no value is passed.\n\n\n\n "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12597351",
            "date": "2008-05-16T04:44:29+0000",
            "content": "I had a quick look and it all looks nice and clean.\nI like the config, though I think \"solr\" is too specific - the source field could be in a vanilla Lucene indexthat lives somewhere on disk, or example.  Thus, I'd change \"solr\" to \"index\".  Oh, I see, you are reading field values from the index of the current core.  I think that is fine, but wouldn't it also be good to be able to read field values from a vanilla Lucene index? (but you wouldn't know the field type and thus would not be able to get the Analyzer for the field)\n\nAlso, and regardless of the above, instead of having \"indexDir\" and \"path\", why not call them both \"location\" and maybe even let them include the file: schema for consistency, if it works with the code that uses those locations?\n\nAlso on TODO:\n\n\tRead dictionary from plain-text files.\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12597354",
            "date": "2008-05-16T05:13:07+0000",
            "content": "Otis, I agree that we should call \"index' instead of \"solr\" for the type and \"path\" can be renamed to \"location\". But indexDir refers to the target for the spell check index whereas \"path\" currently refers to the source of the dictionary, so IMHO we should keep \"indexDir\" as it is (It can also be a relative path).\n\nFor supporting arbitrary lucene indices, user must specify type=\"index\", field=\"fieldName\", location=\"path/to/lucene/index/directory\" which should be enough (TODO). In that case the analyzer can be fixed as something (say WhitespaceAnalyzer or StandardAnalyzer).\n\nI'm not sure I understand your comment on the schema. If this is for text files then I was thinking more about having a text file which would have one word per line and all those words would go into the same dictionary. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12597358",
            "date": "2008-05-16T05:37:15+0000",
            "content": "I see (indexDir comment).  Might be better to make it more obvious then - \"sourceIndex\" for the Lucene index that serves as the source of data) vs. \"targetIndex\" (or \"spellcheckerIndex\") for the resulting spellchecker index.\n\nFor Lucene indices to be used as sources of data type=\"index\", field=\"fieldName\", location=\"path/to/lucene/index/directory\" makes sense.\n\nIgnore my comment about the schema, I'm just complicating things with that.  Yes, one word per line for plain-text file data sources - that can easily be digested with PlainTextDictionary class (part of Lucene SC). "
        },
        {
            "author": "Bojan Smid",
            "id": "comment-12597437",
            "date": "2008-05-16T12:02:11+0000",
            "content": "I added support for file-based dictionaries (they are configured as described in Shalin's post) using Lucene's PlainTextDictionary.\n\nHowever, I had to add property \"field\" to the configuration for this dictionary in order to obtain analyzer (which is passed to FieldSpellChecker). This analyzer is later used to extract tokens from the query.\n\nI guess my current solution is not quite correct (since PlainTextDictionary doesn't really need analyzer), but it also makes me wonder if in case of dictionary built from solr index, same analyzer should be used when building dictionary and parsing query strings? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12597453",
            "date": "2008-05-16T12:47:36+0000",
            "content": "Adding a 'field' attribute is not intuitive. If your data needs custom analyzers create an extra 'type' in the schema and let us dd an extra attribute 'dataType'  eg:\n\n\n<str name=\"dataType\">my_new_data_type</str>\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12597455",
            "date": "2008-05-16T12:50:59+0000",
            "content": "Patch applies cleanly.  Very cool that we have something concrete finally\n\nSome thoughts:\n1. I don't believe we use author tags (is this a Solr policy?  I know it is a Lucene Java convention)\n2. There needs to be unit tests\n3. I think it makes sense to have the option to return extended results\n4. I don't think it should be a default search component, but will defer to others.\n5. numFound should be returned when count > 1 as well, right?  In other words, the structure should be the same for the response no matter what in:\n\nif (count > 1) {\n        response.add(\"suggestions\", spellChecker.getSuggestions(q, count));\n      } else {\n        NamedList suggestions = new NamedList();\n        suggestions.add(\"numFound\", 1);\n        suggestions.add(q, spellChecker.getSuggestion(q));\n        response.add(\"suggestions\", suggestions);\n      }\n\n\nThat way it can be handled uniformly on the client "
        },
        {
            "author": "Bojan Smid",
            "id": "comment-12597466",
            "date": "2008-05-16T13:10:33+0000",
            "content": "The \"field\" attribute for file-based dictionary is basically the same \"field\" attribute as in default dictionary (in both cases they are used to obtain query analyzer), so that is the reason why I used the same name. My question was is it ok for default dictionary to use the same field to build dictionary from solr index and to obtain query analyzer for extracting tokens? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12597472",
            "date": "2008-05-16T13:16:02+0000",
            "content": "Bojan \u2013 Thanks for adding this functionality. I'll work on making things more configurable like SCRH and add a few tests. I think it is OK and may even be needed for a few cases. Though I prefer Noble's suggestion on having fieldType instead of field since it gives more freedom to the user.\n\nGrant \u2013 Thanks for looking into the patch. My comments below:\n\n\tRight, those were generated by my IDE, I'll remove it in the next patch\n\tAgree\n\tAgree, both 2 and 3 are on my todo list\n\tI don't understand what you mean by \"defer to others\" but on making this default or not, I'm fine either way.\n\tActually, the spellChecker.getSuggestion(q, count) returns a complete named list, which already has the numFound element. If you don't specify the count, then it gives back only a String for which we need to create a NamedList ourselves. In other words, the response format is actually the same both ways.\n\n\n\nNoble \u2013 I your suggestion on keeping a fieldType attribute in the configuration for non-Solr dictionaries. We can use the QueryAnalyzer defined for the given fieldType in Solr's schema. If this attribute is not present, we can default to WhitespaceAnalyzer or StandardAnalyzer. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12597481",
            "date": "2008-05-16T13:55:45+0000",
            "content": "\nI don't understand what you mean by \"defer to others\" but on making this default or not, I'm fine either way.\n\nJust meaning, I'm not the only one who has a say in whether or not is a default component.  My guess is not everyone will want it in the default list of components.\n\nVery cool on the other stuff.\n\nOne other thing to think about:  What if we want a different underlying spell checker?  The Lucene spell checker approach isn't exactly state of the art as far as I understand it.  Obviously not your concern at the moment, but might be good to think about the ability to interchange the underlying implementation by abstracting the notion of spelling a bit while still maintaining the same search component interface.  "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12597494",
            "date": "2008-05-16T14:43:11+0000",
            "content": "Grant - I agree it would be nice.  But let's get this one in first.  Perhaps you can add that idea to the list in SOLR-507. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12597814",
            "date": "2008-05-18T15:07:25+0000",
            "content": "Grant - I was trying to implement the onlyMorePopular and extendedResults format of SCRH when I realized that supporting such a response is not possible for text file based dictionaries in the current implementation. Currently, we use Lucene's PlainTextDictionary to load such text files and we don't maintain any frequency information. What do you suggest?\n\nBojan/Otis - The terms loaded from the text files are passed onto Lucene's SpellChecker as it is. As per Noble's suggestion, I've added support for a optional fieldType attribute (this type must be defined in schema.xml). This type's query analyzer is used for queries. Wouldn't it be more consistent to apply the index-analyzer during index time also?\n\nBoth the above problems can be solved if we keep the words loaded from the text files in a Lucene index but I'm not sure if we want to go that way. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12597816",
            "date": "2008-05-18T16:22:31+0000",
            "content": "Shalin\nI think the onlyMorePopular and extendedResults should be optional, so in case of plain text dictionaries this information would just not be present if we cannot derive it.  Even if we take words from plain text files and index them into a Lucene index their frequency will remain 1.\n\nDoes the index-time analyzer make sense?  I don't have the sources handy, but doesn't Lucene SC take the input word and chop it up into 2- and 3-grams before indexing?  If so, how would index-time analyzer come into play?\n\nIn principal, if taking plain text files and indexing words in them into a Lucene SC index solves problems, I think that's acceptable - such indices are likely to be relatively small, so they should be quick to build and not require a lot of memory.\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12597878",
            "date": "2008-05-19T06:39:35+0000",
            "content": "Ok, onlyMorePopular and extendedResults will only be supported for dictionaries built from Solr fields.\n\nYes, the Lucene SpellChecker does create n-grams but think about lowercasing, stemming etc. All this analysis can potentially change the word which eventually gets n-grammed by Lucene. "
        },
        {
            "author": "Bojan Smid",
            "id": "comment-12597913",
            "date": "2008-05-19T10:44:02+0000",
            "content": "I would like to add support for different character encodings in file-based dictionaries (current implementation will take system's default settings). I'm not sure how we'll synchronize your work with my fix? Can you let me know when/how can I start my work? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12597914",
            "date": "2008-05-19T10:54:49+0000",
            "content": "A new patch containing the following changes:\n\n\n\ttype=\"solr\" is now known as type=\"index\"\n\tpath is now called location\n\tRelative paths are supported. They are loaded through SolrResourceLoader.openResource method.\n\tDictionaries can be built on arbitary Lucene indices\n\tindexDir is now called spellcheckIndexDir to clearly highlight it's purpose\n\tDictionaries loaded from a text file can have a fieldType attribute. The analyzer of this fieldType is used at query time. If no fieldType is specified then WhitespaceAnalyzer is used.\n\tFor dictionaries loaded from a text file, if fieldType is specified then index-time analysis is done using the given fieldType's analyzer\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12597917",
            "date": "2008-05-19T11:00:05+0000",
            "content": "Bojan \u2013 I don't want to hold you up so I've uploaded the current state of my work. Please go ahead with your changes. I can continue after you're done.\n\nAnother issue I noticed with the SCRH is that it accepts the accuracy as a request parameter and calls Lucene SpellChecker.setAccuracy before getting the suggestion. However, this is neither thread-safe nor can we guarantee that the accuracy is actually enforced for the suggestion. Therefore, I think we should only have accuracy configurable in the solrconfig.xml and not as a request parameter. "
        },
        {
            "author": "Bojan Smid",
            "id": "comment-12597930",
            "date": "2008-05-19T12:02:42+0000",
            "content": "Character encodings for file-based dictionaries now supported with property characterEncoding. So, configuration for such dictionary would look like this:\n\n\n<lst name=\"dictionary\">\n\t\t<str name=\"name\">external</str>\n\t\t<str name=\"type\">file</str>\n\t\t<str name=\"sourceLocation\">spellings.txt</str>\n\t\t<str name=\"characterEncoding\">UTF-8</str>\n\t\t<str name=\"spellcheckIndexDir \">c:\\spellchecker</str>\n</lst>\n\n\n\nNew code needs latest lucene-spellchecker-2.4*.jar from Lucene trunk.\n\nSince SolrResourceLoader.getLines method doesn't support configurable encodings (treats everything as UTF-8), I wasn't sure how to add that support. I could have added overloaded method to SolrResourceLoader, but there is a TODO comment, so I decided to create getLines() method inside SpellCheckComponent class instead. What do you think of this? "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12598533",
            "date": "2008-05-21T01:03:15+0000",
            "content": "Hey guys I was just wondering if there is a way to get the suggestions not to echo the query if there are no suggestions available. For example, a query where q=food probably should not return a suggestion of \"food\". "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12598549",
            "date": "2008-05-21T04:10:45+0000",
            "content": "Oleg \u2013 Thanks for trying out the patch. No, currently it does not signal if suggestions are not found, it just returns the query terms themselves. I'll add that feature. "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12598716",
            "date": "2008-05-21T17:08:55+0000",
            "content": "Hey guys, I am having trouble creating a file-based dictionary.\n\nThe file looks like this: \n\namerican\nmexican\nclothes\nshoes\n\nand it is in my solr.home/conf directory.\n\nThe solrConfig has the following: <searchComponent name=\"spellcheck\" class=\"org.apache.solr.handler.component.SpellCheckComponent\">\n        <lst name=\"dictionary\">\n                <str name=\"name\">external</str>\n                <str name=\"type\">file</str>\n                <str name=\"sourceLocation\">spellings.txt</str>\n                <str name=\"characterEncoding\">UTF-8</str>\n                <str name=\"spellcheckIndexDir\">/home/csweb/index</str>\n        </lst>\n  </searchComponent>\n\nI hit it with the following URL: http://localhost:8983/solr/select/?q=pizza&spellcheck=true&spellcheck.dictionary=external\n\nand I get the following stacktrace:\n\nSEVERE: java.lang.NullPointerException\n        at org.apache.lucene.search.spell.SpellChecker.indexDictionary(SpellChecker.java:321)\n        at org.apache.solr.handler.component.SpellCheckComponent$FieldSpellChecker.init(SpellCheckComponent.java:391)\n        at org.apache.solr.handler.component.SpellCheckComponent.loadExternalFileDictionary(SpellCheckComponent.java:204)\n        at org.apache.solr.handler.component.SpellCheckComponent.prepare(SpellCheckComponent.java:131)\n        at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:133)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:125)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:966)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:339)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:274)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:175)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:844)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:583)\n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:447)\n        at java.lang.Thread.run(Thread.java:619)\n\n\nAny idea what I am doing wrong? Thanks! "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12598727",
            "date": "2008-05-21T17:42:02+0000",
            "content": "Haven't looked at the code, but the first thing I'd try is using a full/absolute path to your dictionary file. "
        },
        {
            "author": "Bojan Smid",
            "id": "comment-12598728",
            "date": "2008-05-21T17:43:17+0000",
            "content": "I already found the same problem, made a fix and sent it to Shalin, he will incorporate it into next patch when it's ready. If you specify field \"field type\" for that dictionary (and that field type can be found in Solr schema), you'll avoid the problem for now. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12598733",
            "date": "2008-05-21T17:56:02+0000",
            "content": "Just got an idea.  File-based dictionaries don't have word frequency information and with that we use certain value (e.g. so onlyMorePopular cannot be used).  What if we (also) accepted plain-text field dictionaries that included word frequency information?\ne.g.\nball,100\nboil,44\nbowl,77\n...\nI'm not looking at sources now, but could we not feed this word frequency information into Lucene SC, so it makes use of that when figuring out top-N best words to suggest?\n\nAnd how would we figure out the frequency of each word to begin with?  I imagine we can have a tool/class that, given a path to a dictionary file with words and a path to a Lucene/Solr index, looks up each dictionary word's frequency in the given index and outputs \"<word>,<freq>\" for each word.  This class could live in Lucene SC, but could be used by SCRH when rebuilding the SC index for example.\n\nDoes this sound useful and implementable? "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12598735",
            "date": "2008-05-21T18:00:44+0000",
            "content": "Bojan, do you mean adding something like <str name=\"field\">word</str> to the definition for the file-based dictionary? "
        },
        {
            "author": "Bojan Smid",
            "id": "comment-12598738",
            "date": "2008-05-21T18:18:43+0000",
            "content": "Oleg, that field is now called fieldType, so something like <str name=\"fieldType\">word</str> should work for you as long as you have fileType with name word defined in your schema.xml. Let me know if this works. "
        },
        {
            "author": "Bojan Smid",
            "id": "comment-12598752",
            "date": "2008-05-21T19:04:30+0000",
            "content": "I noticed that when searching for suggestion for a word which exists in dictionary, SC returns some similar word instead of returning that same word. Old SCRH had field \"exist\" which returned true if word exists in the dictionary (so the client can treat it as correct word that doesn't need suggestion). \n\nWe can't have exactly the same functionality here (since \"multi-word\" queries should be supported), but we can make SC return field \"spellingCorrect\" in case all words from the query exist in the dictionary. Otherwise, there is no way to know if spelling was correct or we should display suggestion.\n\nThere is a method in Lucene's SC to check if word exists in the index, so it's easy to check if word is correct. However, I'm also thinking of situation when we don't have just simple words in the query, for instance : \"toyata AND miles:[1 to 10000]\", we want to check just toyata in the index, and return suggestion \"toyota AND miles:[1 to 10000]\". Other query types which might pose a problem are:\n\n\tfuzzy query\n\twildcard query\n\tprefix query\n...\n\n "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12598801",
            "date": "2008-05-21T21:43:36+0000",
            "content": "Yes, I've actually run into that problem too. Do you think this is something that you will be able to solve? "
        },
        {
            "author": "Bojan Smid",
            "id": "comment-12598835",
            "date": "2008-05-21T22:42:25+0000",
            "content": "Sure. A quick fix can be done easily, but it probably wouldn't cover all possibilities, hence my post... "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12599166",
            "date": "2008-05-22T19:59:21+0000",
            "content": "OK, I'm working on this.  \n\nSome thoughts:\n1. Why is the initialization done in prepare?  Just to be a little more lazy than in init?\n\n2. In FieldSpellChecker, the getSuggestion method goes through and creates the suggested map, but then the loop over the entry set at the end only uses the value.   I think our response should return the associated correction with the original token. \n\n3. I'm working on the abstraction notion.  The goal is to have a common response, no matter the spell checker, so that we can plug and play spell checkers.  I hope to have a patch soon. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12599168",
            "date": "2008-05-22T20:10:34+0000",
            "content": "Grant, please hold on a bit. I'm working on the patch too and it has some refactorings which may make merging two patches difficult. I'll post my patch in a few minutes and then you can take over. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12599170",
            "date": "2008-05-22T20:13:00+0000",
            "content": "OK.  Kind of too late, but no worries, I will manage the merge, so just do what you think you need to do.\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12599171",
            "date": "2008-05-22T20:18:46+0000",
            "content": "Grant \u2013 please find my comments below:\n\n\n\tI had to move the init to prepare because there were issues in getting access to the IndexReader in inform() method. Please see http://www.nabble.com/Accessing-IndexReader-during-core-initialization-hangs-init-to17259235.html\n\tThe first getSuggestion method aims to return a single suggestion string by combining suggestions for all tokens in the query. It's not perfect but seems to work. This is used when spellcheck.count is missing or one. The second suggestSimilar method returns suggestions for each token and associated suggestion.\n\tThat would be nice to have!\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12599195",
            "date": "2008-05-22T21:28:23+0000",
            "content": "This patch contains the following changes:\n\n\n\tFixes bug reported by Oleg \u2013 Thanks to Bojan for this.\n\tthresholdTokenFrequency can be used to tweak the frequency of tokens being passed to spell check index. This is applied only for index type dictionaries.\n\tMoved getLines as an overloaded method to SolrResourceLoader.\n\tTo avoid having a dependency to Lucene 2.4 (trunk) code, I created a wrapper class for PlainTextDictionary which calls it's protected constructor PlainTextDictionary(Reader)\n\tUses Lucene's SpellChecker's overloaded suggestSimilar method which accepts the IndexReader as a param. This makes sure that when the query is present in the index, a different suggestion is not returned.\n\tImplements the onlyMorePopular only for dictionaries built from Solr fields\n\tImplements the extendedResults only for dictionaries built from Solr fields and only when spellcheck.count is greater than 1\n\tNo need to specify spellcheck.dictionary as a request parameter if only one dictionary is configured.\n\tAccuracy is configurable through solrconfig.xml\n\n\n\nStill to do:\n\n\tIt is possible to implement onlyMorePopular and extendedResults for dictionaries created from arbitary lucene indices too but I haven't looked into that yet.\n\tTests are missing\n\tAdd command to reload dictionaries\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12599202",
            "date": "2008-05-22T21:43:02+0000",
            "content": "Otis \u2013 Sorry, I missed your post earlier. I can't think of a use-case for adding frequency information to plain text files. Spell checker's utility comes from the fact that it can suggest keywords for which Solr can return documents. That is possible only when the tokens (or synonyms) are present in the Solr index. Plain text dictionaries will be used to add additional common keywords which may not be in the Solr fields used for suggestions but may be present in huge fields which you don't want to add to spell checker. For example, I may build my index only on vehicle brands but I may like to include terms such as \"cars\", \"manufacturer\", \"make\" from plain text files, which may be present in my huge default search field. Since the intent would be just to match some document with the given suggestion, frequency may not play a significant role here, IMHO. What do you think?\n\nBojan \u2013 I think we should include an \"exists\" flag in the response. As for your point of queries with non-simple tokens, we can introduce another param like \"spellcheck.q\" to which the application can set the simple query. End users almost never know that Solr is running behind the scenes and the Solr queries are constructed by the application itself which can send the simple query in this way. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12599222",
            "date": "2008-05-22T22:28:33+0000",
            "content": "Shalin \u2013 I think you are right.  I looked at SpellChecker again and see that the frequency in the main/searchable index is checked at \"suggest time\", regardless of what the source of dictionary words (index or file), so frequency will be accounted for even when words are loaded from plain-text dictionary files.\n\nUnless I'm still missing something, that means that \"onlyMorePopular\" can (or should!) be used even when words are loaded from plain-text dictionary files.  No? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12599403",
            "date": "2008-05-23T15:41:38+0000",
            "content": "Is the prepare thread-safe for dictionary creation?  Seems like there is a race-condition on the construction of the dictionaries.  I suppose we need a synchronize in there. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12599412",
            "date": "2008-05-23T16:08:20+0000",
            "content": "Grant \u2013 No, it is not thread-safe. Actually I wanted to put this initialization code in a inform method to avoid this situation. Since that did not work, I moved this into prepare method only as a stop gap arrangement. See http://www.nabble.com/Accessing-IndexReader-during-core-initialization-hangs-init-to17259235.html for details.\n\nI'd suggest doing the following:\n\n\tMove the initial dictionary creation into a inform method if someone with more knowledge about the SolrCore class can fix the issue I described in my mail.\n\tThe code in prepare can be used to reload dictionaries by specifying a request parameter (say spellcheck.rebuild=true)\n\tSince we're already using a ConcurrentHashMap, the above two things should take care of all thread-safety issues.\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12599426",
            "date": "2008-05-23T16:54:09+0000",
            "content": "Otis,\n\nWhat's the use case behind:\n\nOh, I see, you are reading field values from the index of the current core. I think that is fine, but wouldn't it also be good to be able to read field values from a vanilla Lucene index?\n\nSeems kind of strange based on what I know of index-based spelling, but I don't know everything about it. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12599475",
            "date": "2008-05-23T19:18:43+0000",
            "content": "WARNING:  This patch compiles ONLY.  I do NOT claim it is semantically equivalent to the earlier patches although that is my goal and I don't think I am far off.  I have not tested it in any way, shape or form.  I am only putting it up here as a first cut of the abstractions I have in mind, so please provide feedback based on that, especially in regards to the SolrSpellChecker class.  Most interesting, there, is the passing in of the IndexReader.  I know not all spellers are going to need the IndexReader, so ideally, it would be something that is passed in or set during the construction of the speller, but I don't think that will work, or at least I am not aware of how to make it work just yet.  \n\nMy next step is to add unit tests of the individual spell checkers and then the component itself. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12599476",
            "date": "2008-05-23T19:21:45+0000",
            "content": "Move spelling core classes out of component package into it's own package, similar to highlighting, as spelling is just as important.  Same caveats as last patch apply. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12599478",
            "date": "2008-05-23T19:25:18+0000",
            "content": "Also included in that last patch is a (proposed) sample configuration:\n\n\n<searchComponent name=\"spellcheck\" class=\"org.apache.solr.handler.component.SpellCheckComponent\">\n    <lst name=\"spellchecker\">\n      <str name=\"classname\">org.apache.solr.spelling.IndexBasedSpellChecker</str>\n        <lst name=\"dictionary\">\n          <str name=\"name\">default</str>\n          <str name=\"field\">word</str>\n          <str name=\"indexDir\">c:/temp/spellindex</str>\n        </lst>\n    </lst>\n    <lst name=\"spellchecker\">\n      <str name=\"classname\">org.apache.solr.spelling.FileBasedSpellChecker</str>\n      <lst name=\"dictionary\">\n        <str name=\"name\">external</str>\n        <str name=\"sourceLocation\">spellings.txt</str>\n        <str name=\"characterEncoding\">UTF-8</str>\n        <str name=\"spellcheckIndexDir \">./spellchecker</str>\n      </lst>\n\n    </lst>\n\n\n  </searchComponent>\n\n "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12599494",
            "date": "2008-05-23T20:22:20+0000",
            "content": "I'm still confused with some of the names in that config.\nindexDir looks like the path to the spellchecker index.  But there is also spellcheckInexDir.  Is there a functonal difference?\n\nRegarding the \"wouldn't it also be good to be able to read field values from a vanilla Lucene index?\" - the use case is that not all source indices should have to be Solr indices.  What if I have a vanilla Lucene index on the machine and I want the SCRH to build a SC index from that index's \"title\" field?  That is, I want the functionality of SCRH, but I don't have my Lucene index under Solr.  Is that doable? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12599496",
            "date": "2008-05-23T20:26:17+0000",
            "content": "\nindexDir looks like the path to the spellchecker index. But there is also spellcheckInexDir. Is there a functonal difference?\n\nGood point, I fix that.\n\n\nIs that doable?\n\nOf course it is, I just didn't know why you would want to.  I get the file based need, b/c that is where you can put overrides, but I just don't get the need for another index, since wouldn't it have to have the same frequencies, etc. to return appropriate suggestions?   "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12599498",
            "date": "2008-05-23T20:34:11+0000",
            "content": "I think the choice of \"appropriate suggestions\" should be left to the user of this service.  If it's easily doable, let's make it possible and put information about frequencies in an appropriate place. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12599558",
            "date": "2008-05-24T03:56:58+0000",
            "content": "Shalin/Grant:\n\nI think Bojan brings up some good questions:\nhttps://issues.apache.org/jira/browse/SOLR-572?focusedCommentId=12598752#action_12598752\n\nIt looks like the call to SpellChecker.exist(...) really got lost:\n$ curl --silent https://issues.apache.org/jira/secure/attachment/12382691/SOLR-572.patch | grep 'exist(' "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12599832",
            "date": "2008-05-26T12:39:53+0000",
            "content": "OK, this has some tests for the individual spell checkers.  Still haven't tested starting it up as an individual component in Solr.\n\nAlso, still needs a way to account for when the returned suggestion is the same word, thus indicating the word exists in the index. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12599833",
            "date": "2008-05-26T12:40:34+0000",
            "content": "Good stuff, but it ain't \"Major\" "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12599845",
            "date": "2008-05-26T14:18:48+0000",
            "content": "I have a few comments after a quick look at the patch\n\n\tLets make SolrSpellChecker keep the standard init method structure akin to NamedListInitializedPlugin. Let the build method return the dictionary name. In the current patch, even if build fails, the spell checker would get added to the map.\n\tI couldn't find where the SolrSpellChecker#build method is actually called apart from the tests.\n\tLets remove the SolrSpellChecker#getSuggestion(String query, IndexReader reader, boolean onlyMorePopular) method completely. The other getSuggestion method will be called with count=1 if count is absent in the query.\n\tRename AbstractLuceneSpellerSpellChecker to something shorter.\n\tI would very much like to keep short names instead of complete class names. We should not force the user to remember or copy-paste our long internal class names just because we wanted to keep things pluggable. Sane defaults maybe?\n\tThe configuration looks scary. There's no value added by the repeated spellchecker nodes. I propose the following syntax:\n\n<searchComponent name=\"spellcheck\" class=\"org.apache.solr.handler.component.SpellCheckComponent\">\n\t<lst name=\"dictionary\">\n\t  <!-- Optional, it is required when more than one dictionary is configured -->\n\t  <str name=\"name\">default</str>\n\t  <!-- The type is optional, defaults to IndexBasedSpellChecker -->\n\t  <str name=\"type\">org.apache.solr.spelling.IndexBasedSpellChecker</str>\n\t  <!-- Optional, if present, the following lucene index is used as source instead of Solr index  -->\n\t  <str name=\"sourceLocation\">c:/temp/myluceneindex</str>\n\t  <!--\n\t       Load tokens from the following field for spell checking, \n\t       analyzer for the field's type as defined in schema.xml are used\n\t  -->\n\t  <str name=\"field\">word</str>\n\t  <!-- Optional, by default use in-memory index (RAMDirectory) -->\n\t  <str name=\"spellCheckIndexDir\">c:/temp/spellindex</str>\n\t</lst>\n\t<lst name=\"dictionary\">\n\t  <str name=\"name\">external</str>\n\t  <str name=\"type\">org.apache.solr.spelling.FileBasedSpellChecker</str>\n\t  <str name=\"sourceLocation\">spellings.txt</str>\n\t  <!--\n\t       Optional, if provided the analyzers for the given fieldType would be used.\n\t       Otherwise, no analyzer at index-time and WhiteSpaceAnalyzer at query time is used.\n\t       This fieldType should be defined in schema.xml\n\t   -->\n\t  <str name=\"fieldType\">text</str>\n\t  <!-- Optional, defaults to platform encoding -->\n\t  <str name=\"characterEncoding\">UTF-8</str>\n\t  <str name=\"spellcheckIndexDir\">./spellchecker</str>\n\t</lst>\n  </searchComponent>\n\n\n\tLast but not the least, Grant, do you know of a freely available spell checker implementation that someone may want to plugin instead of the Lucene SpellChecker? In other words, is this a real use-case or something we're imagining up? If we don't know of something that can be used right now, maybe we're better off postponing this change until users really need it and ask for it. I don't like the complexity this feature is asking for.\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12599852",
            "date": "2008-05-26T15:04:39+0000",
            "content": "\nLets make SolrSpellChecker keep the standard init method structure akin to NamedListInitializedPlugin. Let the build method return the dictionary name. In the current patch, even if build fails, the spell checker would get added to the map.\nThe approach is to then use the build in the prepare method, much like the cmd=rebuild.  Thus, spelling index creation is much like in the RequestHandler mode and gets around the firstSearcher issue.  I am working on the integration into the Component at the moment, which is why you only see it in the tests.\n\nSo, I am not sure if this makes sense.  Right now, I have it so that we extract the necessary pieces in the init, but then they are applied during build.  I guess the question is what should happen if \"build\" fails?  Should we just remove that speller and log a warning?  Or should it throw an exception?  I am leaning towards the former.\n\n\nRename AbstractLuceneSpellerSpellChecker to something shorter.\nOK, I will try to think of something.\n\n\nLets remove the SolrSpellChecker#getSuggestion(String query, IndexReader reader, boolean onlyMorePopular) method completely. The other getSuggestion method will be called with count=1 if count is absent in the query.\nI was just thinking the same thing.  Done.\n\n\nThe configuration looks scary. There's no value added by the repeated spellchecker nodes. I propose the following syntax:\n\nAll Solr config's look scary to me! However...\n\nI can imagine an implementation that looks like:\n\n<lst name=\"spellchecker\">\n <str name=\"classname\">my.great.SpellChecker</str>\n</lst>\n\n\nI agree, however, we can flatten mine one level, but keep the name spellchecker instead of dictionary.  \n\nHere's an iteration:\n\n<searchComponent name=\"spellcheck\" class=\"org.apache.solr.handler.component.SpellCheckComponent\">\n    <lst name=\"defaults\">\n      <!-- omp = Only More Popular -->\n      <str name=\"sc.omp\">false</str>\n      <!-- exr = Extended Results -->\n      <str name=\"sc.exr\">false</str>\n      <!--  The number of suggestions to return -->\n      <str name=\"sc.cnt\">1</str>\n    </lst>\n    <lst name=\"spellchecker\">\n      <str name=\"classname\">org.apache.solr.spelling.IndexBasedSpellChecker</str>\n      <str name=\"name\">default</str>\n      <str name=\"field\">text</str>\n      <str name=\"indexDir\">c:/temp/spellindex</str>\n      \n    </lst>\n    <lst name=\"spellchecker\">\n      <str name=\"classname\">org.apache.solr.spelling.FileBasedSpellChecker</str>\n      <str name=\"name\">external</str>\n      <str name=\"sourceLocation\">spellings.txt</str>\n      <str name=\"characterEncoding\">UTF-8</str>\n      <str name=\"indexDir\">./spellchecker</str>\n    </lst>\n  </searchComponent>\n\n\n\n\nLast but not the least, Grant, do you know of a freely available spell checker implementation that someone may want to plugin instead of the Lucene SpellChecker? In other words, is this a real use-case or something we're imagining up? If we don't know of something that can be used right now, maybe we're better off postponing this change until users really need it and ask for it. I don't like the complexity this feature is asking for.\n\nYes, I have an immediate need for it.  The Lucene SpellChecker isn't all that good, IMO, and I want to offer something different without having to fork and have my own SpellChecker Component when the output is the same.    "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12599853",
            "date": "2008-05-26T15:15:13+0000",
            "content": "If this is a default component, how do you setup the field to be used for spelling?  Are you just using the default search field?  \nAlso, I don't think we should make it default, since there is this minor nit that it requires building the index first.  I suppose that could be done on the first time spellings are requested, but that seems like it could all of a sudden cause a much longer return.  By making it non-default, I think it forces the person doing the configuration to think more about the setup, since the setup of proper spelling is not trivial. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12599865",
            "date": "2008-05-26T16:17:35+0000",
            "content": "Grant, which spellchecker are you plugging in? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12599939",
            "date": "2008-05-27T01:10:52+0000",
            "content": "More tests, slight reworking of how response gets generated by using SpellingResult so that we can enforce a contract with whatever implementation of the SolrSpellChecker we have (NamedList is just too weakly typed to be effective for this.)\n\nIncorporated suggestions from Shalin on configuration and other pieces. \n\nTODO: more tests, Add easy to find \"exists\" functionality when the suggestion is the same as the token.  \n\nGetting closer to something to commit. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12600059",
            "date": "2008-05-27T07:39:39+0000",
            "content": "The configuration looks fine Grant. Yes, we don't need this as default. Default search fields are usually large, we don't need that overhead by default. The user can always enable and configure this when he needs it. Maybe we should add the sample configuration as a commented section in solrconfig.xml\n\n\n\tWe should change the query parameter to long names so that their purpose is easily understood. Names like \"sc.omp\" and \"sc.exr\" seem cryptic\n\tWe don't need the rebuild command since build and rebuild both do the same thing.\n\tAdd a optional spellcheck.q request parameter for passing in simple queries (to avoid the problem that Bojan pointed out)\n\n\n\nI'll give a patch shortly after making the above changes. Will also try to look into adding exists feature. "
        },
        {
            "author": "Bojan Smid",
            "id": "comment-12600065",
            "date": "2008-05-27T08:16:38+0000",
            "content": "Shalin, I'm not sure we really need spellcheck.q parameter. I think we should handle all queries in a similar way (both complex and simple queries): \n\n\n\tBreak each query into terms, and then for each term check if it was correctly spelled (with spellchecker.exist()). Some term types should be excluded from spell checking (range terms and other types I mentioned in the post above).\n\tIf all terms (which can be spell checked) in a query are correctly spelled, we put a flag correctlySpelled = true in the response, otherwise we put the flag to false and return suggestion (we change only terms for which spechecker.exist() returned false).\n\n\n\nWhat do you think of that? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12600068",
            "date": "2008-05-27T09:05:48+0000",
            "content": "Break each query into terms, and then for each term check if it was correctly spelled (with spellchecker.exist()). Some term types should be excluded from spell checking (range terms and other types I mentioned in the post above).\nWe should not try to do intelligent things which the user can easily do. It's difficult to extract terms which represent range terms, wildcards, fuzzy queries and boolean operators. We will need a parser to identify and remove these things correctly from the query which is not something we should be doing. Since the user always builds the q parameter, he can also build the spellcheck.q parameter if he chooses to do so.\n\nIf all terms (which can be spell checked) in a query are correctly spelled, we put a flag correctlySpelled = true in the response, otherwise we put the flag to false and return suggestion (we change only terms for which spechecker.exist() returned false).\nAgreed "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12600088",
            "date": "2008-05-27T11:32:50+0000",
            "content": "\n\nYeah, I was torn on this one, and am fine either way.  Most of Solr's  \nexisting params are quite short and cryptic.  I guess it is trying to  \nprevent the GET buffer length problem (does that still exist?), but I  \ndon't know.\n\n\nI was just trying to keep some common ground w/ the ReqHandler  \nversion, which uses rebuild, but I agree build is shorter and you  \ncan't rebuild something until you build it, right?\n\n\n\nNot sure I follow here, but I'll wait for your patch.\n\n\n\n\n\n\n\n "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12600111",
            "date": "2008-05-27T12:32:56+0000",
            "content": "Yes, the GET length limit is still with us, but it's 2K+ chars.  Here is info about IE7 and friends, for example: http://support.microsoft.com/kb/208427\nSo I think we still have room a bit of room there. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12600131",
            "date": "2008-05-27T14:06:49+0000",
            "content": "Changes:\n\n\tChanged request parameters to use long names\n\tRemoved the command syntax. Params are spellcheck.build=true or spellcheck.reload=true\n\tUses spellcheck.q if present, otherwise q parameter\n\tReturn correctlySpelled=true in result if all tokens in the input query are present in the index. Added a simple test for this change.\n\tRenamed indexDir in config to spellcheckIndexDir for clarity\n\tUpdated SpellCheckComponentTest and test solrconfig.xml for the above changes\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12600178",
            "date": "2008-05-27T16:25:09+0000",
            "content": "\nReturn correctlySpelled=true in result if all tokens in the input query are present in the index. Added a simple test for this change.\n\nIf I'm reading the code right, this only can be set if extendedResults is true since this is the only time there will be frequency information, yet, I believe with the upgrade to the latest Lucene spell checker that it now returns the original word as a suggestion if it is correctly spelled.\n\n\nRest of changes look good. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12600180",
            "date": "2008-05-27T16:26:02+0000",
            "content": "Some more tests for various edge cases on the Index based Speller. "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12600217",
            "date": "2008-05-27T17:47:29+0000",
            "content": "Did you guys change the required URL parameters structure? I am hitting the following URL: http://localhost:8983/solr/select/?q=pizza&spellcheck=true&spellcheck.dictionary=default and I am getting a nullpointer exception. The config is the one from the sample, and I am using the latest patch. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12600222",
            "date": "2008-05-27T18:04:02+0000",
            "content": "Did you issue a build command first?   Note, I haven't yet fully  \ntested in the Solr container, have been more focused on individual  \nunit tests.\n\nAlso, what's the NPE you are getting?\n\n\n\n\n "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12600269",
            "date": "2008-05-27T20:43:59+0000",
            "content": "I haven't applied/tried the latest patch yet, but maybe it's\nquicker/better to ask here.  I'm wondering/worried about the case\nwhere the input is a multi-term query string and a subset (e.g. 2 of 5\nterms) of the query terms is misspelled.\n\nFor example, what happens when the query is:\n\n\"london brigge is fallinge down\"\n(my 2 year old's current hit)\n\nIn this case the suggestions should be:\n\n\tbrigge => bridge\n\tfallinge => falling (or fall, more likely)\n\n\n\nIs there something in the response that will allow the client to\nfigure out the positioning of the spelling suggestions and piece\ntogether the ideal alternative query, in this case \"london bridge is\nfalling/fall down\"?\n\nIdeally, the client could piece the new query string, so that it can, for example, italicize the misspelled words (see Google's DYM).  If the current SCRH returns the final corrected string, e.g. \"london bridge is falling down\" the client has no easy/accurate way of figuring out what was changed, I think.  If the SCRH returned some mark-up that told the client which word(s) changed, then the client could do something with those changed words, e.g. \"london bridge\n{was:brigge}\n....\"\n\nOr, if that has problems, maybe each word should be returned separately and sequentially:\n\n<word=\"london\"/> <!-- unchanged -->\n<word=\"brigge\">bridge</word>\n\nor maybe with offset info:\n\n<word=\"london\" offset=\"0\"/> <!-- unchanged -->\n<word=\"brigge\" offset=\"6\">bridge</word>\n\nThoughts? "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12600272",
            "date": "2008-05-27T20:53:08+0000",
            "content": "Hello. I am hitting http://localhost:8983/solr/select/?q=pizza&spellcheck=true&spellcheck.dictionary=default&spellcheck.build=true when trying to build the dictionary. My config looks this this: \n<searchComponent name=\"spellcheck\" class=\"org.apache.solr.handler.component.SpellCheckComponent\">\n    <lst name=\"defaults\">\n      <!-- omp = Only More Popular -->\n      <str name=\"spellcheck.onlyMorePopular\">false</str>\n      <!-- exr = Extended Results -->\n      <str name=\"spellcheck.extendedResults\">false</str>\n      <!--  The number of suggestions to return -->\n      <str name=\"spellcheck.count\">1</str>\n    </lst>\n    <lst name=\"spellchecker\">\n      <str name=\"classname\">org.apache.solr.spelling.IndexBasedSpellChecker</str>\n      <str name=\"name\">default</str>\n      <str name=\"fieldType\">text_ws</str>\n      <str name=\"indexDir\">/usr/local/apache/lucene/solr1home/solr/data/spellchecker</str>\n\n    </lst>\n    <lst name=\"spellchecker\">\n      <str name=\"classname\">org.apache.solr.spelling.FileBasedSpellChecker</str>\n      <str name=\"name\">external</str>\n      <str name=\"sourceLocation\">spellings.txt</str>\n      <str name=\"fieldType\">text_ws</str>\n      <str name=\"characterEncoding\">UTF-8</str>\n      <str name=\"indexDir\">/usr/local/apache/lucene/solr1home/solr/data/spellchecker</str>\n    </lst>\n</searchComponent>\n\n\nAnd the NPE is:\n\nSEVERE: java.lang.NullPointerException\n        at org.apache.solr.util.HighFrequencyDictionary.<init>(HighFrequencyDictionary.java:48)\n        at org.apache.solr.spelling.IndexBasedSpellChecker.loadLuceneDictionary(IndexBasedSpellChecker.java:103)\n        at org.apache.solr.spelling.IndexBasedSpellChecker.build(IndexBasedSpellChecker.java:84)\n        at org.apache.solr.handler.component.SpellCheckComponent.prepare(SpellCheckComponent.java:133)\n        at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:132)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:125)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:965)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:339)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:274)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:175)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:844)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:583)\n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:447)\n        at java.lang.Thread.run(Thread.java:619) "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12600275",
            "date": "2008-05-27T21:00:00+0000",
            "content": "I'm working on it.  Will have a new patch soon.\n\n\n\n\n\n "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12600277",
            "date": "2008-05-27T21:06:41+0000",
            "content": "Is it an actual error, or was I missing something? "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12600279",
            "date": "2008-05-27T21:08:29+0000",
            "content": "In response to Otis, I don't think each word should be returned individually. In fact it should probably return the entire phrase, with the suggestions inserted. I believe that is what google does. Although I guess if the words are returned sequentially, you can easily reform the phrase, so that works too... "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12600283",
            "date": "2008-05-27T21:32:00+0000",
            "content": "\n\n\nAll you see from Googs is their frontend, so who knows what their  \nspell checker does.  I think we should return the words individually,  \nthe application is responsible for doing the sewing together of the  \nnew string, IMO.\n\n\n "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12600284",
            "date": "2008-05-27T21:36:54+0000",
            "content": "Should we return suggestions only for the misspelled words, or should we echo the correctly spelled ones as well? "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12600294",
            "date": "2008-05-27T22:28:38+0000",
            "content": "Right, Google only shows you the final output, not what they do in the backend.\nBut the fact that they italicize misspelled words tells us they have a mechanism that allows the front end to identify them.\nSo I think our task here is to figure out the best/easiest way for the client to identify misspelled words and offer the alternative query to the end user.\n\nI think what I outlined above will do that for us:\n\n\toutput all words sequentially\n\tmark the words that are misspelled - it may be best to return the original word plus corrected word:\n\n\n\n<word=\"london\"/> <!-- unchanged -->\n<word=\"brigge\">bridge</word>\n\nor maybe with offset info:\n\n<word=\"london\" offset=\"0\"/> <!-- unchanged -->\n<word=\"brigge\" offset=\"6\">bridge</word>\n\nIt's also fine to (also) return the final corrected string that doesn't mark the corrected words in any way, and let the \"lazy\" clients just use that.\n\nGrant or Shalin, will either of you be adding this? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12600320",
            "date": "2008-05-28T00:17:46+0000",
            "content": "\nGrant or Shalin, will either of you be adding this?\nYes, I am working on it. "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12600323",
            "date": "2008-05-28T00:23:54+0000",
            "content": "I am still confused about my NPE. Was that a config issue on my part, or was it a bug? The way Grant said he was working on it, I assumed that it was a bug  "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12600326",
            "date": "2008-05-28T00:51:49+0000",
            "content": "Your \"field\" is null for your Lucene configuration.  You need to  \nspecify:\n\n<str name=\"field\">fieldName</str>\n\nYou have fieldType instead.\n\n-Grant\n\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12600457",
            "date": "2008-05-28T12:44:22+0000",
            "content": "OK, here's a start on the token stuff.  \n\nNOTE:  This currently does not work!!!!!!!!  The tests do not pass and I haven't fully implemented the SpellingQueryConverter.  I have a few other things to attend to for a couple of days, so I wanted to get this up there as a starting point for others to look at and give comments on the approach for when I can get back to it in a day or two (but feel free to take it up, too).\n\nThe basic gist of it is to hand off analysis to a pluggable piece called the SpellingQueryConverter, which produces a collection of Tokens (which contain offsets into the original query String).\n\nI'm still torn on how to best achieve this.  In some sense, there has to be some interaction with some form of a Query Parser.  I think it needs to be a Query Parser that has the source field's Analyzer as the Analyzer for doing the parsing.  This way, the output Query is properly analyzed and we can then extract just those \"spellcheckable\" terms from it (i.e. TermQuery, PhraseQuery, ????)\n\nDoes this make sense? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12600515",
            "date": "2008-05-28T15:46:28+0000",
            "content": "OK, I changed the SpellingQueryConverter to not be dependent on the Query, instead opting for a simple regex approach.  It is by no means perfect, but I think it is an improvement.  All the tests now pass.  See the test solrconfig for how to configure.\n\nThis time, I mean it, I won't be working on this for a couple of days more or less, depending on other tasks  "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12600520",
            "date": "2008-05-28T16:12:22+0000",
            "content": "Grant, unless I'm mistaken, the reason to add spellcheck.q parameter was to avoid the tedious query parsing logic that may be needed to extract \"spellcheckable\" terms from the q parameter. Do we really need to do this? All the extra things in the q parameter are usually added by the frontend itself, isn't it? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12600524",
            "date": "2008-05-28T16:38:34+0000",
            "content": "\nGrant, unless I'm mistaken, the reason to add spellcheck.q parameter was to avoid the tedious query parsing logic that may be needed to extract \"spellcheckable\" terms from the q parameter. Do we really need to do this? All the extra things in the q parameter are usually added by the frontend itself, isn't it?\n\nIs that practical?  How would an application even know how to generate spellcheck.q without parsing, etc.?   I think the component should just work on the input query.  I guess I hadn't really thought about the need for spellcheck.q before, but now that you put it in that light, I am not sure I see the need for it.    \n\nI don't think all the extra things are necessarily added by the application.  Users can input range queries, etc.  The point is, it all depends on the application.\n\nAt any rate, it is trivial to override the SpellingQueryConverter to not do the original REGEX and just apply the analyzer to produce the tokens.  I suppose, we could offer two converters, one w/ the regex, and one without, or it could just have a flag. "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12600563",
            "date": "2008-05-28T18:36:26+0000",
            "content": "I still have some issues. Here is my config:\n  <lst name=\"spellchecker\">\n      <str name=\"classname\">org.apache.solr.spelling.FileBasedSpellChecker</str>\n      <str name=\"name\">external</str>\n      <str name=\"sourceLocation\">/usr/local/apache/lucene/solr1home/conf/spellings.txt</str>\n      <str name=\"field\">word</str>\n      <str name=\"characterEncoding\">UTF-8</str>\n      <!-<str name=\"indexDir\">/usr/local/apache/lucene/solr1home/solr/data/spellchecker</str>->\n    </lst>\nBut why do I need a field for a filebased dictionary? Also is the correct way to call this URL: http://wil1devsch1.cs.tmcs:8983/solr/select/?q=pizza&spellcheck=true&spellcheck.dictionary=external&spellcheck.builld=true ? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12600582",
            "date": "2008-05-28T19:36:48+0000",
            "content": "Oleg \u2013 You shouldn't need \"field\" for a file-based dictionary. \"fieldType\" is optional for file-based dictionary. \"field\" is necessary only when you're using a IndexBasedSpellChecker. If you're running into a problem it's a bug. Except for the double L in spellcheck.build in your URL, everything else looks Ok. "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12600590",
            "date": "2008-05-28T20:02:43+0000",
            "content": "Here is what I am getting (using yesterday's patch):\n\nHTTP Status 500 - null java.lang.NullPointerException at org.apache.lucene.index.Term.<init>(Term.java:39) at org.apache.lucene.index.Term.<init>(Term.java:36) at org.apache.solr.spelling.AbstractLuceneSpellChecker.getSuggestions(AbstractLuceneSpellChecker.java:67) at org.apache.solr.handler.component.SpellCheckComponent.process(SpellCheckComponent.java:160) at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:153) at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:125) at org.apache.solr.core.SolrCore.execute(SolrCore.java:965) at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:339) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:274) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:175) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286) at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:844) at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:583) at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:447) at java.lang.Thread.run(Thread.java:619) "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12601031",
            "date": "2008-05-30T06:32:02+0000",
            "content": "We must consider committing a basic version of spellchecker without the intelligent query parsing etc. Most of the users need will be met . Adding enhancements later is not a bad idea. (as long as we are not breaking backward compatibility)\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12601057",
            "date": "2008-05-30T09:46:55+0000",
            "content": "Oleg, please try this patch. There was a bug in the previous patch which tried to use \"field\" for suggestions even when it was null. That is why it gave a NullPointerException with FileBasedSpellChecker "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12601081",
            "date": "2008-05-30T10:56:31+0000",
            "content": "I had missed the src/test/test-files/spellings.txt in the previous patch so tests were failing. This patch adds it back. "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12601256",
            "date": "2008-05-30T21:12:58+0000",
            "content": "I installed the latest patch. Still getting a NPE. Here is my config:\n\n<searchComponent name=\"spellcheck\" class=\"org.apache.solr.handler.component.SpellCheckComponent\">\n    <lst name=\"defaults\">\n      <!-- omp = Only More Popular -->\n      <str name=\"spellcheck.onlyMorePopular\">false</str>\n      <!-- exr = Extended Results -->\n      <str name=\"spellcheck.extendedResults\">false</str>\n      <!--  The number of suggestions to return -->\n      <str name=\"spellcheck.count\">1</str>\n    </lst>\n\n    <lst name=\"spellchecker\">\n      <str name=\"classname\">org.apache.solr.spelling.FileBasedSpellChecker</str>\n      <str name=\"name\">external</str>\n      <str name=\"sourceLocation\">spellings.txt</str>\n      <str name=\"characterEncoding\">UTF-8</str>\n      <str name=\"fieldType\">text_ws</str>\n      <str name=\"indexDir\">/usr/local/apache/lucene/solr2home/solr/data/spellIndex</str>\n    </lst>\n  </searchComponent>\n\n\nHere is the URL I am hitting: http://localhost:8983/solr/select/?q=pizza&spellcheck=true&spellcheck.dictionary=external&spellcheck.build=true\n\nHere is the error:\n\nHTTP Status 500 - null java.lang.NullPointerException at org.apache.lucene.index.Term.<init>(Term.java:39) at org.apache.lucene.index.Term.<init>(Term.java:36) at org.apache.lucene.search.spell.SpellChecker.suggestSimilar(SpellChecker.java:228) at org.apache.solr.spelling.AbstractLuceneSpellChecker.getSuggestions(AbstractLuceneSpellChecker.java:71) at org.apache.solr.handler.component.SpellCheckComponent.process(SpellCheckComponent.java:177) at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:153) at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:125) at org.apache.solr.core.SolrCore.execute(SolrCore.java:965) at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:339) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:274) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:175) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286) at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:844) at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:583) at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:447) at java.lang.Thread.run(Thread.java:619)\n\nspellings.txt is in my solr/home/conf. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12602645",
            "date": "2008-06-05T13:41:20+0000",
            "content": "One thing I haven't quite settled in my mind is the use of the File based spell checker.  It seems to me, that the use case for this is as an override where one feels the index based spelling is not correct.  Is that right?  Or am I missing something?\n\nIf it is the case, shouldn't we allow the option, at least, of it truly acting as an override?  Currently, the only way to get at it is by passing the dictionary name as the param.  The only way I can see this as useful is if you are making several round trips to the server, which means you might as well be using a request handler and not a search component.\n\nThoughts? "
        },
        {
            "author": "Bojan Smid",
            "id": "comment-12602651",
            "date": "2008-06-05T14:04:28+0000",
            "content": "File based spell checker would probably be used in cases when Solr index is too small or too young. So a user would compile a dictionary file (for instance, UNIX words file) and use it as a dictionary. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12602654",
            "date": "2008-06-05T14:17:11+0000",
            "content": "\nFile based spell checker would probably be used in cases when Solr index is too small or too young. So a user would compile a dictionary file (for instance, UNIX words file) and use it as a dictionary.\n\nBut how is it useful to return results that aren't in the index?  It's not like querying on them results in anything useful.  Seems to me, that in this case, you just need to rebuild your dictionary on a regular basis.  Or is it that people are using Solr as a spelling server?\n\nNow, I can see it as an override situation.  i.e. one wishes to override certain results from the index based one with ones that are in known to be in the dictionary, but are lower down. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12602687",
            "date": "2008-06-05T15:44:42+0000",
            "content": "Oleg,\n\nCan you try specifying a field value anyway for your bug up above?  I think this is actually a bug in the Lucene Spell checker.  Namely, the docs say that the field value can be null, but, it is trying to construct a Term, which requires a non-null field name.\n\nJust give it the name \"word\", perhaps "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12602696",
            "date": "2008-06-05T16:22:47+0000",
            "content": "Grant, I think it's better to think of people using Solr+SCRH as a (generic) spellchecker service, not necessarily something that absolutely has to tie to a specific index and thus make only suggestions that result in hits.\n\nAnother use case is where Solr is used with indices that are not indices for a narrow domain or that don't have nice, clean, short fields that can be used for populating the SC index.  For example, if the index consists of a pile of web pages, I don't think I'd want to use their data (not even their titles) to populate the SC index.  I'd really want just a plain dictionary-powered SCRH. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12602706",
            "date": "2008-06-05T16:44:45+0000",
            "content": "Grant \u2013 The exception is happening because the SpellCheckComponent always passes Solr's own IndexReader when calling the AbstractLuceneSpellChecker#getSuggestions method even when the underlying spell checker is a FileBasedSpellChecker. In that case, since a non-null IndexReader is passed onto Lucene, it tries to create a term on the null field name. That is when the NullPointerException comes up.\n\nAnother problem will occur when using IndexBasedSpellChecker with an arbitary Lucene index, because then too, the Solr's IndexReader would be passed to Lucene SpellChecker instead of the actual index's reader.\n\nI think a possible solution can be to add another abstract method with the same signature as Lucene's SpellChecker to the AbstractLuceneSpellChecker and let each sub-class get suggestions on it's own. That way FileBasedSpellChecker will pass the correct IndexReader or a null IndexReader into Lucene appropriately. The AbstractLuceneSpellChecker#getSuggestion will just call the underlying suggest method, get the String[] back and process as it does right now. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12602707",
            "date": "2008-06-05T16:51:12+0000",
            "content": "OK, here's another crack at it.  I think I fixed the field issue Oleg was seeing (but haven't fully tested that) and I have it up and running in the Solr example.  After indexing the example docs there, try something like:\n\nhttp://localhost:8983/solr/spellCheckCompRH/?q=iPoo+text:sola&version=2.2&start=0&rows=10&indent=on&spellcheck=true&spellcheck.build=true\n\n\nto build it and spell check the query.\n\nI also have, what I think is a good compromise on spell checking the CommonParams.Q and the Spellcheck.Q, namely, the latter just uses a whitespace tokenizer to create the tokens.\n\nI am also thinking of adding a \"collate\" functionality, which would take the top suggestions and splice them back into the original string, as this seems like somehting many apps would like to have. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12602712",
            "date": "2008-06-05T17:04:48+0000",
            "content": "By \"collate\" you mean that the SCRH would not only return suggestions/corrections for individual token, but it would also try to glue together an already corrected query string based on its suggestions?\n\nExample:\nQuery: cogito ega sum\n\nSCRH returns this correction:\nerga -> ergo\n\nBut also tries to give you the whole thing corrected:\ncogito ergo sum\n\nThat?  Sounds useful - less work for the client app, should the app developers decide that SCRH's collated suggestions are what they would have to do themselves anyway. "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12602743",
            "date": "2008-06-05T18:41:29+0000",
            "content": "Hey guys. Installed the latest patch. Old problem is still there. For example if I do q=pizzzzza I get:\n\n<lst name=\"spellcheck\">\n\n\t<lst name=\"suggestions\">\n\n\t<lst name=\"pizzza\">\n<int name=\"numFound\">1</int>\n<int name=\"startOffset\">0</int>\n<int name=\"endOffset\">6</int>\n\n\t<arr name=\"suggestion\">\n<str>pizza</str>\n</arr>\n</lst>\n</lst>\n</lst>\n\nWhich is good. Then I do q=pizza (pizza is in the dictionary)\n\nlst name=\"spellcheck\">\n\n\t<lst name=\"suggestions\">\n\n\t<lst name=\"pizza\">\n<int name=\"numFound\">1</int>\n<int name=\"startOffset\">0</int>\n<int name=\"endOffset\">5</int>\n\n\t<arr name=\"suggestion\">\n<str>plaza</str>\n</arr>\n</lst>\n</lst>\n</lst>\n\nI don't think it should give me that suggestion. If a word is in the dictionary it should not give any suggestions. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12602745",
            "date": "2008-06-05T18:49:12+0000",
            "content": "\nGrant - The exception is happening because the SpellCheckComponent always passes Solr's own IndexReader when calling the AbstractLuceneSpellChecker#getSuggestions method even when the underlying spell checker is a FileBasedSpellChecker. In that case, since a non-null IndexReader is passed onto Lucene, it tries to create a term on the null field name. That is when the NullPointerException comes up.\n\nYep, I think I fixed this piece.  See also LUCENE-1299\n\n\nI think a possible solution can be to add another abstract method with the same signature as Lucene's SpellChecker to the AbstractLuceneSpellChecker and let each sub-class get suggestions on it's own. That way FileBasedSpellChecker will pass the correct IndexReader or a null IndexReader into Lucene appropriately. The AbstractLuceneSpellChecker#getSuggestion will just call the underlying suggest method, get the String[] back and process as it does right now.\n\nNot sure I follow the solution (I understand the problem)  Which signature are you suggesting? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12602808",
            "date": "2008-06-05T21:58:17+0000",
            "content": "OK, I think this one is pretty good.  I added a test for the alternate location piece.  I think I also fixed the issues w/ the wrong IndexReader being passed around.  \n\nI didn't implement the collate thing yet, but I think that can be handled as a separate patch. "
        },
        {
            "author": "Mike Klaas",
            "id": "comment-12602828",
            "date": "2008-06-05T22:54:15+0000",
            "content": "[quote]Another use case is where Solr is used with indices that are not indices for a narrow domain or that don't have nice, clean, short fields that can be used for populating the SC index. For example, if the index consists of a pile of web pages, I don't think I'd want to use their data (not even their titles) to populate the SC index. I'd really want just a plain dictionary-powered SCRH.[/quote]\n\nIt works great, actually.  That was you get all the abbreviations, jargon, proper names, etc.   Thresholding help prevent most of the cruft from appearing in the index. "
        },
        {
            "author": "Swarag Segu",
            "id": "comment-12602856",
            "date": "2008-06-05T23:36:35+0000",
            "content": "Hey Guys,\nI installed the latest patch and it gives me compile errors :\n\ncompile:\n    [mkdir] Created dir: C:\\Documents and Settings\\Swarag Segu\\workspace\\solrSrc\\build\\core\n    [javac] Compiling 324 source files to C:\\Documents and Settings\\Swarag Segu\\workspace\\solrSrc\\build\\core\n    [javac] C:\\Documents and Settings\\Swarag Segu\\workspace\\solrSrc\\src\\java\\org\\apache\\solr\\spelling\\FileBasedSpellChecker.java:97: cannot find symbol\n    [javac] symbol  : variable MaxFieldLength\n    [javac] location: class org.apache.lucene.index.IndexWriter\n    [javac]                 true, IndexWriter.MaxFieldLength.UNLIMITED);\n    [javac]                                  ^\n    [javac] C:\\Documents and Settings\\Swarag Segu\\workspace\\solrSrc\\src\\java\\org\\apache\\solr\\spelling\\FileBasedSpellChecker.java:96: internal error; cannot instantiate org.apache.lucene.index.IndexWriter.<init> at org.apache.lucene.index.IndexWriter to ()\n    [javac]         IndexWriter writer = new IndexWriter(ramDir, fieldType.getAnalyzer(),\n    [javac]                              ^\n    [javac] Note: Some input files use or override a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] Note: Some input files use unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n    [javac] 2 errors\n\n\nAm I missing something?\nThanks,\nSwarag. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12602863",
            "date": "2008-06-06T00:18:49+0000",
            "content": "What version of Lucene do you have in your lib directory?  Try svn up  \nfrom the root of Solr trunk.\n\n\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12603028",
            "date": "2008-06-06T12:42:32+0000",
            "content": "Small mod to move name and name init up to the SolrSpellChecker abstract class, since name is common to all spellers. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12603055",
            "date": "2008-06-06T14:23:03+0000",
            "content": "Removes \"unmodifiableMap\" factor from the suggestions/token freqs.  Rethinking this, I think it is reasonable to think that someone would want to modify these (or insert directly) "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12603070",
            "date": "2008-06-06T15:06:45+0000",
            "content": "Do these latest patches require Lucene 2.4? Would it be better to stay with 2.3.1? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12603075",
            "date": "2008-06-06T15:18:59+0000",
            "content": "\nDo these latest patches require Lucene 2.4? Would it be better to stay with 2.3.1?\n\nThey require what is checked into Solr's lib directory, which is Lucene's trunk as of yesterday.  There are actually a few changes in Lucene's spell checker that I think are worth having in 2.4.  Additionally, I think we will want LUCENE-1297 before we are through, which is probably another configuration item.  However, that can be added later, unless Otis commits it fairly soon. "
        },
        {
            "author": "Swarag Segu",
            "id": "comment-12603220",
            "date": "2008-06-06T23:14:50+0000",
            "content": "Hey guys. Installed the latest patch. Old problem is still there. For example if I do q=pizzzzza I get:\n\n<lst name=\"spellcheck\">\n\n<lst name=\"suggestions\">\n\n<lst name=\"pizzza\">\n<int name=\"numFound\">1</int>\n<int name=\"startOffset\">0</int>\n<int name=\"endOffset\">6</int>\n\n<arr name=\"suggestion\">\n<str>pizza</str>\n</arr>\n</lst>\n</lst>\n</lst>\n\nWhich is good. Then I do q=golf (golf is in the dictionary)\n\nlst name=\"spellcheck\">\n\n<lst name=\"spellcheck\">\n\u2212\n\t<lst name=\"suggestions\">\n\u2212\n\t<lst name=\"golf\">\n<int name=\"numFound\">1</int>\n<int name=\"startOffset\">0</int>\n<int name=\"endOffset\">4</int>\n\u2212\n\t<arr name=\"suggestion\">\n<str>roof</str>\n</arr>\n</lst>\n</lst>\n</lst>\n\nI don't think it should give me that suggestion. If a word is in the dictionary it should not give any suggestions. Am I right? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12604052",
            "date": "2008-06-10T21:46:02+0000",
            "content": "\nI don't think it should give me that suggestion. If a word is in the dictionary it should not give any suggestions. Am I right?\n\nPossibly.  I think it should give a better suggestion if one exists (i.e. more frequent) but otherwise, yes, it shouldn't give any suggestion.   For your example, I would argree that it should not return a suggestion (assuming golf is in the dictionary).  For example, the index could contain the words gilf and golf, with gilf having a freq. of 1 and golf having a freq of 100000.  If the user enters gilf, I think it is reasonable to assume that the suggestion should be golf, even though gilf exists.\n\nNot saying this is supported yet, or anything, but just laying out the case. "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12604056",
            "date": "2008-06-10T21:59:46+0000",
            "content": "I think that lower frequency suggestions should be optional. Some users might only want to offer suggestions for misspelled words (words not in the dictionary). Would it be hard to check if the query term exists in the dictionary before returning a suggestion? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12604060",
            "date": "2008-06-10T22:08:20+0000",
            "content": "\nWould it be hard to check if the query term exists in the dictionary before returning a suggestion?\nI'd have to double check, but I think the Lucene SC already does this in some cases (onlyMorePopular????)   "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12604062",
            "date": "2008-06-10T22:12:05+0000",
            "content": "I think the frequency awareness may be interesting.  What happens if \"gilf\" has a frequency of 95K and \"golf\" a freq of 100K?  Do we need this to become a SCRH config setting expressed as a percentage? (e.g. \"Show alternative word suggestions even if the input word exists in the index iff freq(input word)/freq(suggested word)*100 < N%?) "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12604101",
            "date": "2008-06-11T01:29:05+0000",
            "content": "Adds collation\nSlight change in SpellingResult results to take advantage of a LinkedHashMap and to explicitly state in the contract that spelling suggestions are in order by best suggestion first.\n\nAlso added some more javadocs.  Getting much closer.  I'd like to see LUCENE-1297 addressed and committed so it could be used in the Lucene SCs.\n\nI've used this API to implement my own spell checker, too, so I'm pretty happy w/ the API if others are.  I'd like to commit in the next week or so, so if people can check it out, kick the tires, that would be great.\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12604248",
            "date": "2008-06-11T15:16:01+0000",
            "content": "Minor change to only return the collation if it is different from the original "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12604504",
            "date": "2008-06-12T12:20:30+0000",
            "content": "Make getSpellChecker protected, add in JMX Stuff.  Handle if the SpellingResult is null "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12604617",
            "date": "2008-06-12T19:16:05+0000",
            "content": "It doesn't appear that you can get both extendedResults and count > 1.  With the below URL, I get 1 suggestion for each misspelled term regardless of the value of spellcheck.count.  If I set spellcheck.extendedResults=false, then I get the requested three suggestions for each term.\n\n\n/solr/spellCheckCompRH/?q=waz+designatd+two+bee+Arvil+25+bye+Pres.+it+waz&version=2.2&start=0&rows=2&indent=on&spellcheck=true&fl=title,url,id,categories,score&hl=on&hl.fl=body&qt=dismax&spellcheck.extendedResults=true&spellcheck.count=3\n\n "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12604639",
            "date": "2008-06-12T20:47:26+0000",
            "content": "the spell checker component handling build/reload seems highly awkward to me.   suggestion component really should just do that... and wrap the other operations as a /spellchecker/rebuild kinda thing and not even necessarily componentize those operations since they don't really necessarily need to be hooked together with other operations as a single request.\n\nanyway, just the overloading of a \"component\" to do managerial operations seems awkward.  food for thought.  not a -1 kinda thing though. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12604818",
            "date": "2008-06-13T12:08:50+0000",
            "content": "\nthe spell checker component handling build/reload seems highly awkward to me. suggestion component really should just do that... and wrap the other operations as a /spellchecker/rebuild kinda thing and not even necessarily componentize those operations since they don't really necessarily need to be hooked together with other operations as a single request.\n\nI've thought about a bit, too, as it bothers me, too, but I think the initialization, etc. gets a bit tricky, like all Solr initialization.  Not sure what to do. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12604830",
            "date": "2008-06-13T12:37:38+0000",
            "content": "Sean,\n\nI see the issue and am working on it.  Good catch.  I'll have a patch shortly. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12604885",
            "date": "2008-06-13T13:29:45+0000",
            "content": "Fixes Sean's issue w/ extended results.  \n\nAlso, slightly modified the extended results results.  See the \n\ntestExtendedResultsCount()\n\n\n\nin SpellCheckComponentTest for the new format.  Basically, though it tries to normalize the map entries so that one can ask for specific things by name,. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12605293",
            "date": "2008-06-16T14:01:10+0000",
            "content": "OK, I'd like to commit this tomorrow or Wednesday.  I am going to open another issue to bring in LUCENE-1297 to the configuration "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12605367",
            "date": "2008-06-16T18:32:17+0000",
            "content": "For those who are just casually following this issue, is there a good summary of current  input options and example output? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12605635",
            "date": "2008-06-17T15:31:18+0000",
            "content": "A few questions/comments:\n\n\n\tWhy is a WhiteSpaceTokenizer being used for tokenizing the value for a spellcheck.q parameter? Wouldn't it be more correct to use the query analyzer if the index is being built from a Solr field?\n\tThe above argument also applies to queryAnalyzerFieldType which is being used for QueryConverter.\n\tI see that we can specify our own query converter through the queryConverter section in solrconfig.xml. But the SpellCheckComponent uses SpellingQueryConverter directly instead of an interface. We should add a QueryConvertor interface if this needs to be pluggable.\n\tIf name is omitted from two dictionaries in solrconfig.xml then both get named as Default from the SolrSpellChecker#init method and they overwrite each other in the spellCheckers map\n\tHow about building the index in the inform() method? I understand that the users can build the index using spellcheck.build=true and they can also use QuerySenderListener to build the index but this limits the user to use FSDirectory because if we use RAMDirectory and solr is restarted, the QuerySenderListener never fires and spell checker is left with no index. It's not a major inconvenience to use FSDirectory always but then RAMDirectory doesn't bring much to the table.\n\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12605645",
            "date": "2008-06-17T16:12:57+0000",
            "content": "\nWhy is a WhiteSpaceTokenizer being used for tokenizing the value for a spellcheck.q parameter? Wouldn't it be more correct to use the query analyzer if the index is being built from a Solr field?\n\nThe above argument also applies to queryAnalyzerFieldType which is being used for QueryConverter\n\nMy understanding was that the sc.q parameter was already analyzed and ready to be checked, thus all it needed was a conversion to tokens.  As for the queryAnalyzerFieldType, that assumes the implementation is the IndexBasedSpellChecker or some other field based one that the SpellCheckComponent doesn't have access to, thus my reasoning that it needs to be handled separately and explicitly, which is why it isn't a part of the spellchecker configuration.\n\n \nI see that we can specify our own query converter through the queryConverter section in solrconfig.xml. But the SpellCheckComponent uses SpellingQueryConverter directly instead of an interface. We should add a QueryConvertor interface if this needs to be pluggable.\n\nI thought about making it an abstract base class, but in my mind it is really easy to override the SpellingQueryConverter and the component should know how to deal with it.\n\n \nIf name is omitted from two dictionaries in solrconfig.xml then both get named as Default from the SolrSpellChecker#init method and they overwrite each other in the spellCheckers map\n\nHmm, not good.  I will fix.\n\n\nHow about building the index in the inform() method? I understand that the users can build the index using spellcheck.build=true and they can also use QuerySenderListener to build the index but this limits the user to use FSDirectory because if we use RAMDirectory and solr is restarted, the QuerySenderListener never fires and spell checker is left with no index. It's not a major inconvenience to use FSDirectory always but then RAMDirectory doesn't bring much to the table.\n\nI think this gets back to our early discussions about it not working in inform b/c we don't have the reader at that point, or something like that.  I really don't know the right answer, but do feel free to try it out.  I do think it belongs in inform, but not sure if Solr is ready at that point.  As for the QuerySenderListener, seems like it should fire if it is restarted, but I admit I don't know a whole lot about that functionality.   "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12605661",
            "date": "2008-06-17T17:09:24+0000",
            "content": "Fix for the default name issue, add a test for it. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12605708",
            "date": "2008-06-17T20:11:54+0000",
            "content": "Thought some more about the comment about the QueryConverter, and decided to abstract it as Shalin suggests.   "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12605902",
            "date": "2008-06-18T09:41:04+0000",
            "content": "Changes\n\n\n\tMoved Analyzer from AbstractLuceneSpellChecker to SolrSpellChecker since some form of query-time analysis would probably be needed for all spell checker implementations. Added a getQueryAnalyzer() method in SolrSpellChecker.\n\tValue specified for spellcheck.q is analyzed using the query analyzer for the dictionary as per the config (using the SolrSpellChecker.getQueryAnalyzer). The value for \"q\" will continue to be analyzed by QueryConvertor.\n\tRemoved the EncodedTextDictionary class. Now that we're using the lucene-2.4 spellchecker, it is no longer needed because the previously protected constructor of PlainTextDictionary is made public in 2.4\n\tAdded org.apache.solr.spelling to package list which can be searched by SolrResourceLoader. Now we can write solr.IndexBasedSpellChecker instead of the fully qualified class name.\n\t\"classname\" attribute in configuration is optional now, it defaults to IndexBasedSpellChecker\n\tMinor additions to javadocs\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12606386",
            "date": "2008-06-19T13:54:41+0000",
            "content": "Committed revision 669485.  Note, I incorporated LUCENE-1297.  See http://wiki.apache.org/solr/SpellCheckComponent for more details on how to use it, as well as the unit tests.\n\nThanks to all who helped/contributed. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12606634",
            "date": "2008-06-20T04:49:16+0000",
            "content": "Why do we need to add the queryConverter definition outside of the speallcheck search component? \nIs it going to be used by any other component other than this?  "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12606719",
            "date": "2008-06-20T12:03:47+0000",
            "content": "Because of the stupid way it gets initialized as a  \nNamedListInitializerWhateverWhatever.  I'm open to alternate  \nsuggestions on how to do it and take advantage of the resource loader,  \netc.\n\nEvery time I go to do initialization stuff in Solr these days I pine  \nfor Spring, since we are basically re-inventing it, albeit not as  \nnicely.\n\n-Grant\n\n "
        },
        {
            "author": "Geoffrey Young",
            "id": "comment-12608487",
            "date": "2008-06-26T16:34:17+0000",
            "content": "I'm seeing random weirdness in the collation results.  the same query shift-refreshed sometimes yields (in json)\n\n\n\n{\n \"responseHeader\":{\n    \"params\":{\n\t\"spellcheck\":\"true\",\n\t\"q\":\"redbull air show\",\n\t\"qf\":\"search-en\",\n\t\"spellcheck.collate\":\"true\",\n\t\"qt\":\"dismax\",\n\t\"wt\":\"json\",\n\t\"rows\":\"0\"}},\n \"response\":{\"numFound\":0,\"start\":0,\"docs\":[]\n },\n \"spellcheck\":{\n  \"suggestions\":[\n\t\"redbull\",[\n\t \"numFound\",1,\n\t \"startOffset\",0,\n\t \"endOffset\",7,\n\t \"suggestion\",[\"redbelly\"]],\n\t\"show\",[\n\t \"numFound\",1,\n\t \"startOffset\",12,\n\t \"endOffset\",16,\n\t \"suggestion\",[\"shot\"]],\n\t\"collation\",\"redbelly airshotw\"]}}\n\n\n\nnote the \"collation\" spacing and extraneous 'w'.  a refresh toggles between that and what you might expect :\n\n\n\"collation\",\"redbelly air shot\"]\n\n\n\nUPDATE: opened new issue as SOLR-606\n\n--Geoff "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12608519",
            "date": "2008-06-26T17:41:58+0000",
            "content": "Can you open a new issue to track this?  Looks like a string replace issue on the offsets.  We probably should do the collation a bit differently to make sure the words fit right.  We'll probably have to right pad or something like that. "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12608527",
            "date": "2008-06-26T17:59:42+0000",
            "content": "For what it is worth, here is the code that I used client side before the collation feature was available.  I haven't looked at how it is done in this patch.  It has some nice features such as delimiting the spelling correction, e.g., with HTML bold tags, and preserving the users initial case on each word.\n\n\n        StringBuilder buff = new StringBuilder();\n        StringBuilder rawBuff = new StringBuilder();\n        int last = 0;\n        String userStr = null;\n        // for each suggestion\n        for( Suggestion s : suggestions ) {\n            // add part before the mispelling\n            userStr = userQuery.substring( last, s.startOffset );\n            buff.append( userStr );\n            rawBuff.append( userStr );\n            String suggestion = s.suggestion;\n            if( _spellCheckPreserveUserCase ) {\n                userStr = userQuery.substring( s.startOffset, s.endOffset );\n                char[] userCh = userStr.toCharArray();\n                boolean initialUpper = Character.isUpperCase( userCh[0] );\n                boolean allUpper = true;\n                for( char c : userCh ) {\n                    if( Character.isLowerCase( c ) ) {\n                        allUpper = false;\n                        break;\n                    }\n                }\n                if( allUpper ) {\n                    suggestion = suggestion.toUpperCase();\n                }\n                else if( initialUpper ) {\n                    userCh = suggestion.toCharArray();\n                    userCh[0] = Character.toUpperCase( userCh[0] );\n                    suggestion = new String( userCh );\n                }\n            }\n            buff.append( _spellCheckStartHighlight ).append( suggestion )\n                .append( _spellCheckEndHighlight );\n            rawBuff.append( suggestion );\n            last = s.endOffset;\n        }\n        // add part after all mispellings\n        userStr = userQuery.substring( last );\n        buff.append( userStr );\n        rawBuff.append( userStr );\n        if( log().isDebugEnabled() ) {\n            log().debug( \"Did you mean: \" + buff );\n            log().debug( \"Did you mean link: \" + rawBuff );\n        }\n\n "
        },
        {
            "author": "Bojan Smid",
            "id": "comment-12610247",
            "date": "2008-07-03T15:02:40+0000",
            "content": "I notice that old pizza->plaza, golf->roof issue is still here. \n\nI created a patch for latest trunk version which deals with this, here is the attachment, I believe the fix should be submitted (maybe it should be implemented differently, but that's open for the discussion, I used spellchecker.exist() method). "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12610276",
            "date": "2008-07-03T16:52:26+0000",
            "content": "Hi Bojan,\n\nThanks for the patch.  I think it would be best to open a new issue for it.  \n\nHowever, I'm not sure what is going on here.  When I look at the Lucene code, it has this:\n\nfinal int freq = (ir != null && field != null) ? ir.docFreq(new Term(field, word)) : 0;\nfinal int goalFreq = (morePopular && ir != null && field != null) ? freq : 0;\n// if the word exists in the real index and we don't care for word frequency, return the word itself\n    if (!morePopular && freq > 0) {\n      return new String[] { word };\n    }\n\n\n\nThe comment says it all, so maybe we have something else going on wrong.\n\nAt a minimum, your patch at least needs to account for when you want to get more popular suggestions even if the word exists.  "
        }
    ]
}