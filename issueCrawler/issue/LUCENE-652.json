{
    "id": "LUCENE-652",
    "title": "Compressed fields should be \"externalized\" (from Fields into Document)",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [
            "2.9"
        ],
        "affect_versions": "1.9,                                            2.0.0,                                            2.1",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Right now, as of 2.0 release, Lucene supports compressed stored fields.  However, after discussion on java-dev, the suggestion arose, from Robert Engels, that it would be better if this logic were moved into the Document level.  This way the indexing level just stores opaque binary fields, and then Document handles compress/uncompressing as needed.\n\nThis approach would have prevented issues like LUCENE-629 because merging of segments would never need to decompress.\n\nSee this thread for the recent discussion:\n\n    http://www.gossamer-threads.com/lists/lucene/java-dev/38836\n\nWhen we do this we should also work on related issue LUCENE-648.",
    "attachments": {
        "LUCENE-652.patch": "https://issues.apache.org/jira/secure/attachment/12402506/LUCENE-652.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-01-13T15:13:22+0000",
            "content": "Implementing this would mean deprecating Field.Store.COMPRESS and the various other places that use/set bits marking a field as compressed.\n\nSeems like a reasonable thing to do.  I will mark this as a 2.9 issue, so that we make sure we deprecate it at or before that time. ",
            "author": "Grant Ingersoll",
            "id": "comment-12558407"
        },
        {
            "date": "2009-03-18T20:04:44+0000",
            "content": "I added o.a.l.document.CompressionTools, with static methods to\ncompress & decompress, and deprecated Field.Store.COMPRESS.\n\nI also found two separate bugs:\n\n\n\tWith Field.Store.COMPRESS we were running compression twice\n    (unnecessarily); I've fixed that.\n\n\n\n\n\tIf you try to make a Field(byte[], int offset, int length,\n    Store.COMPRESS), you'll hit an AIOOBE.  I think we don't need to\n    fix this one since it's in now-deprecated code, and with 2.9,\n    users can migrate to CompressionTools.\n\n\n\nI plan to commit in a day or two. ",
            "author": "Michael McCandless",
            "id": "comment-12683145"
        },
        {
            "date": "2009-03-20T17:38:13+0000",
            "content": "Is an index compressed with Store.COMPRESS still readable? Can i uncompress fields compressed using the old tools also by retrieving the byte array and using CompressionTools? There should be some documentation about that.\n\nAnother question: Compressing was also used for string fields, maybe CompressionTols also suplies a method to compress strings (and convert them to UTF-8 during that to be backwards compatible). This would prevent people from calling String.getBytes() without charset and then wondering, why they cannoit read their index again... ",
            "author": "Uwe Schindler",
            "id": "comment-12683962"
        },
        {
            "date": "2009-03-20T17:50:11+0000",
            "content": "Good questions!\n\nIs an index compressed with Store.COMPRESS still readable?\n\nYes, we have to support that until Lucene 4.0.  But\nField.Store.COMPRESS will be removed in 3.0 (ie you can read previous\ncompressed fields, interact w/ an index that has compressed fields in\nit, etc., just not add docs with Field.Store.COMPRESS to an index as\nof 3.0).\n\nCan i uncompress fields compressed using the old tools also by retrieving the byte array and using CompressionTools?\n\nWell... yes, but: you can't actually get the compressed byte[]\n(because Lucene will decompress it for you).\n\nCompressing was also used for string fields, maybe CompressionTols also suplies a method to compress strings (and convert them to UTF-8 during that to be backwards compatible). This would prevent people from calling String.getBytes() without charset and then wondering, why they cannoit read their index again...\n\nOK I'll add them.  I'll name them compressString and decompressString. ",
            "author": "Michael McCandless",
            "id": "comment-12683967"
        },
        {
            "date": "2009-03-20T18:01:05+0000",
            "content": "OK I'll add them. I'll name them compressString and decompressString.\n\nMaybe it is better to use the new UTF-8 tools to encode/decode (instead of toBytes()). This would be consistent with the rest bof Lucene.\nBut for the old deprecated Field.Store.COMPRESS, keep it how it is (backwards compatibility). ",
            "author": "Uwe Schindler",
            "id": "comment-12683969"
        },
        {
            "date": "2009-03-20T18:22:39+0000",
            "content": "\nMaybe it is better to use the new UTF-8 tools to encode/decode (instead of toBytes()). This would be consistent with the rest bof Lucene.\nBut for the old deprecated Field.Store.COMPRESS, keep it how it is (backwards compatibility).\n\nYou mean UnicodeUtil? I think we can leave that to future optimization (I'd rather focus on the other 2.9 issues, realtime search, etc. at this point). ",
            "author": "Michael McCandless",
            "id": "comment-12683976"
        },
        {
            "date": "2009-03-20T18:24:53+0000",
            "content": "Yes, should I prepare a patch for trunk and add these methods? ",
            "author": "Uwe Schindler",
            "id": "comment-12683977"
        },
        {
            "date": "2009-03-20T18:28:25+0000",
            "content": "Added compress/decompressString, and improved javadocs to say this compression format matches Field.Store.COMPRESS. ",
            "author": "Michael McCandless",
            "id": "comment-12683978"
        },
        {
            "date": "2009-03-20T18:30:55+0000",
            "content": "Yes, should I prepare a patch for trunk and add these methods?\n\nYou mean to switch to UnicodeUtil?  That would be great! ",
            "author": "Michael McCandless",
            "id": "comment-12683981"
        },
        {
            "date": "2009-03-20T18:45:41+0000",
            "content": "If we switch to UnicodeUntil we may want to allow instantiation of CompressionTools, since UnicodeUtil is optimized for re-use.\n\nAnd if we do that we have to think about thread safety & concurrency, probably using CloseableThreadLocal under the hood, and then add a close() method. ",
            "author": "Michael McCandless",
            "id": "comment-12683985"
        },
        {
            "date": "2009-03-20T19:04:57+0000",
            "content": "This is a first version using UnicodeUtils. The deprecated Store.COMPRESS part still uses String.getBytes() because of backwards compatibility (otherwise it would be a change in index format).\nThis version currenty creates a new UTFxResult, because no state and no close method. It can also be synchronized without ThreadLocal, but this may not be so good.\nThe current version has a little performance impact because of array copying. ",
            "author": "Uwe Schindler",
            "id": "comment-12683993"
        },
        {
            "date": "2009-03-20T21:09:30+0000",
            "content": "OK thanks Uwe, it looks good.  We can leave the other changes I\nsuggested to future optimizations.  I'll commit soon! ",
            "author": "Michael McCandless",
            "id": "comment-12684067"
        },
        {
            "date": "2009-03-20T23:14:25+0000",
            "content": "Fine! In my opinion the little overhead of UnicodeUtils is far lower that the one by compression and the ByteArrayStreams.\n\n\nCan i uncompress fields compressed using the old tools also by retrieving the byte array and using CompressionTools?\n\nWell... yes, but: you can't actually get the compressed byte[]\n(because Lucene will decompress it for you).\n\nYou can: With a FieldSelector that load the fields for merge, you get the raw binary values (found out from the code of FieldsReader). ",
            "author": "Uwe Schindler",
            "id": "comment-12685385"
        },
        {
            "date": "2009-03-21T10:30:43+0000",
            "content": "Fine! In my opinion the little overhead of UnicodeUtils is far lower that the one by compression and the ByteArrayStreams.\n\nGood point...\n\nYou can: With a FieldSelector that load the fields for merge, you get the raw binary values (found out from the code of FieldsReader).\n\nAhh, true!  In fact, I will go and deprecate that FieldSelectorResult option. ",
            "author": "Michael McCandless",
            "id": "comment-12688012"
        }
    ]
}