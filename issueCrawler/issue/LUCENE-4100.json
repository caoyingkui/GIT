{
    "id": "LUCENE-4100",
    "title": "Maxscore - Efficient Scoring",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/codecs",
            "core/query/scoring",
            "core/search"
        ],
        "type": "Improvement",
        "fix_versions": [
            "master (8.0)"
        ],
        "affect_versions": "4.0-ALPHA",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "At Berlin Buzzwords 2012, I will be presenting 'maxscore', an efficient algorithm first published in the IR domain in 1995 by H. Turtle & J. Flood, that I find deserves more attention among Lucene users (and developers).\nI implemented a proof of concept and did some performance measurements with example queries and lucenebench, the package of Mike McCandless, resulting in very significant speedups.\n\nThis ticket is to get started the discussion on including the implementation into Lucene's codebase. Because the technique requires awareness about it from the Lucene user/developer, it seems best to become a contrib/module package so that it consciously can be chosen to be used.",
    "attachments": {
        "LUCENE-4100.patch": "https://issues.apache.org/jira/secure/attachment/12891228/LUCENE-4100.patch",
        "contrib_maxscore.tgz": "https://issues.apache.org/jira/secure/attachment/12530651/contrib_maxscore.tgz",
        "maxscore.patch": "https://issues.apache.org/jira/secure/attachment/12530652/maxscore.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-06-02T15:47:52+0000",
            "content": "Attached is a tarball that includes maxscore code (to be unpacked in /lucene/contrib/), and a patch that integrates it into core Lucene (for now, basis for both is Lucene40 trunk r1300967).\n\nFrom the README, included in the tarball:\nThis contrib package implements the 'maxscore' optimization, orginally presented by in the IR domain in 1995 by H. Turtle & J. Flood.\n\nIf you'd like to play with this implementation, for instance, to estimate its usefulness for your kind of queries and index data, follow these steps:\n1) Build a normal Lucene40 index with your data\n2) Rewrite this index using the main method of the class\n   org.apache.lucene.index.IndexRewriter\n   with source and destination directories as arguments. This class will iterate over your index segments, parse them, compute a maxscore for each term using collection statistics of the source index and write them to the destination directory using the Lucene40Maxscore codec. The resulting index should be slightly bigger. Currently, Lucene's DefaultSimilarity will be used to estimate maxscores, meaning that this has to be the Similarity used at querying time for maxscore to be effective.\n3) Apply the patch to a checkout of Lucene4 trunk revision 1300967 and place the maxscore code directory below /lucene/contrib/.\n4) After the patch, there should be the required logic in org.apache.lucene.search.BooleanQuery to use the MaxscoreScorer on the index in 2) when the index is searched as usual:\n\n   int topk = 10;\n   searcher.setSimilarity(new DefaultSimilarity());\n   Query q = queryparser.parse(\"t1 t2 t3 t4\");\n   MaxscoreDocCollector ms_coll = new MaxscoreDocCollector(topk);\n   searcher.search(q, ms_coll);\n\nNote:\n\n\tYour index at 1) does not have to be 'optimized' (it does not have to consist of one index segment only). In fact, maxscore can be more efficient with multiple segments because multiple maxscores are computed for many frequent terms for subsets of documents, resulting in tighter bounds and more effective pruning.\n\tDon't expect totalHits to return the same counts as before. MaxscoreDocCollector sole purpose is to notify you about this by throwing an exception when you try to use the getter.\n\tCurrently, purely disjunctive, flat queries are supported\n\tDefaultSimilarity tested only\n\t@experimental !\n\n ",
            "author": "Stefan Pohl",
            "id": "comment-13287963"
        },
        {
            "date": "2012-07-05T11:11:22+0000",
            "content": "Hello, thank you for working on this! \n\nI have just taken a rough glance at the code, and think we should probably look at what API changes would make \nthis sort of thing fit better into Lucene and it easier to implement.\n\nRandom thoughts:\n\nSpecifically: what you are doing in the PostingsWriter is similar to computing impacts (I don't have a copy of\nthe paper so admittedly don't know the exact algorithm you are using). But it seems to me that you are putting \na maxScore in the term dictionary metadata for all of the terms postings (as a float).\n\nWith the tool you provide, this works because you have access to e.g. the segment's length normalization information\netc (your postingswriter takes a reader). But we would have to think about how to give postingswriters access to this \non flush... it seems possible to me though.\n\nGiving the postingswriter full statistics (e.g. docfreq) for Similarity computation seems difficult: while I think\nwe could accum this stuff in FreqProxTermsWriter before we flush to the codec, it wouldn't solve the problem at merge time,\nso you would have to do a 2-pass merge in the codec somehow...\n\nBut the alternative of splitting the \"impact\" (tf/norm) from the document-independent weight (e.g. IDF) isn't that pretty\neither, because it limits the scoring systems (Similarity implementations) that could use the optimization.\n\nas many terms will be low frequency (e.g. docfreq=1), i think its not\nworth it to encode the maxscore for these low freq terms: we could save space by omitting maxscore for low freq terms \nand just treat it as infinitely large?\n\nthe opposite problem: is it really optimal to encode maxscore for the entire term? or would it be better for high-freq\nterms to encode maxScore for a range of postings (e.g. block). This way, you could skip over ranges of postings that cannot\ncompete (rather than limiting the optimization to an entire term). A codec could put this information into a block header,\nor at certain intervals, into the skip data, etc.\n\ndo we really need a full 4-byte float? How well would the algorithm work with degraded precision: e.g. something like\nSmallFloat. (I think this SmallFloat currently computes a lower bound, we would have to bump to the next byte to make an upper bound).\n\nanother idea: it might be nice if this optimization could sit underneath the codec, such that you dont need a special\nScorer. One idea here would be for your collector to set an attribute on the DocsEnum (maxScore): of course a normal\ncodec would totally ignore this and proceed as today. But codecs like this one could return NO_MORE_DOCS when postings\nfor that term can no longer compete. I'm just not positive if this algorithm can be refactored in this way, and this\nwould also require some clean way of getting these attributes from Collector -> Scorer -> DocsEnum. Currently Scorer\nis in the way here \n\nJust some random thoughts, I'll try to get a copy of this paper so I have a better idea whats going on with this particular\noptimization... ",
            "author": "Robert Muir",
            "id": "comment-13406992"
        },
        {
            "date": "2012-07-05T20:46:19+0000",
            "content": "I spun off a sub-issue (LUCENE-4198) to see how we can first fix this Codec API so that\nyou don't need an IndexRewriter and this patch could work \"live\". ",
            "author": "Robert Muir",
            "id": "comment-13407472"
        },
        {
            "date": "2012-07-11T23:03:45+0000",
            "content": "bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment ",
            "author": "Hoss Man",
            "id": "comment-13412304"
        },
        {
            "date": "2012-07-12T18:34:04+0000",
            "content": "\nYour index at 1) does not have to be 'optimized' (it does not have to consist of one index segment only). In fact, maxscore can be more efficient with multiple segments because multiple maxscores are computed for many frequent terms for subsets of documents, resulting in tighter bounds and more effective pruning.\n\nI've been thinking about this a lot lately: while what you say is true, thats because you reprocess all segments with IndexRewriter (which is fine for a static collection).\n\nBut this algorithm in general is not rank safe with incremental indexing: the problem is that when doing actual scoring,\nscores consist of per-segment/within document stats (term frequency, document length), but also are affected by collection-wide\nstatistics from many other segments (IDF, average document length, ...) or even machines in a distributed collection.\n\nSo I think for this to work and remain rank-safe, we cannot write the entire score into the segment, because the score\nat actual search time is dependent on all the other segments being searched. Instead I think this can only work when\nwe can easily factor out an impact (e.g. in the case of DefaultSimilarity the indexed maxscore excludes the IDF component,\nthis is instead multiplied in at search time).\n\nI don't see how it can be rank-safe with algorithms like BM25 and incremental indexing, where parameters like average document\nlength are not simple multiplicative factors into the formula: and determine exactly how important tf versus document length play\na role in the score, but I'll think about it some more. ",
            "author": "Robert Muir",
            "id": "comment-13413055"
        },
        {
            "date": "2012-07-12T21:33:42+0000",
            "content": "Thanks for the feedback! You're on spot with everything you're saying.\n\nYes, the methods as suggested in the different papers have (semi-)static indexes in mind, that is, such that batch-index many new documents, then recompute maxscores (hence, IndexRewriter) and roll out the new version of the indexes. This is a Lucene use-case common to many large installations (or part thereof) and as such important. Moreover, this approach can easily be generalized to the other Similarities, without that they necessarily have to know about maxscore, and can be simplified by some minor API changes within Lucene. The PoC code as-is might be of help to showcase dependencies in general, and such that currently are not well supported within Lucene (because there was no need for it yet).\n\nIf you really want to go the full distance: I already thought about doing maxscore live and got some ideas in this regard, see below.\n\nComments to your thoughts:\n\n[PostingsWriter]\nYou're right. For simplicity, I was computing each term's overall contribution (as explained in the talk), including all but query-dependent factors. You can consider this as un-quantized impacts (in the sense of Anh et al.) which necessitates a second pass over a static index, hence IndexRewriter.\n\nAs a side note: I noticed a drop in the PKLookup benchmark, suggesting that it might be better not to extend the size of dictionary items, but to store maxscores in the beginning of inverted lists, or next to skip data. This effect should be smaller or disappear though when maxscores are not stored for many terms.\n\n[Length normalization]\nYes, this might be a necessary dependency. It should be a general design-principle though to have as many as possible statistics at hand everywhere, as long as it doesn't hurt performance in terms of efficiency.\n\n[splitting impacts / incremental indexing]\nYes, this would be more intrusive, requiring Similarity-dependent maxscore computations. Here is how it could work:\nVery exotic scoring functions simply don't have to support maxscore and will thus fall back to the current score-all behaviour.\nDefaultSimilarity is simple, but BM25 and LMDirichlet can't as easily be factored out, as you correctly point out, but we could come up with bounds for collection statistics (those that go into the score) within which it is safe to use maxscore, otherwise we fallback to score-all until a merge occurs, or we notify the user to better do a merge/optimize, or Lucene does a segment-rewrite with new maxscore and bound computations on basis of more current collection stats. I got first ideas for an algorithm to compute these bounds.\n\n[docfreq=1 treatment]\nDefinitely agree. Possibly, terms with docfreq < x=10? could not store a maxscore. x configurable and default to be evaluated; x should be stored in index so that it can be determined which terms don't contain maxscores.\nHaving a special treatment for these terms (not considering them for exclusion within the algorithm) allows for easier exchange of the core of the algorithm to get the WAND algorithm, or also to ignore a maxscore for a term for which collection stats went out of bounds.\n\n[maxscores per posting ranges]\n+1. As indicated in the description, having multiple maxscores per term can be more efficient, possibly leading to tighter bounds and more skipping. Chakrabarti'11 opted for one extreme, computing a maxscore for each compressed posting block, whereas the optimal choice might have been a multiple of blocks, or a postings range not well aligned with block size.\nOptimal choice will be very dependent on skip list implementation and its parameters, but also posting de-compression overhead.\nThe question is how to get access to this codec-dependent information inside of the scoring algorithm, tunneled through the TermQuery?\n\n[store <4 bytes per maxscore]\nPossible. As long as the next higher representable real number is stored (ceil, not floor), no docs will be missed and the algorithm remains correct. But because of more loose bounds the efficiency gain will be affected at some point with too few bits.\n\nIf the score is anyway factored out, it might be better to simply store all document-dependent stats (TF, doclen) of the document with the maximum score contribution (as ints) instead of one aggregate intermediate float score contribution.\n\n[implementation inside codec]\nPlease be aware that while terms are at some point excluded from merging, they still are advanced to the docs in other lists to gain complete document knowledge and compute exact scores. Maxscores can also be used to minimize how often this happens, but the gains are often compensated by the more complex scoring. Still having to skip inside of excluded terms complicates your suggested implementation. But we definitely should consider architecture alternatives. The MaxscoreCollector, for instance, does currently only have a user interface function, keeping track of the top-k and their entry threshold could well be done inside the Maxscorer.\nI was thinking though to extend the MaxscoreCollector to provide different scoring information, e.g. an approximation of the number of hits next to the actual number of scored documents (currently totalHits). ",
            "author": "Stefan Pohl",
            "id": "comment-13413246"
        },
        {
            "date": "2012-07-12T23:48:12+0000",
            "content": "\nAs a side note: I noticed a drop in the PKLookup benchmark, suggesting that it might be better not to extend the size of dictionary items, but to store maxscores in the beginning of inverted lists, or next to skip data. This effect should be smaller or disappear though when maxscores are not stored for many terms.\n\nI wouldn't worry about this, I noticed a few things that might speed that up:\n\n1. Currently it does writeVInt(Float.floatToIntBits(term.maxscoreScore)) . But I think this should be writeInt, not writeVInt?\nSo I think currently we often write 5 bytes here, with all the vint checks for each byte, and as an Int it would always be 4 and faster.\n2. Yes, with low freq terms (e.g. docFreq < skipMinimum), its probably best to just omit this at both read and write time. Then PK lookup would be fine.\n3. As far as < 4 bytes ceiling, my motivation there was not to save in the term dictionary, but instead to make these smaller and allow us to add these at regular intervals. We can take advantage of a few things, e.g. it should never be a negative number for a \nwell-formed Similarity (i think that would screw up the algorithm looking at your tests anyway).\n\n\nDefaultSimilarity is simple, but BM25 and LMDirichlet can't as easily be factored out, as you correctly point out, but we could come up with bounds for collection statistics (those that go into the score) within which it is safe to use maxscore, otherwise we fallback to score-all until a merge occurs, or we notify the user to better do a merge/optimize, or Lucene does a segment-rewrite with new maxscore and bound computations on basis of more current collection stats. I got first ideas for an algorithm to compute these bounds.\n\nOk, I'm not sure I totally see how the bounds computation can work, but if it can we might be ok in general. If the different segments are somewhat homogeneous then these stats should pretty much be very close anyway.\n\nThe other idea i had was more intrusive, adding a computeImpact() etc to Similarity or whatever.\n\n\nIf the score is anyway factored out, it might be better to simply store all document-dependent stats (TF, doclen) of the document with the maximum score contribution (as ints) instead of one aggregate intermediate float score contribution.\n\nThat might be a good idea. with TF as a vint and doclen as a byte, we would typically only have two bytes but not actually lose any information (by default, all these sims encode doclen as a byte anyway).\n\n\n[implementation inside codec]\nPlease be aware that while terms are at some point excluded from merging, they still are advanced to the docs in other lists to gain complete document knowledge and compute exact scores. Maxscores can also be used to minimize how often this happens, but the gains are often compensated by the more complex scoring. Still having to skip inside of excluded terms complicates your suggested implementation. But we definitely should consider architecture alternatives. The MaxscoreCollector, for instance, does currently only have a user interface function, keeping track of the top-k and their entry threshold could well be done inside the Maxscorer.\nI was thinking though to extend the MaxscoreCollector to provide different scoring information, e.g. an approximation of the number of hits next to the actual number of scored documents (currently totalHits).\n\nMy current line of thinking is even crazier, but I don't yet have anything close to a plan.\n\nAs a start, I do think that IndexSearcher.search() methods should take a \"Score Mode\" of sorts from the user (some enum), which would allow Lucene to do less work if its not necessary. We would pass this down via Weight.scorer() as a parameter... solely\nlooking at the search side I think this would open up opportunities in general for us to optimize things: e.g. instantiate the appropriate Collector impl, and for Weights to create the most optimal Scorers. Not yet sure how it would tie into the code API.\n\nI started hacking up on a prototype that looks like this (I might have tried to refactor too hard also shoving the Sort options in here...)\n\n/**\n * Different modes of search.\n */\npublic enum ScoreMode {\n  /** \n   * No guarantees that the ranking is correct,\n   * the results may come back in a different order than if all\n   * documents were actually scored. Total hit count may be \n   * unavailable or approximate.\n   */\n  APPROXIMATE,\n  /** \n   * Ranking is the same as {@link COMPLETE}, but total hit \n   * count may be unavailable or approximate.\n   */\n  SAFE,\n  /**\n   * Guarantees complete iteration over all documents, but scores\n   * may be unavailable.\n   */\n  COMPLETE_NO_SCORES,\n  /**\n   * Guarantees complete iteration over all documents, and scores\n   * will be computed, but the maximum score may be unavailable \n   */\n  COMPLETE_NO_MAX_SCORE,\n  /**\n   * Guarantees complete iteration and scoring of all documents.\n   */\n  COMPLETE,\n};\n\n ",
            "author": "Robert Muir",
            "id": "comment-13413335"
        },
        {
            "date": "2012-08-07T03:41:21+0000",
            "content": "rmuir20120906-bulk-40-change ",
            "author": "Robert Muir",
            "id": "comment-13429696"
        },
        {
            "date": "2012-12-29T03:18:30+0000",
            "content": "Stefan, this sounds exciting.  What sort of speedups did you get?  Are you still looking to include this into Lucene (4.1)? ",
            "author": "Otis Gospodnetic",
            "id": "comment-13540741"
        },
        {
            "date": "2013-01-02T08:38:53+0000",
            "content": "Otis, thank you for your interest and I wish everyone a happy New Year!\n\nRegarding speedups, have a look at http://vimeo.com/44300228 from 12 minutes onwards. It obviously depends on your mix of queries and collection size, but on average more than 100% should be achievable for large collections and typical query sets, and much higher speedups for problem queries (many frequent terms).\n\nThe contribution as-is (after adaptation to latest Lucene API/code-base) could already be included as a separate Lucene module for people to use who can live with its current limitations (for static indexes only, smaller totalHitCount).\nIn my spare time (unfortunately not much of that recently), I continue working on different approaches to make this more general, but this is pure experimentation and any production-ready offspring of that will take longer than 4.x and might require API changes (such as the ones suggested above by Robert), so perhaps 5.0 is a good aim.\n\nI suggest to continue rolling this ticket forward, or only attach version 5.0 to it. ",
            "author": "Stefan Pohl",
            "id": "comment-13542058"
        },
        {
            "date": "2013-07-23T18:44:37+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13717004"
        },
        {
            "date": "2013-11-25T19:03:07+0000",
            "content": "Stefan Pohl Stefan, I am curious if there has been any continued work on this? I know early this year you mentioned there hasn't been much time to work on it. It sounds very promising to me, so I hope it does not bit-rot! =) ",
            "author": "Aaron Daubman",
            "id": "comment-13831784"
        },
        {
            "date": "2013-11-26T15:45:29+0000",
            "content": "There is at least one step we made for the codec API (LUCENE-5123) that might make it easier: the codec has the freedom to \"pull\" the data rather than push it now, so it can more easily calculate interesting things that it wants.\n\nStill, there remains some fairly major challenges in my eyes:\n\n\thow to safely implement the optimization for dynamic indexes: See Stefan's note above. I do agree there are use cases for static indexes, the problem is: its lucene, people will try to use it with dynamic indexes regardless.\n\thow to connect such optimizations in a fairly clean way to the scoring api. This is perhaps the hardest part. There is a lot of separation between the scoring API (e.g. pluggable similarity, Scorer, etc) and the codec API.\n\n\n\nBut i don't want to discourage anyone, just saying its probably even more difficult from an API perspective than the stuff to support something \"different\" like BooleanScorer.java. ",
            "author": "Robert Muir",
            "id": "comment-13832677"
        },
        {
            "date": "2013-11-27T08:21:47+0000",
            "content": "Thanks for your interest, Aaron Daubman.\n\nRobert Muir accurately describes the current status and challenges here. There have to be made quite a few major and even more minor steps to eventually arrive at anything general-purpose and user-ready.\nHowever, given the feedback that I got so far (there seem to be many people that don't have extremely high NRT-requirements and who rebuild their whole indexes every few hours/days anyways), I still think that it would be worthwhile to have this as-is (with the limitation to static indexes) in a separate contrib-package that users are discouraged to use, except they really know what they are doing. Adding a few exceptions to guard users from wrong behaviour/misusage might also be useful.\n\nAs long as I, as the reporter, don't close the issue, it won't 'bite rot', don't worry  I also got to hear of quite some interest when I mentioned it at this year's Lucene/Solr-Revolutions.EU conference. ",
            "author": "Stefan Pohl",
            "id": "comment-13833573"
        },
        {
            "date": "2013-11-27T13:53:21+0000",
            "content": "Thanks for the update Stefan Pohl - My particular use-case seems tailor made for this. I have several decently large (10-30G indices) solr instances, all of which run in read-only mode and are created ~2x a day via a snapshot process that rolls the index out to load-balanced servers. Several of these instances routinely match 30-80% (custom MLT-like queries) of the 2-25M docs in the index per-query, so efficient scoring would be a huge win here.\n\nI already have to patch and custom-build solr for our use (until I get around to creating required tests to haver SOLR-2052 accepted) and am wondering if you have any thoughts/guidance on trying out your patch?\n\nThe main use-case is from a custom extension of QueryComponent that overrides perpare() and essentially builds up a custom boosted boolean query and uses rb.setQueryString and rb.setFilters... ",
            "author": "Aaron Daubman",
            "id": "comment-13833812"
        },
        {
            "date": "2013-12-03T13:06:40+0000",
            "content": "Hi Aaron,\nI can't say much about any possible complications Solr might add here, but if you can get your index building and submitting some test queries code compile with the SVN-revision mentioned above (some almost 4.0 revision), and you're fine with the current limitations mentioned above (DefaultSimilarity, flat queries = BooleanQuery of TermQueries) you can simply try it out. The patch should cleanly apply and be functional.\nIt also shouldn't be too hard to upgrade the code to newer Lucene versions that use BlockPostingsFormat (4.1) and the new pull-based codec API (LUCENE-5123).\n\nEither way, it would be very interesting to hear of speedups with respect to MLT-like-queries. ",
            "author": "Stefan Pohl",
            "id": "comment-13837654"
        },
        {
            "date": "2014-03-13T22:31:10+0000",
            "content": "This patch seems almost 2 years old.\nWould it make sense to label this for GSoC? ",
            "author": "Otis Gospodnetic",
            "id": "comment-13934226"
        },
        {
            "date": "2014-03-16T05:47:44+0000",
            "content": "Hi,\n\nthank you for your email. Unfortunately, I'm not in the office until 4. April 2014.\nPlease contact my team for important issue.\nZX-BER-DG-DevTeam-Gold@zanox.com\n\nI apologize for the inconvenient it might have caused.\nThank you for your understanding.\n\nRowanto ",
            "author": "Rowanto Rowanto",
            "id": "comment-13936440"
        },
        {
            "date": "2014-04-16T12:54:46+0000",
            "content": "Move issue to Lucene 4.9. ",
            "author": "Uwe Schindler",
            "id": "comment-13970874"
        },
        {
            "date": "2016-03-13T22:22:06+0000",
            "content": "I would like to apply this issue as aGSoC project if someone is volunteer for being a mentor. ",
            "author": "Furkan KAMACI",
            "id": "comment-15192554"
        },
        {
            "date": "2017-08-22T10:13:59+0000",
            "content": "how to safely implement the optimization for dynamic indexes: See Stefan's note above. I do agree there are use cases for static indexes, the problem is: its lucene, people will try to use it with dynamic indexes regardless\n\nMaybe BM25 is making our lives easier with this? Scores are bounded by IDF * (k1 + 1) if I'm not mistaken so we could use it to feed the MAXSCORE algorithm? It would not be as good as the initial patch since we are totally ignoring the range of values that the term frequency and document length may take but this would not require any index-time changes, does not require the index to be static and might already allow for good speedups if query terms have very different IDF values?\n ",
            "author": "Adrien Grand",
            "id": "comment-16136601"
        },
        {
            "date": "2017-10-10T09:37:50+0000",
            "content": "I have been exploring how we could make this optimization a bit less inconvenient by not requiring that indices be static or codec-level changes, even though this might make the optimization a bit less efficient. Here is a patch that demonstrates the idea:\n\n\tSimScorer.maxScore() returns the maximum score that may be produced by SimScorer.score(). The BM25 impl returns IDF * (k1 + 1). POSITIVE_INFINITY is a fine return value in case a SimScorer doesn't have an upper bound on the produced scores.\n\tScorer.maxScore() returns the maximum score that may be produced by Scorer.score(). For TermScorer, this method delegates to SimScorer.maxScore().\n\tScorer.setMinCompetitiveScore(float) is used to tell the scorer that documents that produce scores that are less than the given score may safely be ignored. This method is called by TopScoreDocCollector whenever the top of the priority queue is updated. Scorer impls are free to ignore it.\n\tThere is a new MaxScoreScorer, which only works on pure disjunctions and is largely inspired from MinShouldMatchSumScorer except that instead of ensuring that freq >= minShouldMatch, it tries to ensure that sum_of_max_scores >= minCompetitiveScore.\n\n\n\nIt is largely untested, but luceneutil finds the same top docs with and without the patch so I expect it to not be completely broken (I had to disable the check that hit counts are the same since Maxscore doesn't give total hit counts). It yields interesting speedups on wikimedium10M, even for OrHighHigh:\n\n\n                    TaskQPS baseline      StdDev   QPS patch      StdDev                Pct diff\n   HighTermDayOfYearSort      102.75      (4.8%)       57.26      (3.7%)  -44.3% ( -50% -  -37%)\n                 LowTerm      708.14      (5.0%)      696.51      (5.4%)   -1.6% ( -11% -    9%)\n              HighPhrase       68.06      (4.0%)       67.00      (3.7%)   -1.6% (  -8% -    6%)\n            HighSpanNear        2.84      (3.4%)        2.80      (3.6%)   -1.3% (  -7% -    5%)\n             MedSpanNear       24.12      (2.5%)       23.90      (2.6%)   -0.9% (  -5% -    4%)\n               MedPhrase       49.14      (2.8%)       48.81      (2.8%)   -0.7% (  -6% -    5%)\n             LowSpanNear       20.48      (2.8%)       20.36      (2.9%)   -0.6% (  -6% -    5%)\n               LowPhrase       24.11      (2.6%)       24.01      (2.5%)   -0.4% (  -5% -    4%)\n              AndHighMed      316.86      (2.7%)      315.66      (2.7%)   -0.4% (  -5% -    5%)\n                 MedTerm      334.25      (3.1%)      333.01      (3.2%)   -0.4% (  -6% -    6%)\n                 Respell      261.62      (3.4%)      260.86      (5.5%)   -0.3% (  -8% -    8%)\n             AndHighHigh       62.76      (2.2%)       62.59      (2.4%)   -0.3% (  -4% -    4%)\n                HighTerm       86.17      (3.9%)       86.24      (2.8%)    0.1% (  -6% -    6%)\n                  IntNRQ       15.03      (6.5%)       15.20      (5.2%)    1.1% (  -9% -   13%)\n         LowSloppyPhrase       99.72      (3.3%)      100.83      (3.3%)    1.1% (  -5% -    8%)\n                 Prefix3       34.06      (7.2%)       34.48      (5.0%)    1.3% ( -10% -   14%)\n                Wildcard      159.32      (8.4%)      161.37      (6.8%)    1.3% ( -12% -   18%)\n         MedSloppyPhrase       42.17      (3.2%)       42.81      (3.0%)    1.5% (  -4% -    7%)\n              AndHighLow     1059.03      (3.6%)     1075.73      (2.9%)    1.6% (  -4% -    8%)\n            OrNotHighLow     1774.47      (4.1%)     1805.36      (4.7%)    1.7% (  -6% -   10%)\n        HighSloppyPhrase       32.84      (4.0%)       33.46      (3.4%)    1.9% (  -5% -    9%)\n            OrNotHighMed      295.65      (6.4%)      309.03      (4.7%)    4.5% (  -6% -   16%)\n       HighTermMonthSort      227.22      (9.9%)      241.42      (8.7%)    6.2% ( -11% -   27%)\n           OrNotHighHigh       43.19      (3.2%)       48.56      (3.1%)   12.4% (   5% -   19%)\n            OrHighNotMed       93.61      (3.5%)      113.32      (4.8%)   21.1% (  12% -   30%)\n           OrHighNotHigh       13.58      (3.2%)       16.44      (4.9%)   21.1% (  12% -   30%)\n                  Fuzzy1      190.81      (5.2%)      236.11     (14.4%)   23.7% (   3% -   45%)\n            OrHighNotLow       68.12      (3.9%)       85.99      (6.1%)   26.2% (  15% -   37%)\n              OrHighHigh       37.43      (5.4%)       51.96      (5.0%)   38.8% (  26% -   52%)\n               OrHighMed       55.45      (5.2%)      130.78      (9.0%)  135.8% ( 115% -  158%)\n                  Fuzzy2       24.33      (4.6%)       81.51     (24.2%)  235.0% ( 197% -  276%)\n               OrHighLow       32.37      (4.4%)      451.34     (47.8%) 1294.2% (1189% - 1408%)\n\n\n\nTODO:\n\n\tdocs, tests\n\tAdd a boolean needsTotalHits to Query.createWeight and Collector similarly to the existing boolean needsScores.\n\tHow should we deal with TopDocs instances that don't have correct total hit counts? Set it to -1? Or maybe -1 - num_collected_docs so that users can get a lower bound of the number of matches?\n\n\n\nFollow-ups:\n\n\tRequire that scores are always positive or null so that such optimizations are easier to reason about and generalize in the future?\n\tCall the setMinCompetitiveScore API from TopFieldDocCollector too when the first sort field is the score.\n\tPropagate Scorer.setMinCompetitiveScore(float) whenever possible so that eg. filtered disjunctions could benefit from the optimization too.\n\tImplement Scorer.setMinCompetitiveScore(float) on other scorers.\n\tStore more metadata in order to be able to compute better higher bounds for the scores, like the maximum term frequency. Maybe even allow similarity-specific stats?\n\n ",
            "author": "Adrien Grand",
            "id": "comment-16198424"
        },
        {
            "date": "2017-10-12T15:50:13+0000",
            "content": "Here is a patch:\n\n\tmore docs and tests\n\treplaces needsScores with a SearchMode enum as suggested by Robert\n\tthe MAXSCORE optimization work with top-level disjunctions and filtered disjunctions (FILTER or MUST_NOT)\n\tTopScoreDocsCollector sets the totalHitCount to -1 when the optimization is used since the total hit count is unknown\n\tMaxScoreScorer was changed to reason on integers rather than doubles to avoid floating-point arithmetic issues. To do that it scales all max scores into 0..2^16, rounding up when working on the max scores of sub clauses, and down when rounding the min competitive score in order to make sure to not miss matches (at the cost of potentially more false positives, but this is fine)\n\n\n\nThe patch is alreay huge (due to the needsScore/searchMode change mostly) so I wanted to do the strict minimum here for this feature to be useful, but we'll need follow-ups to make the optimization work with the paging collector, conjunctions that have more than one scoring clause, TopFieldCollector when the first sort field is the score, integrate it with IndexSearcher (currently you need to create the collector manually to use it), etc. ",
            "author": "Adrien Grand",
            "id": "comment-16202135"
        },
        {
            "date": "2017-10-13T04:52:57+0000",
            "content": "Can we avoid the ScoreMode.merge? This seems really, really confusing. In general I don't think we should support such merging in MultiCollector or anywhere else, we should simply throw exception if things are different.\n\nI think the enum should be further revisited/simplified: essentially at the minimum it must capture 2 booleans from the user: whether scores are needed, and whether exact total hit count is needed. Perhaps instead of the enum two booleans would be easier for now.\n\nI don't understand why we should set the totalHitCount to -1, vs setting to a useful approximation, like google. The user said they didn't need the exact total hit count, so it should be no surprise, and its a hell of a lot more useful than a negative number. ",
            "author": "Robert Muir",
            "id": "comment-16203051"
        },
        {
            "date": "2017-10-13T07:01:30+0000",
            "content": "Thanks for looking!\n\nCan we avoid the ScoreMode.merge? This seems really, really confusing. In general I don't think we should support such merging in MultiCollector or anywhere else, we should simply throw exception if things are different.\n\nWe have been allowing it until now (by ORing the needsScores booleans) so I didn't want to break this behaviour. I can remove this method and make the logic contained in MultiCollector by essentially returning COMPLETE if any of the collectors' score modes are different but I think failing would be surprising to many users and would probably need to be a 8.0 change if we decide to do it?\n\nPerhaps instead of the enum two booleans would be easier for now.\n\nThis is what I wanted to do first but I didn'l like the fact that it would allow passing needsScores=false and needsTotalHits=false, which doesn't make sense. If you still prefer the two booleans approach despite this, I'm happy to make the change.\n\nI don't understand why we should set the totalHitCount to -1, vs setting to a useful approximation, like google. The user said they didn't need the exact total hit count, so it should be no surprise, and its a hell of a lot more useful than a negative number.\n\nI agree with that statement, but how do we compute a good estimate? It sounds challenging as the number of collected documents might be much less than the actual number of hits while the cost of the scorer can be highly overestimated, eg. for phrase queries. Should I return the number of collected documents and add documentation that this is a lower bound of the total number of hits? ",
            "author": "Adrien Grand",
            "id": "comment-16203135"
        },
        {
            "date": "2017-10-13T11:07:26+0000",
            "content": "\nThis is what I wanted to do first but I didn'l like the fact that it would allow passing needsScores=false and needsTotalHits=false, which doesn't make sense. If you still prefer the two booleans approach despite this, I'm happy to make the change.\n\nWhy doesn't it make sense? If i do a query, sorting by reverse time (recency), and retrieve the top 20, then i don't need scores, why do i need an exact hit count too? I think an approximation would suffice.\n\n\nI agree with that statement, but how do we compute a good estimate? It sounds challenging as the number of collected documents might be much less than the actual number of hits while the cost of the scorer can be highly overestimated, eg. for phrase queries. Should I return the number of collected documents and add documentation that this is a lower bound of the total number of hits?\n\nI think naively we want to base it on where we early terminate (as oppose to maxdoc) but i get the idea with many clauses. still, i think this estimate may be \"good enough\" because as you paginate, the estimate would get better? ",
            "author": "Robert Muir",
            "id": "comment-16203384"
        },
        {
            "date": "2017-10-13T13:58:46+0000",
            "content": "Why doesn't it make sense? If i do a query, sorting by reverse time (recency), and retrieve the top 20, then i don't need scores, why do i need an exact hit count too? I think an approximation would suffice.\n\nSorry I wrote needsTotalHits because this is the option name I used on the TopScoreDocCollector factory method, but on the weight we'd need something different. Because there are two situations in which you would need scores but not visit all matches:\n\n\tsorting by score\n\tsorting by a field (potentially with early-termination) then score\n\n\n\nYet these cases are different since we can only apply MAXSCORE in the first case. So what we need is more something like canSkipNonCompetitiveScores but then this implies that needsScores is true, so there would be illegal combinations. Which is why I went with the enum.\n\nI think naively we want to base it on where we early terminate (as oppose to maxdoc) but i get the idea with many clauses. still, i think this estimate may be \"good enough\" because as you paginate, the estimate would get better?\n\nOK I'll give this approach a go. ",
            "author": "Adrien Grand",
            "id": "comment-16203596"
        },
        {
            "date": "2017-10-13T14:03:55+0000",
            "content": "\nSorry I wrote needsTotalHits because this is the option name I used on the TopScoreDocCollector factory method, but on the weight we'd need something different. Because there are two situations in which you would need scores but not visit all matches:\n\nI'm not sure thats true. There is another case right? There is the current sorted-index case today, but you have to be a rocket scientist (custom collector) to use it. Surely the API should handle that case? (it should \"just work\" from indexsearcher).  ",
            "author": "Robert Muir",
            "id": "comment-16203603"
        },
        {
            "date": "2017-10-13T15:05:43+0000",
            "content": "Yes, but this 3rd case is fine since it can be handled solely from the collector side, so we don't need to make weights aware of it?\n\nIdeally we would not need another parameter on Query.createWeight for MAXSCORE either, but the issue is that depending on whether you want to collect all hits or only the top-scoring ones, then we need different Scorer impls.\n\nThere is the current sorted-index case today, but you have to be a rocket scientist (custom collector) to use it\n\n+1 to fix it and bake support for early termination in TopFieldDocCollector. ",
            "author": "Adrien Grand",
            "id": "comment-16203676"
        },
        {
            "date": "2017-10-13T15:21:05+0000",
            "content": "yeah, I think i am looking at it from the top-down (indexsearcher) vs bottom up (queries).\n\nindexsearcher already knows if scores are needed (e.g. from the Sort), but there is no way to tell it that approximate total hit count is acceptable. If we can do that, then I think we can make the early termination case really easy for the sorted case, index order case, and also this maxscore case.\n\n\nIdeally we would not need another parameter on Query.createWeight for MAXSCORE either, but the issue is that depending on whether you want to collect all hits or only the top-scoring ones, then we need different Scorer impls.\n\nwe do? (genuine question). We added needsScores because previously scorers had to always be ready for you to \"lazily\" call score(), and this prevented scoring from doing much more interesting things up-front like caching whole bitsets, but is it really the case for maxScore? I'm just asking because the new scorer here looks a hell of a lot like a disjunction scorer  If we truly need a different impl, we should maybe still think it thru because of stuff like setMinCompetitiveScore() method, which would make no sense except for that case. I do like that in your patch AssertingScorer checks all that stuff, but there it is a bit confusing. ",
            "author": "Robert Muir",
            "id": "comment-16203693"
        },
        {
            "date": "2017-10-13T16:44:43+0000",
            "content": "indexsearcher already knows if scores are needed (e.g. from the Sort), but there is no way to tell it that approximate total hit count is acceptable. If we can do that, then I think we can make the early termination case really easy for the sorted case, index order case, and also this maxscore case.\n\n+1 We need to add a new boolean needsTotalHits to the search(Query, int) and search(Query, int, Sort) methods.\n\nwe do? [...] I'm just asking because the new scorer here looks a hell of a lot like a disjunction scorer\n\nWell it is a disjunction.  Our regular disjunction scorer maintains a single heap and only looks at the 'top' element and callso updateTop after calls to nextDoc or advance. If you want to be able to call advance() sometimes on low-scoring clauses even if only nextDoc() was called on the disjunction, you need to give it the ability to leave some scorers behind, as long as the sum of the max scores of scorers that are behind and scorers that are positioned on the current candidate is not greater than the minimum competitive score (otherwise you might be missing matches). This means you need to move scorers between at least 2 data-structures. In practice, we actually use 3 of them so that we can also easily differenciate scorers that are on the current candidate from scorers that are too advanced. Moving scorers between those data-structures has some overhead. For instance here is what I get when I benchmark the MaxScoreScorer against master's DisjunctionSumScorer (BS1 is disabled in both cases):\n\n\n                    TaskQPS baseline      StdDev   QPS patch      StdDev                Pct diff\n   HighTermDayOfYearSort       33.34      (7.3%)       16.55      (2.6%)  -50.4% ( -56% -  -43%)\n              OrHighHigh       21.91      (4.1%)       12.36      (1.7%)  -43.6% ( -47% -  -39%)\n               OrHighLow       48.08      (4.0%)       29.27      (2.4%)  -39.1% ( -43% -  -34%)\n               OrHighMed       60.66      (4.0%)       39.32      (2.5%)  -35.2% ( -40% -  -29%)\n                  Fuzzy2      117.30      (7.1%)      101.36      (7.5%)  -13.6% ( -26% -    1%)\n                  Fuzzy1      238.83     (10.7%)      212.80      (7.3%)  -10.9% ( -26% -    7%)\n            OrHighNotLow       70.64      (3.4%)       66.82      (2.0%)   -5.4% ( -10% -    0%)\n            OrHighNotMed       65.59      (3.0%)       62.63      (1.6%)   -4.5% (  -8% -    0%)\n           OrNotHighHigh       34.55      (2.3%)       33.24      (1.1%)   -3.8% (  -6% -    0%)\n           OrHighNotHigh       48.98      (2.4%)       47.17      (1.4%)   -3.7% (  -7% -    0%)\n            OrNotHighMed      107.24      (2.1%)      103.40      (1.1%)   -3.6% (  -6% -    0%)\n                  IntNRQ       26.26     (11.5%)       25.64     (12.3%)   -2.3% ( -23% -   24%)\n              AndHighLow      879.33      (3.7%)      860.16      (3.3%)   -2.2% (  -8% -    4%)\n              AndHighMed      168.90      (1.7%)      165.48      (1.3%)   -2.0% (  -4% -    1%)\n              HighPhrase       20.08      (2.8%)       19.69      (2.7%)   -2.0% (  -7% -    3%)\n         MedSloppyPhrase       15.44      (1.7%)       15.15      (1.8%)   -1.9% (  -5% -    1%)\n             LowSpanNear       44.70      (2.1%)       43.88      (1.9%)   -1.8% (  -5% -    2%)\n               MedPhrase       52.07      (3.2%)       51.16      (3.1%)   -1.7% (  -7% -    4%)\n         LowSloppyPhrase      150.90      (1.5%)      148.62      (1.5%)   -1.5% (  -4% -    1%)\n            OrNotHighLow     1174.47      (3.6%)     1157.52      (4.1%)   -1.4% (  -8% -    6%)\n            HighSpanNear       38.46      (3.0%)       37.92      (2.4%)   -1.4% (  -6% -    4%)\n        HighSloppyPhrase       28.13      (2.4%)       27.76      (2.5%)   -1.3% (  -6% -    3%)\n               LowPhrase      131.67      (1.6%)      130.37      (2.0%)   -1.0% (  -4% -    2%)\n             MedSpanNear       54.75      (3.4%)       54.22      (3.0%)   -1.0% (  -7% -    5%)\n                Wildcard       41.58      (5.1%)       41.23      (5.2%)   -0.8% ( -10% -    9%)\n             AndHighHigh       71.08      (1.5%)       70.61      (1.3%)   -0.7% (  -3% -    2%)\n                 Prefix3       88.11      (6.4%)       87.57      (7.5%)   -0.6% ( -13% -   14%)\n                 MedTerm      235.73      (1.4%)      235.35      (3.7%)   -0.2% (  -5% -    4%)\n                HighTerm      144.06      (1.3%)      143.93      (4.2%)   -0.1% (  -5% -    5%)\n                 LowTerm      711.49      (4.2%)      718.15      (4.7%)    0.9% (  -7% -   10%)\n                 Respell      203.69      (8.6%)      206.72      (7.0%)    1.5% ( -12% -   18%)\n       HighTermMonthSort      196.30      (5.2%)      222.54      (8.2%)   13.4% (   0% -   28%)\n\n\n\nI do like that in your patch AssertingScorer checks all that stuff, but there it is a bit confusing.\n\nCan you elaborate on what you find confusing? This looks similar to how you should not call Score.score() if you passed needsScores=false to me? ",
            "author": "Adrien Grand",
            "id": "comment-16203829"
        },
        {
            "date": "2017-10-13T20:24:42+0000",
            "content": "Thanks for the benchmarking! It is unfortunate we have to make the api more complicated / specialize disjunctions even more, but seems like the right tradeoff i suppose.\n\n\nCan you elaborate on what you find confusing? This looks similar to how you should not call Score.score() if you passed needsScores=false to me?\n\nThat's exactly it, i think we should try to avoid situations like that. its basically the opposite of type-safety, and the more of these conditionals / \"methods you should not call\" that we add, the more confusing it should get. That's why i'm still mulling what we can do to keep scorers simpler...\n\nbut for now, to move along, I think we have some basic idea of what to do to fix indexsearcher (a boolean about whether exact total hits are needed, for various purposes), but yeah lets keep it separate from what we do about createWeight. For the latter maybe an explicit boolean for maxScore is the simplest for now, and we can see where it goes. ",
            "author": "Robert Muir",
            "id": "comment-16204134"
        },
        {
            "date": "2017-10-21T14:33:37+0000",
            "content": "\nStore more metadata in order to be able to compute better higher bounds for the scores, like the maximum term frequency. Maybe even allow similarity-specific stats?\n\nmaximum term frequency may be a reasonable idea. Its imperfect but its simple, can be re-computed on merge, validated by CheckIndex, etc. The downside is that per-term stats are still fairly costly i think for blocktree, so would the performance benefits be worth it? I do like that it'd require no additional storage for an omitTF field since it is 1 there by definition.\n\nfor phrases it could still work too (just take min or max across all the terms in the phrase and pass to sim as a bad approximation). So instead of:\n\n/** Return the maximum score that this scorer may produce.\n  * {@code Float.POSITIVE_INFINITY} is a fine return value if scores are not bounded. */\npublic abstract float maxScore();\n\n\nwe'd have\n\n/** Return the maximum score that this scorer may produce.\n  * {@code Float.POSITIVE_INFINITY} is a fine return value if scores are not bounded.\n  * @param maxFreq maximum possible frequency value\n  */\npublic abstract float maxScore(float maxFreq);\n\n\n\nI wonder how much it would improve your performance for BM25 though? I think we shouldn't add such a stat unless it really helps our defaults due to the costly nature of such statistics (not just cpu/storage, but api complexity, codec requirements, etc). On the side, by my math (just playing around with numbers), its still not enough to make the optimization potent for ClassicSimilarity, which may be the worst-case here  \n\nTheoretically it'd be simple to independently expose the min value for field's norm to get even better (similarity could pull that itself from its LeafReader), but it'd be so fragile: messy data such as a single very short doc for the field will effectively negate that opto for the entire field. Alternatively storing min norm's value per-term might do it, once written it does not change so can be easily merged, cross-verified with norm's value in CheckIndex, etc, but its another thing to benchmark and compare the cost.\n\nAllowing the similarity to store its own stuff seems impractical for a number of reasons: for example we couldn't get any better for BM25 without breaking incremental and distributed search (avgFieldLength unknown until runtime).\n\nAnother way to put it: with the current patch the Similarity has all the raw statistics it needs to compute its maxScore, except that norm and tf values are left unbounded, so i'd rather see those raw statistics stored if we want to improve their maxScore functions. As attractive as it sounds to combine them up-front for the best possible performance, I think its not practical given the features we support. ",
            "author": "Robert Muir",
            "id": "comment-16213939"
        },
        {
            "date": "2017-10-21T15:37:22+0000",
            "content": "Also what does the patch look like if Scorer.maxScore() is abstract instead of defaulting to +Infinity? I think it should be the same for Similarity. It is a little trappy, for example in the current patch you can't see that it is missing for things such as phrase scorers (but should work fine if done the same way as TermScorer). We know there are queryparsers forming disjunctions with these kind of subs and it might be able to help, presuming we can refine the algorithm a bit more. ",
            "author": "Robert Muir",
            "id": "comment-16213972"
        },
        {
            "date": "2017-10-22T20:11:22+0000",
            "content": "Agreed, we should make sure that eg. SynonymQuey exposes a max score. ",
            "author": "Adrien Grand",
            "id": "comment-16214433"
        },
        {
            "date": "2017-10-23T14:29:20+0000",
            "content": "Given how large the patch already is, would you be ok to delay exploring with maxFreq and minNorm, and looking into it in follow-up issues? ",
            "author": "Adrien Grand",
            "id": "comment-16215213"
        },
        {
            "date": "2017-10-23T14:36:55+0000",
            "content": "Yes but can we change the API on SimScorer to maxScore(freq)? We can improve already just by looking if the field omits TF (just pass 1), otherwise we pass Integer.MAX_VALUE.\n\nI am also happy to factor this part of the patch (just the similarity api) out to LUCENE-7997 and add tests for it and discover any potential problems. ",
            "author": "Robert Muir",
            "id": "comment-16215224"
        },
        {
            "date": "2017-10-23T14:48:26+0000",
            "content": "I'll fix it at the same time as I make maxScore abstract on Scorer. ",
            "author": "Adrien Grand",
            "id": "comment-16215238"
        },
        {
            "date": "2017-10-23T14:50:46+0000",
            "content": "I think we can also refine the upperbound to Math.min(Integer.MAX_VALUE, 1 + totalTermFreq - docFreq) today as well. ",
            "author": "Robert Muir",
            "id": "comment-16215244"
        },
        {
            "date": "2017-10-24T08:43:05+0000",
            "content": "Here is an updated patch:\n\n\tScorer.maxScore() is now abstract\n\tSimScorer.maxScore takes a float maxFreq that can be used to better bound the max score (but nothing smart is done about it yet, I'd like to reserve it to follow-ups)\n\tTopDocs.totalHits is no longer -1 when needsTotalHitCount is false, it is now an approximation.\n\n ",
            "author": "Adrien Grand",
            "id": "comment-16216542"
        },
        {
            "date": "2017-10-24T11:46:26+0000",
            "content": "Can we add this comment to DisjunctionMaxScorer (and not just leave at infinity):\n\n// TODO: implement but be careful about floating-point errors.\n\n\n\nCan we fix maxFreq calculation. Currently it is: \n\n// TODO: store the max term freq?\nif (ttf == -1) {\n  return Integer.MAX_VALUE;\n} else {\n  return ttf - df + 1;\n}\n\n\nIt should be:\n\n// TODO: store the max term freq?\nif (ttf == -1) {\n // omitTFAP field, tf values are implicitly 1. \n // note: it seems cleaner to check IndexOptions instead of looking at -1? \n  return 1;\n} else {\n  return Math.min(Integer.MAX_VALUE, ttf - df + 1);\n}\n\n\n\nI only partially reviewed the patch... will look at it again tonight. ",
            "author": "Robert Muir",
            "id": "comment-16216744"
        },
        {
            "date": "2017-10-24T13:32:26+0000",
            "content": "// omitTFAP field, tf values are implicitly 1. \n\nI thought codecs could also deliberately not store ttf even if term frequencies are enabled? At least this is my understanding from the following sentence of TermsEnum.totalTermFreq's javadocs: This will be -1 if the codec doesn't support this measure.. ",
            "author": "Adrien Grand",
            "id": "comment-16216902"
        },
        {
            "date": "2017-10-24T13:39:22+0000",
            "content": "Well that's why i wrote the \"note: \" \n\nTechnically the better code would be:\n\nif (indexOptions = DOCS_ONLY) {\n  return 1; // tf values are omitted\n}\n...\n\n\n\nAnd yes when frequencies are present, according to the docs its allowed to return -1 here, but that was solely motivated by PreFlexCodec: (Lucene 3.x indexes). What actually returns -1 here for this case? \n\nWe will never have maxScore working as long as we have such complexity: lets remove the problem. I faced this same issue in LUCENE-7997 and i think its easier to just enforce that docCount and sumDocFreq are always present, and that sumTotalTermFreq, and totalTermFreq are always present when term freqs are stored. ",
            "author": "Robert Muir",
            "id": "comment-16216916"
        },
        {
            "date": "2017-10-24T13:47:10+0000",
            "content": "I changed the unreasonable fix version to set proper expectations, this isn't something we can introduce in a minor release, the underlying changes (e.g. to similarity) are heavy and we need to simplify stuff / do it correctly to prevent a mess. ",
            "author": "Robert Muir",
            "id": "comment-16216925"
        },
        {
            "date": "2017-11-30T16:19:35+0000",
            "content": "Updated patch that applies Robert's feedback and simplifies things a bit by assuming that scores are positive for instance (LUCENE-7996). ",
            "author": "Adrien Grand",
            "id": "comment-16272886"
        },
        {
            "date": "2017-12-07T23:48:23+0000",
            "content": "More Solr tests than usual are failing after the 4fc5a872d commit on this issue, e.g. TestHashQParserPlugin.testHashPartition, which fails for me with any seed.  From https://builds.apache.org/job/Lucene-Solr-Tests-master/2210/:\n\n\nChecking out Revision 5448274f26191a9882aa5c3020e3cbdcbf93551c (refs/remotes/origin/master)\n[...]\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestHashQParserPlugin -Dtests.method=testHashPartition -Dtests.seed=B372ECC8951DB18F -Dtests.multiplier=2 -Dtests.slow=true -Dtests.locale=cs -Dtests.timezone=Asia/Jayapura -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1\n   [junit4] ERROR   3.01s J0 | TestHashQParserPlugin.testHashPartition <<<\n   [junit4]    > Throwable #1: java.lang.UnsupportedOperationException: Query  does not implement createWeight\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([B372ECC8951DB18F:62CFCD2DAE7708E4]:0)\n   [junit4]    > \tat org.apache.lucene.search.Query.createWeight(Query.java:66)\n   [junit4]    > \tat org.apache.lucene.search.IndexSearcher.createWeight(IndexSearcher.java:734)\n   [junit4]    > \tat org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:724)\n   [junit4]    > \tat org.apache.solr.search.SolrIndexSearcher.getProcessedFilter(SolrIndexSearcher.java:1062)\n   [junit4]    > \tat org.apache.solr.search.SolrIndexSearcher.getDocListNC(SolrIndexSearcher.java:1540)\n   [junit4]    > \tat org.apache.solr.search.SolrIndexSearcher.getDocListC(SolrIndexSearcher.java:1416)\n   [junit4]    > \tat org.apache.solr.search.SolrIndexSearcher.search(SolrIndexSearcher.java:583)\n   [junit4]    > \tat org.apache.solr.handler.component.QueryComponent.doProcessUngroupedSearch(QueryComponent.java:1435)\n   [junit4]    > \tat org.apache.solr.handler.component.QueryComponent.process(QueryComponent.java:375)\n   [junit4]    > \tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:295)\n   [junit4]    > \tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:177)\n   [junit4]    > \tat org.apache.solr.core.SolrCore.execute(SolrCore.java:2503)\n   [junit4]    > \tat org.apache.solr.util.TestHarness.query(TestHarness.java:337)\n   [junit4]    > \tat org.apache.solr.util.TestHarness.query(TestHarness.java:319)\n   [junit4]    > \tat org.apache.solr.search.TestHashQParserPlugin.testHashPartition(TestHashQParserPlugin.java:89)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:748)\n[...]\n   [junit4]   2> NOTE: test params are: codec=Lucene70, sim=Asserting(org.apache.lucene.search.similarities.AssertingSimilarity@68e611ad), locale=cs, timezone=Asia/Jayapura\n   [junit4]   2> NOTE: Linux 3.13.0-88-generic amd64/Oracle Corporation 1.8.0_144 (64-bit)/cpus=4,threads=1,free=284221600,total=403177472\n\n \n\nMy browser says that does not implement createWeight occurs 48 times in the https://builds.apache.org/job/Lucene-Solr-Tests-master/2210/consoleText, so this is a problem for several tests. ",
            "author": "Steve Rowe",
            "id": "comment-16282738"
        },
        {
            "date": "2017-12-08T07:50:36+0000",
            "content": "Commit 0e1d6682d6ca66590e279ee0c4ccce745f2accd6 in lucene-solr's branch refs/heads/master from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0e1d668 ]\n\nLUCENE-4100: Fix more queries to implement the new updated createWeight API. ",
            "author": "ASF subversion and git services",
            "id": "comment-16283172"
        }
    ]
}