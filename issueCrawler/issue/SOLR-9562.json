{
    "id": "SOLR-9562",
    "title": "Minimize queried collections for time series alias",
    "details": {
        "components": [],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Reopened",
        "resolution": "Unresolved",
        "priority": "Minor"
    },
    "description": "For indexing time series data(such as large log data), we can create a new collection regularly(hourly, daily, etc.) with a write alias and create a read alias for all of those collections. But all of the collections of the read alias are queried even if we search over very narrow time window. In this case, the docs to be queried may be stored in very small portion of collections. So we don't need to do that.\n\nI suggest this patch for read alias to minimize queried collections. Three parameters for CREATEALIAS action are added.\n\n\n\n\n Key \n Type \n Required \n Default \n Description \n\n\n timeField \n string \n No \n\u00a0\n The time field name for time series data. It should be date type. \n\n\n dateTimeFormat \n string \n No \n\u00a0\n The format of timestamp for collection creation. Every collection should has a suffix(start with \"_\") with this format. \nEx. dateTimeFormat: yyyyMMdd, collectionName: col_20160927\nSee DateTimeFormatter. \n\n\n timeZone \n string \n No \n\u00a0\n The time zone information for dateTimeFormat parameter.\nEx. GMT+9. \nSee DateTimeFormatter. \n\n\n\n\n\n\nAnd then when we query with filter query like this \"timeField:[fromTime TO toTime]\", only the collections have the docs for a given time range will be queried.",
    "attachments": {
        "SOLR-9562-v2.patch": "https://issues.apache.org/jira/secure/attachment/12831449/SOLR-9562-v2.patch",
        "SOLR-9562.patch": "https://issues.apache.org/jira/secure/attachment/12830268/SOLR-9562.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-09-26T12:58:48+0000",
            "author": "David Smiley",
            "content": "Thanks for contributing. I'm missing something... why is this metadata on a Collection Alias?  What do Collection Aliases logically have to do with this feature?  Wouldn't associating with the Shard be better, assuming a design in which there is one Collection & manual sharding?\n\nBTW I consider SimpleDateFormat and friends a dead API with the advent of Java 8's new time API: https://docs.oracle.com/javase/8/docs/api/java/time/package-summary.html ",
            "id": "comment-15522986"
        },
        {
            "date": "2016-09-27T04:31:52+0000",
            "author": "Eungsop Yoo",
            "content": "\nThanks for contributing. I'm missing something... why is this metadata on a Collection Alias? What do Collection Aliases logically have to do with this feature? Wouldn't associating with the Shard be better, assuming a design in which there is one Collection & manual sharding?\nI run a SolrCloud cluster for indexing log data which has 10 billion docs a day and the log data are kept for 10 days. So I create a new collection per a day time frame and delete the oldest collection every day. Read and write aliases are created for those collections. I use Banana to query from SolrCloud with read alias. I think that using read alias is the most transparent way for rolling collections for the Solr client such as Banana.\nSo some metadata are added to Alias.\n\n\nBTW I consider SimpleDateFormat and friends a dead API with the advent of Java 8's new time API: https://docs.oracle.com/javase/8/docs/api/java/time/package-summary.html\nI see. ",
            "id": "comment-15525038"
        },
        {
            "date": "2016-10-04T02:35:55+0000",
            "author": "Eungsop Yoo",
            "content": "Some bugs are fixed.\nSimpleDateFormat is replaced with DateTimeFormatter. ",
            "id": "comment-15544124"
        },
        {
            "date": "2016-10-04T02:43:37+0000",
            "author": "Eungsop Yoo",
            "content": "I backported this patch to my own cluster, Solr 4.10.3-cdh5.4.9.\nIt took over 20 seconds to query against last 30 minutes over the collections of 14 days without this patch, but it takes only 3 seconds now. ",
            "id": "comment-15544140"
        },
        {
            "date": "2016-10-05T15:37:35+0000",
            "author": "David Smiley",
            "content": "Sorry, but IMO aliasing by the collection just doesn't make sense to me.  This approach would lead to potentially lots of collections \u2013 no thanks; I'd much rather have lots of shards.  I think it better conceptually matches what's going on, and it doesn't pollute the collection namespace.  If somehow sharding by time would be difficult for clients (e.g. Banana), I'd like to learn more about how that would be so \u2013 and if we find any issue there then I think fixing that would make more sense than sharding by collection. ",
            "id": "comment-15549087"
        },
        {
            "date": "2016-10-06T06:04:51+0000",
            "author": "Eungsop Yoo",
            "content": "I see. \n\nI found some articles related to this issue and read them.\nhttp://stackoverflow.com/questions/32343813/custom-sharding-or-auto-sharding-on-solrcloud\nhttps://cwiki.apache.org/confluence/display/solr/Shards+and+Indexing+Data+in+SolrCloud?focusedCommentId=61317676#comment-61317676\n\nOn manual sharding, the client should do some work related to shard for indexing and querying. But It seems this work can be moved to SolrCloud server from the client. So we can make new time series router which does the works related to sharding for time series data. How do you think about this approach? ",
            "id": "comment-15550996"
        },
        {
            "date": "2016-10-06T15:31:22+0000",
            "author": "David Smiley",
            "content": "On manual sharding, the client should do some work related to shard for indexing and querying. But It seems this work can be moved to SolrCloud server from the client. So we can make new time series router which does the works related to sharding for time series data. How do you think about this approach?\n\nAbsolutely \u2013 that's definitely how I think it should be.  It's a shame users/clients have to do time sharding manually today; it'd be wonderful if Solr supported this directly!  Basically a custom DocRouter that assumes a date field in the document.  At search time, Solr might look for the presence of a filter query of a date range on this field, and do intelligent routing accordingly. ",
            "id": "comment-15552230"
        },
        {
            "date": "2016-10-06T15:59:31+0000",
            "author": "Jan H\u00f8ydahl",
            "content": "it'd be wonderful if Solr supported this directly!\nYes. If dateRange was a property of each shard, stored in ZK, then shards param could be intelligently chosen during querying.\nAlso, perhaps Solr at some point could have replication on demand, i.e. be able to change replicationFactor on a per-shard basis based on which one gets the most traffic. Many time-series apps have a \"last 30 days\" search mode which would only hit a few shards, which would need some more replicas than the older data. Hey, it could even unload cores of shards that have not been used in a certain time, and load on demand. Wrt deleting content older than N days we already have TTL for that, but perhaps Solr would need to detect empty shards and delete them? ",
            "id": "comment-15552329"
        },
        {
            "date": "2016-10-06T16:24:26+0000",
            "author": "Erick Erickson",
            "content": "We can already add custom properties to replicas, so a mechanism exists for putting this stuff in the replica information. Not quite sure if that is the right place for them to go though since this is more a shard-level property than replica-level.\n\nI'd guess a specialized TimeSeriesUpdateHandler and TimeSeriesQueryComponent would keep this kind of support from interfering with the un-specialized case. The query component/router could then \"know\" what to look at to route the queries. However, before going there how much effort is really spent on a filter query that has no hits? Is it worth the complexity? Hmmm, I guess there'd be more work than I thought if there are multiple fq clauses.  What I'm wondering is if the right place to put this would be in the query routing or at at each replica level. I.e. build a component that intelligently evaluated fq clauses (based on configuration) and short-circuited the rest of the query processing. Hmmmm2. If we ever changed fq processing to handle implicit union rather than intersection that'd break wouldn't it? Siiiiggggh....\n\nAs far as the auto-scaling stuff goes, there's the autoAddReplica work, but that requires shared filesystems, right?\n\nI'm a little leery of how automatically adding replicas would work in practice. Detecting load and adding a replica would entail replicating the entire index (which may be huge) precisely at a time when the system was busy. Would scheduling it be better? I.e. \"at midnight, remove extra replicas of shards 3 days old and add extra replicas for the most recent shard\" or something. Operations people can break out in hives when programs start messing with their network unpredictably. ",
            "id": "comment-15552397"
        },
        {
            "date": "2016-10-06T16:31:26+0000",
            "author": "David Smiley",
            "content": "Woah now... one step at a time   All that sounds interesting but lets not get ahead of ourselves.  Step one is simply about auto doc routing, which would be wonderful and useful unto itself.  Next step is routing queries... but yes we can ascertain how important this is; maybe it isn't or can be optimized in other generic ways that don't directly have to do with this feature.  Future steps that really have nothing to do with the aforementioned feature would be auto-scaling or per-shard replication factor. ",
            "id": "comment-15552412"
        },
        {
            "date": "2016-10-07T03:55:13+0000",
            "author": "Eungsop Yoo",
            "content": "I run a SolrCloud cluster for log indexing with a daily created collection which has only 1 replica and multiple shards. (But they are stored in HDFS with 3 replica and autoAddReplicas feature is enabled.) In my use case the query performance doesn't matter so 1 replica would be enough. The indexing performance for given system resources is best with 1 replica. But in some other use cases your idea would make sense. \nDeleting TTL expired documents(SOLR-5795) is not efficient for large log data. So I create and delete a daily collection every morning in my crontab. We need to find a smarter way for maintaining collections or shards of time series data. ",
            "id": "comment-15554031"
        },
        {
            "date": "2016-10-07T03:56:14+0000",
            "author": "Eungsop Yoo",
            "content": "Yes, AutoAddReplicas requires shared file systems. Actually my cluster is running on HDFS(replication factor 3) with 1 replica and AutoAddReplicas enabled. AutoAddReplicas feature works so so. At first there was a bug of missing docs during failover(SOLR-9236), but it is fixed now. But there is still a problem. It takes very long time to failover, especially transaction log replaying takes longer time than I expect. So I keep tlogs as small as possible now. ",
            "id": "comment-15554034"
        },
        {
            "date": "2016-10-07T04:06:34+0000",
            "author": "Eungsop Yoo",
            "content": "I will come back sooner or later. It would be better to open new issue for time series router, not this? ",
            "id": "comment-15554054"
        },
        {
            "date": "2016-10-07T15:25:47+0000",
            "author": "David Smiley",
            "content": "Yeah; closing as won't fix.  Thanks for looking to contribute to this Eungsop! ",
            "id": "comment-15555375"
        },
        {
            "date": "2017-08-03T05:59:07+0000",
            "author": "David Smiley",
            "content": "tl;dr: I'm re-opening for further discussion on the merits of collection based time series.  I'm leaning towards this solution now.  \n\nI did look at the patch closer.  It solves one aspect of time series, namely what the title says: \"minimize queried collections for time series alias\" (an optimization); and that's it.  That is okay as a first step I guess. It's still on the client to write new data to the appropriate collection (routing writes), and also to eventually delete the oldest collections, and to actually create new collections and perhaps add to an alias while it's at it.  So yeah most things are for other issues \n\nElsewhere, Jan H\u00f8ydahl said:\nQuestion: Is shard-level the best abstraction here or could time-based use cases just as well be solved on the collection level? Create a write-alias pointing to the newest collection, and read aliases pointing to all or some other subset of collections. In this setup, newer collections could have larger replicationFactor to support more queries. And you could reduce #shards for older collections, merge collections and define the oldest collections as \"archive\" which are loaded lazily on demand only etc... People do this already and one could imagine built-in support for all the collection creation and alias housekeeping.\n\nSounds good but I have some questions about some of the potential bonus features you mentioned.  Like what is the methodology to reducing the numShards of a collection while keeping overall data set searchable and no oddities like temporarily searching/counting copies of the same document?  And likewise for merging collections?\n\nA disadvantage to the shard/DocRouter approach (SOLR-9690) is that the numShards & replicationFactor, and node placement rules etc. are fixed and governed at the collection level, not per-shard. But if shard==time-slice then there's a good chance we want to make different choices for the most recent shard. And there are still scalability issues (Overseer related) with very large numbers of shards that is not present if done at the collection level (will be solved eventually I'm sure).  I think for this feature we ought to cater to very high scalability in diverse use cases, and that probably means collection based time slices.  \n\nMaybe there's room for a hybrid where shard based time series is used for all data, but it is augmented by an additional \"realtime\" collection (optional feature) for the most recent data that can of course have it's own configuration catering to both realtime search and high write volume.  Then we devise a way to move the data from the RT collection to the archive collection.  Perhaps a big optimize and then copy the segment(s) across the RT shards over to one new index involving the MERGEINDEXES admin command.  I spoke about this hybrid as a small piece of a larger talk at last year's L/S Revolution but didn't ultimately have time to implement this strategy.  I did at least get to much of the shard based time series portion.\n\nErik Hatcher I wonder if LucidWorks might be interested in open-sourcing Fusion's time series capability, assuming it's in a suitable shape to be donated (e.g. written in Java, etc.)?  I've seen it but not tried it; I don't have insight into the particulars of it's approach.  Regardless I've set aside my time to improve Solr to help get something committed so that Solr has this capability (be it collection based time slices or shard based time slices). ",
            "id": "comment-16112233"
        },
        {
            "date": "2017-08-14T10:47:20+0000",
            "author": "Jan H\u00f8ydahl",
            "content": "what is the methodology to reducing the numShards of a collection while keeping overall data set searchable and no oddities like temporarily searching/counting copies of the same document? And likewise for merging collections?\n\nI wish I had all the answers  Regarding reduce shards, perhaps a pure merge would be simplest, i.e. if you have 6 shards to tackle high indexing rate and NRT, then the API would support a reduce to 3 shards. Reducing to some other number would be harder and likely require re-indexing (streaming) all docs into a new shadow collection? The same could be done for merging collections - stream all docs from collection 1 and 2 into new collection 3. ",
            "id": "comment-16125525"
        },
        {
            "date": "2017-10-16T13:11:22+0000",
            "author": "Radu Gheorghe",
            "content": "My two cents:\n\n\tif data is relatively low-velocity, merging shards of an existing collection that's already \"done\" (e.g. yesterday's collection) by the way of a pure merge should help with scaling the cluster. Here's how Elasticsearch does it: https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-shrink-index.html\n\tif data is high-velocity, one will likely have to to live with the trade-off between many collections (i.e. rotate them more frequently, which would then write faster because of less merging and read faster because they are \"done\" faster => better caching for those \"wrapped up\" collections) or less collections (which imply less shards). I'm saying this because the benefits of merging shards may not be worth the overhead\n\n\n\nThat said, loading/unloading shards might help reduce the overhead of many shards, assuming that old data is rarely touched. I'm probably getting way ahead of myself here, but a read alias that would automatically load shards (that would be closed from a cronjob looking at activity) would be pretty awesome (especially if we think about them in the context of AutoScaling and shared file systems). ",
            "id": "comment-16205860"
        },
        {
            "date": "2017-10-16T13:56:26+0000",
            "author": "Erick Erickson",
            "content": "Radu:\n\nbq: That said, loading/unloading shards might help reduce the overhead of many shards, assuming that old data is rarely touched\n\nIn stand-alone mode there's the whole \"transient core\" concept, essentially Solr cores are cached in a size-limited cache. When an operation is performed on a core, if it's not already in the cache it's loaded on the fly and if loading it goes over the cache size, the least-recently-used core is unloaded. This is all totally automatic, the only thing the user has to configure is the size of the cache.\n\nThis has not been worked through with SolrCloud, all the decisions are made locally. The problems I foresee in the general SolrCloud case mainly have to do with thrashing when, say, updates are distributed... all the replicas for all the shards receiving updates would have to be loaded. There'd need to be some kind of way to re-use replicas in a shard for queries until traffic exceeded some limit (why should Solr reload 10 replicas for a shard for 10 different queries if the QPS rate was 10/minute?). Perhaps some of the new metrics could be used for that case....\n\nAnyway, the transient core stuff was never envisioned with SolrCloud in mind, but it might be useful in this case. ",
            "id": "comment-16205928"
        },
        {
            "date": "2017-10-16T16:09:07+0000",
            "author": "Radu Gheorghe",
            "content": "I didn't know about the transient cores, it sounds like a cool concept. I think it would be a great fit for the time-series use-case, because:\n\n\tnormally you'd only write to the latest collection. Maybe we could even have a configurable limit on how far back we could backfill (which is reasonable for most use-cases)\n\tnormally you wouldn't have many replicas anyway, or maybe we can configure how many replicas to load based on X metrics? This sounds like a case for AutoScaling again \n\n ",
            "id": "comment-16206114"
        },
        {
            "date": "2018-04-04T08:26:34+0000",
            "author": "mosh",
            "content": "I was looking into implementing this, and noticed a few things:\n\n\tThe requested parameters\u00a0timeField, dateTimeFormat, timeZone should be available from the alias metadata, making them redundant.\n\tThe old patch routing logic is implemented in solr-core, perhaps it is better implemented in solrj in the CloudSolrClient class.\n\n\n\nI propose\u00a0we should add a\u00a0method isTimeSeriesAlias, and add a TimeSeries query router which will return the collections which are valid to the time range.\n Waiting to hear your inputs, or counter-proposals ",
            "id": "comment-16425181"
        },
        {
            "date": "2018-04-05T19:30:01+0000",
            "author": "David Smiley",
            "content": "Hello mosh. \nYes, all the settings are now in \"alias properties\" (originally called alias metadata).  There's also a TimeRoutedAlias class in solr-core that parses TRA properties and includes logic to parse the timestamps from the collection names.  Doing the distributed search at the client is impossible; all of Solr's distributed search logic (e.g. merging shard results) is in solr-core.  We very likely ought to have dedicated request parameters to specify the time range since reverse engineering the time range from filter queries 'n such will be brittle and problematic (e.g. consider facets that exclude filters).  I haven't put much thought into this side of things yet; I'm still focused on /update routing efficiency & hardening.  We won't get in each other's way should you want to pursue this. ",
            "id": "comment-16427486"
        },
        {
            "date": "2018-04-09T15:05:05+0000",
            "author": "David Smiley",
            "content": "Mosh, I suppose you meant that CloudSolrClient could determine the set of collections to be searched at its side and then send them via collection=.... That's possible... but if done at the Solr side then clients other than Java could leverage this. ",
            "id": "comment-16430652"
        }
    ]
}