{
    "id": "LUCENE-1612",
    "title": "expose lastDocId in the posting from the TermEnum API",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "2.4",
        "resolution": "Invalid",
        "status": "Resolved"
    },
    "description": "We currently have on the TermEnum api: docFreq() which gives the number docs in the posting.\nIt would be good to also have the max docid in the posting. That information is useful when construction a custom DocIdSet, .e.g determine sparseness of the doc list to decide whether or not to use a BitSet.\n\nI have written a patch to do this, the problem with it is the TermInfosWriter encodes values in VInt/VLong, there is very little flexibility to add in lastDocId while making the index backward compatible. (If simple int is used for say, docFreq, a bit can be used to flag reading of a new piece of information)\n\noutput.writeVInt(ti.docFreq);                       // write doc freq\n    output.writeVLong(ti.freqPointer - lastTi.freqPointer); // write pointers\n    output.writeVLong(ti.proxPointer - lastTi.proxPointer);\n\nAnyway, patch is attached with:TestSegmentTermEnum modified to test this. TestBackwardsCompatibility fails due to reasons described above.",
    "attachments": {
        "lucene-1612-patch.txt": "https://issues.apache.org/jira/secure/attachment/12406414/lucene-1612-patch.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-04-25T03:03:54+0000",
            "content": "Patch attach with test. Index is not backwards compatible. ",
            "author": "John Wang",
            "id": "comment-12702646"
        },
        {
            "date": "2009-04-25T09:32:19+0000",
            "content": "This would be a good test (custom codec) for flexible indexing (LUCENE-1458), ie, allowing you to write whatever you want per-term.\n\nAlso, if lastDocID is always available, this could make merging of postings much faster than it is today, because you could bulk-copy the doc/freq posting bytes while just \"fixing up\" the boundary between them, because docIDs are delta coded. ",
            "author": "Michael McCandless",
            "id": "comment-12702669"
        },
        {
            "date": "2009-04-25T13:17:24+0000",
            "content": "Excellent point Michael! What do you suggest on how to move forward with this? ",
            "author": "John Wang",
            "id": "comment-12702692"
        },
        {
            "date": "2009-04-25T14:37:09+0000",
            "content": "Well... a couple problems with always doing this:\n\n\n\tThe in-memory terms index now consumes another 4 bytes per indexed term\n\n\n\n\n\tThe tii/tis files got larger\n\n\n\nOne way to optimize it might be to only record it for terms whose freq is > X (and for the long tail of low-freq terms you iterate its postings to get the last docID).\n\nAlso, most apps don't need this information.  So I don't think we should turn this on, always.\n\nSo maybe we wait for LUCENE-1458 and then build this as an alternate codec? ",
            "author": "Michael McCandless",
            "id": "comment-12702706"
        },
        {
            "date": "2009-04-25T15:08:26+0000",
            "content": "So maybe we wait for LUCENE-1458 and then build this as an alternate codec?\n\n+1, seems pretty specialized. ",
            "author": "Yonik Seeley",
            "id": "comment-12702710"
        },
        {
            "date": "2009-04-25T23:56:20+0000",
            "content": "I am fine with waiting for LUCENE-1458. But Michael, then how would it help the merge of postings you described? Merging would be outside of the codec, no? ",
            "author": "John Wang",
            "id": "comment-12702827"
        },
        {
            "date": "2009-04-26T00:02:16+0000",
            "content": "Actually I think the codec will handle merging (this was recently proposed for payloads), so it should be able to do that optimization within itself. ",
            "author": "Michael McCandless",
            "id": "comment-12702829"
        }
    ]
}