{
    "id": "LUCENE-6814",
    "title": "PatternTokenizer indefinitely holds heap equal to max field it has ever tokenized",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [
            "5.4",
            "6.0"
        ],
        "priority": "Major",
        "status": "Closed",
        "type": "Bug"
    },
    "description": "Caught by Alex Chow in this Elasticsearch issue: https://github.com/elastic/elasticsearch/issues/13721\n\nToday, PatternTokenizer reuses a single StringBuilder, but it doesn't free its heap usage after tokenizing is done.  We can either stop reusing, or ask it to .trimToSize when we are done ...",
    "attachments": {
        "LUCENE-6814.patch": "https://issues.apache.org/jira/secure/attachment/12762088/LUCENE-6814.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14906061",
            "author": "Uwe Schindler",
            "date": "2015-09-24T08:59:20+0000",
            "content": "It should call something like setLength(MIN_SIZE).trimToSize() on close(). "
        },
        {
            "id": "comment-14906065",
            "author": "Michael McCandless",
            "date": "2015-09-24T09:04:04+0000",
            "content": "on close().\n\nMaybe on end() instead? "
        },
        {
            "id": "comment-14906076",
            "author": "Uwe Schindler",
            "date": "2015-09-24T09:10:31+0000",
            "content": "makes no sense. close() is correct - see documentation  "
        },
        {
            "id": "comment-14906078",
            "author": "Michael McCandless",
            "date": "2015-09-24T09:11:28+0000",
            "content": "Patch, setting length and trimming in end() .... "
        },
        {
            "id": "comment-14906080",
            "author": "Michael McCandless",
            "date": "2015-09-24T09:13:04+0000",
            "content": "makes no sense. close() is correct - see documentation\n\nOK right, sorry \n\nclose() is in fact called each time we analyze a String/Reader ... I always get confused by this  "
        },
        {
            "id": "comment-14906115",
            "author": "Michael McCandless",
            "date": "2015-09-24T09:42:36+0000",
            "content": "New patch, moving the clearing to close and adding a fun test case. "
        },
        {
            "id": "comment-14906171",
            "author": "Alex Chow",
            "date": "2015-09-24T10:25:37+0000",
            "content": "Thanks for taking a look at this, Michael McCandless!\n\nIt'd be great if this can get added to 4.10 so elasticsearch 1.x can pull it in too.\n\n(Never mind the extra stuff that was here. It all made sense when I tried looking through it) "
        },
        {
            "id": "comment-14906550",
            "author": "Michael McCandless",
            "date": "2015-09-24T16:02:27+0000",
            "content": "Commenting on the stuff you edited away because they are good confusions!\n\nI'm curious why there shouldn't there be some trimming in `end()` as well? Or is a `TokenStream` meant to be used only once (no multiple `reset()`, `incrementToken()`, `end()` on the same `TokenStream`)?\n\nThe TokenStream API is confusing \n\nI started with end here too (it seemed correct) but it turns out close is also called (internally, in Lucene's IndexWriter) after all tokens are iterated for a single input, but close is called even on exception (but end is not necessarily I think).\n\nThe TokenStream instance is typically thread-private, and re-used (for that one thread) for analysis on future docs.\n\nElasticsearch seems to never reinstantiate Tokenizers and just reuses them for each field in an index, though I may be wrong. Or elasticsearch is using TokenStream the wrong way?\n\nES using Lucene's Analyzer (well, DelegatingAnalyzerWrapper I think), which (by default) reuses the Tokenizer instance, per thread.\n\nIt'd be great if this can get added to 4.10 so elasticsearch 1.x can pull it in too.\n\nI think it's unlikely Lucene will have another 4.10.x release, and ES is releasing 2.0.0 (using Lucene 5.3.x) shortly.\n\nCan you describe what impact you're seeing from this bug?  How many PatternTokenizer instances is ES keeping in your case, how large are your docs, etc.?  You could probably lower the ES bulk indexing thread pool size (if you don't in fact need so much concurrency) to reduce the impact of the bug ...\n\nI think this bug means PatternTokenizer holds onto the max sized doc it ever saw in heap right?  Does StringBuilder ever reduce its allocated space by itself... "
        },
        {
            "id": "comment-14906601",
            "author": "Uwe Schindler",
            "date": "2015-09-24T16:30:26+0000",
            "content": "Tokenstream consumer workflow is clearly defined:\nhttps://lucene.apache.org/core/5_3_1/core/org/apache/lucene/analysis/TokenStream.html\n\nThe last step is close(). There is nothing confusing, just RTFM.\n\nclose() is defined as \"Releases resources associated with this stream.\", and that's what we do here. end() has a different meaning. Its sole purpose is to get the \"token\" information after the very last token. At this time all resources still need to be hold, because the attributes must be accessible after this call. After close() the resources are freed. In fact, this does not make a difference for this tokenizer, but in general stuff like termattribute must still be in a valid state. "
        },
        {
            "id": "comment-14906797",
            "author": "Alex Chow",
            "date": "2015-09-24T18:33:09+0000",
            "content": "\nTokenstream consumer workflow is clearly defined:\nhttps://lucene.apache.org/core/5_3_1/core/org/apache/lucene/analysis/TokenStream.html\nThe last step is close(). There is nothing confusing, just RTFM.\n\nYeah, just took a little bit to fully understand how things interact with reuse (\"expert mode\").\n\nI think it's unlikely Lucene will have another 4.10.x release, and ES is releasing 2.0.0 (using Lucene 5.3.x) shortly.\n\nThat's pretty unfortunate. The tracker suggests that there are some patches slated for 4.10.5. Is 4.10 dead now?\n\nI'm not able to find what the plan for 1.x is when 2.0 hits GA. 1.7 was cut in July, so that release is probably going to be supported through early 2017. I'll probably have to push for the PatternTokenizer fork to get it into 1.x?\n\nCan you describe what impact you're seeing from this bug? How many PatternTokenizer instances is ES keeping in your case, how large are your docs, etc.? You could probably lower the ES bulk indexing thread pool size (if you don't in fact need so much concurrency) to reduce the impact of the bug...\n\nOur setup has 24 bulk indexing threads, and at peak we go through about 18 tasks/s (8k documents indexed per bulk request) per node with 21 nodes and 168 indices.\n\nWe've been making an effort to reduce heap sizes (from 22gb to 8gb using 3x nodes), and PatternTokenizer ends up retaining around 2gb before we get into a GC death spiral (CMS with instantiating occupancy=75%) but would otherwise grow a bit more. The biggest PatternTokenizer instances get to about 3-4mb. SegmentReaders occupy 3.5gb per node, so there's not too much space to work with unless we want to increase heap at least 1.5x. This pretty much destroys scaling horizontally and ends up being more... diagonally with volume depending on the data?\n\nI'm pretty surprised nobody has really noticed this so far. Does everyone just like huge heaps (or just not use PatternAnalyzer)?\n\nI think this bug means PatternTokenizer holds onto the max sized doc it ever saw in heap right? Does StringBuilder ever reduce its allocated space by itself...\n\nI think this ends up growing to the max sized field. As far as I can tell trimToSize is the only for StringBuilder to shrink its buffer, and even then documentation suggests it isn't guaranteed (lots of \"may\")... "
        },
        {
            "id": "comment-14907036",
            "author": "Michael McCandless",
            "date": "2015-09-24T21:10:08+0000",
            "content": "Is 4.10 dead now?\n\nIt's not that it's dead, it's more that there would need to be a really really bad bug to warrant another 4.10.x release at this point.  We are already well into 5.x, and just released 5.3.1 bug fix today: http://lucene.apache.org/#24-september-2015-apache-lucene-531-and-apache-solr-531-available\n\nI'll probably have to push for the PatternTokenizer fork to get it into 1.x?\n\nYes, maybe ... or patch your ES locally with the change?\n\nand PatternTokenizer ends up retaining around 2gb\n\nHmm, how many PatternTokenizer instances do you have?  With 24 bulk indexing threads you should (I think?) only have at most 24 instances * 4 MB max should be ~100 MB.\n\nDoes everyone just like huge heaps (or just not use PatternAnalyzer)?\n\nI suspect PatternTokenizer is not commonly used ...\n\nThere is a TODO in the code to fix this class to not hold an entire copy of the document ...\n\nI think this ends up growing to the max sized field. \n\nOK I'll put that in the CHANGES when I commit, and fix this issue title. "
        },
        {
            "id": "comment-14907054",
            "author": "Alex Chow",
            "date": "2015-09-24T21:20:25+0000",
            "content": "Hmm, how many PatternTokenizer instances do you have? With 24 bulk indexing threads you should (I think?) only have at most 24 instances * 4 MB max should be ~100 MB.\n\nPatternTokenizer instances also scale with the number of indices (technically shards?) if you use your own analyzer based on a PatternAnalyzer. Custom analyzers get instantiated per index since they have their own mappings. We have 168 indices, so that's a lot. One heap dump I was poking around had 3432 instances, but it'd probably have grown more since some analyzers haven't had to create their TokenStreamComponents yet. "
        },
        {
            "id": "comment-14992374",
            "author": "ASF subversion and git services",
            "date": "2015-11-05T20:09:01+0000",
            "content": "Commit 1712863 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1712863 ]\n\nLUCENE-6814: release heap in PatternTokenizer.close "
        },
        {
            "id": "comment-14992377",
            "author": "ASF subversion and git services",
            "date": "2015-11-05T20:11:00+0000",
            "content": "Commit 1712865 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1712865 ]\n\nLUCENE-6814: release heap in PatternTokenizer.close "
        },
        {
            "id": "comment-14992378",
            "author": "Michael McCandless",
            "date": "2015-11-05T20:11:24+0000",
            "content": "If we ever do another 4.10.x release we should backport this one ... "
        }
    ]
}