{
    "id": "LUCENE-6214",
    "title": "IW deadlocks if commit and reopen happens concurrently while exception is hit",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "5.0",
        "components": [],
        "labels": "",
        "fix_versions": [
            "4.10.4",
            "5.0",
            "5.1",
            "6.0"
        ],
        "priority": "Blocker",
        "status": "Closed",
        "type": "Bug"
    },
    "description": "I just hit this while working on an elasticseach test using a lucene 5.1 snapshot (5.1.0-snapshot-1654549). The test throws random exceptions via MockDirWrapper and deadlocks, jstack says:\n\n\nFound one Java-level deadlock:\n=============================\n\"elasticsearch[node_2][refresh][T#2]\":\n  waiting to lock monitor 0x00007fe51314c098 (object 0x00000007018ee8d8, a java.lang.Object),\n  which is held by \"elasticsearch[node_2][generic][T#1]\"\n\"elasticsearch[node_2][generic][T#1]\":\n  waiting to lock monitor 0x00007fe512d74b68 (object 0x00000007018ee8e8, a java.lang.Object),\n  which is held by \"elasticsearch[node_2][refresh][T#2]\"\n\nJava stack information for the threads listed above:\n===================================================\n\"elasticsearch[node_2][refresh][T#2]\":\n\tat org.apache.lucene.index.IndexWriter.tragicEvent(IndexWriter.java:4441)\n\t- waiting to lock <0x00000007018ee8d8> (a java.lang.Object)\n\tat org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:436)\n\t- locked <0x00000007018ee8e8> (a java.lang.Object)\n\tat org.apache.lucene.index.StandardDirectoryReader.doOpenFromWriter(StandardDirectoryReader.java:281)\n\tat org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:256)\n\tat org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:246)\n\tat org.apache.lucene.index.FilterDirectoryReader.doOpenIfChanged(FilterDirectoryReader.java:104)\n\tat org.apache.lucene.index.DirectoryReader.openIfChanged(DirectoryReader.java:123)\n\tat org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:137)\n\tat org.apache.lucene.search.SearcherManager.refreshIfNeeded(SearcherManager.java:58)\n\tat org.apache.lucene.search.ReferenceManager.doMaybeRefresh(ReferenceManager.java:176)\n\tat org.apache.lucene.search.ReferenceManager.maybeRefreshBlocking(ReferenceManager.java:253)\n\tat org.elasticsearch.index.engine.internal.InternalEngine.refresh(InternalEngine.java:703)\n\tat org.elasticsearch.index.shard.IndexShard.refresh(IndexShard.java:500)\n\tat org.elasticsearch.index.shard.IndexShard$EngineRefresher$1.run(IndexShard.java:954)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\"elasticsearch[node_2][generic][T#1]\":\n\tat org.apache.lucene.index.IndexWriter.prepareCommitInternal(IndexWriter.java:2730)\n\t- waiting to lock <0x00000007018ee8e8> (a java.lang.Object)\n\t- locked <0x00000007018ee8d8> (a java.lang.Object)\n\tat org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2888)\n\t- locked <0x00000007018ee8d8> (a java.lang.Object)\n\tat org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2855)\n\tat org.elasticsearch.index.engine.internal.InternalEngine.commitIndexWriter(InternalEngine.java:722)\n\tat org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:800)\n\tat org.elasticsearch.index.engine.internal.InternalEngine$RecoveryCounter.endRecovery(InternalEngine.java:1520)\n\tat org.elasticsearch.index.engine.internal.InternalEngine$RecoveryCounter.close(InternalEngine.java:1533)\n\tat org.elasticsearch.common.lease.Releasables.close(Releasables.java:45)\n\tat org.elasticsearch.common.lease.Releasables.closeWhileHandlingException(Releasables.java:70)\n\tat org.elasticsearch.common.lease.Releasables.closeWhileHandlingException(Releasables.java:75)\n\tat org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1048)\n\tat org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:635)\n\tat org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:120)\n\tat org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:48)\n\tat org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:141)\n\tat org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:127)\n\tat org.elasticsearch.transport.local.LocalTransport$2.doRun(LocalTransport.java:287)\n\tat org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nFound 1 deadlock.",
    "attachments": {
        "LUCENE-6214.patch": "https://issues.apache.org/jira/secure/attachment/12695776/LUCENE-6214.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14300038",
            "author": "Michael McCandless",
            "date": "2015-02-01T00:31:49+0000",
            "content": "Here's a simple test that seems to reproduce the deadlock.\n\nThe deadlock happens because when IW.getReader hits a tragedy, it's still holding the fullFlushLock when it calls tragicEvent.\n\ndelelteAll does so as well.\n\nI'll try to fix next... "
        },
        {
            "id": "comment-14300139",
            "author": "Michael McCandless",
            "date": "2015-02-01T10:19:00+0000",
            "content": "Patch w/ test + fix.  I reorganized the hairy try/finally clauses in those two methods so that when tragedy strikes, we are not holding the fullFlushLock.\n\nThe new test also uncovered a missing ensureOpen in ReaderPool.get, which was allowing some NRT segment readers to open in one thread even once IndexWriter was closed by tragedy in another.  It's possible this new test will uncover more such cases once Jenkins is running it...\n\nI also added an assert in DW.finishFullFlush to confirm the calling thread is in fact holding the full flush lock.\n\nI think it's ready: I distributed beasted Lucene tests for a few iterations and no new failures were uncovered... "
        },
        {
            "id": "comment-14300190",
            "author": "Simon Willnauer",
            "date": "2015-02-01T12:24:52+0000",
            "content": "LGTM - I was chasing a pending file with that reopen trace as well that would be explained by the missing ensureOpen!! good stuff "
        },
        {
            "id": "comment-14300199",
            "author": "Michael McCandless",
            "date": "2015-02-01T13:16:14+0000",
            "content": "I think we should fix this for 5.0.0. "
        },
        {
            "id": "comment-14300246",
            "author": "Robert Muir",
            "date": "2015-02-01T16:08:40+0000",
            "content": "Can you please fix this in the test:\n\n\ntry {\n  dir.close();\n} catch (RuntimeException re) {\n  // Ignore \"cannot close because of open file handles\" because tragedy can leave open file handles\n  throw re;\n}\n\n\n\nI don't think its correct, a leak in this case would be a bug? thats the whole point of closing, to release resources. I applied your patch, replaced it with just dir.close() and beasted the test 200 times with no issues. "
        },
        {
            "id": "comment-14300732",
            "author": "Michael McCandless",
            "date": "2015-02-01T20:56:18+0000",
            "content": "Woops, yes I'll definitely remove that ... that was a holdover from when I was \"struggling\"   Thanks for beasting. "
        },
        {
            "id": "comment-14300745",
            "author": "ASF subversion and git services",
            "date": "2015-02-01T21:34:34+0000",
            "content": "Commit 1656361 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1656361 ]\n\nLUCENE-6214: don't deadlock when tragedy strikes during getReader and another thread is committing "
        },
        {
            "id": "comment-14300746",
            "author": "ASF subversion and git services",
            "date": "2015-02-01T21:36:49+0000",
            "content": "Commit 1656362 from Michael McCandless in branch 'dev/branches/lucene_solr_5_0'\n[ https://svn.apache.org/r1656362 ]\n\nLUCENE-6214: don't deadlock when tragedy strikes during getReader and another thread is committing "
        },
        {
            "id": "comment-14300751",
            "author": "ASF subversion and git services",
            "date": "2015-02-01T21:46:15+0000",
            "content": "Commit 1656366 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1656366 ]\n\nLUCENE-6214: don't deadlock when tragedy strikes during getReader and another thread is committing "
        },
        {
            "id": "comment-14332978",
            "author": "Anshum Gupta",
            "date": "2015-02-23T05:02:55+0000",
            "content": "Bulk close after 5.0 release. "
        },
        {
            "id": "comment-14336271",
            "author": "Michael McCandless",
            "date": "2015-02-25T09:15:34+0000",
            "content": "Reopen for 4.10.4 "
        },
        {
            "id": "comment-14336306",
            "author": "ASF subversion and git services",
            "date": "2015-02-25T09:52:21+0000",
            "content": "Commit 1662195 from Michael McCandless in branch 'dev/branches/lucene_solr_4_10'\n[ https://svn.apache.org/r1662195 ]\n\nLUCENE-6214: don't deadlock when tragedy strikes during getReader and another thread is committing "
        },
        {
            "id": "comment-14348899",
            "author": "Michael McCandless",
            "date": "2015-03-05T15:36:24+0000",
            "content": "Bulk close for 4.10.4 release "
        }
    ]
}