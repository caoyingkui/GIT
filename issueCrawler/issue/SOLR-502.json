{
    "id": "SOLR-502",
    "title": "Add search time out support",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "1.3"
        ],
        "components": [
            "search"
        ],
        "type": "New Feature",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Uses LUCENE-997 to add time out support to Solr.",
    "attachments": {
        "SOLR-502.patch": "https://issues.apache.org/jira/secure/attachment/12382696/SOLR-502.patch",
        "solrTimeout.patch": "https://issues.apache.org/jira/secure/attachment/12377390/solrTimeout.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Sean Timm",
            "id": "comment-12576374",
            "date": "2008-03-07T21:07:52+0000",
            "content": "This patch adds a \"timeallowed\" parameter which takes a time out value in milliseconds.  On a timeout, an exception is thrown from the searcher which results in a 500 error page with the time out exception message. \n\nI'd like to add support to return partial results, but I haven't done that part yet. "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12577925",
            "date": "2008-03-12T17:07:18+0000",
            "content": "This patch adds a partialResults flag which is set to true in the event of a timeout.  A partial set of results will be returned (including possibly no results).  The flag is supported in the XML, JSON, Ruby, and Python response writers.\n\nA count of the number of timeouts is included in the statistics similar to the errors count.\n\nCaveats/ToDo: SolrJ is not aware of this setting, nor is distributed search (SOLR-303).  Some execution paths may not recognize partial results (such as sorting by field) as I haven't tested those yet. "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12578936",
            "date": "2008-03-14T21:29:13+0000",
            "content": "Better handling of timeout condition with other code paths such as sorting by a field. "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12578939",
            "date": "2008-03-14T21:40:27+0000",
            "content": "It looks like the recent work on playing nice with external HTTP caches (SOLR-127) will need to be aware of the timeout condition.  I do not think a timeout should be cached.  Currently an \"HTTP/1.x 304 Not Modified\" is returned.  I'll try to work this into my next patch update. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12579499",
            "date": "2008-03-17T16:22:35+0000",
            "content": "An updated patch which contains changes to SolrJ to support search timeouts.\n\nChanges\n\n\tSolrQuery has two new methods - setTimeAllowed and getTimeAllowed to specify timeout in milliseconds\n\tSolrDocumentList has isPartialResult and setPartialResult to signal that a timeout occured and the results returned are partial\n\tXMLResponseParser#readDocuments handles the partialResults boolean attribute sent by the server\n\tSolrQueryTest has a trivial test for adding/removing the timeAllowed parameter\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12579518",
            "date": "2008-03-17T17:18:26+0000",
            "content": "My previous patch wasn't generated correctly. This is a corrected patch. "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12580076",
            "date": "2008-03-18T20:55:23+0000",
            "content": "Timeouts should not be cached.  This patch allows suppressing the generation of HTTP cache headers. "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12580093",
            "date": "2008-03-18T21:26:56+0000",
            "content": "This patch includes Shalin's SolrJ patch and includes the SOLR-505 patch.  HTTP cache headers are now suppressed on a timeout. "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12583967",
            "date": "2008-04-01T01:44:59+0000",
            "content": "Added the ability to allows timeouts to occur on one or more shards in a distributed search (SOLR-303) and still be merged.  The resulting set is reported as a partial result and is not cachable in an HTTP cache.\n\nThis fixes the last known issue. "
        },
        {
            "author": "patrick o'leary",
            "id": "comment-12593634",
            "date": "2008-05-01T20:54:06+0000",
            "content": "Has this had any traction in the solr core yet? seems like a critical thing to have  "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12598893",
            "date": "2008-05-22T03:18:16+0000",
            "content": "Yes, I think we should get this in 1.3.  I left the following comment in SOLR-505, but since this issue includes the patch from SOLR-505, I will assume the patch will be developed further as part of this issue and not SOLR-505.\n\n\nI took a quick look at the patch and saw this:\n\n\nrsp.setAvoidHttpCaching(false);\n\n\nAm I the only one who has a harder time reading negative methods like this, esp. when they take false?\nWould it not be nicer to just have:\n\nrsp.setHttpCaching(true/false);\n\nor even\n\nrsp.httpCachingOn() + rsp.httpCachingOff()\n\n\nSimilarly, instead of \n\nisAvoidHttpCaching()\n\n have \n\nisHttpCachingOn()\n\n\nI know this is \"just naming\", but I think it helps with readability a bit.\n\nI notice the unit test mods are not in the patch.  Is there no need to test the modified behaviour? "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12598895",
            "date": "2008-05-22T03:31:22+0000",
            "content": "The patch needs to be brought up to date with trunk.  I believe a person new to this issue (but in need of the functionality) will try to do that tomorrow.\n\n[otis@localhost trunk]$ patch -p0 -i solrTimeout.patch --dry-run \npatching file src/java/org/apache/solr/search/DocSet.java\npatching file src/java/org/apache/solr/search/DocSlice.java\npatching file src/java/org/apache/solr/search/BitDocSet.java\npatching file src/java/org/apache/solr/search/SolrIndexSearcher.java\npatching file src/java/org/apache/solr/search/HashDocSet.java\npatching file src/java/org/apache/solr/request/SolrQueryResponse.java\nHunk #1 succeeded at 88 (offset 9 lines).\nHunk #2 succeeded at 159 with fuzz 2 (offset -11 lines).\npatching file src/java/org/apache/solr/request/JSONResponseWriter.java\npatching file src/java/org/apache/solr/request/XMLWriter.java\npatching file src/java/org/apache/solr/common/params/CommonParams.java\npatching file src/java/org/apache/solr/common/SolrDocumentList.java\nHunk #2 succeeded at 65 with fuzz 2 (offset 8 lines).\npatching file src/java/org/apache/solr/handler/RequestHandlerBase.java\nHunk #3 FAILED at 43.\nHunk #4 succeeded at 126 (offset 7 lines).\nHunk #5 succeeded at 168 with fuzz 2.\n1 out of 5 hunks FAILED \u2013 saving rejects to file src/java/org/apache/solr/handler/RequestHandlerBase.java.rej\npatching file src/java/org/apache/solr/handler/component/SearchHandler.java\nHunk #1 succeeded at 118 (offset -5 lines).\nHunk #2 succeeded at 259 (offset 6 lines).\npatching file src/java/org/apache/solr/handler/component/QueryComponent.java\npatching file src/java/org/apache/solr/handler/SpellCheckerRequestHandler.java\npatching file src/java/org/apache/solr/handler/MoreLikeThisHandler.java\npatching file src/webapp/src/org/apache/solr/servlet/cache/Method.java\npatching file src/webapp/src/org/apache/solr/servlet/cache/HttpCacheHeaderUtil.java\npatching file src/webapp/src/org/apache/solr/servlet/SolrDispatchFilter.java\nHunk #1 FAILED at 263.\nHunk #2 FAILED at 282.\n2 out of 2 hunks FAILED \u2013 saving rejects to file src/webapp/src/org/apache/solr/servlet/SolrDispatchFilter.java.rej\npatching file client/java/solrj/test/org/apache/solr/client/solrj/SolrQueryTest.java\npatching file client/java/solrj/src/org/apache/solr/client/solrj/impl/XMLResponseParser.java\nHunk #1 succeeded at 344 (offset 1 line).\npatching file client/java/solrj/src/org/apache/solr/client/solrj/SolrQuery.java "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12599072",
            "date": "2008-05-22T16:29:01+0000",
            "content": "Sean, do you think you can remove the changes that are a part of SOLR-505 from your patch, as mentioned here: https://issues.apache.org/jira/browse/SOLR-505?focusedCommentId=12598951#action_12598951\n\nThanks. "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12599085",
            "date": "2008-05-22T17:10:53+0000",
            "content": "Otis, I'd be happy to do so.  Is there a way to generate a patch excluding the content of another patch without doing a manual editing of the patch file--which would be error prone?  Or should I wait until SOLR-505 is committed? "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12599091",
            "date": "2008-05-22T17:16:51+0000",
            "content": "I think it's a manual thing, but super simple in this case.  I'll commit SOLR-505 as soon as Thomas does the renaming, so if you want you can wait, svn up, see conflicts, and then manually remove conflicts and re-generate the patch. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12599217",
            "date": "2008-05-22T22:14:27+0000",
            "content": "whew... that's a lot of changes for timeouts (flag on DocSet, DocList, ResponseWriter changes, etc)\n\nIt also seems like we shouldn't add any more conditionals to the inner loop HitCollector.collect().\nIf it's a timed hit collector, perhaps just wrap the original hit collector so non-timed collectors don't take a speed hit. "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12599495",
            "date": "2008-05-23T20:24:40+0000",
            "content": "Patch brought up to date with trunk.  This patch no longer includes SOLR-505, but is dependent on it.\n\nThough I doubt the conditional check, even in the tight loop, has any performance impact, I was able to remove it while also improving the code readability. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12599523",
            "date": "2008-05-23T22:06:06+0000",
            "content": "\n\tDoes timeallowed=-1 mean \"do not time out at all\"?  Is that mentioned anywhere?  I also see a check for timeallowed > 0, so 0 also seems to mean \"do not time out at all\".\n\n\n\n\n\tCamelCase: timeallowed => timeAllowed?\n\n\n\n\n\tI see \"This should only be called using either filterList or filter, but not both.\", but I don't see a check for that.  Should there be a test for the two vars?\n\n\n\n\n\tI see System.out.println( \"partialResults0: \" + partialResults );\n\n\n\nThe rest, from what I can tell, looks good.\n\nP.S.\nSOLR-502-solrj.patch  is just an old patch that we can really remove so it doesn't confuse anyone, correct? "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12600191",
            "date": "2008-05-27T16:54:13+0000",
            "content": "\n\tAdded Javadoc note that a timeallowed param <=0 (or omitted) results in no timeout.\n\n\n\n\n\tFixed the \"CamelCase: timeallowed => timeAllowed\"\n\n\n\n\n\tRemoved the System.out.println(...) statements.\n\n\n\nI see \"This should only be called using either filterList or filter, but not both.\", but I don't see a check for that. Should there be a test for the two vars?\n\nThis comment was copied from the existing getDocListC method (without the timeAllowed parameter).  If there should be a sanity check there, it should probably be added as a separate JIRA issue. "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12600196",
            "date": "2008-05-27T16:59:02+0000",
            "content": "SOLR-502-solrj.patch is just an old patch that we can really remove so it doesn't confuse anyone, correct?\n\nYes, this is an old patch which can be removed.  The solrTimeout.patch files could be removed as well if they are found to be confusing. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12600248",
            "date": "2008-05-27T19:16:55+0000",
            "content": "I've removed the SOLR-502-solrj.patch as per the above comments. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12601761",
            "date": "2008-06-02T21:07:56+0000",
            "content": "I finally had the chance to apply this locally and try it out.  I have not been able to get this time out business to kick in, though.  Here is what I did so far, after applying the patch, and clean dist and deployment of solr war.\n\nI set up 2 Solr instances (actually 1 Jetty with 2 Solr homes defined via JNDI).  Identical schema, each index with 100K docs.\n\nI then hit one of the instances, specifying both shards and asked for q=title:a* (expensive query), while using timeAllowed=1, like this:\n\n\ncurl --silent 'http://localhost:8080/solr1/select/?q=title%3Aa*&version=2.2&start=0&rows=1000&indent=on&shards=localhost:8080/solr1,localhost:8080/solr2&timeAllowed=1' | less\n\n\n\n....Aaaarg, I see one problem.  That \"timeAllowed\" is specified as \"timeallowed\":\n\n\n[otis@localhost SOLR-502]$ grep TIME_ALLOW SOLR-502.patch  | head -1\n+  public static final String TIME_ALLOWED = \"timeallowed\";\n\n\n\nSean, I think this should be camelCase, too.\n\nOK, so changing that:\n\n\ncurl --silent 'http://localhost:8080/solr1/select/?q=title%3Aa*&version=2.2&start=0&rows=1000&indent=on&shards=localhost:8080/solr1,localhost:8080/solr2&timeallowed=1' | less\n\n\n\nHowever, I am still unable to get the timeout to happen.  I see QTime of 257 in the response, clearly above timeallowed=1.  If timeallowed=1, should I ever be seeing QTime over 1?\n\n\n<lst name=\"responseHeader\">\n <int name=\"status\">0</int>\n <int name=\"QTime\">257</int>\n <lst name=\"params\">\n  <str name=\"shards\">localhost:8080/solr1,localhost:8080/solr2</str>\n  <str name=\"indent\">on</str>\n  <str name=\"start\">0</str>\n  <str name=\"q\">title:a*</str>\n  <str name=\"timeallowed\">1</str>\n  <str name=\"version\">2.2</str>\n  <str name=\"rows\">10</str>\n </lst>\n</lst>\n<result name=\"response\" numFound=\"50936\" start=\"0\">\n\n\n\nI also grepped the output for \"partial\" and never find anything.  Am I doing something wrong?\nI also see the latest SOLR-502.patch still has some print statements, so I looked at stdout, but nothing is getting printed there.\n\nI'll see if I can trace this, but if I did something wrong or see a bug in your code, I'm all eyes. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12602068",
            "date": "2008-06-03T20:45:40+0000",
            "content": "Just in case it helps:\n\n\tI used solrconfig.xml from example/solr/conf\n\tI set all cache sizes to 0\n\n\n\nAlso, after adding some lovely print statements to the patch, right in the QueryComponent's process method, it looks like my query requests are not even executing QueryComponent's process method.  Is there something I need to enable to get QueryComponent included?  Standard request handler should be using it already, no? "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12602546",
            "date": "2008-06-05T05:34:47+0000",
            "content": "\n\tAdds partialResults support to the binary response, which is used by distributed search.\n\tReally removes the System.out.println() this time.\n\ttimeallowed param is now camelcase (timeAllowed).\n\n "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12602550",
            "date": "2008-06-05T05:54:54+0000",
            "content": "Sorry about the timeallowed parameter.  For some reason I had in my head that the parameters were not supposed to be camel case and I only switched the parameter variable names.\n\nYou should be seeing a log message similar to:\n\nWARNING: Query: title:s*; Elapsed time: 20Exceeded allowed search time: 1 ms.\n\n\neven with the previous patch.  Though, when using distributed search, the new binary response is used which I hadn't modified to include partial results support.  It should work with this new patch.\n\n\n<lst name=\"responseHeader\">\n <int name=\"status\">0</int>\n <int name=\"QTime\">39</int>\n <lst name=\"params\">\n  <str name=\"shards\">naan.office.aol.com:8973/solr,naan.office.aol.com:8993/solr</str>\n  <str name=\"indent\">on</str>\n\n  <str name=\"start\">0</str>\n  <str name=\"q\">headline:s*</str>\n  <str name=\"timeAllowed\">1</str>\n  <str name=\"version\">2.2</str>\n  <str name=\"rows\">100</str>\n </lst>\n\n</lst>\n<result name=\"response\" numFound=\"0\" start=\"0\" partialResults=\"true\"/>\n\n\n\nIf timeallowed=1, should I ever be seeing QTime over 1?\n\nYes, the TimeLimitedCollector can only interrupt searches during the collect() calls.  Other, sometimes substantial, work is done outside of the collect().\n\nAlso, see the note in the TimeLimitedCollector.setResolution(long) Javadocs http://hudson.zones.apache.org/hudson/job/Lucene-trunk/javadoc//org/apache/lucene/search/TimeLimitedCollector.html#setResolution(long) "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12602559",
            "date": "2008-06-05T06:46:43+0000",
            "content": "Sean: For the namedListCodec changes to be backward compatible (within 1.3) add check the list size before calling a list.get()\n\nif(list.size() > 3)  solrDocs.setPartialResult((Boolean)list.get(3));\n\n "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12602672",
            "date": "2008-06-05T15:07:39+0000",
            "content": "This patch adds a conditional to ensure backwards compatibility within SOLR 1.3 nightly builds, per Noble Paul's suggestion.  Is that necessary? "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12602716",
            "date": "2008-06-05T17:32:44+0000",
            "content": "Just a quick note that the patch now does produce partial results.\n\nI see two \"problems\" with time-limiting search, but they are mostly general and not all are directly related to this patch:\n\n\n\tlots of work can be done outside collect, so the QTime can be radically higher than timeAllowed (e.g. I have timeAllowed=50, but I'm seeing QTime of over 2000)\n\tthe number of hits will vary for the same timeAllowed and the same query.  This may not be good for apps that want to show the exact number of hits in the UI.\n\n\n\nStill, I think having the option of timing out long searches is a good thing.\nI'm +1 on committing this. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12605124",
            "date": "2008-06-15T03:45:07+0000",
            "content": "Yonik, I think Sean addressed the 2 issues from your May 22 comments.  Here is svn st:\n\nM      src/java/org/apache/solr/search/DocSet.java\nM      src/java/org/apache/solr/search/DocSlice.java\nM      src/java/org/apache/solr/search/BitDocSet.java\nM      src/java/org/apache/solr/search/SolrIndexSearcher.java\nM      src/java/org/apache/solr/search/HashDocSet.java\nM      src/java/org/apache/solr/common/params/CommonParams.java\nM      src/java/org/apache/solr/common/SolrDocumentList.java\nM      src/java/org/apache/solr/common/util/NamedListCodec.java\nM      src/java/org/apache/solr/request/BinaryResponseWriter.java\nM      src/java/org/apache/solr/request/JSONResponseWriter.java\nM      src/java/org/apache/solr/request/XMLWriter.java\nM      src/java/org/apache/solr/handler/RequestHandlerBase.java\nM      src/java/org/apache/solr/handler/component/QueryComponent.java\nM      client/java/solrj/test/org/apache/solr/client/solrj/SolrQueryTest.java\nM      client/java/solrj/src/org/apache/solr/client/solrj/impl/XMLResponseParser.java\nM      client/java/solrj/src/org/apache/solr/client/solrj/SolrQuery.java\n\nI'll commit next week if nobody objects. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12605673",
            "date": "2008-06-17T17:40:14+0000",
            "content": "From SOLR-303:\nPerhaps a string should also be added indicating why all results were not able to be returned.\n\nIf we had that (perhaps in the response header) would there still be a need to have this partial results flag on DocSet/DocList?  It always felt a little wrong that this feature wormed it's way to that low of a level (DocSet, response writers, response parsers, etc).  Seems like it could/should me much simpler. "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12605714",
            "date": "2008-06-17T20:22:02+0000",
            "content": "Yonik--\n\nDo you have a suggestion on how to get it into the response header?  That isn't available down at the SolrIndexSearcher level as far as I can tell.  It would be much easier if the ResponseBuilder or some other object was passed all the way down to the searcher level, but I was trying to make the smallest change possible.\n\nI think an easy machine readable value to indicate partial results is important.  I think a descriptive string is optional, but would be a nice addition.\n\n-Sean "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12605719",
            "date": "2008-06-17T20:53:09+0000",
            "content": "Do you have a suggestion on how to get it into the response header? That isn't available down at the SolrIndexSearcher level as far as I can tell.\n\nOff the top of my head, it seems like it might be cleaner to throw an exception in the SolrIndexSearcher method doing the searching (that would have the added benefit of  automatically bypassing DocSet/DocList caching, etc).\n\nCatch that exception in the query component and set a flag in the header indicating that a timeout happened.\n\nOr if it's easier, pass more info down to the SolrIndexSearcher.\n\nAfter all, this only handles timeouts at the query level (not query expansion/rewriting, highlighting, retrieving stored fields, faceting, or any other number of components that will be added in the future).  It also doesn't even totally handle timeouts at the query level... one could construct a query that takes a lot of time yet matches no documents so there is never an opportunity to time out.  Then there is the issue of false positives (a major GC compaction hits and causes all the currently running queries to time out).  Given these restrictions, and the fact that most people would choose not to get partial results, it seems like we should really try to limit the impact/invasiveness of this feature. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12606025",
            "date": "2008-06-18T18:11:29+0000",
            "content": "one could construct a query that takes a lot of time yet matches no documents...\n\nActually, thinking a little further on this point, some of the longest queries are range queries or prefix queries.  Solr uses the constant scoring variety of these, where all of the matching documents are collected up front.  So these queries would never be interrupted in the middle, but only at the end after the majority of the work had been done. "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12606050",
            "date": "2008-06-18T19:24:52+0000",
            "content": "I've been thinking about putting the timeout info in the response header.  Throwing an exception from the searcher will not work because that sacrifices the ability to get partial results.  I really feel that having the partial results flag as an attribute on the response tag makes more sense than putting it in the resonse header as the partial results pertains to the results section of the response.  I will create an alternate patch however with the partial results flag in the response header to compare the two methods. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12606085",
            "date": "2008-06-18T20:46:57+0000",
            "content": "I'm trying to think of Solr installations I've seen with problems, where this would be the solution I would recommend... but I can't say I can think of any (hence my hesitation, wondering if this is more of a solution in search of a problem).\n\nWhat is the underlying problem with some queries taking longer than others?\nIs it more for the server side (don't spend too much time on any 1 query), or for\nthe client side (control latency, even if result set is partial).\n\nI have seen problems where queries started stacking up:\nhttp://www.nabble.com/TermInfosReader-lazy-term-index-reading-to8772766.html#a8775141\nBut this would not have fixed the problem.\n\nIf timeouts and partial results are expected to happen regularly (a non-trivial % of queries), then one needs to start worrying about the bias introduced (it's always low doc numbers that will be returned, which will be older documents if one is doing incremental indexing).  It seems like a better solution is to increase hardware dedicated to the search collection.\n\nIf timeouts and partial results are expected to be rare, then it should have little impact on the overall server load, so why worry about them?\n\nThoughts?\n "
        },
        {
            "author": "Ian Holsman",
            "id": "comment-12606103",
            "date": "2008-06-18T21:44:01+0000",
            "content": "Hi Yonik.\n\nThe scenario I always come up with is when a developer launches something into production without properly testing it out on a large size index and fluffs the query.\n\nWithout a timeout/partial result he will bring the site down quite quickly and it will stay down until the operations guys roll it out again. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12606148",
            "date": "2008-06-18T23:42:20+0000",
            "content": "The scenario I always come up with is when a developer launches something into production without properly testing it out on a large size index and fluffs the query.\n\nHeh... yup I remember seeing a couple of those.\nUnfortunately the ones I remember wouldn't have been saved by this patch because the \"bad\" part of the query was an expensive range query (or once a prefix query) that wasn't pulled out into a separate \"fq\". "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12606404",
            "date": "2008-06-19T14:48:33+0000",
            "content": "Changes the setting of the partialResults flag from the results to the responseHeader. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12606424",
            "date": "2008-06-19T15:59:29+0000",
            "content": "Thanks Sean, that definitely cuts down the patch size, and it seems nicer not to be touching DocSet and ResponseWriters, etc.  What's your take?\n\nAnother thing to consider is perhaps a SolrIndexSearcher.search() method that uses a command pattern to avoid having to always change the signatures when we want to pass something new in or out?  It might be more natural than passing down an un-typed NamedList\n\n\nQueryCommand {\n  Query q\n  Sort s\n  List<Query> filters\n  DocSet filter\n  int flags\n  int timeout\n  ...\n}\n\nQueryResult {\n  DocList list\n  DocSet set\n  boolean timedOut\n  ...\n}\n\n "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12606428",
            "date": "2008-06-19T16:12:11+0000",
            "content": "The timeout is to protect the server side.  The client side can be largely protected by setting a read timeout, but if the client aborts before the server responds, the server is just wasting resources processing a request that will never be used.  The partial results is useful in a couple of scenarios, probably the most important is a large distributed complex where you would rather get whatever results you can from a slow shard rather than throw them away.\n\nAs a real world example, the query \"contact us about our site\" on a 2.3MM document index (partial Dmoz crawl) takes several seconds to complete, while the mean response time is sub 50 ms.  We've had cases where a bot walks the next page links (including expensive queries such as this).  Also users are prone to repeatedly click the query button if they get impatient on a slow site.  Without a server side timeout, this is a real issue.\n\nRate limiting and limiting the number of next pages that can be fetched at the front end are also part of the solution to the above example. "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12606473",
            "date": "2008-06-19T17:35:34+0000",
            "content": "that definitely cuts down the patch size [...] What's your take?\n\nBefore I made the change, I was against it as it seems more logical to have the partialResults associated with the results list, where the total count, etc. are.  But this greatly simplifies the patch.  I could go either way.\n\nAnother thing to consider is perhaps a SolrIndexSearcher.search() method that uses a command pattern\n\nI think I agree.   How is this different than my suggestion of passing the ResponseBuilder into the searcher?  It seems that it would be useful to take it even a step further and pass the ResonseBuilder object around everywhere including the response handlers and writers. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12606483",
            "date": "2008-06-19T18:02:24+0000",
            "content": "I see your point on the overlap between something like QueryCommand and ResponseBuilder... but ResponseBuilder feels like it's at a higher level.  Say that a custom component or handler wants to do a couple queries... or different queries depending on the results of the first query (or something like unsupervised feedback).  Should a different ResponseBuilder object be built for each query that is part of a request/response?\n\nResponseBuilder is also a bit big and ill-defined (but currently gets the job done for communication between different query components).  Upgrading it to serve as something you pass to a SolrIndexSearcher to do a query doesn't feel quite right (or at least that's not the way I've been thinking about it). "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12606798",
            "date": "2008-06-20T15:25:13+0000",
            "content": "Are you thinking the command pattern would be the preferred way of doing a SolrIndexSearcher.search(), possibly even deprecating the existing methods?  I think that makes sense, but seems to be a big change to add to this patch.  I think I'd prefer to see it fleshed out in a separate issue.  The methods returning Hits should probably be deprecated in Solr 1.3 anyway since Hits is going away in Lucene 3.0. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12606810",
            "date": "2008-06-20T16:18:44+0000",
            "content": "I think that makes sense, but seems to be a big change to add to this patch.\n\nThe patch already changes (or adds to) the API.  So instead of passing down extra parameters (timeout and NamedList), pass down an object that encapsulates all the parameters.  Deprecations can wait for another day.\n\nThe primary motivation is that it seems messy passing down the un-typed NamedList<Object> and having SolrIndexSearcher set things in the header (rather than the QueryComponent do it). "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12608812",
            "date": "2008-06-27T15:59:18+0000",
            "content": "Added a SolrIndexSearcher.search() method that uses a command pattern. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12610716",
            "date": "2008-07-05T23:02:28+0000",
            "content": "I'm a bit out of touch with this one now (vacationnnnnnn), but should this patch contain solrj changes as well?\n(a new method for max allowed time?) "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12610718",
            "date": "2008-07-06T01:19:01+0000",
            "content": "That was a bigger change set than I had anticipated - I thought perhaps of just introducing the QueryCommand and passing an extra param to the non-public method(s) to minimize changes. Things look good though, and I just committed this patch.\nThanks Sean! "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12612176",
            "date": "2008-07-09T16:24:51+0000",
            "content": "Yonik--\n\nI just noticed that the Javadoc specifies \"Long\", but the setTimeAllowed function takes an Integer in org.apache.solr.client.solrj.SolrQuery.\n\nThanks,\nSean "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12612258",
            "date": "2008-07-09T20:28:31+0000",
            "content": "Hmmm, so should we change timeAllowed to an int everywhere (1.6 years worth of timeout) or add getLong() methods to SolrParams? "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12612596",
            "date": "2008-07-10T18:00:24+0000",
            "content": "I think that the code is fine as is.  Just the Javadoc comment needs to be changed.  The Integer is explicitly cast to a long when it is used.  And as you note, 1.6 years is plenty long enough.  "
        },
        {
            "author": "Magne Groenhuis",
            "id": "comment-12622245",
            "date": "2008-08-13T15:12:33+0000",
            "content": "I am using the trunk version of Solr and split a lucene index in 2 (2 GB) shards hosting them on 2 different ports.\nEven though i know work is being done outside the timeAllowed period, i get some extreme numbers giving the user impression that timeAllowed is not actually doing anything.\n\nINFO: webapp=/solr path=/select params=\n{fl=bla1&shards=localhost:8983/solr,localhost:7574/solr&start=65850&q=bla2&timeAllowed=1000&wt=javabin&rows=5&version=2.2}\n status=0 QTime=331390 \n\nI am browsing to the end of some search result. I assume that under the hood an extreme amount of id's with scores have to be sent to the merge machine, but i was hoping that the timeAllowed parameter would limit the amount of time seacrhed on the shards and thus limiting the time for the client. Possible resulting in getting no result (because of the browsing so far to the end of a large search result).\n\nBut still the number 1000 and 331390 are a bit far apart.\n\nAny suggestions or do i have to give some more data? "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12622880",
            "date": "2008-08-15T14:57:57+0000",
            "content": "Magne,\nI think the problem is that start=65850.  You have rows=5, so I think result merging is not the problem.\nI'm not sure what exactly happens outside of the collect call (the part that can be time-limited), but that's clearly costing you time.\nGoing deep in search results is a problem for all search engines, as war as I know.  Try going beyond 1000 matches in Google. \nIf you are OK with not returning any results to the client, then why not put the timeout around that client's call to Solr? "
        }
    ]
}