{
    "id": "LUCENE-5999",
    "title": "Add backcompat support for StandardTokenizer before 4.7",
    "details": {
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed",
        "components": [],
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "5.0"
        ]
    },
    "description": "The merge from trunk for branch_5x remove the backcompat support in StandardTokenizer for previous unicode versions.  With 5x, we only need to add support back for 4.0-4.6 (unicode 6.1).",
    "attachments": {
        "LUCENE-5999.patch": "https://issues.apache.org/jira/secure/attachment/12673473/LUCENE-5999.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14162786",
            "author": "Ryan Ernst",
            "content": "Here's a patch which adds back support in the same way other backcompat is done: with a copy of the old code and versioning in the class name.  This is different than was done before, but it makes it so we don't have to have Version based constructors, and we don't have possibility of some other impl detail changing in StandardTokenizer itself that could silently break compat.\n\nThe only thing I'm not happy about with this patch is having to add AbstractStandardTokenizer so that setMaxTokenLength could be available for both impls.  If someone has an idea for a cleaner approach, I'm happy to drop that. ",
            "date": "2014-10-07T23:40:32+0000"
        },
        {
            "id": "comment-14162870",
            "author": "Robert Muir",
            "content": "Instead of:\n\nfinal AbstractStandardTokenizer src;\nif (getVersion().onOrAfter(Version.LUCENE_4_7_0)) {\n  src = new StandardTokenizer();\n} else {\n  src = new StandardTokenizer40();\n}\nsrc.setMaxTokenLength(maxTokenLength);\n\n\n\nyou can do:\n\nfinal Tokenizer src;\nif (getVersion().onOrAfter(Version.LUCENE_4_7_0)) {\n  StandardTokenizer t = new StandardTokenizer();\n  t.setMaxTokenLength(maxTokenLength);\n  src = t;\n} else {\n  StandardTokenizer40 t = new StandardTokenizer();\n  t.setMaxTokenLength(maxTokenLength);\n  src = t;\n}\n\n ",
            "date": "2014-10-08T00:43:00+0000"
        },
        {
            "id": "comment-14162896",
            "author": "Ryan Ernst",
            "content": "If it was just that, I would have done it in the first place.  I was trying to avoid casting inside the anonymous setReader at the end of the function.  Here's a patch which does that. ",
            "date": "2014-10-08T01:15:14+0000"
        },
        {
            "id": "comment-14163156",
            "author": "Uwe Schindler",
            "content": "The only thing I'm not happy about with this patch is having to add AbstractStandardTokenizer so that setMaxTokenLength could be available for both impls. If someone has an idea for a cleaner approach, I'm happy to drop that.\n\nMake the abstract base class package-private, so it is invisible in Javadocs. You solved it already in another way, but if you want to prevent the instanceof checks, this is the way to go. ",
            "date": "2014-10-08T06:40:19+0000"
        },
        {
            "id": "comment-14163684",
            "author": "Ryan Ernst",
            "content": "This new patch keeps the casts (upon seeing the change for real instead of in my head, the casts didn't look so bad, and were better than adding abstractions).  It adds tests for both unicode 6.1 (thanks to Steve's generation script), and tests for each analyzer to ensure passing a Version before 4.7 gets the unicode 6.1 behavior.  I also moved the standard analyzer et al tests from analysis/core to analysis/standard to sit alongside the actual implementations of the things being tested. ",
            "date": "2014-10-08T16:19:10+0000"
        },
        {
            "id": "comment-14163707",
            "author": "Robert Muir",
            "content": "+1. Thanks for setting up the testing especially.\n\nCan we open a followup issue to remove FooTokenizerImpl and FooTokenizerInterface abstractions since we don't use that method for back compat anymore? I can help. ",
            "date": "2014-10-08T16:29:41+0000"
        },
        {
            "id": "comment-14163771",
            "author": "Ryan Ernst",
            "content": "Sure, created LUCENE-6000. ",
            "date": "2014-10-08T16:57:43+0000"
        },
        {
            "id": "comment-14163867",
            "author": "ASF subversion and git services",
            "content": "Commit 1630189 from Ryan Ernst in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1630189 ]\n\nLUCENE-5999: Fix backcompat support for StandardTokenizer ",
            "date": "2014-10-08T17:59:17+0000"
        },
        {
            "id": "comment-14164068",
            "author": "ASF subversion and git services",
            "content": "Commit 1630223 from Ryan Ernst in branch 'dev/trunk'\n[ https://svn.apache.org/r1630223 ]\n\nLUCENE-5999: Move standard analyzer tests on trunk to match location on branch_5x ",
            "date": "2014-10-08T20:05:24+0000"
        },
        {
            "id": "comment-14164069",
            "author": "ASF subversion and git services",
            "content": "Commit 1630224 from Ryan Ernst in branch 'dev/trunk'\n[ https://svn.apache.org/r1630224 ]\n\nLUCENE-5999: Move classic analyzer test next to the classic analyzer ",
            "date": "2014-10-08T20:06:30+0000"
        },
        {
            "id": "comment-14164071",
            "author": "ASF subversion and git services",
            "content": "Commit 1630225 from Ryan Ernst in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1630225 ]\n\nLUCENE-5999: Move classic analyzer test next to the classic analyzer (merged 1630223) ",
            "date": "2014-10-08T20:07:25+0000"
        },
        {
            "id": "comment-14332637",
            "author": "Anshum Gupta",
            "content": "Bulk close after 5.0 release. ",
            "date": "2015-02-23T05:01:05+0000"
        }
    ]
}