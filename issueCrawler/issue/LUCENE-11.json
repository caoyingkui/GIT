{
    "id": "LUCENE-11",
    "title": "QueryParser not recognizing asterisk with UTF-8 index",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/queryparser"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Incomplete",
        "status": "Closed"
    },
    "description": "Version: 1.2-RC3\n\nI've created an index of UTF-8 encoded documents and making sure that all\nqueries are converted to UTF-8. When searching the index with query containing\nnon-ascii UTF-8 characters and an asterisk, no results are found even though\nthere are documents that contain the query word. Searching does work when query\ndoesn't contain non-ascii UTF-8 characters or without asterisk works always.\nTest results with swedish words:\n\"f\u00c3\u00b6dde\" - works ok, returns documents.\n\"f\u00c3\u00b6dd*\" - doesn't return any results.\n\"f\u00c3\u00b6dd\" - works ok, returns documents.\n\"kom*\" - works ok, returns documents.",
    "attachments": {
        "ASF.LICENSE.NOT.GRANTED--patch8.txt": "https://issues.apache.org/jira/secure/attachment/12312184/ASF.LICENSE.NOT.GRANTED--patch8.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2003-03-17T23:16:20+0000",
            "content": "\n\t\n\t\n\t\t\n\t\t\n\t\t\tBug 17954 has been marked as a duplicate of this bug. ***\n\t\t\n\t\t\n\t\n\t\n\n ",
            "author": "sven ruwolt",
            "id": "comment-12320989"
        },
        {
            "date": "2003-11-25T19:39:58+0000",
            "content": "Please provide a self-enclosed unit test that demonstrates this bug. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12320990"
        },
        {
            "date": "2003-12-22T17:35:37+0000",
            "content": "The following test prog returns: \nFound id: \n1 \nFound id: \n1 \nFound id: \n\nThe last search should find the index 1 as well. Tested with lucene-1.3-rc3. \n\n----------------------------- \nimport org.apache.lucene.analysis.*; \nimport org.apache.lucene.index.*; \nimport org.apache.lucene.document.*; \nimport org.apache.lucene.search.*; \nimport org.apache.lucene.queryParser.*; \nimport java.io.*; \n/** \n\n\tSelf contained test for Lucene indexes.\n */ \npublic class LuceneTest { \n\n\n\n  public static void main(String args[]) { \n    String outdirname=\"/tmp/testidx\"; // Index directory \n    try \n{ \n      // Creates index directory, if necessary. \n      File outdir=new File(outdirname); \n      if (!outdir.exists()) \n        outdir.mkdir(); \n      // Create an index with a single document. \n      Analyzer analyzer=new SimpleAnalyzer(); \n      IndexWriter writer = new IndexWriter(outdirname,analyzer,true); \n      addDoc(writer,1, \"f\u00c3\u00b6r\"); // The second letter is o with two dots. \n      writer.optimize(); \n      writer.close(); \n      // Search the index. \n      Searcher searcher=new IndexSearcher(outdirname); \n      searchDoc(analyzer,searcher,\"f\u00c3\u00b6r\"); // Ok \n      searchDoc(analyzer,searcher,\"f*\"); // Ok \n      searchDoc(analyzer,searcher,\"f\u00c3\u00b6*\"); // Wrong! Does not find anything.  \n    }\n catch (Exception e) \n{ \n      e.printStackTrace(); \n      return; \n    }\n \n  } \n  /** \n\n\tAdd a document to index.\n\tThe text is changed to UTF-8.\n   */ \n  private static void addDoc(IndexWriter writer,int id,String text) throws \nException \n{ \n    Document doc=new Document(); \n    doc.add(new Field(\"id\",Long.toString(id),true,false,false)); \n    doc.add(new Field(\"text\",new \nString(text.getBytes(\"UTF-8\")),false,true,true)); \n    writer.addDocument(doc); \n  }\n \n  /** \n\tSearch the index.\n\tThe text is changed to UTF-8.\n   */ \n  private static void searchDoc(Analyzer analyzer, Searcher searcher, String \ntext) throws Exception \n{ \n    Query q=QueryParser.parse(new \nString(text.getBytes(\"UTF-8\")),\"text\",analyzer); \n    Hits hits=searcher.search(q); \n    System.out.println(\"Found id:\"); \n    for (int i=0;i<hits.length();i++) \n      System.out.println(hits.doc(i).get(\"id\")); \n  }\n   \n} \n\n\n ",
            "author": "Tero Favorin",
            "id": "comment-12320991"
        },
        {
            "date": "2004-02-10T17:45:22+0000",
            "content": "Created an attachment (id=10293)\nFix the escaping issues in fields and terms ",
            "author": "Jean-Fran\u00e7ois Halleux",
            "id": "comment-12320992"
        },
        {
            "date": "2004-03-02T23:05:40+0000",
            "content": "Note: patch id=10293 was mistakenly attached to this bug.  It does not fix the\nproblem this bug describes. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12320993"
        },
        {
            "date": "2004-03-27T22:20:29+0000",
            "content": "This is not a Lucene bug. Lucene takes a string so the caller is responsible \nthat the string has been correctly decoded. What happens here is this: \n\ntext.getBytes(\"UTF-8\") returns the String as an array of bytes (UTF-8). Using \nthis as the input for new String() will interpret this array as a byte \nsequence in the platform's default charset (usually iso-8859-1 on Linux). Thus \nthe string is \"broken\"/misinterpreted. As Lucene has analyzers it relies on \nstrings which have not been misinterpreted.  ",
            "author": "Daniel Naber",
            "id": "comment-12320994"
        },
        {
            "date": "2004-03-27T22:21:46+0000",
            "content": "I closed the bug, not sure if only developers are allowed to do that... let me \nknow in case it was not okay to close it.  ",
            "author": "Daniel Naber",
            "id": "comment-12320995"
        }
    ]
}