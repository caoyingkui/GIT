{
    "id": "SOLR-3055",
    "title": "Use NGramPhraseQuery in Solr",
    "details": {
        "affect_versions": "None",
        "status": "Open",
        "fix_versions": [],
        "components": [
            "Schema and Analysis",
            "search"
        ],
        "type": "New Feature",
        "priority": "Minor",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "Solr should use NGramPhraseQuery when searching with default slop on n-gram field.",
    "attachments": {
        "schema.xml": "https://issues.apache.org/jira/secure/attachment/12689220/schema.xml",
        "solrconfig.xml": "https://issues.apache.org/jira/secure/attachment/12689222/solrconfig.xml",
        "SOLR-3055-2.patch": "https://issues.apache.org/jira/secure/attachment/12689219/SOLR-3055-2.patch",
        "SOLR-3055-1.patch": "https://issues.apache.org/jira/secure/attachment/12689218/SOLR-3055-1.patch",
        "SOLR-3055.patch": "https://issues.apache.org/jira/secure/attachment/12512394/SOLR-3055.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Koji Sekiguchi",
            "id": "comment-13196002",
            "date": "2012-01-30T09:39:24+0000",
            "content": "How about introducing something like GramSizeAttribute?\n\nI attached just an idea and draft level patch. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13196066",
            "date": "2012-01-30T11:49:26+0000",
            "content": "Hi Koji: I think as far as attribute+QP, it might not be the best way to go.\n\nFor example, another way (and customization of phrase query) is on SOLR-2660.\nIn that patch i added factory methods to QueryParser so you can override this:\nthen hooks to solr's fieldtype.\n\nBut with the attribute approach, what happens if I omit positions AND use n-grams?\nThis is a totally reasonable thing to do, since positions are redundantly encoded\nin the n-gram term text, it makes sense i might not index any positions at all\nand approximate my phrase queries with boolean AND \n\nI think subclassing is a better approach: because otherwise how do we \ndetermine which would run first in the case of multiple conflicting attributes?\n\nIn this case then the consumer (e.g. Solr) is forced to decide and its more consistent\nwith the way other queries are generated: getXXXQuery() etc. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13196078",
            "date": "2012-01-30T12:10:32+0000",
            "content": "But, an advantage to the approach of this patch, is that it would work when not all text is n-grammed right?\nE.g. the case of CJKAnalyzer, where english does not form ngrams. I think this is important.\n\nMaybe there is some way to have the best of both... "
        },
        {
            "author": "Tomoko Uchida",
            "id": "comment-14255790",
            "date": "2014-12-22T14:45:14+0000",
            "content": "Hi,\nIt seems not active for 3 years... I would like to rework this issue.\n\nI think this should be integrated to Lucene because,\n\n\tNGramPhraseQuery is tightly related to NGramTokenizer, and it seems to be natural that coupling them at Lucene layer.\n\tIt would be also good for all Lucene based search engines to gain performance improvement.\n  https://issues.apache.org/jira/browse/LUCENE-3426\n\n\n\nSo the patch (adding new attribute to Lucene) looks good for me at first glance... more discussion is needed of course.\n\nWould anyone approve me? if so, I'd like to move the discussion to (new) LUCENE issue.\nOr any suggestions are appreciated. \n\nThanks. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-14255811",
            "date": "2014-12-22T15:07:02+0000",
            "content": "Thank you for paying attention to this ticket! It's good to me you start this in Lucene. "
        },
        {
            "author": "Tomoko Uchida",
            "id": "comment-14259343",
            "date": "2014-12-27T11:16:27+0000",
            "content": "Again, I think there are three strategies for implementation.\n\n1. embed gram size information in TokenStraem by adding new attribute (taken by first patch)\n\n\tPros: fully integrated with Lucene, so any application have not to write additional codes to optimize n-gram based phrase query\n\tPros: no configuration is needed because query parser create NGramPhraseQuery automatically\n\tPros: maybe most simple to implement\n\tCons: there might be some kind of conflicts with other attributes?\n\n\n\n2. NGramTokenizer expose \"gramSize\" for later use, and Solr's QueryParser create NGramPhraseQuery\n\n\tPros: no effect to Lucene's default behavior\n\tPros: no configuration is needed because query parser create NGramPhraseQuery automatically\n\tCons: extra codes are needed to use NGramPhraseQuery per each query parser\n\n\n\n3. add \"gramSize\" (or something like) attribute to schema.xml, and Solr's query parser create NGramPhraseQuery using given gramSize by user\n\n\tPros: no effect to Lucene's and Solr's default behavior\n\tCons: new configuration attribute will be introduced\n\tCons: what's happen if user give gramSize value inconsistent with minGramSize or maxGramSize given to NGramTokenizer? maybe it's problematic.\n\n\n\nI attach two patches, one (SOLR-3055-1.patch) for strategy 1 and other (SOLR-3055-2.patch) for strategy 2.\nReviews / suggestions will be appreciated. "
        },
        {
            "author": "Tomoko Uchida",
            "id": "comment-14259351",
            "date": "2014-12-27T11:44:26+0000",
            "content": "I performed brief benchmark by JMeter for Solr 5.0.0 trunk and strategy 1.\nThere seems to be significant performance gain for n-gram based phrase query.\n\n\n\tHardware : MacBook Pro, 2.8GHz Intel Core i5\n\tJava version : 1.7.0_71\n\tSolr version : 5.0.0 SNAPSHOT / 5.0.0 SNAPSHOT with SOLR-3055-1.patch\n\tJava heap : 500MB\n\tDocuments : Wikipedia (Japanese) 100000 docs\n\tSolr config : attached solrconfig.xml (query result cache disabled)\n\tSchema : attached schema.xml (NGramTokenizer's maxGramSize=3, minGramSIze=2)\n\tQueries : \"python\", \"javascript\", \"windows\", \"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\", \"\u30a4\u30f3\u30bf\u30fc\u30cd\u30c3\u30c8\", \"\u30b9\u30de\u30fc\u30c8\u30d5\u30a9\u30f3\" (japanese)\n\tJMeter scenario : execute each 6 queries above 1000 times (i.e. perform 6000 queries)\n\tJMeter Threads : 1\n\n\n\nTo warm up, I performed 2 times JMeter scinario for both settings. \n2nd round results are:\n\n\n\n\n Solr \n Avg. response time \n Throughput \n\n\n 5.0.0-SNAPSHOT \n 7msec \n 137.8/sec \n\n\n 5.0.0-SNAPSHOT with patch-1 \n 4msec \n 201.3/sec \n\n\n\n "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-14259553",
            "date": "2014-12-28T02:42:36+0000",
            "content": "Hi Uchida-san, thank you for your effort for reworking this issue!\n\nAccording to your observation (pros and cons), I like the 1st strategy to go on. And if you agree, why don't you add test cases for that one? And also, don't we need to consider other n-gram type Tokenizers even TokenFilters, such as NGramTokenFilter and CJKBigramFilter?\n\nAnd, I think there is a restriction when minGramSize != maxGramSize. If it's not significant, I think we can examine the restriction separately from this issue because we rarely set different values to those for searching CJK words. But we use a lot NGramTokenizer with fixed gram size for searching CJK words, and we could get a nice performance gain by the patch as you've showed us. "
        },
        {
            "author": "Tomoko Uchida",
            "id": "comment-14259560",
            "date": "2014-12-28T03:55:24+0000",
            "content": "Thank you for your response.\n\nI will add test codes and updated patch to consider other Tokenizers / TokenFilters.\n\nMy patch seems to work well for both case, minGramSize == maxGramSize and minGramSize != maxGramSize. But not optimized for maxGramSize. \nIn the case of minGramSize != maxGramSize, using maxGramSize for optimization derives the best performance improvement. We can examine about that (maybe need another issue.) In practice, we often set fixed gram size for CJK words as you pointed, so I think it is beneficial even if it is not optimized for maxGramSize. "
        },
        {
            "author": "Tomoko Uchida",
            "id": "comment-14264605",
            "date": "2015-01-05T14:05:14+0000",
            "content": "I've created LUCENE-6163 and added a patch (because this patch affects lucene-core and lucene-analysis, does not solr.)\nI hope to keep working there. "
        }
    ]
}