{
    "id": "LUCENE-185",
    "title": "[PATCH] Thai Analysis Enhancement",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Won't Fix",
        "status": "Resolved"
    },
    "description": "Unlike other languages, Thai do not have a clear word boundary within a\nsentence. Words are  written consecutively without a delimiter. The Lucene\nStandardTokenizer currently cannot tokenize a Thai sentence and returns the\nwhole sentence as a token. A special tokenizer to break Thai sentences into\nwords is required.",
    "attachments": {
        "ASF.LICENSE.NOT.GRANTED--thai_analyzer.zip": "https://issues.apache.org/jira/secure/attachment/12312310/ASF.LICENSE.NOT.GRANTED--thai_analyzer.zip"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2004-02-24T15:18:55+0000",
            "content": "Created an attachment (id=10509)\nThai analyzer based on Java BreakIterator ",
            "author": "Pichai Ongvasith",
            "id": "comment-12321575"
        },
        {
            "date": "2004-02-24T21:10:52+0000",
            "content": "I asked licensing@ about the RIWord License (included in the riword file in the\nattached ZIP archive).\nApparently it's a modified MIT license, which is OK:\n\n\"It's a modified MIT Licence\nhttp://www.opensource.org/licenses/mit-license.php, which I'm\npretty/very sure is allowed under the ASF allowed licences list.\n\nI'm not sure if there's anything odd you have to do, ie) package\nthe file separately from the Lucene code when dealing with an allowed\nseparate licence.\n\nHen\" ",
            "author": "Otis Gospodnetic",
            "id": "comment-12321576"
        },
        {
            "date": "2004-02-24T21:13:52+0000",
            "content": "A note from the original contribution email, for archiving purposes:\n\nThai is one of those languages that has no whitespace\nbetween words. Because of this, Lucene\nStandardTokenizer can't tokenize a Thai sentence and\nreturn the whole sentence as a token. \n\nJDK 1.4 comes with a simple dictionary based tokenizer\nfor Thai. With the wrappers, I can use Thai\nBreakIterator to tokenize Thai sentences returned from\nStdTokenizer.\n\nMy design is quite simple. I added <THAI> tag to\nStandardTokenizer.jj (I rename it to\nTestStandardTokenizer.jj in my test). The\nStandardTokenizer then returns a Thai sentence with\nthe tag <THAI>, among other ordinary tokens. Then\nBreakIteratorTokenTokenizer detects the token and\nfurther breaks it down into smaller tokens, which\nrepresent actual Thai words. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12321577"
        },
        {
            "date": "2008-01-02T14:29:05+0000",
            "content": "There is a Thai analysis contribution in contrib/analysis that appears to have taken a similar approach. ",
            "author": "Grant Ingersoll",
            "id": "comment-12555321"
        }
    ]
}