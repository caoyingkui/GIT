{
    "id": "LUCENE-2047",
    "title": "IndexWriter should immediately resolve deleted docs to docID in near-real-time mode",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "Spinoff from LUCENE-1526.\n\nWhen deleteDocuments(Term) is called, we currently always buffer the\nTerm and only later, when it's time to flush deletes, resolve to\ndocIDs.  This is necessary because we don't in general hold\nSegmentReaders open.\n\nBut, when IndexWriter is in NRT mode, we pool the readers, and so\ndeleting in the foreground is possible.\n\nIt's also beneficial, in that in can reduce the turnaround time when\nreopening a new NRT reader by taking this resolution off the reopen\npath.  And if multiple threads are used to do the deletion, then we\ngain concurrency, vs reopen which is not concurrent when flushing the\ndeletes.",
    "attachments": {
        "LUCENE-2047.patch": "https://issues.apache.org/jira/secure/attachment/12424542/LUCENE-2047.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-11-09T20:32:15+0000",
            "content": "Is this going to be an option or default to true only when NRT is on?\n\nAlso, I can create a patch for this. ",
            "author": "Jason Rutherglen",
            "id": "comment-12775115"
        },
        {
            "date": "2009-11-09T20:42:16+0000",
            "content": "I think simply default to true.\n\nYes, please cons up a patch! ",
            "author": "Michael McCandless",
            "id": "comment-12775121"
        },
        {
            "date": "2009-11-11T00:50:25+0000",
            "content": "Deletes occur immediately if poolReader is true\n\nI'm not sure updateDocument needs to delete immediately, as it's also writing a document, the deletes later would be lost in the noise. ",
            "author": "Jason Rutherglen",
            "id": "comment-12776198"
        },
        {
            "date": "2009-11-11T00:55:44+0000",
            "content": "Thanks Jason!\n\nI would think we do want to delete live for updateDocument as well?  It's not clear the deletion will be in the noise (if it hits a disk seek, it won't). ",
            "author": "Michael McCandless",
            "id": "comment-12776203"
        },
        {
            "date": "2009-11-11T00:56:03+0000",
            "content": "Pushing to 3.1... ",
            "author": "Michael McCandless",
            "id": "comment-12776204"
        },
        {
            "date": "2009-11-11T21:39:31+0000",
            "content": "I would think we do want to delete live for\nupdateDocument as well? It's not clear the deletion will be in\nthe noise (if it hits a disk seek, it won't).\n\nThe delete shouldn't hit a disk seek because with poolReaders,\nthe deletes are applied and carried over in RAM. I'd assume the\noverhead of flushing the segment would be the main consumption\nof CPU and perhaps IO? Which with LUCENE-1313, is less of an\nissue. \n\nFor consistency with deleteDocument, I'll add live deleting to\nupdateDocument.  ",
            "author": "Jason Rutherglen",
            "id": "comment-12776674"
        },
        {
            "date": "2009-11-11T22:23:58+0000",
            "content": "We could very likely hit seeks resolving the term -> docID.  EG to consult the terms dict index (eg page fault), terms dict itself, and then to load the posting.  (Once we have pulsing, we can save that seek). ",
            "author": "Michael McCandless",
            "id": "comment-12776691"
        },
        {
            "date": "2009-11-12T03:01:38+0000",
            "content": "likely hit seeks resolving the term -> docID\n\nRight, duh!  Hopefully we won't get into the delete by doc id discussion again.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12776804"
        },
        {
            "date": "2009-11-12T06:05:20+0000",
            "content": "I added deleting live for updateDocument.\n\nTestNRTReaderWithThreads and TestIndexWriterReader passes.\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12776863"
        },
        {
            "date": "2009-11-12T06:12:33+0000",
            "content": "I'd like to remove the syncing in the deleteLive methods, however this causes other tangential exceptions.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12776865"
        },
        {
            "date": "2009-11-12T06:24:56+0000",
            "content": "Without syncing on the writer, we run into the SR's files being deleted out from under it while it's being obtained for deletion (I think). ",
            "author": "Jason Rutherglen",
            "id": "comment-12776868"
        },
        {
            "date": "2009-11-12T07:00:30+0000",
            "content": "After thumbing a bit through the code, somehow, and this is a\nbit too complex for me to venture a guess on, we'd need to\nprevent the deletion of the SR's files we're deleting from, even\nif that SR is no longer live. Which means possibly interacting\nwith the deletion policy, or some other method. It's a bit hairy\nin the segment info transaction shuffling that goes on in IW. \n\nI believe asyncing the live deletes is a good thing, as that way\nwe're taking advantage of concurrency. The possible expense is in\ndeleting from segments that are no longer live while the\ndeleting is occurring. ",
            "author": "Jason Rutherglen",
            "id": "comment-12776877"
        },
        {
            "date": "2009-11-13T01:46:39+0000",
            "content": "I think there's a fundamental design issue here. What happens to\ndocuments that need to be deleted but are still in the RAM\nbuffer? Because we're not buffering the deletes those wouldn't\nbe applied. Or would we still buffer the deletes, then only\napply them only to the new SR created from the RAM buffer when\nflush or getReader is called?\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12777325"
        },
        {
            "date": "2009-11-13T10:30:45+0000",
            "content": "\nwe'd need to\nprevent the deletion of the SR's files we're deleting from, even\nif that SR is no longer live. \n\nIt's strange that anything here is needed, because, when you check a\nreader out from the pool, it's incRef'd, which should mean the files\nneed no protection.  Something strange is up... could it be that when\nyou checkout that reader to do deletions, it wasn't already open, and\nthen on trying to open it, its files were already deleted?  (In which\ncase, that segment has been merged away, and, the merge has committed,\nie already carried over all deletes, and so you should instead be\ndeleting against that merged segment).\n\nSo I think the sync(IW) is in fact necessary?  Note that the current\napproach (deferring resolving term -> docIDs until flush time) aiso\nsync(IW)'d, so we're not really changing that, here.  Though I agree\nit would be nice to not have to sync(IW).  Really what we need to sync\non is \"any merge that is merging this segment away and now wants to\ncommit\".  That's actually a very narrow event so someday (separate\nissue) if we could refine the sync'ing to that, it should be a good\nnet throughput improvement for updateDocument.\n\n\nWhat happens to\ndocuments that need to be deleted but are still in the RAM\nbuffer?\n\nAhh, yes.  We must still buffer for this case, and resolve these\ndeletes against the newly flushed segment.  I think we need a separate\nbuffer that tracks pending delete terms only against the RAM buffer?\n\nAlso, instead of actually setting the bits in SR's deletedDocs, I\nthink you should buffer the deleted docIDs into DW's\ndeletesInRAM.docIDs?  Ie, we do the resolution of Term/Query -> docID,\nbut buffer the docIDs we resolved to.  This is necessary for\ncorrectness in exceptional situations, eg if you do a bunch of\nupdateDocuments, then DW hits an aborting exception (meaning its RAM\nbuffer may be corrupt) then DW currently discards the RAM buffer, but,\nleaves previously flushed segments intact, so that if you then commit,\nyou have a consistent index.  Ie, in that situation, we don't want the\ndocs deleted by updateDocument calls to be committed to the index, so\nwe need to buffer them. ",
            "author": "Michael McCandless",
            "id": "comment-12777436"
        },
        {
            "date": "2009-11-13T20:20:30+0000",
            "content": "It's strange that anything here is needed\n\nI was obtaining the segment infos synced, had a small block of\nunsynced code, then synced obtaining the sometimes defunct\nreaders. Fixed that part, then the errors went away!\n\nthe sync(IW) is in fact necessary? \n\nI'm hoping we can do the deletes unsynced, which will make this\npatch a net performance gain because we're allowing multiple\nthreads to delete concurrently (whereas today we're performing\nthem synced at flush time, i.e. the current patch is merely\nshifting the term/query lookup cost from flush to\ndeleteDocument).\n\nbuffer the deleted docIDs into DW's deletesInRAM.docIDs\n\nI'll need to step through this, as it's a little strange to me\nhow DW knows the doc id to cache for a particular SR, i.e. how\nare they mapped to an SR? Oh there's the DW.remapDeletes method?\nHmm...\n\nCouldn't we save off a per SR BV for the update doc rollback\ncase, merging the special updated doc BV into the SR's deletes\non successful flush, throwing them away on failure?  Memory is\nless of a concern with the paged BV from the pending LUCENE-1526\npatch.  On a delete by query with many hits, I'm concerned about\nstoring too many doc id Integers in BufferedDeletes. \n\nWithout syncing, new deletes could arrive, and we'd need to\nqueue them, and apply them to new segments, or newly merged\nsegments because we're not locking the segments. Otherwise some\ndeletes could be lost. \n\nA possible solution is, deleteDocument would synchronously add\nthe delete query/term to a queue per SR and return.\nAsynchronously (i.e. in background threads) the deletes could be\napplied. Merging would aggregate the incoming SR's queued\ndeletes (as they haven't been applied yet) into the merged\nreader's delete queue. On flush we'd wait for these queued\ndeletes to be applied.  After flush, the queues would be clear\nand we'd start over.  And because the delete queue is per reader,\nit would be thrown away with the closed reader.  ",
            "author": "Jason Rutherglen",
            "id": "comment-12777638"
        },
        {
            "date": "2009-11-14T00:41:24+0000",
            "content": "Also, if the per SR delete queue were implemented, we could expose\nthe callback, and allow users to delete by doc id, edit norms\n(and in the future, update field caches) for a particular\nIndexReader.  We'd pass the reader via a callback that resembles\nIndexReaderWarmer, then deletes, norms updates, etc, could be\nperformed like they can be today with a non-readonly IR. ",
            "author": "Jason Rutherglen",
            "id": "comment-12777773"
        },
        {
            "date": "2009-11-14T13:16:42+0000",
            "content": "\nI'm hoping we can do the deletes unsynced, which will make this\npatch a net performance gain because we're allowing multiple\nthreads to delete concurrently (whereas today we're performing\nthem synced at flush time, i.e. the current patch is merely\nshifting the term/query lookup cost from flush to\ndeleteDocument).\n\nMerely shifting the cost off the critical reopen path is already a\ngood step forward \n\nBut I agree, if we can also allow deletes to happen concurrently, that\nwould be fabulous.\n\nCurrently the buffered deleted docIDs in DW are only due to exceptions\nhit while adding a document.  EG say for a given doc there was an\nexception during analysis, after yielding 100 tokens.  At this point\nDW's RAM buffer (postings) has already been changed to hold this\ndocID, and we can't easily undo that.  So we instead mark the docID as\nimmediately deleted.  These deleted docIDs can then carry in RAM for\nsome time, and get remapped when merges commit.\n\nOn a delete by query with many hits, I'm concerned about storing too many doc id Integers in BufferedDeletes.\n\nI agree, that's a risk if we are buffering an Integer for each deleted\ndocID, so we should avoid that.\n\nWe really just need an efficient IntSet.  Or, it's even possible a\ndata structure that does no deduping (eg a simple growable int[])\nwould be fine, since apps would tend not to delete the same docID over\nand over.  Since we flush when deletes use too much RAM, we'd be\nprotected from silly apps...\n\nOr maybe start with a growable int[], but cutover to BV once that's\ntoo large?  This would protect the worst case of deleting by a single\nquery matching many docs in the index.\n\nWhatever data structure it is, it should live under BufferedDeletes,\nso the exception logic that already discards these deletes on hitting\nan aborting exception will just continue to work.\n\nMemory is less of a concern with the paged BV from the pending LUCENE-1526 patch\n\nI'm not sure we even want to switch to a paged BV for NRT in general\n(just added comment to that effect), though I guess for just this case\n(buffering deletes in IW in NRT mode) it could make sense?\n\n\nCouldn't we save off a per SR BV for the update doc rollback\ncase, merging the special updated doc BV into the SR's deletes\non successful flush, throwing them away on failure? \n\nI like that approach.  Because it means the normal case (no exception\nis hit) is very fast (no merging of the two BufferedDeletes, on flush,\nlike we do today; and no docID remapping required), and only on\nexception must we restore the previously saved deletes.\n\nAnother alternative would be to redefine the semantics of IW on\nhitting an error.... right now, if you hit an aborting exception (one\nthat may have corrupted DW's internal RAM postings), you only lose\nthose docs buffered in DW at the time.  Any already flushed segments &\nnew deletions within this IW session are preserved.  So in theory if\nyou closed your IW, those changes would persist.\n\nWe could consider relaxing this, such that the entire session of IW is\nrolled back, to the last commit/when-IW-was-opened, just like we now\ndo with OOME.\n\n\nA possible solution is, deleteDocument would synchronously add\nthe delete query/term to a queue per SR and return.\nAsynchronously (i.e. in background threads) the deletes could be\napplied. \nI'd prefer not to add further BG threads to IW, ie, take the app's\nthread to perform the deletion.  If the app wants concurrency for\ndeletes, it can use multiple threads.\n\nI believe we only have to sync on the merge committing its deletes,\nright?  So we should make a separate lock for that?\n\nAnd, on commit() we must also wait for all in-flight deletes to\nfinish.\n\nFinally, when a new segment is flushed, we should resolve the buffered\nTerm/Query deletions against it, during the flush?\n\n\nAlso, if the per SR delete queue were implemented, we could expose\nthe callback, and allow users to delete by doc id, edit norms\n(and in the future, update field caches) for a particular\nIndexReader. We'd pass the reader via a callback that resembles\nIndexReaderWarmer, then deletes, norms updates, etc, could be\nperformed like they can be today with a non-readonly IR.\n\nI'd like to strongly decouple this discussion of extensibility, from\nwhat we're doing in this issue.  I think it's a good idea, but let's\nhandle it separately \u2013 maybe under LUCENE-2026 (refactoring IW,\nwhich'd pull out the reader pool).  This issue should all be \"under\nthe hood\" improvements.\n ",
            "author": "Michael McCandless",
            "id": "comment-12777912"
        },
        {
            "date": "2009-11-16T13:22:29+0000",
            "content": "Thinking more on this... I'm actually no longer convinced that this\nchange is worthwhile.\n\nNet/net this will not improve the dps/qps throughput on a given fixed\nhardware, because this is a zero sum game: the deletes must be\nresolved one way or another.\n\nWhether we do it in batch (as today), or incrementally/concurrently,\none at a time as they arrive, the same work must be done.  In fact,\nbatch should be less costly in practice since it clearly has temporal\nlocality in resolving terms -> postings, so on a machine whose IO\ncache can't hold the entire index in RAM, bulk flushing should be\na win.\n\nIt's true that net latency of reopen will be reduced by being\nincremental, but Lucene shouldn't aim to be able to reopen 100s of\ntimes per second: I think that's a mis-feature (most apps don't need\nit), and those that really do can and should use an approach like\nZoie.\n\nFinally, one can always set the max buffered delete terms/docs to\nsomething low, to achieve this same tradeoff.  It's true that won't\nget you concurrent resolving of deleted Terms -> docIDs, but I bet in\npractice that concurrency isn't necessary (ie the performance of a\nsingle thread resolving all buffered deletes is plenty fast). \n\nIf the reopen time today is plenty fast, especially if you configure\nyour writer to flush often, then I don't think we need incremental\nresolving of the deletions? ",
            "author": "Michael McCandless",
            "id": "comment-12778335"
        },
        {
            "date": "2009-11-16T18:57:33+0000",
            "content": "I want to replay how DW handle the updateDoc call to see if my\nunderstanding is correct. \n\n1: Analyzing hits an exception for a doc, it's doc id has\nalready been allocated so we mark it for deletion later (on\nflush?) in BufferedDeletes.\n\n2: RAM Buffer writing hits an exception, we've had updates which\nmarked deletes in current segments, however they haven't been\napplied yet because they're stored in BufferedDeletes docids.\nThey're applied on successful flush. \n\nAre these the two scenarios correct or am I completely off\ntarget? If correct, isn't update doc already deleting in the\nforeground?\n\nprefer not to add further BG threads\n\nMaybe we can use 1.5's ReentrantReadWriteLock to effectively\nallow multiple del/update doc calls to concurrently acquire the\nread lock, and perform the deletes in the foreground. The write\nlock could be acquired during commitDeletes, commit(), and after\na segment is flushed? I'm not sure it would be necessary to\nacquire this write lock anytime segment infos is changed?\n\nI think it's important to remove unnecessary global locks on\nunitary operations (like deletes). We've had great results\nremoving these locks for isDeleted, (NIO)FSDirectory where we\ndidn't think there'd be an improvement, and there was. I think\nthis patch (or a follow on one that implements the shared lock\nsolution) could effectively increase throughput (for deleting\nand updating), measurably.\n\nLucene shouldn't aim to be able to reopen 100s of times\nper second\n\nReopening after every doc could be a valid case that I suspect\nwill come up again in the future. I don't think it's too hard to\nsupport. \n\n It's true that net latency of reopen will be reduced by\nbeing incremental, but Lucene shouldn't aim to be able to reopen\n100s of times per second: \n\nPerhaps update/del throughput will increase because of the\nshared lock which would makes the patch(s) worth implementing.\n\n but I bet in practice that concurrency isn't necessary\n(ie the performance of a single thread resolving all buffered\ndeletes is plenty fast). \n\nWe thought the same thing about the sync in FSDirectory, and it\nturned out that in practice, NIOFSDir is an order of magnitude\nfaster on *nix machines. For NRT, every little bit of\nconcurrency will probably increase throughput. (i.e. most users\nwill have their indexes in IO cache and/or a ram dir, which\nmeans we wouldn't be penalizing concurrency as we are today with\nthe global lock IW for del/up docs). \n\nI'm going to go ahead and wrap up this patch, which will shift\ndeletion cost to the del/up methods (still synchronously). Then\ncreate a separate patch that implements the shared lock\nsolution. \n\nExposing SRs for updates by the user can be done today, I'll\nopen a patch for this. ",
            "author": "Jason Rutherglen",
            "id": "comment-12778452"
        },
        {
            "date": "2009-11-16T22:52:18+0000",
            "content": "Reopening after every doc could be a valid case that I suspect will come up again in the future.\n\nI suspect the vast majority of apps would be fine with 10 reopens per\nsecond.\n\nThose that really must reopen 100s of times per second can use Zoie,\nor an approach like it.\n\nI don't think it's too hard to support.\n\nWhoa!  Merely thinking about and discussing even how to run proper\ntests for NRT, let alone the possible improvements to Lucene on the\ntable, is sucking up all my time \n\n\nI think it's important to remove unnecessary global locks on\nunitary operations (like deletes).\n\nYeah, I agree we should in general always improve our concurrency.  In\nthis case, resolving deletes syncs the entire IW + DW, so that blocks\nindexing new docs, launching/committing merges, flushing, etc. which\nwe should fix.  I just don't think NRT is really a driver for this...\n\n\n1: Analyzing hits an exception for a doc, it's doc id has\nalready been allocated so we mark it for deletion later (on\nflush?) in BufferedDeletes.\n\nAnalyzing or any other \"non-aborting\" exception, right.\n\n\n2: RAM Buffer writing hits an exception, we've had updates which\nmarked deletes in current segments, however they haven't been\napplied yet because they're stored in BufferedDeletes docids.\nThey're applied on successful flush.\n\nNo \u2013 the deletes are buffered as Term, Query or docids, in the\nBufferedDeletes.  The only case that buffers docids now is your #1\nabove.  On successful flush, these buffered things are moved to the\ndeletesFlush (but not resolved).  They are only resolved when we\ndecide it's time to apply them (just before a new merge starts, or,\nwhen we've triggered the time-to-flush-deletes limits).\n\n\nMaybe we can use 1.5's ReentrantReadWriteLock to effectively\nallow multiple del/update doc calls to concurrently acquire the\nread lock, and perform the deletes in the foreground.\n\nI think that should work well? ",
            "author": "Michael McCandless",
            "id": "comment-12778620"
        },
        {
            "date": "2009-11-17T18:44:38+0000",
            "content": "resolving deletes syncs the entire IW + DW, so that\nblocks indexing new docs, launching/committing merges, flushing,\netc... I just don't think NRT is really a driver for\nthis...\n\nRight, I think it's a general improvement that's come to light\nduring NRT development and testing. I think NRT is great in this\nregard because it stresses Lucene in a completely new way, which\nimproves it for the general batch use case (i.e. users can\nsimply start utilizing NRT features when they need to without\nworry). \n\n 1: Analyzing hits an exception for a doc, it's doc id\nhas already been allocated so we mark it for deletion later (on\nflush?) in BufferedDeletes. \n\nSo there's only one use case right now, which is only when\nanalyzing an individual doc fails. The update doc adds the term\nto the BufferedDeletes for later application. Makes sense. I\nthink we can resolve the update doc term in the foreground.  I'm\nwondering if we need a different doc id queue for these? I get\nthe hunch yes, because the other doc ids need to be applied even\non IO exception, whereas update doc id will not be applied?\n\n 2: RAM Buffer writing hits an exception, we've had\nupdates which marked deletes in current segments, however they\nhaven't been applied yet because they're stored in\nBufferedDeletes docids. They're applied on successful flush.\n\nIn essence we need to implement number 2?\n\nAnalyzing or any other \"non-aborting\" exception,\nright.\n\nWhat is an example of a non-aborting exception?\n\n use 1.5's ReentrantReadWriteLock \n\nI'll incorporate RRWL into the follow on concurrent updating\npatch. \n\n Whoa! Merely thinking about and discussing even how to\nrun proper tests for NRT, let alone the possible improvements to\nLucene on the table, is sucking up all my time \n\nYow, I didn't know. Thanks! ",
            "author": "Jason Rutherglen",
            "id": "comment-12779037"
        },
        {
            "date": "2009-11-17T19:33:12+0000",
            "content": "I'm browsing through the applyDeletes call path, I'm tempted to\nrework how we're doing this. For my own thinking I'd still like\nto have a queue of deletes per SR and for the ram doc buffer. I\nthink this gives future flexibility and makes it really clear\nwhen debugging what's happening underneath. I find the remapping\ndoc ids to be confusing, meaning stepping through the code would\nseem to be difficult. If we're storing doc ids alongside the SR\nthe docs correspond to, there's a lot less to worry about and\njust seems clearer. This may make integrating LUCENE-1313\ndirectly into IW more feasible as then we're working directly at\nthe SR level, and can tie the synchronization process together.\nAlso this could make exposing SRs externally easier and aid in\nmaking IW more modular in the future?\n\nI can't find the code that handles aborts.  ",
            "author": "Jason Rutherglen",
            "id": "comment-12779075"
        },
        {
            "date": "2009-11-17T20:09:05+0000",
            "content": "\n\n1: Analyzing hits an exception for a doc, it's doc id has already been allocated so we mark it for deletion later (on flush?) in BufferedDeletes.\n\nSo there's only one use case right now, which is only when\nanalyzing an individual doc fails. The update doc adds the term\nto the BufferedDeletes for later application.\n\nNo, it adds the docID for later application.  This is the one case\n(entirely internal to IW) where we delete by docID in the writer.\n\n\nI think we can resolve the update doc term in the foreground. I'm\nwondering if we need a different doc id queue for these? I get\nthe hunch yes, because the other doc ids need to be applied even\non IO exception, whereas update doc id will not be applied?\n\nI think we can use the same queue \u2013 whether they are applied or not\nfollows exactly the same logic (ie, successful flush moves all\ndeletesInRAM over to deletesFlushed), ie, an aborting exception\ncleares the deletesInRAM.\n\n\n2: RAM Buffer writing hits an exception, we've had\nupdates which marked deletes in current segments, however they\nhaven't been applied yet because they're stored in\nBufferedDeletes docids. They're applied on successful flush.\n\nIn essence we need to implement number 2?\n\nI'm confused \u2013 #2 is already what's implemented in IW, today.\n\nThe changes on the table now are to:\n\n\n\tImmediately resolve deleted (updated) terms/queries -> docIDs\n\n\n\n\n\tChange how we enqueue docIDs to be per-SR, instead.\n\n\n\n\n\tBut: you still must buffer Term/Query for the current RAM buffer,\n    and on flushing it to a real segment, resolve them to docIDs.\n\n\n\nOtherwise we don't need to change what's done today (ie, keep the\ndeletesInRAM vs deletesFlushed)?\n\nWhat is an example of a non-aborting exception?\n\nAnything hit during analysis (ie, TokenStream.incrementToken()), or\nanywhere except DocInverterPerField in the indexing chain (eg if we\nhit something when writing the stored fields, I don't think it'll\nabort).\n\n\nI'm browsing through the applyDeletes call path, I'm tempted to\nrework how we're doing this.\n\nI think this would be a good improvement \u2013 it would mean we don't\nneed to ever remapDeletes, right?\n\nThe thing is... this has to work in non-NRT mode, too.  Somehow, you\nhave to buffer such a deleted docID against the next segment to be\nflushed (ie the current RAM buffer).  And once it's flushed, we move\nthe docID from deletesInRAM (stored per-SR) to the SR's actual deleted\ndocs BV.\n\nWe would still keep the deletes partitioned, into those done during\nthe current RAM segment vs those successfully flushed, right?\n\nI can't find the code that handles aborts.\n\nIt's DW's abort() method, and eg in DocInverterPerField we call\nDW.setAborting on exception to note that this exception is an aborting\none. ",
            "author": "Michael McCandless",
            "id": "comment-12779093"
        },
        {
            "date": "2009-11-18T03:17:10+0000",
            "content": "\n\tThere's pending deletes (aka updateDoc generated deletes) per\nSR. They're stored in a pending deletes BV in SR. \n\n\n\n\n\tcommitMergedDeletes maps the pending deletes into the\nmergeReader. \n\n\n\n\n\tDW.abort clears the pending deletes from all SRs.\n\n\n\n\n\tOn successful flush, the SR pending deletes are merged into the\nprimary del docs BV. \n\n\n\n\n\tDeletes are still buffered, however they're only applied to the\nnewly flushed segment (rather than all readers). If the applying\nfails, I think we need to keep some of the rollback from the\noriginal applyDeletes?\n\n\n\n\n\tThe foreground deleting seems to break a couple of tests in\nTestIndexWriter.\n\n\n\nMike, you mentioned testing getReader missing deletes etc (in\nresponse to potential file handle leakage), which test or\nbenchmark did you use for this? ",
            "author": "Jason Rutherglen",
            "id": "comment-12779274"
        },
        {
            "date": "2009-11-23T02:52:14+0000",
            "content": "In the updateDocument and deleteDocument methods, deletes are\nbuffered per segment reader synchronized on writer. Immediately\nafter, outside the sync block, they're deleted from the existing\nSRs. If a new SR is added, it's because of a flush (which has\nit's own buffered deletes), or from a merge which is\nsynchronized on writer. In other words, we won't lose deletes,\nthey'll always be applied on a flush, and the resolution of\ndeletes to doc ids happens un-synchronized on writer.\n\nUpdate document adds the term to the queued deletes, then\nresolves and adds the doc ids to an Integer list (for now). This\nis where we may want to use an growable int[] or int set.\n\nFlush applies queued update doc deleted doc ids to the SRs.\n\ncommitMerge merges queued deletes from the incoming SRs. Doc ids\nare mapped to the merged reader. ",
            "author": "Jason Rutherglen",
            "id": "comment-12781288"
        },
        {
            "date": "2009-11-23T03:03:31+0000",
            "content": "When docWriter aborts on the RAM buffer, we clear out the queued updateDoc deletes. ",
            "author": "Jason Rutherglen",
            "id": "comment-12781290"
        },
        {
            "date": "2009-11-23T05:09:55+0000",
            "content": "DocWriter abort created a deadlock noticed in TestIndexWriter.testIOExceptionDuringAbortWithThreads.  This is fixed by clearing via the reader pool.  Other tests fail in TIW. ",
            "author": "Jason Rutherglen",
            "id": "comment-12781309"
        },
        {
            "date": "2009-11-24T02:25:22+0000",
            "content": "TestIndexWriter passes, mostly due to removing assertions in\nreader pool release which presumed deletions would be\nsynchronized with closing a reader. They're not anymore.\nDeletions can come in at almost anytime, so a reader may be\nclosed by the pool while still carrying deletes. The releasing\nthread may not be synchronized on writer because we're allowing\ndeletions to occur un-synchronized.\n\nI suppose we need more tests to insure the assertions are in\nfact not needed. ",
            "author": "Jason Rutherglen",
            "id": "comment-12781747"
        },
        {
            "date": "2013-07-23T18:44:35+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13716993"
        },
        {
            "date": "2014-04-16T12:54:47+0000",
            "content": "Move issue to Lucene 4.9. ",
            "author": "Uwe Schindler",
            "id": "comment-13970880"
        }
    ]
}