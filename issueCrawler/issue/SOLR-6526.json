{
    "id": "SOLR-6526",
    "title": "Solr Streaming API",
    "details": {
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [
            "6.0"
        ],
        "components": [
            "clients - java"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Duplicate"
    },
    "description": "It would be great if there was a SolrJ library that could connect to Solr's /export handler (SOLR-5244) and perform streaming operations on the sorted result sets.\n\nThis ticket defines the base interfaces and implementations for the Streaming API. The base API contains three classes:\n\nSolrStream: This represents a stream from a single Solr instance. It speaks directly to the /export handler and provides methods to read() Tuples and close() the stream\n\nCloudSolrStream: This represents a stream from a SolrCloud collection. It speaks with Zk to discover the Solr instances in the collection and then creates SolrStreams to make the requests. The results from the underlying streams are merged inline to produce a single sorted stream of tuples.\n\nTuple: The data structure returned by the read() method of the SolrStream API. It is nested to support grouping and Cartesian product set operations.\n\nOnce these base classes are implemented it paves the way for building Decorator streams that perform operations on the sorted Tuple sets. For example:\n\n\n//Create three CloudSolrStreams to different solr cloud clusters. They could be anywhere in the world.\n\nSolrStream stream1 = new CloudSolrStream(zkUrl1, queryRequest1, \"a\"); // Alias this stream as \"a\"\nSolrStream stream2 = new CloudSolrStream(zkUrl2, queryRequest2, \"b\"); // Alias this stream as \"b\"\nSolrStream stream3 = new CloudSolrStream(zkUrl3, queryRequest3, \"c\"); // Alias this stream as \"c\"\n\n// Merge Join stream1 and stream2 using a comparator to compare tuples.\n\nMergeJoinStream joinStream1 = new MergeJoinStream(stream1, stream2, new MyComp());\n\n//Hash join the tuples from the joinStream1 with stream3 the HashKey()'s define the hashKeys for tuples \nHashJoinStream joinStream2 = new HashJoinStream(joinStream1,stream3, new HashKey(), new HashKey());\n\n//Sum the aliased fields from the joined tuples.\nSumStream sumStream1 = new SumStream(joinStream2, \"a.field1\");\nSumStream sumStream2 = new SumStream(sumStream1, \"b.field2\");\nTuple t = null;\n\n//Read from the stream until it's finished.\nwhile((t != sumStream2().read()) != null);\n\n//Get the sums from the joined data.\n\nlong sum1 = sumStream1.getSum();\nlong sum2 = sumStream2.getSum();",
    "attachments": {
        "SOLR-6526.patch": "https://issues.apache.org/jira/secure/attachment/12669083/SOLR-6526.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Joel Bernstein",
            "id": "comment-14135604",
            "date": "2014-09-16T15:37:09+0000",
            "content": "A patch with the abstract classes to show what the planned interfaces are. "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14136858",
            "date": "2014-09-17T06:39:23+0000",
            "content": "Should the interface be tied to export? One nice future extension would be to have a streaming API which indexes to shard leaders directly (we currently can do direct indexing but that's not streamed). "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14137154",
            "date": "2014-09-17T12:40:37+0000",
            "content": "I think this api should respect the qt and shards.qt params so the user can specify the request handler.\nThe default would be the /export handler. For large streaming operations, the export handler is the only option that will work.\n\nStreaming indexing would be an interesting thing to tackle as well down the road.\n\nWrite-backs from the streaming API to Solr/SolrCloud will be important to support operations like nested distributed joins.\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14137281",
            "date": "2014-09-17T14:03:57+0000",
            "content": "Related to \"export\", I don't think we should be adding more interfaces where we have existing ones that serve just fine.\n\nIf we can't already, we should be able to stream large sets of documents on both the server side (solr) and on the client side (solrj).  This ability shouldn't be tied to a different export handler.\nIt really feels like /export should just be an optimization, not a different interface (perhaps with an execution hint if one wanted to force it to go one way or the other).\n\nThen this issue could add any additional streaming functionality needed over a normal document list response.  IIRC, there is already some streaming functionality to SolrJ, but not sure what else may be needed. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14137336",
            "date": "2014-09-17T14:40:44+0000",
            "content": "On the server side, you really need to do something very different then the normal Solr search to support full result set export. The /export handler is designed to sort and stream millions of results efficiently. An entirely new sorting engine and export engine needed to be written for this purpose. \n\nIn a normal search scenario, you don't need this feature, as normal search scenarios deal with pages of results. The export handler is not designed for normal search scenarios. It is designed for scenarios that were typically handled in an aggregation engine (distributed joins, session analysis etc...).\n\nHaving a separate interface for these very important use cases makes perfect sense.\n\nThe Streaming API is designed to be an elegant API for performing set operations (merges, joins, collapses) on large distributed result sets. This is also an entirely different use case then existing Solrj libraries which was designed for traditional search needs. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14137347",
            "date": "2014-09-17T14:48:45+0000",
            "content": "On the server side, you really need to do something very different\n\nThat's implementation.  I still haven't heard why we need a different interface. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14137372",
            "date": "2014-09-17T15:15:12+0000",
            "content": "The /export handler doesn't mesh with the full range of Solr features. It wasn't designed to because it was built to support specific non-traditional search use cases. For example you can't change the response writer with it or output facets. So it really is a different animal, having it's own request handler makes that clear. \n\nBut the use cases that it does support are extremely important. And having this feature available makes Solr much more powerful.\n\n\n\n\n "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14137394",
            "date": "2014-09-17T15:33:42+0000",
            "content": "Why is it OK for a feature like real time get to have it's own request handler, but it's not OK for /export? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14137422",
            "date": "2014-09-17T15:46:45+0000",
            "content": "The /export handler doesn't mesh with the full range of Solr features.\n\nI know... and in hindsight I don't think it should have gone through for those reasons.  There's no reason it couldn't mesh with the full range of Solr features.\n\nLet's not compound that problem by adding on more usecase-specific APIs when they aren't needed.\nThis streaming feature should work with any document list, not just \"export\".\n\nIf you look at /export (i.e. the xsort response format), it's very very close to using the standard doc list format.\nI think it should just be changed so that it matches.\n\nIt's currently this:\n\n{\"numFound\":32, \"docs\":[{\"id\":\"VA902B\",\"popularity\":6},\n\n\nBut should be this:\n\n{\n  \"response\":{\"numFound\":32,\"start\":0,\"docs\":[\n      {\n        \"id\":\"VA902B\",\n        \"popularity\":6},\n\n\n\nThen all of the existing solr client code and libraries out there can already read it.\n\nAnd given that /export is so restrictive (only works with certain sorts, fields with docvalues, etc) it should not be the default request handler target for this streaming. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14137452",
            "date": "2014-09-17T16:07:00+0000",
            "content": "Bringing the result set formats inline makes perfect sense, and I'll do that in this ticket. That way the streaming API will work with both result sets.\n\nHaving use case specific API's is justified when the use case is very important. Adding streaming aggregation functionality to Solr is very important.  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14137478",
            "date": "2014-09-17T16:22:50+0000",
            "content": "I just took a look at SolrJ again, and there is a StreamingBinaryResponseParser that has a StreamingResponseCallback that is called for every document.  And then SolrServer (the SolrJ client) has\n\n  public QueryResponse queryAndStreamResponse( SolrParams params, StreamingResponseCallback callback ) throws SolrServerException, IOException\n\n\nSo it seems like whatever is done hear should try and either use or align with the existing SolrJ streaming functionality. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14137481",
            "date": "2014-09-17T16:23:52+0000",
            "content": "Also, it's important to keep in mind that existing Solr clients will simply will run out of memory if they pull millions of records. They were built for a specific use case that involves bringing back pages of results.\n\nThe export handler was built to export and sort millions of results. So there is a basic mis-match between how the existing clients operate and how /export handler operates. The use cases a fundamentally different. If you want to return results pages you just use Solr's regular search.\n\nThe Streaming API though could apply to normal Solr searchs and /export'ed result sets so it makes sense to bring them inline. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14137493",
            "date": "2014-09-17T16:27:51+0000",
            "content": "Also, it's important to keep in mind that existing Solr clients will simply will run out of memory if they pull millions of records.\n\nOnly simplistic clients that don't stream.  I've often seen people who pull back very large result sets... it's one of the reasons Solr's response writers have always streamed.  It didn't make sense to have a separate interface just because some people might pull back more documents than others. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14137501",
            "date": "2014-09-17T16:32:23+0000",
            "content": "I'll review the existing StreamingBinaryResponseParser and StreamingResponseCallback. \n\nThose implementations though were not specifically designed for the merging of sorted streams. The decorator design that I have laid out in this ticket is an elegant approach to merging distributed result sets.  \n\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14137514",
            "date": "2014-09-17T16:39:41+0000",
            "content": "Those implementations though were not specifically designed for the merging of sorted streams.\n\nRight, the purpose was more general... streaming in general regardless of what you're doing with the docs being streamed.  We should always be aiming for more general, not more specific.  If there's something missing, or a limitation, we should first consider modifying/enhancing StreamingResponseCallback.  If StreamingResponseCallback is somehow just to limiting in general, then it should be deprecated and we should move onto a better way. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14145294",
            "date": "2014-09-23T19:29:17+0000",
            "content": "Here is how I propose we move this issue forward:\n\n1) As part of this ticket, change the current json format of the /export handler to match-up with the main json response writer.\n2) Open a different ticket to investigate ways to move the /export handlers bulk streaming capabilities into a normal Solr search. \n3) Refine the design, and then build out the Solr Streaming API based on stream decorators as described in this ticket. The inversion of control design of the StreamingResponseCallback is not the right design for scenarios that involve merging streams. For stream merging it's much cleaner to have direct control over the iteration of the streams.\n\nThe StreamingResponseCallback will still be valuable for scenarios where inversion of control works well. \n\n\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14145387",
            "date": "2014-09-23T20:33:16+0000",
            "content": "The inversion of control design of the StreamingResponseCallback is not the right design for scenarios that involve merging streams.\n\nBut I think it probably is the right lowest-level implementation.  It already works with the binary format, and if needed it could easily be made to work with others.  I think what makes most sense is to have an additional API on top of the callback that turns the push into a pull when desired. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14307872",
            "date": "2015-02-05T19:55:54+0000",
            "content": "This ticket has been superseded by SOLR-7082.  "
        }
    ]
}