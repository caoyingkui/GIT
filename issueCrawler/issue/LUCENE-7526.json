{
    "id": "LUCENE-7526",
    "title": "Improvements to UnifiedHighlighter OffsetStrategies",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [
            "modules/highlighter"
        ],
        "labels": "",
        "fix_versions": [
            "6.4"
        ],
        "priority": "Minor",
        "status": "Resolved",
        "type": "Improvement"
    },
    "description": "This ticket improves several of the UnifiedHighlighter FieldOffsetStrategies by reducing reliance on creating or re-creating TokenStreams.\n\nThe primary changes are as follows:\n\n\n\tAnalysisOffsetStrategy - split into two offset strategies\n\t\n\t\tMemoryIndexOffsetStrategy - the primary analysis mode that utilizes a MemoryIndex for producing Offsets\n\t\tTokenStreamOffsetStrategy - an offset strategy that avoids creating a MemoryIndex.  Can only be used if the query distills down to terms and automata.\n\t\n\t\n\n\n\n\n\tTokenStream removal\n\t\n\t\tMemoryIndexOffsetStrategy - previously a TokenStream was created to fill the memory index and then once consumed a new one was generated by uninverting the MemoryIndex back into a TokenStream if there were automata (wildcard/mtq queries) involved.  Now this is avoided, which should save memory and avoid a second pass over the data.\n\t\tTermVectorOffsetStrategy - this was refactored in a similar way to avoid generating a TokenStream if automata are involved.\n\t\tPostingsWithTermVectorsOffsetStrategy - similar refactoring\n\t\n\t\n\n\n\n\n\tCompositePostingsEnum - aggregates several underlying PostingsEnums for wildcard/mtq queries.  This should improve relevancy by providing unified metrics for a wildcard across all it's term matches\n\n\n\n\n\tAdded a HighlightFlag for enabling the newly separated TokenStreamOffsetStrategy since it can adversely affect passage relevancy",
    "attachments": {
        "LUCENE-7526.patch": "https://issues.apache.org/jira/secure/attachment/12838921/LUCENE-7526.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15612996",
            "author": "Timothy M. Rodriguez",
            "date": "2016-10-27T19:49:29+0000",
            "content": "Pull request forthcoming - I had some more merging work to do with master than I anticipated! "
        },
        {
            "id": "comment-15615427",
            "author": "David Smiley",
            "date": "2016-10-28T13:34:20+0000",
            "content": "I'm looking forward to seeing this.  \n\nThe summary heading \"TokenStream removal\" may be confusing to folks... TokenStreams are certainly going to be involved for the Analysis based offset source.  I think you mean that the changes here relating to MultiTermQuery processing will mean that MTQs aren't coerced into a TokenStream over the index any more, except for the refactored-out TokenStreamOffsetStrategy.  So this removes many but not all occurrences of TokenStream within the internal API. "
        },
        {
            "id": "comment-15616239",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T19:03:18+0000",
            "content": "GitHub user Timothy055 opened a pull request:\n\n    https://github.com/apache/lucene-solr/pull/105\n\n    LUCENE-7526 Improvements to UnifiedHighlighter OffsetStrategies\n\n    Pull request for LUCENE-7526\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/Timothy055/lucene-solr master\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/lucene-solr/pull/105.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #105\n\n\ncommit 02e932c4a6146363680b88f4947a693c6697c955\nAuthor: Timothy Rodriguez <trodriguez25@bloomberg.net>\nDate:   2016-09-01T19:23:50Z\n\n    Initial fork of PostingsHighlighter for UnifiedHighlighter\n\ncommit 9d88411b3985a98851384d78d681431dba710e89\nAuthor: Timothy Rodriguez <trodriguez25@bloomberg.net>\nDate:   2016-09-01T23:17:06Z\n\n    Initial commit of the UnifiedHighlighter for OSS contribution\n\ncommit e45e39bc4b07ea33e4423b264c2fefb9aa08777a\nAuthor: David Smiley <david.w.smiley@gmail.com>\nDate:   2016-09-02T12:45:49Z\n\n    Fix misc issues; \"ant test\" now works. (#1)\n\ncommit 046a28ef31acf4cea7d255bbbb4b827e6a714e3d\nAuthor: Timothy Rodriguez <trodriguez25@bloomberg.net>\nDate:   2016-09-02T20:58:31Z\n\n    Minor refactoring of the AnalysisFieldHighlighter\n\ncommit ccd1a2280abd4b48cfef8122696e5d9cfd12920f\nAuthor: David Smiley <dsmiley@apache.org>\nDate:   2016-09-03T12:55:20Z\n\n    AbstractFieldHighlighter: order methods more sensibly; renamed a couple.\n\ncommit d4714a04a3e41d5e95bbe942b275c32ed69b9c2e\nAuthor: David Smiley <dsmiley@apache.org>\nDate:   2016-09-04T01:03:29Z\n\n    Improve javadocs and @lucene.external/internal labeling & scope.\n    \"ant precommit\" now passes.\n\ncommit e0659f18a59bf2893076da6d7643ff30f2fa5a52\nAuthor: David Smiley <dsmiley@apache.org>\nDate:   2016-09-04T01:25:55Z\n\n    Analysis: remove dubious filter() method\n\ncommit ccd7ce707bff2c06da89b31853cca9aecea72008\nAuthor: David Smiley <dsmiley@apache.org>\nDate:   2016-09-04T01:44:01Z\n\n    getStrictPhraseHelper -> rm \"Strict\", getHighlightAccuracy -> getFlags, and only call filterExtractedTerms once.\n\ncommit ffc2a22c700b8abcbf87673d5d05bb3659d177c9\nAuthor: David Smiley <david.w.smiley@gmail.com>\nDate:   2016-09-04T15:21:08Z\n\n    UnifiedHighlighter round 2 (#2)\n\n\n\tAbstractFieldHighlighter: order methods more sensibly; renamed a couple.\n\n\n\n\n\tImprove javadocs and @lucene.external/internal labeling & scope.\n    \"ant precommit\" now passes.\n\n\n\n\n\tAnalysis: remove dubious filter() method\n\n\n\n\n\tgetStrictPhraseHelper -> rm \"Strict\", getHighlightAccuracy -> getFlags, and only call filterExtractedTerms once.\n\n\n\ncommit 5f95e05595db462d3ab5bffc68c2c92f70875072\nAuthor: David Smiley <dsmiley@apache.org>\nDate:   2016-09-04T16:12:33Z\n\n    Refactor: FieldOffsetStrategy\n\ncommit 86fb6265fbbdb955ead6d4baf944bf708175715e\nAuthor: David Smiley <dsmiley@apache.org>\nDate:   2016-09-04T16:21:32Z\n\n    stop passing maxPassages into highlightFieldForDoc()\n\ncommit f6fd80544eae9fab953b94b1e9346c0883f956eb\nAuthor: David Smiley <dsmiley@apache.org>\nDate:   2016-09-04T16:12:33Z\n\n    Refactor: FieldOffsetStrategy\n\ncommit b335a673c2ce45904890c1e9af7cbfda2bd27b0f\nAuthor: David Smiley <dsmiley@apache.org>\nDate:   2016-09-04T16:21:32Z\n\n    stop passing maxPassages into highlightFieldForDoc()\n\ncommit 478db9437b92214cbf459f82ba2e3a67c966a150\nAuthor: David Smiley <dsmiley@apache.org>\nDate:   2016-09-04T18:29:44Z\n\n    Rename subclasses of FieldOffsetStrategy.\n\ncommit dbf4280755c11420a5032445cd618fadb7444b61\nAuthor: David Smiley <dsmiley@apache.org>\nDate:   2016-09-04T18:31:34Z\n\n    Re-order and harmonize params on methods called by UH.getFieldHighlighter()\n\ncommit f0340e27e61dcda2e11992f08ec07a72fad6c24c\nAuthor: David Smiley <dsmiley@apache.org>\nDate:   2016-09-04T18:53:51Z\n\n    FieldHighlighter: harmonize field/param order. And don't apply maxNoHighlightPasses twice.\n\ncommit 817f63c1d48fd523c13b9c40a2ae9b8a4047209a\nAuthor: Timothy Rodriguez <trodriguez25@bloomberg.net>\nDate:   2016-09-06T20:43:20Z\n\n    Merge of renaming changes\n\ncommit 0f644a4f53c1ed4d41d562848f6fe51a87442a75\nAuthor: Timothy Rodriguez <trodriguez25@bloomberg.net>\nDate:   2016-09-06T20:54:13Z\n\n    add visibility tests\n\ncommit 9171f49e117085e7d086267bb73836831ff07f8e\nAuthor: Timothy Rodriguez <trodriguez25@bloomberg.net>\nDate:   2016-09-07T14:26:59Z\n\n    ADd additional extensibility test\n\ncommit 7ce488147cb811e15cb6e9125a835171157746f2\nAuthor: Timothy Rodriguez <trodriguez25@bloomberg.net>\nDate:   2016-09-28T22:04:15Z\n\n    Reduce visibility of MultiTermHighlighting to package protected\n\ncommit 2f08465020448592b0e8750db568ade5a9218267\nAuthor: Timothy M. Rodriguez <timothy.rodriguez@gmail.com>\nDate:   2016-10-11T16:44:29Z\n\n    Initial commit that will use memory index to generate offsets enum if the tokenstream is null\n\ncommit 357f3dfb9ace4deef20787af19bc2e5a6b4ff61e\nAuthor: Timothy M. Rodriguez <timothy.rodriguez@gmail.com>\nDate:   2016-10-11T17:34:51Z\n\n    Switched analysis offset strategy to not re-build a tokenstream\n\ncommit 64153d288db5714cdaf3726328557f65c1635610\nAuthor: Timothy M. Rodriguez <timothy.rodriguez@gmail.com>\nDate:   2016-10-11T17:42:12Z\n\n    Switched to using chars ref builder\n\ncommit f137779b1e1b7e57c4b78652614a04507b9e09e1\nAuthor: Timothy Rodriguez <trodriguez25@bloomberg.net>\nDate:   2016-10-21T19:07:48Z\n\n    minor cleanup\n\ncommit ec814f974db6459eba7aa45bf7a4cdae04e6ad6f\nAuthor: Timothy M. Rodriguez <timothy.rodriguez@gmail.com>\nDate:   2016-10-21T21:25:57Z\n\n    switch to use of a CompositePostingsEnum that wraps the postings of wildcard matches\n\ncommit 955a1e79b5189492fae2c95da39343c29e1cdb25\nAuthor: Timothy M. Rodriguez <timothy.rodriguez@gmail.com>\nDate:   2016-10-21T21:32:48Z\n\n    merge conflicts on PhraseHelper rename\n\ncommit d35bd1cffd3c6e2aed67aeccfd03959bf855670a\nAuthor: Timothy Rodriguez <trodriguez25@bloomberg.net>\nDate:   2016-10-21T22:13:51Z\n\n    minor cleanup of how automata are handled in the FieldOffsetStrategy\n\ncommit aa8c92667272e5f397b9566cde05eae7e31bcce5\nAuthor: Timothy Rodriguez <trodriguez25@bloomberg.net>\nDate:   2016-10-24T20:03:08Z\n\n    Removed most use of TokenStreams except in pure Analysis\n\ncommit db42d6a959ca19a77dee3cc7b09496a21c631bd6\nAuthor: Timothy Rodriguez <trodriguez25@bloomberg.net>\nDate:   2016-10-24T20:08:25Z\n\n    simplified some logic to not use continue statements\n\ncommit 657c2a70c4d8ec8ef850e9f66b93cf85ec16f636\nAuthor: Timothy Rodriguez <trodriguez25@bloomberg.net>\nDate:   2016-10-24T22:56:17Z\n\n    split analysis mode into two, moved all offset sources from readers into the FieldOffsetStrategy\n\n "
        },
        {
            "id": "comment-15616294",
            "author": "Timothy M. Rodriguez",
            "date": "2016-10-28T19:24:32+0000",
            "content": "Thanks David Smiley .  I've just submitted the pull request.  You're right this only removes an additional use of token streams.  In the case of the Analysis strategies a TokenStream is still necessary at least initially to analyze the field.  I'm glad I got to work on this during the wonderful Boston Hackday event (https://github.com/flaxsearch/london-hackday-2016).  Thanks David Smiley for some tips while there and Michael Braun for some initial feedback on the pr. "
        },
        {
            "id": "comment-15616546",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T20:57:46+0000",
            "content": "Github user dsmiley commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85602404\n\n    \u2014 Diff: lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/CompositePostingsEnum.java \u2014\n    @@ -0,0 +1,165 @@\n    +/*\n    + * Licensed to the Apache Software Foundation (ASF) under one or more\n    + * contributor license agreements.  See the NOTICE file distributed with\n    + * this work for additional information regarding copyright ownership.\n    + * The ASF licenses this file to You under the Apache License, Version 2.0\n    + * (the \"License\"); you may not use this file except in compliance with\n    + * the License.  You may obtain a copy of the License at\n    + *\n    + *     http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +package org.apache.lucene.search.uhighlight;\n    +\n    +import java.io.IOException;\n    +import java.util.List;\n    +\n    +import org.apache.lucene.index.PostingsEnum;\n    +import org.apache.lucene.util.BytesRef;\n    +import org.apache.lucene.util.PriorityQueue;\n    +\n    +\n    +final class CompositePostingsEnum extends PostingsEnum {\n    \u2014 End diff \u2013\n\n    A comment would be helpful to explain the scope/purpose. "
        },
        {
            "id": "comment-15616547",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T20:57:46+0000",
            "content": "Github user dsmiley commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85602210\n\n    \u2014 Diff: lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/AnalysisOffsetStrategy.java \u2014\n    @@ -17,174 +17,28 @@\n     package org.apache.lucene.search.uhighlight;\n\n     import java.io.IOException;\n    -import java.util.ArrayList;\n    -import java.util.Arrays;\n    -import java.util.Collections;\n    -import java.util.List;\n\n     import org.apache.lucene.analysis.Analyzer;\n    -import org.apache.lucene.analysis.FilteringTokenFilter;\n     import org.apache.lucene.analysis.TokenStream;\n    -import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;\n    -import org.apache.lucene.index.IndexReader;\n    -import org.apache.lucene.index.LeafReader;\n    -import org.apache.lucene.index.Terms;\n    -import org.apache.lucene.index.memory.MemoryIndex;\n    -import org.apache.lucene.search.spans.SpanQuery;\n     import org.apache.lucene.util.BytesRef;\n    -import org.apache.lucene.util.automaton.Automata;\n     import org.apache.lucene.util.automaton.CharacterRunAutomaton;\n\n    +public abstract class AnalysisOffsetStrategy extends FieldOffsetStrategy {\n    \u2014 End diff \u2013\n\n    All public classes need a javadoc comment.  Remember lucene.internal for this one. "
        },
        {
            "id": "comment-15616548",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T20:57:46+0000",
            "content": "Github user dsmiley commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85603812\n\n    \u2014 Diff: lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/CompositePostingsEnum.java \u2014\n    @@ -0,0 +1,165 @@\n    +/*\n    + * Licensed to the Apache Software Foundation (ASF) under one or more\n    + * contributor license agreements.  See the NOTICE file distributed with\n    + * this work for additional information regarding copyright ownership.\n    + * The ASF licenses this file to You under the Apache License, Version 2.0\n    + * (the \"License\"); you may not use this file except in compliance with\n    + * the License.  You may obtain a copy of the License at\n    + *\n    + *     http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +package org.apache.lucene.search.uhighlight;\n    +\n    +import java.io.IOException;\n    +import java.util.List;\n    +\n    +import org.apache.lucene.index.PostingsEnum;\n    +import org.apache.lucene.util.BytesRef;\n    +import org.apache.lucene.util.PriorityQueue;\n    +\n    +\n    +final class CompositePostingsEnum extends PostingsEnum {\n    +\n    +  private static final int NO_MORE_POSITIONS = -2;\n    +  private final BytesRef term;\n    +  private final int freq;\n    +  private final PriorityQueue<BoundsCheckingPostingsEnum> queue;\n    +\n    +\n    +  /**\n    +   * This class is used to ensure we don't over iterate the underlying\n    +   * postings enum by keeping track of the position relative to the\n    +   * frequency.\n    +   * Ideally this would've been an implementation of a PostingsEnum\n    +   * but it would have to delegate most methods and it seemed easier\n    +   * to just wrap the tweaked method.\n    +   */\n    +  private static final class BoundsCheckingPostingsEnum {\n    +\n    +\n    +    private final PostingsEnum postingsEnum;\n    +    private final int freq;\n    +    private int position;\n    +    private int nextPosition;\n    +    private int positionInc = 1;\n    +\n    +    private int startOffset;\n    +    private int endOffset;\n    +\n    +    BoundsCheckingPostingsEnum(PostingsEnum postingsEnum) throws IOException \n{\n    +      this.postingsEnum = postingsEnum;\n    +      this.freq = postingsEnum.freq();\n    +      nextPosition = postingsEnum.nextPosition();\n    +      position = nextPosition;\n    +      startOffset = postingsEnum.startOffset();\n    +      endOffset = postingsEnum.endOffset();\n    +    }\n    +\n    +    private boolean hasMorePositions() throws IOException \n{\n    +      return positionInc < freq;\n    +    }\n    +\n    +    /**\n    +     * Returns the next position of the underlying postings enum unless\n    +     * it cannot iterate further and returns NO_MORE_POSITIONS;\n    +     * @return\n    +     * @throws IOException\n    +     */\n    +    private int nextPosition() throws IOException {\n    +      position = nextPosition;\n    +      startOffset = postingsEnum.startOffset();\n    +      endOffset = postingsEnum.endOffset();\n    +      if (hasMorePositions()) \n{\n    +        positionInc++;\n    +        nextPosition = postingsEnum.nextPosition();\n    +      }\n else \n{\n    +        nextPosition = NO_MORE_POSITIONS;\n    +      }\n    +      return position;\n    +    }\n    +\n    +  }\n    +\n    +  CompositePostingsEnum(BytesRef term, List<PostingsEnum> postingsEnums) throws IOException {\n    +    this.term = term;\n    +    queue = new PriorityQueue<BoundsCheckingPostingsEnum>(postingsEnums.size()) {\n    +      @Override\n    +      protected boolean lessThan(BoundsCheckingPostingsEnum a, BoundsCheckingPostingsEnum b) {\n    +        return a.position < b.position;\n    \u2014 End diff \u2013\n\n    In the event the positions are equal (e.g. two terms a the same position in which the wildcard matches both), we might want to fall-back on startOffset then endOffset?  Or maybe simply ignore position altogether and just do offsets, so then you needn't even track the position? "
        },
        {
            "id": "comment-15616549",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T20:57:46+0000",
            "content": "Github user dsmiley commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85607262\n\n    \u2014 Diff: lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/TokenStreamOffsetStrategy.java \u2014\n    @@ -0,0 +1,60 @@\n    +/*\n    + * Licensed to the Apache Software Foundation (ASF) under one or more\n    + * contributor license agreements.  See the NOTICE file distributed with\n    + * this work for additional information regarding copyright ownership.\n    + * The ASF licenses this file to You under the Apache License, Version 2.0\n    + * (the \"License\"); you may not use this file except in compliance with\n    + * the License.  You may obtain a copy of the License at\n    + *\n    + *     http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +package org.apache.lucene.search.uhighlight;\n    +\n    +import java.io.Closeable;\n    +import java.io.IOException;\n    +import java.util.Collections;\n    +import java.util.List;\n    +\n    +import org.apache.lucene.analysis.Analyzer;\n    +import org.apache.lucene.analysis.TokenStream;\n    +import org.apache.lucene.index.IndexReader;\n    +import org.apache.lucene.index.PostingsEnum;\n    +import org.apache.lucene.util.BytesRef;\n    +import org.apache.lucene.util.automaton.CharacterRunAutomaton;\n    +\n    +public class TokenStreamOffsetStrategy extends AnalysisOffsetStrategy {\n    +\n    +  private static final BytesRef[] ZERO_LEN_BYTES_REF_ARRAY = new BytesRef[0];\n    +\n    +  public TokenStreamOffsetStrategy(String field, BytesRef[] terms, PhraseHelper phraseHelper, CharacterRunAutomaton[] automata, Analyzer indexAnalyzer) \n{\n    +    super(field, terms, phraseHelper, automata, indexAnalyzer);\n    +    this.automata = convertTermsToAutomata(terms, automata);\n    +    this.terms = ZERO_LEN_BYTES_REF_ARRAY;\n    +  }\n    +\n    +  @Override\n    +  public List<OffsetsEnum> getOffsetsEnums(IndexReader reader, int docId, String content) throws IOException {\n    +    TokenStream tokenStream = tokenStream(content);\n    +    PostingsEnum mtqPostingsEnum = MultiTermHighlighting.getDocsEnum(tokenStream, automata);\n    \u2014 End diff \u2013\n\n    I think there's a case to be made in moving ` MultiTermHighlighting.getDocsEnum` into this class, to thus keep the TokenStream aspect more isolated? "
        },
        {
            "id": "comment-15616550",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T20:57:46+0000",
            "content": "Github user dsmiley commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85602867\n\n    \u2014 Diff: lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/CompositePostingsEnum.java \u2014\n    @@ -0,0 +1,165 @@\n    +/*\n    + * Licensed to the Apache Software Foundation (ASF) under one or more\n    + * contributor license agreements.  See the NOTICE file distributed with\n    + * this work for additional information regarding copyright ownership.\n    + * The ASF licenses this file to You under the Apache License, Version 2.0\n    + * (the \"License\"); you may not use this file except in compliance with\n    + * the License.  You may obtain a copy of the License at\n    + *\n    + *     http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +package org.apache.lucene.search.uhighlight;\n    +\n    +import java.io.IOException;\n    +import java.util.List;\n    +\n    +import org.apache.lucene.index.PostingsEnum;\n    +import org.apache.lucene.util.BytesRef;\n    +import org.apache.lucene.util.PriorityQueue;\n    +\n    +\n    +final class CompositePostingsEnum extends PostingsEnum {\n    +\n    +  private static final int NO_MORE_POSITIONS = -2;\n    +  private final BytesRef term;\n    +  private final int freq;\n    +  private final PriorityQueue<BoundsCheckingPostingsEnum> queue;\n    +\n    +\n    +  /**\n    +   * This class is used to ensure we don't over iterate the underlying\n    +   * postings enum by keeping track of the position relative to the\n    +   * frequency.\n    +   * Ideally this would've been an implementation of a PostingsEnum\n    +   * but it would have to delegate most methods and it seemed easier\n    +   * to just wrap the tweaked method.\n    +   */\n    +  private static final class BoundsCheckingPostingsEnum {\n    +\n    +\n    +    private final PostingsEnum postingsEnum;\n    +    private final int freq;\n    \u2014 End diff \u2013\n\n    Instead of holding `freq` and `nextPosition`, why not just `remainingPositions`? "
        },
        {
            "id": "comment-15616551",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T20:57:46+0000",
            "content": "Github user dsmiley commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85608645\n\n    \u2014 Diff: lucene/highlighter/src/test/org/apache/lucene/search/uhighlight/visibility/TestUnifiedHighlighterExtensibility.java \u2014\n    @@ -79,7 +90,7 @@ public void testFieldOffsetStrategyExtensibility() {\n       @Test\n       public void testUnifiedHighlighterExtensibility() {\n         final int maxLength = 1000;\n\n\tUnifiedHighlighter uh = new UnifiedHighlighter(null, new MockAnalyzer(random())){\n    +    UnifiedHighlighter uh = new UnifiedHighlighter(null, new MockAnalyzer(new Random())){\n\t\n\t\t\n\t\t\n\t\t\tEnd diff \u2013\n\t\t\n\t\t\n\t\n\t\n\n\n\n    ? why not `random()` ?  This will likely fail precommit. "
        },
        {
            "id": "comment-15616552",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T20:57:46+0000",
            "content": "Github user dsmiley commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85606885\n\n    \u2014 Diff: lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/Passage.java \u2014\n    @@ -40,7 +40,7 @@\n         BytesRef matchTerms[] = new BytesRef[8];\n         int numMatches = 0;\n\n\n\tvoid addMatch(int startOffset, int endOffset, BytesRef term) {\n    +    public void addMatch(int startOffset, int endOffset, BytesRef term) {\n\t\n\t\t\n\t\t\n\t\t\tEnd diff \u2013\n\t\t\n\t\t\n\t\n\t\n\n\n\n    Excellent; now it's possible for someone to override `FieldHighlighter.highlightOffsetsEnums()` to make Passages.  But do add @lucene.experimental. "
        },
        {
            "id": "comment-15616553",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T20:57:46+0000",
            "content": "Github user dsmiley commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85607859\n\n    \u2014 Diff: lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/UnifiedHighlighter.java \u2014\n    @@ -116,6 +116,8 @@\n\n       private boolean defaultHighlightPhrasesStrictly = true; // AKA \"accuracy\" or \"query debugging\"\n\n    +  private boolean defaultPassageRelevancyOverSpeed = true; //Prefer using a memory index\n    \u2014 End diff \u2013\n\n    Suggest the comment be: For analysis, prefer MemoryIndex approach "
        },
        {
            "id": "comment-15616554",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T20:57:46+0000",
            "content": "Github user dsmiley commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85603003\n\n    \u2014 Diff: lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/CompositePostingsEnum.java \u2014\n    @@ -0,0 +1,165 @@\n    +/*\n    + * Licensed to the Apache Software Foundation (ASF) under one or more\n    + * contributor license agreements.  See the NOTICE file distributed with\n    + * this work for additional information regarding copyright ownership.\n    + * The ASF licenses this file to You under the Apache License, Version 2.0\n    + * (the \"License\"); you may not use this file except in compliance with\n    + * the License.  You may obtain a copy of the License at\n    + *\n    + *     http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +package org.apache.lucene.search.uhighlight;\n    +\n    +import java.io.IOException;\n    +import java.util.List;\n    +\n    +import org.apache.lucene.index.PostingsEnum;\n    +import org.apache.lucene.util.BytesRef;\n    +import org.apache.lucene.util.PriorityQueue;\n    +\n    +\n    +final class CompositePostingsEnum extends PostingsEnum {\n    +\n    +  private static final int NO_MORE_POSITIONS = -2;\n    +  private final BytesRef term;\n    +  private final int freq;\n    +  private final PriorityQueue<BoundsCheckingPostingsEnum> queue;\n    +\n    +\n    +  /**\n    +   * This class is used to ensure we don't over iterate the underlying\n    +   * postings enum by keeping track of the position relative to the\n    +   * frequency.\n    +   * Ideally this would've been an implementation of a PostingsEnum\n    +   * but it would have to delegate most methods and it seemed easier\n    +   * to just wrap the tweaked method.\n    +   */\n    +  private static final class BoundsCheckingPostingsEnum {\n    +\n    +\n    +    private final PostingsEnum postingsEnum;\n    +    private final int freq;\n    +    private int position;\n    +    private int nextPosition;\n    +    private int positionInc = 1;\n    +\n    +    private int startOffset;\n    \u2014 End diff \u2013\n\n    Don't need these.  They can be fetched on-demand from the head of the queue, easily & cheaply enough. "
        },
        {
            "id": "comment-15616555",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T20:57:46+0000",
            "content": "Github user dsmiley commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85606333\n\n    \u2014 Diff: lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldOffsetStrategy.java \u2014\n    @@ -65,58 +65,88 @@ public String getField() {\n        */\n       public abstract List<OffsetsEnum> getOffsetsEnums(IndexReader reader, int docId, String content) throws IOException;\n\n\n\tprotected List<OffsetsEnum> createOffsetsEnums(LeafReader leafReader, int doc, TokenStream tokenStream) throws IOException {\n\tList<OffsetsEnum> offsetsEnums = createOffsetsEnumsFromReader(leafReader, doc);\n\tif (automata.length > 0) {\n\toffsetsEnums.add(createOffsetsEnumFromTokenStream(doc, tokenStream));\n    +  protected List<OffsetsEnum> createOffsetsEnumsFromReader(LeafReader leafReader, int doc) throws IOException {\n    +    final Terms termsIndex = leafReader.terms(field);\n    +    if (termsIndex == null) \n{\n    +      return Collections.emptyList();\n         }\n\treturn offsetsEnums;\n\t}\n\n\n\n\n\tprotected List<OffsetsEnum> createOffsetsEnumsFromReader(LeafReader atomicReader, int doc) throws IOException {\n         // For strict positions, get a Map of term to Spans:\n         //    note: ScriptPhraseHelper.NONE does the right thing for these method calls\n         final Map<BytesRef, Spans> strictPhrasesTermToSpans =\n\tstrictPhrases.getTermToSpans(atomicReader, doc);\n    +        phraseHelper.getTermToSpans(leafReader, doc);\n         // Usually simply wraps terms in a List; but if willRewrite() then can be expanded\n         final List<BytesRef> sourceTerms =\n\tstrictPhrases.expandTermsIfRewrite(terms, strictPhrasesTermToSpans);\n    +        phraseHelper.expandTermsIfRewrite(terms, strictPhrasesTermToSpans);\n\n\n\n\n\tfinal List<OffsetsEnum> offsetsEnums = new ArrayList<>(sourceTerms.size() + 1);\n    +    final List<OffsetsEnum> offsetsEnums = new ArrayList<>(sourceTerms.size() + automata.length);\n\n\n\n\n\tTerms termsIndex = atomicReader == null || sourceTerms.isEmpty() ? null : atomicReader.terms(field);\n\tif (termsIndex != null) {\n    +    // Handle sourceTerms:\n    +    if (!sourceTerms.isEmpty()) {\n           TermsEnum termsEnum = termsIndex.iterator();//does not return null\n           for (BytesRef term : sourceTerms) {\n\tif (!termsEnum.seekExact(term)) \n{\n    -          continue; // term not found\n    -        }\n\tPostingsEnum postingsEnum = termsEnum.postings(null, PostingsEnum.OFFSETS);\n\tif (postingsEnum == null) \n{\n    -          // no offsets or positions available\n    -          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n    -        }\n\tif (doc != postingsEnum.advance(doc)) { // now it's positioned, although may be exhausted\n\tcontinue;\n    +        if (termsEnum.seekExact(term)) {\n    +          PostingsEnum postingsEnum = termsEnum.postings(null, PostingsEnum.OFFSETS);\n    +\n    +          if (postingsEnum == null) \n{\n    +            // no offsets or positions available\n    +            throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n    +          }\n    +\n    +          if (doc == postingsEnum.advance(doc)) \nUnknown macro: { // now it's positioned, although may be exhausted    +            postingsEnum = phraseHelper.filterPostings(term, postingsEnum, strictPhrasesTermToSpans.get(term));    +            if (postingsEnum != null) {\n    +              offsetsEnums.add(new OffsetsEnum(term, postingsEnum));\n    +            }    +          } \n             }\n\tpostingsEnum = strictPhrases.filterPostings(term, postingsEnum, strictPhrasesTermToSpans.get(term));\n\tif (postingsEnum == null) \n{\n    -          continue;// completely filtered out\n    +      }\n    +    }\n    +\n    +    // Handle automata\n    +    if (automata.length > 0) \n{\n    +      offsetsEnums.addAll(createAutomataOffsetsFromTerms(termsIndex, doc));\n    +    }\n    +\n    +    return offsetsEnums;\n    +  }\n    +\n    +  protected List<OffsetsEnum> createAutomataOffsetsFromTerms(Terms termsIndex, int doc) throws IOException {\n    +    Map<CharacterRunAutomaton, List<PostingsEnum>> automataPostings = new IdentityHashMap<>(automata.length);\n\t\n\t\t\n\t\t\n\t\t\tEnd diff \u2013\n\t\t\n\t\t\n\t\n\t\n\n\n\n    I suggest a parallel array to automata, so that later you can avoid a map lookup on each matching term.  Also, I suggest lazy-initializing the array later... perhaps some wildcards in a disjunction might never match. "
        },
        {
            "id": "comment-15616556",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T20:57:46+0000",
            "content": "Github user dsmiley commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85604667\n\n    \u2014 Diff: lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/CompositePostingsEnum.java \u2014\n    @@ -0,0 +1,165 @@\n    +/*\n    + * Licensed to the Apache Software Foundation (ASF) under one or more\n    + * contributor license agreements.  See the NOTICE file distributed with\n    + * this work for additional information regarding copyright ownership.\n    + * The ASF licenses this file to You under the Apache License, Version 2.0\n    + * (the \"License\"); you may not use this file except in compliance with\n    + * the License.  You may obtain a copy of the License at\n    + *\n    + *     http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +package org.apache.lucene.search.uhighlight;\n    +\n    +import java.io.IOException;\n    +import java.util.List;\n    +\n    +import org.apache.lucene.index.PostingsEnum;\n    +import org.apache.lucene.util.BytesRef;\n    +import org.apache.lucene.util.PriorityQueue;\n    +\n    +\n    +final class CompositePostingsEnum extends PostingsEnum {\n    +\n    +  private static final int NO_MORE_POSITIONS = -2;\n    +  private final BytesRef term;\n    +  private final int freq;\n    +  private final PriorityQueue<BoundsCheckingPostingsEnum> queue;\n    +\n    +\n    +  /**\n    +   * This class is used to ensure we don't over iterate the underlying\n    +   * postings enum by keeping track of the position relative to the\n    +   * frequency.\n    +   * Ideally this would've been an implementation of a PostingsEnum\n    +   * but it would have to delegate most methods and it seemed easier\n    +   * to just wrap the tweaked method.\n    +   */\n    +  private static final class BoundsCheckingPostingsEnum {\n    +\n    +\n    +    private final PostingsEnum postingsEnum;\n    +    private final int freq;\n    +    private int position;\n    +    private int nextPosition;\n    +    private int positionInc = 1;\n    +\n    +    private int startOffset;\n    +    private int endOffset;\n    +\n    +    BoundsCheckingPostingsEnum(PostingsEnum postingsEnum) throws IOException \n{\n    +      this.postingsEnum = postingsEnum;\n    +      this.freq = postingsEnum.freq();\n    +      nextPosition = postingsEnum.nextPosition();\n    +      position = nextPosition;\n    +      startOffset = postingsEnum.startOffset();\n    +      endOffset = postingsEnum.endOffset();\n    +    }\n    +\n    +    private boolean hasMorePositions() throws IOException \n{\n    +      return positionInc < freq;\n    +    }\n    +\n    +    /**\n    +     * Returns the next position of the underlying postings enum unless\n    +     * it cannot iterate further and returns NO_MORE_POSITIONS;\n    +     * @return\n    +     * @throws IOException\n    +     */\n    +    private int nextPosition() throws IOException {\n    +      position = nextPosition;\n    +      startOffset = postingsEnum.startOffset();\n    +      endOffset = postingsEnum.endOffset();\n    +      if (hasMorePositions()) \n{\n    +        positionInc++;\n    +        nextPosition = postingsEnum.nextPosition();\n    +      }\n else \n{\n    +        nextPosition = NO_MORE_POSITIONS;\n    +      }\n    +      return position;\n    +    }\n    +\n    +  }\n    +\n    +  CompositePostingsEnum(BytesRef term, List<PostingsEnum> postingsEnums) throws IOException {\n    +    this.term = term;\n    +    queue = new PriorityQueue<BoundsCheckingPostingsEnum>(postingsEnums.size()) {\n    +      @Override\n    +      protected boolean lessThan(BoundsCheckingPostingsEnum a, BoundsCheckingPostingsEnum b) \n{\n    +        return a.position < b.position;\n    +      }\n    +    };\n    +\n    +    int freqAdd = 0;\n    +    for (PostingsEnum postingsEnum : postingsEnums) \n{\n    +      queue.add(new BoundsCheckingPostingsEnum(postingsEnum));\n    +      freqAdd += postingsEnum.freq();\n    +    }\n    +    freq = freqAdd;\n    +  }\n    +\n    +  @Override\n    +  public int freq() throws IOException \n{\n    +    return freq;\n    +  }\n    +\n    +  @Override\n    +  public int nextPosition() throws IOException {\n    +    int position = NO_MORE_POSITIONS;\n    +    while (queue.size() >= 1) {\n    +      queue.top().nextPosition();\n    +      queue.updateTop(); //the new position may be behind another postingsEnum in the queue\n    +      position = queue.top().position;\n    +\n    +      if (position == NO_MORE_POSITIONS) \n{\n    +        queue.pop(); //this postingsEnum is consumed, let's get rid of it\n    +      }\n else \n{\n    +        break; //we got a new position\n    +      }\n    +\n    +    }\n    +    return position;\n    +  }\n    +\n    +  @Override\n    +  public int startOffset() throws IOException \n{\n    +    return queue.top().startOffset;\n    +  }\n    +\n    +  @Override\n    +  public int endOffset() throws IOException \n{\n    +    return queue.top().endOffset;\n    +  }\n    +\n    +  @Override\n    +  public BytesRef getPayload() throws IOException \n{\n    +    //The UnifiedHighlighter depends on the payload for a wildcard\n    +    //being the term representing it\n    +    return term;\n    +  }\n    +\n    +  @Override\n    +  public int docID() {\n    +    return queue.top().postingsEnum.docID();\n    \u2014 End diff \u2013\n\n    I think this one should be held in a field of CompositePostingsEnum initialized in constructor?  Not a big deal though, since I don't think the highlighter is going to call this, let alone sometime after positions are exhausted (which would trigger an NPE in this case). "
        },
        {
            "id": "comment-15616577",
            "author": "David Smiley",
            "date": "2016-10-28T21:04:54+0000",
            "content": "It seems the ASF's GitHub -> JIRA comment integration doesn't incorporate overall review comments (i.e. comments not attached to a line number).  Please look in GH for that.\n\nAfter a round of edits or two, I'll run the benchmarks to see how this fairs.  I expect it'll be a little slower for the analysis case when no phrases are present due to the change of default to prefer MemoryIndex for the benefit of better passage relevancy (now a toggle'able option).  Hopefully very little.  If the difference seems imperceptible then there's a case for dropping the TokenStreamOffsetStrategy along with some quirks related to it, which would be nice in terms of code maintenance. "
        },
        {
            "id": "comment-15616611",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T21:16:10+0000",
            "content": "Github user Timothy055 commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85611673\n\n    \u2014 Diff: lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/CompositePostingsEnum.java \u2014\n    @@ -0,0 +1,165 @@\n    +/*\n    + * Licensed to the Apache Software Foundation (ASF) under one or more\n    + * contributor license agreements.  See the NOTICE file distributed with\n    + * this work for additional information regarding copyright ownership.\n    + * The ASF licenses this file to You under the Apache License, Version 2.0\n    + * (the \"License\"); you may not use this file except in compliance with\n    + * the License.  You may obtain a copy of the License at\n    + *\n    + *     http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +package org.apache.lucene.search.uhighlight;\n    +\n    +import java.io.IOException;\n    +import java.util.List;\n    +\n    +import org.apache.lucene.index.PostingsEnum;\n    +import org.apache.lucene.util.BytesRef;\n    +import org.apache.lucene.util.PriorityQueue;\n    +\n    +\n    +final class CompositePostingsEnum extends PostingsEnum {\n    +\n    +  private static final int NO_MORE_POSITIONS = -2;\n    +  private final BytesRef term;\n    +  private final int freq;\n    +  private final PriorityQueue<BoundsCheckingPostingsEnum> queue;\n    +\n    +\n    +  /**\n    +   * This class is used to ensure we don't over iterate the underlying\n    +   * postings enum by keeping track of the position relative to the\n    +   * frequency.\n    +   * Ideally this would've been an implementation of a PostingsEnum\n    +   * but it would have to delegate most methods and it seemed easier\n    +   * to just wrap the tweaked method.\n    +   */\n    +  private static final class BoundsCheckingPostingsEnum {\n    +\n    +\n    +    private final PostingsEnum postingsEnum;\n    +    private final int freq;\n    +    private int position;\n    +    private int nextPosition;\n    +    private int positionInc = 1;\n    +\n    +    private int startOffset;\n    \u2014 End diff \u2013\n\n    I thought the same, but unfortunately there's no way to check the current position of the underlying PostingsEnum before calling nextPosition.  Since we call nextPosition the PostingsEnum has moved on before the highlighter calls startOffset or endOffset. "
        },
        {
            "id": "comment-15616618",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T21:18:16+0000",
            "content": "Github user Timothy055 commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85611978\n\n    \u2014 Diff: lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/CompositePostingsEnum.java \u2014\n    @@ -0,0 +1,165 @@\n    +/*\n    + * Licensed to the Apache Software Foundation (ASF) under one or more\n    + * contributor license agreements.  See the NOTICE file distributed with\n    + * this work for additional information regarding copyright ownership.\n    + * The ASF licenses this file to You under the Apache License, Version 2.0\n    + * (the \"License\"); you may not use this file except in compliance with\n    + * the License.  You may obtain a copy of the License at\n    + *\n    + *     http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +package org.apache.lucene.search.uhighlight;\n    +\n    +import java.io.IOException;\n    +import java.util.List;\n    +\n    +import org.apache.lucene.index.PostingsEnum;\n    +import org.apache.lucene.util.BytesRef;\n    +import org.apache.lucene.util.PriorityQueue;\n    +\n    +\n    +final class CompositePostingsEnum extends PostingsEnum {\n    +\n    +  private static final int NO_MORE_POSITIONS = -2;\n    +  private final BytesRef term;\n    +  private final int freq;\n    +  private final PriorityQueue<BoundsCheckingPostingsEnum> queue;\n    +\n    +\n    +  /**\n    +   * This class is used to ensure we don't over iterate the underlying\n    +   * postings enum by keeping track of the position relative to the\n    +   * frequency.\n    +   * Ideally this would've been an implementation of a PostingsEnum\n    +   * but it would have to delegate most methods and it seemed easier\n    +   * to just wrap the tweaked method.\n    +   */\n    +  private static final class BoundsCheckingPostingsEnum {\n    +\n    +\n    +    private final PostingsEnum postingsEnum;\n    +    private final int freq;\n    +    private int position;\n    +    private int nextPosition;\n    +    private int positionInc = 1;\n    +\n    +    private int startOffset;\n    +    private int endOffset;\n    +\n    +    BoundsCheckingPostingsEnum(PostingsEnum postingsEnum) throws IOException \n{\n    +      this.postingsEnum = postingsEnum;\n    +      this.freq = postingsEnum.freq();\n    +      nextPosition = postingsEnum.nextPosition();\n    +      position = nextPosition;\n    +      startOffset = postingsEnum.startOffset();\n    +      endOffset = postingsEnum.endOffset();\n    +    }\n    +\n    +    private boolean hasMorePositions() throws IOException \n{\n    +      return positionInc < freq;\n    +    }\n    +\n    +    /**\n    +     * Returns the next position of the underlying postings enum unless\n    +     * it cannot iterate further and returns NO_MORE_POSITIONS;\n    +     * @return\n    +     * @throws IOException\n    +     */\n    +    private int nextPosition() throws IOException {\n    +      position = nextPosition;\n    +      startOffset = postingsEnum.startOffset();\n    +      endOffset = postingsEnum.endOffset();\n    +      if (hasMorePositions()) \n{\n    +        positionInc++;\n    +        nextPosition = postingsEnum.nextPosition();\n    +      }\n else \n{\n    +        nextPosition = NO_MORE_POSITIONS;\n    +      }\n    +      return position;\n    +    }\n    +\n    +  }\n    +\n    +  CompositePostingsEnum(BytesRef term, List<PostingsEnum> postingsEnums) throws IOException {\n    +    this.term = term;\n    +    queue = new PriorityQueue<BoundsCheckingPostingsEnum>(postingsEnums.size()) {\n    +      @Override\n    +      protected boolean lessThan(BoundsCheckingPostingsEnum a, BoundsCheckingPostingsEnum b) \n{\n    +        return a.position < b.position;\n    +      }\n    +    };\n    +\n    +    int freqAdd = 0;\n    +    for (PostingsEnum postingsEnum : postingsEnums) \n{\n    +      queue.add(new BoundsCheckingPostingsEnum(postingsEnum));\n    +      freqAdd += postingsEnum.freq();\n    +    }\n    +    freq = freqAdd;\n    +  }\n    +\n    +  @Override\n    +  public int freq() throws IOException \n{\n    +    return freq;\n    +  }\n    +\n    +  @Override\n    +  public int nextPosition() throws IOException {\n    +    int position = NO_MORE_POSITIONS;\n    +    while (queue.size() >= 1) {\n    +      queue.top().nextPosition();\n    +      queue.updateTop(); //the new position may be behind another postingsEnum in the queue\n    +      position = queue.top().position;\n    +\n    +      if (position == NO_MORE_POSITIONS) \n{\n    +        queue.pop(); //this postingsEnum is consumed, let's get rid of it\n    +      }\n else \n{\n    +        break; //we got a new position\n    +      }\n    +\n    +    }\n    +    return position;\n    +  }\n    +\n    +  @Override\n    +  public int startOffset() throws IOException \n{\n    +    return queue.top().startOffset;\n    +  }\n    +\n    +  @Override\n    +  public int endOffset() throws IOException \n{\n    +    return queue.top().endOffset;\n    +  }\n    +\n    +  @Override\n    +  public BytesRef getPayload() throws IOException \n{\n    +    //The UnifiedHighlighter depends on the payload for a wildcard\n    +    //being the term representing it\n    +    return term;\n    +  }\n    +\n    +  @Override\n    +  public int docID() {\n    +    return queue.top().postingsEnum.docID();\n    \u2014 End diff \u2013\n\n    K "
        },
        {
            "id": "comment-15616620",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T21:18:32+0000",
            "content": "Github user Timothy055 commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85612011\n\n    \u2014 Diff: lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/AnalysisOffsetStrategy.java \u2014\n    @@ -17,174 +17,28 @@\n     package org.apache.lucene.search.uhighlight;\n\n     import java.io.IOException;\n    -import java.util.ArrayList;\n    -import java.util.Arrays;\n    -import java.util.Collections;\n    -import java.util.List;\n\n     import org.apache.lucene.analysis.Analyzer;\n    -import org.apache.lucene.analysis.FilteringTokenFilter;\n     import org.apache.lucene.analysis.TokenStream;\n    -import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;\n    -import org.apache.lucene.index.IndexReader;\n    -import org.apache.lucene.index.LeafReader;\n    -import org.apache.lucene.index.Terms;\n    -import org.apache.lucene.index.memory.MemoryIndex;\n    -import org.apache.lucene.search.spans.SpanQuery;\n     import org.apache.lucene.util.BytesRef;\n    -import org.apache.lucene.util.automaton.Automata;\n     import org.apache.lucene.util.automaton.CharacterRunAutomaton;\n\n    +public abstract class AnalysisOffsetStrategy extends FieldOffsetStrategy {\n    \u2014 End diff \u2013\n\n    Thanks "
        },
        {
            "id": "comment-15616638",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T21:26:34+0000",
            "content": "Github user Timothy055 commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85613130\n\n    \u2014 Diff: lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldOffsetStrategy.java \u2014\n    @@ -65,58 +65,88 @@ public String getField() {\n        */\n       public abstract List<OffsetsEnum> getOffsetsEnums(IndexReader reader, int docId, String content) throws IOException;\n\n\n\tprotected List<OffsetsEnum> createOffsetsEnums(LeafReader leafReader, int doc, TokenStream tokenStream) throws IOException {\n\tList<OffsetsEnum> offsetsEnums = createOffsetsEnumsFromReader(leafReader, doc);\n\tif (automata.length > 0) {\n\toffsetsEnums.add(createOffsetsEnumFromTokenStream(doc, tokenStream));\n    +  protected List<OffsetsEnum> createOffsetsEnumsFromReader(LeafReader leafReader, int doc) throws IOException {\n    +    final Terms termsIndex = leafReader.terms(field);\n    +    if (termsIndex == null) \n{\n    +      return Collections.emptyList();\n         }\n\treturn offsetsEnums;\n\t}\n\n\n\n\n\tprotected List<OffsetsEnum> createOffsetsEnumsFromReader(LeafReader atomicReader, int doc) throws IOException {\n         // For strict positions, get a Map of term to Spans:\n         //    note: ScriptPhraseHelper.NONE does the right thing for these method calls\n         final Map<BytesRef, Spans> strictPhrasesTermToSpans =\n\tstrictPhrases.getTermToSpans(atomicReader, doc);\n    +        phraseHelper.getTermToSpans(leafReader, doc);\n         // Usually simply wraps terms in a List; but if willRewrite() then can be expanded\n         final List<BytesRef> sourceTerms =\n\tstrictPhrases.expandTermsIfRewrite(terms, strictPhrasesTermToSpans);\n    +        phraseHelper.expandTermsIfRewrite(terms, strictPhrasesTermToSpans);\n\n\n\n\n\tfinal List<OffsetsEnum> offsetsEnums = new ArrayList<>(sourceTerms.size() + 1);\n    +    final List<OffsetsEnum> offsetsEnums = new ArrayList<>(sourceTerms.size() + automata.length);\n\n\n\n\n\tTerms termsIndex = atomicReader == null || sourceTerms.isEmpty() ? null : atomicReader.terms(field);\n\tif (termsIndex != null) {\n    +    // Handle sourceTerms:\n    +    if (!sourceTerms.isEmpty()) {\n           TermsEnum termsEnum = termsIndex.iterator();//does not return null\n           for (BytesRef term : sourceTerms) {\n\tif (!termsEnum.seekExact(term)) \n{\n    -          continue; // term not found\n    -        }\n\tPostingsEnum postingsEnum = termsEnum.postings(null, PostingsEnum.OFFSETS);\n\tif (postingsEnum == null) \n{\n    -          // no offsets or positions available\n    -          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n    -        }\n\tif (doc != postingsEnum.advance(doc)) { // now it's positioned, although may be exhausted\n\tcontinue;\n    +        if (termsEnum.seekExact(term)) {\n    +          PostingsEnum postingsEnum = termsEnum.postings(null, PostingsEnum.OFFSETS);\n    +\n    +          if (postingsEnum == null) \n{\n    +            // no offsets or positions available\n    +            throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n    +          }\n    +\n    +          if (doc == postingsEnum.advance(doc)) \nUnknown macro: { // now it's positioned, although may be exhausted    +            postingsEnum = phraseHelper.filterPostings(term, postingsEnum, strictPhrasesTermToSpans.get(term));    +            if (postingsEnum != null) {\n    +              offsetsEnums.add(new OffsetsEnum(term, postingsEnum));\n    +            }    +          } \n             }\n\tpostingsEnum = strictPhrases.filterPostings(term, postingsEnum, strictPhrasesTermToSpans.get(term));\n\tif (postingsEnum == null) \n{\n    -          continue;// completely filtered out\n    +      }\n    +    }\n    +\n    +    // Handle automata\n    +    if (automata.length > 0) \n{\n    +      offsetsEnums.addAll(createAutomataOffsetsFromTerms(termsIndex, doc));\n    +    }\n    +\n    +    return offsetsEnums;\n    +  }\n    +\n    +  protected List<OffsetsEnum> createAutomataOffsetsFromTerms(Terms termsIndex, int doc) throws IOException {\n    +    Map<CharacterRunAutomaton, List<PostingsEnum>> automataPostings = new IdentityHashMap<>(automata.length);\n\t\n\t\t\n\t\t\n\t\t\tEnd diff \u2013\n\t\t\n\t\t\n\t\n\t\n\n\n\n    One minor problem with that was that the code would not longer by type-safe because of the lack of generic arrays in java.  I wouldn't be able to do `List<PostingsEnum>[]` = new ArrayList<PostingsEnum>[automata.length];` but could do `List<PostingsEnum>[]` = new ArrayList[automata.length];` with unchecked casts.  Seem worth it? "
        },
        {
            "id": "comment-15616646",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T21:31:39+0000",
            "content": "Github user Timothy055 commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85613814\n\n    \u2014 Diff: lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldOffsetStrategy.java \u2014\n    @@ -65,58 +65,88 @@ public String getField() {\n        */\n       public abstract List<OffsetsEnum> getOffsetsEnums(IndexReader reader, int docId, String content) throws IOException;\n\n\n\tprotected List<OffsetsEnum> createOffsetsEnums(LeafReader leafReader, int doc, TokenStream tokenStream) throws IOException {\n\tList<OffsetsEnum> offsetsEnums = createOffsetsEnumsFromReader(leafReader, doc);\n\tif (automata.length > 0) {\n\toffsetsEnums.add(createOffsetsEnumFromTokenStream(doc, tokenStream));\n    +  protected List<OffsetsEnum> createOffsetsEnumsFromReader(LeafReader leafReader, int doc) throws IOException {\n    +    final Terms termsIndex = leafReader.terms(field);\n    +    if (termsIndex == null) \n{\n    +      return Collections.emptyList();\n         }\n\treturn offsetsEnums;\n\t}\n\n\n\n\n\tprotected List<OffsetsEnum> createOffsetsEnumsFromReader(LeafReader atomicReader, int doc) throws IOException {\n         // For strict positions, get a Map of term to Spans:\n         //    note: ScriptPhraseHelper.NONE does the right thing for these method calls\n         final Map<BytesRef, Spans> strictPhrasesTermToSpans =\n\tstrictPhrases.getTermToSpans(atomicReader, doc);\n    +        phraseHelper.getTermToSpans(leafReader, doc);\n         // Usually simply wraps terms in a List; but if willRewrite() then can be expanded\n         final List<BytesRef> sourceTerms =\n\tstrictPhrases.expandTermsIfRewrite(terms, strictPhrasesTermToSpans);\n    +        phraseHelper.expandTermsIfRewrite(terms, strictPhrasesTermToSpans);\n\n\n\n\n\tfinal List<OffsetsEnum> offsetsEnums = new ArrayList<>(sourceTerms.size() + 1);\n    +    final List<OffsetsEnum> offsetsEnums = new ArrayList<>(sourceTerms.size() + automata.length);\n\n\n\n\n\tTerms termsIndex = atomicReader == null || sourceTerms.isEmpty() ? null : atomicReader.terms(field);\n\tif (termsIndex != null) {\n    +    // Handle sourceTerms:\n    +    if (!sourceTerms.isEmpty()) {\n           TermsEnum termsEnum = termsIndex.iterator();//does not return null\n           for (BytesRef term : sourceTerms) {\n\tif (!termsEnum.seekExact(term)) \n{\n    -          continue; // term not found\n    -        }\n\tPostingsEnum postingsEnum = termsEnum.postings(null, PostingsEnum.OFFSETS);\n\tif (postingsEnum == null) \n{\n    -          // no offsets or positions available\n    -          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n    -        }\n\tif (doc != postingsEnum.advance(doc)) { // now it's positioned, although may be exhausted\n\tcontinue;\n    +        if (termsEnum.seekExact(term)) {\n    +          PostingsEnum postingsEnum = termsEnum.postings(null, PostingsEnum.OFFSETS);\n    +\n    +          if (postingsEnum == null) \n{\n    +            // no offsets or positions available\n    +            throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n    +          }\n    +\n    +          if (doc == postingsEnum.advance(doc)) \nUnknown macro: { // now it's positioned, although may be exhausted    +            postingsEnum = phraseHelper.filterPostings(term, postingsEnum, strictPhrasesTermToSpans.get(term));    +            if (postingsEnum != null) {\n    +              offsetsEnums.add(new OffsetsEnum(term, postingsEnum));\n    +            }    +          } \n             }\n\tpostingsEnum = strictPhrases.filterPostings(term, postingsEnum, strictPhrasesTermToSpans.get(term));\n\tif (postingsEnum == null) \n{\n    -          continue;// completely filtered out\n    +      }\n    +    }\n    +\n    +    // Handle automata\n    +    if (automata.length > 0) \n{\n    +      offsetsEnums.addAll(createAutomataOffsetsFromTerms(termsIndex, doc));\n    +    }\n    +\n    +    return offsetsEnums;\n    +  }\n    +\n    +  protected List<OffsetsEnum> createAutomataOffsetsFromTerms(Terms termsIndex, int doc) throws IOException {\n    +    Map<CharacterRunAutomaton, List<PostingsEnum>> automataPostings = new IdentityHashMap<>(automata.length);\n\t\n\t\t\n\t\t\n\t\t\tEnd diff \u2013\n\t\t\n\t\t\n\t\n\t\n\n\n\n    How about List<List<PostingsEnum>> that should give better locality without lose of type safety? "
        },
        {
            "id": "comment-15616654",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T21:35:30+0000",
            "content": "Github user Timothy055 commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85614277\n\n    \u2014 Diff: lucene/highlighter/src/test/org/apache/lucene/search/uhighlight/visibility/TestUnifiedHighlighterExtensibility.java \u2014\n    @@ -79,7 +90,7 @@ public void testFieldOffsetStrategyExtensibility() {\n       @Test\n       public void testUnifiedHighlighterExtensibility() {\n         final int maxLength = 1000;\n\n\tUnifiedHighlighter uh = new UnifiedHighlighter(null, new MockAnalyzer(random())){\n    +    UnifiedHighlighter uh = new UnifiedHighlighter(null, new MockAnalyzer(new Random())){\n\t\n\t\t\n\t\t\n\t\t\tEnd diff \u2013\n\t\t\n\t\t\n\t\n\t\n\n\n\n    fixed "
        },
        {
            "id": "comment-15616710",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T21:53:49+0000",
            "content": "Github user Timothy055 commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85616651\n\n    \u2014 Diff: lucene/highlighter/src/test/org/apache/lucene/search/uhighlight/visibility/TestUnifiedHighlighterExtensibility.java \u2014\n    @@ -79,7 +90,7 @@ public void testFieldOffsetStrategyExtensibility() {\n       @Test\n       public void testUnifiedHighlighterExtensibility() {\n         final int maxLength = 1000;\n\n\tUnifiedHighlighter uh = new UnifiedHighlighter(null, new MockAnalyzer(random())){\n    +    UnifiedHighlighter uh = new UnifiedHighlighter(null, new MockAnalyzer(new Random())){\n\t\n\t\t\n\t\t\n\t\t\tEnd diff \u2013\n\t\t\n\t\t\n\t\n\t\n\n\n\n    I recall now why I changed it.  I started getting this error and figured it was a change elsewhere: java.lang.IllegalStateException: No context information for thread: Thread[id=1, name=main, state=RUNNABLE, group=main]. Is this thread running under a class com.carrotsearch.randomizedtesting.RandomizedRunner runner context? Add @RunWith(class com.carrotsearch.randomizedtesting.RandomizedRunner.class) to your test class. Make sure your code accesses random contexts within @BeforeClass and @AfterClass boundary (for example, static test class initializers are not permitted to access random contexts). "
        },
        {
            "id": "comment-15616715",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T21:55:43+0000",
            "content": "Github user Timothy055 commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85616894\n\n    \u2014 Diff: lucene/highlighter/src/test/org/apache/lucene/search/uhighlight/visibility/TestUnifiedHighlighterExtensibility.java \u2014\n    @@ -79,7 +90,7 @@ public void testFieldOffsetStrategyExtensibility() {\n       @Test\n       public void testUnifiedHighlighterExtensibility() {\n         final int maxLength = 1000;\n\n\tUnifiedHighlighter uh = new UnifiedHighlighter(null, new MockAnalyzer(random())){\n    +    UnifiedHighlighter uh = new UnifiedHighlighter(null, new MockAnalyzer(new Random())){\n\t\n\t\t\n\t\t\n\t\t\tEnd diff \u2013\n\t\t\n\t\t\n\t\n\t\n\n\n\n    I'm a bit confused.  What I should I change? "
        },
        {
            "id": "comment-15616723",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T21:59:47+0000",
            "content": "Github user Timothy055 commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/105\n\n    I've pushed some more changes now.  Still taking a look at what we might be able to do further with CompositePostingsEnum "
        },
        {
            "id": "comment-15616738",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T22:08:09+0000",
            "content": "Github user Timothy055 commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85618489\n\n    \u2014 Diff: lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/CompositePostingsEnum.java \u2014\n    @@ -0,0 +1,165 @@\n    +/*\n    + * Licensed to the Apache Software Foundation (ASF) under one or more\n    + * contributor license agreements.  See the NOTICE file distributed with\n    + * this work for additional information regarding copyright ownership.\n    + * The ASF licenses this file to You under the Apache License, Version 2.0\n    + * (the \"License\"); you may not use this file except in compliance with\n    + * the License.  You may obtain a copy of the License at\n    + *\n    + *     http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +package org.apache.lucene.search.uhighlight;\n    +\n    +import java.io.IOException;\n    +import java.util.List;\n    +\n    +import org.apache.lucene.index.PostingsEnum;\n    +import org.apache.lucene.util.BytesRef;\n    +import org.apache.lucene.util.PriorityQueue;\n    +\n    +\n    +final class CompositePostingsEnum extends PostingsEnum {\n    +\n    +  private static final int NO_MORE_POSITIONS = -2;\n    +  private final BytesRef term;\n    +  private final int freq;\n    +  private final PriorityQueue<BoundsCheckingPostingsEnum> queue;\n    +\n    +\n    +  /**\n    +   * This class is used to ensure we don't over iterate the underlying\n    +   * postings enum by keeping track of the position relative to the\n    +   * frequency.\n    +   * Ideally this would've been an implementation of a PostingsEnum\n    +   * but it would have to delegate most methods and it seemed easier\n    +   * to just wrap the tweaked method.\n    +   */\n    +  private static final class BoundsCheckingPostingsEnum {\n    +\n    +\n    +    private final PostingsEnum postingsEnum;\n    +    private final int freq;\n    \u2014 End diff \u2013\n\n    Hmm, did you mean to calculate freq dynamically in the method and just use a remainingPositions count as walking over the postings? Instead of re-computing it's cached in the constructor, but if we modified it, the freq count would change as the postings were iterated. "
        },
        {
            "id": "comment-15616778",
            "author": "ASF GitHub Bot",
            "date": "2016-10-28T22:22:29+0000",
            "content": "Github user Timothy055 commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/105#discussion_r85619786\n\n    \u2014 Diff: lucene/highlighter/src/java/org/apache/lucene/search/uhighlight/FieldOffsetStrategy.java \u2014\n    @@ -65,58 +65,88 @@ public String getField() {\n        */\n       public abstract List<OffsetsEnum> getOffsetsEnums(IndexReader reader, int docId, String content) throws IOException;\n\n\n\tprotected List<OffsetsEnum> createOffsetsEnums(LeafReader leafReader, int doc, TokenStream tokenStream) throws IOException {\n\tList<OffsetsEnum> offsetsEnums = createOffsetsEnumsFromReader(leafReader, doc);\n\tif (automata.length > 0) {\n\toffsetsEnums.add(createOffsetsEnumFromTokenStream(doc, tokenStream));\n    +  protected List<OffsetsEnum> createOffsetsEnumsFromReader(LeafReader leafReader, int doc) throws IOException {\n    +    final Terms termsIndex = leafReader.terms(field);\n    +    if (termsIndex == null) \n{\n    +      return Collections.emptyList();\n         }\n\treturn offsetsEnums;\n\t}\n\n\n\n\n\tprotected List<OffsetsEnum> createOffsetsEnumsFromReader(LeafReader atomicReader, int doc) throws IOException {\n         // For strict positions, get a Map of term to Spans:\n         //    note: ScriptPhraseHelper.NONE does the right thing for these method calls\n         final Map<BytesRef, Spans> strictPhrasesTermToSpans =\n\tstrictPhrases.getTermToSpans(atomicReader, doc);\n    +        phraseHelper.getTermToSpans(leafReader, doc);\n         // Usually simply wraps terms in a List; but if willRewrite() then can be expanded\n         final List<BytesRef> sourceTerms =\n\tstrictPhrases.expandTermsIfRewrite(terms, strictPhrasesTermToSpans);\n    +        phraseHelper.expandTermsIfRewrite(terms, strictPhrasesTermToSpans);\n\n\n\n\n\tfinal List<OffsetsEnum> offsetsEnums = new ArrayList<>(sourceTerms.size() + 1);\n    +    final List<OffsetsEnum> offsetsEnums = new ArrayList<>(sourceTerms.size() + automata.length);\n\n\n\n\n\tTerms termsIndex = atomicReader == null || sourceTerms.isEmpty() ? null : atomicReader.terms(field);\n\tif (termsIndex != null) {\n    +    // Handle sourceTerms:\n    +    if (!sourceTerms.isEmpty()) {\n           TermsEnum termsEnum = termsIndex.iterator();//does not return null\n           for (BytesRef term : sourceTerms) {\n\tif (!termsEnum.seekExact(term)) \n{\n    -          continue; // term not found\n    -        }\n\tPostingsEnum postingsEnum = termsEnum.postings(null, PostingsEnum.OFFSETS);\n\tif (postingsEnum == null) \n{\n    -          // no offsets or positions available\n    -          throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n    -        }\n\tif (doc != postingsEnum.advance(doc)) { // now it's positioned, although may be exhausted\n\tcontinue;\n    +        if (termsEnum.seekExact(term)) {\n    +          PostingsEnum postingsEnum = termsEnum.postings(null, PostingsEnum.OFFSETS);\n    +\n    +          if (postingsEnum == null) \n{\n    +            // no offsets or positions available\n    +            throw new IllegalArgumentException(\"field '\" + field + \"' was indexed without offsets, cannot highlight\");\n    +          }\n    +\n    +          if (doc == postingsEnum.advance(doc)) \nUnknown macro: { // now it's positioned, although may be exhausted    +            postingsEnum = phraseHelper.filterPostings(term, postingsEnum, strictPhrasesTermToSpans.get(term));    +            if (postingsEnum != null) {\n    +              offsetsEnums.add(new OffsetsEnum(term, postingsEnum));\n    +            }    +          } \n             }\n\tpostingsEnum = strictPhrases.filterPostings(term, postingsEnum, strictPhrasesTermToSpans.get(term));\n\tif (postingsEnum == null) \n{\n    -          continue;// completely filtered out\n    +      }\n    +    }\n    +\n    +    // Handle automata\n    +    if (automata.length > 0) \n{\n    +      offsetsEnums.addAll(createAutomataOffsetsFromTerms(termsIndex, doc));\n    +    }\n    +\n    +    return offsetsEnums;\n    +  }\n    +\n    +  protected List<OffsetsEnum> createAutomataOffsetsFromTerms(Terms termsIndex, int doc) throws IOException {\n    +    Map<CharacterRunAutomaton, List<PostingsEnum>> automataPostings = new IdentityHashMap<>(automata.length);\n\t\n\t\t\n\t\t\n\t\t\tEnd diff \u2013\n\t\t\n\t\t\n\t\n\t\n\n\n\n    Pushed that for now to see what you think. "
        },
        {
            "id": "comment-15618320",
            "author": "ASF GitHub Bot",
            "date": "2016-10-29T15:46:28+0000",
            "content": "Github user Timothy055 commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/105\n\n    I don't think there's a way to avoid keeping the position state, unfortunately.  The reason is that we can move one of the postings enums to the next position, but then realize the next position for that term is behind the position for a different term (and postings enum) that also matches the wildcard.  Then we'll update the top and switch to the next postings enum (by offset now), but once it's exhausted or we switch back to the previous one from interleaving the position is lost.  :/  An alternative to avoid this would be to change PostingsEnum to allow fetching of the currentPosition, then nearly all the house keeping would go away. "
        },
        {
            "id": "comment-15618483",
            "author": "ASF GitHub Bot",
            "date": "2016-10-29T17:42:25+0000",
            "content": "Github user dsmiley commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/105\n\n    I started playing with a bit and realized the same thing.  It looks straight-forward but it's deceptively more complicated.  Then it hit me \u2013 lets not try to return the correct position at all!  A \"normal\" PostingsEnum should but this one is only used for offsets.  So always return -1 \u2013 we can get away with it for this internal use. "
        },
        {
            "id": "comment-15618647",
            "author": "ASF GitHub Bot",
            "date": "2016-10-29T19:58:50+0000",
            "content": "Github user Timothy055 commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/105\n\n    Hmm, clever! But not sure I find it very clean though.  I feel like that can lead to trouble down the road if code ever expects the offsets to be ordered.  If we went that route we wouldn't even need the priority queue though.  Btw, I MultiTermHighlighting is nearly gone except for one method that is used in the UnifiedHighlighter and MemoryIndexOffsetStrategy for extracting automata from a query.  Any ideas on good ways to move it? Perhaps the UnifiedHighlighter should do all automata extraction and pass that in?  "
        },
        {
            "id": "comment-15619010",
            "author": "ASF GitHub Bot",
            "date": "2016-10-30T01:09:32+0000",
            "content": "Github user dsmiley commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/105\n\n    Check it out: https://github.com/dsmiley/lucene-solr/commit/50e2ea89c7eb15c863aa6e04e14fd32085ee85bd\n    Remember this is a package-private class internal to the UnifiedHighlighter.  Not completely implementing an interface if the caller won't care is okay.\n\n    RE MultiTermHighlighting: I think it's fine as-is.  The UH class is very long to adopt this \u2013 too long as it is IMO. "
        },
        {
            "id": "comment-15628823",
            "author": "ASF GitHub Bot",
            "date": "2016-11-02T12:37:14+0000",
            "content": "Github user Timothy055 commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/105\n\n    Hmm, I know we're already knowingly breaking the PostingsEnum contract, but rather than throwing a IllegaStateException, maybe we could still return positions, they just happen to be unordered, that way when the final position is exhausted we return -1 as the sentinel value still?  Otherwise, I'm good with this, I'll merge in.  Btw, we've seen other needs for something like a CompositePostingsEnum that abstracts over a set of terms, but since this is still internal, dropping the house-keeping will also make this code do less. We can always try to cross that bridge later. "
        },
        {
            "id": "comment-15628844",
            "author": "ASF GitHub Bot",
            "date": "2016-11-02T12:43:33+0000",
            "content": "Github user Timothy055 commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/105\n\n    Forget it, you can't do that because the next position on one enum might be -1, but there's more enums left in the queue so the user of this class could inadvertently terminate early if looking for -1. "
        },
        {
            "id": "comment-15628847",
            "author": "ASF GitHub Bot",
            "date": "2016-11-02T12:44:01+0000",
            "content": "Github user Timothy055 commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/105\n\n    Merged your commit. "
        },
        {
            "id": "comment-15628861",
            "author": "ASF GitHub Bot",
            "date": "2016-11-02T12:48:12+0000",
            "content": "Github user dsmiley commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/105\n\n    (I wrote this as you made your last comments, but rather than delete it I'll comment any way)\n\n    The documentation for `PostingsEnum.nextPosition()` states that calling it more than `freq()` times is undefined.  Thus it's quite valid to throw an IllegalStateException.\n\n    > Btw, we've seen other needs for something like a CompositePostingsEnum that abstracts over a set of terms, but since this is still internal, dropping the house-keeping will also make this code do less. \n\n    I don't think I quite get what you're saying.  By \"other needs\" do you mean Bloomberg internally?  If so, how would that relate this this one inside the UH?  Are you advocating a general purpose Multi-PosrtingsEnum?  On the latter... a highlighter wouldn't be where to introduce such a thing.  There is a `org.apache.lucene.index.MultiPostingsEnum` which I looked at while at the Lucene hackday code sprint as it got my curiosity.  Unfortunately, it doesn't seem quite general purpose enough for us to use \u2013 it demands a MultiTermsEnum parent.  Perhaps that could be improved to be useful without demanding a MultiTermsEnum parent... but that seems like a separate issue. "
        },
        {
            "id": "comment-15628874",
            "author": "ASF GitHub Bot",
            "date": "2016-11-02T12:53:08+0000",
            "content": "Github user Timothy055 commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/105\n\n    Fair enough on both ends.  And yes, we have seen a potential use case for a unified view over terms.  I'll take a look at MultiPostingsEnum and see if we can use that. "
        },
        {
            "id": "comment-15628983",
            "author": "ASF GitHub Bot",
            "date": "2016-11-02T13:31:39+0000",
            "content": "Github user dsmiley commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/105\n\n    Nevermind on `MultiPostingsEnum`.  MPE assumes the PostingsEnum are for a disjoint set of documents \u2013 i.e. each PostingsEnum matches documents disjoint from the others.  This makes sense given it's use for a single view over multiple segments.  In our case we want what might be called a MultiPositionsPostingsEnum or MultiOffsetsPostingsEnum or \"Composite\" instead of \"Multi\" (synonymous, really).  We merge the positions (yet we don't care about the actual position #s; it's the other stuff we expose per-position).  BTW my code you merged had a \"nocommit\" to consider changing the class name.   "
        },
        {
            "id": "comment-15629097",
            "author": "ASF GitHub Bot",
            "date": "2016-11-02T14:15:37+0000",
            "content": "Github user Timothy055 commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/105\n\n    Ah, missed that.  Fixed "
        },
        {
            "id": "comment-15660328",
            "author": "Timothy M. Rodriguez",
            "date": "2016-11-12T21:40:10+0000",
            "content": "I've merged with the changes from LUCENE-7544 and also ran some benchmarks. (Thanks David Smiley for the fix on LUCENE-7546!)\n\nOriginal:\n\n\n\n\nImpl\nTerms\nPhrases\nWildcards\n\n\n(search)\n1.14\n1.43\n2.44\n\n\nSH_A\n7.36\n7.49\n16.37\n\n\nUH_A\n5.32\n4.55\n9.24\n\n\nSH_V\n4.12\n4.42\n8.47\n\n\nFVH_V\n3.46\n2.98\n7.13\n\n\nUH_V\n3.7\n3.45\n6.61\n\n\nPH_P\n3.76\n3.45\n9.6\n\n\nUH_P\n3.34\n2.91\n9.33\n\n\nUH_PV\n3.26\n2.8\n6.72\n\n\n\n\n\nWith improvements from LUCENE-7526:\n\n\n\n\nImpl\nTerms\nPhrases\nWildcards\n\n\n(search)\n1.18\n1.38\n2.52\n\n\nSH_A\n7.98\n7.53\n16.62\n\n\nUH_A\n5.46\n4.6\n9.43\n\n\nSH_V\n4.13\n4.42\n8.26\n\n\nFVH_V\n3.45\n3.05\n6.93\n\n\nUH_V\n3.79\n3.43\n6.62\n\n\nPH_P\n3.82\n3.47\n9.4\n\n\nUH_P\n3.33\n3.03\n9.46\n\n\nUH_PV\n3.24\n2.81\n6.92\n\n\n\n\n\nIf you disable the new option to prefer passage relevancy over speed you'll get the following for analysis:\n\n\n\n\nImpl\nTerms\nPhrases\nWildcards\n\n\n(search)\n1.1\n1.43\n2.44\n\n\nUH_A\n5.31\n4.66\n9.14\n\n\n\n\n\nI wasn't able to get very consistent times with the benchmarks, but it looks like the changes keep close performance while simplifying the code and improving relevancy in the Analysis case (unless preferPassageRelevancyOverSpeed is disabled).  If that option is disabled the timings line up pretty closely with the originals, providing a minor speed boost. There should also be a memory savings by avoiding re-creation of TokenStreams, but that was difficult to measure, but could prove beneficial if there is memory pressure.\n\nI performed these benchmark on a machine with the following configuration:\n\nProcessor: AMD Phenom II X4 960T 3.0GHz\nMemory: 24GB DDR3\nDisk: Crucial CT256MX SSD\nOS: Windows 10\nJava: Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)\n\nAll versions of the benchmarks incorporated above included the changes from LUCENE-7544.\n\nDavid Smiley It looks like my older processor took significantly longer to highlight across the board than in your initial run for LUCENE-7438.  I'd be curious how this set of changes performs on your machine now. "
        },
        {
            "id": "comment-15660330",
            "author": "Timothy M. Rodriguez",
            "date": "2016-11-12T21:41:07+0000",
            "content": "Other than that, I think this code is in good shape for committing. "
        },
        {
            "id": "comment-15662505",
            "author": "David Smiley",
            "date": "2016-11-14T01:47:44+0000",
            "content": "RE Benchmarks:\n\nYeah it seems hard to get consistency. I even upped the iterations counts to 2000 (from 500) and saw more variability than I'd like.  Nonetheless your analysis looks good to me; it's how I would sum up my observations today.\n\nI ran my benchmarks with \"-Xms4G -Xmx4G -XX:NewRatio=1\" (to reduce the effects of GC).  My machine is a MacBook Pro Retina \"Late 2013\", 2 GHz i7, 8GB RAM.  This index is on an external spinning disk but there is no disk activity after warm-up because it's all in the O.S. Cache.\n\nAs I looked over things carefully, I made a few more changes; all pretty minor.  A few were in reaction to \"ant precommit\". https://github.com/dsmiley/lucene-solr/commits/uh_Tim \u2013 the test & some javadoc issues.  I also did a little bit of refactoring I hope you'll be good with.  In a couple cases I merely moved a method up or down so that the code flows top to bottom better.  Regarding Passage: I'll create a separate issue expressly for making Passage usable by anyone customizing the highlighter.  It's not as simple as addMatch being public; there are other methods.  \n\nWhat's there now is in good shape for committing.\n\nA couple ideas occurred to me; feel free to punt to another issue or never:\n\n\tMultiValueTokenStream isn't needed for MemoryIndexOffsetStrategy, albeit with a change to loop over content.split(separatorChar).  MemoryIndex.addField is overloaded to take the position increment gap.  Then, MultiValueTokenStream could move to an inner class of TokenStreamOffsetStrategy, and it wouldn't generally be used (as it's no longer by default).  That'd be nice \u2013 keeping the complexity over there, and it's a bit of a hack too.\n\tWe had made OffsetsEnum & TokenStreamPostingsEnum implement Closeable to ameliorate the ramifications of the text analysis code throwing an exception, i.e. due to a bug.  The only beneficiary of this now is TokenStreamOffsetStrategy, which isn't the default anymore. It could be removed to simplify things.  But then again, perhaps it could be useful for those implementing custom OffsetStrategies.  I guess it should stay; there's very little to this after all.\n\n\n\nProposed CHANGES.txt in \"Improvements\":\n\n\tEnhanced UnifiedHighlighter's passage relevancy for queries with wildcards and sometimes just terms. Added shouldPreferPassageRelevancyOverSpeed() which can be overridden to return false to eek out more speed in some cases. (Tim Rodriguez, David Smiley)\n\n "
        },
        {
            "id": "comment-15665317",
            "author": "Timothy M. Rodriguez",
            "date": "2016-11-14T23:03:35+0000",
            "content": "Added the proposed changes.  I'm on the fence around the refactor for MultiValueTokenStream, I'd much prefer to get rid of it completely if we could.  But for now having some symmetry between the two impls seems worthwhile to me?  I'd like to punt on that one. "
        },
        {
            "id": "comment-15666181",
            "author": "David Smiley",
            "date": "2016-11-15T05:44:10+0000",
            "content": "I understand RE MultiValueTokenStream.  I was motivated by considering highlighting a TokenStream without a MemoryIndex resulted in some complexity in various places (not introduced here, I mean pre-existing); some of it is addressed with this patch, and others still linger.  Another hack that could move is the getPayoad() returning the Term.  If this offset strategy returned a subclassed OffsetEnum, that quirk could be better isolated.  Any way; let that be for another day.\n\nAttached is the patch delta from master.  I'll commit tomorrow night. "
        },
        {
            "id": "comment-15668354",
            "author": "ASF subversion and git services",
            "date": "2016-11-15T21:17:07+0000",
            "content": "Commit 7af454ad767c3a0364757d6fcf55bff9f063febe in lucene-solr's branch refs/heads/master from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7af454a ]\n\nLUCENE-7526: UnifiedHighlighter: enhance MTQ passage relevancy. TokenStreamFromTermVector isn't used by the UH anymore. Refactor AnalysisOffsetStrategy into TokenStream and MemoryIndex strategies, and related refactorings from that. "
        },
        {
            "id": "comment-15668358",
            "author": "ASF subversion and git services",
            "date": "2016-11-15T21:18:37+0000",
            "content": "Commit 0790c34cc555ec9aaaab09cae04da6e61f465cfc in lucene-solr's branch refs/heads/branch_6x from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0790c34 ]\n\nLUCENE-7526: UnifiedHighlighter: enhance MTQ passage relevancy. TokenStreamFromTermVector isn't used by the UH anymore. Refactor AnalysisOffsetStrategy into TokenStream and MemoryIndex strategies, and related refactorings from that.\n\n(cherry picked from commit 7af454a) "
        },
        {
            "id": "comment-15668378",
            "author": "David Smiley",
            "date": "2016-11-15T21:21:47+0000",
            "content": "Committed. Thanks Tim and thanks Michael Braun for some internal code reviewing. "
        },
        {
            "id": "comment-15962205",
            "author": "ASF GitHub Bot",
            "date": "2017-04-09T18:07:48+0000",
            "content": "Github user Timothy055 closed the pull request at:\n\n    https://github.com/apache/lucene-solr/pull/105 "
        }
    ]
}