{
    "id": "LUCENE-1241",
    "title": "0xffff char is not a string terminator",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Won't Fix",
        "status": "Resolved"
    },
    "description": "Current trunk index.DocumentWriter uses \"\\uffff\" as a string terminator, but it should not to be for some reasons. \\uffff is not a terminator char itself and we can't handle a string that really contains \\uffff. And also, we can calculate the end char position in a character sequence from the string length that we already know.\n\nHowever, I agree with the usage for assertion, that \"\\uffff\" is placed after at the end of a string in a char sequence.",
    "attachments": {
        "LUCENE-1241.take2.patch": "https://issues.apache.org/jira/secure/attachment/12378960/LUCENE-1241.take2.patch",
        "LUCENE-1241.patch": "https://issues.apache.org/jira/secure/attachment/12378367/LUCENE-1241.patch",
        "ComparableCharSequence.java": "https://issues.apache.org/jira/secure/attachment/12378535/ComparableCharSequence.java"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-03-21T06:06:32+0000",
            "content": "Created a patch that is aware of string length. ",
            "author": "Hiroaki Kawai",
            "id": "comment-12581008"
        },
        {
            "date": "2008-03-21T10:17:57+0000",
            "content": "\nwe can't handle a string that really contains \\uffff\nThis is an invalid UTF16 string for interchange.  The standard explicitly allows for certain characters (including this one) to be used for internal purposes.\n\n\nHowever, I agree with the usage for assertion, that \"\\uffff\" is placed after at the end of a string in a char sequence.\nI don't think this is necessary for assertion.  The memory cost for this is sizable.  Right now tracking a string's length consumes 2 bytes (0xffff char) per posting.  By adding length we're consuming an additional 4 bytes.  While indexing, there are a large number of postings (one per unique term) so this added RAM usage is not negligible.\n\nI think we should do one or the other, but not both.\n\nReally the tradeoff we are exploring here is whether using up 2 more bytes per term, which causes us to flush sooner & merge more often for a given RAM buffer size, is offset by the speedup of not having to check for 0xffff and compute length in certain places.\n\nOne problem with the patch is you forgot to add another int (4 bytes) POSTING_NUM_BYTE in DocumentsWriter.  This is important because the tradeoff we are exploring here is whether increasing RAM usage of a Posting, which causes more frequent flushing, while then saving some of not having to compare to 0xffff in certain places, is net/net a performance \"win\".  Can you fix this?  Thanks.\n\nHave you run any performance tests to assess the impact of this change?  I think that's critical here since if this is net/net a performance loss we shouldn't make the change. ",
            "author": "Michael McCandless",
            "id": "comment-12581049"
        },
        {
            "date": "2008-03-21T16:13:39+0000",
            "content": "\nwe can't handle a string that really contains \\uffff\n\nThis is an invalid UTF16 string for interchange. The standard explicitly allows for certain characters (including this one) to be used for internal purposes.\n\nI strongly suspect, however, that \"internal purposes\" is meant to be taken as application-internal, not leaf-library-internal. ",
            "author": "Steve Rowe",
            "id": "comment-12581110"
        },
        {
            "date": "2008-03-25T04:07:50+0000",
            "content": "I think we should not use \\uffff as a terminator in Lucene library regardless of the fact that it is allowed in Unicode standard, because it is unnecessary.\n\nReading commit log in svn repository, and the code base at revision 553235, I suspect termination with \"\\uffff\" is introduced at 553236 referring the implementation of java.text.CharacterIterator. Isn't it? ( java.text.CharacterIterator.DONE is class static and is \"\\uffff\". The class java.text.CharacterIterator is for supporting internationalization interface of bidirectional string scan. And we can determine whether we reached the end of a string by comparing what we get with java.text.CharacterIterator.DONE. )\n\nI came to the idea of introducing a new class that implements CharSequence, Comparable and has a good hashCode() that will use the buffer of original memory allocation (String, StringBuffer, char[], CharBuffer, or etc.). ",
            "author": "Hiroaki Kawai",
            "id": "comment-12581768"
        },
        {
            "date": "2008-03-25T04:39:26+0000",
            "content": "ComparableCharSequence illustrates the idea. I wanted to name it shorter, but have no idea right now. ",
            "author": "Hiroaki Kawai",
            "id": "comment-12581777"
        },
        {
            "date": "2008-03-25T09:23:00+0000",
            "content": "\nI think we should not use \\uffff as a terminator in Lucene library regardless of the fact that it is allowed in Unicode standard, because it is unnecessary.\n\nI'm not yet convinced it's unecessary.  We need to run performance\ntests to understand the time/space tradeoff here.  If this change\nspeeds up indexing we should do it.  RAM is cheap.\n\nBy far, the Posting instances consume the most RAM in DocumentsWriter.\nRight now each Posting is 66 bytes; this patch, once finished\nincreases that to 68 bytes.\n\nI don't like increasing the byte usage of Posting unless there's a\ngood counterbalance, which I think this change may have if we see\nthat it improves indexing speed.\n\nI just checked: when indexing Wikipedia with a 64 MB buffer, each\nsegment flushed has ~430,000 Posting instances.  So the Posting\ninstances alone account for 27 MB of the buffer.\n\nThat means the added 2 bytes from this change will consume ~840 KB\nadditional RAM, which is not insignificant loss of RAM efficiency.\n\n[Aside: by Zipf's law, the vast majority of these terms should occur\nrarely.  Eg roughly half will occur only once.  If we could find some\nway to represent these rare terms with a much more compact structure\n(Posting has alot of \"overhead\" to efficiently manage a long posting\nlist) then we would greatly increase DW's RAM efficiency.]\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12581843"
        },
        {
            "date": "2008-03-25T09:24:03+0000",
            "content": "\nReading commit log in svn repository, and the code base at revision 553235, I suspect termination with \"\\uffff\" is introduced at 553236 referring the implementation of java.text.CharacterIterator. Isn't it? ( java.text.CharacterIterator.DONE is class static and is \"\\uffff\". The class java.text.CharacterIterator is for supporting internationalization interface of bidirectional string scan. And we can determine whether we reached the end of a string by comparing what we get with java.text.CharacterIterator.DONE. )\n\nIndeed, CharacterIterator.DONE also uses U+FFFF as the termination\ncharacter, though I hadn't realized that until now.\n\n\nI came to the idea of introducing a new class that implements CharSequence, Comparable and has a good hashCode() that will use the buffer of original memory allocation (String, StringBuffer, char[], CharBuffer, or etc.).\n\nThis looks neat, but, can you pull this all together into a single\nworkable patch that we can run some performance tests on?\n ",
            "author": "Michael McCandless",
            "id": "comment-12581844"
        },
        {
            "date": "2008-03-25T10:31:13+0000",
            "content": "\nThis looks neat, but, can you pull this all together into a single\nworkable patch that we can run some performance tests on?\n\nOK, I can. But, do you really want a huge single patch? And, this is yet another issue to do. I want to do the right thing, and performance is also yet another issue.  ",
            "author": "Hiroaki Kawai",
            "id": "comment-12581860"
        },
        {
            "date": "2008-03-25T11:51:27+0000",
            "content": "OK how about a separate issue for ComparableCharSequence?\n\nBut it'd be great to first bring closure to this issue, ie, fixing the issues I found (above) so we can assess performance impact of this change. ",
            "author": "Michael McCandless",
            "id": "comment-12581884"
        },
        {
            "date": "2008-03-25T12:09:17+0000",
            "content": "\nOK how about a separate issue for ComparableCharSequence? \n\nI'm now working for it \n\nI'll open later. ",
            "author": "Hiroaki Kawai",
            "id": "comment-12581887"
        },
        {
            "date": "2008-03-27T10:09:58+0000",
            "content": "Your commit of rev 641303 was so huge that my current working copy got broken perfectly. I can't help giving up right now. ",
            "author": "Hiroaki Kawai",
            "id": "comment-12582584"
        },
        {
            "date": "2008-03-27T10:24:25+0000",
            "content": "Woops, sorry.  I can take over?  I'll start from your patch, update to the current trunk, and fold in my feedback above, then test performance. ",
            "author": "Michael McCandless",
            "id": "comment-12582587"
        },
        {
            "date": "2008-03-31T13:25:40+0000",
            "content": "Attached take2 patch.  I fixed it to apply to trunk, and I removed\n0xffff entirely.  All tests pass, but...\n\nUnfortunately, this change causes a significant net slowdown (5.9%) in\nindexing throughput.  I ran this alg:\n\n  analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer\n  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker\n  docs.file=/Volumes/External/lucene/wiki.txt\n  doc.stored = true\n  doc.term.vector = true\n  doc.add.log.step=2000\n  directory=FSDirectory\n  autocommit=false\n  compound=false\n  ram.flush.mb=64\n  { \"Rounds\"\n    ResetSystemErase\n    { \"BuildIndex\"\n\n\tCreateIndex\n      { \"AddDocs\" AddDoc > : 200000\n      - CloseIndex\n    }\n    NewRound\n  } : 5\n  RepSumByPrefRound BuildIndex\n\n\n\nI ran the test on an Intel quad core Mac Pro with 4-drive RAID 0.  JVM\nis 1.5 and I run with \"-Xms1024M -Xmx1024M -Xbatch -server\".\n\nTrunk gets 897.3 rec/s and the patch gets 844.3 rec/s, best of 5 =\n5.9% slower.\n\nI don't think we should commit this. ",
            "author": "Michael McCandless",
            "id": "comment-12583675"
        }
    ]
}