{
    "id": "LUCENE-5634",
    "title": "Reuse TokenStream instances in Field",
    "details": {
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed",
        "components": [],
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [
            "4.9",
            "6.0"
        ]
    },
    "description": "If you don't reuse your Doc/Field instances (which is very expert: I\nsuspect few apps do) then there's a lot of garbage created to index each\nStringField because we make a new StringTokenStream or\nNumericTokenStream (and their Attributes).\n\nWe should be able to re-use these instances via a static\nThreadLocal...",
    "attachments": {
        "LUCENE-5634.patch": "https://issues.apache.org/jira/secure/attachment/12642736/LUCENE-5634.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-13986149",
            "author": "Michael McCandless",
            "content": "Initial patch, but I'd like to find a way to reuse NumericTokenStream\ntoo ... but it's trickier since the precStep is final (maybe we can\nun-final it and add a setter?)\n\nI ran a quick test, indexing all (8.6 M docs) of Geonames, which is a\ngood test for \"tiny documents\" ... with trunk it 56 seconds and with\nthe patch it's 45 seconds, ~20% faster. ",
            "date": "2014-04-30T22:04:11+0000"
        },
        {
            "id": "comment-13986177",
            "author": "Shay Banon",
            "content": "this optimization has proven to help a lot in the context of ES, but we can use a static thread local since we are fully in control of the threading model. With Lucene itself, where it can be used in many different environment, then this can cause some unexpected behavior. For example, this might cause Tomcat to warn on leaking resources when unloading a war. ",
            "date": "2014-04-30T22:36:50+0000"
        },
        {
            "id": "comment-13986204",
            "author": "Michael McCandless",
            "content": "Here's an alternate approach, pulled from early iterations on LUCENE-5611, to specialize indexing just a single string ... there are still nocommits, it needs to be more generic to any not tokenized field, etc.  It's sort of silly to build up an entire TokenStream when really you just need to index the one token ...\n\nThis patch indexes geonames in ~38.5 sec, or ~31% faster than trunk ",
            "date": "2014-04-30T23:13:31+0000"
        },
        {
            "id": "comment-13987645",
            "author": "Uwe Schindler",
            "content": "but it's trickier since the precStep is final (maybe we can un-final it and add a setter?)\n\nPlease don't do this. It is maybe better to do it like in Elasticsearch: Have a pool of NTS for each precision step.\n\nthis optimization has proven to help a lot in the context of ES, but we can use a static thread local since we are fully in control of the threading model. With Lucene itself, where it can be used in many different environment, then this can cause some unexpected behavior. For example, this might cause Tomcat to warn on leaking resources when unloading a war.\n\nThanks Shay: This is really the reason why we always refused to use static  ThreadLocals in Lucene, especially for those heavy used components.\n\nMaybe we can do a similar thing like with StringField in Mike's patch. Its a bit crazy to move out the TokenStreams from the field, but we can do this for performance here. Just have a lazy init pool of NumericTokenStreams for each precisionStep in each per thread DocumentsWriter (DefaultIndexingChain).\n\n-1 to add thread locals in Lucene here!\n\nAnother idea how to manage the pools: Maybe add a protected method to Field that can get the DocumentsWriter instance and add some caching functionality for arbitrary TokenStreams (not just NumericTS or StringTS): Maybe some method on the per thread DocumentsWriter to set aTokenStream for reuse per field. The field (also custom ones) then could use setCachedTokenStream/getCachedTokenStream through the DocumentsWriter accessor from inside the Field. ",
            "date": "2014-05-02T12:44:38+0000"
        },
        {
            "id": "comment-13987648",
            "author": "Uwe Schindler",
            "content": "Another idea: Maybe add a parameter to Field#tokenStream(), passing the previously cached instance! By this the field could obviously reuse the TokenStream, if the type (instanceof check) is correct. If not, throw it away and create a new one. The indexer then manages the cache (its just a field in DefaultIndexingChain or DocumentsWriter). ",
            "date": "2014-05-02T12:48:26+0000"
        },
        {
            "id": "comment-13987729",
            "author": "Robert Muir",
            "content": "\nAnother idea: Maybe add a parameter to Field#tokenStream(), passing the previously cached instance! By this the field could obviously reuse the TokenStream, if the type (instanceof check) is correct. If not, throw it away and create a new one. The indexer then manages the cache (its just a field in DefaultIndexingChain or DocumentsWriter).\n\nI like this idea better. Lets try and see how bad it looks. ",
            "date": "2014-05-02T14:22:49+0000"
        },
        {
            "id": "comment-13987789",
            "author": "Michael McCandless",
            "content": "Maybe add a parameter to Field#tokenStream(), passing the previously cached instance! \n\nThis sounds like a good idea! ",
            "date": "2014-05-02T15:22:59+0000"
        },
        {
            "id": "comment-13987790",
            "author": "Robert Muir",
            "content": "here is a patch. Tests seem happy, but i didnt benchmark or yet write explicit test.\n\nPersonally I think its bogus: I don't like that these fields (StringField, NumericField) \"backdoor\" the analyzer and to me thats the real bug. But I am ok with the change as a step, because it only makes the low-level interface api more bogus. ",
            "date": "2014-05-02T15:23:03+0000"
        },
        {
            "id": "comment-13987806",
            "author": "Michael McCandless",
            "content": "+1, patch looks good.\n\nI ran IndexGeoNames again, it took 37.6 seconds, which is a big speedup over trunk (55.6 seconds).  However, it's only doing StringField right now ... I'll re-test w/ NumericField too. ",
            "date": "2014-05-02T15:31:06+0000"
        },
        {
            "id": "comment-13987808",
            "author": "Michael McCandless",
            "content": "OK with NumericField, full Geonames index takes 129.7 sec on trunk and 101.0 sec with last patch... nice speedup. ",
            "date": "2014-05-02T15:39:26+0000"
        },
        {
            "id": "comment-13987827",
            "author": "Michael McCandless",
            "content": "BTW, that test was with precStep=8.  If I use precStep=4 (still the default, we really have to fix LUCENE-5609!) then indexing time for Geonames with the patch is 164.8 sec (63% slower!). ",
            "date": "2014-05-02T15:52:55+0000"
        },
        {
            "id": "comment-13987841",
            "author": "Uwe Schindler",
            "content": "Patch looks fine. I was afraid of complexity, but that looks quite good. I am not sure about backwards compatibility issues, but implementing your own IndexableField instance is still very expert. With Java 8 we could handle that with default interface methods (LOOOOOOL).\n\nThe current patch is fine for the 2 special cases, although its a bit risky, if we add new \"settings\" to NTS or change its API (we should have equals...). Maybe in LUCENE-5605 we can improve the check. If we pass FieldType directly to NTS and NRQ, we can handle the whole thing by comparing the field type and not rely on crazy internals like precStep.\n\nIt would be great if we could in the future remove the ThreadLocal from Analyzer, too - by using the same trick. Unfortunately with the current contract on TokenStream its hard to compare, unless we have a well-defined TokenStream#equals(). Ideally TokenStream#equals() should compare the \"settings\" of the stream and its inputs (for Filters), but that is too advanced for the simple 2 cases.\n\nAnother solution for this would be to have some \"holder\" around the TokenStream thats cached and provides hashcode/equals. By that a Field could determine better if its his own tokenstream (e.g. by putting a refernce to its field type into the holder). ",
            "date": "2014-05-02T16:00:17+0000"
        },
        {
            "id": "comment-13987843",
            "author": "Uwe Schindler",
            "content": "BTW, that test was with precStep=8. If I use precStep=4 (still the default, we really have to fix LUCENE-5609!) then indexing time for Geonames with the patch is 164.8 sec (63% slower!).\n\nH\u00c4? How comes, makes no sense to me. Are you sure you are doing the right thing? Or are you comparing  the speedup by this patch in combination with the precision step change? ",
            "date": "2014-05-02T16:02:34+0000"
        },
        {
            "id": "comment-13987844",
            "author": "Robert Muir",
            "content": "I would prefer to simply break the interface rather than do anything sophisticated here. Its a very expert low-level one. The patch had very minimal impact to the codebase.\n\nI think its good to defer stuff with Analyzer and not do that here, that has a lot of consumers like QueryParsers, MoreLikeThis, Suggesters, ... Thats a more complex issue. I am unsure that adding things like equals is a good idea, it might make things very complex. For now, if you implement your own subclass, you can just ignore the parameter, and its the same performance and so on.\n\nI will upload a new patch with tests (including doing stupid things).  ",
            "date": "2014-05-02T16:04:25+0000"
        },
        {
            "id": "comment-13987855",
            "author": "Uwe Schindler",
            "content": "I would prefer to simply break the interface rather than do anything sophisticated here. Its a very expert low-level one. The patch had very minimal impact to the codebase.\n\n+1. Nevertheless as we change a public interface, it should be mentioned in \"Backwards Breaks\". ",
            "date": "2014-05-02T16:09:42+0000"
        },
        {
            "id": "comment-13987974",
            "author": "Robert Muir",
            "content": "Updated patch with tests. ",
            "date": "2014-05-02T17:26:32+0000"
        },
        {
            "id": "comment-13988024",
            "author": "Uwe Schindler",
            "content": "+1 I am fine with that patch! ",
            "date": "2014-05-02T18:02:53+0000"
        },
        {
            "id": "comment-13988035",
            "author": "ASF subversion and git services",
            "content": "Commit 1591992 from Robert Muir in branch 'dev/trunk'\n[ https://svn.apache.org/r1591992 ]\n\nLUCENE-5634: Reuse TokenStream instances for string and numeric Fields ",
            "date": "2014-05-02T18:07:39+0000"
        },
        {
            "id": "comment-13988060",
            "author": "Michael McCandless",
            "content": "Or are you comparing the speedup by this patch in combination with the precision step change?\n\nBaseline was the patch w/ precStep=8 and comp was the patch w/ precStep=4.  I just re-ran to be sure; this is IndexGeoNames.java in luceneutil if you want to try ... it's easy to run, you just need to download/unzip geonames corpus first.  Net/net precStep=4 is very costly and doesn't seem to buy much query time speedups from my tests on LUCENE-5609. ",
            "date": "2014-05-02T18:28:52+0000"
        },
        {
            "id": "comment-13988080",
            "author": "ASF subversion and git services",
            "content": "Commit 1592005 from Robert Muir in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1592005 ]\n\nLUCENE-5634: Reuse TokenStream instances for string and numeric Fields ",
            "date": "2014-05-02T18:43:26+0000"
        }
    ]
}