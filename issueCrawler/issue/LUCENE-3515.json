{
    "id": "LUCENE-3515",
    "title": "Possible slowdown of indexing/merging on 3.x vs trunk",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Bug",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Opening an issue to pursue the possible slowdown Marc Sturlese uncovered.",
    "attachments": {
        "LUCENE-index-34.patch": "https://issues.apache.org/jira/secure/attachment/12498890/LUCENE-index-34.patch",
        "TestGenerationTime.java.40": "https://issues.apache.org/jira/secure/attachment/12498896/TestGenerationTime.java.40",
        "TestGenerationTime.java.3x": "https://issues.apache.org/jira/secure/attachment/12498895/TestGenerationTime.java.3x",
        "LUCENE-index-40.patch": "https://issues.apache.org/jira/secure/attachment/12498891/LUCENE-index-40.patch",
        "LUCENE-3515.patch": "https://issues.apache.org/jira/secure/attachment/12498922/LUCENE-3515.patch",
        "stdout-snow-leopard.tar.gz": "https://issues.apache.org/jira/secure/attachment/12498904/stdout-snow-leopard.tar.gz"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2011-10-13T15:47:48+0000",
            "content": "Patches from Marc showing the issue (thanks Marc!). ",
            "author": "Michael McCandless",
            "id": "comment-13126677"
        },
        {
            "date": "2011-10-13T16:04:04+0000",
            "content": "we should run the comparisons as a public static void main i think instead of extending LuceneTestCase.\n\n4.0 could be getting SimpleText etc  ",
            "author": "Robert Muir",
            "id": "comment-13126686"
        },
        {
            "date": "2011-10-13T16:13:10+0000",
            "content": "Same tests, just cut over to static void main instead. ",
            "author": "Michael McCandless",
            "id": "comment-13126692"
        },
        {
            "date": "2011-10-13T16:16:33+0000",
            "content": "3.x:\n\n    [junit] Time taken indexing: 114\n    [junit] Time taken closing: 20\n    [junit] Time taken whole process: 134\n\n4.x:\n\n    [junit] Time taken indexing: 133\n    [junit] Time taken closing: 1\n    [junit] Time taken whole process: 134\n\nSo for me, its the same speed.\n\nI didnt use the public static void main, i used the test as-is, except i disabled assertions and forced StandardCodec. ",
            "author": "Robert Muir",
            "id": "comment-13126693"
        },
        {
            "date": "2011-10-13T16:31:11+0000",
            "content": "I actually see 3.x running slower:\n\nTrunk:\n\n  Time taken indexing: 184\n  Time taken closing: 2\n  Time taken whole process: 187\n\n\n\n3.x:\n\nTime taken indexing: 205\n  Time taken closing: 40\n  Time taken whole process: 245\n\n\n\nBut, this is because 3.x does a level 2 merge just before close, and\nthe close must wait for that merge to complete.  Whereas trunk never\ngets to the level 2 merge (only level 0/1), likely because trunk's RAM\nefficiency is a better and so it packs more docs into each level 0 segment.\n\nIf we pushed the number of doc up to maybe 1.1M then we should\nsimilarly see trunk trigger a level 2 merge.\n\nReally when benchmarking indexing it's best to just close the IW\nwithout waiting for merges; otherwise you're comparing apples &\noranges. ",
            "author": "Michael McCandless",
            "id": "comment-13126704"
        },
        {
            "date": "2011-10-13T17:57:25+0000",
            "content": "Seeing others results the \"problem\" seems to be merges are very slow on trunk on Snow Leopard (where I've been running the tests).\nI've attached the whole stdout for both executions. Soon will run the tests on a debian box and post the results too.\nHere are the results:\ntrunk\nTime taken indexing: 927\nTime taken closing: 283\nTime taken whole process: 1211\n\n3.4\nTime taken indexing: 303\nTime taken closing: 37\nTime taken whole process: 340 ",
            "author": "Marc Sturlese",
            "id": "comment-13126762"
        },
        {
            "date": "2011-10-13T18:08:32+0000",
            "content": "something is severely wrong on mac os X with NIOFSDirectory (when merging, on the read side).\n\nwith mmapdirectory, its fast. ",
            "author": "Robert Muir",
            "id": "comment-13126773"
        },
        {
            "date": "2011-10-13T18:24:38+0000",
            "content": "simplefs is slow too. ",
            "author": "Robert Muir",
            "id": "comment-13126786"
        },
        {
            "date": "2011-10-13T18:40:02+0000",
            "content": "I've tried snow leopard and MMap. Trunk went much better than before:\n\ntrunk\nTime taken indexing: 416\nTime taken closing: 1\nTime taken whole process: 417\n\n3.4:\nTime taken indexing: 321\nTime taken closing: 46 \nTime taken whole process: 368\n\n ",
            "author": "Marc Sturlese",
            "id": "comment-13126795"
        },
        {
            "date": "2011-10-13T19:10:46+0000",
            "content": "it looks like the bug will especially affect any directory that uses bufferedindexinput (NIOFS/SimpleFS).\n\nThe problem is multitermsenum doesnt reuse the sub-docs&positionsenums, so for each term*segment we clone the input, and\nbufferedindexinput.clone() sets the clone's buffer to null.\n\nso across lots of low freq-terms we re-read 4096 bytes (MERGE_BUFFER_SIZE) to refill the buffer on each one...\n\nmmapdirectory is less affected because it has no buffer to re-read but seems like fixing the reusing would even help it... ",
            "author": "Robert Muir",
            "id": "comment-13126820"
        },
        {
            "date": "2011-10-13T20:41:50+0000",
            "content": "Patch, fixing the lack of reuse in MultiTermsEnum (used during merging).\n\nThe lack of re-use meant we were cloning 2 IndexInputs (frq/prx) per term being merged!  For NIOFSDir this then meant we would read the same 4K region of the file over and over and over again.\n\nI added a test to catch \"over-cloning\", but, the test sometimes fails when it gets pulsing codec because that codec does not properly reuse (a known issue that we should now fix!). ",
            "author": "Michael McCandless",
            "id": "comment-13126898"
        },
        {
            "date": "2011-10-13T20:43:01+0000",
            "content": "This seed will fail due to pulsing codec:\n\notv TestForTooMuchCloning.test -seed 6407e19e4835e90d:-d7cdae0d4a378eb:-ee1a92b677887\n\n ",
            "author": "Michael McCandless",
            "id": "comment-13126900"
        },
        {
            "date": "2011-10-13T21:18:27+0000",
            "content": "i opened LUCENE-3517 for the pulsing bug.\n\nBut i think we should fix this 'general bug' which affects all codecs first?\nI think LUCENE-3517 might involve attributes-policeman \n\nwe could do this:\n\n    String bodyCodec = CodecProvider.getDefault().getFieldCodec(\"body\");\n    assumeFalse(\"PulsingCodec fails this test because of over-cloning\", bodyCodec.equals(\"Pulsing\") || bodyCodec.equals(\"MockRandom\"));\n\n ",
            "author": "Robert Muir",
            "id": "comment-13126940"
        },
        {
            "date": "2011-10-14T12:05:46+0000",
            "content": "New patch.\n\nI changed the test to just use random terms, and also added the\nworkaround for Pulsing's non-reuse.\n\nI discovered SimpleText also fails to properly reuse (it failed the\nnew test) and fixed that.\n\nI think it's ready! ",
            "author": "Michael McCandless",
            "id": "comment-13127466"
        },
        {
            "date": "2011-10-14T19:41:43+0000",
            "content": "Thank you Marc and Erick!  This was a devious issue and severely impacted merge performance for non-MMapDir impls. ",
            "author": "Michael McCandless",
            "id": "comment-13127801"
        },
        {
            "date": "2011-10-24T22:46:15+0000",
            "content": "This bug was only present in 4.0. ",
            "author": "Michael McCandless",
            "id": "comment-13134592"
        }
    ]
}