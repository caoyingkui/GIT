{
    "id": "LUCENE-1581",
    "title": "LowerCaseFilter should be able to be configured to use a specific locale.",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Won't Fix",
        "status": "Resolved"
    },
    "description": "//Since I am a .Net programmer, Sample codes will be in c# but I don't think that it would be a problem to understand them.\n//\n\nAssume an input text like \"\u0130\" and and analyzer like below\n\n\tpublic class SomeAnalyzer : Analyzer\n    \t{\n\t\tpublic override TokenStream TokenStream(string fieldName, System.IO.TextReader reader)\n\t        {\n            \t\tTokenStream t = new SomeTokenizer(reader);\n\t\t        t = new Lucene.Net.Analysis.ASCIIFoldingFilter(t);\n\t\t\tt = new LowerCaseFilter(t);\n\t\t        return t;\n\t\t}\n        \n    \t}\n\n\n\n\nASCIIFoldingFilter will return \"I\" and after, LowerCaseFilter will return\n\t\"i\" (if locale is \"en-US\") \n\tor \n\t\"\u0131' if(locale is \"tr-TR\") (that means,this token should be input to another instance of ASCIIFoldingFilter)\n\n\n\nSo, calling LowerCaseFilter before ASCIIFoldingFilter would be a solution, but a better approach can be adding\na new constructor to LowerCaseFilter and forcing it to use a specific locale.\n\n    public sealed class LowerCaseFilter : TokenFilter\n    {\n        /* +++ */System.Globalization.CultureInfo CultureInfo = System.Globalization.CultureInfo.CurrentCulture;\n\n        public LowerCaseFilter(TokenStream in) : base(in)\n        {\n        }\n\n        /* +++ */  public LowerCaseFilter(TokenStream in, System.Globalization.CultureInfo CultureInfo) : base(in)\n        /* +++ */  {\n        /* +++ */      this.CultureInfo = CultureInfo;\n        /* +++ */  }\n\t\t\n        public override Token Next(Token result)\n        {\n            result = Input.Next(result);\n            if (result != null)\n            {\n\n                char[] buffer = result.TermBuffer();\n                int length = result.termLength;\n                for (int i = 0; i < length; i++)\n                    /* +++ */ buffer[i] = System.Char.ToLower(buffer[i],CultureInfo);\n\n                return result;\n            }\n            else\n                return null;\n        }\n    }\n\n\n\nDIGY",
    "attachments": {
        "TestTurkishCollation.java": "https://issues.apache.org/jira/secure/attachment/12410529/TestTurkishCollation.java"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-03-29T03:50:57+0000",
            "content": "I guess we were telepathying or something because I reviewed LowerCaseFilter 2 days ago for the same reason \nThing is, in Java Character.toLowerCase does not accept a Locale, just char. Unlike String which has two variants for toLowerCase and toUpperCase, that accept in addition to the String, a Locale parameter.\n\nI believe that Character.toLowerCase in Java works ok, since it's based on the UNICODE spec (at least it writes so) - however I have to admit I haven't tested this character specifically. ",
            "author": "Shai Erera",
            "id": "comment-12693512"
        },
        {
            "date": "2009-03-29T08:04:52+0000",
            "content": "I believe also that Character.toLowerCase in Java works ok, But the proplem is:\n\nI --> i (in US)\nI --> \u0131 (in TR) .\n\nSo, I think, I should be able to choose the conversions.\n\nDIGY. ",
            "author": "Digy",
            "id": "comment-12693528"
        },
        {
            "date": "2009-03-29T10:50:57+0000",
            "content": "From the javadocs (http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Character.html#toLowerCase(char)):\n\nIn general, String.toLowerCase() should be used to map characters to lowercase. String case mapping methods have several benefits over Character case mapping methods. String case mapping methods can perform locale-sensitive mappings, context-sensitive mappings, and 1:M character mappings, whereas the Character case mapping methods cannot.\n\nSo I agree this is a problem, but I see no easy way (and efficient) to fix it. Suppose that we allow LowerCaseFilter to accept Locale. What would it do with it? We could add in LowerCaseFilter a Map<Locale, char[65536]> and allow one to pass in the Locale. If it's not null, and there's an entry in the map, lookup every character the filter receives. The lookup will be quite fast, as the character will serve as the index to the array (notice that we cover only 2-byte characters though) and if it's \\uFFFF we can assume there's no special handling and call Character.toLowerCase.\n\nThat is very fragile though as it's not easy to cover all the special case characters. Also, every time (including this one) we will find a special character that was not handled properly by the filter, it'd break back-compt, no?\n\nBTW, when characters are uppercase, I don't think we have a problem, as they will always be lowercased to a single character (even if it's the wrong one, it will be consistent in indexing and search). The problem comes with the lowercase characters. The character \\u0131 (lowercase I in Turkish) is lowercased to \\u0131, while its uppercase version (I) is lowercased to 'i'. Therefore there is a mismatch and we'll fail if the user will enter a lowercase query (as they often do). ",
            "author": "Shai Erera",
            "id": "comment-12693540"
        },
        {
            "date": "2009-03-29T11:48:09+0000",
            "content": "This a bit larger of a problem. It also pertains to upper casing, too.\n\nI don't remember exactly, but I seem to remember that Java is behind with regard to the Unicode spec and Locale support (e.g. it does not include fa, farsi). I find that ICU4J keeps current with the spec.\n\nI don't remember which way it goes, maybe it's both, but some Locales don't have a corresponding upper or lower case for some characters.\n\nI'm not sure, but I think efficiency pertains to how it is normalized in Unicode (e.g. NFC, NFKC, NFD, or NFKD). These might produce different performance results.\n\n(It is a different issue, but it is critical that the search requests perform the same Unicode normalization as the indes. As Lucene does not have these normalization filters, I find, I have to do this externally in my own filters using ICU.)\n\n(Again a different issue: Another related kind of folding is that of base 10 number shaping.)\n\nRegarding: \nI see no easy way (and efficient) to fix it. Suppose that we allow LowerCaseFilter to accept Locale. What would it do with it?\n\nI think that we need Upper and Lower case filters that operates on the token as a whole, using the string-level method to do case conversion.\n\nWhat I'd like to see is that lucene has a pluggable way to handle ICU, in so far as it does Locale specific things such as this. Such as using a base class UpperCaseFolder that provides the Java implementation, but that can take an alternate implementation, perhaps by reflection.\n\n ",
            "author": "DM Smith",
            "id": "comment-12693545"
        },
        {
            "date": "2009-03-29T13:07:41+0000",
            "content": "Although, it is not directly related to this issue, It is good to remember some existing problems in Lucene.\nhttps://issues.apache.org/jira/browse/LUCENENET-51\n\nDIGY ",
            "author": "Digy",
            "id": "comment-12693553"
        },
        {
            "date": "2009-03-29T15:08:02+0000",
            "content": "What I'd like to see is that lucene has a pluggable way to handle ICU, in so far as it does Locale specific things such as this. Such as using a base class UpperCaseFolder that provides the Java implementation, but that can take an alternate implementation, perhaps by reflection.\n\nWhy do this? What prevents you in your application from creating such a filter? Lucene does not provide too many analyzers, or a single Analyzer for use by all, with configurable options. So why provide in Lucene a filter which uses ICU4J? I'm asking that for core Lucene. Of course such a module can sit in contrib, as do the other analyzers for other languages ...\n\nBTW, I've had some experience with ICU4J and it had several performance issues, such as large consecutive array allocations. It also operates on strings, and does not have the efficient API Lucene has in tokenization (i.e., working on char[]).\nHowever, I've worked with it long time ago, and perhaps things have changed since. ",
            "author": "Shai Erera",
            "id": "comment-12693568"
        },
        {
            "date": "2009-03-29T16:12:35+0000",
            "content": "Why do this?\nLucene has a bias toward English texts and does not have a fundamental architecture focused on internationalization and localization. IMHO, it should.\n\nJava does not implement Unicode well and does not keep abreast with it's changes. It's not that ICU is the right solution. It is a robust solution.\n\nWhat prevents you in your application from creating such a filter?\nNothing at all. But I think that proper behavior regarding Unicode and locales is something that many want. Especially for non-English indexes. As such it belongs with Lucene not individual projects.\n\nWith that in mind, I think it would be great if Lucene were fully internationalized and localized, at least from a fundamental architecture perspective. (There is a separate issue on what core and contrib should be. I'm not clear where \"analyzers\" fall wrt that.)\n\nAs an implementation, if ICU is present it is used, with potential performance impacts, if not behavior degrades predictably and gracefully. This would create a quasi dependency not a hard one. ",
            "author": "DM Smith",
            "id": "comment-12693579"
        },
        {
            "date": "2009-03-29T18:09:04+0000",
            "content": "some comments I have on this topic:\n\nthe problems i have with default internationalization support in lucene revolve around the following:\n\n1. breaking text into words (parsing) is not unicode-sensitive\ni.e. if i have a word containing s + macron (s\u0304) it will not tokenize it correctly.\n\n2. various filters like lowercase as mentioned here, but also accent removal are not unicode-sensitive\n i.e. if i have s + macron (s\u0304) it will not remove the macron.\nthis is not a normalization problem, but its true it also doesn't seem to work correctly on decomposed NF(K)D text for similar reasons. in this example, there is no composed form for s + macron available in unicode so I cannot 'hack' around the problem by running NFC on this text before i feed it to lucene.\n\n3. unicode text must be normalized so that both queries and text are in a consistent representation.\n\none option I see is to have at least a basic analyzer that uses ICU to do the following.\n1. Break text into words correctly.\n2. common filters to do things like lowercase and accent-removal correctly.\n3. uses a filter to normalize text to one unicode normal form (say, NFKC by default)\n\nIn my opinion, having this available would solve a majority of the current problems.\n\nI kinda started trying to implement some of this with lucene-1488... (at least it does step 1!)\n ",
            "author": "Robert Muir",
            "id": "comment-12693591"
        },
        {
            "date": "2009-06-13T02:37:43+0000",
            "content": "i've attached a testcase showing how the collation filters in contrib solve your problem.\n\nI think its the best way to get locale-specific matching behavior when you know the locale: case differences, normalization, accents, the whole shebang.\n\njust set the strength and locale appropriately  ",
            "author": "Robert Muir",
            "id": "comment-12719067"
        },
        {
            "date": "2009-06-13T03:15:36+0000",
            "content": "For reference, I think the concept of LowerCaseFilter, either with or without Locale is incorrect for lucene when the intent is really to erase case differences.\n\nThere is an important distinction between converting to lowercase (for presentation), and erasing case differences (for matching and searching).\n\nHere is an example from the unicode std:\nCharacters may also have different case mappings, depending on the context. For example,\nU+03A3 \"\u03a3\" greek capital letter sigma lowercases to U+03C3 \"\u03c3\" greek small letter\nsigma if it is followed by another letter, but lowercases to U+03C2 \"\u03c2\" greek small\nletter final sigma if it is not.\n\nThe only correct methods to erase case differences are:\n1) Localized (for a specific language): use a collator as recommended here.\n2) Multilingual (for a mix of languages): use either the UCA (collator with ROOT locale) or unicode case-folding, either of which is only an approximation of the language-specific rules involved.\n\nthanks! ",
            "author": "Robert Muir",
            "id": "comment-12719069"
        },
        {
            "date": "2009-06-16T19:04:47+0000",
            "content": "So it sounds like we may end up closing this with the solution being the collation filters in contrib? It sounds like a direct core fix is not the immediate cards? ",
            "author": "Mark Miller",
            "id": "comment-12720298"
        },
        {
            "date": "2009-06-16T19:07:25+0000",
            "content": "you could add the JDK collation key filter to core if you wanted a core fix.\n\nbut the icu one is up to something like 30x faster than the jdk, so why bother  ",
            "author": "Robert Muir",
            "id": "comment-12720300"
        },
        {
            "date": "2009-06-28T05:41:28+0000",
            "content": "\nyou could add the JDK collation key filter to core if you wanted a core fix.\n\nbut the icu one is up to something like 30x faster than the jdk, so why bother \n\nLUCENE-1719 contains some timings I made about the relative speeds of these two implementations.  In short, for the platform/language/collator/JVM version combinations I tested, the ICU4J implementation's speed advantage ranges from 1.4x to 5.5x. ",
            "author": "Steve Rowe",
            "id": "comment-12724926"
        },
        {
            "date": "2009-06-28T11:26:05+0000",
            "content": "steven is right here, i should have said icu collation [in general] because the specific task of key generation isnt that much faster.\n\nhttp://site.icu-project.org/charts/collation-icu4j-sun\n\nsince its what is stored on disk its also useful to mention that the generated sort keys are smaller than the jdk keys (see link above).\n ",
            "author": "Robert Muir",
            "id": "comment-12724945"
        },
        {
            "date": "2009-12-01T19:09:46+0000",
            "content": "Hi, after what Shai brought up on the user list, I think we should attack this turkish issue, but in a different way.\nthe problem is when you want to do something AFTER lowercasing, such as stemming.\n\nBut I don't think we should use the localized lowercase function to do it,  because it doesn't really solve the problem in a general way (i.e. it wont solve greek, german, problems)\nalso performance will be poor, and if you arent careful with locales at both index and search time, things might not work.\n\nultimately I still think case folding is the right way for this across the board, but you can't do that without a 3rd party library.\n\nwhat do you guys think? ",
            "author": "Robert Muir",
            "id": "comment-12784342"
        },
        {
            "date": "2009-12-01T19:29:33+0000",
            "content": "ultimately I still think case folding is the right way for this across the board, but you can't do that without a 3rd party library.\nI think that any project that is serious about texts in various non-latin languages will already be using ICU4J. The significant non-latin language work is in contrib. 3-rd party libraries are not that big a deal there. ",
            "author": "DM Smith",
            "id": "comment-12784358"
        },
        {
            "date": "2009-12-01T19:37:11+0000",
            "content": "DM, you know I agree with you, but this is a latin script language \n\nMy motivation here is really to fix SnowballAnalyzer/TurkishStemmer in snowball pkg, which currently does not work correctly, and i don't think we want to introduce an ICU dependency there for case folding (with alternate mappings if \"Turkish\" is selected) + normalization?\nyou need both for it to work, because case folding will fold to NFD forms, but the stemmers themselves require NF(K)C\n\nThe only workaround I know of is for someone to use CharFilter right now with 4 rules for turkish, which isn't obvious. ",
            "author": "Robert Muir",
            "id": "comment-12784366"
        },
        {
            "date": "2009-12-01T19:45:38+0000",
            "content": "What about adding a new option to ASCIIFoldingFilter just to return lowercased tokens?\nLowercasing ascii chars would be very fast.\n\nDIGY ",
            "author": "Digy",
            "id": "comment-12784373"
        },
        {
            "date": "2009-12-01T19:50:42+0000",
            "content": "Digy, if we do that the turkish stemmer still will not work, because then 'i, \u0131, \u0130, and I' will all be folded to 'i'\n\nhere is a testcase showing how the turkish stemmer is sensitive to the correct i:\n\npublic void testNounAccusative() throws IOException {\n    // A\u011eACI in turkish lowercases to a\u011fac\u0131, but with lowercase filter a\u011faci.\n    // this fails due to wrong casing, because the stemmer\n    // will only remove -\u0131, not -i\n    assertAnalyzesTo(analyzer, \"a\u011fac\u0131\", new String[] { \"a\u011fa\u00e7\" });\n    assertAnalyzesTo(analyzer, \"A\u011eACI\", new String[] { \"a\u011faci\" });\n  }\n\n ",
            "author": "Robert Muir",
            "id": "comment-12784375"
        },
        {
            "date": "2009-12-01T19:53:30+0000",
            "content": "btw so my recommendation is one i hinted at on the user list, just bite the bullet and have a TurkishLowerCaseFilter for turkish/azeri:\n\nswitch(ch) {\n  case 'I' -> \u0131\n  case '\u0130' (in both nfd or nfc) -> i\n  default: Character.toLowerCase()\n} ",
            "author": "Robert Muir",
            "id": "comment-12784379"
        },
        {
            "date": "2009-12-01T20:26:19+0000",
            "content": "Hi, if no one objects, I would like to resolve this issue and handle the turkish case specifically in LUCENE-2102 ?\n\nAs mentioned before, I think that locale-based lowercasing won't really help the problem for all languages (but might make users thing it will), because lowercasing != casefolding. ",
            "author": "Robert Muir",
            "id": "comment-12784399"
        },
        {
            "date": "2009-12-01T20:34:12+0000",
            "content": "OK. thanks.\n\nDIGY ",
            "author": "Digy",
            "id": "comment-12784405"
        },
        {
            "date": "2009-12-01T20:41:12+0000",
            "content": "will fix the turkish case specifically in LUCENE-2102.\nLocale-based lowercasing is a bit misleading, and not really suitable for language-specific 'matching', although very suited for display.\nit also has some performance problems (requires String) ",
            "author": "Robert Muir",
            "id": "comment-12784408"
        }
    ]
}