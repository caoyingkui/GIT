{
    "id": "SOLR-561",
    "title": "Solr replication by Solr (for windows also)",
    "details": {
        "affect_versions": "1.4",
        "status": "Closed",
        "fix_versions": [
            "1.4"
        ],
        "components": [
            "replication (java)"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "The current replication strategy in solr involves shell scripts . The following are the drawbacks with the approach\n\n\tIt does not work with windows\n\tReplication works as a separate piece not integrated with solr.\n\tCannot control replication from solr admin/JMX\n\tEach operation requires manual telnet to the host\n\n\n\nDoing the replication in java has the following advantages\n\n\tPlatform independence\n\tManual steps can be completely eliminated. Everything can be driven from solrconfig.xml .\n\t\n\t\tAdding the url of the master in the slaves should be good enough to enable replication. Other things like frequency of\nsnapshoot/snappull can also be configured . All other information can be automatically obtained.\n\t\n\t\n\tStart/stop can be triggered from solr/admin or JMX\n\tCan get the status/progress while replication is going on. It can also abort an ongoing replication\n\tNo need to have a login into the machine\n\tFrom a development perspective, we can unit test it\n\n\n\nThis issue can track the implementation of solr replication in java",
    "attachments": {
        "SOLR-561.patch": "https://issues.apache.org/jira/secure/attachment/12383186/SOLR-561.patch",
        "deletion_policy.patch": "https://issues.apache.org/jira/secure/attachment/12383728/deletion_policy.patch",
        "SOLR-561-fixes.patch": "https://issues.apache.org/jira/secure/attachment/12392741/SOLR-561-fixes.patch",
        "SOLR-561-full.patch": "https://issues.apache.org/jira/secure/attachment/12388993/SOLR-561-full.patch",
        "SOLR-561-core.patch": "https://issues.apache.org/jira/secure/attachment/12386307/SOLR-561-core.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Yonik Seeley",
            "id": "comment-12599474",
            "date": "2008-05-23T19:17:06+0000",
            "content": "How about posting a snapshot of what you have, with a few paragraphs explaining how things work, etc.  Early feedback is better, and it allows more people to add their expertise.  I'm sure many are interested in the ease-of-use gains this patch can bring. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12599559",
            "date": "2008-05-24T04:17:03+0000",
            "content": "We shall post a patch in the next few days\n\nThe design is as follows:\n\n\tSnapShooter.java :  registered as a listener on postCommit/postOptimize . It makes a copy of the latest index to a new snapshot folder (same as it is today). Only in master. It can optionally take in a 'snapDir' as configuration if the snapshot is to be created ina folder other than the data directory.\n\tReplicationHandler: A requesthandler. This is registered in master & slave. It takes in the following config in the slave. Master node just needs an empty requesthandler registration.\nsolrconfig.xml\n \n<requestHandler name=\"replication\" class=\"solr.ReplicationHandler\">\n    <str name=\"masterUrl\">http://<host>:<port>/solr/<corename>/replication</str>\n    <str name=\"pollInterVal\">HH:MM:SS</str>\n</requestHandler>\n\n\n\n\n\nReplicationHandler Implements the following methods. Every method is invoked over http GET. These methods are usually trigerred from the slave (over http) or timer (for snappull). Admin can provide means to invoke some methods  like snappull,snapshoot .\n\n\tCMD_GET_FILE: (command=filecontent&snapshhot=<snapshotname>&file=< filename>&offset=<fileOffset>&len=<length-ofchunk>&checksum=<true|false>) .  This is invoked by a slave  only to fetch a file or a part of it . This uses a custom format (described later)\n\tCMD_LATEST_SNAP: (command=latestsnap). Returns the name of the latest snapshot (a namedlist response)\n\tCMD_GET_SNAPSHOTS: (command=snaplist). Returns a list of all snapshot names (a namedlist response)\n\tCMD_GET_FILE_LIST: (command=filelist&snap=<snapshotname>) . A list of all the files in the snapshot .conains name, lastmodified,size. (a namedlist response)\n\tCMD_SNAP_SHOOT: (command=snapshoot). Do a force snapshoot.\n\tCMD_DISABLE_SNAPPOLL: (command=disablesnappoll). For stopping the timer task\n\tCMD_SNAP_PULL : (command=snappull). Does the following operations (done in slave). It is mostly triggered from a timertask based on the pollInterval value.\n\t\n\t\tcalls a CMD_LATEST_SNAP to the master and get the latest snapshot name\n\t\tchecks if it has the same (or if a snappull is going on)\n\t\tif it is to be pulled, call CMD_GET_FILE_LIST to the master\n\t\tfor each file in the list make a call CMD_GET_FILE to the master. This command works in the following way\n\t\t\n\t\t\tthe server reads the file stream\n\t\t\tIt uses a CustomStreamResponseWriter (wt=filestream) to write the content. It has a packetSize (say 1mb)\n\t\t\tIt writes an int for length and another long for Adler32 checksum (if checksum=true). The packets are written one after another till EOF or an Exception.\n\t\t\tSnapPuller.java In the client reads the packet length and checksum and tries to read the packet.If it is unable to read the given packet or the checksum does not match or there is an exception , it closes the connection and makes a new CMD_GET_FILE  command with the offset = (totalbytesReceived). If everything is fine the packets are read till the _bytesDownloaded == fileSize_\n\t\t\tThis is continued till all the files are downloaded.\n\t\t\n\t\t\n\t\tcreates a folder index.tmp\n\t\tfor each file in the copied snapshot , try to create a hardlink in the index.tmp folder.(runs an OS specific command)\n\t\tIf hardlink creation fails use a copy\n\t\trename index.tmp to index\n\t\tcalls a commit on the updatehandler\n\t\n\t\n\n\n\n\n**note: The download tries to use the same stream to download the complete file .\n\nPlease comment on the design "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12599831",
            "date": "2008-05-26T12:34:35+0000",
            "content": "I think the above sounds more or less right (read it quickly).\n\n\n\tShould there exist a mechanism for preventing infinite loops (try to get a file, fail for some reason, try again, and over and over and over until some disk gets filled over night, for example)?\n\tI see &snapshhot=<snapshotname> as well as &snap=<snapshotname>.  This may be a typo in the JIRA comment only, I don't know.\n\n\n\nThinking about the Admin display of replication information:\n\n\tIs there anything that keeps track of overall data transfer progress?\n\t\n\t\tthe name of the snapshot being replicated currently\n\t\tthe name of the file being replicated currently\n\t\tthe total number of bytes transfered vs. the size of the snapshot\n\t\tany failures (number of failures + info)\n...\n...\n\t\n\t\n\n\n\nI imagine those wanting Enterprise Solr will desire this type of stuff, so even if we don't have any of this in the UI at this point, it might be good keeping this in mind and providing the necessary hooks, callbacks, etc. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12599848",
            "date": "2008-05-26T14:34:47+0000",
            "content": "Otis: All the points you have enumerated are valid . We actually think they should be there in the final solution.\n\n\tIt should have a way to prevent infinite loops.\n\tsnap=<snapshotname> is the correct command\n\n\n\n\nAll the admin related changes are planned exactly as you have asked .  But we can leave the hooks open and push through with the basic stuff. The design documentation just tries to cover everything which the scripts currently cover. \nIf everything else is fine we shall give a rough patch for your review in another 2-3 days "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12600706",
            "date": "2008-05-29T08:41:10+0000",
            "content": "There are problems with index replacement in windows.\nWindows does not allow as to delete the index folder, because it is being used .\n How do we solve this? "
        },
        {
            "author": "Andrew Savory",
            "id": "comment-12600745",
            "date": "2008-05-29T11:28:36+0000",
            "content": "This looks like an extremely useful addition. More comments when the patch is available, but an initial observation:\n<str name=\"pollInterVal\">HH:MM:SS</str>\nFor consistency, could this be specified cron-style instead? e.g.\n<str name=\"pollInterVal\">*/30 * * * *</str> "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12600772",
            "date": "2008-05-29T13:46:59+0000",
            "content": "I think hh:MM:ss is universally recognizable and very intuitive. We should also keep in mind that this solution will be used on a multiple platforms and an OS like Windows does not have cron so it's administrators may not be familiar with the cron format. "
        },
        {
            "author": "Andrew Savory",
            "id": "comment-12600775",
            "date": "2008-05-29T14:21:39+0000",
            "content": "Shalin,\n\nI'm assuming that pollInterVal is intended to specify the frequency of replication. hh:MM:ss is universally recognizable for specifying a single time, certainly. But how do you represent \"every hour\", or \"four times a day\", or \"the first tuesday of each month\" with that notation?\n\nCertainly Windows doesn't have cron but we're talking about a pure java implementation, so that's not a problem. Quartz might be a perfect solution for scheduling: http://www.opensymphony.com/quartz/ "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12600779",
            "date": "2008-05-29T14:30:20+0000",
            "content": "Does anybody really do things like \"first tuesday of each month\" for polling the Solr master? The slave's poll is usually set to run every few minutes. Atleast that's how we use it in our production environments. Quartz is nice but the thing is that we don't need all those features. A timer task is good enough for our needs.\n\nYes, hh:MM:ss represents time but it isn't difficult to view it as a countdown timer. It's definitely easier to understand than specifying number of seconds/minutes as an integer for a poll interval. What do you think? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12600792",
            "date": "2008-05-29T14:58:55+0000",
            "content": "for polling a simple interval this syntax may be enough. Polling is not a very expensive operation. It just sends a request and get the latest snapshotname. So we can schedule it to run even every minute also\n\nIf there is a need for such complex scheduling we can consider that syntax.\n\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12600794",
            "date": "2008-05-29T15:05:32+0000",
            "content": "A possible solution to the windows replication problem would be.\n\n\tMake changes to Solr Core to load the index from a given directory instead of hardcoding the directory name.In our case we can give the new snapshot directory\n\tAfter the new IndexSearcher/writer is loaded, Close the original index searcher . Then it is ok to delete that\n\tDelete old contents and Copy hardlinks to the index directory. So if you restart solr it will get the index from the right place\n\n\n "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12600823",
            "date": "2008-05-29T16:27:33+0000",
            "content": "Re poll interval: I think the HH:MM:ss is enough.  Does that allow polling, say, every 72 hours?  Just use 72:00:00, right?\n\nRe Winblows problem: I'd like the switch to the current/latest snapshot, but this prevents us from always knowing the location of the active directory.  We'd have to rely on sorting the dir with snapshot names and assuming the currently active index is the one with the most recent snapshot, no?  Symlinks would be great here, but again, Winblows doesn't have them (and I think using shortcuts for this wouldn't work). "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12600827",
            "date": "2008-05-29T16:40:40+0000",
            "content": "Re poll interval: I think the HH:MM:ss is enough. Does that allow polling, say, every 72 hours? Just use 72:00:00, right?\nCorrect, 72:00:00 will work.\n\nRe Winblows problem: I'd like the switch to the current/latest snapshot, but this prevents us from always knowing the location of the active directory. We'd have to rely on sorting the dir with snapshot names and assuming the currently active index is the one with the most recent snapshot, no? Symlinks would be great here, but again, Winblows doesn't have them (and I think using shortcuts for this wouldn't work).\n\nAs Noble suggested, once the new searcher is in use and the older one is closed, hopefully windows will kindly grant us permission to delete the files in the index directory. We can then create links to the files in the snapshot being used into the index directory. The latest snapshot directory will be the active one but we'll know what index is being used through the links in the index folder. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12600850",
            "date": "2008-05-29T17:37:00+0000",
            "content": "windows has symlinks/hardlink. \"fsutil create hardlink \" is the command. It woks well as long as your windows version>win2K\n\n "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12600989",
            "date": "2008-05-30T00:52:55+0000",
            "content": "Is there a reason why IndexDeletionPolicy is not being used?  It allows keeping snapshot files available for replication without creating a specific snapshot directory.  This would be cleaner than creating an external process.   "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12601007",
            "date": "2008-05-30T04:31:47+0000",
            "content": "The strategy of keeping the index directory name hard coded is a bit tricky. We need to do a lot of File System specific jugglery. The best strategy would be.\n\n\tkeep a file index.properties in the data dir\n\tHave an entry currentindex=<new.index> in that file\n\tThis file may keep other extra information if we need it\n\tWhen a new indexsearcher/writer is loaded, read this property and try to load the index from that folder\n\tif it is absent , default to the hardcoded value\n\n\n\nThis way we never need to make hardlinks etc .  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12601184",
            "date": "2008-05-30T16:08:57+0000",
            "content": "Is there a reason why IndexDeletionPolicy is not being used? \n\nRight, that's what I suggested in the initial email thread.\nWith a little more smarts, it seems like the new files could be replicated directly from the master index directory, directly to the slave index directory.  One wouldn't want to copy all the new files though... only those files that are part of the latest index... (hmmm, does Lucene have a way of getting that info?)\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12601276",
            "date": "2008-05-30T22:29:34+0000",
            "content": "Just checked: Lucene's IndexCommit.getFileNames() returns all the files associated with a particular commit point. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12601355",
            "date": "2008-05-31T06:10:00+0000",
            "content": "Yonik: This would be very useful in optimizing the file transfers .We must incorporate this if possible\n\nBTW . What do you recommend for windows Index deletion?. Is the solution proposed by me fine? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12601448",
            "date": "2008-06-01T07:42:29+0000",
            "content": "The first cut. very crude , but worx .No OS specific commands\nThe design for snapshoot , snappull is same as described in the design overview\nsnapinstall is done the following way\n\n\twrite out a file index.properties in dataDirectory.\n\tcall commit command\n\tThe SolrCore is modified. A new method getNewIndexDir() is added. It loads the properties file (if exists, else fall back to the old behavior), read a property 'index'  and return\n\tAny new SolrIndexWriter, SolrIndexSearcher is loaded from the new index dir\n\tOld ones continue to use the old index dir . getindexDir() returns the index dir used by the current SolrIndexReader/SolrIndexWriter\nUse the following configuration\nin master \n\n\n\tregister snapshooter\n\n  <listener event=\"postCommit\" class=\"solr.SnapShooter\">    \n          <bool name=\"wait\">true</bool>\n    </listener>\n\n\n\tregister replication Handler\n\n <requestHandler name=\"/replication\" class=\"solr.ReplicationHandler\" />\n\n\n\tregister a new ResponseWriter\n\n<queryResponseWriter name=\"filestream\" class=\"org.apache.solr.request.CustomBinaryResponseWriter\"/>\n\n\n\n\n\nIn the Slave \n\n\tregister the replication handler\n\n  <requestHandler name=\"/replication\" class=\"solr.ReplicationHandler\" > \n      <str name=\"masterUrl\">http://localhost:8080/solr/replication</str>\n    <str name=\"pollInterval\">00:00:30</str>\n    </requestHandler>  \n\n\n\n\n "
        },
        {
            "author": "Thomas Peuss",
            "id": "comment-12601450",
            "date": "2008-06-01T08:03:18+0000",
            "content": "A library like \"Quartz\" (http://www.opensymphony.com/quartz/) would give you the possibility to provide both types of schedules (HH:MM:SS and cron like). Quartz uses the ASF 2.0 license. So there are at least no licensing issues.\n\nI have not looked at the code but it is possible to do snapshots/snappull at a certain time (e.g. every day 1am)? Quartz would give you the possibility to do that as well. Quartz would even provide scenarios like every 1st monday of the month. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12601451",
            "date": "2008-06-01T08:24:59+0000",
            "content": "Thomas: This is something that can be considered. But , I am still not very convinced that people use it that way. It is going to introduce some dependency on a new library. Let us see if there is enough demand from users\n\nnote :All the operations can be triggerred using http get. So a wget from cron can do the trick in the current form "
        },
        {
            "author": "Thomas Peuss",
            "id": "comment-12601457",
            "date": "2008-06-01T09:35:56+0000",
            "content": "\nThomas: This is something that can be considered. But , I am still not very convinced that people use it that way. It is going to introduce some dependency on a new library. Let us see if there is enough demand from users\n\nThe basic functionality is much more important of course (and much harder to do).\n\n\nnote :All the operations can be triggerred using http get. So a wget from cron can do the trick in the current form\n\nOK. Then ignore my comment.  "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12601604",
            "date": "2008-06-02T11:55:25+0000",
            "content": "This patch includes \n\n\ta new method in ReplicationHandler filechecksum  , which can return the checksums of a given list of files in a snapshot\n\tThe snappuller will request for the checksums of the files if the name and size are same (compared to the current index)\n\tOnly if the checksums are different the file is downloaded\n\tOther files are copied from the current index\n\n\n\n "
        },
        {
            "author": "Andrew Savory",
            "id": "comment-12601659",
            "date": "2008-06-02T16:13:12+0000",
            "content": "Please don't ignore Thomas's repeat suggestion to use Quartz!\n\nHaving replication built-in but then having to use an external cron job to trigger the operations seems suboptimal to me. Being able to configure everything related to replication within the solr deployment seems far more elegant. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12601663",
            "date": "2008-06-02T16:22:35+0000",
            "content": "This feature is far from complete . Enhanced admin features is probably the next priority\nIf scheduling is indeed important we will take it up.  \nMeanwhile we need to ensure that the solution is usable and bug free. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12601775",
            "date": "2008-06-02T22:27:57+0000",
            "content": "If scheduling is indeed important we will take it up.\nMeanwhile we need to ensure that the solution is usable and bug free.\n\nagreed ... the challenge here is (efficient) pure java equivalents of snapshooter/snappuller/snapinstaller ... the scheduling mechanism is largely orthogonal, particularly since Paul is using a \"ReplicationHandler\" as the main API.  it could easily be dealt with later (or in parallel if anyone wants to take on the task)\n\ni don't think the ReplicationHandler should know anything about scheduling or recurance. A generic Scheduling system could be hooked into SolrCore that can hit arbitrary RequestHandlers according to whatever configuration it has (similar to the QuerySendEventListener) which would handle this case, as well as other interesting use cases (ie: rebuild a spelling dictionary using an external datasource every hour, even if the index hasn't changed)\n\nthe scheduling aspect can easily be dealt with later (or in parallel if anyone wants to take on the task) "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12601824",
            "date": "2008-06-03T04:03:48+0000",
            "content": "A generic Scheduling system could be hooked into SolrCore that can hit arbitrary RequestHandlers according to whatever configuration.\nhoss: currently the timer task itself is a part of SnapPuller.java . I endorse your idea of having a scheduling feature built into SolrCore if it is useful to more than one components. As you mentioned every operation is triggerred by the ReplicationHandler's REST API. So if another servive can give a callback at the right time it is the best solution.\n\nthe challenge here is (efficient) pure java equivalents of snapshooter/snappuller/snapinstaller\n\nYes , this indeed is the challenge. I wish people to look into the implementation and comment on how these operations can be made more efficient. I already am thinking of caching the file checksums because there are more than one slaves requesting for the same. \n\nThe other important item that needs review is the changes made to SolrCore.getNewIndexDir() "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12601891",
            "date": "2008-06-03T10:49:44+0000",
            "content": "Should we plan this feature for Solr 1.3 release ?.  If yes, what all are the items pending to be completed? "
        },
        {
            "author": "Andrew Savory",
            "id": "comment-12601951",
            "date": "2008-06-03T14:27:19+0000",
            "content": "I'd certainly like to see this in 1.3, it would make my life easier!\nI'm trying out the code now and hope to feedback in depth soon.\nMeanwhile some initial comments: there's inconsistency between 4 space and 2 space tabs in the code, and a few System.out.println that you probably want to remove or replace with proper logging. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12602536",
            "date": "2008-06-05T04:27:45+0000",
            "content": "The next step is to replicate files in conf folder. \nThre strategy is as follows,\n\n\tMention the files to be replicated from the master, in the ReplicationHandler\n\n  <requestHandler name=\"/replication\" class=\"solr.ReplicationHandler\" > \n      <str name=\"confFiles\">schema.xml,stopwords.txt,elevate.xml</str>  \n    </requestHandler>  \n\n\n\tFor the CMD_FILE_LIST command response include these files also\n\tThe slave can compare the files with its local copy and if it is modified download them\n\tA backup of the current files are taken and the new files are placed into the conf folder\n\tIf a conf file is changed the the SolrCore must be reloaded\n\tThere must be separate strategies for reloading core for single core or multicore\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12603040",
            "date": "2008-06-06T13:34:13+0000",
            "content": "can we make SOLR-551 a subproject of this? "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12603081",
            "date": "2008-06-06T15:44:58+0000",
            "content": "I think so.  You already started doing that with your comment from 04/Jun.\n2 quick thoughts:\n\n\n\tDoes it make sense to support regular expressions in that confFiles file list?  Something a la FileFilter - http://java.sun.com/javase/6/docs/api/java/io/FileFilter.html\n\tI like the backup idea.   As a matter of fact, I'd make backups with timestamps, so we don't have just one backup, but have a bit of history.  Backups > N days old can be periodically deleted either as part of this java-based replication mechanism, or via a cron job\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12603097",
            "date": "2008-06-06T16:59:32+0000",
            "content": "Does it make sense to support regular expressions in that confFiles file list?\nIt;s easy to implement with a wild card . But , very few files need to be replicated .Isn't it  better to explicitly mention the names so that no file accidentally gets replicated. \nI like the backup idea.  As a matter of fact, I'd make backups with timestamps\nYes, timestamps, the same format used by the snapshots\nBackups > N days old can be periodically deleted \nThis is a good idea. It must be a feature of replication. Old conf files as well as indexes should be purged periodically "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12603764",
            "date": "2008-06-10T04:03:52+0000",
            "content": "Attaching deletion_policy.patch\nThis exports a SolrDeletionPolicy via UpdateHandler.getDeletionPolicy()\n\nIt can be used to get the latest SolrIndexCommit, which lists the files that are part of the commit, and can be used to reserve/lease the commit point for a certain amount of time.  This could be used to enable replication directly out of the index directory and avoid copying on systems like Windows.\n\nEach SolrIndexCommit has an id, which can be used by a client as a correlation id.  Since a single file can be part of multiple commit points, a replication client should specify what commit point it is copying.  The server can then look up that commit point and extend the lease. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12603766",
            "date": "2008-06-10T04:23:28+0000",
            "content": "This can be used for a very optimized index copy.  I shall incorporate this also in the next patch. A few points stand out\n\n\tIs it relevant to use this in  a postOptimize? . I guess not\n\tTaking snapshots can serve as a backup . If we adopt this strategy only users will lose that feature of the existing mechanism.\n\tLet us make it configurable for user to choose which strategy he prefers . say <bool name=\"snapshoot\">true</bool> .\n\n "
        },
        {
            "author": "Let me out",
            "id": "comment-12608660",
            "date": "2008-06-27T05:35:20+0000",
            "content": "I'm using Solr to build a search service for my company. From operation or maybe performance point view, we need to use java to replicate index.\n\nFrom very high level, my design is similar to what Noble mentioned here. It is like this:\n\n1) First we have an active master, some standby masters and search slaves. The active master handles crawling data and update index; standby masters are redundant to active master. If active master goes away, one of the standby will become active. Standby masters replicate index from active master to act as backup; search slaves only replicate index from active master.\n\n2) On active master, there is a index snapshots manager. Whenever there's an update, it takes a snapshot. On window, it uses copy (I should try fsutil) and on linux it uses hard link..The snapshot manager also clean up old snapshots. From time to time, I still got index corruption when commit update. When that happen, shapshot manager allows us to rollback to previous good snapshot.\n\n3) On active master, there is a replication server component which listens at a specific port (The reason I did not use http port is I do not use solr as it is. I embed solr in our application server, so go through http would be not very efficient for us). Each standby and slave has replication client component. The following is the protocol between the replication client and server:\n  a) client ping the a directory server for the location of active master\n  b) connect to the active master at the specific port\n  c) handshake: right now just check for version and authentication. in the future, it will negotiate security, compression, etc.\n  d) client sends SNAPSHOT_OPEN command followed by index name. The master could manage multiple indexes. Server sends index_not_found if index does not exist or ok followed by snapshot name of the latest snapshot;\n  e) if the index is found, client compares the timestamp with that of local snapshot. The timestamp of snapshot is derived from snapshot name because part of snapshot name is encoded timestamps. If local is newer, tell the server to close the snapshot; otherwise, ask server for a list of files in the snapshot. If ok, server sends ok op, followed by a file list including filename, timestamp, etc.\n  f) client creates a tmp directory and hard link everything from its local index directory, then for each file in the file list, if it does not exit locally, get new file from server; if it is newer than local one, ask server for update like rsync; if local files do not exist in file list, delete them. in the case of compound file is used for index, the file update will update only diff blocks.\n  g) if everything goes well, tell server to close the snapshot, rename the tmp directory to a proper place, create solr-core using this new index, warmup any cache if necessary, route new request to this solr-core, close old solr-core, remove old index directory.\n\nRight now a client replicates index from active master every 3 mins. for a slow change datasource. It works fine because create new solr-core and warmup cache take less than 3 mins. We plan to use it for a fast changing datasource, so create new solr-core and dump all the cache is not feasible. Any suggestion?  "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12608668",
            "date": "2008-06-27T06:20:29+0000",
            "content": "bq: First we have an active master, some standby masters and search slaves\n\nThis looks like a good approach. In the current design I must allow users to specify multiple 'materUrl' . This must take care of one or more standby masters.  It can automatically fallback to another master if one fails. \n\nOn active master, there is a index snapshots manager. Whenever there's an update, it takes a snapshot. On window, it uses copy (I should try fsutil) and on linux it uses hard link..The snapshot manager also clean up old snapshots. From time to time, I still got index corruption when commit update. When that happen, shapshot manager allows us to rollback to previous good snapshot.\n\nHow can I know if the index got corrupted? if I can know it the best way to implement that would be to add a command to ReplicationHandler to rollback to latest .\n\nOn active master, there is a replication server component which listens at a specific port \nplain socket communication is more work than relying over the simple http protocol .The little extra efficiency you may achieve may not justify that (http is not too solw either). In this case the servlet container provides you with sockets , threads etc etc. Take a look at the patch on how efficiently is it done in the current patch. \n\n\nclient creates a tmp directory and hard link everything from its local index directory, then for each file in the file list, if it does not exit locally, get new file from server; if it is newer than local one, ask server for update like rsync; if local files do not exist in file list, delete them. in the case of compound file is used for index, the file update will update only diff blocks.\nThe current implementation is more or less like what you have done. For a compound file I am not sure if a diff based sync can be more efficient. Because it is hard to get the similar blocks in the file. I rely on checksums  of whole file. If there is an efficient mechanism to obtain identical blocks, share the code I can incorporate that\nThe hardlink approach may be not necessary now as I made the SolrCore not to hardcode the index folder. \n\n\n\n\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12611989",
            "date": "2008-07-09T11:29:46+0000",
            "content": "This patch relies on the IndexDeletionPolicy to identify files to be replicated. It also supports replication of conf files. No need to register any listeners/ QueryResponseWriters\n\nThe configuration is as follows\non master \nsolrconfig.xml\n<requestHandler name=\"/replication\" class=\"solr.ReplicationHandler\" >\n    <lst name=\"master\">\n        <!--Replicate on 'optimize' it can also be  'commit' -->\n        <str name=\"replicateAfter\">commit</str>\n        <!--Config files to be to be replicated-->\n         <str name=\"confFiles\">schema.xml,stopwords.txt,elevate.xml</str>          \n    </lst>\n</requestHandler>\n\n\non slave\nsolrconfig.xml\n<requestHandler name=\"/replication\" class=\"solr.ReplicationHandler\" >\n    <lst name=\"slave\">\n        <str name=\"masterUrl\">http://localhost:port/solr/corename/replication</str>  \n        <str name=\"pollInterval\">00:00:20</str>  \n     </lst>\n</requestHandler>\n\n\n\nThe Replication strategy is changed as follows\n\n\tCMD_INDEX_VERSION: (command=indexversion)gets the version of the current IndexCommit to be replicated from the master. if the version is same, no need to replicate. If it is different\n\tCMD_FILE_LIST : (command=filelist)Get the list of file names for the current IndexCommit . Checks with the local index and identifies modified files by comparing names an sizes. It also returns the details of the conf files\n\tCMD-FILE_CONTENT : (command=filecontent)For each files to be downloaded, issue this command an download the content to a temp folder. After successful completion copy them to the index folder  and isse a commit\n\tIf the current index is stale, or not able to synchronize, copy all the files . An index.properties file is written, which has the location of the new index directory\n\tCoreDescriptor has a new method to reload core.\n\tIf conf files are modified they are copied to the conf folder after taking a backup of the old. Then the core is reloaded\n\n\n\n\n\n\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12614146",
            "date": "2008-07-16T23:13:26+0000",
            "content": "I love how easy this is to set up!\n\nA couple of issues I noticed while testing:\n\n\tit doesn't seem like old files are being removed on the slave for me... actually I think this is related to the fact that I don't see old searchers being cleaned up... my slave currently has 4 open - one for each index version.\n\tsegment_* should be copied last... then if we crash in the middle, everything will work fine... lucene will open the previous index version automatically.\n\tsince a single index file is likely to be part of multiple indicies, commands from the slave to the master to replicate this file should reference the index version being replicated.  This allows time-based reservation of a specific index commit point.\n\n\n\nWhat happens when the slave is replicating an index, and some of the files become missing on the master?  Seems like the slave should simply abandon the current replication effort.  Next time the master is polled, the new index version will be discovered and the process can start again as normal.\n\nWhat happens if replication takes a really long time?  I assume that no new replications will be kicked off until the current one has finished? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12614215",
            "date": "2008-07-17T04:26:32+0000",
            "content": "Thanks.\nI guess Lucene must be cleaning it up because that is what the\ndeletion policy says\nGood point. Will incorporate that\nBecause the file names are unique it did not matter if I used the\nindex version (or does it) . Please clarify\nYeah . It does that . If all the files are not copied completely it aborts.\nYou are right. When the replication process starts , a lock is\nacquired. The lock is released only after the process completes\n\n\n\n\u2013 \n--Noble Paul "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12614218",
            "date": "2008-07-17T04:36:06+0000",
            "content": "it doesn't seem like old files are being removed on the slave for me... actually I think this is related to the fact that I don't see old searchers being cleaned up... my slave currently has 4 open - one for each index version.\n\nOK, I found the bug that caused this one...\nLine SnapPuller.java:172\n  core.getSearcher().get()....\nThat pattern is almost always a bug... getSearcher() returns a RefCounted object that performs the reference counting on the SolrIndexSearcher.  It must be decremented (normally via a finally block).\n\nOh... and more internal code comments would be welcome (I don't know if it's practical to add them after the fact... I find myself adding them for my own notes/thoughts as I develop). "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12614222",
            "date": "2008-07-17T04:56:43+0000",
            "content": "Good catch . but it is not obvious that the refCount was incremented . Should we not have a method to return the searcher without\nincrementing the refcount ? something like SolrCore#getSearcherNoIncRef()\nAnyone who is not using the IndexSearcher for searching will need that "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12614228",
            "date": "2008-07-17T05:25:33+0000",
            "content": "Because the file names are unique it did not matter if I used the index version (or does it).\n\nYes, file names are unique since Lucene doesn't change existing files once they are written.  But, if I completely delete an index and start again, the same file name would be reused with different contents (and a different timestamp).\n\nBut that's not the point I was trying to make...\nIf someone is replicating an older IndexCommit, then we want to extend the lease on it.  In order to do that, we need to know what IndexCommit the client is replicating.  The file name is not enough as a single file is normally part of more than one IndexCommit. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12614230",
            "date": "2008-07-17T05:49:48+0000",
            "content": "If someone is replicating an older IndexCommit, then we want to extend the lease on it. In order to do that, we need to know what IndexCommit the client is replicating. The file name is not enough as a single file is normally part of more than one IndexCommit.\n\nGot the point. I assumed that the SOLR-617 should have enough features to make people configure that. \n\nIt is hard to drive it from the replication handler. The lease can be extended only when we get the onInit() onCommit() callback on SolrIndexDeletionPolicy. We can't reliably expect it to happen during the time of downloading "
        },
        {
            "author": "Guillaume Smet",
            "id": "comment-12614249",
            "date": "2008-07-17T06:56:17+0000",
            "content": "I read the patch quickly. I noticed a small typo in SnapPuller.DFAULT_CHUNK_SIZE (should be DEFAULT).\n\nI like the idea of configuration files replication (yeah, no more scp schema.xml everywhere).\n\nI usually replicate on optimize only but I wonder if people use the current ability to replicate on commit and on optimize. It doesn't seem to be possible with your current patch.\n\nAnyway, really nice work. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12614267",
            "date": "2008-07-17T07:35:47+0000",
            "content": "I usually replicate on optimize only but I wonder if people use the current ability to replicate on commit and on optimize. It doesn't seem to be possible with your current patch.\n\ntechnically it is possible just add two entries for replicateAfter. The code is not handling it because NamedList did not have a getAll() method at that time. The next patch will take care of it "
        },
        {
            "author": "Guillaume Smet",
            "id": "comment-12614276",
            "date": "2008-07-17T09:04:52+0000",
            "content": "The next patch will take care of it\n\nNice.\n\nNoble Paul@06/Jun/08 09:59 AM> It's easy to implement with a wild card . But , very few files need to be replicated .Isn't it better to explicitly mention the names so that no file accidentally gets replicated.\n\nI'm thinking of a use case: if you have a lot of synonym/stopwords dictionaries for different languages and field types, it might be a bit awkward to specify each file. A synonyms_*.txt, stopwords_*.txt would be welcome.\n\nFurthermore, I wonder if we shouldn't disable explicitely the replication of solrconfig.xml. Any opinion? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12614336",
            "date": "2008-07-17T13:51:00+0000",
            "content": "Replication can be disabled by not registering the handler in solrconfig.xml and an HTTP API call should be added to disable replication on master/slave. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12614348",
            "date": "2008-07-17T14:12:29+0000",
            "content": "Could you guys pull out all the changes to MultiCore, CoreDescriptor, SolrCore, etc (everything not related to replication) into a separate patch.  I think that will help things get committed.  Ryan also has a need to get the MultiCore and I think perhaps a getMultiCore() should just be added to the CoreDescriptor. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12614374",
            "date": "2008-07-17T14:53:31+0000",
            "content": "Sure Yonik, we shall separate the changes in core classes into separate issues. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12614378",
            "date": "2008-07-17T15:00:17+0000",
            "content": "Actually there are a handful of other HTTP methods which can be invoked over HTTP. These can be used to control the feature from admin interface\n\n\n\tAbort copying snap from master to slave\ncommand : http://slave_host:port/solr/replication?command=abortsnappull\n\tForce a snapshot on master\ncommand : http://master_host:port/solr/replication?command=snapshoot\n\tForce a snap pull on slave from master\ncommand : http://slave_host:port/solr/replication?command=snappull\n\tDisable polling for snapshot from slave\nommand : http://slave_host:port/solr/replication?command=disablepoll\n\tEnable polling for snapshot from slave\ncommand : http://slave_host:port/solr/replication?command=enablepoll\n\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12614379",
            "date": "2008-07-17T15:04:40+0000",
            "content": "Just the changes required to the core "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12614380",
            "date": "2008-07-17T15:06:14+0000",
            "content": "new patch that takes care of the refcount. this is a complete patch  "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12614760",
            "date": "2008-07-18T15:35:32+0000",
            "content": "I just committed SOLR-638 \u2013 I think this patch depends on that "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12615169",
            "date": "2008-07-21T09:08:09+0000",
            "content": "This patch is to \n\n\tsync with the trunk (SOLR-638 changes)\n\tuses java1.4 ScheduledExcecutorService instead of timer\n\tSolrCore.close() is done in a refcounted way\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12615241",
            "date": "2008-07-21T13:30:30+0000",
            "content": "I haven't had a chance to check out the latest patch, but it sounds like \"SolrCore.close() is done in a refcounted way\" is a generic multi-core change that is potentially sticky enough that it deserves it's own JIRA issue. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12615256",
            "date": "2008-07-21T14:27:59+0000",
            "content": "Yes , we may need another issue to track it. Directly calling Solr.close()   can cause exceptions on in-flight requests "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12615953",
            "date": "2008-07-23T10:59:11+0000",
            "content": "The core reload functionality has to close the old core . "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12618735",
            "date": "2008-07-31T15:58:11+0000",
            "content": "Comment about the solrconfig entry for replication on the master:\n\n\n<requestHandler name=\"/replication\" class=\"solr.ReplicationHandler\" >\n    <lst name=\"master\">\n        <!--Replicate on 'optimize' it can also be  'commit' -->\n        <str name=\"replicateAfter\">commit</str>\n         <str name=\"confFiles\">schema.xml,stopwords.txt,elevate.xml</str>          \n    </lst>\n</requestHandler>\n\n\n \n\nReading the above makes one think that it is the master that does the actual replication.  In fact, the master only creates a snapshot of the index and other files after either commit or optimize.  It is the slaves that copy the snapshots.  So while we refer to the whole process as replication, I think the configuration elements' names should reflect the actual actions to ease understanding and avoid confusion.\n\nConcretely, I think \"replicateAfter\" should be called \"snapshootAfter\" or some such.\n\n+1 for Hoss' suggestion to decouple scheduling from the handler that can replicate/copy on-demand\n+1 for Shalin's suggestion to expose an HTTP interface to enable/disable snapshooting on masters and copying/replication on slaves. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12618775",
            "date": "2008-07-31T17:49:28+0000",
            "content": "Reading the above makes one think that it is the master that does the actual replication. In fact, the master only creates a snapshot of the index \n\nThe new replication does not create snapshots for replication. The replication is done from/to a live index. Hence the change in name "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12626079",
            "date": "2008-08-27T11:23:11+0000",
            "content": "This patch contains only replication related changes. It depends on SOLR-658 and SOLR-617 and must be applied after the patches in those issues. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12626080",
            "date": "2008-08-27T11:28:10+0000",
            "content": "This patch contains all the replication related changes as well as changes in Solr itself. It contains the changes in SOLR-617 and SOLR-658.\n\nUn-tested version. I just compiled all the various patches and brought them up to trunk. "
        },
        {
            "author": "Akshay K. Ukey",
            "id": "comment-12629123",
            "date": "2008-09-08T10:46:19+0000",
            "content": "Patch in sync with the trunk, includes changes for SOLR-617 and SOLR-658. "
        },
        {
            "author": "Akshay K. Ukey",
            "id": "comment-12630559",
            "date": "2008-09-12T12:39:51+0000",
            "content": "Full patch with:\n1. Support for reserving commit point. Configurable with commitReserveDuration configuration in ReplicationHandler section.\n\n<requestHandler name=\"/replication\" class=\"solr.ReplicationHandler\" >\n    <lst name=\"master\">\n        <str name=\"replicateAfter\">commit</str>\n         <str name=\"confFiles\">schema.xml,stopwords.txt,elevate.xml</str>\n\t\t <str name=\"commitReserveDuration\">01:00:00</str>\n    </lst>\n</requestHandler>\n\n\n\n2. Admin page for displaying replication details.\n3. Combines changes for SOLR 617 and SOLR 658. "
        },
        {
            "author": "Akshay K. Ukey",
            "id": "comment-12638511",
            "date": "2008-10-10T09:02:52+0000",
            "content": "Patch includes changes for SOLR-658 and SOLR-561.\nChanges:\nAbort command implementation in ReplicationHandler to abort ongoing replication.\nMore information displayed in the replication admin page, such as file being copied, time elapsed, time remaining, size of file downloaded and so on. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12639406",
            "date": "2008-10-14T13:02:31+0000",
            "content": "Why are files downloaded to a temp directory first?  Since all index files are versioned, would it make sense to copy directly into the index dir (provided you copy segments_n last)? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12639465",
            "date": "2008-10-14T16:00:32+0000",
            "content": "Why are files downloaded to a temp directory first? \nIf Solr crashes while downloading that will leave unnecessary/incomplete files in the index directory. We did not want the index directory to be polluted. The files are 'moved' to index directory after they are downloaded . \n\nThe segments_n file is copied in the end. from temp directory to index directory. \n(OK , that patch is coming  ) "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12639493",
            "date": "2008-10-14T17:05:15+0000",
            "content": "If Solr crashes while downloading that will leave unnecessary/incomplete files in the index directory.\n\nIf we don't want to try and pick up from where we left off, it seems like Lucene's deletion policy can clean up old index files that are unreferenced.\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12639518",
            "date": "2008-10-14T18:06:02+0000",
            "content": "If the files are not part of any indexcommit (this is true if the segments_n file didn't get downloaded) will it still clean it up?.  And when solr restarts ReplicationHandler will have difficulty in cleaning up those files if replication kicks off before Lucene cleans it up (If it actually does that)\n "
        },
        {
            "author": "Akshay K. Ukey",
            "id": "comment-12640119",
            "date": "2008-10-16T09:53:26+0000",
            "content": "Patch with following changes:\n\n\tsegments_ file moved in the end.\n\tSome minor changes in replication admin page\n\tTest case for index and config files replication.\n\tSome minor bug fixes.\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12640450",
            "date": "2008-10-17T06:22:46+0000",
            "content": "Thanks Akshay.\n\nOn a first glance, this is looking really good. I am planning to commit this in a few days. We can take up the enhancements or bug fixes through new issues. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12640939",
            "date": "2008-10-20T07:06:32+0000",
            "content": "Updated patch with a couple of bug fixes related to closing connections and refcounted index searcher. Other cosmetic changes include code formatting and javadocs.\n\nNoble has put up a wiki page at http://wiki.apache.org/solr/SolrReplication detailing the features and configuration. "
        },
        {
            "author": "Akshay K. Ukey",
            "id": "comment-12640976",
            "date": "2008-10-20T11:06:23+0000",
            "content": "Patch with minor fixes related to the admin page. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12640999",
            "date": "2008-10-20T12:52:35+0000",
            "content": "Another iteration over Akshay's patch.\n\n\n\tMade the collections used for keeping statistics synchronized to avoid concurrent modification exceptions.\n\tRemoved @author tags and put @version and @since 1.4 tags\n\n "
        },
        {
            "author": "Akshay K. Ukey",
            "id": "comment-12641295",
            "date": "2008-10-21T06:57:12+0000",
            "content": "Again a minor fix in replication admin page "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12641321",
            "date": "2008-10-21T09:45:22+0000",
            "content": "Committed revision 706565.\n\nThanks Noble, Yonik and Akshay! "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12641412",
            "date": "2008-10-21T15:43:39+0000",
            "content": "Snappuller should use getNewestSearcher() rather than getSearcher() to avoid pulling the same snapshot more than once if warming takes a long time. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12641418",
            "date": "2008-10-21T16:06:26+0000",
            "content": "I didn't catch earlier how reservations were done: currently, the commit point is reserved for a certain time when the file list is initially fetched.  This requires that the user estimate how long a snap pull will last, and if they get it wrong things will fail.  On the other side, setting the time high requires more free disk space.\n\nIt seems like renewing a lease (a short term reservation) whenever an access is done would solve both of these problems (and is what I initially had in mind).  All requests should indicate what commit point is being copied so that the lease can be extended. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12641436",
            "date": "2008-10-21T17:03:18+0000",
            "content": "Files are downloaded in one HTTP request... the response is read and written one chunk at a time.  Has anyone tested this with a large files (say 5G or more) to ensure that:\n\n\tthe response is correctly streamed (not buffered) as it is written to the socket?\n\tthe servlet container can handle sending responses of that size\n\tthe servlet container won't time out (test big file over slow connection)\n\tthe client side (HTTPClient) doesn't buffer the response, can handle the big size, and won't time out.\n\n\n\nThe first 3 go through servlet container code and thus should probably be tested with tomcat, jetty, and resin. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12641467",
            "date": "2008-10-21T18:22:18+0000",
            "content": "\n\twe did extensive testing with very large index around 7GB with retries also (for failed connections)\n\tTest was conducted on both on jetty and tomcat\n\tPerformance of new replication~= rsync based replication. The replication speed is largely network IO bound\n\n\n\nthe servlet container can handle sending responses of that size\nThe servlet container usually have a small chunk size by default(~8KB(in tomcat). It keeps flushing the stream after that size is crossed.\n\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12641468",
            "date": "2008-10-21T18:27:19+0000",
            "content": "All requests should indicate what commit point is being copied so that the lease can be extended.\n\nThis is a good idea . But when the index is large it tends to have 1 very large file and a few other smaller files. It is that very large file that takes a lot of time(In our case a 6GB file across data centers took around 2 hrs) So we may also need to do call reserve even while download is going on. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12641495",
            "date": "2008-10-21T18:49:05+0000",
            "content": "Thanks for going through this Yonik.\n\nSnappuller should use getNewestSearcher() rather than getSearcher() to avoid pulling the same snapshot more than once if warming takes a long time.\nThe SnapPuller calls commit with waitSearcher=true, so the call will wait for the searcher to get registered and warmed. The reentrant lock in SnapPuller will be released only after the commit call returns. So it should be OK, right?\n\nIt seems like renewing a lease (a short term reservation) whenever an access is done would solve both of these problems (and is what I initially had in mind). All requests should indicate what commit point is being copied so that the lease can be extended.\nSince files are transferred in one go, the master knows about the access time but it does not know if the transfer has ended so the lease may expire in between the transfer leading to a failure. We'll need to track the transfers individually as well. If the slave dies in between the transfer, we'll need to track that as well and time-out the lease appropriately. If I compare the state of things to the old way of replication, not sure if this feature is worth the effort. What do you think?\n\nHas anyone tested this with a large files (say 5G or more)...\nWe have been testing it with a large index (wikipedia articles, around 7-8GB index on disk) with Tomcat across networks (transfer rate between servers is around 700-800 KB/sec). We haven't seen any problem yet. We'll continue to test this with Tomcat and other containers and report performance numbers and problems, if any. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12641543",
            "date": "2008-10-21T19:16:11+0000",
            "content": "The SnapPuller calls commit with waitSearcher=true, so the call will wait for the searcher to get registered and warmed.\n\nA commit could come from somewhere else though, or we could be starting up and no searcher is yet registered.  It's always safe (and clearer) to just use the newest reader opened, right?\n\nIs there a reason that SnapPuller waits for the new searcher to be registered?\nIf not, I'll change this... I'm currently creating a patch with some little thread-safety fixes. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12641550",
            "date": "2008-10-21T19:30:12+0000",
            "content": "Since files are transferred in one go, the master knows about the access time but it does not know if the transfer has ended so the lease may expire in between the transfer leading to a failure.\n\nRight, as Noble pointed out, lease extension will need to be done periodically during the download (every N blocks written to the socket).\n\nWe'll need to track the transfers individually as well.\n\nEach file request can optionally specify the commit point it is copying.\n\nIf the slave dies in between the transfer, we'll need to track that as well and time-out the lease appropriately.\n\nThe lease is just the current reservation mechanism, but called more often and with a very short reservation (on the order of seconds, not minutes I would think), so I don't see a need to time them out.\n\nWe have been testing it with a large index (wikipedia articles, around 7-8GB index on disk)\n\nCool.  Hopefully one of the test indexes contain a single file greater than 4G to test that we don't hit any 32 bit overflow in the stack.  If not, re-doing your wikipedia test with compound index format and after an optimize should do the trick. "
        },
        {
            "author": "Akshay K. Ukey",
            "id": "comment-12641712",
            "date": "2008-10-22T04:26:30+0000",
            "content": "Cool. Hopefully one of the test indexes contain a single file greater than 4G to test that we don't hit any 32 bit overflow in the stack. If not, re-doing your wikipedia test with compound index format and after an optimize should do the trick.\n\nYes, one of the files in the index is of size 6.3G, created on optimize. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12641713",
            "date": "2008-10-22T04:30:21+0000",
            "content": "patch contains changes for reserve being set for 10secs by default after  every 5 packets (5 MB) are written.\n\nThe commitReserveDuration is now supposed to be a small value (default is 10 secs). If the network is particularly slow user can tweak it to set a bigger number.\n\nevery command for fetching file content has an extra attribute indexversion , so that the master now knows which IndexCommit is being downloaded. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12641976",
            "date": "2008-10-22T21:50:34+0000",
            "content": "Thanks Noble, reviewing now... "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12642026",
            "date": "2008-10-23T01:00:36+0000",
            "content": "Committed with 2 changes:\n\n\tgetSearcher() isn't allowed in inform() so I changed to getNewestSearcher()\n\tchanged cleanReserves() to not collect ids to delete in a separate list (not needed for ConcurrentHashMap)\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12642050",
            "date": "2008-10-23T04:24:09+0000",
            "content": "silly me. The packetsWriitten variable was not incremented "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12642204",
            "date": "2008-10-23T17:12:21+0000",
            "content": "Attaching some little thread safety fixes (mostly adding volatile to values modified and read from different threads). "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12642215",
            "date": "2008-10-23T17:54:18+0000",
            "content": "Updated the fixes patch with more thread safety fixes.\n\nQ: what is ReplicationHandler.getIndexVersion() supposed to return, and why?  It currently returns the version of the visible index (registered).  Should it be the most recent version of the index we have?  Any reason it isn't using ReplicationHandler.indexCommitPoint?\n\nAlso, I think we should all work at adding more comments to code as it is written.  Lack of comments made this patch harder to review. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12642217",
            "date": "2008-10-23T18:06:56+0000",
            "content": "I think there's an issue with SnapShooter in that it never does any reservations for the commit point it's trying to copy. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12642359",
            "date": "2008-10-24T01:15:57+0000",
            "content": "Here's an update to the \"fixes\" patch that fixes an issue with setReserveDuration when called with different reserveTimes.  Previously, the new value overwrites the old, regardless of it's value.  The approach to fix is a basic spin loop (see below).  Anyone see issues with this approach?\n\n\n  public void setReserveDuration(Long indexVersion, long reserveTime) {\n    long timeToSet = System.currentTimeMillis() + reserveTime;\n    for(;;) {\n      Long previousTime = reserves.put(indexVersion, timeToSet);\n\n      // this is the common success case: the older time didn't exist, or\n      // came before the new time.\n      if (previousTime == null || previousTime <= timeToSet) break;\n\n      // At this point, we overwrote a longer reservation, so we want to restore the older one.\n      // the problem is that an even longer reservation may come in concurrently\n      // and we don't want to overwrite that one too.  We simply keep retrying in a loop\n      // with the maximum time value we have seen.\n      timeToSet = previousTime;      \n    }\n  }\n\n\n\nI think this is also a great example of where comments explaining how things work are really needed. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12642380",
            "date": "2008-10-24T05:06:51+0000",
            "content": "what is ReplicationHandler.getIndexVersion() supposed to return, and why?\n\nThis is the method called by the slaves. they must only see the current \"replicatable\" index version. For instance, if the 'replicateAfter' is set to 'optimize' then the slave should not see the index version that is a commit.\n\nThe getDetails() ( command=details) method gives the actual current index version\n\nI think there's an issue with SnapShooter in that it never does any reservations for the commit point it's trying to copy. \n\nright, Snaphsooter has to reserve.\n\nThe new setReserveDuration() looks right. \n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12642389",
            "date": "2008-10-24T06:31:58+0000",
            "content": "The SnapShooter is not written right (thread safety). soon after you commit the patch , I can give a patch . After I fix it I can update the wiki w/ proper documentation. \n\nSnapshoot is not a very important feature in  the current scheme of things. It is useful only if somebody wants to do periodic backups\n\nShould we try OS specific copy? \nHardlinks can be used in *nix and Windows can also do hardlinks if fsutils is present. If not ,we can do a proper copy "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12642449",
            "date": "2008-10-24T11:48:11+0000",
            "content": "Yonik. If you can commit this patch I can give a patch with comments . The code badly needs some comments.  "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12643172",
            "date": "2008-10-28T09:57:55+0000",
            "content": "comments only "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12643214",
            "date": "2008-10-28T14:07:28+0000",
            "content": "Comments committed. Thanks! "
        },
        {
            "author": "Patrick Eger",
            "id": "comment-12645632",
            "date": "2008-11-07T00:17:49+0000",
            "content": "Hi, i have a couple comments about the implementation, specifically SnapShooter.java just pulled from TRUNK:\n\n-------------------------------------\ncreateSnapshot() uses the following pattern, which seems unreliable to me, under the prospect of concurrent snapshot requests: \n\nlockFile = new File(snapDir, directoryName + \".lock\");\n      if (lockFile.exists()) \n{\n        return;\n      }\n\n... <1> ...\n\nlockFile.createNewFile();\n\n... <2> ...\n\nif (lockFile != null) \n{\n        lockFile.delete();\n      }\n\nAFAIK, java.nio.channels.FileLock should be used for any type of file-based locking of the sort for cross-vm synchronization. If you are worried about in-vm synchronization, it might be best to just use j.u.c Locks or synchronized{} blocks. This would remove the possiblity of junk .lock files if, say the VM dies during <2>.\n\n-------------------------------------\nAdditionally, these lines seem suspect to me. transferTo() needs to be done in a loop for the full copy to work. \n\n      fis = new FileInputStream(file);\n      File destFile = new File(toDir, file.getName());\n      fos = new FileOutputStream(destFile);\n      fis.getChannel().transferTo(0, fis.available(), fos.getChannel());\n      destFile.setLastModified(file.lastModified());\n\n\n\nAm i crazy or are these real problems? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12645647",
            "date": "2008-11-07T00:57:23+0000",
            "content": "Am i crazy or are these real problems?\n\nRight, as Noble & I noted, there are still known problems with SnapShooter.  Luckily, it's not necessary in the current replication scheme which no longer relies on snapshots. "
        },
        {
            "author": "Patrick Eger",
            "id": "comment-12645650",
            "date": "2008-11-07T01:01:54+0000",
            "content": "Gotcha, i will focus efforts elsewhere then  "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12645677",
            "date": "2008-11-07T03:40:46+0000",
            "content": "We need to cleanup the SnapShooter. it was given low priority because\nsnapshoot is not at all necessary in the new replication\nimplementation. It is only useful for periodic backups\n\n\n\n\n\u2013 \n--Noble Paul "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12646968",
            "date": "2008-11-12T17:38:54+0000",
            "content": "I wonder if it might be useful to add copy throttle support to the replication.  See SOLR-849 and the referenced email thread. "
        },
        {
            "author": "Bill Bell",
            "id": "comment-12755894",
            "date": "2009-09-16T06:06:59+0000",
            "content": "I am not a huge fan of PollInterval. It would be great to add an option to get the Index based on exact time: PollTime=\"*/15 * * * *\" That would run at every 15 minutes based on the clock. i.e. 1:00pm, 1:15pm, 1:30pm, 1:45pm, etc.  All my slaves are sync'd using NTP, so this would work better. Since each slave starts differently, we cannot set the PollInterval=\"00:15:00\" since they would get different indexes based on when they start. The other option would be to suspend polling - and start - which would be very manual I guess. Setting the PollInterval to 10 seconds would be getting a new index when the old one is still warming up. Even 10 seconds interval would not be good, since we get so many updates, each server would have different indexes. With Snap we don't have this issue.\n\nWe get SOLR updates frequently and since they are large we cannot wait to do a commit at the 15 minute mark using cron. Optimize just takes too long.\n\nOn our system we need to limit how often the slaves get the new index. We would like all slaves to get the index at the same time.\n\nBill "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12755908",
            "date": "2009-09-16T06:50:26+0000",
            "content": "The default pollInterval can behave the vway you want (so that the fetches are synchronized in time by the clock). Raise a separate issue and we can fix it "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12769584",
            "date": "2009-10-24T04:54:08+0000",
            "content": "change component from scripts to java "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12775510",
            "date": "2009-11-10T15:51:42+0000",
            "content": "Bulk close for Solr 1.4 "
        }
    ]
}