{
    "id": "SOLR-10751",
    "title": "Master/Slave IndexVersion conflict",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "7.0",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "I\u2019ve been looking at some failures in the replica types tests. One strange failure I noticed is, master and slave share the same version, but have different generation. The IndexFetcher code does more or less this:\n\nmasterVersion = fetchMasterVersion()\nmasterGeneration = fetchMasterGeneration()\n\nif (masterVersion == 0 && slaveGeneration != 0 && forceReplication) {\n   delete my index\n   commit locally\n   return\n} \nif (masterVersion != slaveVersion) {\n  fetchIndexFromMaster(masterGeneration)\n} else {\n  //do nothing, master and slave are in sync.\n}\n\n\nThe problem I see happens with this sequence of events:\n\ndelete index in master (not a DBQ=*:*, I mean a complete removal of the index files and reload of the core)\nreplication happens in slave (sees a version 0, deletes local index and commit)\nadd document in master and commit\n\nif the commit in master and in the slave happen at the same millisecond*, they both end up with the same version, but different indices. \nI think that in addition of checking for the same version, we should validate that slave and master have the same generation and If not, consider them not in sync, and proceed to the replication.\nTrue, this is a situation that's difficult to happen in a real prod environment and it's more likely to affect tests, but I think the change makes sense.",
    "attachments": {
        "SOLR-10751.patch": "https://issues.apache.org/jira/secure/attachment/12869938/SOLR-10751.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-05-25T21:14:51+0000",
            "content": "Hoss Man and I had a conversation about this on IRC yesterday, and his concern was \"Why is master creating an index with version 0 and the slave is not\". After investigating some more, I noticed this code in the ReplicationHandler\n\nif (commitPoint != null && replicationEnabled.get()) {\n        //\n        // There is a race condition here.  The commit point may be changed / deleted by the time\n        // we get around to reserving it.  This is a very small window though, and should not result\n        // in a catastrophic failure, but will result in the client getting an empty file list for\n        // the CMD_GET_FILE_LIST command.\n        //\n        core.getDeletionPolicy().setReserveDuration(commitPoint.getGeneration(), reserveCommitDuration);\n        rsp.add(CMD_INDEX_VERSION, IndexDeletionPolicyWrapper.getCommitTimestamp(commitPoint));\n        rsp.add(GENERATION, commitPoint.getGeneration());\n      } else {\n        // This happens when replication is not configured to happen after startup and no commit/optimize\n        // has happened yet.\n        rsp.add(CMD_INDEX_VERSION, 0L);\n        rsp.add(GENERATION, 0L);\n      }\n\n\nso, \"0\" is not really the version of the index, but it's that the master responds to the slaves when there is no replicable index.  ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16025410"
        },
        {
            "date": "2017-05-26T18:13:57+0000",
            "content": "so, \"0\" is not really the version of the index, but it's that the master responds to the slaves when there is no replicable index. \n\nAnd to elaborate on our IRC conversation, at the point where we were theorizing why the master might return \"0\" (before tomas had found this particular bit of code and verified it matched our theory) \u2013 i posed the following straw man suggestion(s) for dealing with this special \"sentinal\" value of \"i have no index\"...\n\n\n\tWe could change solr core/updateHanlder initialization so there is never a situation where a solr core is responding to requests, but has no index / commitPoint \u2013 thus completely eliminating the need for the sentinal value & special case logic on slaves, because they will always have something they can fetch\n\t\n\t\tie: on startup, if no index, create & commit immediately\n\t\n\t\n\twe could \"fix\" the semantics of replication on the slave side...\n\t\n\t\tif the master returns indexVersion==0, the slave treats that as a \"master has nothing to replicate, i should do nothing\" (and possibly 'fail' if the replication was explicitly requested vs/timmer based)\n\t\tas opposed to current logic which is \"master has nothing to replicate, i will blindly create my own arbitrary index indepdent of master (via deleteAll)\n\t\n\t\n\n\n\nI still think either one of these options would be a good idea \u2013 depending on what we want the semantics to be:  \n\n\n\tShould a situation where an external force blows away the master index (or someone forces a node w/o an index to be a leader) cause slaves/replicas to immediately purge all data?\n\tOr should slaves/replicas keep what they've got until the master/leader actually has something for them to replicate?\n\n\n\nPersonally i think #2 makes more sense.\n\nAs a practical example: Assume someone is doing classic master/slave replication and their master has a hardware failure.  the slaves are still serving queries just fine.  rather then swap out an existing slave to be the new master the admin creates an entirely new serve to be the master and plans on rebuilding the index \u2013 but by reusing the master.company.com hostname, the new node starts recieving /replication requests immediately from the existing slaves.  Should those slaves really be immediately deleting all docs from their local indexes even though the master is explicitly telling them \"i have nothing for you to replicate\" ? ... that sounds like a bug to me.\n\nOn the flip side: if chaos has rained down on a SolrCloud cluster, and a new leader w/o any index at all has popped up \u2013 i think it's \"ok\" for replicas to serve stale data until the leader has new data for them ... but if think that in the cloud case it's important that all replicas should immediately \"recover\" the \"theoretically empty if it did exist\" version of the index from their leader, then perhaps the leader election code should involve a special case to force a commit on the leader if it has no existing commit points? \n\n\n\nEither way, i ALSO have the vague impression that tomas's primary suggestion of always checking generation is correct as well ... but it seems so obvious i'm not sure if there is some good why the code doesn't already do that that i'm oblivious too? ",
            "author": "Hoss Man",
            "id": "comment-16026620"
        },
        {
            "date": "2017-05-26T19:10:37+0000",
            "content": "OK, I  see now why this hasn't been a problem so far. Note that the \"delete my index\" only happens in case of a \"forced replication\". Forced replications in Master/Slave can only happen in a retry, which should not happen if the master is returning version 0 (unless I'm misunderstanding something here, this code should never be executed if you are running Master/Slave). In SolrCloud mode, a forced replication can happen if the last attempt to replicate was unsuccessful. Until now the replication in SolrCloud was only for recovery, and Cloud mode it's \"OK\" to have different versions of the index, plus, in the particular test example I described in the issue, the replication would have been followed by the application of the buffered updates, so indices would be soon in sync. This becomes an issue only now that we have TLOG and PULL replicas.\n\nIn any case, we need to fix it now for the new scenario. I also like your #2 option (#1 sounds like too big of a change), and It should be easy to implement, although NRT replicas still need this logic I believe.  ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16026695"
        }
    ]
}