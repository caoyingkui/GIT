{
    "id": "LUCENE-1476",
    "title": "BitVector implement DocIdSet, IndexReader returns DocIdSet deleted docs",
    "details": {
        "labels": "",
        "priority": "Trivial",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "2.4",
        "resolution": "Won't Fix",
        "status": "Closed"
    },
    "description": "Update BitVector to implement DocIdSet.  Expose deleted docs DocIdSet from IndexReader.",
    "attachments": {
        "sortBench2.py": "https://issues.apache.org/jira/secure/attachment/12399028/sortBench2.py",
        "searchdeletes.alg": "https://issues.apache.org/jira/secure/attachment/12398605/searchdeletes.alg",
        "TestDeletesDocIdSet.java": "https://issues.apache.org/jira/secure/attachment/12398938/TestDeletesDocIdSet.java",
        "sortCollate2.py": "https://issues.apache.org/jira/secure/attachment/12399029/sortCollate2.py",
        "LUCENE-1476.patch": "https://issues.apache.org/jira/secure/attachment/12395236/LUCENE-1476.patch",
        "quasi_iterator_deletions_r3.diff": "https://issues.apache.org/jira/secure/attachment/12399050/quasi_iterator_deletions_r3.diff",
        "quasi_iterator_deletions_r2.diff": "https://issues.apache.org/jira/secure/attachment/12398365/quasi_iterator_deletions_r2.diff",
        "quasi_iterator_deletions.diff": "https://issues.apache.org/jira/secure/attachment/12397479/quasi_iterator_deletions.diff",
        "hacked-deliterator.patch": "https://issues.apache.org/jira/secure/attachment/12399117/hacked-deliterator.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-12-04T00:00:47+0000",
            "content": "LUCENE-1476.patch\n\nBitVector extends DocIdSet.  \n\nTestBitVector implements testDocIdSet method that is based on TestSortedVIntList tests  ",
            "author": "Jason Rutherglen",
            "id": "comment-12653069"
        },
        {
            "date": "2008-12-04T00:00:53+0000",
            "content": "But, SegmentReader needs random access to the bits (DocIdSet only provides an iterator)? ",
            "author": "Michael McCandless",
            "id": "comment-12653070"
        },
        {
            "date": "2008-12-04T00:09:12+0000",
            "content": "Looks like we need a new abstract class.  RABitSet?   ",
            "author": "Jason Rutherglen",
            "id": "comment-12653075"
        },
        {
            "date": "2008-12-04T00:11:31+0000",
            "content": "BitSet is already random access, DocIdSet is not. ",
            "author": "robert engels",
            "id": "comment-12653076"
        },
        {
            "date": "2008-12-04T00:16:09+0000",
            "content": "BitVector does not implement the methods of java.util.BitSet.  RABitSet could be implemented by OpenBitSet and BitVector.  This way an OpenBitSet or another filter such as P4Delta could be used in place of BitVector in SegmentReader.  \n\nThe IndexReader.flush type of methods would need to either automatically not save, throw an exception, and there needs to be a setting.  This helps the synchronization issue in SegmentReader.isDeleted by allowing access to it.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12653080"
        },
        {
            "date": "2008-12-05T11:29:45+0000",
            "content": "But, SegmentReader needs random access to the bits (DocIdSet only provides an iterator)? \n\nAlthough IndexReader.isDeleted exposes a random-access API to deleted docs, I think it may be overkill.\n\nIe, in most (all?) uses of deleted docs throughout Lucene core/contrib, a simple iterator (DocIdSet) would in fact suffice.\n\nEG in SegmentTermDocs iteration we are always checking deletedDocs by ascending docID.  It might be a performance gain (pure speculation) if we used an iterator API, because we could hold \"nextDelDocID\" and only advance that (skipTo) when the term's docID has moved past it.  It's just like an \"AND NOT X\" clause.\n\nSimilarly, norms, which also now expose a random-access API, should be fine with an iterator type API as well.\n\nThis may also imply better VM behavior, since we don't actually require norms/deletions to be fully memory resident.\n\nThis would be a biggish change, and it's not clear whether/when we should explore it, but I wanted to get the idea out there.\n\nMarvin, in KS/Lucy are you using random-access or iterator to access deletedDocs & norms? ",
            "author": "Michael McCandless",
            "id": "comment-12653752"
        },
        {
            "date": "2008-12-05T14:21:31+0000",
            "content": "I don't think you can change this...\n\nIn many cases after you have read an index, and retrieved document numbers, these are lazily returned to the client.\n\nBy the time some records are needed to be read, they may have already been deleted (at least this was the usage in old lucene, where deletions happened in the reader).\n\nI think a lot of code assumes this, and calls the isDeleted() to ensure the document is still valid. ",
            "author": "robert engels",
            "id": "comment-12653793"
        },
        {
            "date": "2008-12-05T18:22:40+0000",
            "content": "> Marvin, in KS/Lucy are you using random-access or iterator to access \n> deletedDocs & norms?\n\nBoth. There's a DelEnum class which is used by NOTScorer and MatchAllScorer, but it's implemented using BitVectors which get the next deleted doc num by calling nextSetBit() internally. \n\n I happened to be coding up those classes this spring when there was the big brouhaha about IndexReader.isDeleted().  It seemed wrong to pay the method call overhead for IndexReader.isDeleted() on each iter in NOTScorer.next() or MatchAllScorer.next(), when we could just store the next deletion:\n\n\ni32_t\nMatchAllScorer_next(MatchAllScorer* self) \n{\n    do {\n        if (++self->doc_num > self->max_docs) {\n            self->doc_num--;\n            return 0;\n        }\n        if (self->doc_num > self->next_deletion) {\n            self->next_deletion \n                = DelEnum_Skip_To(self->del_enum, self->doc_num);\n        }\n    } while (self->doc_num == self->next_deletion);\n    return self->doc_num;\n}\n\n\n\n(Note: Scorer.next() in KS returns the document number; doc nums start at 1, and 0 is the sentinel signaling iterator termination. I expect that Lucy will be the same.)\n\nPerhaps we could get away without needing the random access, but that's because IndexReader.isDeleted() isn't exposed and because IndexReader.fetchDoc(int docNum) returns the doc even if it's deleted \u2013 unlike Lucene which throws an exception. Also, you can't delete documents against an IndexReader, so Robert's objection doesn't apply to us.\n\nI had always assumed we were going to have to expose isDeleted() eventually, but maybe we can get away with zapping it. Interesting!\n\nI've actually been trying to figure out a new design for deletions because writing them out for big segments is our last big write bottleneck, now that we've theoretically solved the sort cache warming issue.  I figured we would continue to need bit-vector files because they're straightforward to mmap, but if we only need iterator access, we can use vbyte encoding instead... Hmm, we still face the problem of outsized write cost when a segment has a large number of deletions and you add one more... ",
            "author": "Marvin Humphrey",
            "id": "comment-12653883"
        },
        {
            "date": "2008-12-05T18:42:51+0000",
            "content": "\nIt seemed wrong to pay the method call overhead for IndexReader.isDeleted() on each iter in NOTScorer.next() or MatchAllScorer.next(), when we could just store the next deletion:\n\nNice!  This is what I had in mind.\n\nI think we could [almost] do this across the board for Lucene.\nSegmentTermDocs would similarly store nextDeleted and apply the same\n\"AND NOT\" logic.\n\nthat's because IndexReader.isDeleted() isn't exposed and because IndexReader.fetchDoc(int docNum) returns the doc even if it's deleted\n\nHmm \u2013 that is very nicely enabling.\n\nI've actually been trying to figure out a new design for deletions because writing them out for big segments is our last big write bottleneck\n\nOne approach would be to use a \"segmented\" model.  IE, if a few\ndeletions are added, write that to a new \"deletes segment\", ie a\nsingle \"normal segment\" would then have multiple deletion files\nassociated with it.  These would have to be merged (iterator) when\nused during searching, and, periodically coalesced.\n\nif we only need iterator access, we can use vbyte encoding instead\n\nRight: if there are relatively few deletes against a segment, encoding\nthe \"on bits\" directly (or deltas) should be a decent win since\niteration is much faster. ",
            "author": "Michael McCandless",
            "id": "comment-12653889"
        },
        {
            "date": "2008-12-05T18:48:35+0000",
            "content": "\n\nIn many cases after you have read an index, and retrieved document numbers, these are lazily returned to the client.\n\nBy the time some records are needed to be read, they may have already been deleted (at least this was the usage in old lucene, where deletions happened in the reader).\n\nI think a lot of code assumes this, and calls the isDeleted() to ensure the document is still valid.\n\nBut isn't that an uncommon use case?  It's dangerous to wait a long\ntime after getting a docID from a reader, before looking up the\ndocument.  Most apps pull the doc right away, send it to the user, and\nthe docID isn't kept (I think?).\n\nBut still I agree: we can't eliminate random access to isDeleted\nentirely.  We'd still have to offer it for such external cases.\n\nI'm just saying the internal uses of isDeleted could all be switched\nto iteration instead, and, we might get some performance gains from\nit especially when the number of deletes on a segment is relatively low. ",
            "author": "Michael McCandless",
            "id": "comment-12653891"
        },
        {
            "date": "2008-12-05T19:55:23+0000",
            "content": "but IndexReader.document throws an exception if the document is deleted...0 so you still need random access ",
            "author": "robert engels",
            "id": "comment-12653912"
        },
        {
            "date": "2008-12-05T20:01:51+0000",
            "content": "but IndexReader.document throws an exception if the document is deleted...0 so you still need random access \n\nDoes it really need to throw an exception?  (Of course for back compat it does, but we could move away from that to a new method that doesn't check). ",
            "author": "Michael McCandless",
            "id": "comment-12653915"
        },
        {
            "date": "2008-12-05T20:51:58+0000",
            "content": "It would be great if instead of relying on Lucene to manage the deletedDocs file, the API would be pluggable enough such that a DocIdBitSet (DocIdSet with random access) could be set in a SegmentReader, and the file access (reading and writing) could be managed from outside.  Of course this is difficult to do and still make things backwards compatible, however for 3.0 I would really like to be a part of efforts to create a completely generic and pluggable API that is cleanly separated from the underlying index format and files.  This would mean that the analyzing, querying, scoring portions of Lucene could access an IndexReader like pluggable class where the underlying index files, when and how the index files are written to disk is completely separated.  \n\nOne motivation for this patch is to allow custom queries access to the deletedDocs in a clean way (meaning not needing to be a part of the o.a.l.s. package) \n\nI am wondering if it is good to try to get IndexReader.clone working again, or if there is some other better way related to this patch to externally manage the deletedDocs.  \n\nImproving the performance of deletedDocs would help for every query so it's worth looking at.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12653923"
        },
        {
            "date": "2008-12-05T21:55:30+0000",
            "content": "> Does it really need to throw an exception?\n\nAside from back compat, I don't see why it would need to.  I think the only rationale is to serve as a backstop protecting against invalid reads. ",
            "author": "Marvin Humphrey",
            "id": "comment-12653939"
        },
        {
            "date": "2008-12-05T22:44:45+0000",
            "content": "That's my point, in complex multi-treaded software with multiple readers, etc. it is a good backspot against errors..  ",
            "author": "robert engels",
            "id": "comment-12653954"
        },
        {
            "date": "2008-12-05T22:46:29+0000",
            "content": "> It would be great if instead of relying on Lucene to manage the \n> deletedDocs file, the API would be pluggable\n\nIn LUCENE-1478, \"IndexComponent\" was proposed, with potential subclasses including PostingsComponent, LexiconComponent/TermDictComponent, TermVectorsComponent, and so on.  Since then, it has become apparent that SnapshotComponent and DeletionsComponent also belong at the top level.\n\nIn Lucy/KS, these would all be specified within a Schema: \n\n\nclass MySchema extends Schema {\n  DeletionsComponent deletionsComponent() { \n    return new DocIdBitSetDeletionsComponent();\n  }\n\n  void initFields() {\n    addField(\"title\", \"text\");\n    addField(\"content\", \"text\");\n  }\n\n  Analyzer analyzer() {\n    return new PolyAnalyzer(\"en\");\n  }\n}\n\n\n\nMike, you were planning on managing IndexComponents via IndexReader and IndexWriter constructor args, but won't that get unwieldy if there are too many components?  A Schema class allows you to group them together.  You don't have to use it to manage fields the way KS does \u2013 just leave that out. ",
            "author": "Marvin Humphrey",
            "id": "comment-12653959"
        },
        {
            "date": "2008-12-06T10:18:15+0000",
            "content": "Mike, you were planning on managing IndexComponents via IndexReader and IndexWriter constructor args, but won't that get unwieldy if there are too many components? A Schema class allows you to group them together. You don't have to use it to manage fields the way KS does - just leave that out.\n\nAgreed.  I'll try to do something along these lines under LUCENE-1458. ",
            "author": "Michael McCandless",
            "id": "comment-12654047"
        },
        {
            "date": "2008-12-08T00:57:21+0000",
            "content": "> One approach would be to use a \"segmented\" model. \n\nThat would improve the average performance of deleting a document, at the cost\nof some added complexity.  Worst-case performance \u2013 which you'd hit when you\nconsolidated those sub-segment deletions files \u2013 would actually degrade a\nbit.\n\nTo manage consolidation, you'd need a deletions merge policy that operated\nindependently from the primary merge policy.  Aside from the complexity penalty, \nhaving two un-coordinated merge policies would be bad for real-time search, \nbecause you want to be able to control exactly when you pay for a big merge.\n\nI'm also bothered by the proliferation of small deletions files.  Probably\nyou'd want automatic consolidation of files under 4k, but you still could end\nup with a lot of files in a big index.\n\nSo... what if we wrote, merged, and removed deletions files on the same\nschedule as ordinary segment files?  Instead of going back and quasi-modifying\nan existing segment by associating a next-generation .del file with it, we write\ndeletions to a NEW segment and have them reference older segments.  \n\nIn other words, we add \"tombstones\" rather than \"delete\" documents.\n\nLogically speaking, each tombstone segment file would consist of an array of\nsegment identifiers, each of which would point to a \"tombstone row\" array of\nvbyte-encoded doc nums:\n\n\n// _6.tombstone\n   _2: [3, 4, 25]\n   _3: [13]\n\n// _7.tombstone\n   _2: [5]\n\n// _8.tombstone\n   _1: [94]\n   _2: [7, 8]\n   _5: [54, 55]\n\n\n\nThe thing that makes this possible is that the dead docs marked by tombstones\nnever get their doc nums shuffled during segment merging \u2013 they just\ndisappear.   If deleted docs lived to be consolidated into new segments and\nacquire new doc nums, tombstones wouldn't work.  However, we can associate\ntombstone rows with segment names and they only need remain valid as long \nas the segments they reference survive.  \n\nSome tombstone rows will become obsolete once the segments they reference go\naway, but we never arrive at a scenario where we are forced to discard valid\ntombstones.  Merging tombstone files simply involves dropping obsolete\ntombstone rows and collating valid ones.\n\nAt search time, we'd use an iterator with an internal priority queue to\ncollate tombstone rows into a stream \u2013 so there's still no need to slurp the\nfiles at IndexReader startup. ",
            "author": "Marvin Humphrey",
            "id": "comment-12654269"
        },
        {
            "date": "2008-12-08T21:05:59+0000",
            "content": "\nI like this approach!!\n\nIt's also incremental in cost (cost of flush/commit is in proportion\nto how many deletes were done), but you are storing the \"packet\" of\nincremental deletes with the segment you just flushed and not against\nthe N segments that had deletes.  And you write only one file to hold\nall the tombstones, which for commit() (file sync) is much less cost.\n\nAnd it's great that we don't need a new merge policy to handle all the\ndelete files.\n\nThough one possible downside is, for a very large segment in a very\nlarge index you will likely be merging (at search time) quite a few\ndelete packets.  But, with the cutover to\ndeletes-accessed-only-by-iterator, this cost is probably not high\nuntil a large pctg of the segment's docs are deleted, at which point\nyou should really expungeDeletes() or optimize() or optimize(int)\nanyway.\n\nIf only we could write code as quickly as we can dream... ",
            "author": "Michael McCandless",
            "id": "comment-12654571"
        },
        {
            "date": "2008-12-08T21:58:30+0000",
            "content": "Marvin: \n\"I'm also bothered by the proliferation of small deletions files. Probably\nyou'd want automatic consolidation of files under 4k, but you still could end\nup with a lot of files in a big index.\"\n\nA transaction log might be better here if we want to go to 0ish millisecond realtime. \nOn Windows at least creating files rapidly and deleting them creates significant IO overhead.\nUNIX is probably faster but I do not know.\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12654592"
        },
        {
            "date": "2008-12-08T22:06:43+0000",
            "content": "Wouldn't it be good to remove BitVector and replace it with OpenBitSet?  OBS is faster, has the DocIdSetIterator already.  It just needs to implement write to disk compression of the bitset (dgaps?).  This would be a big win for almost all searches.  We could also create an interface so that any bitset implementation could be used.\n\nSuch as:\n\npublic interface WriteableBitSet {\n public void write(IndexOutput output) throws IOException;\n}\n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12654595"
        },
        {
            "date": "2009-01-08T00:00:37+0000",
            "content": "While converting over PostingList (the Lucy/KS analogue to TermDocs) to use a deletions iterator, it occurred to me that the because the iterator has to keep state, a unique DelEnum object has to be created for every PostingList. In contrast, a BitVector object, which is accessed only via get(), can be shared.\n\nIt bugs me that each PostingList will have its own DelEnum performing interleaving of tombstones.  With very large queries against indexes with large numbers of deletions, it seems like a lot of duplicated work.\n\nTo minimize CPU cycles, it would theoretically make more sense to handle deletions much higher up, at the top level Scorer, Searcher, or even the HitCollector level.  PostingList would be completely ignorant of deletions, as would classes like NOTScorer and MatchAllScorer:\n\n\ni32_t\nMatchAllScorer_next(MatchAllScorer* self) \n{\n    if (++self->doc_num > self->max_docs) {\n        self->doc_num--;\n        return 0;\n    }\n    return self->doc_num;\n}\n\n\n\n\nvoid\nScorer_collect(Scorer *self, Hitcollector *collector, DelEnum *del_enum)\n{\n    i32_t next_deletion = del_enum ? 0 : I32_MAX;\n    i32_t doc_num = 1;\n    while (1) {\n        while (doc_num >= next_deletion) {\n            next_deletion = DelEnum_Skip_To(del_enum, target);\n            while (doc_num == next_deletion) {\n                doc_num++;\n                next_deletion = DelEnum_Skip_To(del_enum, doc_num);\n            }\n        }\n        doc_num = Scorer_Skip_To(scorer, doc_num);\n        if (doc_num) {\n            HC_Collect(collector, doc_num, Scorer_Tally(scorer));\n        }\n        else { \n            break; \n        }\n    }\n}\n\n\n\nThe problem is that PostingLists spun off from indexReader.postingList(field, term) would include deleted documents, as would Scorers.\n\nI suppose you could band-aid indexReader.postingList() by adding a boolean suppressDeletions argument which would default to true, but that seems messy.\n\nJason, I think the inefficiency of needing individual iterator objects applies to OpenBitSet as well, right?  As soon as we do away with random access, we have to keep state.  Dunno if it's going to be noticeable, but it's conceptually annoying. ",
            "author": "Marvin Humphrey",
            "id": "comment-12661786"
        },
        {
            "date": "2009-01-08T01:44:45+0000",
            "content": "I like this idea of tombstones and we should figure out a way to support it.  This issue https://issues.apache.org/jira/browse/LUCENE-584 had an implementation of a \"matcher\" class that the scorers implemented which I do not think made it into the committed patch.  \n\n> I think the inefficiency of needing individual iterator objects applies to OpenBitSet as well, right? \n\nYes that is true.  \n\nFor realtime search where a new transaction may only have a handful of deletes the tombstones may not be optimal because too many tombstones would accumulate (I believe).  For this scenario rolling bitsets may be better.  Meaning pool bit sets and throw away unused readers.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12661814"
        },
        {
            "date": "2009-01-08T02:38:21+0000",
            "content": "Jason Rutherglen:\n\n> For realtime search where a new transaction may only have a handful of \n> deletes the tombstones may not be optimal \n\nThe whole tombstone idea arose out of the need for (close to) realtime search!  It's intended to improve write speed.\n\nWhen you make deletes with the BitSet model, you have to rewrite files that scale with segment size, regardless of how few deletions you make. Deletion of a single document in a large segment may necessitate writing out a substantial bit vector file. \n\nIn contrast, i/o throughput for writing out a tombstone file scales with the number of tombstones.\n\n> because too many tombstones would accumulate (I believe).\n\nSay that you make a string of commits that are nothing but deleting a single document \u2013 thus adding a new segment each time that contains nothing but a single tombstone.  Those are going to be cheap to merge, so it seems unlikely that we'll end up with an unwieldy number of tombstone streams to interleave at search-time.\n\nThe more likely problem is the one McCandless articulated regarding a large segment accumulating a lot of tombstone streams against it.  But I agree with him that it only gets truly serious if your merge policy neglects such segments and allows them to deteriorate for too long.\n\n> For this scenario rolling bitsets may be better. Meaning pool bit sets and \n> throw away unused readers. \n\nI don't think I understand.  Is this the \"combination index reader/writer\" model, where the writer prepares a data structure that then gets handed off to the reader? ",
            "author": "Marvin Humphrey",
            "id": "comment-12661829"
        },
        {
            "date": "2009-01-08T07:51:32+0000",
            "content": "Jason,\n\nThis issue LUCENE-584 had an implementation of a \"matcher\" class that the scorers implemented which I do not think made it into the committed patch.\n\nThe only functional difference between the DocIdSetIterator as committed and the Matcher class at 584 is the Matcher.explain() method, which did not make it into DocIdSetIterator. ",
            "author": "Paul Elschot",
            "id": "comment-12661880"
        },
        {
            "date": "2009-01-08T10:57:38+0000",
            "content": "\n\n> PostingList would be completely ignorant of deletions, as would classes like NOTScorer and MatchAllScorer:\n\nThis is a neat idea! Deletions are then applied just like a Filter.\n\nFor a TermQuery (one term) the cost of the two approaches should be\nthe same.\n\nFor OR'd Term queries, it actually seems like your proposed approach\nmay be lower cost?  Ie rather than each TermEnum doing the \"AND NOT\ndeleted\" intersection, you only do it once at the top.  There is added\ncost in that each TermEnum is now returning more docIDs than before,\nbut the deleted ones are eliminated before scoring.\n\nFor AND (and other) queries I'm not sure.  In theory, having to\nprocess more docIDs is more costly, eg a PhraseQuery or SpanXXXQuery\nmay see much higher net cost.  We should test.\n\nConceivably, a future \"search optimization phase\" could pick & choose\nthe best point to inject the \"AND NOT deleted\" filter.  In fact, it\ncould also pick when to inject a Filter... a costly per-docID search\nwith a very restrictive filter could be far more efficient if you\napplied the Filter earlier in the chain.\n\nI'm also curious what cost you see of doing the merge sort for every\nsearch; I think it could be uncomfortably high since it's so\nhard-for-cpu-to-predict-branch-intensive.  We could take the first\nsearch that doesn't use skipTo and save the result of the merge sort,\nessentially doing an in-RAM-only \"merge\" of those deletes, and let\nsubsequent searches use that single merged stream.  (This is not MMAP\nfriendly, though).\n\nIn my initial rough testing, I switched to iterator API for\nSegmentTermEnum and found if %tg deletes is < 10% the search was a bit\nfaster using an iterator vs random access, but above that was slower.\nThis was with an already \"merged\" list of in-order docIDs.\n\nSwitching to an iterator API for accessing field values for many docs\n(LUCENE-831 \u2013 new FieldCache API, LUCENE-1231 \u2013 column stride\nfields) shouldn't have this same problem since it's the \"top level\"\nthat's accessing the values (ie, one iterator per field X query).\n ",
            "author": "Michael McCandless",
            "id": "comment-12661934"
        },
        {
            "date": "2009-01-08T11:35:52+0000",
            "content": "To minimize CPU cycles, it would theoretically make more sense to handle deletions much higher up, at the top level Scorer, Searcher, or even the HitCollector level.\n\nHow about a SegmentSearcher? ",
            "author": "Paul Elschot",
            "id": "comment-12661944"
        },
        {
            "date": "2009-01-08T14:32:57+0000",
            "content": "Paul Elschot:\n\n> How about a SegmentSearcher?\n\nI like the idea of a SegmentSearcher in general.  A little while back, I wondered whether exposing SegmentReaders was really the best way to handle segment-centric search.  Upon reflection, I think it is.  Segments are a good unit.  They're pure inverted indexes (notwithstanding doc stores and tombstones); the larger composite only masquerades as one.\n\nI noticed that in one version of the patch for segment-centric search (LUCENE-1483), each sorted search involved the creation of sub-searchers, which were then used to compile Scorers. It would make sense to cache those as individual SegmentSearcher objects, no? \n\nAnd then, to respond to the original suggestion, the SegmentSearcher level seems like a good place to handle application of a deletions quasi-filter.  I think we could avoid having to deal with segment-start offsets that way.  ",
            "author": "Marvin Humphrey",
            "id": "comment-12661977"
        },
        {
            "date": "2009-01-08T14:48:40+0000",
            "content": "How about if we model deletions-as-iterator on BitSet.nextSetBit(int tick) instead of a true iterator that keeps state? \n\nYou can do that now by implementing BitVector.nextSetBit(int tick) and using that in TermDocs to set a nextDeletion member var instead of checking every doc num with BitVector.get().\n\nThat way, the object that provides deletions can still be shared. ",
            "author": "Marvin Humphrey",
            "id": "comment-12661982"
        },
        {
            "date": "2009-01-08T15:18:22+0000",
            "content": "Mike McCandless:\n\n> For a TermQuery (one term) the cost of the two approaches should be\n> the same.\n\nIt'll be close, but I don't think that's quite true.  TermScorer pre-fetches\ndocument numbers in batches from the TermDocs object.  At present, only\nnon-deleted doc nums get cached.  If we move the deletions filtering up, then\nwe'd increase traffic through that cache.  However, filling it would be\nslightly cheaper, because we wouldn't be performing the deletions check.\n\nIn theory.  I'm not sure there's a way to streamline away that deletions check\nin TermDocs and maintain backwards compatibility.  And while this is a fun\nbrainstorm, I'm still far from convinced that having TermDocs.next() and\nScorer.next() return deleted docs by default is a good idea.\n\n> For AND (and other) queries I'm not sure. In theory, having to\n> process more docIDs is more costly, eg a PhraseQuery or SpanXXXQuery\n> may see much higher net cost.\n\nIf you were applying deletions filtering after Scorer.next(), then it seems\nlikely that costs would go up because of extra hit processing.  However, if\nyou use Scorer.skipTo() to jump past deletions, as in the loop I provided\nabove, then PhraseScorer etc. shouldn't incur any more costs themselves.\n\n> a costly per-docID search\n> with a very restrictive filter could be far more efficient if you\n> applied the Filter earlier in the chain.\n\nUnder the skipTo() loop, I think the filter effectively does get applied\nearlier in the chain.  Does that make sense?\n\nI think the potential performance downside comes down to prefetching in\nTermScorer, unless there are other classes that do similar prefetching.\n\n ",
            "author": "Marvin Humphrey",
            "id": "comment-12661995"
        },
        {
            "date": "2009-01-08T15:19:27+0000",
            "content": "I noticed that in one version of the patch for segment-centric search (LUCENE-1483), each sorted search involved the creation of sub-searchers, which were then used to compile Scorers. It would make sense to cache those as individual SegmentSearcher objects, no?\n\nThats a fairly old version I think (based on using MutliSearcher as a hack). Now we are using one queue and running it through each subreader of the MultiReader. ",
            "author": "Mark Miller",
            "id": "comment-12661998"
        },
        {
            "date": "2009-01-08T16:45:34+0000",
            "content": "Marvin: \"The whole tombstone idea arose out of the need for (close to) realtime search! It's intended to improve write speed.\"\n\nIt does improve the write speed.  I found in making the realtime search write speed fast enough that writing to individual files per segment can become too costly (they accumulate fast, appending to a single file is faster than creating new files, deleting the files becomes costly).  For example, writing to small individual files per commit, if the number of segments is large and the delete spans multiple segments will generate many files.  This is variable based on how often the updates are expected to occur.  I modeled this after the extreme case of the frequency of updates of a MySQL instance backing data for a web application.\n\nThe MySQL design, translated to Lucene is a transaction log per index.  Where the updates consisting of documents and deletes are written to the transaction log file.  If Lucene crashed for some reason the transaction log would be replayed.  The in memory indexes and newly deleted document bitvectors would be held in RAM (LUCENE-1314) until flushed (the in memory indexes and deleted documents) manually or based on memory usage.  Many users may not want a transaction log as they may be storing the updates in a separate SQL database instance (this is the case where I work) and so a transaction log is redundant and should be optional.  The first implementation of this will not have a transaction log.\n\nMarvin: \"I don't think I understand. Is this the \"combination index reader/writer\" model, where the writer prepares a data structure that then gets handed off to the reader?\"\n\nIt would be exposed as a combination reader writer that manages the transaction status of each update.  The internal architecture is such that after each update a new reader representing the new documents and deletes for the transaction is generated and put onto a stack.  The reader stack is drained based on whether a reader is too old to be useful anymore (i.e. no references to it, or it's has N number of readers ahead of it).   ",
            "author": "Jason Rutherglen",
            "id": "comment-12662033"
        },
        {
            "date": "2009-01-08T19:11:38+0000",
            "content": "Jason Rutherglen:\n\n> I found in making the realtime search write speed fast enough that writing\n> to individual files per segment can become too costly (they accumulate fast,\n> appending to a single file is faster than creating new files, deleting the\n> files becomes costly). \n\nI saw you mentioning i/o overhead on Windows in particular.  I can't see a way\nto mod Lucene so that it doesn't generate a bunch of files for each commit,\nand FWIW Lucy/KS is going to generate even more files than Lucene.\n\nHalf-seriously... how about writing a single-file Directory implementation?\n\n> For example, writing to small individual files per commit, if the number of\n> segments is large and the delete spans multiple segments will generate many\n> files. \n\nThere would be a maximum of two files per segment to hold the tombstones: one\nto hold the tombstone rows, and one to map segment identifiers to tombstone\nrows.  (In Lucy/KS, the mappings would probably be stored in the JSON-encoded\n\"segmeta\" file, which stores human-readable metadata on behalf of multiple \ncomponents.)\n\nSegments containing tombstones would be merged according to whatever merge\npolicy was in place.  So there won't ever be an obscene number of tombstone\nfiles unless you allow an obscene number of segments to accumulate.\n\n> Many users may not want a transaction log as they may be storing the updates\n> in a separate SQL database instance (this is the case where I work) and so a\n> transaction log is redundant and should be optional. \n\nI can see how this would be quite useful at the application level.  However, I\nthink it might be challenging to generalize the transaction log concept at the\nlibrary level:\n\n\nCustomAnalyzer analyzer = new CustomAnalyzer();\nIndexWriter indexWriter = new IndexWriter(analyzer, \"/path/to/index\");\nindexWriter.add(nextDoc());\nanalyzer.setFoo(2); // change of state not recorded by transaction log\nindexWriter.add(nextDoc());\n\n\n\nMySQL is more of a closed system than Lucene, which I think makes options\navailable that aren't available to us.\n\n> The reader stack is drained based on whether a reader is too old to be\n> useful anymore (i.e. no references to it, or it's has N number of readers\n> ahead of it).\n\nRight, this is the kind of thing that Lucene has to do because of the\nsingle-reader model, and that were trying to get away from in Lucy/KS by\nexploiting mmap and making IndexReaders cheap wrappers around the system i/o\ncache.\n\nI don't think I can offer any alternative design suggestions that meet your\nneeds.   There's going to be a change rate that overwhelms the multi-file\ncommit system, and it seems that you've determined you're up against it.  \n\nWhat's killing us is something different: not absolute change rate, but poor \nworst-case performance.\n\nFWIW, we contemplated a multi-index system with an index on a RAM disk for\nfast changes and a primary index on the main file system.  It would have\nworked fine for pure adds, but it was very tricky to manage state for\ndocuments which were being \"updated\", i.e.  deleted and re-added.  How are you\nhandling all these small adds with your combo reader/writer?  Do you not have\nthat problem? ",
            "author": "Marvin Humphrey",
            "id": "comment-12662065"
        },
        {
            "date": "2009-01-08T19:43:36+0000",
            "content": "\n> There's going to be a change rate that overwhelms the multi-file\n> commit system, and it seems that you've determined you're up against\n> it.\n\nWell... IndexWriter need not \"commit\" in order to allow a reader to\nsee the files?\n\nCommit is for crash recovery, and for knowing when it's OK to delete\nprior commits.  Simply writing the files (and not syncing them), and\nperhaps giving IndexReader.open the SegmentInfos to use directly (and\nnot writing a segments_N via the filesystem) would allow us to search\nadded docs without paying the cost of sync'ing all the files.\n\nAlso: brand new, tiny segments should be written into a RAMDirectory\nand then merged over time into the real Directory. ",
            "author": "Michael McCandless",
            "id": "comment-12662089"
        },
        {
            "date": "2009-01-08T19:48:27+0000",
            "content": "\n> If Lucene crashed for some reason the transaction log would be replayed.\n\nI think the transaction log is useful for some applications, but could\n(should) be built as a separate (optional) layer entirely on top of\nLucene's core.  Ie, neither IndexWriter nor IndexReader need to be\naware of the transaction log, which update belongs to which\ntransaction, etc? ",
            "author": "Michael McCandless",
            "id": "comment-12662092"
        },
        {
            "date": "2009-01-08T20:00:14+0000",
            "content": "\n\n> It would be exposed as a combination reader writer that manages the transaction status of each update. \n\nI think the transactions layer would also sit on top of this\n\"realtime\" layer?  EG this \"realtime\" layer would expose a commit()\nmethod, and the transaction layer above it would maintain the\ntransaction log, periodically calling commit() and truncating the\ntransaction log?\n\nThis \"realtime\" layer, then, would internally maintain a single\nIndexWriter and the readers.  IndexWriter would flush (not commit) new\nsegments into a RAMDir and yield its in-RAM SegmentInfos to\nIndexReader.reopen.  MergePolicy periodically gets those into the real\nDirectory.  When reopening a reader we have the freedom to use old\n(already merged away) segments if the newly merged segment isn't warm\nyet.\n\nWe \"just\" need to open some things up in IndexWriter:\n\n\n\tIndexReader.reopen with the in-RAM SegmentInfos\n\n\n\n\n\tWillingness to allow an IndexReader to maintain & updated deleted\n    docs even though IndexWriter has the write lock\n\n\n\n\n\tAccess to segments that were already merged away (I think we could\n    make a DeletionPolicy that pays attention to when the newly merged\n    segment is not yet warmed and keeps thue prior segments around).\n    I think this'd require allowing DeletionPolicy to see \"flush\n    points\" in addition to commit points (it doesn't today).\n\n\n\nBut I'm still hazy on the details on exactly how to open up\nIndexWriter. ",
            "author": "Michael McCandless",
            "id": "comment-12662097"
        },
        {
            "date": "2009-01-08T20:05:20+0000",
            "content": "Mike McCandless:\n\n> I'm also curious what cost you see of doing the merge sort for every\n> search; I think it could be uncomfortably high since it's so\n> hard-for-cpu-to-predict-branch-intensive. \n\nProbably true.  You're going to get accelerating degradation as the number of\ndeletions increases.  In a large index, you could end up merging 20, 30\nstreams.  Based on how the priority queue in ORScorer tends to take up space\nin profiling data, that might not be good.\n\nIt'd be manageable if you can keep your index reasonably in good shape, but you'll \nbe suckin' pondwater if it gets flabby.\n\n> We could take the first search that doesn't use skipTo and save the result\n> of the merge sort, essentially doing an in-RAM-only \"merge\" of those\n> deletes, and let subsequent searches use that single merged stream. \n\nThat was what I had in mind when proposing the pseudo-iterator model.\n\n\nclass TombStoneDelEnum extends DelEnum {\n  int nextDeletion(int docNum) {\n    while (currentMax < docNum) { nextInternal(); }\n    return bits.nextSetBit(docNum);\n  }\n  // ...\n}\n\n\n\n> (This is not MMAP friendly, though).\n\nYeah.  Ironically, that use of tombstones is more compatible with the Lucene\nmodel. \n\nI'd be reluctant to have Lucy/KS realize those large BitVectors in per-object \nprocess RAM.  That'd spoil the \"cheap wrapper around system i/o cache\" \nIndexReader plan.\n\nI can't see an answer yet.  But the one thing I do know is that Lucy/KS needs\na pluggable deletions mechanism to make experimentation easier \u2013 so that's\nwhat I'm working on today. ",
            "author": "Marvin Humphrey",
            "id": "comment-12662100"
        },
        {
            "date": "2009-01-08T20:09:04+0000",
            "content": "\n> If we move the deletions filtering up, then we'd increase traffic through that cache\n\nOK, right.  So we may have some added cost because of this.  I think\nit's only TermScorer that uses the bulk API though.\n\n\n> If you were applying deletions filtering after Scorer.next(), then it seems\n> likely that costs would go up because of extra hit processing. However, if\n> you use Scorer.skipTo() to jump past deletions, as in the loop I provided\n> above, then PhraseScorer etc. shouldn't incur any more costs themselves.\n\nAhhh, now I got it!  Good, you're right.\n\n\n> Under the skipTo() loop, I think the filter effectively does get applied\n> earlier in the chain. Does that make sense?\n\nRight.  This is how Lucene works today.  Excellent.\n\nSo, net/net it seems like \"deletes-as-a-filter\" approach is compelling? ",
            "author": "Michael McCandless",
            "id": "comment-12662101"
        },
        {
            "date": "2009-01-08T20:11:25+0000",
            "content": "\n> How about if we model deletions-as-iterator on BitSet.nextSetBit(int tick) instead of a true iterator that keeps state? \nThat works if under-the-hood it's a non-sparse representation.  But if it's sparse, you need an iterator (state) to remember where you are. ",
            "author": "Michael McCandless",
            "id": "comment-12662102"
        },
        {
            "date": "2009-01-08T20:28:53+0000",
            "content": "Mike McCandless:\n\n> Commit is for crash recovery, and for knowing when it's OK to delete\n> prior commits. Simply writing the files (and not syncing them), and\n> perhaps giving IndexReader.open the SegmentInfos to use directly (and\n> not writing a segments_N via the filesystem) would allow us to search\n> added docs without paying the cost of sync'ing all the files.\n\nMmm.  I think I might have given IndexWriter.commit() slightly different\nsemantics.  Specifically, I might have given it a boolean \"sync\" argument\nwhich defaults to false.\n\n> Also: brand new, tiny segments should be written into a RAMDirectory\n> and then merged over time into the real Directory.\n\nTwo comments.  First, if you don't sync, but rather leave it up to the OS when\nit wants to actually perform the actual disk i/o, how expensive is flushing? Can \nwe make it cheap enough to meet Jason's absolute change rate requirements?\n\nSecond, the multi-index model is very tricky when dealing with \"updates\".  How\ndo you guarantee that you always see the \"current\" version of a given\ndocument, and only that version?  When do you expose new deletes in the\nRAMDirectory, when do you expose new deletes in the FSDirectory, how do you\nmanage slow merges from the RAMDirectory to the FSDirectory, how do you manage\nnew adds to the RAMDirectory that take place during slow merges...\n\nBuilding a single-index, two-writer model that could handle fast updates while\nperforming background merging was one of the main drivers behind the tombstone\ndesign. ",
            "author": "Marvin Humphrey",
            "id": "comment-12662107"
        },
        {
            "date": "2009-01-08T20:33:43+0000",
            "content": "Mike McCandless:\n\n> if it's sparse, you need an iterator (state) to remember where you are.\n\nWe can hide the sparse representation and the internal state, having the\nobject lazily build the a non-sparse representation.  That's what I had in\nmind with the code for TombstoneDelEnum.nextDeletion().\nTombstoneDelEnum.nextInternal() would be a private method used for building up\nthe internal BitVector. ",
            "author": "Marvin Humphrey",
            "id": "comment-12662110"
        },
        {
            "date": "2009-01-08T22:11:14+0000",
            "content": "Mike McCandless:\n\n> So, net/net it seems like \"deletes-as-a-filter\" approach is compelling?\n\nIn terms of CPU-cycles, maybe.  \n\nMy gut tells me that it's all but mandatory if we use merged-on-the-fly\ntombstone streams, but if Lucene goes that route it should cache a BitVector\nand use a shared pseudo-iterator \u2013 in which case the costs will no longer be\nsignificantly more than the current system.\n\nUnder the current system, I'm not certain that the deletions checks are that\nexcessive.  Consider this loop from TermDocs.read():\n\n\n    while (i < length && count < df) {\n      // manually inlined call to next() for speed\n      final int docCode = freqStream.readVInt();\n      doc += docCode >>> 1;       // shift off low bit\n      if ((docCode & 1) != 0)       // if low bit is set\n        freq = 1;         // freq is one\n      else\n        freq = freqStream.readVInt();     // else read freq\n      count++;\n\n      if (deletedDocs == null || !deletedDocs.get(doc)) {\n        docs[i] = doc;\n        freqs[i] = freq;\n        ++i;\n      }\n    }\n\n\n\nThe CPU probably does a good job of predicting the result of the null check on\ndeletedDocs.  The readVInt() method call is already a pipeline killer.\n\nHere's how that loop looks after I patch the deletions check for pseudo-iteration.\n\n\n      while (i < length && count < df) {\n        // manually inlined call to next() for speed\n        final int docCode = freqStream.readVInt();\n        doc += docCode >>> 1;       // shift off low bit\n        if ((docCode & 1) != 0)       // if low bit is set\n          freq = 1;         // freq is one\n        else\n          freq = freqStream.readVInt();     // else read freq\n        count++;\n\n        if (docNum >= nextDeletion) {\n            if (docNum > nextDeletion) {\n              nextDeletion = deletedDocs.nextSetBit(docNum);\n            }\n            if (docNum == nextDeletion) {\n              continue;\n            }\n        }\n\n        docs[i] = doc;\n        freqs[i] = freq;\n        ++i;\n      }\n      return i;\n    }\n\n\n\nAgain, the CPU is probably going to do a pretty good job of predicting the\nresults of the deletion check.  And even then, we're accessing the same shared\nBitVector across all TermDocs, and its bits are hopefully a cache hit.\n\nTo really tighten this loop, you have to do what Nate and I want with Lucy/KS:\n\n\n\tRemove all function/method call overhead.\n\tOperate directly on the memory mapped postings file.\n\n\n\n\nSegPList_bulk_read(SegPostingList *self, i32_t *doc_nums, i32_t *freqs,\n                   u32_t request)\n{\n    i32_t       doc_num   = self->doc_num;\n    const u32_t remaining = self->doc_freq - self->count;\n    const u32_t num_got   = request < remaining ? request : remaining;\n    char       *buf       = InStream_Buf(instream, C32_MAX_BYTES * num_got);\n    u32_t       i;\n\n    for (i = 0; i < num_got; i++) {\n        u32_t doc_code = Math_decode_c32(&buf); /* static inline function */\n        u32_t freq     = (doc_code & 1) ? 1 : Math_decode_c32(&buf);\n        doc_num        += doc_code >> 1; \n        doc_nums[i]    = doc_num;\n        freqs[i]       = freq;\n        ++i;\n    }\n\n    InStream_Advance_Buf(instream, buf);\n    self->doc_num = doc_num;\n    self->count += num_got;\n\n    return num_got;\n}\n\n\n\n(That loop would be even better using PFOR instead of vbyte.)\n\nIn terms of public API, I don't think it's reasonable to change Lucene's\nScorer and TermDocs classes so that their iterators start returning deleted\ndocs.  \n\nWe could potentially make that choice with Lucy/KS, thus allowing us to remove\nthe deletions check in the PostingList iterator (as above) and getting a\npotential speedup.  But even then I hesitate to push the deletions API upwards\ninto a space where users of raw Scorer and TermDocs classes have to deal with\nit \u2013 especially since iterator-style deletions aren't very user-friendly. ",
            "author": "Marvin Humphrey",
            "id": "comment-12662143"
        },
        {
            "date": "2009-01-09T01:40:24+0000",
            "content": "M.M.:\" I think the transactions layer would also sit on top of this\n\"realtime\" layer? EG this \"realtime\" layer would expose a commit()\nmethod, and the transaction layer above it would maintain the\ntransaction log, periodically calling commit() and truncating the\ntransaction log?\"\n\nOne approach that may be optimal is to expose from IndexWriter a createTransaction method that accepts new documents and deletes.  All documents have an associated UID.  The new documents could feasibly be encoded into a single segment that represents the added documents for that transaction.  The deletes would be represented as document long UIDs rather than int doc ids.  Then the commit method would be called on the transaction object who returns a reader representing the latest version of the index, plus the changes created by the transaction.  This system would be a part of IndexWriter and would not rely on a transaction log.  IndexWriter.commit would flush the in ram realtime indexes to disk.  The realtime merge policy would flush based on the RAM usage or number of docs.\n\n\nIndexWriter iw = new IndexWriter();\nTransaction tr = iw.createTransaction();\ntr.addDocument(new Document());\ntr.addDocument(new Document());\ntr.deleteDocument(1200l);\nIndexReader ir = tr.flush(); // flushes transaction to the index (probably to a ram index)\nIndexReader latestReader = iw.getReader(); // same as ir\niw.commit(boolean doWait); // commits the in ram realtime index to disk\n\n\n\nWhen commit is called, the disk segment reader flush their deletes to disk which is fast.  The in ram realtime index is merged to disk.  The process is described in more detail further down.\n\nM.H.: \"how about writing a single-file Directory implementation?\"\n\nI'm not sure we need this because and appending rolling transaction log should work.  Segments don't change, only things like norms and deletes which can be appended to a rolling transaction log file system.  If we had a generic transaction logging system, the future column stride fields, deletes, norms, and future realtime features could use it and be realtime.  \n\nM.H.: \"How do you guarantee that you always see the \"current\" version of a given document, and only that version? \n\nEach transaction returns an IndexReader.  Each \"row\" or \"object\" could use a unique id in the transaction log model which would allow documents that were merged into other segments to be deleted during a transaction log replay.  \n\nM.H.: \"When do you expose new deletes in the RAMDirectory, when do you expose new deletes in the FSDirectory\"\n\nWhen do you expose new deletes in the RAMDir, when do you expose new deletes in the FSDirectory, how do you manage slow merges from the RAMDir to the FSDirectory, how do you manage new adds to the RAMDir that take place during slow merges...\"\n\nQueue deletes to the RAMDir, while copying the RAMDir to the FSDir in the background, perform the deletes after the copy is completed, then instantiate a new reader with the newly merged FSDirectory and a new RAMDirs.  Writes that were occurring during this process would be happening to another new RAMDir.  \n\nOne way to think of the realtime problem is in terms of segments rather than FSDirs and RAMDirs.  Some segments are on disk, some in RAM.  Each transaction is an instance of some segments and their deletes (and we're not worried about the deletes being flushed or not so assume they exist as BitVectors).  The system should expose an API to checkpoint/flush at a given transaction level (usually the current) and should not stop new updates from happening.    \n\nWhen I wrote this type of system, I managed individual segments outside of IndexWriter's merge policy and performed the merging manually by placing each segment in it's own FSDirectory (the segment size was 64MB) which minimized the number of directories.  I do not know the best approach for this when performed within IndexWriter.  \n\nM.H.: \"Two comments. First, if you don't sync, but rather leave it up to the OS when\nit wants to actually perform the actual disk i/o, how expensive is flushing? Can\nwe make it cheap enough to meet Jason's absolute change rate requirements?\"\n\nWhen I tried out the transaction log a write usually mapped pretty quickly to a hard disk write.  I don't think it's safe to leave writes up to the OS.\n\nM.M.: \"maintain & updated deleted docs even though IndexWriter has the write lock\"\n\nIn my previous realtime search implementation I got around this by having each segment in it's own directory.  Assuming this is non-optimal, we will need to expose an IndexReader that has the writelock of the IndexWriter.     ",
            "author": "Jason Rutherglen",
            "id": "comment-12662214"
        },
        {
            "date": "2009-01-09T03:36:43+0000",
            "content": "Here's a patch implementing BitVector.nextSetBit() and converting\nSegmentTermDocs over to use the quasi-iterator style. Tested but \nnot benchmarked. ",
            "author": "Marvin Humphrey",
            "id": "comment-12662236"
        },
        {
            "date": "2009-01-09T05:31:50+0000",
            "content": "To really tighten this loop, you have to [ ... ] remove all function/method call overhead [and] operate directly on the memory mapped postings file.\n\nThat sounds familiar...\n\nhttp://svn.apache.org/viewvc/lucene/java/trunk/src/gcj/org/apache/lucene/index/GCJTermDocs.cc?view=markup ",
            "author": "Doug Cutting",
            "id": "comment-12662247"
        },
        {
            "date": "2009-01-09T11:58:49+0000",
            "content": "\n> Mmm. I think I might have given IndexWriter.commit() slightly different\n> semantics. Specifically, I might have given it a boolean \"sync\" argument\n> which defaults to false.\n\nIt seems like there are various levels of increasing \"durability\" here:\n\n\n\tMake available to a reader in the same JVM (eg flush new segment\n    to a RAMDir) \u2013 not exposed today.\n\n\n\n\n\tMake available to a reader sharing the filesystem right now (flush\n    to Directory in \"real\" filesystem, but don't sync) \u2013 exposed\n    today (but deprecated as a public API) as flush.\n\n\n\n\n\tMake available to readers even if OS/machine crashes (flush to\n    Directory in \"real\" filesystem, and sync) \u2013 exposed today as commit.\n\n\n\n\n> Two comments. First, if you don't sync, but rather leave it up to the OS when\n> it wants to actually perform the actual disk i/o, how expensive is flushing? Can\n> we make it cheap enough to meet Jason's absolute change rate requirements?\n\nRight I've been wondering the same thing.  I think this should be our\nfirst approach to realtime indexing, and then we swap in RAMDir if\nperformance is not good enough.\n\n\n> Second, the multi-index model is very tricky when dealing with \"updates\". How\n> do you guarantee that you always see the \"current\" version of a given\n> document, and only that version? When do you expose new deletes in the\n> RAMDirectory, when do you expose new deletes in the FSDirectory, how do you\n> manage slow merges from the RAMDirectory to the FSDirectory, how do you manage\n> new adds to the RAMDirectory that take place during slow merges...\n>\n> Building a single-index, two-writer model that could handle fast updates while\n> performing background merging was one of the main drivers behind the tombstone\n> design.\n\nI'm not proposing multi-index model (at least I think I'm not!).  A\nsingle IW could flush new tiny segments into a RAMDir and later merge\nthem into a real Dir.  But I agree: let's start w/ a single Dir and\nmove to RAMDir only if necessary.\n\n\n> Building a single-index, two-writer model that could handle fast updates while\n> performing background merging was one of the main drivers behind the tombstone\n> design.\n\nI think carrying the deletions in RAM (reopening the reader) is\nprobably fastest for Lucene.  Lucene with the \"reopened stream of\nreaders\" approach can do this, but Lucy/KS (with mmap) must use\nfilesystem as the intermediary. ",
            "author": "Michael McCandless",
            "id": "comment-12662339"
        },
        {
            "date": "2009-01-09T12:01:40+0000",
            "content": "\n\n> We can hide the sparse representation and the internal state, having the\n> object lazily build the a non-sparse representation. That's what I had in\n> mind with the code for TombstoneDelEnum.nextDeletion().\n> TombstoneDelEnum.nextInternal() would be a private method used for building up\n> the internal BitVector.\n\nGot it, though for a low deletion rate presumably you'd want to store\nthe int docIDs directly so iterating through them doesn't require O(N)\nscan for the next set bit.\n\nI think what you'd want to lazily do is merge the N tombstone streams\nfor this one segment into a single data structure; whether that data\nstructure is sparse or unsparse is a separate decision. ",
            "author": "Michael McCandless",
            "id": "comment-12662342"
        },
        {
            "date": "2009-01-09T12:04:44+0000",
            "content": "\n> We could potentially make that choice with Lucy/KS, thus allowing us to remove\n> the deletions check in the PostingList iterator (as above) and getting a\n> potential speedup. But even then I hesitate to push the deletions API upwards\n> into a space where users of raw Scorer and TermDocs classes have to deal with\n> it - especially since iterator-style deletions aren't very user-friendly.\n\nCan't you just put sugar on top?  Ie, add an API that matches today's\nAPI and efficiently applies the \"deleted docs filter\" for you?  Then\nyou have 100% back compat? ",
            "author": "Michael McCandless",
            "id": "comment-12662345"
        },
        {
            "date": "2009-01-09T12:08:31+0000",
            "content": "\n\n> Under the current system, I'm not certain that the deletions checks are that\n> excessive. \n\nI made a simple test that up-front converted the deleted docs\nBitVector into an int[] containing the deleted docsIDs, and then did\nthe same nextDeletedDoc change to SegmentTermDocs.\n\nThis was only faster if < 10% of docs were deleted, though I didn't do\nvery thorough testing.\n\nI think the problem is with this change the CPU must predict a new\nbranch (docNum >= nextDeletion) vs doing a no-branching lookup of the\nbit. ",
            "author": "Michael McCandless",
            "id": "comment-12662347"
        },
        {
            "date": "2009-01-09T13:28:18+0000",
            "content": "\n\n> One way to think of the realtime problem is in terms of segments rather than FSDirs and RAMDirs. Some segments are on disk, some in RAM.\n\nI think that's a good approach.  IndexWriter could care less which dir\neach segment resides in.\n\nFor starters let's flush all new segments into a single Directory (and\nswap in RAMDir, single-file-Dir, etc., if/when necessary for better\nperformance).\n\nThinking out loud.... what if we added IndexWriter.getReader(), which\nreturns an IndexReader for the current index?\n\nIt would read segments that are not \"visible\" to a newly opened\nIndexReader on that directory (ie, the SegmentInfos inside IndexWriter\nwas used to do the open, not the latest segments_N in the Directory).\n\nThat IndexReader is free to do deletions / set norms (shares write\nlock under the hood w/ IndexWriter), and when reopen is called it\ngrabs the current SegmentInfos from IndexWriter and re-opens that.\n\nWe would un-deprecate flush().  The, the transaction layer would make\nchanges, call flush(), call reopen(), and return the reopened reader. ",
            "author": "Michael McCandless",
            "id": "comment-12662362"
        },
        {
            "date": "2009-01-14T17:50:59+0000",
            "content": "To simplify the patch, can we agree to add a method,\nIndexReader.getDeletedDocs that returns a DocIdSet? This way\napplications may have access to deleteDocs and not be encumbered by\nthe IR.isDeleted method. ",
            "author": "Jason Rutherglen",
            "id": "comment-12663815"
        },
        {
            "date": "2009-01-14T18:36:27+0000",
            "content": "IndexReader.getDeletedDocs that returns a DocIdSet\n\nSeems like a good idea, but the difficulty lies in defining the semantics of such a call.\nAre subsequent deletes reflected in the returned DocIdSet?  Perhaps make this undefined - may or may not be.\nCan the DocIdSet be used concurrently with IndexReader.deleteDocument()?  This could work (w/o extra undesirable synchronization) if we're careful. ",
            "author": "Yonik Seeley",
            "id": "comment-12663837"
        },
        {
            "date": "2009-01-14T22:01:47+0000",
            "content": "I think it should be up to the user. If the user concurrently\nmodifies then they're responsible for the possibly spurious effects.\nHowever if we want to be protective, a philosophy I don't think works\nwell in LUCEN-1516 either, we can offer IR.getDeletedDocs only from a\nread only index reader. This solves the issues brought up such as\n\"undesirable synchronization\". ",
            "author": "Jason Rutherglen",
            "id": "comment-12663903"
        },
        {
            "date": "2009-01-15T10:42:45+0000",
            "content": "I think it's fine to expose this API, mark it expert & subject to\nchange, and state that it simply returns the current deleted docs\nBitVector and any synchronization issues must be handled by the app.\nProbably it should be package private or protected.\n\nAlso, I think Multi*Reader would not implement this API?  Meaning\nyou must get  the individual SegmentReader and ask it.\n\nSince we are discussing possible changes to deleted docs, eg switching\nto iterator-only accesss, maybe applying deletions as a filter, maybe\nusing \"tombstones\" under-the-hood, etc., this API could very well\nchange.\n\nFor our current thinking on realtime search, I think one weak part of\nthe design is the blocking copy-on-write required for the first\ndeletion on a newly cloned reader.  The time taken is in proportion to\nthe size of the segment.  Switching to a design that can background\nthis copy-on-write will necessarily impact how we represent\nand access deletions. ",
            "author": "Michael McCandless",
            "id": "comment-12664083"
        },
        {
            "date": "2009-01-16T17:05:19+0000",
            "content": "\nit's fine to expose this API, mark it expert & subject to\nchange, and state that it simply returns the current deleted docs\nBitVector and any synchronization issues must be handled by the app.\nProbably it should be package private or protected. \n\n+1 Sounds good\n\n\nAlso, I think Multi*Reader would not implement this API? Meaning\nyou must get the individual SegmentReader and ask it.\n\nWhy?  The returned iterator can traverse the multiple bitvectors.\n\n\nI think one weak part of\nthe design is the blocking copy-on-write required for the first\ndeletion on a newly cloned reader. The time taken is in proportion to\nthe size of the segment. \n\nIf the segment is large, tombstones can solve this. ",
            "author": "Jason Rutherglen",
            "id": "comment-12664566"
        },
        {
            "date": "2009-01-16T17:19:12+0000",
            "content": "Why? The returned iterator can traverse the multiple bitvectors.\n\nWoops, sorry: I missed that it would return a DocIdSet (iterator only) vs underlying (current) BitVector.  So then MultiReader could return a DocIdSet.\n\nIf the segment is large, tombstones can solve this.\n\nRight; I was saying, as things are today (single BitVector holds all deleted docs), one limitation of the realtime approach we are moving towards is the copy-on-write cost of the first delete on a freshly cloned reader for a large segment.\n\nIf we moved to using only iterator API for accessing deleted docs within Lucene then we could explore fixes for the copy-on-write cost w/o changing on-disk representation of deletes.  IE tombstones are perhaps overkill for Lucene, since we're not using the filesystem as the intermediary for communicating deletes to a reopened reader.  We only need an in-RAM incremental solution. ",
            "author": "Michael McCandless",
            "id": "comment-12664570"
        },
        {
            "date": "2009-01-16T17:28:02+0000",
            "content": "\nIf we moved to using only iterator API for accessing deleted docs within Lucene then we could explore fixes for the copy-on-write cost w/o changing on-disk representation of deletes. IE tombstones are perhaps overkill for Lucene, since we're not using the filesystem as the intermediary for communicating deletes to a reopened reader. We only need an in-RAM incremental solution.\n\n+1 Agreed.  Good point about not needing to change the on disk representation as that would make implementation a bit more complicated.  Sounds like we need a tombstones patch as well that plays well with IndexReader.clone.\n\nExposing deleted docs as a DocIdSet allows possible future implementations that DO return deleted docs as discussed (via a flag to IndexReader) from TermDocs.  Deleted docs DocIdSet can then be used on a higher level as a filter/query.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12664573"
        },
        {
            "date": "2009-01-16T23:22:09+0000",
            "content": "Do we want to implement SegmentTermDocs using DocIdSet in another patch? ",
            "author": "Jason Rutherglen",
            "id": "comment-12664732"
        },
        {
            "date": "2009-01-21T01:41:25+0000",
            "content": "First cut.  All tests pass.\n\n\n\tImplemented IndexReader.getDeletedDocs in all readers\n\n\n\n\n\tCreated MultiDocIdSet that iterates over multiple DocIdSets which is\nused by MultiSegmentReader and MultiReader\n\n\n\n\n\tIncorporated Marvin's patch into SegmentTermDocs and BitVector.\nSegmentTermDocs iterates using deleted docs DocIdSet instead of\ncalling BitVector.get. \n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12665679"
        },
        {
            "date": "2009-01-21T02:29:11+0000",
            "content": "Jason,\n\n> Incorporated Marvin's patch into SegmentTermDocs and BitVector.\n\nI believe there's an inefficiency in my original patch.  Code like this occurs in three places:\n\n\n+      if (doc >= nextDeletion) {\n+        if (doc > nextDeletion) \n+          nextDeletion = deletedDocs.nextSetBit(doc);\n+        if (doc == nextDeletion)\n+          continue;\n       }\n\n\n\nWhile technically correct, extra work is being done.  When nextSetBit() can't\nfind any more bits, it returns -1 (just like java.util.BitSet.nextSetBit() does).  \nThis causes the deletion loop to be checked on each iteration.\n\nThe solution is to test for -1, as in the new version of the patch (also\ntested but not benchmarked):\n\n\n+      if (doc >= nextDeletion) {\n+        if (doc > nextDeletion) {\n+          nextDeletion = deletedDocs.nextSetBit(doc);\n+          if (nextDeletion == -1) {\n+            nextDeletion = Integer.MAX_VALUE;\n+          }\n+        }\n+        if (doc == nextDeletion) {\n+          continue;\n+        }\n       }\n\n\n\nTheoretically, we could also change the behavior of nextSetBit() so that it\nreturns Integer.MAX_VALUE when it can't find any more set bits.  However,\nthat's a little misleading (since it's a positive number and could thus\nrepresent a true set bit), and also would break the intended API mimicry by\nBitVector.nextSetBit() of java.util.BitSet.nextSetBit(). ",
            "author": "Marvin Humphrey",
            "id": "comment-12665688"
        },
        {
            "date": "2009-01-21T04:22:31+0000",
            "content": "Mike McCandless:\n\n>> If we move the deletions filtering up, then we'd increase traffic through\n>> that cache\n>\n> OK, right. So we may have some added cost because of this. I think it's only\n> TermScorer that uses the bulk API though.\n\nThe original BooleanScorer also pre-fetches.  (That doesn't affect KS because\nORScorer, ANDScorer, NOTScorer and RequiredOptionScorer (which have\ncollectively replaced BooleanScorer) all proceed doc-at-a-time and implement\nskipping.)\n\nAnd I'm still not certain it's a good idea from an API standpoint: it's\nstrange to have the PostingList and Scorer iterators included deleted docs.\n\nNevertheless, I've now changed over KS to use the deletions-as-filter approach\nin svn trunk. The tie-breaker was related to the ongoing modularization of\nIndexReader: if PostingList doesn't have to handle deletions, then\nPostingsReader doesn't have to know about DeletionsReader, and if\nPostingsReader doesn't have to know about DeletionsReader, then all\nIndexReader sub-components can be implemented independently.\n\nThe code to implement the deletion skipping turned out to be more verbose and\nfiddly than anticipated.  It's easy to make fencepost errors when dealing with\nadvancing two iterators in sync, especially when you can only invoke the\nskipping iterator method once for a given doc num.\n\n\nvoid\nScorer_collect(Scorer *self, HitCollector *collector, DelEnum *deletions,\n               i32_t doc_base)\n{\n    i32_t   doc_num        = 0;\n    i32_t   next_deletion  = deletions ? 0 : I32_MAX;\n\n    /* Execute scoring loop. */\n    while (1) {\n        if (doc_num > next_deletion) {\n            next_deletion = DelEnum_Advance(deletions, doc_num);\n            if (next_deletion == 0) { next_deletion = I32_MAX; }\n            continue;\n        }\n        else if (doc_num == next_deletion) {\n            /* Skip past deletions. */\n            while (doc_num == next_deletion) {\n                /* Artifically advance scorer. */\n                while (doc_num == next_deletion) {\n                    doc_num++;\n                    next_deletion = DelEnum_Advance(deletions, doc_num);\n                    if (next_deletion == 0) { next_deletion = I32_MAX; }\n                }\n                /* Verify that the artificial advance actually worked. */\n                doc_num = Scorer_Advance(self, doc_num);\n                if (doc_num > next_deletion) {\n                    next_deletion = DelEnum_Advance(deletions, doc_num);\n                }\n            }\n        }\n        else {\n            doc_num = Scorer_Advance(self, doc_num + 1);\n            if (doc_num >= next_deletion) { \n                next_deletion = DelEnum_Advance(deletions, doc_num);\n                if (doc_num == next_deletion) { continue; }\n            }\n        }\n\n        if (doc_num) {\n            HC_Collect(collector, doc_num + doc_base, Scorer_Tally(self));\n        }\n        else { \n            break; \n        }\n    }\n}\n\n\n ",
            "author": "Marvin Humphrey",
            "id": "comment-12665697"
        },
        {
            "date": "2009-01-21T07:36:24+0000",
            "content": ">>> If we move the deletions filtering up, then we'd increase traffic through\n>>> that cache\n>>\n>> OK, right. So we may have some added cost because of this. I think it's only\n>> TermScorer that uses the bulk API though.\n\n>The original BooleanScorer also pre-fetches. (That doesn't affect KS because\n> ORScorer, ANDScorer, NOTScorer and RequiredOptionScorer (which have\n> collectively replaced BooleanScorer) all proceed doc-at-a-time and implement\n> skipping.)\n\nFor OR like queries, that use Scorer.next(), deletions might be treated as early\nas possible, since each hit will cause a matching doc and a corresponding score calculation.\n\nFor AND like queries, that use Scorer.skipTo(), deletions can be treated later,\nfor example in skipTo() of the conjunction/and/scorer.\n\nIn the same way, prefetching into a larger term documents buffer helps for OR queries,\nbut gets in the way for AND queries. ",
            "author": "Paul Elschot",
            "id": "comment-12665748"
        },
        {
            "date": "2009-01-21T17:58:25+0000",
            "content": "> For OR like queries, that use Scorer.next(), deletions might be treated as\n> early as possible, since each hit will cause a matching doc and a\n> corresponding score calculation.\n\nIt depends whether the OR-like query spawns a Scorer that pre-fetches.\n\nAt present, deletions are handled in Lucene at the SegmentTermDocs level.\nFiltering deletions that early minimizes overhead from all Scorer.next()\nimplementations, including those that pre-fetch like the original\nBooleanScorer.  The cost is that we must check every single posting in every\nsingle SegmentTermDocs to see if the doc has been deleted.\n\nThe Scorer_Collect() routine above, which skips past deletions, consolidates\nthe cost of checking every single posting into one check.  However, the\noriginal BooleanScorer can't skip, so it will incur overhead from scoring\ndocuments which have been deleted.  Do the savings and the additional costs\nbalance each other out?  Its hard to say.\n\nHowever, with OR-like Scorers which implement skipping and don't pre-fetch \u2013\nLucene's DisjunctionSumScorer and its KS analogue ORScorer \u2013 you get the best\nof both worlds.  Not only do you avoid the per-posting-per-SegTermDocs\ndeletions check, you get to skip past deletions and avoid the extra overhead\nin Scorer.next().\n\nOf course the problem is that because ScorerDocQueue isn't very efficient,\nDisjunctionSumScorer and ORScorer are often slower than the distributed sort\nin the original BooleanScorer.\n\n> For AND like queries, that use Scorer.skipTo(), deletions can be treated\n> later, for example in skipTo() of the conjunction/and/scorer.\n\nYes, and it could be implemented by adding a Filter sub-clause to the\nconjunction/and/scorer.\n ",
            "author": "Marvin Humphrey",
            "id": "comment-12665894"
        },
        {
            "date": "2009-01-21T18:27:06+0000",
            "content": "\nM.M.: I believe there's an inefficiency in my original patch.\n\nMarvin, I implemented the SegmentTermsDocs using DocIdSetIterator.skipTo rather than nextSetBit.  Do you think the skipTo can be optimized? ",
            "author": "Jason Rutherglen",
            "id": "comment-12665905"
        },
        {
            "date": "2009-01-21T19:24:45+0000",
            "content": "> I implemented the SegmentTermsDocs using DocIdSetIterator.skipTo rather than\n> nextSetBit. \n\nAh, I see.  And you handled the -1 value properly.  Well done.\n\n> Do you think the skipTo can be optimized?\n\nProvided that the compiler is clever enough to inline the various methods\nwithin BitVectorDocIdSetIterator (it should be), I don't see a way to improve\non that part.\n\nIt's probably possible to optimize this loop from my patch in\nBitVector.nextSetBit(), by retrieving the byte value and operating directly on\nit rather than calling get().  (The compiler would have to be quite clever to\nfigure out that optimization on its own.)\n\n\n+        // There's a non-zero bit in this byte. \n+        final int base = pos * 8;\n+        for (int i = 0; i < 8; i++) {\n+          final int candidate = base + i;\n+          if (candidate < size && candidate >= bit) {\n+            if (get(candidate)) {\n+              return candidate;\n+            }\n+          }\n+        }\n\n\n\nAt a higher level, I'd imagine we'd want to store the deleted doc IDs as an\ninteger array rather than a BitVector if there aren't very many of them.  But\nI think that will mess with the random access required by\nIndexReader.isDeleted(). ",
            "author": "Marvin Humphrey",
            "id": "comment-12665924"
        },
        {
            "date": "2009-01-21T19:30:09+0000",
            "content": "Previous patch did not have MultiDocIdSet, this one does. ",
            "author": "Jason Rutherglen",
            "id": "comment-12665930"
        },
        {
            "date": "2009-01-21T19:38:33+0000",
            "content": "\nM.M.: \"At a higher level, I'd imagine we'd want to store the deleted doc IDs as an\ninteger array rather than a BitVector if there aren't very many of them. But\nI think that will mess with the random access required by\nIndexReader.isDeleted().\"\n\nIndeed, how should we solve isDeleted for the tombstones\nimplementation? Or do we simply assume it will be slow (requiring a\nlinear scan?) Or perhaps borrow from Solr's HashDocSet (a minimal\nprimitive int based hash) to implement? ",
            "author": "Jason Rutherglen",
            "id": "comment-12665931"
        },
        {
            "date": "2009-01-21T19:54:04+0000",
            "content": "I am going to run the contrib benchmark tests on this patch vs trunk to see if there is a difference in performance.  Do the benchmarks have deleted docs? ",
            "author": "Jason Rutherglen",
            "id": "comment-12665939"
        },
        {
            "date": "2009-01-21T19:57:05+0000",
            "content": "\nIndeed, how should we solve isDeleted for the tombstones\nimplementation?\n\nShould we deprecate isDeleted?  And also deprecate document() checking whether a document is deleted (ie switch to a new API that returns document w/o checking deletions).\n\nOr perhaps move isDeleted to a new API that makes it clear that there is a performance cost to random-access of deletions?  (And the first time it's called, it materializes a full bit vector). ",
            "author": "Michael McCandless",
            "id": "comment-12665942"
        },
        {
            "date": "2009-01-21T21:08:23+0000",
            "content": "\nI am going to run the contrib benchmark tests on this patch vs trunk to see if there is a difference in performance\n\nI think this is the most important next step before we go too much\nfurther.\n\nThere are two separate questions we are exploring here:\n\n  1) Should we switch to iterator only API for accessing deleted docs\n     (vs random-access API we use today)?\n\n  2) Should we take deletes into account at a higher level\n     (BooleanScorer/2, as a top-level Filter) vs the lowest level\n     (each posting, in SegmentTermDocs that we do today)?\n\nWe need to understand the performance impact with each combination of\n1) and 2), across different queries.  Likely the answer will be \"it\ndepends on the query\", but we should understand just how much.  I\nthink performance cost/benefit is the driver here, ",
            "author": "Michael McCandless",
            "id": "comment-12665965"
        },
        {
            "date": "2009-01-21T23:01:47+0000",
            "content": "Is there info on the wiki about how to use the contrib/benchmarks? (didn't find any)\n\n\nM.M.: \"1) Should we switch to iterator only API for accessing deleted docs\n(vs random-access API we use today)?\"\n\nI think we can by default support both. We may want to test (later\non) the difference in speed of iterating over an OpenBitSet vs\nBitVector for the deleted docs.\n\n\nM.M.: \"2) Should we take deletes into account at a higher level\n(BooleanScorer/2, as a top-level Filter) vs the lowest level (each\nposting, in SegmentTermDocs that we do today)?\"\n\nWe probably need to support both as implementing top level deleted\ndocs filtering may have unknown side effects. The user may decide\nbased on their queries and other variables such as the number of\ndeleted docs.  ",
            "author": "Jason Rutherglen",
            "id": "comment-12665998"
        },
        {
            "date": "2009-01-22T01:44:07+0000",
            "content": "\nShould we deprecate isDeleted? And also deprecate document()\nchecking whether a document is deleted (ie switch to a new API that\nreturns document w/o checking deletions). \n\nDeprecating isDeleted might be good. Would we need the read only\nreaders? We can still offer get method access to the deleted docs if\nit's a bit set by creating an abstract class DocIdBitSet that\nBitVector, java.util.BitSet, and OpenBItSet implement. This can\nhappen by casting IR.getDeletedDocs to DocIdBItSet. \n\n\nOr perhaps move isDeleted to a new API that makes it clear\nthat there is a performance cost to random-access of deletions? (And\nthe first time it's called, it materializes a full bit vector).\n\nSeems best to keep deleted docs access to the DocIdSet and DocIdBitSet\nand not have IR.isDeleted. ",
            "author": "Jason Rutherglen",
            "id": "comment-12666034"
        },
        {
            "date": "2009-01-23T20:17:09+0000",
            "content": "\n\nWe probably need to support both as implementing top level deleted\ndocs filtering may have unknown side effects. The user may decide\nbased on their queries and other variables such as the number of\ndeleted docs.\n\nI agree... and then, if the performance difference is large enough, it\nseems like we'll need some simple \"search policy\" for the interesting\n(Boolean) query scorers to pick the best way to execute a query.\n\nThis could include which order to visit the segments in (we broached\nthis in LUCENE-1483, since depending on the query different orders may\nperform better).  And when (high vs low) & how (iterator vs random\naccess) to apply a filter would also be decided by the search policy.\n\nDeprecating isDeleted might be good.\n\nI wonder how this method is used [externally] by applications,\ntoday... I'll go ask on java-user.  And, whether all such uses could\nmigrate to an iterator API instead without much cost.\n\nWould we need the read only readers?\n\nGood question... I'm guessing there would still be a performance\nbenefit if the underlying data structures for deletions &\ncolumn-stride fields know they cannot change? ",
            "author": "Michael McCandless",
            "id": "comment-12666681"
        },
        {
            "date": "2009-01-23T20:44:56+0000",
            "content": "\nFor the perf tests, I would use an optimized index with maybe 2M\nwikipedia docs in it.\n\nThen test with maybe 0, 1, 5, 10, 25, 50, 75 percent deletions, across\nvarious kinds of queries (single term, OR, AND, phrase/span).\nBaseline w/ trunk, and then test w/ this patch (keeps deletion access\nlow (@ SegmentTermDocs) but switches to iterator API).  I'd love to\nalso see numbers for deletion-applied-as-filter (high) eventually.\n\n[Actually if ever deletion %tg is > 50%, we should presumably invert\nthe bit set and work with that instead.  And same with filters.]\n\nYou might want to start with the Python scripts attached to\nLUCENE-1483; with some small mods you could easily fix them to run\nthese tests. ",
            "author": "Michael McCandless",
            "id": "comment-12666694"
        },
        {
            "date": "2009-01-23T21:02:29+0000",
            "content": "searchdeletes.alg uses reuters, deletes many docs, then performs\nsearches. If it's working properly, iteration rather than calling\nBitVector.get has some serious performance drawbacks. \n\nCompare:\nDocIdSet SrchNewRdr_8: 32.0 rec/s\nDelDoc.get SrchNewRdr_8: 2,959.5 rec/s\n\nNext step is running JProfiler. Perhaps BitVector needs to be\nreplaced by OpenBitSet for iterating, or there's some other issue. \n\nBitVector.get:\n[java] Operation         round mrg buf   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem\n[java] CreateIndex           0  10 100        1            1         17.2        0.06     3,953,984      9,072,640\n[java] CloseIndex -  -  -  - 0  10 100 -  -   1 -  -  -  - 1 -  - 1,000.0 -  -   0.00 -   3,953,984 -  - 9,072,640\n[java] Populate              0  10 100        1       200003      6,539.7       30.58     8,665,528     10,420,224\n[java] Deletions -  -  -  -  0  10 100 -  -   1 -  -  - 8002 -  533,466.7 -  -   0.01 -   8,665,528 -   10,420,224\n[java] OpenReader(false)     0  10 100        1            1      1,000.0        0.00     8,691,040     10,420,224\n[java] Seq_8000 -  -  -  -   0  10 100 -  -   1 -  -  - 8000 -  800,000.0 -  -   0.01 -   8,833,912 -   10,420,224\n[java] CloseReader           0  10 100        9            1      2,250.0        0.00     7,672,217     10,420,224\n[java] SrchNewRdr_8 -  -  -  0  10 100 -  -   1 -  -  - 4016 -  - 2,959.5 -  -   1.36 -   8,232,384 -   10,420,224\n[java] OpenReader            0  10 100        8            1      1,333.3        0.01     7,579,584     10,420,224\n[java] Seq_500 -  -  -  -  - 0  10 100 -  -   8 -  -  -  500 -  - 2,963.0 -  -   1.35 -   8,591,199 -   10,420,224\n\nDocIdSet:\n[java] Operation         round mrg buf   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem\n[java] CreateIndex           0  10 100        1            1         17.5        0.06     3,954,376      9,076,736\n[java] CloseIndex -  -  -  - 0  10 100 -  -   1 -  -  -  - 1 -  - 1,000.0 -  -   0.00 -   3,954,376 -  - 9,076,736\n[java] Populate              0  10 100        1       200003      6,503.1       30.75     5,951,816     10,321,920\n[java] Deletions -  -  -  -  0  10 100 -  -   1 -  -  - 8002 -  500,125.0 -  -   0.02 -   6,190,816 -   10,321,920\n[java] OpenReader(false)     0  10 100        1            1      1,000.0        0.00     5,976,960     10,321,920\n[java] Seq_8000 -  -  -  -   0  10 100 -  -   1 -  -  - 8000 -  727,272.8 -  -   0.01 -   6,122,904 -   10,321,920\n[java] CloseReader           0  10 100        9            1      3,000.0        0.00     7,727,980     10,321,920\n[java] SrchNewRdr_8 -  -  -  0  10 100 -  -   1 -  -  - 4016 -  -  - 32.0 -  - 125.67 -   7,960,824 -   10,321,920\n[java] OpenReader            0  10 100        8            1      1,333.3        0.01     7,742,855     10,321,920\n[java] Seq_500 -  -  -  -  - 0  10 100 -  -   8 -  -  -  500 -  -  - 31.8 -  - 125.66 -   8,744,057 -   10,321,920 ",
            "author": "Jason Rutherglen",
            "id": "comment-12666706"
        },
        {
            "date": "2009-01-23T21:10:58+0000",
            "content": "Jason can you format those results using a Jira table?  It's real hard to read as is (and something tells me we will be looking at alot of these tables shortly....  ).\n\nAlso, showing %tg change vs baseline helps. ",
            "author": "Michael McCandless",
            "id": "comment-12666711"
        },
        {
            "date": "2009-01-27T16:24:03+0000",
            "content": "The percentage performance decrease in the previous \nresults is 99%.  \n\n\nJason can you format those results using a Jira table? \n\nPerhaps this should be an option in the benchmark output?\n\n\nM.M. LUCENE-1516 comment: \"I think the larger number of\n[harder-for-cpu-to-predict] if statements may be the cause of the\nslowdown once %tg deletes gets high enough?\" \n\nI have been looking at the performance with YourKit and don't have\nany conclusions yet. The main difference between using skipto and \nBV.get is the if statements and some added method calls, which even \nif they are inlined I suspect will not make up the difference.\n\nNext steps: \n1. Deletes as a NOT boolean query which probably should\nbe it's own patch \n2.  Pluggable alternative representations such as\nOpenBitSet and int array, part of this patch? \n ",
            "author": "Jason Rutherglen",
            "id": "comment-12667713"
        },
        {
            "date": "2009-01-27T18:33:06+0000",
            "content": "> The percentage performance decrease in the previous\n> results is 99%. \n\nThat's pretty strange. I look forward to seeing profiling data. ",
            "author": "Marvin Humphrey",
            "id": "comment-12667748"
        },
        {
            "date": "2009-01-27T19:38:28+0000",
            "content": "Perhaps this should be an option in the benchmark output?\n\nThat's a great idea!\n\nSomething silly must be going on... 99% performance drop can't be right. ",
            "author": "Michael McCandless",
            "id": "comment-12667770"
        },
        {
            "date": "2009-01-28T22:54:06+0000",
            "content": "I created basic test code TestDeletesDocIdSet.java because I thought\nthe previous results may have something to do with my\nmisunderstanding of the contrib/benchmark code. \n\nThe results show a 17% increase in performance for this patch's\ndeleted docs skipto code. I think now that it's probably the fact\nthat ANT_OPTS isn't using the -server option properly as\ncontrib/benchmark build.xml forks the process. -server on the MacOSX\nseems to optimize method calls a far better than -client. \n\nTrunk:\nwarmup search duration: 6208\nfinal search duration: 6371\n\nLUCENE-1476, skipto:\nwarmup search duration: 5926\nfinal search duration: 5450 ",
            "author": "Jason Rutherglen",
            "id": "comment-12668207"
        },
        {
            "date": "2009-01-28T23:04:08+0000",
            "content": "Patch the previously mentioned performance test uses. ",
            "author": "Jason Rutherglen",
            "id": "comment-12668211"
        },
        {
            "date": "2009-01-29T01:35:51+0000",
            "content": "I implemented code that uses skipto deleted docs where BitVector is\nreplaced with OpenBitSet, and BitVector is faster. I'll save\nimplementing the int array docidset for the tombstone patch\nLUCENE-1526.\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12668273"
        },
        {
            "date": "2009-01-29T16:01:23+0000",
            "content": "\nI had some trouble w/ the latest patch (I'm trying to reproduce your\nstrange problem w/ contrib/benchmark & run perf tests):\n\n\n\tMultiDocIdSet was missing (I just pulled from earlier patch)\n\n\n\n\n\tCompilation errors because \n{Filter,Parallel}\nIndexReader failed to\n    mark getDeletedDocs public (I made them public).\n\n\n\n\n\tcontrib/instantiated failed to compile, because its IndexReader\n    impl failed to implement getDocIdSet... but we can't add abstract\n    IndexReader.getDocIdSet (this breaks back compat).  I switched to\n    non-abstract default method that throws\n    UnsupportedOperationException instead, and added nocommit comment\n    to remind us to fix all IndexReader subclasses in contrib.\n\n\n\n\n\tI don't think you should change MemoryIndexReader from private to\n    public?  Why was that done?  (I reverted this).\n\n\n\n\n\tReplaced MemoryIndex.NullDocIdSet with the new\n    DocIdSet.EMPTY_DOCIDSET\n\n\n\n\n\tSomehow you lost recent fixes to MemoryIndex.java (committed as\n    part of LUCENE-1316).  This was causing NPE test failure (I\n    reverted this, test passes now).\n\n\n\nNew patch attached.  All tests pass... next I'll try to repro the\ncontrib/benchmark oddness. ",
            "author": "Michael McCandless",
            "id": "comment-12668505"
        },
        {
            "date": "2009-01-29T17:24:54+0000",
            "content": "Hmm... when I run the TestDeletesDocIdSet, I don't see as much\nimprovement: trunk gets 10.507 seconds, patch gets 10.158 (~3.3%\nfaster).  I'm running on OS X 10.5.6, quad core Intel; java version is\n\"1.6.0_07-b06-153\" and I run \"java -server -Xbatch -Xmx1024M\n-Xms1024M\".\n\nBut that test is rather synthetic: you create 15,000 docs, then delete\n1 in 8, then do a search (for \"text\") that matches all of the docs.\n\nSo I went back to contrib/benchmark...  I created a single-segment\nindex, with 0%, 1%, 2%, 5%, 10%, 20% and 50% deletions, using first 2M\ndocs from Wikipedia, then ran 5 different queries and compared qps w/\npatch vs trunk.  Results:\n\n\n\n\n%tg deletes\nquery\nhits\nqps\nqpsnew\n%tg change\n\n\n0%\n147\n   4984\n5560.1\n5486.2\n -1.3%\n\n\n0%\ntext\n  97191\n 347.3\n 339.4\n -2.3%\n\n\n0%\n1 AND 2\n 234634\n  22.9\n  22.8\n -0.4%\n\n\n0%\n1\n 386435\n  88.4\n  87.2\n -1.4%\n\n\n0%\n1 OR 2\n 535584\n  20.9\n  20.9\n  0.0%\n\n\n1%\n147\n   4933\n5082.0\n1419.2\n-72.1%\n\n\n1%\ntext\n  96143\n 313.9\n 142.0\n-54.8%\n\n\n1%\n1 AND 2\n 232250\n  22.1\n  18.6\n-15.8%\n\n\n1%\n1\n 382498\n  81.0\n  62.2\n-23.2%\n\n\n1%\n1 OR 2\n 530212\n  20.2\n  17.5\n-13.4%\n\n\n2%\n147\n   4883\n5133.6\n1959.0\n-61.8%\n\n\n2%\ntext\n  95190\n 315.8\n 149.2\n-52.8%\n\n\n2%\n1 AND 2\n 229870\n  22.2\n  18.4\n-17.1%\n\n\n2%\n1\n 378641\n  81.2\n  58.9\n-27.5%\n\n\n2%\n1 OR 2\n 524873\n  20.3\n  17.1\n-15.8%\n\n\n5%\n147\n   4729\n5073.6\n2600.8\n-48.7%\n\n\n5%\ntext\n  92293\n 315.2\n 166.9\n-47.0%\n\n\n5%\n1 AND 2\n 222859\n  22.5\n  17.8\n-20.9%\n\n\n5%\n1\n 367000\n  81.0\n  56.2\n-30.6%\n\n\n5%\n1 OR 2\n 508632\n  20.4\n  16.3\n-20.1%\n\n\n10%\n147\n   4475\n5049.6\n2953.7\n-41.5%\n\n\n10%\ntext\n  87504\n 314.8\n 180.9\n-42.5%\n\n\n10%\n1 AND 2\n 210982\n  22.9\n  17.8\n-22.3%\n\n\n10%\n1\n 347664\n  81.5\n  53.8\n-34.0%\n\n\n10%\n1 OR 2\n 481792\n  21.2\n  16.5\n-22.2%\n\n\n20%\n147\n   4012\n5045.0\n3455.5\n-31.5%\n\n\n20%\ntext\n  77980\n 317.2\n 204.7\n-35.5%\n\n\n20%\n1 AND 2\n 187605\n  23.9\n  19.2\n-19.7%\n\n\n20%\n1\n 309040\n  82.0\n  54.7\n-33.3%\n\n\n20%\n1 OR 2\n 428232\n  22.3\n  17.5\n-21.5%\n\n\n50%\n147\n   2463\n5283.2\n4731.4\n-10.4%\n\n\n50%\ntext\n  48331\n 336.9\n 290.2\n-13.9%\n\n\n50%\n1 AND 2\n 116887\n  28.4\n  25.9\n -8.8%\n\n\n50%\n1\n 193154\n  86.4\n  74.9\n-13.3%\n\n\n50%\n1 OR 2\n 267525\n  27.6\n  24.9\n -9.8%\n\n\n\n\n\nI think one major source of slowness is the BitVector.nextSetBit()\nimpl: it now checks one bit at a time, but it'd be much better to use\nOpenBitSet's approach.\n\nI also think, realistically, this approach won't perform very well\nuntil we switch to a sparse representation for the bit set, so that\nnext() and skipTo() perform well. ",
            "author": "Michael McCandless",
            "id": "comment-12668523"
        },
        {
            "date": "2009-01-29T19:11:49+0000",
            "content": "Interesting results. Mike, can you post your test code? The Mac can\nbe somewhat unreliable for performance results as today the\nTestDeletesDocIdSet is about the same speed as trunk. \n\nOpenBitSet didn't seem to make much of a difference however when\nrunning your tests I can check it out. The other option is something\nlike P4Delta which stores the doc ids in a compressed form solely for\niterating. Is this what you mean by sparse representation?  ",
            "author": "Jason Rutherglen",
            "id": "comment-12668554"
        },
        {
            "date": "2009-01-29T19:26:35+0000",
            "content": "\nI'm attaching the Python code I use to run (it's adapted from lucene-1483).  You also need the following nocommit diff applied:\n\n\nIndex: contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java\n===================================================================\n--- contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java\t(revision 738896)\n+++ contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java\t(working copy)\n@@ -62,6 +62,9 @@\n     super(runData);\n   }\n \n+  // nocommit\n+  static boolean first = true;\n+\n   public int doLogic() throws Exception {\n     int res = 0;\n     boolean closeReader = false;\n@@ -102,6 +105,12 @@\n         }\n         //System.out.println(\"q=\" + q + \":\" + hits.totalHits + \" total hits\"); \n \n+        // nocommit\n+        if (first) {\n+          System.out.println(\"NUMHITS=\" + hits.totalHits);\n+          first = false;\n+        }\n+\n         if (withTraverse()) {\n           final ScoreDoc[] scoreDocs = hits.scoreDocs;\n           int traversalSize = Math.min(scoreDocs.length, traversalSize());\n\n\n\nJust run sortBench2.py in contrib/benchmark of trunk & patch areas.  Then run sortCollate2.py to make the Jira table (-jira) or print a human readable output (default).  You'll have to make your own Wikipedia indices with the pctg deletes, then edit sortBench2.py & sortCollate2.py to fix the paths.\n\nAll they do is write an alg file, run the test, and parse the output file to gather best of 5. ",
            "author": "Michael McCandless",
            "id": "comment-12668558"
        },
        {
            "date": "2009-01-29T20:17:45+0000",
            "content": "\nActually I made one mistake running your standalone test \u2013 I had\nallowed the \"createIndex\" to run more than once, and so I think I had\ntested 30K docs with 1875 deletes (6.25%).\n\nI just removed the index and recreated it, so I have 15K docs and 1875\ndeletes (12.5%).  On the mac pro I now see the patch at 4.0% slower\n(4672 ms to 4859 ms), and on a Debian Linux box (kernel 2.6.22.1, java\n1.5.0_08-b03) I see it 0.8% slower (7298 ms to 7357 ms).\n\nThe Mac can be somewhat unreliable for performance results \n\nI've actually found it to be quite reliable.  What I love most about\nit is, as long as you shut down all extraneous processes, it gives\nvery repeatable results.  I haven't found the same true (or, less so)\nof various Linux's & Windows.\n\nOpenBitSet didn't seem to make much of a difference\n\nThis is very hard to believe \u2013 the nextSetBit impl in BitVector (in\nthe patch) is extremely inefficient.  OpenBitSet's impl ought to be\nmuch faster.\n\n\nThe other option is something like P4Delta which stores the doc\nids in a compressed form solely for iterating.\n\nI think that will be too costly here (but is a good fit for\npostings).\n\nIs this what you mean by sparse representation?\n\nActually I meant a simple sorted list of ints, but even for that I'm\nworried about the skipTo cost (if we use a normal binary search).  I'm\nnot sure it can be made fast enough (ie faster than random access\nwe have today). ",
            "author": "Michael McCandless",
            "id": "comment-12668583"
        },
        {
            "date": "2009-01-29T22:08:07+0000",
            "content": "bq: shut down all extraneous processes\n\nIt's a desktop machine though so it's going to have some stuff\nrunning the background, most of which I'm not aware of being a Mac\nnewbie.\n\nbq: Actually I meant a simple sorted list of ints, but even for that\nI'm worried about the skipTo cost (if we use a normal binary search)\n\nSkipping is slower because it unnecessarily checks bits that are not\nuseful to the query. A higher level deletions Filter implemented\nperhaps in IndexSearcher requires docs that are deleted, pass through\nthe SegmentTermDocs doc[] cache which could add unnecessary overhead\nfrom the vint decoding. \n\nThe main problem we're trying to solve is potential allocation of a\nlarge del docs BV byte array for the copy on write of a cloned\nreader. An option we haven't looked at is a MultiByteArray where\nmultiple byte arrays make up a virtual byte array checked by BV.get.\nOn deleteDocument, only the byte array chunks that are changed are\nreplaced in the new version, while the previously copied chunks are\nkept. The overhead of the BV.get can be minimal, though in our tests\nwith an int array version the performance can either be equal to or\ndouble based on factors we are not aware of.  ",
            "author": "Jason Rutherglen",
            "id": "comment-12668630"
        },
        {
            "date": "2009-01-29T22:26:15+0000",
            "content": "> the nextSetBit impl in BitVector (in the patch) is extremely inefficient.\n\nHere's an improved version that doesn't call get(). \n(quasi_iterator_deletions_r3.diff)\n\nThe private method firstBitInNonZeroByte() in this patch could potentially \nbe replaced with a 256-byte lookup table. ",
            "author": "Marvin Humphrey",
            "id": "comment-12668642"
        },
        {
            "date": "2009-01-30T00:03:42+0000",
            "content": "Numbers with Marvin's latest patch:\n\n\n\n\n%tg deletes\nquery\nhits\nqps\nqpsnew\npctg\n\n\n0%\n147\n   4984\n5560.1\n5507.8\n -0.9%\n\n\n0%\ntext\n  97191\n 347.3\n 338.8\n -2.4%\n\n\n0%\n1 AND 2\n 234634\n  22.9\n  22.8\n -0.4%\n\n\n0%\n1\n 386435\n  88.4\n  86.9\n -1.7%\n\n\n0%\n1 OR 2\n 535584\n  20.9\n  20.7\n -1.0%\n\n\n1%\n147\n   4933\n5082.0\n3292.2\n-35.2%\n\n\n1%\ntext\n  96143\n 313.9\n 260.8\n-16.9%\n\n\n1%\n1 AND 2\n 232250\n  22.1\n  21.9\n -0.9%\n\n\n1%\n1\n 382498\n  81.0\n  79.6\n -1.7%\n\n\n1%\n1 OR 2\n 530212\n  20.2\n  20.3\n  0.5%\n\n\n2%\n147\n   4883\n5133.6\n3092.0\n-39.8%\n\n\n2%\ntext\n  95190\n 315.8\n 232.8\n-26.3%\n\n\n2%\n1 AND 2\n 229870\n  22.2\n  21.2\n -4.5%\n\n\n2%\n1\n 378641\n  81.2\n  76.4\n -5.9%\n\n\n2%\n1 OR 2\n 524873\n  20.3\n  19.8\n -2.5%\n\n\n5%\n147\n   4729\n5073.6\n3478.5\n-31.4%\n\n\n5%\ntext\n  92293\n 315.2\n 219.1\n-30.5%\n\n\n5%\n1 AND 2\n 222859\n  22.5\n  20.5\n -8.9%\n\n\n5%\n1\n 367000\n  81.0\n  68.5\n-15.4%\n\n\n5%\n1 OR 2\n 508632\n  20.4\n  18.9\n -7.4%\n\n\n10%\n147\n   4475\n5049.6\n3547.8\n-29.7%\n\n\n10%\ntext\n  87504\n 314.8\n 222.2\n-29.4%\n\n\n10%\n1 AND 2\n 210982\n  22.9\n  19.4\n-15.3%\n\n\n10%\n1\n 347664\n  81.5\n  61.6\n-24.4%\n\n\n10%\n1 OR 2\n 481792\n  21.2\n  18.4\n-13.2%\n\n\n20%\n147\n   4012\n5045.0\n3741.8\n-25.8%\n\n\n20%\ntext\n  77980\n 317.2\n 232.9\n-26.6%\n\n\n20%\n1 AND 2\n 187605\n  23.9\n  20.5\n-14.2%\n\n\n20%\n1\n 309040\n  82.0\n  59.0\n-28.0%\n\n\n20%\n1 OR 2\n 428232\n  22.3\n  18.8\n-15.7%\n\n\n50%\n147\n   2463\n5283.2\n4712.1\n-10.8%\n\n\n50%\ntext\n  48331\n 336.9\n 296.2\n-12.1%\n\n\n50%\n1 AND 2\n 116887\n  28.4\n  26.6\n -6.3%\n\n\n50%\n1\n 193154\n  86.4\n  77.3\n-10.5%\n\n\n50%\n1 OR 2\n 267525\n  27.6\n  25.7\n -6.9%\n\n\n\n\n\nIt's definitely better than before but still slower than trunk. ",
            "author": "Michael McCandless",
            "id": "comment-12668672"
        },
        {
            "date": "2009-01-30T00:28:15+0000",
            "content": "\n\nThe main problem we're trying to solve is potential allocation of a\nlarge del docs BV byte array for the copy on write of a cloned\nreader.\n\nRight, as long as normal search performance does not get worse.\nActually, I was hoping \"deletes as iterator\" and \"deletes higher up as\nfilter\" might give us some gains in search performance.\n\n\nAn option we haven't looked at is a MultiByteArray where\nmultiple byte arrays make up a virtual byte array checked by BV.get.\nOn deleteDocument, only the byte array chunks that are changed are\nreplaced in the new version, while the previously copied chunks are\nkept. The overhead of the BV.get can be minimal, though in our tests\nwith an int array version the performance can either be equal to or\ndouble based on factors we are not aware of. \n\nI think that'd be a good approach (it amortizes the copy on write\ncost), though it'd be a double deref per lookup with the\nstraightforward impl so I think it'll hurt normal search perf too.\n\nAnd I don't think we should give up on iterator access just yet... I\nthink we should try list-of-sorted-ints?\n ",
            "author": "Michael McCandless",
            "id": "comment-12668675"
        },
        {
            "date": "2009-01-30T04:24:32+0000",
            "content": "> Numbers with Marvin's latest patch:\n\nPresumably you spliced the improved nextSetBit into Jason's patch, correct?  I wonder how my patch on its own would do, since there's less abstraction. Didn't you have a close-to-ideal patch using sorted ints that performed well up to 10% deletions?  What did that look like?\n\n> I think we should try list-of-sorted-ints?\n\nThat should help with the situation where deletes are sparse, particularly when the term is rare (such as those searches for \"147\"), since it will remove the cost of scanning through a bunch of empty bits.\n\nI'm also curious what happens if we do without the null-check here:\n\n\n+      if (deletedDocsIt != null) {\n+        if (doc > nextDeletion) {\n+          if (deletedDocsIt.skipTo(doc)) \n+            nextDeletion = deletedDocsIt.doc();\n+        } \n+        if (doc == nextDeletion)\n+          continue;\n       }\n\n\n\nWhen there are no deletions, nextDeletion is set and left at Integer.MAX_VALUE, so we'd get a comparison that's always false for the life of the TermDocs instead of an always-null null check. Possibly we'd slow down the no-deletions case while speeding up all others, but maybe the processor does a good job at predicting the result of the comparison.\n\nI also suspect that when there are many deletions, the sheer number of method calls to perform the deletions iteration is a burden.  The iterator has to compete with an inline-able method from a final class (BitVector).\n ",
            "author": "Marvin Humphrey",
            "id": "comment-12668740"
        },
        {
            "date": "2009-01-30T09:53:34+0000",
            "content": "Presumably you spliced the improved nextSetBit into Jason's patch, correct?\n\nActually I used your entire patch on its own.\n\nDidn't you have a close-to-ideal patch using sorted ints that performed well up to 10% deletions? What did that look like?\n\nI thought I did \u2013 but it was rather hacked up (I \"fixed\" SegmentReader to do always do an up-front conversion into int[] deletedDocs).  I'll re-test it to try to repro my initial rough results.\n\nI also suspect that when there are many deletions, the sheer number of method calls to perform the deletions iteration is a burden. The iterator has to compete with an inline-able method from a final class (BitVector).\n\nRight, for a highish %tg deletion it seems likely that random-access will win. ",
            "author": "Michael McCandless",
            "id": "comment-12668797"
        },
        {
            "date": "2009-01-30T12:13:29+0000",
            "content": "Alas.... I had a bug in my original test (my SegmentTermDocs was\nincorrectly returning some deleted docs).  But even with that bug I\ncan't repro my original \"it's faster at < 10% deletions\".  Here are my\nresults using a pre-computed array of deleted docIDs:\n\n\n\n\n%tg deletes\nquery\nhits\nqps\nqpsnew\npctg\n\n\n0%\n147\n   4984\n5560.1\n5392.5\n -3.0%\n\n\n0%\ntext\n  97191\n 347.3\n 334.1\n -3.8%\n\n\n0%\n1 AND 2\n 234634\n  22.9\n  22.8\n -0.4%\n\n\n0%\n1\n 386435\n  88.4\n  86.0\n -2.7%\n\n\n0%\n1 OR 2\n 535584\n  20.9\n  20.8\n -0.5%\n\n\n1%\n147\n   4933\n5082.0\n3643.5\n-28.3%\n\n\n1%\ntext\n  96143\n 313.9\n 304.9\n -2.9%\n\n\n1%\n1 AND 2\n 232250\n  22.1\n  22.3\n  0.9%\n\n\n1%\n1\n 382498\n  81.0\n  82.3\n  1.6%\n\n\n1%\n1 OR 2\n 530212\n  20.2\n  20.2\n  0.0%\n\n\n2%\n147\n   4883\n5133.6\n3299.6\n-35.7%\n\n\n2%\ntext\n  95190\n 315.8\n 289.7\n -8.3%\n\n\n2%\n1 AND 2\n 229870\n  22.2\n  22.1\n -0.5%\n\n\n2%\n1\n 378641\n  81.2\n  80.9\n -0.4%\n\n\n2%\n1 OR 2\n 524873\n  20.3\n  20.2\n -0.5%\n\n\n5%\n147\n   4729\n5073.6\n2405.2\n-52.6%\n\n\n5%\ntext\n  92293\n 315.2\n 259.0\n-17.8%\n\n\n5%\n1 AND 2\n 222859\n  22.5\n  22.0\n -2.2%\n\n\n5%\n1\n 367000\n  81.0\n  77.6\n -4.2%\n\n\n5%\n1 OR 2\n 508632\n  20.4\n  19.7\n -3.4%\n\n\n10%\n147\n   4475\n5049.6\n1738.8\n-65.6%\n\n\n10%\ntext\n  87504\n 314.8\n 232.6\n-26.1%\n\n\n10%\n1 AND 2\n 210982\n  22.9\n  21.7\n -5.2%\n\n\n10%\n1\n 347664\n  81.5\n  74.0\n -9.2%\n\n\n10%\n1 OR 2\n 481792\n  21.2\n  20.2\n -4.7%\n\n\n20%\n147\n   4012\n5045.0\n1117.6\n-77.8%\n\n\n20%\ntext\n  77980\n 317.2\n 208.9\n-34.1%\n\n\n20%\n1 AND 2\n 187605\n  23.9\n  21.4\n-10.5%\n\n\n20%\n1\n 309040\n  82.0\n  68.2\n-16.8%\n\n\n20%\n1 OR 2\n 428232\n  22.3\n  20.2\n -9.4%\n\n\n50%\n147\n   2463\n5283.2\n 522.3\n-90.1%\n\n\n50%\ntext\n  48331\n 336.9\n 176.4\n-47.6%\n\n\n50%\n1 AND 2\n 116887\n  28.4\n  23.0\n-19.0%\n\n\n50%\n1\n 193154\n  86.4\n  63.5\n-26.5%\n\n\n50%\n1 OR 2\n 267525\n  27.6\n  22.4\n-18.8%\n\n\n\n\n\nI've attached my patch, but note that some tests fail because I don't update the list of deleted docs when deleteDocument is called.\n\nI'm now feeling like we're gonna have to keep random-access to deleted docs.... ",
            "author": "Michael McCandless",
            "id": "comment-12668838"
        },
        {
            "date": "2009-01-30T17:38:14+0000",
            "content": "\n> Actually I used your entire patch on its own.\n\nAh.  Thanks for running all these benchmarks.  (I'm going to have to port the\nbenchmark suite sooner rather than later so that Lucy doesn't have to rely on\nhaving algos worked out under Lucene.)\n\nI think it's important to note that the degradation at low deletion\npercentages is not as bad as the chart seems to imply.  The worst performance\nwas on the cheapest query, and the best performance was on the most expensive\nquery.\n\nFurthermore, a 50% deletion percentage is very high.  Regardless of how\ndeletions are implemented, the default merge policy ought to trigger\nabsorbtion of segments with deletion percentages over a certain threshold.\nThe actual threshold percentage is an arbitrary number because the ideal\ntradeoff between index-time sacrifices and search-time sacrifices varies, but\nmy gut pick would be somewhere between 10% and 30% for bit vector deletions\nand between 5% and 20% for iterated deletions.\n\n> I'm now feeling like we're gonna have to keep random-access to deleted\n> docs....\n\nHa, you're giving up a little early, amigo.  \n\nI do think it's clear that maintaining an individual deletions iterator for\neach posting list isn't going to work well, particularly when it's implemented\nusing an opaque DocIdSetIterator and virtual methods.  That can't compete with\nwhat is essentially an array lookup (since BitVector.get() can be inlined) on\na shared bit vector, even at low deletion rates.\n\nI also think we can conclude that high deletion rates cause an accelerating\ndegradation with iterated deletions, and that if they are implemented, that\nproblem probably needs to be addressed with more aggressive segment merging.\n\nHowever, I don't think the benchmark data we've seen so far demonstrates that\nthe filtered deletions model won't work.  Heck, with that model,\ndeletions get pulled out out TermDocs so we lose the per-iter null-check,\npossibly yielding a small performance increase in the common case of zero\ndeletions.\n\nMaybe we should close this issue with a won't-fix and start a new one for\nfiltered deletions? ",
            "author": "Marvin Humphrey",
            "id": "comment-12668946"
        },
        {
            "date": "2009-01-30T17:56:39+0000",
            "content": "> Maybe we should close this issue with a won't-fix and start a new one for filtered deletions?\n\nWe need more performance data before going ahead with tombstone deletions.  Sounds like the next step.  ",
            "author": "Jason Rutherglen",
            "id": "comment-12668954"
        },
        {
            "date": "2009-01-30T18:34:54+0000",
            "content": "\nJust run sortBench2.py in contrib/benchmark of trunk & patch\nareas. Then run sortCollate2.py to make the Jira table (-jira) or\nprint a human readable output (default). You'll have to make your own\nWikipedia indices with the pctg deletes, then edit sortBench2.py &\nsortCollate2.py to fix the paths.\n\nAll they do is write an alg file, run the test, and parse the output\nfile to gather best of 5. \n\nThis seems like something we can port to Java and get into\ncontrib/benchmark. Particularly automatically creating the indexes. ",
            "author": "Jason Rutherglen",
            "id": "comment-12668971"
        },
        {
            "date": "2009-01-30T21:07:55+0000",
            "content": "\nThanks for running all these benchmarks. (I'm going to have to port the\nbenchmark suite sooner rather than later so that Lucy doesn't have to rely on\nhaving algos worked out under Lucene.)\n\nNo problem  And I don't mind working these things out / discussing\nthem in Lucene \u2013 many of these ideas turn out very well!  EG\nLUCENE-1483.\n\nThough... iteration may work better in C than it does in Java, so it's\nhard to know what to conclude here for Lucy.\n\n\nI think it's important to note that the degradation at low deletion\npercentages is not as bad as the chart seems to imply. The worst performance\nwas on the cheapest query, and the best performance was on the most expensive\nquery.\n\nTrue.\n\n\nFurthermore, a 50% deletion percentage is very high. Regardless of how\ndeletions are implemented, the default merge policy ought to trigger\nabsorbtion of segments with deletion percentages over a certain threshold.\nThe actual threshold percentage is an arbitrary number because the ideal\ntradeoff between index-time sacrifices and search-time sacrifices varies, but\nmy gut pick would be somewhere between 10% and 30% for bit vector deletions\nand between 5% and 20% for iterated deletions.\n\nRight, we should have the default merge policy try to coalsce segments\nwith too many deletions (Lucene does not do this today \u2013 you have to\ncall expungeDeletes yourself, and even that's not perfect since it\neliminates every single delete).\n\nHa, you're giving up a little early, amigo.\n\n\n\n\nI do think it's clear that maintaining an individual deletions iterator for\neach posting list isn't going to work well, particularly when it's implemented\nusing an opaque DocIdSetIterator and virtual methods. That can't compete with\nwhat is essentially an array lookup (since BitVector.get() can be inlined) on\na shared bit vector, even at low deletion rates.\n\nRight.\n\n\nI also think we can conclude that high deletion rates cause an accelerating\ndegradation with iterated deletions, and that if they are implemented, that\nproblem probably needs to be addressed with more aggressive segment merging.\n\nAgreed.\n\n\nHowever, I don't think the benchmark data we've seen so far demonstrates that\nthe filtered deletions model won't work. Heck, with that model,\ndeletions get pulled out out TermDocs so we lose the per-iter null-check,\npossibly yielding a small performance increase in the common case of zero\ndeletions.\n\nBy \"filtered deletions model\", you mean treating deletions as a\n\"normal\" filter and moving \"up\" when they are applied?  Right, we have\nnot explored that yet, here.\n\nExcept, filters are accessed by iteration in Lucene now (they used to\nbe random access before LUCENE-584).\n\nSo... now I'm wondering if we should go back and scrutinize the\nperformance cost of switching from random access to iteration for\nfilters, especially in cases where under-the-hood the filter needed to\ncreate a non-sparse repr anyway.  It looks like there was a fair\namount of discussion about performance, but no hard conclusions, in\nLUCENE-584, on first read.  Hmmm.\n\nI think to do a fair test, we need to move deleted docs \"up higher\",\nbut access it w/ random access API?\n\nExcept... the boolean query QPS are not that different with the\nno-deletions case vs the 1% deletions.  And it's only the non-simple\nqueries (boolean, span, phrase, etc.) that would see any difference\nin moving deletions \"up\" as a filter?  Ie their QPS is already so low\nthat the added cost of doing the deletes \"at the bottom\" appears to be\nquite minor.\n\n(Conceptually, I completely agree that deletions are really the same\nthing as filters).\n\n\nMaybe we should close this issue with a won't-fix and start a new one for\nfiltered deletions?\nMaybe discuss a bit more here or on java-dev first? ",
            "author": "Michael McCandless",
            "id": "comment-12669024"
        },
        {
            "date": "2009-01-30T21:12:01+0000",
            "content": "\nThis seems like something we can port to Java and get into\ncontrib/benchmark. Particularly automatically creating the indexes.\nThat would be great!  Maybe it's not so much work... contrib/benchmark can already do the \"rounds\" to enumerate through the different combinations of query, index, etc.\n\nAutomatically creating the indices is trickier; you'd want to 1) create the first index, 2) copy it and do X%tg deletions, 3) repeat 2 for different values of X.\n\nAlso tricky is gathering a series of results and saving it somehow so that you can then make a baseline for other runs.\n\nOr we could keep using Python (pickling is so easy ).  It's nice to have a \"scripting\" layer on top of contrib/benchmark. ",
            "author": "Michael McCandless",
            "id": "comment-12669025"
        },
        {
            "date": "2009-01-30T21:19:25+0000",
            "content": "We need more performance data before going ahead with tombstone deletions. Sounds like the next step. \n\nOr, I think morphing the tombstones idea into the \"incremental copy on write block BitVector\" idea you mentioned, is a good next step to explore (for realtime search). ",
            "author": "Michael McCandless",
            "id": "comment-12669026"
        },
        {
            "date": "2011-01-24T21:16:47+0000",
            "content": "Won't be working on these and they're old ",
            "author": "Jason Rutherglen",
            "id": "comment-12986026"
        }
    ]
}