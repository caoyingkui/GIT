{
    "id": "LUCENE-2617",
    "title": "coord should still apply to missing terms/clauses",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Bug",
        "fix_versions": [
            "3.1",
            "4.0-ALPHA"
        ],
        "affect_versions": "3.1,                                            4.0-ALPHA",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Missing terms in a boolean query \"disappear\" (i.e. they don't even affect the coord factor).",
    "attachments": {
        "LUCENE-2617.patch": "https://issues.apache.org/jira/secure/attachment/12452891/LUCENE-2617.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-08-24T01:45:38+0000",
            "content": "For solr users, this is easy to see in the example data.\n\nThe following query has a coord of 1/2 for the solr document (since apple appears in a different doc).  Press CTRL-U to see the explain properly formatted:\n\nhttp://localhost:8983/solr/select?q=solr%20apple&debugQuery=true\n\nThe following query has no coord since zzz does not appear in the example corpus:\n\nhttp://localhost:8983/solr/select?q=solr%20zzz&debugQuery=true ",
            "author": "Yonik Seeley",
            "id": "comment-12901695"
        },
        {
            "date": "2010-08-24T01:49:32+0000",
            "content": "This happens because of short circuiting during scorer creation - if the term doesn't exist in the index, then TermWeight.scorer() returns null.  In the higher level BooleanWeight.scorer(), the clause is completely disregarded (unless it is mandatory). ",
            "author": "Yonik Seeley",
            "id": "comment-12901698"
        },
        {
            "date": "2010-08-24T01:55:15+0000",
            "content": "OK, the fix should be easy - just keep track of maxcoord in the weights and pass it into the scorer instead of having the scorers add it up.  I'll get to it tomorrow. ",
            "author": "Yonik Seeley",
            "id": "comment-12901701"
        },
        {
            "date": "2010-08-24T04:35:02+0000",
            "content": "Here's an unfinished (and untested) draft patch. ",
            "author": "Yonik Seeley",
            "id": "comment-12901726"
        },
        {
            "date": "2010-08-24T06:01:54+0000",
            "content": "Wouldn't it be easier to use a non matching Scorer instead of null? ",
            "author": "Paul Elschot",
            "id": "comment-12901740"
        },
        {
            "date": "2010-08-24T13:27:52+0000",
            "content": "Wouldn't it be easier to use a non matching Scorer instead of null?\n\nThat was my bias, but people decided against that in LUCENE-1754\n ",
            "author": "Yonik Seeley",
            "id": "comment-12901874"
        },
        {
            "date": "2010-08-24T13:42:43+0000",
            "content": "OK, this bug does manifest in 3.1 also - just not with term queries.  A phrase query of non-existing terms can tickle it:\nhttp://localhost:8983/solr/select?q=solr+\"zzz+zzz\"&debugQuery=true\n\nThis does still work on Solr 1.4/lLucene 2.9.  Anyone know if null is actually ever returned from Weight.scorer() in 2.9? ",
            "author": "Yonik Seeley",
            "id": "comment-12901880"
        },
        {
            "date": "2010-08-24T13:50:47+0000",
            "content": "Are we sure that non-existent terms/phrases should contribute to coord?  I have no idea which way is \"right\"   I'm just asking...\n\nAnyone know if null is actually ever returned from Weight.scorer() in 2.9?\n\nHmm LUCENE-1754 was fixed in 2.9?  Didn't in bring allowed null return from Weight.scorer()? ",
            "author": "Michael McCandless",
            "id": "comment-12901882"
        },
        {
            "date": "2010-08-24T13:58:46+0000",
            "content": "> Anyone know if null is actually ever returned from Weight.scorer() in 2.9?\n\nThis behavior goes back a long time \u2013 at least back as far as svn trunk just\nprior to 1.9, which is what I worked off of when I was porting. ",
            "author": "Marvin Humphrey",
            "id": "comment-12901886"
        },
        {
            "date": "2010-08-24T14:17:16+0000",
            "content": "Amazingly, I just hit a random test failure (seed = 5148889059941714836L reproduces it):\n\n\n    [junit] Testsuite: org.apache.lucene.search.TestMultiSearcherRanking\n    [junit] Testcase: testTwoTermQuery(org.apache.lucene.search.TestMultiSearcherRanking):\tFAILED\n    [junit] expected:<0.48314467> but was:<0.60746753>\n    [junit] junit.framework.AssertionFailedError: expected:<0.48314467> but was:<0.60746753>\n    [junit] \tat org.apache.lucene.search.TestMultiSearcherRanking.checkQuery(TestMultiSearcherRanking.java:102)\n    [junit] \tat org.apache.lucene.search.TestMultiSearcherRanking.testTwoTermQuery(TestMultiSearcherRanking.java:47)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:380)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.run(LuceneTestCase.java:372)\n\n\n\nThe failure is in fact caused by this issue!  (The patch fixes it).  The RandomIndexWriter in this test w/ the above seed creates a multi-segment index, and then tests on a 2-term BQ.  Some segments did/didn't have both of the terms and this causes the score difference.\n\nSo...  I think we really should fix this, for consistency.  Otherwise, how your docs fall into segments can alter your scores, which is awful. ",
            "author": "Michael McCandless",
            "id": "comment-12901896"
        },
        {
            "date": "2010-08-24T14:21:30+0000",
            "content": "So... I think we really should fix this, for consistency. Otherwise, how your docs fall into segments can alter your scores, which is awful.\n\n+1 ",
            "author": "Robert Muir",
            "id": "comment-12901902"
        },
        {
            "date": "2010-08-24T14:24:50+0000",
            "content": "Are we sure that non-existent terms/phrases should contribute to coord?\n\nYep \n\nAdding an unrelated document and having scores radically change on other docs isn't a good thing.\nIt also makes testing and relevancy tuning hard (hoss & I were really scratching our heads last night trying to figure out why coord wasn't being applied).  idf is currently the only similarity factor that varies with index size + content... adding coord to that list wouldn't be good.\n\nActually, I just thought of something potentially much worse.  With per-segment searching, doesn't this make coord vary per-segment?  That means we're adding up scores that aren't comparable... and all of a sudden this is looking like a much worse bug.  A match in a very small segment is likely to contribute much more to a score than a match in a large segment. ",
            "author": "Yonik Seeley",
            "id": "comment-12901904"
        },
        {
            "date": "2010-08-24T14:40:50+0000",
            "content": "All tests pass for me: I'll commit shortly to trunk and then backport to 3.1 ",
            "author": "Yonik Seeley",
            "id": "comment-12901909"
        },
        {
            "date": "2010-08-27T16:31:53+0000",
            "content": "I backported the test to the 3.0 branch, and everything looks good.\nBoth 2.9 and 3.0 remove null scorers w/o accounting for them in coord, but I don't believe we actually ever return any null scorers in those versions. ",
            "author": "Yonik Seeley",
            "id": "comment-12903467"
        },
        {
            "date": "2011-03-30T15:49:54+0000",
            "content": "Bulk close for 3.1 ",
            "author": "Grant Ingersoll",
            "id": "comment-13013298"
        }
    ]
}