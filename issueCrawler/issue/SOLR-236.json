{
    "id": "SOLR-236",
    "title": "Field collapsing",
    "details": {
        "affect_versions": "1.3",
        "status": "Closed",
        "fix_versions": [
            "3.3"
        ],
        "components": [
            "search"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Duplicate"
    },
    "description": "This patch include a new feature called \"Field collapsing\".\n\n\"Used in order to collapse a group of results with similar value for a given field to a single entry in the result set. Site collapsing is a special case of this, where all results for a given web site is collapsed into one or two entries in the result set, typically with an associated \"more documents from this site\" link. See also Duplicate detection.\"\nhttp://www.fastsearch.com/glossary.aspx?m=48&amid=299\n\nThe implementation add 3 new query parameters (SolrParams):\n\"collapse.field\" to choose the field used to group results\n\"collapse.type\" normal (default value) or adjacent\n\"collapse.max\" to select how many continuous results are allowed before collapsing\n\nTODO (in progress):\n\n\tMore documentation (on source code)\n\tTest cases\n\n\n\nTwo patches:\n\n\t\"field_collapsing.patch\" for current development version\n\t\"field_collapsing_1.1.0.patch\" for Solr-1.1.0\n\n\n\n\nP.S.: Feedback and misspelling correction are welcome",
    "attachments": {
        "NonAdjacentDocumentCollapser.java": "https://issues.apache.org/jira/secure/attachment/12437981/NonAdjacentDocumentCollapser.java",
        "field-collapse-solr-236-2.patch": "https://issues.apache.org/jira/secure/attachment/12409459/field-collapse-solr-236-2.patch",
        "collapsing-patch-to-1.3.0-ivan_3.patch": "https://issues.apache.org/jira/secure/attachment/12396300/collapsing-patch-to-1.3.0-ivan_3.patch",
        "SOLR-236-1_4_1-NPEfix.patch": "https://issues.apache.org/jira/secure/attachment/12470202/SOLR-236-1_4_1-NPEfix.patch",
        "field_collapsing_dsteigerwald.diff": "https://issues.apache.org/jira/secure/attachment/12372522/field_collapsing_dsteigerwald.diff",
        "field_collapsing_1.3.patch": "https://issues.apache.org/jira/secure/attachment/12368570/field_collapsing_1.3.patch",
        "field-collapse-5.patch": "https://issues.apache.org/jira/secure/attachment/12417524/field-collapse-5.patch",
        "field-collapse-solr-236.patch": "https://issues.apache.org/jira/secure/attachment/12409377/field-collapse-solr-236.patch",
        "collapsing-patch-to-1.3.0-dieter.patch": "https://issues.apache.org/jira/secure/attachment/12399009/collapsing-patch-to-1.3.0-dieter.patch",
        "SOLR-236.patch": "https://issues.apache.org/jira/secure/attachment/12428295/SOLR-236.patch",
        "SOLR-236-1_4_1-paging-totals-working.patch": "https://issues.apache.org/jira/secure/attachment/12459716/SOLR-236-1_4_1-paging-totals-working.patch",
        "SOLR-236-branch_3x.patch": "https://issues.apache.org/jira/secure/attachment/12471418/SOLR-236-branch_3x.patch",
        "SOLR-236-distinctFacet.patch": "https://issues.apache.org/jira/secure/attachment/12459815/SOLR-236-distinctFacet.patch",
        "field-collapse-3.patch": "https://issues.apache.org/jira/secure/attachment/12414513/field-collapse-3.patch",
        "SOLR-236-FieldCollapsing.patch": "https://issues.apache.org/jira/secure/attachment/12358794/SOLR-236-FieldCollapsing.patch",
        "collapsing-patch-to-1.3.0-ivan_2.patch": "https://issues.apache.org/jira/secure/attachment/12395742/collapsing-patch-to-1.3.0-ivan_2.patch",
        "DocSetScoreCollector.java": "https://issues.apache.org/jira/secure/attachment/12437980/DocSetScoreCollector.java",
        "field-collapsing-extended-592129.patch": "https://issues.apache.org/jira/secure/attachment/12369053/field-collapsing-extended-592129.patch",
        "SOLR-236-1_4_1.patch": "https://issues.apache.org/jira/secure/attachment/12448216/SOLR-236-1_4_1.patch",
        "collapsing-patch-to-1.3.0-ivan.patch": "https://issues.apache.org/jira/secure/attachment/12393877/collapsing-patch-to-1.3.0-ivan.patch",
        "SOLR-236_collapsing.patch": "https://issues.apache.org/jira/secure/attachment/12403590/SOLR-236_collapsing.patch",
        "field_collapsing_1.1.0.patch": "https://issues.apache.org/jira/secure/attachment/12357681/field_collapsing_1.1.0.patch",
        "field-collapse-4-with-solrj.patch": "https://issues.apache.org/jira/secure/attachment/12416108/field-collapse-4-with-solrj.patch",
        "quasidistributed.additional.patch": "https://issues.apache.org/jira/secure/attachment/12424470/quasidistributed.additional.patch",
        "SOLR-236-trunk.patch": "https://issues.apache.org/jira/secure/attachment/12440022/SOLR-236-trunk.patch",
        "NonAdjacentDocumentCollapserTest.java": "https://issues.apache.org/jira/secure/attachment/12437982/NonAdjacentDocumentCollapserTest.java",
        "solr-236.patch": "https://issues.apache.org/jira/secure/attachment/12383622/solr-236.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Emmanuel Keller",
            "id": "comment-12495188",
            "date": "2007-05-11T22:14:42+0000",
            "content": "Field Collapsing "
        },
        {
            "author": "Emmanuel Keller",
            "id": "comment-12495193",
            "date": "2007-05-11T22:48:48+0000",
            "content": "Remplacing HashDocSet by BitDocSet for hasMoreResult for better performances "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12495334",
            "date": "2007-05-13T01:59:11+0000",
            "content": "This looks good.  Someone with better lucene chops should look at the IndexSearcher getDocListAndSet part...\n\nA few comments/questions about the interface:\n\nIf you apply all the example docs and hit:\nhttp://localhost:8983/solr/select/?q=*:*&collapse=true\n\nyou get 500.  We should use:  params.required().get( \"collapse.field\" ) to have a nicer error:\n\nWith:\nhttp://localhost:8983/solr/select/?q=*:*&collapse=true&collapse.field=manu&collapse.max=1\n\nthe collapse info at the bottom says:\n\n<lst name=\"collapse_counts\">\n <int name=\"has_more_results\">3</int>\n <int name=\"has_more_results\">5</int>\n <int name=\"has_more_results\">9</int>\n</lst>\n\nwhat does that mean?  How would you use it? How does it relate to the <result docs?\n\n\n\n\n\n\n "
        },
        {
            "author": "Emmanuel Keller",
            "id": "comment-12495356",
            "date": "2007-05-13T11:03:05+0000",
            "content": "My turn to miss something \nYou are right, we have to use params.required().get(\"collapse.field\"). \n\nAbout collapse info:\n<int name=\"has_more_results\">3</int> means that the third doc of the result has been collapsed and that some consecutive results having same field has been removed. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12495367",
            "date": "2007-05-13T14:45:27+0000",
            "content": "Thanks for looking into this Emmanuel.\nIt appears as if this only collapses adjacent documents, correct?\n\nWe should really try to get everyone on the same page... hash out the exact semantics of \"collapsing\", and the most useful interface.  An efficient implementation  can follow.\n\nA good starting point might be here: "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12495368",
            "date": "2007-05-13T14:45:47+0000",
            "content": "A good starting point might be here:\nhttp://www.nabble.com/result-grouping--tf2910425.html#a8131895 "
        },
        {
            "author": "Emmanuel Keller",
            "id": "comment-12495376",
            "date": "2007-05-13T15:51:42+0000",
            "content": "Yonik,\n\nYou are right, only adjacent documents are collapsed. \nI work on a large index ( 2.000.000 documents) growing every day.  The first goal was to group results, preserving score ranking and achieving good performances.  This \"light\" implementation meets our needs.\nI am currently working on a second implementation taking care of the semantics.\n\nP.S.: Congratulations for this great application. "
        },
        {
            "author": "Emmanuel Keller",
            "id": "comment-12495417",
            "date": "2007-05-13T21:09:45+0000",
            "content": "This release is more conform with the semantics of \"field collapsing\".\n\nParameters are:\n\ncollapse=true                   // enable collapsing\ncollapse.field=[field]       // indexed field used for collapsing\ncollapse.max=[integer]  // Start collapsing after n document\ncollapse.type=[normal|adjacent] // Default value is \"normal\"\n\n\n\t\"adjacent\" collapse only consecutive documents.\n\t\"normal\" collapse all documents having equal collapsing field.\n\n "
        },
        {
            "author": "Emmanuel Keller",
            "id": "comment-12495525",
            "date": "2007-05-14T09:27:31+0000",
            "content": "Corrects a bug on the previous version when using a value greater than 1 as collapse.max parameter. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12496617",
            "date": "2007-05-17T16:40:56+0000",
            "content": "Question:\nDo you need collapse=true when you can detect whether collapse.field has been specified or not? "
        },
        {
            "author": "Emmanuel Keller",
            "id": "comment-12496805",
            "date": "2007-05-18T07:38:24+0000",
            "content": "You're right. As collapse.field is a required field, we don't need more information.  My first idea was to copy the behavior of facet. "
        },
        {
            "author": "Emmanuel Keller",
            "id": "comment-12497153",
            "date": "2007-05-19T14:19:44+0000",
            "content": "The last version of the patch.\n\n\n\tResults are now cached using \"CollapseCache\" (a new instance of SolrCache added on solrconfig.xml)\n\tThe parameter \"collapse\" has been removed.\n\n\n\nThis version has been fully tested.\n\nFeedbacks are welcome. "
        },
        {
            "author": "Emmanuel Keller",
            "id": "comment-12497154",
            "date": "2007-05-19T14:24:29+0000",
            "content": "I still maintain a version for the release 1.1.0 (The version we used on our production environment). "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12501083",
            "date": "2007-06-04T02:47:27+0000",
            "content": "I updated the patch so that is applies cleanly with trunk, while I was at it, I:\n\n\tfixed a few spelling errors\n\tmade the \"collapse.type\" parameter parsing to throw an error if the passed field is unknown (rather then quietly using 'normal')\n\tchanged the patch name to include the number. \u2013 as we update the patch, use this same name again so it is easy to tell what is the most current.\n\n\n\nI also made a wiki page so there are direct links to interesting queries:\nhttp://wiki.apache.org/solr/FieldCollapsing\n\n\n\t- - - - - -\n\n\n\nAgain, I will leave any discussion about the lucene implementation to other more qualified and will just focus on the response interface.\n\nCurrently if you send the query:\nhttp://localhost:8983/solr/select/?q=*:*&collapse.field=cat&collapse.max=1&collapse.type=normal\n\nyou get a response that looks like:\n<lst name=\"collapse_counts\">\n <int name=\"hard\">1</int>\n <int name=\"electronics\">2</int>\n <int name=\"memory\">2</int>\n <int name=\"monitor\">1</int>\n <int name=\"software\">1</int>\n</lst>\n\nIt looks like that says: for the field 'cat', there is one more result with cat=hard, 2 more results with cat=electronics, ...\n\nHow is a client supposed to know how to deal with that?  \"hard\" is tokenized version of \"hard drive\" \u2013 unless it were a 'string' field, the client would need to know how to do that \u2013 or the response needs to change.\n\nFrom a client, it would be more useful to have output that looked something like:\n<lst name=\"collapse_counts\">\n <str name=\"field\">cat</str>\n <lst name=\"doc\">\n  <int name=\"SP2514N\">1</int>\n  <int name=\"6H500F0\">1</int>\n  <int name=\"VS1GB400C3\">2</int>\n  <int name=\"VS1GB400C3\">1</int>\n </lst>\n <lst name=\"count\">\n  <int name=\"hard\">1</int>\n  <int name=\"electronics\">1</int>\n  <int name=\"memory\">2</int>\n  <int name=\"monitor\">1</int>\n </lst>\n</lst>\n\n\"field\" says what field was collapsed on,\n\"doc\" is a map of doc id -> how many more collapsed on that field\n\"count\" is a map of 'token'-> how many more collapsed on that field\n\nThis way, the client would know what collapse counts apply to which documents without knowing about the schema.\n\nthoughts?\n\n\n\n "
        },
        {
            "author": "Emmanuel Keller",
            "id": "comment-12501393",
            "date": "2007-06-04T22:09:03+0000",
            "content": "Right, It's more useful.\n\nThis new version includes the result as you expect it.\n\nYou should add the following constraint on the wiki: The collapsing field must be un-tokenized. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12501405",
            "date": "2007-06-04T23:20:15+0000",
            "content": "I just took a look at this using the example data:\nhttp://localhost:8983/solr/select/?q=*:*&collapse.field=cat&collapse.max=1&collapse.type=normal&rows=10\n\n<lst name=\"collapse_counts\">\n <str name=\"field\">cat</str>\n <lst name=\"doc\">\n  <int>1</int>\n  <int name=\"1\">2</int>\n  <int name=\"2\">2</int>\n  <int name=\"4\">1</int>\n  <int name=\"7\">1</int>\n </lst>\n <lst name=\"count\">\n  <int>1</int>\n  <int name=\"card\">2</int>\n  <int name=\"drive\">2</int>\n  <int name=\"hard\">1</int>\n  <int name=\"music\">1</int>\n </lst>\n</lst>\n\n\n\t- -\n\n\n\nwhat is the \"<int>1</int>\" at the front of each response?\n\nPerhaps the 'doc' results should be renamed 'offset' or 'index', and then have another one named 'doc' that uses the uniqueKey as the index...  this would be useful to build a Map.\n\n\n\t- -\n\n\n\nAlso, check:\nhttp://localhost:8983/solr/select/?q=*:*&collapse.field=cat&collapse.max=1&collapse.type=adjacent&rows=50\n\n ArrayIndexOutOfBoundsException:\n\n\n\t- -\n\n\n\n> You should add the following constraint on the wiki: The collapsing field must be un-tokenized.\n\nAnyone can edit the wiki (you just have to make an account) \u2013 it would be great if you could help keep the page accurate / useful.  JIRA discussion comment trails don't work so well at that...\n\nRe: tokenized...  what about it does not work?  Are the limitations an different if it is mult-valued?  Is it just that if any token matches within the field it will collapse and that may or may not be what you expect?\n\n\n\t- -\n\n\n\nDid you get a chance to look at the questions from the previous discussion?  I just noticed Yonik posted something new there:\nhttp://www.nabble.com/result-grouping--tf2910425.html#a10959848 "
        },
        {
            "author": "Emmanuel Keller",
            "id": "comment-12501517",
            "date": "2007-06-05T10:33:06+0000",
            "content": "Sorry, my last post was buggy. Here is the correct one. There is no more exception now.\nAbout tokens, if any token matches within the field it will collapse.\nWhen I start implementing collapsing, my need was to to group documents having exact identical field.\n\nI believe that faceting has identical behavior. Lookt at \"Graphic card\" as example:\nhttp://localhost:8983/solr/select/?q=cat:graphic%20card&version=2.2&start=0&rows=10&indent=on&facet=true&facet.field=cat\n\nI will try to maintain the wiki page. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12501550",
            "date": "2007-06-05T12:59:53+0000",
            "content": "I guess adjacent collapsing can make sense when one is sorting by the field that is being collapsed.\n\nFor the normal collapsing though, this patch appears to implement it by changing the sort order to the collapsing field (normally not desired).  For example, if sorting by relevance and collapsing on a field, one would normally want the groups sorted by relevance (with the group relevance defined as the max score of it's members).\n\nAs far as how to do paging, it makes sense to rigidly define it in terms of number of documents, regardless of how many documents are in each group.  Going back to google, it always displays the first 10 documents, but a variable number of groups.   That does mean that a group could be split across pages.  It would actually be much simpler (IMO) to always return a fixed number of groups rather than a fixed number of documents, but I don't think this would be less useful to people.  Thoughts? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12501557",
            "date": "2007-06-05T13:42:50+0000",
            "content": "Will Johnson brings up other use-cases:\n[...] \n> it's also heavily used in\n> ecommerce settings.  Check out BestBuy.com/circuitcity/etc and do a\n> search for some really generic word like 'cable' and notice all the\n> groups of items; BB shows 3 per group, CC shows 1 per group.  In each\n> case it's not clear that the number of docs is really limited at all, ie\n> it's more important to get back all the categories with n docs per\n> category and the counts per category than it is to get back a fixed\n> number of results or even categories for that matter.  Also notice that\n> neither of these sites allow you to page through the categorized\n> results.\n\nSome of this seems very closely related to faceted search, and much of it could be implemented that way now on the client side, but it would take multiple queries to do so.\n\nOne could also think about supporting multi-valued fields in the same manner that faceting does. "
        },
        {
            "author": "Emmanuel Keller",
            "id": "comment-12501578",
            "date": "2007-06-05T14:41:56+0000",
            "content": "Adjacent collapsing is useful because it preserves the pertinence of the sort.\nThe sorting is not modified. I copy the current sort to do a new search.\n\nI am currently working on taking care of type field (int). "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12501579",
            "date": "2007-06-05T14:47:39+0000",
            "content": "> The sorting is not modified. I copy the current sort to do a new search. \n\nPerhaps if you outlined the algorithm you use, it would clear up some things.\n\nIt looks like you make a copy of the Sort and insert a primary sort on the field to be collapsed, and then process the same way as you would for the \"ADJACENT\" option.  If the original sort was by relevance, this doesn't give you the groups sorted by relevance, right? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12501582",
            "date": "2007-06-05T15:00:01+0000",
            "content": "Oh I see... the modified sort is just to build the filter.\n\nThe building-the-filter part is a problem though... asking for all matching docs in sorted order isn't that scalable.\nIf we get the interface right though, more efficient implementations can follow.\nFor that reason, it might be good for implementatin details like \"collapseCache\" to be private. "
        },
        {
            "author": "Emmanuel Keller",
            "id": "comment-12501583",
            "date": "2007-06-05T15:02:33+0000",
            "content": "Correct, except that collapse result is only used as filter to the final result to hide collapsed documents.\n\nP.S.: Sorry, if my answers are a little short, I am not perfectly fluent in english. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12503125",
            "date": "2007-06-09T22:57:59+0000",
            "content": "Any thoughts on what the faceting semantics for field collapsing should be?\n\nThat is, should faceting apply to the collapsed results or the pre-collapsed results?  \n\nI think the pre-collapsed results. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12503131",
            "date": "2007-06-10T00:09:09+0000",
            "content": "Yes, it seems like faceting should be for pre-collapsed. "
        },
        {
            "author": "Emmanuel Keller",
            "id": "comment-12503162",
            "date": "2007-06-10T11:31:54+0000",
            "content": "Do we have to make a choice ? Both behaviors are interesting. \nWhat about a new parameter like collapse.facet=[pre|post] ?\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12503185",
            "date": "2007-06-10T16:57:23+0000",
            "content": "We facet on the complete set of documents matching a query, even when the user only requests the top 10 matches.  It seems we should do the same here.  The set of documents is the same, the only difference is what \"top\" documents are returned. "
        },
        {
            "author": "Emmanuel Keller",
            "id": "comment-12503327",
            "date": "2007-06-11T08:39:02+0000",
            "content": "New release:\n\n\tFieldcollapsing added on DisMaxRequestHandler\n\tTypes are correctly handled on collapsed field\n\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12505338",
            "date": "2007-06-15T18:31:17+0000",
            "content": "No real changes.  Updated to apply with trunk.\nMoved the valid values for CollapseType to a 'common' package\n\n\n\t- - -\n\n\n\nas a side note, when you make a patch, its easiest to deal with if the path is relative to the solr root directory.\n\nsrc/java/org/apache/solr/search/SolrIndexSearcher.java\n is better then:\n/Users/ekeller/Documents/workspace/solr/src/java/org/apache/solr/search/SolrIndexSearcher.java "
        },
        {
            "author": "Emmanuel Keller",
            "id": "comment-12508577",
            "date": "2007-06-27T15:40:02+0000",
            "content": "This new patch resolves a performance issues.\nI have added time informations for monitoring performances:\n\n<str name=\"time\">57/5</str>\n\nThe first value is the elapsed time (in milliseconds) needed to compute collapsed informations (CollapseFilter.ajacentCollapse method).\nThe second value is the elapsed time needed to compute results informations (CollapseFilter.getMoreResults method).\n\nWe are using Solr (with collapsing patch) on a large index in production environnment (120GB with more than 3 000 000 documents).\n\nP.S.: This time, the patch is relative to the solr root directory. "
        },
        {
            "author": "nunol",
            "id": "comment-12516119",
            "date": "2007-07-28T00:57:50+0000",
            "content": "It would be nice for this patch to also report on what documents were actually collapsed - for example, if the result list contained:\n\ndoc1\ndoc2\ndoc3\n\nand doc2 and doc3 were collapsed, this would be reflected in the XML result as, so that one could determine that (forgive my crap visual representation):\n\ndoc1\n -> doc2\n -> doc3\n\nRegards. "
        },
        {
            "author": "Brian Mertens",
            "id": "comment-12525761",
            "date": "2007-09-07T16:03:33+0000",
            "content": "Imagine a case where a Solr database contains news stories from many newspapers and some wire services.\n\nA single wire story will typically be picked up and reprinted in many different papers, ranging from national papers like the NYTimes, to small town papers. My database will have all of them, and possibly also the original from the wire service. Each paper will choose their own headline, and will edit the story differently for length to fill a hole on the printed page, so they cannot be trivially detected as duplicates, but to my users, they basically are.\n\nI need to detect and group together these \"duplicates\" when displaying search results.\n\nSo let's say every story has had an integer hash value calculated of the first X words of the lead paragraph, and that value is indexed and stored (e.g. \"similarity_hash\"), as a way to detect duplicate stories.\n\nI would want to Field Collapse my results on that hash value, so that all occurrences of the same story are lumped together.\n\nAlso, my users would much prefer the most \"authoritative\" version of the story to be displayed as the primary result, with a count and link to the collapsed results. Authoritativeness could be coded as simple as 1) Wire Service, 2) National Paper, 3) Regional Paper, 4) Small Town Paper, which could be index and stored as an integer \"authority\". (For finer-grained authority we could store the newspapers circulation numbers.)\n\nThen I could display to users:\n\"Dog Bites Man\" \nNew York Times, link to see 77 other duplicates\n\nSo, finally getting to the point, would it be possible to make this feature work such that it field collapses results on one field (\"similarity_hash\"), selects the one to return based on another field (\"authority\" or \"circulation')? (While allowing the results to be sorted by a third field, e.g. date or relevance.)\n\nPerhaps by a new parameter?\n collapse.authority=[field] // indexed field used for selecting which result from collapsed group to return, default being... ?\n\nIf this sounds familiar, it is somewhat similar to what Google News is doing:\n  http://www.pcworld.com/article/id,136680/article.html\n\nFinal question: Do you think Field Collapse could work nicely with SOLR-303 Federated Search, or is that a bridge too far? "
        },
        {
            "author": "Dima Brodsky",
            "id": "comment-12535999",
            "date": "2007-10-18T18:16:27+0000",
            "content": "Hi,\n\nI am new to the list and to Solr, so I appologize in advance if I say something silly.\n\nI have been playing with the field collapse patch and I have a couple of questions and I have noticed a couple of issues.  What is the intended use / audience for the field collapsing patch.  One of the issues I see is that the sort order is changed during normal field collapsing and this causes problems if I want the results ordered based on relevancy.  Another issue, is that the backfilling of the results, if there is not enough, is done from the deduped results rather than getting more results from the index.  Is this by design?\n\nThanks!!\nttyl\nDima "
        },
        {
            "author": "Tracy Flynn",
            "id": "comment-12538302",
            "date": "2007-10-28T10:29:12+0000",
            "content": "Hi,\n\nI am new to Solr, and this thread in particular, so please excuse any questions that seem obvious.\n\nI am investigating converting an existing FAST installation to Solr. I've been able to see how to convert all my queries to Solr/Lucene with little or no trouble, with the exception of field collapsing. I've actually implemented a demo of our main search with a Ruby/Rails front end in a few hours. Nice work everyone!\n\nI have found this thread, looked at the patch for field collapsing and have a couple of questions.\n\nI've looked at the Subversion tree and\n\n\n\tDon't find a 1.3 branch\n\tDon't find the patch code in the trunk\n\n\n\nIs there a 'private' sandbox Solr developers work in that's not visible to the pubic (i.e. me)?\n\nIf not, what revision of the trunk does the patch apply to?\n\nAny help would be appreciated. If I can get a demo that includes field collapsing, my management may be persuaded to let me move our main search to Solr.\n\nRegards,\n\nTracy "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12538327",
            "date": "2007-10-28T17:55:00+0000",
            "content": "Hi Tracy-\n\nThere has not been much movement on this while we get SOLR-281 sorted (I hope this happens soon) \u2013 once that is in, there will hopefully be an updated patch on the 1.3 branch that will be posted here.\n\n\"1.3\" is not a branch yet \u2013 it is the trunk revision that most patches work with.  Only when it becomes an official release, will it actually get called 1.3 in the repository.\n\nIf you need to show field collapsing soon, I think your best bet (i have not tried it) is to apply the ' field_collapsing_1.1.0.patch' to the 1.1.0 branch ( http://svn.apache.org/repos/asf/lucene/solr/tags/release-1.1.0/ )  But if you can wait a few weeks, it will hopefully be available in trunk (or easily patchable from trunk)\n\nryan\n "
        },
        {
            "author": "Tracy Flynn",
            "id": "comment-12538332",
            "date": "2007-10-28T19:17:52+0000",
            "content": "Ryan,\n\nThanks for the quick reply and clarification. I'll follow your suggestion as to where to apply and try the patch.\n\nI'll be eagerly waiting for the updated trunk.\n\nRegards,\n\nTracy\n "
        },
        {
            "author": "Emmanuel Keller",
            "id": "comment-12538339",
            "date": "2007-10-28T20:54:25+0000",
            "content": "Here is the patch for solr 1.3 rev 589395.\n\nI made some performance improvement. No more cache. I use bitdocset or hashdocset depending on solrconfig.hashdocsetmaxsize variable.\n\nRegards,\nEmmanuel Keller. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12538341",
            "date": "2007-10-28T21:11:20+0000",
            "content": "It looks like the latest patch only includes changed files and not new ones (like CollapseFilter?) "
        },
        {
            "author": "Emmanuel Keller",
            "id": "comment-12538343",
            "date": "2007-10-28T21:20:21+0000",
            "content": "Thank you Yonik ! \nHere is the complete version.\n\nP.S.: It's time to go to bed in Europe ...\n\nEmmanuel. "
        },
        {
            "author": "Karsten Sperling",
            "id": "comment-12539507",
            "date": "2007-11-02T02:12:22+0000",
            "content": "I've just looked at the implementation of this patch again \u2013 it ends up calling SolrIndexSearcher.getDocListC() with a DocSet derived from the CollapseFilter as the 'filter' parameter. The comment on that method says that only filter or filterList should be provided, but not both. However with the field collapsing patch both WILL be provided if filter queries are passed to the dismax request handler by the client. Can anybody shed any light on what the implications of this are? "
        },
        {
            "author": "Karsten Sperling",
            "id": "comment-12540597",
            "date": "2007-11-06T22:06:00+0000",
            "content": "I've done some work on the field collapsing patch and made some additions and changes and posting this patch (against revision 592129) here for discussion.\n\n\n\tAdded a collapse.facet = before|after parameter to control if faceting happens before or after collapsing.\n\tChanged collapse.max to collapse.threshold \u2013 this value controls after which number of collapsible hits collapsing actually kicks in (collapse.max is still supported as an alias).\n\tAdded a collapse.maxdocs parameter that limits the number of documents that CollapseFilter will process to create the filter DocSet. The intention of this is to be able to limit the time collapsing will take for very large result sets (obviously at the expense of accurate collapsing in those cases).\n\tInverted the logic of the filter DocSet created by CollapseFilter to contain the documents that are to be collapsed instead of the ones that are to be kept. Without this collapse.maxdocs doesn't work.\n\tAdded collapse.info.doc and collapse.info.count parameters to provide more control over what gets returned in the collapse_counts extra results.\n\tMade a minimal change to SolrIndexSearcher.getDocListC() to support passing both the filter and filterList parameters. In most cases this was already handled anyway.\n\tDid some general refactoring and added comments and a test case.\n\n\n\nIf somebody with deeper Solr/Lucene knowledge could review these changes it would be much appreciated.\n\nKarsten "
        },
        {
            "author": "Doug Steigerwald",
            "id": "comment-12556032",
            "date": "2008-01-04T19:40:25+0000",
            "content": "I've created a CollapseComponent for field collapsing.  Everything seems to work fine with it.  Only issue I'm having is I cannot use the query component because when it isn't commented out, the non-field collapsed results are displayed and I can't figure out how to remove them.  Someone might be able to figure that part out.\n\n[http://localhost:8983/solr/search?q=id:[0%20TO%20*]&collapse=true&collapse.field=inStock&collapse.type=normal&collapse.threshold=0]\n\nHere's the config I'm using:\n\n    <searchComponent name=\"collapse\"     class=\"org.apache.solr.handler.component.CollapseComponent\" /> \n    <requestHandler name=\"/search\" class=\"solr.SearchHandler\">\n        <lst name=\"defaults\">\n            <str name=\"echoParams\">explicit</str>\n        </lst>\n        <arr name=\"components\">\n            <!--       <str>query</str> -->\n            <str>facet</str>\n            <!--       <str>mlt</str> -->\n            <!--       <str>highlight</str> -->\n            <!--       <str>debug</str> -->\n            <str>collapse</str>\n        </arr>\n  </requestHandler> "
        },
        {
            "author": "Charles Hornberger",
            "id": "comment-12556652",
            "date": "2008-01-07T19:02:18+0000",
            "content": "UPDATE: Doug Steigerwald's patch (field_collapsing_dsteigerwald.diff) applies cleanly to trunk\n\nI'm having trouble applying field_collapsing_1.3.patch to the head of trunk.\n\n\ncharlie@macbuntu:~/solr/src/java$ patch -p0 < /home/charlie/downloads/field_collapsing_1.3.patch \npatching file org/apache/solr/search/CollapseFilter.java\npatching file org/apache/solr/search/SolrIndexSearcher.java\nHunk #1 succeeded at 694 (offset -8 lines).\nHunk #2 succeeded at 1252 (offset -1 lines).\npatching file org/apache/solr/common/params/CollapseParams.java\npatching file org/apache/solr/handler/StandardRequestHandler.java\nHunk #1 FAILED at 33.\nHunk #2 FAILED at 90.\nHunk #3 FAILED at 117.\n3 out of 3 hunks FAILED -- saving rejects to file org/apache/solr/handler/StandardRequestHandler.java.rej\npatching file org/apache/solr/handler/DisMaxRequestHandler.java\nHunk #1 FAILED at 31.\nHunk #2 FAILED at 40.\nHunk #3 FAILED at 311.\nHunk #4 FAILED at 339.\n4 out of 4 hunks FAILED -- saving rejects to file org/apache/solr/handler/DisMaxRequestHandler.java.rej\n\n\n\nI'm guessing that maybe the field collapsing patch needs to be updated for the SearchHandler refactoring that was does as part of SOLR-281? If so, I'll take a whack at migrating the changes to the SearchHandler.java, and see if I can produce a better patch. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12556732",
            "date": "2008-01-07T22:07:26+0000",
            "content": "Charles - try applying Doug Steigerwald's latest patch:   field_collapsing_dsteigerwald.diff \n\nI have not tested it, but it does apply without errors "
        },
        {
            "author": "Charles Hornberger",
            "id": "comment-12557505",
            "date": "2008-01-10T01:11:27+0000",
            "content": "Doug \u2013 I just started looking into field collapsing the other day, but from glancing at the code in QueryComponent.java and CollapseComponent.java, it seems like perhaps you're not supposed to be using both components \u2013 after all, their prepare() methods are identical, and their process() methods both execute the user's search and shove the resulting DocList into the \"response\" entry of the response object's internal storage Map. (The QueryComponent additionally stores the DocListAndSet in the ResponseBuilder object via builder.setResults() \u2013 I'm not sure why this is \u2013 and prefetches documents if the result set is small enough.) My guess is that if you want to enable collapsing, you should use the CollapseComponent; if you want to disable it, use the QueryComponent. Maybe someone who understand the design of the search handling components better than me can confirm this or correct my misunderstanding(s) ... "
        },
        {
            "author": "Charles Hornberger",
            "id": "comment-12557510",
            "date": "2008-01-10T01:17:12+0000",
            "content": "Attaching a new copy of Doug Steigerwald's patch that omits the System.out.println() call in CollapseComponent.java. "
        },
        {
            "author": "Doug Steigerwald",
            "id": "comment-12557639",
            "date": "2008-01-10T13:24:17+0000",
            "content": "I copied what was in  QueryComponent.prepare() method because I was having to disable the query component because of the extra results I was getting.  Initially I had CollapseComponent.prepare() empty, but I had results from the query component and then adding the collapse component results being returned (2 'response' in the results.\n\nEasy solution for me was to copy the prepare from QueryComponent and disable the query component in the request handler.  There may be another way, but I was unable to figure it out. "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12563779",
            "date": "2008-01-30T01:16:32+0000",
            "content": "Hello, I am new to Solr, so forgive me if what I say doesn't make sense... None of the patches for 1.3 work any more, since the file org.apache.solr.handler.SearchHandler has been removed from the nightly builds. Will someone write a new patch that works with teh current nightly builds? If not, could we get a copy of an old nightly build somewhere? Thanks a lot. "
        },
        {
            "author": "Charles Hornberger",
            "id": "comment-12563791",
            "date": "2008-01-30T01:33:20+0000",
            "content": "It seems like SearchHandler was simply moved down into the org.apache.solr.handler.components package as part of r610426 - http://svn.apache.org/viewvc?view=rev&revision=610426\n\nYou should be able to modify the import statements field_collapsing_dsteigerwald.diff to make it work, no? "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12563806",
            "date": "2008-01-30T02:28:26+0000",
            "content": "Oh, I didn''t notice. I will give a try tomorrow morning. Thank you. "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12564204",
            "date": "2008-01-30T22:47:45+0000",
            "content": "That works, thanks  "
        },
        {
            "author": "Charles Hornberger",
            "id": "comment-12564952",
            "date": "2008-02-01T21:59:21+0000",
            "content": "NegatedDocSet is throwing \"Unsupported Operation\" exceptions:\n\norg.apache.solr.common.SolrException:Unsupported Operation \n at org.apache.solr.search.NegatedDocSet.iterator(NegatedDocSet.java:77)\n at org.apache.solr.search.DocSetBase.getBits(DocSet.java:183)  \n at org.apache.solr.search.NegatedDocSet.getBits(NegatedDocSet.java:27) \n at org.apache.solr.search.DocSetBase.intersection(DocSet.java:199)      \n at org.apache.solr.search.BitDocSet.intersection(BitDocSet.java:30)     \n at org.apache.solr.search.SolrIndexSearcher.getDocListAndSetNC(SolrIndexSearcher.java:1109)\n at org.apache.solr.search.SolrIndexSearcher.getDocListC(SolrIndexSearcher.java:811)\n at org.apache.solr.search.SolrIndexSearcher.getDocListAndSet(SolrIndexSearcher.java:1258)\n at org.apache.solr.handler.component.CollapseComponent.process(CollapseComponent.java:103)\n at org.apache.solr.handler.SearchHandler.handleRequestBody(SearchHandler.java:155)\n at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:117)\n at org.apache.solr.core.SolrCore.execute(SolrCore.java:902)     \n at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:275)\n at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:232)\n at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)\n at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)\n at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:213)\n at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:174)\n at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)\n at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:117)\n at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:108)\n at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:151)\n at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:874)\n at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection(Http11BaseProtocol.java:665)\n at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:528)\n at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:81)\n at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:689)\n at java.lang.Thread.run(Thread.java:595)\n\nNot quite sure what search is triggering this path thru the code, but it is not happening on every request; just some ... am firing up the debugger now to see what I can learn, but thought I'd post this anyway to see if anyone has any tips. "
        },
        {
            "author": "Charles Hornberger",
            "id": "comment-12564966",
            "date": "2008-02-01T22:55:50+0000",
            "content": "Ah ... got the beginnings of a diagnosis. The problem appears when the DocSet qDocSet returned by DocSetHitCollector.getDocSet() \u2013 called at org.apache.solr.search.SolrIndexSearcher:1101 in trunk, or 1108 with the field_collapsing patch applied, inside getDocListAndSetNC()) \u2013 is a BitDocSet, and not when it's a HashDocSet. As the stack trace above shows, calling intersection() on a BitDocSet object invokes the superclass' DocSetBase.intersection() method, which invokes a call chain that blows up when it hits the iterator() method of the NegatedDocSet passed in as the filter parameter to getDocListAndSetNC(); NegatedDocSet.iterator() blows up by design:\n\n\npublic DocIterator iterator() {\n    throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, \"Unsupported Operation\");\n}\n\n\n\nI see that DocSetBase.intersection(DocSet other) has special-casing logic for dealing with other parameters that are instances of HashDocSet; does it also need special casing logic for dealing with other parameters that are NegatedDocSets? Or should NegatedDocSet really implement iterator()? Or something else entirely? "
        },
        {
            "author": "Charles Hornberger",
            "id": "comment-12565012",
            "date": "2008-02-02T02:12:25+0000",
            "content": "Here's the simplest change I could think of to make DocSetBase subclasses that don't override intersection() (which just means BitDocSet at the moment) stop choking when their  intersection() gets called with a NegatedDocSet as the other parameter; it's probably horribly stupid. Also, there should be a test.\n\n\nIndex: src/java/org/apache/solr/search/DocSet.java\n===================================================================\n--- src/java/org/apache/solr/search/DocSet.java (revision 617738)\n+++ src/java/org/apache/solr/search/DocSet.java (working copy)\n@@ -193,7 +193,18 @@\n     if (other instanceof HashDocSet) {\n       return other.intersection(this);\n     }\n-\n+    // you can't call getBits() on a NegatedDocSet, because\n+    // getBits() // calls iterator(), and iterator() isn't \n+    // supported by NegatedDocSet\n+    if (other instanceof NegatedDocSet) {\n+        BitDocSet newdocs = new BitDocSet();\n+        for (DocIterator iter = iterator(); iter.hasNext();) {\n+          int next = iter.nextDoc();\n+          if (other.exists(next))\n+           newdocs.add(next);\n+        }\n+        return newdocs;\n+    }\n     // Default... handle with bitsets.\n     OpenBitSet newbits = (OpenBitSet)(this.getBits().clone());\n     newbits.and(other.getBits());\n\n\n\nComments? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12565019",
            "date": "2008-02-02T02:31:22+0000",
            "content": "I haven't been following this, so I don't know why there is a need for a NegatedDocSet (or if introducing it is the best solution), but it looks like you have two cases to handle: one negative set or two negative sets.\nIf you have a and -b, then return a.andNot(b)\nif both a and b are negative (-a.intersection(-b))  then return NegatedDocSet(a.union(b))  // per De Morgan, -a&-b == -(a|b)\n\nThat's only for intersection() of course. "
        },
        {
            "author": "Karsten Sperling",
            "id": "comment-12565158",
            "date": "2008-02-03T09:31:08+0000",
            "content": "NegatedDocSet got introduced because the filter logic expects to use the intersection operation to apply a number of filters to a result. Introducing a negated docset was much easier than supporting both intersection as well as and-not type filters.\n\nNegatedDocSet does not support iteration because the negation of a finite set is (at least theoretically) infinite. Even though it would in practice be possible to limit the negated set via the known maximum document id, this would probably not be very efficient. However, it is simply not necessary to ever iterate over the elements of a NegatedDocSet, because we know that the end-result of all DocSet operations is going to be a finite set of results, not an infinite one. A NegatedDocSet will only ever be used to \"subtract\" from a finite DocSet. As Yonik has pointed out, operations on a NegatedDocSet can be rewritten as (different) operations on the set being negated. The operation methods inside NegatedDocSet do this.\n\nThe reason the bug occurs is because of the naive way the binary set operation calls are dispatched: DocSet clients simply call e.g. set1.intersection(set2), arbitrarily leaving the choice of implementation to the logic defined by the class of set1. Currently, BitDocSet does not know about NegatedDocSet, and hence doesn't perform the necessary rewriting or delegation to NegatedDocSet.\n\nHowever, instead of requiring each and every DocSet subclass to know about all other ones (and in the absence of language support for multiple dispatch), I think it would be better to centralize this knowledge in a single class DocSetOp with static methods that selects the appropriate implementation for an operation based on the type of both parameters. Either the client code could be changed to call DocSetOp.intersection(a, b) instead of a.intersection(b), but this would involve changing the DocSet interface. A backwards compatible solution would be to simply have final DocSetBase.intersection() delegating to DocSetOp.intersection. "
        },
        {
            "author": "Charles Hornberger",
            "id": "comment-12565194",
            "date": "2008-02-03T17:39:13+0000",
            "content": "As Yonik has pointed out, operations on a NegatedDocSet can be rewritten as (different) operations on the set being negated. The operation methods inside NegatedDocSet do this.\n\nRight. I realized, sheepishly, after I posted the first suggested patch that it'd be much simpler to just mimic the first if-clause in DocSet.intersection():\n\n\n  if (other instanceof NegatedDocSet) {\n    other.intersection(this);\n  }\n\n "
        },
        {
            "author": "Charles Hornberger",
            "id": "comment-12565468",
            "date": "2008-02-04T18:59:18+0000",
            "content": "However, instead of requiring each and every DocSet subclass to know about all other ones (and in the absence of language support for multiple dispatch), I think it would be better to centralize this knowledge in a single class DocSetOp with static methods that selects the appropriate implementation for an operation based on the type of both parameters.\n\n+1 for this ... whether or not NegatedDocSet is part of the final implementation of this feature. FWIW, I just noticed that there's another bug lurking in BitDocSet.andNot(), which will fail if a NegatedDocSet is passed in. It seems to me that it might be easier \u2013 at least for me \u2013 to read/write/extend a test suite that exercised all the paths thru DocSetOp, than to write a set of tests that exercised all the paths thru DocSetBase and its subclasses.\n\nAlso, I think that maybe there's a clear distinction to be made between intrinsic operations on a set (add(), exists(), et al.) and ones that involve another set (intersection(), union(), andNot()). Not sure it's a useful one, but it make sense to me. I don't know, though, whether it make sense to go further than that and say \u2013 as the current implementation of NegatedDocSet implies \u2013 that there are some set operations (iterator() and size()) that are in fact optional.\n\nOff the top of my head: Would it be simpler to just modify add a filterType flag to the getDocList*() family of methods in SolrSearchInterface to cause it to call a.andNot(b) rather than a.intersection(b) when applying b as a filter? (I'm really completely ignorant \u2013 or nearly completely \u2013 of how the seach code works, so feel free not to dignify this with a response if it's a useless idea ... ) "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12566864",
            "date": "2008-02-08T00:15:00+0000",
            "content": "Hello everyone. I am planning to implement chain collapsing on a high traffic production environment, so I'd like to use a stable version of Solr. It doesn't seem like you have a chain collapse patch for Solr 1.2, so I tried the Solr 1.1 patch. It seems to work fine at collapsing, but how do I get a countt for the documents other then the one being displayed?\n\nAs a result I see:\n\n<lst name=\"collapse_counts\">\n    <int name=\"Restaurant\">2414</int>\n    <int name=\"Bar/Club\">9</int>\n    <int name=\"Directory & Services\">37</int>\n</lst>\n\nDoes that mean that there are 2414 more Restaurants, 9 more Bars and 37 more Directory & Services? If so, then that's great.\n\nHowever when I collapse on some  fields I get an empty collapse_counts list. It could be that those fields have a large number of different values that it collapses on. Is there a limit to the number of values that collaose_counts displays?\n\nThanks in advance for any help you can provide! "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12567184",
            "date": "2008-02-08T20:18:15+0000",
            "content": "Also, is field collapse going to be a part of the upcoming Solr 1.3 release, or will we need to run a patch on it? "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12567224",
            "date": "2008-02-08T22:05:57+0000",
            "content": "OK, I think I have the first issue figured out. If the current resultset (lets say the first 10 rows) doesn't have the field that we are collapsing on, the counts don't show up. Is that correct? "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12569134",
            "date": "2008-02-14T23:38:42+0000",
            "content": "Latest patch file fixes an issue where facet searching would throw a NullPointerException when using the fieldCollapse requestHandler. Also, updated the import path for SearchHandler. Thank you Dave for these tips! "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12569330",
            "date": "2008-02-15T17:25:28+0000",
            "content": "That thanks should be to Charles not Dave  Sorry about that! "
        },
        {
            "author": "Nikolai Kordulla",
            "id": "comment-12572660",
            "date": "2008-02-26T21:17:39+0000",
            "content": "A good thing were to apply this CollapseComponent for the mlt results. "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12574745",
            "date": "2008-03-03T21:31:20+0000",
            "content": "Are there any plans to add collapse controls to SolrJ? "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12587027",
            "date": "2008-04-09T01:29:16+0000",
            "content": "None of the patches work on the current nightly build anymore. Could anyone help? Thanks  "
        },
        {
            "author": "Bojan Smid",
            "id": "comment-12599660",
            "date": "2008-05-25T07:42:51+0000",
            "content": "I will try to bring this patch up to date. Currently I see two main problems:\n\n1) The patch applies to trunk, but it doesn't compile. The problem occurs mainly because of changes in Search Components (for instance, some method signatures which CollapseComponent implements were changed). I have this fixed locally (more or less), but I have to test it before posting new version of patch.\n\n2) It seems that CollapseComponent can't be used in chain with QueryComponent, but instead of it. CollapseComponent basically copies QueryComponent querying logic and adds some of it's own. I guess this isn't the right way to go. CollapseComponent should contain only collapsing logic and should be chainable with other components. Can anyone confirm if I'm right here? Of course, there might be some fundamental reason why CollapseComponent had to be implemented this way.\n\nDoes anyone else see any other issues with this component? "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12599689",
            "date": "2008-05-25T15:21:45+0000",
            "content": "Hey Bojan. I actually hacked collapsecomponent quite a bit, in order to get it to work with Distributed Search, but I am not going to upload it, since its horribly buggy. Do you think that's a feature that can be added? "
        },
        {
            "author": "Bojan Smid",
            "id": "comment-12599714",
            "date": "2008-05-25T20:21:26+0000",
            "content": "Hi Oleg. I'll look into this also. In case you have any working code, you can mail it to me, and I'll see what can be reused. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12600281",
            "date": "2008-05-27T21:10:29+0000",
            "content": "It's amazing this issue/patch has so many votes and watchers, yet it's stuck...\nRyan, Yonik, Emmanuel, Doug, Charles, Karsten\n\nI think Bojan is onto something here.  Isn't the ability to chain QueryComponent (QC) and CollapseComponent (CC) essential?\n\nI'm looking at  field_collapsing_dsteigerwald.diff  and see that the CC.prepare method there is identical to the QC.prepare method, while process methods are different.  Could we solve this particular copy/paste situation by making CC extend QC and simply override the process method?\n\nAs for chaining, could CC take the same approach as the MLT Component, which simply does it's thing to find \"more like this\" docs and stuffs them into the \"moreLikeThis\" element in the response?\n\nI could be misunderstanding something, so please correct me if I'm wrong.  I'd love to get this one in 1.3 \u2013 it's been waiting in JIRA for too long.  "
        },
        {
            "author": "Bojan Smid",
            "id": "comment-12603302",
            "date": "2008-06-07T12:36:31+0000",
            "content": "I updated the patch so that it can be compiled on Solr trunk. Also, since CollapseComponent essentially copied QueryComponent's prepare method (and it seems that it is supposed to be used instead of it), I made it extend QueryComponent (with collapsing-specific process() method, and prepare() method inherited from super class). "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12603616",
            "date": "2008-06-09T16:59:47+0000",
            "content": "I'd like to request some distributed search functionality for this feature as well. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12604033",
            "date": "2008-06-10T20:49:01+0000",
            "content": "There is so little interest in this patch/functionality now, that I doubt it will get distributed search support in time for 1.3  I would like to commit Bojan's patch for 1.3, though. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12605612",
            "date": "2008-06-17T14:23:05+0000",
            "content": "Since this is adding new interface/API, it would be very nice if one could easily review it.  It's very important that the interface and the exact semantics are nailed down IMO (there seem to be a lot of options).\nIs http://wiki.apache.org/solr/FieldCollapsing up-to-date?\n\nThere don't seem to be any tests either. "
        },
        {
            "author": "JList",
            "id": "comment-12606953",
            "date": "2008-06-21T01:16:23+0000",
            "content": "Although field collpasing worked fine in my brief testing, when I put it to work with more documents, I got exceptions. It seems to have something to do with the queries (or documents, since different queries return different documents). With some queries, this exception does not happen.\n\nIf I remove the collapse.* parameters, the error does not happen. Any idea why this is happening? Thanks.\n\n\nHTTP ERROR: 500\nUnsupported Operation\n\norg.apache.solr.common.SolrException: Unsupported Operation\n        at org.apache.solr.search.NegatedDocSet.iterator(NegatedDocSet.java:77)\n        at org.apache.solr.search.DocSetBase.getBits(DocSet.java:183)\n        at org.apache.solr.search.NegatedDocSet.getBits(NegatedDocSet.java:27)\n        at org.apache.solr.search.DocSetBase.intersection(DocSet.java:199)\n        at org.apache.solr.search.BitDocSet.intersection(BitDocSet.java:30)\n        at org.apache.solr.search.SolrIndexSearcher.getDocListAndSetNC(SolrIndexSearcher.java:1109)\n        at org.apache.solr.search.SolrIndexSearcher.getDocListC(SolrIndexSearcher.java:811)\n        at org.apache.solr.search.SolrIndexSearcher.getDocListAndSet(SolrIndexSearcher.java:1282)\n        at org.apache.solr.handler.component.CollapseComponent.process(CollapseComponent.java:57)\n        at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:156)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:125)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:965)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:338)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:272)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1089)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:365)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:712)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:405)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:211)\n        at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:139)\n        at org.mortbay.jetty.Server.handle(Server.java:285)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:502)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:821)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:513)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:208)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:378)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:226)\n        at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:442) "
        },
        {
            "author": "Bojan Smid",
            "id": "comment-12606986",
            "date": "2008-06-21T07:28:14+0000",
            "content": "You can check discussion about this same problem in the posts above (starting with 1st Feb 2008). It seems like a rather complex issue which could require some serious refactoring of collapsing code. "
        },
        {
            "author": "JList",
            "id": "comment-12607010",
            "date": "2008-06-21T16:35:57+0000",
            "content": "Sorry about the dup. I obviously didn't check the comments before I posted the bug. Anyway, it's still there, it's still happening  "
        },
        {
            "author": "JList",
            "id": "comment-12607039",
            "date": "2008-06-21T21:08:17+0000",
            "content": "Not sure if it's related to the query string or the documents that the query hits. If the latter, it would be trickier to reproduce.\nAnyway I tried a few English words and the error didn't happen. So far I was only able to reproduce it with CJK (Simplified Chinese to be exact) queries.\n\nThis is an example query that triggers this problem (in UTF-8):\n'\\xe5\\x9c\\xb0\\xe9\\x9c\\x87'\n\nThe query string:\nhttp://localhost:8983/solr/select/?q=%E5%9C%B0%E9%9C%87&version=2.2&start=0&rows=10&indent=on&collapse.field=domain "
        },
        {
            "author": "Matthias Epheser",
            "id": "comment-12621470",
            "date": "2008-08-11T14:59:30+0000",
            "content": "I just tried to apply the last patch and ran into 2 issues:\n\nFirst: \n\nThe new getDocListAndSet(Query query, List<Query>..) method in SolrIndexSearcher calls the getDocListC(..) method using the old signature. I changed the call to the new signature and it worked very well:\n\nDocListAndSet ret = new DocListAndSet();\nQueryResult queryResult = new QueryResult();\nqueryResult.setDocListAndSet(ret);\nqueryResult.setPartialResults(false);\nQueryCommand queryCommand = new QueryCommand();\nqueryCommand.setQuery(query);\nqueryCommand.setFilterList(filterList);\nqueryCommand.setFilter(docSet);\nqueryCommand.setSort(lsort);\nqueryCommand.setOffset(offset);\nqueryCommand.setLen(len);\nqueryCommand.setFlags(flags |= GET_DOCSET);\ngetDocListC(queryResult, queryCommand);\n\n\nSecond:\n\nAfter adding more docs (~3000), I got an Exception in SolrIndexSearcher at line ~1300:\nqr.setDocSet(filter == null ? qDocSet : qDocSet.intersection(filter));\n\nAs the NegotiatedDocSet doesn't implement the iterator() function, this call lead to an Unsupported Operation exception. I just naively tried to implement this funtion using \"return source.iterator()\". Works fine for me.\n\n\nAs the first issue is very clear, I wanted to check my approach for the second one before I post a patch. Maybe there are some side effects that I missed.   "
        },
        {
            "author": "Doug Steigerwald",
            "id": "comment-12624418",
            "date": "2008-08-21T16:32:15+0000",
            "content": "I'm in the process of updating our Solr build and I'm running into issues with this patch now.  I added the code in the first issue Matthias mentioned.  Unfortunately whenever I try to do any field collapsing, I get a NPE:\n\njava.lang.NullPointerException\n\tat org.apache.solr.search.CollapseFilter.getCollapseInfo(CollapseFilter.java:263)\n\tat org.apache.solr.handler.component.CollapseComponent.process(CollapseComponent.java:65)\n...\n\nMy request handler for testing is simple.  It only has the collapse component in it.  Posting the example docs and trying to execute the following query gives me the NPE.\n\nhttp://localhost:8983/solr/search?q=*:*&collapse.field=cat&collapse.type=normal\n\nUpdated my trunk this morning (r687489). "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12624421",
            "date": "2008-08-21T16:44:57+0000",
            "content": "I was able to hack the latest patch in, and to get it to work, but it required some pretty heavy naive changes...\n\nIf you are getting an NPE try this: in the SolrIndexSearcher class, in the getDocListC method change out = new DocListAndSet(); to\n\nDocListAndSet out = null;\n    if(qr.getDocListAndSet() == null)\n        out = new DocListAndSet();\n    else\n        out = qr.getDocListAndSet(); "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12636978",
            "date": "2008-10-06T01:25:05+0000",
            "content": "Sorting twice (when not sorting on the collapse field) only makes sense if we are doing external sorts (harddrive), correct ? It seems to me that this should be closer to the facet stuff (in using the field cache) and then use a hash table of accumulators: linear time (is that generally?) right? (edit: looks like thats too memory intensive)\n\nAs Otis mentions above, this issue appears very popular. We should finish it up. "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12638355",
            "date": "2008-10-09T18:36:59+0000",
            "content": "What's a hard drive sort? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12638359",
            "date": "2008-10-09T18:45:31+0000",
            "content": "What's a hard drive sort? \n\nSorry - was not very clear.\n\nJust like sorting, finding dupes can be done in memory or using external storage (harddrive). I am only just looking into this stuff myself, but it seems in the best case you would want to do it in memory with a hash system which can be linear scalability. If you have too many items to look for dupes in, you have to use external storage - one good method is two sorts (we get one from the search), but there are other options too I think. In this case, the sorts are able to be done in memory though, but I think the hashtable method of identifying dupes is much less memory efficient (too many unique terms). "
        },
        {
            "author": "Vaijanath N. Rao",
            "id": "comment-12644200",
            "date": "2008-10-31T03:22:43+0000",
            "content": "Hi All,\n\nI am trying to apply this patch to solr-1.4 code and getting following errors.\nAt line number 58 of the CollapseComponent.java and the error is:\nThe method getDocListAndSet (Query, List<Query>, Sort, int , int , int) in the type SolrIndexSearcher is not applicable for the arguments  (Query, List<Query>, DocSet, Sort, int , int , int)\n\nCan anyone tell me the correction I need to do to get this code working.\n\n--Thanks and Regards\nVaijanath "
        },
        {
            "author": "Vaijanath N. Rao",
            "id": "comment-12644233",
            "date": "2008-10-31T08:01:42+0000",
            "content": "Hi  All,\n\nI got this patch working but for 1.3 code and not 1.4. I will try to get this working and will tell you the results. I pulled in some code from older version namely for \ngetDocListAndSet \ngetDocListNC\ngetDocListC.\n\n\nI also added an constructor DocSetHitCollector (int maxDoc) with following code\n  public DocSetHitCollector(int maxDoc) {\n\t  this(HashDocSet.DEFAULT_INVERSE_LOAD_FACTOR,maxDoc,maxDoc);\n\t// TODO Auto-generated constructor stub\n}\n\nI wanted to know if any of the additions harm any other component of solr.\n\nDo I need to make any changes to solrconfig other than the following \n\nAdding <arr name=\"first-components\">  <str>collapse</str>  </arr> this to standard and dismax query handler\n<searchComponent name=\"collapse\" class=\"org.apache.solr.handler.component.CollapseComponent\" />\n\nI will check this with highlighting and let you all know of any observation that I make.\n\n--Thanks and Regards\nVaijanath "
        },
        {
            "author": "Iv\u00e1n de Prado",
            "id": "comment-12647325",
            "date": "2008-11-13T16:33:32+0000",
            "content": "A patch for field collapsing over Solr 1.3.0. It changes the behavior to be more memory friendly when the parameter collapse.maxdocs is used.  "
        },
        {
            "author": "Iv\u00e1n de Prado",
            "id": "comment-12647335",
            "date": "2008-11-13T16:59:54+0000",
            "content": "I attached a patch named collapsing-patch-to-1.3.0-ivan.patch. The patch applies to Solr 1.3.0.\n\nKarsten commented in the comment \"Karsten Sperling - 06/Nov/07 02:06 PM\":\n\nInverted the logic of the filter DocSet created by CollapseFilter to contain the documents that are to be collapsed instead of the ones that are to be kept. Without this collapse.maxdocs doesn't work.\n\nI found that this way of doing consumes a lot of memory, even if your query is bounded to a few number of documents. And I found that there is not advantage on using collapse.maxdocs if you don't speed up queries and reduces the amount of needed memory. \n\nSo, I decided to revert the Karsten change in order to make field collapsing faster and less resources consuming when querying for smaller datasets.\n\nWARNING: This patch changes the semantic of collapse.maxdocs. Before this patch, the collapse.maxdocs was used just for reduce the number of docs cheked for grouping, but presenting the rest of documents that were not grouped in the result. \n\nWith current patch, only documents that were examinated for grouping can appear in the result. This semantic have two benefits:\n\n\tThe amount of resources can be controled per each query\n\tNot ungrouped content is presented.\n\n "
        },
        {
            "author": "Doug Steigerwald",
            "id": "comment-12654950",
            "date": "2008-12-09T20:30:08+0000",
            "content": "I'm having an issue with Ivan's latest patch.  I'm testing on a data set of 8113 documents.  All the documents have a string field called site.  There are only two sites, Site1 and Site2.\n\nSite1 has 3466 documents.\nSite2 has 4647 documents.\n\nWith the following simple query, I only get 1 result:\nhttp://localhost:8983/solr/core1/search?q=*:*&collapase=true&collapse.field=site\n\n....\n<lst name=\"collapse_counts\">\n <str name=\"field\">site</str>\n <lst name=\"doc\">\n  <int name=\"site2-doc-2981790\">4646</int>\n </lst>\n <lst name=\"count\">\n  <int name=\"Site2\">4646</int>\n </lst>\n <str name=\"debug\">HashDocSet(2) Time(ms): 0/0/0/0</str>\n</lst>\n<result name=\"response\" numFound=\"1\" start=\"0\">\n....\n\nThe only result displayed is for Site2.\n\nI have an older patch working with Solr 1.3.0, but I can't get it to mesh with localsolr properly.  My localsolr gives 1656 results, and collapsed on the site it should give 2 results but gives 8 results, some of which are duplicate documents.  Without localsolr, my field collapsing patch seems to work fine. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12654979",
            "date": "2008-12-09T21:58:25+0000",
            "content": "What is the \"localsolr\" field you are talking about?\n\nIs it the solr stuff from http://sourceforge.net/projects/locallucene ? "
        },
        {
            "author": "Doug Steigerwald",
            "id": "comment-12655039",
            "date": "2008-12-10T00:39:48+0000",
            "content": "Yes, that localsolr.  I've just been trying to get the two components working together but haven't had much luck.\n\nSeparately they work fine, but together not so much.  I can't get the field collapsing to work correctly with an existing reset set from the localsolr component in the response builder. "
        },
        {
            "author": "Iv\u00e1n de Prado",
            "id": "comment-12655269",
            "date": "2008-12-10T16:31:10+0000",
            "content": "I have attached new patch with the problems solved in my first submitted patch. Doug Steigerwald, could you check if this patch works well for you? Thanks.  "
        },
        {
            "author": "Doug Steigerwald",
            "id": "comment-12655285",
            "date": "2008-12-10T17:16:34+0000",
            "content": "Looks fine from my little bit of testing. "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12655750",
            "date": "2008-12-11T19:48:00+0000",
            "content": "I'm using Ivan's patch and running into some trouble with faceting...\n\nBasically, I can tell that faceting is happening after the collapse - because the facet counts are definitely lower than they would be otherwise.  For example, with one search, I'd have 196 results with no collapsing, I get 120 results with collapsing - but the facet count is 119???  In other searches the difference is more drastic - In another search, I get 61 results without collapsing, 61 with collapsing, but the facet count is 39.\n\nLooking at it for a while now, I think I can guess what the problem might be...\n\nThe incorrect counts seem to only happen when the term in question does not occur evenly across all duplicates of a document.  That is, multiple document records may exist for the same image (it's an image search engine), but each document will have different terms in different fields depending on the audience it's targeting.  So, when you collapse, the counts are lower than they should be because when you actually execute a search with that facet's term included in the query, all the documents after collapsing will be ones that have that term.\n\nHere's an illustration:\n\nCollapse field is \"link_id\", facet field is \"keyword\":\n\n\nDoc 1:\nid: 123456,\nlink_id: 2,\nkeyword: Black, Printed, Dress\n\nDoc 2:\nid: 123457,\nlink_id: 2,\nkeyword: Black, Shoes, Patent\n\nDoc 3:\nid: 123458,\nlink_id: 2,\nkeyword: Red, Hat, Felt\n\nDoc 4:\nid: 123459,\nlink_id:1,\nkeyword: Felt, Hat, Black\n\nSo, when you collapse, only two of these documents are in the result set (123456, 123459), and only the keywords Black, Printed, Dress, Felt, and Hat are counted.  The facet count for Black is 2, the facet count for Felt is 1.  If you choose Black and add it to your query, you get 2 results (great).  However, if you add Felt to your query, you get 2 results (because a different document for link_id 2 is chosen in that query than is in the more general query from which the facets are produced).\n\nI think what needs to happen here is that all the terms for all the documents that are collapsed together need to be included (just once) with the document that gets counted for faceting.  In this example, when the document for link_id 2 is counted, it would need to appear to the facet counter to have keywords Black, Printed, Dress, Shoes, Patent, Red, Hat, and Felt, as opposed to just Black, Printed, and Dress. "
        },
        {
            "author": "Iv\u00e1n de Prado",
            "id": "comment-12655952",
            "date": "2008-12-12T10:16:52+0000",
            "content": "You can try with collapse.facet=before, but then you'll notice that the list of documents returned is all, not only the collapsed ones.  "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12656337",
            "date": "2008-12-13T21:43:22+0000",
            "content": "Yes, this is basically what I'm doing for now... At least it's reasonable enough to explain to a client that the counts are for unfilitered results.  However, ideally, it should be able to facet properly on filitered results as well...\n\nAlso, with simply collapse.facet=before, the results returned are the unfilitered results.  You have to specify collapse.facet=after to get filtered results at all, and run the query component right before the facet component then to get the unfilitered facet counts... which doesn't seem to be ideal.  This is with release version of SOLR 1.3 and Iv\u00e1n's most recent patch.  All in all it took a lot of experimenting but at least now I have a method that works that we can go live with and then we'll just update the software as the situation improves.\n\nThanks for all your efforts on the patch!  I complain but really, the fact it works at all is a miracle for us. "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12656716",
            "date": "2008-12-15T19:36:46+0000",
            "content": "I get an error on certain searches with Ivan's latest patch.\n\nDec 15, 2008 2:32:00 PM org.apache.solr.core.SolrCore execute\nINFO: [ss_image_core] webapp=/solr path=/select params=\n{collapse=true&facet.limit=5&wt=json&rows=50&json.nl=map&start=0&sort=add_date+desc,+object_id+asc&facet=true&collapse.facet=after&f.season.facet.limit=-1&facet.mincount=1&fl=object_id&q=object_type:image+AND+classif_name:(19097)+AND+market:(49154)+AND+perms:(1835+OR+4785+OR+1725+OR+1690+OR+2816+OR+3149+OR+3082+OR+2815+OR+2814+OR+3083+OR+4783)&version=1.2&f.classif_name.facet.limit=-1&collapse.field=link_id&collapse.threshold=1&facet.field=classif_name&facet.field=market&facet.field=season&facet.field=city&facet.field=designer&facet.field=category&facet.field=keywords&facet.field=lifestyle}\n hits=263059 status=500 QTime=4508 \nDec 15, 2008 2:32:00 PM org.apache.solr.common.SolrException log\nSEVERE: java.lang.ArrayIndexOutOfBoundsException: 41386\n\tat org.apache.solr.util.OpenBitSet.fastSet(OpenBitSet.java:235)\n\tat org.apache.solr.search.CollapseFilter.addDoc(CollapseFilter.java:214)\n\tat org.apache.solr.search.CollapseFilter.adjacentCollapse(CollapseFilter.java:171)\n\tat org.apache.solr.search.CollapseFilter.<init>(CollapseFilter.java:139)\n\tat org.apache.solr.handler.component.CollapseComponent.process(CollapseComponent.java:52)\n\tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:169)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1204)\n\tat org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:303)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:232)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1115)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:361)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n\tat org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n\tat org.mortbay.jetty.Server.handle(Server.java:324)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)\n\tat org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)\n\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)\n\n\nUnfortunate really, it happens every time this specific search is run, but many, many other searches of similar result set size and considerably more complexity or equivalent complexity will execute fine... I can't honestly tell you what's special about this one search that would make it fail.\n\nFor now the patch is offline until we can figure something out for it...  I can provide access to the machine (I managed to reproduce it in a test environment)  if it would help determine what the problem is / make the software better for everyone. "
        },
        {
            "author": "Karsten Sperling",
            "id": "comment-12657167",
            "date": "2008-12-16T21:20:55+0000",
            "content": "I'm pretty sure the problem Stephen ran into is an off-by-one error in the bitset allocation inside the collapsing code; I ran into the same problem when I customized it for internal use about half a year ago \u2013 and unfortunately forgot all about the problem until reading Stephen's comment just now. Basically the bitset gets allocated 1 bit too small, so there's about a 1/32 chance that if the bit for the document with the highest ID gets set it will cause the AIOOB exception. "
        },
        {
            "author": "Iv\u00e1n de Prado",
            "id": "comment-12657372",
            "date": "2008-12-17T12:43:09+0000",
            "content": "Karsten Sperling was right. Seems that there was a wrong bounds initialization for the OpenBitSet. I have solved it and attached a new patch. \n\nStephen Weiss, can you test if now the error has disappeared?\n\nThanks. "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12658461",
            "date": "2008-12-22T07:55:39+0000",
            "content": "Yes!  It does work.  Thank you both so much!  It's been running for 5 days now without a hiccup.  This is going into production use now (we'll be monitoring), they simply can't wait for the functionality.  From here it looks like if you get faceting tidied up and some docs written, they should be including this soon! "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12658609",
            "date": "2008-12-22T18:09:56+0000",
            "content": "I see there is a patch agains 1.3, is there any current patch against trunk?  (we would need something against trunk in order to consider this for 1.4) "
        },
        {
            "author": "Thomas Traeger",
            "id": "comment-12659300",
            "date": "2008-12-27T01:46:48+0000",
            "content": "I tested 1.3 and ivans latest patch.\n\nWhen I add a Filter Query (fq param) to my query I get an exception \"Either filter or filterList may be set in the QueryCommand, but not both.\". I'm not that familiar with java but at least disabled the exception in SolrIndexSearch.java. I can use Filter Queries now and no problems occured so far. But surely this has to be handled in another way.\n\nBtw, I think this had already been fixed by Karsten back in 2007 in some way (patch field-collapsing-extended-592129.patch). He commented it with:\n\n\"Made a minimal change to SolrIndexSearcher.getDocListC() to support passing both the filter and filterList parameters. In most cases this was already handled anyway.\"\n "
        },
        {
            "author": "dieter grad",
            "id": "comment-12668485",
            "date": "2009-01-29T14:40:01+0000",
            "content": "\nI had to make a patch to fix two issues that we needed for our system. I am not used to this code, so maybe someone can pick this patch and make it something useful for everybody.\n\nThe fixes are:\n\n1) When collapsing.facet=before, only the collapsed documents are returned (and not the whole collection).\n\n2) When collapsing is normal, the selected sort order is preserved by returning the first document of the collapsed group.\n\nFor example, if the values of the collapsing field are:\n\n1) Y\n2) X  \n3) X\n4) Y\n5)X\n6)Z\n\nthe documents returned are 1, 2 and 6, in that order.\n\nSo, for example, if you sort by price ascending, you will get the result sorted by price, where each item is the cheapest item of its collapsed group.\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12674128",
            "date": "2009-02-17T07:29:28+0000",
            "content": "Marked for 1.5 "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12674803",
            "date": "2009-02-18T22:06:05+0000",
            "content": "Are the any concrete plans on where this feature is going? Is it ever going to get support for distributed search? "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12679603",
            "date": "2009-03-06T14:12:45+0000",
            "content": "Help!!\n\nWe've been using this patch in production for months now, and suddenly in the last 3 days it is crashing constantly.\n\nEdit - It's Ivan's latest patch, #3, with Solr 1.3 dist\n\nMar 6, 2009 5:23:50 AM org.apache.solr.common.SolrException log\nSEVERE: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.solr.util.OpenBitSet.ensureCapacityWords(OpenBitSet.java:701)\n\tat org.apache.solr.util.OpenBitSet.ensureCapacity(OpenBitSet.java:711)\n\tat org.apache.solr.util.OpenBitSet.expandingWordNum(OpenBitSet.java:280)\n\tat org.apache.solr.util.OpenBitSet.set(OpenBitSet.java:221)\n\tat org.apache.solr.search.CollapseFilter.addDoc(CollapseFilter.java:217)\n\tat org.apache.solr.search.CollapseFilter.adjacentCollapse(CollapseFilter.java:171)\n\tat org.apache.solr.search.CollapseFilter.<init>(CollapseFilter.java:139)\n\tat org.apache.solr.handler.component.CollapseComponent.process(CollapseComponent.java:52)\n\tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:169)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1204)\n\tat org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:303)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:232)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1115)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:361)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n\tat org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n\tat org.mortbay.jetty.Server.handle(Server.java:324)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)\n\tat org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)\n\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)\n\n\nIt seems to happen randomly - there's no special request happening, nothing new added to the index, nothing.  We've made no configuration changes. The only thing that's happened is more documents have been added since then.  The schema is the same, we have perhaps 200000 more documents in the index now than we did when we first went live with it.\n\nIt was a 32-bit machine allocated 2GB of RAM for Java before.  We just upgraded it to 64-bit and increased the heap space to 3GB, and still it went down last night.  I'm at my wits end, I don't know what to do but this functionality has been live so long now it's going to be extremely painful to take it away.  Someone, please tell me if there's anything I can do to save this thing. "
        },
        {
            "author": "Iv\u00e1n de Prado",
            "id": "comment-12679606",
            "date": "2009-03-06T14:52:25+0000",
            "content": "That is one of the problems that this patch has: The consumption of resources (memory and CPU) increases with the amount of results in the query and with the amount of requests.\n\nIs not trivial to change that. I imaging that deep changes in Solr or Lucene would be needed to have an efficient collapsing. \n\nThe advices I can give you are:\n\n\n\tIncrease the amount of memory or your Solr\n\tUse the parameter \"collapse.maxdocs\" . This parameter limits the number of document that are seen when collapsing. By using it, you'll limit the amount or memory and resources used per each query. But if the query you did has more than maxdocs documents, then the collapsing won't be perfect. Some documents won't be collapsed.\n\n\n\nI hope it helps something.  "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12679610",
            "date": "2009-03-06T15:12:19+0000",
            "content": "Thank you so much for your prompt response Ivan, I really appreciate your help.\n\nI have already maxed out the RAM on the machine - it seems very strange to me that adding a whole other GB of RAM did not fix the issue already.  So I will have to try the next option, collapse.maxdocs.\n\nHow does this work though?  Does this mean, let's say I set collapse.maxdocs to 10000, that means the first 10000 documents will be collapsed, and after that they won't be?  Or is it more random? "
        },
        {
            "author": "Iv\u00e1n de Prado",
            "id": "comment-12679615",
            "date": "2009-03-06T15:24:09+0000",
            "content": "Is not random. I don't remember pretty well, but I think that documents are sorted by the collapsing field. After that, they are being grouped sequentially until reaching maxdocs. The groups that results from there are the documents that are presented. So the number of groups resulted are always smaller than the number of maxdocs. \n\nSummary: only maxdocs are scanned to generate the resulting groups.\n "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12679618",
            "date": "2009-03-06T15:43:02+0000",
            "content": "Unfortunately I don't think that will work for us.  The collapse.maxdocs seems to collapse the oldest documents in the index - but we sort from newest to oldest, so effectively the newest documents in the index are just left out.  Not only do they not collapse but they don't appear at all.  If this is the only solution then we will have to stop using the patch... and unfortunately this means in general we will probably have to stop using Solr.  The company has already made clear that this functionality is required, and especially since it has been working now for several months they will be very unlikely to accept that they can't have it anymore.\n\nAnyway I don't want to give up yet...\n\nI'm really not convinced this is really a problem of running out of the necessary memory to complete the operation - it only started doing this very recently.  How does it run for 3 months with 2GB of RAM without any trouble, and now it fails even with 3GB of RAM?  It's not like we just added those 200000 documents yesterday - they have accumulated over the past few months, in the past 3 days we've only perhaps added 20,000 documents.  20,000 more documents (with barely any new search terms at all) means it needs more than 1GB of memory more than what it was already using?  If we grow by 25% every year that means by December we will need 50GB of RAM in the machine. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12679622",
            "date": "2009-03-06T15:49:12+0000",
            "content": "How much RAM does the machine have total? 4 GB?\n\nDo you ever commit rapidly?\n\nYou might try decreasing your cache sizes if you are using them. "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12679638",
            "date": "2009-03-06T16:37:48+0000",
            "content": "The machine has 4GB total.  In response to this issue, and especially now that we have upgraded it to be 64 bit (again, for this issue), we have already ordered another 16 GB for the machine to try and stave off the problem.  We should have it in next week.\n\nI restrict commits severely - a commit is only allowed once an hour, in practice they happen even less frequently - perhaps 5 or 6 times a day, and very spread out.  We are freakishly paranoid   But honestly that's all we need - new documents come in in chunks and generally they want them to go in all at once, and not piecemeal, so that the site updates cleanly (the commits are synchronized with other content updates - new images on the home page, etc).\n\nSome more information... just trying to toss out anything that matters.  We have a very small set of possible terms - only 60,000 or so which tokenize to perhaps 200,000 total distinct words.  We do not use synonyms at index time (only at query time).  We use faceting, collapsing, and sorting - that's about it, no more like this or spellchecker (although we'd like to, we haven't gotten there yet).  Faceting we do use heavily though - there are 16 different fields on which we return facet counts.  All these fields together represent no more than 15,000 unique terms.  There are approx. 4M documents in the index total, and none of them are larger than 1K.\n\nMemory usage on the machine seems to steadily increase - after restart and warming, 40% of the RAM on the machine is in use.  Then, as searches come in, it steadily increases.  Right now it is using 61%, in an hour it will probably be closer to 75% - the danger zone.  This is also unusual because before, it used to stay pretty steady around 52-53%.\n\nThis is a multi-core system - we have 2 cores, the one I'm describing now is only one of them.  The other core is very, very small - total 8000 documents, which are also no more than 1 K each.  We do use faceting there but no collapsing (it is not necessary for that part).  It is essentially irrelevant, with or without that core the machine consumes about the same amount of resources.\n\nIn response to this problem I have already dramatically reduced the following options:\n\n<     <mergeFactor>2</mergeFactor>\n<     <maxBufferedDocs>100</maxBufferedDocs>\n\u2014\n>     <mergeFactor>10</mergeFactor>\n>     <maxBufferedDocs>1000</maxBufferedDocs>\n42c42\n<     <maxFieldLength>2500</maxFieldLength>\n\u2014\n>     <maxFieldLength>10000</maxFieldLength>\n50,51c50,51\n<     <mergeFactor>2</mergeFactor>\n<     <maxBufferedDocs>100</maxBufferedDocs>\n\u2014\n>     <mergeFactor>10</mergeFactor>\n>     <maxBufferedDocs>1000</maxBufferedDocs>\n53c53\n<     <maxFieldLength>2500</maxFieldLength>\n\u2014\n>     <maxFieldLength>10000</maxFieldLength>\n\n\n( diff of solrconfig.xml - < indicates current values, > indicates values when the problem started happening).\n\nThis actually seemed to make the search much faster (strangely enough), but it doesn't seem to have helped memory consumption very much.\n\nThese are our cache parameters:\n\n    <filterCache\n      class=\"solr.LRUCache\"\n      size=\"65536\"\n      initialSize=\"4096\"\n      autowarmCount=\"2048\"/>\n\n    <queryResultCache\n      class=\"solr.LRUCache\"\n      size=\"512\"\n      initialSize=\"512\"\n      autowarmCount=\"256\"/>\n\n    <documentCache\n      class=\"solr.LRUCache\"\n      size=\"16384\"\n      initialSize=\"16384\"\n      autowarmCount=\"0\"/>\n\n    <cache name=\"collapseCache\"\n      class=\"solr.LRUCache\"\n      size=\"512\"\n      initialSize=\"512\"\n      autowarmCount=\"0\"/>\n\nI'm actually not sure if the collapseCache even does anything since it does not appear in the admin listing.  I'm going to try reducing the filterCache to 32K entries and see if that makes a difference.  I think that may be the right track since otherwise it seems like a big memory leak is happening.\n\nIs there any way to specify the size of the cache in terms of the actual size it should take up in memory, as opposed to the number of entries?  64K sounded quite small to me but now I'm thinking that 64K could mean GB's of memory depending on what the entries are, I honestly don't understand what the correlation would be between an entry and the size that entry takes in RAM. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12679649",
            "date": "2009-03-06T16:57:04+0000",
            "content": "we have already ordered another 16 GB for the machine to try and stave off the problem. We should have it in next week. \n\nGreat. You've got a lot going on here, and 4 GB is on the extremely low end of what I'd suggest.\n\nI restrict commits severely -\n\nGood news again.\n\nIn response to this problem I have already dramatically reduced the following options:\n\nDropping the merge factor is not likely to help much. It will increase the time it takes to add docs (merges occur much more often) for the benefit of maintaining an almost optimized index at all times (hence the faster search speed). Not a big RAM factor though.\n\nAlso, dropping the max buffered docs is also probably not a huge saver, and will only affect RAM usage during indexing. Going from 1000 to 100 will likely hurt indexing performance and not save that much RAM in the larger scheme of things.\n\nAnd dropping the maxFieldLength will hide parts of the document that are over that length - perhaps youll end up with a handful fewer index terms, but again, not likely a big savings here and may do more harm than good.\n\nMy suggestion of lowering your cache sizes was just a thought to eek out some more RAM for you. Its not really suggested though if you can get more RAM. For best performance, those caches should be set correctly. If you are using the fieldcache method for faceting, you want the size of the filter cache to be the same as the number of unique terms you are faceting on. The other caches are not so large that I would suggest trimming them.\n\nThe reality is, you've got 4 million docs, sorting (uses field caches), faceting (likely uses field caches), and this resource intensive field collapse patch. More RAM is probably your best bet. Every document you add potentially adds to the RAM usage of each of these things. That doesn't mean you don't have a different problem (it does seem weird it ballooned all of a sudden), but your running some RAM hungry stuff here, and it wouldn't blow my mind that 3 gig is not enough to handle it. It could be that only recently the right searches started coming in at the right times to fire up all your needs at once. Much of this may be lazy loaded or loaded on the fly depending on if and how you have configured your warming searches.\n "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12679656",
            "date": "2009-03-06T17:25:57+0000",
            "content": "Thanks.  In the wiki next to each one of these parameters it explicitly says that reducing this parameter will decrease memory usage, this is why we reduced these parameters (it did not mention the filterCache at all).  \n\nI really do hope the RAM will help.  It certainly can't help.\n\nMy filterCache stats are great- you  know it's set to 64K but right now, with almost all the RAM used up (we're at 71.9% now), but it's only using 36290 entries at the moment and it's holding pretty steady there (even as RAM usage increased by 10%).  None of the other caches have gone up much either.  We have no cache evictions, at all, but a 99% hit ratio.\n\nI'm going to try lowering the filterCache to be just above the number it's at now, since that amount seems to be all it needs.  It's possible at crash time all the sudden is uses a lot more of it for some reason - I have a feeling it might be related to a new permissions group that was added 3 days ago.  That might trigger a lot more filters.  It is barely used at all yet except by one client - I'm going to go check and see if there's any correspondence between when that client logs in and when the problem occurs - I bet there is.\n\nThanks for all your help guys. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12679662",
            "date": "2009-03-06T17:39:52+0000",
            "content": "Thanks. In the wiki next to each one of these parameters it explicitly says that reducing this parameter will decrease memory usage, this is why we reduced these parameters (it did not mention the filterCache at all).\n\nThey will save RAM to a certain extent for certain situations. But not very helpful at the sizes you are working with (and not settings I would use to save RAM anyway, unless the amount I need to save was pretty small). Also, the savings are largely index side - not likely a huge part of your RAM concerns, which are search side.\n\nMy filterCache stats are great- you know it's set to 64K but right now, with almost all the RAM used up (we're at 71.9% now), but it's only using 36290 entries at the moment and it's holding pretty steady there(even as RAM usage increased by 10%). None of the other caches have gone up much either. We have no cache evictions, at all, but a 99% hit ratio.\n\nThe sizes may be higher than you need then. They should be adjusted to the best settings based on the wiki info. I was originally suggesting you might sacrifice speed with the caches for RAM - but, its always best to use the best settings and have the necessary RAM. "
        },
        {
            "author": "Dmitry Lihachev",
            "id": "comment-12689028",
            "date": "2009-03-25T08:24:10+0000",
            "content": "When I add a Filter Query (fq param) to my query I get an exception \"Either filter or filterList may be set in the QueryCommand, but not both.\" "
        },
        {
            "author": "Dmitry Lihachev",
            "id": "comment-12689029",
            "date": "2009-03-25T08:26:20+0000",
            "content": "This patch (based on dieter patch) allows using fq parameter "
        },
        {
            "author": "dredford",
            "id": "comment-12694851",
            "date": "2009-04-02T00:54:11+0000",
            "content": "There is an issue with collapsed result ordering when querying with only the unique Id and score fields in the request.\n\n[Update: this is only an issue when both standard results and collapse results are present - which I was using for testing]\n\neg: \nq=ford&version=2.2&start=0&rows=10&indent=on&fl=Id,score&collapse.field=PrimaryId&collapse.max=1\n\ngives wrong ordering (note: Id is our unique Id)\n\nbut adding a another field - even a bogus one - works.\nq=ford&version=2.2&start=0&rows=10&indent=on&fl=Id,score,bogus&collapse.field=PrimaryId&collapse.max=1\n\nAlso using an fq makes it work \neg:\nfq=Type:articles&q=ford&version=2.2&start=0&rows=10&indent=on&fl=Id,score&collapse.field=PrimaryId&collapse.max=1\n\nI'm using the latest Dmitry patch (25/mar/09) against 1.3.0.\n\nApart from that great so far...thanks to all "
        },
        {
            "author": "Jeff",
            "id": "comment-12699903",
            "date": "2009-04-16T22:09:58+0000",
            "content": "We have tried to integrate the most recent patch into our 1.4 install.  The patching was smooth and overall it works good.  However, it appears the issue with fq has returned.  Whenever I try to filter the query it gives \"Either filter or filterList may be set in the QueryCommand, but not both.\"  Not sure what happened.  What part of the patch makes it possible for fq to work as it may not be there now.\n\nAdditionally, the collapse.facet=before seems to not work.  Any help in this area would be greatly appreciated. "
        },
        {
            "author": "Domingo G\u00f3mez Garc\u00eda",
            "id": "comment-12701862",
            "date": "2009-04-23T09:19:57+0000",
            "content": "I made checkout on svn release-1.3.0 and applied SOLR-236_collapsing.patch.\nIs there any way of integrate with solrj? "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12704162",
            "date": "2009-04-29T15:45:36+0000",
            "content": "How did you fix the memory issue? "
        },
        {
            "author": "Domingo G\u00f3mez Garc\u00eda",
            "id": "comment-12704167",
            "date": "2009-04-29T15:49:14+0000",
            "content": "-XX:PermSize=1524m -XX:MaxPermSize=1524m -Xmx128m\nIt's not a real fix, but works for now... "
        },
        {
            "author": "Thomas Traeger",
            "id": "comment-12706644",
            "date": "2009-05-06T22:48:48+0000",
            "content": "This patch is based on the latest patch by Dmitry, it addresses the following issues:\n\n\tthe CollapseComponent now simply falls back to the process method of QueryComponent when no collapse.field is defined. This fixes issues with the fq param when collapsing was disabled and makes CollapseComponent a fully compatible replacement for QueryComponent.\n\tcollapse.facet=before is now fixed, the previous patch ignored any filter queries (fq) and therefore returned wrong facet counts\n\tResponseBuilder \"builder\" renamed to \"rb\" to match QueryComponent\n\n\n\nThis patch applies to trunk (rev. 772433) but works with Solr 1.3 too. For 1.3 you have to move CollapseParams.java from common/org/apache/solr/common/params to java/org/apache/solr/common/params/ as the location of this file has been changed in trunk.\n\nThis is my first contribution so any feedback is much appreciated. This is a great feature so lets get it into Solr as soon as possible. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12714442",
            "date": "2009-05-29T12:52:11+0000",
            "content": "Hi,\n\nI have modified the latest patch of Thomas and made two performance improvements: \n1) Improved normal field collapsing. I tested it with an index 1.1 million documents. When collapsing on all documents and with no sorting specified (so sorting on score) the query time is around 130ms compared with the previous patch which is around 1.5 s. When I then add sorting on string field the query time is around 220 ms compared with the previous patch which is around 5.2 s. \n\nThe reason why it is faster is because the latest patch queries for a doclist instead of a docset. In the normal collapse method it keeps track of the most relevant documents, so the end result is the same, also creating a docList of 1.1 million documents (and ordering it) is very expensive.\n\nNote: I did not improved adjacent collapsing, because the adjacent method needs (as far as I understand it) a completely sorted list of documents (docList).\n\n2) Slightly improved facetation in combination with field collapsing, by reusing the uncollapsed docset that is created during the collapsing process (the previous patch made invoked a second search).\n\nI also have added documentation, added a few unit tests for the collapsing process itself and made the debug information more readable.\nThis patch works from revision 779335 (last Wednesday) and up. This patch depends on some changes in Solr and a change inside Lucene.\n\nI'm very interested in other people's experiences with this patch and feedback on the patch itself. \n\nCheers,\n\nMartijn  "
        },
        {
            "author": "Thomas Traeger",
            "id": "comment-12714676",
            "date": "2009-05-30T07:14:02+0000",
            "content": "I made some tests with your patch and trunk (rev. 779497). It looks good so far but I have some problems with occasional null pointer exceptions when using the sort parameter:\n\nhttp://localhost:8983/solr/select?q=*:*&collapse.field=manu&sort=score%20desc,alphaNameSort%20asc\n\njava.lang.NullPointerException\n\tat org.apache.lucene.search.FieldComparator$RelevanceComparator.copy(FieldComparator.java:421)\n\tat org.apache.solr.search.CollapseFilter$DocumentComparator.compare(CollapseFilter.java:649)\n\tat org.apache.solr.search.CollapseFilter$DocumentPriorityQueue.lessThan(CollapseFilter.java:596)\n\tat org.apache.lucene.util.PriorityQueue.insertWithOverflow(PriorityQueue.java:153)\n\tat org.apache.solr.search.CollapseFilter.normalCollapse(CollapseFilter.java:321)\n\tat org.apache.solr.search.CollapseFilter.<init>(CollapseFilter.java:211)\n\tat org.apache.solr.handler.component.CollapseComponent.process(CollapseComponent.java:67)\n\tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:195)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1328)\n\tat org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:341)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:244)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1089)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:365)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:712)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:405)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:211)\n\tat org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:139)\n\tat org.mortbay.jetty.Server.handle(Server.java:285)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:502)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:821)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:513)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:208)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:378)\n\tat org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:226)\n\tat org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:442)\n\nThese queries work as expected:\nhttp://localhost:8983/solr/select?q=*:*&collapse.field=manu&sort=score%20desc\nhttp://localhost:8983/solr/select?q=*:*&sort=score%20desc,alphaNameSort%20asc "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12714695",
            "date": "2009-05-30T11:26:37+0000",
            "content": "Thanks for the feedback, I fixed the problem you described and I have added a new patch containing the fix.\nThe problem occurred when sorting was done on one ore more normal fields and on scoring. \n "
        },
        {
            "author": "Thomas Traeger",
            "id": "comment-12714738",
            "date": "2009-05-30T16:11:12+0000",
            "content": "The problem is solved, thanks. I will use your patch for my current project that is planned for golive in 5 weeks. If I find any more issues I will report them here. "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12714742",
            "date": "2009-05-30T16:25:22+0000",
            "content": "Hey guys, are there any plans to make field collapsing work on multi shard systems? "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12714750",
            "date": "2009-05-30T17:39:58+0000",
            "content": "I'm looking forward in your experiences with this patch, particular in production. \n\nI think in order to make collapsing work on multi shard systems the process method of the CollapseComponent needs to be modified.\nCollapseComponent already subclasses QueryComponent (which already supports querying on multi shard systems), so it should not be that difficult. "
        },
        {
            "author": "Ron Veenstra",
            "id": "comment-12716128",
            "date": "2009-06-04T02:15:55+0000",
            "content": "I require assistance.  I've installed a fresh Solr (1.3.0), and all appears/operates well.  I then patch using SOLR-236_collapsing.patch [by  \tThomas Traeger]  (the last patch i saw claimed to work with 1.3.0), without error.  I then add to solrconfig.xml the following (per: http://wiki.apache.org/solr/FieldCollapsing) :\n\n  <searchComponent name=\"collapse\"     class=\"org.apache.solr.handler.component.CollapseComponent\" />\n\nUpon restart, I get a long configuration error, which seems to hinge on:\n\nHTTP Status 500 - Severe errors in solr configuration. Check your log files for more detailed information on what may be wrong. If you want solr to continue after configuration errors, change: <abortOnConfigurationError>false</abortOnConfigurationError> in solrconfig.xml ------------------------------------------------------------- org.apache.solr.common.SolrException: Error loading class 'org.apache.solr.handler.component.CollapseComponent' at org.apache.solr.core.SolrResourceLoader.findClass(SolrResourceLoader.java:273)\n\n[the full error can be included if desired.]\n\nI've verified that the CollapseComponent file exists in the proper place.\nI've moved CollapseParams as required, (move CollapseParams.java from common/org/apache/solr/common/params to java/org/apache/solr/common/params/ )\nI've tried multiple iterations of the patch (on fresh installs), all with the same issue.\n\nAre there additional steps, patches, or configurations that are required?\nIs this a known issue?\nAny help is very much appreciated. "
        },
        {
            "author": "Thomas Traeger",
            "id": "comment-12716364",
            "date": "2009-06-04T18:35:41+0000",
            "content": "ron, your approach should work, I just verified it on my Ubuntu 9.04 box. Here are my steps to a working example installation of solr 1.3.0 with collapsing enabled:\n\n\njava -version\n> java version \"1.6.0_13\"\n> Java(TM) SE Runtime Environment (build 1.6.0_13-b03)\n> Java HotSpot(TM) 64-Bit Server VM (build 11.3-b02, mixed mode)\n\nwget http://www.apache.org/dist/lucene/solr/1.3.0/apache-solr-1.3.0.tgz\ntar xvzf apache-solr-1.3.0.tgz \nwget http://issues.apache.org/jira/secure/attachment/12407410/SOLR-236_collapsing.patch\ncd apache-solr-1.3.0/\npatch -p0 <../SOLR-236_collapsing.patch \nmv src/common/org/apache/solr/common/params/CollapseParams.java src/java/org/apache/solr/common/params/\nant example\ncd example/\nvi solr/conf/solrconfig.xml \n\n\n\nadd the collapse component class definition:\n\n\n<searchComponent name=\"collapse\" class=\"org.apache.solr.handler.component.CollapseComponent\" />\n\n\n\nset the components in the standard requestHandler:\n\n\n    <arr name=\"components\">\n      <str>collapse</str>\n    </arr>\n\n\n\nstart jetty\n\njava -jar start.jar\n\n\nadd example docs\n\ncd example/exampledocs\nsh post.sh *.xml\n\n\nand open http://localhost:8983/solr/select/?q=*:*&collapse.field=cat in your browser. "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12716373",
            "date": "2009-06-04T19:14:42+0000",
            "content": "The problem sounds very familiar to me, I remember going through something similar when I was first trying to get the patch to work.  My configuration ended up being:\n\n <searchComponent name=\"collapse\"     class=\"org.apache.solr.handler.component.CollapseComponent\" />\n\n  <requestHandler name=\"standard\" class=\"solr.StandardRequestHandler\">\n    <!-- default values for query parameters -->\n     <lst name=\"defaults\">\n       <str name=\"echoParams\">explicit</str>\n       <!-- \n       <int name=\"rows\">10</int>\n       <str name=\"fl\">*</str>\n       <str name=\"version\">2.1</str>\n        -->\n     </lst>\n     <arr name=\"components\">\n        <str>query</str>\n        <str>facet</str>\n        <str>collapse</str>\n        <str>mlt</str>\n        <str>highlight</str>\n        <str>debug</str>\n     </arr>\n  </requestHandler>\n\nAll I remember is if I didn't have that <arr name=\"components\"> section arranged exactly like that (even if I rearranged other items without rearranging the \"collapse\" part), either 1) faceting would completely stop working correctly at all, giving me totally bogus numbers or 2) I would get something a lot like the error described above and nothing would work at all.\n\nHowever, I'm using an older version of the patch (collapsing-patch-to-1.3.0-ivan_3.patch) so it's totally possible that this has nothing to do with that.\n\nOn that note... have people found in general that the newer versions of the patch give any particular benefits in particular?  I saw someone say that the latest patches were faster but I wasn't sure if they were faster in all cases or only when not sorting (we always sort so if it's only for unsorted sets it doesn't do us much good). "
        },
        {
            "author": "Ron Veenstra",
            "id": "comment-12716412",
            "date": "2009-06-04T21:25:19+0000",
            "content": "Thanks for the replies.\n\nThomas, I followed your steps, verifying same java version and build, etc. (all matched.  I'm working with a CentOS 5 machine..Any potential for the problem being related to that?)  \nPatching and installing all appeared successful, but the resulting jetty powered page still resulted in:\n\norg.apache.solr.common.SolrException: Error loading class 'org.apache.solr.handler.component.CollapseComponent'\n[followed by the long line of tracebacks..]\n\nMy solrconfig.xml included the following (included in case there is an obvious flaw):\n\n\n<searchComponent name=\"collapse\" class=\"org.apache.solr.handler.component.CollapseComponent\" />\n\n  <requestHandler name=\"standard\" class=\"solr.SearchHandler\" default=\"true\">\n    <!-- default values for query parameters -->\n     <lst name=\"defaults\">\n       <str name=\"echoParams\">explicit</str>\n       <!-- \n       <int name=\"rows\">10</int>\n       <str name=\"fl\">*</str>\n       <str name=\"version\">2.1</str>\n        -->\n     </lst>\n\n  <arr name=\"components\">\n      <str>collapse</str>\n    </arr>\n  </requestHandler>\n\n\nStephen: I attempted your configuration as well, with the most recent patch and the patch you referenced, but the results were the same.\n\nI am going to attempt a fresh try on an Ubuntu Machine, but any other ideas would be most appreciated. "
        },
        {
            "author": "Thomas Traeger",
            "id": "comment-12716433",
            "date": "2009-06-04T22:34:49+0000",
            "content": "Strange, maybe something went wrong during building and CollapseComponent is not included into the war. You might look into solr.war and check for CollapseComponent.class:\n\n\ncd apache-solr-1.3.0/example/webapps\nunzip solr.war\ncd WEB-INF/lib\nunzip apache-solr-core-1.3.0.jar\ncd org/apache/solr/handler/component/\n\n\nIs the file CollapseComponent.class there? "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12716441",
            "date": "2009-06-04T22:46:05+0000",
            "content": "Hi Stephan, when I was doing performance tests on the latest patch for doing normal collapsing (not adjacent collapsing), I found that there was a significant performance improvement during field collapsing compared to the old patch. This applies for both specifying sorting and not specifying sorting in the request. If you have other questions / comments about the latest patch just ask.  "
        },
        {
            "author": "Ron Veenstra",
            "id": "comment-12716459",
            "date": "2009-06-05T00:03:59+0000",
            "content": "Thomas,\n\nAgain thanks.  I've verified that the CollapseComponent is indeed NOT present in the war.  That'd suggest something going amiss during the patching process, correct?  And as it appears to be happening each time, either there's an issue with the patch (which others have verified as working) or something conflicts with my current setup (solr / tomcat / CentOS).  Can I manually create apache-solr-core and force the file in? "
        },
        {
            "author": "Ron Veenstra",
            "id": "comment-12716527",
            "date": "2009-06-05T07:06:53+0000",
            "content": "Quick update :  starting fresh, i was able to get the issue resolved once ant properly rebuilt the solr-core file. Uncertain why previous attempts failed so completely.  Many thanks for your help. "
        },
        {
            "author": "Earwin Burrfoot",
            "id": "comment-12717110",
            "date": "2009-06-07T23:42:04+0000",
            "content": "I have implemented collapsing on a high-volume project of mine in much less flexible, but more practical manner.\n\nPart I. You have to guarantee that all documents having the same value of collapse-field are dropped into Lucene index as a sequential batch. That guarantees they get sequential docIds, and with some more work - that they all end up in the same segment.\nPart II. When doing collection you always get docIds in sequential order, and thus, thanks to Part I you get the docs-to-be-collapsed already grouped by collapse-field, even before you drop the docs into PriorityQueue to sort them.\n\nCons:\nYou can only collapse on a single predetermined at index creation time field.\nIf one document changes, you have to reindex all docs that have the same collapse-field value, so it's best if you have either low update/add rates, or few documents sharing the same collapse-field value.\n\nPros:\nThe CPU and memory costs for collapsing compared to usual search are very close to zero and do not depend on index size/total docs found.\nThe same idea works with new Lucene per-segment collection and in distributed mode (sharded index).\nWithin collapsed group you can sort hits however you want, and select one that will represent the group for usual sort/paging.\nThe implementation is not brain-dead simple, but nears it. "
        },
        {
            "author": "Kevin Cunningham",
            "id": "comment-12717827",
            "date": "2009-06-09T21:35:01+0000",
            "content": "Martijn,\nYou mentioned your latest patch update \"depends on some changes in Solr and a change inside Lucene\".  Does this mean it is not compatible with 1.3? "
        },
        {
            "author": "emmanuel vecchia",
            "id": "comment-12718116",
            "date": "2009-06-10T15:46:43+0000",
            "content": "I applied the latest patch field-collapse-solr-236-2.patch to http://www.apache.org/dist/lucene/solr/1.3.0/apache-solr-1.3.0.tgz and tried to compile it seems to require org.apache.lucene.search.FieldComparator and org.apache.lucene.search.Collector and maybe other classes from lucene. I checked out a few version of lucene but looking at LUCENE-1483 it seems that only the current trunk have the classes needed. So it doesn't seem to be possible to use the patch with 1.3  "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12718145",
            "date": "2009-06-10T17:24:50+0000",
            "content": "Keven, that is correct my patch is not compatible with 1.3. It works from revision 779497 (which is 1.4-dev). "
        },
        {
            "author": "Shekhar",
            "id": "comment-12718969",
            "date": "2009-06-12T20:38:12+0000",
            "content": "Hi,\n\nHas anyone successfully used localsolr and collapse patch together in Solr 1.4-dev. I am getting two result-sets one from localsolr and other from collapse. I need a merged result-set..\nI am using localsolr 1.5 and field-collapse-solr-236-2.patch.\nAny pointers  ???\n "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12719360",
            "date": "2009-06-15T00:26:10+0000",
            "content": "Shekar, can you show how you configured local solr and field collapsing in the solrconfig.xml file? "
        },
        {
            "author": "Shekhar",
            "id": "comment-12719677",
            "date": "2009-06-15T18:05:35+0000",
            "content": "Here is the solfconfig file.\n\n\n<requestHandler name=\"geo\" class=\"solr.SearchHandler\">\n    <lst name=\"defaults\">     \n        <str name=\"echoParams\">explicit</str>\n    </lst>    \n\n    <arr name=\"components\">\n      <str>localsolr</str>\n\t<str>collapse</str> \t\n    </arr>\n\n</requestHandler>\n\nYou can get more details from http://www.gissearch.com/localsolr\n\n\n===================================================\n\nFollowing are the results I am getting :\n\n<response>\n\u2212\n<lst name=\"responseHeader\">\n<int name=\"status\">0</int>\n<int name=\"QTime\">146</int>\n\u2212\n<lst name=\"params\">\n<str name=\"lat\">41.883784</str>\n<str name=\"radius\">50</str>\n<str name=\"collapse.field\">resource_id</str>\n<str name=\"rows\">2</str>\n<str name=\"indent\">on</str>\n<str name=\"fl\">resource_id,geo_distance</str>\n<str name=\"q\">TV</str>\n<str name=\"qt\">geo</str>\n<str name=\"long\">-87.637668</str>\n</lst>\n</lst>\n\u2212\n<result name=\"response\" numFound=\"4294\" start=\"0\">\n\u2212\n<doc>\n<int name=\"resource_id\">10018</int>\n<double name=\"geo_distance\">26.16691883965225</double>\n</doc>\n\u2212\n<doc>\n<int name=\"resource_id\">10102</int>\n<double name=\"geo_distance\">39.90588996589528</double>\n</doc>\n</result>\n\u2212\n<lst name=\"collapse_counts\">\n<str name=\"field\">resource_id</str>\n\u2212\n<lst name=\"doc\">\n<int name=\"10022\">116</int>\n<int name=\"11701\">4</int>\n</lst>\n\u2212\n<lst name=\"count\">\n<int name=\"10015\">116</int>\n<int name=\"10018\">4</int>\n</lst>\n\u2212\n<lst name=\"debug\">\n<str name=\"Docset type\">BitDocSet(5201)</str>\n<long name=\"Total collapsing time(ms)\">46</long>\n<long name=\"Create uncollapsed docset(ms)\">22</long>\n<long name=\"Collapsing normal time(ms)\">24</long>\n<long name=\"Creating collapseinfo time(ms)\">0</long>\n<long name=\"Convert to bitset time(ms)\">0</long>\n<long name=\"Create collapsed docset time(ms)\">0</long>\n</lst>\n</lst>\n\u2212\n<result name=\"response\" numFound=\"5201\" start=\"0\">\n\u2212\n<doc>\n<int name=\"resource_id\">10015</int>\n</doc>\n\u2212\n<doc>\n<int name=\"resource_id\">10018</int>\n</doc>\n</result>\n</response> "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12720264",
            "date": "2009-06-16T17:55:35+0000",
            "content": "The LocalSolrQueryComponent and the CollapseComponent are both doing a search, that is why there are two result sets. \nI think if you want field collapsing and local search you cannot use the version of localsolr that you are currently using, but you can use the latest\nlocal solr patch (SOLR-773). The latest patch does local search in a different manner, the DistanceCalculatingComponent (LocalSolrQueryComponent is removed) self does not do a search, but it adds a filter query (based on the lat, long and radius) to the normal search, that is then executed in the collapse component, so it should work in the way you expect it.\n\nExample configuration for the latest patch:\n<searchComponent name=\"geodistance\" class=\"org.apache.solr.spatial.tier.DistanceCalculatingComponent\" />  \n<queryParser name=\"spatial_tier\" class=\"org.apache.solr.spatial.tier.SpatialTierQueryParserPlugin\" />  \n<requestHandler name=\"geo\" class=\"org.apache.solr.handler.component.SearchHandler\">  \n\t<lst name=\"defaults\">  \n\t\t<str name=\"echoParams\">explicit</str>  \n\t\t<str name=\"defType\">spatial_tier</str>  \n\t</lst>  \n\t<lst name=\"invariants\">  \n\t\t<str name=\"latField\">lat</str>  \n\t\t<str name=\"lngField\">lng</str>  \n\t\t<str name=\"distanceField\">geo_distance</str>  \n\t\t<str name=\"tierPrefix\">tier</str>  \n\t</lst>  \n       <arr name=\"components\">\n                <str>collapse</str>\n        </arr>\n\t<arr name=\"last-components\">  \n\t\t<str>geodistance</str>  \n\t</arr>  \n</requestHandler> "
        },
        {
            "author": "Shekhar",
            "id": "comment-12720908",
            "date": "2009-06-17T21:28:26+0000",
            "content": "Thanks a lot Martijn for you help..\nCould you please point me to the example you are referring to. I could not find any example which is using DistanceCalculatingComponent.  "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12721100",
            "date": "2009-06-18T07:05:36+0000",
            "content": "I have not found an online example yet, but I copied this config from the javadoc of the DistanceCalculatingComponent class and modified it. The patch also modifies the solr examples, so i f you look there you can see how the patch is used (example/solr/conf/schema.xml and example/solr/conf/solrconfig.xml). You need to add an extra update processor and an extra field and dynamic field in order to make it work. "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12725283",
            "date": "2009-06-29T17:57:34+0000",
            "content": "Hello all. We implemented the old Field Collapse patch ( 2008-02-14 03:38 PM) a few months ago into our production environment with some custom code to make it work over distributed search.  Shortly after deployment we started noticing extremely slow queries (3-12 seconds) on a completely random basis. After disabling field collapse these random queries disappeared. Does anyone here know of the issue that might have caused this, and any idea if it has been fixed in the patches since the one we used? "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12728044",
            "date": "2009-07-07T11:43:13+0000",
            "content": "Hi Oleg, I have checked your latest patch, but I could not find the code that deals with the distributed search. How did you make collapsing work for distributed search? Which parameters did you use while doing a search? What I can tell is that the latest patches do not support field collapsing for distributed search. "
        },
        {
            "author": "David Smiley",
            "id": "comment-12728050",
            "date": "2009-07-07T11:52:17+0000",
            "content": "Auto-reply: I'm on Vacation this week. "
        },
        {
            "author": "Jay Hill",
            "id": "comment-12735199",
            "date": "2009-07-24T22:54:37+0000",
            "content": "I've tried applying the most recent patch against a completely fresh check out of the trunk, but I'm getting compile errors related to a class updated in the patch:\n\ncompile:\n    [mkdir] Created dir: /Users/jayhill/solrwork/trunk/build/solr\n    [javac] Compiling 367 source files to /Users/jayhill/solrwork/trunk/build/solr\n    [javac] /Users/jayhill/solrwork/trunk/src/java/org/apache/solr/util/DocSetScoreCollector.java:31: org.apache.solr.util.DocSetScoreCollector is not abstract and does not override abstract method acceptsDocsOutOfOrder() in org.apache.lucene.search.Collector\n    [javac] public class DocSetScoreCollector extends Collector {\n    [javac]        ^\n    [javac] Note: Some input files use or override a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] Note: Some input files use unchecked or unsafe operations.\n    [javac] Note: Recompile with -Xlint:unchecked for details.\n    [javac] 1 error\n\nI noticed that FieldCollapsing is targeted for release 1.5, but I've noticed that some folks have been using it in production, and I was curious to work with it in 1.4 is possible. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12735270",
            "date": "2009-07-25T12:58:11+0000",
            "content": "Hey Jay, I have fixed this issue in the new patch. So if you apply the new patch everything should be fine.\nThe compile error was a result of of the upgrade of the Lucene libraries is Solr. Because of LUCENE-1630 a new method was added to the Collector class. \nIn this patch I also removed the invocations to ExtendedFieldCache methods and changed them to FieldCache methods. ExtendedFieldCache is now deprecated in the updated Lucene libraries. If you have any problems with this patch let me know.\n\nImportant:\nOnly use this patch from revision 794328 (07/15/2009) and up. Use the previous patch if you are using an older 1.4-dev revision. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12741530",
            "date": "2009-08-10T20:20:25+0000",
            "content": "Because the lucene jars have been updated, the previous patch does not work with the current trunk.\nUse this patch for rev 801872 and up. For revisions before that use the older patches.\n\nI have also included SolrJ support for fieldcollapsing in this patch. Might be handy those integrating with Solr via SolrJ.\nBy invoking enableFieldCollapsing(...) with a fieldname as parameter on SolrQuery class you enable fieldcollapsing for the current request.\nIf the search was successful one can execute getFieldCollapseResponse() on SolrResponse for retrieving a FieldCollapseResponse object from which one can retrieve the field collapse information.    "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12747066",
            "date": "2009-08-24T20:59:11+0000",
            "content": "I have updated the field collapse patch and made the following changes:\n\n\tRefactored the collapse code into a strategy pattern.  The two distinct manners of collapsing are now in two different classes, which in my understanding makes the code cleaner and easier to understand. I have removed the CollapseFilter and created a DocumentCollapser which is an interface. The DocumentCollapser has two concrete implementation the AdjacentDocumentCollapser and the NonAdjacentDocumentCollapser. Both implementation share the same abstract base class AbstractDocumentCollapser that has fields and methods that are common in both concrete implementation.\n\tRemoved deprecated Lucene methods in the PredefinedScorer.\n\tFixed a normal field collapse bug. Filter queries were handled as normal queries (were added together via a boolean query), and thus were also used for scoring.\n\tAdded more unit and integration tests, including two tests that tests facets in combination with field collapsing. These tests test the collapse before collapsing and after collapsing.\n\n\n\nThis patch only works with the Solr 1.4-dev from revision 804700 and later. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12749311",
            "date": "2009-08-30T19:27:12+0000",
            "content": "I was trying to come up with a solution to implement distributed field collapsing, but I ran into a problem that I could not solve in an efficient manner.\n\nField collapsing keeps track of the number of document collapsed per unique field value and the total count documents encountered per unique field. If the total count is greater than the specified collapse\nthreshold then the number of documents collapsed is the difference between the total count and threshold. Lets say we have two shards each shard has one document with the same field value. The collapse threshold is one, meaning that if we run the collapsing algorithm on the shard individually both documents will never be collapsed.  But when the algorithm applies to both shards, one of the documents must be collapsed however neither shared knows that its document is the one to collapse.\n\nThere are more situations described as above, but it all boils down to the fact that each shard does not have meta information about the other shards in the cluster. Sharing the intermediate collapse results between the shards is in my opinion not an option. This is because if you do that then you also need to share information about documents / fields that have a collapse count of zero. This is totally impractical for large indexes.\n\nBesides that there is also another problem with distributed field collapsing. Field collapsing only keeps the most relevant document in the result set and collapses the less relevant ones. If scoring is used to sort then field collapsing will fail to do this properly, because of the fact there is no global scoring (idf).\n\nDoes anyone have an idea on how to solve this? The first problem seems related to same kind of problem implementing global score has. "
        },
        {
            "author": "Thomas Traeger",
            "id": "comment-12749314",
            "date": "2009-08-30T19:58:59+0000",
            "content": "Hi Martin, I tested your latest patch, found no problem so far. The code is indeed better to understand now, good work.\n\nFor my current project I need to know which documents have been removed during collapsing. The current idea is to change the collapsing info and add an array with all document IDs that are removed from the result. Any suggestion on how/where to implement this? "
        },
        {
            "author": "Tarjei Huse",
            "id": "comment-12749433",
            "date": "2009-08-31T09:15:09+0000",
            "content": "Hi,\n\nI tested the latest patch (field-collapse-5.patch ) and got:\n\nHTTP Status 500 - 8452333 java.lang.ArrayIndexOutOfBoundsException: 8452333 at org.apache.lucene.search.FieldComparator$StringOrdValComparator.copy(FieldComparator.java:660) at org.apache.solr.search.NonAdjacentDocumentCollapser$DocumentComparator.compare(NonAdjacentDocumentCollapser.java:254) at org.apache.solr.search.NonAdjacentDocumentCollapser$DocumentPriorityQueue.lessThan(NonAdjacentDocumentCollapser.java:192) at org.apache.lucene.util.PriorityQueue.insertWithOverflow(PriorityQueue.java:158) at org.apache.solr.search.NonAdjacentDocumentCollapser.doCollapsing(NonAdjacentDocumentCollapser.java:99) at org.apache.solr.search.AbstractDocumentCollapser.collapse(AbstractDocumentCollapser.java:174) at org.apache.solr.handler.component.CollapseComponent.doProcess(CollapseComponent.java:98) at org.apache.solr.handler.component.CollapseComponent.process(CollapseComponent.java:67) at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:195) at \n...\n\n\nI can provide the complete stacktrace if needed. \n "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12749464",
            "date": "2009-08-31T11:09:43+0000",
            "content": "Hi Thomas, currently both collapsing algorithms do not store the the ids of the collapsed documents. \nIn order to have this functionality I think the following has to be done:\n1) In the doCollapsing(...) methods of both concrete implementations of DocumentCollapser, the collapsed documents have to be stored. Depending on what you want you can store it in one big list or store it a list per most relevant document. The most relevant document is the document that does not collapse.\n2) In the getCollapseInfo(...) method in the AbstractDocumentCollapser you then need to output these collapsed documents. If you are storing the collapsed documents in one big list then adding a new NamedList with collapsed document would be fine I guess. If you are storing the collapsed documents per document head, then I would add the collapsed document ids to existing resDoc named list. It is important that you return the Solr unique id instead of the lucene id.\n\nThis is just one approach, but what is the reason that you want this functionality? I guess what would be much easier, is to do a second query after the collapse query. In this second query you disable field collapsing (by not setting collapse.field) and you set fq=[collapse.field]=[collapse.value] for example.\n\nPotentially the number of collapsed documents can be very large and in that situation it can have a impact on performance. Therefore I think that this functionality should be disabled by default. In the same way collapseInfoDoc and collapseInfoCount are managed. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12749469",
            "date": "2009-08-31T11:14:00+0000",
            "content": "Hi Tarjei, that doesn't look good  Besides the the complete stacktrace I'm also interested in what request url to Solr resulted in this exception (with what params etc.) and what version of Solr you are currently using?  "
        },
        {
            "author": "Thomas Traeger",
            "id": "comment-12749631",
            "date": "2009-08-31T21:27:35+0000",
            "content": "I use collapsing in an online store and need to do a quite complex price calculation for every collapse group based on the products behind that group. I also thought about doing a second query, but that is not an option as I would have to do that for every group (i have up to 100 groups per request). So doing the calculation outside the scope of solr but retrieving the necessary data from solr seems to be the best approach for me. I agree that this functionality should be disabled by default.\n\nThanks for the pointer, I will have a look at it... "
        },
        {
            "author": "Darrell Silver",
            "id": "comment-12749645",
            "date": "2009-08-31T21:56:33+0000",
            "content": "Hi there, Martijn & Thomas,\n\nWe're using FieldCollapse exactly in this way.  In order to retrieve the collapsed results we do subqueries over each the results returned from the (outer) collapsing query, as Martijn suggests.  It would be a fantastic option if the documents in the collapse could be returned.  Knowing how many there are would be a big improvement as well (maybe this is possible and I don't know how?).\n\nRight now, in order to manage the load, we're calling the subquery only on the results in the page in the user's view.  Because this is all happening in a web environment, we also selectively choose to make some requests on that page while we're generating the search results page, and make the others via ajax from the browser, which gives the user a much faster response.\n\nThanks,\n\nD "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12749959",
            "date": "2009-09-01T16:05:44+0000",
            "content": "Hi Thomas, I agree that in your situation this feature is very handy. Assuming that you want to return the whole document (with all fields) and you have groups of reasonable sizes then this increases your response time dramatically.  What I think would be a better approach is to only return the fields you want to use for your calculation. Lets say an average price per group. So instead of returning 10 fields per group (let say 7000 documents) you will only return one and that will save you a lot response time. \nWhat do you think about this approach?\n\nI also find the Ajax response solution, that Darrell describes is a good way to go.  "
        },
        {
            "author": "Thomas Traeger",
            "id": "comment-12750147",
            "date": "2009-09-01T22:25:59+0000",
            "content": "yes, returning only one field would perfectly fit my needs, but Darrell seems to need more or even the complete document. So I think we need a collapse parameter that defines the field(s) of the removed documents that have to be included in the response. The Ajax approach is quite interesting but unfortunatly does not fit our needs in this case.\n\nDarrell, the counts are already inluded in the repsonse by default, look for \"collapse_count\". "
        },
        {
            "author": "Darrell Silver",
            "id": "comment-12750254",
            "date": "2009-09-02T03:20:59+0000",
            "content": "Ha, so it is!  Thanks for the note; I'd totally missed that.\n\nReturning only select fields of the collapsed documents would be a good option for us.  Also, In our subquery of the collapsed documents we're finding the first and last result (they're time sorted so this makes sense).  I guess this is similar to Thomas' average problem, but for us it's not necessary to iterate over the entire subquery results. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12750582",
            "date": "2009-09-02T18:17:15+0000",
            "content": "Yes, specifying which collapse fields to return is a good idea. Just like the fl parameter for a normal request. \nI was thinking about how to fit this new feature into the current patch and I thought that it might be a good idea to revise the current field collapse result format. So that the results of this feature can fit nicely into the response. \n\nCurrently the collapse response is like this:\n\n<lst name=\"collapse_counts\">\n        <str name=\"field\">venue</str>\n        <lst name=\"doc\">\n            <int name=\"233238\">1</int>\n        </lst>\n        <lst name=\"count\">\n            <int name=\"melkweg\">1</int>\n        </lst>\n</lst>\n\n\n\nI think a response format like the following would be more ....\n\n<lst name=\"collapse_counts\">\n        <str name=\"field\">venue</str>\n        <lst name=\"results\">\n            <lst name=\"233238\">\n                 <str name=\"fieldValue\">melkweg</str>\n                 <int name=\"collapseCount\">2</int>\n                 <lst name=\"collapsedValues\">\n                     <str name=\"price\">10.99, \"1.999,99\"</str>\n                     <str name=\"name\">adapter, laptop</str>\n                 </lst>\n            </lst>\n        </lst>\n</lst>\n\n\nAs you can see the data is more banded together and therefore easier to parse. The collapsedValues can have one or more fields, each containing collapsed field values in a comma separated format. The collapseValues element will off course only be added when the client specifies the collapsed fields in the request.\nWhat do you think about this new result format?  "
        },
        {
            "author": "Thomas Traeger",
            "id": "comment-12750658",
            "date": "2009-09-02T20:49:43+0000",
            "content": "Hi Martijn,\n\ni also thought about changing the reponse format and introducing two new parameters \"collapse.response\" and \"collapse.response.fl\".\n\nWhat do you think of these values for \"collapse.response\":\n\n\"counts\": the default and current behavior, maybe even current response format to provide backward compatibility\n\"docs\": returns the counts and the collapsed docs inside the collapse response (essentialy instead of removing the doc from the result just move it from the result to the collapse response). The parameter \"collapse.response.fl\" can be used to specify the field(s) to be returned in the collapse response.\n\nSo starting with your proposal the new collapse reponse format might look like this:\n\n\n<lst name=\"collapse_counts\">\n    <str name=\"field\">venue</str>\n    <lst name=\"results\">\n        <lst name=\"233238\">\n            <str name=\"fieldValue\">melkweg</str>\n            <int name=\"collapseCount\">2</int>\n             <lst name=\"collapsedDocs\">\n                <doc>\n                    <str name=\"id\">233239</str>\n                    <str name=\"name\">Foo Bar</str>\n                    ...\n                </doc>\n                <doc>\n                    <str name=\"id\">233240</str>\n                    <str name=\"name\">Foo Bar 2</str>\n                    ...\n                </doc>\n            </lst>\n        </lst>\n    </lst>\n</lst>\n\n\n\nI think just moving the collapsed docs into the collapse response when desired provides us the necessary flexibility and is hopefully easy to implement. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12751181",
            "date": "2009-09-03T21:51:58+0000",
            "content": "Hi Thomas,\n\nComparing my format proposal with yours, the difference is how I output the collapsed documents. I chose to add all collapsed values in an element per field, because that would make it more compact and thus easier to transmit on the wire (certainly if the number of collapsed documents to return is large). This approach is not standard in Solr and your result structure is more common. I think that most of time is properly spent at reading the collapsed field values from the index anyway (i/o), therefore I think that your result structure is right now properly the best way to go.\n\nI think that supporting the 'old' format is not that good of an idea, because this only increases complexity in the code. Also field collapsing is just a patch (although it is around for while) and is not a core Solr feature. I think people using this patch (and a patch in general) should always be aware that everything in a patch is subject to change. I think that collapse.response should be named something like collapse.includeCollapsedDocs when this is specified it includes the collapsed documents. The collapse.includeCollapsedDocs.fl would then only include the specified fields in the collapsed documents. So specifying _collapse.includeCollapsedDocs=true would result into the following result:\n\n<lst name=\"collapse_counts\">\n    <str name=\"field\">venue</str>\n    <lst name=\"results\">\n        <lst name=\"233238\">\n            <str name=\"fieldValue\">melkweg</str>\n            <int name=\"collapseCount\">2</int>\n             <lst name=\"collapsedDocs\">\n                <doc>\n                    <str name=\"id\">233239</str>\n                    <str name=\"name\">Foo Bar</str>\n                    ...\n                </doc>\n                <doc>\n                    <str name=\"id\">233240</str>\n                    <str name=\"name\">Foo Bar 2</str>\n                    ...\n                </doc>\n            </lst>\n        </lst>\n    </lst>\n</lst>\n\n\nNot specifying the collapse.includeCollaspedDocs would result into the following response output:\n\n<lst name=\"collapse_counts\">\n    <str name=\"field\">venue</str>\n    <lst name=\"results\">\n        <lst name=\"233238\">\n            <str name=\"fieldValue\">melkweg</str>\n            <int name=\"collapseCount\">2</int>\n        </lst>\n    </lst>\n</lst>\n\n\nThis will be the default and only response format.\nAnd when for example collapse.info.doc=false is specified then the following result will be returned:\n\n<lst name=\"collapse_counts\">\n    <str name=\"field\">venue</str>\n    <lst name=\"results\"> \n        <lst name=\"melkweg\"> <!-- we can not use the head document id any more, so we use the field value --> \n            <int name=\"collapseCount\">2</int>\n        </lst>\n    </lst>\n</lst>\n\n\nWhen collapse.info.count=false is specified this would just remove the fieldValue from the response. I do not know if these parameters are actually set to false by many people, but it is something to keep in mind. I also recently added support for field collapsing to solrj in the patch, obviously this has to be updated to the latest response format.\n\nIn general it must be made clear to the Solr user that this feature is handy, but it can dramatically influence the performance in a negative way. This is because the response can contain a lot of documents and each field value has to be read from the index, which results in a lot of i/o activity on the Solr side. Just because of the fact that a lot of data is returned in the response; simply viewing the response in the browser can become quite a challenge.\n\nBut more important do you think that these changes are acceptable (response format / request parameters)? "
        },
        {
            "author": "Abdul Chaudhry",
            "id": "comment-12751243",
            "date": "2009-09-04T00:27:55+0000",
            "content": "I have some ideas for performance improvements.\n\nI noticed that the code fetches the field cache twice, once for the collapse and then for the response object, assuming you asked for the info count in the response.\n\nThat seems expensive, especially for real-time content.\n\nI think its better to use FieldCache.StringIndex instead of returning a large string array and keep it around for the collapse and the response object.\n\nI changed the code so that I keep the cache around like so\n\n  /**\n\n\tKeep the field cached for the collapsed fields for the response object as well\n   */\n  private FieldCache.StringIndex collapseIndex;\n\n\n\n\nTo get the index use something like this instead of getting the string array for all docs\n\ncollapseIndex = FieldCache.DEFAULT.getStringIndex(searcher.getReader(), collapseField)\n\nwhen collapsing , you can get the current value using something like this and remove the code passing the array\n\n      int currentId = i.nextDoc();\n      String currentValue = collapseIndex.lookup[collapseIndex.order[currentId]];\n\nwhen building the response for the info count, you can reference the same cache like so:-\n\n          if (collapseInfoCount) \n{\n            resCount.add(collapseFieldType.indexedToReadable(\n              collapseIndex.lookup[collapseIndex.order[id]]), count);\n          }\n\nI also added timing for the cache access as it could be slow if you are doing a lot of updates\n\nI have added code for displaying selected fields for the duplicates but its difficult to submit . I hope this gets committed as its hard to sumbit  a patch as its not in svn and I cannot submit a patch to a patch to a patch .. you get the idea.\n "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12751358",
            "date": "2009-09-04T08:31:37+0000",
            "content": "Hi Abdul, nice improvements. It makes absolutely sense to keep the field values around during the collapsing as a StringIndex. From what I understand the StringIndex does not have duplicate string values, whereas the plain string array has. This will lower the memory footprint. I will add these improvements to the next patch. Thanks for pointing this out!  "
        },
        {
            "author": "Abdul Chaudhry",
            "id": "comment-12751615",
            "date": "2009-09-04T21:13:17+0000",
            "content": "If this helps you fix your unit tests. I fixed the unit tests by changing the CollapseFilter constructor that's used for testing to take a StringIndex like so :-\n\n\n\tCollapseFilter(int collapseMaxDocs, int collapseTreshold) {\n+  CollapseFilter(int collapseMaxDocs, int collapseTreshold, FieldCache.StringIndex index) {\n+    this.collapseIndex = index;\n\n\n\nand then I changed the unit test cases to move values into a StringIndex in CollapseFilterTest like so:-\n\n   public void testNormalCollapse_collapseThresholdOne() {\n\n\tcollapseFilter = new CollapseFilter(Integer.MAX_VALUE, 1);\n+    String[] values = new String[]\n{\"a\", \"b\", \"c\"}\n;\n+    int[] order = new int[]\n{0, 1, 0, 2, 1, 0, 1}\n;\n+    FieldCache.StringIndex index = new FieldCache.StringIndex(order, values);\n+    int[] docIds = new int[]\n{1, 2, 0, 3, 4, 5, 6}\n;\n+\n+    collapseFilter = new CollapseFilter(Integer.MAX_VALUE, 1, index);\n\n\n\n\n\tString[] values = new String[]\n{\"a\", \"b\", \"a\", \"c\", \"b\", \"a\", \"b\"}\n;\n\n\n "
        },
        {
            "author": "Paul Nelson",
            "id": "comment-12753335",
            "date": "2009-09-10T00:06:10+0000",
            "content": "Hey All:  Just upgraded to 1.4 to get the new patch (many thanks, Martijn). The new algorithm appears to be sensitive to the size and complexity of the query (rather than simply the count of documents) - should this be the case? Unfortunately, we have rather large and complex queries with dozens of terms and several phrases, and while these queries are <0.5sec without collapsing, they are 3-4sec with collapsing. Meanwhile, collapse using *:* or other simple queries come back in <0.5sec - so it appears to be primarily a query-complexity issue.\n\nI'm wondering if the filter cache (or some other cache) might be able to help with this situation? "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12753893",
            "date": "2009-09-10T23:24:42+0000",
            "content": "I have updated the field collapse patch with the following:\n1. Added the return collapse documents feature. When the parameter collapse.includeCollapsedDocs with value true is specified then the collapsed documents will returned per distinct field value. When this feature is enabled a collapsedDocs element is added to the field collapse response part. It looks like this:\n\n<lst name=\"collapsedDocs\">\n  <result name=\"Amsterdam\" numFound=\"2\" start=\"0\">\n\t<doc>\n\t <str name=\"id\">262701</str>\n\t <str name=\"title\">Bitterzoet, 100% Halal, Appletree Records &amp; Deux d'Amsterdam presents</str>\n\t</doc>\n\t<doc>\n\t <str name=\"id\">327511</str>\n\t <str name=\"title\">Salsa Danscaf\u00e9</str>\n\t</doc>\n  </result>\n </lst>\n\n\nIt is also possible to return only specific fields with the collapse.includeCollapsedDocs.fl parameter. It expects fieldnames delimited by comma, just like the normal fl parameter. \n\nThese feature can dramatically impact the performance, because a group can potently contain many documents which all have to retrieved from the index and transported over the wire. So it is certainly wise to use it in combination with the fl parameter. \n2. Added Solrj support for collapsed documents feature. \n3. Added the performance improvements that Abdul suggested.\n4. The debug information is now not returned by default. When the parameter collapse.debug with value true is specified, then the debug information is returned.\n5. When field collapsing is done on a field that is multivalued or tokenized then an exception is thrown. I have chosen to do this because collapsing on such fields lead to unexpected results. For example when a field is tokenized only the last token of the field can be retrieved from the fieldcache (the fieldcache is used for retrieving the fields from the index in a cached manner for grouping documents into groups of distinct field values). This results in collapsing only on the last token of a field value instead of the complete field value. Multivalued fields have similar behaviour, plus for multivalued fields the Lucene FieldCache throws an exception when there are more tokens for a field than documents. Personally I think that throwing an exception is better then have unexpected results, at least it is clear that something field collapse related is wrong.\n6. When doing a normal field collapse and not sorting on score the Solr caching mechanism is used. Unfortunately this was previously not the case.\n\n@Paul\nWhen doing non adjacent collapsing (aka normal collapsing) the Solr caches are not being used. The current patch uses the Solr caches when doing a search without scoring, but still the most common case is of course field collapsing and sorting on score. This is because the non adjacent field collapse algorithm requires the score of all results, which is collected with a Lucene collector. The search method on the SolrIndexSearcher that specifies a collector, does not have caching capabilities. In the next patch I will fix this problem, so that normal field collapse search uses the Solr caches as they should. The adjacent collapsing algorithm does use the solr caches, but the algorithm is much slower than non adjacent collapsing. "
        },
        {
            "author": "Paul Nelson",
            "id": "comment-12754182",
            "date": "2009-09-11T15:55:01+0000",
            "content": "Thanks Martijn!\n\nAlso, while I was doing testing on collapse, I've noticed some threading issues as well. I think they are primarily centered around the collapseRequest field.\n\nSpecifically, when I run two collapse queries at the same time, I get the following exception:\n\n\njava.lang.IllegalStateException: Invoke the collapse method before invoking getCollapseInfo method\n        at org.apache.solr.search.AbstractDocumentCollapser.getCollapseInfo(AbstractDocumentCollapser.java:183)\n        at org.apache.solr.handler.component.CollapseComponent.doProcess(CollapseComponent.java:115)\n        at org.apache.solr.handler.component.CollapseComponent.process(CollapseComponent.java:67)\n        at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:195)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1299)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:338)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:241)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:849)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:583)\n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:454)\n        at java.lang.Thread.run(Thread.java:619)\n\n\n\nAnd when I run a second (non-collapsing) query at the same time I run the collapse query I get this exception:\n\n\njava.lang.NullPointerException\n        at org.apache.solr.handler.component.CollapseComponent.doProcess(CollapseComponent.java:109)\n        at org.apache.solr.handler.component.CollapseComponent.process(CollapseComponent.java:67)\n        at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:195)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1299)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:338)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:241)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:849)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:583)\n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:454)\n        at java.lang.Thread.run(Thread.java:619)\n\n\n\nThese errors occurred with the 2009-08-24 patch, but (upon brief inspection) it looks like the same situation would occur with the latest patch.\n\nIf I get the chance, I'll try and debug further. "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12754212",
            "date": "2009-09-11T16:54:56+0000",
            "content": "Hey Martijn,\nHave you made any progress on making field collapsing distributed?\nOleg "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12754508",
            "date": "2009-09-12T11:22:01+0000",
            "content": "Hi Paul, thanks for pointing this out. I also tried to hammer my Solr instance and I got the same exceptions, which is not good. I have attached a patch that fixes these exceptions. The problem was indeed centred around the collapseRequest field and I have fixed this by using a ThreadLocal that holds the CollapseRequest instance. Because of this the reference to the CollapseRequest is not shared across the search requests and thus a new thread cannot interfere with a collapse request that is still being used by another thread. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12754511",
            "date": "2009-09-12T11:41:35+0000",
            "content": "Hi Oleg, no I have not made any progress. I'm still not clear how to solve it in an efficient manner as I have written in my previous comment:\n\n\nI was trying to come up with a solution to implement distributed field collapsing, but I ran into a problem that I could not solve in an efficient manner.\n\nField collapsing keeps track of the number of document collapsed per unique field value and the total count documents encountered per unique field. If the total count is greater than the specified collapse\nthreshold then the number of documents collapsed is the difference between the total count and threshold. Lets say we have two shards each shard has one document with the same field value. The collapse threshold is one, meaning that if we run the collapsing algorithm on the shard individually both documents will never be collapsed. But when the algorithm applies to both shards, one of the documents must be collapsed however neither shared knows that its document is the one to collapse.\n\nThere are more situations described as above, but it all boils down to the fact that each shard does not have meta information about the other shards in the cluster. Sharing the intermediate collapse results between the shards is in my opinion not an option. This is because if you do that then you also need to share information about documents / fields that have a collapse count of zero. This is totally impractical for large indexes.\n\nBesides that there is also another problem with distributed field collapsing. Field collapsing only keeps the most relevant document in the result set and collapses the less relevant ones. If scoring is used to sort then field collapsing will fail to do this properly, because of the fact there is no global scoring (idf).\n\nDoes anyone have an idea on how to solve this? The first problem seems related to same kind of problem implementing global score has.\n\nI recently read something about Katta and . Katta facilitates distributed search and has for support global scoring. I'm not completely sure how it is implemented in Katta, but maybe with Katta it is relative efficient to share the intermediate collapse results between shards. "
        },
        {
            "author": "Uri Boness",
            "id": "comment-12754523",
            "date": "2009-09-12T12:45:17+0000",
            "content": "Martijn, I think a more appropriate way to fix the threading issue is to bind the collapseRequest to the request context and drop the class field all together. So:\n\n\npublic void prepare(ResponseBuilder rb) throws IOException {\n    super.prepare(rb);\n    rb.req.getContext().put(\"collapseRequest\", resolveCollapseRequest(rb));\n}\n\n\n\nand \n\n\npublic void process(ResponseBuilder rb) throws IOException {\n    CollapseRequest collapseRequest = rb.req.getContext().remove(\"collapseRequest\");\n    if (collapseRequest == null) {\n      super.process(rb);\n      return;\n    }\n    doProcess(rb, collapseRequest);\n}\n\n "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12754544",
            "date": "2009-09-12T15:31:18+0000",
            "content": "You are right Uri, using the requestContext is much more appropriate than using a ThreadLocale. I have updated the patch with this change. "
        },
        {
            "author": "Thomas Traeger",
            "id": "comment-12754708",
            "date": "2009-09-13T15:04:45+0000",
            "content": "Hi Martijn, I made some tests with the new collapsedDocs feature. Looks very good, but in some cases it seems to return wrong collapsed docs. There seems to be a connection between sorting and this problem. Here an example using the example docs collapsed on field inStock and sorting by popularity:\n\nhttp://localhost:8983/solr/select/?q=*:*&sort=popularity%20asc&fl=id&collapse.field=inStock&collapse.includeCollapsedDocs=true&collapse.includeCollapsedDocs.fl=id\n\nFor inStock:T document id:VDBDB1A16 remains in the result after collapsing. But this document is also returned in the collapsedDocs response and in addition document id:SP2514N is missing there. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12754747",
            "date": "2009-09-13T19:59:50+0000",
            "content": "Hi Thomas. I tried to reproduce something similar here, but I did run into the problems you described. Can you tell me what the fieldtypes are for your sort field and collapse field? "
        },
        {
            "author": "Thomas Traeger",
            "id": "comment-12754764",
            "date": "2009-09-13T21:17:05+0000",
            "content": "I found the problem with my real world data and reproduced it with the solr example schema and data. In the solr example popularity is of type \"int\" and inStock is \"boolean\". I made some more tests and could reproduce other fieldtypes too, here some examples using the field manu_exact (string):\n\nhttp://localhost:8983/solr/select/?q=*:*&sort=manu_exact%20asc&fl=id&collapse.field=inStock&collapse.includeCollapsedDocs=true\n-> as in the previous example document id:VDBDB1A16 is in result and collapsedDocs\n\nhttp://localhost:8983/solr/select/?q=*:*&sort=manu_exact%20desc&fl=id&collapse.field=inStock&collapse.includeCollapsedDocs=true\n-> document id:VA902B is in result and collapsedDocs\n\nhttp://localhost:8983/solr/select/?q=*:*&sort=popularity%20desc&fl=id&collapse.field=manu_exact&collapse.includeCollapsedDocs=true\n-> document id:VS1GB400C3 is in result and collapsedDocs\n "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12755175",
            "date": "2009-09-14T20:15:06+0000",
            "content": "Hi Thomas, I  have fixed the problem and updated the patch. I was able to reproduce the bug on the Solr example dataset. The problem was not limited to field collapsing with sorting on a field alone. The problem was located in the NonAdjactentFieldCollapser in the doCollapse(...) method in this specific part:\n\n      // dropoutId has a value smaller than the smallest value in the queue and therefore it was removed from the queue\n      collapseDoc.priorityQueue.insertWithOverflow(currentId);\n\n      // check if we have reached the collapse threshold, if so start counting collapsed documents\n      if (++collapseDoc.totalCount > collapseTreshold) {\n        collapseDoc.collapsedDocuments++;\n        if (dropOutId != null) {\n          addCollapsedDoc(currentId, currentValue);\n        }\n      }\n\n\nLets say that that the currentId has the most relevent field value and the collapseThreshold is met. When the currentId is added to the queue it stays there and another document id will be dropped out. In this situation a document that is the most relevant field value is added to the collapsed documents and it stays in the queue and therefore it will also be added to the normal results. \n\nI changed it to this.\n\n      // dropoutId has a value smaller than the smallest value in the queue and therefore it was removed from the queue\n      Integer dropOutId = (Integer) collapseDoc.priorityQueue.insertWithOverflow(currentId);\n\n      // check if we have reached the collapse threshold, if so start counting collapsed documents\n      if (++collapseDoc.totalCount > collapseTreshold) {\n        collapseDoc.collapsedDocuments++;\n        if (dropOutId != null) {\n          addCollapsedDoc(dropOutId, currentValue);\n        }\n      }\n\n\nNow only a document that will never and up in the final results is added to the collapsed documents (and not the current document that might be more relevant then other documents in the priority queue). The above code change fixes the bug in my test setups, can you also confirm that this fixes the issue on your side? "
        },
        {
            "author": "Thomas Traeger",
            "id": "comment-12755212",
            "date": "2009-09-14T21:39:51+0000",
            "content": "Hi Martijn, this fixed the problem, thanks  "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12759909",
            "date": "2009-09-26T15:32:29+0000",
            "content": "I have created a new patch that has the following changes:\n1) Non adajacent collasping with sorting on score also uses the Solr caches now. So now every field collapse searches are using the Solr caches properly. This was not the case in my previous versions of the patch. This improvement will make field collapsing perform better and reduce the query time for regular searches. The downside was, that in order to make this work I had to modify some methods in the SolrIndexSearcher. \n\nWhen sorting on score the non adjacent collapsing algorithm needs the score per document. The score is collected in a Lucene collector. The previous version of the patch uses the searcher.search(Query, Filter, Collector) method to collect the documents (as a DocSet) and scores, but by using this method the Solr caches were ignored.\n\nThe methods that return a DocSet in the SolrIndexSearcher do not offer the ability the specify your own collector. I changed that so you can specify your own collector and still benefit from the Solr caches. I did this in a non intrusive manner, so that nothing changes for existing code that uses the normal versions of these methods. \n\n   public DocSet getDocSet(Query query) throws IOException {\n    DocSetCollector collector = new DocSetCollector(maxDoc()>>6, maxDoc());\n    return getDocSet(query, collector);\n   }\n\n   public DocSet getDocSet(Query query, DocSetAwareCollector collector) throws IOException {\n    ....\n   }\n\n  DocSet getPositiveDocSet(Query q) throws IOException {\n    DocSetCollector collector = new DocSetCollector(maxDoc()>>6, maxDoc());\n    return getPositiveDocSet(q, collector);\n   }\n\n  DocSet getPositiveDocSet(Query q, DocSetAwareCollector collector) throws IOException {\n    .....\n   }\n\n  public DocSet getDocSet(List<Query> queries) throws IOException {\n    DocSetCollector collector = new DocSetCollector(maxDoc()>>6, maxDoc());\n    return getDocSet(queries, collector);\n   }\n\n  public DocSet getDocSet(List<Query> queries, DocSetAwareCollector collector) throws IOException {\n   .......\n   }\n\n  protected DocSet getDocSetNC(Query query, DocSet filter) throws IOException {\n    DocSetCollector collector = new DocSetCollector(maxDoc()>>6, maxDoc());\n    return getDocSetNC(query,  filter, collector);\n   }\n\n  protected DocSet getDocSetNC(Query query, DocSet filter, DocSetAwareCollector collector) throws IOException {\n   .........\n   }\n\n\nI also made a DocSetAwareCollector that both DocSetCollector and DocSetScoreCollector implement.\n2) The collapse.includeCollapsedDocs parameters has been removed. In order to include the collapsed documents the parameter collapse.includeCollapsedDocs.fl must be specified. collapse.includeCollapsedDocs.fl=* will include all fields of the collapsed documents and collapse.includeCollapsedDocs.fl=id,name will only include the id and name field of the collapsed documents. "
        },
        {
            "author": "Aytek Ekici",
            "id": "comment-12765076",
            "date": "2009-10-13T13:42:19+0000",
            "content": "Hi all,\nJust applied \"field-collapse-5.patch\" and i guess there are problems with filter queries.\n\nHere it is:\n\n1- select?q=:&fq=lat:[37.2 TO 39.8]\nnumFound: 6284\n\n2- select?q=:&fq=lng:[24.5 TO 29.9]\nnumFound: 16912\n\n3- select?q=:&fq=lat:[37.2 TO 39.8]&fq=lng:[24.5 TO 29.9]\nnumFound: 19419\n\n4- When using \"q\" instead of \"fq\" which is: \nselect?q=lat:[37.2 TO 39.8] AND lng:[24.5 TO 29.9]\nnumFound: 3777 (which is the only correct number)\n\nThe thing is, as i understand, instead of applying \"AND\" for each filter query it applies \"OR\". Checked select?q=lat:[37.2 TO 39.8] OR lng:[24.5 TO 29.9]\nnumFound: 19419 (same as 3rd one)\n\nAny idea how to fix this?\nThx. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12765196",
            "date": "2009-10-13T19:29:43+0000",
            "content": "Hi Aytek,\n\nHow I understand filter queries work is that each separate filter query produces a result set and each of this result set is intersected together. Which means that it works as you want it.\nI'm not sure but I think that this issue is not related to the patch. I have tried to reproduce this situation (on a different data set), but it behaved as it should. With the patch and without.\nHave you tried fq=lat:[37.2 TO 39.8] AND lng:[24.5 TO 29.9] instead of having it in two separate fqs?\n\nMartijn "
        },
        {
            "author": "An\u0131l \u00c7etin",
            "id": "comment-12765429",
            "date": "2009-10-14T06:44:52+0000",
            "content": "Hi Martijn, \n\nto clarify the problem;\n\n1) select/?q=:&fq=+lat:[37.2 TO 39.8] +lng:[24.5 TO 29.9]\n\n2) select/?q=:&fq=lat:[37.2 TO 39.8]&fq=lng:[24.5 TO 29.9]\n\nExpected result set for these queries are identical (isnt it?) but actually with patch the results becomes different. \n\nAlso without patch there is no problem. "
        },
        {
            "author": "Aytek Ekici",
            "id": "comment-12765605",
            "date": "2009-10-14T16:03:41+0000",
            "content": "Hi Martijn,\nIntersection of results sets is also a kind of \"AND\", right? Intersection result of A docset and B docset is equal to resultset of \"conA AND condB\" i think.\n\nYour suggestion \"fq=lat:[37.2 TO 39.8] AND lng:[24.5 TO 29.9]\" works. And also Anil's suggestion \"fq=+lat:[37.2 TO 39.8] +lng:[24.5 TO 29.9]\" works. \nBut they don't allow multiple selections for a facet field. I can't use excludes. It throws parsing errors. \nUsing \"AND\" between two filters in a filter query results with one item in FilterList of QueryCommand, that must be the reason not to be able to parse/support ex/tag things there i guess.\n\nI have two solr instances here one with patch and another without patch. And i just copied configurations and data from one to other. Only difference is field_collapsing patch as i can see. I'm trying to see what makes the difference in results but new in solr so it takes time to see/catch what is going on. So any help/tip would be appreciated.\n\nThanks,\nAytek "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12765760",
            "date": "2009-10-14T21:22:19+0000",
            "content": "Hi Aytek,\n\nI was able to reproduce the same situation you described earlier. When I was testing yesterday I thought I was testing on a Solr instance without the patch, but I wasn't. Anyhow I have fixed bug and I have attached a new patch. Good thing you noticed this bug it was really corrupting the search results.\n\nMartijn "
        },
        {
            "author": "Aytek Ekici",
            "id": "comment-12766018",
            "date": "2009-10-15T11:16:58+0000",
            "content": "Hi Martijn,\nThanks a lot it works.\n\nAytek "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12769878",
            "date": "2009-10-25T22:13:39+0000",
            "content": "I have attached a new patch which includes a major refactoring which makes the code more flexible and cleaner. The patch also includes a new aggregate functionality and a bug fix.\n\nAggregate function and bug fix\nThe new patch allows you to execute aggregate functions on the collapsed documents (for example sum the stock amount or calculating the minimum price of a collapsed group). Currently there are four aggregate functions available: sum(), min(), max() and avg(). To execute one or more functions the collapse.aggregate parameter has to be added to the request url. The parameter expects the following syntax: function_name(field_name)[, function_name(field_name)]. For example: collapse.aggregate=sum(stock), min(price) and might have a result like this:\n\n<lst name=\"aggregatedResults\">\n   <lst name=\"sum(stock)\">\n      <str name=\"Amsterdam\">10</str>\n      ...\n   </lst>\n   <lst name=\"min(price)\">\n      <str name=\"Amsterdam\">5.99</str>\n      ...\n   </lst>\n</lst>\n\n\n\nThe patch also fixes a bug inside the NonAdjacentDocumentCollapser that was reported on the solr-user mailing list a few days ago. An index out of bounds exception was thrown when documents were removed from an index and a field collapse search was done afterwards.  \n\nCode refactoring\nThe code refactoring includes the following things:\n\n\tThe notion of a CollapseGroup. A collapse group defines what an unique group is in the search result. For the adjacent and non adjacent document collapser this is different. For adjacent field collapsing a group is defined by its field value and the document id of the most relevant document in that group. More then one collapse group may have the same fieldvalue. For normal field collapsing (non adjacent) the group is defined just by the field value.\n\tThe notion of a CollapseCollector that receives the collapsed documents from a DocumentCollector and does something with it. For example keeps a count of how many documents were collapsed per collapse group or computes an average of a certain field like price. As you can see in the code instead of using field values or document ids a collapse group is used for identifying a collapse group.\n\n/**\n * A <code>CollapseCollector</code> is responsible for receiving collapse callbacks from the <code>DocumentCollapser</code>.\n * An implementation can choose what to do with the received callbacks and data. Whatever an implementation collects it\n * is responsible for adding its results to the response.\n *\n * Implementation of this interface don't need to be thread safe!\n */\npublic interface CollapseCollector {\n\n  /**\n   * Informs the <code>CollapseCollector</code> that a document has been collapsed under the specified collapseGroup.\n   *\n   * @param docId The id of the document that has been collasped\n   * @param collapseGroup The collapse group the docId has been collapsed under\n   * @param collapseContext The collapse context\n   */\n  void documentCollapsed(int docId, CollapseGroup collapseGroup, CollapseContext collapseContext);\n\n  /**\n   * Informs the <code>CollapseCollector</code> about the document head.\n   * The document head is the most relevant id for the specified collapseGroup.\n   *\n   * @param docHeadId The identifier of the document head\n   * @param collapseGroup The collapse group of the document head\n   * @param collapseContext The collapse context\n   */\n  void documentHead(int docHeadId, CollapseGroup collapseGroup, CollapseContext collapseContext);\n\n  /**\n   * Adds the <code>CollapseCollector</code> implementation specific result data to the result.\n   *\n   * @param result The response result \n   * @param docs The documents to be added to the response\n   * @param collapseContext The collapse context\n   */\n  void getResult(NamedList result, DocList docs, CollapseContext collapseContext);\n\n}\n\n\nThere is also a CollapseContext that allows you store data that can be shared between CollapseCollectors. \n\tA CollapseCollectorFactory is responsible for creating a CollepseCollector. It does this based on the SolrQueryRequest. All the logic for when to enable a certain CollapseCollector must be placed in the factory.\n\n/**\n * A concrete <code>CollapseCollectorFactory</code> implementation is responsible for creating {@link CollapseCollector}\n * instances based on the {@link SolrQueryRequest}.\n */\npublic interface CollapseCollectorFactory {\n\n  /**\n   * Creates an instance of a CollapseCollector specified by the concrete subclass.\n   * The concrete subclass decides based on the specified request if an new instance has to be created and\n   * can return <code>null</code> for that matter.\n   * \n   * @param request The specified request\n   * @return an instance of a CollapseCollector or <code>null</code>\n   */\n  CollapseCollector createCollapseCollector(SolrQueryRequest request);\n\n}\n\n\nCurrently there are four CollapseCollectorFactories implementations:\n\n\n\tDocumentGroupCountCollapseCollectorFactory creates CollapseCollectors that collect the collapse counts per document group and return the counts in the response per collapsed group most relevant document id.\n\tFieldValueCountCollapseCollectorFactory creates CollapseCollectors that collect the collapse count per collapsed group and return the counts in the response per collepsed group field value.\n\tDocumentFieldsCollapseCollectorFactory creates CollapseCollectors that collect predefined fieldvalues from collapsed documents.\n\tAggregateCollapseCollectorFactory creates CollapseCollectors that create aggregate statistics based on the collapsed documents.\nCollapseCollectorFactories are configured in the solrconfig.xml and by default all implementations in the patch are configured. The following configuration is sufficient \n\n<searchComponent name=\"collapse\" class=\"org.apache.solr.handler.component.CollapseComponent\" />\n\n\nThe following configurations configures the same CollapseCollectorFactories as the previous configuration:\n\n<searchComponent name=\"collapse\" class=\"org.apache.solr.handler.component.CollapseComponent\">\n    <arr name=\"collapseCollectorFactories\">\n        <str>groupDocumentsCounts</str>\n        <str>groupFieldValue</str>\n        <str>groupDocumentsFields</str>\n        <str>groupAggregatedData</str>\n    </arr>\n  </searchComponent>\n\n  <fieldCollapsing>\n    <collapseCollectorFactory name=\"groupDocumentsCounts\" \nclass=\"solr.fieldcollapse.collector.DocumentGroupCountCollapseCollectorFactory\" />\n\n    <collapseCollectorFactory name=\"groupFieldValue\" class=\"solr.fieldcollapse.collector.FieldValueCountCollapseCollectorFactory\" />\n\n    <collapseCollectorFactory name=\"groupDocumentsFields\" \n class=\"solr.fieldcollapse.collector.DocumentFieldsCollapseCollectorFactory\" />\n\n    <collapseCollectorFactory name=\"groupAggregatedData\"\n class=\"org.apache.solr.search.fieldcollapse.collector.AggregateCollapseCollectorFactory\">\n        <lst name=\"aggregateFunctions\">\n            <str name=\"sum\">org.apache.solr.search.fieldcollapse.collector.aggregate.SumFunction</str>\n            <str name=\"avg\">org.apache.solr.search.fieldcollapse.collector.aggregate.AverageFunction</str>\n            <str name=\"min\">org.apache.solr.search.fieldcollapse.collector.aggregate.MinFunction</str>\n            <str name=\"max\">org.apache.solr.search.fieldcollapse.collector.aggregate.MaxFunction</str>\n        </lst>\n    </collapseCollectorFactory>\n  </fieldCollapsing>\n\n\nThe CollapseCollectorFactories configured can be shared among different CollapseComponents. Most users do not have to do this, but when you creating your own implementations or someone else's then you have to do this in order to configure the CollapseCollectorFactory implementation. The order in collapseCollectorFactories does matter. CollapseCollectors may share data via the CollapseContext for that reason the order is depend. The CollapseCollectorFactories in the patch do not share data, but other implementations may.\n\n\n\nThe new patch contains a lot of changes, but I personally think that the patch is really an improvement especially the introduction of the CollapseCollectors that allows a lot of flexibility. Btw any feedback or questions are welcome. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12770358",
            "date": "2009-10-27T02:39:43+0000",
            "content": "This looks like a really nice rework! This JIRA has been a marathon (2.5 years!), but maybe the last miles are here.\n\nSince this JIRA has so many comments, it is hard to navigate. Maybe it is a good time to close it and start a new active JIRA for the field collapsing project.  "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12770555",
            "date": "2009-10-27T16:28:16+0000",
            "content": "I have updated the patch that fixes the bug that was reported yesterday on the solr-user mailing list:\n\nfound another exception, i cant find specific steps to reproduce\nbesides starting with an unfiltered result and then given an int field\nwith values (1,2,3) filtering by 3 triggers it sometimes, this is in\nan index with very frequent updates and deletes\n\n\n--joe\n\n\njava.lang.NullPointerException\n       at org.apache.solr.search.fieldcollapse.collector.FieldValueCountCollapseCollectorFactory\n$FieldValueCountCollapseCollector.getResult(FieldValueCountCollapseCollectorFactory.java:84)\n       at org.apache.solr.search.fieldcollapse.AbstractDocumentCollapser.getCollapseInfo(AbstractDocumentCollapser.java:191)\n       at org.apache.solr.handler.component.CollapseComponent.doProcess(CollapseComponent.java:179)\n       at org.apache.solr.handler.component.CollapseComponent.process(CollapseComponent.java:121)\n       at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:195)\n       at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n       at org.apache.solr.core.RequestHandlers$LazyRequestHandlerWrapper.handleRequest(RequestHandlers.java:233)\n       at org.apache.solr.core.SolrCore.execute(SolrCore.java:1316)\n       at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:338)\n       at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:241)\n       at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1148)\n       at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:387)\n       at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n       at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)\n       at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)\n       at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)\n       at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n       at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n       at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n       at org.mortbay.jetty.Server.handle(Server.java:326)\n       at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)\n       at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)\n       at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:539)\n       at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n       at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n       at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)\n       at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:520) "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12771155",
            "date": "2009-10-28T22:17:00+0000",
            "content": "It certainly has be going on for a long time \nTalking about the last miles there are a few things in my mind about field collapsing:\n\n\tChange the response format. Currently if I look at the response even I get confused sometimes about the information returned. The response should more structured. Something like this:\n\n<lst name=\"collapse_counts\">\n    <str name=\"field\">venue</str>\n    <lst name=\"results\">\n        <lst name=\"233238\"> <!-- id of most relevant document of the group -->\n            <str name=\"fieldValue\">melkweg</str>\n            <int name=\"collapseCount\">2</int>\n            <!-- and other CollapseCollector specific collapse information -->\n        </lst>\n        ...\n    </lst>\n</lst>\n\n\nCurrently when doing adjacent field collapsing the collapse_counts gives results that are unusable to use. The collapse_counts use the field value as key which is not unique for adjacent collapsing as shown in the example: \n\n<lst name=\"collapse_counts\">\n <int name=\"hard\">1</int>\n <int name=\"hard\">1</int>\n <int name=\"electronics\">1</int>\n <int name=\"memory\">2</int>\n <int name=\"monitor\">1</int>\n</lst>\n\n\n\tAdd the notion of a CollapseMatcher, that decides whether document field values are equal or not and thus whether they are allowed to be collapsed. This opens the road for more exotic features like fuzzy field collapsing and collapsing on more than one field. Also this allows users of the patch to easily implement their own matching rules.\n\tDistributed field collapsing. Although I have some ideas on how to get started, from my perspective it not going to be performed. Because somehow the field collapse state has to be shared between shards in order to do proper field collapsing. This state can potentially be a lot of data depending on the specific search and corpus.\n\tAnd maybe add a collapse collector that collects statistics about most common field value per collapsed group.\n\n\n\nI think that this is somewhat the roadmap from my side for field collapsing at moment, but feel free to elaborate on this.\nBtw I have recently written a blog about field collapsing in general, that might be handy for someone who is implementing field collapsing.  "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12771598",
            "date": "2009-10-29T20:43:40+0000",
            "content": "Getting the refactoring right is important.\n\nScaling needs to be on the roadmap as well. The data created in collapsing has to be cached in some way. If I do a collapse on my 500m test index, the first one takes 110ms and the second one takes 80-90ms. Searches that walk from one result page to the next have to be fast the second time.  Field collapsing probably needs some explicit caching. This is a show-stopper for getting this committed.\n\nWhen I sort or facet the work done up front is reused in some way. In sorting there is a huge amount of work pushed to the first query and explicitly cached. Faceting seems to leave its work in the existing caches and runs much faster the second time. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12771877",
            "date": "2009-10-30T08:41:00+0000",
            "content": "I agree about the caching. When searching with fieldcollapsing for the same query more than ones, then some caching should kick in. I think that the execution of the doCollapse(...) method should be cached. In this method the field collapse logic is executed, which takes the most time of a field collapse search. "
        },
        {
            "author": "Michael Gundlach",
            "id": "comment-12775192",
            "date": "2009-11-09T23:44:57+0000",
            "content": "I've found an NPE that occurs when performing quasi-distributed field collapsing.\n\nMy company only has one use case for field collapsing: collapsing on email address.  Our index is spread across multiple cores.  We found that if we shard by email address, so that all documents with a given email address are guaranteed to appear on the same core, then we can do distributed field collapsing.\n\nWe add &collapse.field=email and &shards=core1,core2,... to a regular query.  Each core collapses on email and sends the results back to the requestor.  Since no emails appear on more than one core, we've accomplished distributed search.  We do lose the <collapse_count> section, but that's not needed for our purpose \u2013 we just need an accurate total document count, and to have no more than one document for a given email address in the results.\n\nUnfortunately, this throws an NPE when searching on a tokenized field.  Searching string fields is fine.  I don't understand exactly why the NPE appears, but I did bandaid over it by checking explicitly for nulls at the appropriate line in the code.  No more NPE.\n\nThere's a downside, which is that if we attempt to collapse on a field other than email \u2013 one which has documents appearing in multiple cores \u2013 the results are buggy: the first search returns few documents, and the number of documents actually displayed don't always match the \"numFound\" value.  Then upon refresh we get what we think is the correct numFound, and the correct list of documents.  This doesn't bother me too much, as you're guaranteed to get incorrect answers from the collapse code anyway when collapsing on a field that you didn't use as your key for sharding.\n\nIn the spirit of Yonik's law of patches, I have made two imperfect patches attempting to contribute the fix, or at least point out the error:\n\n1. I pulled trunk, applied the latest SOLR-236 patch, made my 2 line change, and created a patch file.  The resultant patch file looks very different from the latest SOLR-236 patchfile, so I assume I did something wrong.\n\n2. I pulled trunk, made my 2 line change, and created another patch file.  This file is tiny but of course is missing all of the field collapsing changes.\n\nWould you like me to post either of these patchfiles to this issue?  Or is it sufficient to just tell you that the NPE occured in QueryComponent.java on line 556? (\"rb._responseDocs.set(sdoc.positionInResponse, doc);\" where sdoc was null.)  Perhaps my use case is extraordinary enough that you're happy leaving the NPE in place and telling other users to not do what I'm doing?\n\nThanks!\nMichael "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12775350",
            "date": "2009-11-10T08:32:31+0000",
            "content": "With the current patch if you try to collapse on a field that is tokenized or multivalued an exception is thrown indicating that you cannot do that and the search is cancelled. What I guess is that when the search results are retrieved from the shards on the master a NPE is thrown because the shard result is not there. This is a limitation in itself, but it boils down to the fact how the FieldCache handles such field types (or at least how I think the FieldCache handles it).\n\nI think it is good idea to share your patch and from there we might be able to get the change in a proper manner. So others will also benefit from quasi-distributed field collapsing.\n\nAnyhow to properly implement distributed field collapsing the distributed methods have to be overriden in the collapse component, so that is where I would start. We might then also include the collapse_count in the response. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12775418",
            "date": "2009-11-10T13:56:55+0000",
            "content": "I'm using Martijn's patch from 2009-10-27. The FieldCollapseResponse#parseDocumentIdCollapseCounts assumes the unique key is a long. Is that a bug or an undocumented limitation?\n\nNice work guys! We should definitely get this into Solr 1.5 "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12775430",
            "date": "2009-11-10T14:24:24+0000",
            "content": "Hi Shalin, it was not my intention (Usually in my case I use a long as id). I'm currently refactoring the response format as described in a previous comment, so I have to change the SolrJ classes anyway. I will submit a patch shortly. "
        },
        {
            "author": "Michael Gundlach",
            "id": "comment-12775922",
            "date": "2009-11-10T16:04:56+0000",
            "content": "Martijn,\n\nI probably wasn't clear \u2013 we are sharding and collapsing on a non-tokenized \"email\" field.  We can perform distributed collapsing fine when searching on some other nontokenized field; the NPE occurs when we perform a search on a tokenized field.\n\nAnyway, I'll attach the small patch now, which just adds the null check to Solr trunk. "
        },
        {
            "author": "Michael Gundlach",
            "id": "comment-12775925",
            "date": "2009-11-10T16:12:27+0000",
            "content": "This patch (quasidistributed.additional.patch) does not apply field collapsing.\n\nApply this patch in addition to the latest field collapsing patch, to avoid an NPE when:\n\n\n\tyou are collapsing on a field F,\n\tyou are sharding into multiple cores, using the hash of field F as your sharding key, AND\n\tyou perform a distributed search on a tokenized field.\n\n\n\nNote that if you attempt to use this patch to collapse on a field F1 and shard according to a field F2, you will get buggy search behavior. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12776302",
            "date": "2009-11-11T06:07:11+0000",
            "content": "I have updated the field collapse patch and improved the response format. Check my blog for more details. "
        },
        {
            "author": "Thomas Woodard",
            "id": "comment-12777582",
            "date": "2009-11-13T17:45:40+0000",
            "content": "I'm trying to get field collapsing to work against the 1.4.0 release. I applied the latest patch, moved the file, did a clean build, and set up a config based on the example. If I run a search without collapsing everything is fine, but if it actually tries to collapse, I get the following error:\n\njava.lang.NoSuchMethodError: org.apache.solr.search.SolrIndexSearcher.getDocSet(Lorg/apache/lucene/search/Query;Lorg/apache/solr/search/DocSet;Lorg/apache/solr/search/DocSetAwareCollector;)Lorg/apache/solr/search/DocSet;\n\tat org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser.doQuery(NonAdjacentDocumentCollapser.java:60)\n\tat org.apache.solr.search.fieldcollapse.AbstractDocumentCollapser.collapse(AbstractDocumentCollapser.java:168)\n\tat org.apache.solr.handler.component.CollapseComponent.doProcess(CollapseComponent.java:160)\n\tat org.apache.solr.handler.component.CollapseComponent.process(CollapseComponent.java:121)\n\tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:195)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1316)\n\tat org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:338)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:241)\n\nThe tricky part is that the method is there in the source and I wrote a little test JSP that can find it just fine. That implies a class loader issue of some sort, but I'm not seeing it. Any help would be greatly appreciated. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12777649",
            "date": "2009-11-13T20:45:03+0000",
            "content": "Thomas, the method that cannot be found ( SolrIndexSearcher.getDocSet(...) ) is a method that is part of the patch. So if the patch was successful applied then this should not happen. \nWhen I released the latest patch I only tested against the solr trunk, but I have tried the following to verify that the patch works with 1.4.0 release:\n\n\tDowloaded 1.4.0 release from Solr site\n\tApplied the patch\n\tExecuted: ant clean dist example\n\tIn the example config (example/solr/conf/solrconfig.xml) I added the following line under the standard request handler:\n\n<searchComponent name=\"query\" class=\"org.apache.solr.handler.component.CollapseComponent\" />\n\n\tStarted the Jetty with Solr with the following command: java -jar start.jar\n\tAdded example data to Solr with the following command in the exampledocs dir: ./post.sh *.xml\n\tI Browsed to the following url: http://localhost:8983/solr/select/?q=*:*&collapse.field=inStock and saw that the result was collapsed on the inStock field.\n\n\n\nIt seems that everything is running fine. Can you tell something about how you deployed Solr on your machine? "
        },
        {
            "author": "Thomas Woodard",
            "id": "comment-12777659",
            "date": "2009-11-13T21:06:53+0000",
            "content": "I tried the build again, and you are right, it does work fine with the default search handler. I had been trying to get it working with our search handler, which is dismax. That still doesn't work. Here is the handler configuration, which works fine until collapsing is added.\n\n\n<requestHandler name=\"glsearch\" class=\"solr.SearchHandler\">\n\t<lst name=\"defaults\">\n\t\t<str name=\"defType\">dismax</str>\n\t\t<str name=\"qf\">name^3 description^2 long_description^2 search_stars^1 search_directors^1 product_id^0.1</str>\n\t\t<str name=\"tie\">0.1</str>\n\t\t<str name=\"facet\">true</str>\n\t\t<str name=\"facet.field\">stars</str>\n\t\t<str name=\"facet.field\">directors</str>\n\t\t<str name=\"facet.field\">keywords</str>\n\t\t<str name=\"facet.field\">studio</str>\n\t\t<str name=\"facet.mincount\">1</str>\n\t</lst>\n</requestHandler>\n\n\n\nEdit: The search fails even if you don't pass a collapse field. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12778147",
            "date": "2009-11-15T18:42:54+0000",
            "content": "What kind of exception is occurring if you use dismax (with and without field collapsing)? If I do a collapse search with dismax in the example setup (http://localhost:8983/solr/select/?q=power&collapse.field=inStock&qt=dismax) field collapsing appears to be working.  "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12778175",
            "date": "2009-11-15T20:55:14+0000",
            "content": "I have attached a new patch, that incorporates Micheal's quasi distributed patch so you don't have to patch twice. In addition to that the new patch also merges the collapse_count data from each individual shard response. When using this patch you will still need to make sure that all documents of one collapse group stay on one shard, otherwise your collapse result will be incorrect. The documents of a different collapse group can stay on a different shard.   "
        },
        {
            "author": "Thomas Woodard",
            "id": "comment-12778443",
            "date": "2009-11-16T18:33:20+0000",
            "content": "And this morning, without changing anything, it is working fine. I don't know what happened on Friday, but the changes I made then must have fixed it without showing up for some reason. In any case, thank you for the assistance. "
        },
        {
            "author": "German Attanasio Ruiz",
            "id": "comment-12779061",
            "date": "2009-11-17T19:19:39+0000",
            "content": "Sorting of results doesn't work properly. Next, I detail the steps I followed and the problem I faced\n\nI am using solr as a search engine for web pages, from which I use  a field named \"site\" for collapsing and sort over scord\n\nSteps\nAfter downloading the last version of solr \"solr-2009-11-15\" and applying the patch \"field-collapse-5.patch 2009-11-15 08:55 PM Martijn van Groningen 239 kB\"\n\nSTEP 1 - I make a search using fieldcollapsing and the result is correct, the number with greatest scord is 0.477\nSTEP 2 - I make the same search and the fieldcollapsing throws other result with scord 0.17, the (correct) result of step 1 does not appear again\n\nPossible problem\nStep 1 stores the document in the cache for future searches\nat Step 2 the search is don over the cache and does not find the previously stored document\n\nPossible solution\nI believe that the problem is in the storing of the document in the cache since if we make step 2 again we have the same result and the document with scord of 0.17 is not removed from the results, the only result removed is the document with scord 0.477\n\nConclusion\nDocuments are not sorted properly when using \"fieldcollapsing + solrcache\", that is when documents stored in solr cache are required "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12779418",
            "date": "2009-11-18T11:38:25+0000",
            "content": "I can confirm this bug. I will attach a new patch that fixes this issue shortly. Thanks for noticing.  "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12781232",
            "date": "2009-11-22T22:00:04+0000",
            "content": "The reason why the search results after the first search were incorrect was, because the scores were not preserved in the cache. The result of that was that the collapsing algorithm could not properly group the documents into the collapse groups (the most relevant document per document group could not be determined properly), because there was no score information when retrieving the documents from cache (as DocSet in SolrIndexSearcher) . \n\nI made sure that in the attached patch the score is also saved in the cache, so the collapsing algorithm can do its work properly when the documents are retrieved from the cache. Because the scores are now stored with the cached documents the actual size of the filterCache in memory will increase.  "
        },
        {
            "author": "German Attanasio Ruiz",
            "id": "comment-12781279",
            "date": "2009-11-23T02:08:11+0000",
            "content": "Tomorrow I'm going to try the patch , the next time I hope to help and not only communicate the problem "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12783484",
            "date": "2009-11-29T21:55:22+0000",
            "content": "I have attached a new patch that has the following changes:\n\n\tAdded caching for the field collapse functionality. Check the solr wiki for how to configure field-collapsing with caching.\n\tRemoved the collapse.max parameter (collapse.threshold must be used instead). It was deprecated for a long time.\n\n "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12787760",
            "date": "2009-12-08T21:43:58+0000",
            "content": "I have updated the patch and fixed the following issues:\n\n\tThe issue that Marc described on the solr-dev list. The collapsed groups identifiers disappeared when the id field was anything other then a plain field (int, long etc...).\n\tThe caching was not properly working when the collapse.field was changed between requests. Queries that should not have been cached were.\n\n "
        },
        {
            "author": "Marc Menghin",
            "id": "comment-12789381",
            "date": "2009-12-11T16:53:46+0000",
            "content": "Hi,\n\nnew to Solr, so sorry for my likely still incomplete setup. I got everything from Solr SVN and applied the Patch (field-collapse-5.patch\t2009-12-08 09:43 PM). As I search I get a NPE because I seem to not have a cache for the collapsing. It wants to add a entry to the cache but can't. There is none at that time, which it checks before in AbstractDocumentCollapser.collapse but still wants to use it later in AbstractDocumentCollapser.createDocumentCollapseResult. I suppose thats a bug? Or is something wrong on my side?\n\nException I get is:\n\njava.lang.NullPointerException\n\tat org.apache.solr.search.fieldcollapse.AbstractDocumentCollapser.createDocumentCollapseResult(AbstractDocumentCollapser.java:278)\n\tat org.apache.solr.search.fieldcollapse.AbstractDocumentCollapser.executeCollapse(AbstractDocumentCollapser.java:249)\n\tat org.apache.solr.search.fieldcollapse.AbstractDocumentCollapser.collapse(AbstractDocumentCollapser.java:172)\n\tat org.apache.solr.handler.component.CollapseComponent.doProcess(CollapseComponent.java:173)\n\tat org.apache.solr.handler.component.CollapseComponent.process(CollapseComponent.java:127)\n\tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:195)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1316)\n\nI fixed it locally by only adding something to the cache if there is one (fieldCollapseCache != null). But I'm not very into the code so not sure if thats a good/right way to fix it.\n\nThanks,\nMarc "
        },
        {
            "author": "Chad Kouse",
            "id": "comment-12789550",
            "date": "2009-12-11T21:37:24+0000",
            "content": "Just wanted to comment that I am experiencing the same behavior as Marc Menghin above (NPE) \u2013 the patch did NOT install cleanly (1 hunk failed) \u2013 but I couldn't really tell why since it looked like it should have worked \u2013 I just manually copied the hunk into the correct class.... Sorry I didn't note what failed.... "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12789776",
            "date": "2009-12-12T18:30:00+0000",
            "content": "@Marc. This was a silly bug, that occurs when you do not define a field collapse cache in the solrconfig.xml. I have attached a patch that fixes this bug, so you can use field collapse without configuring a field collapse cache. Caching with field collapsing is an optional feature.\n\n@Chad. Due to changes in the trunk applying the previous patch will result into merge conflicts. The new patch can be applied without merge conflicts. This means that applying this patch on 1.4 source will properly result in merge conflicts.  "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12791672",
            "date": "2009-12-16T23:29:11+0000",
            "content": "Martijn, I'm about to upgrade our production servers to Solr 1.4 with this latest patch you just posted and the difference is incredible.  The time from startup to first collapsed query results has gone from 90 down to about 20 seconds, subsequent searches seem to execute about twice as fast on average.  SOLR-236 has come a very long way in the year since we last patched.  Thanks for all the hard work, it's truly great.\n\nFYI, it doesn't patch clean against the 1.4 distribution tarball but I don't even understand what the conflict is, reading the patch the original code in that area that failed looked identical to what the patch was expecting:\n\n(in QueryComponent.java)\n\n        sreq.params.remove(ResponseBuilder.FIELD_SORT_VALUES); // this was there\n+       \n+       // disable collapser\n+ \t    sreq.params.remove(\"collapse.field\");\n+ \n        // make sure that the id is returned for correlation.  // and so was this?\n\nMaybe it's a whitespace issue?  Anyway it works fine if you just paste it in place. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12791836",
            "date": "2009-12-17T07:50:48+0000",
            "content": "Does anybody have a reason for why this should not be committed to trunk as it stands right now? "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12791850",
            "date": "2009-12-17T08:47:56+0000",
            "content": "Well that is nice to hear Stephen . I think I will add a 1.4 comparable patch to the issue, so people do not have issues while patching. \nI think it is a good idea Shalin to add the patch to the trunk as it is. The patch is quite stable now. For any future work related to field-collapsing we should open new issues (this is the longest issue I've ever seen). Does anyone else has a reason why field-collapsing shouldn't be committed to the trunk? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12791949",
            "date": "2009-12-17T14:42:36+0000",
            "content": "Patch in sync with trunk.\n\n\n\tCollapseComponent is PluginInfoInitialized. Removed changes to SolrConfig. Note, the collapseCollectorFactories array and the separate fieldCollapsing element has been removed from configuration.  this patch has the following configuration:\n\n<searchComponent name=\"collapse\" class=\"org.apache.solr.handler.component.CollapseComponent\">\n    <collapseCollectorFactory name=\"groupDocumentsCounts\" class=\"solr.fieldcollapse.collector.DocumentGroupCountCollapseCollectorFactory\" />\n\n    <collapseCollectorFactory name=\"groupFieldValue\" class=\"solr.fieldcollapse.collector.FieldValueCountCollapseCollectorFactory\" />\n\n    <collapseCollectorFactory name=\"groupDocumentsFields\" class=\"solr.fieldcollapse.collector.DocumentFieldsCollapseCollectorFactory\" />\n\n    <collapseCollectorFactory name=\"groupAggregatedData\" class=\"org.apache.solr.search.fieldcollapse.collector.AggregateCollapseCollectorFactory\">\n        <lst name=\"aggregateFunctions\">\n            <str name=\"sum\">org.apache.solr.search.fieldcollapse.collector.aggregate.SumFunction</str>\n            <str name=\"avg\">org.apache.solr.search.fieldcollapse.collector.aggregate.AverageFunction</str>\n            <str name=\"min\">org.apache.solr.search.fieldcollapse.collector.aggregate.MinFunction</str>\n            <str name=\"max\">org.apache.solr.search.fieldcollapse.collector.aggregate.MaxFunction</str>\n        </lst>\n    </collapseCollectorFactory>\n\n   <fieldCollapseCache\n      class=\"solr.FastLRUCache\"\n      size=\"512\"\n      initialSize=\"512\"\n      autowarmCount=\"128\"/>\n  </searchComponent>\n\n\n\n\n\n\n\tI couldn't find where the fieldCollapseCache was being regenerated. It seems it is not being thrown away after commits? I have changed it to be re-created on newSearcher event.\n\tRemoved changes to JettySolrRunner,CoreContainer and SolrDispatchFilter for the distributed test case. We will refactor it to use BaseDistributedSearchTestCase (not implemented yet)\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12791952",
            "date": "2009-12-17T14:49:29+0000",
            "content": "shalin, the names may not be necessary on the collapseCollectorFactory  becaus they are never referred by the name\n\nhow about making the functions also plugis as\n\n<collapseCollectorFactory class=\"org.apache.solr.search.fieldcollapse.collector.AggregateCollapseCollectorFactory\">        \n      <function name=\"sum\" class=\"org.apache.solr.search.fieldcollapse.collector.aggregate.SumFunction\"/>\n      <function name=\"avg\" class=\"org.apache.solr.search.fieldcollapse.collector.aggregate.AverageFunction\"/>\n      <function name=\"min\" class=\"org.apache.solr.search.fieldcollapse.collector.aggregate.MinFunction\"/>\n      <function name=\"max\" class=\"org.apache.solr.search.fieldcollapse.collector.aggregate.MaxFunction\"/>\n</collapseCollectorFactory>\n        \n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12791953",
            "date": "2009-12-17T14:56:08+0000",
            "content": "Does anybody have a reason for why this should not be committed to trunk as it stands right now? \n\nIt's been a while, but the last time I looked at it (3-4 mos. ago) I had the impression that it wouldn't scale.  Has anyone benchmarked this at large scale? "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12791968",
            "date": "2009-12-17T15:19:08+0000",
            "content": "How do we define \"large scale\"?  I have an index of about 5 million docs.  Does that qualify?   I'm working on it right now, I can run whatever benchmarks you like. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12791972",
            "date": "2009-12-17T15:26:56+0000",
            "content": "I'd define large scale for this in a couple of ways:\n1. Lots of docs in the result set (10K+)\n2. Lots of overall docs (100M+)\n3. Lots of queries (> 10 QPS) "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12791977",
            "date": "2009-12-17T15:31:59+0000",
            "content": "Is there a typo on the http://wiki.apache.org/solr/FieldCollapsing page in regards to the outputs?  There are two different output results, but the URL for the examples are the same.  See http://wiki.apache.org/solr/FieldCollapsing#Examples.  I think the second one is intended to show a collapse count for fields?\n\nAlso, I'm not sold on having separate collapse elements from the actual response, but I know other things do it too, so it isn't a huge deal), but the list of \"parallel arrays\" that one needs to traverse in order to render results is growing (highlighter, MLT, now Field Collapsing. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12791986",
            "date": "2009-12-17T15:55:20+0000",
            "content": "Shalin.\n1. This configuration also looks fine by me. The reason I added <fieldCollapsing> ... </fieldCollapsing> was to be able support sharing of collapseCollectorFactory instances between different collapse components in the near future. You think that is a valid reason for that? Or do you think that collapseCollectorFactories shouldn't be shared? \n2. I forgot to create that, so a good thing you added it.\n3. I think leaving out those changes will make the distributed integration tests fail (Haven't checked it).\n\nNoble. \n1. The reason I gave a name to collaspeCollectorFactory was for using an instance twice for different collapse components. \n2. Moving the classname to the class attribute looks better, then in the function element. So I think we should change that.\n\nGrant. \n1. I think you also referring to sharding. Sharding is supported, but not in a very elegant way. You will need to partition your documents to your shards in such a way that all documents belonging to a collapse group appear on one shard. To be honest I have never tested the patch on a corpus of 100M docs.\n2. Field collapsing can impact the search time in a very negative way. I wrote a small paragraph about it on my blog.\n3. The first two response examples are for 'old' patches. The last response example is for the more recent patches (and current patch).  "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12791992",
            "date": "2009-12-17T16:08:30+0000",
            "content": " I think you also referring to sharding. Sharding is supported, but not in a very elegant way. You will need to partition your documents to your shards in such a way that all documents belonging to a collapse group appear on one shard. To be honest I have never tested the patch on a corpus of 100M docs.\n\nThat doesn't seem good and I don't think it will work w/ all the distributed work going on.  I will likely have some time next week to help out.  Has anyone looked at how Google or others do this?  Clearly they collapse at very large scale w/ no noticeable detrimental effect.  Anyone looked at the literature on this?\n\nThe first two response examples are for 'old' patches. The last response example is for the more recent patches (and current patch).\n\nOK, good to know.  Can you update the page to reflect the latest patch? "
        },
        {
            "author": "Oleg Gnatovskiy",
            "id": "comment-12791995",
            "date": "2009-12-17T16:17:24+0000",
            "content": "Grant - I agree regarding the current distributed implementation. The implementation is pretty much pseudo-distributed and would cause many companies (ours included) to have to completely restructure their indexes. What we tried long ago, was to have the process method on each shard to return the id that is being collapsed on, along with documentId and score. Then, in mergeIds we would do another level of collapse - basically keeping only 1 of the documents with a unique collapseId, and removing the others from all other shards. \n\nObviously this caused several problems, not the least of which being that facet counts would always be slightly off, since we might have removed a document that was counted by the facetComponent. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12792115",
            "date": "2009-12-17T20:30:31+0000",
            "content": "\nI'd define large scale for this in a couple of ways:\n1. Lots of docs in the result set (10K+)\n2. Lots of overall docs (100M+)\n3. Lots of queries (> 10 QPS) \n\nGrant, this patch may not be perfect but I think we all agree that it is a great start. This is stable, used by many and has been well supported by the community. This is also a large patch and as I have known from my DataImportHandler experience, maintaining a large patch is quite a pain (and DataImportHandler didn't even touch the core). How about we commit this (after some review, of course), mark this as experimental (no guarantees of any sort) and then start improving it one issue at a time? Alternately, if you are not comfortable adding it to trunk, we can commit this on a branch and merge into trunk later.\n\nWhat do you think? "
        },
        {
            "author": "Uri Boness",
            "id": "comment-12792189",
            "date": "2009-12-17T22:47:41+0000",
            "content": "\nGrant, this patch may not be perfect but I think we all agree that it is a great start. This is stable, used by many and has been well supported by the community. This is also a large patch and as I have known from my DataImportHandler experience, maintaining a large patch is quite a pain (and DataImportHandler didn't even touch the core). How about we commit this (after some review, of course), mark this as experimental (no guarantees of any sort) and then start improving it one issue at a time? Alternately, if you are not comfortable adding it to trunk, we can commit this on a branch and merge into trunk later.\n\nI think managing a separate branch will be just as hard as managing a patch. I do however agree that it's about time this patch will be committed to the trunk. Even though the current solution is not scalable in terms of distributed search (and I agree that the current solution for that is not really a viable solution), many are already using it and it is the most wanted feature in JIRA after all. One think you can do, is apply the changed to the core (which are not really many) and commit the rest of the patch as a contrib (along with all the disclaimers Shalin mentioned above).  "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12792190",
            "date": "2009-12-17T22:51:44+0000",
            "content": "I have updated the response examples on the wiki.\n\nSome time ago I tried to come up with an accurate distributed solution, but I ran a problem as I have described in a previous comment:\n\n....\nField collapsing keeps track of the number of document collapsed per unique field value and the total count documents encountered per unique field. If the total count is greater than the specified collapse\nthreshold then the number of documents collapsed is the difference between the total count and threshold. Lets say we have two shards each shard has one document with the same field value. The collapse threshold is one, meaning that if we run the collapsing algorithm on the shard individually both documents will never be collapsed. But when the algorithm applies to both shards, one of the documents must be collapsed however neither shared knows that its document is the one to collapse.\n\nThere are more situations described as above, but it all boils down to the fact that each shard does not have meta information about the other shards in the cluster. Sharing the intermediate collapse results between the shards is in my opinion not an option. This is because if you do that then you also need to share information about documents / fields that have a collapse count of zero. This is totally impractical for large indexes.\n....\n\nI'm really curious how others have addressed this issue. I have not stumbled on any literature on this particular issue, maybe someone else has.  "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12792191",
            "date": "2009-12-17T22:58:19+0000",
            "content": "I'll just add my 0,02\u20ac - the main thing to vet now that it works (first make it work), is the interface to the client.  are the request params ideal?  is the response data structure locked down?   if so, get this committed ASAP and iterate on the internals of distributed and performance issues (then make it right).\n\nAdmittedly I've not tried this feature out myself though.  Committed stuff I'll try out easier than patches actually. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12792199",
            "date": "2009-12-17T23:05:45+0000",
            "content": "\nGrant, this patch may not be perfect but I think we all agree that it is a great start. This is stable, used by many and has been well supported by the community. This is also a large patch and as I have known from my DataImportHandler experience, maintaining a large patch is quite a pain (and DataImportHandler didn't even touch the core). How about we commit this (after some review, of course), mark this as experimental (no guarantees of any sort) and then start improving it one issue at a time? Alternately, if you are not comfortable adding it to trunk, we can commit this on a branch and merge into trunk later.\n\nWhich is why it should not go in unless it is ready.  Adding a large patch that isn't right just b/c it's been around for a while and is \"hard to maintain\" is no reason to just go commit something.  The problem w/ committing something that isn't ready is then we have to do even more work to maintain it, thus taking away from the opportunity to make it better.   \n\nAs for the voting and the popularity, I think that is all the more reason why it needs to be done right and not just be a \"good start\".  With this many eyes on it, it shouldn't be easy to get people testing it and giving feedback.\n\nIf the issue is that the patch is to big, then perhaps it needs to be broken up into smaller pieces that lay the framework for field collapsing to work.   "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12792205",
            "date": "2009-12-17T23:13:43+0000",
            "content": "Shalin, I have updated your patch.\n\n\tThe CollapseComponentTest was failing. The field collapseCollectorFactories in CollapseComponent was null when not specifying any collapse collector factories in the solrconfig.xml which resulted in a NPE.\n\tRemoved a system.out that I accidentally added in my previous patch.\n\n\n\nThe DistributedFieldCollapsingIntegrationTest is still failing, because you left out changes in JettySolrRunner, CoreContainer and SolrDispatchFilter from my original patch. That allowed my the specify different schema file for this particular test. I think it is important for the test coverage to have this test. Should I add the fields of the  schema-fieldcollapse.xml to the schema.xml that the other tests use? The test should then succeed.\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12792349",
            "date": "2009-12-18T05:35:33+0000",
            "content": "I think that is all the more reason why it needs to be done right and not just be a \"good start\".\n\nThe fact that it has been around for so long means that the \"good start\" is gonna take longer to happen.  According to me , we should fix the obvious stuff and commit this with a clear warning in the javadocs and wiki that this has perf isssues and the code/API/configuration may change incompatibly in the future.\n\n\n\nCommitted stuff I'll try out easier than patches actually.\n\n\n+1 There is a better chances of developers taking a look at it if it is already in the trunk. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12792350",
            "date": "2009-12-18T05:41:13+0000",
            "content": "For Martijn:\n\n\nThe reason I added <fieldCollapsing> ... </fieldCollapsing> was to be able support sharing of collapseCollectorFactory instances between different collapse components in the near future. You think that is a valid reason for that? Or do you think that collapseCollectorFactories shouldn't be shared?\n\nI just don't think that we should introduce new tags and new kinds of components in solrconfig.xml, particularly those that are useful to only a single component. That introduces changes in SolrConfig.java so that it knows how to load such things. That is why I moved that configuration inside CollapseComponent. Ideally, all components will use PluginInfo and load whatever they need from their own PluginInfo object and SolrConfig would not need to be changed unless we introduce new kinds of Solr plugins.\n\nJust curious, what would be a use-case for sharing factories (other than reducing duplication of configuration) and having multiple CollapseComponent?\n\n\nThe CollapseComponentTest was failing. The field collapseCollectorFactories in CollapseComponent was null when not specifying any collapse collector factories in the solrconfig.xml which resulted in a NPE.\n\nOops, sorry about that. I only ran the tests inside org.apache.solr.search.fieldcollapse. I didn't notice there are other tests too. Thanks!\n\nThe DistributedFieldCollapsingIntegrationTest is still failing, because you left out changes in JettySolrRunner, CoreContainer and SolrDispatchFilter from my original patch.\n\nI don't think we need to add that functionality to CoreContainer and SolrDispatchFilter. It is still possible to specify a different solrconfig and schema for a test. Let me see if I can make this work with BaseDistributedSearchTestCase "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12792420",
            "date": "2009-12-18T10:16:15+0000",
            "content": "Changes:\n\n\n\tModified configuration as Noble suggested. The AggregateCollapseCollectorFactory is now PluginInfoInitialized instead of NamedListInitialzed and functions are plugins. The \"name\" attribute is removed from \"collapseCollectorFactory\" since it is no longer necessary:\n\n<searchComponent name=\"collapse\" class=\"org.apache.solr.handler.component.CollapseComponent\">\n    <collapseCollectorFactory class=\"solr.fieldcollapse.collector.DocumentGroupCountCollapseCollectorFactory\" />\n\n    <collapseCollectorFactory class=\"solr.fieldcollapse.collector.FieldValueCountCollapseCollectorFactory\" />\n\n    <collapseCollectorFactory class=\"solr.fieldcollapse.collector.DocumentFieldsCollapseCollectorFactory\" />\n\n    <collapseCollectorFactory class=\"org.apache.solr.search.fieldcollapse.collector.AggregateCollapseCollectorFactory\">\n      <function name=\"sum\" class=\"org.apache.solr.search.fieldcollapse.collector.aggregate.SumFunction\"/>\n      <function name=\"avg\" class=\"org.apache.solr.search.fieldcollapse.collector.aggregate.AverageFunction\"/>\n      <function name=\"min\" class=\"org.apache.solr.search.fieldcollapse.collector.aggregate.MinFunction\"/>\n      <function name=\"max\" class=\"org.apache.solr.search.fieldcollapse.collector.aggregate.MaxFunction\"/>\n    </collapseCollectorFactory>\n\n  \t<fieldCollapseCache\n      class=\"solr.FastLRUCache\"\n      size=\"512\"\n      initialSize=\"512\"\n      autowarmCount=\"128\"/>\n    \n  </searchComponent>\n\n\n\tChanged DistributedFieldCollapsingIntegrationTest to use BaseDistributedSearchTestCase. This fails right now. I believe there is a bug with the distributed implementation. The distributed version returns one extra group when compared to the non-distributed version. I've put an @Ignore annotation on that test.\n\n\n\nWe can consider creating the functions through a factory so that they can accept initialization parameters. The schema-fieldcollapse.xml and solrconfig-fieldcollapse.xml are no longer necessary and can be removed.\n\nNext steps:\n\n\tLet us open issues for all the modifications needed in Solr to support this feature. That will help us break down this patch into more manageable (and easily reviewable) pieces. I guess we need one for providing custom Collectors for SolrIndexSearcher methods. Any others?\n\tThe response format is not very clear in the wiki. We should add more examples and explain the format.\n\n "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12792426",
            "date": "2009-12-18T10:29:14+0000",
            "content": "For Shalin: \n\nI just don't think that we should introduce new tags and new kinds of components in solrconfig.xml, particularly those that are useful to only a single component. That introduces changes in SolrConfig.java so that it knows how to load such things. That is why I moved that configuration inside CollapseComponent. Ideally, all components will use PluginInfo and load whatever they need from their own PluginInfo object and SolrConfig would not need to be changed unless we introduce new kinds of Solr plugins.\n\nI agree about the PluginInfo and I think it is the right place for field collapse config.\n\nJust curious, what would be a use-case for sharing factories (other than reducing duplication of configuration) and having multiple CollapseComponent?\n\nBesides different configured CollapseCollectorFactories none.\n\nI don't think we need to add that functionality to CoreContainer and SolrDispatchFilter. It is still possible to specify a different solrconfig and schema for a test. Let me see if I can make this work with BaseDistributedSearchTestCase\n\nThat would be great! "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12792446",
            "date": "2009-12-18T11:51:27+0000",
            "content": "I'm curious as to whether anyone has just thought of using the Clustering component for this?  If your \"collapse\" field was a single token, I wonder if you would get the results you're looking for.   "
        },
        {
            "author": "Uri Boness",
            "id": "comment-12792458",
            "date": "2009-12-18T12:42:31+0000",
            "content": "I'm curious as to whether anyone has just thought of using the Clustering component for this? If your \"collapse\" field was a single token, I wonder if you would get the results you're looking for.\n\nThe main difference between the two components is that while the clustering works more as a function where the input is the doclist/docset and the output is a separate data structure representing the groups, the collapse component operates directly on the docset & doclist modifies them and incorporates the groups within the final search result.\n\nIn all occurrences where we found the need for the collapse component, we needed to incorporate the grouping within the search result, and adjust the sorting and the pagination accordingly. As far as I know you cannot do that with the clustering component. This tight integration with the result is also the reason why the collapse component right now is actually a replacement to the query component. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12792477",
            "date": "2009-12-18T13:39:44+0000",
            "content": "I'm with Grant on this one. Trunk is not a sandbox, and getting more developer attention is not a good reason to put something in trunk. Issues should go in when they are ready.\n\nTons of interest and votes doesn't mean rush to trunk - if that type of thing moves you, it means start putting some work into it to make it ready for trunk.\n\nThis patch has quite a resource/performance hit. I've seen and read about the resource hit. Its rather large. The performance hit is not any better. The linked to blog marks performance with collapsing as 5-10 times slower than without.\n\nPersonally, I don't think this issue is ready for trunk. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12792509",
            "date": "2009-12-18T15:18:42+0000",
            "content": "This patch has quite a resource/performance hit. I've seen and read about the resource hit. Its rather large.\n\nThe performance price is paid only if you use this component.  Having the functionality itself in Solr is quite important. Performance can obviously be improved. (Faceting fot a 50 times perf boost in 1.4) . As long as the performance of the component is within the acceptable range we should leave that call to the user.  The cost actually depends on the data set too.\n\nAs long as the component has a correct public API (req params/response format/configuration) I believe it can be committed with a clear warning. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12792510",
            "date": "2009-12-18T15:23:06+0000",
            "content": "(Faceting fot a 50 times perf boost in 1.4)\n\nNo it didn't. Certain cases have gotten a boost (I think you might be referring to multi-valued field faceting cases?). And general faceting was always relatively fast and scalable.\n\nI'm against committing features to trunk with a warning that the feature is not ready for trunk. "
        },
        {
            "author": "David Smiley",
            "id": "comment-12792514",
            "date": "2009-12-18T15:46:49+0000",
            "content": "I've been watching this thread forever without saying anything but want to offer my two cents and I'll but out.\n\nI very much disagree with a policy blocking non-production-ready code from being in source control.  All code starts off this way and it would be quite a shame not to leverage the advantages of source control simply because it isn't ready yet.  If people are uncomfortable with it being in trunk then simply use a branch.  Of course, how simple \"simple\" is depends on one's comfort with source control and the particular source control technology used and tools to help you (e.g. IDEs).  By the way, git makes \"feature branches\" (which is what this would be) easy to manage and integrates bidirectionally with subversion.  If you're not comfortable with branching because you're not familiar with it then you need to learn.  By \"you\" I don't mean anyone in particular, I mean all professional software developers.  Source control and branching are tools of our trade. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12792518",
            "date": "2009-12-18T16:11:53+0000",
            "content": "I very much disagree with a policy blocking non-production-ready code from being in source control\n\nJust to be clear, there is no such policy that I've seen - each decision just comes down to consensus. And as far as I know, our branch policy is pretty much \"anything goes\" - trunk is very different than svn. Anyone (anyone with access to svn that is) can play around with a branch for anything if they want.\n\n\nI agree with your thoughts on a branch - if the argument is, we want it to be easier for devs to check out and work on this, or for users to checkout and build this without applying patches, why not just make a branch? Merging is annoying but not difficult - I've been doing plenty of branch merging lately, and while its not glorious work, modern tools make it more of a grind than a challenge. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12792535",
            "date": "2009-12-18T17:18:31+0000",
            "content": "The main problem with the patch is that the performance/resource consumption is unacceptable. \n\n\n\tIs it true that the perf cost is avoidable?\n\tor are their implementation details which can be optimized?\n\n\n\nWe are working to make to ready for trunk. So anything that helps us move towards the objective is welcome "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12792539",
            "date": "2009-12-18T17:33:00+0000",
            "content": "I'm not sold on the output yet, either.  Have we considered it being inline?  We're getting more and more parallel arrays we need to consider.  I think with the other Solr issues that are looking at pseudo-fields and the ability for components to add results, that we could rework these things.\n\nAlso, why don't the aggregate functions just work w/ all the existing functions?   "
        },
        {
            "author": "Patrick Eger",
            "id": "comment-12792587",
            "date": "2009-12-18T19:07:29+0000",
            "content": "Hi, possibly not important but would like to give my perspective as a user. Specifically, the code is very much production ready in our opinion, albeit under a limited set of circumstances that we are comfortable with (< 5 million docs, no distributed search). Within those confines it works great and satisfies our needs, and we are more than willing to pay the performance hit since it's absolutely essential to the correct functionality. I suppose i'd disagree with the assertion that the performance is \"unacceptable\", as i think that is a value judgement each user will have to make.\n\nModulo the discussion about the request format, output format and config (stuff that is hard to change later). I would much rather have the code be in and documented with those caveats clearly spelled out and probably tracked in separate JIRA issues. IE DO NOT USE IF SHARDING, >5 million docs, etc, etc. Again, just my 2c as a satisfied user. "
        },
        {
            "author": "Uri Boness",
            "id": "comment-12792686",
            "date": "2009-12-18T22:50:12+0000",
            "content": "Essentially it boils down to two options:\n\n\n\tKeep it out of the trunk, in which case users that will need this functionality will only get it by working with a patched Solr version of their own, or use a branch (in both cases, most likely they will miss the continuous work done on the trunk unless they keep on merging the changes)\n\tKeep in the trunk with some caveats, in which case they users have a chance to use this functionality out of the box\n\n\n\nIn both cases, the user have a choice to make:\n\n\tbe satisfied by the performance of this feature\n\tlook for an alternative solution (other products)\n\tgive up this functionality all together (if their business requirements allow that)\n\n\n\nSo the main difference here I would say is in how easy you'd like to provide this functionality to the users. On the Solr development part, indeed once this is committed to the trunk there's much more responsibility on the committers to make it work (enhance performance and fix bugs)... but this is a good thing as there is a high demand for this feature and as a community driven project this demand should to be satisfied. And I do think that the number of users using this patch already is a good indicator that it is good enough for quite a lot of use cases.\n\nI do agree though that before committing anything, the public API should be re-evaluated to minimize chances for BWC issues later on. BTW, regarding the response, Solr already has a few places where the response format is still marked as experimental and as subject to changes in the future (but it doesn't stop people from using this functionality as they take the responsibility to adapt to any such future changes when the come).\n\nNow... writing this, it suddenly occurred to me that there might be another solution to this all discussion which is in a way a combination of many of the suggestions in this thread. What if, this patch would be split to two: the changes to the core and the component itself. Now, if the changes to the core are not that drastic and make sense (or at least everyone can live with them) then perhaps they can be committed to the trunk. As for the rest of the patch (which consists of the search components and its other supporting classes), this can be put in SVN as separate branch for contrib. The good thing about this solution is that the work done on this functionality will be in SVN so you benefit from it as David mentioned above. The other benefit is that with this layout you can actually build the branched code base separately and distribute this functionality as a separate jar which can be deployed in Solr 1.5x distribution. Again, a bit of work left to the users (too much to my taste) but at least they're not forced to use a patched version of Solr. Would that be a possible solution? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12792777",
            "date": "2009-12-19T04:37:01+0000",
            "content": "olr already has a few places where the response format is still marked as experimental and as subject to changes in the future ....\n\nMarking the output format as experimental is just trying to be safe. We strive hard to ensure that we don't change it or even if we do it it is not disruptive. So let us not take this as an excuse to be lax of the review of the public API. \n\non keeping a separate branch....\n\nI would say a branch is less useful than an patch. if the patch applies to the trunk , I can be sure that I have the latest and greatest stuff. On the other hand if the code  lives in a branch it is more work to keep it synced w/ the trunk than the patch itself.\n\n\n@Uri\nI support your suggestion on splitting this issue into two. i.e make the core changes in a separate patch . That is the plan anyway. \n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12792781",
            "date": "2009-12-19T05:08:08+0000",
            "content": "bq . On the other hand if the code lives in a branch it is more work to keep it synced w/ the trunk than the patch itself.\n\nIs that true? Syncing a branch is the same as syncing a patch - non conflicts are merged automatically and conflicts must be handled - same with a patch or a branch. And a patch gets out of date just as easily as a branch. The main difference I see is that its easier for non committers to share updated patches, whereas merging the branch will require the help of a committer if you want to share the merge with others. Anyone can checkout the branch and merge with trunk though - its literally the same effort as updating an out of date patch. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12792784",
            "date": "2009-12-19T05:15:10+0000",
            "content": "The main difference I see is that its easier for non committers to share updated patches\n\nThis is a huge difference. Considering the no:of involvement of the non-committers involved in this issue.\n\nIf your patch does not modify any existing files  you never have to sync it w/ trunk. It is always synced. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12792793",
            "date": "2009-12-19T06:09:09+0000",
            "content": "This is a huge difference. Considering the no:of involvement of the non-committers involved in this issue.\n\nIts not really any different than putting it in trunk. Non committers can still post patches to the branch in JIRA, the same as if the issue was in trunk. Smaller, more focused patches. If there are no benefits to a branch in this regard, what is the argument to putting this in trunk for further dev? Might as well just stay in patch form until its ready then.\n\nIf your patch does not modify any existing files you never have to sync it w/ trunk. It is always synced.\n\nYou have to apply the patch. With a branch you have to type a merge command. Its the same effort - a single command. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12792916",
            "date": "2009-12-19T22:27:52+0000",
            "content": "First, thanks to everyone who has spent so much time working on this - lack of committer attention doesn't equate to lack of interest... this is a very much needed feature!\n\nI'd agree with Erik that the most important thing is the interface to the client, and making it well thought out and semantically \"tight\". Martijn's recent improvements to the response structure is an example of improvements in this area. It's also important to think about the interface in terms of how easy it will be to add further features, optimizations, and support distributed search. If the code isn't sufficiently standalone, we also need to see how easily it fits into the rest of Solr (what APIs it adds or modifies, etc). Actually implementing performance improvements and more distributed search can come later - as long as we've thought about it now so we haven't boxed ourselves in.\n\nIt seems like field collapsing should just be additional functionality of the query component rather than a separate component since it changes the results?\n\nThe most basic question about the interface would be how to present groups. Do we stick with a linear document list and supplement that with extra info in a different part of the response (as the current approach takes)? Or stick that extra info in with some of the documents somehow? Or if collapse=true, replace the list of documents with a list of groups, each which can contain many documents? Which will be easiest for clients to deal with? If you were starting from scratch and didn't have to deal with any of Solr's current shortcomings, what would it look like?\n\nFrom the wiki:\ncollapse.maxdocs - what does this actually mean? I assume it collects arbitrary documents up to the max (normally by index order)? Does this really make sense? Does it affect faceting, etc? If it does make sense, it seems like it would also make sense for normal non-collapsed query results too, in which case it should be implemented at that level.\n\ncollapse.info.doc - what does that do? I understand counts per group, but what's count per doc?\n\ncollapse.includeCollapsedDocs.fl - I don't understand this one, and can't find an example on the wiki or blogs. It says \"Parameter indicating to return the collapsed documents in the response\"... but I thought documents were included up until collapse.threshold.\n\ncollapse.debug - should perhaps just be rolled into debugQuery, or another general debug param (someone recently suggested using a comma separated list... debug=timings,query, etc.\n\nShould I be able to specify a completely different sort within a group? collapse.sort=... seems nice... what are the implications? One bit of strangeness: it would seem to allow a highly ranked document responsible for the group being at the top of the list being dropped from the group due to a different sort criteria within the group. It's not necessarily an implementation problem though (sort values for the group should be maintained separately).\n\nIs there a way to specify the number of groups that I want back instead of the number of documents? Or am I supposed to just over-request (rows=num_groups_I_want*threshold) and ignore if I get too many documents back?\n\nRandom thought: We need a test to make sure this works with multi-select faceting (SimpleFacets asks for the docset of be base query...)\n\nDistributed Search: should be able to use the same type of algorithm that faceting does to ensure accurate counts.\n\nPerformance: yes, it looks like the current code uses a lot of memory.\nHere's an algorithm that I thought of on my last plane ride that can do much better (assuming max() is the aggregation function):\n\n\n=================== two pass collapsing algorithm for collapse.aggregate=max ====================\nFirst pass: pretend that collapseCount=1\n  - Use a TreeSet as a priority queue since one can remove and insert entries.\n  - A HashMap<Key,TreeSetEntry> will be used to map from collapse group to top entry in the TreeSet\n  - compare new doc with smallest element in treeset. If smaller discard and go to the next doc.\n  - If new doc is bigger, look up it's group. Use the Map to find if the group has been added to the TreeSet and add it if not.\n  - If the new bigger doc is already in the TreeSet, compare with the document in that group. If bigger, update the node,\n    remove and re-add to the TreeSet to re-sort.\n\nefficiency: the treeset and hashmap are both only the size of the top number of docs we are looking at (10 for instance)\nWe will now have the top 10 documents collapsed by the right field with a collapseCount of 1. Put another way, we have the top 10 groups.\n\nSecond pass (if collapseCount>1):\n - create a priority queue for each group (10) of size collapseCount\n - re-execute the query (or if the sort within the collapse groups does not involve score, we could just use the docids gathered during phase 1)\n - for each document, find it's appropriate priority queue and insert\n - optimization: we can use the previous info from phase1 to even avoid creating a priority queue if no other items matched.\n\nSo instead of creating collapse groups for every group in the set (as is done now?), we create it for only 10 groups.\nInstead of collecting the score for every document in the set (40MB per request for a 10M doc index is *big*) we re-execute the query if needed.\nWe could optionally store the score as is done now... but I bet aggregate throughput on large indexes would be better by just re-executing.\n\nOther thought: we could also cache the first phase in the query cache which would allow one to quickly move to the 2nd phase for any collapseCount.\n\n  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12792917",
            "date": "2009-12-19T22:28:17+0000",
            "content": "First, thanks to everyone who has spent so much time working on this - lack of committer attention doesn't equate to lack of interest... this is a very much needed feature!\n\nI'd agree with Erik that the most important thing is the interface to the client, and making it well thought out and semantically \"tight\".  Martijn's recent improvements to the response structure is an example of improvements in this area.  It's also important to think about the interface in terms of how easy it will be to add further features, optimizations, and support distributed search.  If the code isn't sufficiently standalone, we also need to see how easily it fits into the rest of Solr (what APIs it adds or modifies, etc).  Actually implementing performance improvements and more distributed search can come later - as long as we've thought about it now so we haven't boxed ourselves in.\n\nIt seems like field collapsing should just be additional functionality of the query component rather than a separate component since it changes the results?\n\nThe most basic question about the interface would be how to present groups.  Do we stick with a linear document list and supplement that with extra info in a different part of the response (as the current approach takes)?  Or stick that extra info in with some of the documents somehow?  Or if collapse=true, replace the list of documents with a list of groups, each which can contain many documents?  Which will be easiest for clients to deal with?  If you were starting from scratch and didn't have to deal with any of Solr's current shortcomings, what would it look like?\n\nFrom the wiki:\ncollapse.maxdocs - what does this actually mean?  I assume it collects arbitrary documents up to the max (normally by index order)?  Does this really make sense?  Does it affect faceting, etc?  If it does make sense, it seems like it would also make sense for normal non-collapsed query results too, in which case it should be implemented at that level.\n\ncollapse.info.doc - what does that do?  I understand counts per group, but what's count per doc?\n\ncollapse.includeCollapsedDocs.fl - I don't understand this one, and can't find an example on the wiki or blogs.  It says \"Parameter indicating to return the collapsed documents in the response\"... but I thought documents were included up until collapse.threshold.\n\ncollapse.debug - should perhaps just be rolled into debugQuery, or another general debug param (someone recently suggested using a comma separated list... debug=timings,query, etc.\n\nShould I be able to specify a completely different sort within a group?  collapse.sort=...  seems nice... what are the implications?  One bit of strangeness: it would seem to allow a highly ranked document responsible for the group being at the top of the list being dropped from the group due to a different sort criteria within the group.  It's not necessarily an implementation problem though (sort values for the group should be maintained separately).\n\nIs there a way to specify the number of groups that I want back instead of the number of documents?  Or am I supposed to just over-request (rows=num_groups_I_want*threshold) and ignore if I get too many documents back?\n\nRandom thought: We need a test to make sure this works with multi-select faceting (SimpleFacets asks for the docset of be base query...)\n\nDistributed Search: should be able to use the same type of algorithm that faceting does to ensure accurate counts.\n\nPerformance: yes, it looks like the current code uses a lot of memory.\nHere's an algorithm that I thought of on my last plane ride that can do much better (assuming max() is the aggregation function):\n\n\n=================== two pass collapsing algorithm for collapse.aggregate=max ====================\nFirst pass: pretend that collapseCount=1\n  - Use a TreeSet as  a priority queue since one can remove and insert entries.\n  - A HashMap<Key,TreeSetEntry> will be used to map from collapse group to top entry in the TreeSet\n  - compare new doc with smallest element in treeset.  If smaller discard and go to the next doc.\n  - If new doc is bigger, look up it's group.  Use the Map to find if the group has been added to the TreeSet and add it if not.\n  - If the new bigger doc is already in the TreeSet, compare with the document in that group.  If bigger, update the node,\n    remove and re-add to the TreeSet to re-sort.\n\nefficiency: the treeset and hashmap are both only the size of the top number of docs we are looking at (10 for instance)\nWe will now have the top 10 documents collapsed by the right field with a collapseCount of 1.  Put another way, we have the top 10 groups.\n\nSecond pass (if collapseCount>1):\n - create a priority queue for each group (10) of size collapseCount\n - re-execute the query (or if the sort within the collapse groups does not involve score, we could just use the docids gathered during phase 1)\n - for each document, find it's appropriate priority queue and insert\n - optimization: we can use the previous info from phase1 to even avoid creating a priority queue if no other items matched.\n\nSo instead of creating collapse groups for every group in the set (as is done now?), we create it for only 10 groups.\nInstead of collecting the score for every document in the set (40MB per request for a 10M doc index is *big*) we re-execute the query if needed.\nWe could optionally store the score as is done now... but I bet aggregate throughput on large indexes would be better by just re-executing.\n\nOther thought: we could also cache the first phase in the query cache which would allow one to quickly move to the 2nd phase for any collapseCount.\n\n "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12792997",
            "date": "2009-12-20T15:46:05+0000",
            "content": "ttdi,\nThe latest patch is not in sync with the latest trunk. You can try to patch to the trunk or use a previous patch for the 1.4 code.\n\nYonik,\nThe parameters description is a bit poor. The response format of the older patches contains two separate lists of collapse group counts. A list with counts per most relevant document id that is enabled or disabled with collapse.info.doc param. The second list with counts per fieldvalue of the most relevant document that is controlled with collapse.info.count  param. Now that the response format has changed we should rename it to something more descriptive. Maybe something like collapse.showCount that adds the collapse count to the collapse group in the response (default to true) and collapse.showFieldValue that adds the fieldvalue of the most relevant document to the group (defaults to false)?\n\nThe collapse.maxdocs specifies when to abort field-collapsing after n document have been processed. I have never used is. I can imagine that one would use it to shorten the search time. \n\nThe collapse.includeCollapsedDocs.fl enables a collapse collector that collects the documents that have been discarded and output the specified fields of the discarded documents to the fieldcollapse response per collapse group (* for all fields). The parameter name does not reflect that behaviour entirely. You think that collapse.collectDiscardedDocuments.fl is better? However personally I would not use this, because of the negative impact it has on performance. Usually one wants to know something like the average / highest / lowest price of a collapse group. The AggregateCollapseCollector would fit the needs better.\n\nShould I be able to specify a completely different sort within a group? collapse.sort=... seems nice... what are the implications? One bit of strangeness: it would seem to allow a highly ranked document responsible for the group being at the top of the list being dropped from the group due to a different sort criteria within the group. It's not necessarily an implementation problem though (sort values for the group should be maintained separately).\n\nI'm not sure about that. It would make things more complicated. Sorting the discarded documents in combination with the collapse.includeCollapsedDocs.fl functionality would maybe make more sense. \n\nThe most basic question about the interface would be how to present groups. Do we stick with a linear document list and supplement that with extra info in a different part of the response (as the current approach takes)? Or stick that extra info in with some of the documents somehow? Or if collapse=true, replace the list of documents with a list of groups, each which can contain many documents? Which will be easiest for clients to deal with? If you were starting from scratch and didn't have to deal with any of Solr's current shortcomings, what would it look like?\n\nI think the latter would make more sense, because field-collapsing does change the search result. It would just make it more obvious.\n\nIs there a way to specify the number of groups that I want back instead of the number of documents?\nNo there is not, but if the list of documents is replaced with a list of groups then the rows parameter should be used to indicate the number of groups to be displayed instead the number of documents to be displayed.\n\nJust one thought I had about the algorithm you propose. If you only create collapse groups for the top ten documents then what about the total count of the search? Unique documents outside the top ten documents are not being grouped (if I understand you correctly) and that would impact the total count with how it currency works. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12793048",
            "date": "2009-12-20T22:07:52+0000",
            "content": "I support your suggestion on splitting this issue into two. i.e make the core changes in a separate patch . That is the plan anyway.\n\nThe changes in the core that should be in a separate patch are:\n\n\tSolrIndexSearcher\n\tDocSetHitCollector\n\tDocSetAwareCollector\n\n\n\nThe above files where changes because of the following reasons:\n\n\tThe getDocSet(...) methods in the SolrIndexSearcher did not allow me to specify a Lucene Collector, which I needed to get the uncollapsed docset and levering the Solr caches whilst doing that. I changed them so I was able to do that.\n\tThe patch also contains an extra getDocListAndSet(...) method that allows specifying a filter docset, which in the case of field collapsing is the collapsed docset.\n\n\n\nThe QueryComponent has changed as well. The only reason these changes where made, was to support the psuedo distributed field-collapsing. Maybe for the distributed field collapsing a separate patch should created with this change as a start. Last but not least the SolrJ code. I think for these changes a separate patch should be created as well. Maybe for each patch a sub issue should be created in Jira. \n\nThe rest of the files in the patch do not impact any core files and I think should remain in one patch.  "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12793090",
            "date": "2009-12-21T05:47:33+0000",
            "content": "We need to open a separate issue for the core related changes. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12793101",
            "date": "2009-12-21T07:31:39+0000",
            "content": "How about we change the current field collapsing response format to the following?\n\nWe add new well-known fields to the document itself, say \n\n\t\"collapse.value\" - contains the group field's value for this document\n\t\"collapse.count\" - the number of results collapsed under this document\n\t\"collapse.aggregate.function(field-name)\" - the aggregate value for the given function applied to the given field for this document's group\n\n\n\nExample:\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response>\n  <lst name=\"responseHeader\">\n    <int name=\"status\">0</int>\n    <int name=\"QTime\">2</int>\n    <lst name=\"params\">\n      <str name=\"collapse.field\">manu_exact</str>\n      <str name=\"collapse.aggregate\">max(field1)</str>\n      <str name=\"collapse.aggregate\">avg(field1)</str>\n      <str name=\"q\">title:test</str>\n      <str name=\"field.collapse\">title</str>\n      <str name=\"qt\">collapse</str>\n    </lst>\n  </lst>\n  <result name=\"response\" numFound=\"30\" start=\"0\">\n    <doc>\n      <str name=\"id\">F8V7067-APL-KIT</str>\n      <str name=\"collapse.value\">Belkin</str>\n      <int name=\"collapse.count\">1</int>\n      <int name=\"collapse.aggregate.max(field1)\">100</int>\n      <float name=\"collapse.aggregate.avg(field1)\">50.0</float>\n    </doc>\n    <doc>\n      <str name=\"id\">TWINX2048-3200PRO</str>\n      <str name=\"collapse.value\">Corsair Microsystems Inc.</str>\n      <int name=\"collapse.count\">3</int>\n      <int name=\"collapse.aggregate.max(field1)\">100</int>\n      <float name=\"collapse.aggregate.avg(field1)\">50.0</float>\n    </doc>\n  </result>\n</response>\n\n\n\nNo need to have another section and correlate based on uniqueKeys. For this to work, CollapseComponent must generate a custom SolrDocumentList and set it as \"results\" in the response.\n\nFor request parameters:\n\n\t\"collapse.aggregate\" - Can we make this a multi-valued parameter instead of comma separated?\n\n "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12793365",
            "date": "2009-12-21T21:19:58+0000",
            "content": "We need to open a separate issue for the core related changes. \nAs you properly have noticed I have split the patch into smaller patches and created sub issues for each patch.\n\nHow about we change the current field collapsing response format to the following? \nLooks okay at first sight.\n\nFor this to work, CollapseComponent must generate a custom SolrDocumentList and set it as \"results\" in the response.\nMaybe we need a more elegant solution for this. All these extra fields are calculated values. If we were to put the calculated values into a certain context and the response writers can then look values up in the context and write them to the response. Other functionalities might also benefit from this solution like distances from a central point when doing a geo search. It is just an idea. I recall there an issue in Jira that propose something like this, but I couldn't find it.\n\n\"collapse.aggregate\" - Can we make this a multi-valued parameter instead of comma separated?\nI think that is good idea, other parameters (like the fq) are also multi-valued.\n\nBTW I think we should continue further technical discussions in the sub issues. We got space there for a lot of comments   "
        },
        {
            "author": "Uri Boness",
            "id": "comment-12793411",
            "date": "2009-12-21T23:17:50+0000",
            "content": "@Shalin\n\nI think mixing the collapse information with document fields is wrong. The collapse fields don't really belong to the document, but to the group the document represents, while the other field do belong to it. The response format should somehow indicate this difference. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12793476",
            "date": "2009-12-22T03:06:45+0000",
            "content": "You think that collapse.collectDiscardedDocuments.fl is better?\n\nIs this something that's really needed?  If so, some other name ideas could be\ncollapse.discarded.fl  \ncollapse.discarded.limit  (doesn't seem to be a good idea to have an unbounded number).\n\nJust one thought I had about the algorithm you propose. If you only create collapse groups for the top ten documents then what about the total count of the search? Unique documents outside the top ten documents are not being grouped (if I understand you correctly) and that would impact the total count with how it currency works.\n\nRight - one would not be able to tell the total number of collapsed docs, or the total number of hits (or the DocSet) after collapsing.  So only collapse.facet=before would be supported.  I do think that just like faceting, there will be multiple ways of doing collapsing.\n\nAnyway, this is a great example of trying to make sure the interface doesn't preclude optimizations.  Perhaps the total count of the search (numFound) should be pre-collapsing if collapse.facet=before, or perhaps it should always be pre-collapsing, and we should have another optional count for post-collapsing? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12793534",
            "date": "2009-12-22T06:40:16+0000",
            "content": "I think mixing the collapse information with document fields is wrong\n\nWhy is it wrong. it is about adding meta-info to the docs. This is what we plan to do with SOLR-1566\n\nEven when we collapse what we are expecting is simple search results. So a drastic deviation from the standard format is not a good idea. \n\nMoreover , if we keep it in the document, it keeps parsing and processing simpler "
        },
        {
            "author": "Uri Boness",
            "id": "comment-12793554",
            "date": "2009-12-22T09:01:07+0000",
            "content": "Why is it wrong. it is about adding meta-info to the docs. This is what we plan to do with SOLR-1566\n\nThis is exactly the point, it's not really meta-data over the document, but on the group the document belongs to. And you also need a more obvious way to mark this document as a group representation (to distinguish it from other normal documents).\n\nEven when we collapse what we are expecting is simple search results. So a drastic deviation from the standard format is not a good idea.\nI definitely agree that BWC should be kept, specially here when we're dealing with a query component. But extending the current <doc> element, doesn't mean we break BWC. Adding a <collapse-info> (or <collapse-meta-data>) sub element to it, will certainly not break anything, specially when we still don't have a formal xsd for the responses (I know we're working on it, but it's still not out there so it's safe).\n "
        },
        {
            "author": "Uri Boness",
            "id": "comment-12793565",
            "date": "2009-12-22T09:30:27+0000",
            "content": "@Yonik\n\nAs far as I understand from your collapse algorithm proposal, in order to save memory you'd like to restrict the group creation to only those that belong in the requested results page. Beyond loosing the faceting support over the collapsed DocSet, I think there might be a problem with pagination as well. For every page you'll end up with a different total count and therefore different number of pages. This can be very confusing from the user perspective - imagine going to the first page and calculating (and displaying) that you have 3 pages of results, then when the user asks for the second page, s/he gets a response with 2 pages and different total count.  "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12793607",
            "date": "2009-12-22T13:00:32+0000",
            "content": "\nThis is exactly the point, it's not really meta-data over the document, but on the group the document belongs to. And you also need a more obvious way to mark this document as a group representation (to distinguish it from other normal documents).\n\nWe show the highest scoring document of a group, so does the fact that the metadata belongs to the group and not the document matter at all?\n\n\nBut extending the current <doc> element, doesn't mean we break BWC. Adding a <collapse-info> (or <collapse-meta-data>) sub element to it, will certainly not break anything, specially when we still don't have a formal xsd for the responses (I know we're working on it, but it's still not out there so it's safe).\n\nWe are not extending anything. We're just adding a couple of fields which may not exist in the index and this is a capability we plan to introduce anyway (however this issue does not need to depend on SOLR-1566). The response format remains exactly the same. There is no break in compatibility. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12793621",
            "date": "2009-12-22T14:07:14+0000",
            "content": "As far as I understand from your collapse algorithm proposal, in order to save memory you'd like to restrict the group creation to only those that belong in the requested results page.\n\nA ton of memory, and probably a good amount of time too.  It may be the only variant that certain people would be able to use (but note that it is just a variant - I'm not proposing doing away with the other options).\n\nI think there might be a problem with pagination as well\n\nYes, pagination is a sticky issue... but I don't think this algorithm messes it up further.\n\nIf we are returning a number of documents (as opposed to a number of groups) to the user, how do they avoid splitting on a page in the middle of the group?  I guess they over-request a little.  What if they want a fixed number of groups?  I guess they over-request by a lot (nGroups*collapse.threshold).  Then they need to keep track of how many documents they actually used.\n\nThe only thing this algorithm can't do (related to pagination) is give the total number of documents after collapsing (and hence can't calculate the exact number of pages).  This can be fine in many circumstances as long as the gui handles it (people don't seem to mind google doing it... I just tried it.  Google didn't show the result count right unless displaying the last page). "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12793644",
            "date": "2009-12-22T15:50:03+0000",
            "content": "Quick note on the collapse cache - we just went into production with 1.4 and right away we had to turn off the collapse cache.  This was with 1.4 dist and the patch from 12/12.  With the cache enabled, RAM consumption was through the roof on the production servers - I guess with the variety of queries coming in, it filled up very fast.   It almost maxed out a machine with 18GB devoted to jetty in about 20 minutes.   We just used the sample config (maxSize=512), it looks like there were about 60 entries in the cache before we restarted.  We would see the memory usage jump by as much as 2% after just one query.\n\nWithout the cache the performance is still quite good (far better than what we had before) so we're not plussed, but it may indicate there needs to be more optimization there...  Generally our consumption rarely goes over 50% on this machine unless we have a lot of commits coming in.  The cache did provide some performance benefits on some of the queries that return large numbers of results (1M+) so it would be nice to have.  Of course, it's possible with our index that these levels of RAM consumption would be unavoidable.  I'm not sure if there's any further specifics I could provide that would be helpful, let me know. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12793808",
            "date": "2009-12-22T21:56:57+0000",
            "content": "It almost maxed out a machine with 18GB devoted to jetty in about 20 minutes. \nHmmm.... that doesn't seem right. This is an issue.\n\nAre you using any extra field collapse features? Such as aggregate functions. Also the collapse groups you collapse on do these have large field values?\nI'm going over the code and re-consider the way stuff is cached right now. "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12793820",
            "date": "2009-12-22T22:28:22+0000",
            "content": "Are you using any extra field collapse features? Such as aggregate functions. Also the collapse groups you collapse on do these have large field values?  I'm going over the code and re-consider the way stuff is cached right now.\n\nNo, we're very simple in our usage of the collapse features themselves, we don't even use the output that the collapse patch adds.  However we do facet on a number of fields in this query as well, and sort by a date field.  We also use local filter queries which we exclude for the facets individually (my favorite new feature).    This packs a lot more action into one query then we had been doing previously (without that, we were running 8+ queries to get the same information), I was worried at first that this was the cause of the ram consumption.  The field we are collapsing on is type \"pint\", it can be positive or negative depending on what system the document is coming in from.  Each document has several stored fields, but a whole document's stored fields are under 1K together, always (it's only image metadata - there's no body text to any of these documents, this is for an image search engine). "
        },
        {
            "author": "ttdi",
            "id": "comment-12793898",
            "date": "2009-12-23T02:22:07+0000",
            "content": "hi,Martijn van Groningen experts,\n    when i use http://localhost:8080/search/?page=1\nthis can collapse the page=1 result,but when i use http://localhost:8080/search/?page=2\nit can only collapse the page=2 result, not collapse all record?\ni want collapse the all record use pagination ,how can i do it?\nThanks! "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12793958",
            "date": "2009-12-23T07:19:33+0000",
            "content": "@ttdi - Please post your questions to solr-user mailing list. This issue is strictly for Solr related development (not usage). "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12794046",
            "date": "2009-12-23T13:12:44+0000",
            "content": "Updated the patch, so it patch without conflicts with the current trunk. Also included a bugfix regarding to field collapsing and filter cache that was noticed by Varun Gupta on the user mailing list. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12794050",
            "date": "2009-12-23T13:25:44+0000",
            "content": "is't the patch built on the one given by shalin? the configuration looks different... "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12794052",
            "date": "2009-12-23T13:36:28+0000",
            "content": "Yes, I used his patch. Made a small bugfix and made sure that is in sync with the latest trunk. "
        },
        {
            "author": "Uri Boness",
            "id": "comment-12794252",
            "date": "2009-12-23T22:42:09+0000",
            "content": "If we are returning a number of documents (as opposed to a number of groups) to the user, how do they avoid splitting on a page in the middle of the group?\n\nAs far as I know (Martijn, correct me if I'm wrong), Martijn's patch returns the number of groups and documents, where each group is actually represented as a document. So in that sense, the total count applies to the result set as is (groups count as documents) and therefore pagination just works. \n\nThe only thing this algorithm can't do (related to pagination) is give the total number of documents after collapsing (and hence can't calculate the exact number of pages). This can be fine in many circumstances as long as the gui handles it (people don't seem to mind google doing it... I just tried it. Google didn't show the result count right unless displaying the last page).\n\nFirst of all, I must admit that I never noticed that in Google, so I guess you're right . But when you think about it, with Google, how many time do you get a low hit count that only fits in 2-3 pages? Well, I hardly ever get it, and when I do I don't even bother to check the result I just try to improve my search. With Solr, a lot of times its different, specially when all these discovery features and faceting are so often used to narrow the search extensively... I'm not saying not having a perfect pagination mechanism is a problem... not at all, I'm just saying that it might be an issue for specific use cases or specific domains.... but that's just an assumption (or a gut feeling)  "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12794374",
            "date": "2009-12-24T09:54:22+0000",
            "content": "\n\tPatch updated for SOLR-1685 and SOLR-1686\n\tThe last patch had reverted changes to CollapseComponent configuration in solrconfig.xml and solrconfig-fieldcollapse.xml. Synced it back\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12795063",
            "date": "2009-12-29T14:43:12+0000",
            "content": "I'm curious as to whether anyone has just thought of using the Clustering component for this? If your \"collapse\" field was a single token, I wonder if you would get the results you're looking for.\n\nI would note, in looking at the Carrot2 code, they actually have a ByFieldClusteringAlgorithm (what they call synthetic clustering) which does field collapsing/clustering on a value of a field.  To quote the javadocs:\n\nClusters documents into a flat structure based on the values of some field of the\ndocuments. By default the \nUnknown macro: {@link Document#SOURCES} \n field is used\nand\n\n\n\tName of the field to cluster by. Each non-null scalar field value with distinct\n\thash code will give raise to a single cluster, named using the\n\tUnknown macro: {@link Object#toString()} \n value of the field. If the field value is a collection,\n\tthe document will be assigned to all clusters corresponding to the values in the\n\tcollection. Note that arrays will not be 'unfolded' in this way.\n\t\n\n\n\nI don't know how it performs, but it seems like it would at least be worth investigating.\n\nNote, they also have a synthetic one for collapsing based on URL: ByUrlClusteringAlgorithm\n\nJust food for thought. "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12795067",
            "date": "2009-12-29T15:18:52+0000",
            "content": "Hi Grant,\n\n\nI would note, in looking at the Carrot2 code, they actually have a ByFieldClusteringAlgorithm (what they call synthetic clustering) which does field collapsing/clustering on a value of a field. To quote the javadocs:\n\nClusters documents into a flat structure based on the values of some field of the documents. By default the {@link Document#SOURCES} field is used and  Name of the field to cluster by. Each non-null scalar field value with distinct hash code will give raise to a single cluster, named using the {@link Object#toString()} value of the field. If the field value is a collection, the document will be assigned to all clusters corresponding to the values in the collection. Note that arrays will not be 'unfolded' in this way.\n\nI don't know how it performs, but it seems like it would at least be worth investigating.\n\nCarrot2's ByFieldClusteringAlgorithm is very simple. It literally throws everything into a hash map based on the field value (source code). This algorithm is used in our live demo to cluster by news source.\n\n\nNote, they also have a synthetic one for collapsing based on URL: ByUrlClusteringAlgorithm\n\nThis one creates a hierarchy based on the URL segments and might be useful to create \"by-domain\" collapsing if needed.\n\nIn general, my rough guess is that it's the criteria for content-based collapsing would be closer to duplicate detection rather than the type of grouping Carrot2 produces. "
        },
        {
            "author": "Patrick Jungermann",
            "id": "comment-12797716",
            "date": "2010-01-07T18:06:17+0000",
            "content": "Hi all,\n\nwe using the Solr's trunk with the latest patch of 2009-12-24 09:54 AM. Within the index, there are ~3.5 million documents with string-based identifiers of a length up to 50 chars.\n\nThe result document of our prefix query, which was at position 1 without collapsing, was with collapsing not even within the top 10 results. We using the option collapse.maxdocs=150 and after changing this option to the value 15000, the results seem to be as expected. Because of that, we concluded, that there has to be a problem with the sorting of the uncollapsed docset.\n\n\nAlso, we noticed a huge memory leak problem, when using collapsing. We configured the component with <searchComponent name=\"query\" class=\"org.apache.solr.handler.component.CollapseComponent\"/>.\nWithout setting the option collapse.field, it works normally, there are far no memory problems. If requests with enabled collapsing are received by the Solr server, the whole memory (oldgen could not be freed; eden space is heavily in use; ...) gets full after some few requests. By using a profiler, we noticed that the filterCache was extraordinary large. We supposed that there could be a caching problem (collapeCache was not enabled).\n\n\nAdditionally it might be very useful, if the parameter collapse=true|false would work again and could be used to enabled/disable the collapsing functionality. Currently, the existence of a field choosen for collapsing enables this feature and there is no possibility to configure the fields for collapsing within the request handlers. With that, we could configure it and only enable/disable it within the requests like it will be conveniently used by other components (highlighting, faceting, ...).\n\n\nPatrick "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12797794",
            "date": "2010-01-07T21:27:15+0000",
            "content": "The result document of our prefix query, which was at position 1 without collapsing, was with collapsing not even within the top 10 results. We using the option collapse.maxdocs=150 and after changing this option to the value 15000, the results seem to be as expected. Because of that, we concluded, that there has to be a problem with the sorting of the uncollapsed docset.\n\nThe collapse.maxdocs aborts collapsing after the threshold is met, but it is doing that based on the uncollapsed docset which is not sorted in any way. The result of that is that documents that would normally appear in the first page don't appear at all in the search result. Eventually the collapse component uses the collapsed docset as the result set and not the uncollapsed docset.\n\nAlso, we noticed a huge memory leak problem, when using collapsing. We configured the component with <searchComponent name=\"query\" class=\"org.apache.solr.handler.component.CollapseComponent\"/>. Without setting the option collapse.field, it works normally, there are far no memory problems. If requests with enabled collapsing are received by the Solr server, the whole memory (oldgen could not be freed; eden space is heavily in use; ...) gets full after some few requests. By using a profiler, we noticed that the filterCache was extraordinary large. We supposed that there could be a caching problem (collapeCache was not enabled).\n\nI agree it gets huge. This applies for both the filterCache and field collapse cache. This is something that has to be addressed and certainly will in the new field-collapse implementation. In the patch you're using too much is being cached (some data can even be neglected in the cache). Also in some cases strings are being cached that actually could be replaced with hashcodes.\n\nAdditionally it might be very useful, if the parameter collapse=true|false would work again and could be used to enabled/disable the collapsing functionality. Currently, the existence of a field choosen for collapsing enables this feature and there is no possibility to configure the fields for collapsing within the request handlers. With that, we could configure it and only enable/disable it within the requests like it will be conveniently used by other components (highlighting, faceting, ...).\n\nThat actually makes sense for using the collapse.enable parameter again in the patch. \n\nMartijn "
        },
        {
            "author": "Kevin Cunningham",
            "id": "comment-12799409",
            "date": "2010-01-12T21:59:01+0000",
            "content": "Which patch is recommended for those running a stock 1.4 release?   "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12799692",
            "date": "2010-01-13T11:06:33+0000",
            "content": "I believe the field-collapse-5.patch should work for 1.4. Some bugs were fixed in later patches so I recommend using the latest patch on the latest successful nightly build if that is an option for you. \nApplying the latest patch on the 1.4 sources will properly result in some minor merge errors, but I think these should be easy the fix.  "
        },
        {
            "author": "Michael Gundlach",
            "id": "comment-12801987",
            "date": "2010-01-18T23:15:44+0000",
            "content": "I've found the need to collapse on an analyzed field which contains one token (an email field, which is analyzed in order to lowercase it.)  I had to apply a patch on top of field-collapse-5.patch in order to comment out the isTokenized() check in AbstractCollapseComponent.java , at which point the code worked perfectly.\n\nIs there a strong argument for keeping the isTokenized() check in?  Anyone who needs to collapse an analyzed, single-token field is out of luck with this check in place.  I understand that the current version protects users from incorrect results if they collapse a multi-token tokenized field, but maybe collapsing on analyzed fields is worth that risk.  (Or someone could come after me and write a patch that checks for multi-tokened fields somehow and throws an exception.) "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12802186",
            "date": "2010-01-19T10:46:37+0000",
            "content": "If the field is tokenized and has more than one token your field collapse result will become incorrect. What happens if I remember correctly is that it will only collapse on the field's last token. This off course leads to weird collapse groups. For the users that only have one token per collapse field are because of this check out of luck. Somehow I think we should make the user know that is not possible to collapse on a tokenized field (at least with multiple tokens). Maybe adding a warning in the response. Still I think the exception is more clear, but also prohibits it off course. \n\nOr someone could come after me and write a patch that checks for multi-tokened fields somehow and throws an exception.\nChecking if a tokenized field contains only one token is really inefficient, because you have the check all every collapse field of all documents. Now do check is done based on the field's definition in the schema. "
        },
        {
            "author": "Yaniv S.",
            "id": "comment-12802334",
            "date": "2010-01-19T16:59:42+0000",
            "content": "Hi All, this is a very exciting feature and I'm trying to apply it on our system.\nI've tried patching on 1.4 and on the trunk version but both give me build errors.\nAny suggestions on how I can build 1.4 or latest with this patch?\n\nMany Thanks,\nYaniv "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12802512",
            "date": "2010-01-19T22:39:18+0000",
            "content": "Hi Yaniv, I tried the same on 1.4 branch (from svn) and the svn trunk. Applying the patch on both sources went fine, but when building (ant dist) on trunk I also got compile errors. This had to do with that SolrQueryResponse changed package from request package to response package. I will update the patch shortly. Building on the 1.4 branch went without any problems (ant dist). What errors did occur when running ant dist on 1.4 branch? "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12802993",
            "date": "2010-01-20T21:16:23+0000",
            "content": "Attached updated patch that works with the latest trunk. This patch is not compatible with 1.4 branch. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12828039",
            "date": "2010-02-01T09:06:37+0000",
            "content": "A random comment, don't we need to check collapse.field is indexed in checkCollapseField()?\n\n\nprotected void checkCollapseField(IndexSchema schema) {\n  SchemaField schemaField = schema.getFieldOrNull(collapseField);\n  if (schemaField == null) {\n    throw new RuntimeException(\"Could not collapse, because collapse field does not exist in the schema.\");\n  }\n\n  if (schemaField.multiValued()) {\n    throw new RuntimeException(\"Could not collapse, because collapse field is multivalued\");\n  }\n\n  if (schemaField.getType().isTokenized()) {\n    throw new RuntimeException(\"Could not collapse, because collapse field is tokenized\");\n  }\n}\n\n\n\nI accidentally specified an unindexed field for collapse.field, I got unexpected result without any errors. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12828285",
            "date": "2010-02-01T21:50:05+0000",
            "content": "I agree! I've updated the patch that adds a check if a field is indexed. If not an exception is thrown. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12829522",
            "date": "2010-02-04T08:10:37+0000",
            "content": "The following snippet in CollapseComponent.doProcess():\n\n\nDocListAndSet results = searcher.getDocListAndSet(rb.getQuery(),\n      collapseResult == null ? rb.getFilters() : null,\n      collapseResult.getCollapsedDocset(),\n      rb.getSortSpec().getSort(),\n      rb.getSortSpec().getOffset(),\n      rb.getSortSpec().getCount(),\n      rb.getFieldFlags());\n\n\n\n2nd line implies that collapseResult may be null. If it is null, we got NPE at 3rd line? "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12829727",
            "date": "2010-02-04T19:40:54+0000",
            "content": "If you look into the AbstractDocumentCollapser#createDocumentCollapseResult() you will see that the collapseResult will never be null. Therefore I think the null check is not necessary. \nIt think the following code is sufficient:\n\nDocListAndSet results = searcher.getDocListAndSet(rb.getQuery(),\n      collapseResult.getCollapsedDocset(),\n      rb.getSortSpec().getSort(),\n      rb.getSortSpec().getOffset(),\n      rb.getSortSpec().getCount(),\n      rb.getFieldFlags());\n\n\nAlso specifying the filters is unnecessary, because it was already taken into account when creating the uncollapsed docset.  "
        },
        {
            "author": "Kevin Cunningham",
            "id": "comment-12830305",
            "date": "2010-02-05T21:29:59+0000",
            "content": "Regarding Patrick's comment about a memory leak, we are seeing something similar - very large memory usage and eventually using all the available memory.  Were there any confirmed issues that may have been addressed with the later patches?  We're using the 12-24 patch.  Any toggles we can switch to still get the feature, yet minimize the memory footprint?\n\nWe had been running the 11-29 field-collapse-5.patch patch and saw nothing near this amount of memory consumption.\n\nWhat fixes would we be missing if ran Solr 1.4 with the last \"field-collapse-5.patch\" patch? "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12831150",
            "date": "2010-02-08T21:56:01+0000",
            "content": "Regarding Patrick's comment about a memory leak, we are seeing something similar - very large memory usage and eventually using all the available memory. Were there any confirmed issues that may have been addressed with the later patches? We're using the 12-24 patch. Any toggles we can switch to still get the feature, yet minimize the memory footprint?\nAre you using any other features besides plain collapsing? The field collapse cache gets large very quickly, I suggest you turn it off (if you are using it). Also you can try to make your filterCache smaller. \n\nWhat fixes would we be missing if ran Solr 1.4 with the last \"field-collapse-5.patch\" patch?\nNot much I believe, some are using it in production without too many problems. "
        },
        {
            "author": "Kevin Cunningham",
            "id": "comment-12831617",
            "date": "2010-02-09T19:38:11+0000",
            "content": "No, just field collapsing.  We went back to the field-collapse-5.patch for the time being.  So far its been good and we updated just to get closer to the latest not because we were seeing issues.  Thanks. "
        },
        {
            "author": "Gerald DeConto",
            "id": "comment-12832703",
            "date": "2010-02-11T22:15:46+0000",
            "content": "I have been able to apply and use the solr-236 patch successfully. Very, very cool and powerful. \n\nAre there any plans/hacks to include the non-collapsed document in the collapseCount and aggregate function values (ie so that it includes ALL documents, not just the collapsed ones)?  Possibly via some parameter like collapse.includeAllDocs? \n\nI think this would be a great addition to the collapse code (and solr functionality), via what I would think is a small change, since solr doesnt have any other aggregation mechanism (as yet).\n\nAm trying to see how to change the code myself but Java is not my primary language. "
        },
        {
            "author": "Peter Karich",
            "id": "comment-12835230",
            "date": "2010-02-18T14:49:48+0000",
            "content": "We are facing OutOfMemory problems too. We are using https://issues.apache.org/jira/secure/attachment/12425775/field-collapse-5.patch\n\n> Are you using any other features besides plain collapsing? The field collapse cache gets large very quickly,\n> I suggest you turn it off (if you are using it). Also you can try to make your filterCache smaller.\n\nHow can I turn off the collapse cache or make the filterCache smaller?\nAre there other workarounds? E.g. via using a special version of the patch ?\n\nI read that it could help to specify collapse.maxdocs but this didn't help in our case ... could collapse.type=adjacent help here?  (https://issues.apache.org/jira/browse/SOLR-236?focusedCommentId=12495376&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12495376)\n\nWhat do you think?\n\nBTW: We really like this patch and would like to use it !!  "
        },
        {
            "author": "Peter Karich",
            "id": "comment-12835258",
            "date": "2010-02-18T16:05:49+0000",
            "content": "Trying the latest patch from 1th Feb 2010. It compiles against solr-2010-02-13 from nightly build dir, but does not work. If I query \n\nhttp://server/solr-app/select?q=*:*&collapse.field=myfield\n\nit fails with: \n\n\n \n\nHTTP Status 500 - null java.lang.NullPointerException at org.apache.solr.schema.FieldType.toExternal(FieldType.java:329) at \norg.apache.solr.schema.FieldType.storedToReadable(FieldType.java:348) at \norg.apache.solr.search.fieldcollapse.collector.AbstractCollapseCollector.getCollapseGroupResult(AbstractCollapseCollector.java:58) at \norg.apache.solr.search.fieldcollapse.collector.DocumentGroupCountCollapseCollectorFactory$DocumentCountCollapseCollector.getResult(DocumentGroupCountCollapseCollectorFactory.ja\nva:84) at org.apache.solr.search.fieldcollapse.AbstractDocumentCollapser.getCollapseInfo(AbstractDocumentCollapser.java:193) at \norg.apache.solr.handler.component.CollapseComponent.doProcess(CollapseComponent.java:192) at \norg.apache.solr.handler.component.CollapseComponent.process(CollapseComponent.java:127) at \norg.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:195) at\n...\n \n \n\n\nI only need the OutOfMemory problem solved ...  "
        },
        {
            "author": "Leon Messerschmidt",
            "id": "comment-12839545",
            "date": "2010-03-01T03:38:11+0000",
            "content": "The OutOfMemory problem affects both field-collapse-5.patch on Solr 1.4 and SOLR-236.patch on the trunk.\n\nThe root cause of the problem is DocSetScoreCollector that creates an array of float that is the size of the maxID document that matches the query.  If you have a large index (we have several million documents) and a document with a very large id is matched you may end up with a huge array (in our case several hundred MB).  Only a really small subset of the array is being used at any given time (especially if you're matching just a few documents with big doc ids).  \n\nThe implementation can rather use a sparse array or a map to keep track of scores. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12839657",
            "date": "2010-03-01T11:58:46+0000",
            "content": "That makes sense. I initially made it an array to maintain the document order for the scores, but this order is already in the openbitset. I think a Map is a good idea.  "
        },
        {
            "author": "Yao Ge",
            "id": "comment-12840470",
            "date": "2010-03-03T05:18:20+0000",
            "content": "I just applied the latest patch to trunk and I don't quite understand how the \"numFound\" in the response list is computed. With rows=10&collapse.threshold=1, I got numFound=11, with rows=10&collapse.threshold=2, I got numFound=22.\nI both cases the actual doc in the list is 10. Why is the numFound reported this way? "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12840648",
            "date": "2010-03-03T13:14:16+0000",
            "content": "The numFound attribute holds the total number of documents found for the specified query, so also the documents beyond the first result page. The reason that for the first query, the numFound is lower the the second query is that the collapse.threshold is higher. Only documents with the same collapse field value, that appear more then twice will be omitted from the result. This results in less document being collapsed. "
        },
        {
            "author": "Peter Karich",
            "id": "comment-12841147",
            "date": "2010-03-04T09:46:17+0000",
            "content": "regarding the OutOfMemory problem: we are now testing the suggested change in production.\n\nI replaced the float array with a TreeMap<Integer, Float>. The change was nearly trivial (I cannot provide a patch easily, because we are using an older patch, althoug I could post the 3 changed files.)\n\nThe point why I used a TreeMap instead a HashMap was that in the method advance in the class NonAdjacentDocumentCollapser.PredefinedScorer I needed the tailMap method:\n\n\npublic int advance(int target) throws IOException {\n            // now we need a treemap method:\n            iter = scores.tailMap(target).entrySet().iterator();\n            if (iter.hasNext())\n                return target;\n            else\n                return NO_MORE_DOCS;\n}\n\n \n\nThen -  I think - I discovered a bug/inconsistent behaviour: If I run the test FieldCollapsingIntegrationTest.testNonAdjacentCollapse_withFacetingBefore then the scores arrays will be created ala new float[maxDocs] in the old version. But the array will never be filled with some values so Float value1 = values.get(doc1); will return null in the method NonAdjacentDocumentCollapser.FloatValueFieldComparator.compare (the size of TreeMap is 0!); I work around this via \n\n\n \nif (value1 == null)\n                value1 = 0f;\nif (value2 == null)\n                value2 = 0f;\n\n \n\nI think the compare method should NOT be called if no docs are in the scores array ... ? "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12841558",
            "date": "2010-03-04T22:04:36+0000",
            "content": "Shouldn't the float array in DocSetScoreCollector be changed to a Map? Because that is actually being cached and requires the most memory. The float array in the NonAdjacentDocumentCollapser.PredefinedScorer isn't being cached. Though changing this to a Map can be an improvement. \n\nI think the compare method should NOT be called if no docs are in the scores array ... ?\nI would expect that every docId has a score. "
        },
        {
            "author": "Peter Karich",
            "id": "comment-12841752",
            "date": "2010-03-05T08:47:39+0000",
            "content": "> Shouldn't the float array in DocSetScoreCollector be changed to a Map?\n\nhmmh, maybe I expressed myself a bit weird: I already changed this all to a Map (a SortedMap) ... \nI started this change in DocSetScoreCollector and changed all the other occurances of the float array (otherwise I would have to copy the entire map)\n\n> > I think the compare method should NOT be called if no docs are in the scores array ... ?\n\n> I would expect that every docId has a score.\n\nYes, me too. So I expect there is somewhere a bug. But as I sayd this breaks only one test (collapse with faceting before). It could be even a but in the testcase though. "
        },
        {
            "author": "Peter Karich",
            "id": "comment-12841756",
            "date": "2010-03-05T08:52:15+0000",
            "content": "It seems to me that the provided changes are necessary to make the OutOfMemory exception gone (see appended 3 files). Please apply the files with caution, because I made the changes from an old patch (from Nov 2009) "
        },
        {
            "author": "Robert Zotter",
            "id": "comment-12848978",
            "date": "2010-03-24T00:20:32+0000",
            "content": "What are the required steps to get this patch working with a clean 1.4? Is it even compatible? I've read in the above comments that the 12/12 field-collapse-5.patch will patch correctly but it has horrible memory bugs. Has there been any updates on this? Recommendations anyone? "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12850756",
            "date": "2010-03-28T23:36:20+0000",
            "content": "I've attached a new patch, which included the following changes:\n\n\tPatch uses the new Solr trunk. Everything in the patch is relative to the trunk directory.\n\tThe changes Peter Karich made to DocSetScoreCollector and NonAdjacentDocumentCollapserTest that make it much more memory efficient.\n\tThe change Yonik suggested to make field collapsing more efficient.\nefficiency: the treeset and hashmap are both only the size of the top number of docs we are looking at (10 for instance) We will now have the top 10 documents collapsed by the right field with a collapseCount of 1. Put another way, we have the top 10 groups.\n\n\n\nThis also means that the total count of a search with field collapsing does not represent all the found documents. The total count now represents: start + count "
        },
        {
            "author": "Thomas Heigl",
            "id": "comment-12850921",
            "date": "2010-03-29T14:24:57+0000",
            "content": "@Martijn:\n\nThere is a small problem with the latest patch file. Both TortoiseSVN and patch complain that the file is malformed because there is an \"empty\" patch for FieldCollapseResponse.java around line 2199. Simply removing lines 2195-2199 does the trick.\n\nApart from that, the patch works perfectly for me. "
        },
        {
            "author": "Thomas Heigl",
            "id": "comment-12850925",
            "date": "2010-03-29T14:30:23+0000",
            "content": "@Robert:\n\nI just tried the field collapsing patch with a clean version of the 1.4 release. The only recent patch that seems to be applicable without manually resolving conflicts is 2009-12-08. In addition to the patch you should also add the three individual files uploaded by Peter Karich to deal with the worst memory issues. "
        },
        {
            "author": "Robert Zotter",
            "id": "comment-12850930",
            "date": "2010-03-29T14:42:57+0000",
            "content": "@Thomas. Thanks for the input. Do you think its best to go with a clean version of 1.4 or the latest from trunk? Basically I'm asking if you think trunk is semi-stable enough for a production environment. Thanks "
        },
        {
            "author": "Thomas Heigl",
            "id": "comment-12850934",
            "date": "2010-03-29T14:51:30+0000",
            "content": "@Robert:\n\nWhat is your use case for field collapsing? I think under \"normal\" conditions (collapsing on a field with reasonably many unique values) you can go with the slightly older patch and the OOM fixes. I compared the performance of the newest patch for the trunk with the 1.4 release patched as described above and didn't notice much difference under these conditions. I will must likely go with the trunk, however, as I have millions of documents with millions of unique values on the collapse field and need every bit of performance I can get. "
        },
        {
            "author": "Robert Zotter",
            "id": "comment-12850944",
            "date": "2010-03-29T15:30:28+0000",
            "content": "@Thomas Essentially my use case involves a product listing of sorts whereas there are many closely related items being sold by any number of sellers. I would like to distribute the search results across as many sellers as possible giving each seller a fair chance to sell their products, so I was going to use field collapsing to limit the number of items being displayed per seller.\n\nIdeally it would be nice if there were some way to evenly distribute closely related documents (scores within some defined percentage of each other)\n\nFor example instead of:\n\nItem 1 sold by Seller A\nItem 2 sold by Seller A\nItem 3 sold by Seller A\nItem 4 sold by Seller B\nItem 5 sold by Seller B\nItem 6 sold by Seller B\n\nAssuming all of these ideas are within a certain percentage of each other it would be nice to have:\n\nItem 1 sold by Seller A\nItem 4 sold by Seller B\nItem 2 sold by Seller A\nItem 5 sold by Seller B\n....\n\nAlthough I do not achieve this exact behavior with this particular patch It will at least get me closer to my goal.\n\nFYI my document count is around 6 million and I am already utilizing the document deduper. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12851094",
            "date": "2010-03-29T21:08:23+0000",
            "content": "@Thomas\nSomehow the solrj code was left out the when I created the patch yesterday. I guess, I accidentally deleted it when I was moving the code the new trunk. Anyhow I have updated the patch that includes the solrj code and applying it should go flawless. "
        },
        {
            "author": "Pierre-Luc",
            "id": "comment-12855469",
            "date": "2010-04-09T16:49:17+0000",
            "content": "Hi all,\n\nWe have integrated the most recent patch into our 1.4 install and the Out of memory fix suggested by Peter. I am facing memory issues only when collapsing. I would like to know why the class CacheValue is static in AbstractDocumentCollapser. If I remove the static attribute of that class, the memory footprint is greatly reduced and everything works fine.\n\nMy document count is around 5 million.\n\nAny help would be greatly appreciated.\nThank you. "
        },
        {
            "author": "Claus Schr\u00f6ter",
            "id": "comment-12857706",
            "date": "2010-04-16T06:59:56+0000",
            "content": "Hi all,\n\nI applied Martijns last Patch to the trunk and encountered a problem with document counts:\n\nwhenever I set the rows= value to the query, the \"numFound\" result parameter is limited to exactly the value of rows.\nThe facet counts are also limited to this value.\n\nIf I omit the rows parameter everything is fine. I tried to track back the problem. It seems that the SolrSearcher query is limited to \"rows\" value\nbefore collapsing is done.\n\nDoes anybody encounter a similar problem?\n\nCheers!\nclausi "
        },
        {
            "author": "Billy Morgan",
            "id": "comment-12858333",
            "date": "2010-04-18T20:11:41+0000",
            "content": "@Claus\n\nI am having the same issue "
        },
        {
            "author": "Lukas Kahwe Smith",
            "id": "comment-12861420",
            "date": "2010-04-27T14:59:07+0000",
            "content": "Its my understanding that this patch atm only produces a score for each collapsed group of the max() of the scores inside the group. Is there any work being done to increase the score to include all documents inside the group? Like taking into account the collapse_count or the individual scores (summing or via some custom algorithm).\n "
        },
        {
            "author": "Karel Braeckman",
            "id": "comment-12862181",
            "date": "2010-04-29T12:31:35+0000",
            "content": "Hi all, \n\nI wondered if it is possible to sort the collapsed results based on an aggregate function (e.g., sort by sum(price))? \n\nWhat is to be done to make this possible? (could it be done via a plugin? )\n\nKind regards,\nKarel "
        },
        {
            "author": "Eric Caron",
            "id": "comment-12862327",
            "date": "2010-04-29T18:27:25+0000",
            "content": "Using the latest from trunk as of 2010-04-29, and the SOLR-236-trunk.patch from 2010-03-29 05:08, I get a nullpointerexception whenever I use collapse.field and a fq.\n\nWorks:\n/solr/select/?q=sales&fq=country%3A1\nWorks:\n/solr/select/?q=sales&collapse.field=company\nDoesn't work:\n/solr/select/?q=sales&collapse.field=company&fq=country%3A1\n\nThe top of the trace is:\njava.lang.NullPointerException\n\tat org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser$FloatValueFieldComparator.compare(NonAdjacentDocumentCollapser.java:450)\n\tat org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser$DocumentComparator.compare(NonAdjacentDocumentCollapser.java:262)\n\tat org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser$DocumentPriorityQueue.lessThan(NonAdjacentDocumentCollapser.java:196)\n\tat org.apache.lucene.util.PriorityQueue.insertWithOverflow(PriorityQueue.java:148)\n\tat org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser.doCollapsing(NonAdjacentDocumentCollapser.java:113)\n\tat org.apache.solr.search.fieldcollapse.AbstractDocumentCollapser.executeCollapse(AbstractDocumentCollapser.java:259)\n\tat org.apache.solr.search.fieldcollapse.AbstractDocumentCollapser.collapse(AbstractDocumentCollapser.java:179)\n\tat org.apache.solr.handler.component.CollapseComponent.doProcess(CollapseComponent.java:173)\n\tat org.apache.solr.handler.component.CollapseComponent.process(CollapseComponent.java:127)\n\tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:195) "
        },
        {
            "author": "Sergey Shinderuk",
            "id": "comment-12867121",
            "date": "2010-05-13T12:07:07+0000",
            "content": "@Claus\nI faced the same issue. Did you find any solution or maybe workaround?\n\nWhen collapsing is enabled, numFound is equal to the number of rows requested and NOT the total number of distinct documents found.\n\nI applied the latest SOLR-236-trunk.patch to the trunk checked out on the date of patch, because patching the latest revision fails.\nAm I doing something wrong?\n\nI want to collapse near-duplicate documents in search results based on document signature. But with this issue I can't paginate through results, because I don't know how many.\n\nBesides, an article at http://blog.jteam.nl/2009/10/20/result-grouping-field-collapsing-with-solr/ shows examples with correct numFound returned. How can I get it working??? "
        },
        {
            "author": "Sergey Shinderuk",
            "id": "comment-12867227",
            "date": "2010-05-13T19:32:55+0000",
            "content": "Finally I applied SOLR-236.patch to rev 899572 (dtd. 2010-01-15) of the trunk and I get correct numFound values with collapsing enabled. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12867673",
            "date": "2010-05-14T20:56:31+0000",
            "content": "I've updated the patch for the trunk The following changes are included:\n\n\tThe patch has been updated to the latest trunk. So no patch conflicts should occur.\n\tEric Caron reported NPEs when using field collapsing in combination with a filter query. After some digging I found the cause of the NPE. When using a fq the scores are being  cached in the filter cache, but due to a bug in DelegateDocSet the scores where not returned in some cases (null was returned). This resulted in a NPE in a later stage of the query execution. I've also updated the integration test to cover this situation. This also explains why the first time everything was fine. When doing a normal refresh (F5 / \uf8ff - R) the result comes from the HTTP cache so everything is still fine. However when doing a hard refresh a second query is executed and result are then retrieved from the Solr configured caches in most cases and resulting in this NPE.\n\n "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12867680",
            "date": "2010-05-14T21:08:34+0000",
            "content": "Another note. The numFound count (all document found) in this patch does mean all documents found. This number currently represents all documents returned in the response. This is due to a performance improvement made and was discussed on this page a while ago. However you can disable this performance improvement by commenting / deleting the lines 99 and 106 to 110 in NonAdjacentDocumentCollapser.java file (latest patch). My experiences with this improvement is that it saves memory, but the the search time improvements were minimal. So whether you do this I guess depends in your situation. "
        },
        {
            "author": "Joseph Freeman",
            "id": "comment-12867685",
            "date": "2010-05-14T21:33:11+0000",
            "content": "collapse.includeCollapsedDocs.count ? \n\nWhen I use collapse.includeCollapsedDocs.fl, I get ALL the collapsed documents. \n\nIt seems like we should have a collapse.includeCollapsedDocs.count parameter to limit this result set?\n "
        },
        {
            "author": "Eric Caron",
            "id": "comment-12867691",
            "date": "2010-05-14T21:42:12+0000",
            "content": "Regarding the numFound count, one of the loudest complaints from the Sphinx community is the inability to see the total number pre-collapse. Is it possible to dictate which value (possibly both) is calculated at run-time? When FieldCollapse gains the attention it deserves, I'd expect an onslaught of requests along these lines. (I personally want both, the pre-value to display the number of matches, and the post-value to calculate pagination). "
        },
        {
            "author": "Varun Gupta",
            "id": "comment-12867855",
            "date": "2010-05-15T11:36:04+0000",
            "content": "I applied the latest patch on the trunk and got the below exception after I made some commits to the index:\n\nSEVERE: java.lang.NullPointerException\n\tat org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser$FlatValueFieldComparator.compare(NonAdjacentDocumentCollapser.java:450)\n\tat org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser$DoumentComparator.compare(NonAdjacentDocumentCollapser.java:262)\n\tat org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser$DoumentPriorityQueue.lessThan(NonAdjacentDocumentCollapser.java:196)\n\tat org.apache.lucene.util.PriorityQueue.upHeap(PriorityQueue.java:221)\n\tat org.apache.lucene.util.PriorityQueue.add(PriorityQueue.java:130)\n\tat org.apache.lucene.util.PriorityQueue.insertWithOverflow(PriorityQueu.java:146)\n\tat org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser.doollapsing(NonAdjacentDocumentCollapser.java:113)\n\tat org.apache.solr.search.fieldcollapse.AbstractDocumentCollapser.execueCollapse(AbstractDocumentCollapser.java:259)\n\tat org.apache.solr.search.fieldcollapse.AbstractDocumentCollapser.collase(AbstractDocumentCollapser.java:179)\n\tat org.apache.solr.handler.component.CollapseComponent.doProcess(CollapeComponent.java:173)\n\tat org.apache.solr.handler.component.CollapseComponent.process(Collapseomponent.java:127)\n\tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SerchHandler.java:195)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHanderBase.java:131)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1321)\n\tat org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilte.java:341)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFiltr.java:244)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(AppicationFilterChain.java:235)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationilterChain.java:206)\n\nI also got an error while doing optimizing index. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12867946",
            "date": "2010-05-16T03:29:56+0000",
            "content": "Eric Caron added a comment - 29/Apr/10 02:27 PM \nUsing the latest from trunk as of 2010-04-29, and the SOLR-236-trunk.patch from 2010-03-29 05:08, I get a nullpointerexception whenever I use collapse.field and a fq. \n\nVarun Gupta added a comment - 15/May/10 07:36 AM \nI applied the latest patch on the trunk and got the below exception after I made some commits to the index: \n\nEric, Varun: Please create unit tests that show these bugs.\n "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12868004",
            "date": "2010-05-16T16:05:21+0000",
            "content": "Varun I noticed the same NPE. I've updated the patch and fixed the issue. In the patch I've also added a test that simulated the problem that you have described. "
        },
        {
            "author": "Kallin Nagelberg",
            "id": "comment-12871168",
            "date": "2010-05-25T14:37:10+0000",
            "content": "I tried asking this question on the user list, but perhaps this is a more appropriate forum. \n\nAs I understand field collapsing has been disabled on multi-valued fields. Is this really necessary?\n\nLet's say I have a multi-valued field, 'my-mv-field'. I have a query like (my-mv-field:1 OR my-mv-field:5) that returns docs with the following values for 'my-mv-field':\n\nDoc1: 1, 2, 3, \nDoc2: 1, 3 \nDoc3: 2, 4, 5, 6\nDoc4: 1\n\nIf I collapse on that field with that query I imagine it should mean 'collect the docs, starting from the top, so that I find 1 and 5'. In this case if it returned Doc1 and Doc3 I would be happy. \n\nThere must be some ambiguity or implementation detail I am unaware that is preventing this. It may be a critical piece of functionality for an application I'm working on, so I'm curious if there is point in pursuing development of this functionality or if I am missing something.\n\nThanks,\nKallin Nagelberg "
        },
        {
            "author": "Christophe Biocca",
            "id": "comment-12871470",
            "date": "2010-05-26T00:56:27+0000",
            "content": "I'd just like to throw in a suggestion about the AbstractDocumentCollapser & CollapseCollectorFactory APIs: It seems to me that changing the factory.createCollapseCollector(SolrRequest req) to factory.createCollapseCollector(ResponseBuilder rb) would allow for more specialized collapse collectors, that would be able to use, amongst other things, the SortSpec in the implementation of the collector. Our use case is that we want to show possibly more than one document for a given value of a collapse field, depending on relative scores. Passing in the ResponseBuilder would allow us to do that much more easily. Since the caching uses the ResponseBuilder object as its key, it won't introduce any new issues. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12872436",
            "date": "2010-05-27T22:05:13+0000",
            "content": "Bulk updating 240 Solr issues to set the Fix Version to \"next\" per the process outlined in this email...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E\n\nSelection criteria was \"Unresolved\" with a Fix Version of 1.5, 1.6, 3.1, or 4.0.  email notifications were suppressed.\n\nA unique token for finding these 240 issues in the future: hossversioncleanup20100527 "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12877576",
            "date": "2010-06-10T20:58:21+0000",
            "content": "It's the three-year anniversary for SOLR-236! And it's still active, unfinished and uncommitted. Is this a record?\n "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12879898",
            "date": "2010-06-17T19:08:34+0000",
            "content": "I've attached a new patch that is compatible with the current trunk (rev 955615). The reason the previous patch did not work, was that the StringIndex class was removed. DocTermsIndex is used instead. See LUCENE-2380 for more details on this. "
        },
        {
            "author": "Doug Steigerwald",
            "id": "comment-12882299",
            "date": "2010-06-24T19:39:46+0000",
            "content": "I keep running into an ArrayIndexOutOfBoundsException when sorting with field collapsing.  I'm running Solr 1.4.1 with the field-collapse-5.patch along with the 3 files from Peter for OOM issues.\n\nWe've got a basic query that returns all event type records in the index (object_class:events), and one fq to make sure we're grabbing data for the correct site (site_id:86).  I'm sorting on a category_id (TrieIntField).  Collapsing on a string (collapse.type=normal).  Here's a basic query that doesn't work for us.\n\nq=object_class:events&fq=site_id:86&sort=category_id+desc&collapse.field=rollup&collapse.type=normal\n\nJun 24, 2010 3:20:12 PM org.apache.solr.common.SolrException log\nSEVERE: java.lang.ArrayIndexOutOfBoundsException: -4294\n\tat org.apache.lucene.search.FieldComparator$IntComparator.copy(FieldComparator.java:328)\n\tat org.apache.lucene.search.TopFieldCollector$OutOfOrderOneComparatorNonScoringCollector.collect(TopFieldCollector.java:133)\n\tat org.apache.solr.search.SolrIndexSearcher.sortDocSet(SolrIndexSearcher.java:1487)\n\tat org.apache.solr.search.SolrIndexSearcher.getDocListC(SolrIndexSearcher.java:931)\n\tat org.apache.solr.search.SolrIndexSearcher.getDocListAndSet(SolrIndexSearcher.java:1289)\n\tat org.apache.solr.handler.component.CollapseComponent.doProcess(CollapseComponent.java:176)\n\tat org.apache.solr.handler.component.CollapseComponent.process(CollapseComponent.java:127)\n\tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:195)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1316)\n\n\nThis is happening to one of our sites in production (the only site left using our events calendar) and I can't seem to make it happen in development with some fake data.  We wiped all data from our production indexes and reindexed recently (upgraded to Solr 1.4.0 a few weeks ago).  Does anyone have any ideas what might be causing this?  I'm going to try and pull the database to our development servers and see if I can reindex and reproduce the issue, but that will take some time.  The copied index from production to development does show this issue.\n\nAny hints?  This is happening when sorting on any TrieIntField or string field.  Normal collapsing or adjacent. "
        },
        {
            "author": "Jasper van Veghel",
            "id": "comment-12883106",
            "date": "2010-06-28T11:17:02+0000",
            "content": "I'm getting the same Exception as Eric Caron, only without using an fq. It seems to have something to do with caching and potentially stemming.\n\nThese queries are being run against a set of Dutch political news articles. The following works:\n\n/select?q=rosenthal&collapse.field=url_exact\n\nAnd this doesn't:\n\n/select?q=roos&collapse.field=url_exact\n\nWhere due to stemming 'roos' is also highlighted in results for the former query; hence the (expected) results for the latter query are a subset of the former. The exception is:\n\njava.lang.NullPointerException\n\tat org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser$FloatValueFieldComparator.compare(NonAdjacentDocumentCollapser.java:451)\n\tat org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser$DocumentComparator.compare(NonAdjacentDocumentCollapser.java:263)\n\tat org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser$DocumentPriorityQueue.lessThan(NonAdjacentDocumentCollapser.java:197)\n\tat org.apache.lucene.util.PriorityQueue.insertWithOverflow(PriorityQueue.java:148)\n\tat org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser.doCollapsing(NonAdjacentDocumentCollapser.java:114)\n\tat org.apache.solr.search.fieldcollapse.AbstractDocumentCollapser.executeCollapse(AbstractDocumentCollapser.java:259)\n\tat org.apache.solr.search.fieldcollapse.AbstractDocumentCollapser.collapse(AbstractDocumentCollapser.java:183)\n\tat org.apache.solr.handler.component.CollapseComponent.doProcess(CollapseComponent.java:173)\n\tat org.apache.solr.handler.component.CollapseComponent.process(CollapseComponent.java:127)\n\tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:195)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1322)\n\tat org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:337)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:240)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:388) "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12883142",
            "date": "2010-06-28T13:28:08+0000",
            "content": "Attached a new patch. This patch in a backport of the last patch for Solr 1.4.1. There are currently many changes in the trunk which make maintaining this patch difficult. To apply this patch checkout: http://svn.apache.org/repos/asf/lucene/solr/tags/release-1.4.1/ and apply the patch in checkout directory. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12883143",
            "date": "2010-06-28T13:31:32+0000",
            "content": "@Doug Steigerwald and Jasper van Veghel\nCan you check if your errors still occur in the latest patch for 1.4.1 release? "
        },
        {
            "author": "Doug Steigerwald",
            "id": "comment-12883208",
            "date": "2010-06-28T16:29:26+0000",
            "content": "Excellent!  Everything looks good with our issue.  Thanks for the quick turn around. "
        },
        {
            "author": "Jasper van Veghel",
            "id": "comment-12883329",
            "date": "2010-06-28T22:06:12+0000",
            "content": "Seconded. The NPE's were occurring rather randomly, but I haven't seen them since I've switched to 1.4.1 + your latest patch. Good stuff! It's also nice to have a patch against an actual release version (FYI, I was using r955615 before as per your patch note).\n\nThe only thing I'm still running into at this point is that I'm trying to get this to run using multiple cores / shards. Documents with the same collapse-field values don't span across shards so I figured it should work, and it does. But when including:\n\n  collapse.includeCollapsedDocs.fl=content\n\nThe actual documents returned in the collapse-counts/results are listed as:\n\n  <result name=\"collapsedDocs\" numFound=\"1\" start=\"0\">\n    <doc/>\n  </result>\n\nSo the actual field requested (content) doesn't get added. It does work when I remove the shards= parameter, only querying one core. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12883435",
            "date": "2010-06-29T06:53:43+0000",
            "content": "Seconded. The NPE's were occurring rather randomly, but I haven't seen them since I've switched to 1.4.1 + your latest patch. Good stuff! It's also nice to have a patch against an actual release version (FYI, I was using r955615 before as per your patch note).\nA lot of stuff is changing (or already has changed) in Lucene / Solr internally, so that might have been the cause of these exceptions.\n\nSo the actual field requested (content) doesn't get added. It does work when I remove the shards= parameter, only querying one core.\nI think that this part of the response is not copied from the shard's responses into the response that is returned to the client. So that 'll have to be added in order to get these collapsed documents\n\nOne important notice about this patch is that it is not going to be committed. Child issues of SOLR-236 like SOLR-1682 on the other hand will get committed to the trunk, but it might take some time till all the functionality that patches of SOLR-236 provide are implemented in a efficient manner. Just to make some things clear, because this is a long, very long and complicated issue. "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12887130",
            "date": "2010-07-11T01:08:00+0000",
            "content": "Oh Martijn, I hope you're reading.   After a few months of calm we had some OOM's again on our production servers.  So I tried your latest patch with the solr 1.4.1 release, since bundled in there are various fixes for memory leaks.  The performance difference is great - far less CPU and RAM usage all around.  But there's a catch!  Something was introduced to change the \"numFound\" that is reported.  After we noticed this, I found your comment and removed these lines from NonAdjacentDocumentCollapser.java:\n\n+        if (collapsedGroupPriority.size() > maxNumberOfGroups) \n{\n+          NonAdjacentCollapseGroup inferiorGroup = collapsedGroupPriority.first();\n+          collapsedDocs.remove(inferiorGroup.fieldValue);\n+          collapsedGroupPriority.remove(inferiorGroup);\n+        }\n\nWe did NOT remove line 99 as suggested because this caused compiler problems:\n\n\n    [javac] /home/sweiss/apache-solr-1.4.1/src/java/org/apache/solr/search/fieldcollapse/NonAdjacentDocumentCollapser.java:99: cannot find symbol\n    [javac] symbol  : variable collapseDoc\n    [javac] location: class org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser\n    [javac]       if (collapseDoc == null) {\n\nAfter doing this, I noticed a huge performance drop - far worse than what we had even with 1.4 and your patch from December.  Searches were taking >10s to complete (before we were just over 1s for the worst searches).  So, I went back and tried to find a way to get the \"numFound\" through other means - and I figured I could just facet on the same field we're collapsing on, and then count the number of facets.  Looks good - the count of the facets is the right count, and it would appear to be working.\n\nBut, there's a snag.  It seems that the results being returned by your patch, unaltered, are incorrect.  For an example - my search for \"orange\" returns 7200 collapsed results, either using the real numFound from the altered patch, or using the facet method wtih the new patch.  This equates to 160 pages of results.  However, with the unaltered patch, if we actually try to retrieve page 158, or really any result over 130 or so, we get the exact same results.  With the altered patch (removing those few lines), page 158 actually is page 158.  Basically, it seems like your patch throws away good results - and I get the feeling that it throws away those good results somewhere in those 5 lines.\n\nNow, I'm stuck.  I really don't know what to do... I don't want the OOMs to continue, but it looks like they will regardless because both the old version (1.4 + December patch) and the new, altered patched version are using too many resources.  But if I used the latest patch without changing it, I'm not getting the right results all the way through.\n\nIs there anything we can do?  I appreciate your help...  "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12887131",
            "date": "2010-07-11T01:25:56+0000",
            "content": "Actually I'm testing more (I want to make sure it's not just my own error), and it seems like paging in general is just broken with this patch - Any page between 4 and 80 seems to have the exact same results on it as well.  Then the results change a little, every 20 pages or so. "
        },
        {
            "author": "Pavel Minchenkov",
            "id": "comment-12892111",
            "date": "2010-07-25T15:46:38+0000",
            "content": "Latest patch for current trunk has many conflicts in SolrIndexSearcher.java. "
        },
        {
            "author": "cruz fernandez",
            "id": "comment-12894129",
            "date": "2010-07-30T19:58:07+0000",
            "content": "I'm having an issue with the facet exclude filter parameters (http://wiki.apache.org/solr/SimpleFacetParameters#Tagging_and_excluding_Filters). I have added this exclude tags and the facet result I'm getting is without collapsing (it's counting the uncollapsed items).\n\nFor example, in my first page it shows something like this (the facet result gives something like this):\n\n\n\tbook (11)\n\twebsite (20)\n\tjournal (5)\n\n\n\nafter clicking on book it shows 11 results correctly, but the faceting with the exclude applied shows:\n\n\n\tbook (230)\n\twebsite (25)\n\tjournal (5)\n\n\n\nI am using the parameter collapse.facet=after\n\nThe collapsed count of books is 11, and the uncollapsed count is 230, I verified it. "
        },
        {
            "author": "Pavel Minchenkov",
            "id": "comment-12894896",
            "date": "2010-08-03T15:04:39+0000",
            "content": "Please, update patch for trunk. "
        },
        {
            "author": "David Tu\u0161ka",
            "id": "comment-12895297",
            "date": "2010-08-04T15:39:21+0000",
            "content": "Hello, I find some bug in \"Field collapsing\",\nI will tested it for solr-1.4.1-patch and try test for trunk-patch(rev.955615) too.\n\n1) No collapse_counts/results will be returned if collapseCount==1, \nalthough no-collapse will be returned.\n\nhttp://localhost:8080/solr_tour/select/?q=nl_counter%3A1%0D%0A&start=0&rows=10&indent=on&sort=c_price_from_orig+asc&collapse.field=nl_tour_id&collapse.threshold=1&collapse.type=adjacent&collapse.debug=true\n\n\n \n<lst name=\"collapse_counts\">\n  <str name=\"field\">nl_tour_id</str>\n  <lst name=\"results\"/>\n  <lst name=\"debug\">\n    <str name=\"Docset type\">HashDocSet(26)</str>\n    <long name=\"Total collapsing time(ms)\">0</long>\n    <long name=\"Create uncollapsed docset(ms)\">0</long>\n    <long name=\"Get fieldvalues from fieldcache (ms)\">0</long>\n    <long name=\"AdjacentDocumentCollapser collapsing time(ms)\">0</long>\n    <long name=\"Creating collapseinfo time(ms)\">0</long>\n    <long name=\"Convert to bitset time(ms)\">0</long>\n    <long name=\"Create collapsed docset time(ms)\">0</long>\n  </lst>\n</lst>\n<result name=\"response\" numFound=\"26\" start=\"0\">\n10x <doc></doc> \n...\n\n\n\nIf I look into code, I find some problematic part of code:\n\nIn NonAdjacentDocumentCollapser.java in function doCollapsing is bad condition and priorityQueue:\n\nNonAdjacentDocumentCollapser.java\nprotected void doCollapsing(DocSet uncollapsedDocset, FieldCache.StringIndex values) {\n\n  for (DocIterator i = uncollapsedDocset.iterator(); i.hasNext();) {\n    int currentId = i.nextDoc();\n    String currentValue = values.lookup[values.order[currentId]];\n\n    NonAdjacentCollapseGroup collapseDoc = collapsedDocs.get(currentValue);\n\n    if (collapseDoc == null) {\n      ..\n    }\n\n    Integer dropOutId = (Integer) collapseDoc.priorityQueue.insertWithOverflow(currentId);\n\n    // IMHO HERE must be >= NO > !!!!\n    if (++collapseDoc.totalCount > collapseThreshold) {\n      collapseDoc.collapsedDocuments++;\n\n      //HERE IS PROBLEM TOO, if doc is only one, then is not returned by collapseDoc.priorityQueue.insertWithOverflow for collapse.threshold=1\n      if (dropOutId != null)\n      {\n        for (CollapseCollector collector : collectors) {\n          collector.documentCollapsed(dropOutId, collapseDoc, collapseContext);\n        }\n      }\n    }\n}\n\n \n\nIn AdjacentDocumentCollapser.java in doCollapsing is problem in Initializing condition, \nif doc is only one, then only inicializing condition is process, else-if, else part not will be processed and collector.documentCollapsed or collector.documentHead not will be call.\n\n\nNonAdjacentDocumentCollapser.java\nprotected void doCollapsing(DocSet uncollapsedDocset, FieldCache.StringIndex values) {\n  ...\n  String collapseValue = null;\n  ...\n  for (DocIterator i = uncollapsedDocset.iterator(); i.hasNext();) {\n    int currentId = i.nextDoc();\n    String currentValue = values.lookup[values.order[currentId]];\n\n    // Initializing\n    if (collapseValue == null) {\n      repeatCount = 0;\n      collapseCount = 0;\n      collapseId = currentId;\n      collapseValue = currentValue;\n\n      // Collapse the document if the field value is the same and\n      // we have a run of at least collapseThreshold uncollapsedDocset.\n    }\n    //IMHO HERE MUST BE if NO else-if !!!!    \n    else if (collapseValue.equals(currentValue))\n    {\n      if (++repeatCount >= collapseThreshold) {\n        collapseCount++;\n        for (CollapseCollector collector : collectors) {\n          CollapseGroup valueToCollapse = new AdjacentCollapseGroup(collapseId, currentValue);\n          collector.documentCollapsed(currentId, valueToCollapse, collapseContext);\n        }\n      } else {\n        addDoc(currentId);\n      }\n    }\n    else\n    {\n      ...\n    }\n    ...\n  }\n  ...\n}\n\n \n\n2) I have problem with sorting, I need sort CollapseGroup by c_price_from_orig field, \nbut if I have in request \"sort=c_price_from_orig+asc\",\nreturned CollapseGroup will be sorted by c_price_from_orig (minimum of collapsed doc in group),\nbut some CollapseGroup will be skiped and doc with c_price_from_orig will not be returned firts !!!\n\nI try debug this problem and report this better.\n\n\nthanks for your reply,\nsorry for my english and\n\nbest regards\nDavid "
        },
        {
            "author": "wyhw whon",
            "id": "comment-12895966",
            "date": "2010-08-06T06:20:57+0000",
            "content": "when i use fq=xxxx:1302 , i got a error as follow, but it can work with other fq.\n\nHTTP Status 500 - -1073634 java.lang.ArrayIndexOutOfBoundsException: -1073634 at org.apache.lucene.search.FieldComparator$StringOrdValComparator.copy(FieldComparator.java:659) at org.apache.lucene.search.TopFieldCollector$OutOfOrderOneComparatorNonScoringCollector.collect(TopFieldCollector.java:133) at org.apache.solr.search.SolrIndexSearcher.sortDocSet(SolrIndexSearcher.java:1529) at org.apache.solr.search.SolrIndexSearcher.getDocListC(SolrIndexSearcher.java:973) at org.apache.solr.search.SolrIndexSearcher.search(SolrIndexSearcher.java:347) at org.apache.solr.search.SolrIndexSearcher.getDocListAndSet(SolrIndexSearcher.java:1503) at org.apache.solr.handler.component.CollapseComponent.doProcess(CollapseComponent.java:183) at org.apache.solr.handler.component.CollapseComponent.process(CollapseComponent.java:134) at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:195) at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131) at org.apache.solr.core.SolrCore.execute(SolrCore.java:1316) at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:339) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:242) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:175) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286) at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:844) at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:583) at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:447) at java.lang.Thread.run(Thread.java:619)\n\n\nbut , \nit can work if not use fq\nif i disable useFilterForSortedQuery in solrconfig.xml, it also work .\n "
        },
        {
            "author": "Evgeniy Serykh",
            "id": "comment-12900225",
            "date": "2010-08-19T09:07:41+0000",
            "content": "I've patched release of solr 1.4.1.  When I try to execute query with collapsing 'numFound' value always equals 10 while 'rows' param not specified.  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12900964",
            "date": "2010-08-21T02:43:42+0000",
            "content": "Since everyone seems to be watching this issue, I'll comment here.\nI've just committed the first parts to field collapsing to trunk! See SOLR-1682\nThanks to everyone who has worked on these related issues for so long!\nI chose to back off and bite of a manageable piece, but I referenced all the\ngreat work that has been done in the various related issues, and tried\nto give credit to everyone who's submitted patches (let me know if I missed anyone.)\n\nThis is really just a start to build from of course - there's much left to do! "
        },
        {
            "author": "Peter Kieltyka",
            "id": "comment-12903028",
            "date": "2010-08-26T20:26:25+0000",
            "content": "Hey guys,\n\nHow difficult would it be to add the ability to specify if for any collapsed values, to not return any of the documents.. to just purge all duplicates from the results.\n\nThis could be done by adding a new field: collapse.purge which can be true or false, and defaults to false\n\nI could really use that. I have a scenario where I have the following data set of documents:\n\nALL: <1,2,3,4,5>\nA: <1,2>\nB: <3,4>\nC: <4,5>\n\nand I want to search the text within the subset of documents: (ALL - A) = <3,4,5>\n\nCollapse would do this ..\n\nq => text:something AND -(group_id:[* TO *] AND -group_id:A)\ncollapse.field => uid\ncollapse.purge => true\n\nCheers!\n "
        },
        {
            "author": "Amit Nithian",
            "id": "comment-12904806",
            "date": "2010-08-31T22:33:42+0000",
            "content": "Two questions and one comment:\nComment:\n1) This is a neat patch! Thanks for this contribution.\n\nQuestions:\n1) Which patch should we start using.. this one or the one Yonik referenced?\n2) Will the cache config in the component be retrieved via the CacheConfig  instead of as a child element in the component?\n\nExcited to see the final product. I am using it for a simple app right now and it's working fairly well. "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12909116",
            "date": "2010-09-14T05:17:51+0000",
            "content": "FWIW, I fixed my earlier OOM issues with some garbage collection tuning.\n\nNow I'm noticing NPEs very similar to those people were reporting back before the patch from Jun 28th:\n\nSEVERE: java.lang.NullPointerException\n\tat org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser$FloatValueFieldComparator.compare(NonAdjacentDocumentCollapser.java:450)\n\tat org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser$DocumentComparator.compare(NonAdjacentDocumentCollapser.java:262)\n... it's the same backtrace ...\n\nI'm guessing it's because I added those 5 lines back into the patch to get the paging working again.\n\nIt's rather infrequent, it's probably something I can deal with until the new patch is complete.  It doesn't happen every time at all like it seemed to happen to many people - just once in a while, and on queries that honestly run all the time, so it seems random and not related to a particular query (except perhaps in the size of the filter queries - these fqs relatively large #'s of documents).  But if any of this code makes it to the new patch I thought it would be worth mentioning. "
        },
        {
            "author": "Varun Gupta",
            "id": "comment-12910058",
            "date": "2010-09-16T09:07:07+0000",
            "content": "I am using the patch SOLR-1682 committed on trunk for field collapsing. It works great but gives problem when I include other components like Facet and Highlighter. Is there any workaround to use Highlight and Facet components along with grouping? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12910131",
            "date": "2010-09-16T12:46:23+0000",
            "content": "It works great but gives problem when I include other components like Facet and Highlighter.\n\nSee the list of sub-tasks on this issue starting with \"SearchGrouping:\".\nI fixed faceting yesterday - and I hope to fix highlighting and debugging today. "
        },
        {
            "author": "Jamie",
            "id": "comment-12913618",
            "date": "2010-09-22T15:28:34+0000",
            "content": "When using collapse.includeCollapsedDocs.fl to and sorting by a field (not score) the returned collapsed results aren't sorted correctly. "
        },
        {
            "author": "Thorsten Maus",
            "id": "comment-12927370",
            "date": "2010-11-02T12:06:41+0000",
            "content": "After applying the patch SOLR-236-1_4_1.patch the ant test task fails on org.apache.solr.spelling.SpellingQueryConverterTest. Can it be ignored? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12927593",
            "date": "2010-11-02T21:03:39+0000",
            "content": "Here's a refactoring patch that pulls all the grouping stuff out of SolrIndexSearcher (I'm sure many of you will be glad about that  and uses subclasses rather than instanceof checks for different behavior of grouping commands.\n\nThis isn't the end of refactoring, but it's a good start I think, and should make additional changes easier. "
        },
        {
            "author": "Martijn van Groningen",
            "id": "comment-12927636",
            "date": "2010-11-02T21:54:02+0000",
            "content": "After applying the patch SOLR-236-1_4_1.patch the ant test task fails on org.apache.solr.spelling.SpellingQueryConverterTest. Can it be ignored?\nI think so, since the patch your referrer has nothing to do with spelling. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12931734",
            "date": "2010-11-13T21:47:43+0000",
            "content": "Just committed a patch that the random testing I'm developing uncovered - we lost the default of group.sort to sort during the last refactoring. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12931843",
            "date": "2010-11-14T16:49:56+0000",
            "content": "Two more corner cases not yet fixed:\n1) if rows==0, we get an NPE\n2) if group.limit and group.offset are both 0, then the counts for the resulting doclists are all zero. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12931949",
            "date": "2010-11-15T01:00:02+0000",
            "content": "NOTE: there was a serious bug when sort != group.sort (i.e. when TopGroupSortCollector was used).\nThe wrong comparators were used in one place, leading to errors finding the top groups.  I just committed a fix for this.\n\nThe NPE when rows==0 has also been fixed. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12931971",
            "date": "2010-11-15T03:59:34+0000",
            "content": "Random testing found another bug - while finding the top groups, we forgot to setBottom on the priority queue when it changed. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12932071",
            "date": "2010-11-15T15:04:00+0000",
            "content": "NOTE: there was a serious bug when sort != group.sort (i.e. when TopGroupSortCollector was used.\n\nActually, I think it's worse.  The algorithm added in SOLR-1682 (TopGroupSortCollector) that handled when sort != group.sort seems broken.\nThe problem: a high ranking group may be demoted to a lower ranking group because it's top document changed (and the sorts used to find the top doc in a group and the top group are different).  But we may have already discarded higher ranking groups based on the original high ranking, so now we have permanently lost information. "
        },
        {
            "author": "Bill Bell",
            "id": "comment-12932201",
            "date": "2010-11-15T21:10:36+0000",
            "content": "We are having an issue with this patch.\n\n\nhttp://localhost:8983/solr/provs/select?fl=hgid,score&q.alt=*:*&start=5&rows=10&qt=standard&group=true&group.field=hgid\n\n\n\nWe get 15 results. 10+5 ? It should be 10 rows. This does not appear to be working right with start and rows. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12932319",
            "date": "2010-11-16T02:52:44+0000",
            "content": "We get 15 results. 10+5 ? It should be 10 rows.\n\nYes, I've reproduced this with the random testing too.  Not sure what to make of it yet. \nIt looks like the orderedGroups TreeSet acquires too many entries for some reason. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12932475",
            "date": "2010-11-16T14:22:36+0000",
            "content": "OK Bill, this should be fixed in the latest trunk... can you try it out? "
        },
        {
            "author": "Bill Bell",
            "id": "comment-12932529",
            "date": "2010-11-16T16:45:34+0000",
            "content": "Yonik,\n\nI am testing. Will get back to you on the starts/rows.\n\nAlso is there a way to get the total number of results based on ther grouping?  I get the following:\n\n\n <lst name=\"grouped\">\n- <lst name=\"hgid\">\n  <int name=\"matches\">6</int> \n- <arr name=\"groups\">\n- <lst>\n\n\n\n\nBut no total number. Also the matches=6, includes those fields not returned (the group has 2 entries, but I only return 1). It should show matches=6, results=4 (since 2 are hidden), totalNumber=6747.\n\nOtherwise we cannot page.\n\nIf we do a http://localhost:8983/select?q=test&facet=true&face.field=hgid there are too many results (thousands). ANy other way to group by and get a total?  "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12932536",
            "date": "2010-11-16T16:55:01+0000",
            "content": "Just chiming in on that last comment... we also rely on functional paging and total counts when collapsing as well.  I once raised the idea of not providing this information in our search results to my boss and he looked at me like I had 3 heads, it's just not an option.  In most of the patches on this ticket we could get this data, but for some it seemed like eliminating totals and paging wasn't a big deal and provided a significant performance boost.  I can understand the reasons for not including this for every collapsed query (if you don't need the totals or paging then the performance boost is nice), but if there was a way we could have an option to turn this on or off (even with the performance hit, having it is better than not being able to collapse at all), maybe that could help keep everyone happy.  I remember it only was a difference of 5 or 6 lines of code either way. "
        },
        {
            "author": "James Dyer",
            "id": "comment-12932542",
            "date": "2010-11-16T17:15:15+0000",
            "content": "We also have a hard requirement for field collapsing with total # of groups for a project scheduled for Production Q1 2011.  So far, best I can tell I would have to facet on the group-by field with facet.limit=-1 to get this.  Surely we would have less overhead if the group-by functionality could compute this by itself and just return the number.  Turning it on/off makes sense as some won't want the performance/memory hit. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12932549",
            "date": "2010-11-16T17:33:16+0000",
            "content": "I remember it only was a difference of 5 or 6 lines of code either way. \n\nNot with what is committed in trunk.  To be scalable wrt to the number of groups, we only keep the top 10 groups in memory at any one time (and hence we never know the total number of groups).  The ability to retrieve the number of groups will require a different algorithm with different tradeoffs.  I'm sure we'll get to it in time, but it is not just a tweak to the existing algorithm. "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12932558",
            "date": "2010-11-16T17:50:08+0000",
            "content": "If you need help James, I have a version of 1.4.1 patched that does do the collapsing and provide this data - it was based on some of the comments above along with a patch that came out a while ago (back when it really was only 5 or 6 lines of difference).  The faceting route really doesn't work out well once you hit a certain number of collapse groups.  Anyway, I've been using this version in production for quite a while now, and while it is a bit of a memory hog, if you manage the memory properly, keep your indexes optimized and provide enough RAM to cover your indexes then it's pretty stable and gets the job done. "
        },
        {
            "author": "James Dyer",
            "id": "comment-12932564",
            "date": "2010-11-16T18:02:02+0000",
            "content": "Stephen,\n\nI would be very interested in seeing your patch if you can upload it.  Luckily, the index we're migrating to SOLR for this project is small and I think I won't have to scale very much in either case.  Your patch might be better than the current SOLR-1682/236 patches for our needs however. "
        },
        {
            "author": "Bill Bell",
            "id": "comment-12932565",
            "date": "2010-11-16T18:03:13+0000",
            "content": "Is the older CollapseComponent still available in the trunk?\n\nOr do we need to use the newer group parameters? \n\nHow do I get the older one to work? "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12932579",
            "date": "2010-11-16T18:23:40+0000",
            "content": "This would be the patch that I'm describing... I used it with the Solr 1.4.1 release tarball.  It's just Martijn's latest patch minus a few lines (by his suggestion) that mess up the totals and paging.  Again, you want to make sure your server is well configured - we are not really Java people and it took a while to get the settings to a place where we didn't have OOM errors every day.   We're using these startup options with Jetty:\n\n-Xms10240m -Xmx10240m -XX:NewRatio=5 -XX:+UseParNewGC\n\nThat RAM total is half the RAM available on the machine - we leave the rest of the RAM open for disk caches.  It will take up it's half of the RAM very quickly but then it hovers there and has only ever gone over the limit once since September, which seemed to be related to an unoptimized index (after replacing an unusually large # of docs). "
        },
        {
            "author": "Bill Bell",
            "id": "comment-12932640",
            "date": "2010-11-16T20:21:19+0000",
            "content": "I found a solution, but is is not ideal. I need to be able to get a count of facets (left side and not left side):\n\nhttp://localhosT:8983/solr/select?facet=true&facet.field=hgid&facet.limit=100000&facet.mincount=1\n\n(HGID is the group by)\n\nI need to I get the following but need left side number. So instead of 10, I need \"3\". Is there anyway to do that? Just return 3.\n\n\n\t<lst name=\"hgid\">\n  <int name=\"HGPY0056D09F7B57442E8\">4</int> \n  <int name=\"HGPY00A33AD7808996941\">3</int> \n  <int name=\"HGPY00D6274FD07B4EE7A\">3</int> \n  </lst>\n\n "
        },
        {
            "author": "Bill Bell",
            "id": "comment-12932655",
            "date": "2010-11-16T20:59:55+0000",
            "content": "Yonik,\n\n> OK Bill, this should be fixed in the latest trunk... can you try it out? \n\nYes paging seems to work right now.\n\nQuestion: Is there a way to return or do it with some sort of ord() value?\n\n<lst name=\"hgid\">\n<int name=\"HGPY0056D09F7B57442E8\">4</int> \n<int name=\"HGPY00A33AD7808996941\">3</int> \n<int name=\"HGPY00D6274FD07B4EE7A\">3</int> \n</lst> \n<facetname name=\"hgid\">3</facetname>\n "
        },
        {
            "author": "Bill Bell",
            "id": "comment-12932848",
            "date": "2010-11-17T07:14:03+0000",
            "content": "Here is an idea. If we go with the terminology -\n\n<int name=\"name\">value</int> \n\nThen we can just return the name distinct by a few mods to SimpleFacet.java. All other parameters still apply. Default will be off.\n\nfacet.<field>.namedistinct=1\n\n<lst name=\"hgid\">\n<int name=\"count\">3</int> \n</lst> \n\nI can have a patch for this today. Would this be something that we could go with?\n\nBill "
        },
        {
            "author": "peterwang",
            "id": "comment-12932905",
            "date": "2010-11-17T11:19:14+0000",
            "content": "SOLR-236-1_4_1-paging-totals-working.patch patch failed with following errors:\n\npatch: **** malformed patch at line 3348: Index: src/test/org/apache/solr/search/fieldcollapse/DistributedFieldCollapsingIntegrationTest.java\n\nseems caused by hand edit SOLR-236-1_4_1.patch to produce SOLR-236-1_4_1-paging-totals-working.patch (delete 6 lines without fix diff hunk number) \npossible fix:\n\n\ndiff -u SOLR-236-1_4_1-paging-totals-working.patch.orig SOLR-236-1_4_1-paging-totals-working.patch\n--- SOLR-236-1_4_1-paging-totals-working.patch.orig     2010-11-17 19:26:05.000000000 +0800\n+++ SOLR-236-1_4_1-paging-totals-working.patch  2010-11-17 19:17:20.000000000 +0800\n@@ -2834,7 +2834,7 @@\n ===================================================================\n --- src/java/org/apache/solr/search/fieldcollapse/NonAdjacentDocumentCollapser.java    (revision )\n +++ src/java/org/apache/solr/search/fieldcollapse/NonAdjacentDocumentCollapser.java    (revision )\n-@@ -0,0 +1,517 @@\n+@@ -0,0 +1,511 @@\n +/**\n + * Licensed to the Apache Software Foundation (ASF) under one or more\n + * contributor license agreements.  See the NOTICE file distributed with\n\n  "
        },
        {
            "author": "Bill Bell",
            "id": "comment-12933084",
            "date": "2010-11-17T19:08:25+0000",
            "content": "TO do distinct facet counts. "
        },
        {
            "author": "Bill Bell",
            "id": "comment-12933085",
            "date": "2010-11-17T19:08:32+0000",
            "content": "OK I have a patch to add namedistinct. Note that is optional, and to be careful of the number of facets when using it.\n\nOn sample data:\n\nhttp://localhost:8983/solr/select?q=*:*&facet=true&facet.field=manu&facet.mincount=1&facet.limit=-1&f.manu.facet.namedistinct=0&facet.field=price&f.price.facet.namedistinct=1\n\nIt works on facet.fields.\n\nSOLR-236-distinctFacet.patch\n\n\n "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-12933153",
            "date": "2010-11-17T20:41:58+0000",
            "content": "Cheers peterwang, you're probably right.  I didn't actually use this patch, I made the modifications by hand after applying Martijn's patch.  I generally don't make my own patch files, I just let SVN do it for me, so I'm not really aware of the syntax...  The point is to just delete those extra lines. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12934354",
            "date": "2010-11-21T21:25:39+0000",
            "content": "I've just committed a fix to the sort != group.sort problem.\nAs I previously said, the algorithm for handling this was broken (the TopGroupSortCollector class), so I've redefined what sort means.\nSort does not order groups by the first document in each group, but orders groups by the highest ranking document by \"sort\" in that group.\nI've updated the randomized grouping tests to reflect this change, and enabled tests where sort != group.sort "
        },
        {
            "author": "Bill Bell",
            "id": "comment-12934393",
            "date": "2010-11-22T04:16:10+0000",
            "content": "Yonik and team,\n\nIs anyone working on the ability to calculate facets AFTER the group? Without a patch for that, the facet numbering is not correct.\n\nThank you.\nBill "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12934396",
            "date": "2010-11-22T04:55:59+0000",
            "content": "Is anyone working on the ability to calculate facets AFTER the group? Without a patch for that, the facet numbering is not correct.\n\nThere's no correctness issue or bug here.  Many use cases require the current behavior (the number of docs per group shown having no effect on faceting), and other use cases require what you seek.  Both are valid, but we only have one implemented so far. "
        },
        {
            "author": "Ingmar Seeliger",
            "id": "comment-12934415",
            "date": "2010-11-22T08:11:45+0000",
            "content": "Field  collapsing is a very nice feature - thank you for that!\n\nI've just tested it with (pseudo-)distributed search, that means the data on each solr-server has one specific value for the collapse field, and realized one problem:\nI want to include the documents in the result list, using collapse.includeCollapsedDocs.fl=...\nThe result list has empty docs:\n<result name=\"collapsedDocs\" numFound=\"4\" start=\"0\">\n  <doc/>\n  <doc/>\n  <doc/>\n  <doc/>\n</result>\nWhen I remove the distributed search, everything works fine on one server. Perhaps someone can look for that? Thanks! "
        },
        {
            "author": "Luke Bochsler",
            "id": "comment-12934825",
            "date": "2010-11-23T13:21:14+0000",
            "content": "\"Is anyone working on the ability to calculate facets AFTER the group?\"\n\nThis would be great to have that possibility! Sorry I'm not a Java Programmer so I cannot contribute a solution, instead I contribute to other open source systems. However, would that be a big deal for you guys to implement it? I'm using Solr in a web project as search solution and desperately need this feature along with the great grouping functionality. The grouping in general has made my life so much easier so far, so it seems we are just one step away from having it all covered by Solr!\n\nThank you so much!\n\nLuke "
        },
        {
            "author": "Joseph McElroy",
            "id": "comment-12966881",
            "date": "2010-12-05T01:35:56+0000",
            "content": "Hi there, \n\nGreat work on this feature, it has been something i have been waiting for a while to be implemented in SOLR, thank you all for this.\n\nTwo questions however:\n\n\tAn option to sort  the groups on the number of documents each group has? so the group with the largest number of documents would be the highest ranked.\n\tAbility to return the number of groups within the result set? This would allow for pagination.\n\n\n\nThanks\nJoe "
        },
        {
            "author": "Shekhar Nirkhe",
            "id": "comment-12975329",
            "date": "2010-12-27T20:32:14+0000",
            "content": "I am getting null pointer exception in\nat org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser$FloatValueFieldComparator.compare(NonAdjacentDocumentCollapser.java:443)\n\nI am using Solr 1.4.1 with following patches.\n\nSOLR-236-1_4_1.patch\nSOLR-236-1_4_1.fix.patch\n\nAm I missing something ? "
        },
        {
            "author": "Jerry Mindek",
            "id": "comment-12975331",
            "date": "2010-12-27T20:34:50+0000",
            "content": "I will be out from Dec 25 and returning to the office Monday Jan 4th. Thanks! "
        },
        {
            "author": "Ron Veenstra",
            "id": "comment-12978012",
            "date": "2011-01-05T22:46:51+0000",
            "content": "I have also been getting a null pointer exception:\nmessage null java.lang.NullPointerException at org.apache.solr.search.fieldcollapse.NonAdjacentDocumentCollapser$PredefinedScorer.docID(NonAdjacentDocumentCollapser.java:397)\n\nThe error is repeatable for a given search term when sorted by \"score desc,\" followed by any other field. It seems to crop up whenever there is only one result that should be returned in the collapsed field group, but does not happen for every possible query where this is the case (leading me to believe something else is at work).  Changing the sort order to anything else (moving score to second, or omitting a second field) eliminates the error.  This was the simple solution for my problem, but wanted to post this in case any of the information proved useful.\n\nUsing Solr 1.4.1 with SOLR-236-1_4_1-paging-totals-working.patch "
        },
        {
            "author": "Samuel Garc\u00eda Mart\u00ednez",
            "id": "comment-12979193",
            "date": "2011-01-08T20:13:17+0000",
            "content": "The NPE noticed by Shekhar Nirkhe is caused by some errors on filter query cache and the signature key that is using to store cached results. \n\nTo sum up, if you perform a filter query and then, you perform that query using collapse field, that query result is already cached, but not cached as expected by this component. Resulting that the DocSet implementation is not the expected one, and, as cached result, the DocumentCollector is not executed at any time.\n\nAs soon as i can ill post a patch using combined key to cache results, formed by the collector class and the query itself.\n\nColbenson - Findability Experts \nhttp://www.colbenson.es/\n "
        },
        {
            "author": "Steven Fuchs",
            "id": "comment-12985812",
            "date": "2011-01-24T16:58:40+0000",
            "content": "Great feature! But it seems to be missing a capability I need. I'll explain it:\n\nI'd like to use group results in my query, namely exclude all documents in a group when any document in that group has a certain value. It could be as simple as a field value although the ability to do more complex queries would be nice also. Please consider adding functionality like this to your sub tasks list. Or better yet if this capability exists and I missed it please someone point it out.\n\nTIA\nsteve "
        },
        {
            "author": "Hsiu Wang",
            "id": "comment-12990227",
            "date": "2011-02-03T18:42:43+0000",
            "content": "I applied the SOLR-236-1_4_1-paging-totals-working.patch to 3x branch. when I ran unit test FieldCollapsingIntegrationTest, I got \"Insane FieldCache usage(s) found expected:<0> but was:<1>\" on all 3 sort related tests(testNonAdjacentFieldCollapse_sortOnNameAndCollectAggregates, testNonAdjacentFieldCollapse_sortOnNameAndCollectCollapsedDocs, and testForArrayOutOfBoundsBugWhenSorting). "
        },
        {
            "author": "Cameron",
            "id": "comment-12990387",
            "date": "2011-02-03T23:53:14+0000",
            "content": "Uploading SOLR-236-1_4_1-NPEfix.patch as a simple patch for the NullPointerException Shekhar and Ron have reported. The patch is intended to be applied AFTER the SOLR-236-1_4_1-paging-totals-working.patch has already been applied, for brevity. \n\nI didn't actually fix the filterCache key issue as Samuel suggested. Rather I'm preventing the NPE from occurring. I believe this is ok because the collapsed results will stay sorted by score as the collapser performs the collapsing. "
        },
        {
            "author": "Doug Steigerwald",
            "id": "comment-12995362",
            "date": "2011-02-16T16:09:04+0000",
            "content": "Has anyone successfully applied field collapsing to the branch_3x branch? "
        },
        {
            "author": "Doug Steigerwald",
            "id": "comment-12996491",
            "date": "2011-02-18T16:20:55+0000",
            "content": "Attaching a patch for the 3x branch (SOLR-236-branch_3x.patch). This based off of SOLR-236-1_4_1-paging-totals-working.patch and SOLR-236-1_4_1-NPEfix.patch.\n\nTests work and some basic spot checking I've done looks good. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12996550",
            "date": "2011-02-18T18:28:04+0000",
            "content": "Why are people still working on this SOLR-236 patch?\nDoesn't SOLR-1682 supercede it?\nAnd isn't SOLR-1682 the one that's in trunk, while nothing from SOLR-236 was ever applied to trunk?\nThanks. "
        },
        {
            "author": "Doug Steigerwald",
            "id": "comment-12996557",
            "date": "2011-02-18T18:38:55+0000",
            "content": "I started to try and backport SOLR-1682 to the 3x branch, but that seemed to get out of hand pretty quickly from what I remember (was a few weeks ago). It was much easier making this work with the 3x branch than backporting SOLR-1682.\n\nWe want/need new features in 3.1 when it is released and we won't be allowed to deploy trunk to our production environment. "
        },
        {
            "author": "Yuriy Akopov",
            "id": "comment-13010942",
            "date": "2011-03-24T21:29:30+0000",
            "content": "Hi,\n\nFirst of all, thanks you guys for working on that! However, I have encountered a problem with this patch which is hopefully caused by my mistakes, so please correct me if I have done something wrong.\n\nSo, I have applied SOLR-236 patch to release-1.4.1 and gained support for collapse.*, which works. However, two issues discussed above in this thread are still there:\n\na) When collapsing is requested, only grouped results are returned. So, if the document has got a unique value in the field collapsed (i.e. it has no other docs to group with) it is excluded from the results. Instead of expected \"unique documents plus non-unique grouped to the most relevant one\" just grouped ones are returned.\n\nb) The number of results matching the query (\"numFound\") returned is always equal to \"rows\" parameter provided or 10 if not supplied (i.e. it represents the number of results on the page is returned, not the total number of matched documents).\n\nThere is a way around the latter \"numFound\" issue: faceting by the field collapsed as it was suggested before, but the number retrieved with that facet is also useless as it includes unique (non-grouped) documents as well, but they are not returned.\n\nSo far, I'm stuck with that. Is there any chance of resolving that? What about the SOLR-1682 patch - if it fixes that, should be applied to the original release-1.4.1 or to the release-1.4.1 patched with SOLR-236 beforehand?\n\nThanks in advance.\n\nP.S. As I understand, grouping is planned in Solr 4.0. Does anybody know by any chance if it is safe to use its nightly builds? I ran through its pending critical issues and they doesn't look fatal, but still I'm afraid of possible implications. "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-13010948",
            "date": "2011-03-24T21:37:49+0000",
            "content": "Yuriy... try my patch:  SOLR-236-1_4_1-paging-totals-working.patch.  I don't have either of the problems you describe (problem B was actually the purpose of my patch, I never saw Problem A and I have tons of \"single\", non-grouped documents so I'm sure I would be seeing it if it were happening).  Some people had problems using the patch (I didn't use it myself, I made it after the fact) but if you look up in the comments people explain how to make it work.  Note that I'm not using the SOLR-236-1_4_1-NPEfix.patch patch, I never had the NPE problem they describe so I never bothered with it, not sure what it does. "
        },
        {
            "author": "Yuriy Akopov",
            "id": "comment-13010997",
            "date": "2011-03-24T23:14:10+0000",
            "content": "I didn't expect the reply to come so quickly! Thanks, Stephen, I'll try it and post the results then. "
        },
        {
            "author": "Yuriy Akopov",
            "id": "comment-13011213",
            "date": "2011-03-25T14:12:08+0000",
            "content": "By the way, a noob question: after build completes along with the \"apache-solr-1.4.2-dev.war\" the following jars are generated:\n\napache-solr-cell-1.4.2-dev.jar\napache-solr-clustering-1.4.2-dev.jar\napache-solr-core-1.4.2-dev.jar\napache-solr-dataimporthandler-1.4.2-dev.jar\napache-solr-dataimporthandler-extras-1.4.2-dev.jar\napache-solr-solrj-1.4.2-dev.jar\nsolrj-lib/commons-codec-1.3.jar\nsolrj-lib/commons-httpclient-3.1.jar\nsolrj-lib/commons-io-1.4.jar\nsolrj-lib/geronimo-stax-api_1.0_spec-1.0.1.jar\nsolrj-lib/jcl-over-slf4j-1.5.5.jar\nsolrj-lib/slf4j-api-1.5.5.jar\nsolrj-lib/wstx-asl-3.2.7.jar\n\nDo I need also transfer these libraries as well, or it is only needed to replace war file to get the patched version working properly? In my previous tries I copied solrj-lib/*.jar files to lib folder of Solr instance home. Maybe that was the problem? "
        },
        {
            "author": "Yuriy Akopov",
            "id": "comment-13011433",
            "date": "2011-03-25T21:36:32+0000",
            "content": "Stephen, apparently the version you've advised works fine! At least those two issues I complained about are gone. Many thanks for your help! "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-13011563",
            "date": "2011-03-26T05:00:27+0000",
            "content": "Just be careful Yuriy, there are reasons why this thing is not in Solr 1.4.1 already  The code does not scale particularly well beyond a few million documents, especially if you use the version that preserves totals and paging.  It was enough to keep my software from being scrapped, but if you plan on scaling much past that point any time soon, you may need to start thinking about alternative solutions.  I know I certainly am... I have a sinking worry my application may outgrow the limits of this patch's stability before something truly production ready comes to fore, possibly even this year if growth continues.  However, given that the very concept of grouping is critical to the site that I support with SOLR, and attempts to provide the same functionality without actually grouping have failed repeatedly over the past few months, it is very sadly starting to look like I will have to cut very useful features (to no end of complaints, I'm sure) in order to ensure it's overall stability unless some miracle happens.  Mama always told me I should have learned Java!\n\nLong story short, if you don't have to have this patch yet, and your software hasn't been written to do anything like this yet, I would not start doing it now!  You will regret it when you run out of options later on and your servers start crashing all over the place.  See if you can keep it under wraps until a real release comes out with it. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-13011603",
            "date": "2011-03-26T11:09:35+0000",
            "content": "Keep in mind an alternative approach that scales, but loses some attributes of this patch (total groups for instance) is committed on trunk and will likely be backported to 3.2. "
        },
        {
            "author": "Yuriy Akopov",
            "id": "comment-13011752",
            "date": "2011-03-27T07:44:09+0000",
            "content": "Stephen, Grant, thanks for the notice. Currently the total number of documents we deal with is about 800K and I expect it to group up to 2M in a year, but every user is allowed to search not the whole amount but a subset of it (so, for every search, additional filtering conditions are applied). I hope we will be fine until Solr4 comes out.\n\nBut if we encounter any critical problems, would it be enough to remove collapsing parameters from the request sent to Solr to prevent the external functions from failing, or it is needed to replace the Solr core with unpatched one? I mean, the failure on a large set of documents is possible even when collapse.* parameters are not supplied, or only if the collapsing was requested? "
        },
        {
            "author": "Yuriy Akopov",
            "id": "comment-13011754",
            "date": "2011-03-27T07:55:52+0000",
            "content": "In other words, if I use additional filtering conditions in my request to make sure the returned set of documents to be grouped is never larger than, say, 1 million items, can I expect the described problem to happen, or I'll be safe? Or regardless of the particular query and its resulting set to be collapsed I'm in danger if my index contains few millions documents?\n\n(sorry for commenting twice on the same problem) "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-13011810",
            "date": "2011-03-27T15:29:50+0000",
            "content": "It would work fine as long as you weren't sending the collapse parameters, I don't think you'd need to replace the WAR. "
        },
        {
            "author": "George P. Stathis",
            "id": "comment-13011812",
            "date": "2011-03-27T15:58:26+0000",
            "content": "Bump on Yuriy's last question:\n\n\tAre performance issues around the number of documents matched, the size of the index, or both?\n\n\n\nE.g. our index contains over 12 million documents already. Should we even consider using this feature?\n\nAdding a few more questions:\n\n\tAre performance concerns around the 1.4 patch, the current Solr 4.0 branch or both?\n\tIs sharding an option to alleviate some of these issues? Reading the comments in this ticket, it seems there are caveats getting this to work with shards?\n\n "
        },
        {
            "author": "Yuriy Akopov",
            "id": "comment-13013878",
            "date": "2011-03-31T08:23:38+0000",
            "content": "Another question:\n\nThe patched version of .war starts and works as expected if I place the following simple instruction in solrconfig.xml:\n\n\t<searchComponent name=\"collapse\" class=\"org.apache.solr.handler.component.CollapseComponent\">\n\t</searchComponent>\n\nBut if I add additional factories like it is advised by the sample config, it produces an error when searching with collapsing turned on:\n\n\t<searchComponent name=\"collapse\" class=\"org.apache.solr.handler.component.CollapseComponent\">\n\t\t<collapseCollectorFactory class=\"solr.fieldcollapse.collector.DocumentGroupCountCollapseCollectorFactory\" />\n\t\t<collapseCollectorFactory class=\"solr.fieldcollapse.collector.FieldValueCountCollapseCollectorFactory\" />\n\t\t<collapseCollectorFactory class=\"solr.fieldcollapse.collector.DocumentFieldsCollapseCollectorFactory\" />\n\t\t<collapseCollectorFactory name=\"groupAggregatedData\" class=\"org.apache.solr.search.fieldcollapse.collector.AggregateCollapseCollectorFactory\">\n\t\t\t<function name=\"sum\" class=\"org.apache.solr.search.fieldcollapse.collector.aggregate.SumFunction\"/>\n\t\t\t<function name=\"avg\" class=\"org.apache.solr.search.fieldcollapse.collector.aggregate.AverageFunction\"/>\n\t\t\t<function name=\"min\" class=\"org.apache.solr.search.fieldcollapse.collector.aggregate.MinFunction\"/>\n\t\t\t<function name=\"max\" class=\"org.apache.solr.search.fieldcollapse.collector.aggregate.MaxFunction\"/>\n\t\t</collapseCollectorFactory>\n\t</searchComponent>\n\nSo far it does what I expect from it without additional factories mentioned, but still it bothers me that it fails when they're listed. Maybe I placed the libraries in a wrong place? "
        },
        {
            "author": "Yuriy Akopov",
            "id": "comment-13028248",
            "date": "2011-05-03T14:37:53+0000",
            "content": "Hi and sorry for breaking the silence.\n\nSo far the patch is working okay in our system, thanks again.\n\nHowever I've noticed that the collapse.facet parameter set to 'after' doesn't produce very precise figures. When results are collapsed, it may give, say, 366 results for the facet item while actually there are 396 returned by Solr after collapsing.\n\nThe figures are never very different from the actual ones but they vary in some narrow interval. I mean, for number of results up to 10000 they differ by <100 only. My collapsing-related part of the query is the following:\n\n\t$search_options['qt']\t\t\t= 'collapse';\n\t$search_options['collapse.field']\t= 'my_string_field'; // name of the field to collapse on, in my case it is a string field\n\t$search_options['collapse.type']\t= 'normal'; // it is always 'normal' and never 'adjacent' in my case\n\t$search_options['collapse.facet']\t= 'after';\n\nWhen collapsing is turned off, facet figures are calculated precisely, as expected. Have anybody else experienced that, and if so, is there a solution available? Thanks in advance. "
        },
        {
            "author": "Stephen Weiss",
            "id": "comment-13028252",
            "date": "2011-05-03T14:48:17+0000",
            "content": "Yes, I've had this too:\n\nhttps://issues.apache.org/jira/browse/SOLR-236?focusedCommentId=12655750&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12655750\n\nI'm pretty sure I know the reason for it, but I don't know how to fix it... to the best of my knowledge no one on the ticket really said if the problem could be fixed or not yet either.  At the moment we just use facet.before and explain to our users that the facets are for \"unfiltered\" results...  almost no one complains once we explain it to them.  However, a fix would be wonderful... people ask about it often enough that clearly it's not very intuitive. "
        },
        {
            "author": "Yuriy Akopov",
            "id": "comment-13028256",
            "date": "2011-05-03T14:54:06+0000",
            "content": "Thanks, Stephen. So it isn't just me doing something else wrong.\n\nI'm thinking of displaying not the actual figures against the facet items but something like 100+, 200+, 300+ etc. Should be okay as the difference is not dramatic but seems to remain within the relatively narrow interval. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043864",
            "date": "2011-06-03T16:47:27+0000",
            "content": "Bulk move 3.2 -> 3.3 "
        },
        {
            "author": "Yuriy Akopov",
            "id": "comment-13051888",
            "date": "2011-06-20T09:49:54+0000",
            "content": "I am trying to migrate from Solr 1.4.1 to Solr 3.2 and so I need to patch the 3.2 branch.\n\nWhen I use \"SOLR-236-branch_3x.patch\" file on the dev/tags/release-3.2 branch WAR file is built successfully but then it fails on loading with \"org.apache.solr.common.SolrException: Error loading class 'org.apache.solr.handler.component.CollapseComponent'\" message as if the collapsing functionality was not implemented.\n\nShould I try using 1.4.1 patch instead on 3.2 sources? That doesn't feel right but maybe they're compatible, I don't know. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13051995",
            "date": "2011-06-20T14:44:50+0000",
            "content": "I think you should consider the group by now included in 3_x branch (SOLR-2524 was recently committed) "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13052111",
            "date": "2011-06-20T18:06:15+0000",
            "content": "Resolving this looooon issue as a duplicate of SOLR-2524, which brings grouping (finally!) to Solr 3.x via the new (factored out from Solr's trunk grouping impl then backported to 3.x) grouping module. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13058956",
            "date": "2011-07-02T02:43:13+0000",
            "content": "Bulk close for 3.3 "
        },
        {
            "author": "kishore padman",
            "id": "comment-13170207",
            "date": "2011-12-15T13:57:52+0000",
            "content": "Hi,\n\nI have applied these 2 patches to solr1.4.1 for the field collapsing.\n\nApply patch SOLR-236-1_4_1-paging-totals-working.patch\nApply patch SOLR-236-1_4_1-NPEfix.patch\n\nThe collapsing works fine, and facet counts shows correctly on the collpased records as I am using collpase.facet=after.\nBut when a filter is done on a facet, all the corresponding facet counts is calculated on the basis of uncollapsed records.\n\nHas anyone faced this issue.Please let me know the resolution\n\nThanks\nKishore Padman "
        }
    ]
}