{
    "id": "SOLR-11417",
    "title": "Crashed leader's hanging emphemral will make restarting followers stuck in recovering",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "fix_versions": [
            "7.3"
        ],
        "affect_versions": "6.3",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "If replicas are starting up after leader crash and within the ZK session timeout, replicas\n\n\twill lose leader election due to hanging ephemerals\n\twill read stale data from ZK about current leader\n\twill fail recovery and stuck in recovering state\n\n\n\nIf leader is down permanently (eg. hardware failure) and all replicas are affected, shard will not come up (see also SOLR-7065).\n\nTested on 6.3. See attached image for details.",
    "attachments": {
        "SOLR-11417.png": "https://issues.apache.org/jira/secure/attachment/12889518/SOLR-11417.png"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-09-29T00:19:19+0000",
            "content": "I'd have to look closer, but my first thought would be, we should make sure that if a replica comes up and attempts to be leader but fails due to a connection issue, it should make sure it can still successfully be a leader in a later attempt.\n\nwill read stale data from ZK about current leader\n\nWe can always read stale data, so we have to make sure still operate correctly in the face of that.\n\nwill fail recovery and stuck in recovering state\n\nI guess the key question is, why are we stuck in recovering? What happens so that we cannot be leader when the ephemeral goes away? Do we overwrite the last active stat as not ACTIVE? In that case, it seems we should try and prevent that when a replica simply has connection problems with the leader vs failing a sync or something. ",
            "author": "Mark Miller",
            "id": "comment-16185154"
        },
        {
            "date": "2017-09-29T09:17:35+0000",
            "content": "Thank you Mark Miller for the followup.\n\nBased on your comment, I see to things to change and either would prevent this scenario:\nWe can always read stale data, so we have to make sure still operate correctly in the face of that.\nThe ZkController.getLeader() is currently double checks the leader, but only in ZK. If we would also make sure if the leader confirms that he is the leader, we could make it more robust.\n\nI guess the key question is, why are we stuck in recovering? What happens so that we cannot be leader when the ephemeral goes away? Do we overwrite the last active stat as not ACTIVE? In that case, it seems we should try and prevent that when a replica simply has connection problems with the leader vs failing a sync or something.\n\nYes, the last active state becomes RECOVERING, so it won't participate anymore. We might want to restart the process if the failure is due to the leader.\n\nI will evaluate both directions and prepare a patch with at least one. ",
            "author": "Mano Kovacs",
            "id": "comment-16185552"
        },
        {
            "date": "2017-10-02T17:50:41+0000",
            "content": "becomes RECOVERING, so it won't participate anymore.\n\nMy first thought to try would be detecting a connection based error and in that case, use the method that publishes state but does not update the last state variable that gets checked.\n\nIt might even make sense to do that on any fail, not just connection errors - I'm not sure its preferable to have a replica disable it's own ability to be a leader - kind of defeats the repeated attempts. ",
            "author": "Mark Miller",
            "id": "comment-16188501"
        },
        {
            "date": "2017-10-03T14:21:42+0000",
            "content": "I'm not sure its preferable to have a replica disable it's own ability to be a leader - kind of defeats the repeated attempts.\nAgreed. Going with that approach. ",
            "author": "Mano Kovacs",
            "id": "comment-16189758"
        },
        {
            "date": "2018-07-06T08:38:52+0000",
            "content": "Fixed by\u00a0SOLR-12011 ",
            "author": "Mano Kovacs",
            "id": "comment-16534578"
        }
    ]
}