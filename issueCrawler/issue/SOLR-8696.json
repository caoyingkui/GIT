{
    "id": "SOLR-8696",
    "title": "Start the Overseer before actions that need the overseer on init and when reconnecting after zk expiration and improve init logic.",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [
            "6.0"
        ],
        "affect_versions": "5.4.1",
        "status": "Resolved",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "ZkController.publishAndWaitForDownStates() occurs before overseer election.  That means if there is currently no overseer, there is ironically no one to actually service the down state changes it's waiting on.  This particularly affects a single-node cluster such as you might run locally for development.\n\nAdditionally, we're doing an unnecessary ZkStateReader forced refresh on all Overseer operations.  This isn't necessary because ZkStateReader keeps itself up to date.",
    "attachments": {
        "SOLR-8696-followup.patch": "https://issues.apache.org/jira/secure/attachment/12790541/SOLR-8696-followup.patch",
        "SOLR-8696.patch": "https://issues.apache.org/jira/secure/attachment/12788507/SOLR-8696.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-02-18T19:40:25+0000",
            "author": "Scott Blum",
            "content": "Shalin Shekhar Mangar ",
            "id": "comment-15152951"
        },
        {
            "date": "2016-02-18T20:20:46+0000",
            "author": "Mark Miller",
            "content": "In one of the leader election improvement patches, I think I took the wait out altogether. Starting the Overseer election first makes sense though.\n\nAdditionally, we're doing an unnecessary ZkStateReader forced refresh on all Overseer operations. This isn't necessary because ZkStateReader keeps itself up to date.\n\nIt keeps itself up to date, but with some delay. Are we sure we don't need to know we have the latest state at that point? What about the case when we just took over for another Overseer? No races? ",
            "id": "comment-15153015"
        },
        {
            "date": "2016-02-18T20:37:52+0000",
            "author": "Scott Blum",
            "content": "It keeps itself up to date, but with some delay. Are we sure we don't need to know we have the latest state at that point? What about the case when we just took over for another Overseer? No races?\n\nForcing a refresh for that case (just took over) seems totally reasonable to me. ",
            "id": "comment-15153041"
        },
        {
            "date": "2016-02-18T21:41:38+0000",
            "author": "Scott Blum",
            "content": "Actually on second thought, to become the new Overseer, you would have had to observe the ephemeral node disappear for the previous overseer.  If you've observed that, and subsequently updated yourself as overseer leader, you would have had to observe all previous cluster state changes committed by the previous Overseer.  So I think it's actually fine in that case. ",
            "id": "comment-15153164"
        },
        {
            "date": "2016-02-18T22:18:37+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks Scott.\n\nThe ZkStateReader refresh is a safe guard to ensure that before processing any operation, the effects of previous operations are reflected in the local cluster state. This is less important now because most operations do wait for the cluster state to reflect their results (e.g. collection creation/deletion) etc.\n\nI audited all overseer operations and the following operations do not have such a wait loop:\n\n\tADDREPLICA does not wait for the replica to show up in the local cluster state (which may cause the same core name to be assigned again for a subsequent addReplica operation)\n\tADDREPLICAPROP\n\tDELETEREPLICAPROP\n\tBALANCESHARDUNIQUE\n\tMODIFYCOLLECTION\n\n\n\nOf all these only add replica should be fixed. I don't foresee any damage for the others. ",
            "id": "comment-15153226"
        },
        {
            "date": "2016-02-18T22:39:47+0000",
            "author": "Scott Blum",
            "content": "Great!  Would you like me to include a fix for ADDREPLICA in this patch?  Seems like a good thing to have regardless. ",
            "id": "comment-15153267"
        },
        {
            "date": "2016-02-18T22:41:16+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "That'd be great, thanks! ",
            "id": "comment-15153270"
        },
        {
            "date": "2016-02-18T23:12:13+0000",
            "author": "Scott Blum",
            "content": "Shalin Shekhar Mangar Am I crazy or doesn't the call to waitToSeeReplicasInState() do what you're talking about?  Loop until the new replica appears in the local cluster state?\n\nI tried to find a way to step through, but I don't actually see an AddReplicaTest.... ",
            "id": "comment-15153309"
        },
        {
            "date": "2016-02-18T23:18:14+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Yeah but that only happens for clusters configured with legacyCloud=false.\n\nTo give you some context behind this: the default is legacyCloud=true which means that any core admin create command can create corresponding replica in clusterstate. This also means that if you deleted a replica which was down at the time, it may re-create itself in the cluster state once it comes back up. To avoid such problems, a new cluster property called 'legacyCloud' was introduced which required all cluster state modifications to be from the overseer only. The idea was that this \"Zookeeper as truth\" mode would eventually become the default but we haven't made much progress since. ",
            "id": "comment-15153321"
        },
        {
            "date": "2016-02-19T00:56:39+0000",
            "author": "Scott Blum",
            "content": "Okay, I can't figure this out after digging for quite a while.  I need more help with this!  I can't figure out how the cluster state is getting update in legacy mode.  I've tried putting breakpoints in ZkStateWriter, SliceMutator.addReplica, etc, and run the following tests:\n\nCollectionsAPISolrJTests.testAddAndDeleteReplica\nCollectionsAPIDistributedZkTest.addReplicaTest\n\nBut I'm not getting a hit.  What gives? ",
            "id": "comment-15153460"
        },
        {
            "date": "2016-02-19T19:47:00+0000",
            "author": "Mark Miller",
            "content": "I made a run at removing legacyMode before the 6 release, but while it was pretty easy to take from non test code, it requires a really large change to the tests to move away from it. ",
            "id": "comment-15154744"
        },
        {
            "date": "2016-02-20T00:28:59+0000",
            "author": "Mark Miller",
            "content": "But I'm not getting a hit. What gives?\n\nI'm confused. Does that test even try to run in legacy mode? Can you elaborate a bit? Not sure I fully understand. If I set a break point at SliceMutator.addReplica and run CollectionsAPISolrJTests.testAddAndDeleteReplica, I hit the break point. What other change are you making? ",
            "id": "comment-15155214"
        },
        {
            "date": "2016-02-20T20:56:57+0000",
            "author": "Scott Blum",
            "content": "What git sha are you on?  I'm absolutely not hitting that break point running that exact test.  I'm at 091889cf79e15909963b6fad6c0a5394a34764bc.   The only local changes I have are to comment out the calls to the other tests in that class so that testAddAndDeleteReplica is the only test method run.\n\nNor am I seeing this log message in the test logs:\n\n\n  public ZkWriteCommand addReplica(ClusterState clusterState, ZkNodeProps message) {\n    log.info(\"createReplica() {} \", message);\n\n ",
            "id": "comment-15155750"
        },
        {
            "date": "2016-02-20T22:03:48+0000",
            "author": "Mark Miller",
            "content": "I was on recent master at the time.\n\nI'll replicate and report steps and results. ",
            "id": "comment-15155772"
        },
        {
            "date": "2016-02-20T22:04:20+0000",
            "author": "Mark Miller",
            "content": "The only local changes I have are to comment out the calls to the other tests in that class so that testAddAndDeleteReplica is the only test method run.\n\nSame single change I made. Strange. ",
            "id": "comment-15155773"
        },
        {
            "date": "2016-02-20T23:48:00+0000",
            "author": "Mark Miller",
            "content": "SliceMutator.addReplica\n\nAh, got you sorry, check out: OverseerCollectionMessageHandler#addReplica? ",
            "id": "comment-15155815"
        },
        {
            "date": "2016-02-20T23:53:27+0000",
            "author": "Mark Miller",
            "content": "I'd love to get legacy mode out for 6, but I doubt I'll have the time to tackle it myself. The code change is pretty easy, the test framework ramifications are quite large.\n\nSOLR-8256: Remove cluster setting 'legacy cloud' in 6x. ",
            "id": "comment-15155817"
        },
        {
            "date": "2016-02-21T04:08:27+0000",
            "author": "Scott Blum",
            "content": "Mark Miller\n\nYes, I get the breakpoint in OverseerCollectionMessageHandler#addReplica.  I'm trying to follow up on Shalin Shekhar Mangar's request to add a wait loop at the end of that method.  But where I'm struggling is, I don't actually understand where and how the ZK cluster state ever gets updated in legacy mode.  Who updates the ZK cluster state as a result of OverseerCollectionMessageHandler#addReplica? ",
            "id": "comment-15155888"
        },
        {
            "date": "2016-02-21T04:20:32+0000",
            "author": "Mark Miller",
            "content": "addReplica calls create core on a node. When a core is created, it registers with zk and you have the new replica. Just doing a state update on registering will get it in the clusterstate. ",
            "id": "comment-15155890"
        },
        {
            "date": "2016-02-23T00:11:10+0000",
            "author": "Mark Miller",
            "content": "The history of legacy mode:\n\nInitially, we had no collections API. Collections were created by a SolrCore registering with ZK and doing state updates. This allowed you to pre-configure SolrCores in solr.xml as you could with non SolrCloud. It was basically a way to quickly get going. But it has many problems.\n\nEventually we got the collections API and for back compat reasons added the legacyMode property. When not in legacy mode, state updates will not created collections - you must explicitly use the collections API. It's the start of what we call \"zookeeper=truth\" though currently only a small part of that is implemented. ",
            "id": "comment-15157991"
        },
        {
            "date": "2016-02-23T03:30:38+0000",
            "author": "Mark Miller",
            "content": "I think we should break up these two changes. Can we fix the nasty overseer init on reconnect bug here and open another to continue on the explicit state update? ",
            "id": "comment-15158216"
        },
        {
            "date": "2016-02-23T15:54:14+0000",
            "author": "Mark Miller",
            "content": "New patch. Fixes the same overseer situation on zk reconnect (I've seen the logging that shows this problem is really and kind of ugly there too) and removes the forced refresh change. ",
            "id": "comment-15159076"
        },
        {
            "date": "2016-02-23T16:00:38+0000",
            "author": "Mark Miller",
            "content": "Need to remove the duplicate call to zkStateReader.createClusterStateWatchersAndUpdate(); ",
            "id": "comment-15159086"
        },
        {
            "date": "2016-02-23T18:27:43+0000",
            "author": "Scott Blum",
            "content": "Splitting it up sounds like a great idea. ",
            "id": "comment-15159373"
        },
        {
            "date": "2016-02-23T18:29:58+0000",
            "author": "Scott Blum",
            "content": "https://issues.apache.org/jira/browse/SOLR-8722 ",
            "id": "comment-15159378"
        },
        {
            "date": "2016-02-23T18:32:42+0000",
            "author": "Scott Blum",
            "content": "Patch LGTM.  On an unrelated note, I've never understood why it's so important to register all cores as down on startup, since presumably we're about to bring them all up.. ",
            "id": "comment-15159381"
        },
        {
            "date": "2016-02-23T18:38:05+0000",
            "author": "Mark Miller",
            "content": "The stale state could say ACTIVE. No guarantee shutdown publishes DOWN (a crash won't).\n\nThe one in init, I'm less sure of. I think Timothy Potter may have had to add that when he worked on recovering from partitions properly. ",
            "id": "comment-15159390"
        },
        {
            "date": "2016-02-23T18:54:04+0000",
            "author": "Scott Blum",
            "content": "Thanks.  Way beyond the scope of this ticket, I was just curious about it.  It seemed a little silly on the face of it, because solr already has to tolerate long periods of things being marked ACTIVE in cluster state that aren't really up, since a node can crash and take a long time to recover.  I feel like all a recovering node really needs to do is verify that all the cores it owns are in the correct state immediately before publishing its live node entry.  We could save a lot of unnecessary overseer ops here on node restart. ",
            "id": "comment-15159413"
        },
        {
            "date": "2016-02-23T18:59:30+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 8ac4fdd6bb84b225919d30f6073e7dad02aeb0a1 in lucene-solr's branch refs/heads/master from markrmiller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=8ac4fdd ]\n\nSOLR-8696: Start the Overseer before actions that need the overseer on init and when reconnecting after zk expiration and improve init logic. ",
            "id": "comment-15159426"
        },
        {
            "date": "2016-02-23T19:00:46+0000",
            "author": "Mark Miller",
            "content": "Thanks Scott! ",
            "id": "comment-15159430"
        },
        {
            "date": "2016-02-25T01:57:23+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 8ac4fdd6bb84b225919d30f6073e7dad02aeb0a1 in lucene-solr's branch refs/heads/jira/SOLR-445 from markrmiller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=8ac4fdd ]\n\nSOLR-8696: Start the Overseer before actions that need the overseer on init and when reconnecting after zk expiration and improve init logic. ",
            "id": "comment-15166546"
        },
        {
            "date": "2016-02-25T06:18:54+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks guys and sorry for not paying attention here. ",
            "id": "comment-15166791"
        },
        {
            "date": "2016-02-29T19:39:00+0000",
            "author": "Scott Blum",
            "content": "Mark Miller Shalin Shekhar Mangar\n\nI think there's a slight problem with the fix as landed (merge artifact?)\n\n\n      zkStateReader.createClusterStateWatchersAndUpdate();\n      Stat stat = zkClient.exists(ZkStateReader.LIVE_NODES_ZKNODE, null, true);\n      if (stat != null && stat.getNumChildren() > 0) {\n        zkStateReader.createClusterStateWatchersAndUpdate();\n        publishAndWaitForDownStates();\n      }\n\n\n\ncreateClusterStateWatchersAndUpdate() shouldn't be called twice, as it sets up duplicate watchers.  I actually think we should just have a single call at the top, right after createClusterZkNodes() and right before joining the overseer election, so that if we get elected we have a valid ClusterState to start with.\n\n(For the record, I'm not super happy with the fact that external code needs to worry so much about initializing ZkStateReader exactly once)\n\nAttached a patch with a suggested fix. ",
            "id": "comment-15172458"
        },
        {
            "date": "2016-02-29T19:44:27+0000",
            "author": "Mark Miller",
            "content": "Strange, I'll have to look at the code. I had though I had fixed that:\n\nNeed to remove the duplicate call to zkStateReader.createClusterStateWatchersAndUpdate();\n\nHave a memory of it, but perhaps it didn't get staged right or something. ",
            "id": "comment-15172473"
        },
        {
            "date": "2016-02-29T21:09:34+0000",
            "author": "ASF subversion and git services",
            "content": "Commit a9aec24236df61a3f1cfe533b64169fae84fc6f7 in lucene-solr's branch refs/heads/master from Mark Miller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a9aec24 ]\n\nSOLR-8696: Straighten out calls to ZkStateReader#createClusterStateWatchersAndUpdate. ",
            "id": "comment-15172616"
        }
    ]
}