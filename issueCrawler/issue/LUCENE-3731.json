{
    "id": "LUCENE-3731",
    "title": "Create a analysis/uima module for UIMA based tokenizers/analyzers",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "As discussed in SOLR-3013 the UIMA Tokenizers/Analyzer should be refactored out in a separate module (modules/analysis/uima) as they can be used in plain Lucene. Then the solr/contrib/uima will contain only the related factories.",
    "attachments": {
        "LUCENE-3731_speed.patch": "https://issues.apache.org/jira/secure/attachment/12514659/LUCENE-3731_speed.patch",
        "LUCENE-3731_3.patch": "https://issues.apache.org/jira/secure/attachment/12514346/LUCENE-3731_3.patch",
        "LUCENE-3731.patch": "https://issues.apache.org/jira/secure/attachment/12513135/LUCENE-3731.patch",
        "LUCENE-3731_2.patch": "https://issues.apache.org/jira/secure/attachment/12513931/LUCENE-3731_2.patch",
        "LUCENE-3731_rsrel.patch": "https://issues.apache.org/jira/secure/attachment/12514811/LUCENE-3731_rsrel.patch",
        "LUCENE-3731_4.patch": "https://issues.apache.org/jira/secure/attachment/12514478/LUCENE-3731_4.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-02-03T15:18:17+0000",
            "content": "this patch adds a modules/analysis/uima module ",
            "author": "Tommaso Teofili",
            "id": "comment-13199762"
        },
        {
            "date": "2012-02-03T15:27:20+0000",
            "content": "Hi,\n\n\n+      clearAttributes();\n+      AnnotationFS next = iterator.next();\n+      termAttr.setEmpty();\n+      termAttr.append(next.getCoveredText());\n+      termAttr.setLength(next.getCoveredText().length());\n\n\n\nAs you clear the attributes already, the length of termAttr is 0, so setEmpty is not needed. termAttr.setLength() is also not useful, as append will initialize the length already. All you need is termAttr.append(next.getCoveredText());\n\nUwe ",
            "author": "Uwe Schindler",
            "id": "comment-13199770"
        },
        {
            "date": "2012-02-03T15:30:23+0000",
            "content": "right Uwe, thanks so much for the quick review  ",
            "author": "Tommaso Teofili",
            "id": "comment-13199771"
        },
        {
            "date": "2012-02-03T15:35:03+0000",
            "content": "Thanks for starting this Tommaso:\n\nI was unable to apply the patch (were there some svn-copies?)\n\nBut I suggest in general using the BaseTokenStreamTestCase.assertTokenStreamContents/assertAnalyzesTo:\ne.g. instead of:\n\n// check that 'the big brown fox jumped on the wood' tokens have the expected PoS types\n   String[] expectedPos = new String[]{\"at\", \"jj\", \"jj\", \"nn\", \"vbd\", \"in\", \"at\", \"nn\"};\n   int i = 0;\n   while (ts.incrementToken()) {\n     assertNotNull(offsetAtt);\n     assertNotNull(termAtt);\n     assertNotNull(typeAttr);\n     assertEquals(typeAttr.type(), expectedPos[i]);\n     i++;\n   }\n\n\n\nyou could use:\n\n   assertTokenStreamContents(ts, \n     new String[] { \"the\", \"big\", \"brown\", ... }, /* expected terms */\n     new String[] { \"at\", \"jj\", \"jj\", ... }, /* expected types */\n\n\n\nThere are also variants that let you supply expected start/end offsets, I think that would be good.\n\nFinally, to check for lots of other bugs (including thread-safety, compatibility with charfilters, etc),\nI would recommend:\n\n  /** blast some random strings through the analyzer */\n  public void testRandomStrings() throws Exception {\n    Analyzer a = new Analyzer() {\n\n      @Override\n      protected TokenStreamComponents createComponents(String fieldName, Reader reader) {\n        Tokenizer tokenizer = new MyTokenizer(reader);\n        return new TokenStreamComponents(tokenizer, tokenizer);\n      } \n    };\n    checkRandomData(random, a, 10000*RANDOM_MULTIPLIER);\n  }\n\n\n\nIf you look at BaseTokenStreamTestCase you will see all of these methods are insanely nitpicky\nand find all kinds of bugs in analysis components, so it will really help test coverage I think. ",
            "author": "Robert Muir",
            "id": "comment-13199778"
        },
        {
            "date": "2012-02-03T17:30:51+0000",
            "content": "Hey Robert, that's super, thanks! I'm going to collect your suggestions in a new patch shortly. ",
            "author": "Tommaso Teofili",
            "id": "comment-13199893"
        },
        {
            "date": "2012-02-09T08:58:50+0000",
            "content": "Updated patch which incorporates Robert's suggestions. \nThe random strings testing highlights some corner cases where the endOffset is not set correctly, probably due to Redear to String explicit conversion in BaseUIMATokenizer which needs to get rid of line.separator property.\n\nNew patch to fix the above will follow. ",
            "author": "Tommaso Teofili",
            "id": "comment-13204373"
        },
        {
            "date": "2012-02-13T12:24:34+0000",
            "content": "Updated patch which fixes corner cases with wrong endOffsets. ",
            "author": "Tommaso Teofili",
            "id": "comment-13206835"
        },
        {
            "date": "2012-02-13T12:33:15+0000",
            "content": "Hi Tommaso, I think it would be cleaner to set the final offset in end() instead? ",
            "author": "Robert Muir",
            "id": "comment-13206837"
        },
        {
            "date": "2012-02-13T17:27:59+0000",
            "content": "Hi Tommaso, I think it would be cleaner to set the final offset in end() instead?\n\nok, +1. ",
            "author": "Tommaso Teofili",
            "id": "comment-13206997"
        },
        {
            "date": "2012-02-14T11:16:10+0000",
            "content": "patch with finalOffset setting in the end() method. ",
            "author": "Tommaso Teofili",
            "id": "comment-13207636"
        },
        {
            "date": "2012-02-14T21:57:54+0000",
            "content": "I'm going to commit this one shortly if no one objects. ",
            "author": "Tommaso Teofili",
            "id": "comment-13208058"
        },
        {
            "date": "2012-02-14T22:15:23+0000",
            "content": "Thanks for factoring this out Tommaso. ",
            "author": "Robert Muir",
            "id": "comment-13208075"
        },
        {
            "date": "2012-02-14T22:15:38+0000",
            "content": "committed on trunk in r1244236 ",
            "author": "Tommaso Teofili",
            "id": "comment-13208076"
        },
        {
            "date": "2012-02-14T23:45:55+0000",
            "content": "Hi Tommaso,\n\nI just committed modifications to the IntelliJ IDEA and Maven configurations.\n\nSomething strange is happening, though: one test method consistently fails under both IntelliJ and Maven: UIMABaseAnalyzerTest.testRandomStrings().  However, under Ant, this always succeeds, including with the seeds that fail under either IntelliJ or Maven.  Also, under both IntelliJ and Maven, the following sequence is printed out literally thousands of times to STDERR (with increasing time stamps) - however, I don't see this at all under Ant:\n\n\nFeb 14, 2012 6:34:18 PM WhitespaceTokenizer initialize\nINFO: \"Whitespace tokenizer successfully initialized\"\nFeb 14, 2012 6:34:18 PM WhitespaceTokenizer typeSystemInit\nINFO: \"Whitespace tokenizer typesystem initialized\"\nFeb 14, 2012 6:34:18 PM WhitespaceTokenizer process\nINFO: \"Whitespace tokenizer starts processing\"\nFeb 14, 2012 6:34:18 PM WhitespaceTokenizer process\nINFO: \"Whitespace tokenizer finished processing\"\n\n\n\nHere are two different example failures, from Maven - they seem to have different causes, which is baffling:\n\n\nThe following exceptions were thrown by threads:\n*** Thread: Thread-1 ***\njava.lang.RuntimeException: java.io.IOException: org.apache.uima.analysis_engine.AnalysisEngineProcessException: Annotator processing failed.    \n        at org.apache.lucene.analysis.BaseTokenStreamTestCase$AnalysisThread.run(BaseTokenStreamTestCase.java:289)\nCaused by: java.io.IOException: org.apache.uima.analysis_engine.AnalysisEngineProcessException: Annotator processing failed.    \n        at org.apache.lucene.analysis.uima.UIMAAnnotationsTokenizer.incrementToken(UIMAAnnotationsTokenizer.java:73)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:333)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:295)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase$AnalysisThread.run(BaseTokenStreamTestCase.java:287)\nCaused by: org.apache.uima.analysis_engine.AnalysisEngineProcessException: Annotator processing failed.    \n        at org.apache.uima.analysis_engine.impl.PrimitiveAnalysisEngine_impl.callAnalysisComponentProcess(PrimitiveAnalysisEngine_impl.java:391)\n        at org.apache.uima.analysis_engine.impl.PrimitiveAnalysisEngine_impl.processAndOutputNewCASes(PrimitiveAnalysisEngine_impl.java:295)\n        at org.apache.uima.analysis_engine.asb.impl.ASB_impl$AggregateCasIterator.processUntilNextOutputCas(ASB_impl.java:567)\n        at org.apache.uima.analysis_engine.asb.impl.ASB_impl$AggregateCasIterator.&lt;init&gt;(ASB_impl.java:409)\n        at org.apache.uima.analysis_engine.asb.impl.ASB_impl.process(ASB_impl.java:342)\n        at org.apache.uima.analysis_engine.impl.AggregateAnalysisEngine_impl.processAndOutputNewCASes(AggregateAnalysisEngine_impl.java:267)\n        at org.apache.uima.analysis_engine.impl.AnalysisEngineImplBase.process(AnalysisEngineImplBase.java:267)\n        at org.apache.lucene.analysis.uima.BaseUIMATokenizer.analyzeInput(BaseUIMATokenizer.java:57)\n        at org.apache.lucene.analysis.uima.UIMAAnnotationsTokenizer.analyzeText(UIMAAnnotationsTokenizer.java:61)\n        at org.apache.lucene.analysis.uima.UIMAAnnotationsTokenizer.incrementToken(UIMAAnnotationsTokenizer.java:71)\n        ... 3 more\nCaused by: java.lang.NullPointerException\n        at org.apache.uima.impl.UimaContext_ImplBase$ComponentInfoImpl.mapToSofaID(UimaContext_ImplBase.java:655)\n        at org.apache.uima.cas.impl.CASImpl.getView(CASImpl.java:2646)\n        at org.apache.uima.jcas.impl.JCasImpl.getView(JCasImpl.java:1415)\n        at org.apache.uima.examples.tagger.HMMTagger.process(HMMTagger.java:250)\n        at org.apache.uima.analysis_component.JCasAnnotator_ImplBase.process(JCasAnnotator_ImplBase.java:48)\n        at org.apache.uima.analysis_engine.impl.PrimitiveAnalysisEngine_impl.callAnalysisComponentProcess(PrimitiveAnalysisEngine_impl.java:377)\n        ... 12 more\n*** Thread: Thread-2 ***\njava.lang.AssertionError: token 0 does not exist\n        at org.junit.Assert.fail(Assert.java:93)\n        at org.junit.Assert.assertTrue(Assert.java:43)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:121)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:371)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:295)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase$AnalysisThread.run(BaseTokenStreamTestCase.java:287)\nNOTE: reproduce with: ant test -Dtestcase=UIMABaseAnalyzerTest -Dtestmethod=testRandomStrings(org.apache.lucene.analysis.uima.UIMABaseAnalyzerTest) -Dtests.seed=-1dad4a7ede576939:-f9f5c77dffb3eb0:607bf59bf7da50eb -Dargs=&quot;-Dfile.encoding=Cp1252&quot;\n\n\n\n\nThe following exceptions were thrown by threads:\n*** Thread: Thread-5 ***\njava.lang.RuntimeException: java.io.IOException: org.apache.uima.analysis_engine.AnalysisEngineProcessException\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase$AnalysisThread.run(BaseTokenStreamTestCase.java:289)\nCaused by: java.io.IOException: org.apache.uima.analysis_engine.AnalysisEngineProcessException\n        at org.apache.lucene.analysis.uima.UIMAAnnotationsTokenizer.incrementToken(UIMAAnnotationsTokenizer.java:73)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:121)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:371)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:295)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase$AnalysisThread.run(BaseTokenStreamTestCase.java:287)\nCaused by: org.apache.uima.analysis_engine.AnalysisEngineProcessException\n        at org.apache.uima.analysis_engine.asb.impl.ASB_impl$AggregateCasIterator.processUntilNextOutputCas(ASB_impl.java:701)\n        at org.apache.uima.analysis_engine.asb.impl.ASB_impl$AggregateCasIterator.&lt;init&gt;(ASB_impl.java:409)\n        at org.apache.uima.analysis_engine.asb.impl.ASB_impl.process(ASB_impl.java:342)\n        at org.apache.uima.analysis_engine.impl.AggregateAnalysisEngine_impl.processAndOutputNewCASes(AggregateAnalysisEngine_impl.java:267)\n        at org.apache.uima.analysis_engine.impl.AnalysisEngineImplBase.process(AnalysisEngineImplBase.java:267)\n        at org.apache.lucene.analysis.uima.BaseUIMATokenizer.analyzeInput(BaseUIMATokenizer.java:57)\n        at org.apache.lucene.analysis.uima.UIMAAnnotationsTokenizer.analyzeText(UIMAAnnotationsTokenizer.java:61)\n        at org.apache.lucene.analysis.uima.UIMAAnnotationsTokenizer.incrementToken(UIMAAnnotationsTokenizer.java:71)\n        ... 4 more\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 2\n        at java.util.ArrayList.RangeCheck(ArrayList.java:547)\n        at java.util.ArrayList.get(ArrayList.java:322)\n        at org.apache.uima.flow.impl.FixedFlowController$FixedFlowObject.next(FixedFlowController.java:222)\n        at org.apache.uima.analysis_engine.asb.impl.FlowContainer.next(FlowContainer.java:100)\n        at org.apache.uima.analysis_engine.asb.impl.ASB_impl$AggregateCasIterator.processUntilNextOutputCas(ASB_impl.java:546)\n        ... 11 more\n*** Thread: Thread-7 ***\njava.lang.RuntimeException: java.io.IOException: org.apache.uima.analysis_engine.AnalysisEngineProcessException\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase$AnalysisThread.run(BaseTokenStreamTestCase.java:289)\nCaused by: java.io.IOException: org.apache.uima.analysis_engine.AnalysisEngineProcessException\n        at org.apache.lucene.analysis.uima.UIMAAnnotationsTokenizer.incrementToken(UIMAAnnotationsTokenizer.java:73)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:121)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:371)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:295)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase$AnalysisThread.run(BaseTokenStreamTestCase.java:287)\nCaused by: org.apache.uima.analysis_engine.AnalysisEngineProcessException\n        at org.apache.uima.analysis_engine.asb.impl.ASB_impl$AggregateCasIterator.processUntilNextOutputCas(ASB_impl.java:701)\n        at org.apache.uima.analysis_engine.asb.impl.ASB_impl$AggregateCasIterator.&lt;init&gt;(ASB_impl.java:409)\n        at org.apache.uima.analysis_engine.asb.impl.ASB_impl.process(ASB_impl.java:342)\n        at org.apache.uima.analysis_engine.impl.AggregateAnalysisEngine_impl.processAndOutputNewCASes(AggregateAnalysisEngine_impl.java:267)\n        at org.apache.uima.analysis_engine.impl.AnalysisEngineImplBase.process(AnalysisEngineImplBase.java:267)\n        at org.apache.lucene.analysis.uima.BaseUIMATokenizer.analyzeInput(BaseUIMATokenizer.java:57)\n        at org.apache.lucene.analysis.uima.UIMAAnnotationsTokenizer.analyzeText(UIMAAnnotationsTokenizer.java:61)\n        at org.apache.lucene.analysis.uima.UIMAAnnotationsTokenizer.incrementToken(UIMAAnnotationsTokenizer.java:71)\n        ... 4 more\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 2\n        at java.util.ArrayList.RangeCheck(ArrayList.java:547)\n        at java.util.ArrayList.get(ArrayList.java:322)\n        at org.apache.uima.flow.impl.FixedFlowController$FixedFlowObject.next(FixedFlowController.java:222)\n        at org.apache.uima.analysis_engine.asb.impl.FlowContainer.next(FlowContainer.java:100)\n        at org.apache.uima.analysis_engine.asb.impl.ASB_impl$AggregateCasIterator.processUntilNextOutputCas(ASB_impl.java:546)\n        ... 11 more\n*** Thread: Thread-6 ***\njava.lang.AssertionError: end of stream\n        at org.junit.Assert.fail(Assert.java:93)\n        at org.junit.Assert.assertTrue(Assert.java:43)\n        at org.junit.Assert.assertFalse(Assert.java:68)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:148)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:371)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:295)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase$AnalysisThread.run(BaseTokenStreamTestCase.java:287)\n*** Thread: Thread-4 ***\norg.junit.ComparisonFailure: term 8 expected:&lt;-[]&gt; but was:&lt;-[- f(]&gt;\n        at org.junit.Assert.assertEquals(Assert.java:125)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:124)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:371)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:295)\n        at org.apache.lucene.analysis.BaseTokenStreamTestCase$AnalysisThread.run(BaseTokenStreamTestCase.java:287)\nNOTE: reproduce with: ant test -Dtestcase=UIMABaseAnalyzerTest -Dtestmethod=testRandomStrings(org.apache.lucene.analysis.uima.UIMABaseAnalyzerTest) -Dtests.seed=2be0c24a1df9b25e:-42f203968285c6ed:5f8c85cdbae32724 -Dargs=&quot;-Dfile.encoding=Cp1252&quot;\n\n ",
            "author": "Steve Rowe",
            "id": "comment-13208129"
        },
        {
            "date": "2012-02-15T00:04:57+0000",
            "content": "Thank you very much Steven for reporting.\n\nThe \n\nFeb 14, 2012 6:34:18 PM WhitespaceTokenizer initialize\nINFO: \"Whitespace tokenizer successfully initialized\"\nFeb 14, 2012 6:34:18 PM WhitespaceTokenizer typeSystemInit\nINFO: \"Whitespace tokenizer typesystem initialized\"\n\n\n\nmessages are due to UIMA WhitespaceTokenizer Annotator which logs the initialization/processing/etc. calls.\nThat is printed out many times because the testRandomStrings test method just does lots of tricky tests on the UIMABaseAnalyzer which require the above calls to be executed repeatedly.\n\nI'll take a look to the other failures which didn't show up on the tests I had done till now. ",
            "author": "Tommaso Teofili",
            "id": "comment-13208145"
        },
        {
            "date": "2012-02-15T11:45:17+0000",
            "content": "Ok, I noticed this was due to an issue on the UIMA side.\nI think the best option (as those are used just for testing) is to use a dummy implementation of both UIMA based whitespace tokenizer and PoS tagger thus also avoiding the log lines when executing tests using Maven. ",
            "author": "Tommaso Teofili",
            "id": "comment-13208393"
        },
        {
            "date": "2012-02-15T13:28:04+0000",
            "content": "fix for the issues reported by Steven committed in r1244474 ",
            "author": "Tommaso Teofili",
            "id": "comment-13208460"
        },
        {
            "date": "2012-02-15T16:42:52+0000",
            "content": "Hi Tommaso, I noticed some of the tests here are really slow... digging in I think we have some perf issues:\n\njunit-sequential:\n   [junit] Testsuite: org.apache.lucene.analysis.uima.UIMABaseAnalyzerTest\n   [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 52.302 sec\n   [junit]\n   [junit] Testsuite: org.apache.lucene.analysis.uima.UIMATypeAwareAnalyzerTest\n   [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 38.506 sec\n   [junit]\n   [junit] Testsuite: org.apache.lucene.analysis.uima.ae.BasicAEProviderTest\n   [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.02 sec\n   [junit]\n   [junit] Testsuite: org.apache.lucene.analysis.uima.ae.OverridingParamsAEProviderTest\n   [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 1.042 sec\n   [junit]\n\n\n\nSee the patch below (which cannot be committed yet)... currently i had to add sync around the analysis engine, but it avoids recreating the CAS for each document... which seems to cause the perf problems\n\n\njunit-sequential:\n    [junit] Testsuite: org.apache.lucene.analysis.uima.UIMABaseAnalyzerTest\n    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 3.858 sec\n    [junit] \n    [junit] Testsuite: org.apache.lucene.analysis.uima.UIMATypeAwareAnalyzerTest\n    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.731 sec\n    [junit] \n    [junit] Testsuite: org.apache.lucene.analysis.uima.ae.BasicAEProviderTest\n    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.009 sec\n    [junit] \n    [junit] Testsuite: org.apache.lucene.analysis.uima.ae.OverridingParamsAEProviderTest\n    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 1.058 sec\n    [junit] \n\n ",
            "author": "Robert Muir",
            "id": "comment-13208571"
        },
        {
            "date": "2012-02-15T17:25:46+0000",
            "content": "Hi Robert,\nreusing the CAS is good, as you note in the patch we need to take care of how to let each tokenizer instance get its own AE, in the previous Solr version core names were used to cache and get AEs.\nAs said on dev@ we may start with letting each tokenizer have its own AE and then improve the design once concurrency is fixed.\nI'm doing tests with other types of UIMA Flow controllers, right now the WhiteboardFlowController seems to behave slightly better. ",
            "author": "Tommaso Teofili",
            "id": "comment-13208595"
        },
        {
            "date": "2012-02-15T17:39:44+0000",
            "content": "Tommaso, I will make another prototype patch trying this approach.\n\nIn my opinion the caching done in BasicAEProvider/OverridingParamsAEProvider would still useful even with \nallowing each tokenstream to have a new AE, because we would just cache the description itself \n(so we e.g. only parse xml a single time), butreturn a new AE each time... then we could remove the \nsynchronized and still avoid a 'heavy' construction for first time initialization of a new thread \n(after that, the tokenstream is reused, so there is no issue).\n\nIll see how it goes and upload a patch if I can make it look nice. ",
            "author": "Robert Muir",
            "id": "comment-13208609"
        },
        {
            "date": "2012-02-15T18:14:20+0000",
            "content": "updated patch:\n\n\teach tokenstream gets its own AnalysisEngine instance\n\tproviders only cache the description itself, but return new instances.\n\treuse logic factored into BaseUIMATokenizer class\n\trelease CAS/AE resources in BaseUIMATokenizer.close()\n\n\n\nNow there are only a few nocommits, in the providers, but these are mostly nitpicks and more minor ",
            "author": "Robert Muir",
            "id": "comment-13208628"
        },
        {
            "date": "2012-02-15T18:43:34+0000",
            "content": "ok new patch, with more refactoring... no more nocommits \n\nnow the OverridingParams provider just extends the base one, but sets configuration.\n\nalso the xml inputstream is closed. ",
            "author": "Robert Muir",
            "id": "comment-13208662"
        },
        {
            "date": "2012-02-15T20:14:32+0000",
            "content": "Thanks Robert for taking care of this, nice improvement \nI agree on the OverridingParams extending the base one, it was also my intent to do that. ",
            "author": "Tommaso Teofili",
            "id": "comment-13208753"
        },
        {
            "date": "2012-02-15T20:20:41+0000",
            "content": "OK, if there is no objection I will commit this one. \n\nI think it will fix the jenkins fails... of course sometimes it takes\na few days of jenkins chewing on it to be sure ",
            "author": "Robert Muir",
            "id": "comment-13208761"
        },
        {
            "date": "2012-02-15T20:26:57+0000",
            "content": "OK, if there is no objection I will commit this one.\n\n+1, I'll post my progress on other possible improvements in performances I'm testing later. ",
            "author": "Tommaso Teofili",
            "id": "comment-13208766"
        },
        {
            "date": "2012-02-15T20:54:09+0000",
            "content": "Thanks Tommaso: i committed this.\n\nAlso a tiny change to end() methods:\n\n   public void end() throws IOException {\n-    if (offsetAttr.endOffset() < finalOffset)\n-      offsetAttr.setOffset(finalOffset, finalOffset);\n+    offsetAttr.setOffset(finalOffset, finalOffset);\n     super.end();\n   }\n\n\n\nUnless there is a bug, we should not need the if...\nNot sure if we should be reading the attribute values at this\nstage and if thats defined either, and if endOffset is somehow\npast the reader's final offset, well we are already in trouble \n\nI ran the tests many times and with -Dtests.multiplier=100 and there\nwere no issues. ",
            "author": "Robert Muir",
            "id": "comment-13208784"
        },
        {
            "date": "2012-02-16T10:15:30+0000",
            "content": "Right, everything seems ok now.\nI also tried to comment the \n\n<property name=\"tests.threadspercpu\" value=\"0\" />\n\n\nline in build.xml in order to execute tests in parallel.\nMultiple parallel tests executions, with also -Dtests.multiplier=100, with Java6 passed flawlessly; will see if that is the case for Java7 too. ",
            "author": "Tommaso Teofili",
            "id": "comment-13209247"
        },
        {
            "date": "2012-02-16T12:12:32+0000",
            "content": "some improvement in performance came out releasing the CAS and AE on close() call\n\n\n  @Override\n  public void close() throws IOException {\n    super.close();\n    // release UIMA resources\n    cas.release();\n    ae.destroy();\n  }\n\n\n\nNow investigating the use of CASPool for improving throughput on high usages scenarios. ",
            "author": "Tommaso Teofili",
            "id": "comment-13209301"
        },
        {
            "date": "2012-02-16T12:17:20+0000",
            "content": "Is that safe to do in Tokenizer.close() ?\n\nBecause Tokenizer.close() is misleading/confusing, the instance is still reused after \nthis for subsequent documents... in other words Tokenizer.close() closes resources like\nthe Reader itself... it just happens to be that CAS/AE don't complain about you \ncontinuing to use them after they are release()'ed/destroy()'ed  ",
            "author": "Robert Muir",
            "id": "comment-13209304"
        },
        {
            "date": "2012-02-16T15:39:18+0000",
            "content": "Because Tokenizer.close() is misleading/confusing, the instance is still reused after \nthis for subsequent documents.\n\nWhen I call close() it looks the correct way one could reuse that Tokenizer instance is by calling reset(someOtherInput) before doing anything else, so, after adding \n\n\nassert reader != null : \"input has been closed, please reset it\";\n\n\n\nas first line inside the toString(Reader reader) method in BaseUIMATokenizer, I tried this test:\n\n  @Test\n  public void testSetReaderAndClose() throws Exception {\n    StringReader input = new StringReader(\"the big brown fox jumped on the wood\");\n    Tokenizer t = new UIMAAnnotationsTokenizer(\"/uima/AggregateSentenceAE.xml\", \"org.apache.uima.TokenAnnotation\", input);\n    assertTokenStreamContents(t, new String[]{\"the\", \"big\", \"brown\", \"fox\", \"jumped\", \"on\", \"the\", \"wood\"});\n    t.close();\n    try {\n      t.incrementToken();\n      fail(\"should've been failing as reader is not set\");\n    } catch (AssertionError error) {\n      // ok\n    }\n    input = new StringReader(\"hi oh my\");\n    t = new UIMAAnnotationsTokenizer(\"/uima/TestAggregateSentenceAE.xml\", \"org.apache.lucene.uima.ts.TokenAnnotation\", input);\n    assertTrue(\"should've been incremented \", t.incrementToken());\n    t.close();\n    try {\n      t.incrementToken();\n      fail(\"should've been failing as reader is not set\");\n    } catch (AssertionError error) {\n      // ok\n    }\n    t.reset(new StringReader(\"hey what do you say\"));\n    assertTrue(\"should've been incremented \", t.incrementToken());\n  }\n\n\n\n\nand it looks to me it's behaving correctly.\nStill working on improving it and trying to catch possible corner cases. ",
            "author": "Tommaso Teofili",
            "id": "comment-13209439"
        },
        {
            "date": "2012-02-16T16:17:37+0000",
            "content": "Right, after you reset(Reader) you set a new reader.\n\nBut the question is: is it safe to use CAS/AE after you call release()/destroy() on them?\n\nBecause close() is called on tokenstreams after each invocation, in other words:\n\nTokenizer t = new Tokenizer(reader);\n... stuff ...\nt.close();\nt.reset(someOtherReader);\n.. stuff ...\nt.close();\n\n\n\nSo what does CAS.release() really mean? If it means you should not use the CAS again afterwards,\nthen we cannot have it in TokenStream.close(), and same with AE.destroy() ",
            "author": "Robert Muir",
            "id": "comment-13209474"
        },
        {
            "date": "2012-02-16T16:47:29+0000",
            "content": "But the question is: is it safe to use CAS/AE after you call release()/destroy() on them?\n\nno it isn't, so you're right: those methods should not be inside the close() method.\n\n ",
            "author": "Tommaso Teofili",
            "id": "comment-13209490"
        },
        {
            "date": "2012-02-22T23:10:31+0000",
            "content": "After some more testing I think the CasPool is good just for scenarios where the pool serves different CAS to different clients (the tokenizers), so not really helpful in the current implementation, however it may be useful if we abstract the operation of obtaining and releasing a CAS outside the BaseTokenizer.\n\nIn the meantime I noticed the AEProviderFactory getAEProvider() methods have a keyPrefix parameter that came from Solr implementation and was intended to hold the core name, so, at the moment I think it'd be better to have (also) methods which don't need that paramater for the Lucene uses. ",
            "author": "Tommaso Teofili",
            "id": "comment-13214093"
        },
        {
            "date": "2012-02-25T14:06:26+0000",
            "content": "the two methods analyzeText() and analyzeInput() are confusing so the first one should just be renamed as initializeIterator() as its main purpose is to prepare the FSIterator which holds the annotations that will be used inside the incrementToken() method. ",
            "author": "Tommaso Teofili",
            "id": "comment-13216459"
        },
        {
            "date": "2012-02-29T08:07:28+0000",
            "content": "I think we can mark this one as resolved, just I'd keep this only for trunk and backport the whole thing to 3.x once SOLR-3013 is resolved and committed to trunk too. ",
            "author": "Tommaso Teofili",
            "id": "comment-13218975"
        }
    ]
}