{
    "id": "SOLR-6462",
    "title": "forward updates asynchronously to peer clusters/leaders",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [],
        "components": [],
        "type": "Sub-task",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "http://heliosearch.org/solr-cross-data-center-replication/#UpdateFlow\n\n\n\tAn update will be received by the shard leader and versioned\n\tUpdate will be sent from the leader to it\u2019s replicas\n\tConcurrently, update will be sent (synchronously or asynchronously) to the shard leader in other clusters\n\tShard leader in the other cluster will receive already versioned update (and not re-version it), and forward the update to it\u2019s replicas",
    "attachments": {
        "cdcr_version_replication.patch": "https://issues.apache.org/jira/secure/attachment/12676736/cdcr_version_replication.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Yonik Seeley",
            "id": "comment-14117535",
            "date": "2014-09-01T16:21:33+0000",
            "content": "Although we eventually want synchronous operation (on a per-update basis), let's start with async.\nThe code on the receiving side (the peer leader) should be pretty simple... I think almost all the necessary code should already be in there since it's the same type of operation needed during shard splitting (a leader accepting already versioned updates). "
        },
        {
            "author": "Renaud Delbru",
            "id": "comment-14169296",
            "date": "2014-10-13T13:38:43+0000",
            "content": "I have started to implement the CDCR request handler that will handle CDCR life-cycle actions and forward updates to the peer clusters.\nWhile trying to implement the synchronisation of the life-cycle status amongst all the nodes of a cluster by using zookeeper, I have encountered a limitation of the SolrZkClient. The SolrZkClient provides methods such as getData or exists. The problem is that the client automatically wraps the provided watcher with a new watcher (see here) which breaks the guarantee that \"a watch object, or function/context pair, will only be triggered once for a given notification\". This creates undesirable effects when we are registering the same watch is the Watcher callback method.\n\nI have created the issue SOLR-6621 to notify about the problem. "
        },
        {
            "author": "Jakub Kotowski",
            "id": "comment-14182006",
            "date": "2014-10-23T21:34:50+0000",
            "content": "To achieve not changing the version of received updates, I created the CdcrUpdateProcessor (extends DistributedUpdateProcessor) which checks if there is a \"cdcr.update\" parameter in the request and in such a case preserves the version.\n\nWe decided to subclass the DistributedUpdateProcessor in order to minimize the number of changes to it and to keep the CDCR-related logic as separate as possible. We set the PEER_SYNC flag on the update in order to avoid executing leader logic which creates new versions. It's done in such a way that the leader will still distribute the command to replicas.\n\nProcessing add and delete by id commands works, including the forwarding logic (sending updates to a non-leader). There is still a problem with delete by query. The test in the patch tests all the highlighted possible problems and fails the very last delete by query test in about 50% cases. I'll continue investigating the problem.\n\nIn the process, I also discovered a possible bug in optimistic locking - it seems it does not work with forwarding logic because version is normally not preserved when forwarding delete by id and add updates. I'll open an issue about it. "
        },
        {
            "author": "Jakub Kotowski",
            "id": "comment-14187048",
            "date": "2014-10-28T16:33:35+0000",
            "content": "The problem with version replication in delete by query was that the CDCR updates weren't distributed from leader to replicas. The reason is that doDeleteByQuery() is structured differently than processAdd() and processDelete() in DistributedUpdateProcessor. We refactored doDeleteByQuery() by extracting a portion of its code into a helper method versionDeleteByQuery() which is then overriden in the CdcrUpdateProcessor. This way doDeleteByQuery() is structurally similar to the other two cases and we are able to keep the CDCR logic completely separated. All the test pass now, see the new patch. "
        }
    ]
}