{
    "id": "SOLR-6399",
    "title": "Implement unloadCollection in the Collections API",
    "details": {
        "affect_versions": "None",
        "status": "Open",
        "fix_versions": [
            "6.0"
        ],
        "components": [],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "There is currently no way to unload a collection without deleting its contents. There should be a way in the collections API to unload a collection and reload it later, as needed.\n\nA use case for this is the following: you store logs by day, with each day having its own collection. You are required to store up to 2 years of data, which adds up to 730 collections.  Most of the time, you'll want to have 3 days of data loaded for search. Having just 3 collections loaded into memory, instead of 730 will make managing Solr easier.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "dfdeshom",
            "id": "comment-14104698",
            "date": "2014-08-20T22:18:05+0000",
            "content": "Looking at the `deleteCollection` API implementation, it looks like it's using `CoreContainer.unload()`, but with the option of deleting the cores' contents. Relevant source code: https://github.com/apache/lucene-solr/blob/trunk/solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java#L944-953 \n\nOne could modify this `deleteCollection` to accepts parameters to `DELETE_INSTANCE_DIR` and `DELETE_DATA_DIR` . I tried doing that and the issues I ran into were:\n\n\n\tafter the core was unloaded, it was absent from the collection list, as if it never existed. On the other hand, re-issuing a CREATE call with the same collection restored the collection, along with its data\n\n\n\n\n\tupon restart Solr tried to reload the previously-unloaded collection.\n\n\n\nIn both cases, it looks like having an `isActive` attribute to a collection would help. Any pointers as to where to set and check that data would be appreciated. "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-15184986",
            "date": "2016-03-08T14:43:23+0000",
            "content": "Any chance of this issue see the light of day?\n\nIn setups with thousand of collections this feature is very useful to not use resources in collections without activity. "
        },
        {
            "author": "David Smiley",
            "id": "comment-15261536",
            "date": "2016-04-28T04:48:02+0000",
            "content": "I think this could be closed once SOLR-5750 (SolrCloud Collection Backup/Restore) is committed.  At least it addresses the use-case in the description. "
        },
        {
            "author": "Jerome Yang",
            "id": "comment-15393152",
            "date": "2016-07-26T03:36:42+0000",
            "content": "Backup/Restore is much heavier than just unload/load a collection. "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-15394033",
            "date": "2016-07-26T16:22:13+0000",
            "content": "With a flag in the core.properties saying that the core is not loaded on startup, a new state in zookeeper saying that collection is unloaded to not route queries and not trigger recoveries or notify that collection is down, and a command to load the collection on demand It's enough.\n\nI don't want to do a backup with a restore, I want notify the cluster to not load data to memory to save resources, but if necessary loading the collection on the fly.\n\nBackup data involve extra space somewhere, with 1T collection you needs 1T in other location to backup, to say nothing of transfer data over the network ...\n\nBackup and restore is a nice feature, but in huge clusters with a lot of data you not always can do it without huge amount of resources. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-16420139",
            "date": "2018-03-30T04:28:38+0000",
            "content": "I don't think this can be dealt with by backup/restore for the reasons Yago outlines. Loading a collection might take a few minutes, whereas a multi-terabyte index could take hours/days. "
        }
    ]
}