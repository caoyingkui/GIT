{
    "id": "LUCENE-6991",
    "title": "WordDelimiterFilter bug",
    "details": {
        "resolution": "Unresolved",
        "affect_versions": "4.10.4,                                            5.3.1",
        "components": [],
        "labels": "",
        "fix_versions": [],
        "priority": "Minor",
        "status": "Open",
        "type": "Bug"
    },
    "description": "I was preparing analyzer which contains WordDelimiterFilter and I realized it sometimes gives results different then expected.\n\nI prepared a short test which shows the problem. I haven't used Lucene tests for this but this doesn't matter for showing the bug.\n\n\n    String urlIndexed = \"144.214.37.14 - - [05/Jun/2013:08:39:27 +0000] \\\"GET /products/key-phrase-extractor/ HTTP/1.1\\\"\" +\n            \" 200 3437 http://www.google.com/url?sa=t&rct=j&q=&esrc=s&\" +\n            \"source=web&cd=15&cad=rja&ved=0CEgQFjAEOAo&url=http%3A%2F%2Fwww.sematext.com%2Fproducts%2Fkey-\" +\n            \"phrase-extractor%2F&ei=TPOuUbaWM-OKiQfGxIGYDw&usg=AFQjCNGwYAFYg_M3EZnp2eEWJzdvRrVPrg&sig2\" +\n            \"=oYitONI2EIZ0CQar7Ej8HA&bvm=bv.47380653,d.aGc\\\" \\\"Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:20.0) \" +\n            \"Gecko/20100101 Firefox/20.0\\\"\";\n\n    List<String> tokens1 = new ArrayList<String>();\n    List<String> tokens2 = new ArrayList<String>();\n    WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer();\n    TokenStream tokenStream = analyzer.tokenStream(\"test\", urlIndexed);\n    tokenStream = new WordDelimiterFilter(tokenStream,\n            WordDelimiterFilter.GENERATE_WORD_PARTS |\n            WordDelimiterFilter.CATENATE_WORDS |\n            WordDelimiterFilter.SPLIT_ON_CASE_CHANGE,\n        null);\n    CharTermAttribute charAttrib = tokenStream.addAttribute(CharTermAttribute.class);\n    tokenStream.reset();\n    while(tokenStream.incrementToken()) {\n      tokens1.add(charAttrib.toString());\n      System.out.println(charAttrib.toString());\n    }\n    tokenStream.end();\n    tokenStream.close();\n\n    urlIndexed = \"144.214.37.14 - - [05/Jun/2013:08:39:27 +0000] \\\"GET /products/key-phrase-extractor/ HTTP/1.1\\\"\" +\n        \" 200 3437 \\\"http://www.google.com/url?sa=t&rct=j&q=&esrc=s&\" +\n        \"source=web&cd=15&cad=rja&ved=0CEgQFjAEOAo&url=http%3A%2F%2Fwww.sematext.com%2Fproducts%2Fkey-\" +\n        \"phrase-extractor%2F&ei=TPOuUbaWM-OKiQfGxIGYDw&usg=AFQjCNGwYAFYg_M3EZnp2eEWJzdvRrVPrg&sig2\" +\n        \"=oYitONI2EIZ0CQar7Ej8HA&bvm=bv.47380653,d.aGc\\\" \\\"Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:20.0) \" +\n        \"Gecko/20100101 Firefox/20.0\\\"\";\n\n\n    System.out.println(\"\\n\\n====\\n\\n\");\n    tokenStream = analyzer.tokenStream(\"test\", urlIndexed);\n    tokenStream = new WordDelimiterFilter(tokenStream,\n            WordDelimiterFilter.GENERATE_WORD_PARTS |\n            WordDelimiterFilter.CATENATE_WORDS |\n            WordDelimiterFilter.SPLIT_ON_CASE_CHANGE,\n        null);\n    charAttrib = tokenStream.addAttribute(CharTermAttribute.class);\n    tokenStream.reset();\n    while(tokenStream.incrementToken()) {\n      tokens2.add(charAttrib.toString());\n      System.out.println(charAttrib.toString());\n    }\n    tokenStream.end();\n    tokenStream.close();\n\n    assertEquals(Joiner.on(\",\").join(tokens1), Joiner.on(\",\").join(tokens2));",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15115029",
            "author": "Pawel Rog",
            "date": "2016-01-25T10:44:56+0000",
            "content": "Below you can see tokens produced from first token stream and second token stream\n\n\nJun\n0000\nGET\nproducts\nproductskeyphraseextractor\nkey\nphrase\nextractor\nHTTP\n200\n3437\nhttp\nhttpwwwgooglecomurlsatrctjqesrcssourcewebcd\nwww\ngoogle\ncom\nurl\nsa\nt\nrct\nj\nq\nesrc\ns\nsource\nweb\ncd\ncad\ncadrjaved\nrja\nved\nQFj\nQFjAEOAourlhttp\nAEOAo\nurl\nhttp\nsematext\nsematextcom\ncom\nphrase\nphraseextractor\nextractor\nei\neiTPOuUbaWMOKiQfGxIGYDwusgAFQjCNGwYAFYgM3EZnp2eEWJzdvRrVPrgsig2oYitONI2EIZ0CQar7Ej8HAbv\nTPOu\nUba\nWM\nOKi\nQf\nGx\nIGYDw\nusg\nAFQj\nCNGw\nYAFYg\nM3EZnp2e\nEWJzdv\nRr\nVPrg\nsig2\no\nYit\nONI2EIZ0CQar7Ej8HA\nbv\nm\nmbv\nbv\nd\ndaGc\na\nGc\nMozilla\nX11\nUbuntu\nLinux\ni686\nrv\nGecko\nFirefox\n\n\n\n\nJun\n0000\nGET\nproducts\nproductskeyphraseextractor\nkey\nphrase\nextractor\nHTTP\n200\n3437\nhttp\nhttpwwwgooglecomurlsatrctjqesrcssourcewebcd\nwww\ngoogle\ncom\nurl\nsa\nt\nrct\nj\nq\nesrc\ns\nsource\nweb\ncd\ncad\ncadrjaved\nrja\nved\nQFj\nQFjAEOAourlhttp\nAEOAo\nurl\nhttp\nsematext\nsematextcom\ncom\nphrase\nphraseextractor\nextractor\nei\neiTPOuUbaWMOKiQfGxIGYDwusgAFQjCNGwYAFYgM3EZnp2eEWJzdvRrVPrgsig2oYitONI2EIZ0CQar7Ej8HAb\nTPOu\nUba\nWM\nOKi\nQf\nGx\nIGYDw\nusg\nAFQj\nCNGw\nYAFYg\nM3EZnp2e\nEWJzdv\nRr\nVPrg\nsig2\no\nYit\nONI2EIZ0CQar7Ej8HA\nb\nvm\nvmbv\nbv\nd\ndaGc\na\nGc\nMozilla\nX11\nUbuntu\nLinux\ni686\nrv\nGecko\nFirefox\n\n\n\n\nThe difference in input string is quotation mark before \"http\". The difference in output is in a few terms:\n\neiTPOuUbaWMOKiQfGxIGYDwusgAFQjCNGwYAFYgM3EZnp2eEWJzdvRrVPrgsig2oYitONI2EIZ0CQar7Ej8HAbv vs eiTPOuUbaWMOKiQfGxIGYDwusgAFQjCNGwYAFYgM3EZnp2eEWJzdvRrVPrgsig2oYitONI2EIZ0CQar7Ej8HAb\n\nor \nmbv vs vmbv  "
        },
        {
            "id": "comment-15115405",
            "author": "Jack Krupansky",
            "date": "2016-01-25T15:33:32+0000",
            "content": "Does seem odd and wrong.\n\nI also notice that it is not generating terms for the single letters from the %-escapes: %3A, %2F.\n\nIt also seems odd that that long token of catenated word parts is not all of the word parts from the URL. It seems like a digit not preceded by a letter is causing a break, while a digit preceded by a letter prevents a break.\n\nSince you are using the white space tokenizer, the WDF is only seeing each space-delimited term at a time. You might try your test with just the URL portion itself, both with and without the escaped quote, just to see if that affects anything. "
        },
        {
            "id": "comment-15115414",
            "author": "Pawel Rog",
            "date": "2016-01-25T15:40:27+0000",
            "content": "Thanks for the suggestion. When I changed whitespace tokenizer to keyword tokenizer the test passes. Nevertheless I think the problem stays in WordDelimiterFilter. Right? "
        }
    ]
}