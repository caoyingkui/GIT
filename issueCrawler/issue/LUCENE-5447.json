{
    "id": "LUCENE-5447",
    "title": "StandardTokenizer should break at consecutive chars matching Word_Break = MidLetter, MidNum and/or MidNumLet",
    "details": {
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed",
        "components": [
            "modules/analysis"
        ],
        "affect_versions": "4.6.1",
        "status": "Closed",
        "fix_versions": [
            "4.7",
            "6.0"
        ]
    },
    "description": "StandardTokenizer should split all of the following sequences into two tokens each, but they are all instead kept intact and output as single tokens:\n\n\n\"A::B\"           (':' is in \\p{Word_Break = MidLetter})\n\"1..2\", \"A..B\"   ('.' is in \\p{Word_Break = MidNumLet})\n\"A.:B\"\n\"A:.B\"\n\"1,,2\"           (',' is in \\p{Word_Break = MidNum})\n\"1,.2\"\n\"1.,2\"\n\n\n\nUnfortunately, the word break test data released with Unicode, e.g. for Unicode 6.3 http://www.unicode.org/Public/6.3.0/ucd/auxiliary/WordBreakTest.txt, and incorporated into a versioned Lucene test, e.g. WordBreakTestUnicode_6_3_0, doesn't cover these cases.",
    "attachments": {
        "LUCENE-5447-take2.patch": "https://issues.apache.org/jira/secure/attachment/12629808/LUCENE-5447-take2.patch",
        "LUCENE-5447.patch": "https://issues.apache.org/jira/secure/attachment/12629694/LUCENE-5447.patch",
        "LUCENE-5447-test.patch": "https://issues.apache.org/jira/secure/attachment/12629114/LUCENE-5447-test.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-13901931",
            "author": "Steve Rowe",
            "content": "Patch with tests that demonstrate the problem ",
            "date": "2014-02-14T20:58:19+0000"
        },
        {
            "id": "comment-13904886",
            "author": "Steve Rowe",
            "content": "Patch fixing the issue; includes LUCENE-5447-test.patch.\n\nCommitting shortly. ",
            "date": "2014-02-19T00:43:15+0000"
        },
        {
            "id": "comment-13904945",
            "author": "Steve Rowe",
            "content": "Final patch, adding a test for UAX29URLEmailTokenizer and a CHANGES.txt entry. ",
            "date": "2014-02-19T01:20:17+0000"
        },
        {
            "id": "comment-13904947",
            "author": "ASF subversion and git services",
            "content": "Commit 1569586 from Steve Rowe in branch 'dev/trunk'\n[ https://svn.apache.org/r1569586 ]\n\nLUCENE-5447: StandardTokenizer should break at consecutive chars matching Word_Break = MidLetter, MidNum and/or MidNumLet ",
            "date": "2014-02-19T01:21:25+0000"
        },
        {
            "id": "comment-13904984",
            "author": "ASF subversion and git services",
            "content": "Commit 1569601 from Steve Rowe in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1569601 ]\n\nLUCENE-5447: StandardTokenizer should break at consecutive chars matching Word_Break = MidLetter, MidNum and/or MidNumLet (merged trunk r1569586) ",
            "date": "2014-02-19T01:50:03+0000"
        },
        {
            "id": "comment-13905446",
            "author": "Steve Rowe",
            "content": "Committed to trunk, branch_4x and lucene_solr_4_7. ",
            "date": "2014-02-19T13:46:56+0000"
        },
        {
            "id": "comment-13905665",
            "author": "Steve Rowe",
            "content": "In looking at the committed diffs (when JIRA was down last night and earlier today, the lucene_solr_4_7 commit didn't put a comment on this issue, which sucks), I see that I didn't fully patch StandardTokenizerImpl.jflex, although I did correctly patch UAX29URLEmailTokenizerImpl, which is basically a superset of StandardTokenizerImpl.jflex.\n\nI've added some more tests to show the problem (existing tests didn't fail), patch forthcoming.  Here's an example that should be split by StandardTokenizer but isn't currently - the issue is triggered via a preceding char matching Word_Break = ExtendNumLet, e.g. the underscore character:\n\nA:B_A::B <- left intact, but should output \"A:B_A\", \"B\"\n\nBy contrast, the current UAX29URLEmailTokenizer gets the above right.\n\nIn the JFlex 1.5.0 release, I added the ability to include external files into the rules section of the scanner specification, and I want to take advantage of this to refactor StandardTokenizer and UAX29URLEmailTokenizer so that there is only one definition of the shared rules.  (That would have prevented the problem for which I'm reopening this issue.)  I'll make a separate issue for that. ",
            "date": "2014-02-19T16:48:57+0000"
        },
        {
            "id": "comment-13905671",
            "author": "Robert Muir",
            "content": "A random question here Steve, is it possible to add this test to the unicode tests and send upstream? or is it already fixed in recent versions? ",
            "date": "2014-02-19T16:51:23+0000"
        },
        {
            "id": "comment-13905677",
            "author": "Steve Rowe",
            "content": "random question here Steve, is it possible to add this test to the unicode tests and send upstream? or is it already fixed in recent versions?\n\nGood idea, I'll check if it's already fixed, and if not, send upstream. ",
            "date": "2014-02-19T17:01:57+0000"
        },
        {
            "id": "comment-13905678",
            "author": "Steve Rowe",
            "content": "Patch with more tests illustrating the StandardTokenizerImpl problem, along with the scanner specification fix.\n\nCommitting shortly. ",
            "date": "2014-02-19T17:03:08+0000"
        },
        {
            "id": "comment-13905704",
            "author": "ASF subversion and git services",
            "content": "Commit 1569831 from Steve Rowe in branch 'dev/trunk'\n[ https://svn.apache.org/r1569831 ]\n\nLUCENE-5447: Fully patch StandardTokenizerImpl.jflex, to bring parity with rules in UAX29URLEmailTokenizerImpl.jflex; add tests that fail without this fix and succeed with it. ",
            "date": "2014-02-19T17:22:18+0000"
        },
        {
            "id": "comment-13905742",
            "author": "ASF subversion and git services",
            "content": "Commit 1569849 from Steve Rowe in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1569849 ]\n\nLUCENE-5447: Fully patch StandardTokenizerImpl.jflex, to bring parity with rules in UAX29URLEmailTokenizerImpl.jflex; add tests that fail without this fix and succeed with it. (merged trunk r1569831) ",
            "date": "2014-02-19T17:43:19+0000"
        },
        {
            "id": "comment-13905758",
            "author": "ASF subversion and git services",
            "content": "Commit 1569855 from Steve Rowe in branch 'dev/branches/lucene_solr_4_7'\n[ https://svn.apache.org/r1569855 ]\n\nLUCENE-5447: Fully patch StandardTokenizerImpl.jflex, to bring parity with rules in UAX29URLEmailTokenizerImpl.jflex; add tests that fail without this fix and succeed with it. (merged branch_4x r1569849) ",
            "date": "2014-02-19T17:51:44+0000"
        },
        {
            "id": "comment-13905768",
            "author": "Steve Rowe",
            "content": "Committed the new tests and the fully patched StandardTokenizerImpl.jflex to to trunk, branch_4x and lucene_solr_4_7. ",
            "date": "2014-02-19T17:59:19+0000"
        },
        {
            "id": "comment-13906610",
            "author": "Steve Rowe",
            "content": "\nrandom question here Steve, is it possible to add this test to the unicode tests and send upstream? or is it already fixed in recent versions?\n\nGood idea, I'll check if it's already fixed, and if not, send upstream.\n\nIt's not fixed in recent versions - in fact the proposed 7.0 version is exactly the same as the 6.3.0 version, with the exception of the header.\n\nI converted the tests to the WordBreakTest.txt format and submitted them (along with an explanation pointing to this issue) through the Unicode.org contact form at http://www.unicode.org/reporting.html. ",
            "date": "2014-02-20T04:58:27+0000"
        },
        {
            "id": "comment-13907069",
            "author": "Robert Muir",
            "content": "Thanks Steve! ",
            "date": "2014-02-20T15:37:15+0000"
        },
        {
            "id": "comment-13907186",
            "author": "Steve Rowe",
            "content": "In the JFlex 1.5.0 release, I added the ability to include external files into the rules section of the scanner specification, and I want to take advantage of this to refactor StandardTokenizer and UAX29URLEmailTokenizer so that there is only one definition of the shared rules. (That would have prevented the problem for which I'm reopening this issue.) I'll make a separate issue for that.\n\nSee LUCENE-5464 ",
            "date": "2014-02-20T17:04:53+0000"
        },
        {
            "id": "comment-13907950",
            "author": "Steve Rowe",
            "content": "I converted the tests to the WordBreakTest.txt format and submitted them (along with an explanation pointing to this issue) through the Unicode.org contact form at http://www.unicode.org/reporting.html.\n\nThe message I sent is now recorded as the second email in the feedback for Proposed Update UAX #29, Unicode Text Segmentation: http://www.unicode.org/review/pri265/ ",
            "date": "2014-02-21T04:40:19+0000"
        },
        {
            "id": "comment-13998249",
            "author": "Steve Rowe",
            "content": "\nI converted the tests to the WordBreakTest.txt format and submitted them (along with an explanation pointing to this issue) through the Unicode.org contact form at http://www.unicode.org/reporting.html.\n\nThe message I sent is now recorded as the second email in the feedback for Proposed Update UAX #29, Unicode Text Segmentation: http://www.unicode.org/review/pri265/\n\nThe Unicode Technical Committee emailed me today to tell me that they would be adding test cases for this problem to Unicode 8.0, but not to the upcoming 7.0 release. ",
            "date": "2014-05-14T23:06:51+0000"
        }
    ]
}