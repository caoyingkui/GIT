{
    "id": "SOLR-1535",
    "title": "Pre-analyzed field type",
    "details": {
        "affect_versions": "1.5",
        "status": "Closed",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "components": [],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "PreAnalyzedFieldType provides a functionality to index (and optionally store) content that was already processed and split into tokens using some external processing chain. This implementation defines a serialization format for sending tokens with any currently supported Attributes (eg. type, posIncr, payload, ...). This data is de-serialized into a regular TokenStream that is returned in Field.tokenStreamValue() and thus added to the index as index terms, and optionally a stored part that is returned in Field.stringValue() and is then added as a stored value of the field.\n\nThis field type is useful for integrating Solr with existing text-processing pipelines, such as third-party NLP systems.",
    "attachments": {
        "SOLR-1535.patch": "https://issues.apache.org/jira/secure/attachment/12450397/SOLR-1535.patch",
        "preanalyzed.patch": "https://issues.apache.org/jira/secure/attachment/12423699/preanalyzed.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12834770",
            "date": "2010-02-17T14:10:18+0000",
            "content": "Patch updated to the current trunk. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12835728",
            "date": "2010-02-19T12:38:06+0000",
            "content": "Oops .. previous patch produced NPEs. This one doesn't. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12835729",
            "date": "2010-02-19T12:39:23+0000",
            "content": "Sigh ... attach correct patch. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12872409",
            "date": "2010-05-27T22:04:43+0000",
            "content": "Bulk updating 240 Solr issues to set the Fix Version to \"next\" per the process outlined in this email...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E\n\nSelection criteria was \"Unresolved\" with a Fix Version of 1.5, 1.6, 3.1, or 4.0.  email notifications were suppressed.\n\nA unique token for finding these 240 issues in the future: hossversioncleanup20100527 "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12891976",
            "date": "2010-07-24T12:42:17+0000",
            "content": "Updated patch. This patch implements also getAnalyzer()/getQueryAnalyzer() so that it's possible to test fields in analysis.jsp. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043729",
            "date": "2011-06-03T16:46:46+0000",
            "content": "Bulk move 3.2 -> 3.3 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13106401",
            "date": "2011-09-16T14:50:55+0000",
            "content": "3.4 -> 3.5 "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13132768",
            "date": "2011-10-21T15:54:39+0000",
            "content": "Became aware of this during EuroCon. This is great stuff.\nHave you thought about going <buzzwordAlert>Avro</buzzwordAlert> for the serialization format? It would better support changing serialization format in new versions, and be more compact, especially when serializing binary data (instead of using base64). The Avro version of the document could also be the new binary serialization format to replace JavaBin so that other clients than SolrJ can benefit from binary streaming. "
        },
        {
            "author": "David Smiley",
            "id": "comment-13132964",
            "date": "2011-10-21T19:31:26+0000",
            "content": "Yes, definitely Avro instead of home-grown serialization/binary format.  Same for JavaBin. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13197390",
            "date": "2012-01-31T23:46:27+0000",
            "content": "Avro adds yet another dependency, which would make sense if Solr used Avro instead of JavaBin - but that's a separate discussion that merits a separate JIRA issue... as it isn't used now I'd rather avoid putting additional burden on clients just for the sake of this patch.\n\nJSON could be a nice alternative, if it only supported binary data natively (it doesn't, one has to use base64 - however, it's not that awful as you could think). I wanted to avoid complex formats like XML - too much boilerplate for such small bits of data. So the current custom serialization tried to strike a balance between simplicity, flexibility and low overhead.\n\nSerialization of terms was also discussed in SOLR-1632 - e.g. this patch doesn't serialize binary terms properly. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13234706",
            "date": "2012-03-21T18:08:57+0000",
            "content": "Bulk of fixVersion=3.6 -> fixVersion=4.0 for issues that have no assignee and have not been updated recently.\n\nemail notification suppressed to prevent mass-spam\npsuedo-unique token identifying these issues: hoss20120321nofix36 "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13253253",
            "date": "2012-04-13T10:00:45+0000",
            "content": "Patch updated to the latest trunk. This still uses the custom serialization format. Please weigh in with suggestions about how to proceed - I see the following options:\n\n\n\tkeep the custom format as is (it's compact and easy to produce)\n\tuse JSON instead (easy to produce, but more chatty, binary values would have to be base64 encoded)\n\tuse Avro (compact and back-forward compatible, self-describing, but added dependencies, not that easy to construct by hand)\n\n "
        },
        {
            "author": "Chris Male",
            "id": "comment-13253263",
            "date": "2012-04-13T10:20:49+0000",
            "content": "Can't we just provide an abstraction so people can choose whatever format they want? You might use JSON out-of-box, but Jan could implement an Avro alternative if he wanted too.  That also gives us a way to grow the format as our needs change. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13253325",
            "date": "2012-04-13T12:34:29+0000",
            "content": "I wish I had time to do the Avro stuff now, but just go ahead with whatever you choose.\n\nSince this format potentially will be adopted by many 3rd party frameworks we should take multi language support and back-compat seriously, so we do not end up in a similar situation as with JavaBin v1/v2... Perhaps a JSON structure with Base64 for binaries and a mandatory version attribute is a good generic start? "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13253363",
            "date": "2012-04-13T13:26:32+0000",
            "content": "Nice idea about a pluggable format... Hmm. This should be specified then in the field type definition, I think, and not in a preamble of the data itself (UTF BOM mess comes to mind). I can implement the JSON version, and the current \"simple\" format, each with a version attrib.\n\nNew patch coming soon. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13253919",
            "date": "2012-04-14T01:06:27+0000",
            "content": "This patch contains the following improvements:\n\n\n\tabstracted parser implementations (PreAnalyzedParser)\n\tconfigurable implementation via field init args\n\tJsonPreAnalyzedParser that supports a JSON-based format, used as default.\n\n "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13257311",
            "date": "2012-04-19T07:03:51+0000",
            "content": "The latest patch implements requested improvements. If there are no objections I'd like to commit it shortly, and track further improvements as separate issues. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13257392",
            "date": "2012-04-19T09:17:56+0000",
            "content": "+1 (not tested but positve to splitting up the elephant) "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13257520",
            "date": "2012-04-19T14:51:40+0000",
            "content": "+1 - I have not fully reviewed the patch, but did a quick eye scan. It looks isolated enough (eg doesnt affect existing classes) and this issue has been open long enough (and look at the votes watchers) - lets get it into trunk  now and we can iterate there if/as needed. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13257537",
            "date": "2012-04-19T15:10:42+0000",
            "content": "Committed with minor tweaks in rev. 1327982. "
        },
        {
            "author": "Neil Hooey",
            "id": "comment-13273408",
            "date": "2012-05-11T16:38:06+0000",
            "content": "When I asked Hoss at Lucene Revolution yesterday, he said you could manually set term frequency in a pre-analyzed field, but I couldn't find any reference to it in the JSON parser.\n\nIs there a way to specify term frequency for each term in the field? "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13273501",
            "date": "2012-05-11T18:45:24+0000",
            "content": "Hoss was wrong  there is no way to do this, as there is no way to do this in TokenStream - you should view the PreAnalyzed field type as a serialized TokenStream (with the added functionality to specify the stored part independently).\n\nEdit: I started adding some documentation to http://wiki.apache.org/solr/PreAnalyzedField . "
        },
        {
            "author": "Alexandre Rafalovitch",
            "id": "comment-13541555",
            "date": "2013-01-01T01:34:52+0000",
            "content": "Not sure where to ask this, as the feature is so new. But how does this work at query time? This needs to be pared somehow with a query-time tokenizer/filters, right? I could not find a trivial (or complex) example showing this in action. "
        },
        {
            "author": "John Berryman",
            "id": "comment-13605549",
            "date": "2013-03-18T19:59:34+0000",
            "content": "Hey Aldrzej, would it be possible to get a minimal example posted on the documentation page? I'd like to use this feature, but I don't really know where to start.\n\nUPDATE: Looking over your tests in your code, I realize that this is currently a Lucene-only thing. I wonder what it would take to get this into Solr or maybe SolrJ. Food for thought. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13607919",
            "date": "2013-03-20T17:54:22+0000",
            "content": "John, this is primarily a Solr feature. I added a short example to the wiki page. "
        },
        {
            "author": "John Berryman",
            "id": "comment-13608011",
            "date": "2013-03-20T18:58:46+0000",
            "content": "Ah, I see. This is a bit lower level than I was thinking. Still useful, but different. I was thinking about having PreAnalyzedField extend directly from TextField rather than from FieldType, and then be able to build up whatever analysis chain that you want in the usual TextField sense. Query analysis would proceed as with a normal TextField, but index analysis would smart detect whether this input was already parsed or not. If the input was not parsed, then it would go through the normal analysis. On the other hand, if the input was already parsed, then the token stream would go straight into the index (the assumption being that someone upstream understands what they're doing).\n\nThis way, in the SolrJ client you could build up some extra functionality so that the PreAnalyzedTextFields would be parsed client side and sent to Solr. In my current application, we have one Solr and N-indexers on different machines. The setup described here would take a big load off of Solr. The other benefit of this setup is that query analysis proceeds as it always does. I don't understand how someone would search over a PreAnalyzed field as it currently stands, without a bit of extra work/custom code on the client.\n\nOne pitfall to my idea is that you'd have to create a similar PreAnalyzedIntField, PreAnalyzedLocationField, PreAnalyzedDateField etc. I wish Java had mixins or multiple inheritance.\n\nThoughts? "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13608045",
            "date": "2013-03-20T19:18:11+0000",
            "content": "Let's move this discussion to SOLR-4619 . "
        }
    ]
}