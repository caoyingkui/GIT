{
    "id": "LUCENE-1278",
    "title": "Add optional storing of document numbers in term dictionary",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/index"
        ],
        "type": "New Feature",
        "fix_versions": [],
        "affect_versions": "2.3.1",
        "resolution": "Won't Fix",
        "status": "Resolved"
    },
    "description": "Add optional storing of document numbers in term dictionary.  String index field cache and range filter creation will be faster.  \n\nExample read code:\n\nTermEnum termEnum = indexReader.terms(TermEnum.LOAD_DOCS);\ndo {\n  Term term = termEnum.term();\n  if (term == null || term.field() != field) break;\n  int[] docs = termEnum.docs();\n} while (termEnum.next());\n\n\n\nExample write code:\n\nDocument document = new Document();\ndocument.add(new Field(\"tag\", \"dog\", Field.Store.YES, Field.Index.UN_TOKENIZED, Field.Term.STORE_DOCS));\nindexWriter.addDocument(document);",
    "attachments": {
        "lucene.1278.5.5.2008.2.patch": "https://issues.apache.org/jira/secure/attachment/12381422/lucene.1278.5.5.2008.2.patch",
        "lucene.1278.5.7.2008.test.patch": "https://issues.apache.org/jira/secure/attachment/12381637/lucene.1278.5.7.2008.test.patch",
        "lucene.1278.5.5.2008.patch": "https://issues.apache.org/jira/secure/attachment/12381416/lucene.1278.5.5.2008.patch",
        "lucene.1278.5.7.2008.patch": "https://issues.apache.org/jira/secure/attachment/12381636/lucene.1278.5.7.2008.patch",
        "TestTermEnumDocs.java": "https://issues.apache.org/jira/secure/attachment/12381439/TestTermEnumDocs.java",
        "lucene.1278.5.4.2008.patch": "https://issues.apache.org/jira/secure/attachment/12381397/lucene.1278.5.4.2008.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-05-04T22:41:26+0000",
            "content": "Patch generated using TortoiseSVN.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12594126"
        },
        {
            "date": "2008-05-04T22:44:26+0000",
            "content": "Is there a way to know the number of documents for a term in DocumentsWriter.appendPostings before running through all of them?  Currently a non-optimal linkedlist is used.  Otherwise will implement a growable int array. ",
            "author": "Jason Rutherglen",
            "id": "comment-12594127"
        },
        {
            "date": "2008-05-04T22:48:53+0000",
            "content": "Test cases being worked on ",
            "author": "Jason Rutherglen",
            "id": "comment-12594128"
        },
        {
            "date": "2008-05-05T09:40:26+0000",
            "content": "lucene.1278.5.5.2008.patch - added and implemented IntArrayList, cleaned up comments, fixed bugs in segmentmerger, and multisegmentreader ",
            "author": "Jason Rutherglen",
            "id": "comment-12594199"
        },
        {
            "date": "2008-05-05T10:09:35+0000",
            "content": "Would there be any performance measurements for this? It might be quite good for terms that occur in very many documents, an area in which some improvement is possible I think.\nBtw, for this case it might also be good to use a SortedVIntList instead of an IntArrayList.\n\nI had a look at today's patch, but I stopped at DocumentsWriter because it contains a lot of layout changes, so it's hard to focus on the functional differences.\n\nAre there any index format changes involved in this? ",
            "author": "Paul Elschot",
            "id": "comment-12594206"
        },
        {
            "date": "2008-05-05T11:59:06+0000",
            "content": "\nIs there a way to know the number of documents for a term in DocumentsWriter.appendPostings before running through all of them?\n\nI don't think so.  You have to run through the list.\n ",
            "author": "Michael McCandless",
            "id": "comment-12594219"
        },
        {
            "date": "2008-05-05T12:00:40+0000",
            "content": "\nI had a look at today's patch, but I stopped at DocumentsWriter because it contains a lot of layout changes, so it's hard to focus on the functional differences.\n\nI also stopped at DocumentsWriter: it seems like nearly all the\nchanges are cosmetic.  SegmentTermEnum is also hard to read.\n\nIn general it's best to not make cosmetic changes (moving around\nimport lines, changing whitespace, re-justifying whole paragraphs in\njavadocs, etc.) at the same time as a \"real\" change, when possible.  I\ndo admit there is a strong temptation \n\nAlso, indentation should be two spaces, not tab.  A number of sources\nwere changed to tab in the patch.\n ",
            "author": "Michael McCandless",
            "id": "comment-12594220"
        },
        {
            "date": "2008-05-05T12:33:00+0000",
            "content": "\nIt looks like the .tii file is also storing the int[] docIDs (as\ninlined byte blob)?  I think that shouldn't be necessary?\n\nThis change adds a posting list like the frq file, except that it\nstores only docIDs (no freq information), is stored inline in the term\ndict, and includes a reader that materializes the full doc list as an\nint[] instead of offering an iterator like (nextDoc()) interface\nalone.\n\nI think these changes would fit cleanly into what's been proposed for\nflexible indexing.  EG, case 1a talks about storing only docID in a\nposting list, here:\n\n    http://wiki.apache.org/jakarta-lucene/FlexibleIndexing\n\nAnd recent discussions on the dev list around how to be flexible as to\nwhich index file(s) (one or many) things are stored in, eg:\n\n   http://www.mail-archive.com/java-dev@lucene.apache.org/msg15681.html\n\nshould allow you to store this data inlined into the terms dict, or as\na separate file.\n\nSome other initial comments/questions:\n\n\n\tI think this would bloat the index because the docIDs are being\n    double stored (in the terms dict, and, in the frq file).  Would\n    you propose changing the frq file to not store the docID when the\n    term dict is doing so?\n\n\n\n\n\tWhy store the byte blob in the term dict, and not a separate (new)\n    index file?  We lose locality for cases where one wants to iterate\n    through terms and not loads these docs (eg RangeQuery).\n\n\n\n\n\tCould you, instead, make a reader that reads in the full byte blob\n    from the frq file for a term, and then processes that into the\n    int[]?  This would require no change to indexing & the index\n    format, and wouldn't waste space double-storing the docIDs.\n\n\n\n\n\tI'm worried how well this scales up.  For very common terms\n    allocating then decoding & holding entirely in RAM the full list\n    of docIDs can become extremely costly.  Also, I don't have a clear\n    sense of how apps would use the returned int[].  For example,\n    would the int[] for many terms need to remain resident at the same\n    time?  (Eg when running a RangeQuery).  If so, that compounds the\n    scale challenge.\n\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12594225"
        },
        {
            "date": "2008-05-05T12:51:23+0000",
            "content": "lucene.1278.5.5.2008.2.patch - Source formatting fixed.  SortedVIntList is for unchanging arrays.  There are index format changes. ",
            "author": "Jason Rutherglen",
            "id": "comment-12594229"
        },
        {
            "date": "2008-05-05T13:02:54+0000",
            "content": "Storing the docs is off by default and will add index size only if the user wishes.  The byte blob allows not reading the docs when loaddocs is false.  Field cache and range query loading is very slow because of the dual seeks per term (for termenum then termdocs).  If in a separate file the terms are redundant.  \n\nAn field cache example:\n\nprotected Object createValue(IndexReader reader, Object entryKey)\n        throws IOException {\n      Entry entry = (Entry) entryKey;\n      String field = entry.field;\n      IntParser parser = (IntParser) entry.custom;\n      final int[] retArray = new int[reader.maxDoc()];\n      // TermDocs termDocs = reader.termDocs();  \n      //TermEnum termEnum = reader.terms (new Term (field, \"\"));\n      TermEnum termEnum = reader.terms (new Term (field, \"\"), true);\n      try {\n        do {\n          Term term = termEnum.term();\n          if (term==null || term.field() != field) break;\n          int termval = parser.parseInt(term.text());\n          int[] docs = termEnum.docs();\n          for (int x=0; x < docs.length; x++) \n{\n            retArray[docs[x]] = termval;\n          }\n          //termDocs.seek (termEnum);\n          //while (termDocs.next()) \n{\n          //  retArray[termDocs.doc()] = termval;\n          //}\n        } while (termEnum.next());\n      } finally \n{\n        //termDocs.close();\n        termEnum.close();\n      }\n      return retArray;\n    } ",
            "author": "Jason Rutherglen",
            "id": "comment-12594231"
        },
        {
            "date": "2008-05-05T13:45:42+0000",
            "content": "Had a quick look at today's second patch. Some details:\nThere are still some layout changes in TermInfosReader (iirc).\nThere is an addition to Fieldable, which is a public interface so there is a backward compatibility issue there (sigh).\nThere is an extra constructor for SortedVIntList which is not used.\n\nInstead of returning a full int[] for common terms, perhaps one could return a DocIdSetIterator that does the decoding of the doc ids as it is needed.\n\nI'm getting curious about performance improvements from this, any numbers available? ",
            "author": "Paul Elschot",
            "id": "comment-12594233"
        },
        {
            "date": "2008-05-05T17:30:29+0000",
            "content": "Was going to write a Lucene test case but need an example and svn is down.\n\nThe example test is extremely poor because the term and field saturation is nil.  Normal documents will have far more terms and the file cache will not have cached as much of the term docs as it will be larger.  However it does illustrate the speed up.  Please suggest other tests.\n\nLaptop Windows XP SP2 Java6 core2duo, about the same on 3 separate runs:\n3360 millis termenum loaddocs\n25641 millis termdocs\n7.6 times speedup\n\nThere have been previous discussions regarding the speed issue.  \nhttp://www.gossamer-threads.com/lists/lucene/java-dev/53786\nThe conclusion was to use payloads which do not speed up stringindex or range queries.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12594277"
        },
        {
            "date": "2008-05-05T19:45:27+0000",
            "content": "Returning DocIdSetIterator from TermEnum is good, will implement decoding bytes directly from file.\n\nFlexible indexing is good, will implement when it's completed. ",
            "author": "Jason Rutherglen",
            "id": "comment-12594317"
        },
        {
            "date": "2008-05-07T00:26:35+0000",
            "content": "Implemented returning DocIdSetIterator however when running org.apache.lucene.search.TestSort remote search fails.  Reading the docs from a DocIdSetIterator directly from the file is troublesome due to the way termenum is designed with the other parts of Lucene.  My own basic unit test works, however TestSort does not and it is probably due to the file pointer not being on the correct position during enumeration.  \n\nPerhaps there is a way for the int array work?  \n\nOr is it best to create a separate file that is very similar to the term dictionary file but only stores terms and docs? ",
            "author": "Jason Rutherglen",
            "id": "comment-12594744"
        },
        {
            "date": "2008-05-07T02:48:53+0000",
            "content": "What if the int array is saved in TermInfo only if the docfreq was below a certain threshold?  Otherwise on int[] docs = TermEnum.docs() the docs are loaded from the file.  This solves the main issue with the int array, the potential for high numbers of docs being stored in ram. ",
            "author": "Jason Rutherglen",
            "id": "comment-12594761"
        },
        {
            "date": "2008-05-07T22:01:33+0000",
            "content": "lucene.1278.5.7.2008.patch - RangeFilter, FieldCacheImpl, ExtendedFieldCacheImpl automatically use termenum loaddocs=true.  A byte buffer is used to load the docs, if the bytes read will exceed the buffer, a new IndexInput is created and a DocIdSetIterator will read over that when TermEnum.docs() is called.  This makes the usual case of a small number of docs (usually 1) to a term fast using a byte buffer.  It also removes the issue if a term has too many docs and the whole byte array is loaded into ram.  TermEnum.docs() returns DocIdSetIterator.\n\nlucene.1278.5.7.2008.test.patch - TestSort stores documents with Field.TermDocs.STORE\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12595058"
        },
        {
            "date": "2008-05-14T13:45:36+0000",
            "content": "Some comments on the 5.7.2008 patch:\n\nThe test with 7.6 times speedup for very few docs per term makes me wonder why this never showed up as a performance problem before. It certainly shows an advantage of flexible indexing for the case in which the within document term frequencies are not needed (for example primary/foreign keys, which normally end up in a keyword field.)\n\nIn the patch, DocIdSetIterator is used in the org.apache.lucene.index package, so it would be a good idea to move it from o.a.l.search to o.a.l.index or to o.a.l.util to avoid a circular dependency involving the index and search packages. As DocIdSetIterator is not yet released, this move should be no problem.\n\nThe DocIdSetReader class in the patch has so much code in common with SortedVIntList that it might be better to merge the two into a single one, and try and refactor common code into new methods there.\nThat would also be an easy way to get rid of the unsupported skipTo() operation.\n ",
            "author": "Paul Elschot",
            "id": "comment-12596762"
        },
        {
            "date": "2008-05-21T19:57:26+0000",
            "content": "Have a new patch that handles deleted docs but realized that returning DocIdSetIterator is not needed.  This implementation can integrate with TermDocs transparently.  The issue is then whether to keep the Fieldable.isStoreTermDocs or make the implementation a default for untokenized fields.  For untokenized fields, this would mean not having to store the docs in the segment.frq file.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12598770"
        },
        {
            "date": "2008-05-21T21:05:43+0000",
            "content": "Thought of some simple logic for this that will make it work automatically with no user interaction and no API additions.\n\nIf the term is located in less than or equal to the skipinterval of termdocs docs, and the term frequency for each doc is 1, then the docs should be stored in segment.tis.  Otherwise they should be stored as usual in segment.frq.  \n\nThe problem is knowing whether the logic is true in the DocumentsWriter.appendPostings method.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12598793"
        },
        {
            "date": "2008-07-20T11:01:50+0000",
            "content": "in light of Mike's comments hier (Michael McCandless - 05/May/08 05:33 AM), I think it is worth mentioning that I am working on LUCENE-1340, that is storing postings without additional frq info. \n\ncorrect me if I am wrong, the only difference is that this approach with *.frq needs one seek more... at the same time, this could potentially increase term dict size, so we loose some locality.\n\nYour your last proposal sounds interesting,  \"inline short postings\" into term dict , so for short postings (about the size of offset pointer into *.frq) with tf==1 (that is the always the case if you use omitTf(true) from LUCENE-1340)  we spare one seek()... this could be a lot. Also, there is no need to store postings into *frq  (this complicates maintenance I guess)   ",
            "author": "Eks Dev",
            "id": "comment-12615077"
        },
        {
            "date": "2008-07-22T12:56:11+0000",
            "content": "In order for the proposal I mentioned to work, DocumentsWriter.appendPostings needs to be changed to store the docs in an IntArrayList or something or the sort, then decide where to store the postings.  \n\nI started working on LUCENE-1292 to address this problem outside of reworking core Lucene.  LUCENE-1278 only addresses half of my problem.  I also want realtime updates to an in memory term index.  The most efficient way to achieve this is what is outlined in LUCENE-1292. ",
            "author": "Jason Rutherglen",
            "id": "comment-12615611"
        },
        {
            "date": "2010-04-15T11:05:29+0000",
            "content": "I think the pulsing codec (wraps any other codec, but inlines low-freq terms directly into the terms dict) solves this? ",
            "author": "Michael McCandless",
            "id": "comment-12857277"
        }
    ]
}