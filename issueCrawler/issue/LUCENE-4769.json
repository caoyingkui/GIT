{
    "id": "LUCENE-4769",
    "title": "Add a CountingFacetsAggregator which reads ordinals from a cache",
    "details": {
        "components": [
            "modules/facet"
        ],
        "fix_versions": [
            "4.2",
            "6.0"
        ],
        "affect_versions": "None",
        "priority": "Major",
        "labels": "",
        "type": "New Feature",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Mike wrote a prototype of a FacetsCollector which reads ordinals from a CachedInts structure on LUCENE-4609. I ported it to the new facets API, as a FacetsAggregator. I think we should offer users the means to use such a cache, even if it consumes more RAM. Mike tests show that this cache consumed x2 more RAM than if the DocValues were loaded into memory in their raw form. Also, a PackedInts version of such cache took almost the same amount of RAM as straight int[], but the gains were minor.\n\nI will post the patch shortly.",
    "attachments": {
        "LUCENE-4769.patch": "https://issues.apache.org/jira/secure/attachment/12568816/LUCENE-4769.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2013-02-11T12:36:38+0000",
            "content": "Patch adds CachedIntsCountingFacetsAggregator. I modified CountingFacetsAggregator test to randomly pick it, and it passed for many iterations (still, it does not generate many documents).\n\nMike, can you give it a try w/ luceneutil? Measure the time differences as well as the RAM footprint?\n\nIf it works and we'll decide to keep it (not as default though!), then I want to create an AbstractCountingFacetsAggregator which will implement rollupValues and requiresScore, which are now duplicated among 3 counting aggregators. The other aggregators should just implement aggregate. ",
            "author": "Shai Erera",
            "id": "comment-13575762"
        },
        {
            "date": "2013-02-11T14:19:45+0000",
            "content": "Full (6.6M) wikibig index, 7 facet dims:\n\n\n                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff\n                 Respell       46.60      (3.4%)       45.82      (4.1%)   -1.7% (  -8% -    6%)\n            HighSpanNear        3.49      (1.7%)        3.51      (2.2%)    0.8% (  -3% -    4%)\n              HighPhrase       17.13     (10.5%)       17.42     (11.0%)    1.7% ( -17% -   26%)\n                  Fuzzy2       53.25      (2.8%)       54.19      (3.1%)    1.8% (  -4% -    7%)\n              AndHighLow      587.43      (2.3%)      597.84      (2.6%)    1.8% (  -3% -    6%)\n         LowSloppyPhrase       20.30      (2.3%)       20.68      (2.3%)    1.9% (  -2% -    6%)\n             LowSpanNear        8.24      (2.3%)        8.42      (2.9%)    2.1% (  -3% -    7%)\n             AndHighHigh       23.36      (1.3%)       23.95      (0.9%)    2.5% (   0% -    4%)\n        HighSloppyPhrase        0.92      (5.1%)        0.94      (6.1%)    2.8% (  -7% -   14%)\n               LowPhrase       21.02      (6.2%)       21.63      (6.7%)    2.9% (  -9% -   16%)\n             MedSpanNear       28.31      (1.3%)       29.20      (1.5%)    3.1% (   0% -    6%)\n         MedSloppyPhrase       25.98      (1.7%)       26.79      (1.7%)    3.1% (   0% -    6%)\n                 MedTerm       47.54      (1.9%)       49.49      (3.4%)    4.1% (  -1% -    9%)\n                  Fuzzy1       47.28      (2.2%)       49.27      (2.6%)    4.2% (   0% -    9%)\n              AndHighMed      105.55      (0.9%)      112.03      (1.2%)    6.1% (   3% -    8%)\n                Wildcard       27.63      (1.2%)       30.03      (1.6%)    8.7% (   5% -   11%)\n               MedPhrase      109.43      (5.6%)      122.45      (7.4%)   11.9% (   0% -   26%)\n                 LowTerm      110.94      (1.9%)      128.73      (1.8%)   16.0% (  12% -   20%)\n               OrHighLow       17.11      (2.2%)       22.44      (3.7%)   31.1% (  24% -   37%)\n               OrHighMed       16.63      (2.1%)       21.89      (3.8%)   31.6% (  25% -   38%)\n                HighTerm       19.17      (1.9%)       26.30      (3.5%)   37.2% (  31% -   43%)\n              OrHighHigh        8.77      (2.4%)       12.45      (4.7%)   42.1% (  34% -   50%)\n                 Prefix3       13.06      (1.8%)       18.66      (2.2%)   42.9% (  38% -   47%)\n                  IntNRQ        3.59      (1.6%)        6.45      (3.3%)   79.8% (  73% -   86%)\n\n\n\ntrunk DVs take 61.4 MB while the int[] cache takes 202.9 MB (3.3X\nmore).  Also, if users use the int[] cache they must remember to use\n(and maybe we check / warn about it somehow) a disk-backed DV else\nit's silly since you'd be double-caching in RAM.\n\nCuriously these gains are not that much better (except IntNRQ) than\nLUCENE-4764, which was only ~31% larger... which is odd because we had\npreviously tested [close to] LUCENE-4764 against int[] cache and it\nwas faster. ",
            "author": "Michael McCandless",
            "id": "comment-13575802"
        },
        {
            "date": "2013-02-11T14:36:16+0000",
            "content": "Previously, on LUCENE-4609, we tested on the full 2.2M ordinals, which produced larger DVs. So I think in order to compare to those results, we need to repeat that test on the full set of ordinals. And separately, it would be good to tell whether LUCENE-4764 is also faster (and if so, by how much) on the full set of ordinals.\n\nThen, we can compare fast-DV to int[]. ",
            "author": "Shai Erera",
            "id": "comment-13575812"
        },
        {
            "date": "2013-02-11T20:57:51+0000",
            "content": "Full (6.6M docs) wikibig, 9 dims.  Base is trunk, comp is int[] cache:\n\n                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff\n                 Respell       44.87      (3.2%)       44.32      (3.8%)   -1.2% (  -7% -    5%)\n              AndHighLow       64.10      (1.8%)       63.33      (1.1%)   -1.2% (  -4% -    1%)\n             LowSpanNear        7.10      (2.1%)        7.18      (2.0%)    1.1% (  -2% -    5%)\n                  Fuzzy2       28.55      (2.0%)       29.06      (1.6%)    1.8% (  -1% -    5%)\n              HighPhrase       13.69      (8.7%)       13.98      (8.3%)    2.1% ( -13% -   20%)\n         LowSloppyPhrase       15.40      (2.6%)       15.85      (1.6%)    3.0% (  -1% -    7%)\n              AndHighMed       39.48      (1.0%)       41.07      (0.8%)    4.0% (   2% -    5%)\n            HighSpanNear        2.91      (1.3%)        3.03      (1.1%)    4.2% (   1% -    6%)\n               LowPhrase       15.01      (4.4%)       15.80      (4.7%)    5.2% (  -3% -   14%)\n             MedSpanNear       17.68      (1.4%)       18.87      (1.2%)    6.7% (   3% -    9%)\n         MedSloppyPhrase       16.56      (1.3%)       17.82      (1.3%)    7.6% (   5% -   10%)\n               MedPhrase       41.08      (2.5%)       44.27      (2.8%)    7.8% (   2% -   13%)\n                  Fuzzy1       24.08      (1.5%)       25.97      (1.9%)    7.9% (   4% -   11%)\n        HighSloppyPhrase        0.82      (6.2%)        0.89      (5.6%)    9.1% (  -2% -   22%)\n                 LowTerm       34.22      (1.1%)       39.13      (1.3%)   14.3% (  11% -   16%)\n             AndHighHigh       11.87      (1.3%)       13.96      (1.2%)   17.6% (  14% -   20%)\n                Wildcard       13.02      (1.9%)       16.02      (1.5%)   23.1% (  19% -   27%)\n                 MedTerm       20.04      (2.2%)       24.86      (2.4%)   24.1% (  19% -   29%)\n               OrHighMed        7.02      (2.7%)        9.85      (2.3%)   40.3% (  34% -   46%)\n               OrHighLow        7.08      (2.8%)        9.95      (2.8%)   40.5% (  33% -   47%)\n                HighTerm        7.52      (2.5%)       10.78      (2.1%)   43.4% (  37% -   49%)\n                 Prefix3        5.71      (2.4%)        8.51      (1.5%)   48.9% (  43% -   54%)\n              OrHighHigh        3.99      (2.3%)        6.26      (2.6%)   56.8% (  50% -   63%)\n                  IntNRQ        1.91      (2.3%)        3.66      (3.0%)   91.9% (  84% -   99%)\n\n\n\nCache is 291.5 MB, and trunk is 129.0 MB = 2.26X larger. ",
            "author": "Michael McCandless",
            "id": "comment-13576119"
        },
        {
            "date": "2013-02-11T21:00:46+0000",
            "content": "Nice results. x2.26 more on such large DVs is something you can live with . And the gains are better than fast DV, I guess because when u have that many ords, the VInt+Dgap decoding time is significant, and stands out more.\n\nSo I think we should commit this Aggregator? ",
            "author": "Shai Erera",
            "id": "comment-13576121"
        },
        {
            "date": "2013-02-11T21:11:27+0000",
            "content": "I dont know what an aggregator is, but this could also be done as a codec (similar fashion as 4764)?\n\nisn't it basically DirectFacetingDocValuesFormat, would just hold its data as int[]s and the collector could get at it like 4764? ",
            "author": "Robert Muir",
            "id": "comment-13576128"
        },
        {
            "date": "2013-02-11T21:31:26+0000",
            "content": "FacetsAggregator is an abstraction of the facets package that lets you compute different functions on the aggregated ordinals. E.g. counting is equivalent to #sum(1), while SumScoreFacetsAggregator does #sum(score) etc.\n\nYou're right that this could be implemented as a Codec, and then we won't even need to alert the user that if he uses that caching method, he should use DiskValuesFormat. But it looks an awkward decision to me. Usually, caching does not force you to index stuff in a specific way. Rather, you decide at runtime if you want to cache the data or not. You can even choose to stop using the cache, while the app is running. Also, it's odd that if the app already indexed documents with the default Codec, it won't be able to using this caching method, unless it reindexes, or until those segments are merged (b/c their DVFormat will be different, and so the aggregator would need to revert to a different counting code).\n\nI dunno ... it's certainly doable, but it doesn't feel right to me. ",
            "author": "Shai Erera",
            "id": "comment-13576144"
        },
        {
            "date": "2013-02-11T21:45:29+0000",
            "content": "I dont look at it as a cache, but a different codec representation that holds the stuff in int[]s.\n\nJust like DirectPostingsFormat. ",
            "author": "Robert Muir",
            "id": "comment-13576152"
        },
        {
            "date": "2013-02-11T22:03:56+0000",
            "content": "It's not like DirectPostingsFormat though. DPF hides the int[] from you, and you interact with the general API, not knowing that under the covers it does things more efficiently. I think that on LUCENE-4764, if we can prove that this specialization doesn't help much (i.e. you don't need to cast to FacetsDV and pull the addresses and bytes), then it'd be compelling. And if we had a DV type that had .get(doc, IntsRef), then an int[] DVFormat would also make sense.\n\nBut if we implement that as a Codec, then the app would need to set both the Codec and the matching FacetsAggregator. Also, it will be ineffective to use this Codec on existing large indexes, as you won't gain anything. I treat this like FieldCache .. you have something indexed one way, and read another way. Again, if there was a DVFormat that would let me ask for all integers of a document, it'd be a different story I think. ",
            "author": "Shai Erera",
            "id": "comment-13576164"
        },
        {
            "date": "2013-02-11T22:14:02+0000",
            "content": "i dont think we should add a docvalues format for that though: because what lucene/facets is doing isn't a natural search use case.\n\nits (ab)using DV as a way to store custom per-doc pointers to its own separate data structures: i think the approach mike is taking on LUCENE-4764 is right, this encoding and so on should be private to facets. \n\nyou can extends BinaryDocValues (FacetDocValues) and add your own int[] api on top and instanceof: you don't need indexwriter assistance for that. ",
            "author": "Robert Muir",
            "id": "comment-13576168"
        },
        {
            "date": "2013-02-12T04:36:38+0000",
            "content": "I didn't propose that we add a DV format, I was saying that if there was one, then a DirectFacets format would make sense, b/c the app wouldn't need to write special code to work with it ... it would just return the ints more efficiently.\n\nAnd we're abusing DV now, just like we abused payloads before, so nothing has changed .\n\nI did propose on another issue (forgot where, maybe the migration layer issue?) to develop a FacetsCodec, but you were against it. Perhaps after you worked on DV 2.0 you now think it makes more sense? It will solve a slew of problems I think.\n\nThis FacetsCodec today is mimicked by CategoryListIterator which exposes that getInts API. But Mike and I saw that the DV abstraction (getBytes) + CLI (getInts) hurts performance, therefore the *fast* aggregators / collectors sidestep the CLI abstrtaction and uses only DV. On LUCENE-4764, mike sidesteps the DV abstraction too, which results in more duplicated code. I'm all for those specializations, but it becomes harder to maintain. I just think of all the places we'd need to change if someone will find a better encoding than gap+vint . \n\nPlus, the specialization doesn't serve the different facet features. I.e. if I'm interested in fast sum-score, I need to write a specialized one. If I'm interested in fast sum-association, I need to write one. Just to be clear, I'm not complaining and I think it makes sense for expert apps to write some specialized code. What I am saying is that if we could make the abstractions FAST, then we'd lower the bar of when apps would need to do that ...\n\nSo far, our latest optimizations only pertain to the counting case. It is the common case and I think it's important that we did that. Perhaps the rest of the API changes also improved the other cases too, but it's clear that if we want to really speed them up, we should specialize them.\n\nMaybe if we had a FacetsCodec, with CategoryListFormat (an extension to Codec, private to Facets), then LUCENE-4764 and this issue would benefit out-of-the-box all facet features. Because that format will expose what facets need - a getInts API. And if we make this one a Codec and FastDV a Codec, then we anyway force the app to declare a special facets Codec, so at least from that aspect, we won't require more ...\n\nAnd if we do a FacetsCodec w/ CategoryListFormat, then at first it can continue to abuse DV, but then in the future we can explore a different format to manage the per-document categories (and support category associations). Maybe even a way to manage the taxonomy in the main index, in its own data structure ...\n\nPerhaps these two issues show the usefulness of having such Codec? ",
            "author": "Shai Erera",
            "id": "comment-13576367"
        },
        {
            "date": "2013-02-12T04:58:15+0000",
            "content": "I still dont get why you need anything special here, more than what Lucene already provides.\n\nYou seem hell-bent on the idea that lucene should have a getInts(docid, IntsRef) api for facets.\n\nWhat I'm trying to say is, if you really think thats the API you should have, nothing in lucene stops you from doing this, just add it:\n\n\npackage org.apache.lucene.facet.index;\n\nabstract class FacetDocValues extends BinaryDocValues {\n  abstract void getInts(int docid, IntsRef ref);\n}\n\n\n\nWhat is the problem? You can then make a jazillion implementations like LUCENE-4764 extend that API and have one collector that uses it. ",
            "author": "Robert Muir",
            "id": "comment-13576377"
        },
        {
            "date": "2013-02-12T06:35:01+0000",
            "content": "Ok .. I think I know where the confusion is, and it's mostly due my lack of proper understanding of Codecs ..\n\nWe basically mean the same thing, only what you propose is more realistic w/ today's IndexReader API, which only exposes docValues. While what I had in mind (taking a look again at notes I wrote few months ago) is that facets could have a CompositeReader impl which adds facets specific API. Until then, we have no other choice but to piggy-back on DV API, and that means extending DVFormat. Thanks for insisting, it made me understand how this should work ... (sorry, but I didn't write a Codec yet).\n\nPerhaps separately we can think about an IndexReader impl for facets, which will open the road to many different optimizations, e.g. maintaining a per-segment taxonomy and top-level reader global-ordinal map (all in-memory), encoding facet ordinals in their own structure (and not DV) and maybe even managing the global taxonomy as part of the search index (through sidecar files or something), w/o the sidecar index, which I think today is a barrier for apps as well as integrating that into Solr or ES. But that should be done separately as it's a major refactoring to how facets work.\n\nEven FacetsDV are sort of a refactoring (i.e. replacing CategoryListIterator with that .. if we want to do it right), so I think that for now I'm going to still commit that cache as an aggregator and we can get rid of it once we do FacetsDV.\n\nOh .. and there was one thing that bothered me in that statement:\n\nYou seem hell-bent on the idea that lucene should have a getInts(docid, IntsRef) api for facets\n\nFirst, I'm not hell-bent on anything (don't even know what that means). Second, facets are now a *lucene* module, and not private to me. From my perspective, lucene doesn't need to have anything for me, but lucene should have the best facets module. So far I've been busy refactoring facets so they work faster and have cleaner API ... not to me, to lucene users. I'm sure things can be simplified even further and improved even more. I think about it constantly. If you have a better idea of how facets should work (while maintaining current capabilities, as much as possible), I'm all open to suggestions, really. ",
            "author": "Shai Erera",
            "id": "comment-13576418"
        },
        {
            "date": "2013-02-12T06:52:56+0000",
            "content": "Added IntRollupFacetsAggregator which is an abstract FacetsAggregator that implements rollupValues by summing the values from FacetArrays.getIntArray(). Moved the 3 counting aggregators (including the one in this patch) to extend it and focus on implementing aggregate only. ",
            "author": "Shai Erera",
            "id": "comment-13576425"
        },
        {
            "date": "2013-02-12T12:49:11+0000",
            "content": "\nUntil then, we have no other choice but to piggy-back on DV API, and that means extending DVFormat.\n\nWell mainly I'm trying to make sure we only have the minimum DocValues types and APIs we actually need. Additional types are very costly to us.\n\nI'm still unsure myself that lucene should have a byte[] docvalues type that is unsorted: I don't see any real use cases for it directly.\n\nBut for someone who wants to encode their own data structures, having a per-document byte[] type where your codec can see all the values is pretty powerful. So if having this \"catch-all\" type prevents additional types from being added to lucene, then maybe its worth it.\n\n\nPerhaps separately we can think about an IndexReader impl for facets, which will open the road to many different optimizations, e.g. maintaining a per-segment taxonomy and top-level reader global-ordinal map (all in-memory), encoding facet ordinals in their own structure (and not DV) and maybe even managing the global taxonomy as part of the search index (through sidecar files or something), w/o the sidecar index, which I think today is a barrier for apps as well as integrating that into Solr or ES. But that should be done separately as it's a major refactoring to how facets work.\n\nI think a custom IndexReader impl would prevent barriers for integration with those systems too, just in a different way. Personally I think the current design (sidecar) is the most performant. But we should consider adding other possibilities to lucene that make different tradeoffs, e.g. work without it. \n\n\nFirst, I'm not hell-bent on anything (don't even know what that means). Second, facets are now a lucene module, and not private to me. From my perspective, lucene doesn't need to have anything for me, but lucene should have the best facets module. So far I've been busy refactoring facets so they work faster and have cleaner API ... not to me, to lucene users. I'm sure things can be simplified even further and improved even more. I think about it constantly. If you have a better idea of how facets should work (while maintaining current capabilities, as much as possible), I'm all open to suggestions, really.\n\nI know, you are doing a great job. I'm just explaining my opinion on this situation: having facets \"build on top of\" BinaryDocValues doesnt hurt it in the slightest. Sometimes I wonder if you are having this argument with me to avoid a single type cast in the facets codebase or for some other cosmetic reason  ",
            "author": "Robert Muir",
            "id": "comment-13576593"
        },
        {
            "date": "2013-02-12T13:27:41+0000",
            "content": "Sometimes I wonder if you are having this argument with me to avoid a single type cast in the facets codebase or for some other cosmetic reason\n\nNot at all. I'm looking for the cleanest solution. I'm just much less familiar that you when it comes to Codecs and Formats, and therefore I fail to think of ways to piggy-back (abuse? ) them for facets.\n\nI certainly think your FacetsBDV idea is good. I don't mind casting if it will help the API. Basically .getInts() is the API of CategoryListIterator today ... maybe we can nuke CLI in favor of FacetsBDV? Definitely worth looking at. Lets's explore that in a separate issue though. So according to the results on LUCENE-4764, we don't need to do any specialization to get the bytes of a certain document. And in the future, with FacetsBDV we won't need CachedInts as aggregator, but as another Codec, and again, won't need to specialize (I hope!).\n\nAnd if we ever develop a custom IR for facets, we can add the .getInts API higher-up the chain, and not necessarily depend on DocValues. ",
            "author": "Shai Erera",
            "id": "comment-13576608"
        },
        {
            "date": "2013-02-12T15:26:04+0000",
            "content": "I wonder, since CachedInts for a segment is created only once, if we should make it more generic, and work w/ IntDecoder instead of hard-coding to dgap+vint? Then, we can make this class top-level, e.g. as OrdinalsCache with a static getCachedInts/Ords for a segment? Otherwise, I'll add an assert to CachedIntsCountingFacetsAggregator to ensure the decoder used is DGapVInt. ",
            "author": "Shai Erera",
            "id": "comment-13576681"
        },
        {
            "date": "2013-02-12T16:11:46+0000",
            "content": "Patch adds OrdinalsCache to handle the caching and CachedOrdsCountingFacetsAggregator uses it. OrdinalsCache uses IntDecoder to decode the values from the category list.\n\nI think this is ready to commit. ",
            "author": "Shai Erera",
            "id": "comment-13576728"
        },
        {
            "date": "2013-02-12T16:22:12+0000",
            "content": "[trunk commit] Shai Erera\nhttp://svn.apache.org/viewvc?view=revision&revision=1445234\n\nLUCENE-4769: Add a CountingFacetsAggregator which reads ordinals from a cache ",
            "author": "Commit Tag Bot",
            "id": "comment-13576737"
        },
        {
            "date": "2013-02-12T16:40:48+0000",
            "content": "Committed to trunk and 4x ",
            "author": "Shai Erera",
            "id": "comment-13576758"
        },
        {
            "date": "2013-02-12T16:48:32+0000",
            "content": "[branch_4x commit] Shai Erera\nhttp://svn.apache.org/viewvc?view=revision&revision=1445251\n\nLUCENE-4769: Add a CountingFacetsAggregator which reads ordinals from a cache ",
            "author": "Commit Tag Bot",
            "id": "comment-13576766"
        },
        {
            "date": "2013-05-10T10:33:56+0000",
            "content": "Closed after release. ",
            "author": "Uwe Schindler",
            "id": "comment-13654073"
        }
    ]
}