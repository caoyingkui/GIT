{
    "id": "SOLR-12820",
    "title": "Auto pick method:dvhash based on thresholds",
    "details": {
        "type": "Improvement",
        "status": "Open",
        "labels": "",
        "fix_versions": [],
        "components": [
            "Facet Module"
        ],
        "priority": "Major",
        "resolution": "Unresolved",
        "affect_versions": "None"
    },
    "description": "I've worked with two users last week where explicitly using method:dvhash improved the faceting speeds drastically.\n\nThe common theme in both the use-cases were:\u00a0 One collection hosting data for multiple users.\u00a0 We always filter documents for one user ( therby limiting the number of documents drastically ) and then perfoming a complex nested JSON facet.\n\nBoth use-cases fit perfectly in this criteria that Yonik Seeley mentioed on SOLR-9142\nfaceting on a string field with a high cardinality compared to it's domain is less efficient than it could be.\nAnd DVHASH was the perfect optimization for these use-cases.\n\nWe are using the facet stream expression in one of the use-cases which doesn't expose the method param. We could expose the method param to facet stream but I feel the better approach to solve this problem would be to address this TODO in the code withing the JSON Facet Module\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 if (mincount > 0 && prefix == null && (ntype != null || method == FacetMethod.DVHASH)) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 // TODO can we auto-pick for strings when term cardinality is much greater than DocSet cardinality?\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 //\u00a0\u00a0 or if we don't know cardinality but DocSet size is very small\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 return new FacetFieldProcessorByHashDV(fcontext, this, sf);\n\nI thought about this a little and this was the approach I am thinking currently to tackle this problem\n\nint matchingDocs = fcontext.base.size();\nint totalDocs = fcontext.searcher.getIndexReader().maxDoc();\n//if matchingDocs is close to the totalDocs then we aren't filtering many documents.\n//that means the array approach would probably be better than the dvhash approach\n\n//Trying to find the cardinality for the matchingDocs would be expensive.\n//Also for totalDocs we don't have a global cardinality present at index time but we have a per segment cardinality\n\n//So using the number of matches as an alternate heuristic would do the job here?\n\nAny thoughts if this approach makes sense? it could be I'm thinking of this approach just because both the users I worked with last week fell in this cateogory.\n\n\u00a0\n\ncc David Smiley Joel Bernstein",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16636441",
            "content": "Makes sense to me.  It'd be nice to consider FacetMethod here as well so that a user that sets the FacetMethod to \"DV\" then he/she gets the current ordinal array algorithm.  Or maybe the ratio could be configurable.  Looking back... hmm... I suppose if the ratio were configurable, then there would be no need for DVHASH enum.\n\nWhat ratio of docSet to numDocs?  Perhaps 1/16th or smaller use hash? ",
            "author": "David Smiley",
            "date": "2018-10-03T05:09:38+0000"
        },
        {
            "id": "comment-16636921",
            "content": "// Trying to find the cardinality for the matchingDocs would be expensive.\n\nThe heuristic I had in mind would just use the cardinality of the whole field in conjunction with fcontext.base.size()\nFor example, if one is faceting on US states (50 values) you're pretty much always going to want to use the array approach.  Comparing to maxDoc isn't too meaningful here.\n\nEven though it may not be implemented yet, we should also keep multi-valued fields in mind when thinking about the API access/control for this. ",
            "author": "Yonik Seeley",
            "date": "2018-10-03T13:09:16+0000"
        },
        {
            "id": "comment-16685527",
            "content": "In SOLR-12795 the method param is exposed so the user can at-least explicitly chose dvhash. I think this Jira is still valid as we could try to pick the default based on some heurestics ",
            "author": "Varun Thacker",
            "date": "2018-11-13T17:43:04+0000"
        }
    ]
}