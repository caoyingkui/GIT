{
    "id": "LUCENE-7648",
    "title": "Millions of fields in an index makes some operations slow, opening a new searcher in particular",
    "details": {
        "labels": "",
        "priority": "Minor",
        "resolution": "Unresolved",
        "affect_versions": "4.10.4",
        "status": "Open",
        "type": "Improvement",
        "components": [],
        "fix_versions": []
    },
    "description": "Got a Solr user who was experiencing very slow commit times on their index \u2013 10 seconds or more.  This is on a 650K document index sized at about 420MB, with all Solr cache autowarm counts at zero.\n\nAfter some profiling of their Solr install, they finally determined that the problem was an abuse of dynamic fields.  The largest .fnm file in their index was 130MB, with the total of all .fnm files at 140MB.  The user estimates that they have about 2 million fields in this index.  They will be fixing the situation so the field count is more reasonable.\n\nWhile I do understand that millions of fields in an index is a pathological setup, and that some parts of Lucene operation are always going to be slow on an index like that, 10 seconds for a new searcher seemed excessive to me.  Perhaps there is an opportunity for a little bit of optimization?\n\nThe version is old \u2013 4.10.4.  They have not yet tried a newer version.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15832113",
            "date": "2017-01-20T17:07:16+0000",
            "content": "If this issue gets some attention, the summary likely needs to be changed once somebody figures out what needs work. ",
            "author": "Shawn Heisey"
        },
        {
            "id": "comment-15832122",
            "date": "2017-01-20T17:13:50+0000",
            "content": "At this point, the assumption is that Lucene is where the time is taken, but if it turns out that it's in Solr, we can move the issue. ",
            "author": "Shawn Heisey"
        },
        {
            "id": "comment-15832135",
            "date": "2017-01-20T17:22:06+0000",
            "content": "I've also seen this. Even if you reindex all the docs without any of those fields they're still not purged. Your only real choice is to re-index into a new collection and stop abusing dynamic fields .\n\nIn a conversation I had it was pointed out that purging unused fields from merged segments would impose some performance penalties for regular merges, and penalizing everyone else to support this kind of scenario is not something I'd vote for.\n\nI do wonder though if there's any support for throwing an exception if some (configurable) limit was exceeded. In the case I saw it was a programming error rather than intentional.\n\nBut frankly most of the situations I see where there are so many fields are either programming errors or should be approached in a different way (payloads, indexing fieldX_keyword tokens rather than keywords in their own fields and the like). Not always possible of course.... ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-15832147",
            "date": "2017-01-20T17:32:23+0000",
            "content": "I do wonder though if there's any support for throwing an exception if some (configurable) limit was exceeded. In the case I saw it was a programming error rather than intentional.\n\nI was also thinking of opening a SOLR issue to drop a warning in the log on core startup (but not reload) if the number of fields is over some arbitrary number, maybe 5K or 50K, and maybe making the number configurable.\n\nI agree that something's probably very wrong when there are so many fields, and maybe it's not possible to optimize any further.  If that's the case, this issue can be closed. ",
            "author": "Shawn Heisey"
        },
        {
            "id": "comment-15832151",
            "date": "2017-01-20T17:34:09+0000",
            "content": "I'm just repeating what I remember much more knowledgeable people told me, I'll absolutely defer to their judgement on this. ",
            "author": "Erick Erickson"
        },
        {
            "id": "comment-15832187",
            "date": "2017-01-20T18:00:46+0000",
            "content": "Lucene really is not designed to handle a crazy number of unique fields; it's better to re-design how you use Lucene to map things down to one field instead.  It's just not a use case we try to handle at all. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-15832292",
            "date": "2017-01-20T19:25:56+0000",
            "content": "It's just not a use case we try to handle at all.\n\nUnderstandable.  With your input, I can point at this issue as concrete proof that a user shouldn't create tons of fields. ",
            "author": "Shawn Heisey"
        }
    ]
}