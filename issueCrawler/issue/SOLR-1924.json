{
    "id": "SOLR-1924",
    "title": "Solr's updateRequestHandler does not have a fast way of guaranteeing document delivery",
    "details": {
        "affect_versions": "1.4",
        "status": "Closed",
        "fix_versions": [],
        "components": [],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Duplicate"
    },
    "description": "It is currently not possible, without performing a commit on every document, to use updateRequestHandler to guarantee delivery into the index of any document.  The reason is that whenever Solr is restarted, some or all documents that have not been committed yet are dropped on the floor, and there is no way for a client of updateRequestHandler to know which ones this happened to.\n\nI believe it is not even possible to write a middleware-style layer that stores documents and performs periodic commits on its own, because the update request handler never ACKs individual documents on a commit, but merely everything it has seen since the last time Solr bounced.  So you have this potential scenario:\n\n\n\tmiddleware layer receives document 1, saves it\n\tmiddleware layer receives document 2, saves it\nNow it's time for the commit, so:\n\tmiddleware layer sends document 1 to updateRequestHandler\n\tsolr is restarted, dropping all uncommitted documents on the floor\n\tmiddleware layer sends document 2 to updateRequestHandler\n\tmiddleware layer sends COMMIT to updateRequestHandler, but solr adds only document 2 to the index\n\tmiddleware believes incorrectly that it has successfully committed both documents\n\n\n\nAn ideal solution would be for Solr to separate the semantics of commit (the index building variety) from the semantics of commit (the 'I got the document' variety).  Perhaps this will involve a persistent document queue that will persist over a Solr restart.\n\nAn alternative mechanism might be for updateRequestHandler to acknowledge specifically committed documents in its response to an explicit commit.  But this would make it difficult or impossible to use autocommit usefully in such situations.  The only other alternative is to require clients that need guaranteed delivery to commit on every document, with a considerable performance penalty.\n\nThis ticket is related to LCF in that LCF is one of the clients that really needs some kind of guaranteed delivery mechanism.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-12913867",
            "date": "2010-09-23T00:37:24+0000",
            "content": "I'm very much in favour of a robust way of knowing whether your docs are secure.\n\nBuilding a persistent input document queue introduces a bit too much overhead+latency for NRT cases, and should be avoided.\n\nOne way to go could be ACK through polling.\n1. Client performs an ADD (or other operation), with new optional parameters clientId=\"myClient\", batchId=\"fooBar0001\"\n2. Solr adds batch (group of docs) in memory and adds the batchId to a AckPendingQueue\n3. Client performs other requests, with or without explicit commit's, each with unique batchIds\n4. At every successful COMMIT, i.e. segment secured on disk, Solr moves all associated batchIds to an AckPersistedQueue\n5. Client polls as often it likes with a new operation <STATUS clientId=\"myClient\">\n6. The response from Solr is a list of pending and persisted batchIds for this clientId, \n    e.g. <persisted count=\"2\">fooBar0001 fooBar0002</persisted> <pending count=\"1\">fooBar0003</pending>\n7. Client can now update its state accordingly and know for sure it does not need to resubmit the persisted batches\n\nIn case of Solr restart or some error, all batchIds would disappear from both pending and persisted queues and the client can detect it's lost when parsing the next STATUS response, and resubmit the lost batches.\nThe overhead of this approach is maintaining the status memory queues on the indexers. Entries should expire after some time or queue size.\n\nBesides, in SolrJ we could then even implement auto polling, auto batch resubmit, auto feed speed throttling if #pending>someMaxValue, and give true callbacks to client code such as batchPersisted(), batchLost(), serverDown(), serverUp()... This will offload much complexity from the shoulders of the API consumer.\n\nDo you see any glitches in this general approach? "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-12921919",
            "date": "2010-10-17T23:11:38+0000",
            "content": "In a multi node environment, it would also be useful to maintain state as to whether a batch is replicated to the slaves. This is because in case of disaster crash on a master, the feeding client may have got callback that a batch is secured, but it was not yet replicated, i.e. the only copy was on the now crashed master. The master should be able to keep track of whether at least one replica has fetched a certain version of the index through the ReplicationHandler. In this way, a client could choose to act on the replication status instead of persisted status. The <STATUS> operation would now return an additional state:\n<replicated count=\"1\">fooBar0000</replicated> <persisted count=\"2\">fooBar0001 fooBar0002</persisted> <pending count=\"1\">fooBar0003</pending> "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13243609",
            "date": "2012-03-31T23:45:01+0000",
            "content": "This will be solved with SolrCloud as soon as SOLR-2700 is committed "
        }
    ]
}