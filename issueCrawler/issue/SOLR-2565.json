{
    "id": "SOLR-2565",
    "title": "Prevent IW#close and cut over to IW#commit",
    "details": {
        "affect_versions": "4.0-ALPHA",
        "status": "Closed",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "components": [
            "update"
        ],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Spinnoff from SOLR-2193. We already have a branch to work on this issue here https://svn.apache.org/repos/asf/lucene/dev/branches/solr2193 \n\nThe main goal here is to prevent solr from closing the IW and use IW#commit instead. AFAIK the main issues here are:\n\nThe update handler needs an overhaul.\n\nA few goals I think we might want to look at:\n\n1. Expose the SolrIndexWriter in the api or add the proper abstractions to get done what we now do with special casing:\n2. Stop closing the IndexWriter and start using commit (still lazy IW init though).\n3. Drop iwAccess, iwCommit locks and sync mostly at the Lucene level.\n4. Address the current issues we face because multiple original/'reloaded' cores can have a different IndexWriter on the same index.\n\nEventually this is a preparation for NRT support in Solr which I will create a followup issue for.",
    "attachments": {
        "dump.txt": "https://issues.apache.org/jira/secure/attachment/12490884/dump.txt",
        "fix+hossmans-test.patch": "https://issues.apache.org/jira/secure/attachment/12491538/fix%2Bhossmans-test.patch",
        "slowtests.txt": "https://issues.apache.org/jira/secure/attachment/12490879/slowtests.txt",
        "SOLR-2565__HuperDuperAutoCommitTest.patch": "https://issues.apache.org/jira/secure/attachment/12491526/SOLR-2565__HuperDuperAutoCommitTest.patch",
        "SOLR-2565.patch": "https://issues.apache.org/jira/secure/attachment/12483787/SOLR-2565.patch",
        "SOLR-2565-revert.patch": "https://issues.apache.org/jira/secure/attachment/12490892/SOLR-2565-revert.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Simon Willnauer",
            "id": "comment-13042025",
            "date": "2011-06-01T07:40:49+0000",
            "content": "we should improve the tests for file handles etc. in solr to make sure all file handles are released properly. This should give more confidence to commit large refactorings in solr "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13042048",
            "date": "2011-06-01T08:55:41+0000",
            "content": "I merged the initial commit from SOLR-2279 (which enables MockDirectoryWrapper for solr tests/tracking file handles)... no issues found from the branch.\n\nI'm gonna look now at some other general ways we can beef up the test coverage in Solr, especially by reusing what we have done in Lucene. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13042761",
            "date": "2011-06-02T12:57:23+0000",
            "content": "As far as tests go on this one, personally I am happy.\n\nIt would be good if people could review the branch (its really not a lot of code), and maybe give some opinions on the TODOs such as indexwriter setting changes on core reloads, etc.\n\nI think it would be better to merge this into trunk sooner than later and iterate in trunk myself. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13054738",
            "date": "2011-06-24T23:10:54+0000",
            "content": "here is a patch, showing all changes from the throwaway branch.\n\nI'm going on vacation for a week, i hope this thing is committed... if not I may just commit it myself the day I come back. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13054755",
            "date": "2011-06-24T23:35:44+0000",
            "content": "Threats, threats... "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13057318",
            "date": "2011-06-29T16:04:46+0000",
            "content": "If this patch applies, I'll commit it. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13057425",
            "date": "2011-06-29T19:42:58+0000",
            "content": "Okay - looks like this applies and tests pass. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13057829",
            "date": "2011-06-30T14:01:57+0000",
            "content": "Committed - there is still some wiki work to do. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13059587",
            "date": "2011-07-04T19:37:37+0000",
            "content": "I've still got to put a note in changes about how you should reload SolrCores after this change.  "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-13075980",
            "date": "2011-08-02T00:39:58+0000",
            "content": "This issue says committed in the comments, however it's status is: \"Unresolved\"? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13075993",
            "date": "2011-08-02T01:48:41+0000",
            "content": "Yeah, sorry - it's open as a reminder for me to make that changes note (or at least evaluate if something should be done) and do the wiki documentation. I'll try and do that tomorrow if I can and get this closed. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13081775",
            "date": "2011-08-09T17:24:46+0000",
            "content": "I think the removal of all of the synchronization in the update handler leads to a race condition on reopen.\n\nBasically, the order that we start a reopen in different threads does not have to correspond to the order that the reopen finishes, or the order that the new searcher is registered.  This can mean temporarily lost documents - a request can add a document + repoen and due to scheduling have an older reopen registered last in a different thread (meaning the document won't be visible).\n\nSo it looks like we need to synchronize to ensure that only one thread reopens at a time.\n\nAs far as I can tell, I don't think that this synchronization needs to extend to the IW.commit() for a hard commit.  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13081804",
            "date": "2011-08-09T18:09:15+0000",
            "content": "My TestRealTimeGet currently fails after a few minutes... it may be due to this (hopefully).  I'll try fixing. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13081883",
            "date": "2011-08-09T20:13:31+0000",
            "content": "With the following patch, TestRealTimeGet ran without errors (I stopped it after it ran for over an hour). "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13081967",
            "date": "2011-08-09T22:12:36+0000",
            "content": "+1 - makes sense, looks right.\n\nWithout the patch, your stress test fails for me on my new linux box in 6-12 seconds regularly. With the patch I just ran it for half an hour. "
        },
        {
            "author": "Vadim Kisselmann",
            "id": "comment-13083094",
            "date": "2011-08-11T12:39:40+0000",
            "content": "I tested the nightly build #1595 with an new patch (2565), but NRT doesn't work in my case.\n\nI index 10 docs/sec, it takes 1-30sec. to see the results.\nsame behavior when i update an existing document.\n\nMy addedDate is an timestamp (default=\"NOW\"). In worst case i can see what the document which i indexed is already more when 30\nseconds in my index, but i can't see it.\n\nMy Settings:\n<autoCommit>\n<maxDocs>1000</maxDocs>\n<maxTime>60000</maxTime>\n</autoCommit>\n\n<autoSoftCommit>\n<maxDocs>1</maxDocs>\n<maxTime>1000</maxTime>\n</autoSoftCommit>\n\nAre my settings wrong or need you more details?\nShould i use the coldSearcher (default=false)? Or set maxWarmingSearchers higher than 2?\n\nUPDATE:\nIf i only use autoSoftCommit and uncomment autoCommit it works. \nBut i should use the \"hard\" autoCommit, right? \nMark said yes:\nhttp://www.lucidimagination.com/blog/2011/07/11/benchmarking-the-new-solr-\u2018near-realtime\u2019-improvements/ "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13084487",
            "date": "2011-08-13T00:23:25+0000",
            "content": "Thanks Vadim - I'll look at this tomorrow. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13084494",
            "date": "2011-08-13T00:41:00+0000",
            "content": "Checked it real quick before I head out - it's a mistake in trying to not hard and soft commit at the same time. It's just not correct.\n\nI have a test case and fix - I'll post a patch tomorrow. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13084699",
            "date": "2011-08-13T18:58:41+0000",
            "content": "here is the fix and a test, I'll commit soon. "
        },
        {
            "author": "Vadim Kisselmann",
            "id": "comment-13084810",
            "date": "2011-08-14T09:46:03+0000",
            "content": "Hi Mark,\nthanks for the quick reply and fix.\nI test it tomorrow at work and report.\nRegards Vadim "
        },
        {
            "author": "Vadim Kisselmann",
            "id": "comment-13085072",
            "date": "2011-08-15T13:14:13+0000",
            "content": "Hi Mark,\nit works, thanks!\nThe docs are equally available.\n\n\nNow i'm fighting with \"Internal Server Errors\" caused by such queries:\nhttp://localhost:8080/solr/clustering?wt=ruby&rows=40&start=0&fl=description+pubDate&sort=pubDate+desc&q=london\nIt's this Issue: https://issues.apache.org/jira/browse/LUCENE-2208?focusedCommentId=13082315#comment-13082315 "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-13086755",
            "date": "2011-08-18T02:46:05+0000",
            "content": "Can this one be backported to 3.x?  It would probably be fairly useful for people to use now? "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13086986",
            "date": "2011-08-18T12:29:25+0000",
            "content": "+1 to backport; this is a serious limitation in Solr.\n\nElasticSearch has been using this approach (NRT & commit, not IW.close) for quite some time already. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13087361",
            "date": "2011-08-18T22:43:41+0000",
            "content": "thread dumps of slow tests "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13087368",
            "date": "2011-08-18T22:54:45+0000",
            "content": "I get the same thread dumps, sometimes also with a MockDirectoryFactory.maybeYield (seldom), and this one. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13087396",
            "date": "2011-08-18T23:30:16+0000",
            "content": "This is the \"minimal revert\". With this patch applied, the SolrJ tests are running fast again. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13087405",
            "date": "2011-08-18T23:38:33+0000",
            "content": "This is the thread about the slowness: http://www.lucidimagination.com/search/document/a7dcdd77c8b0d14d/solr_tests_in_trunk_sloooooooooooooooooooooooooow "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13087410",
            "date": "2011-08-18T23:43:39+0000",
            "content": "I committed the partial revert in revision: 1159448 "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13087453",
            "date": "2011-08-19T01:42:58+0000",
            "content": "Summary of IRC discussion (as i remember it) that didn't make it to the mailing list...\n\n\tUse and rmuir found that they could consistently reproduce \"slow\" and \"fast\" behavior in some tests using fixed seeds...\n\t\n\t\tant test -Dtestcase=SolrExampleBinaryTest -Dtests.seed=1:1:1 ... always slow\n\t\tant test -Dtestcase=SolrExampleBinaryTest -Dtests.seed=2:2:2 ... always fast\n\t\n\t\n\tfor Uwe and sarowe (on windows), slow ment \"taking so long they finally just had to give up and kill the JVM\"\n\tFor rmuir on his linux box, slow vs. fast was ~1-10 seconds vs 100 seconds\n\tFor hossman on my linux box, results were roughly consistent with rmuir, but slightly slower since i have a crappier box\n\twith the path Uwe commited in r1159448 rmuir, uwe, and sarowe all agreed the tests were \"fast\" again\n\ttestUnicode (where the majority of the slowness is seen) is one of the few test methods in these slow classes that uses random testing in a loop, so the compounded speed issues there are likely from some cumulative slow down that is typically only done rarely in other tests (likely \"commit\" given the nature of the commit that seemed to cause the problem)\n\tJenkin's is using a test multiplier of \"3\" which is specifically used in testUnicode to pick the number of iterations, so that obviously compounds the effects of this slowdown in jenkins \u2013 but makes me highly suspicious about why Uwe and sarowe were seeing so much longer test times on their windows machines then rmuir or i were seeing on linux (since they weren't using any special test multiplier)\n\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13090331",
            "date": "2011-08-24T16:34:59+0000",
            "content": "Hossman has been working on a new test and it has picked up a further issue along the lines of the one Vadim brought up - if using time based auto commit for both hard and soft commits, the soft commits won't happen when triggered by deletes. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13090376",
            "date": "2011-08-24T18:07:46+0000",
            "content": "As mark mentioned, i started looking into a better overall meme for testing autocommit \u2013 i could never reproduce any of the failures charlie cron was complaining about, but the tests didn't make any sense to me anyway.\n\npath demonstrates a new overall approach, using more detailed monitor of events \u2013 not just \"the most recent event\" but all of them in a queue of (rough) timestamps.\n\nat the moment one of the tests has an @Ignore:nocommit because of the delete bug that miller mentioned, but it would be helpful to know if people who were seeing problems running AutoCOmmitTest.testSoftAndHardCommitMaxTime (ie: charlie cron and simon) could try this patch out and see if it works better for them. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13090412",
            "date": "2011-08-24T19:11:06+0000",
            "content": "Looks cool, I'm beasting this test on a couple of machines now... "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13090434",
            "date": "2011-08-24T19:39:14+0000",
            "content": "a fix for the delete issue and hossmans test with the test method that was ignored not ignored anymore.\n\nthat test now fails for me due to what looks like a timing issue  "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13090463",
            "date": "2011-08-24T20:42:29+0000",
            "content": "I ran the original patch (with the @ignore still enabled) 100x each on my fast and slow machine. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13096346",
            "date": "2011-09-02T21:48:41+0000",
            "content": "So now that commits no longer block adds, we should revisit what the best defaults are.\nThe transaction logging in SOLR-2700 needs to keep track of uncommitted documents - hence if it doesn't affect performance too much, we should probably commit more often some how.  Autocommit based on number of documents doesn't work well across the broad spectrum of users (what value would work well for twitter indexers and book indexers).  A size-based approach would probably work best, but we don't have that.  Maybe a time based approach?  That would limit the transaction log size to the number of documents indexable in a given time period, which should be roughly proportional to the document size.  I guess such a time period should be somewhere between 10 and 60 seconds?  A lot of data can be indexed in 60 sec, and the goal is to limit the transaction log size while not impacting performance too much due to increased commit frequency.\n\nThe other issue is soft commits... should we configure a soft commitWithin by default (prob within the range of 1-10 sec)? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13096716",
            "date": "2011-09-03T16:37:06+0000",
            "content": "I'd been considering a default auto-soft commit by time.\n\nIf you think every minute is a good hard commit upper bound due to the transaction log, my initial thoughts are:\n\nAuto commit by time\nhard commit: 60 seconds\nsoft commit: 1 second "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13096774",
            "date": "2011-09-04T00:02:01+0000",
            "content": "I did some performance tests w/ autoCommit=60000 (60 sec) vs no commits at all (10M small docs)\n6 runs for each configuration, times in seconds.\n\nno commits (not even at the end): 202,222,217,206,203,196 average=207\nautoCommit=60000: 201,206,210,226,220,201 average=210\n\nSo committing every 60 seconds actually came out faster (probably due to chance because of the high variability).\n\nAside: there could be a race issue w/ autocommit.  I was checking the segments_n files to verify that commits were actually taking place periodically.  On a new index, the first file is segments_1.  All of the tests ran around 3 minutes 30 seconds... which should have meant 4 more commits, or a segments_5 file after 4 minutes.  I only checked a few times, but one time I saw a segments_6, and two other times I saw a segments_9 file.  This suggests that too many commits are happening for some reason. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13096778",
            "date": "2011-09-04T00:46:32+0000",
            "content": "there could be a race issue w/ autocommit\n\nI wouldn't be surprised - the AutoCommit tracker has never been 100% thread safe code. Though I did make some improvements, I didn't address everything I saw. I think it's generally been benign stuff though, so could be something else too. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13102088",
            "date": "2011-09-10T17:25:20+0000",
            "content": "Further work to be followed up in other issues - I've put in the basic doc on the wiki around this. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13141651",
            "date": "2011-11-01T21:53:47+0000",
            "content": "that test now fails for me due to what looks like a timing issue\n\nTwo of the tests in hossmans new AutoCommit test where failing for me on my linux box - i tweaked both to pass and also tested them on my mac. I've also reenabled the AutoCommit test class, but removed the tests involving soft commit - they will be tested with this new method in the new test class instead - later we can move the original auto commit test methods to the new style.\n\nI'll commit soon. "
        }
    ]
}