{
    "id": "SOLR-7021",
    "title": "Leader will not publish core as active without recovering first, but never recovers",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "4.10",
        "status": "Resolved",
        "resolution": "Cannot Reproduce",
        "priority": "Critical"
    },
    "description": "A little background: 1 core solr-cloud cluster across 3 nodes, each with its own shard and each shard with a single replica hence each replica is itself a leader. \n\nFor reasons we won't get into, we witnessed a shard go down in our cluster. We restarted the cluster but our core/shards still did not come back up. After inspecting the logs, we found this:\n\n\n015-01-21 15:51:56,494 [coreZkRegister-1-thread-2] INFO  cloud.ZkController  - We are http://xxx.xxx.xxx.35:8081/solr/xyzcore/ and leader is http://xxx.xxx.xxx.35:8081/solr/xyzcore/\n2015-01-21 15:51:56,496 [coreZkRegister-1-thread-2] INFO  cloud.ZkController  - No LogReplay needed for core=xyzcore baseURL=http://xxx.xxx.xxx.35:8081/solr\n2015-01-21 15:51:56,496 [coreZkRegister-1-thread-2] INFO  cloud.ZkController  - I am the leader, no recovery necessary\n2015-01-21 15:51:56,496 [coreZkRegister-1-thread-2] INFO  cloud.ZkController  - publishing core=xyzcore state=active collection=xyzcore\n2015-01-21 15:51:56,497 [coreZkRegister-1-thread-2] INFO  cloud.ZkController  - numShards not found on descriptor - reading it from system property\n2015-01-21 15:51:56,498 [coreZkRegister-1-thread-2] INFO  cloud.ZkController  - publishing core=xyzcore state=down collection=xyzcore\n2015-01-21 15:51:56,498 [coreZkRegister-1-thread-2] INFO  cloud.ZkController  - numShards not found on descriptor - reading it from system property\n2015-01-21 15:51:56,501 [coreZkRegister-1-thread-2] ERROR core.ZkContainer  - :org.apache.solr.common.SolrException: Cannot publish state of core 'xyzcore' as active without recovering first!\n\tat org.apache.solr.cloud.ZkController.publish(ZkController.java:1075)\n\n\n\nAnd at this point the necessary shards never recover correctly and hence our core never returns to a functional state.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2015-01-22T23:35:41+0000",
            "author": "James Hardwick",
            "content": "The key items to note being:\n\n\n\tcloud.ZkController  - I am the leader, no recovery necessary\n\tcore.ZkContainer  - :org.apache.solr.common.SolrException: Cannot publish state of core 'xyzcore' as active without recovering first!\n\n ",
            "id": "comment-14288455"
        },
        {
            "date": "2015-01-23T09:45:44+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "James, this sounds suspiciously similar to SOLR-6530 which was fixed in 4.10.2. The root cause is that some node marks a leader node as down via the leader-initiated-recovery logic because a commit couldn't be sent to it. ",
            "id": "comment-14289025"
        },
        {
            "date": "2015-01-23T09:57:11+0000",
            "author": "James Hardwick",
            "content": "Yep, we were looking at that one and we're wondering the same. The symptom is different but sounds like the solution might be the same. We'll give it a try! ",
            "id": "comment-14289032"
        },
        {
            "date": "2015-01-23T09:59:36+0000",
            "author": "James Hardwick",
            "content": "In the mean time, how do we best get around this? It still does not recover when we restart the cluster. Should manually kicking off a core reload for each node do the trick?  ",
            "id": "comment-14289034"
        },
        {
            "date": "2015-01-23T10:14:00+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "In the mean time, how do we best get around this? It still does not recover when we restart the cluster. Should manually kicking off a core reload for each node do the trick?\n\nYou should delete the /collections/your_collection_name/leader_initiated_recovery/shard_name node from zookeeper and restart the cluster. ",
            "id": "comment-14289047"
        },
        {
            "date": "2015-01-23T16:08:31+0000",
            "author": "James Hardwick",
            "content": "That worked Shalin. Thank you! ",
            "id": "comment-14289451"
        },
        {
            "date": "2015-04-02T22:22:09+0000",
            "author": "Nishanth Shajahan",
            "content": "We had to use the same workaround mentioned by Shalin.Thanks for that.This was in version 4.10.3 though, where we took down individual  solr nodes  one by one  for a fail over testing but  encountered the same bug.Shard 4 did not come up at all.The set up is that each shard has a leader and a follower.Leader was in recovery failed state and follower was in recovering state.\n\n\n2015-04-02 21:55:05,932 [coreZkRegister-1-thread-2] INFO  org.apache.solr.cloud.ShardLeaderElectionContext- I am the new leader: http://xxxx:8082/solr/coll1_replica1/ shard4\n2015-04-02 21:55:05,933 [coreZkRegister-1-thread-2] INFO  org.apache.solr.common.cloud.SolrZkClient- makePath: /collections/coll1/leaders/shard4\n2015-04-02 21:55:06,089 [zkCallback-2-thread-1] INFO  org.apache.solr.common.cloud.ZkStateReader- A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.js\non, has occurred - updating... (live nodes size: 16)\n2015-04-02 21:55:06,116 [coreZkRegister-1-thread-2] INFO  org.apache.solr.cloud.ZkController- We are http://xxx:8082/solr/coll1_replica1/ and leader is http://xxx:8082/sol\nr/coll1_replica1/\n2015-04-02 21:55:06,117 [coreZkRegister-1-thread-2] INFO  org.apache.solr.cloud.ZkController- No LogReplay needed for core=coll1_replica1 baseURL=http://xx:8082/solr\n2015-04-02 21:55:06,117 [coreZkRegister-1-thread-2] INFO  org.apache.solr.cloud.ZkController- I am the leader, no recovery necessary\n2015-04-02 21:55:06,117 [coreZkRegister-1-thread-2] INFO  org.apache.solr.cloud.ZkController- publishing core=coll1_replica1 state=active collection=coll1\n2015-04-02 21:55:06,121 [ExecutorThreadPool_SOLR_9] INFO  org.apache.solr.handler.admin.CoreAdminHandler- Going to wait for coreNodeName: core_node5, state: recovering, checkLive: true, onlyIfLeade\nr: true, onlyIfLeaderActive: true\n2015-04-02 21:55:06,123 [coreZkRegister-1-thread-2] INFO  org.apache.solr.cloud.ZkController- publishing core=coll1_replica1 state=down collection=coll1\n2015-04-02 21:55:06,132 [coreZkRegister-1-thread-2] ERROR org.apache.solr.core.ZkContainer- :org.apache.solr.common.SolrException: Cannot publish state of core 'coll1_replica1' as active without recovering first!\n        at org.apache.solr.cloud.ZkController.publish(ZkController.java:1082)\n        at org.apache.solr.cloud.ZkController.publish(ZkController.java:1045)\n        at org.apache.solr.cloud.ZkController.publish(ZkController.java:1041)\n        at org.apache.solr.cloud.ZkController.register(ZkController.java:856)\n        at org.apache.solr.cloud.ZkController.register(ZkController.java:770)\n        at org.apache.solr.core.ZkContainer$2.run(ZkContainer.java:221)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745) ",
            "id": "comment-14393589"
        },
        {
            "date": "2015-05-11T11:37:44+0000",
            "author": "Andrey Prokopenko",
            "content": "In my experience I've overcome this issue by first stop the cluster, then starting nodes in the same shard which were not in recovery state at the time when all the replicas in particular shard went down.\nSeems we have a deadlock conditions here: node is trying to recover from the leader, which in turn cannot recover itself. ",
            "id": "comment-14537838"
        },
        {
            "date": "2015-08-20T10:42:52+0000",
            "author": "Adrian Fitzpatrick",
            "content": "Also have seen this issue on Solr 4.10.3, on a 3 node cluster. Issue affected one of 3 collections only, and each of the 3 collections configured with 5 shards and 3 replicas. In the affected collection, for each of the 5 shards, the leader was on the same node (hadoopnode02) and was showing as down for all shards. Other replicas for each shard were reporting that were waiting for leader (eg \"I was asked to wait on state recovering for shard3 in the_collection_20150818161800 on hadoopnode01:8983_solr but I still do not see the requested state. I see state: recovering live:true leader from ZK: http://hadoopnode02:8983/solr/the_collection_20150818161800_shard3_replica2\")\n\nSomething like the work-around suggested by Andrey worked - we shut down the whole cluster, brought back up all nodes except the one which was reporting leader errors (hadoopnode02). This seemed to trigger a leader election but without a quorum. Then brought up hadoopnode02 - election then completed successfully and cluster state returned to normal.\n\n ",
            "id": "comment-14704663"
        },
        {
            "date": "2016-02-23T05:06:22+0000",
            "author": "Mark Miller",
            "content": "This is probably due to a bunch of Leader Initiated Recovery bugs that have since been fixed. ",
            "id": "comment-15158290"
        },
        {
            "date": "2016-06-06T12:33:42+0000",
            "author": "Forest Soup",
            "content": "Is there any plan to fix this? We found same log in v5.3.2 solrcloud. ",
            "id": "comment-15316424"
        },
        {
            "date": "2016-06-06T19:12:23+0000",
            "author": "James Hardwick",
            "content": "Forest Soup since updating to Solr 5.5+ we haven't had such issues.  ",
            "id": "comment-15317027"
        },
        {
            "date": "2016-11-30T23:39:47+0000",
            "author": "Erick Erickson",
            "content": "Looks to me like this has been fixed by other JIRAs according to the comments, so closing. We can open new ones for issues in 6x.  ",
            "id": "comment-15710221"
        }
    ]
}