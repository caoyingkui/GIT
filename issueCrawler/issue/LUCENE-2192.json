{
    "id": "LUCENE-2192",
    "title": "Memory Leak",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "2.9",
        "resolution": "Invalid",
        "status": "Closed"
    },
    "description": "Hi All ,\n\nI have been working on a problem with Lucene and now gave up after trying many different possibilites which gives me a feeling that There is a bug on this .\n\nThe scenario is we have an CMS applicaton into which we add new content every week , instead of updating the index which is a bit tricky, I prefer to delete all index documents and add them again which is straightforward . The problem is Lucene doesn't delete the old data somehow and increase the index size every time during the update . I also profile it with java tools and see that even if I close the IndexWriter class and sent it to Garbage Collector it holds all the docs in the memory .\n\nHere is the code I use \n\nDirectory directory = new SimpleFSDirectory(new File(path));\nwriter = new IndexWriter(directory, analyzer, false,IndexWriter.MaxFieldLength.LIMITED);\nwriter.deleteAll();\n//after adding docs close the indexwriter \nwriter.close();\n\nThe above code invoked every time we need to update the index . I tried many different scenario here to overcome the problem which includes physically removing the index directory( see how desperate I am ) , optimizing , flushing, commiting indexwriter, create=true parameter and so on . \n\n\n\nHere is the index file size during creation. If I shutdown the application and restart it , index size starts with 2,458 which is correct size.\n\nAny help will be appreciated\n\n\n_17.cfs   2,458 KB\n_18.cfs   3,990 KB\n_19.cfs  5,149 KB\n\nhere is the Lucene logs during creationg of index files 3 times in a row \nIFD [http-8080-1]: setInfoStream deletionPolicy=org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy@6649\nIW 0 [http-8080-1]: setInfoStream: dir=org.apache.lucene.store.SimpleFSDirectory@C:\\Documents and Settings\\rvarlikli\\workspace\\.metadata\\.plugins\\org.eclipse.wst.server.core\\tmp0\\wtpwebapps\\Clipbank3.5\\lucene autoCommit=false mergePolicy=org.apache.lucene.index.LogByteSizeMergePolicy@3b626c mergeScheduler=org.apache.lucene.index.ConcurrentMergeScheduler@baa6ba ramBufferSizeMB=16.0 maxBufferedDocs=-1 maxBuffereDeleteTerms=-1 maxFieldLength=10000 index=\nIW 0 [http-8080-1]: now flush at close\nIW 0 [http-8080-1]:   flush: segment=_17 docStoreSegment=_17 docStoreOffset=0 flushDocs=true flushDeletes=true flushDocStores=true numDocs=2765 numBufDelTerms=0\nIW 0 [http-8080-1]:   index before flush \nIW 0 [http-8080-1]: DW: flush postings as segment _17 numDocs=2765\nIW 0 [http-8080-1]: DW: closeDocStore: 2 files to flush to segment _17 numDocs=2765\nIW 0 [http-8080-1]: DW:   oldRAMSize=7485440 newFlushedSize=2472818 docs/MB=1,172.473 new/old=33.035%\nIFD [http-8080-1]: now checkpoint \"segments_1j\" [1 segments ; isCommit = false]\nIFD [http-8080-1]: now checkpoint \"segments_1j\" [1 segments ; isCommit = false]\nIFD [http-8080-1]: delete \"_17.fdx\"\nIFD [http-8080-1]: delete \"_17.tis\"\nIFD [http-8080-1]: delete \"_17.frq\"\nIFD [http-8080-1]: delete \"_17.nrm\"\nIFD [http-8080-1]: delete \"_17.fdt\"\nIFD [http-8080-1]: delete \"_17.fnm\"\nIFD [http-8080-1]: delete \"_17.tii\"\nIFD [http-8080-1]: delete \"_17.prx\"\nIFD [http-8080-1]: now checkpoint \"segments_1j\" [1 segments ; isCommit = false]\nIW 0 [http-8080-1]: LMP: findMerges: 1 segments\nIW 0 [http-8080-1]: LMP:   level 6.2247195 to 6.400742: 1 segments\nIW 0 [http-8080-1]: CMS: now merge\nIW 0 [http-8080-1]: CMS:   index: _17:c2765\nIW 0 [http-8080-1]: CMS:   no more merges pending; now return\nIW 0 [http-8080-1]: CMS: now merge\nIW 0 [http-8080-1]: CMS:   index: _17:c2765\nIW 0 [http-8080-1]: CMS:   no more merges pending; now return\nIW 0 [http-8080-1]: now call final commit()\nIW 0 [http-8080-1]: startCommit(): start sizeInBytes=0\nIW 0 [http-8080-1]: startCommit index=_17:c2765 changeCount=5\nIW 0 [http-8080-1]: now sync _17.cfs\nIW 0 [http-8080-1]: done all syncs\nIW 0 [http-8080-1]: commit: pendingCommit != null\nIW 0 [http-8080-1]: commit: wrote segments file \"segments_1k\"\nIFD [http-8080-1]: now checkpoint \"segments_1k\" [1 segments ; isCommit = true]\nIFD [http-8080-1]: deleteCommits: now decRef commit \"segments_1j\"\nIFD [http-8080-1]: delete \"_16.cfs\"\nIFD [http-8080-1]: delete \"segments_1j\"\nIW 0 [http-8080-1]: commit: done\nIW 0 [http-8080-1]: at close: _17:c2765\nIFD [http-8080-1]: setInfoStream deletionPolicy=org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy@fb1ba7\nIW 1 [http-8080-1]: setInfoStream: dir=org.apache.lucene.store.SimpleFSDirectory@C:\\Documents and Settings\\rvarlikli\\workspace\\.metadata\\.plugins\\org.eclipse.wst.server.core\\tmp0\\wtpwebapps\\Clipbank3.5\\lucene autoCommit=false mergePolicy=org.apache.lucene.index.LogByteSizeMergePolicy@1d49559 mergeScheduler=org.apache.lucene.index.ConcurrentMergeScheduler@1990e2d ramBufferSizeMB=16.0 maxBufferedDocs=-1 maxBuffereDeleteTerms=-1 maxFieldLength=10000 index=\nIW 1 [http-8080-1]: now flush at close\nIW 1 [http-8080-1]:   flush: segment=_18 docStoreSegment=_18 docStoreOffset=0 flushDocs=true flushDeletes=true flushDocStores=true numDocs=2765 numBufDelTerms=0\nIW 1 [http-8080-1]:   index before flush \nIW 1 [http-8080-1]: DW: flush postings as segment _18 numDocs=2765\nIW 1 [http-8080-1]: DW: closeDocStore: 2 files to flush to segment _18 numDocs=2765\nIW 1 [http-8080-1]: DW:   oldRAMSize=9517056 newFlushedSize=4042307 docs/MB=717.242 new/old=42.474%\nIFD [http-8080-1]: now checkpoint \"segments_1k\" [1 segments ; isCommit = false]\nIFD [http-8080-1]: now checkpoint \"segments_1k\" [1 segments ; isCommit = false]\nIFD [http-8080-1]: delete \"_18.nrm\"\nIFD [http-8080-1]: delete \"_18.frq\"\nIFD [http-8080-1]: delete \"_18.fdx\"\nIFD [http-8080-1]: delete \"_18.tii\"\nIFD [http-8080-1]: delete \"_18.fdt\"\nIFD [http-8080-1]: delete \"_18.prx\"\nIFD [http-8080-1]: delete \"_18.fnm\"\nIFD [http-8080-1]: delete \"_18.tis\"\nIFD [http-8080-1]: now checkpoint \"segments_1k\" [1 segments ; isCommit = false]\nIW 1 [http-8080-1]: LMP: findMerges: 1 segments\nIW 1 [http-8080-1]: LMP:   level 6.2247195 to 6.6112633: 1 segments\nIW 1 [http-8080-1]: CMS: now merge\nIW 1 [http-8080-1]: CMS:   index: _18:c2765\nIW 1 [http-8080-1]: CMS:   no more merges pending; now return\nIW 1 [http-8080-1]: CMS: now merge\nIW 1 [http-8080-1]: CMS:   index: _18:c2765\nIW 1 [http-8080-1]: CMS:   no more merges pending; now return\nIW 1 [http-8080-1]: now call final commit()\nIW 1 [http-8080-1]: startCommit(): start sizeInBytes=0\nIW 1 [http-8080-1]: startCommit index=_18:c2765 changeCount=5\nIW 1 [http-8080-1]: now sync _18.cfs\nIW 1 [http-8080-1]: done all syncs\nIW 1 [http-8080-1]: commit: pendingCommit != null\nIW 1 [http-8080-1]: commit: wrote segments file \"segments_1l\"\nIFD [http-8080-1]: now checkpoint \"segments_1l\" [1 segments ; isCommit = true]\nIFD [http-8080-1]: deleteCommits: now decRef commit \"segments_1k\"\nIFD [http-8080-1]: delete \"segments_1k\"\nIFD [http-8080-1]: delete \"_17.cfs\"\nIW 1 [http-8080-1]: commit: done\nIW 1 [http-8080-1]: at close: _18:c2765\nIFD [http-8080-1]: setInfoStream deletionPolicy=org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy@7ceec1\nIW 2 [http-8080-1]: setInfoStream: dir=org.apache.lucene.store.SimpleFSDirectory@C:\\Documents and Settings\\rvarlikli\\workspace\\.metadata\\.plugins\\org.eclipse.wst.server.core\\tmp0\\wtpwebapps\\Clipbank3.5\\lucene autoCommit=false mergePolicy=org.apache.lucene.index.LogByteSizeMergePolicy@1edacc mergeScheduler=org.apache.lucene.index.ConcurrentMergeScheduler@1ae9ba8 ramBufferSizeMB=16.0 maxBufferedDocs=-1 maxBuffereDeleteTerms=-1 maxFieldLength=10000 index=\nIW 2 [http-8080-1]: now flush at close\nIW 2 [http-8080-1]:   flush: segment=_19 docStoreSegment=_19 docStoreOffset=0 flushDocs=true flushDeletes=true flushDocStores=true numDocs=2765 numBufDelTerms=0\nIW 2 [http-8080-1]:   index before flush \nIW 2 [http-8080-1]: DW: flush postings as segment _19 numDocs=2765\nIW 2 [http-8080-1]: DW: closeDocStore: 2 files to flush to segment _19 numDocs=2765\nIW 2 [http-8080-1]: DW:   oldRAMSize=11188224 newFlushedSize=5229106 docs/MB=554.457 new/old=46.738%\nIFD [http-8080-1]: now checkpoint \"segments_1l\" [1 segments ; isCommit = false]\nIFD [http-8080-1]: now checkpoint \"segments_1l\" [1 segments ; isCommit = false]\nIFD [http-8080-1]: delete \"_19.tis\"\nIFD [http-8080-1]: delete \"_19.prx\"\nIFD [http-8080-1]: delete \"_19.nrm\"\nIFD [http-8080-1]: delete \"_19.fnm\"\nIFD [http-8080-1]: delete \"_19.fdx\"\nIFD [http-8080-1]: delete \"_19.fdt\"\nIFD [http-8080-1]: delete \"_19.tii\"\nIFD [http-8080-1]: delete \"_19.frq\"\nIFD [http-8080-1]: now checkpoint \"segments_1l\" [1 segments ; isCommit = false]\nIW 2 [http-8080-1]: LMP: findMerges: 1 segments\nIW 2 [http-8080-1]: LMP:   level 6.2247195 to 6.722014: 1 segments\nIW 2 [http-8080-1]: CMS: now merge\nIW 2 [http-8080-1]: CMS:   index: _19:c2765\nIW 2 [http-8080-1]: CMS:   no more merges pending; now return\nIW 2 [http-8080-1]: CMS: now merge\nIW 2 [http-8080-1]: CMS:   index: _19:c2765\nIW 2 [http-8080-1]: CMS:   no more merges pending; now return\nIW 2 [http-8080-1]: now call final commit()\nIW 2 [http-8080-1]: startCommit(): start sizeInBytes=0\nIW 2 [http-8080-1]: startCommit index=_19:c2765 changeCount=5\nIW 2 [http-8080-1]: now sync _19.cfs\nIW 2 [http-8080-1]: done all syncs\nIW 2 [http-8080-1]: commit: pendingCommit != null\nIW 2 [http-8080-1]: commit: wrote segments file \"segments_1m\"\nIFD [http-8080-1]: now checkpoint \"segments_1m\" [1 segments ; isCommit = true]\nIFD [http-8080-1]: deleteCommits: now decRef commit \"segments_1l\"\nIFD [http-8080-1]: delete \"_18.cfs\"\nIFD [http-8080-1]: delete \"segments_1l\"\nIW 2 [http-8080-1]: commit: done\nIW 2 [http-8080-1]: at close: _19:c2765",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2010-01-04T14:23:41+0000",
            "content": "Is there a reader open on the index, when you run the above code (calling IndexWriter.deleteAll)? ",
            "author": "Michael McCandless",
            "id": "comment-12796187"
        },
        {
            "date": "2010-01-04T14:50:23+0000",
            "content": "No , \nWould it affect if it was open ? ",
            "author": "Ramazan VARLIKLI",
            "id": "comment-12796199"
        },
        {
            "date": "2010-01-04T17:01:53+0000",
            "content": "An open reader would prevent deletion of the index files... but from your log above, it looks like that's not happening.\n\nIt's curious because from the log I can see that _17.cfs and _18.cfs are being deleted.\n\nCan you run the oal.index.CheckIndex tool on your 3-segment index and post the output? ",
            "author": "Michael McCandless",
            "id": "comment-12796236"
        },
        {
            "date": "2010-01-04T17:26:36+0000",
            "content": "Now I am testing it with V3 , result is the same \n\nFor the first time I create the index files the output as follows \n\nSegments file=segments_1r numSegments=1 version=FORMAT_DIAGNOSTICS [Lucene 2.9]\n  1 of 1: name=_1e docCount=2765\n    compound=true\n    hasProx=true\n    numFiles=1\n    size (MB)=2.348\n    diagnostics = \n{os.version=5.1, os=Windows XP, lucene.version=3.0.0 883080 - 2009-11-22 15:43:58, source=flush, os.arch=x86, java.version=1.6.0_12, java.vendor=Sun Microsystems Inc.}\n    no deletions\n    test: open reader.........OK\n    test: fields..............OK [8 fields]\n    test: field norms.........OK [8 fields]\n    test: terms, freq, prox...OK [55843 terms; 505243 terms/docs pairs; 856135 tokens]\n    test: stored fields.......OK [2765 total field count; avg 1 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n\nNo problems were detected with this index.\n\n\nThe second time is \n\n\n\nOpening index @ C:\\Documents and Settings\\rvarlikli\\workspace\\.metadata\\.plugins\\org.eclipse.wst.server.core\\tmp0\\wtpwebapps\\Clipbank3.5\\lucene\n\nSegments file=segments_1s numSegments=1 version=FORMAT_DIAGNOSTICS [Lucene 2.9]\n  1 of 1: name=_1f docCount=2765\n    compound=true\n    hasProx=true\n    numFiles=1\n    size (MB)=3.821\n    diagnostics = \n{os.version=5.1, os=Windows XP, lucene.version=3.0.0 883080 - 2009-11-22 15:43:58, source=flush, os.arch=x86, java.version=1.6.0_12, java.vendor=Sun Microsystems Inc.}\n    no deletions\n    test: open reader.........OK\n    test: fields..............OK [8 fields]\n    test: field norms.........OK [8 fields]\n    test: terms, freq, prox...OK [55843 terms; 505243 terms/docs pairs; 1712270 tokens]\n    test: stored fields.......OK [2765 total field count; avg 1 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n\nNo problems were detected with this index.\n ",
            "author": "Ramazan VARLIKLI",
            "id": "comment-12796251"
        },
        {
            "date": "2010-01-04T18:06:04+0000",
            "content": "So there seems to be two problems:\n\n\n\tThe old _X.cfs files are not getting removed\n\n\n\n\n\tEach _X.cfs file is growing in size, even though you sent it exactly the same docs\n\n\n\nIs that right? ",
            "author": "Michael McCandless",
            "id": "comment-12796268"
        },
        {
            "date": "2010-01-04T18:45:56+0000",
            "content": "no ,\nThe old _X.cfs files are removed correctly \nbut _X.cfs file is growing in size . \n\nActually  I tried to remove all _X.cfs files with java io commands but it didn't work. Lucene is keeping everything in memory and adds new document to it . I just want to remind this problem happens as long as within one JVM instance . If you shutdown it , it will start from scratch ",
            "author": "Ramazan VARLIKLI",
            "id": "comment-12796286"
        },
        {
            "date": "2010-01-04T19:36:10+0000",
            "content": "OK so it's only the 2nd problem.\n\nFrom your CheckIndex output, the 2nd segment has precisely 2X the number of tokens than the first segment (and the same number of documents and same number of unique terms).\n\nCan you double check how you create the Document that you pass to Lucene?  Is it possible the field in the Document is just getting added twice?  Can you post the code that constructs the document? ",
            "author": "Michael McCandless",
            "id": "comment-12796312"
        },
        {
            "date": "2010-01-05T09:56:06+0000",
            "content": "Hi Michael ,\nWhat I am doing is reading all database tables from scratch and put into index . Basically algorithm is like this\n\nopen index \ndeleteall index \nselect * from our table (without conditions ,just everything )\nadd them to index . \n\nNumber of fields you see in checkindex reports is the number of rows in our table I guess , It remains the same all the time but number of the tokens increased in double . It gives me feeling that IndexWriter.deleteAll actually doesn't delete anything . Even if I remove the index files physically , it keeps them in memory . \nIs it How it supposed to work ? if so , what it is the command to remove all data ?\n\n\n\n ",
            "author": "Ramazan VARLIKLI",
            "id": "comment-12796601"
        },
        {
            "date": "2010-01-05T11:16:11+0000",
            "content": "Given that you're creating a new IW to index the 2nd batch, and that this IW indeed discards the segment from the last one (via deleteAll), I think it's very unlikely that IW is incorrectly retaining your past docs.\n\ndeleteAll is indeed supposed to discard everything and give you a new starting index.  It does exactly the same thing as opening a new IW with create=true, except, deleteAll can be done without opening a new IW.\n\nI think it's more likely that you're somehow accidentally creating docs with 2X the content.  Are you sharing Document or Field instances when you create your docs?  Can you post the actual code?\n\nOr, alternatively, could you make this problem happen with a simplified standalone test case?  Ie, a code fragment that creates docs with random content instead of pulling from your DB?  That would help us isolate where the extra 2X content is coming from... ",
            "author": "Michael McCandless",
            "id": "comment-12796624"
        },
        {
            "date": "2010-01-05T13:38:39+0000",
            "content": "I found the problem finally  with your help , was trying to write a test case for you to make this problem analyzable and seen what it is causing the problem . We are holding in a map in singleton class , we need to reset it each time we need to re create the index. so works fine Lucene is great\nI think I so focused on that There might be a problem with Lucene and couldn't see my mistake .Actually What it causes me to see a  problem in Lucene is that when I was profiling the application , I see number of Lucene Field class in increasing double each time I invoke index creator .I thought it would be a problem with Lucene .\n\nThank you for your help  ",
            "author": "Ramazan VARLIKLI",
            "id": "comment-12796658"
        },
        {
            "date": "2010-01-05T13:41:45+0000",
            "content": "So the problem isn't with Lucene but was in your application, right? ",
            "author": "Michael McCandless",
            "id": "comment-12796661"
        },
        {
            "date": "2010-01-05T13:50:25+0000",
            "content": "Yes, The problem is not with Lucene but our application ",
            "author": "Ramazan VARLIKLI",
            "id": "comment-12796663"
        },
        {
            "date": "2010-01-05T14:29:01+0000",
            "content": "OK thanks for bringing closure! ",
            "author": "Michael McCandless",
            "id": "comment-12796674"
        }
    ]
}