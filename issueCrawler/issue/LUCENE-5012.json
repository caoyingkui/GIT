{
    "id": "LUCENE-5012",
    "title": "Make graph-based TokenFilters easier",
    "details": {
        "components": [
            "modules/analysis"
        ],
        "fix_versions": [],
        "affect_versions": "None",
        "priority": "Major",
        "labels": "",
        "type": "Improvement",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "SynonymFilter has two limitations today:\n\n\n\tIt cannot create positions, so eg dns -> domain name service\n    creates blatantly wrong highlights (SOLR-3390, LUCENE-4499 and\n    others).\n\n\n\n\n\tIt cannot consume a graph, so e.g. if you try to apply synonyms\n    after Kuromoji tokenizer I'm not sure what will happen.\n\n\n\nI've thought about how to fix these issues but it's really quite\ndifficult with the current PosInc/PosLen graph representation, so I'd\nlike to explore an alternative approach.",
    "attachments": {
        "LUCENE-5012.patch": "https://issues.apache.org/jira/secure/attachment/12584065/LUCENE-5012.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2013-05-21T20:05:19+0000",
            "content": "Initial patch showing the approach.  This patch also includes\nwrite-once attr bindings (LUCENE-2450).  There are some big changes\nhere:\n\n\n\tAll the changes from LUCENE-2450: no global attribute bindings;\n    instead each stage owns/controls what the next stage can see.\n\n\n\n\n\tInstead of PosInc/LenAtt, there is ArcAttribute, that has from and\n    to node (ints).  Tokens are arcs, and are free to have arbitrary\n    to/from.\n\n\n\n\n\tAn \"adapter\" StageAnalyzer that takes the write-once Stage and\n    creates an analyzer, converting the ArcAtribute into PosIncAtt in\n    the end.  So, positions are only \"assigned\" in the final adapter\n    stage (StageToTokenStream).\n\n\n\n\n\tA SynonymFilterStage that fixes the above two issues (and is also\n    quite a bit simpler).\n\n\n\n\n\tA SplitOnDashStage that shows how a decompounder works.\n\n\n\n\n\tHoles are done with a new DeletedAttribute, i.e. the token still\n    runs through the entire chain, but it's marked as deleted so that\n    stages along the way know to ignore it.  E.g. this would make it\n    possible for a tokenizer to produce punctuation tokens that are\n    skipped for indexing but prevent a SynonymFilter from matching\n    \"over\" the punctuation.\n\n\n\nThere is some added tracking of nodes that are not \"done yet\",\nnecessary to allow incremental consumption of the graph by all\nstages.  It adds some hair to graph stages but I don't see how to\nsimplify it while keeping incrementality...\n\nOne nice side effect of this change is it's no longer possible to\ncreate a first token with position=-1, since the mapping of node id ->\nposition is done for you.\n\nAlso, the graph is intact throughout the chain, until the very end\nwhere it is \"cast\" to a sausage (what indexer requires), vs today\nwhere SynonynmFilter does its own sausagizing.\n\nWhile the patch is a just a prototype and there's still tons to do\n(long, long ways from committing, very much \"exploratory\"), I think\nit's far enough along that it shows the promise of both write-once\nattr bindings and an easier API for graph-based analysis components.\nTricky cases that don't work with TokenStream today, e.g. a\ndecompounder followed by a syn filter, do work in the patch. ",
            "author": "Michael McCandless",
            "id": "comment-13663318"
        },
        {
            "date": "2013-05-21T20:26:55+0000",
            "content": "Will this Jira include some test code that query parsers can use so that they can retrieve the graph for a stream containing multiple multi-term synonyms so that they can then individually sausage the term sequences as well as generate \"OR\" operators for string of sausages? ",
            "author": "Jack Krupansky",
            "id": "comment-13663340"
        },
        {
            "date": "2013-05-21T21:08:45+0000",
            "content": "Will this Jira include some test code that query parsers can use so that they can retrieve the graph for a stream containing multiple multi-term synonyms so that they can then individually sausage the term sequences as well as generate \"OR\" operators for string of sausages?\n\nI think so ... the test case (TestStages) sort of does that already: it turns the tokens into an automaton, to check that the automaton accepts the specified sequence of tokens (and ONLY those sequences of tokens).\n\nBut, I think ideally we'd have a WordAutomatonQuery, which could take the Automaton directly and match documents using that, instead of enumerating all phrases an OR'ing them (which would be equivalent but presumably slower...).  I would do WordAutomatonQuery on a separate issue ... ",
            "author": "Michael McCandless",
            "id": "comment-13663402"
        },
        {
            "date": "2013-05-21T21:22:13+0000",
            "content": "This looks very promising! I've been looking at a few TokenFilters recently and anything that would make working with graphs easier is very welcome! Maybe we should create a branch to make it easier to collaborate and to track incremental updates? ",
            "author": "Adrien Grand",
            "id": "comment-13663416"
        },
        {
            "date": "2013-05-21T21:38:19+0000",
            "content": "Good idea Adrien! I'll cut a branch and commit the patch ... ",
            "author": "Michael McCandless",
            "id": "comment-13663433"
        },
        {
            "date": "2013-05-21T21:39:23+0000",
            "content": "[lucene5012 commit] mikemccand\nhttp://svn.apache.org/viewvc?view=revision&revision=1484977\n\nLUCENE-5012: create branch ",
            "author": "Commit Tag Bot",
            "id": "comment-13663434"
        },
        {
            "date": "2013-05-21T21:47:14+0000",
            "content": "[lucene5012 commit] mikemccand\nhttp://svn.apache.org/viewvc?view=revision&revision=1484980\n\nLUCENE-5012: initial prototype ",
            "author": "Commit Tag Bot",
            "id": "comment-13663444"
        },
        {
            "date": "2013-05-21T21:47:53+0000",
            "content": "OK I committed the initial patch to this branch: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5012 ",
            "author": "Michael McCandless",
            "id": "comment-13663445"
        },
        {
            "date": "2013-05-21T22:08:51+0000",
            "content": "WordAutomatonQuery\n\nSounds quite promising.\n\nBack to the query parsers... So, they would present a term or quoted string - and eventually hopefully a sequence of terms if the query parser sees that there is only white space between them (an issue Robert filed long ago) - and invoke analysis. Then what? Sometimes a single term or a clean sausage string comes out and a TermQuery or simple BooleanQuery or PhraseQuery needs to be generated, but if synonym-like filtering has generated a graph, then the query parser would hand \"it\" directly to WordAutomatonQuery, if I understand correctly. Then the question becomes how to tell that a WordAutomatonQuery graph is needed - unless WordAutomatonQuery automatically detects the cases for TermQuery and BooleanQuery/PhraseQuery as internal optimizations. (Well, I don't expect that WordAutomatonQuery would know how to do BooleanQuery vs. PhraseQuery, unless it has a \"phrase\" flag.)\n\nIn short, it would be nice if this issue directly or at least partially produced enough logic for that Term vs. Boolean vs. Phrase vs. WordAutomaton Query generation. Either to actually generate the final query, or at least some example code that documents the design pattern that a query parser needs for consumption of a \"query phrase\" graph.\n\nIn other words, the query parsers should not simply do a \"next\" for the entire output of query term analysis. A new design pattern is needed.\n\nAlso, at index time, the output of analysis is consumed as a single sausage stream, using \"next\" and token position increment, but any multiple multi-word synonyms would traditionally get somewhat mangled. There may not be a clean solution for the current index term posting format, but at a minimum we should reconsider how the output of index-time term analysis is consumed and flag potential improvements for the future for posting of multiple multi-term phrases at the same token position.\n\nIn any case, thanks for moving the multiple multi-term synonym ball forward! ",
            "author": "Jack Krupansky",
            "id": "comment-13663471"
        },
        {
            "date": "2013-05-24T18:18:47+0000",
            "content": "Great! I was asking several people about it at Lucene/Solr Revolution 2013.\nJust don't forget, that if you set your default operator to AND, you should still use ORs for synonyms. I was trying to solve this problem partly in SOLR-4533. I'm not sure, how the combination of ORs and ANDs is done in an FSA if you are going to use WordAutomaton right away. ",
            "author": "Artem Lukanin",
            "id": "comment-13666519"
        },
        {
            "date": "2013-05-26T22:13:31+0000",
            "content": "[lucene5012 commit] mikemccand\nhttp://svn.apache.org/viewvc?view=revision&revision=1486483\n\nLUCENE-5012: add CharFilter, fix some bugs with SynFilter, add new InsertDeletedPunctuationStage ",
            "author": "Commit Tag Bot",
            "id": "comment-13667418"
        },
        {
            "date": "2013-05-26T22:13:53+0000",
            "content": "I committed some changes:\n\n\n\tGot charFilter working\n\n\n\n\n\tFixed a few bugs in SynFilterStage not clearing it's state on end\n    / reset\n\n\n\n\n\tCreated a new fun stage: InsertDeletedPunctuationStage.  This\n    stage detects when a tokenizer has skipped over punctuation chars\n    and inserts a deleted token representing the punctuation, e.g. to\n    prevent a synonym or phrase query from matching over the\n    punctuation.  I had previously thought we would need to modify\n    Tokenizers to do this but now I think maybe this Stage could do it\n    for any Tokenizer ...\n\n ",
            "author": "Michael McCandless",
            "id": "comment-13667419"
        },
        {
            "date": "2013-07-01T07:34:36+0000",
            "content": "I guess WordDelimiterFilter is a good candidate for transforming into a graph-based filter (see LUCENE-5051). ",
            "author": "Artem Lukanin",
            "id": "comment-13696626"
        },
        {
            "date": "2014-05-23T15:41:57+0000",
            "content": "Commit 1597118 from Michael McCandless in branch 'dev/branches/lucene5012'\n[ https://svn.apache.org/r1597118 ]\n\nLUCENE-5012: merge trunk, but some tests are failing ",
            "author": "ASF subversion and git services",
            "id": "comment-14007257"
        },
        {
            "date": "2014-05-25T14:10:09+0000",
            "content": "Commit 1597427 from Michael McCandless in branch 'dev/branches/lucene5012'\n[ https://svn.apache.org/r1597427 ]\n\nLUCENE-5012: get tests passing again ",
            "author": "ASF subversion and git services",
            "id": "comment-14008345"
        },
        {
            "date": "2015-08-06T14:29:08+0000",
            "content": "Commit 1694511 from Michael McCandless in branch 'dev/branches/lucene5012'\n[ https://svn.apache.org/r1694511 ]\n\nLUCENE-5012: don't separate interface from impl for attributes ",
            "author": "ASF subversion and git services",
            "id": "comment-14660091"
        },
        {
            "date": "2016-11-16T14:55:04+0000",
            "content": "Seems very promising. Is LUCENE-2450 a dependency on this issue?  There's no dependency JIRA issue link the first comment suggests it is. ",
            "author": "David Smiley",
            "id": "comment-15670617"
        },
        {
            "date": "2017-01-17T15:37:38+0000",
            "content": "Michael McCandless So I was looking into supporting incoming graphs in SynonymGraphFilter and found this when you mentioned it in LUCENE-7638.  What do you think the state of this patch is?  Would it be best to look into advancing this instead of just SynonymGraphFilter itself? ",
            "author": "Matt Weber",
            "id": "comment-15826257"
        },
        {
            "date": "2017-01-17T19:39:09+0000",
            "content": "I think we really should advance this: the synonym filter here already handles incoming graphs correctly, and the token stream API improvements here make it much easier to consume and create graph token streams.  I think this will only get more important with time, e.g. the new WordDelimiterGraphFilter now creates correct graphs, but if you want to run the current SynonymGraphFilter after it, it won't work.\n\nThat said, there is still a lot of work to make this committable.  I think we need to find a way to make the stage-based analysis components interchangeable with the current API to give us the freedom to gradually cutover the many tokenizers and tokenfilters Lucene has today. ",
            "author": "Michael McCandless",
            "id": "comment-15826675"
        },
        {
            "date": "2017-01-18T06:11:19+0000",
            "content": "Patch for current master.  \n\ntestSynAfterDecompoundStageAnalyzer and testSynStageAnalyzer randomly fail so will need to dig into this when I find some more time.\n\nMichael McCandless Can you take a look and make sure I didn't miss anything from the original?  The attached patch wasn't up to date since you were working from a branch.  Here is what I ran to get the latest:\n\n\nsvn diff --ignore-properties --old https://svn.apache.org/repos/asf/lucene/dev/trunk@1597052 --new https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5012@1694511 > LUCENE-5012.patch\n\n ",
            "author": "Matt Weber",
            "id": "comment-15827482"
        },
        {
            "date": "2017-01-18T11:58:58+0000",
            "content": "Wow, thanks for modernizing the patch Matt Weber; I'll push this to branch on my github account for easier iterating... ",
            "author": "Michael McCandless",
            "id": "comment-15827939"
        },
        {
            "date": "2017-01-18T23:02:29+0000",
            "content": "Matt Weber, I realized I had more private changes that I never pushed to that old branch, so I recovered them, fixed to apply to current master, and pushed here: https://github.com/mikemccand/lucene-solr/commits/graph_token_filters\n\nI also removed the controversial InsertDeletedPunctuationStage.\n\nSome tests are still failing ... I'll try to fix them.\n\nI think the ideas here are very promising.  The write-once attributes (LUCENE-2450, folded into this branch) is cleaner than what Lucene has today, and the ease of making new positions without having to re-number previous ones makes graph token streams much easier.\n\nI tried to add the equivalent of CharFilter here, by using a new TextAttribute that stages before tokenization can use to read from a Reader or a String, and remap; I like that this makes offset correction more local than what the correctOffset exposes today.  And it means char filtering is simply another stage, not a separate class.\n\nI also added int[] parts to OffsetAttribute; the idea here is to empower token filters (not just tokenizers) to properly correct offsets, so that e.g. WDGF could work \"correctly\", but I'm not sure it's worth the hassle: I haven't fully implemented it, and doing so is surprisingly tricky. ",
            "author": "Michael McCandless",
            "id": "comment-15828939"
        },
        {
            "date": "2017-01-19T15:52:21+0000",
            "content": "Thanks Michael McCandless there was a lot of additional changes!  I am going to start getting familiar with this and hopefully will be able to help move it forward as I get time. ",
            "author": "Matt Weber",
            "id": "comment-15830136"
        },
        {
            "date": "2017-11-28T18:39:37+0000",
            "content": "I just have a question. I am using the word delimiter graph set to both split and concat followed by a shingle filter set to bigrams. I am getting what appears to be incorrect results. Is that expected and is the aim of this issue to fix that? ",
            "author": "Ryan Pedela",
            "id": "comment-16269235"
        },
        {
            "date": "2017-12-17T13:20:16+0000",
            "content": "This issue should make it easier to fix the bug you're seeing, but we can also fix the bug (in ShingleFilter I'm guessing?) before doing this more ambitious change.\n\nIt sounds like ShingleFilter is not looking at PositionLengthAttribute? ",
            "author": "Michael McCandless",
            "id": "comment-16294139"
        }
    ]
}