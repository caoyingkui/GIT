{
    "id": "LUCENE-100",
    "title": "[PATCH] Add support for Chinese, Japanese, and Korean to the core build.",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Moved from todo.xml:\n\nChe Dong's CJKTokenizer for Chinese, Japanese, and Korean.\nhttp://nagoya.apache.org/eyebrowse/ReadMsg?listName=lucene-\ndev@jakarta.apache.org&msgId=330905 \n\nand his sigram patch to StandardTokenizer.jj\nhttp://nagoya.apache.org/eyebrowse/SearchList?listId=&listName=lucene-\ndev@jakarta.apache.org&searchText=sigram&defaultField=subject&Search=Search\n\nI know there was some discussion about keeping language variant analyzers out \nof the core a while back, but the sigram change to StandardTokenizer would make \nthe StandardAnalyzer usable for Asian languages. From what I understand about \nsearching in Asian languages the bigram approach used in CJKTokenizer will give \nbetter results. \n\nI'm not sure of the impact of this change on the QueryParser how/if either of \nthese approaches makes sense along with some of the query syntax. For instance \nif I had the string of Chinese characters ABCDEFG (notice the lack of spaces \nbetween words in this language) and the actual words are AB, CDE and FG how \nwould a Chinese user expect to enter a query that we would do in English as AB \n+CDE FG?\n\nI wish I spoke one of these languages so I would have a better understanding of \nthe search issues. Perhaps Che Dong can give us an idea of if/how a Chinese \nuser would expect to interact with the query syntax. I know I've been \nstruggling with that problem in my own app that needs to support Chinese and \nJapanese content.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2003-04-11T14:44:44+0000",
            "content": "for most without natural space based word segment languages(like Chinese \nJapanese Korean) I use bigram to and make query parser default boolean query \nwith 'and' relationship(It's an option in QueryParser of 1.3 release).\n\nfor example: \nC1C2C3C4 will segment to  C1C2 C2C3 C3C4 while indexing\nand query parsed to C1C2 +C2C3 +C3C4 while searching.\n\nRegards\n\nChe, Dong\nhttp://www.chedong.com/tech/ ",
            "author": "Che, Dong",
            "id": "comment-12321262"
        },
        {
            "date": "2003-04-11T22:55:46+0000",
            "content": "I'm using the CJKTokenizer in my app (so I'm using the bigram approach). I'm \nprobably getting some false hits since I default to OR queries but the most \nrelevant docs should still be on the top of my list.\n\nI'm still curious though, do you think users expect any support for the query \nsyntax functionality available in QueryParser? Does any of the query syntax \nmake sense for these languages given the tokenization strategy? ",
            "author": "Eric Isakson",
            "id": "comment-12321263"
        },
        {
            "date": "2003-09-29T13:57:48+0000",
            "content": "I just hit upon this bug purely by chance and not until a moment ago did I know\nabout lucene so that some or all of the following may not be relevant, for which\nI apologize to you in advance. To read my comment, you have to set the character\nencoding of your browser to UTF-8 because it inclues some Korean characters in\nUTF-8.\n\nKorean is NOT like Chinese and Japanese. (Modern) Korean texts do use spaces\nbetween words. However, the Korean orthographic standard is rather 'liberal' in\nallowing (the norm is to add spaces between nouns) multiple nouns to be put\ntogether without spaces between them when they are used to refer to a single\n'entity'/'concept'.  Therefore, Korean texts are full of 'megawords' a la German\ncompound words. For instance, in German, 'quantum mechanics' is\n'Quantenmechaniker'. In Korean, it's either '\u00ec\u0096\u2018\u00ec\u009e\u0090 \u00ec\u0097\u00ad\u00ed\u0095\u0099' (the norm: with a space:\nEnglish-like) or '\u00ec\u0096\u2018\u00ec\u009e\u0090\u00ec\u0097\u00ad\u00ed\u0095\u0099'(more widely used. German-like). \n\nThe following comment may be off-topic here.\nWhat's more relevant to Korean tokenizer (and Japanese tokenizer as well.\nbecause both languages are aggultinating languages. On the other hand, Chinese\nis an isolating language) is the ability to  split apart word stems from\nprefices/sufficies  that play a various gramatical roles (tense, honorific form,\nmode, and so forth)  and particles(denoting subject, object,etc). In many\napplications, gramatically-functional prefices/suffices/particles/words have to\nbe excluded from indexing because they are not 'content-bearing'. Basis\nTechnology's Korean analyzer (www.basistech.com) is quite good (not perfect) at\nthis. \n ",
            "author": "Jungshik Shin",
            "id": "comment-12321264"
        },
        {
            "date": "2003-12-24T02:53:09+0000",
            "content": "I think this is really a duplicate of the patch in bug 23545 entry, which was\njust taken care of. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12321265"
        }
    ]
}