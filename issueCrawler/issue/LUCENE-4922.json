{
    "id": "LUCENE-4922",
    "title": "A SpatialPrefixTree based on the Hilbert Curve and variable grid sizes",
    "details": {
        "components": [
            "modules/spatial"
        ],
        "fix_versions": [],
        "affect_versions": "None",
        "priority": "Major",
        "labels": "",
        "type": "New Feature",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "My wish-list for an ideal SpatialPrefixTree has these properties:\n\n\tHilbert Curve ordering\n\tVariable grid size per level (ex: 256 at the top, 64 at the bottom, 16 for all in-between)\n\tCompact binary encoding (so-called \"Morton number\")\n\tWorks for geodetic (i.e. lat & lon) and non-geodetic\n\n\n\nSome bonus wishes for use in geospatial:\n\n\tUse an equal-area projection such that each cell has an equal area to all others at the same level.\n\tWhen advancing a grid level, if a cell's width is less than half its height. then divide it as 4 vertically stacked instead of 2 by 2. The point is to avoid super-skinny cells which occurs towards the poles and degrades performance.\n\n\n\nAll of this requires some basic performance benchmarks to measure the effects of these characteristics.",
    "attachments": {
        "LUCENE-4922.patch": "https://issues.apache.org/jira/secure/attachment/12663324/LUCENE-4922.patch",
        "HilbertConverter.zip": "https://issues.apache.org/jira/secure/attachment/12582871/HilbertConverter.zip"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2013-04-12T05:27:40+0000",
            "content": "Hi David,\n\nI believe this would require to implement a class that extends the SpatialPrefixTree class. It should support all porperties mentioned. We get it to perform Hilbert space filling curve based spatial data structure traversal in making range queries etc.\nApart from performance benchmarks to measure effects of the new feature, are the bonus wishes for use in geospatial to be considered as optimizations given availability of time?\n\nThanks,\nHatim. ",
            "author": "Hatim Hakeel",
            "id": "comment-13629809"
        },
        {
            "date": "2013-04-12T13:20:48+0000",
            "content": "Thanks for your interest Hatim!\n\nYour assessment of just needing to extend SpatialPrefixTree is mostly correct.  It also involves implementing Cell (formerly known as Node) which is tightly related.  There are lots of potential optimizations here.  As part of this task, modifying SpatialPrefixTree & Cell itself is absolutely okay if we find better (more optimized) ways of doing things, though we need to retain backwards compatibility with the two existing SpatialPrefixTree implementations \u2013 geohash & quad.  If retaining that backwards compatibility turns out to be too constraining on making this SPT be even better, then we could make SPT an interface.  But the general API of SPT & Cell should be the same as it is now.  \n\nWhat will help in this task is to know that there are extensive tests of the PrefixTree based spatial search strategies, such that if supplying a new SPT causes a failure, then you know you've got a bug .  But there are few unit level tests of the SPT itself, and so knowing precisely what is broken could be hard.\n\nYou are correct that the bonus wishes are purely optional optimizations, time permitting.  Very optional; in general I want to see that without these optimizations that the SPT is as good as it can reasonably be.  It may turn out that these geo specific optimizations have extra runtime costs (such as calculating the sin() a lot or complexity that make it not worthwhile; I don't know. ",
            "author": "David Smiley",
            "id": "comment-13630054"
        },
        {
            "date": "2013-04-23T07:04:25+0000",
            "content": "Hi David,\n\nThanks for the feedback. I have drafted a proposal based on your comments and stuff I figured out by digging into the background.\n\nhttps://google-melange.appspot.com/gsoc/proposal/review/google/gsoc2013/hatimhakeel/1\n\nHope it satisfies the requirements of the project. \n\nThanks,\nHatim. ",
            "author": "Hatim Hakeel",
            "id": "comment-13638856"
        },
        {
            "date": "2013-04-23T15:25:36+0000",
            "content": "Thanks for the proposal Hatim!  That's a good proposal.  I enjoyed reading some of the research papers you referenced.  I'm excited to think that at the end of the summer, there's going to be an awesome optimized PrefixTree; and I'm a tad jealous that a lucky student is going to build it instead of me  \n\np.s. I independently developed my own outline of tasks to do, which I'll share with the student that is accepted (which is likely you but theoretically could be another student if there is interest from others) ",
            "author": "David Smiley",
            "id": "comment-13639152"
        },
        {
            "date": "2013-04-23T17:03:42+0000",
            "content": "Hi David,\n\nThanks for the compliments . I'm eagerly looking forward to the announcement of accepted proposals with fingers crossed.\n ",
            "author": "Hatim Hakeel",
            "id": "comment-13639242"
        },
        {
            "date": "2013-05-09T06:59:01+0000",
            "content": "In case you find it useful, this python function converts from an x,y coordinate (vals[0],vals[1]) on a grid from mins and maxs values to a point on the Hilbert curve in base4\n\n\ndef hilbert_point(vals,depth,mins=[],maxs=[]):\n    \"\"\"\n    cell_names\n\n    3 ---- 2\n          |    \n    0 ---- 1\n\n\n    second generation\n\n    33    30 -- 23 -- 22\n    |     |           |\n    32 -- 31    20 -- 21\n                |\n    01 -- 02    13 -- 12\n    |     |           |\n    00    03 -- 10 -- 11\n\n\n    \"\"\"\n    if(mins):\n        # then convert vals to some place in 1-by-1 grid at the origin\n        vals[0] = (vals[0]-mins[0])/(maxs[0]-mins[0])\n        vals[1] = (vals[1]-mins[1])/(maxs[1]-mins[1])\n    \n    if(depth == 0):\n        return ''\n\n    if( vals[0]<=0.5 and vals[1]<=0.5 ):\n        cell_name = '0'\n        x = vals[1]*2\n        y = vals[0]*2\n    elif( vals[0]>=0.5 and vals[1]<=0.5 ):\n        cell_name = '1'\n        x = (vals[0] - 0.5)*2\n        y = vals[1]*2\n    elif( vals[0]>=0.5 and vals[1]>=0.5 ):\n        cell_name = '2'\n        x = (vals[0] - 0.5)*2\n        y = (vals[1] - 0.5)*2\n    elif( vals[0]<=0.5 and vals[1]>=0.5 ):\n        cell_name = '3'\n        x = 1 - (vals[1] - 0.5)*2\n        y = 1 - vals[0]*2\n\n    return cell_name + hilbert_point([x,y],depth-1)\n\n\n\nTry hilbert_point([0.125435,0.854320], 8, [0,0], [1,1]). It should be 33133010. ",
            "author": "John Berryman",
            "id": "comment-13652785"
        },
        {
            "date": "2013-05-09T13:07:09+0000",
            "content": "Cool John.  I suggest that any future code-samples be attached to the issue.\n\nBTW another thing about the implementation of this grid I want to explore is the efficacy of working with integers and fixed precision (configured), versus floating point math. ",
            "author": "David Smiley",
            "id": "comment-13652934"
        },
        {
            "date": "2013-05-09T19:45:02+0000",
            "content": "Hmmm... integer representation huh. Well here's a thought then:\n\nAs a first got at this idea, let's define something like a geohash where x are interleaved, but here's how we do it. At the top level, number squares from 0 to 3.\n\n\n   0 --- 1\n   |     |\n   2 --- 3\n\n\n\nAt the next level, number things similarly, \n\n\n    00 -- 01 -- 10 -- 11\n    |     |     |     |\n    02 -- 03 -- 12 -- 13\n    |     |     |     |\n    20 -- 21 -- 30 -- 31\n    |     |     |     |\n    22 -- 23 -- 32 -- 33\n\n\n\nEven though this looks like the hilbert thing I did above, notice that this is actually the Z-ordering which is a little easier to compute.\n\nIn this case, the first two bits encodes which of the four big boxes the point is in, the next two bits encodes which of the four sub boxes the point is in, etc. So for example [0.375, 0.625] would be encoded to a depth of 2 by \"03\" which can be stored in half a byte.\n\nGot it? So... now since we have the original point encoded in z-ordering, we can create a new hilbert_point algorithm that takes a byte array representing the z-ordering encoding of a point rather than a 2-vector of doubles. And the code looks much the same except that instead of the \"val[0]*2\" etc. we're actually just iterating through the byte array 2 bits at a time which is effectively the same as multiplying by 2.\n\nThis would make for some exquisitely indecipherable byte-munging code. But would it ultimately be more efficient? It largely depends upon how complex the Z-ordering encoding is. What do you think? ",
            "author": "John Berryman",
            "id": "comment-13653095"
        },
        {
            "date": "2013-05-12T22:55:14+0000",
            "content": "This is a Java example of converting from a set of x,y values (within specified bounds) to a HilbertOrdered value.\n\nThe strategy is to\n\n\n\tConvert the x,y value so doubles between 0 and 1.\n\tConvert these doubles to large integers with max value 256^numBytes (user specifies numBytes). Note: there's probably a way to do these two last steps simultaneously and regain some precision. Note: numBytes must be <= 7 for now.\n\tInterleave the bits so that you get a z-order value (note, I did this the \"obvious way\". In the code I've pointed to a website with a much more efficient method - so called morten numbers.\n\tConvert the z-ordered value to a hilbert-ordered value.\n\n\n\nHow to do this last step really deserves a big whiteboard session - it's an inherently visual discussion. However, as a clue to what's happening in the code:\n\n\n\tThere are only 4 shapes that compose the Hilbert curve. In the code I've called them \"D,U,n, and C\" because of the way they look. These are the states of a state machine.\n\tI convert from z to hilbert 2 bits at a time.\n\tOn the first iteration I assume that I'm in the \"D\" state. In this simplistic case, I convert from 2 z-ordered bits to 2 hilbert-ordered bits based upon a lookup table that goes with the D state. I replace the z-ordered bits with the hilbert-ordered bits.\n\tI then check for which state I should go to next based upon a different lookup table that goes with the D state. It directs me to another state.\n\tI then get the next 2 bits from the byte array and repeat this method until I'm out of bits.\n\n\n\nI've spot checked the input/output and it looks good (you'll see where I've done this in the code). No tests!\n\nAlso. This method could be slower than expected because I'm doing all operations on 2 bits at a time. As is, the method in the python code might even be faster because (correct me if I'm wrong) a double multiply can take place in one clock cycle.\n\nThat said, the methods here can be extended to operate at the processor word size.\n\nWhat's more, I'm working with lookup tables. I suspect that you could use magic numbers and bitmasks and all that jazz and make something that took up very little space or time.\n\nAlso, I couldn't find them now, but there exist known efficient algorithms for doing this conversion. (I guess this weekend I just felt like making my own.:-/) I've run across algorithms that are even for higher dimension Hilbert Curves ",
            "author": "John Berryman",
            "id": "comment-13655667"
        },
        {
            "date": "2013-05-12T22:56:01+0000",
            "content": "I just attached code called \"HilbertConverter\". This is a Java example of converting from a set of x,y values (within specified bounds) to a HilbertOrdered value.\n\nThe strategy is to\n\n\n\tConvert the x,y value so doubles between 0 and 1.\n\tConvert these doubles to large integers with max value 256^numBytes (user specifies numBytes). Note: there's probably a way to do these two last steps simultaneously and regain some precision. Note: numBytes must be <= 7 for now.\n\tInterleave the bits so that you get a z-order value (note, I did this the \"obvious way\". In the code I've pointed to a website with a much more efficient method - so called morten numbers.\n\tConvert the z-ordered value to a hilbert-ordered value.\n\n\n\nHow to do this last step really deserves a big whiteboard session - it's an inherently visual discussion. However, as a clue to what's happening in the code:\n\n\n\tThere are only 4 shapes that compose the Hilbert curve. In the code I've called them \"D,U,n, and C\" because of the way they look. These are the states of a state machine.\n\tI convert from z to hilbert 2 bits at a time.\n\tOn the first iteration I assume that I'm in the \"D\" state. In this simplistic case, I convert from 2 z-ordered bits to 2 hilbert-ordered bits based upon a lookup table that goes with the D state. I replace the z-ordered bits with the hilbert-ordered bits.\n\tI then check for which state I should go to next based upon a different lookup table that goes with the D state. It directs me to another state.\n\tI then get the next 2 bits from the byte array and repeat this method until I'm out of bits.\n\n\n\nI've spot checked the input/output and it looks good (you'll see where I've done this in the code). No tests!\n\nAlso. This method could be slower than expected because I'm doing all operations on 2 bits at a time. As is, the method in the python code might even be faster because (correct me if I'm wrong) a double multiply can take place in one clock cycle.\n\nThat said, the methods here can be extended to operate at the processor word size.\n\nWhat's more, I'm working with lookup tables. I suspect that you could use magic numbers and bitmasks and all that jazz and make something that took up very little space or time.\n\nAlso, I couldn't find them now, but there exist known efficient algorithms for doing this conversion. (I guess this weekend I just felt like making my own.:-/) I've run across algorithms that are even for higher dimension Hilbert Curves ",
            "author": "John Berryman",
            "id": "comment-13655668"
        },
        {
            "date": "2013-05-13T13:08:43+0000",
            "content": "Thanks for your interest in this John. You've gotten the ball rolling. I confess I don't do any byte or bit manipulation work so it'll take some time to fully digest what's going on in this code which uses a lot of it.  More comments would have helped. It might be interesting to look at how Lucene Trie floating point fields work.  Thanks for the reference to efficient ways to interleave bits; I'll re-post here for convenient reference: http://www-graphics.stanford.edu/~seander/bithacks.html#InterleaveBMN\n (there are 2 approaches there).  I'm not sure when I'll have significant time to contribute more directly other than add to the conversation. ",
            "author": "David Smiley",
            "id": "comment-13655944"
        },
        {
            "date": "2013-05-14T19:47:33+0000",
            "content": "That's fine. Pull me into the conversations you have with Hatim and I can probably help things move along faster. ",
            "author": "John Berryman",
            "id": "comment-13657418"
        },
        {
            "date": "2014-03-03T07:00:40+0000",
            "content": "I am Varun and I will be really interested to pursue this as my GSOC2014 project. Any documentation, references, papers etc on this topic will be extremely helpful. ",
            "author": "Varun  V Shenoy",
            "id": "comment-13917781"
        },
        {
            "date": "2014-03-03T16:54:30+0000",
            "content": "Hi,\nI have provided a link to the proposal I prepared last year for the benefit of anyone looking for related background material like documentation, papers etc. Hope it would be helpful.\n\nhttp://hatimhakeel.blogspot.com/2014/03/this-is-my-gsoc-2013-project-proposal.html ",
            "author": "Hatim Hakeel",
            "id": "comment-13918257"
        },
        {
            "date": "2014-03-09T20:37:58+0000",
            "content": "Thanks Hatim, I have been going through your proposal, and it helped me a lot. I am getting really excited. ",
            "author": "Varun  V Shenoy",
            "id": "comment-13925320"
        },
        {
            "date": "2014-04-22T08:13:48+0000",
            "content": "Hi,\nI will be tackling this issue as an accepted GSOC student and below is my branch on GitHub.\nhttps://github.com/shenoyvvarun/lucene-solr/tree/4992 ",
            "author": "Varun  V Shenoy",
            "id": "comment-13976526"
        },
        {
            "date": "2014-08-21T02:22:11+0000",
            "content": "As my GSOC project, I have implemented a spatial prefix tree with the above mentioned features. This tree, named \"FlexPrefixTree\" has the following features.\n\n\tFPT performs 25% faster for circular queries and 36% faster for rectangular queries compared to QuadPrefixTree.\n\tThe implementation should be easy to adapt to a future integer based spatial application when floating point isn\u2019t desired.\n\tVery compact index sizes are possible. Upto 14% lessar than GeoHashPrefixTree and 60% lessar than QuadPrefixTree implementations.\n\tFlexible trade-offs between index size and search speed. Variable number of sub-cells allows us to achieve very small index sizes.\n\tFPT internally uses int, to speed up spatial calculations.\n\tDue to reuse of a FlexCell per level, BytesRef per FPT, Rectangle shape per level, FlexPrefixTreeIterator per level and compact index sizes, we could achieve upto 48% reduction in average used memory and 38% in average total used memory.\n\n ",
            "author": "Varun  V Shenoy",
            "id": "comment-14104962"
        },
        {
            "date": "2014-08-22T04:00:08+0000",
            "content": "This is awesome Varun  V Shenoy!  It's been a pleasure working with you over the summer.  I'll add some more context to those reading along... \n\nWhat we have here is a new SPT impl that aims to be fast, and flexibly trade-off index size with search speed similar to how Lucene's single-dimensional Trie fields let you pick a precisionStep.  We call it \"flex\" for short; the class is FlexPrefixTree2D.  But it's more flexible than simply picking the equivalent of a precisionStep as it can be configured to vary at each level (step).  As I suspected,  we concluded the fastest results were when the highest and lowest levels had many sub-cells, but few in the middle.  At its best, targeting a similar precision & index size as geohash, I found Flex to do point-radius queries that were twice as fast on my machine. Your mileage will certainly vary; Varun's numbers above are on the conservative end.  Note that currently the sub-cells must be a power of 4 and limited to 4, 16, or 64 since it uses a single byte per level, and some bits are reserved for something called a leaf.  Adding support for 256 should be easy, so long as you only index points since points have no leaf bits.\n\nAnother interesting thing about Flex is that it internally uses a square world model.  This means each cell is square, which is more desirable for heat-map display usage than rectangles produced from Geohash (and Quad when given rectangular world bounds).  This also means the top level is only half-used but it's of marginal consequence.\n\nFlex internally has it's spatial coordinates expressed in integers on a power of 2 grid, which eliminates floating point division in lieu of bit shifting.  Since on the surface the shapes in & out are floating point, there is a conversion step, but it's optimized away in the case of a rectangle, hence the performance boost when doing rectangle queries.  Use of integers vs longs hinders maximum precision but if it becomes a problem (I doubt it), it could be easily changed.  Using an integer space also better lends itself to spatial applications that are not floating point (like using points to represent time durations).  More work is needed to fully realize that.\n\nAs followers here may infer by it's absence, there is no Hilbert Curve ordering in Flex yet.  It was lower on the priority list because I suspect whatever performance boost comes of it will be substantially less than the variable grid size support. ",
            "author": "David Smiley",
            "id": "comment-14106429"
        }
    ]
}