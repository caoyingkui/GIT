{
    "id": "LUCENE-8389",
    "title": "Could not limit Lucene's memory consumption",
    "details": {
        "components": [
            "core/index"
        ],
        "status": "Closed",
        "resolution": "Won't Fix",
        "fix_versions": [],
        "affect_versions": "3.3",
        "labels": "",
        "priority": "Major",
        "type": "Bug"
    },
    "description": "We are running Jira 7.6.1 with Lucene 3.3 on SLES 12 SP1\n\nWe configured 16GB Jira heap on 64GB server\n\nHowever, each time, when we run background re-index, the memory will be used out by Lucene and we could not only limit its memory consumption.\n\nThis definitely will cause overall performance issue on a system with heavy load.\n\nWe have around 500 concurrent users, 400K issues.\n\nCould you please help to advice if there were workaround\u00a0 or fix for this?\n\nThanks.\n\n\u00a0\n\nBTW: I did check a lot and found a blog introducing the new behavior of Lucene 3.3\n\nhttp://blog.thetaphi.de/2012/07/use-lucenes-mmapdirectory-on-64bit.html",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16534619",
            "author": "changchun huang",
            "content": "As we can not just limit Lucene's memory consuption besides JAVA Thread with configuration or cgroup. This is need help ",
            "date": "2018-07-06T09:27:50+0000"
        },
        {
            "id": "comment-16534836",
            "author": "Uwe Schindler",
            "content": "You are not fully precise what memory is used out, so your question is unclear.\n\nIf you give 16 GiB of heap, JIRA and Lucene it will of course use it out - thats fine. This has nothing to do with MMapDirectory (see blog post), because the filesystem cache is outside of heap, so Lucene uses lots of off-heap space, but that's just virtual and has nothing to do with allocated heap space. If you run TOP on your Linux installation you will see a column \"RES\", which should be around the heap size plus a bit or extra memory (around 20 GiB) heap. In addition the column \"VIRT\" of top is showing the reserved address space, which will be RES plus the size of all open indexes. Depending on index size this can be up to several hundreds of gigabytes.\n\nThe general rule is: Keep a MINIMUM of 50% of physical RAM free to allow file system caching. So if you use 16 GiB heap space, you should have at least 32 GiB, better 48 GiB of physical RAM in the machine (depending on index size).\n\nSo high VM usage in VIRT is wanted and fine. If JIRA is taking lots of heap space and really needs 16 GiB this is not our problem, so ask JIRA for help. I will close this bug report as \"Not a bug\", as it's not our issue. \n\nThere are rumors that JIRA will update the Lucene support to a later version, maybe this will help. Lucene 3.3 is out of maintenance since 6 years. ",
            "date": "2018-07-06T13:51:19+0000"
        },
        {
            "id": "comment-16536456",
            "author": "changchun huang",
            "content": "Thanks for quickly reply.\n\nDefinitely I am not talking about the JAVA Heap.\n\nWhen\u00a0we were\u00a0triggering background re-index from Jira, we can see during the re-indexing, the physical memory was reserved, I think\u00a0it\u00a0should be caused\u00a0by the\u00a0Lucene.\u00a0 we have 16 Heap, 64 Physical Memory allocated to the server. we could see the all Physical memory got reserved during the re-indexing(Jira background re-index, single thread).\n\nThe problem is, we could not even set memory limit only for Lucene as the typical situation is, Lucence\u00a0is not a standalone application, and it is\u00a0embedded as JAVA application, so in a heavy load JAVA Application\u00a0server which really care about performance and downtime, re-index with only 1 singe thread still reserves all free physical memory left, and this has conflicts with JAVA application even we configure the same Xms and Xmx.\n\nSo, I am asking a help like workaround, suggestion .\u00a0We have\u00a0JAVA 1.8 with G1GC, there\u00a0is no OOME, but\u00a0during re-index, the chance of (GC pause (G1 Evacuation Pause) (young) (to-space exhausted) increased a lot. During that time, we were having performance issue. ",
            "date": "2018-07-08T23:45:09+0000"
        },
        {
            "id": "comment-16536543",
            "author": "Uwe Schindler",
            "content": "Hi,\nthis is not an Lucene issue. Please ask Atlassian Support to help you.\n\nThis is a bug tracker not a support forum, so unless there is a bug in an up-to-date Lucene version, please do not reopen this bug report. ",
            "date": "2018-07-09T05:21:39+0000"
        }
    ]
}