{
    "id": "LUCENE-4216",
    "title": "Token X exceeds length of provided text sized X",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/highlighter"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "4.0-ALPHA",
        "resolution": "Not A Problem",
        "status": "Resolved"
    },
    "description": "I'm facing this exception:\norg.apache.lucene.search.highlight.InvalidTokenOffsetsException: Token \u0631\u0623\u064a\u0643\u0645 exceeds length of provided text sized 170\n\tat org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:233)\n\tat classes.myApp$16$1.run(myApp.java:1508)\n\n\nI tried to find anything wrong in my code when i start migrating Lucene 3.6 to 4.0 without successful. i found similar issues with HTMLStripCharFilter e.g. LUCENE-3690, LUCENE-2208 but not with SimpleHTMLFormatter so I'm triggering this here to see if there is really a bug or it is something wrong in my code with v4. The code that im using:\n\nfinal Highlighter highlighter = new Highlighter(new SimpleHTMLFormatter(\"<font color=red>\", \"</font>\"), new QueryScorer(query));\n.......\nfinal TokenStream tokenStream = TokenSources.getAnyTokenStream(defaultSearcher.getIndexReader(), j, \"Line\", analyzer);\nfinal TextFragment[] frag = highlighter.getBestTextFragments(tokenStream, doc.get(\"Line\"), false, 10);\n\n\nPlease note that this is working fine with v3.6",
    "attachments": {
        "ArabicTokenizer.java": "https://issues.apache.org/jira/secure/attachment/12539248/ArabicTokenizer.java",
        "myApp.zip": "https://issues.apache.org/jira/secure/attachment/12539148/myApp.zip",
        "ArabicAnalyzer.java": "https://issues.apache.org/jira/secure/attachment/12539422/ArabicAnalyzer.java"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-08-03T13:22:27+0000",
            "content": "Hello, can you give a little more information, such as what Analyzer you use, and maybe an example document or something to try to reproduce the problem?  ",
            "author": "Robert Muir",
            "id": "comment-13428072"
        },
        {
            "date": "2012-08-04T10:05:33+0000",
            "content": "Please find the attached Test case ",
            "author": "Ibrahim",
            "id": "comment-13428595"
        },
        {
            "date": "2012-08-04T10:35:50+0000",
            "content": "The bugs are in your custom tokenizer. I would recommend looking at lucene-test-framework.jar (especially BaseTokenStreamTestCase) and writing some tests for it.\n\nProblems I see at a glance:\n\n\tit doesn't implement reset(), so its not safe at all. This is the main reason it doesn't work for you in 4.0, because Analysis reuse is mandatory and it doesn't reset its state.\n\tit doesn't implement end(), so multi-valued fields wont work\n\tit doesn't call correctOffset(), so charfilters won't work\n\tit removes tashkeel in the tokenizer itself without adjusting offsets, thats unsafe.\n\n\n\nReally you can fix this easily, by:\n1. instead of extending Tokenizer, extend CharTokenizer and implement isTokenChar via isArabicChar. Or just use StandardTokenizer, it tokenizes arabic just fine.\n2. instead of removing tashkeel in your tokenizer itself with your pattern ([\\u0650\\u064D\\u064E\\u064B\\u064F\\u064C\\u0652\\u0651]), just pass that pattern to PatternReplaceFilter. ",
            "author": "Robert Muir",
            "id": "comment-13428596"
        },
        {
            "date": "2012-08-05T07:10:00+0000",
            "content": "Appreciated. It starts working after implementing reset(), end() and use of correctOffset(). For Tashkeel, we should not adjust the offset since it is part of the word but not necessary to be written when searching/indexing. it is the way how Arabic is written.\nI have also another Tokenizer dealing with Arabic by considering the roots where there is a index of the Arabic roots (>600,000). I might suggest it later to be in the contrib if you allow the big size of the roots index (16 MB)\n\nThanks again ",
            "author": "Ibrahim",
            "id": "comment-13428819"
        },
        {
            "date": "2012-08-05T13:02:07+0000",
            "content": "\nFor Tashkeel, we should not adjust the offset since it is part of the word but not necessary to be written when searching/indexing. it is the way how Arabic is written.\n\nIt has nothing to do with arabic. offsets will be wrong for the rest of your document if you dont fix this. ",
            "author": "Robert Muir",
            "id": "comment-13428844"
        },
        {
            "date": "2012-08-06T08:19:38+0000",
            "content": "I have decreased the offset by the difference in length before and after Tashkeel,\nOn the other, I really do not know what it means. I have tested it in both cases with multi-value field (since offset is affecting end()) but found it is working. ",
            "author": "Ibrahim",
            "id": "comment-13429021"
        },
        {
            "date": "2012-08-06T08:30:17+0000",
            "content": "Hi,\n\n\n/** A tokenizer that will return tokens in the arabic alphabet. This tokenizer\n * is a bit rude since it also filters digits and punctuation, even in an arabic\n * part of stream. Well... I've planned to write a\n * \"universal\", highly configurable, character tokenizer.\n * @author Pierrick Brihaye, 2003\n */\n\n\n\nYou don't need to implement your own ArabicTokenizer, just subclass the abstract Lucene class CharTokenizer which has all the functionality this comment in your source code offers. The change is easy: Subclass directly and remove all code exept isArabicChar and rename this method to isTokenChar (it takes int not char, but thats just a cast). The Tashkel stuff should be done with PatternReplaceFilter wrapped on top of this Tokenizer, there is no need to have this in the Tokenizer itsself and makes code complex. Then you can 100% be sure that all offsets are correct, the code you use is a du\u00fcplicate and it is too risky to reinvent the wheel if a well-tested variant is available with the Lucene distribution. It is much easier, trust me, no need to implement any crazy reset,... methods! ",
            "author": "Uwe Schindler",
            "id": "comment-13429024"
        },
        {
            "date": "2012-08-06T08:31:53+0000",
            "content": "It is also much more performant, as your code creates regex mathcers all the time and copies the token chars to new Strings all the time instead of working directly on the CharTermAttribute (which extends CharSequence, so can do regexes directly). ",
            "author": "Uwe Schindler",
            "id": "comment-13429025"
        },
        {
            "date": "2012-08-07T05:48:46+0000",
            "content": "greatly appreciated. it worked out without the low level implementation for incrementToken(). ",
            "author": "Ibrahim",
            "id": "comment-13429947"
        }
    ]
}