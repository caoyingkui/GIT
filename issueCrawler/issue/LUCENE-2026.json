{
    "id": "LUCENE-2026",
    "title": "Refactoring of IndexWriter",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "I've been thinking for a while about refactoring the IndexWriter into\ntwo main components.\n\nOne could be called a SegmentWriter and as the\nname says its job would be to write one particular index segment. The\ndefault one just as today will provide methods to add documents and\nflushes when its buffer is full.\nOther SegmentWriter implementations would do things like e.g. appending or\ncopying external segments [what addIndexes*() currently does].\n\nThe second component's job would it be to manage writing the segments\nfile and merging/deleting segments. It would know about\nDeletionPolicy, MergePolicy and MergeScheduler. Ideally it would\nprovide hooks that allow users to manage external data structures and\nkeep them in sync with Lucene's data during segment merges.\n\nAPI wise there are things we have to figure out, such as where the\nupdateDocument() method would fit in, because its deletion part\naffects all segments, whereas the new document is only being added to\nthe new segment.\n\nOf course these should be lower level APIs for things like parallel\nindexing and related use cases. That's why we should still provide\neasy to use APIs like today for people who don't need to care about\nper-segment ops during indexing. So the current IndexWriter could\nprobably keeps most of its APIs and delegate to the new classes.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2009-11-04T01:27:16+0000",
            "content": "+1 ",
            "author": "John Wang",
            "id": "comment-12773329"
        },
        {
            "date": "2009-11-04T09:11:15+0000",
            "content": "+1!  IndexWriter has become immense.\n\nI think we should also pull out ReaderPool? ",
            "author": "Michael McCandless",
            "id": "comment-12773429"
        },
        {
            "date": "2009-11-04T09:38:43+0000",
            "content": "\nI think we should also pull out ReaderPool?\n\n+1! ",
            "author": "Michael Busch",
            "id": "comment-12773432"
        },
        {
            "date": "2009-12-10T18:52:19+0000",
            "content": "We need an ability to see segment write (and probably deleted doc list write) as a discernible atomic operation. Right now it looks like several file writes, and we can't, say - redirect all files belonging to a certain segment to another Directory (well, in a simple manner). 'Something' should sit between a Directory (or several Directories) and IndexWriter.\n\nIf we could do this, the current NRT search implementation will be largely obsoleted, innit? Just override the default impl of 'something' and send smaller segments to ram, bigger to disk, copy ram segments to disk asynchronously if we want to. Then we can use your granma's IndexReader and IndexWriter, totally decoupled from each other, and have blazing fast addDocument-commit-reopen turnaround. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12788838"
        },
        {
            "date": "2009-12-10T18:53:14+0000",
            "content": "Oh, forgive me if I just said something stupid  ",
            "author": "Earwin Burrfoot",
            "id": "comment-12788840"
        },
        {
            "date": "2009-12-10T19:31:12+0000",
            "content": "I think what you're describing is in fact the approach that LUCENE-1313 is taking; it's doing the switching internally between the main Dir & a private RAM Dir.\n\nBut in my testing so far (LUCENE-2061), it doesn't seem like it'll help performance much.  Ie, the OS generally seems to do a fine job putting those segments in RAM, itself.  Ie, by maintaining a write cache.  The weirdness is: that only holds true if you flush the segments when they are tiny (once per second, every 100 docs, in my test) \u2013 not yet sure why that's the case.  I'm going to re-run perf tests on a more mainstream OS (my tests are all OpenSolaris) and see if that strangeness still happens.\n\nBut I think you still need to not do commit() during the reopen.\n\nI do think refactoring IW so that there is a separate component that keeps track of segments in the index, may simplify NRT, in that you can go to that source for your current \"segments file\" even if that segments file is uncommitted.  In such a world you could do something like IndexReader.open(SegmentState) and it would be able to open (and, reopen) the real-time reader.  It's just that it's seeing changes to the SegmentState done by the writer, even if they're not yet committed. ",
            "author": "Michael McCandless",
            "id": "comment-12788856"
        },
        {
            "date": "2009-12-11T19:18:14+0000",
            "content": "If I understand everything right, with current uberfast reopens (thanks per-segment search), the only thing that makes index/commit/reopen cycle slow is the 'sync' call. That sync call on memory-based Directory is noop.\n\nAnd no, you really should commit() to be able to see stuff on reopen()  My god, seeing changes that aren't yet commited - that violates the meaning of 'commit'.\n\nThe original purporse of current NRT code was.. well.. let me remember.. NRT search!  With per-segment caches and sync lag defeated you get the delay between doc being indexed and becoming searchable under tens of milliseconds. Is that not NRT enough to introduce tight coupling between classes that have absolutely no other reason to be coupled??\nLucene 4.0. Simplicity is our candidate! Vote for Simplicity!\n\n*: Okay, there remains an issue of merges that piggyback on commits, so writing and commiting one smallish segment suddenly becomes a time-consuming operation. But that's a completely separate issue. Go, fix your mergepolicies and have a thread that merges asynchronously. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12789473"
        },
        {
            "date": "2009-12-11T21:47:21+0000",
            "content": "If I understand everything right, with current uberfast reopens (thanks per-segment search), the only thing that makes index/commit/reopen cycle slow is the 'sync' call.\n\nI agree, per-segment searching was the most important step towards\nNRT.  It's a great step forward...\n\nBut the fsync call is a killer, so avoiding it in the NRT path is\nnecessary.  It's also very OS/FS dependent.\n\nThat sync call on memory-based Directory is noop.\n\nUntil you need to spillover to disk because your RAM buffer is full?\n\nAlso, if IW.commit() is called, I would expect any changes in RAM\nshould be committed to the real dir (stable storage)?\n\nAnd, going through RAM first will necessarily be a hit on indexing\nthroughput (Jake estimates 10% hit in Zoie's case).  Really, our\ncurrent approach goes through RAM as well, in that OS's write cache\n(if the machine has spare RAM) will quickly accept the small index\nfiles & write them in the BG.  It's not clear we can do better than\nthe OS here...\n\nAnd no, you really should commit() to be able to see stuff on reopen()  My god, seeing changes that aren't yet commited - that violates the meaning of 'commit'.\n\nUh, this is an API that clearly states that its purpose is to search\nthe uncommitted changes.  If you really want to be \"pure\"\ntransactional, don't use this API \n\nThe original purporse of current NRT code was.. well.. let me remember.. NRT search!  With per-segment caches and sync lag defeated you get the delay between doc being indexed and becoming searchable under tens of milliseconds. Is that not NRT enough to introduce tight coupling between classes that have absolutely no other reason to be coupled?? Lucene 4.0. Simplicity is our candidate! Vote for Simplicity!\n\nIn fact I favor our current approach because of its simplicity.\n\nHave a look at LUCENE-1313 (adds RAMDir as you're discussing), or,\nZoie, which also adds the RAMDir and backgrounds resolving deleted\ndocs \u2013 they add complexity to Lucene that I don't think is warranted.\n\nMy general feeling at this point is with per-segment searching, and\nfsync avoided, NRT performance is excellent.\n\nWe've explored a number of possible tweaks to improve it \u2013\nwriting first to RAMDir (LUCENE-1313), resolving deletes in the\nforeground (LUCENE-2047), using paged BitVector for deletions\n(LUCENE-1526), Zoie (buffering segments in RAM & backgrounds resolving\ndeletes), etc., but, based on testing so far, I don't see the\njustification for the added complexity.\n\n*: Okay, there remains an issue of merges that piggyback on commits, so writing and commiting one smallish segment suddenly becomes a time-consuming operation. But that's a completely separate issue. Go, fix your mergepolicies and have a thread that merges asynchronously.\n\nThis already runs in the BG by default.  But warming the reader on the\nmerged segment (before lighting it) is important (IW does this today). ",
            "author": "Michael McCandless",
            "id": "comment-12789555"
        },
        {
            "date": "2009-12-11T23:16:56+0000",
            "content": "Until you need to spillover to disk because your RAM buffer is full?\nNo, buffer is there only to decouple indexing from writing. Can be spilt over asynchronously without waiting for it to be filled up.\n\nOkay, we agree on a zillion of things, except simpicity of the current NRT, and approach to commit().\n\nGood commit() behaviour consists of two parts:\n1. Everything commit()ed is guaranteed to be on disk.\n2. Until commit() is called, reading threads don't see new/updated records.\n\nNow we want more speed, and are ready to sacrifice something if needed.\nYou decide to sacrifice new record (in)visibility. No choice, but to hack into IW to allow readers see its hot, fresh innards.\n\nI say it's better to sacrifice write guarantee. In the rare case the process/machine crashes, you can reindex last few minutes' worth of docs. Now you don't have to hack into IW and write specialized readers. Hence, simpicity. You have only one straightforward writer, you have only one straightforward reader (which is nicely immutable and doesn't need any synchronization code).\n\nIn fact you don't even need to sacrifice write guarantee. What was the reason for it? The only one I can come up with is - the thread that does writes and sync() is different from the thread that calls commit(). But, commit() can return a Future. \nSo the process goes as:\n\n\tYou index docs, nobody sees them, nor deletions.\n\tYou call commit(), the docs/deletes are written down to memory (NRT case)/disk (non-NRT case). Right after calling commit() every newly reopened Reader is guaranteed to see your docs/deletes.\n\tBackground thread does write-to-disk+sync(NRT case)/just sync (non-NRT case), and fires up the Future returned from commit(). At this point all data is guaranteed to be written and braced for a crash, ram cache or not, OS/raid controller cache or not.\n\n\n\nFor back-compat purporses we can use another name for that Future-returning-commit(), and current commit() will just call this new method and wait on future returned.\n\nOkay, with that I'm probably shutting up on the topic until I can back myself up with code. Sadly, my current employer is happy with update lag in tens of seconds  ",
            "author": "Earwin Burrfoot",
            "id": "comment-12789604"
        },
        {
            "date": "2009-12-11T23:57:53+0000",
            "content": "> I say it's better to sacrifice write guarantee. \n\nI don't grok why sync is the default, especially given how sketchy hardware \ndrivers are about obeying fsync:    \n\n\n  But, beware: some hardware devices may in fact cache writes even during \n  fsync,  and return before the bits are actually on stable storage, to give the     \n  appearance of faster performance.\n\n\nIMO, it should have been an option which defaults to false, to be enabled only by \nusers who have the expertise to ensure that fsync() is actually doing what \nit advertises. But what's done is done (and Lucy will probably just do something \ndifferent.)\n\nWith regard to Lucene NRT, though, turning sync() off would really help.  If and \nwhen some sort of settings class comes about, an enableSync(boolean enabled) \nmethod seems like it would come in handy. ",
            "author": "Marvin Humphrey",
            "id": "comment-12789614"
        },
        {
            "date": "2009-12-12T00:03:27+0000",
            "content": "Now we want more speed, and are ready to sacrifice something if needed.\nYou decide to sacrifice new record (in)visibility. No choice, but to hack into IW to allow readers see its hot, fresh innards.\n\nChiming in here that of course, you don't need (ie there is a choice) to hack into the IW to do this.  Zoie is a completely user-land solution which modifies no IW/IR internals and yet achieves millisecond index-to-query-visibility turnaround while keeping speedy indexing and query performance.  It just keeps the RAMDir outside encapsulated in an object (an IndexingSystem) which has IndexReaders built off of both the RAMDir and the FSDir, and hides the implementation details (in fact the IW itself) from the user.  \n\nThe API for this kind of thing doesn't have to be tightly coupled, and I would agree with you that it shouldn't be. ",
            "author": "Jake Mannix",
            "id": "comment-12789618"
        },
        {
            "date": "2009-12-12T10:55:53+0000",
            "content": "\nUntil you need to spillover to disk because your RAM buffer is full?\n\nNo, buffer is there only to decouple indexing from writing. Can be spilt over asynchronously without waiting for it to be filled up.\n\nBut this is where things start to get complex... the devil is in the\ndetails here.  How do you carry over your deletes?  This spillover\nwill take time \u2013 do you block all indexing while that's happening\n(not great)?  Do you do it gradually (start spillover when half full,\nbut still accept indexing)?  Do you throttle things if index rate\nexceeds flush rate?  How do you recover on exception?\n\nNRT today let's the OS's write cache decide how to use RAM to speed up\nwriting of these small files, which keeps things alot simpler for us.\nI don't see why we should add complexity to Lucene to replicate what\nthe OS is doing for us (NOTE: I don't really trust the OS in the\nreverse case... I do think Lucene should read into RAM the data\nstructures that are important).\n\nYou decide to sacrifice new record (in)visibility. No choice, but to hack into IW to allow readers see its hot, fresh innards.\n\nNow you don't have to hack into IW and write specialized readers.\n\nProbably we'll just have to disagree here... NRT isn't a hack \n\nIW is already hanging onto completely normal segments.  Ie, the index\nhas been updated with these segments, just not yet published so\noutside readers can see it.  All NRT does is let a reader see this\nprivate view.\n\nThe readers that an NRT reader expoes are normal SegmentReaders \u2013\nit's just that rather than consult a segments_N on disk to get the\nsegment metadata, they pulled from IW's uncommitted in memory\nSegmentInfos instance.\n\nYes we've talked about the \"hot innards\" solution \u2013 an IndexReader\nimpl that can directly search DW's ram buffer \u2013 but that doesn't look\nnecessary today, because performance of NRT is good with the simple\nsolution we have now.\n\nNRT reader also gains performance by carrying over deletes in RAM.  We\nshould eventually do the same thing with norms & field cache.  No\nreason to write to disk, then right away read again.\n\n\n\n\tYou index docs, nobody sees them, nor deletions.\n\tYou call commit(), the docs/deletes are written down to memory (NRT case)/disk (non-NRT case). Right after calling commit() every newly reopened Reader is guaranteed to see your docs/deletes.\n\tBackground thread does write-to-disk+sync(NRT case)/just sync (non-NRT case), and fires up the Future returned from commit(). At this point all data is guaranteed to be written and braced for a crash, ram cache or not, OS/raid controller cache or not.\n\n\n\nBut this is not a commit, if docs/deletes are written down into RAM?\nIe, commit could return, then the machine could crash, and you've lost\nchanges?  Commit should go through to stable storage before returning?\nMaybe I'm just missing the big picture of what you're proposing\nhere...\n\nAlso, you can build all this out on top of Lucene today?  Zoie is a\nproof point of this.  (Actually: how does your proposal differ from\nZoie?  Maybe that'd help shed light...).\n\nI say it's better to sacrifice write guarantee. In the rare case the process/machine crashes, you can reindex last few minutes' worth of docs. \n\nIt is not that simple \u2013 if you skip the fsync, and OS crashes/you\nlose power, your index can easily become corrupt.  The resulting\nCheckIndex -fix can easily need to remove large segments.\n\nThe OS's write cache makes no gurantees on the order in which the\nfiles you've written find their way to disk.\n\nAnother option (we've discussed this) would be journal file approach\n(ie transaction log, like most DBs use).  You only have one file to\nfsync, and you replay to recover.  But that'd be a big change for\nLucene, would add complexity, and can be accomplished outside of\nLucene if an app really wants to...\n\nLet me try turning this around: in your componentization of\nSegmentReader, why does it matter who's tracking which components are\nneeded to make up a given SR?  In the IndexReader.open case, it's a\nSegmntInfos instance (obtained by loading segments_N file from disk).\nIn the NRT case, it's also a SegmentInfos instace (the one IW is\nprivately keeping track of and only publishing on commit).  At the\ncomponent level, creating the SegmentReader should be no different? ",
            "author": "Michael McCandless",
            "id": "comment-12789708"
        },
        {
            "date": "2009-12-12T11:22:09+0000",
            "content": "\n> I say it's better to sacrifice write guarantee.\n\nI don't grok why sync is the default, especially given how sketchy hardware \ndrivers are about obeying fsync:\n\n\nBut, beware: some hardware devices may in fact cache writes even during \nfsync, and return before the bits are actually on stable storage, to give the \nappearance of faster performance.\n\n\nIt's unclear how often this scare-warning is true in practice (scare\nwarnings tend to spread very easily without concrete data); it's in\nthe javadocs for completeness sake.  I expect (though have no data to\nback this up...) that most OS/IO systems \"out there\" do properly\nimplement fsync.\n\n\nIMO, it should have been an option which defaults to false, to be enabled only by \nusers who have the expertise to ensure that fsync() is actually doing what \nit advertises. But what's done is done (and Lucy will probably just do something \ndifferent.)\n\nI think that's a poor default (trades safety for performance), unless\nLucy eg uses a transaction log so you can concretely bound what's lost\non crash/power loss.  Or, if you go back to autocommitting I guess...\n\nIf we did this in Lucene, you can have unbounded corruption.  It's not\njust the last few minutes of updates...\n\nSo, I don't think we should even offer the option to turn it off.  You\ncan easily subclass your FSDir impl and make sync() a no-op if your\nreally want to...\n\n\nWith regard to Lucene NRT, though, turning sync() off would really help. If and \nwhen some sort of settings class comes about, an enableSync(boolean enabled) \nmethod seems like it would come in handy.\n\nYou don't need to turn off sync for NRT \u2013 that's the whole point.  It\ngives you a reader without syncing the files.  Really, this is your\nsafety tradeoff \u2013 it means you can commit less frequently, since the\nNRT reader can search the latest updates.  But, your app has\ncomplete control over how it wants to to trade safety for performance. ",
            "author": "Michael McCandless",
            "id": "comment-12789714"
        },
        {
            "date": "2009-12-12T11:29:56+0000",
            "content": "Zoie is a completely user-land solution which modifies no IW/IR internals and yet achieves millisecond index-to-query-visibility turnaround while keeping speedy indexing and query performance. It just keeps the RAMDir outside encapsulated in an object (an IndexingSystem) which has IndexReaders built off of both the RAMDir and the FSDir, and hides the implementation details (in fact the IW itself) from the user.\n\nRight, one can always not use NRT and build their own layers on top.\n\nBut, Zoie has alot of code to accomplish this \u2013 the devil really is\nin the details to \"simply write first to a RAMDir\".  This is why I'd\nlike Earwin to look @ Zoie and clarify his proposed approach, in\ncontrast...\n\nActually, here's a question: how quickly can Zoie turn around a\ncommit()?  Seems like it must take more time than Lucene, since it does\nextra stuff (flush RAM buffers to disk, materialize deletes) before\neven calling IW.commit.\n\nAt the end of the day, any NRT system has to trade safety for\nperformance (bypass the sync call in the NRT reader)....\n\nThe API for this kind of thing doesn't have to be tightly coupled, and I would agree with you that it shouldn't be.\n\nI don't consider NRT today to be a tight coupling (eg, the pending\nrefactoring of IW would nicely separate it out).  If we implement the\nIR that searches DW's RAM buffer, then I'd agree  ",
            "author": "Michael McCandless",
            "id": "comment-12789716"
        },
        {
            "date": "2009-12-13T15:38:28+0000",
            "content": "> I think that's a poor default (trades safety for performance), unless\n> Lucy eg uses a transaction log so you can concretely bound what's lost\n> on crash/power loss. Or, if you go back to autocommitting I guess...\n\nSearch indexes should not be used for canonical data storage \u2013 they should be\nbuilt on top of canonical data storage.  Guarding against power failure\ninduced corruption in a database is an imperative.  Guarding against power\nfailure induced corruption in a search index is a feature, not an imperative.\n\nUsers have many options for dealing with the potential for such corruption.\nYou can go back to your canonical data store and rebuild your index from\nscratch when it happens.  In a search cluster environment, you can rsync a\nknown-good copy from another node.  Potentially, you might enable\nfsync-before-commit and keep your own transaction log.  However, if the time\nit takes to rebuild or recover an index from scratch would have caused you\nunacceptable downtime, you can't possibly be operating in a\nsingle-point-of-failure environment where a power failure could take you down\nanyway \u2013 so other recovery options are available to you.\n\nTurning on fsync is only one step towards ensuring index integrity; others\nsteps involve making decisions about hard drives, RAID arrays, failover\nstrategies, network and off-site backups, etc, and are outside of our domain\nas library authors.  We cannot meet the needs of users who need guaranteed\nindex integrity on our own.\n\nFor everybody else, what turning on fsync by default achieves is to make an\nexceedingly rare event rarer.  That's valuable, but not essential.  My\nargument is that since the search indexes should not be used for canonical\nstorage, and since fsync is not testably reliable and not sufficient on its\nown, it's a good engineering compromise to prioritize performance.  \n\n> If we did this in Lucene, you can have unbounded corruption. It's not\n> just the last few minutes of updates...\n\nWasn't that a possibility under autocommit as well?   All it takes is for the\nOS to finish flushing the new snapshot file to persistent storage before it\nfinishes flushing a segment data file needed by that snapshot, and for the\npower failure to squeeze in between. \n\nIn practice, locality of reference is going to make the window very very\nsmall, since those two pieces of data will usually get written very close to\neach other on the persistent media.\n\nI've seen a lot more messages to our user lists over the years about data\ncorruption caused by bugs and misconfigurations than by power failures.\n\nBut really, that's as it should be.  Ensuring data integrity to the degree\nrequired by a database is costly \u2013 it requires far more rigorous testing, and\nfar more conservative development practices.  If we accept that our indexes\nmust never go corrupt, it will retard innovation.\n\nOf course we should work very hard to prevent index corruption.  However, I'm\nmuch more concerned about stuff like silent omission of search results due to\noverzealous, overly complex optimizations than I am about problems arising\nfrom power failures.  When a power failure occurs, you know it \u2013 so you get\nthe opportunity to fsck the disk, run checkIndex(), perform data integrity\nreconciliation tests against canonical storage, and if anything fails, take\nwhatever recovery actions you deem necessary.\n\n> You don't need to turn off sync for NRT - that's the whole point. It\n> gives you a reader without syncing the files. \n\nI suppose this is where Lucy and Lucene differ.  Thanks to mmap and the\nnear-instantaneous reader opens it has enabled, we don't need to keep a\nspecial reader alive.  Since there's no special reader, the only way to get\ndata to a search process is to go through a commit.  But if we fsync on every\ncommit, we'll drag down indexing responsiveness.  Fishishing the commit and\nreturning control to client code as quickly as possible is a high priority for\nus.\n\nFurthermore, I don't want us to have to write the code to support a\nnear-real-time reader hanging off of IndexWriter a la Lucene.  The\narchitectural discussions have made for very interesting reading, but the\ndesign seems to be tricky to pull off, and implementation simplicity in core\nsearch code is a high priority for Lucy.  It's better for Lucy to kill two\nbirds with one stone and concentrate on making all index opens fast. \n\n> Really, this is your safety tradeoff - it means you can commit less\n> frequently, since the NRT reader can search the latest updates. But, your\n> app has complete control over how it wants to to trade safety for\n> performance.\n\nSo long as fsync is an option, the app always has complete control, regardless\nof whether the default setting is fsync or no fsync.\n\nIf a Lucene app wanted to increase NRT responsiveness and throughput, and if\nabsolute index integrity wasn't a concern because it had been addressed\nthrough other means (e.g. multi-node search cluster), would turning off fsync\nspeed things up under any of the proposed designs? ",
            "author": "Marvin Humphrey",
            "id": "comment-12789905"
        },
        {
            "date": "2009-12-14T02:52:11+0000",
            "content": "I think large scale NRT installations may eventually require a\ndistributed transaction log. The implementation details have yet\nto be determined however it could potentially solve the issue of\ndata loss being discussed. One candidate is a combo of Zookeeper\n+ Bookeeper. I would venture to guess this could be implemented\nas a part of Solr, however we've got a lot of work to do for\nSolr to be reasonably NRT efficient (see the tracking issue\nSOLR-1606), and we're just starting on the Zookeeper\nimplementation SOLR-1277...  ",
            "author": "Jason Rutherglen",
            "id": "comment-12789971"
        },
        {
            "date": "2009-12-15T22:20:27+0000",
            "content": "\nI think that's a poor default (trades safety for performance), unless Lucy eg uses a transaction log so you can concretely bound what's lost on crash/power loss. Or, if you go back to autocommitting I guess...\n\nSearch indexes should not be used for canonical data storage - they should be\nbuilt on top of canonical data storage.\n\nI agree with that, in theory, but I think in practice it's too\nidealistic to force/expect apps to meet that ideal.\n\nI expect for many apps it's a major cost to unexpectedly lose the\nsearch index on power loss / OS crash.\n\n\nUsers have many options for dealing with the potential for such corruption.\nYou can go back to your canonical data store and rebuild your index from\nscratch when it happens. In a search cluster environment, you can rsync a\nknown-good copy from another node. Potentially, you might enable\nfsync-before-commit and keep your own transaction log. However, if the time\nit takes to rebuild or recover an index from scratch would have caused you\nunacceptable downtime, you can't possibly be operating in a\nsingle-point-of-failure environment where a power failure could take you down\nanyway - so other recovery options are available to you.\n\nTurning on fsync is only one step towards ensuring index integrity; others\nsteps involve making decisions about hard drives, RAID arrays, failover\nstrategies, network and off-site backups, etc, and are outside of our domain\nas library authors. We cannot meet the needs of users who need guaranteed\nindex integrity on our own.\n\nYes, high availability apps will already take their measures to\nprotect the search index / recovery process, going beyond fsync.\nEG, making a hot backup of Lucene index is now straightforwarded.\n\n\nFor everybody else, what turning on fsync by default achieves is to make an\nexceedingly rare event rarer. That's valuable, but not essential. My\nargument is that since the search indexes should not be used for canonical\nstorage, and since fsync is not testably reliable and not sufficient on its\nown, it's a good engineering compromise to prioritize performance.\n\nLosing power to the machine, or OS crash, or the user doing a hard\npower down because OS isn't responding, I think are not actually\nthat uncommon in an end user setting.  Think of a desktop app\nembedding Lucene/Lucy...\n\n\nIf we did this in Lucene, you can have unbounded corruption. It's not just the last few minutes of updates...\n\nWasn't that a possibility under autocommit as well? All it takes is for the\nOS to finish flushing the new snapshot file to persistent storage before it\nfinishes flushing a segment data file needed by that snapshot, and for the\npower failure to squeeze in between.\n\nNot after LUCENE-1044... autoCommit simply called commit() at certain\nopportune times (after finish big merges), which does the right thing\n(I hope!).  The segments file is not written until all files it\nreferences are sync'd.\n\n\nIn practice, locality of reference is going to make the window very very\nsmall, since those two pieces of data will usually get written very close to\neach other on the persistent media.\n\nNot sure about that \u2013 it depends on how effectively the OS's write cache\n\"preserves\" that locality.\n\n\nI've seen a lot more messages to our user lists over the years about data\ncorruption caused by bugs and misconfigurations than by power failures.\n\nI would agree, though, I think it may be a sampling problem... ie\npeople whose machines crashed and they lost the search index would\noften not raise it on the list (vs say a persistent config issue that keeps\nleading to corruption).\n\n\nBut really, that's as it should be. Ensuring data integrity to the degree\nrequired by a database is costly - it requires far more rigorous testing, and\nfar more conservative development practices. If we accept that our indexes\nmust never go corrupt, it will retard innovation.\n\nIt's not really that costly, with NRT \u2013 you can get a searcher on the\nindex without paying the commit cost.  And now you can call commit\nhowever frequently you need to.  Quickly turning around a new\nsearcher, and how frequently you commit, are now independent.\n\nAlso, having the app explicitly decouple these two notions keeps the\ndoor open for future improvements.  If we force absolutely all sharing\nto go through the filesystem then that limits the improvements we can\nmake to NRT.\n\n\nOf course we should work very hard to prevent index corruption. However, I'm\nmuch more concerned about stuff like silent omission of search results due to\noverzealous, overly complex optimizations than I am about problems arising\nfrom power failures. When a power failure occurs, you know it - so you get\nthe opportunity to fsck the disk, run checkIndex(), perform data integrity\nreconciliation tests against canonical storage, and if anything fails, take\nwhatever recovery actions you deem necessary.\n\nWell... I think search performance is important, and we should pursue it\neven if we risk bugs.\n\n\nYou don't need to turn off sync for NRT - that's the whole point. It gives you a reader without syncing the files.\n\nI suppose this is where Lucy and Lucene differ. Thanks to mmap and the\nnear-instantaneous reader opens it has enabled, we don't need to keep a\nspecial reader alive. Since there's no special reader, the only way to get\ndata to a search process is to go through a commit. But if we fsync on every\ncommit, we'll drag down indexing responsiveness. Fishishing the commit and\nreturning control to client code as quickly as possible is a high priority for\nus.\n\nNRT reader isn't that special \u2013 the only things different is 1) it\nloaded the segments_N \"file\" from IW instead of the filesystem, and 2)\nit uses a reader pool to \"share\" the underlying SegmentReaders with\nother places that have loaded them.  I guess, if Lucy won't allow\nthis, then, yes, forcing a commit in order to reopen is very costly,\nand so sacrificing safety is a tradeoff you have to make.\n\nAlternatively, you could keep the notion \"flush\" (an unsafe commit)\nalive?  You write the segments file, but make no effort to ensure it's\ndurability (and also preserve the last \"true\" commit).  Then a normal\nIR.reopen suffices...\n\n\nFurthermore, I don't want us to have to write the code to support a\nnear-real-time reader hanging off of IndexWriter a la Lucene. The\narchitectural discussions have made for very interesting reading, but the\ndesign seems to be tricky to pull off, and implementation simplicity in core\nsearch code is a high priority for Lucy. It's better for Lucy to kill two\nbirds with one stone and concentrate on making all index opens fast.\n\nBut shouldn't you at least give an option for index durability?  Even\nif we disagree about the default?\n\n\nReally, this is your safety tradeoff - it means you can commit less frequently, since the NRT reader can search the latest updates. But, your app has complete control over how it wants to to trade safety for performance.\n\nSo long as fsync is an option, the app always has complete control,\nregardless of whether the default setting is fsync or no fsync.\n\nWell it is an \"option\" in Lucene \u2013 \"it's just software\"    I don't\nwant to make it easy to be unsafe.  Lucene shouldn't sacrifice safety\nof the index... and with NRT there's no need to make that tradeoff.\n\n\nIf a Lucene app wanted to increase NRT responsiveness and throughput, and if\nabsolute index integrity wasn't a concern because it had been addressed\nthrough other means (e.g. multi-node search cluster), would turning off fsync\nspeed things up under any of the proposed designs?\n\nYes, turning off fsync would speed things up \u2013 you could fall back to\nsimple reopen and get good performance (NRT should still be faster\nsince the readers are pooled).  The \"use RAMDir on top of Lucene\"\ndesigns would be helped less since fsync is a noop in RAMDir. ",
            "author": "Michael McCandless",
            "id": "comment-12790988"
        },
        {
            "date": "2009-12-16T19:46:27+0000",
            "content": ">> Wasn't that a possibility under autocommit as well? All it takes is for the\n>> OS to finish flushing the new snapshot file to persistent storage before it\n>> finishes flushing a segment data file needed by that snapshot, and for the\n>> power failure to squeeze in between.\n> \n> Not after LUCENE-1044... autoCommit simply called commit() at certain\n> opportune times (after finish big merges), which does the right thing (I\n> hope!). The segments file is not written until all files it references are\n> sync'd.\n\nFWIW, autoCommit doesn't really have a place in Lucy's\none-segment-per-indexing-session model.\n\nRevisiting the LUCENE-1044 threads, one passage stood out:\n\n\n    http://www.gossamer-threads.com/lists/lucene/java-dev/54321#54321\n\n    This is why in a db system, the only file that is sync'd is the log\n    file - all other files can be made \"in sync\" from the log file - and\n    this file is normally striped for optimum write performance. Some\n    systems have special \"log file drives\" (some even solid state, or\n    battery backed ram) to aid the performance. \n\n\nThe fact that we have to sync all files instead of just one seems sub-optimal.\n\nYet Lucene is not well set up to maintain a transaction log.  The very act of\nadding a document to Lucene is inherently lossy even if all fields are stored,\nbecause doc boost is not preserved.\n\n> Also, having the app explicitly decouple these two notions keeps the\n> door open for future improvements. If we force absolutely all sharing\n> to go through the filesystem then that limits the improvements we can\n> make to NRT.\n\nHowever, Lucy has much more to gain going through the file system than Lucene\ndoes, because we don't necessarily incur JVM startup costs when launching a\nnew process.  The Lucene approach to NRT \u2013 specialized reader hanging off of\nwriter \u2013 is constrained to a single process.  The Lucy approach \u2013 fast index\nopens enabled by mmap-friendly index formats \u2013 is not.\n\nThe two approaches aren't mutually exclusive.  It will be possible to augment\nLucy with a specialized index reader within a single process.  However, A)\nthere seems to be a lot of disagreement about just how to integrate that\nreader, and B) there seem to be ways to bolt that functionality on top of the\nexisting classes.  Under those circumstances, I think it makes more sense to\nkeep that feature external for now.\n\n> Alternatively, you could keep the notion \"flush\" (an unsafe commit)\n> alive? You write the segments file, but make no effort to ensure it's\n> durability (and also preserve the last \"true\" commit). Then a normal\n> IR.reopen suffices...\n\nThat sounds promising.  The semantics would differ from those of Lucene's\nflush(), which doesn't make changes visible.\n\nWe could implement this by somehow marking a \"committed\" snapshot and a\n\"flushed\" snapshot differently, either by adding an \"fsync\" property to the\nsnapshot file that would be false after a flush() but true after a commit(),\nor by encoding the property within the snapshot filename.  The file purger\nwould have to ensure that all index files referenced by either the last\ncommitted snapshot or the last flushed snapshot were off limits.  A rollback()\nwould zap all changes since the last commit().  \n\nSuch a scheme allows the the top level app to avoid the costs of fsync while\nmaintaining its own transaction log \u2013 perhaps with the optimizations\nsuggested above (separate disk, SSD, etc).  ",
            "author": "Marvin Humphrey",
            "id": "comment-12791549"
        },
        {
            "date": "2009-12-17T14:06:50+0000",
            "content": "\nFWIW, autoCommit doesn't really have a place in Lucy's\none-segment-per-indexing-session model.\n\nWell, autoCommit just means \"periodically call commit\".  So, if you\ndecide to offer a commit() operation, then autoCommit would just wrap\nthat?  But, I don't think autoCommit should be offered... app should\ndecide.\n\n\nRevisiting the LUCENE-1044 threads, one passage stood out:\n\nhttp://www.gossamer-threads.com/lists/lucene/java-dev/54321#54321\n\nThis is why in a db system, the only file that is sync'd is the log\nfile - all other files can be made \"in sync\" from the log file - and\nthis file is normally striped for optimum write performance. Some\nsystems have special \"log file drives\" (some even solid state, or\nbattery backed ram) to aid the performance.\n\nThe fact that we have to sync all files instead of just one seems sub-optimal.\n\nYes, but, that cost is not on the reopen path, so it's much less\nimportant.  Ie, the app can freely choose how frequently it wants to\ncommit, completely independent from how often it needs to reopen.\n\n\nYet Lucene is not well set up to maintain a transaction log. The very act of\nadding a document to Lucene is inherently lossy even if all fields are stored,\nbecause doc boost is not preserved.\n\nI don't see that those two statements are related.\n\nOne can \"easily\" (meaning, it's easily decoupled from core) make a\ntransaction log on top of lucene \u2013 just serialize your docs/analzyer\nselection/etc to the log & sync it periodically.\n\nBut, that's orthogonal to what Lucene does & doesn't preserve in its\nindex (and, yes, Lucene doesn't precisely preserve boosts).\n\n\nAlso, having the app explicitly decouple these two notions keeps the door open for future improvements. If we force absolutely all sharing to go through the filesystem then that limits the improvements we can make to NRT.\n\nHowever, Lucy has much more to gain going through the file system than Lucene\ndoes, because we don't necessarily incur JVM startup costs when launching a\nnew process. The Lucene approach to NRT - specialized reader hanging off of\nwriter - is constrained to a single process. The Lucy approach - fast index\nopens enabled by mmap-friendly index formats - is not.\n\nThe two approaches aren't mutually exclusive. It will be possible to augment\nLucy with a specialized index reader within a single process. However, A)\nthere seems to be a lot of disagreement about just how to integrate that\nreader, and B) there seem to be ways to bolt that functionality on top of the\nexisting classes. Under those circumstances, I think it makes more sense to\nkeep that feature external for now.\n\nAgain: NRT is not a \"specialized reader\".  It's a normal read-only\nDirectoryReader, just like you'd get from IndexReader.open, with the\nonly difference being that it consulted IW to find which segments to\nopen.  Plus, it's pooled, so that if IW already has a given segment\nreader open (say because deletes were applied or merges are running),\nit's reused.\n\nWe've discussed making it specialized (eg directly asearching DW's ram\nbuffer, caching recently flushed segments in RAM, special\nincremental-copy-on-write data structures for deleted docs, etc.) but\nso far these changes don't seem worthwhile.\n\nThe current approach to NRT is simple... I haven't yet seen\nperformance gains strong enough to justify moving to \"specialized\nreaders\".\n\nYes, Lucene's approach must be in the same JVM.  But we get important\ngains from this \u2013 reusing a single reader (the pool), carrying over\nmerged deletions directly in RAM (and eventually field cache & norms\ntoo \u2013 LUCENE-1785).\n\nInstead, Lucy (by design) must do all sharing & access all index data\nthrough the filesystem (a decision, I think, could be dangerous),\nwhich will necessarily increase your reopen time.  Maybe in practice\nthat cost is small though... the OS write cache should keep everything\nfresh... but you still must serialize.\n\n\nAlternatively, you could keep the notion \"flush\" (an unsafe commit) alive? You write the segments file, but make no effort to ensure it's durability (and also preserve the last \"true\" commit). Then a normal IR.reopen suffices...\n\nThat sounds promising. The semantics would differ from those of Lucene's\nflush(), which doesn't make changes visible.\n\nWe could implement this by somehow marking a \"committed\" snapshot and a\n\"flushed\" snapshot differently, either by adding an \"fsync\" property to the\nsnapshot file that would be false after a flush() but true after a commit(),\nor by encoding the property within the snapshot filename. The file purger\nwould have to ensure that all index files referenced by either the last\ncommitted snapshot or the last flushed snapshot were off limits. A rollback()\nwould zap all changes since the last commit().\n\nSuch a scheme allows the the top level app to avoid the costs of fsync while\nmaintaining its own transaction log - perhaps with the optimizations\nsuggested above (separate disk, SSD, etc).\n\nIn fact, this would make Lucy's approach to NRT nearly identical to\nLucene NRT.\n\nThe only difference is, instead of getting the current uncommitted\nsegments_N via RAM, Lucy uses the filesystem.  And, of course\nLucy doesn't pool readers.  So this is really a Lucy-ification of\nLucene's approach to NRT.\n\nSo it has the same benefits as Lucene's NRT, ie, lets Lucy apps\ndecouple decisions about safety (commit) and freshness (reopen\nturnaround time). ",
            "author": "Michael McCandless",
            "id": "comment-12791936"
        },
        {
            "date": "2009-12-18T20:19:54+0000",
            "content": "> Well, autoCommit just means \"periodically call commit\". So, if you\n> decide to offer a commit() operation, then autoCommit would just wrap\n> that? But, I don't think autoCommit should be offered... app should\n> decide.\n\nAgreed, autoCommit had benefits under legacy Lucene, but wouldn't be important\nnow.  If we did add some sort of \"automatic commit\" feature, it would mean\nsomething else: commit every change instantly.  But that's easy to implement\nvia a wrapper, so there's no point cluttering the the primary index writer\nclass to support such a feature.\n\n> Again: NRT is not a \"specialized reader\". It's a normal read-only\n> DirectoryReader, just like you'd get from IndexReader.open, with the\n> only difference being that it consulted IW to find which segments to\n> open. Plus, it's pooled, so that if IW already has a given segment\n> reader open (say because deletes were applied or merges are running),\n> it's reused.\n\nWell, it seems to me that those two features make it special \u2013 particularly\nthe pooling of SegmentReaders.  You can't take advantage of that outside the\ncontext of IndexWriter:\n\n> Yes, Lucene's approach must be in the same JVM. But we get important\n> gains from this - reusing a single reader (the pool), carrying over\n> merged deletions directly in RAM (and eventually field cache & norms\n> too - LUCENE-1785).\n\nExactly.  In my view, that's what makes that reader \"special\": unlike ordinary\nLucene IndexReaders, this one springs into being with its caches already\nprimed rather than in need of lazy loading.\n\nBut to achieve those benefits, you have to mod the index writing process.\nThose modifications are not necessary under the Lucy model, because the mere\nact of writing the index stores our data in the system IO cache.\n\n> Instead, Lucy (by design) must do all sharing & access all index data\n> through the filesystem (a decision, I think, could be dangerous),\n> which will necessarily increase your reopen time. \n\nDangerous in what sense?\n\nGoing through the file system is a tradeoff, sure \u2013 but it's pretty nice to\ndesign your low-latency search app free from any concern about whether\nindexing and search need to be coordinated within a single process.\nFurthermore, if separate processes are your primary concurrency model, going\nthrough the file system is actually mandatory to achieve best performance on a\nmulti-core box.  Lucy won't always be used with multi-threaded hosts.\n\nI actually think going through the file system is dangerous in a different\nsense: it puts pressure on the file format spec.  The easy way to achieve IPC\nbetween writers and readers will be to dump stuff into one of the JSON files\nto support the killer-feature-du-jour \u2013 such as what I'm proposing with this\n\"fsync\" key in the snapshot file.  But then we wind up with a bunch of crap\ncluttering up our index metadata files.  I'm determined that Lucy will have a\nmore coherent file format than Lucene, but with this IPC requirement we're\nsetting our community up to push us in the wrong direction.  If we're not\ncareful, we could end up with a file format that's an unmaintainable jumble.\n\nBut you're talking performance, not complexity costs, right?\n\n> Maybe in practice that cost is small though... the OS write cache should\n> keep everything fresh... but you still must serialize.\n\nAnecdotally, at Eventful one of our indexes is 5 GB with 16 million records\nand 900 MB worth of sort cache data; opening a fresh searcher and loading all\nsort caches takes circa 21 ms.\n\nThere's room to improve that further \u2013 we haven't yet implemented\nIndexReader.reopen() \u2013 but that was fast enough to achieve what we wanted to\nachieve. ",
            "author": "Marvin Humphrey",
            "id": "comment-12792625"
        },
        {
            "date": "2009-12-18T20:29:16+0000",
            "content": "Anecdotally, at Eventful one of our indexes is 5 GB with 16 million records\nand 900 MB worth of sort cache data; opening a fresh searcher and loading all\nsort caches takes circa 21 ms.\n\nMarvin, very cool!  Are you using the mmap module you mentioned at ApacheCon? ",
            "author": "Jason Rutherglen",
            "id": "comment-12792629"
        },
        {
            "date": "2009-12-18T20:51:03+0000",
            "content": "Yes, this is using the sort cache model worked out this spring on lucy-dev.\nThe memory mapping happens within FSFileHandle (LUCY-83). SortWriter \nand SortReader haven't made it into the Lucy repository yet. ",
            "author": "Marvin Humphrey",
            "id": "comment-12792638"
        },
        {
            "date": "2009-12-19T00:10:54+0000",
            "content": "\nAgain: NRT is not a \"specialized reader\". It's a normal read-only DirectoryReader, just like you'd get from IndexReader.open, with the only difference being that it consulted IW to find which segments to open. Plus, it's pooled, so that if IW already has a given segment reader open (say because deletes were applied or merges are running), it's reused.\n\nWell, it seems to me that those two features make it special - particularly\nthe pooling of SegmentReaders. You can't take advantage of that outside the\ncontext of IndexWriter:\n\nOK so mabye a little special  But, really that pooling should be\nfactored out of IW.  It's not writer specific.\n\n\nYes, Lucene's approach must be in the same JVM. But we get important gains from this - reusing a single reader (the pool), carrying over merged deletions directly in RAM (and eventually field cache & norms too - LUCENE-1785).\n\nExactly. In my view, that's what makes that reader \"special\": unlike ordinary\nLucene IndexReaders, this one springs into being with its caches already\nprimed rather than in need of lazy loading.\n\nBut to achieve those benefits, you have to mod the index writing process.\n\nMod the index writing, and the reader reopen, to use the shared pool.\nThe pool in itself isn't writer specific.\n\nReally the pool is just like what you tap into when you call reopen \u2013\nthat method looks at the current \"pool\" of already opened segments,\nsharing what it can.\n\nThose modifications are not necessary under the Lucy model, because the mere act of writing the index stores our data in the system IO cache.\n\nBut, that's where Lucy presumably takes a perf hit.  Lucene can share\nthese in RAM, not usign the filesystem as the intermediary (eg we do\nthat today with deletions; norms/field cache/eventual CSF can do the\nsame.)  Lucy must go through the filesystem to share.\n\n\nInstead, Lucy (by design) must do all sharing & access all index data through the filesystem (a decision, I think, could be dangerous), which will necessarily increase your reopen time.\n\nDangerous in what sense?\n\nGoing through the file system is a tradeoff, sure - but it's pretty nice to\ndesign your low-latency search app free from any concern about whether\nindexing and search need to be coordinated within a single process.\nFurthermore, if separate processes are your primary concurrency model, going\nthrough the file system is actually mandatory to achieve best performance on a\nmulti-core box. Lucy won't always be used with multi-threaded hosts.\n\nI actually think going through the file system is dangerous in a different\nsense: it puts pressure on the file format spec. The easy way to achieve IPC\nbetween writers and readers will be to dump stuff into one of the JSON files\nto support the killer-feature-du-jour - such as what I'm proposing with this\n\"fsync\" key in the snapshot file. But then we wind up with a bunch of crap\ncluttering up our index metadata files. I'm determined that Lucy will have a\nmore coherent file format than Lucene, but with this IPC requirement we're\nsetting our community up to push us in the wrong direction. If we're not\ncareful, we could end up with a file format that's an unmaintainable jumble.\n\nBut you're talking performance, not complexity costs, right?\n\nMostly I was thinking performance, ie, trusting the OS to make good\ndecisions about what should be RAM resident, when it has limited\ninformation...\n\nBut, also risky is that all important data structures must be\n\"file-flat\", though in practice that doesn't seem like an issue so\nfar?  The RAM resident things Lucene has \u2013 norms, deleted docs, terms\nindex, field cache \u2013 seem to \"cast\" just fine to file-flat.  If we\nswitched to an FST for the terms index I guess that could get\ntricky...\n\nWouldn't shared memory be possible for process-only concurrent models?\nAlso, what popular systems/environments have this requirement (only\nprocess level concurrency) today?\n\nIt's wonderful that Lucy can startup really fast, but, for most apps\nthat's not nearly as important as searching/indexing performance,\nright?  I mean, you start only once, and then you handle many, many\nsearches / index many documents, with that process, usually?\n\n\nMaybe in practice that cost is small though... the OS write cache should keep everything fresh... but you still must serialize.\n\nAnecdotally, at Eventful one of our indexes is 5 GB with 16 million records\nand 900 MB worth of sort cache data; opening a fresh searcher and loading all\nsort caches takes circa 21 ms.\n\nThat's fabulously fast!\n\nBut you really need to also test search/indexing throughput, reopen time\n(I think) once that's online for Lucy...\n\n\nThere's room to improve that further - we haven't yet implemented\nIndexReader.reopen() - but that was fast enough to achieve what we wanted to\nachieve.\n\nIs reopen even necessary in Lucy? ",
            "author": "Michael McCandless",
            "id": "comment-12792713"
        },
        {
            "date": "2009-12-20T02:09:27+0000",
            "content": "> But, that's where Lucy presumably takes a perf hit. Lucene can share\n> these in RAM, not usign the filesystem as the intermediary (eg we do\n> that today with deletions; norms/field cache/eventual CSF can do the\n> same.) Lucy must go through the filesystem to share.\n\nFor a flush(), I don't think there's a significant penalty.  The only extra\ncosts Lucy will pay are the bookkeeping costs to update the file system state\nand to create the objects that read the index data.  Those are real, but since\nwe're skipping the fsync(), they're small.  As far as the actual data, I don't\nsee that there's a difference.  Reading from memory mapped RAM isn't any\nslower than reading from malloc'd RAM.\n\nIf we have to fsync(), there'll be a cost, but in Lucene you have to pay that\nsame cost, too.  Lucene expects to get around it with IndexWriter.getReader().\nIn Lucy, we'll get around it by having you call flush() and then reopen a\nreader somewhere, often in another proecess.  \n\n\n\tIn both cases, the availability of fresh data is decoupled from the fsync.\n\tIn both cases, the indexing process has to be careful about dropping data\n    on the floor before a commit() succeeds.\n\tIn both cases, it's possible to protect against unbounded corruption by\n    rolling back to the last commit.\n\n\n\n> Mostly I was thinking performance, ie, trusting the OS to make good\n> decisions about what should be RAM resident, when it has limited\n> information...\n\nRight, for instance because we generally can't force the OS to pin term\ndictionaries in RAM, as discussed a while back.  It's not an ideal situation,\nbut Lucene's approach isn't bulletproof either, since Lucene's term\ndictionaries can get paged out too.  \n\nWe're sure not going to throw away all the advantages of mmap and go back to\nreading data structures into process RAM just because of that.\n\n> But, also risky is that all important data structures must be \"file-flat\",\n> though in practice that doesn't seem like an issue so far? \n\nIt's a constraint.  For instance, to support mmap, string sort caches\ncurrently require three \"files\" each: ords, offsets, and UTF-8 character data.  \n\nThe compound file system makes the file proliferation bearable, though.  And\nit's actually nice in a way to have data structures as named files, strongly\nseparated from each other and persistent.\n\nIf we were willing to ditch portability, we could cast to arrays of structs in\nLucy \u2013 but so far we've just used primitives.  I'd like to keep it that way,\nsince it would be nice if the core Lucy file format was at least theoretically\ncompatible with a pure Java implementation.  But Lucy plugins could break that\nrule and cast to structs if desired.  \n\n> The RAM resident things Lucene has - norms, deleted docs, terms index, field\n> cache - seem to \"cast\" just fine to file-flat. \n\nThere are often benefits to keeping stuff \"file-flat\", particularly when the\nfile-flat form is compressed.  If we were to expand those sort caches to\nstring objects, they'd take up more RAM than they do now.\n\nI think the only significant drawback is security: we can't trust memory\nmapped data the way we can data which has been read into process RAM and\nchecked on the way in.  For instance, we need to perform UTF-8 sanity checking\neach time a string sort cache value escapes the controlled environment of the\ncache reader.  If the sort cache value was instead derived from an existing\nstring in process RAM, we wouldn't need to check it.\n\n> If we switched to an FST for the terms index I guess that could get\n> tricky...\n\nHmm, I haven't been following that.  Too much work to keep up with those\ngiganto patches for flex indexing, even though it's a subject I'm intimately\nacquainted with and deeply interested in.  I plan to look it over when you're\ndone and see if we can simplify it.  \n\n> Wouldn't shared memory be possible for process-only concurrent models?\n\nIPC is a platform-compatibility nightmare.  By restricting ourselves to\ncommunicating via the file system, we save ourselves oodles of engineering\ntime.  And on really boring, frustrating work, to boot.\n\n> Also, what popular systems/environments have this requirement (only process\n> level concurrency) today?\n\nPerl's threads suck.  Actually all threads suck.  Perl's are just worse than\naverage \u2013 and so many Perl binaries are compiled without them.  Java threads\nsuck less, but they still suck \u2013 look how much engineering time you folks\nblow on managing that stuff.  Threads are a terrible programming model.\n\nI'm not into the idea of forcing Lucy users to use threads.  They should be\nable to use processes as their primary concurrency model if they want.\n\n> It's wonderful that Lucy can startup really fast, but, for most apps that's\n> not nearly as important as searching/indexing performance, right? \n\nDepends.  \n\nTotal indexing throughput in both Lucene and KinoSearch has been pretty decent\nfor a long time.  However, there's been a large gap between average index\nupdate performance and worst case index update performance, especially when\nyou factor in sort cache loading.  There are plenty of applications that may\nnot have very high throughput requirements but where it may not be acceptable\nfor an index update to take several seconds or several minutes every once in a\nwhile, even if it usually completes faster.\n\n> I mean, you start only once, and then you handle many, many\n> searches / index many documents, with that process, usually?\n\nSometimes the person who just performed the action that updated the index is\nthe only one you care about.  For instance, to use a feature request that came\nin from Slashdot a while back, if someone leaves a comment on your website,\nit's nice to have it available in the search index right away.\n\nConsistently fast index update responsiveness makes personalization of the\ncustomer experience easier.\n\n> But you really need to also test search/indexing throughput, reopen time\n> (I think) once that's online for Lucy...\n\nNaturally.\n\n> Is reopen even necessary in Lucy?\n\nProbably.  If you have a boatload of segments and a boatload of fields, you\nmight start to see file opening and metadata parsing costs come into play.  If\nit turns out that for some indexes reopen() can knock down the time from say,\n100 ms to 10 ms or less, I'd consider that sufficient justification. ",
            "author": "Marvin Humphrey",
            "id": "comment-12792939"
        },
        {
            "date": "2009-12-20T15:41:25+0000",
            "content": "\nBut, that's where Lucy presumably takes a perf hit. Lucene can share these in RAM, not usign the filesystem as the intermediary (eg we do that today with deletions; norms/field cache/eventual CSF can do the same.) Lucy must go through the filesystem to share.\n\nFor a flush(), I don't think there's a significant penalty. The only extra\ncosts Lucy will pay are the bookkeeping costs to update the file system state\nand to create the objects that read the index data. Those are real, but since\nwe're skipping the fsync(), they're small. As far as the actual data, I don't\nsee that there's a difference.\n\nBut everything must go through the filesystem with Lucy...\n\nEg, with Lucene, deletions are not written to disk until you commit.\nFlush doesn't write the del file, merging doesn't, etc.  The deletes\nare carried in RAM.  We could (but haven't yet \u2013 NRT turnaround time\nis already plenty fast) do the same with norms, field cache, terms\ndict index, etc.\n\n\nReading from memory mapped RAM isn't any slower than reading from malloc'd RAM.\n\nRight, for instance because we generally can't force the OS to pin term\ndictionaries in RAM, as discussed a while back. It's not an ideal situation,\nbut Lucene's approach isn't bulletproof either, since Lucene's term\ndictionaries can get paged out too.\n\nAs long as the page is hot... (in both cases!).\n\nBut by using file-backed RAM (not malloc'd RAM), you're telling the OS\nit's OK if it chooses to swap it out. Sure, malloc'd RAM can be\nswapped out too... but that should be less frequent (and, we can\ncontrol this behavior, somewhat, eg swappiness).\n\nIt's similar to using a weak v strong reference in java.  By using\nfile-backed RAM you tell the OS it's fair game for swapping.\n\n\nIf we have to fsync(), there'll be a cost, but in Lucene you have to pay that\nsame cost, too. Lucene expects to get around it with IndexWriter.getReader().\nIn Lucy, we'll get around it by having you call flush() and then reopen a\nreader somewhere, often in another proecess.\n\nIn both cases, the availability of fresh data is decoupled from the fsync.\nIn both cases, the indexing process has to be careful about dropping data\non the floor before a commit() succeeds.\nIn both cases, it's possible to protect against unbounded corruption by\nrolling back to the last commit.\n\nThe two approaches are basically the same, so, we get the same\nfeatures \n\nIt's just that Lucy uses the filesystem for sharing, and Lucene shares\nthrough RAM.\n\nWe're sure not going to throw away all the advantages of mmap and go back to reading data structures into process RAM just because of that.\n\nI guess my confusion is what are all the other benefits of using\nfile-backed RAM?  You can efficiently use process only concurrency\n(though shared memory is technically an option for this too), and you\nhave wicked fast open times (but, you still must warm, just like\nLucene).  What else?  Oh maybe the ability to inform OS not to cache\neg the reads done when merging segments.  That's one I sure wish\nLucene could use...\n\nIn exchange you risk the OS making poor choices about what gets\nswapped out (LRU policy is too simplistic... not all pages are created\nequal), must down cast all data structures to file-flat, must share\neverything through the filesystem, (perf hit to NRT).\n\nI do love how pure the file-backed RAM approach is, but I worry that\ndown the road it'll result in erratic search performance in certain\napp profiles.\n\n\nBut, also risky is that all important data structures must be \"file-flat\", though in practice that doesn't seem like an issue so far?\n\nIt's a constraint. For instance, to support mmap, string sort caches\ncurrently require three \"files\" each: ords, offsets, and UTF-8 character data.\n\nYeah, that you need 3 files for the string sort cache is a little\nspooky... that's 3X the chance of a page fault.\n\n\nThe compound file system makes the file proliferation bearable, though. And\nit's actually nice in a way to have data structures as named files, strongly\nseparated from each other and persistent.\n\nBut the CFS construction must also go through the filesystem (like\nLucene) right?  So you still incur IO load of creating the small\nfiles, then 2nd pass to consolidate.\n\nI agree there's a certain design purity to having the files clearly\nseparate out the elements of the data structures, but if it means\nerratic search performance... function over form?\n\n\nIf we were willing to ditch portability, we could cast to arrays of structs in\nLucy - but so far we've just used primitives. I'd like to keep it that way,\nsince it would be nice if the core Lucy file format was at least theoretically\ncompatible with a pure Java implementation. But Lucy plugins could break that\nrule and cast to structs if desired.\n\nSomeday we could make a Lucene codec that interacts with a Lucy\nindex... would be a good exercise to go though to see if the flex API\nreally is \"flex\" enough...\n\n\nThe RAM resident things Lucene has - norms, deleted docs, terms index, field cache - seem to \"cast\" just fine to file-flat.\n\nThere are often benefits to keeping stuff \"file-flat\", particularly when the\nfile-flat form is compressed. If we were to expand those sort caches to\nstring objects, they'd take up more RAM than they do now.\n\nWe've leaving them as UTF8 by default for Lucene (with the flex\nchanges).  Still, the terms index once loaded does have silly RAM\noverhead... we can cut that back a fair amount though.\n\n\nI think the only significant drawback is security: we can't trust memory\nmapped data the way we can data which has been read into process RAM and\nchecked on the way in. For instance, we need to perform UTF-8 sanity checking\neach time a string sort cache value escapes the controlled environment of the\ncache reader. If the sort cache value was instead derived from an existing\nstring in process RAM, we wouldn't need to check it.\n\nSigh, that's a curious downside... so term decode intensive uses\n(merging, range queries, I guess maybe term dict lookup) take the\nbrunt of that hit?\n\n\nIf we switched to an FST for the terms index I guess that could get tricky...\n\nHmm, I haven't been following that.\n\nThere's not much to follow \u2013 it's all just talk at this point.  I\ndon't think anyone's built a prototype yet \n\n\nToo much work to keep up with those giganto patches for flex indexing,\neven though it's a subject I'm intimately acquainted with and deeply\ninterested in. I plan to look it over when you're done and see if we\ncan simplify it.\n\nAnd then we'll borrow back your simplifications  Lather, rinse,\nrepeat.\n\n\nWouldn't shared memory be possible for process-only concurrent models?\n\nIPC is a platform-compatibility nightmare. By restricting ourselves to\ncommunicating via the file system, we save ourselves oodles of\nengineering time. And on really boring, frustrating work, to boot.\n\nI had assumed so too, but I was surprised that Python's\nmultiprocessing module exposes a simple API for sharing objects from\nparent to forked child.  It's at least a counter example (though, in\nall fairness, I haven't looked at the impl  ), ie, there seems to be\nsome hope of containing shared memory under a consistent API.\n\nI'm just pointing out that \"going through the filesystem\" isn't the\nonly way to have efficient process-only concurrency.  Shared memory\nis another option, but, yes it has tradeoffs.\n\n\n\nAlso, what popular systems/environments have this requirement (only process level concurrency) today?\n\nPerl's threads suck. Actually all threads suck. Perl's are just worse than\naverage - and so many Perl binaries are compiled without them. Java threads\nsuck less, but they still suck - look how much engineering time you folks\nblow on managing that stuff. Threads are a terrible programming model.\n\nI'm not into the idea of forcing Lucy users to use threads. They should be\nable to use processes as their primary concurrency model if they want.\n\nYes, working with threads is a nightmare (eg have a look at Java's\nmemory model).  I think the jury is still out (for our species) just\nhow, long term, we'll make use of concurrency with the machines.  I\nthink we may need to largely take \"time\" out of our programming\nlanguages, eg switch to much more declarative code, or\nsomething... wanna port Lucy to Erlang?\n\nBut I'm not sure process only concurrency, sharing only via\nfile-backed memory, is the answer either \n\n\nIt's wonderful that Lucy can startup really fast, but, for most apps that's not nearly as important as searching/indexing performance, right?\n\nDepends.\n\nTotal indexing throughput in both Lucene and KinoSearch has been pretty decent\nfor a long time. However, there's been a large gap between average index\nupdate performance and worst case index update performance, especially when\nyou factor in sort cache loading. There are plenty of applications that may\nnot have very high throughput requirements but where it may not be acceptable\nfor an index update to take several seconds or several minutes every once in a\nwhile, even if it usually completes faster.\n\nI mean, you start only once, and then you handle many, many searches / index many documents, with that process, usually?\n\nSometimes the person who just performed the action that updated the index is\nthe only one you care about. For instance, to use a feature request that came\nin from Slashdot a while back, if someone leaves a comment on your website,\nit's nice to have it available in the search index right away.\n\nConsistently fast index update responsiveness makes personalization of the\ncustomer experience easier.\n\nTurnaround time for Lucene NRT is already very fast, as is.  After an\nimmense merge, it'll be the worst, but if you warm the reader first,\nthat won't be an issue.\n\nUsing Zoie you can make reopen time insanely fast (much faster than I\nthink necessary for most apps), but at the expense of some expected\nhit to searching/indexing throughput.  I don't think that's the right\ntradeoff for Lucene.\n\nI suspect Lucy is making a similar tradeoff, ie, that search\nperformance will be erratic due to page faults, at a smallish gain in\nreopen time.\n\nDo you have any hard numbers on how much time it takes Lucene to load\nfrom a hot IO cache, populating its RAM resident data structures?  I\nwonder in practice what extra cost we are really talking about... it's\nRAM to RAM \"translation\" of data structures (if the files are hot).\nFieldCache we just have to fix to stop doing uninversion... (ie we\nneed CSF).\n\n\nIs reopen even necessary in Lucy?\n\nProbably. If you have a boatload of segments and a boatload of fields, you\nmight start to see file opening and metadata parsing costs come into play. If\nit turns out that for some indexes reopen() can knock down the time from say,\n100 ms to 10 ms or less, I'd consider that sufficient justification.\n\n\nOK.  Then, you are basically pooling your readers  Ie, you do allow\nin-process sharing, but only among readers. ",
            "author": "Michael McCandless",
            "id": "comment-12792996"
        },
        {
            "date": "2009-12-22T00:07:23+0000",
            "content": "> I guess my confusion is what are all the other benefits of using\n> file-backed RAM? You can efficiently use process only concurrency\n> (though shared memory is technically an option for this too), and you\n> have wicked fast open times (but, you still must warm, just like\n> Lucene). \n\nProcesses are Lucy's primary concurrency model.  (\"The OS is our JVM.\")\nMaking process-only concurrency efficient isn't optional \u2013 it's a core\nconcern.\n\n> What else? Oh maybe the ability to inform OS not to cache\n> eg the reads done when merging segments. That's one I sure wish\n> Lucene could use...\n\nLightweight searchers mean architectural freedom.  \n\nCreate 2, 10, 100, 1000 Searchers without a second thought \u2013 as many as you\nneed for whatever app architecture you just dreamed up \u2013 then destroy them\njust as effortlessly.  Add another worker thread to your search server without\nhaving to consider the RAM requirements of a heavy searcher object.  Create a\ncommand-line app to search a documentation index without worrying about\ndaemonizing it.  Etc.\n\nIf your normal development pattern is a single monolithic Java process, then\nthat freedom might not mean much to you.  But with their low per-object RAM\nrequirements and fast opens, lightweight searchers are easy to use within a\nlot of other development patterns. For example: lightweight searchers work \nwell for maxing out multiple CPU cores under process-only concurrency.\n\n> In exchange you risk the OS making poor choices about what gets\n> swapped out (LRU policy is too simplistic... not all pages are created\n> equal), \n\nThe Linux virtual memory system, at least, is not a pure LRU.  It utilizes a\npage aging algo which prioritizes pages that have historically been accessed\nfrequently even when they have not been accessed recently:\n\n\n    http://sunsite.nus.edu.sg/LDP/LDP/tlk/node40.html\n\n    The default action when a page is first allocated, is to give it an\n    initial age of 3. Each time it is touched (by the memory management\n    subsystem) it's age is increased by 3 to a maximum of 20. Each time the\n    Kernel swap daemon runs it ages pages, decrementing their age by 1.\n\n\nAnd while that system may not be ideal from our standpoint, it's still pretty\ngood.  In general, the operating system's virtual memory scheme is going to\nwork fine as designed, for us and everyone else, and minimize memory\navailability wait times.\n\nWhen will swapping out the term dictionary be a problem?  \n\n\n\tFor indexes where queries are made frequently, no problem.\n\tFoir systems with plenty of RAM, no problem.\n\tFor systems that aren't very busy, no problem.\n\tFor small indexes, no problem.\n\n\n\nThe only situation we're talking about is infrequent queries against large\nindexes on busy boxes where RAM isn't abundant.  Under those circumstances, it\nmight be noticable that Lucy's term dictionary gets paged out somewhat\nsooner than Lucene's.\n\nBut in general, if the term dictionary gets paged out, so what?  Nobody was\nusing it.  Maybe nobody will make another query against that index until next\nweek.  Maybe the OS made the right decision.\n\nOK, so there's a vulnerable bubble where the the query rate against \na large index an index is neither too fast nor too slow, on busy machines \nwhere RAM isn't abundant.  I don't think that bubble ought to drive major \narchitectural decisions.\n\nLet me turn your question on its head.  What does Lucene gain in return for\nthe slow index opens and large process memory footprint of its heavy\nsearchers?\n\n> I do love how pure the file-backed RAM approach is, but I worry that\n> down the road it'll result in erratic search performance in certain\n> app profiles.\n\nIf necessary, there's a straightforward remedy: slurp the relevant files into\nRAM at object construction rather than mmap them.  The rest of the code won't \nknow the difference between malloc'd RAM and mmap'd RAM.  The slurped files \nwon't take up any more space than the analogous Lucene data structures; more \nlikely, they'll take up less.\n\nThat's the kind of setting we'd hide away in the IndexManager class rather\nthan expose as prominent API, and it would be a hint to index components\nrather than an edict.\n\n> Yeah, that you need 3 files for the string sort cache is a little\n> spooky... that's 3X the chance of a page fault.\n\nNot when using the compound format.\n\n> But the CFS construction must also go through the filesystem (like\n> Lucene) right? So you still incur IO load of creating the small\n> files, then 2nd pass to consolidate.\n\nYes.\n\n> I think we may need to largely take \"time\" out of our programming\n> languages, eg switch to much more declarative code, or\n> something... wanna port Lucy to Erlang?\n> \n> But I'm not sure process only concurrency, sharing only via\n> file-backed memory, is the answer either\n\nI think relying heavily on file-backed memory is particularly appropriate for\nLucy because the write-once file format works well with MAP_SHARED memory\nsegments.  If files were being modified and had to be protected with\nsemaphores, it wouldn't be as sweet a match.\n\nFocusing on process-only concurrency also works well for Lucy because host\nthreading models differ substantially and so will only be accessible via a\ngeneralized interface from the Lucy C core.  It will be difficult to tune\nthreading performance through that layer of indirection \u2013 I'm guessing beyond\nthe ability of most developers since few will be experts in multiple host\nthreading models.  In contrast, expertise in process level concurrency will be\neasier to come by and to nourish.\n\n> Using Zoie you can make reopen time insanely fast (much faster than I\n> think necessary for most apps), but at the expense of some expected\n> hit to searching/indexing throughput. I don't think that's the right\n> tradeoff for Lucene.\n\nBut as Jake pointed out early in the thread, Zoie achieves those insanely fast\nreopens without tight coupling to IndexWriter and its components.  The\nauxiliary RAM index approach is well proven.\n\n> Do you have any hard numbers on how much time it takes Lucene to load\n> from a hot IO cache, populating its RAM resident data structures?\n\nHmm, I don't spend a lot of time working with Lucene directly, so I might not\nbe the person most likely to have data like that at my fingertips.  Maybe that\nMcCandless dude can help you out, he runs a lot of benchmarks.   \n\nOr maybe ask the Solr folks?  I see them on solr-user all the time talking \nabout \"MaxWarmingSearchers\". \n\n> OK. Then, you are basically pooling your readers  Ie, you do allow\n> in-process sharing, but only among readers.\n\nNot sure about that. Lucy's IndexReader.reopen() would open new SegReaders for\neach new segment, but they would be private to each parent PolyReader.  So if\nyou reopened two IndexReaders at the same time after e.g.  segment \"seg_12\"\nhad been added, each would create a new, private SegReader for \"seg_12\".\n\nEdit: updated to correct assertions about virtual memory performance with\nsmall indexes. ",
            "author": "Marvin Humphrey",
            "id": "comment-12793431"
        },
        {
            "date": "2009-12-22T19:08:41+0000",
            "content": "\n\nProcesses are Lucy's primary concurrency model. (\"The OS is our JVM.\")\nMaking process-only concurrency efficient isn't optional - it's a core\nconcern.\n\nOK\n\n\nLightweight searchers mean architectural freedom.\n\nCreate 2, 10, 100, 1000 Searchers without a second thought - as many as you\nneed for whatever app architecture you just dreamed up - then destroy them\njust as effortlessly. Add another worker thread to your search server without\nhaving to consider the RAM requirements of a heavy searcher object. Create a\ncommand-line app to search a documentation index without worrying about\ndaemonizing it. Etc.\n\nThis is definitely neat.\n\n\nThe Linux virtual memory system, at least, is not a pure LRU. It utilizes a\npage aging algo which prioritizes pages that have historically been accessed\nfrequently even when they have not been accessed recently:\n\nhttp://sunsite.nus.edu.sg/LDP/LDP/tlk/node40.html\n\nVery interesting \u2013 thanks.  So it also factors in how much the page\nwas used in the past, not just how long it's been since the page was\nlast used.\n\n\nWhen will swapping out the term dictionary be a problem?\n\nFor indexes where queries are made frequently, no problem.\nFoir systems with plenty of RAM, no problem.\nFor systems that aren't very busy, no problem.\nFor small indexes, no problem.\nThe only situation we're talking about is infrequent queries against large\nindexes on busy boxes where RAM isn't abundant. Under those circumstances, it\nmight be noticable that Lucy's term dictionary gets paged out somewhat\nsooner than Lucene's.\n\nEven smallish indexes can see the pages swapped out?  I'd think at\nlow-to-moderate search traffic, any index could be at risk, depdending\non whether other stuff in the machine wanting RAM or IO cache is\nrunning.\n\n\nBut in general, if the term dictionary gets paged out, so what? Nobody was\nusing it. Maybe nobody will make another query against that index until next\nweek. Maybe the OS made the right decision.\n\nYou can't afford many page faults until the latency becomes very\napparent (until we're all on SSDs... at which point this may all be\nmoot).\n\nRight \u2013 the metric that the swapper optimizes is overall efficient\nuse of the machine's resources.\n\nBut I think that's often a poor metric for search apps... I think\nconsistency on the search latency is more important, though I agree it\ndepends very much on the app.\n\nI don't like the same behavior in my desktop \u2013 when I switch to my\nmail client, I don't want to wait 10 seconds for it to swap the pages\nback in.\n\n\nLet me turn your question on its head. What does Lucene gain in return for\nthe slow index opens and large process memory footprint of its heavy\nsearchers?\n\nConsistency in the search time.  Assuming the OS doesn't swap our\npages out...\n\nAnd of course Java pretty much forces threads-as-concurrency (JVM\nstartup time, hotspot compilation, are costly).\n\n\nIf necessary, there's a straightforward remedy: slurp the relevant files into\nRAM at object construction rather than mmap them. The rest of the code won't \nknow the difference between malloc'd RAM and mmap'd RAM. The slurped files \nwon't take up any more space than the analogous Lucene data structures; more \nlikely, they'll take up less.\n\nThat's the kind of setting we'd hide away in the IndexManager class rather\nthan expose as prominent API, and it would be a hint to index components\nrather than an edict.\n\nRight, this is how Lucy would force warming.\n\n\nYeah, that you need 3 files for the string sort cache is a little spooky... that's 3X the chance of a page fault.\n\nNot when using the compound format.\n\nBut, even within that CFS file, these three sub-files will not be\nlocal?  Ie you'll still have to hit three pages per \"lookup\" right?\n\n\nI think relying heavily on file-backed memory is particularly appropriate for\nLucy because the write-once file format works well with MAP_SHARED memory\nsegments. If files were being modified and had to be protected with\nsemaphores, it wouldn't be as sweet a match.  \n\nWrite-once is good for Lucene too.\n\n\nFocusing on process-only concurrency also works well for Lucy because host\nthreading models differ substantially and so will only be accessible via a\ngeneralized interface from the Lucy C core. It will be difficult to tune\nthreading performance through that layer of indirection - I'm guessing beyond\nthe ability of most developers since few will be experts in multiple host\nthreading models. In contrast, expertise in process level concurrency will be\neasier to come by and to nourish.\n\nI'm confused by this \u2013 eg Python does a great job presenting a simple\nthreads interface and implementing it on major OSs.  And it seems like\nLucy would not need anything crazy-os-specific wrt threads?\n\n\nDo you have any hard numbers on how much time it takes Lucene to load from a hot IO cache, populating its RAM resident data structures?\n\nHmm, I don't spend a lot of time working with Lucene directly, so I might not\nbe the person most likely to have data like that at my fingertips. Maybe that\nMcCandless dude can help you out, he runs a lot of benchmarks.  \n\nHmm  I'd guess that field cache is slowish; deleted docs & norms are\nvery fast; terms index is somewhere in between.\n\nOr maybe ask the Solr folks? I see them on solr-user all the time talking about \"MaxWarmingSearchers\". \n\nHmm \u2013 not sure what's up with that.  Looks like maybe it's the\nauto-warming that might happen after a commit.\n\n\nOK. Then, you are basically pooling your readers Ie, you do allow in-process sharing, but only among readers.\n\nNot sure about that. Lucy's IndexReader.reopen() would open new SegReaders for\neach new segment, but they would be private to each parent PolyReader. So if\nyou reopened two IndexReaders at the same time after e.g. segment \"seg_12\"\nhad been added, each would create a new, private SegReader for \"seg_12\".\n\nYou're right, you'd get two readers for seg_12 in that case.  By\n\"pool\" I meant you're tapping into all the sub-readers that the\nexisting reader have opened \u2013 the reader is your pool of sub-readers. ",
            "author": "Michael McCandless",
            "id": "comment-12793737"
        },
        {
            "date": "2009-12-23T03:59:23+0000",
            "content": "> Very interesting - thanks. So it also factors in how much the page\n> was used in the past, not just how long it's been since the page was\n> last used.\n\nIn theory, I think that means the term dictionary will tend to be favored over\nthe posting lists.  In practice... hard to say, it would be difficult to test.\n\n\n> Even smallish indexes can see the pages swapped out? \n\nYes, you're right \u2013 the wait time to get at a small term dictionary isn't\nnecessarily small.  I've amended my previous post, thanks.\n\n> And of course Java pretty much forces threads-as-concurrency (JVM\n> startup time, hotspot compilation, are costly).\n\nYes.  Java does a lot of stuff that most operating systems can also do, but of\ncourse provides a coherent platform-independent interface.  In Lucy we're\ngoing to try to go back to the OS for some of the stuff that Java likes to\ntake over \u2013 provided that we can develop a sane genericized interface using\nconfiguration probing and #ifdefs.  \n\nIt's nice that as long as the box is up our OS-as-JVM is always running, so we\ndon't have to worry about its (quite lengthy) startup time. \n\n> Right, this is how Lucy would force warming.\n\nI think slurp-instead-of-mmap is orthogonal to warming, because we can warm\nfile-backed RAM structures by forcing them into the IO cache, using either the\ncat-to-dev-null trick or something more sophisticated.  The\nslurp-instead-of-mmap setting would cause warming as a side effect, but the\nmain point would be to attempt to persuade the virtual memory system that\ncertain data structures should have a higher status and not be paged out as\nquickly.\n\n> But, even within that CFS file, these three sub-files will not be\n> local? Ie you'll still have to hit three pages per \"lookup\" right?\n\nThey'll be next to each other in the compound file because CompoundFileWriter\norders them alphabetically.  For big segments, though, you're right that they\nwon't be right next to each other, and you could possibly incur as many as\nthree page faults when retrieving a sort cache value.\n\nBut what are the alternatives for variable width data like strings?  You need\nthe ords array anyway for efficient comparisons, so what's left are the\noffsets array and the character data.\n\nAn array of String objects isn't going to have better locality than one solid\nblock of memory dedicated to offsets and another solid block of memory\ndedicated to file data, and it's no fewer derefs even if the string object\nstores its character data inline \u2013 more if it points to a separate allocation\n(like Lucy's CharBuf does, since it's mutable). \n\nFor each sort cache value lookup, you're going to need to access two blocks of\nmemory.  \n\n\n\tWith the array of String objects, the first is the memory block dedicated\n    to the array, and the second is the memory block dedicated to the String\n    object itself, which contains the character data.\n\tWith the file-backed block sort cache, the first memory block is the\n    offsets array, and the second is the character data array.\n\n\n\nI think the locality costs should be approximately the same... have I missed \nanything?\n\n> Write-once is good for Lucene too.\n\nHellyeah.\n\n> And it seems like Lucy would not need anything crazy-os-specific wrt\n> threads?\n\nIt depends on how many classes we want to make thread-safe, and it's not just\nthe OS, it's the host.\n\nThe bare minimum is simply to make Lucy thread-safe as a library.  That's\npretty close, because Lucy studiously avoided global variables whenever\npossible.  The only problems that have to be addressed are the VTable_registry\nHash, race conditions when creating new subclasses via dynamic VTable\nsingletons, and refcounts on the VTable objects themselves.\n\nOnce those issues are taken care of, you'll be able to use Lucy objects in\nseparate threads with no problem, e.g. one Searcher per thread.\n\nHowever, if you want to share Lucy objects (other than VTables) across\nthreads, all of a sudden we have to start thinking about \"synchronized\",\n\"volatile\", etc.  Such constructs may not be efficient or even possible under\nsome threading models.\n\n> Hmm I'd guess that field cache is slowish; deleted docs & norms are\n> very fast; terms index is somewhere in between.\n\nThat jibes with my own experience.  So maybe consider file-backed sort caches\nin Lucene, while keeping the status quo for everything else?\n\n> You're right, you'd get two readers for seg_12 in that case. By\n> \"pool\" I meant you're tapping into all the sub-readers that the\n> existing reader have opened - the reader is your pool of sub-readers.\n\nEach unique SegReader will also have dedicated \"sub-reader\" objects: two\n\"seg_12\" SegReaders means two \"seg_12\" DocReaders, two \"seg_12\"\nPostingsReaders, etc.  However, all those sub-readers will share the same\nfile-backed RAM data, so in that sense they're pooled. ",
            "author": "Marvin Humphrey",
            "id": "comment-12793918"
        },
        {
            "date": "2009-12-23T16:26:58+0000",
            "content": "\nVery interesting - thanks. So it also factors in how much the page was used in the past, not just how long it's been since the page was  last used.\n\nIn theory, I think that means the term dictionary will tend to be\nfavored over the posting lists. In practice... hard to say, it would\nbe difficult to test.\n\nRight... though, I think the top \"trunks\" frequently used by the\nbinary search, will stay hot.  But as you get deeper into the terms\nindex, it's not as clear.\n\n\nAnd of course Java pretty much forces threads-as-concurrency (JVM startup time, hotspot compilation, are costly).\n\nYes. Java does a lot of stuff that most operating systems can also do, but of\ncourse provides a coherent platform-independent interface. In Lucy we're\ngoing to try to go back to the OS for some of the stuff that Java likes to\ntake over - provided that we can develop a sane genericized interface using\nconfiguration probing and #ifdefs.\n\nIt's nice that as long as the box is up our OS-as-JVM is always running, so we\ndon't have to worry about its (quite lengthy) startup time.\n\nOS as JVM is a nice analogy.  Java of course gets in the way, too,\nlike we cannot properly set IO priorities, we can't give hints to the\nOS to tell it not to cache certain reads/writes (ie segment merging),\ncan't pin pages , etc.\n\n\nRight, this is how Lucy would force warming.\n\nI think slurp-instead-of-mmap is orthogonal to warming, because we can warm\nfile-backed RAM structures by forcing them into the IO cache, using either the\ncat-to-dev-null trick or something more sophisticated. The\nslurp-instead-of-mmap setting would cause warming as a side effect, but the\nmain point would be to attempt to persuade the virtual memory system that\ncertain data structures should have a higher status and not be paged out as\nquickly.\n\nWoops, sorry, I misread \u2013 now I understand.  You can easily make\ncertain files ram resident, and then be like Lucene (except the data\nstructures are more compact).  Nice.\n\n\nBut, even within that CFS file, these three sub-files will not be local? Ie you'll still have to hit three pages per \"lookup\" right?\n\nThey'll be next to each other in the compound file because CompoundFileWriter\norders them alphabetically. For big segments, though, you're right that they\nwon't be right next to each other, and you could possibly incur as many as\nthree page faults when retrieving a sort cache value.\n\nBut what are the alternatives for variable width data like strings? You need\nthe ords array anyway for efficient comparisons, so what's left are the\noffsets array and the character data.\n\nAn array of String objects isn't going to have better locality than one solid\nblock of memory dedicated to offsets and another solid block of memory\ndedicated to file data, and it's no fewer derefs even if the string object\nstores its character data inline - more if it points to a separate allocation\n(like Lucy's CharBuf does, since it's mutable).\n\nFor each sort cache value lookup, you're going to need to access two blocks of\nmemory.\n\nWith the array of String objects, the first is the memory block dedicated\nto the array, and the second is the memory block dedicated to the String\nobject itself, which contains the character data.\nWith the file-backed block sort cache, the first memory block is the\noffsets array, and the second is the character data array.\nI think the locality costs should be approximately the same... have I missed \nanything?\n\nYou're right, Lucene risks 3 (ord array, String array, String object)\npage faults on each lookup as well.\n\nActually why can't ord & offset be one, for the string sort cache?\nIe, if you write your string data in sort order, then the offsets are\nalso in sort order?  (I think we may have discussed this already?)\n\n\nAnd it seems like Lucy would not need anything crazy-os-specific wrt threads?\n\nIt depends on how many classes we want to make thread-safe, and it's not just\nthe OS, it's the host.\n\nThe bare minimum is simply to make Lucy thread-safe as a library. That's\npretty close, because Lucy studiously avoided global variables whenever\npossible. The only problems that have to be addressed are the VTable_registry\nHash, race conditions when creating new subclasses via dynamic VTable\nsingletons, and refcounts on the VTable objects themselves.\n\nOnce those issues are taken care of, you'll be able to use Lucy objects in\nseparate threads with no problem, e.g. one Searcher per thread.\n\nHowever, if you want to share Lucy objects (other than VTables) across\nthreads, all of a sudden we have to start thinking about \"synchronized\",\n\"volatile\", etc. Such constructs may not be efficient or even possible under\nsome threading models.\n\nOK it is indeed hairy.  You don't want to have to create Lucy's\nequivalent of the JMM...\n\n\nHmm I'd guess that field cache is slowish; deleted docs & norms are very fast; terms index is somewhere in between.\n\nThat jibes with my own experience. So maybe consider file-backed sort caches\nin Lucene, while keeping the status quo for everything else?\n\nPerhaps, but it'd still make me nervous   When we get\nCSF (LUCENE-1231) online we should make it\npluggable enough so that one could create an mmap impl.\n\n\nYou're right, you'd get two readers for seg_12 in that case. By \"pool\" I meant you're tapping into all the sub-readers that the existing reader have opened - the reader is your pool of sub-readers.\n\nEach unique SegReader will also have dedicated \"sub-reader\" objects: two\n\"seg_12\" SegReaders means two \"seg_12\" DocReaders, two \"seg_12\"\nPostingsReaders, etc. However, all those sub-readers will share the same\nfile-backed RAM data, so in that sense they're pooled.\n\nOK ",
            "author": "Michael McCandless",
            "id": "comment-12794095"
        },
        {
            "date": "2009-12-23T18:26:18+0000",
            "content": "> we can't give hints to the OS to tell it not to cache certain reads/writes\n> (ie segment merging), \n\nFor what it's worth, we haven't really solved that problem in Lucy either.\nThe sliding window abstraction we wrapped around mmap/MapViewOfFile largely\nsolved the problem of running out of address space on 32-bit operating\nsystems.  However, there's currently no way to invoke madvise through Lucy's\nIO abstraction layer \u2013 it's a little tricky with compound files.  \n\nLinux, at least, requires that the buffer supplied to madvise be page-aligned.\nSo, say we're starting off on a posting list, and we want to communicate to\nthe OS that it should treat the region we're about to read as MADV_SEQUENTIAL.\nIf the start of the postings file is in the middle of a 4k page and the file\nright before it is a term dictionary, we don't want to indicate that that\nregion should be treated as sequential.\n\nI'm not sure how to solve that problem without violating the encapsulation of\nthe compound file model.  Hmm, maybe we could store metadata about the virtual\nfiles indicating usage patterns (sequential, random, etc.)?  Since files are\ngenerally part of dedicated data structures whose usage patterns are known at \nindex time.\n\nOr maybe we just punt on that use case and worry only about segment merging.  \nHmm, wouldn't the act of deleting a file (and releasing all file descriptors) tell\nthe OS that it's free to recycle any memory pages associated with it?\n\n> Actually why can't ord & offset be one, for the string sort cache?\n> Ie, if you write your string data in sort order, then the offsets are\n> also in sort order? (I think we may have discussed this already?)\n\nRight, we discussed this on lucy-dev last spring:\n\n    http://markmail.org/message/epc56okapbgit5lw\n\nIncidentally, some of this thread replays our exchange at the top of\nLUCENE-1458 from a year ago.  It was fun to go back and reread that: in the\ninterrim, we've implemented segment-centric search and memory mapped field\ncaches and term dictionaries, both of which were first discussed back then.\n\n\nOrds are great for low cardinality fields of all kinds, but become less\nefficient for high cardinality primitive numeric fields.  For simplicity's\nsake, the prototype implementation of mmap'd field caches in KS always uses\nords.\n\n> You don't want to have to create Lucy's equivalent of the JMM...\n\nThe more I think about making Lucy classes thread safe, the harder it seems.\n  I'd like to make it possible to share a Schema across threads, for\ninstance, but that means all its Analyzers, etc have to be thread-safe as\nwell, which isn't practical when you start getting into contributed\nsubclasses.  \n\nEven if we succeed in getting Folders and FileHandles thread safe, it will be\nhard for the user to keep track of what they can and can't do across threads.\n\"Don't share anything\" is a lot easier to understand.\n\nWe reap a big benefit by making Lucy's metaclass infrastructure thread-safe.\nBeyond that, seems like there's a lot of pain for little gain. ",
            "author": "Marvin Humphrey",
            "id": "comment-12794137"
        },
        {
            "date": "2009-12-23T19:08:20+0000",
            "content": "\nFor what it's worth, we haven't really solved that problem in Lucy either.\nThe sliding window abstraction we wrapped around mmap/MapViewOfFile largely\nsolved the problem of running out of address space on 32-bit operating\nsystems. However, there's currently no way to invoke madvise through Lucy's\nIO abstraction layer - it's a little tricky with compound files.\n\nLinux, at least, requires that the buffer supplied to madvise be page-aligned.\nSo, say we're starting off on a posting list, and we want to communicate to\nthe OS that it should treat the region we're about to read as MADV_SEQUENTIAL.\nIf the start of the postings file is in the middle of a 4k page and the file\nright before it is a term dictionary, we don't want to indicate that that\nregion should be treated as sequential.\n\nI'm not sure how to solve that problem without violating the encapsulation of\nthe compound file model. Hmm, maybe we could store metadata about the virtual\nfiles indicating usage patterns (sequential, random, etc.)? Since files are\ngenerally part of dedicated data structures whose usage patterns are known at \nindex time.\n\nOr maybe we just punt on that use case and worry only about segment merging. \n\nStoring metadata seems OK.  It'd be optional for codecs to declare that...\n\n\nHmm, wouldn't the act of deleting a file (and releasing all file descriptors) tell\nthe OS that it's free to recycle any memory pages associated with it?\n\nIt better!\n\n\nActually why can't ord & offset be one, for the string sort cache? Ie, if you write your string data in sort order, then the offsets are also in sort order? (I think we may have discussed this already?)\n\nRight, we discussed this on lucy-dev last spring:\n\nhttp://markmail.org/message/epc56okapbgit5lw\n\nOK I'll go try to catch up... but I'm about to drop [sort of]\noffline for a week and a half!  There's alot of reading there!  Should\nbe a prereq that we first go back and re-read what we said \"the last\ntime\"... \n\n\nIncidentally, some of this thread replays our exchange at the top of\nLUCENE-1458 from a year ago. It was fun to go back and reread that: in the\ninterrim, we've implemented segment-centric search and memory mapped field\ncaches and term dictionaries, both of which were first discussed back then.\n\nNice!  \n\n\nOrds are great for low cardinality fields of all kinds, but become less\nefficient for high cardinality primitive numeric fields. For simplicity's\nsake, the prototype implementation of mmap'd field caches in KS always uses\nords.\n\nRight...\n\n\nYou don't want to have to create Lucy's equivalent of the JMM...\n\nThe more I think about making Lucy classes thread safe, the harder it seems.\n I'd like to make it possible to share a Schema across threads, for\ninstance, but that means all its Analyzers, etc have to be thread-safe as\nwell, which isn't practical when you start getting into contributed\nsubclasses.\n\nEven if we succeed in getting Folders and FileHandles thread safe, it will be\nhard for the user to keep track of what they can and can't do across threads.\n\"Don't share anything\" is a lot easier to understand.\n\nWe reap a big benefit by making Lucy's metaclass infrastructure thread-safe.\nBeyond that, seems like there's a lot of pain for little gain.\n\nYeah.  Threads are not easy  ",
            "author": "Michael McCandless",
            "id": "comment-12794161"
        },
        {
            "date": "2012-04-02T21:53:14+0000",
            "content": "Hi,\n\nI am a Computer Science student from Germany. I would like to contribute to this project under GSoC 2012. I have very good experience in Java. I have some questions to this project, can someone help me? IRC or instant messanger? \n\nThank You\nTim ",
            "author": "Tim A.",
            "id": "comment-13244701"
        },
        {
            "date": "2012-04-03T10:57:22+0000",
            "content": "Is there anyone who can volunteer to be a mentor for this issue...? ",
            "author": "Michael McCandless",
            "id": "comment-13245125"
        },
        {
            "date": "2012-04-03T12:19:52+0000",
            "content": "I would but I am so overloaded with other work right now. I can be the primary mentor if you could help when I am totally blocked. \n\nHi Tim, as we are in the Apache Foundation and a open source project we make everything public. So if you have questions please go and start a thread on the dev@l.a.o mailing list and I am happy to help you. For GSoC internal or private issues while GSoC is running we can do private communication.\n\nsimon ",
            "author": "Simon Willnauer",
            "id": "comment-13245257"
        },
        {
            "date": "2012-04-03T15:21:48+0000",
            "content": "Hello Michael, hello Simon,\n\nthanks for the fast response. \n\nSo if you have questions please go and start a thread on the dev@l.a.o [...]\nOkay, I do this and start a thread. I have some special questions to the task (Refactoring IndexWriter).\n\nFor example: \n1. Exist unit tests for the code (IndexWriter.java)? \n2. Where i can find the code/software btw. component? (svn, git etc.)\n3. Which IDE I can use for this project? Your Suggestion (Eclipse)? \n4. What's about coding style guides?\n5. [...] ",
            "author": "Tim A.",
            "id": "comment-13245395"
        },
        {
            "date": "2012-12-31T19:04:07+0000",
            "content": "Been a long time since this has seen action - pushing out of 4.1. ",
            "author": "Mark Miller",
            "id": "comment-13541460"
        },
        {
            "date": "2013-07-23T18:44:18+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13716909"
        },
        {
            "date": "2014-04-16T12:54:56+0000",
            "content": "Move issue to Lucene 4.9. ",
            "author": "Uwe Schindler",
            "id": "comment-13970928"
        },
        {
            "date": "2016-03-13T22:18:58+0000",
            "content": "I would like to apply this issue as aGSoC project if someone is volunteer for being a mentor. ",
            "author": "Furkan KAMACI",
            "id": "comment-15192552"
        }
    ]
}