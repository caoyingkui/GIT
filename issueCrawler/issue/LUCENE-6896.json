{
    "id": "LUCENE-6896",
    "title": "Fix/document various Similarity bugs around extreme norm values",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [
            "5.5",
            "6.0"
        ],
        "priority": "Major",
        "status": "Closed",
        "type": "Bug"
    },
    "description": "Spinoff from LUCENE-6818:\n\nAhmet Arslan found problems with every Similarity (except ClassicSimilarity) when trying to test how they behave on every possible norm value, to ensure they are robust for all index-time boosts.\n\nThere are several problems:\n1. buggy normalization decode that causes the smallest possible norm value (0) to be treated as an infinitely long document. These values are intended to be encoded as non-negative finite values, but going to infinity breaks everything.\n2. various problems in the less practical functions that already have documented warnings that they do bad things for extreme values. These impact DFR models D, Be, and P and IB distribution SPL.",
    "attachments": {
        "LUCENE-6896.patch": "https://issues.apache.org/jira/secure/attachment/12772406/LUCENE-6896.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15005915",
            "author": "Robert Muir",
            "date": "2015-11-15T14:45:40+0000",
            "content": "Patch:\n\n\n\tadds tests that normalization decode is well behaved: only finite non-negative values, with increasing boost / decreasing length.\n\tfixes normalization decode for BM25Similarity and SimilarityBase to adhere to this, by setting len[0] = 1 / len[255] (5.6493154E19) instead of going to infinity.\n\tadds a delta to DFR model P to avoid going infinite when normalized tf approaches zero.\n\tadds/elaborates on warnings for the other 3 models and adds TODOs: maybe there is a similar simple fix for those.\n\n "
        },
        {
            "id": "comment-15007056",
            "author": "Michael McCandless",
            "date": "2015-11-16T18:24:56+0000",
            "content": "+1 to not divide by 0! "
        },
        {
            "id": "comment-15007103",
            "author": "Adrien Grand",
            "date": "2015-11-16T18:50:06+0000",
            "content": "+1\n\nI'm curious what the reasoning is for\n\nNORM_TABLE[0] = 1.0f / NORM_TABLE[255];\n\n\nIs it just a way to get a high float value that would be unlikely to overflow to Infinity (eg. when multiplied) or is it more than that? "
        },
        {
            "id": "comment-15007121",
            "author": "Robert Muir",
            "date": "2015-11-16T18:56:39+0000",
            "content": "Its completely arbitrary. But setting largest value to inverse of the smallest value does not seem too surprising. "
        },
        {
            "id": "comment-15008285",
            "author": "Adrien Grand",
            "date": "2015-11-17T08:43:35+0000",
            "content": "That works for me, thanks for the explanation. "
        },
        {
            "id": "comment-15104896",
            "author": "ASF subversion and git services",
            "date": "2016-01-18T07:48:25+0000",
            "content": "Commit 1725178 from Robert Muir in branch 'dev/trunk'\n[ https://svn.apache.org/r1725178 ]\n\nLUCENE-6896: don't treat smallest possible norm value as an infinitely long doc in SimilarityBase or BM25Similarity "
        },
        {
            "id": "comment-15104905",
            "author": "ASF subversion and git services",
            "date": "2016-01-18T08:08:07+0000",
            "content": "Commit 1725181 from Robert Muir in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1725181 ]\n\nLUCENE-6896: don't treat smallest possible norm value as an infinitely long doc in SimilarityBase or BM25Similarity "
        },
        {
            "id": "comment-15104906",
            "author": "Robert Muir",
            "date": "2016-01-18T08:09:33+0000",
            "content": "I committed the fixes and tests. I did discard my changes (delta) to model P after some investigation, as it does not fix all P's problems with abnormal TF values. "
        }
    ]
}