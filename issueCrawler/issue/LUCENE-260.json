{
    "id": "LUCENE-260",
    "title": "Possible Memory leak in Sort",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/search"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "1.4",
        "resolution": "Incomplete",
        "status": "Closed"
    },
    "description": "Lucene 1.4.1\nTomcat 5.0.27\nSDK's IBM 1.4.2, and Sun 1.4.2_05-b04\nOS Fedora Core2\n\nUsing the sort feature within my search application quickly uses up the 1000MB\nJava Heap I have assigned to Tomcat 5.0.27 and kills the server.\nhits = searcher.search(query,new Sort(new SortField(\"byNumber\",3)));\n\n\nNot sorting works fine\n\nHappens: always\n\n\nJohn",
    "attachments": {
        "ASF.LICENSE.NOT.GRANTED--RTERes.java": "https://issues.apache.org/jira/secure/attachment/12312375/ASF.LICENSE.NOT.GRANTED--RTERes.java"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2004-08-13T03:14:26+0000",
            "content": "The use of Tomcat always makes me suspicious.\n\nCan you reproduce this problem outside Tomcat, using a stand-alone application\n(class with main())? ",
            "author": "Otis Gospodnetic",
            "id": "comment-12321819"
        },
        {
            "date": "2004-08-13T20:00:57+0000",
            "content": "The following sort code gets to iterate 37 times (j=37) before an out of memory\nerror occurs. \n\npackage ie_rte_search_test;\n\nimport org.apache.lucene.analysis.standard.StandardAnalyzer;\nimport org.apache.lucene.analysis.*;\nimport org.apache.lucene.document.*;\nimport org.apache.lucene.index.*;\nimport org.apache.lucene.search.*;\nimport org.apache.lucene.queryParser.*;\n\npublic class RTERes {\n\n\n\n\n    public static void main(String[] args) {\n        for (int j=0; j<500; j++){\n            System.out.println(j);\n\n            boolean error = false;                  //used to control flow for\nerror messages\n            String indexName = \"/opt/lucene/index2\";       //local copy of the\nconfiguration variable\n            IndexSearcher searcher = null;          //the searcher used to\nopen/search the index\n            Query query = null;                     //the Query created by the\nQueryParser\n            Hits hits = null;                       //the search results\n            int startindex = 0;                     //the first index displayed\non this page\n            int maxpage    = 10;                    //the maximum items\ndisplayed on this page\n            String queryString = \"RTE\";              //the query entered in the\nprevious page\n            int maxresults  = 10;              \t\t//string version of maxpage\n            int thispage = 0;                       //used for the for/next\neither maxpage or\n            String SortDate = \"true\";\n\n            try \n{\n                searcher = new IndexSearcher(IndexReader.open(indexName));    \n//create an indexSearcher for our page\n                \n            }\n            catch (Exception e) \n{\n                System.out.println(e.getMessage());\n            }\n            //did we open the index?\n\n            Analyzer analyzer = new StopAnalyzer();               //construct\nour usual analyzer\n            try {\n\n                String[] fields = \n{\"contents\", \"level\", \"title\"}\n;\n                int[] flags = \n{MultiFieldQueryParser.NORMAL_FIELD,\nMultiFieldQueryParser.NORMAL_FIELD, MultiFieldQueryParser.NORMAL_FIELD}\n;\n\n                query = MultiFieldQueryParser.parse(queryString, fields, flags,\nanalyzer);\n                query = query.rewrite(IndexReader.open(indexName));\n\n\n\n                hits = searcher.search(query,new Sort(new SortField(\"byNumber\",3)));\n                //hits = searcher.search(query);\n\n            }\n            catch (Exception e) \n{System.out.println(e.getMessage());};\n            \n            for (int i = 0; i < 10; i++) {  // for each element\n                \n                try{\n                    Document doc = hits.doc(i);                    //get the\nnext document \n                    //System.out.println(doc.get(\"url\"));\n                    //System.out.println(doc.get(\"title\"));\n                    //System.out.println(doc.get(\"date\"));\n                    //System.out.println(doc.get(\"contents\"));      \n                    searcher.close();\n                    \n                }catch (Exception e) {System.out.println(e.getMessage());}\n;\n\n            }\n        }\n    }\n} ",
            "author": "John Moylan",
            "id": "comment-12321820"
        },
        {
            "date": "2004-08-13T20:11:51+0000",
            "content": "Created an attachment (id=12418)\nfile used for testing ",
            "author": "John Moylan",
            "id": "comment-12321821"
        },
        {
            "date": "2004-08-14T02:03:04+0000",
            "content": "Does it also happen with Lucene 1.4.1? Something has been changed with respect \nto sorting in that version. Also, are you aware that you close the searcher \ninside the loop? Do you get any exception before you see the out of memory \nerror? I tried your code but I cannot reproduce the problem here. Of course my \nindex is different to yours. \n ",
            "author": "Daniel Naber",
            "id": "comment-12321822"
        },
        {
            "date": "2004-08-14T04:08:58+0000",
            "content": "That code looks wrong.  You are creating your IndexSearcher inside a for loop,\nwhich results in 500 instances of IndexSearcher.  That is not what you wanted to\ndo, is it?  You should re-use the IndexSearcher for the same index.\nYes, you do close your IndexSearcher, but as Daniel pointed out, that too looks\nsuspicious, as you close that inside an inner for loop.\n\nBecause of this, I'll mark this bug as invalid.  However, there were some issues\nwith Sort in Lucene 1.4 (yes, you are using 1.4.1), so please re-open if you\nfind that you run out of memory even when you re-use IndexSearcher. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12321823"
        },
        {
            "date": "2004-08-16T16:15:30+0000",
            "content": "Created an attachment (id=12440)\namended test file ",
            "author": "John Moylan",
            "id": "comment-12321824"
        },
        {
            "date": "2004-08-16T16:22:08+0000",
            "content": "True, in my haste to release the test code, I closed the searcher during the\ninner loop instead of during the other loop. I have amended this is the newly\nattached file.\n\nMy origional code was written as a JSP on Tomcat and tested with Jmeter using 2\nthreads requesting a simple query and sort. The outer for loop is to mimick the\ntype of request pattern on the jsp code so that I can compare like with like (ie\n500 requests in rapid succession) With sort I get an OutOfMemoryError after 37\nrequests, without a sort everything works fine for-ever. No other exceptions occur.\n ",
            "author": "John Moylan",
            "id": "comment-12321825"
        },
        {
            "date": "2004-08-16T16:36:50+0000",
            "content": "John, you still open a new IndexSearcher per loop. That doesn't make sense, \nopen one searcher and use it for all of your queries. If that doesn't change \nanything, please try Lucene 1.4.1 and tell us how bug your index is and what \nfields it consists of.  ",
            "author": "Daniel Naber",
            "id": "comment-12321826"
        },
        {
            "date": "2004-08-16T16:59:39+0000",
            "content": "OK, I have just tested using one searcher outside the outer for loop and the\ncode seems to run indefinitely without any issues. So I guess the problem is not\nquite as simple as I had first thought - as the new test proves that sort does\nnot have a memory leak. I will try use use one instance of IndexSearcher accross\nall of my JSP sessions and everything should be fine - a question though - (and\nquite possibly a naive one)....is it always deemed necessary to use one\nIndexSearcher, would it not be more robust to be able to use a new one for each\nrequest?\n ",
            "author": "John Moylan",
            "id": "comment-12321827"
        },
        {
            "date": "2004-08-16T20:27:58+0000",
            "content": "It wouldn't be any more robust that using a single IndexSearcher, it would just\nuse more resources, as you've witnessed. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12321828"
        }
    ]
}