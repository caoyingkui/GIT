{
    "id": "LUCENE-6747",
    "title": "FingerprintFilter - a TokenFilter for clustering/linking purposes",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [
            "modules/analysis"
        ],
        "labels": "",
        "fix_versions": [
            "5.4",
            "6.0"
        ],
        "priority": "Minor",
        "status": "Closed",
        "type": "New Feature"
    },
    "description": "A TokenFilter that emits a single token which is a sorted, de-duplicated set of the input tokens.\nThis approach to normalizing text is used in tools like OpenRefine[1] and elsewhere [2] to help in clustering or linking texts.\nThe implementation proposed here has a an upper limit on the size of the combined token which is output.\n\n[1] https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth\n[2] https://rajmak.wordpress.com/2013/04/27/clustering-text-map-reduce-in-python/",
    "attachments": {
        "fingerprintv1.patch": "https://issues.apache.org/jira/secure/attachment/12751296/fingerprintv1.patch",
        "fingerprintv2.patch": "https://issues.apache.org/jira/secure/attachment/12751448/fingerprintv2.patch",
        "fingerprintv3.patch": "https://issues.apache.org/jira/secure/attachment/12751735/fingerprintv3.patch",
        "fingerprintv4.patch": "https://issues.apache.org/jira/secure/attachment/12752231/fingerprintv4.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14703262",
            "author": "Mark Harwood",
            "date": "2015-08-19T16:15:39+0000",
            "content": "Proposed implementation and test "
        },
        {
            "id": "comment-14703316",
            "author": "Adrien Grand",
            "date": "2015-08-19T16:41:02+0000",
            "content": "If you could tolerate that these fingerprints are not be reliable identifiers of your input, I'm wondering that we could make it more efficient by just using a hash function that doesn't depend on the order of its inputs?\n\nOtherwise this looks rather good to me. Instead of taking the min offset and the max offset as offsets for the final token, I'm wondering that it might make more sense to use 0 and the final offset (the one returned after end() has been called) instead so that we don't treat token chars differently depending on whether they appear before/after the tokens or in the middle? By the way even with the current approach, we don't need to call Math.min/max: As tokens are supposed to be emitted in order, the start offset would be the start offset of the first token and the end offset would be the end offset of the last token. "
        },
        {
            "id": "comment-14704519",
            "author": "Mark Harwood",
            "date": "2015-08-20T08:42:14+0000",
            "content": "Thanks for taking a look, Adrien.\nAdded a v2 patch with following changes:\n\n1) added call to input.end() to get final offset state\n2) final state is retained using captureState()  \n3) Added a FingerprintFilterFactory class\n\nAs for the alternative hashing idea :\nFor speed reasons this would be a nice idea but reduces the read-ability of results if you want to debug any collisions or otherwise display connections.\n\nFor compactness reasons (storing in doc values etc) it would always be possible to chain a conventional hashing algo in a TokenFilter on the end of this text-normalizing filter. (Do we already have a conventional hashing TokenFilter?)\n\n "
        },
        {
            "id": "comment-14704836",
            "author": "Adrien Grand",
            "date": "2015-08-20T13:06:53+0000",
            "content": "\n+      if (item instanceof char[]) {\n+        sb.append((char[]) item);\n+      } else {\n+        sb.append(item);\n+      }\n\n\n\nCan we get something else than a char[]? If no I think we should just forcefully cast?\n\nOtherwise +1.  "
        },
        {
            "id": "comment-14706831",
            "author": "Mark Harwood",
            "date": "2015-08-21T14:57:37+0000",
            "content": "Updated patch - removed instanceof check and added entry to Changes.txt.\n\nWill commit to trunk and 5.x in a day or two if there's no objections "
        },
        {
            "id": "comment-14711209",
            "author": "Mark Harwood",
            "date": "2015-08-25T12:51:49+0000",
            "content": "Some final tweaks:\n1) Found a bug where separator not appended if first token is length ==1\n2) Randomized testing identified issue with input.end() not being called when IOExceptions occur\n3) Added missing SPI entry for FingerprintFilterFactory and associated test class for FingerprintFilterFactory "
        },
        {
            "id": "comment-14716626",
            "author": "ASF subversion and git services",
            "date": "2015-08-27T13:11:07+0000",
            "content": "Commit 1698145 from mharwood@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1698145 ]\n\nLUCENE-6747: FingerprintFilter is a new TokenFilter that outputs a single token which is a concatenation of the sorted and de-duplicated set of input tokens. "
        },
        {
            "id": "comment-14716928",
            "author": "ASF subversion and git services",
            "date": "2015-08-27T16:01:49+0000",
            "content": "Commit 1698188 from mharwood@apache.org in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1698188 ]\n\nLUCENE-6747: FingerprintFilter is a new TokenFilter that outputs a single token which is a concatenation of the sorted and de-duplicated set of input tokens. "
        },
        {
            "id": "comment-14716937",
            "author": "Mark Harwood",
            "date": "2015-08-27T16:06:08+0000",
            "content": "Commited to trunk and 5.x "
        },
        {
            "id": "comment-14727122",
            "author": "Adrien Grand",
            "date": "2015-09-02T10:09:41+0000",
            "content": "Mark, I see this issue under \"Lucene 6.0.0\" in the CHANGES.txt file on trunk, should it be moved to 5.4.0? "
        }
    ]
}