{
    "id": "LUCENE-1534",
    "title": "idf(t) is not actually squared during scoring?",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/query/scoring"
        ],
        "type": "Bug",
        "fix_versions": [
            "2.9"
        ],
        "affect_versions": "2.1,                                            2.2,                                            2.3,                                            2.3.1,                                            2.3.2,                                            2.4",
        "resolution": "Invalid",
        "status": "Closed"
    },
    "description": "The javadocs for Similarity:\n\n  http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/search/Similarity.html\n\nshow idf(t) as being squared when computing net query score.  But I\ndon't think it is actually squared, in looking at the sources?  Maybe\nit used to be, eg this interesting discussion:\n\n  http://markmail.org/message/k5pl7scmiac5wosb\n\nOr am I missing something?  We just need to fix the javadocs to take\naway the \"squared\"...",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2009-02-02T17:32:28+0000",
            "content": "hmmm...we do multiply it in twice, but a bit happens in between - we multiply by idf(t) in sumOfSquaredWeights()  and then again in normalize(float queryNorm).\n\nTechnically that is boost * idf(t) * norm * idf(t), right? For idf(t)^2 * boost * norm? And then that times tf in the scorer...  ",
            "author": "Mark Miller",
            "id": "comment-12669663"
        },
        {
            "date": "2009-02-02T18:15:55+0000",
            "content": "Right.... and explain explains it by having 1 idf factor in the queryWeight and 1 in the fieldWeight:\n\n\n0.6433005 = (MATCH) weight(text:solr in 14), product of:\n  0.99999994 = queryWeight(text:solr), product of:\n    3.6390574 = idf(docFreq=1, numDocs=26)\n    0.27479643 = queryNorm\n  0.64330053 = (MATCH) fieldWeight(text:solr in 14), product of:\n    1.4142135 = tf(termFreq(text:solr)=2)\n    3.6390574 = idf(docFreq=1, numDocs=26)\n    0.125 = fieldNorm(field=text, doc=14)\n\n ",
            "author": "Yonik Seeley",
            "id": "comment-12669673"
        },
        {
            "date": "2009-02-02T21:36:04+0000",
            "content": "But sumOfSquaredWeights is only used as a fixed normalization across\nall sub-queries in the Query?\n\nEG for a single TermQuery, the queryWeight will always be 1.0 (except\nfor roundoff errors), cancelling out that idf factor, leaving only one\nidf factor? ",
            "author": "Michael McCandless",
            "id": "comment-12669743"
        },
        {
            "date": "2009-02-02T21:38:48+0000",
            "content": "I've always found \"idf squared\" an unhelpful description.  We're computing a dot-product of two vectors, the angle between them.  Terms are dimensions.  The magnitude in each dimension is the weight of the term in a query or document.  Our heuristic for computing weights is (sqrt(tf)*idf)/norm.  Put all that together, and you do indeed get an \"idf squared\" factor in each addend of the score.  But if we feel that over-emphasizes terms with large idfs, then we should not remove an idf factor from one vector, but rather rework our weight heuristic, perhaps replacing idf with sqrt(idf), no? ",
            "author": "Doug Cutting",
            "id": "comment-12669750"
        },
        {
            "date": "2009-02-02T21:43:13+0000",
            "content": "sumOfSquaredWeights properly normalizes query vectors to the unit sphere.  We can't easily do that with document vectors, since idfs change as the collection changes.  So we instead use a heuristic to normalize documents, sqrt(numTokens), which is usually a good approximation.  Regardless of how it's normalized, the global term weight factors twice in each addend, once from each vector. ",
            "author": "Doug Cutting",
            "id": "comment-12669754"
        },
        {
            "date": "2009-02-02T21:52:20+0000",
            "content": "\nEG for a single TermQuery, the queryWeight will always be 1.0 (except\nfor roundoff errors), cancelling out that idf factor, leaving only one\nidf factor?\n\nYes, for a score returned to the user only one idf factor remains because of the normalization.\nBut the more important part of the scoring is how terms are scored relative to each other in the same query - and that is still idf**2 ",
            "author": "Yonik Seeley",
            "id": "comment-12669756"
        },
        {
            "date": "2009-02-02T23:08:46+0000",
            "content": "But the more important part of the scoring is how terms are scored relative to each other in the same query - and that is still idf**2\n\nAhh OK, now I get it \u2013 idf is indeed factored in twice.  A single TermQuery is a somewhat degenerate case; queries with more than one term will show the effect.  Thanks for clarifying  ",
            "author": "Michael McCandless",
            "id": "comment-12669791"
        },
        {
            "date": "2009-02-02T23:14:46+0000",
            "content": "But if we feel that over-emphasizes terms with large idfs, then we should not remove an idf factor from one vector, but rather rework our weight heuristic, perhaps replacing idf with sqrt(idf), no?\n\nI agree, that should be the approach if we decide idf^2 is too much, but I don't have an opinion (yet!) on whether it's too much (but that thread referenced above is nonetheless interesting). ",
            "author": "Michael McCandless",
            "id": "comment-12669794"
        },
        {
            "date": "2009-02-03T02:06:04+0000",
            "content": "[quote]But if we feel that over-emphasizes terms with large idfs, then we should not remove an idf factor from one vector, but rather rework our weight heuristic, perhaps replacing idf with sqrt(idf), no?[/quote]\n\nFWIW, having implemented web search on a large (500m) corpus, we found the stock idf factor in lucene is too high, and ended up sqrt()'ing it in Similarity.\n\nThat said, much of the score in this system came from anchor text, link analysis scores, and term proximity, so it's hard to measure the impact the idf change independently. ",
            "author": "Mike Klaas",
            "id": "comment-12669843"
        },
        {
            "date": "2009-02-03T20:34:39+0000",
            "content": "Now, on the cusp of 3.0, might be a good time to think about changing the default ranking algorithm.  This is potentially a disruptive change, but we can easily provide a back-compatible Similarity implementation.  Are there other changes to the default Similarity that may be of general utility?  Or do folks thinks its better to leave this alone? ",
            "author": "Doug Cutting",
            "id": "comment-12670103"
        }
    ]
}