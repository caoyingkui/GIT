{
    "id": "LUCENE-1227",
    "title": "NGramTokenizer to handle more than 1024 chars",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "Current NGramTokenizer can't handle character stream that is longer than 1024. This is too short for non-whitespace-separated languages.\n\nI created a patch for this issues.",
    "attachments": {
        "LUCENE-1227.patch": "https://issues.apache.org/jira/secure/attachment/12377998/LUCENE-1227.patch",
        "NGramTokenizer.patch": "https://issues.apache.org/jira/secure/attachment/12377756/NGramTokenizer.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-03-13T08:25:45+0000",
            "content": "LUCENE-1227's NGramTokenizer.patch will also fix https://issues.apache.org/jira/browse/LUCENE-1225  ",
            "author": "Hiroaki Kawai",
            "id": "comment-12578182"
        },
        {
            "date": "2008-03-13T15:44:27+0000",
            "content": "bugfix that I made a mistake about char array addressing. ",
            "author": "Hiroaki Kawai",
            "id": "comment-12578348"
        },
        {
            "date": "2008-03-15T18:06:47+0000",
            "content": "Hi Hiroaki,\n\nThanks for the patch.  Can you add unit tests for your patch? ",
            "author": "Grant Ingersoll",
            "id": "comment-12579068"
        },
        {
            "date": "2008-03-16T14:59:29+0000",
            "content": "NGramTokenizerTest#testIndexAndQueryLong will test this issues. ",
            "author": "Hiroaki Kawai",
            "id": "comment-12579198"
        },
        {
            "date": "2008-05-14T06:06:30+0000",
            "content": "Thanks for the test and for addressing this!\n\nCould you add some examples for NO_OPTIMIZE and QUERY_OPTIMIZE?  I can't tell from looking at the patch what those are about.  Also, note how existing variables use camelCaseLikeThis.  It would be good to stick to the same pattern (instead of bufflen, buffpos, etc.), as well as to the existing style (e.g. space between if and open paren, spaces around == and =, etc.)\n\nI'll commit as soon as you make these changes, assuming you can make them.  Thank you. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12596637"
        },
        {
            "date": "2009-01-05T20:59:31+0000",
            "content": "Any progress on getting this patch into a release? I can take a look if nobody else is.  ",
            "author": "Michael Dodsworth",
            "id": "comment-12660924"
        },
        {
            "date": "2009-01-06T13:18:12+0000",
            "content": "Yes, please do have a look and let us know what you think. ",
            "author": "Grant Ingersoll",
            "id": "comment-12661125"
        },
        {
            "date": "2011-07-25T04:06:17+0000",
            "content": "I avoided this problem by putting NGramFilterFactory and WhitespaceTokenizerFactory together in Solr schema.\n\n<tokenizer class=\"solr.NGramTokenizerFactory\" maxGramSize=\"2\" minGramSize=\"2\" />\n\nis written to:\n\n<filter class=\"solr.NGramFilterFactory\" minGramSize=\"2\" maxGramSize=\"2\"/>\n<tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n\nPlatform: Solr 1.4.1 ",
            "author": "Torao Takami",
            "id": "comment-13070309"
        },
        {
            "date": "2012-01-11T01:21:50+0000",
            "content": "For me, this also works (for my purposes, at least):\n\n        String str =  <read contents of reader to string>\n\n\tTokenStream tokens = new KeywordTokenizer(new StringReader(str.trim());\n\ttokens= new NGramTokenFilter(tokens, minNGram, maxNGram);\n ",
            "author": "Raimund Merkert",
            "id": "comment-13183775"
        },
        {
            "date": "2013-01-09T08:46:02+0000",
            "content": "As long as this issue is not fixed, please mention the 1024 character truncation in the Javadoc.\n\nThe combination of KeywordTokenizer and NGramTokenFilter does not scale well for large inputs, as KeywordTokenizer reads the entire input stream into a character buffer. ",
            "author": "Harald Wellmann",
            "id": "comment-13547774"
        },
        {
            "date": "2013-04-26T20:08:28+0000",
            "content": "I proposed a patch that fixes this problem on another ticket:\n\nhttps://issues.apache.org/jira/browse/LUCENE-2947 ",
            "author": "David Byrne",
            "id": "comment-13643200"
        },
        {
            "date": "2013-04-26T22:11:19+0000",
            "content": "LUCENE-4955 fixed this issue. ",
            "author": "Adrien Grand",
            "id": "comment-13643291"
        },
        {
            "date": "2013-04-26T22:31:56+0000",
            "content": "David, sorry I didn't know about your patch and happened to fix this issue as part of LUCENE-4955. Your patch seems to operate very similarly and adds supports for whitespace collapsing, is that correct? Don't hesitate to tell me if you think the current implementation needs improvements. ",
            "author": "Adrien Grand",
            "id": "comment-13643326"
        }
    ]
}