{
    "id": "LUCENE-2096",
    "title": "Investigate parallelizing Ant junit tests",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "general/build"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Duplicate",
        "status": "Resolved"
    },
    "description": "Ant Contrib has a \"ForEach\" construct that may speed up running all of the Junit tests by parallelizing them with a configurable number of threads. I envision this in several stages. First, see if ForEach works for us with hard-coded lists, distribute this for testing then make the changes \"for real\". I intend to hard-code the list for the first pass, ordered by the time they take. This won't do for check-in, but will give us a fast proof-of-concept.\n\nThis approach will be most useful for multi-core machines.\n\nIn particular, we need to see whether the parallel tasks are isolated enough from each other to prevent mutual interference.\n\nAll this assumes the fragmentary reference I found is still available...",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2009-11-29T17:39:08+0000",
            "content": "Parallelizing tests is proving trickier than I'd hoped. Part of the problem is my not-wonderful ant skills...\n\nBut what I've found so far with trying to use ForEach is that stuff gets in the way. In particular, the <sequential> tag in the test-macro body I'm pretty sure defeats any parallelizing attempts by ForEach. Taking it out isn't straightforward.\n\nIn some of my experiments, I got tests to fire off in parallel, but then started running into wonky errors that were so strange now I can't remember them, but some having to do with what looked like file contention for some temporary test files.\n\nGoogling around I think I remember posts by Jason Ruthgren trying to so something similar in SOLR . Jason: if I'm remembering right did you find any joy?\n\nThen we'd have to rework how success and failure are handled because there's contention for that file as well.\n\nNow I'm wondering if the \"scary python script\" gets us more bang for the buck. I wrote a Groovy script the probably is a near-cousin for experiments and I'm wondering what would happen if we wrote a special testcase-type target that did NOT depend upon compile-test or, really, much of anything else and counted on the user to make sure to build the system first before using whatever script wecame up with. We don't really lose functionality by recursively looking for Test*.java files because that's what's done internally in the build files anyway. So doing that outside or inside the ant files doesn't seem like a loss.\n\nI'm putting this in the JIRA issue to preserve it for posterity. Meanwhile, I'll appeal to Ant gurus if they want to try whacking the Ant build files, and see what the script notion brings...\n ",
            "author": "Erick Erickson",
            "id": "comment-12783436"
        },
        {
            "date": "2009-12-05T14:50:37+0000",
            "content": "Maybe for later ",
            "author": "Erick Erickson",
            "id": "comment-12786419"
        }
    ]
}