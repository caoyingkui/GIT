{
    "id": "SOLR-3280",
    "title": "to many / sometimes stale CLOSE_WAIT connections from SnapPuller during / after replication",
    "details": {
        "affect_versions": "3.5,                                            3.6,                                            4.0-ALPHA",
        "status": "Closed",
        "fix_versions": [],
        "components": [],
        "type": "Bug",
        "priority": "Minor",
        "labels": "",
        "resolution": "Incomplete"
    },
    "description": "There are sometimes to many and also stale CLOSE_WAIT connections during/after replication left over on SLAVE server.\nNormally GC should clean up this but this is not always the case.\nAlso if a CLOSE_WAIT is hanging then the new replication won't load.\n\nDirty work around so far is to fake a TCP connection as root to that connection and close it. \nAfter that the new replication will load, the old index and searcher released and the system will\nreturn to normal operation.\n\nBackground:\nThe SnapPuller is using Apache httpclient 3.x and uses the MultiThreadedHttpConnectionManager.\nThe manager holds a connection in CLOSE_WAIT after its use for further requests.\nThis is done by calling releaseConnection. But if a connection is stuck it is not available any more and a new\nconnection from the pool is used.\n\nSolution:\nAfter calling releaseConnection clean up with closeIdleConnections(0).",
    "attachments": {
        "SOLR-3280.patch": "https://issues.apache.org/jira/secure/attachment/12520081/SOLR-3280.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Bernd Fehling",
            "id": "comment-13239328",
            "date": "2012-03-27T09:04:42+0000",
            "content": "This patch will fix the CLOSE_WAIT issue. "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13246130",
            "date": "2012-04-04T09:17:34+0000",
            "content": "I did some testing around replication: 2 nodes on same lan, heavy replication/heavy indexing and did not see any sockets in CLOSE_WAIT state after running it for about 1 hour. \n\nPerhaps you have a firewall between master and slave that drops \"idle\" connections somehow wrongly? "
        },
        {
            "author": "Bernd Fehling",
            "id": "comment-13246139",
            "date": "2012-04-04T09:49:13+0000",
            "content": "Nope, no firewall. I have 1 master and 2 slaves on the same lan. After replication finished the connection on master is closed, the connection on slave is in CLOSE_WAIT with a Receive-Queue 1 byte. If everything goes well the connection will be reused by MultiThreadedHttpConnectionManager, but if something goes wrong (which is very seldom on my systems) the connection will hang on CLOSE_WAIT and the new index is not swapped in.\nIf you use jvisualvm on that slave and go to the MBeans tab you can see \"solr/\" in the tree but you can't open it because there is no sub-tree.\nThe patch is just releasing the connection, if it hangs or not, and keeps everything operational. So no harm or performance impact for replication.\n "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13246154",
            "date": "2012-04-04T10:14:28+0000",
            "content": "but if something goes wrong (which is very seldom on my systems) the connection will hang on CLOSE_WAIT and the new index is not swapped in.\n\ndo you have idea what this something is, anything in the logs?\n\n\nThe patch is just releasing the connection, if it hangs or not, and keeps everything operational. So no harm or performance impact for replication.\n\nYeah I agree. The performance impact is minimal. "
        },
        {
            "author": "Bernd Fehling",
            "id": "comment-13246196",
            "date": "2012-04-04T12:10:39+0000",
            "content": "Sorry I can't specify it any closer, a \"network hiccup\" or the computing center is configuring something at the network. I don't know. There is nothing in the solr logs, just hanging. The old index is still at work and serving the requests.\nI located this with the server sys logs because the space the index located in data directory had doubled its size for longer than 1 day. One slave had this in August and October last year (solr 3.3) the other slave in October (solr 3.3) and January this year (solr 3.5). After seeing with netstat the CLOSE_WAIT and forcing it to close the system went back to normal operation, started a new searcher with new index and close the old searcher and deleted the old index.\n "
        },
        {
            "author": "David Fu",
            "id": "comment-13741426",
            "date": "2013-08-15T20:11:22+0000",
            "content": "I am facing the same problem. Any update on when this will be resolved? "
        },
        {
            "author": "Bernd Fehling",
            "id": "comment-13741972",
            "date": "2013-08-16T06:27:44+0000",
            "content": "After going from solr 3.6 to 4.2.1 I haven't seen this anymore. There was pretty much rework done in SnapPuller due to multicore. Which version are you using? "
        },
        {
            "author": "David Fu",
            "id": "comment-13742478",
            "date": "2013-08-16T18:12:27+0000",
            "content": "I am still on 3.4 now. I noticed the solr4 pretty much reimplemented the snappuller and am thinking about upgrading to v4. Just out of the curiosity, what are some issues you faced in the process of upgrading from 3.6 to 4.2.1? "
        },
        {
            "author": "Bernd Fehling",
            "id": "comment-13743565",
            "date": "2013-08-19T06:07:15+0000",
            "content": "Just carefully read the CHANGES.txt. There is also a section \"Upgrading from Solr 3.6\". "
        },
        {
            "author": "Alexandre Rafalovitch",
            "id": "comment-15538862",
            "date": "2016-10-01T17:28:28+0000",
            "content": "This is no longer relevant to the current replication method. "
        }
    ]
}