{
    "id": "LUCENE-2098",
    "title": "make BaseCharFilter more efficient in performance",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "fix_versions": [
            "2.9.4",
            "3.0.3",
            "3.1",
            "4.0-ALPHA"
        ],
        "affect_versions": "3.1",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Performance degradation in Solr 1.4 was reported. See:\n\nhttp://www.lucidimagination.com/search/document/43c4bdaf5c9ec98d/html_stripping_slower_in_solr_1_4\n\nThe inefficiency has been pointed out in BaseCharFilter javadoc by Mike:\n\n\nNOTE: This class is not particularly efficient. For example, a new class instance is created for every call to addOffCorrectMap(int, int), which is then appended to a private list.",
    "attachments": {
        "LUCENE-2098.patch": "https://issues.apache.org/jira/secure/attachment/12438866/LUCENE-2098.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-03-15T22:07:31+0000",
            "content": "i haven't benchmarked to see if this is any faster, maybe even worse.\n\nbut its no longer a linear algorithm ",
            "author": "Robert Muir",
            "id": "comment-12845558"
        },
        {
            "date": "2010-03-16T09:35:07+0000",
            "content": "Why did this cause Solr to slowdown...?  Did Solr previously have a more efficient impl and then they cutover to Lucene's? ",
            "author": "Michael McCandless",
            "id": "comment-12845774"
        },
        {
            "date": "2010-03-16T09:38:42+0000",
            "content": "Patch looks like it should be a good net/net improvement \u2013 lookups of the offset correction should now be fast (though insertion cost is probably higher \u2013 we create likely 3 new objects (2 ints, one TreeMap$Entry) per insert) but I expect that's a good tradeoff. ",
            "author": "Michael McCandless",
            "id": "comment-12845776"
        },
        {
            "date": "2010-03-16T09:57:17+0000",
            "content": "Why did this cause Solr to slowdown...? Did Solr previously have a more efficient impl and then they cutover to Lucene's? \n\nSolr used another Filter in 1.3. ",
            "author": "Uwe Schindler",
            "id": "comment-12845785"
        },
        {
            "date": "2010-03-16T10:11:35+0000",
            "content": "Ahh ok.\n\nProbably we should switch to parallel arrays here, to make it very fast... yes this will consume RAM (8 bytes per position, if we keep all of them).\n\nReally most apps do not need all positions stored, ie, they only need to see typically the current token.  So maybe we could make a filter that takes a \"lookbehind size\" and it'd only keep that number of mappings cached?  That'd have to be > the max size of any token you may analyze, so hard to bound perfectly, but eg setting this to the max allowed token in IndexWriter would guarantee that we'd never have a miss?\n\nFor analyzers that buffer tokens... they'd have to set this max to infinity, or, ensure they remap the offsets before capturing the token's state? ",
            "author": "Michael McCandless",
            "id": "comment-12845788"
        },
        {
            "date": "2010-03-16T12:46:47+0000",
            "content": "Mark did some quick tests and this patch only seems to make things slower.\n\nReally most apps do not need all positions stored, ie, they only need to see typically the current token.\n\nI think this is why it got slower with my patch, in practice it didn't matter that this thing did 'backwards linear lookup' due to this reason? ",
            "author": "Robert Muir",
            "id": "comment-12845887"
        },
        {
            "date": "2010-03-16T14:40:09+0000",
            "content": "I think this is why it got slower with my patch, in practice it didn't matter that this thing did 'backwards linear lookup' due to this reason?\n\nAhh yes since presumably the test was simply looking up the offsets for the current token... ",
            "author": "Michael McCandless",
            "id": "comment-12845919"
        },
        {
            "date": "2010-03-16T15:20:41+0000",
            "content": "I think the best way to proceed would be to make it easy to benchmark \nCharFilters in contrib/benchmark, especially this HTML stripping one.\n\nHonestly we don't even know for sure any performance degradation reported\nin the original link is really due to BaseCharFilter yet, so I think we need\nto benchmark and profile. ",
            "author": "Robert Muir",
            "id": "comment-12845934"
        },
        {
            "date": "2010-08-27T06:50:59+0000",
            "content": "ok, i think this one is fixed.\n\ni ran a loop with the example doc in the tests and tested both removing the object creation and switching to binary search, both help.\n\nI'd like to commit to trunk and 3x tomorrow. ",
            "author": "Robert Muir",
            "id": "comment-12903266"
        },
        {
            "date": "2010-08-27T07:23:43+0000",
            "content": "here are the files i tested, htmlStripCharFilterTest.html (from the test, 12kb file) and http://en.wikipedia.org/wiki/Benjamin_Franklin (360kb file)\n\ni ran each 3 times:\n\n\n\n\nfile\nbefore\nafter\n\n\nhtmlStripCharFilterTest.html\n9709ms,9560ms,9587ms\n8755ms,8697ms,8708ms\n\n\nbenFranklin.html\n26877ms,26963ms,26495ms\n17593ms,17674ms,17694ms\n\n\n\n\n\nhere was the code (crude but i think it shows the point, the larger the files the worse the offset correction was):\n\n    Charset charset = Charset.forName(\"UTF-8\");\n    WhitespaceTokenizer tokenizer = new WhitespaceTokenizer(Version.LUCENE_CURRENT, new StringReader(\"\"));\n    long startMS = System.currentTimeMillis();\n    for (int i = 0; i < 10000; i++) {\n      InputStream stream = HTMLStripCharFilterTest.class.getResourceAsStream(\"htmlStripReaderTest.html\");\n      HTMLStripCharFilter reader = new HTMLStripCharFilter(CharReader.get(new InputStreamReader(stream, charset)));\n      tokenizer.reset(reader);\n      tokenizer.reset();\n      while (tokenizer.incrementToken())\n        ;\n    }\n    System.out.println(\"time=\" + (System.currentTimeMillis() - startMS));\n\n ",
            "author": "Robert Muir",
            "id": "comment-12903276"
        },
        {
            "date": "2010-08-27T13:45:32+0000",
            "content": "Patch looks good, Robert!\n\nI added a check code for currentOff before doing binary search. ",
            "author": "Koji Sekiguchi",
            "id": "comment-12903411"
        },
        {
            "date": "2010-08-27T14:55:39+0000",
            "content": "Committed 990161 (trunk) 990167 (3x) ",
            "author": "Robert Muir",
            "id": "comment-12903438"
        },
        {
            "date": "2010-10-30T10:35:30+0000",
            "content": "reopening for potential backport.\n\nI think this one (n^2) is really buggish territory, the original user reports > 2x slowdown with solr 1.4\n\nI think the fix is safe, its baked in trunk/3.x a while, and if we have perf bugs like this with no api breaks, \nit makes sense... but if someone objects I won't backport. ",
            "author": "Robert Muir",
            "id": "comment-12926558"
        },
        {
            "date": "2010-10-30T11:10:22+0000",
            "content": "Robert, please backport. ",
            "author": "Koji Sekiguchi",
            "id": "comment-12926563"
        },
        {
            "date": "2010-10-30T14:43:36+0000",
            "content": "Committed revision 1029077 to 3.0.x.\nCommitted revision 1029087 to 2.9.x.\n\nI also cleared up the svn:eol-style problems on these branches, \nif you are merging on windows you shouldn't see any property conflicts anymore. ",
            "author": "Robert Muir",
            "id": "comment-12926583"
        }
    ]
}