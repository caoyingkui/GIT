{
    "id": "SOLR-9320",
    "title": "A REPLACENODE command to decommission an existing node with another new node",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Sub-task",
        "labels": "",
        "fix_versions": [
            "6.2",
            "7.0"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "The command should accept a source node and target node. recreate the replicas in source node in the target and do a DLETENODE of source node",
    "attachments": {
        "REPLACENODE_call_response.jpeg": "https://issues.apache.org/jira/secure/attachment/12819766/REPLACENODE_call_response.jpeg",
        "REPLACENODE_After.jpeg": "https://issues.apache.org/jira/secure/attachment/12819765/REPLACENODE_After.jpeg",
        "SOLR-9320_followup.patch": "https://issues.apache.org/jira/secure/attachment/12823475/SOLR-9320_followup.patch",
        "REPLACENODE_Before.jpeg": "https://issues.apache.org/jira/secure/attachment/12819767/REPLACENODE_Before.jpeg",
        "SOLR-9320.patch": "https://issues.apache.org/jira/secure/attachment/12819763/SOLR-9320.patch",
        "DELETENODE.jpeg": "https://issues.apache.org/jira/secure/attachment/12819764/DELETENODE.jpeg"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-07-20T04:51:53+0000",
            "author": "Nitin Sharma",
            "content": "Clarification on this. When you mean recreate, does the naming matter? Lets say x_shard1_replica1 is on source node, if you want to move it to destination node, we can\na)  either create a new replica (x_shard1_replica2) and delete the source. That will leave uneven naming conventions in the cluster. (there will not be a replica1 but a replica2). \nb) Preserve the exact same name as the replica in source node. We can achieve this by creating a temp replica on destination first, deleting the  replica on source,  recreating the replica (with same name) on destination and then cleaning up the temp. \n\n\nOption (b) can be thought of as a migrate core. \n\nLet me know which sounds more usable.  ",
            "id": "comment-15385323"
        },
        {
            "date": "2016-07-20T05:55:58+0000",
            "author": "Noble Paul",
            "content": "That will leave uneven naming conventions in the cluster.\n\nreplica names do not matter. You can create a new replica in any name as you want ",
            "id": "comment-15385408"
        },
        {
            "date": "2016-07-20T15:21:31+0000",
            "author": "Erick Erickson",
            "content": "(b) would copy the index twice, correct? I've seen (and I wouldn't lie) TB size indexes on a single replica so I think that would be prohibitively expensive. ",
            "id": "comment-15386025"
        },
        {
            "date": "2016-07-20T16:48:45+0000",
            "author": "Nitin Sharma",
            "content": "Thanks for clarifying. I am aligning towards option a) as well (for simplicity and performance) but wanted to confirm the semantics.  ",
            "id": "comment-15386191"
        },
        {
            "date": "2016-07-23T04:49:39+0000",
            "author": "Nitin Sharma",
            "content": "Patch for REPLACENODE & DELETENODE api as per spec. \n\nDELETENODE- Deletes all cores on the given node.\n\nREPLACENODE - Replaces all cores from a source node to a dest node and then calls DELETENODE on the source node. \n\nAttached screenshots against test cluster with api calls & before/after status of the cluster ",
            "id": "comment-15390535"
        },
        {
            "date": "2016-07-27T18:24:34+0000",
            "author": "Noble Paul",
            "content": "REPLACENODE is just like any other command that we already have. This patch seems to be too complex for a very simple functionality like that. Please refer implementations of other commands. ",
            "id": "comment-15396107"
        },
        {
            "date": "2016-07-27T18:56:26+0000",
            "author": "Nitin Sharma",
            "content": "Noble Paul Can you explain about the complex part? The code does what was mentioned in the spec. It identifies all the cores on the node to be replaced and recreates them on the destination node. After that it calls \"DELETENODE\" on the source node. Which part of this complex? The multithreaded part? ",
            "id": "comment-15396152"
        },
        {
            "date": "2016-07-27T19:00:02+0000",
            "author": "Noble Paul",
            "content": "I really didn't mean that the code was complex to understand. I meant adding so many files for such a simple functionality is not worth it. We generally follow a pattern for multiple commands.  ",
            "id": "comment-15396160"
        },
        {
            "date": "2016-07-27T19:07:39+0000",
            "author": "Nitin Sharma",
            "content": "This has the patch for both REPLACENODE and DELETENODE inside it. I can reduce the num files again and re-upload if you like.  ",
            "id": "comment-15396174"
        },
        {
            "date": "2016-07-27T19:10:17+0000",
            "author": "Noble Paul",
            "content": "If I'm not wrong you just need to add one method. This is a trivial functionality ",
            "id": "comment-15396181"
        },
        {
            "date": "2016-07-27T19:56:27+0000",
            "author": "Anshum Gupta",
            "content": "I am having issues applying this patch against master. \nI tried to take a look at this without applying the patch and the patch does add complexity, as Noble pointed out. ",
            "id": "comment-15396244"
        },
        {
            "date": "2016-08-01T14:59:07+0000",
            "author": "Noble Paul",
            "content": "without tests ",
            "id": "comment-15402192"
        },
        {
            "date": "2016-08-01T18:31:29+0000",
            "author": "Nitin Sharma",
            "content": "I have a concern when this comes to scaling to 100s of collections. If you have 100+ collections, this command will eventually timeout (> 3 mins) and this does this in serial. The patch i sent (has multi threading capability), I ran with 500 collections and it finished around 4+ mins. Can you share the numbers with collections of that scale? ",
            "id": "comment-15402587"
        },
        {
            "date": "2016-08-02T04:22:09+0000",
            "author": "Noble Paul",
            "content": "Multithreading is already of part of OverseerCollectionMessageHandler . You just have to invoke the command with the async flag and It will do everything in a thread of its own\n\n BTW Why would you run REPLACENODE with 500 collections? It runs one node at a time , right? ",
            "id": "comment-15403320"
        },
        {
            "date": "2016-08-02T04:26:55+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Multithreading is already of part of OverseerCollectionMessageHandler\n\nThat is only for executing collection APIs concurrently. It does not help when you are trying to add replica to target and remove replica from source serially for each replicas on a particular node. Nitin is right that it should be multi-threaded otherwise it will take a long time for a node hosting a lot of replicas. ",
            "id": "comment-15403323"
        },
        {
            "date": "2016-08-02T04:29:15+0000",
            "author": "Nitin Sharma",
            "content": "Thanks for clarifying on async. Isnt that only for executing multiple api in parallel. If you want parallelism within the api, you still need your own exec service right?\n\nReg 500 collections: When you run replace node on a node x and the node happens to have 500 cores, then all these cores need to be moved to the destination (and then removed from source). If you have too many cores, adding a new replica for every core on the destination host will take time. Wondering if you ran any tests with such high capacity ",
            "id": "comment-15403326"
        },
        {
            "date": "2016-08-02T04:35:18+0000",
            "author": "Noble Paul",
            "content": "I don't think so. The create replica call can be async as well. ",
            "id": "comment-15403331"
        },
        {
            "date": "2016-08-02T04:45:46+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "I don't think so. The create replica call can be async as well.\n\nIt can be but in your patch it is not. The addReplica method call will wait for the async core create call to complete and also for the new replica to be visible in cluster state. Similarly, the deleteReplica method waits for the async core unload to complete and wait for coreNode name to be deleted from the cluster state. ",
            "id": "comment-15403340"
        },
        {
            "date": "2016-08-02T04:57:28+0000",
            "author": "Noble Paul",
            "content": "Yeah, It's not in the patch. What I meant to say was the Multithreading framework is there and the code just needs to take advantage of that.\n\nOTOH, I wonder how performant does it need to be ? Replacing a node may take a while and as long as it completes without timing out, is it no good enough? The health of the system is not degraded or no action is blocked  while the REPLACENODE is happening. I think, instead of creating 500 cores on the same node (vying for the same disk/CPU) it is better to do it serially ",
            "id": "comment-15403350"
        },
        {
            "date": "2016-08-02T16:15:53+0000",
            "author": "Erick Erickson",
            "content": "If we're going to take advantage of multithreading we need to take some care to throttle replication. If we try to copy 500 cores' indexes to some other node all at once with 500 separate threads I'd worry about network bandwidth issues. I've seen saturated I/O cause nodes to go into recovery and the like. Not to mention beating up the disks on both machines.\n\nThe number of replace operations to carry out in parallel is probably the easiest. I can think of two tuning parameters, max # of simultaneous threads and max bandwidth consumption. \n\nThinking about it for a bit, though, the bandwidth parameter seems like it's difficult to do well. There'd have to be some kind of cross-copy communications I should think. And other ugliness. I suppose one could get this behavior on a case-by-case basis by \n1> specifying the max # of replace ops\n2> specifying <str name=\"maxWriteMBPerSec\">${maxWriteMbPerSec:100000}</str> in the replication handler and overriding with a sys var when doing a REPLACENODE....\n ",
            "id": "comment-15404292"
        },
        {
            "date": "2016-08-02T16:58:09+0000",
            "author": "Noble Paul",
            "content": "Erick Erickson I'm inclined to create cores one after another. Even if takes some time, it is better to ensure that we don't overload that one node. This is an admin command which is issued asynchronously. So, there is no harm even if it takes a while. ",
            "id": "comment-15404382"
        },
        {
            "date": "2016-08-02T17:25:52+0000",
            "author": "Erick Erickson",
            "content": "Noble Paul That certainly works. My goal was just to be sure bandwidth figured in to the design if people wanted to multi-thread.\n\nAnd doing one after the other could still be throttled by maxWriteMBPerSec if a single node overwhelmed the network or disks (I've actually seen this in the wild FWIW).\n ",
            "id": "comment-15404430"
        },
        {
            "date": "2016-08-02T17:28:12+0000",
            "author": "Noble Paul",
            "content": "And doing one after the other could still be throttled by maxWriteMBPerSec\n\nThe problem is, we can't do that from the REPLACENODE command. The downloads happen in a different thread from the target node  ",
            "id": "comment-15404435"
        },
        {
            "date": "2016-08-02T17:45:21+0000",
            "author": "Nitin Sharma",
            "content": "Would be good to establish context on when this command should/can be run. In our current version of rebalance, we use this to switch serving nodes in production and hence the time taken to migrate a node is limited (time bound). If this is considered an offline process, then may be serial works just fine. \n\nI agree with Erick on throttling but we can do that at a system level (unix traffic shaping tc etc). That will leave the bandwidth management outside the responsibility of solr. The same node serving solr can be used to run other services in some production setups. Keeping that outside solr will put that onus on the maintainer.  Just my $0.02 ",
            "id": "comment-15404456"
        },
        {
            "date": "2016-08-02T18:14:24+0000",
            "author": "Noble Paul",
            "content": "The best we can do is pass an extra parameter to speed up the execution (say parallel=true). But the real risk of screwing up the node/network exists ",
            "id": "comment-15404513"
        },
        {
            "date": "2016-08-04T13:31:05+0000",
            "author": "Noble Paul",
            "content": "can be executed in 2 modes \n\nwith a parameter parallel=true. This ensures that all cores are created and deleted in parallel in multiple threads. If the param is not passed they will be created one after other.\n\nI have not done any performance benchmarking. But with the parallel=true flag finishes much faster ",
            "id": "comment-15407767"
        },
        {
            "date": "2016-08-06T04:43:13+0000",
            "author": "Nitin Sharma",
            "content": "The patch i attached follows the original spec you proposed. Replace node after creating new replicas will call \"DELETENODE\" on the destination host which will remove all cores on the node. Can you merge this with that as well? Thanks. \n\nI will patch this on top of 6.1 and give it a shot\n ",
            "id": "comment-15410472"
        },
        {
            "date": "2016-08-06T04:57:12+0000",
            "author": "Noble Paul",
            "content": "Is the current patch missing anything? ",
            "id": "comment-15410477"
        },
        {
            "date": "2016-08-06T07:53:33+0000",
            "author": "Noble Paul",
            "content": "with tests ",
            "id": "comment-15410537"
        },
        {
            "date": "2016-08-06T17:43:17+0000",
            "author": "Nitin Sharma",
            "content": "Functionality seems to be fine to me. The spec you mentioned in the description was that REPLACE does a create and then invokes a DELETENODE. The recent patch seems to do iterate and do a delete replica instead of having a DELETENODE action and calling that. Hence the original question. \n\nI tried the first one and it works fine. I will try your new patch.  ",
            "id": "comment-15410678"
        },
        {
            "date": "2016-08-08T09:16:56+0000",
            "author": "Noble Paul",
            "content": "no need to do deletes serially ",
            "id": "comment-15411572"
        },
        {
            "date": "2016-08-08T11:37:21+0000",
            "author": "Noble Paul",
            "content": "this contains SOLR-9318 as well ",
            "id": "comment-15411693"
        },
        {
            "date": "2016-08-09T00:03:40+0000",
            "author": "Nitin Sharma",
            "content": "Most of the patch looks good. In ReplaceNodeCmd, instead of calling DeleteNodeCmd.deletereplicas, can you just called DELETENODE cmd on the source node itself? Looks much cleaner that way and one command doesnt need to know the details of the other cmd.  ",
            "id": "comment-15412691"
        },
        {
            "date": "2016-08-09T06:06:54+0000",
            "author": "Varun Thacker",
            "content": "Patch looks good!  Here are a few comments on that patch\n\nAny reason why we needed to change modify AsyncCollectionAdminRequest#setAsyncId ?\n\nCollectionAdminRequest#DeleteNode and ReplaceNode - Can we add a little bit of Javadocs explaining what the parameters are?\n\nIn CollectionParams, should REPLACENODE have lock level as none? What if we are replicaing a node and someone fires a split shard request? To be correct we need to lock right?\n\nIn OverseerCollectionMessageHandler#addReplica can't we make node and coreName final to start with?\n\n    final String fnode = node;\n    final String fcoreName = coreName;\n\n\n\nCan the tests please extend SolrCloudTestCase ? On my machine it takes one min for RepliceNodeTest to run. I'd guess it will be a lot faster when its using SolrCloudTestCase\n\nThe logging in this patch needs to be improved. It's too verbose and redundant.\n\nFor example these log line is so verbose and adds little value -\n\n\n42134 INFO  (TEST-ReplaceNodeTest.test-seed#[F960E2C81499AC90]) [    ] o.a.s.c.ReplaceNodeTest excluded_node : 127.0.0.1:51079_x%2Ftc  coll_state : {\n  \"replicationFactor\":\"2\",\n  \"shards\":{\n    \"shard1\":{\n      \"range\":\"80000000-b332ffff\",\n      \"state\":\"active\",\n      \"replicas\":{\n        \"core_node1\":{\n          \"core\":\"replacenodetest_coll_shard1_replica2\",\n          \"base_url\":\"http://127.0.0.1:51075/x/tc\",\n          \"node_name\":\"127.0.0.1:51075_x%2Ftc\",\n          \"state\":\"active\",\n          \"leader\":\"true\"},\n        \"core_node5\":{\n          \"core\":\"replacenodetest_coll_shard1_replica1\",\n          \"base_url\":\"http://127.0.0.1:51089/x/tc\",\n          \"node_name\":\"127.0.0.1:51089_x%2Ftc\",\n          \"state\":\"active\"}}},\n    \"shard2\":{\n      \"range\":\"b3330000-e665ffff\",\n      \"state\":\"active\",\n      \"replicas\":{\n        \"core_node2\":{\n          \"core\":\"replacenodetest_coll_shard2_replica1\",\n          \"base_url\":\"http://127.0.0.1:51084/x/tc\",\n          \"node_name\":\"127.0.0.1:51084_x%2Ftc\",\n          \"state\":\"active\"},\n        \"core_node9\":{\n          \"core\":\"replacenodetest_coll_shard2_replica2\",\n          \"base_url\":\"http://127.0.0.1:51094/x/tc\",\n          \"node_name\":\"127.0.0.1:51094_x%2Ftc\",\n          \"state\":\"active\",\n          \"leader\":\"true\"}}},\n    \"shard3\":{\n      \"range\":\"e6660000-1998ffff\",\n      \"state\":\"active\",\n      \"replicas\":{\n        \"core_node6\":{\n          \"core\":\"replacenodetest_coll_shard3_replica2\",\n          \"base_url\":\"http://127.0.0.1:51071/x/tc\",\n          \"node_name\":\"127.0.0.1:51071_x%2Ftc\",\n          \"state\":\"active\"},\n        \"core_node8\":{\n          \"core\":\"replacenodetest_coll_shard3_replica1\",\n          \"base_url\":\"http://127.0.0.1:51065/x/tc\",\n          \"node_name\":\"127.0.0.1:51065_x%2Ftc\",\n          \"state\":\"active\",\n          \"leader\":\"true\"}}},\n    \"shard4\":{\n      \"range\":\"19990000-4ccbffff\",\n      \"state\":\"active\",\n      \"replicas\":{\n        \"core_node3\":{\n          \"core\":\"replacenodetest_coll_shard4_replica2\",\n          \"base_url\":\"http://127.0.0.1:51075/x/tc\",\n          \"node_name\":\"127.0.0.1:51075_x%2Ftc\",\n          \"state\":\"active\",\n          \"leader\":\"true\"},\n        \"core_node7\":{\n          \"core\":\"replacenodetest_coll_shard4_replica1\",\n          \"base_url\":\"http://127.0.0.1:51089/x/tc\",\n          \"node_name\":\"127.0.0.1:51089_x%2Ftc\",\n          \"state\":\"active\"}}},\n    \"shard5\":{\n      \"range\":\"4ccc0000-7fffffff\",\n      \"state\":\"active\",\n      \"replicas\":{\n        \"core_node4\":{\n          \"core\":\"replacenodetest_coll_shard5_replica1\",\n          \"base_url\":\"http://127.0.0.1:51084/x/tc\",\n          \"node_name\":\"127.0.0.1:51084_x%2Ftc\",\n          \"state\":\"active\",\n          \"leader\":\"true\"},\n        \"core_node10\":{\n          \"core\":\"replacenodetest_coll_shard5_replica2\",\n          \"base_url\":\"http://127.0.0.1:51094/x/tc\",\n          \"node_name\":\"127.0.0.1:51094_x%2Ftc\",\n          \"state\":\"active\"}}}},\n  \"router\":{\"name\":\"compositeId\"},\n  \"maxShardsPerNode\":\"3\",\n  \"autoAddReplicas\":\"false\"}\n\n\n\nOn the redundant part:\n\nIn this patch I guess the purpose of adding logging to OverseerCollectionMessageHandler#deleteReplica and OverseerCollectionMessageHandler#addReplica was to be able to see when replicanode/deletenode calles addReplica and deleteReplica.\n\nThat log line in ReplaceNodeCmd should be like - \"calling addReplica for collection=[] shards=[] on node=[]\" . We shouldn't add a generic line to OverseerCollectionMessageHandler#addReplica . That will make anyone use AddReplica directly to see the same entry being logged twice ( the overseer logs it automatically and second one is the new line that is added in this patch) . Also in this patch its logged as log.info(\"going to create replica {}\", Utils.toJSONString(sourceReplica)); which is tougher to read then the approach mentioned above.\n\nSame goes with the DeleteReplaceNodeCmd\n\nThe tests will fail on our slower Jenkins. We should modify the hardcoded 200 count for loop with an infinite loop. if the command doesn't complete the test suite will fail which is fine. ",
            "id": "comment-15412998"
        },
        {
            "date": "2016-08-09T07:03:43+0000",
            "author": "Noble Paul",
            "content": "Thanks Varun Thacker\nCollectionAdminRequest#DeleteNode and ReplaceNode - Can we add a little bit of Javadocs explaining what the parameters are?\n\nsure.\n\nAny reason why we needed to change modify AsyncCollectionAdminRequest#setAsyncId ?\n\nYeah. Keeping that abstract was bad. That is deprecated\n\nIn OverseerCollectionMessageHandler#addReplica can't we make node and coreName final to start with?\n\nNo, they are modified in between\n\nThe logging in this patch needs to be improved. It's too verbose and redundant.\n\n\nLogging is screwed up. I added them for my testing. I shall clean them up. ",
            "id": "comment-15413069"
        },
        {
            "date": "2016-08-09T09:55:25+0000",
            "author": "Noble Paul",
            "content": "In CollectionParams, should REPLACENODE have lock level as none? What if we are replicaing a node and someone fires a split shard request? To be correct we need to lock right?\n\nREPLACENNODE just adds/removes replicas, so. it is no big deal. The operation happens across collections, so we have no idea what to lock here ",
            "id": "comment-15413307"
        },
        {
            "date": "2016-08-09T10:08:29+0000",
            "author": "Varun Thacker",
            "content": "REPLACENNODE just adds/removes replicas, so. it is no big deal. The operation happens across collections, so we have no idea what to lock here\n\n\nWon't we need to block updates to all shards of all collections that have replicas in the source node? What if we are doing a split shard - Replace node might copy the wrong data? ",
            "id": "comment-15413326"
        },
        {
            "date": "2016-08-09T10:50:15+0000",
            "author": "Noble Paul",
            "content": "We would need a node level locking, ideally. But, we don't have such a thing now ",
            "id": "comment-15413372"
        },
        {
            "date": "2016-08-11T06:44:19+0000",
            "author": "ASF subversion and git services",
            "content": "Commit ae60c74f8c6ea2f62e1870802accbcd73bbfdc48 in lucene-solr's branch refs/heads/master from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ae60c74 ]\n\nSOLR-9320: A REPLACENODE command to decommission an existing node with another new node and SOLR-9318 the DELETENODE command that deletes all replicas in a node ",
            "id": "comment-15416650"
        },
        {
            "date": "2016-08-11T06:54:16+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 519be6acf0866ce2f27e825b5b0035d39147af0b in lucene-solr's branch refs/heads/branch_6x from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=519be6a ]\n\nSOLR-9320: A REPLACENODE command to decommission an existing node with another new node and SOLR-9318 the DELETENODE command that deletes all replicas in a node ",
            "id": "comment-15416658"
        },
        {
            "date": "2016-08-11T07:09:48+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 4bd7d7fadbc72883f8ec9f2266daf0cd1f50b514 in lucene-solr's branch refs/heads/branch_6x from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=4bd7d7f ]\n\nSOLR-9320: SOLR-9318 The last commit screwed up ",
            "id": "comment-15416675"
        },
        {
            "date": "2016-08-12T14:24:17+0000",
            "author": "Varun Thacker",
            "content": "\n\tThe OverseerCollectionMessageHandler#checkRequired is not required . We already validate it earlier in CollectionsHandler\n\tFixed logging\n\tChanged OverseerCollectionMessageHandler#Cmd#call to return void as we don't need to return anything . All results are passed in the namedlist.\n\n\n\nNoble - What do you think about this patch? ",
            "id": "comment-15418905"
        },
        {
            "date": "2016-08-13T10:52:29+0000",
            "author": "Varun Thacker",
            "content": "New patch. It has the following changes.\n\n\n\tFixed logging\n\tChanged OverseerCollectionMessageHandler#Cmd#call to return void as we don't need to return anything . All results are passed in the named list.\n\n\n\nI am not messing with removing {[checkRequired}} from this patch as its not related. I'll commit this shortly. ",
            "id": "comment-15419872"
        },
        {
            "date": "2016-08-13T11:08:05+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 70d27aec83f9257da459f157acd9fc70764f7195 in lucene-solr's branch refs/heads/master from Varun Thacker\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=70d27ae ]\n\nSOLR-9320: Improve logging ",
            "id": "comment-15419877"
        },
        {
            "date": "2016-08-13T11:09:09+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 5fc35a65c39b7fb4ca49675d3ef9cd7f9d6c0fa8 in lucene-solr's branch refs/heads/branch_6x from Varun Thacker\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5fc35a6 ]\n\nSOLR-9320: Improve logging ",
            "id": "comment-15419878"
        },
        {
            "date": "2016-08-26T13:59:40+0000",
            "author": "Michael McCandless",
            "content": "Bulk close resolved issues after 6.2.0 release. ",
            "id": "comment-15439003"
        }
    ]
}