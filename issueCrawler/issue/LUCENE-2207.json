{
    "id": "LUCENE-2207",
    "title": "CJKTokenizer generates tokens with incorrect offsets",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/analysis"
        ],
        "type": "Bug",
        "fix_versions": [
            "2.9.2",
            "3.0.1",
            "4.0-ALPHA"
        ],
        "affect_versions": "2.9.1,                                            3.0",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "If I index a Japanese multi-valued document with CJKTokenizer and highlight a term with FastVectorHighlighter, the output snippets have incorrect highlighted string. I'll attach a program that reproduces the problem soon.",
    "attachments": {
        "TestCJKOffset.java": "https://issues.apache.org/jira/secure/attachment/12430150/TestCJKOffset.java",
        "LUCENE-2207.patch": "https://issues.apache.org/jira/secure/attachment/12430151/LUCENE-2207.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-01-13T16:57:36+0000",
            "content": "Attached the program that reproduces the problem. In the program, I didn't use FastVectorHighlighter, instead, I printed out offsets from TermVectorOffsetInfo. You'll see the following results:\n\n\n=== WhitespaceAnalyzer ===\n\u3042\u3044(0,2)\n\u3046\u3048\u304a(3,6)\n=== CJKAnalyzer ===\n\u3042\u3044(0,2)\n\u3046\u3048(4,6)\n\u3048\u304a(5,7)\n=== BasicNGramAnalyzer ===\n\u3042\u3044(0,2)\n\u3046\u3048(3,5)\n\u3048\u304a(4,6)\n\n\n\nFor people who are seeing garbage characters, I want to rephrase using 'Cn' symbols as follows:\n\n\n=== WhitespaceAnalyzer ===\nC1C2(0,2)\nC3C4C5(3,6)\n=== CJKAnalyzer ===\nC1C2(0,2)\nC3C4(4,6)\nC4C5(5,7)\n=== BasicNGramAnalyzer ===\nC1C2(0,2)\nC3C4(3,5)\nC4C5(4,6)\n\n\n\nAs you can see, the start offset of 'C3' is 3 in WhitespaceAnalyzer and BasicNGramAnalyzer (an analyzer which uses BasicNGramTokenizer. BasicNGramTokenizer is used in FastVectorHighlighter test code. It works as a 2-gram tokenizer for not only CJK but also ASCII), but is 4 in CJKAnalyzer \u2013 incorrect!\n\nI'll look into it tomorrow or after, but volunteers are welcome! ",
            "author": "Koji Sekiguchi",
            "id": "comment-12799827"
        },
        {
            "date": "2010-01-13T17:09:10+0000",
            "content": "Hi Koji, this looks like a bug in CJK offset calculations, probably involving end()\n\nPersonally i find the offset logic a little complex. It currently does both additions and subtractions to the offset and I think there is an off-by-one error in this.\n\nI will play around and see if I can simplify this logic, but please don't wait on me, if you have an idea already how to fix it! ",
            "author": "Robert Muir",
            "id": "comment-12799831"
        },
        {
            "date": "2010-01-13T17:48:19+0000",
            "content": "ok i found the bug. the problem is incrementToken() unconditionally increments the offset before it starts its main loop:\n\nline 165:\n\noffset++;\n\n\n\nso, when incrementToken() has no more text to return and returns false, it needs to subtract from this.\n\nagain i think in the future we try to refactor this offset logic to be simpler, but for the short term, this fixes the bug and all tests pass.\n\nKoji, can you review? ",
            "author": "Robert Muir",
            "id": "comment-12799841"
        },
        {
            "date": "2010-01-13T17:59:46+0000",
            "content": "i added a testcase for end() to my patch that fails on trunk, but passes with the fix. ",
            "author": "Robert Muir",
            "id": "comment-12799848"
        },
        {
            "date": "2010-01-13T18:16:02+0000",
            "content": "hello, this tokenizer has more serious offset/end problems than I originally thought.\n\nattached is my previous patch and testcase but with 3 more testcases, one still fails.  ",
            "author": "Robert Muir",
            "id": "comment-12799857"
        },
        {
            "date": "2010-01-17T03:51:27+0000",
            "content": "Hi Robert, thank you for looking this so quickly!\n\n\nok i found the bug. the problem is incrementToken() unconditionally increments the offset before it starts its main loop:\n\nline 165:\n\noffset++;\n\nIndeed.\n\nIn attached patch, I added one more offset-- line and two more testcases. All tests pass and this patch fixes the original problem that was found in Solr with FastVectorHighlighter. ",
            "author": "Koji Sekiguchi",
            "id": "comment-12801312"
        },
        {
            "date": "2010-01-17T07:25:15+0000",
            "content": "In attached patch, I added one more offset-- line and two more testcases. All tests pass and this patch fixes the original problem that was found in Solr with FastVectorHighlighter.\n\nnice, fix looks good to me. ",
            "author": "Robert Muir",
            "id": "comment-12801336"
        },
        {
            "date": "2010-01-17T08:50:41+0000",
            "content": "Koji, I am testing other end() offsets with all other tokenizers, I noticed that CJKTokenizer does not call correctOffset() in end:\n\n\nfinal int finalOffset = offset;\n\n\n\nI think instead this should be\n\nfinal int finalOffset = correctOffset(offset);\n\n\n\nin case there is a CharFilter present. ",
            "author": "Robert Muir",
            "id": "comment-12801346"
        },
        {
            "date": "2010-01-17T10:27:24+0000",
            "content": "\nI think instead this should be\n\nfinal int finalOffset = correctOffset(offset);\n\nAgreed, thank you for pointing this!\nI think this is ready to commit. Robert, can you do it? And it'd be great if it could go 2.9 branch so that Solr can use the fix. ",
            "author": "Koji Sekiguchi",
            "id": "comment-12801363"
        },
        {
            "date": "2010-01-17T10:31:08+0000",
            "content": "Koji, sure I can take care of it.\n\nAlso i added LUCENE-2219 to find these bugs in other tokenizers.\n\nIn the future I also want to explore if we can somehow use a fake CharFilter in BaseTokenStreamTest to also ensure that correctOffset() is called when setting offsets in both incrementToken() and end(), don't yet know how it would work yet. ",
            "author": "Robert Muir",
            "id": "comment-12801366"
        },
        {
            "date": "2010-01-17T21:45:27+0000",
            "content": "thanks Koji! ",
            "author": "Robert Muir",
            "id": "comment-12801518"
        }
    ]
}