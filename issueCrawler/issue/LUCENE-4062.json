{
    "id": "LUCENE-4062",
    "title": "More fine-grained control over the packed integer implementation that is chosen",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/other"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.0-ALPHA",
            "6.0"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "In order to save space, Lucene has two main PackedInts.Mutable implentations, one that is very fast and is based on a byte/short/integer/long array (Direct*) and another one which packs bits in a memory-efficient manner (Packed*).\n\nThe packed implementation tends to be much slower than the direct one, which discourages some Lucene components to use it. On the other hand, if you store 21 bits integers in a Direct32, this is a space loss of (32-21)/32=35%.\n\nIf you accept to trade some space for speed, you could store 3 of these 21 bits integers in a long, resulting in an overhead of 1/3 bit per value. One advantage of this approach is that you never need to read more than one block to read or write a value, so this can be significantly faster than Packed32 and Packed64 which always need to read/write two blocks in order to avoid costly branches.\n\nI ran some tests, and for 10000000 21 bits values, this implementation takes less than 2% more space and has 44% faster writes and 30% faster reads. The 12 bits version (5 values per block) has the same performance improvement and a 6% memory overhead compared to the packed implementation.\n\nIn order to select the best implementation for a given integer size, I wrote the PackedInts.getMutable(valueCount, bitsPerValue, acceptableOverheadPerValue) method. This method select the fastest implementation that has less than acceptableOverheadPerValue wasted bits per value. For example, if you accept an overhead of 20% (acceptableOverheadPerValue = 0.2f * bitsPerValue), which is pretty reasonable, here is what implementations would be selected:\n\n\n\t1: Packed64SingleBlock1\n\t2: Packed64SingleBlock2\n\t3: Packed64SingleBlock3\n\t4: Packed64SingleBlock4\n\t5: Packed64SingleBlock5\n\t6: Packed64SingleBlock6\n\t7: Direct8\n\t8: Direct8\n\t9: Packed64SingleBlock9\n\t10: Packed64SingleBlock10\n\t11: Packed64SingleBlock12\n\t12: Packed64SingleBlock12\n\t13: Packed64\n\t14: Direct16\n\t15: Direct16\n\t16: Direct16\n\t17: Packed64\n\t18: Packed64SingleBlock21\n\t19: Packed64SingleBlock21\n\t20: Packed64SingleBlock21\n\t21: Packed64SingleBlock21\n\t22: Packed64\n\t23: Packed64\n\t24: Packed64\n\t25: Packed64\n\t26: Packed64\n\t27: Direct32\n\t28: Direct32\n\t29: Direct32\n\t30: Direct32\n\t31: Direct32\n\t32: Direct32\n\t33: Packed64\n\t34: Packed64\n\t35: Packed64\n\t36: Packed64\n\t37: Packed64\n\t38: Packed64\n\t39: Packed64\n\t40: Packed64\n\t41: Packed64\n\t42: Packed64\n\t43: Packed64\n\t44: Packed64\n\t45: Packed64\n\t46: Packed64\n\t47: Packed64\n\t48: Packed64\n\t49: Packed64\n\t50: Packed64\n\t51: Packed64\n\t52: Packed64\n\t53: Packed64\n\t54: Direct64\n\t55: Direct64\n\t56: Direct64\n\t57: Direct64\n\t58: Direct64\n\t59: Direct64\n\t60: Direct64\n\t61: Direct64\n\t62: Direct64\n\n\n\nUnder 32 bits per value, only 13, 17 and 22-26 bits per value would still choose the slower Packed64 implementation. Allowing a 50% overhead would prevent the packed implementation to be selected for bits per value under 32. Allowing an overhead of 32 bits per value would make sure that a Direct* implementation is always selected.\n\nNext steps would be to:\n\n\tmake lucene components use this getMutable method and let users decide what trade-off better suits them,\n\twrite a Packed32SingleBlock implementation if necessary (I didn't do it because I have no 32-bits computer to test the performance improvements).\n\n\n\nI think this would allow more fine-grained control over the speed/space trade-off, what do you think?",
    "attachments": {
        "LUCENE-4062.patch": "https://issues.apache.org/jira/secure/attachment/12527599/LUCENE-4062.patch",
        "PackedZero.java": "https://issues.apache.org/jira/secure/attachment/12534476/PackedZero.java",
        "PackedIntsBenchmark.java": "https://issues.apache.org/jira/secure/attachment/12532181/PackedIntsBenchmark.java",
        "measurements_te_xeon.txt": "https://issues.apache.org/jira/secure/attachment/12533634/measurements_te_xeon.txt",
        "measurements_te_graphs.pdf": "https://issues.apache.org/jira/secure/attachment/12533631/measurements_te_graphs.pdf",
        "measurements_te_p4.txt": "https://issues.apache.org/jira/secure/attachment/12533633/measurements_te_p4.txt",
        "measurements_te_i7.txt": "https://issues.apache.org/jira/secure/attachment/12533632/measurements_te_i7.txt",
        "Packed64calc.java": "https://issues.apache.org/jira/secure/attachment/12533598/Packed64calc.java",
        "Packed64SingleBlock.java": "https://issues.apache.org/jira/secure/attachment/12533826/Packed64SingleBlock.java",
        "LUCENE-4062-2.patch": "https://issues.apache.org/jira/secure/attachment/12532111/LUCENE-4062-2.patch",
        "Packed64Strategy.java": "https://issues.apache.org/jira/secure/attachment/12533990/Packed64Strategy.java"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-05-16T09:59:21+0000",
            "content": "Here is the patch. ",
            "author": "Adrien Grand",
            "id": "comment-13276630"
        },
        {
            "date": "2012-05-16T10:53:03+0000",
            "content": "This is a great idea!  I don't think it's necessary to create a Packed32SingleBlock right now ... this (Packed64SingleBlock) is already a great improvement.  We can do it later...\n\nSomehow we need to fix the fasterButMoreRAM places (FieldCache, DocValues) to make use of this; maybe we change them from a boolean to the float acceptableOverhead instead? ",
            "author": "Michael McCandless",
            "id": "comment-13276656"
        },
        {
            "date": "2012-05-16T12:14:27+0000",
            "content": "Hi Mike, thanks for your quick feedback.\n\nAgreed on Packed32SingleBlock. Regarding fasterButMoreRAM/roundFixedSize, the acceptableOverhead parameter currently is in bits per value. However I think most use-cases would be more interested in the overhead ratio than in its absolute value. So I think they should take a float acceptableOverheadRatio and call getMutable(valueCount, bitsPerValue, bitsPerValue * acceptableOverheadRatio). I'm interested in doing this, would you like me to include these changes in the patch or does it deserve a separate commit?\n ",
            "author": "Adrien Grand",
            "id": "comment-13276688"
        },
        {
            "date": "2012-05-16T12:20:22+0000",
            "content": "Hi Adrien,\n\nI think we should do the cutover as part of this issue, so yes, please, take a stab at that!\n\nI agree the overhead should be measured in %tg not as bits-per-value.  Maybe we also have public constants for the two extremes (zero overhead but possibly slow; super-fast but possibly lots of overhead)? ",
            "author": "Michael McCandless",
            "id": "comment-13276692"
        },
        {
            "date": "2012-05-16T12:25:53+0000",
            "content": "Can we also move the concrete impls (eg Packed64SingleBlock21) either as static inner classes inside Packed64SingleBlock, or as separate source files?  Thanks.  We've had trouble w/ multiple classes in a single java source file in the past... ",
            "author": "Michael McCandless",
            "id": "comment-13276695"
        },
        {
            "date": "2012-05-16T12:31:09+0000",
            "content": "As a quick test I temporarily forced the PackedInts impl to always use Packed64SingleBlock when possible and all tests passed!  Good  ",
            "author": "Michael McCandless",
            "id": "comment-13276698"
        },
        {
            "date": "2012-05-16T14:35:27+0000",
            "content": "I added the changes we discussed in the patch, replacing fasterButMoreRam=true by PackedInts.FAST and fasterButMoreRam=false by PackedInts.DEFAULT = 20%. All Lucene tests passed. ",
            "author": "Adrien Grand",
            "id": "comment-13276769"
        },
        {
            "date": "2012-05-16T15:49:45+0000",
            "content": "Updated patch with improved performance when bitsPerValue is close (depending on the acceptable overhead) to 24 or 48. The two new implementations store values on three contiguous blocks (byte and short) and have very close performance to the Direct* implementations. Their maximum capacity is lower (by a factor 3) so PackedInts.getMutable decides whether they should be selected based on valueCount. All tests still pass. ",
            "author": "Adrien Grand",
            "id": "comment-13276824"
        },
        {
            "date": "2012-05-16T16:23:37+0000",
            "content": "Mike, I just noticed that I should have updated the PackedInts#getReader method as well but I have no time to work on this issue until next week. I will try to finish this patch next monday. ",
            "author": "Adrien Grand",
            "id": "comment-13276855"
        },
        {
            "date": "2012-05-16T23:08:29+0000",
            "content": "New patch looks great (but yeah we should do getReader too...)!  I like the new impls for 24/48 cases.  Thanks Adrien! ",
            "author": "Michael McCandless",
            "id": "comment-13277244"
        },
        {
            "date": "2012-05-21T13:23:32+0000",
            "content": "New patch with updated getReader method. All tests still pass. ",
            "author": "Adrien Grand",
            "id": "comment-13280137"
        },
        {
            "date": "2012-05-21T14:05:45+0000",
            "content": "Updated javadocs (there were still a few references to `fasterButMoreRam`). ",
            "author": "Adrien Grand",
            "id": "comment-13280162"
        },
        {
            "date": "2012-05-21T23:27:16+0000",
            "content": "Thanks Adrien!\n\nHmm I'm seeing some tests failing with this patch (eg TestRollingUpdates).  I haven't tried to track it down...\n\nThis new copyData worries me: I don't think, at read time, we should be rewriting all packed ints?  I think we should sort that out at write time and then reading just use whatever reader can best decode what was written?  Also, it seems like the logic may be broken?  Eg it uses Direct16 when bitsPerValue is < Short.SIZE?  (Same for Direct32)... maybe the if cases are just swapped... ",
            "author": "Michael McCandless",
            "id": "comment-13280616"
        },
        {
            "date": "2012-05-22T16:17:10+0000",
            "content": "Oh right, cases are swapped, I uploaded the wrong patch. Here is a correct version.\n\nI guess there are two options:\n 1. always writing the most compact form to disk and deserializing it in a fast PackedInts.Mutable implementation (depending on acceptableOverheadRatio),\n 2. writing a possibly aligned version on disk (depending on acceptableOverheadRatio) and then selecting the fastest PackedInts.Mutable implementation that exactly matches bitsPerValue (with no padding bits).\n\nA third option could be to write padding bits (Packed64SingleBlock subclasses may have such padding bits) as well, but I really dislike the fact that the on-disk format is implementation-dependent.\n\nOption 1 is likely to make deserialization slower since some decoding might occur (the copyData method) but on the other hand, option 2 would prevent us from using implementations that add padding bits (such as Packed64SingleBlock21 which has one padding bit every 3 21-bits integers or Packed64SingleBlock12 which has 4 padding bits every 5 12-bits integers but not Packed64SingleBlock\n{1,2,4} since 64%{1,2,4}\n=0).\n\nI initially chose option 1 because I think it is nice to have Packed64SingleBlock21 when bitsPerValue is close to 21, since it might be significantly faster than Packed64.\n\nIn order to know how slower deserialization is (option 1), I ran a micro-benchmark which:\n\n\tloads a PackedInts.Reader from a ByteArrayDataInput,\n\tperforms n random reads on the reader.\n\n\n\nHere is the code for the micro-benchmark in case you would like to run it.\n\nint valueCount = 10000000;\nint bitsPerValue = 21;\nint[] offsets = new int[valueCount];\nRandom random = new Random();\nfor (int i = 0; i < valueCount; ++i) {\n  offsets[i] = random.nextInt(valueCount);\n}\nbyte[] bytes = new byte[valueCount * 4];\nDataOutput out = new ByteArrayDataOutput(bytes);\nPackedInts.Writer writer = PackedInts.getWriter(out, valueCount, bitsPerValue);\nfor (int i = 0; i < valueCount; ++i) {\n  writer.add(random.nextInt(1 << bitsPerValue));\n}\nwriter.finish();\nfor (int i = 0; i < 50; ++i) {\n  long start = System.nanoTime();\n  DataInput in = new ByteArrayDataInput(bytes);\n  PackedInts.Reader reader = PackedInts.getReader(in, 0f); // Packed64\n  // PackedInts.Reader reader = PackedInts.getReader(in, 0.1f); // Packed64SingleBlock\n  for (int j = 0, n = valueCount; j < n; ++j) {\n    reader.get(offsets[j]);\n  }\n  long end = System.nanoTime();\n  System.out.println(end - start);\n}\n\n\n\nI ran this microbenchmark for bitsPerValue=21 and valueCount in (1 000 000, 10 000 000). The loading time (n = 0) is 2x to 3x slower with Packed64SingleBlock21. However, as soon as you perform valueCount/4 or more random reads (n >= valueCount/4), the total time is better for Packed64SingleBlock21. When n=valueCount, the total time is even 2x better for Packed64SingleBlock21.\n\nThe loading overhead doesn't seem too bad.\n\nHowever I guess that some people might still be more interested in the loading time than in the query time (for write-intensive applications), but in this case they could still request a reader in its compact form (Packed 64 or Direct* when bitsPerValue is 8, 16, 32 or 64). If they want to be sure to have a fast reader too, they could also make sure that they used 8, 16, 32 or 64 as the bitsPerValue parameter of PackedInts.getWriter.\n\nMike, what do you think? ",
            "author": "Adrien Grand",
            "id": "comment-13281071"
        },
        {
            "date": "2012-05-22T20:19:47+0000",
            "content": "Hi Adrien. I wrote back to dev list, don't know if you caught that \u2013 the benchmark could probably be improved by calculating something off reader.get() because otherwise it's a pure read without side effects and can be removed by the jit (or at least optimized in unpredictable ways). A rolling sum or something will probably do. ",
            "author": "Dawid Weiss",
            "id": "comment-13281188"
        },
        {
            "date": "2012-05-22T21:01:49+0000",
            "content": "Good catch, David. I modified the code to perform a rolling sum and the results are actually a little different. Packed64SingleBlock21 is now faster than Packed64 when n>=valueCount (instead of valueCount/4).\n ",
            "author": "Adrien Grand",
            "id": "comment-13281211"
        },
        {
            "date": "2012-05-23T07:42:57+0000",
            "content": "You didn't attach the updated benchmark \u2013 I didn't say it explicitly but you should do something with the resulting value (jit optimizer is quite smart . A field store (write the result to a field) should do the trick. So is System.out.println of course...\n\nAll this may sound paranoid but really isn't. This is a source of many problems with microbenchmarks \u2013 the compiler just throws away (or optimizes loops/ branches) in a way that doesn't happen later on in real code. My recent favorite example of such a problem in real life code (it's a bug in jdk) is this one:\n\nhttp://hg.openjdk.java.net/jdk8/tl/jdk/rev/332bebb463d1 ",
            "author": "Dawid Weiss",
            "id": "comment-13281442"
        },
        {
            "date": "2012-05-23T08:46:47+0000",
            "content": "Hi David. Thanks for the link, it's very interesting!\n\nI added a print statement to make sure that the sum is actually computed. Here is the code (for values of n > valueCount, just modify the k loop):\n\n\nint valueCount = 10000000;\nint bitsPerValue = 21;\nint[] offsets = new int[valueCount];\nRandom random = new Random();\nfor (int i = 0; i < valueCount; ++i) {\n  offsets[i] = random.nextInt(valueCount);\n}\nbyte[] bytes = new byte[valueCount * 4];\nDataOutput out = new ByteArrayDataOutput(bytes);\nPackedInts.Writer writer = PackedInts.getWriter(out, valueCount, bitsPerValue);\nfor (int i = 0; i < valueCount; ++i) {\n  writer.add(random.nextInt(1 << bitsPerValue));\n}\nwriter.finish();\nlong sum = 0L;\nfor (int i = 0; i < 50; ++i) {\n  long start = System.nanoTime();\n  DataInput in = new ByteArrayDataInput(bytes);\n  // PackedInts.Reader reader = PackedInts.getReader(in, 0f); // Packed64\n  PackedInts.Reader reader = PackedInts.getReader(in, 0.1f); // Packed64SingleBlock\n  for (int k = 0; k < 1; ++k) {\n      for (int j = 0, n = valueCount / 2; j < n; ++j) {\n        sum += reader.get(offsets[j]);\n      }\n  }\n  long end = System.nanoTime();\n  System.out.println(\"sum is \" + sum);\n  System.out.println(end - start);\n}\n\n\n\nI'm on a different computer today and n >= valueCount/3 is enough to make the benchmark faster with Packed64SingleBlock. ",
            "author": "Adrien Grand",
            "id": "comment-13281460"
        },
        {
            "date": "2012-05-23T17:48:50+0000",
            "content": "A third option could be to write padding bits (Packed64SingleBlock subclasses may have such padding bits) as well, but I really dislike the fact that the on-disk format is implementation-dependent.\n\nActually, I think we should stop specializing based on 32 bit vs 64\nbit JRE, and always use the impls backed by long[] (Packed64*).  Then, I\nthink it's fine if we write the long[] image (with padding bits)\ndirectly to disk? ",
            "author": "Michael McCandless",
            "id": "comment-13281751"
        },
        {
            "date": "2012-05-23T18:42:17+0000",
            "content": "Mike, I am not sure how we should do it. For 21-bits values how would the reader know whether it should use a Packed64SingleBlock21 or a Packed64? Should we add a flag to the data stream in order to know what implementation serialized the integers? ",
            "author": "Adrien Grand",
            "id": "comment-13281793"
        },
        {
            "date": "2012-05-23T18:46:06+0000",
            "content": "Should we add a flag to the data stream in order to know what implementation serialized the integers?\n\nI think so? ",
            "author": "Michael McCandless",
            "id": "comment-13281797"
        },
        {
            "date": "2012-05-23T19:01:01+0000",
            "content": "Isn't it a problem to break compatibility? Or should we use special (> 64) values of bitsPerValue so that current trunk indexes will still work after the patch is applied? ",
            "author": "Adrien Grand",
            "id": "comment-13281805"
        },
        {
            "date": "2012-05-23T19:14:22+0000",
            "content": "Isn't it a problem to break compatibility? \n\nIt isn't.\n\n3x indices never store packed ints ... so we are only breaking doc values in 4.0, and we are allowed (for only a bit more time!) to break 4.0's index format.  So we should just break it and not pollute 4.0's sources with false back compat code...\n\nSeparately, if somehow we did need to preserve back compat for packed ints file format... we should use the version in the codec header to accomplish that (ie, we don't have to stuff version information inside the bitsPerValue). ",
            "author": "Michael McCandless",
            "id": "comment-13281814"
        },
        {
            "date": "2012-05-24T12:58:12+0000",
            "content": "New patch. I added a VInt flag to the streams generated by writers so that the readers can know how to parse the stream. All tests passed. ",
            "author": "Adrien Grand",
            "id": "comment-13282480"
        },
        {
            "date": "2012-05-25T16:51:04+0000",
            "content": "Thanks Adrien, this looks great!  I'll commit soon... ",
            "author": "Michael McCandless",
            "id": "comment-13283593"
        },
        {
            "date": "2012-05-25T18:10:13+0000",
            "content": "Thanks Adrien! ",
            "author": "Michael McCandless",
            "id": "comment-13283655"
        },
        {
            "date": "2012-06-14T16:20:04+0000",
            "content": "I have run more tests on PackedInts impls over the last days to test their relative performance.\n\nIt appears that the specializations in Packed64SingleBlock don't help much and even hurt performance in some cases. Moreover, replacing the naive bulk operations by a System.arraycopy in Direct64 is a big win. (See attached patch.)\n\nYou can look at the details of the tests here: http://people.apache.org/~jpountz/packed_ints.html (contiguous=Packed64, padding=Packed64SingleBlock,3 blocks=Packed*ThreeBlocks,direct=Direct*).\n\nThe tests were run on a 64-bit computer (Core 2 Duo E5500) with valueCount=10 000 000. \"Memory overhead\" is \n{unused space in bits}\n/\n{bits per value}\n while the other charts measure the number of gets/sets per second.\n\nThe random get/set results are very good for the packed versions, probably because they manage to fit much more values into the CPU caches than other impls. The reason why bulk get/set is faster when bitsPerValue>32 is that Direct64 uses System.arraycopy instead of naive copy (in a for loop).\n\nInterestingly, the different impls have very close random get performance. ",
            "author": "Adrien Grand",
            "id": "comment-13295139"
        },
        {
            "date": "2012-06-14T17:09:57+0000",
            "content": "What's on the axes in those plots? System.copyarray is an intrinsic \u2013 it'll be much faster than any other loop that doesn't eliminate bounds checks (and I think with more complex logic this will not be done). ",
            "author": "Dawid Weiss",
            "id": "comment-13295167"
        },
        {
            "date": "2012-06-14T17:30:24+0000",
            "content": "The x axis is the number of bits per value while the y axis is the number of values that are read or written per second. For every bitsPerValue and bit-packing scheme, I took the impl with the lowest working bitsPerValue. (For example, bitsPerValue=19 would give a Direct32, a Packed64(bitsPerValue=19), a Packed8ThreeBlocks(24 bits per value) and a Packed64SingleBlock(bitsPerValue=21)). There are 4 lines because we currently have 4 different bit-packing schemes.\n\nIn the two first cases, values are read at random offsets while the two bulk tests read/write a large number of values sequentially. I didn't want to test System.arraycopy against a naive for-loop, I just noticed that Direct64 bulk operations didn't use arraycopy, so I fixed that and added a few words about it so that people understand why the throughput increases when bitsPerValue > 32, which is counter-intuitive. ",
            "author": "Adrien Grand",
            "id": "comment-13295176"
        },
        {
            "date": "2012-06-14T17:57:18+0000",
            "content": "Ok, thanks - makes sense. Is the code for these benchmarks somewhere?  ",
            "author": "Dawid Weiss",
            "id": "comment-13295195"
        },
        {
            "date": "2012-06-15T00:08:09+0000",
            "content": "Very cool graphs!  Somehow you should turn them into a blog post  ",
            "author": "Michael McCandless",
            "id": "comment-13295395"
        },
        {
            "date": "2012-06-15T10:20:18+0000",
            "content": "@Dawid I attached the file I used to compute these numbers. There is a lot of garbage in the output because I print some stuff to prevent the JVM from doing optimizations.\n\n@Mike Yes, I was actually thinking of it too...  ",
            "author": "Adrien Grand",
            "id": "comment-13295569"
        },
        {
            "date": "2012-06-15T10:24:02+0000",
            "content": "I thought about the full cycle you used to produce the pretty graphs  ",
            "author": "Dawid Weiss",
            "id": "comment-13295571"
        },
        {
            "date": "2012-06-15T10:31:33+0000",
            "content": "Oh, sorry! When the program ends (it takes to lot of time to run), there are 4 sections at the end (SET, GET, BULKSET and BULKGET) that contain a list of arrays (which are actually JSON arrays). To generate the charts, just take the HTML source of http://people.apache.org/~jpountz/packed_ints.html (which uses Google Charts API https://developers.google.com/chart/) and replace the JSON arrays with the ones you just generated (lines 101, 184, 267 and 349).  ",
            "author": "Adrien Grand",
            "id": "comment-13295575"
        },
        {
            "date": "2012-06-15T10:53:39+0000",
            "content": "Ah, nice. We were using google charts but then switched to this one \u2013 http://code.google.com/p/flot/. It is nice but of course comes with its own limitations. \n\nAs for the benchmark \u2013 you may want to peek at Google Caliper (http://code.google.com/p/caliper/) or JUnitBenchmarks (http://labs.carrotsearch.com/junit-benchmarks.html) instead of writing your own code for doing warmups, etc. Caliper is nice in that it spawns a separate JVM so you can minimize the influence of code order execution on hotspot optimizations. Just a thought. ",
            "author": "Dawid Weiss",
            "id": "comment-13295580"
        },
        {
            "date": "2012-06-15T12:06:08+0000",
            "content": "We were using google charts but then switched to this one \u2013 http://code.google.com/p/flot/.\n\nWhat was your motivation?\n\nAs for the benchmark \u2013 you may want to peek at Google Caliper (http://code.google.com/p/caliper/) or JUnitBenchmarks (http://labs.carrotsearch.com/junit-benchmarks.html)\n\nThanks for the JUnitBenchmarks link, I didn't know it. How does it compare to Caliper? ",
            "author": "Adrien Grand",
            "id": "comment-13295612"
        },
        {
            "date": "2012-06-15T12:34:03+0000",
            "content": "What was your motivation?\n\nMostly aesthetics. I guess same things can be done with both.\n\nThanks for the JUnitBenchmarks link, I didn't know it. How does it compare to Caliper?\n\nThey're different. I wanted something I could use with JUnit (existing toolchains) without much extra overhead. Caliper is more advanced, supports running under different jvms (forks the task), supports cartesian sets of attributes. It's nice. I use both from time to time. ",
            "author": "Dawid Weiss",
            "id": "comment-13295630"
        },
        {
            "date": "2012-06-19T13:08:58+0000",
            "content": "Committed (r1351682 on trunk and r1351698 on branch 4.x). Thanks Mike and Dawid for your comments. ",
            "author": "Adrien Grand",
            "id": "comment-13396747"
        },
        {
            "date": "2012-06-27T03:03:51+0000",
            "content": "I tried running the PackedIntsBenchmark on an i7 processor and I agree on the overall conclusion with regard to the speed, or rather lack of speed, for Packed64. However, my the winners for the different BPVs were not always the same as Adrien observed. I suspect that CPU architecture and memory system, especially caching, plays a very large role here.\n\nRe-thinking the no-branching idea for Packed64, it seems that in reality it is slower to perform two memory requests (where the second will most probably be cached) than take the pipeline flush. Therefore I have created Packed64calc (see attachment), which is a full replacement for Packed64.\n\nMy own tests shows Packed64calc to be significantly faster than Packed64 and in many cases faster than Packed64SingleBlock. I suspect the latter to be caused either by caching or the fact that Packed64SingleBlock uses division and modulo for set & get. While the modulo can be avoided in Packed64SingleBlock, I never did find a reliable way to bypass the division when I experimented with it.\n\nI have attached an updated PackedIntsBenchmark which can be used with Packed64calc and hope that Adrien will take a look. ",
            "author": "Toke Eskildsen",
            "id": "comment-13401924"
        },
        {
            "date": "2012-06-27T09:12:14+0000",
            "content": "Thanks for your patch, Toke. All tests seem to pass, I'll try to generate graphs for your impl as soon as possible! ",
            "author": "Adrien Grand",
            "id": "comment-13402075"
        },
        {
            "date": "2012-06-27T09:44:08+0000",
            "content": "I ran the test on three different machines. results are attached as measurements*.txt along with a PDF with graphs generated from iteration #6 (which should probably be the mean or max of run 2-5). The setter-graph for the p4 looks extremely strange for Direct, but I tried generating a graph for iteration #5 instead and it looked the same. in the same vein, the Direct performance for the Xeon is suspiciously low, so I wonder if there's some freaky JITting happening to the test code.\n\nUnfortunately I did not find an AMD machine to test on. For the three tested Intels, it seems that the Packed64calc does perform very well. ",
            "author": "Toke Eskildsen",
            "id": "comment-13402089"
        },
        {
            "date": "2012-06-27T12:56:22+0000",
            "content": "Thanks for sharing your results. Here are mines: http://people.apache.org/~jpountz/packed_ints_calc.html (E5500 @ 2.80GHz, java 1.7.0_02, hotspot build 22.0-b10). Funny to see those little bumps when the number of bits per value is 8, 16, 32 or 64 (24 as well, although it is smaller)!\n\nIt is not clear whether this impl is faster or slower than the single-block impl (or even the 3 blocks impl, I am happily surprised by the read throughput on the intel 4 machine) depending on the hardware. However, this new impl seems to be consistently better than the current Packed64 class so I think we should replace it with your new impl. What do you think? Can you write a patch? ",
            "author": "Adrien Grand",
            "id": "comment-13402201"
        },
        {
            "date": "2012-06-27T15:01:52+0000",
            "content": "Making Packed64calc the new Packed64 seems like a safe bet. I'd be happy to create a patch for it. Should I open a new issue or add the patch here? If I do it here, how do we avoid confusing the original fine-grained-oriented patch from the Packed64 replacement?\n\nI think it it hard to see a clear pattern as to which Mutable implementation should be selected for the different size & bpv-requirements, with the current available measurements. I'll perform some more experiments with JRE1.6/JRE1.7 on different hardware and see if the picture gets clearer. ",
            "author": "Toke Eskildsen",
            "id": "comment-13402279"
        },
        {
            "date": "2012-06-27T15:06:00+0000",
            "content": "Yes, a new issue will make things clearer. Thanks, Toke! ",
            "author": "Adrien Grand",
            "id": "comment-13402283"
        },
        {
            "date": "2012-06-28T12:47:04+0000",
            "content": "I have tried running the performance test again with different setups. Graphs at http://ekot.dk/misc/packedints/\n\nMost of the tests under JRE 1.7 are still running, but from the single JRE 1.6 vs. JRE 1.7 (at the bottom), it would seem that there is a difference in the performance of Packed64SingleBlock vs. Packed64. Take that with a grain of salt though, as the results from that machine are quite strange (especially for Direct sets).\n\nIf the Java version influences which choice is best and if we throw valueCount into the mix (or rather cache utilization, which is very much non-determinable when a Mutable is requested), I am afraid that it will be very hard to auto-recommend a specific implementation.\n\nI will update the page and notify here, when more tests has finished. ",
            "author": "Toke Eskildsen",
            "id": "comment-13403064"
        },
        {
            "date": "2012-06-28T13:00:40+0000",
            "content": "Very interesting. Out of curiousity could you try to run the tests on your machines with the attached Packed64SingleBlock.java? It generates sub-classes that might help the JVM optimize some multiplications/remainders. I'd be interested to see what the difference is on your machines (on mine, it is slightly faster than the current Packed64SingleBlock impl). ",
            "author": "Adrien Grand",
            "id": "comment-13403074"
        },
        {
            "date": "2012-06-29T14:38:36+0000",
            "content": "Nice trick with the inheritance in Packed64SingleBlock.java. I really helped a lot on the machines I tested with. I tried to use the Strategy Pattern myself, using switch on the final bitsPerValue in set & get, but it seems that Hotspot is not keen on optimizing such a conditional away. I have now used the inheritance method to provide faster implementations in Packed64 for 1, 2, 4, 8, 16, 32 and 64 bpv. Due to the existence of the Direct classes, this is only relevant for 1, 2 & 4 bpv though.\n\nSome new measurements at http://ekot.dk/misc/packedints/ with the following implementations:\n\n\tcontiguous: The Packed64 that is being reviewed at LYCENE-4171\n\tpadding: The new Packed64SingleBlock.java with sub-classes\n\t3 blocks: As before\n\tdirect: As before\n\told padding: Packed64SingleBlock.java without sub-classes\n\tcontiguous strategy: Packed64Strategy.java, which uses sub-classes to optimize for bpv 2^n\n\n\n\nI changed PackedIntsBenchmark to report the highest measured performance from the iterations instead of the mean. The rationale is that the mean is vulnerable to machine load fluxations and GC throughout the whole measurement process.\n\nMy observations are that the measurements from atria & pc254, which are relatively old machines, are very similar and fits well with the theory: Padding does give higher or equal performance than Packed64 at all other bpv's than 2^n.\n\nThe story is not so clear for mars, which is a very fast machine: For the current test case of 10M values, the padding only provides better performance after 18-20 bpv and for some bpv, Packed64 is a bit faster. I suspect that point would be moved if other processes were competing for memory cache. Still pending are measurements from my i7-machine. I hope to add them this evening or tomorrow.\n\nAlso pending is why the performance of Direct set is so low for mars. It makes no sense: Even if there is some freaky hit for requesting bytes, shorts or ints on that machine, performance should be >= everything else at 64bpv.\n\nWhen should we stop optimizing? For 1 bpv, a list of booleans would probably be faster, so should we make a PackedBoolean? Similarly, Packed8, Packed16 and Packed32 would also make sense if we were really determined. None of the implementations are hard to make, but it becomes a hassle to update when a new feature is needed. ",
            "author": "Toke Eskildsen",
            "id": "comment-13403941"
        },
        {
            "date": "2012-06-29T15:31:42+0000",
            "content": "This is very interesting!\n\nResults computed on mars are very strange so I think we should not take them into account to make decisions...\n\nWhen should we stop optimizing?\n\nThis is a relevant question. I think there is no problem going further provided that we have evidence that the changes improve performance in most cases and that the code remains maintainable. For example, it is probably not necessary to work on specialized Packed64 for the 2^n bits cases since there are direct impls for the 8, 16, 32 and 64 bits cases and since the new Packed64SingleBlock impl seems to be as fast for 1, 2 and 4 bits per value.\n\nHowever, I think it makes sense to replace Packed64SingleBlock with the inheritance-based impl since it seems faster than the current impl and than Packed64. I will open an issue if the graphs computed on your core i7 computer confirm this trend. ",
            "author": "Adrien Grand",
            "id": "comment-13403971"
        },
        {
            "date": "2012-06-29T21:31:44+0000",
            "content": "+1 for replacing the Packed64SingleBlock with the optimized version. It is consistently better.\n\nI have updated the charts at http://ekot.dk/misc/packedints/ with i7 measurements. For the three non-i7-based Intel processors, Packed64SingleBlock seems clear-cut for the \"Best possible performance without going all-out with Direct\".\n\nI tried running two tests in parallel on te-prime (see http://ekot.dk/misc/packedints/te-prime_parallel.html) and got very consistent results:\n\n\tFor set, contiguous strategy is faster than or equal to padded for bpvs 3-12 and 16-21 (with bpv 1 and 2 being hard to measure). Since padded only supports bpv 1-10, 12, 16, 21 & 32 with non-conforming bpvs rounded up, this effectively means that there is zero gain in using padded over contiguous with regard to set on that machine.\n\tFor get, the picture is nearly the same, except for bpv 17-21 where contiguous is slower than padded (observed in process 2 as well as the single-thread run). The difference is less than 10% though. The same pattern, although noisier, can be seen on mars.\n\n\n\nMy preliminary conclusion for i7-processors is thus that Packed64Strategy is the right choice for \"Best possible performance without going all-out with Direct\". I am getting very curious about the untested AMD architecture now.\n\nA by-product of the te-prime parallel test is that the amount of cache seems to matter little when it comes to selecting the most fitting implementation. Thank $diety for small favors. ",
            "author": "Toke Eskildsen",
            "id": "comment-13404234"
        },
        {
            "date": "2012-06-30T21:21:44+0000",
            "content": "Another angle on the auto-selection problem: Packed64SingleBlock supports 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 16, 21, 32 as bpv's. The Direct-classes handles 8, 16 & 32 bpv and with the inheritance-trick, Packed64 handles 1, 2 & 4 exactly as Packed64SingleBlock. This leaves us with 3, 5, 6, 7, 9, 10, 12 and 21 bpv as \"unique\" implementations. In the hope of getting a better overview, I tried making bar-charts with those bps's: http://ekot.dk/misc/packedints/padding.html\n\nAgain, it would be illuminating to get measurements from an older as well as a newer AMD machine. I also hope to get the time to run the test on another i7-machine to see if the trend from te-prime and mars can be confirmed, but I am leaving for vacation on Tuesday. As of now, the most fitting implementation depends on the machine and since the choice of implementation affects the persistence format, I find this problematic. It is not optimal for a heterogeneous distributed setup. ",
            "author": "Toke Eskildsen",
            "id": "comment-13404629"
        },
        {
            "date": "2012-07-01T20:41:50+0000",
            "content": "The graphs are updated with data from my i7 desktop machine. The data looks a lot like the measurements from the other two i7-machines. ",
            "author": "Toke Eskildsen",
            "id": "comment-13404801"
        },
        {
            "date": "2012-07-02T11:10:15+0000",
            "content": "Thanks for your graphs, Toke. It is very interesting to see how the different impls compare on various computers!\n\nI created LUCENE-4184 to replace Packed64SingleBlock with the inheritance-based impl.\n\nInitially, I chose valueCount=10,000,000 because it sounded like a reasonable segment size and because it would yield a lot of cache misses on my 2M-cache CPU (when bitsPerValue >= 4 at least). Given Mars and your i7 machines have big CPU caches, I'd be interested to know whether graphs still have the same shape with valueCount=100,000,000 for example.\n\nOtherwise, I am a little bit reluctant to try to improve the impl selection routine used in PackedInts.getMutable given how different results are based on the arch, unless we manage to find some clear (and easily accessible from Java) patterns that make an impl faster than another. ",
            "author": "Adrien Grand",
            "id": "comment-13405004"
        },
        {
            "date": "2012-07-02T23:00:18+0000",
            "content": "Attached improved benchmark that allows for user specified value count, bpv's and implementations. Also attached is PackedZero that is a dummy Mutable that is used for measuring the maximum possible throughput. ",
            "author": "Toke Eskildsen",
            "id": "comment-13405418"
        },
        {
            "date": "2012-07-02T23:23:30+0000",
            "content": "Using the old benchmark, I've measured performance for 100M values on the i7 server. The graphs are at http://ekot.dk/misc/packedints/100m.html\n\nThe \"unique\" bpvs for padded are 3, 5, 6, 7, 9, 10, 12 and 21. Rounding up to nearest valid bpv extends these to 3, 5, 6, 7, 9-12 and 17-21. For the set method on the i7 server, padding is slower than contiguous for all of these bpvs. For the get method, padding is a smidgen faster for 17-21 bpvs. These results mirrors the 10M test on the i7 laptop.\n\nI have started measurement with 100M values on the i7 laptop, but I am running out of time. I expect to check back in 1-2 weeks. Note that I have uploaded a version of PackedIntsBenchmark that makes it easy to experiment with different valueCounts and bpvs. ",
            "author": "Toke Eskildsen",
            "id": "comment-13405437"
        },
        {
            "date": "2012-07-03T00:04:04+0000",
            "content": "Thanks, Toke! Really strange how the direct impl remains the slowest one at setting values... ",
            "author": "Adrien Grand",
            "id": "comment-13405457"
        },
        {
            "date": "2012-07-08T20:06:29+0000",
            "content": "One solution later and the mystery is still there: Tricking the JVM to request the old memory value before setting the new one in Direct64 does increase its speed to the expected level on the i7 server mars:\n\n  public static long MASK = 0L;\n  public void set(final int index, final long value) {\n    values[index] = values[index] & MASK | value;\n  }\n\n\nI am guessing it somehow triggers either a pre-fetch of the relevant 4K page in main memory or marks the cache as hotter and thus delaying cache flush. Alas, I am not a hardware guy and this is a bit beyond me.\n\nAnyway, the solution is not usable as it slows execution on other machines. I can get access to another i7 server of the same generation, but it is identical to mars so I doubt the results would be different. Unless someone steps up with another current-generation i7 server to test on, I think we should file this under pixies.\n\nAlso unresolved, but more relevant, is the question about auto-selection of Mutable implementation. Based on the data so far, the current auto-selector will result in sub-optimal selection on i7-machines. Without a reliable architecture detection, another way of viewing the issue is whether to optimize the selector towards older machines (padded) or newer (contiguous).\n\nI've looked around for a way to determine the CPU and cache size, but so far it seems that the only fairly reliable way is to determine the OS, then call some platform-specific native code and parse the output. Besides the ugliness of this solution, I guess it gets disqualified for the extra needed permissions from the security manager. ",
            "author": "Toke Eskildsen",
            "id": "comment-13409056"
        }
    ]
}