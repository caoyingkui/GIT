{
    "id": "LUCENE-4201",
    "title": "Add Japanese character filter to normalize iteration marks",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/analysis"
        ],
        "type": "New Feature",
        "fix_versions": [
            "4.0-BETA"
        ],
        "affect_versions": "4.0-ALPHA,                                            6.0",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "For some applications it might be useful to normalize kanji and kana iteration marks such as \u3005, \u309e, \u309d, \u30fd and \u30fe to make sure they are treated uniformly.",
    "attachments": {
        "LUCENE-4201.patch": "https://issues.apache.org/jira/secure/attachment/12535379/LUCENE-4201.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-07-06T15:57:12+0000",
            "content": "Patch attached. ",
            "author": "Christian Moen",
            "id": "comment-13408098"
        },
        {
            "date": "2012-07-06T16:00:21+0000",
            "content": "Sequences of iteration marks are supported.  In case an illegal sequence of iteration marks is encountered, the implementation emits the illegal source character as-is without considering its script.  For example, with input \"?\u309d\", we get \"??\" even though \"?\" isn't hiragana.\n\nNote that a full stop punctuation character \"\u3002\" (U+3002) can not be iterated\u3000(see below). Iteration marks themselves can be emitted in case they are illegal,\u3000i.e. if they go back past the beginning of the character stream.\n\nThe implementation buffers input until a full stop punctuation character (U+3002) or EOF is reached in order to not keep a copy of the character stream in memory. Vertical iteration marks, which are even rarer than horizonal iteration marks in contemporary Japanese, are unsupported. ",
            "author": "Christian Moen",
            "id": "comment-13408101"
        },
        {
            "date": "2012-07-06T16:04:40+0000",
            "content": "I've indexed the Japanese Wikipedia using this filter and things look okay.  I'm seeing a ~8% performance overhead (versus no filter).\n\nMy thinking is that this filter should be available for applications that need it, but it should not be part of our default Japanese configuration. ",
            "author": "Christian Moen",
            "id": "comment-13408106"
        },
        {
            "date": "2012-07-06T16:18:48+0000",
            "content": "updated patch with some additional tests: for CharFilters its useful to use MockTokenizer + checkRandomData because MockTokenizer has a lot of asserts.\n\nThis fails sometimes: the first one i hit was the valid unicode assert in MockTokenizer, I think sometimes we might be doubling a high or low surrogate? a simple workaround would be to never double surrogates. ",
            "author": "Robert Muir",
            "id": "comment-13408121"
        },
        {
            "date": "2012-07-06T16:19:47+0000",
            "content": "\nI've indexed the Japanese Wikipedia using this filter and things look okay. I'm seeing a ~8% performance overhead (versus no filter).\n\nBeware of LUCENE-4185 here, it might be not so bad (i just put a patch up there) ",
            "author": "Robert Muir",
            "id": "comment-13408123"
        },
        {
            "date": "2012-07-06T16:22:18+0000",
            "content": "Also do we need to worry about offsets tests? Does this filter need to do any offsets corrections (it seems it does not, which would be nice) ",
            "author": "Robert Muir",
            "id": "comment-13408125"
        },
        {
            "date": "2012-07-06T16:24:52+0000",
            "content": "Thanks a lot, Robert.\n\nI'll looking into the random checks and a couple of other things as well. ",
            "author": "Christian Moen",
            "id": "comment-13408129"
        },
        {
            "date": "2012-07-06T16:29:10+0000",
            "content": "We shouldn't need any offset corrections since we never add or remove characters (we just replace them). ",
            "author": "Christian Moen",
            "id": "comment-13408133"
        },
        {
            "date": "2012-07-06T16:33:34+0000",
            "content": "I thought that might be the case: when i first wrote the tests i used japaneseAnalyzer\nand they always passed... So I think this is just the one corner case that MockTokenizer finds.\n\nNot correcting offsets keeps things simple: so if possible I think we could just not do\nanything with iteration marks + surrogates and leave as-is, otherwise to actually\nreplace the iteration mark with those, we would need offsets corrections. ",
            "author": "Robert Muir",
            "id": "comment-13408137"
        },
        {
            "date": "2012-07-08T16:46:36+0000",
            "content": "Thanks, Robert.\n\nI've attached a new patch that deals with surrogates and I've also fixed a couple of others issues found by further testing. ",
            "author": "Christian Moen",
            "id": "comment-13408964"
        },
        {
            "date": "2012-07-09T00:36:19+0000",
            "content": "Added additional Solr factory tests to test parameters.  I think it's ready. ",
            "author": "Christian Moen",
            "id": "comment-13409118"
        },
        {
            "date": "2012-07-09T14:43:06+0000",
            "content": "patch looks great. +1 to commit ",
            "author": "Robert Muir",
            "id": "comment-13409530"
        },
        {
            "date": "2012-07-10T11:42:40+0000",
            "content": "Thanks, Robert.  Attached final patch with CHANGES.txt details. ",
            "author": "Christian Moen",
            "id": "comment-13410265"
        },
        {
            "date": "2012-07-10T11:46:36+0000",
            "content": "Committed revision 1359613 on trunk ",
            "author": "Christian Moen",
            "id": "comment-13410268"
        },
        {
            "date": "2012-07-10T12:21:59+0000",
            "content": "Added svn:eol-style native to trunk with revision 1359632. ",
            "author": "Christian Moen",
            "id": "comment-13410286"
        },
        {
            "date": "2012-07-10T13:02:09+0000",
            "content": "Committed revision 1359645 on branch_4x ",
            "author": "Christian Moen",
            "id": "comment-13410318"
        },
        {
            "date": "2012-07-11T23:04:40+0000",
            "content": "hoss20120711-manual-post-40alpha-change ",
            "author": "Hoss Man",
            "id": "comment-13412321"
        },
        {
            "date": "2012-07-11T23:05:45+0000",
            "content": "bah .. wrong textbox ",
            "author": "Hoss Man",
            "id": "comment-13412323"
        }
    ]
}