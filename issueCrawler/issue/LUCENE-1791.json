{
    "id": "LUCENE-1791",
    "title": "Enhance QueryUtils and CheckHIts to wrap everything they check in MultiReader/MultiSearcher",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Test",
        "fix_versions": [
            "2.9"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "methods in CheckHits & QueryUtils are in a good position to take any Searcher they are given and not only test it, but also test MultiReader & MultiSearcher constructs built around them",
    "attachments": {
        "LUCENE-1791.patch": "https://issues.apache.org/jira/secure/attachment/12415823/LUCENE-1791.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-08-07T06:39:08+0000",
            "content": "Patch showing what i have in mind.\n\nCurrent patch causes 14 failures in TestComplexExplanations, all of which stem from calling checkFirstSkipTo() on IndexSearcher created by wrapUnderlyingReader().  \n\nThe failure is QueryUtils.java:305: \"unstable skipTo(0) score! expected:<NaN> but was:<NaN>\" which seems like it could easily be a bug introduced by the patch, except...\n\n\n\tWhy only cause 14/22 in TestComplexExplanations to fail?\n\tWhy wouldn't the original IndexSearcher have produced a NaN score?\n\n ",
            "author": "Hoss Man",
            "id": "comment-12740427"
        },
        {
            "date": "2009-08-07T06:45:32+0000",
            "content": "FWIW: beyond just fixing the current failures, i'd like to enhance this patch (possibly using some sort of static cache of the wrapped readers it creates) so that it can work well in conjunction with LUCENE-1749.  \n\nAt the moment, the wrapping readers are go out of scope and are likely to be GCed well before anything interesting happens with the original IndexSearcher, so sanity checking the field cache isn't likely to find anything suspicious. ",
            "author": "Hoss Man",
            "id": "comment-12740429"
        },
        {
            "date": "2009-08-12T20:04:10+0000",
            "content": "I just retried this patch against the trunk now that the FieldCacheSanityChecker and some other patches have been committed.  In addition to the possibly false-negatives from TestComplexExplanation (NaN score) this is now surfacing FielCache sanity failures from TestCustomScoreQuery, TestFieldScoreQuery, and TestOrdValues (suggesting that there are code paths where those query types don't correctly use the subreaders to get the FieldCache) as well as checkFirstSkipTo() failures for TestSpansAdvanced2 and an ArrayIndexOutOfBoundsException from TestBoolean2.testRandomQueries  (grr.... it uses it's own random so no seed was logged)\n\nI don't pretend this patch is perfect, but i can't imagine these are all false-negatives.  \n\nWe should get to the bottom of this before 2.9.  I'll start trying to figure it out on the train tonight. ",
            "author": "Hoss Man",
            "id": "comment-12742548"
        },
        {
            "date": "2009-08-12T20:08:40+0000",
            "content": "(grr.... it uses it's own random so no seed was logged)\n\nCorrection: it does log the seed, i was just looking for stderr when i should have been looking for stdout...\n\n\nfailed query: +field:w2 field:w3 field:xx field:w4 field:w2\nNOTE: random seed of testcase 'testRandomQueries' was: 5695251427490718890\n\n ",
            "author": "Hoss Man",
            "id": "comment-12742552"
        },
        {
            "date": "2009-08-12T21:31:42+0000",
            "content": "I'm guess the NAN failures are not a problem - looks like they fail because NAN != NAN ? Havn't looked closer.\n\nI don't think the fieldcache insanity is multi-reader related - it looks to me like some entries have a parser, and some null for the parser, even though the default parser is being used in both cases. The FieldSource types grab a FieldCache and may pass null as the parser, which ends up putting null in the cache entry - but if you specifically ask for the default parser, that puts the default parser in the fieldcache entry - same stuff now, doubled entry.\n\nAs for the out of bounds - havn't look at that one yet - odd one ...\n\n... interesting - it alternates between nullpointer and out of bounds exceptions ... ",
            "author": "Mark Miller",
            "id": "comment-12742586"
        },
        {
            "date": "2009-08-12T21:51:53+0000",
            "content": "I don't think the fieldcache insanity is multi-reader related - it looks to me like some entries have a parser, and some null for the parser, even though the default parser is being used in both cases. The FieldSource types grab a FieldCache and may pass null as the parser, which ends up putting null in the cache entry - but if you specifically ask for the default parser, that puts the default parser in the fieldcache entry - same stuff now, doubled entry.\n\nWell that explains half the output anyway - even if thats fixed there is still a fail. Its because the tests doesn't expand fully into the subreaders - just needed the top level before - with this test, we need to recursively grab them. ",
            "author": "Mark Miller",
            "id": "comment-12742598"
        },
        {
            "date": "2009-08-12T22:00:00+0000",
            "content": "I'm guess the NAN failures are not a problem - looks like they fail because NAN != NAN?\n\nright \u2013 but why would the scores be NaN when wrapped in a MultiReader? when it's not wrapped in a MultiReader the test passes, so the scores must not be NaN in that case.\n\nI don't think the fieldcache insanity is multi-reader related [...] same stuff now, doubled entry.\n\nThe sanity checker ignores when two CacheEntries differ only by parser (precisely because of the the null/default parser issue) and the resulting value object is the same.  but it does include all related CacheEntry objects in an Insanity object so that you have them all for debugging.\n\nLooking at TestCustomScoreQuery.testCustomScoreByte (for example)...\n\n\n*** BEGIN org.apache.lucene.search.function.TestCustomScoreQuery.testCustomScoreByte: Insane FieldCache usage(s) ***\nSUBREADER: Found caches for decendents of org.apache.lucene.index.DirectoryReader@88d2ae+iii\n\t'org.apache.lucene.index.DirectoryReader@88d2ae'=>'iii',byte,null=>[B#841343 (size =~ 33 bytes)\n\t'org.apache.lucene.index.DirectoryReader@88d2ae'=>'iii',byte,org.apache.lucene.search.FieldCache.DEFAULT_BYTE_PARSER=>[B#841343 (size =~ 33 bytes)\n\t'org.apache.lucene.index.CompoundFileReader$CSIndexInput@77daaa'=>'iii',byte,org.apache.lucene.search.FieldCache.DEFAULT_BYTE_PARSER=>[B#981898 (size =~ 33 bytes)\n\t'org.apache.lucene.index.CompoundFileReader$CSIndexInput@77daaa'=>'iii',byte,null=>[B#981898 (size =~ 33 bytes)\n\n*** END org.apache.lucene.search.function.TestCustomScoreQuery.testCustomScoreByte: Insane FieldCache usage(s) ***\n\n\n\n\nThe insanity type is \"SUBREADER\", so it's specificly identified a problem with that type of relationship.  There are 4 CacheEntries listed in the error all from the same field, but from two different readers.  If you note the value identity hashcodes (just before the size estimate) each reader has only one value cached for that field (with different parsers) which is why there isn't a seperate error about the multiple values. \nas the first line of hte Instanity.toString() states: what it found is that DirectoryReader@88d2ae and at least one of it's decendents both have cached entires for the same field.\n ",
            "author": "Hoss Man",
            "id": "comment-12742600"
        },
        {
            "date": "2009-08-12T22:02:55+0000",
            "content": "just fully rolling out to all of the subreaders makes the test pass I believe. ",
            "author": "Mark Miller",
            "id": "comment-12742601"
        },
        {
            "date": "2009-08-12T22:04:36+0000",
            "content": "\nWell that explains half the output anyway - even if thats fixed there is still a fail. Its because the tests doesn't expand fully into the subreaders - just needed the top level before - with this test, we need to recursively grab them.\nYou lost me there... are you saying the tests needs to be changed? ... why?  \n\nFor this patch to trigger an error in an existing test, that test must either be using CheckHits or QueryUtils to execute a query against a seracher and validate the results are ok ... why would the test be responsible for any subreader expansion in this case? ",
            "author": "Hoss Man",
            "id": "comment-12742603"
        },
        {
            "date": "2009-08-12T22:09:48+0000",
            "content": "midair collision (x2) ... i think i see what you mean in your revised patch ... the tests don't need changed, it's just the test utility methods that were trying to recurse the readers that weren't doing the entire job. ",
            "author": "Hoss Man",
            "id": "comment-12742606"
        },
        {
            "date": "2009-08-12T22:11:23+0000",
            "content": "Okay - so the first the original Parser issue:\n\nThe output looked odd - it showed multiple entries with the same type, field, and reader, but null and DefaultParser.\n\nWhen I quickly switched the code to use DefaultParser instead of null, those extra entries went away, and it just showed the segmentreader and directory reader that were doubled up. Thats what had me thinking that was involved. I'm not sure I understand why that was happening now though.\n\nAnyway, all of the issues appear to be because the test code was written expecting all of the readers to be top level.\n\nThe tests, in certain cases, use a Reader to grab from a fieldcache - that reader has to be the right subreader and not the top level reader -\n\nthe tests were just using getSequentialSubReaders - they need use gatherSubReaders instead - because you introduced the multi level reader stuff. They should have used gatherSubReaders from the start, but because it wasn't needed at the time (for the tests to pass), it didn't even occur to me. ",
            "author": "Mark Miller",
            "id": "comment-12742608"
        },
        {
            "date": "2009-08-12T22:23:21+0000",
            "content": "FYI: with mark's updated path, we're back to just the NaN failures from TestComplexExplanations.  (and possibly TestBoolean2.testRandomQueries, but i can't confirm that) ",
            "author": "Hoss Man",
            "id": "comment-12742614"
        },
        {
            "date": "2009-08-12T22:50:24+0000",
            "content": "I only get the NAN issue showing up now.\n\nI don't know why its returning NAN as the score at the moment, but as far as the test is concerned, that particular failure appears to be false. Its just checking that two calls to score return the same things - but if it returns NAN, they are not considered equal.\n\nSo I guess the question is - does it make sense that the score is NAN? ",
            "author": "Mark Miller",
            "id": "comment-12742634"
        },
        {
            "date": "2009-08-13T06:30:00+0000",
            "content": "I figured out the problem with TestComplexExplanations ... the test uses a searcher with a Custom Similarity, and the new code wasn't setting the same Similarity on the new Searcher & MultiSearcher being created. ",
            "author": "Hoss Man",
            "id": "comment-12742736"
        },
        {
            "date": "2009-08-13T07:53:44+0000",
            "content": "one other thing i experimented with earlier today was making the \"wrapped\" MultiReaders and MultiSearchers a little more complicated (deeper structures, and adding some deleted docs to the other subreaders)\n\nat first i ran into problems because the deleted docs would through off the docIds of real docs (and some tests expected certain docs to have specific IDs) but even when the deleted docs only appear \"after\" the real docs, there are still some test failures with this latest patch.\n\nTestSimpleExplanations and TestComplexExplanations are the ones i've been looking at but there may be others (my laptop gets to hot if i try to run the full test suite too often)\n\nmy head hurts trying to figure out why the deeper nested structures and deleted docs might be causing these new failures ... i'll try to look at it tomorow. ",
            "author": "Hoss Man",
            "id": "comment-12742754"
        },
        {
            "date": "2009-08-13T08:06:16+0000",
            "content": "I just looked at the patch - one little minor thing. Would it make sense to randomized reader / searcher indexes in  wrapUnderlyingReader / wrapSearchers to catch possible array index related issues? This could also be done for the num of deleted terms and types of searcher / readers.\n\nsimon ",
            "author": "Simon Willnauer",
            "id": "comment-12742758"
        },
        {
            "date": "2009-08-13T12:45:46+0000",
            "content": "Have you started working on the ItemizedFilter at all? It can't use outside doc ids like that. Saying to match doc 3 will match every doc 3 in every reader. ",
            "author": "Mark Miller",
            "id": "comment-12742813"
        },
        {
            "date": "2009-08-13T15:33:30+0000",
            "content": "I guess the out of bounds exception is still there:\n\n    [junit] Testsuite: org.apache.lucene.search.TestBoolean2\n    [junit] Tests run: 11, Failures: 0, Errors: 1, Time elapsed: 0.723 sec\n    [junit] ------------- Standard Output ---------------\n    [junit] failed query: field:yy field:zzz\n    [junit] NOTE: random seed of testcase 'testRandomQueries' was: -2696660952902803926\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testRandomQueries(org.apache.lucene.search.TestBoolean2):\tCaused an ERROR\n    [junit] -1\n    [junit] java.lang.ArrayIndexOutOfBoundsException: -1\n\nNo sure what to do about the ItemizedFilter - its not really a legal way to check things ... ",
            "author": "Mark Miller",
            "id": "comment-12742868"
        },
        {
            "date": "2009-08-13T17:49:19+0000",
            "content": "Okay, I've got all tests passing.\n\nI rigged up a smarted ItemizedFilter that works for these tests. ",
            "author": "Mark Miller",
            "id": "comment-12742900"
        },
        {
            "date": "2009-08-13T21:14:55+0000",
            "content": "every so slight cleaned up \n\nremoved a couple unused imports\nadded a comment about whats going on with the ItemizedFilter ",
            "author": "Mark Miller",
            "id": "comment-12742972"
        },
        {
            "date": "2009-08-14T00:12:04+0000",
            "content": "Okay, I've got all tests passing.\n\nSweet! ... Even the random boolean query test? (i see changes to ReqExclScorer in your patch, but i don't relaly see how that could solve the problem)\n\nI rigged up a smarted ItemizedFilter that works for these tests.\n\nI'm clearly missing something ... why does ItemizedFilter need to change?  I know it's an abomination and you would never want to use something like that in real code, but in this test the docIds passed to it on construction shouldn't change \u2013 even when we wrap the reader/searcher in other searchers, i specificly only put empty indexes (with no deletions) in front of the original index when bulding the multireader/searcher so the docIds will be the same.  why doesn't the existing implementation work?\n\nWould it make sense to randomized reader / searcher indexes\n\nSimon: randomized wrapping would be nice, but i didn't try to do that for two reasons:\n\n\tthe utility has no way to get the seed from LuceneTestCase, and i didn't want to introduce randomness unless it was predictable randomness that could be logged.\n\ti needed to be sure we didn't add an index with deletions prior to the original index (but based on Mark's comments, it looks like that's not really an issue since we have to deal with it anyway)\n\n ",
            "author": "Hoss Man",
            "id": "comment-12743032"
        },
        {
            "date": "2009-08-14T00:30:15+0000",
            "content": "Sweet! ... Even the random boolean query test? (i see changes to ReqExclScorer in your patch, but i don't relaly see how that could solve the problem)\n\nYes - the issue was again how it was dealing with the sub readers - my original fix was not fully correct  - it just happened to work with the previous tests. \n\nBy the way - any changes to non tests should have been pulled - sorry - I'll put up another patch.\n\nI'm clearly missing something ... why does ItemizedFilter need to change? I know it's an abomination and you would never want to use something like that in real code, but in this test the docIds passed to it on construction shouldn't change - even when we wrap the reader/searcher in other searchers, i specificly only put empty indexes (with no deletions) in front of the original index when bulding the multireader/searcher so the docIds will be the same. why doesn't the existing implementation work?\n\nBecause it uses external ids - that only works if you have a single reader. The filter is applied per sub-reader - so if you use a filter that just sets doc 0 - every sub reader will match its first doc. You could do this in the past, because it just worked on the top level reader - in that case, external ids match completely against the reader - but with the sub-reader approach, each sub-reader has an id space that starts at 0. ",
            "author": "Mark Miller",
            "id": "comment-12743036"
        },
        {
            "date": "2009-08-14T00:49:08+0000",
            "content": "The filter is applied per sub-reader\n\nDoh! .. right, that's the part i was forgetting \u2013 filters are another thing that happen per reader now.\n\nsweet.  i'll review the patch for real tomorow or sat, and maybe rip out the ItemizedFilter and replace it with something more sensical (it's a bad example that people might stumble upon, that seems even more confusing now, and i feel responsible since i'm the one that wrote it for those explanation tests anyway) ",
            "author": "Hoss Man",
            "id": "comment-12743039"
        },
        {
            "date": "2009-08-14T03:14:53+0000",
            "content": "sweet. i'll review the patch for real tomorow or sat, and maybe rip out the ItemizedFilter and replace it with something more sensical (it's a bad example that people might stumble upon, that seems even more confusing now, and i feel responsible since i'm the one that wrote it for those explanation tests anyway)\n\nCool - it is very confusing now, and not super clean really - its kind of a hack - I just wasn't sure how else to handle it.\n\nEssentially, every time it sees a reader, it assumes its a reader thats part of a sequence of sub readers and adds maxDoc to a base to figure out what doc ids it should really set - and then if it sees the top level reader, or the the first sub reader, it resets that base - so it can start over if the set of sub readers need to come through again. It works for these tests, but it makes no sense to do such a thing in the real world. ",
            "author": "Mark Miller",
            "id": "comment-12743061"
        },
        {
            "date": "2009-08-16T05:13:23+0000",
            "content": "i put the doc \"ids\" into a KEY field and refactored ItemizedFilter to be a trivial subclass of FieldCacheTermsFilter.\n\nI also added more wrap permutations to address some of the possible edge cases Simon pointed out (good catch SImon) but didn't introduce any randomization for hte reasons mentioned before (even with the change to not rely on consistent docIds in ItemizedFilter, we can't allow deletions before the wrapped searcher/reader because CheckHIts does it magic based on docIds. \n\n(hmm... i suppose the wrap functions could return some metadata about what offset the old ids have in the new search/reader and CheckHits could use that .... hmmm ... seems kludgy so i'm not going to worry about it)\n\nI think we're good to go here unless anyone has any objections ",
            "author": "Hoss Man",
            "id": "comment-12743813"
        },
        {
            "date": "2009-08-16T22:38:23+0000",
            "content": "One more minor entry from me:\n\nminor javadoc/spelling fixes\nremoved unused imports ",
            "author": "Mark Miller",
            "id": "comment-12743920"
        },
        {
            "date": "2009-08-18T19:52:39+0000",
            "content": "You going to resolve this Hoss? Or was that commit just a tease? ",
            "author": "Mark Miller",
            "id": "comment-12744670"
        },
        {
            "date": "2009-08-18T20:24:48+0000",
            "content": "Geezz... you expect me to commit and resolve ... i thought this open source stuff was all about volunteering and cooperation, how come i gotta do both ?!?! ",
            "author": "Hoss Man",
            "id": "comment-12744692"
        },
        {
            "date": "2009-08-18T20:29:35+0000",
            "content": "I would have been all over it  But who knows if you have your reasons for not resolving yet.\n\nThanks for all your work on this ! Brilliant issue that caught a lot of great stuff. ",
            "author": "Mark Miller",
            "id": "comment-12744694"
        }
    ]
}