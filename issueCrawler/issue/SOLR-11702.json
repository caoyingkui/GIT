{
    "id": "SOLR-11702",
    "title": "Redesign current LIR implementation",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [
            "7.3",
            "master (8.0)"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "I recently looked into some problem related to racing between LIR and Recovering. I would like to propose a totally new approach to solve SOLR-5495 problem because fixing current implementation by a bandage will lead us to other problems (we can not prove the correctness of the implementation).\n\nFeel free to give comments/thoughts about this new scheme.\nhttps://docs.google.com/document/d/1dM2GKMULsS45ZMuvtztVnM2m3fdUeRYNCyJorIIisEo/edit?usp=sharing",
    "attachments": {
        "SOLR-11702.patch": "https://issues.apache.org/jira/secure/attachment/12901189/SOLR-11702.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-12-08T03:10:08+0000",
            "content": "A patch for this ticket implemented according to the design including the fix for SOLR-10525 by Mike Drob. It will need more tests but all current tests are passed. ",
            "author": "Cao Manh Dat",
            "id": "comment-16282963"
        },
        {
            "date": "2017-12-08T03:27:57+0000",
            "content": "Ooooh, good approach. This is similar in concept to how RAFT works, I think.\n\nOne thing that is unclear from design doc (haven't looked at code yet) is who updated the ZK terms when replica joins recovery. Is that the result of the leader acknowledging the PrepRecoveryCmd? ",
            "author": "Mike Drob",
            "id": "comment-16282992"
        },
        {
            "date": "2017-12-08T03:30:26+0000",
            "content": "Mike Drob That's right. I borrowed term's idea from Raft. All the replicas can update its term equals to the leader's term. Only leader can increase terms of other replicas. ",
            "author": "Cao Manh Dat",
            "id": "comment-16282999"
        },
        {
            "date": "2017-12-08T12:30:38+0000",
            "content": "Really like this approach, Cao Manh Dat. Not just a cleaner and more robust approach, but I believe it could be an alternative solution for the problems that motivates SOLR-7065 and SOLR-7034. Correct me if I am wrong, but replica could become leader, regardless of their previous state or the number of replicas participating, as their (and others) term number would explicitly say if they are in sync or behind. Is my assumption correct? ",
            "author": "Mano Kovacs",
            "id": "comment-16283455"
        },
        {
            "date": "2017-12-10T23:59:51+0000",
            "content": "Mano Kovacs Yeah, that's the idea of term. But the current design does not tell anything about DOWN replica, I postpone the fix for SOLR-7065 and SOLR-7034 to another issue, where we introduce a new rule like this: \"only return success if all DOWN replicas have term less than leader's term\" ",
            "author": "Cao Manh Dat",
            "id": "comment-16285422"
        },
        {
            "date": "2017-12-11T07:43:24+0000",
            "content": "Updated patch for this ticket, including some cleanup and more tests. ",
            "author": "Cao Manh Dat",
            "id": "comment-16285605"
        },
        {
            "date": "2017-12-31T14:55:40+0000",
            "content": "Shalin Shekhar Mangar Mark Miller I pushed all the changes to jira/solr-11702. Do you mind to take a look? Thanks! ",
            "author": "Cao Manh Dat",
            "id": "comment-16307221"
        },
        {
            "date": "2017-12-31T17:54:45+0000",
            "content": "Cao Manh Dat As it happens I'm working on understanding why a replica going into LIR and I have a test setup that let's me reproduce it reasonably reliably (although it may take a few hours). I'm determining whether having basic auth enabled is necessary or not. I believe I've seen this on 7x and master\n\nThe point is, when you think the patch is ready I'd be happy to give it a go in my test environment, although it may take me a week, let me know.\n ",
            "author": "Erick Erickson",
            "id": "comment-16307273"
        },
        {
            "date": "2018-01-01T02:36:51+0000",
            "content": "The current logic is quite stable. If the test can help us find some bugs in current implementation that will be great! ",
            "author": "Cao Manh Dat",
            "id": "comment-16307357"
        },
        {
            "date": "2018-01-01T16:28:21+0000",
            "content": "OK, I'm giving it a try. My test case is quite simple, set up a 1-shard, 4-replica collection and fire a bunch of updates at it. So far this happens on 6.3 (where the problem was first reported) and on 7.x. I suspect on master too, but don't want to spend the time since it happens on 7x.\n\nAnyway, the patch applied cleanly and I'm running the test now. Basic auth doesn't seem to be necessary. I'll report back later. ",
            "author": "Erick Erickson",
            "id": "comment-16307487"
        },
        {
            "date": "2018-01-09T18:11:22+0000",
            "content": "OK, reporting back. My problem was totally unrelated unfortunately. Mine went away with upgrading Jetty. ",
            "author": "Erick Erickson",
            "id": "comment-16318845"
        },
        {
            "date": "2018-01-15T05:03:50+0000",
            "content": "Thanks Dat. A few comments/questions:\n\n\n\tDUP.setupRequest skips replicas having terms. If I understand correctly, this will mean that updates are no longer forwarded to replicas\u00a0until they publish themselves in recovery? Is that right?\n\tCreateCollectionCmd \u2013 throw InterruptedException directly from the method instead of trying to handle it here\n\tMark LIR related classes/methods as deprecated \u2013 those are more likely to get attention right before 8.0 I think.\n\tElectionContext \u2013 Minor typo - \"this replica is registered its term\" \u2013 s/is/has\n\tRecoveringCoreTermWatcher \u2013 Shouldn't lastTermDoRecovery be set after recovery completes? If not, how do we ensure that recoveries are stacked up?\n\tRecoveringCoreTermWatcher catches NullPointerException. Do a null check instead.\n\tRecoveryStrategy \u2013 why pingLeader? isn't it sufficient to use ZkStateReader.getLeaderRetry as we used to do earlier?\n\tZkCollectionTerms \u2013 if getShard and remove methods need to be synchronized then seems like close can interfere. Perhaps better to synchronize on the terms map itself.\n\tCan you explain the purpose of \"new\".equals(cd.getCoreProperty(\"lirVersion\", \"new\"))) used in various places?\n\n\n\nI'm still going through the rest of the changes. I'll add some more comments later. ",
            "author": "Shalin Shekhar Mangar",
            "id": "comment-16325893"
        },
        {
            "date": "2018-01-15T05:52:22+0000",
            "content": "Thanks Shalin Shekhar Mangar\n\n1. DUP.setupRequest skips replicas having terms. If I understand correctly, this will mean that updates are no longer forwarded to replicas\u00a0until they publish themselves in recovery? Is that right?\nRight, if term of a replica is less than leader term, leader will stop sending updates to that replica.\n\n\n2. CreateCollectionCmd \u2013 throw InterruptedException directly from the method instead of trying to handle it here\nThe code of deleting old term nodes in CreateCollectionCmd is handled exactly same as the code below it, I do not understand the problem here.\n\n\n3. Mark LIR related classes/methods as deprecated \u2013 those are more likely to get attention right before 8.0 I think.\nSure, this is a good idea\n\n\n5. RecoveringCoreTermWatcher \u2013 Shouldn't lastTermDoRecovery be set after recovery completes? If not, how do we ensure that recoveries are stacked up?\nI do not see any problem in the current implementation, after we call doRecovery, the recovery process will start shortly\n\n\n6. RecoveringCoreTermWatcher catches NullPointerException. Do a null check instead.\nSure!\n\n\n7. RecoveryStrategy \u2013 why pingLeader? isn't it sufficient to use ZkStateReader.getLeaderRetry as we used to do earlier?\nImagine this case, when there are network partition between leader and replica\n\n\tLeader increase term of replica\n\tRecoveringCoreTermWatcher trigger recovery process of replica, replica goes into recovery ( hence increase its term )\n\tLeader increase term of replica ( because it failed to send update to replica and now term of replica is equals to leader's term)\n\tRecoveringCoreTermWatcher trigger recovery process of replica, replica goes into recovery ( hence increase its term )\n\t... this process will be repeated forever until the network is healed\n\n\n\n\n8. ZkCollectionTerms \u2013 if getShard and remove methods need to be synchronized then seems like close can interfere. Perhaps better to synchronize on the terms map itself.\nThis is a good idea\n\n\n9. Can you explain the purpose of \"new\".equals(cd.getCoreProperty(\"lirVersion\", \"new\"))) used in various places?\nThat flag mostly used for testing rolling updates and can be removed in SOLR-11812 ",
            "author": "Cao Manh Dat",
            "id": "comment-16325907"
        },
        {
            "date": "2018-01-15T11:52:56+0000",
            "content": "I'm not sure\u00a0that\u00a0ZkShardTerms.refreshTerms behaves correctly on ZK Reconnect. Say a watcher was set (numWatcher=1) but not fired and the zk client disconnects.\u00a0Then on re-connect, the\u00a0OnReconnectListener in ZkController fires which re-registers cores and calls refreshTerms again. Now\u00a0watcher won't be initialized in this method (because numWatcher=1) and therefore won't be set on terms znode anymore. Can you please verify? ",
            "author": "Shalin Shekhar Mangar",
            "id": "comment-16326166"
        },
        {
            "date": "2018-01-15T12:32:33+0000",
            "content": "\nI'm not sure that ZkShardTerms.refreshTerms behaves correctly on ZK Reconnect. Say a watcher was set (numWatcher=1) but not fired and the zk client disconnects. Then on re-connect, the OnReconnectListener in ZkController fires which re-registers cores and calls refreshTerms again. Now watcher won't be initialized in this method (because numWatcher=1) and therefore won't be set on terms znode anymore. Can you please verify?\nThe logic you described match with the code, but as I observed watcher is always fired on reconnect, at least on DISCONNECT event.  ",
            "author": "Cao Manh Dat",
            "id": "comment-16326193"
        },
        {
            "date": "2018-01-23T03:37:33+0000",
            "content": "Ok, thanks for clarifying Dat. A few more questions/comments:\n\n\tLIRRollingUpdatesTest.testNewReplicaOldLeader \u2013 why\u00a0is the\u00a0proxy closed for both leader and replica? Isn't closing for replica sufficient to force LIR?\n\tLIRRollingUpdatesTest calls TestInjection.reset() in tearDown but fault injection isn't used anywhere in the test so it can be removed.\n\tJavadocs for ZkShardTerms.ensureTermIsHigher says \"Ensure that leader's term is lower than some replica's terms\" but shouldn't\u00a0the leader have a higher term? This is also mentioned in the design document \"The idea of term is only replicas (in the same shard) with highest term are considered healthy\". The impl is doing the opposite i.e. it is increasing the replica's term to leaderTerm+1.\n\tCan you add javadocs to the various methods in the ZkShardTerms.Terms class?\n\n ",
            "author": "Shalin Shekhar Mangar",
            "id": "comment-16335353"
        },
        {
            "date": "2018-01-23T03:52:05+0000",
            "content": "LIRRollingUpdatesTest.testNewReplicaOldLeader \u2013 why is the proxy closed for both leader and replica? Isn't closing for replica sufficient to force LIR?\nYeah, you're right, closing leader's proxy is not necessary. That call is only for safety, I just want to simulate the real network partition between leader and replica\n\nLIRRollingUpdatesTest calls TestInjection.reset() in tearDown but fault injection isn't used anywhere in the test so it can be removed.\n+1\n\nJavadocs for ZkShardTerms.ensureTermIsHigher says \"Ensure that leader's term is lower than some replica's terms\" but shouldn't the leader have a higher term? This is also mentioned in the design document \"The idea of term is only replicas (in the same shard) with highest term are considered healthy\". The impl is doing the opposite i.e. it is increasing the replica's term to leaderTerm+1.\n+1, the javadoc is miss typed \n\nCan you add javadocs to the various methods in the ZkShardTerms.Terms class?\nSure ",
            "author": "Cao Manh Dat",
            "id": "comment-16335360"
        },
        {
            "date": "2018-01-29T08:56:30+0000",
            "content": "Commit 27ef6530646a9af6f8fdf491afd80185bc4f7fee in lucene-solr's branch refs/heads/master from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=27ef653 ]\n\nSOLR-11702: Redesign current LIR implementation ",
            "author": "ASF subversion and git services",
            "id": "comment-16343094"
        },
        {
            "date": "2018-01-29T08:58:09+0000",
            "content": "Commit 8c8d78a4bb6c0f3322471af5765a01848247409c in lucene-solr's branch refs/heads/branch_7x from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=8c8d78a ]\n\nSOLR-11702: Redesign current LIR implementation ",
            "author": "ASF subversion and git services",
            "id": "comment-16343096"
        },
        {
            "date": "2018-03-02T07:29:31+0000",
            "content": "Commit f1ce5419eebfa361f572802eb4a8b637c2849bb5 in lucene-solr's branch refs/heads/master from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f1ce541 ]\n\nSOLR-11702: Remove old LIR call in SolrCmdDistributor and let DistributedUpdateProcessor handle it on finish() ",
            "author": "ASF subversion and git services",
            "id": "comment-16383285"
        },
        {
            "date": "2018-03-02T07:30:16+0000",
            "content": "Commit ce2386aaabc401bc89990597279eefeb67a914b0 in lucene-solr's branch refs/heads/branch_7x from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ce2386a ]\n\nSOLR-11702: Remove old LIR call in SolrCmdDistributor and let DistributedUpdateProcessor handle it on finish() ",
            "author": "ASF subversion and git services",
            "id": "comment-16383286"
        },
        {
            "date": "2018-03-09T03:35:11+0000",
            "content": "Commit dae572819ba479bffd990ea7d8f0c4f7b76da5b0 in lucene-solr's branch refs/heads/master from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=dae5728 ]\n\nSOLR-11702: Fix precommit, only throw error to client if the replica is not in the same shard as leader ",
            "author": "ASF subversion and git services",
            "id": "comment-16392347"
        },
        {
            "date": "2018-03-09T03:36:24+0000",
            "content": "Commit b992bbb2d7480d4cf2ff1d9302a7e20732c1100c in lucene-solr's branch refs/heads/branch_7x from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b992bbb ]\n\nSOLR-11702: Fix precommit, only throw error to client if the replica is not in the same shard as leader ",
            "author": "ASF subversion and git services",
            "id": "comment-16392349"
        },
        {
            "date": "2018-03-11T00:06:12+0000",
            "content": "Commit e926f435d7e318b30b2d9ec38be87ad9ab7eed45 in lucene-solr's branch refs/heads/master from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=e926f43 ]\n\nSOLR-11702: Remove noise of exception messages on failed to ping leader ",
            "author": "ASF subversion and git services",
            "id": "comment-16394367"
        },
        {
            "date": "2018-03-11T00:06:47+0000",
            "content": "Commit 1f994c97301fbe8926115925102c78a8a133e26b in lucene-solr's branch refs/heads/branch_7x from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1f994c9 ]\n\nSOLR-11702: Remove noise of exception messages on failed to ping leader ",
            "author": "ASF subversion and git services",
            "id": "comment-16394368"
        },
        {
            "date": "2018-03-15T11:29:11+0000",
            "content": "Commit dab739ae4cc8c3ff4ece24992ad8c633f7a4b19c in lucene-solr's branch refs/heads/master from Shalin Shekhar Mangar\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=dab739a ]\n\nSOLR-11702: Minor edits to log and exception messages ",
            "author": "ASF subversion and git services",
            "id": "comment-16400263"
        },
        {
            "date": "2018-03-15T11:29:40+0000",
            "content": "Commit 4b52a19f4adfed57c9265ebee85d4e03321f6dbb in lucene-solr's branch refs/heads/branch_7x from Shalin Shekhar Mangar\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=4b52a19 ]\n\nSOLR-11702: Minor edits to log and exception messages\n\n(cherry picked from commit dab739a) ",
            "author": "ASF subversion and git services",
            "id": "comment-16400264"
        },
        {
            "date": "2018-03-15T11:30:12+0000",
            "content": "Commit 1afe333844bf133538923a6ca1a3de0b2076d788 in lucene-solr's branch refs/heads/branch_7_3 from Shalin Shekhar Mangar\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1afe333 ]\n\nSOLR-11702: Minor edits to log and exception messages\n\n(cherry picked from commit dab739a)\n\n(cherry picked from commit 4b52a19) ",
            "author": "ASF subversion and git services",
            "id": "comment-16400266"
        },
        {
            "date": "2018-04-27T18:10:02+0000",
            "content": "This looks like a massive improvement for many long standing issues, great work Cao Manh Dat! ",
            "author": "Mark Miller",
            "id": "comment-16456851"
        }
    ]
}