{
    "id": "LUCENE-2909",
    "title": "NGramTokenFilter may generate offsets that exceed the length of original text",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "2.9.4",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "Whan using NGramTokenFilter combined with CharFilters that lengthen the original text (such as \"\u00df\" -> \"ss\"), the generated offsets exceed the length of the origianal text.\nThis causes InvalidTokenOffsetsException when you try to highlight the text in Solr.\n\nWhile it is not possible to know the accurate offset of each character once you tokenize the whole text with tokenizers like KeywordTokenizer, NGramTokenFilter should at least avoid generating invalid offsets.",
    "attachments": {
        "LUCENE-2909_assert.patch": "https://issues.apache.org/jira/secure/attachment/12470430/LUCENE-2909_assert.patch",
        "TokenFilterOffset.patch": "https://issues.apache.org/jira/secure/attachment/12470420/TokenFilterOffset.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2011-02-07T05:16:20+0000",
            "content": "The patch that fixes the problem, including tests. ",
            "author": "Shinya Kasatani",
            "id": "comment-12991263"
        },
        {
            "date": "2011-02-07T09:49:57+0000",
            "content": "Is the bug really in NGramTokenFilter? \n\nThis seems to be a larger problem that would affect all tokenfilters that break larger tokens\ninto smaller ones and recalculate offsets, right?\n\nFor example: EdgeNGramTokenFilter, ThaiWordFilter, SmartChineseAnalyzer's WordTokenFilter, etc?\n\nI think WordDelimiterFilter has special code that might avoid the problem (line 352), so it might\nbe ok.\n\nIs there any better way we could solve this: for example maybe instead of the tokenizer calling\ncorrectOffset() it gets called somewhere else? This seems to be what is causing the problem. ",
            "author": "Robert Muir",
            "id": "comment-12991316"
        },
        {
            "date": "2011-02-07T10:10:19+0000",
            "content": "The problem has nothing to do with CharFilters. This problem always occurs, if endOffset - startOffset != termAtt.length().\n\nIf you e.g. put a Stemmer before ngramming, that creates longer tokens (like Portugise -\u00e3 -> -\u00e3o or German \u00df -> ss) you have the same problem. A solution might be to use some \"factor\" to correct this in these offsets: (endOffset - startOffset) / termAtt.length() ",
            "author": "Uwe Schindler",
            "id": "comment-12991319"
        },
        {
            "date": "2011-02-07T10:12:45+0000",
            "content": "You are right, some stemmers increase the size, so this assumption that end - start = termAtt.length is a problem.\n\nSo, between this and LUCENE-2208, I think we need to add some more checks/asserts to BaseTokenStreamTestCase (at least to validate offset < end, but maybe some other ideas?)\n\nIf the highlighter hits this condition, it (rightfully) complains and throws an exception, among other problems. So I think we need to improve this situation everywhere. ",
            "author": "Robert Muir",
            "id": "comment-12991320"
        },
        {
            "date": "2011-02-07T10:21:22+0000",
            "content": "here's a check we can add to BaseTokenStreamTestCase for this condition. ",
            "author": "Robert Muir",
            "id": "comment-12991322"
        }
    ]
}