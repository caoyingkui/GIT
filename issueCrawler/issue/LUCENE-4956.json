{
    "id": "LUCENE-4956",
    "title": "the korean analyzer that has a korean morphological analyzer and dictionaries",
    "details": {
        "components": [
            "modules/analysis"
        ],
        "fix_versions": [],
        "affect_versions": "4.2",
        "priority": "Major",
        "labels": "",
        "type": "New Feature",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "Korean language has specific characteristic. When developing search service with lucene & solr in korean, there are some problems in searching and indexing. The korean analyer solved the problems with a korean morphological anlyzer. It consists of a korean morphological analyzer, dictionaries, a korean tokenizer and a korean filter. The korean anlyzer is made for lucene and solr. If you develop a search service with lucene in korean, It is the best idea to choose the korean analyzer.",
    "attachments": {
        "lucene4956.patch": "https://issues.apache.org/jira/secure/attachment/12583731/lucene4956.patch",
        "LUCENE-4956.patch": "https://issues.apache.org/jira/secure/attachment/12597912/LUCENE-4956.patch",
        "eval.patch": "https://issues.apache.org/jira/secure/attachment/12608891/eval.patch",
        "lucene-4956.patch": "https://issues.apache.org/jira/secure/attachment/12602525/lucene-4956.patch",
        "kr.analyzer.4x.tar": "https://issues.apache.org/jira/secure/attachment/12580446/kr.analyzer.4x.tar"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2013-04-25T01:28:06+0000",
            "content": "8edffacb15b3964f25054c82c0d4ea92 ",
            "author": "SooMyung Lee",
            "id": "comment-13641280"
        },
        {
            "date": "2013-04-25T03:38:59+0000",
            "content": "Thanks again, SooMyung!\n\nI'm seeing that Steven has informed you about the grant process on the mailing list.  I'm happy to also facilitate this process with Steven.\n\nLooking forward to getting Korean supported. ",
            "author": "Christian Moen",
            "id": "comment-13641365"
        },
        {
            "date": "2013-04-25T17:06:00+0000",
            "content": "Thanks for your help and your great concern , Christian!\n\nI visited your website. I noticed that you are not a Japanese and you developed a Japanese Morphological Analyzer.\n\nHow could it be possible? I'm surprising at your work. ",
            "author": "soomyung",
            "id": "comment-13641960"
        },
        {
            "date": "2013-04-25T21:06:38+0000",
            "content": "That's because Christian has ninja superpowers.\nhttp://goo.gl/5EPMr ",
            "author": "Dawid Weiss",
            "id": "comment-13642218"
        },
        {
            "date": "2013-04-26T22:32:18+0000",
            "content": "The IP clearance form for this donation is here: http://incubator.apache.org/ip-clearance/lucene-korean-analyzer.html.  I don't have karma to rebuild the website after I commit changes to the XML source, so there will be delays of a day or so between updates and those updates' appearance on the website.  ",
            "author": "Steve Rowe",
            "id": "comment-13643327"
        },
        {
            "date": "2013-04-26T23:14:02+0000",
            "content": "I think this donation should be packaged in its own jar, similarly to kuromoji, smartcn, morfologik and stempel, and so should end up at lucene/analysis/korean/.\n\nsoomyung, do you have a good name for the analysis module this will become, rather than \"korean\"?  I'd prefer a name that would allow us to add more Korean analysis modules in the future without having to rename this one.\n\nThe Lucene PMC received notification today that SooMyung's code grant and ICLA paperwork have been received and recorded.\n\nChristian Moen, now that we have SooMyung's code grant and ICLA recorded, we can start making header modifications.  I suggest we create a branch off trunk, create the new module there, check in the files from the tarball attached here, commit, iterate on headers/licensing, and finally hook the new module into the build. ",
            "author": "Steve Rowe",
            "id": "comment-13643352"
        },
        {
            "date": "2013-04-27T05:07:08+0000",
            "content": "Hi Steve, I think \"arirang\" is the best name for the korean analysis modules. \"arirang\" is the name of traditional korean song. So, I think \"arirang\" can represent korean analysis modules well. ",
            "author": "SooMyung Lee",
            "id": "comment-13643527"
        },
        {
            "date": "2013-04-27T05:50:04+0000",
            "content": "Hi Steve, I think \"arirang\" is the best name for the korean analysis modules. \"arirang\" is the name of traditional korean song. So, I think \"arirang\" can represent korean analysis modules well.\n\nThanks SooMyung, \"arirang\" it is. ",
            "author": "Steve Rowe",
            "id": "comment-13643545"
        },
        {
            "date": "2013-04-27T13:12:32+0000",
            "content": "As a user trying to browse and find analyzers and tokenizers for specific languages, I object. I mean, I should be able to look at the language code and guess what module it might be in. It's one thing if the module name is reasonably general and there is a reasonable expectation that average users would readily associate it with specific langauges, or to categorically group languages, but just giving an artificial, non-obvious name to the module than would not be obvious to an average user seems like a poor choice, to me.\n\nEven if you just called the module \"korean\", at least that would be a helpful guide to people like me browsing the list of modules. and then the package name can distinguish the implementations for that language.\n\nAlso, it should be possible to mix multiple implementations for the same langauge in the same application, so, the package name does not to have some unique name, unless there is guaranteed to be only one implementation for that language.\n\nI would suggest that there should be two choices for language-based analysis modules:\n\n1. Category name, where there is some general approach that covers a number of langauges and need to share classes.\n2. Language code, hyphen, some arbitrary name for implementations that cover only a single language.\n\nEven for #1, I would suggest that there should be a prefix that covers the \"type\" of languages covered (eastern european, asian, etc.)\n\nThat said, I would not stand in the way of adding Korean analysis as soon as possible. I mean, this contribution shouldn't have to correct all of the sins of past contributions. ",
            "author": "Jack Krupansky",
            "id": "comment-13643659"
        },
        {
            "date": "2013-04-27T13:43:00+0000",
            "content": "Jack, I think documentation can address most of your concerns.  See e.g. the descriptions for the analyzer packages in the API javadocs section of the top-level per-release docs: http://lucene.apache.org/core/4_2_1/index.html.  Fortunately, a module's name is not the only opportunity to describe its functionality.\n\nEven if you just called the module \"korean\", at least that would be a helpful guide to people like me browsing the list of modules. and then the package name can distinguish the implementations for that language.\n\n-1.  The stempel and morfologik analysis modules are both Polish analyzers - if the first one had been named \"polish\", what would we have done with the second one?\n\nAlso, it should be possible to mix multiple implementations for the same langauge in the same application, so, the package name does not to have some unique name, unless there is guaranteed to be only one implementation for that language.\n\nI agree that mixing same-language implementations should be possible in the same application.  I have no idea what you're saying after that.  Maybe an example?\n ",
            "author": "Steve Rowe",
            "id": "comment-13643665"
        },
        {
            "date": "2013-04-27T15:28:31+0000",
            "content": "The stempel and morfologik analysis modules are both Polish analyzers - if the first one had been named \"polish\", what would we have done with the second one?\n\nThat's exactly what I was talking about.\n\nWe have four distinct concepts:\n\n1. Module name. \n2. Package name.\n3. Source tree path.\n4. Module jar name.\n\nThey should incorporate both the language code and the \"implementation name\" (e.g., \"stempel\" or \"morphologik\").\n\nThe module should be something like \"analysis/pl/stempel\" or \"analysis/stempel/pl\". I prefer the former - it says that the first priority is to organize by language, and secondarily by implementation.\n\nAnd the package name should be something like \"org.apache.lucene.analysis.pl.stempel\" or \"org.apache.lucene.analysis.stempel.pl\". I prefer the former, for the same rationale as for module name.\n\nThere seems to be a third form of name \"analyzer-xxx\". But as far as I can tell it is only an artifact of the doc or make some old Lucene thing.\n\nAnd then there are the partial names for the individual jar files. There seems to be both \"lucene-analyzers-stempel-x.y.z\" and \"lucene-analyzers-morphologik-x.y.z\" in contrib/lucene-libs and then multiple \"morpologik-a.b.c\" jars in contrib.lib.\n\nIn short, to answer your question more directly, in my ideal world we would have srource tree and package names like:\n\nlucene/analysis/pl/stempel/src\nlucene/analysis/pl/morphologik/src\nlucene/analysis/ko/arirang/src\n\norg.apache.lucene.analysis.pl.stempel\norg.apache.lucene.analysis.pl.morfologik\norg.apache.lucene.analysis.ko.arirang\n\nThis would allow multiple implementations for a single language in the same application.\n\nAlthough I could see reversing the language and implementation names if there is some need to share implementation code across languages. ",
            "author": "Jack Krupansky",
            "id": "comment-13643703"
        },
        {
            "date": "2013-04-27T16:52:20+0000",
            "content": "Yes, including the ISO language code in the naming would be a very good idea. You still get into odd situations like Bokmal and Nynorsk, but you are still way ahead. ",
            "author": "Walter Underwood",
            "id": "comment-13643736"
        },
        {
            "date": "2013-04-28T04:47:47+0000",
            "content": "The Korean analyzer should be named named org.apache.lucene.analysis.kr.KoreanAnalyzer and we'll provide a ready-to-use field type text_kr in schema.xml for Solr users, which is consistent with what we do for other languages.\n\nAs for where the analyzer code itself lives, I think it's fine to put it in lucene/analysis/arirang.  The file lucene/analysis/README.txt documents what these modules are and the code is easily and directly retrievable in IDEs by looking up KoreanAnalyzer (the source code paths will be set up by ant eclipse and ant idea).\n\nOne reason analyzers have not been put in {{lucene/analysis/common} in the past is that they require dictionaries that are several megabytes large.\n\nOverall, I don't think the scheme we are using is all that problematic, but it's true that MorfologikAnalyzer and SmartChineseAnalyzer doesn't align with it.  The scheme doesn't easily lend itself to different implementations for one language, but that's not a common case today although it might become more common in the future.\n\nIn the case of Norwegian (no), there are ISO language codes for both Bokm\u00e5l (bm) and Nynorsk (nn), and one way of supporting this is also to consider these as options to NorwegianAnalyzer since both languages are Norwegian.  See SOLR-4565 for thoughts on how to extend support in NorwegianMinimalStemFilter for this.\n\nA similar overall approach might make sense when there are multiple implementations of a language; end-users can use a analyzer named <Language>Analyzer without requiring users to study the difference in implementation before using.  I also see problems with this, but it's just a thought...\n\nI'm all for improving our scheme, but perhaps we can open up a separate JIRA for this and keep this one focused on Korean?\n\n\n ",
            "author": "Christian Moen",
            "id": "comment-13643901"
        },
        {
            "date": "2013-04-28T17:28:18+0000",
            "content": "As for where the analyzer code itself lives, I think it's fine to put it in lucene/analysis/arirang.\n\n+1\n\nI'm all for improving our scheme, but perhaps we can open up a separate JIRA for this and keep this one focused on Korean?\n\n+1 ",
            "author": "Steve Rowe",
            "id": "comment-13644065"
        },
        {
            "date": "2013-04-28T18:03:19+0000",
            "content": "Looking at the actual tar file, I notice that it has the factory classes placed in \"solr\" directories rather than in the lucene directories as factories are normally organized.\n\nBy all means proceed with producing a normal patch that shows the final organization of this new analysis package.\n\nSome other issues:\n\n1. Complete absence of Java doc for the tokenizer factory and token filter factory classes - it is not \"Solr user-ready\" at present. There should be an XML example of a token filter with the parameters, as is the usual practice in Lucene/Solr.\n\n2. No Apache license headers in the \"Solr\" code. I thought this stuff was already supposed to be ASL 2.0?\n\n3. No Solr schema.xml change to add the text_ko field type.\n\n4. At least the KoreanAnalyzer.java and KoreanTokenizer.java source code have tab characters - odd format. Need to be normalized for Lucene project conventions.\n\n5. There is a hardwired stop word list in KoreanAnalyzer that appears to be nearly identical or close to StopAnalyzer.ENGLISH_STOP_WORDS_SET. Why doesn't that static code copy the StopAnalyzer list and then add the few extra terms that are needed? If there is a reason, place it in a comment.\n\nBut as I said, by all means proceed to a normal patch file now that the tar contribution is \"legal\". ",
            "author": "Jack Krupansky",
            "id": "comment-13644076"
        },
        {
            "date": "2013-04-28T22:52:49+0000",
            "content": "I think this would be a valuable addition to the Apache Lucene (P.S., I'm Korean as you may know). \n\nIt would be nice if you can remove all the korean comments or strings, and author tags in source code to avoid any compiling and installing problems. Otherwise, SVN server/client settings and build-script's encoding options etc. will be somewhat tricky. For example, \n\n\nif(entry!=null&&!(\"\uc744\".equals(end)&&entry.getFeature(WordEntry.IDX_REGURA)==IrregularUtil.IRR_TYPE_LIUL)) {\n\nand, \n\n/**\n * \ubcf5\ud569\uba85\uc0ac\uc758 \uac1c\ubcc4\ub2e8\uc5b4\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \ub2f4\uace0\uc788\ub294 \ud074\ub798\uc2a4 \n * @author S.M.Lee\n *\n */\n\n ",
            "author": "Edward J. Yoon",
            "id": "comment-13644162"
        },
        {
            "date": "2013-04-29T05:40:30+0000",
            "content": "Hi Steve, What should I do in the present situation, Do I need to make a correction to all issues and submit new tarball? Please let me know what I have to do to move forward! ",
            "author": "SooMyung Lee",
            "id": "comment-13644282"
        },
        {
            "date": "2013-05-01T21:29:22+0000",
            "content": "SooMyung, I don't think you need to do anything at this point.  I think a good next step is that we create a new branch and check the code you have submitted onto that branch.  We can then start looking into addressing the headers and other items that people have pointed out in comments.  (Thanks, Jack and Edward!)\n\nSteve, will there be a vote after the code has been checked onto the branch?  If you think the above is a good next step, I'm happy to start working on this either later this week or next week.  Kindly let me know you prefer to proceed.  Thanks. ",
            "author": "Christian Moen",
            "id": "comment-13646960"
        },
        {
            "date": "2013-05-01T21:35:04+0000",
            "content": "Steve, will there be a vote after the code has been checked onto the branch?\n\nChristian, before the VOTE on incubator-general can be called, the file header and licensing issues need to be completely addressed and vetted by us, working with SooMyung to make sure we get everything right.\n\nIf you think the above is a good next step, I'm happy to start working on this either later this week or next week.\n\n+1.  Thanks for working on this! ",
            "author": "Steve Rowe",
            "id": "comment-13646967"
        },
        {
            "date": "2013-05-04T17:48:54+0000",
            "content": "A quick status update on my side is as follows:\n\nI've put the code into an a module called arirang on my local setup and made a few changes necessary to make things work on trunk. KoreanAnalyzer now produces Korean tokens and some tests I've made passes when run from my IDE.\n\nLoading the dictionaries as resources need some work and I'll spend time on this during the weekend.  I'll also address the headers, etc. to prepare for the incubator-general vote.\n\nHopefully, I'll have all this on a branch this weekend.  I'll keep you posted and we can take things from there. ",
            "author": "Christian Moen",
            "id": "comment-13649125"
        },
        {
            "date": "2013-05-05T02:30:37+0000",
            "content": "[lucene4956 commit] cm\nhttp://svn.apache.org/viewvc?view=revision&revision=1479228\n\nBranch to work on Korean (LUCENE-4956) ",
            "author": "Commit Tag Bot",
            "id": "comment-13649241"
        },
        {
            "date": "2013-05-05T03:47:52+0000",
            "content": "Hello SooMyoung,\n\nCould you comment about the origins and authorship of org.apache.lucene.analysis.kr.utils.StringUtil in your tar file?\n\nI'm seeing a lot of authors in this file. Is this from Apache Commons Lang?  Thanks! ",
            "author": "Christian Moen",
            "id": "comment-13649258"
        },
        {
            "date": "2013-05-05T04:00:45+0000",
            "content": "I've created branch lucene4956 and checked in an arirang module in lucene/analysis.  I've added a basic test that tests segmentation, offsets, etc.\n\nOther updates:\n\n\n\tSome compilation warnings related to generics have been fixed, but several are to go.\n\tLicense headers have been added to all source code files\n\tAuthor tags have been removed from all files, except StringUtils pending SooMyoung's feedback (see above)\n\tAdded IntelliJ IDEA config to make ant idea set things up correctly.  Eclipse is TODO.\n\n\n\nMy next step is to fix the compilation related warning altogether and once we confirmed StringUtils, I think we can do the incubator-general vote.  I'll keep you posted.\n\nI think we should also consider rewriting and optimise some of the code here and there, but that's for later.  It's great if you can be involved in this process, SooMyoung!  I'll probably need your help and good advice here and there.  ",
            "author": "Christian Moen",
            "id": "comment-13649260"
        },
        {
            "date": "2013-05-05T05:40:04+0000",
            "content": "[lucene4956 commit] sarowe\nhttp://svn.apache.org/viewvc?view=revision&revision=1479239\n\nLUCENE-4956: add IntelliJ test run config for Arirang; add Maven config for Arirang ",
            "author": "Commit Tag Bot",
            "id": "comment-13649280"
        },
        {
            "date": "2013-05-05T06:01:48+0000",
            "content": "I've created branch lucene4956 and checked in an arirang module in lucene/analysis. I've added a basic test that tests segmentation, offsets, etc.\n\nCool!\n\nLicense headers have been added to all source code files\n\nI can see one that doesn't have one: TestKoreanAnalyzer.java.  I'll take a pass over all the files.\n\nEclipse is TODO.\n\nI ran ant eclipse and it seemed to do the right thing already -I can see Arirang entries in the .classpath file that gets produced - I don't think there's anything to be done.  I don't use Eclipse, though, so I can't be sure.\n\nI added Maven config and an IntelliJ Arirang module test run configuration. ",
            "author": "Steve Rowe",
            "id": "comment-13649284"
        },
        {
            "date": "2013-05-05T07:56:54+0000",
            "content": "Thanks, Steve.  I've added the missing license header to TestKoreanAnalyzer.java. ",
            "author": "Christian Moen",
            "id": "comment-13649296"
        },
        {
            "date": "2013-05-05T08:13:07+0000",
            "content": "I have seen the Tokenizer also uses JFlex, but an older version as used for Lucene's other tokenizers (like StandardTokenizer). Can we add the ANT tasks like we have for StandardTokenizer to regenerate the source file from build.xml. Finally we should regenerate the Java files with the JFlex trunk version and compare with the one committed here (if there are differences). ",
            "author": "Uwe Schindler",
            "id": "comment-13649300"
        },
        {
            "date": "2013-05-05T17:40:57+0000",
            "content": "Good points, Uwe.  I'll look into this. ",
            "author": "Christian Moen",
            "id": "comment-13649379"
        },
        {
            "date": "2013-05-05T19:21:42+0000",
            "content": "Thanks, Steve. I've added the missing license header to TestKoreanAnalyzer.java.\n\nI looked over the rest of the files, and the only things missing license headers are the dictionary files and the korean.properties file, all under src/resources/.  I committed a license header to korean.properties.\n\nI tried adding '#'-commented-out headers to the .dic files (a couple of them already have '######' and '//######' lines), but that triggered a test failure, so more work will need to be done to make the license headers inline in the dictionary files.   ",
            "author": "Steve Rowe",
            "id": "comment-13649395"
        },
        {
            "date": "2013-05-05T19:44:56+0000",
            "content": "\nCould you comment about the origins and authorship of org.apache.lucene.analysis.kr.utils.StringUtil in your tar file?\nI'm seeing a lot of authors in this file. Is this from Apache Commons Lang? Thanks!\n\nI looked at the file content, and it's definitely from Apache Commons Lang (the class is named StringUtils there, renamed StringUtil here), circa early 2010, maybe with a little pulled in from another Commons Lang class.\n\nI've eliminated StringUtil - it's almost all calls to StringUtils.split(String, separators) - its javadoc is:\n\n\n/**\n * <p>Splits the provided text into an array, separators specified.\n * This is an alternative to using StringTokenizer.</p>\n *\n * <p>The separator is not included in the returned String array.\n * Adjacent separators are treated as one separator.\n * For more control over the split use the StrTokenizer class.</p>\n *\n * <p>A <code>null</code> input String returns <code>null</code>.\n * A <code>null</code> separatorChars splits on whitespace.</p>\n *\n * <pre>\n * StringUtil.split(null, *)         = null\n * StringUtil.split(\"\", *)           = []\n * StringUtil.split(\"abc def\", null) = [\"abc\", \"def\"]\n * StringUtil.split(\"abc def\", \" \")  = [\"abc\", \"def\"]\n * StringUtil.split(\"abc  def\", \" \") = [\"abc\", \"def\"]\n * StringUtil.split(\"ab:cd:ef\", \":\") = [\"ab\", \"cd\", \"ef\"]\n * </pre>\n *\n * @param str  the String to parse, may be null\n * @param separatorChars  the characters used as the delimiters,\n *  <code>null</code> splits on whitespace\n * @return an array of parsed Strings, <code>null</code> if null String input\n */\n\n\n\nI'm replacing calls to this method with calls to String.split(regex), where regex is \"[char]+\", and char is the (in all cases singular) split character.\n\nI'll commit the changes and the StringUtil.java removal in a little bit once I've got it compiling and the tests succeed. ",
            "author": "Steve Rowe",
            "id": "comment-13649404"
        },
        {
            "date": "2013-05-05T20:02:43+0000",
            "content": "[lucene4956 commit] sarowe\nhttp://svn.apache.org/viewvc?view=revision&revision=1479362\n\nLUCENE-4956: Remove o.a.l.analysis.kr.utils.StringUtil and all calls to it (mostly StringUtil.split, replaced with String.split) ",
            "author": "Commit Tag Bot",
            "id": "comment-13649410"
        },
        {
            "date": "2013-05-05T20:30:20+0000",
            "content": "This looks like a typo to me, in KoreanEnv.java - the second FILE_DICTIONARY should instead be FILE_EXTENSION:\n\n\n/**\n * Initialize the default property values.\n */\nprivate void initDefaultProperties() {\n  defaults = new Properties();\n\t\n  defaults.setProperty(FILE_SYLLABLE_FEATURE,\"org/apache/lucene/analysis/kr/dic/syllable.dic\");\n  defaults.setProperty(FILE_DICTIONARY,\"org/apache/lucene/analysis/kr/dic/dictionary.dic\");\n  defaults.setProperty(FILE_DICTIONARY,\"org/apache/lucene/analysis/kr/dic/extension.dic\");\t\t\n\n ",
            "author": "Steve Rowe",
            "id": "comment-13649420"
        },
        {
            "date": "2013-05-05T20:44:22+0000",
            "content": "[lucene4956 commit] sarowe\nhttp://svn.apache.org/viewvc?view=revision&revision=1479386\n\nLUCENE-4956: fix typo ",
            "author": "Commit Tag Bot",
            "id": "comment-13649421"
        },
        {
            "date": "2013-05-05T20:58:52+0000",
            "content": "[lucene4956 commit] sarowe\nhttp://svn.apache.org/viewvc?view=revision&revision=1479391\n\nLUCENE-4956: Add license headers to dictionary files, and modify FileUtil.readLines() to ignore lines beginning with comment char '!' ",
            "author": "Commit Tag Bot",
            "id": "comment-13649424"
        },
        {
            "date": "2013-05-05T21:29:52+0000",
            "content": "I added license headers to the dictionary files, so AFAICT all files now have Apache License headers.\n\nI've updated http://incubator.apache.org/ip-clearance/lucene-korean-analyzer.html - it looks ready to go to me.  (Again, I can only the control the XML version of this, at http://svn.apache.org/repos/asf/incubator/public/trunk/content/ip-clearance/lucene-korean-analyzer.xml, so it might be a day or so before the HTML version catches up.)\n\nI think we're ready for the incubator-general vote.  Christian Moen, do you agree?\n\nWe don't need to wait for the vote result to continue making improvements, e.g. tabs->space, svn:eol-style->native, etc. - the vote email will point to the revision on the branch we think is vote-worthy: r1479391. ",
            "author": "Steve Rowe",
            "id": "comment-13649436"
        },
        {
            "date": "2013-05-05T21:54:53+0000",
            "content": "soomyung, I don't understand the following method in WordSpaceAnalyzer.java - what's the point of the method always returning false? (i.e.: if(true) return false;): \n\n\nprivate boolean isNounPart(String str, int jstart) throws MorphException  {\n    \n  if(true) return false;\n    \n  for(int i=jstart-1;i>=0;i--) {      \n    if(DictionaryUtil.getWordExceptVerb(str.substring(i,jstart+1))!=null)\n      return true;\n  }\n    \n  return false;\n}\n\n\n\nisNounPart() is only called from one method in the same class: findJosaEnd(snipt,jstart):\n\n\nif(DictionaryUtil.existJosa(str) && !findNounWithinStr(snipt,i,i+2) && !isNounPart(snipt,jstart)) {\n\n ",
            "author": "Steve Rowe",
            "id": "comment-13649445"
        },
        {
            "date": "2013-05-05T22:26:37+0000",
            "content": "[lucene4956 commit] sarowe\nhttp://svn.apache.org/viewvc?view=revision&revision=1479410\n\nLUCENE-4956: - svn:eol-style -> native\n\n\ttabs -> spaces\n\tregularized java code indents to 2 spaces per level\n\n ",
            "author": "Commit Tag Bot",
            "id": "comment-13649452"
        },
        {
            "date": "2013-05-06T09:52:42+0000",
            "content": "Christian Moen I'm sorry that I didn't reply to your comment on the last weekend! I'm seeing that Steve Rowe solved your problem. am I right?\nSteve Rowe I checked the method. isNounPart() is no more necessary. \nSpaces should be inserted between phrases in a korean sentence, but many people are confused in where inserting spaces. \n\nThe isNounPart() method examine if spaces should be inserted at a specific position only when a noun existing in the dictionary precede it.\nAfter testing, I found that the method is superfluous.\nI'm sorry not to correct the source code before contributing. ",
            "author": "SooMyung Lee",
            "id": "comment-13649633"
        },
        {
            "date": "2013-05-06T16:24:06+0000",
            "content": "I think we're ready for the incubator-general vote. Christian Moen, do you agree?\n\n+1  ",
            "author": "Christian Moen",
            "id": "comment-13649833"
        },
        {
            "date": "2013-05-06T16:51:23+0000",
            "content": "I am not really familiar with the \"incubator-general vote\". From looking at the legal clearance page, it sounds like the vote is simply \"accepting the donation\", as opposed to voting that the branch is ready to commit to trunk, correct?\n\nI did a Jira search and found no previous references to \"incubator-general vote\" - from Google search I got the impression it was more related to podlings rather than simple code module contributions. ",
            "author": "Jack Krupansky",
            "id": "comment-13649857"
        },
        {
            "date": "2013-05-06T17:03:03+0000",
            "content": "Jack, thats correct.\n\nIt is a vote for IP clearance. For example, Simon called an IP clearance vote on the incubator list for Kuromoji before we integrated it into Lucene. ",
            "author": "Robert Muir",
            "id": "comment-13649868"
        },
        {
            "date": "2013-05-06T17:17:09+0000",
            "content": "Hi Jack, \n\nFrom http://incubator.apache.org/ip-clearance/, which is (quoting from that page):\n\n\nIntellectual property clearance\n\nOne of the Incubator's roles is to ensure that proper attention is paid to intellectual property. From time to time, an external codebase is brought into the ASF that is not a separate incubating project but still represents a substantial contribution that was not developed within the ASF's source control system and on our public mailing lists. This is a short form of the Incubation checklist, designed to allow code to be imported with alacrity while still providing for oversight.\n[...]\nOnce a PMC directly checks-in a filled-out short form, the Incubator PMC will need to approve the paper work after which point the receiving PMC is free to import the code.\n\nThe \"short form\" referred to above is an XML template, which I've completed for this code base, and which is at some (apparently regular?) interval converted to HTML (this is also linked from the above-linked IP clearance page as \"Korean Analyzer\"): http://incubator.apache.org/ip-clearance/lucene-korean-analyzer.html ",
            "author": "Steve Rowe",
            "id": "comment-13649879"
        },
        {
            "date": "2013-05-07T19:38:22+0000",
            "content": "Yesterday I called a vote for this contribution on general@incubator.apache.org: http://mail-archives.apache.org/mod_mbox/incubator-general/201305.mbox/%3c7AD4D4E3-530B-41E3-8323-DA3D66A40E7E@apache.org%3e ",
            "author": "Steve Rowe",
            "id": "comment-13651216"
        },
        {
            "date": "2013-05-08T12:12:36+0000",
            "content": "Updates:\n\n\n\tAdded text_kr field type to schema.xml\n\tFixed Solr factories to load field type text_kr in the example\n\tUpdated javadoc so that it compiles cleanly (mostly removed illegal javadoc)\n\tUpdated various build things related to include Korean in the Solr distribution\n\tAdded placeholder stopwords file\n\tAdded services for arirang\n\n\n\nKorean analysis using field type text_kr seems to be doing the right thing out-of-the-box now, but some configuration options in the factories aren't working as of now.  There are several other things that needs polishing up, but we're making progress. ",
            "author": "Christian Moen",
            "id": "comment-13651826"
        },
        {
            "date": "2013-05-08T23:22:04+0000",
            "content": "Great job!  ",
            "author": "Edward J. Yoon",
            "id": "comment-13652502"
        },
        {
            "date": "2013-05-10T01:07:07+0000",
            "content": "Hi Christian,\nThanks for your great work.\n\nI'd like to ask you to modify the text_kr field type definition in schema.xml as follows\n\n    <fieldType name=\"text_kr\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer type=\"index\">\n        <tokenizer class=\"solr.KoreanTokenizerFactory\"/>\n        <filter class=\"solr.KoreanFilterFactory hasOrigin=\"true\" hasCNoun=\"true\"  bigrammable=\"true\"\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_kr.txt\"/>\n      </analyzer>\n      <analyzer type=\"query\">\n        <tokenizer class=\"solr.KoreanTokenizerFactory\"/>\n        <filter class=\"solr.KoreanFilterFactory hasOrigin=\"false\" hasCNoun=\"false\"  bigrammable=\"false\"\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"lang/stopwords_kr.txt\"/>\n      </analyzer>      \n    </fieldType>\n\n ",
            "author": "SooMyung Lee",
            "id": "comment-13653453"
        },
        {
            "date": "2013-05-10T15:45:15+0000",
            "content": "Yesterday I called a vote for this contribution on general@incubator.apache.org: http://mail-archives.apache.org/mod_mbox/incubator-general/201305.mbox/%3c7AD4D4E3-530B-41E3-8323-DA3D66A40E7E@apache.org%3e\n\nThis vote has passed, so we're now free to incorporate this contribution into the code base when and as we see fit. ",
            "author": "Steve Rowe",
            "id": "comment-13654554"
        },
        {
            "date": "2013-05-11T02:50:19+0000",
            "content": "Cool! Thanks, Steve ",
            "author": "SooMyung Lee",
            "id": "comment-13655122"
        },
        {
            "date": "2013-05-14T08:46:34+0000",
            "content": "Thanks, Steve & co.! ",
            "author": "Christian Moen",
            "id": "comment-13656901"
        },
        {
            "date": "2013-05-14T09:13:32+0000",
            "content": "Hello SooMyung,\n\nThanks for the above regarding field type.  The general approach we have taken in Lucene is to do the same analysis at both index and query side.  For example, the Japanese analyzer also has functionality to do compound splitting and we've discussed doing this one the index side only per default for field type text_ja, but we decided against it.\n\nI've included your field type in the latest code I've checked in just now, but it's likely that we will change this in the future.\n\nI'm wondering if you could help me with a few sample sentences that illustrates the various options KoreanFilter has.  I'd like to add some test-cases for these to better understand the differences between them and to verify correct behaviour.  Test-cases for this is also a useful way to document functionality in general.  Thanks for any help with this! ",
            "author": "Christian Moen",
            "id": "comment-13656919"
        },
        {
            "date": "2013-05-14T14:56:46+0000",
            "content": "Hi, Christian.\nUntil this week, I'll prepare some test cases and documents that explain how the options work and why those are needed. ",
            "author": "SooMyung Lee",
            "id": "comment-13657105"
        },
        {
            "date": "2013-05-17T09:44:08+0000",
            "content": "I've run KoreanAnalyzer on Korean Wikipedia and also had a look at memory/heap usage.  Things look okay overall.\n\nI believe KoreanFilter uses wrong offsets for synonym tokens, which was discovered by random-blasting.  Looking into the issue... ",
            "author": "Christian Moen",
            "id": "comment-13660528"
        },
        {
            "date": "2013-05-18T10:05:49+0000",
            "content": "Hi, christian. \n\nI have made some changes of the source code and uploaded that.\nI have changed the source code relating to keyword extraction. \nI have removed the properties relating to keyword extraction and changed the keyword extraction logic. \nI've also added a test case that describe how the korean analyzer works.\n\nI hope this is of some help to you! ",
            "author": "SooMyung Lee",
            "id": "comment-13661327"
        },
        {
            "date": "2013-05-22T00:25:09+0000",
            "content": "Hi, Christian.\n\nI have named the korean analyzer package as \"kr\" but recently I found that It is incorrect. \"kr\" is the country code of the south korean and \"kp\" is the country code of the north korea. I think \"ko\" is more suitable for the name of the korean anzlyzer package. \"ko\" is the korean language code. So, I want you to rename the korean analyzer package from \"kr\" to \"ko\". ",
            "author": "SooMyung Lee",
            "id": "comment-13663622"
        },
        {
            "date": "2013-05-22T00:29:18+0000",
            "content": "Yes, \"ko\" is correct. Use country codes for locales, but language codes for stemmers. ",
            "author": "Walter Underwood",
            "id": "comment-13663626"
        },
        {
            "date": "2013-05-22T14:58:21+0000",
            "content": "\nI have named the korean analyzer package as \"kr\" but recently I found that It is incorrect. \"kr\" is the country code of the south korean and \"kp\" is the country code of the north korea. I think \"ko\" is more suitable for the name of the korean anzlyzer package. \"ko\" is the korean language code. So, I want you to rename the korean analyzer package from \"kr\" to \"ko\"\n\nHi SooMyung, thanks, I'll make the switch (unless Christian beats me to it). ",
            "author": "Steve Rowe",
            "id": "comment-13664163"
        },
        {
            "date": "2013-05-22T16:18:21+0000",
            "content": "I'm happy to take care of this unless you want to do it, Steve.  I can do this either tomorrow or on Friday.  Thanks. ",
            "author": "Christian Moen",
            "id": "comment-13664223"
        },
        {
            "date": "2013-05-22T16:21:33+0000",
            "content": "Hi Christian,\n\nI'm in process now, should be done in a little bit. \n\nBTW, I also brought the branch up-to-date with trunk.\n\nSteve ",
            "author": "Steve Rowe",
            "id": "comment-13664225"
        },
        {
            "date": "2013-05-22T16:27:38+0000",
            "content": "Thanks a lot! ",
            "author": "Christian Moen",
            "id": "comment-13664228"
        },
        {
            "date": "2013-05-22T16:46:36+0000",
            "content": "I'm in process now, should be done in a little bit.\n\nDone: committed the 'kr'->'ko' switch at r1486269 on branches/lucene4956/. ",
            "author": "Steve Rowe",
            "id": "comment-13664245"
        },
        {
            "date": "2013-07-10T02:51:47+0000",
            "content": "Hi, Steve\n\nI see you created 4.4 branch for releasing.\nAfter I looked over it, I found that the Korean analyzer(Arirang) is missing.\n\nCan you tell me when the korean analyzer can be incorporated into the release.\n ",
            "author": "SooMyung Lee",
            "id": "comment-13704145"
        },
        {
            "date": "2013-07-10T02:58:02+0000",
            "content": "Hello SooMyung,\n\nI'm the one who haven't followed up properly on this as I've been too bogged down with other things.  I've set aside time next week to work on this and I hope to have Korean merged and integrated with trunk then.  I'm not sure we can make 4.4, but I'm willing to put in extra effort if there's a chance we can get it in in time. ",
            "author": "Christian Moen",
            "id": "comment-13704147"
        },
        {
            "date": "2013-07-10T04:34:14+0000",
            "content": "Hi, Christian\n\nI can understand your situation. I know you run the company.\n\nI was just wondering if there is any problem with integrating it.\nIf you need any help, please let me know it. ",
            "author": "SooMyung Lee",
            "id": "comment-13704200"
        },
        {
            "date": "2013-08-14T06:51:04+0000",
            "content": "I've now aligned the branch with trunk, updated the example schema.xml to use text_ko naming for the Korean field type.\n\nI've also indexed Korean Wikipedia continuously for a few hours and the JVM heap looks fine.\n\nThere are several additional things that can be done with this code, including generating the parser using JFlex at build time, fixing some of the position issues with random-blasting, cleanups and dead-code removal, etc.  This said, I believe the code we have is useful to Korean users as-is and I'm thinking it's a good idea to integrate it into trunk and iterate further from there.\n\nPlease share your thoughts.  Thanks. ",
            "author": "Christian Moen",
            "id": "comment-13739301"
        },
        {
            "date": "2013-08-14T08:08:31+0000",
            "content": "Attaching a patch against trunk (r1513348). ",
            "author": "Christian Moen",
            "id": "comment-13739387"
        },
        {
            "date": "2013-08-14T08:12:23+0000",
            "content": "SooMyung, let's sync up regarding your latest changes (the patch you attached).  I'm thinking perhaps we can merge to trunk first and iterate from there.  Thanks. ",
            "author": "Christian Moen",
            "id": "comment-13739392"
        },
        {
            "date": "2013-08-14T11:40:41+0000",
            "content": "Hi, Christian\nThank you for your effort.\n\nI'll review the changes what you made.\nI've been also made some improvements. I'll upload the patch soon. ",
            "author": "SooMyung Lee",
            "id": "comment-13739548"
        },
        {
            "date": "2013-09-11T04:51:33+0000",
            "content": "Hi Christian,\n\nI have sync up and I made some modification.\nI'm attaching the patch. ",
            "author": "SooMyung Lee",
            "id": "comment-13763990"
        },
        {
            "date": "2013-10-05T03:00:42+0000",
            "content": "Hi Christian,\n\nI didn't hear any news from you since last August. \nDo you have any problem with moving to next step?\n\nI run a Korean developers community for the Korean Analyzer.\nI announced that Arirang analyzer will be incorporated into lucene and solr soon. \nSo, many developers are waiting for that.\n\nI want we go to next step quickly. If you need any help, Please let me know.  ",
            "author": "SooMyung Lee",
            "id": "comment-13786899"
        },
        {
            "date": "2013-10-05T03:11:54+0000",
            "content": "Thanks for pushing me on this.  I'll have a look at your recent changes and commit to trunk shortly if everything seems fine.  I hope to have this committed to trunk early next week.  Sorry for this having dragged out. ",
            "author": "Christian Moen",
            "id": "comment-13786905"
        },
        {
            "date": "2013-10-08T08:52:48+0000",
            "content": "SooMyung,\n\nThe patch you uploaded on September 11th, was that made against the latest lucene4956 branch?\n\nThe patch doesn't apply properly against on lucene4956 for me.  Could you clarify its origin and instruct me how it can be applied?  If you can make a patch against the code on lucene4956, that would be much appreciated.\n\nThanks! ",
            "author": "Christian Moen",
            "id": "comment-13789052"
        },
        {
            "date": "2013-10-08T18:44:49+0000",
            "content": "Christian,\n\nYes, I worked my last patch against on lucene4956. I'll check the problem and inform you how to solve it within today. ",
            "author": "SooMyung Lee",
            "id": "comment-13789512"
        },
        {
            "date": "2013-10-09T02:00:33+0000",
            "content": "Thanks a lot. ",
            "author": "Christian Moen",
            "id": "comment-13789944"
        },
        {
            "date": "2013-10-14T10:48:30+0000",
            "content": "Soomyung and myself met up in Seoul today and we've merged his latest locally.  I'll commit the changes to this branch when I'm back in Tokyo and Soomyung will follow up with fixing a known issue afterwards.  Hopefully we can commit this to trunk very soon. ",
            "author": "Christian Moen",
            "id": "comment-13794046"
        },
        {
            "date": "2013-10-14T17:27:14+0000",
            "content": "Hi,\nI have seen the same code at a customer and found a big bug in FileUtils and JarResources. We should fix and replace this code completely. It's not platform independent. We should fix the following (in my opinion) horrible code parts:\n\n\tFileUtils: The code with getProtectionDomain is very crazy... It also will never work if the JAR file is not a local file, but maybe some other resource. Its also using APIs that are not intended for the use-case. getProtectionDomain() is for sure not to be used to get the JAR file of the classloader.\n\tFileUtils converts the JAR file URL (from getProtectionDomain) in a wrong way to a filesystem path: We should add URL#getPath() to the forbidden APIs, it is almost always a bug!!! The code should use toURI() and then new File(uri). The other methods in FileUtil are also having similar bugs or try to prevent them. The whole class must be removed, sorry!\n\tJarResources is some crazy caching for resources and in combination with FileUtils its just wrong. Its also does not scale if you create an uber-jar. The idea of this class is to not always open a stream again, so it loads all resources of the JAR file to memory. This is the wrong way to do. Please remove this!\n\n\n\nWe should remove both classes completely and load resources correctly with Class#getResourceAsStream. ",
            "author": "Uwe Schindler",
            "id": "comment-13794294"
        },
        {
            "date": "2013-10-15T00:12:59+0000",
            "content": "Uwe Schindler Thank you for your advice,\nI have opened this source code in sourceforege since 2009 and have many users. but, nobody told me the bugs and I also didn't know that. Christian and myself will fix the bugs soon. Thank you again. ",
            "author": "SooMyung Lee",
            "id": "comment-13794683"
        },
        {
            "date": "2013-10-16T08:22:47+0000",
            "content": "SooMyung, I've committed the latest changes we merged in Seoul on Monday.  It's great if you can fix the decompounding issue we came across, which we disabled a test for.\n\nUwe, +1 to use Class#getResourceAsStream and remove FileUtils and JarResources.  I'll make these changes and commit to the branch.\n\nOverall, I think there's a lot of things we can do to improve this code.  Would very much like to hear your opinion on what we should fix before committing to trunk and getting this on the 4.x branch and improve from there.  My thinking is that it might be good to get this committed so we'll have Korean working even though the code needs some work.  SooMyung has a community in Korea that uses and it's serving their needs as far as I understand.\n\nHappy to hear people's opinion on this. ",
            "author": "Christian Moen",
            "id": "comment-13796545"
        },
        {
            "date": "2013-10-16T09:42:24+0000",
            "content": "Do we need the Tokenizer here at all or just the filter?\n\nStandardTokenizer is now tagging runs of hangul text with <HANGUL> and cjk text with <IDEOGRAPHIC> in TypeAttribute, isnt that essentially what is needed there?\n\nThe current tokenizer here just seems to be a clone of an old version of standardtokenizer.\n\nThe filter needs a reset() at the very least, that seems to be the issue with testRandom. ",
            "author": "Robert Muir",
            "id": "comment-13796610"
        },
        {
            "date": "2013-10-16T10:26:35+0000",
            "content": "Commit 1532707 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1532707 ]\n\nLUCENE-4956: First step in remove buggy resources stuff:\n\n\tno more properties file\n\tMoved files into correct packages\n\tRemoved KoreanEnv, DictionaryResources is new class\n\n\n\nStill needs more refactoring! ",
            "author": "ASF subversion and git services",
            "id": "comment-13796643"
        },
        {
            "date": "2013-10-16T10:34:22+0000",
            "content": "I committed a cleanup of most of the broken and slow resources stuff. It now only uses Class.getResourceAsStream. I also removed code from the FileUtils class (now named DictionaryResources) which was clearly code cloned from somewhere else.\n\nThe resource loading can be further improved:\n\n\tIt should not be lazy (isn't thread safe), it should load all resources (like kuromoji) exactly one time into a singleton \"holder\" class.\n\tWe should use WordListLoader and nuke the remaining stuff.\n\tThere is very ineffective and slow code at some places, reloading the same file over and over again, just to do a lookup.\n\n\n\nThe code also has legal problems:\n\n\tTrie.java seems to be GPLed (thanks Robert). It seems to be just copied from GNUTella (the name says all). So its defeinitely not Apache Licensed\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-13796648"
        },
        {
            "date": "2013-10-16T11:49:51+0000",
            "content": "Commit 1532737 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1532737 ]\n\nLUCENE-4956: Remove stuff not really needed. TODO: add attribution, because this code is borrowed, too! ",
            "author": "ASF subversion and git services",
            "id": "comment-13796685"
        },
        {
            "date": "2013-10-16T11:53:23+0000",
            "content": "I removed more stuff. Some code was borrowed from common-lang without attribution, too. We have to review the whole code, so we don't violate copyrights or licenses!\n\nOne thing we need to change, too: This code uses the pattern \"catch all Exceptions\" and rethrow as another one. This affects MorphException. This class should be removed and all methods should simply declare the Exceptions throw. Especially we are not allowed to swallow stack traces! Also code sometimes prints to System.out!\n\nMorphException is crazy alltogether: It morphs itsself sometimes  ",
            "author": "Uwe Schindler",
            "id": "comment-13796687"
        },
        {
            "date": "2013-10-16T12:02:49+0000",
            "content": "Commit 1532739 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1532739 ]\n\nLUCENE-4956: More obsolete stuff (not even used), some moves to classes where code parts are solely used ",
            "author": "ASF subversion and git services",
            "id": "comment-13796696"
        },
        {
            "date": "2013-10-16T12:46:03+0000",
            "content": "Commit 1532747 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1532747 ]\n\nLUCENE-4956: Hide ctor of static utility classes ",
            "author": "ASF subversion and git services",
            "id": "comment-13796734"
        },
        {
            "date": "2013-10-16T12:48:14+0000",
            "content": "Commit 1532748 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1532748 ]\n\nLUCENE-4956: Cleanup imports ",
            "author": "ASF subversion and git services",
            "id": "comment-13796739"
        },
        {
            "date": "2013-10-16T12:49:13+0000",
            "content": "Commit 1532749 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1532749 ]\n\nLUCENE-4956: Cleanup imports ",
            "author": "ASF subversion and git services",
            "id": "comment-13796741"
        },
        {
            "date": "2013-10-16T12:53:52+0000",
            "content": "Commit 1532750 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1532750 ]\n\nLUCENE-4956: Replace StringBuffer by StringBuilder ",
            "author": "ASF subversion and git services",
            "id": "comment-13796749"
        },
        {
            "date": "2013-10-17T07:14:51+0000",
            "content": "I did a very quick and dirty evaluation of various analyzers (short queries only) with the HANTEC-2 test collection (http://ir.kaist.ac.kr/anthology/2000.10-%EA%B9%80%EC%A7%80%EC%98%81.pdf)\n\nI compared 4 different analyzers for index time, size, and mean average precision for the \"L2\" relevance set: \n\n\tStandardAnalyzer (whitespace on hangul / unigrams on hanja)\n\tCJKAnalyzer (bigram technique)\n\tKoreanAnalyzer\n\tMecabAnalyzer via JNI (https://github.com/bibreen/mecab-ko-lucene-analyzer)\n\n\n\nFor each one, I used 3 different ranking strategies: DefaultSimilarity, BM25Similarity, and DFR GL2, no parameter tuning of any sort.\n\n\n\n\nAnalyzer\nIndex Time\nIndex Size\nMAP(TFIDF)\nMAP(BM25)\nMAP(GL2)\n\n\nStandard\n31s\n128MB\n.0959\n.1018\n.1028\n\n\nCJK\n30s\n162MB\n.1746\n.1894\n.1910\n\n\nKorean\n195s\n125MB\n.2055\n.2096\n.2058\n\n\nMecab\n138s\n147MB\n.1877\n.1960\n.1928\n\n\n\n\n\nNote that on the first try, I was unable to actually index the entire collection with KoreanAnalyzer, so I had to hack the filter to prevent this:\n\nxception in thread \"main\" java.lang.StringIndexOutOfBoundsException: String index out of range: 4\n\tat java.lang.String.substring(String.java:1907)\n\tat org.apache.lucene.analysis.ko.KoreanFilter.analysisChinese(KoreanFilter.java:405)\n\tat org.apache.lucene.analysis.ko.KoreanFilter.incrementToken(KoreanFilter.java:147)\n\tat org.apache.lucene.analysis.core.LowerCaseFilter.incrementToken(LowerCaseFilter.java:54)\n\tat org.apache.lucene.analysis.util.FilteringTokenFilter.incrementToken(FilteringTokenFilter.java:54)\n\tat org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:174)\n\n\n\nSee the patch for more information (you can also download the data from http://www.kristalinfo.com/TestCollections/ and set some constants and run it yourself).\n\nDon't read too far into it, this was really quick and dirty and might somehow be biased. For example, there are several charset issues in the test collection... But it looks like the analyzer here is effective. ",
            "author": "Robert Muir",
            "id": "comment-13797678"
        },
        {
            "date": "2013-10-17T13:18:04+0000",
            "content": "As a potential user of this technology, I'd like to ask for it to have documentation of its linguistic approach.\n\n\n\tWhat is the goal of the tokenizer? Is it to deliver eojeol or hyung-tae-so? If eojeol, does it split up the case where Korean writers are sometimes relaxed about whitespace between them?\n\tSimilarly, what does it set out to index? Does it index eojeol and them also their contained eumjeol or hyung-tae-so, using position-increment / position-length to indicate compound relationships.\n\n\n ",
            "author": "Benson Margulies",
            "id": "comment-13797876"
        },
        {
            "date": "2013-10-17T18:26:01+0000",
            "content": "Benson Margulies Korean Tokenizer has the feature that identify language (Korean, English or Chinese) in Korean sentence. Usually, eojeol in Korean sentence has some different cases. First, eojeol consists of only Korean letters, Second, eojeol can be a combination of Korean letter and alphanumeric letter. Third, eojeol consists of only all alphanumeric letters. Fourth eojeol consists of Chinese letters. Tokinizer treat first and second case as Korean so Korean Morphological analysis is processed in Korean-filter. In third case, I copied code from standard-filter for korean-filter. In fourth case, Korean-filter map Chinese letter to Korean sound and then if it is a compound noun, decompounding is processed based on dictionary. ",
            "author": "SooMyung Lee",
            "id": "comment-13798230"
        },
        {
            "date": "2013-10-17T19:39:53+0000",
            "content": "Hi, all.\n\nI' going to explain how I develop this code  as Christian recommended because of license and legal problem that Jack Krupansky mentioned in previous comment. \n\nI started to write this code and dictionary in 2006 based on a book which author is Seung-Shik, Kang who is a professor of Kookmin university now.\n\nthe dictionary consist of several files but major files are total.dic, josa.dic, eomi.dic and syllable.dic. in first step of developing dictionary, I collected basic stem words for total.dic and particles for josa.dic and eomi.dic from book and various websites. and then I surveyed how basic stem words can be used on online dictionaries. and  I only referred to the book to make syllable.dic. \nthe rest of files is created by myself during developing except for mapHanja.dic. I added this file two years ago. I'm not sure that this file has not legal problem because many data came from projects result so it is better to remove that data.\n\nto make source code, I referred to the book so major logic was based on the book except for some utilities classes such as String, File and Trie.java. I  copied most of utilities classes from apache common project but Trie.java from other website. I cannot remember the exact website now because it was happend long time ago. but I remember that I read the license that was Apache license.\n\nI finished first version in 2008 and created an online community on a website (called Naver) and uploaded the source code.  the number of community members are over 3700 currently.\nI attended an opensource contest held by Korean government organization in 2009. During the contest, I uploaded the source code to the Sourceforge and got a BlackDuck license test with this code and passed the test.\n\nI have supported users through the online community (http://cafe.naver.com/korlucene). so some users improved dictionaries and source codes and then posted it on the website. and I merged it and opened it again.\n\nThis is the wohle process how I developed the code. If anybody has something to recommend, Please let me know it. ",
            "author": "SooMyung Lee",
            "id": "comment-13798319"
        },
        {
            "date": "2013-10-17T20:49:48+0000",
            "content": "I am told (I don't read Korean myself) that people often leave out the white space between eojeol that are made up entirely of Hangul letters (Korean letters). Are you just defining these very long things to be single eojeol? Prof Kang in his own work has a module that splits these using some rules. ",
            "author": "Benson Margulies",
            "id": "comment-13798393"
        },
        {
            "date": "2013-10-17T21:17:38+0000",
            "content": "Commit 1533264 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533264 ]\n\nLUCENE-4956: Remove StringEscapeUtils by unescaping the mapHanja.dic ",
            "author": "ASF subversion and git services",
            "id": "comment-13798434"
        },
        {
            "date": "2013-10-17T21:39:00+0000",
            "content": "Hi SooMyung Lee,\n\nthanks for the clarification. It was not Jack Krupansky that mentioned the GPL violation, it was Robert and me. I am glad that you are aware of this and you are trying to clarify this. Indeed the License of this file is hard to find out, because the Gnutella one (which is the original) has no license header. But the whole Gnutella project is GPL licensed. Those people also started to donate this code to Google Guava and wanted to relicense to ASF2, but this is not yet done. So we cannot use this code. The missing License header may be the reason for the Blackduck test to be happy.\n\nChristian Moen offered to donate a PatriciaTrie he wrote himself. Maybe we can replace the gnutella one by this one. I would prefer the solution to not use a trie at all. Instead we should use Lucene's FST feature and bundle the whole dictionary as a serialized FST (like kuromoji does).\n\nAbout the other copypasted code: I already removed all commons-io and commons-lang stuff. Commons-io was completely unneeded, because the resource handling to load resources from JAR files was not very good and can be done much easier by a simple Class#getResourceAsStream. I already implemented that and moved some class around, so be sure to update your svn before working more on the module.\n\nI also removed the \\u-escaping from the mapHanja.dic file, so I was able to remove the StringEscapeUtil class, which did too much unescaping (not only \\u, also \\n, \\t,...)! But we should really check the license of this file or create a new one from Unicode tables. I left the file in SVN (converted to plain UTF-8) for now.\n\nI am currently working on rewriting some code that creates too many small objects like strings all the time, because this slows down indexing! E.g. HanjaUtils should not use a String just to lookup a single char in a map. There are better data structures to hold the mapHanja table.\n\nWe should also not use readLines() to load all dictionaries into heap, then use an iterator over it and convert them to something else. We should use a BufferedReader and read it line by line and do the processing directly. ",
            "author": "Uwe Schindler",
            "id": "comment-13798463"
        },
        {
            "date": "2013-10-17T22:03:20+0000",
            "content": "the rest of files is created by myself during developing except for mapHanja.dic. I added this file two years ago. I'm not sure that this file has not legal problem because many data came from projects result so it is better to remove that data.\n\nDo you have some documentation who gave the file to you or where you downloaded it? Some university? Some CD-ROM? ",
            "author": "Uwe Schindler",
            "id": "comment-13798498"
        },
        {
            "date": "2013-10-17T22:21:13+0000",
            "content": "Maybe we can reconstitute this file from other hanja-hangul mappings with clear licenses?\n\nI have not done any processing, I will investigate sources such as https://code.google.com/p/google-input-tools/source/browse/src/chrome/os/nacl-hangul/misc/symbol.txt and unihan and see what it looks like. ",
            "author": "Robert Muir",
            "id": "comment-13798530"
        },
        {
            "date": "2013-10-17T22:21:25+0000",
            "content": "Commit 1533277 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533277 ]\n\nLUCENE-4956: Remove lazy dictionary loading, don't convert to string all the time. This may be improved further if we use an array and substract the smallest codepoint value ",
            "author": "ASF subversion and git services",
            "id": "comment-13798531"
        },
        {
            "date": "2013-10-17T22:27:42+0000",
            "content": "Commit 1533278 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533278 ]\n\nLUCENE-4956: More improvements ",
            "author": "ASF subversion and git services",
            "id": "comment-13798536"
        },
        {
            "date": "2013-10-17T23:04:29+0000",
            "content": "Commit 1533282 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533282 ]\n\nLUCENE-4956: Remove thread-unsafe lazy loading. Initialize in static ctor ",
            "author": "ASF subversion and git services",
            "id": "comment-13798566"
        },
        {
            "date": "2013-10-17T23:16:47+0000",
            "content": "Commit 1533286 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533286 ]\n\nLUCENE-4956: Remove thread-unsafe lazy loading. Initialize in static ctor ",
            "author": "ASF subversion and git services",
            "id": "comment-13798582"
        },
        {
            "date": "2013-10-17T23:38:48+0000",
            "content": "Commit 1533293 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533293 ]\n\nLUCENE-4956: Remove MorphException, it is no longer needed. Fix lots of Exception blocks. Remove unused classes. ",
            "author": "ASF subversion and git services",
            "id": "comment-13798607"
        },
        {
            "date": "2013-10-18T00:51:29+0000",
            "content": "Benson Margulies WordSpaceAnalyzer has the feature. If fail to analyze Korean Morphology, KoreanFilter makes WordSpaceAnalyzer try to split eojeol. I'll add some test case. ",
            "author": "SooMyung Lee",
            "id": "comment-13798678"
        },
        {
            "date": "2013-10-18T01:05:06+0000",
            "content": "Hi Robert Muir,\n\nThank you for your comment. I can reconstitute the hanja-hangul mappings file by myself if we cannot find other sources with clear licenses. I can easily get hanja list that often appear in Korean sentence. after then I'll look up online dictionary. I can start with 3,000~4,000 hanjas that is most often appeared in Korean sentences. ",
            "author": "SooMyung Lee",
            "id": "comment-13798686"
        },
        {
            "date": "2013-10-18T04:36:52+0000",
            "content": "Hi, I think we have some good sources with acceptable licenses.\n\nI will add my processing to tools/ and generate a new mapHanja.dic from sources with clear licenses, and you can then review and help clean up afterwards. I had to first understand precisely how it was being used in analysisChinese but now I get it \n\nI will reply back soon. ",
            "author": "Robert Muir",
            "id": "comment-13798791"
        },
        {
            "date": "2013-10-18T04:49:21+0000",
            "content": "Great, thanks a lot ! ",
            "author": "SooMyung Lee",
            "id": "comment-13798796"
        },
        {
            "date": "2013-10-18T06:11:31+0000",
            "content": "Commit 1533329 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533329 ]\n\nLUCENE-4956: generate new mapHanja.dic from sources with clear license ",
            "author": "ASF subversion and git services",
            "id": "comment-13798832"
        },
        {
            "date": "2013-10-18T06:15:46+0000",
            "content": "OK, see new file here: http://svn.apache.org/viewvc/lucene/dev/branches/lucene4956/lucene/analysis/arirang/src/resources/org/apache/lucene/analysis/ko/dic/mapHanja.dic?revision=1533329&view=markup\n\nGenerated with: http://svn.apache.org/viewvc/lucene/dev/branches/lucene4956/lucene/analysis/arirang/src/tools/java/org/apache/lucene/analysis/ko/GenerateHanjaMap.java?revision=1533329&view=markup\n\nhanja keys: 27784\nhanja/hangul mappings: 28861\n ",
            "author": "Robert Muir",
            "id": "comment-13798836"
        },
        {
            "date": "2013-10-18T07:55:47+0000",
            "content": "Robert Muir, Thanks again.\n\nI have run a test case with new hanja-hangul mapping files. It works very well. ",
            "author": "SooMyung Lee",
            "id": "comment-13798887"
        },
        {
            "date": "2013-10-18T08:10:25+0000",
            "content": "Hi,\nI reviewed the Trie.java code and its usage yesterday. Trie.java is only used at 2 places with same usage pattern:\n\n\tDictionaryUtil#dictionary\n\tTagger#occurences\n\n\n\nIn both cases there are only 2 types of matches:\n\n\tDictionaryUtil#findWithPrefix: returns an Iterator of all entries with a given prefix\n\tDictionaryUtil#getWord: returns WordEntry for an exact match\n\tTagger#getGR: returns iterator of all entries with a given prefix\n\n\n\nThese use cases are not really the ones a Trie is made for, so the ideal and most performant would be to USE Lucene's FST implementation. We would also get an Iterator-like interface to look up prefixes. So I would suggest to replace these 3 methods by an FST backing them. The dictionary would then (like for kuromoji) be preprocessed and saved as serialized FST in the resource file. The original dictionary as text file would only be available in the Lucene source distribution to regenerate the FST. ",
            "author": "Uwe Schindler",
            "id": "comment-13798894"
        },
        {
            "date": "2013-10-18T08:15:17+0000",
            "content": "Commit 1533355 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533355 ]\n\nLUCENE-4956: Remove useless synchronization (no lazy loading anymore) ",
            "author": "ASF subversion and git services",
            "id": "comment-13798897"
        },
        {
            "date": "2013-10-18T08:24:09+0000",
            "content": "Commit 1533358 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533358 ]\n\nLUCENE-4956: Reorder loading ",
            "author": "ASF subversion and git services",
            "id": "comment-13798903"
        },
        {
            "date": "2013-10-18T08:47:36+0000",
            "content": "Commit 1533362 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533362 ]\n\nLUCENE-4956: Make WordEntry components final ",
            "author": "ASF subversion and git services",
            "id": "comment-13798918"
        },
        {
            "date": "2013-10-18T09:44:28+0000",
            "content": "Commit 1533371 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533371 ]\n\nLUCENE-4956: Quick fix to remove the Trie for the Tagger. The file is very slow and a TreeMap is perfectly fine. Can still be improved, but the primary concern is to remove Trie.java ",
            "author": "ASF subversion and git services",
            "id": "comment-13798959"
        },
        {
            "date": "2013-10-18T10:15:36+0000",
            "content": "Commit 1533378 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533378 ]\n\nLUCENE-4956: Remove debug output, make map unmodifiable ",
            "author": "ASF subversion and git services",
            "id": "comment-13798980"
        },
        {
            "date": "2013-10-18T10:49:37+0000",
            "content": "Commit 1533382 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533382 ]\n\nLUCENE-4956: Don't load full files into big List<>, instead process them line by line (the current code uses iterator anyway). ",
            "author": "ASF subversion and git services",
            "id": "comment-13798997"
        },
        {
            "date": "2013-10-18T11:47:09+0000",
            "content": "Commit 1533403 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533403 ]\n\nLUCENE-4956: Move empty line removal up ",
            "author": "ASF subversion and git services",
            "id": "comment-13799043"
        },
        {
            "date": "2013-10-18T14:33:10+0000",
            "content": "\nSo I would suggest to replace these 3 methods by an FST backing them.\n\nI'm starting on this today.  ",
            "author": "Robert Muir",
            "id": "comment-13799140"
        },
        {
            "date": "2013-10-18T15:01:41+0000",
            "content": "So I would suggest to replace these 3 methods by an FST backing them.\n\nThis one is already gone: Tagger#getGR(prefix)\n\nSo the big dictionary is the thing to do. ",
            "author": "Uwe Schindler",
            "id": "comment-13799158"
        },
        {
            "date": "2013-10-18T15:39:17+0000",
            "content": "Commit 1533517 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533517 ]\n\nLUCENE-4956: Make parser more strict, remove bullshit from data files ",
            "author": "ASF subversion and git services",
            "id": "comment-13799205"
        },
        {
            "date": "2013-10-18T15:45:18+0000",
            "content": "Commit 1533521 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533521 ]\n\nLUCENE-4956: Unify Exceptions ",
            "author": "ASF subversion and git services",
            "id": "comment-13799214"
        },
        {
            "date": "2013-10-18T16:43:32+0000",
            "content": "Commit 1533549 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533549 ]\n\nLUCENE-4956: add TestCoverageHack ",
            "author": "ASF subversion and git services",
            "id": "comment-13799274"
        },
        {
            "date": "2013-10-18T16:44:12+0000",
            "content": "Commit 1533550 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533550 ]\n\nLUCENE-4956: Tagger is completely dead code! Why did I put work into it? ",
            "author": "ASF subversion and git services",
            "id": "comment-13799276"
        },
        {
            "date": "2013-10-18T16:53:50+0000",
            "content": "Commit 1533557 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533557 ]\n\nLUCENE-4956: remove empty dir ",
            "author": "ASF subversion and git services",
            "id": "comment-13799290"
        },
        {
            "date": "2013-10-18T17:20:27+0000",
            "content": "Commit 1533562 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533562 ]\n\nLUCENE-4956: remove some dead code ",
            "author": "ASF subversion and git services",
            "id": "comment-13799314"
        },
        {
            "date": "2013-10-19T05:59:44+0000",
            "content": "Commit 1533695 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533695 ]\n\nLUCENE-4956: move data to src/data and setup regeneration (for now simple copy) ",
            "author": "ASF subversion and git services",
            "id": "comment-13799794"
        },
        {
            "date": "2013-10-19T08:42:16+0000",
            "content": "Commit 1533709 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533709 ]\n\nLUCENE-4956: Remove unused file constants ",
            "author": "ASF subversion and git services",
            "id": "comment-13799835"
        },
        {
            "date": "2013-10-19T16:14:31+0000",
            "content": "Commit 1533781 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533781 ]\n\nLUCENE-4956: allow use of these with datainput ",
            "author": "ASF subversion and git services",
            "id": "comment-13799937"
        },
        {
            "date": "2013-10-19T19:54:38+0000",
            "content": "Commit 1533813 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533813 ]\n\nLUCENE-4956: refactor syllable handling to not be a list of thousands of arrays ",
            "author": "ASF subversion and git services",
            "id": "comment-13799989"
        },
        {
            "date": "2013-10-19T20:07:17+0000",
            "content": "Commit 1533815 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533815 ]\n\nLUCENE-4956: Fix error handling in HanjaUtil to prevent NPE on broken classpath ",
            "author": "ASF subversion and git services",
            "id": "comment-13799991"
        },
        {
            "date": "2013-10-19T20:12:40+0000",
            "content": "Commit 1533817 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533817 ]\n\nLUCENE-4956: Fix error handling in HanjaUtil to prevent NPE on broken classpath ",
            "author": "ASF subversion and git services",
            "id": "comment-13799992"
        },
        {
            "date": "2013-10-19T20:23:38+0000",
            "content": "Commit 1533821 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533821 ]\n\nLUCENE-4956: Use IOUtils.decodingReader to load data files ",
            "author": "ASF subversion and git services",
            "id": "comment-13799995"
        },
        {
            "date": "2013-10-19T21:16:31+0000",
            "content": "Commit 1533835 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533835 ]\n\nLUCENE-4956: more cleanups and visibility fixes ",
            "author": "ASF subversion and git services",
            "id": "comment-13800001"
        },
        {
            "date": "2013-10-19T21:58:48+0000",
            "content": "Commit 1533838 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533838 ]\n\nLUCENE-4956: Move/Rename some files and make pkg-private ",
            "author": "ASF subversion and git services",
            "id": "comment-13800011"
        },
        {
            "date": "2013-10-19T23:25:56+0000",
            "content": "Commit 1533842 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533842 ]\n\nLUCENE-4956: Fix stopwords file, Cleanup analyzer (load stopwords file, no hardcoded stops), and filter (fix broken incrementToken, implement reset), remove unused varaibles in CompoundNounAnalyzer ",
            "author": "ASF subversion and git services",
            "id": "comment-13800034"
        },
        {
            "date": "2013-10-19T23:32:38+0000",
            "content": "Commit 1533843 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533843 ]\n\nLUCENE-4956: Make filter final, add one more nocommit ",
            "author": "ASF subversion and git services",
            "id": "comment-13800035"
        },
        {
            "date": "2013-10-20T00:27:02+0000",
            "content": "Commit 1533846 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533846 ]\n\nLUCENE-4956: Fix factory to check for incorrect parameter keys, remove bogus parameters, remove bogus matchVersion on KoreanTokenizer ",
            "author": "ASF subversion and git services",
            "id": "comment-13800049"
        },
        {
            "date": "2013-10-20T08:20:06+0000",
            "content": "Commit 1533857 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533857 ]\n\nLUCENE-4956: Partially fix posIncrAtt to preserve increment of first token. The morphQueue still has a bug, added nocommit! ",
            "author": "ASF subversion and git services",
            "id": "comment-13800098"
        },
        {
            "date": "2013-10-20T08:21:42+0000",
            "content": "Commit 1533858 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533858 ]\n\nLUCENE-4956: Rename IndexWord to Token like in Kuromoji! ",
            "author": "ASF subversion and git services",
            "id": "comment-13800099"
        },
        {
            "date": "2013-10-20T08:58:24+0000",
            "content": "Commit 1533862 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533862 ]\n\nLUCENE-4956: Simplier fix for the broken posIncr. I also cleaned up the Token class and made private to the Filter ",
            "author": "ASF subversion and git services",
            "id": "comment-13800108"
        },
        {
            "date": "2013-10-20T09:01:26+0000",
            "content": "Commit 1533863 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533863 ]\n\nLUCENE-4956: Remove useless getters in private class ",
            "author": "ASF subversion and git services",
            "id": "comment-13800110"
        },
        {
            "date": "2013-10-20T09:53:44+0000",
            "content": "Commit 1533865 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533865 ]\n\nLUCENE-4956: fix malformed entries ",
            "author": "ASF subversion and git services",
            "id": "comment-13800113"
        },
        {
            "date": "2013-10-20T10:20:15+0000",
            "content": "Commit 1533872 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533872 ]\n\nLUCENE-4956: remove slow caseless match in trie, don't read headers as actual entries ",
            "author": "ASF subversion and git services",
            "id": "comment-13800117"
        },
        {
            "date": "2013-10-20T11:12:42+0000",
            "content": "Commit 1533877 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533877 ]\n\nLUCENE-4956: ban dictionary corrumption ",
            "author": "ASF subversion and git services",
            "id": "comment-13800123"
        },
        {
            "date": "2013-10-20T16:24:10+0000",
            "content": "Commit 1533923 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1533923 ]\n\nLUCENE-4956: Rewrite iterator-consumer; make \"stupid\" Exception more selective. I have no idea how to fix! ",
            "author": "ASF subversion and git services",
            "id": "comment-13800156"
        },
        {
            "date": "2013-10-21T04:22:56+0000",
            "content": "Commit 1534021 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1534021 ]\n\nLUCENE-4956: clean up compound / feature processing a bit (more coming) ",
            "author": "ASF subversion and git services",
            "id": "comment-13800344"
        },
        {
            "date": "2013-10-21T05:52:01+0000",
            "content": "Commit 1534029 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1534029 ]\n\nLUCENE-4956: more morph cleanups ",
            "author": "ASF subversion and git services",
            "id": "comment-13800382"
        },
        {
            "date": "2013-10-21T06:05:29+0000",
            "content": "Commit 1534030 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1534030 ]\n\nLUCENE-4956: move dictionary entry classes to dictionary package ",
            "author": "ASF subversion and git services",
            "id": "comment-13800391"
        },
        {
            "date": "2013-10-21T06:22:48+0000",
            "content": "Commit 1534032 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1534032 ]\n\nLUCENE-4956: don't use wordentry for uncompound processing ",
            "author": "ASF subversion and git services",
            "id": "comment-13800398"
        },
        {
            "date": "2013-10-21T06:56:31+0000",
            "content": "Commit 1534040 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1534040 ]\n\nLUCENE-4956: don't hold thousands of arrays in dictionary ",
            "author": "ASF subversion and git services",
            "id": "comment-13800411"
        },
        {
            "date": "2013-10-21T12:04:44+0000",
            "content": "Commit 1534115 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1534115 ]\n\nLUCENE-4956: commit working state ",
            "author": "ASF subversion and git services",
            "id": "comment-13800577"
        },
        {
            "date": "2013-10-21T12:31:35+0000",
            "content": "Commit 1534128 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1534128 ]\n\nLUCENE-4956: remove trie ",
            "author": "ASF subversion and git services",
            "id": "comment-13800602"
        },
        {
            "date": "2013-10-21T12:44:44+0000",
            "content": "Commit 1534135 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1534135 ]\n\nLUCENE-4956: Fix file not found case, add close on finally ",
            "author": "ASF subversion and git services",
            "id": "comment-13800611"
        },
        {
            "date": "2013-10-21T13:06:10+0000",
            "content": "Commit 1534141 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1534141 ]\n\nLUCENE-4956: add some cleanups, remove packing, add missing close, lazy-load compound data until you ask for it ",
            "author": "ASF subversion and git services",
            "id": "comment-13800634"
        },
        {
            "date": "2013-10-21T20:29:10+0000",
            "content": "Commit 1534364 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1534364 ]\n\nLUCENE-4956: use a byte1 jamo FST, smaller and much faster ",
            "author": "ASF subversion and git services",
            "id": "comment-13801042"
        },
        {
            "date": "2013-10-22T03:36:19+0000",
            "content": "Commit 1534472 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1534472 ]\n\nLUCENE-4956: do this simpler/faster like kuromoji ",
            "author": "ASF subversion and git services",
            "id": "comment-13801444"
        },
        {
            "date": "2013-10-22T03:38:44+0000",
            "content": "Commit 1534473 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1534473 ]\n\nLUCENE-4956: pull out broken acronym/etc handling, user can just use classicfilter for that ",
            "author": "ASF subversion and git services",
            "id": "comment-13801445"
        },
        {
            "date": "2013-10-22T03:49:54+0000",
            "content": "Commit 1534477 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1534477 ]\n\nLUCENE-4956: don't captureState unless we have to ",
            "author": "ASF subversion and git services",
            "id": "comment-13801446"
        },
        {
            "date": "2013-10-22T06:38:27+0000",
            "content": "Commit 1534514 from Uwe Schindler in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1534514 ]\n\nLUCENE-4956: More optimization on captureState ",
            "author": "ASF subversion and git services",
            "id": "comment-13801543"
        },
        {
            "date": "2013-10-27T18:15:45+0000",
            "content": "Commit 1536174 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1536174 ]\n\nLUCENE-4956: move this out of dictionaryutil ",
            "author": "ASF subversion and git services",
            "id": "comment-13806405"
        },
        {
            "date": "2013-10-27T18:50:56+0000",
            "content": "Commit 1536184 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1536184 ]\n\nLUCENE-4956: replace some getWord != null with hasWord ",
            "author": "ASF subversion and git services",
            "id": "comment-13806422"
        },
        {
            "date": "2013-10-27T22:35:28+0000",
            "content": "Commit 1536214 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1536214 ]\n\nLUCENE-4956: more speedup,style,refactoring ",
            "author": "ASF subversion and git services",
            "id": "comment-13806475"
        },
        {
            "date": "2013-10-28T00:43:31+0000",
            "content": "Commit 1536231 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1536231 ]\n\nLUCENE-4956: more refactoring of decompounding ",
            "author": "ASF subversion and git services",
            "id": "comment-13806497"
        },
        {
            "date": "2013-10-28T00:57:01+0000",
            "content": "Commit 1536233 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1536233 ]\n\nLUCENE-4956: move all the list creation out of compoundnounanalyzer ",
            "author": "ASF subversion and git services",
            "id": "comment-13806498"
        },
        {
            "date": "2013-10-28T01:53:00+0000",
            "content": "Commit 1536234 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1536234 ]\n\nLUCENE-4956: more compound cleanups ",
            "author": "ASF subversion and git services",
            "id": "comment-13806510"
        },
        {
            "date": "2013-10-28T02:31:12+0000",
            "content": "Commit 1536235 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1536235 ]\n\nLUCENE-4956: improve the runtime of maxWord ",
            "author": "ASF subversion and git services",
            "id": "comment-13806519"
        },
        {
            "date": "2013-10-28T04:27:31+0000",
            "content": "Commit 1536244 from Robert Muir in branch 'dev/branches/lucene4956'\n[ https://svn.apache.org/r1536244 ]\n\nLUCENE-4956: more cleanups and remove n^2 in filterIncorrect ",
            "author": "ASF subversion and git services",
            "id": "comment-13806556"
        },
        {
            "date": "2013-10-29T01:30:19+0000",
            "content": "Could you share the trick of unpacking the big tarball, locale-wise? I ended up with:\n\n[benson] /data/HANTEC-2.0  % ls relevance_file\n%B0%FA%C7\u0431%E2%BC%FA%BA\u043e%DF   %C0%FC\u00fc\n\nwhich does not work so well.\n\nDid you set LOCALE to something before unpacking? ",
            "author": "Benson Margulies",
            "id": "comment-13807567"
        },
        {
            "date": "2013-10-29T01:35:09+0000",
            "content": "unzip -O cp949 HANTEC-2.0.zip ",
            "author": "Robert Muir",
            "id": "comment-13807568"
        },
        {
            "date": "2013-10-29T11:18:25+0000",
            "content": "Hmm. When I followed the link, I found a .tar.gz. I guess the zip was further down the page. ",
            "author": "Benson Margulies",
            "id": "comment-13807877"
        },
        {
            "date": "2013-10-29T12:27:12+0000",
            "content": "Something's funny here. On this page (http://www.kristalinfo.com/TestCollections/), the zip file has directories like\n\nHANTEC-2.0/relevance_file/\uacfc\ud559\uae30\uc220\ubd84\uc57c/\nHANTEC-2.0/relevance_file/\uc804\uccb4/\n\nThe first translates as 'Science and Technology' and the second as 'All'.\n\nThe code in the patch expects the word 'full' in latin-alphabet, no funny full-width, in the that intermediate directory. So I don't see how a code-page option to unzip got there. I'm suspecting that an 'mv' is in order. ",
            "author": "Benson Margulies",
            "id": "comment-13807918"
        },
        {
            "date": "2013-10-29T13:49:20+0000",
            "content": "nothing is funny, i renamed them locally, sorry. ",
            "author": "Robert Muir",
            "id": "comment-13807987"
        },
        {
            "date": "2013-11-02T15:47:35+0000",
            "content": "Looks like mapHanja.dic needs some adjustment of the legal notice? Or was this going to be replaced? ",
            "author": "Benson Margulies",
            "id": "comment-13812034"
        },
        {
            "date": "2013-11-02T15:58:46+0000",
            "content": "please point to specific files in svn that you have concerns about.\n\nI recreated this file myself from clearly attributed sources, from scratch. \n\nIt has MORE THAN ENOUGH legal notice. ",
            "author": "Robert Muir",
            "id": "comment-13812039"
        },
        {
            "date": "2013-11-02T16:05:29+0000",
            "content": "My point is that it might have a bit too much legal notice. Generally, when someone grants a license, the headers all move up to some global NOTICE file, and the file is left with just an Apache license. \n\nI also noted the following:\n\n! Except as contained in this notice, the name of a copyright holder shall not be \n! used in advertising or otherwise to promote the sale, use or other dealings in \n! these Data Files or Software without prior written authorization of the copyright holder.\n\nand then noticed:\n\nthat http://www.apache.org/legal/resolved.html says that it approves of \n\n\n\tBSD (without advertising clause).\n\n\n\nSo that Unicode license is possibly an issue.\n\nRight now I'm using the git clone, but I just did a pull, and the pathname is lucene/analysis/arirang/src/data/mapHanja.dic\n\n ",
            "author": "Benson Margulies",
            "id": "comment-13812044"
        },
        {
            "date": "2013-11-02T16:07:45+0000",
            "content": "\nSo that Unicode license is possibly an issue.\n\nNo, its not. https://issues.apache.org/jira/browse/LEGAL-108 ",
            "author": "Robert Muir",
            "id": "comment-13812047"
        },
        {
            "date": "2013-11-02T16:10:36+0000",
            "content": "That jira concerns a different license. The license on the file pointed-to there has no advertising clause that I can spot. Which isn't to say that legal would have a problem with this, just that I don't think that the JIRA in question tells us. ",
            "author": "Benson Margulies",
            "id": "comment-13812050"
        },
        {
            "date": "2013-11-02T16:12:41+0000",
            "content": "This is the unicode license that all of their data and code comes from. There is only one.\n\nPlease, don't waste my time here, if you want to waste the legal team's time, thats ok  ",
            "author": "Robert Muir",
            "id": "comment-13812051"
        },
        {
            "date": "2013-11-02T16:15:30+0000",
            "content": "The exact question about using unicode data tables has been answered explicitly already:\n\nhttp://mail-archives.apache.org/mod_mbox/www-legal-discuss/200903.mbox/%3C3d4032300903030415w4831f6e4u65c12881cbb8642c@mail.gmail.com%3E\n\nI don't think it needs any further discussion ",
            "author": "Robert Muir",
            "id": "comment-13812052"
        },
        {
            "date": "2013-11-02T16:19:07+0000",
            "content": "Rob, I got shat on at great length over this for merely test data over at the WS project.  I had to make the build pull the data over the network to get certain directors off of my back. I'm trying to spare you the experience. That's all.\n\nAs a low-intensity member of the UTC, I would also expect there to be only one license. However, I compare:\n\n\n#  Copyright (c) 1991-2011 Unicode, Inc. All Rights reserved.\n#  \n#  This file is provided as-is by Unicode, Inc. (The Unicode Consortium). No\n#  claims are made as to fitness for any particular purpose. No warranties of\n#  any kind are expressed or implied. The recipient agrees to determine\n#  applicability of information provided. If this file has been provided on\n#  magnetic media by Unicode, Inc., the sole remedy for any claim will be\n#  exchange of defective media within 90 days of receipt.\n#  \n#  Unicode, Inc. hereby grants the right to freely use the information\n#  supplied in this file in the creation of products supporting the\n#  Unicode Standard, and to make copies of this file in any form for\n#  internal or external distribution as long as this notice remains\n#  attached.\n\n\n\nwith\n\n\n! Copyright (c) 1991-2013 Unicode, Inc. \n! All rights reserved. \n! Distributed under the Terms of Use in http://www.unicode.org/copyright.html.\n!\n! Permission is hereby granted, free of charge, to any person obtaining a copy \n! of the Unicode data files and any associated documentation (the \"Data Files\") \n! or Unicode software and any associated documentation (the \"Software\") to deal \n! in the Data Files or Software without restriction, including without limitation \n! the rights to use, copy, modify, merge, publish, distribute, and/or sell copies \n! of the Data Files or Software, and to permit persons to whom the Data Files or \n! Software are furnished to do so, provided that (a) the above copyright notice(s) \n! and this permission notice appear with all copies of the Data Files or Software, \n! (b) both the above copyright notice(s) and this permission notice appear in \n! associated documentation, and (c) there is clear notice in each modified Data \n! File or in the Software as well as in the documentation associated with the Data \n! File(s) or Software that the data or software has been modified.\n!\n! THE DATA FILES AND SOFTWARE ARE PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, \n! EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n! FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS. IN NO \n! EVENT SHALL THE COPYRIGHT HOLDER OR HOLDERS INCLUDED IN THIS NOTICE BE LIABLE FOR \n! ANY CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES \n! WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF \n! CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION \n! WITH THE USE OR PERFORMANCE OF THE DATA FILES OR SOFTWARE.\n! \n! Except as contained in this notice, the name of a copyright holder shall not be \n! used in advertising or otherwise to promote the sale, use or other dealings in \n! these Data Files or Software without prior written authorization of the copyright holder.\n\n\n\nThey look pretty different to me. Go figure?\n ",
            "author": "Benson Margulies",
            "id": "comment-13812053"
        },
        {
            "date": "2013-11-02T16:29:08+0000",
            "content": "\nRob, I got shat on at great length over this for merely test data over at the WS project. I had to make the build pull the data over the network to get certain directors off of my back. I'm trying to spare you the experience. That's all.\n\nThen perhaps you should push back hard when people don't know what they are talking about, like I do. As i said, the question about using unicode data tables has already been directly answered.\n\n\nAs a low-intensity member of the UTC, I would also expect there to be only one license. However, I compare:\n\nI am also one. this means nothing.\n\n\nThey look pretty different to me. Go figure?\n\nThere is only one license from the terms of use page http://www.unicode.org/copyright.html\n\nThat is what I include. Whoever created your \"other license\" decided to omit some of the information, which I did not. ",
            "author": "Robert Muir",
            "id": "comment-13812056"
        },
        {
            "date": "2013-11-02T16:36:49+0000",
            "content": "OK, I see, the email thread about Unicode data in general does certainly cover this. Sometimes the workings of Legal are pretty perplexing. ",
            "author": "Benson Margulies",
            "id": "comment-13812059"
        },
        {
            "date": "2013-11-02T16:44:17+0000",
            "content": "Just so anyone reading the thread knows: the clause Benson mentioned is not an advertising clause:\n\n\nExcept as contained in this notice, the name of a copyright holder shall not be used in advertising or otherwise to promote the sale, use or other dealings in these Data Files or Software without prior written authorization of the copyright holder.\n\nThe BSD advertising clause reads like this:\n\n\nAll advertising materials mentioning features or use of this software must display the following acknowledgement: This product includes software developed by the <organization>.\n\nThese are very different. ",
            "author": "Robert Muir",
            "id": "comment-13812065"
        },
        {
            "date": "2013-12-26T01:07:26+0000",
            "content": "Robert Muir, Uwe Schindler, Christian Moen: is there anything blocking merging the branch into trunk and branch_4x? ",
            "author": "Steve Rowe",
            "id": "comment-13856699"
        },
        {
            "date": "2013-12-26T01:41:08+0000",
            "content": "Yes, the nocommits.\n\nIn general there are not enough tests to proceed fixing more things. I took it as far as I could with TestCoverageHack, but the stuff like the AIOOBE-catching is just the tip of the iceberg to some problems in WSOutput/WordSpaceAnalyzer.\n\nThe main challenge here is just that, there are many many many special cases happening in the analysis logic. There needs to be good tests for these, rather than just testing that the analysis \"does not change\" because currently the analysis does really funky things in some situations and needs to change.\n\nTokenStream logic needs cleanup too: offsets/posincs and so on need to work and BaseTokenStreamTestCase.checkRandomData etc should pass. ",
            "author": "Robert Muir",
            "id": "comment-13856704"
        },
        {
            "date": "2013-12-26T14:01:39+0000",
            "content": "Hi Robert,\n\nThank you for your effort. \n\nYes,  I also found some problems in some case like WordSpaceAnalyzer and offset/posincs, \nand I made some improvement. so, I'll post the patch soon.\n\nI want to help you for the progress. \nbut, I'm feeling strange when I work with you in this project. It is too difficult for me to figure out how I can help you.\nPlease, let me know something that I can help you more detail. Is it helpful to you If I make some test cases ? ",
            "author": "SooMyung Lee",
            "id": "comment-13856878"
        },
        {
            "date": "2013-12-26T21:18:07+0000",
            "content": "Hi,\nI have the same problems like Robert with some code parts. Partly the code is un-understandable and it looks like some places just have \"workarounds\" around silly bugs in the original code (like the catch ArrayIndexOutOfBoundsException and resuming with completely different code paths).\n\nAlso the code generates like 5 completely new java.util.Collections (Lists, Maps,...) per token, without even reusing the previous ones!\n\nThe code has lots of problems with offsets and positions (sometimes we workarounded using Math.max(0, positionFromCrazyCode). The code as it is will not pass TestRandomChains!\n\nRobert and I already rewrote lots of the code and also removed the GPL code. At this point it is still not in a state that can be committed to Lucene trunk or even backporting it. ",
            "author": "Uwe Schindler",
            "id": "comment-13857081"
        },
        {
            "date": "2013-12-26T21:42:16+0000",
            "content": "There is one other problem we have to solve: The code ships with a slightly modified version of a very old version of StandardTokenizer, compiled with the not JVM invariant version of JFlex (the one which uses unicode tables from jvm crafting the source code).\n\nWe should use the default StandardTokenizer and modify the filter to use the newly added types. ",
            "author": "Uwe Schindler",
            "id": "comment-13857097"
        },
        {
            "date": "2013-12-26T21:52:10+0000",
            "content": "Uwe Schindler Thank you for your comment, Uwe. \nI think I can make some improvements with above the problem like WordSpaceAnalyzer, posinc/offset and KoreanTokenizer. I'll upload the patch by this weekend. ",
            "author": "SooMyung Lee",
            "id": "comment-13857107"
        },
        {
            "date": "2014-01-12T08:45:41+0000",
            "content": "Uwe Schindler I'm trying to change the code to use StandardTokenizer but I found a problem. when a text with Chinese characters is passed into the StandardTokenizer, It tokenizes Chinese characters into each character. That makes it difficult to extract index keywords and map Chinese character to Hangul Character. So, to use StandardTokenizer for KoreanAnalyzer, consecutive Chinese characters should not be tokenized. \nCan you change the StandardTokenizer as I mentioned? ",
            "author": "SooMyung Lee",
            "id": "comment-13868971"
        },
        {
            "date": "2014-02-08T19:27:48+0000",
            "content": "Dear Lucene Korean Team,\n\nI posted the following at sourceforge too.  Thank you for your time.  Would appreciate any inputs or assistance you can provide.\n\nRespectfully,\nDeqi\n\nDear Lucene Korean Team,\nHi, I'm a translator working with OmegaT and the OmegaT developers (see Yahoo! OmegaT group). Thank you all very much for the hard work you've put into this analyzer. I was so excited when I came across it!\nAs a result, I asked the OmegaT developers if they could include your Korean analyzer into OmegaT and they did. The unfortunate part is that the analyzer does not appear to be working. See the e-mails pasted below for more information.\nAnd I would respectfully like to ask a few questions. Would you happen to know why this is happening? If there's a problem, do you know if it will be fixed in future releases? Finally, may I ask how this analyzer and the one here are related: https://issues.apache.org/jira/browse/LUCENE-4956\nThank you all in advance for your time.\nRespectfully,\nDeqi\nDear Colleagues,\nRE: http://groups.yahoo.com/neo/groups/OmegaT/conversations/messages/20023\nI'm interested in adding a Korean-specific analyzer/tokenizer to OmT 3.0.8 because of the simplicity of the CJK tokenizer described in the RE. To that end, I downloaded KoreanAnalyzer-20100302.jar and, since I'm using a Mac, put in the .app lib folder and updated the Info.plist file to point to the new jar file.\nDoes anyone else know what needs to be done? How do I make OmT aware of the new analyzer and use it by default? I'd be very grateful for any assistance and apologize in advance if I don't know the difference between an analyzer and a tokenizer.\nFor those working in Korean, there's another apparently related analyzer, but I have no idea of how to work with it:\nhttps://issues.apache.org/jira/browse/LUCENE-4956\nV/R,\nDai Deqi\nHi Aaron,\nGood news and bad news. I built OmT with the new Korean analyzer that you so graciously added with no problems at all. However, the new Korean-only analyzer doesn't appear to be working as well as the CJK analyzer. I'm assuming analyzer/tokenizer differences will show up most noticeably in the Glossary pane. And that's where I'm seeing big differences.\nFor example, the simple sentence below\n\uadf8 \uc804\ubb38\uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4.\nproduces Transtips and Glossary hits using the CJK analyzer, but nothing with the new Korean-only analyzer. That was quite disappointing.\nIf there are any other tests you or anyone else can suggest or would like me to try, please let me know. I've never done this kind of testing before.\nAll the Best,\nDai Deqi\nHello.\nI just did a quick test of the KoreanAnalyzer lib and found that while the tokenizer seems to work fine, the analyzer part (which is used for glossary and Transtips, etc.) doesn't seem to work at all.\nInput: \"\uadf8 \uc804\ubb38\uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4.\"\nTokenizer output: [ \"\uadf8\", \"\uc804\ubb38\uc740\", \"\ub2e4\uc74c\uacfc\", \"\uac19\ub2e4\" ]\nAnalyzer output: [ ]\nIn other words the analyzer simply does not output anything, which means that no matches will be found.\nI'm not sure what to make of this, as we are using the library in the same way as any other Lucene analyzer. This suggests to me that the code is broken; if there's some workaround then perhaps the author of the library can help us, but otherwise we will just have to wait until the standalone library is fixed or a final version is integrated into Lucene.\n-Aaron\nActually, sorry, I was wrong; the analyzer's output is empty for the example sentence you supplied, but that is not true for the general case.\nFor a sentence I took from Wikipedia:\nInput: \"\uc704\ud0a4\ubc31\uacfc\ub294 \uc804 \uc138\uacc4 \uc5ec\ub7ec \uc5b8\uc5b4\ub85c \ub9cc\ub4e4\uc5b4 \ub098\uac00\ub294 \uc790\uc720 \ubc31\uacfc\uc0ac\uc804\uc73c\ub85c, \ub204\uad6c\ub098 \ucc38\uc5ec\ud558\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\"\nTokenization: [ \"\uc704\ud0a4\ubc31\uacfc\ub294\", \"\uc804\", \"\uc138\uacc4\", \"\uc5ec\ub7ec\", \"\uc5b8\uc5b4\ub85c\", \"\ub9cc\ub4e4\uc5b4\", \"\ub098\uac00\ub294\", \"\uc790\uc720\", \"\ubc31\uacfc\uc0ac\uc804\uc73c\ub85c\", \"\ub204\uad6c\ub098\", \"\ucc38\uc5ec\ud558\uc2e4\", \"\uc218\", \"\uc788\uc2b5\ub2c8\ub2e4\" ]\nAnalysis: [ \"\uc704\ud0a4\ubc31\uacfc\ub294\", \"\uc704\ud0a4\ubc31\", \"\uc704\ud0a4\", \"\ud0a4\ubc31\" ]\nI thought at first this was the result of a very aggressive stopwords filter or something, but the result is the same even when supplying an empty stopwords set. Plus, Google Translate tells me that the analysis result is basically:\n[ \"Wikipedia\", \"Wikipedia\", \"Wiki\", \"pedia\" ] (all substrings of the first token)\nSo it seems the conclusion is the same: The analysis is broken, or at least behaves completely differently from all standard Lucene analyzers.\n-Aaron ",
            "author": "Dai Deqi",
            "id": "comment-13895699"
        },
        {
            "date": "2014-02-08T19:39:47+0000",
            "content": "This is a patch, not an accepted component of Apache Lucene. There's no guarantee that anyone will work on it. ",
            "author": "Benson Margulies",
            "id": "comment-13895706"
        },
        {
            "date": "2014-02-08T22:34:31+0000",
            "content": "Hi Dai Deqi,\nI created SourceForge project and contribute it to Apache Lucene project.\nI'm working on fixing some problems mentioned in this Jira issue. \nBut the problem you mentioned about \"\uc704\ud0a4\ubc31\uacfc\ub294\" is to be solved if you add the word \"\uc704\ud0a4\" into the dictionary.\nThere is no perfect way to analyze Korean sentences according to only a algorithm such as Porter Stemming in English sentence. So, we use both algorithm and dictionary to analyze Korean sentence. The dictionary for the Korean analyzer has around 40,000 words. I think most of the basic Korean word is included in it. But about many loan words such as \"\uc704\ud0a4\" is not included. Usually the user of Korean Analyzer in Korea builds his own dictionary for his purpose. ",
            "author": "SooMyung Lee",
            "id": "comment-13895754"
        },
        {
            "date": "2014-02-08T23:24:50+0000",
            "content": "Hello. I am involved with the OmegaT project; I am the \"Aaron\" referenced in Dai Deqi's quoted emails.\n\nWe are using the version of this tokenizer that is hosted on SourceForge. Our issue should not have been brought up here at all. I apologize for the intrusion.\n\nI will follow up with SooMyung Lee privately. ",
            "author": "Aaron Madlon-Kay",
            "id": "comment-13895773"
        },
        {
            "date": "2014-02-09T02:35:19+0000",
            "content": "Dear Aaron and All,\n\nI humbly apologize if I breached an unspoken internet/project protocol.\u00a0 Such was not my intention.\u00a0 In fact, I thought I was helping.\u00a0 In any event, I'll be much more careful in the future.\n\nVery Respectfully,\nDai Deqi\n\n\n\n\nOn Saturday, February 8, 2014 6:50 PM, Aaron Madlon-Kay (JIRA) <jira@apache.org> wrote:\n\n\n\u00a0 \u00a0 [ https://issues.apache.org/jira/browse/LUCENE-4956?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13895773#comment-13895773] \n\nAaron Madlon-Kay edited comment on LUCENE-4956 at 2/8/14 11:43 PM:\n-------------------------------------------------------------------\n\nHello. I am involved with the OmegaT project; I am the \"Aaron\" referenced in Dai Deqi's quoted emails.\n\nWe are using the version of this tokenizer that is hosted on SourceForge. Our issue should not have been brought up here at all. I apologize for the intrusion.\n\nI will follow up with SooMyung Lee privately.\n\n\nwas (Author: amake):\nHello. I am involved with the OmegaT project; I am the \"Aaron\" referenced in Dai Deqi's quoted emails.\n\nWe are using the version of this tokenizer that is hosted on SourceForge. This issue should not have been brought up here at all. I apologize for the intrusion.\n\nI will follow up with SooMyung Lee privately.\n\n\n\n\n\u2013\nThis message was sent by Atlassian JIRA\n(v6.1.5#6160) ",
            "author": "Dai Deqi",
            "id": "comment-13895808"
        }
    ]
}