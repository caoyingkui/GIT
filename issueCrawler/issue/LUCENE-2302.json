{
    "id": "LUCENE-2302",
    "title": "Replacement for TermAttribute+Impl with extended capabilities (byte[] support, CharSequence, Appendable)",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "affect_versions": "4.0-ALPHA",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "For flexible indexing terms can be simple byte[] arrays, while the current TermAttribute only supports char[]. This is fine for plain text, but e.g NumericTokenStream should directly work on the byte[] array.\nAlso TermAttribute lacks of some interfaces that would make it simplier for users to work with them: Appendable and CharSequence\n\nI propose to create a new interface \"CharTermAttribute\" with a clean new API that concentrates on CharSequence and Appendable.\nThe implementation class will simply support the old and new interface working on the same term buffer. DEFAULT_ATTRIBUTE_FACTORY will take care of this. So if somebody adds a TermAttribute, he will get an implementation class that can be also used as CharTermAttribute. As both attributes create the same impl instance both calls to addAttribute are equal. So a TokenFilter that adds CharTermAttribute to the source will work with the same instance as the Tokenizer that requested the (deprecated) TermAttribute.\n\nTo also support byte[] only terms like Collation or NumericField needs, a separate getter-only interface will be added, that returns a reusable BytesRef, e.g. BytesRefGetterAttribute. The default implementation class will also support this interface. For backwards compatibility with old self-made-TermAttribute implementations, the indexer will check with hasAttribute(), if the BytesRef getter interface is there and if not will wrap a old-style TermAttribute (a deprecated wrapper class will be provided): new BytesRefGetterAttributeWrapper(TermAttribute), that is used by the indexer then.",
    "attachments": {
        "LUCENE-2302.patch": "https://issues.apache.org/jira/secure/attachment/12438192/LUCENE-2302.patch",
        "LUCENE-2302-toString.patch": "https://issues.apache.org/jira/secure/attachment/12441289/LUCENE-2302-toString.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-03-07T23:01:46+0000",
            "content": "A CollationFilter will not be needed anymore after that change, as any Tokenizer-Chain that wants to use collation can simply supply a special AttributeFactory to the ctor, that creates a special TermAttributeImpl class with modified getBytesRef(). ",
            "author": "Uwe Schindler",
            "id": "comment-12842499"
        },
        {
            "date": "2010-03-07T23:13:50+0000",
            "content": "Hmm maybe this is too much magic? Wouldn't it be simpler to have two completely separate attributes? E.g. CharTermAttribute and ByteTermAttribute. Plus an API in the indexer that specifies which one to use?  ",
            "author": "Michael Busch",
            "id": "comment-12842502"
        },
        {
            "date": "2010-03-07T23:31:46+0000",
            "content": "A CollationFilter will not be needed anymore after that change, as any Tokenizer-Chain that wants to use collation can simply supply a special AttributeFactory to the ctor, that creates a special TermAttributeImpl class with modified getBytesRef(). \n\nMike M. noted on LUCENE-1435 that the way to do \"internal-to-indexing\" collation is to store the original string in the term dictionary, sorted via user-specifiable collation. ",
            "author": "Steve Rowe",
            "id": "comment-12842505"
        },
        {
            "date": "2010-03-08T00:10:48+0000",
            "content": "I like that this change would mean indexer has a single getBytes interface for getting the terms data.\n\nIt'd mean the UTF16->UTF8 encoding it now does would move into CharTermAttr, hidden to the indexer.\n\nSo the indexer only ever works with opaque byte[] data for terms.\n\nAnd it'd mean others can make their own term attrs \u2013 maybe my terms are shorts and I send 2 bytes to indexer per term. ",
            "author": "Michael McCandless",
            "id": "comment-12842509"
        },
        {
            "date": "2010-03-08T01:36:56+0000",
            "content": "\nA CollationFilter will not be needed anymore after that change, as any Tokenizer-Chain that wants to use collation can simply supply a special AttributeFactory to the ctor, that creates a special TermAttributeImpl class with modified getBytesRef().\n\nMike M. noted on LUCENE-1435 that the way to do \"internal-to-indexing\" collation is to store the original string in the term dictionary, sorted via user-specifiable collation.\n\nThis works in flex now \u2013 every field can specify its own Comparator (via the Codec).\n\nBut collation during indexing also works... but'd work better w/ these changes (not have to use indexable binary strings). ",
            "author": "Michael McCandless",
            "id": "comment-12842516"
        },
        {
            "date": "2010-03-08T15:08:00+0000",
            "content": "Here a first patch.\n\nTo discuss:\n\n\tNames of classes/interfaces\n\tAPI finalization\n\n\n\nThe following applies to the patch:\n\n\tAll tests pass\n\tYou can use old and new TermAttribute in the same TokenStream!\n\tIf somebody implemented an own TermAttributeImpl (old style), the TermsHashPerField wraps the old attribute using a helper class (per thread). \u2013 Test still missing, but worked by commenting out some code that used the new api.\n\n\n\nCurrently backwards tests fail because of some checks that are not valid anymore (toString() output of TermAttribute and TestAttributeSource instanceof checks). ",
            "author": "Uwe Schindler",
            "id": "comment-12842687"
        },
        {
            "date": "2010-03-08T16:35:20+0000",
            "content": "New patch, updated for trunk:\n\n\tCharTermAttributeImpl.subSequence uses now new String, instead CharBuffer.wrap, as it must be a  \"new sequence\", no wrapped\n\tOptimized append() to use CharBuffer's array, else just iterates over sequence.\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-12842725"
        },
        {
            "date": "2010-03-08T16:49:47+0000",
            "content": "Again an update, removed the whole lazy init stuff; so initTermBuffer() and additonal checks removed. ",
            "author": "Uwe Schindler",
            "id": "comment-12842730"
        },
        {
            "date": "2010-03-08T17:20:16+0000",
            "content": "Removed usage of deprecated API in Token, also javadocs updated. ",
            "author": "Uwe Schindler",
            "id": "comment-12842740"
        },
        {
            "date": "2010-03-08T17:50:33+0000",
            "content": "Patch looks great Uwe!  Great simplification that indexer only deals\nin byte[] now for a term.  It's agnostic as to whether those bytes are\nutf8 or something else.  And it means analyzer chains can now do\ndirect binary terms (eg NumericTokenStream).\n\nSome day... we should also try to have indexer not be responsible for\ncreating the TokenStream.  Ie it should simply receive, always, an\nAttrSource for a field that needs to be indexed.  This puts nice\ndistance b/w indexer core and analysis \u2013 indexer is then fully\nagnostic to how that AttrSource came to be.\n\nI see \"noncommit\" \u2013 can you rename to \"nocommit\" \u2013 let's try to be\nconsistent \n\nMaybe rewword \"The given AttributeSource has no term attribute\" -->\n\"Could not find a term attribute (that implements\nTermToBytesAttribute) in the AttributeSource\"?\n\nI think we should rename TermsHashPerField's utf8 var (and in the per\nthread) \u2013 it's now just bytes, not necessarily utf8.  Maybe termBytes?\n\nWhen you temporarily override the length of a too-long term, maybe\nrestore it in a try/finally? ",
            "author": "Michael McCandless",
            "id": "comment-12842753"
        },
        {
            "date": "2010-03-18T11:20:22+0000",
            "content": "Updated patch for current flex HEAD. Still backwards needs to be fixed.\n\nHow do we want to preceed?:\n\n\tName of new Attrubute?\n\tIs new CharSeq/Appendable API fine\n\tsetEmpty()?\n\n\n\nThanks for reviewing! ",
            "author": "Uwe Schindler",
            "id": "comment-12846850"
        },
        {
            "date": "2010-03-18T12:34:44+0000",
            "content": "I like the name CharTermAttribute.\n\nHow about instead of TermToBytesRefAttribute we name it TermBytesAttribute?  (Ie, drop the \"To\" and \"Ref\"). ",
            "author": "Michael McCandless",
            "id": "comment-12846877"
        },
        {
            "date": "2010-03-18T14:35:29+0000",
            "content": "How about instead of TermToBytesRefAttribute we name it TermBytesAttribute? (Ie, drop the \"To\" and \"Ref\").\n\nThis attribute is special, it only has this getter for the bytesref.\n\nIf we need a real \"BytesTermAttribute\" it should be explicitely defined. Now open is NumericTokenStream and so on ",
            "author": "Uwe Schindler",
            "id": "comment-12846933"
        },
        {
            "date": "2010-03-18T14:36:16+0000",
            "content": "Was accidently committed with merge. Sorry.\n\nRevision: 924791 ",
            "author": "Uwe Schindler",
            "id": "comment-12846934"
        },
        {
            "date": "2010-03-18T15:15:09+0000",
            "content": "Add note to backwards compatibility section:\n\n\tTermAttribute now changed toString() behaviour\n\tToken now implemnts CharSequence but violates its contract\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-12846952"
        },
        {
            "date": "2010-03-30T10:51:16+0000",
            "content": "Uwe is this issue done? ",
            "author": "Michael McCandless",
            "id": "comment-12851346"
        },
        {
            "date": "2010-03-30T21:14:31+0000",
            "content": "Will add the javadocs and think about the CharSequence problems again. It's tricky \n\nI have less time at the moment, will do hopefully until the weekend. The same for LUCENE-2354, which needs some test rewriting. ",
            "author": "Uwe Schindler",
            "id": "comment-12851596"
        },
        {
            "date": "2010-04-06T12:28:22+0000",
            "content": "Token now implemnts CharSequence but violates its contract\n\nI don't think this is correct. I don't care at all about Token's .toString method, but i care that the analysis api isn't broken.\nif we do this, then the analysis API is completely wrong when using a Token Attribute Factory.\nIn my opinion we should do one of the following two things in the backwards compatibility section, but not break the analysis API:\n\n\n\tToken and TokenAttributeFactory was completely removed due to its backwards compatibility problems.\n\tToken's toString method was changed to match the CharSequence interface.\n\n ",
            "author": "Robert Muir",
            "id": "comment-12853905"
        },
        {
            "date": "2010-04-06T21:42:11+0000",
            "content": "I will create a patch with option #2 and lots of documentation and changed backwards tests. ",
            "author": "Uwe Schindler",
            "id": "comment-12854199"
        },
        {
            "date": "2010-04-06T21:48:03+0000",
            "content": "I will create a patch with option #2 and lots of documentation and changed backwards tests.\n\n+1. what do we think of deprecating it too? \n\nI complained about supporting an old \"token-like\" api easily a while ago, but you added AttributeSource.copyTo,\nwhich works well (see Solr SynonymsFilter) and does not have Token's limitations. ",
            "author": "Robert Muir",
            "id": "comment-12854209"
        },
        {
            "date": "2010-04-09T11:39:06+0000",
            "content": "Patch that fixes the toString() problems in Token and adds missing CHANGES.txt, fixes backwards tests and updates javadocs to document the \"backwards\" break.\n\nDeprecating Token should be done in another issue.\n\nI will commit this soon, to be able to go forward with tokenstream conversion! ",
            "author": "Uwe Schindler",
            "id": "comment-12855355"
        },
        {
            "date": "2010-04-09T11:50:39+0000",
            "content": "Committed revision: 932369 ",
            "author": "Uwe Schindler",
            "id": "comment-12855357"
        }
    ]
}