{
    "id": "SOLR-7215",
    "title": "non reproducible Suite failures due to excessive sysout due to HDFS lease renewal WARN logs due to connection refused -- even if test doesn't use HDFS (ie: threads leaking between tests)",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "On my local machine, i've noticed lately a lot of sporadic, non reproducible, failures like these...\n\n\n  2> NOTE: reproduce with: ant test  -Dtestcase=ScriptEngineTest -Dtests.seed=E254A7E69EC7212A -Dtests.slow=true -Dtests.locale=sv -Dtests.timezone=SystemV/CST6 -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n[14:34:23.749] ERROR   0.00s J1 | ScriptEngineTest (suite) <<<\n   > Throwable #1: java.lang.AssertionError: The test or suite printed 10984 bytes to stdout and stderr, even though the limit was set to 8192 bytes. Increase the limit with @Limit, ignore it completely with @SuppressSysoutChecks or run with -Dtests.verbose=true\n   > \tat __randomizedtesting.SeedInfo.seed([E254A7E69EC7212A]:0)\n   > \tat org.apache.lucene.util.TestRuleLimitSysouts.afterIfSuccessful(TestRuleLimitSysouts.java:212)\n\n\n\nInvariably, looking at the logs of test that fail for this reason, i see multiple instances of these WARN msgs...\n\n\n  2> 601361 T3064 oahh.LeaseRenewer.run WARN Failed to renew lease for [DFSClient_NONMAPREDUCE_-253604438_2947] for 92 seconds.  Will retry shortly ... java.net.ConnectException: Call From frisbee/127.0.1.1 to localhost:40618 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n  2> \tat sun.reflect.GeneratedConstructorAccessor268.newInstance(Unknown Source)\n  2> \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n ...\n\n\n\n...the full stack traces of these exceptions typically being 36 lines long (not counting the supressed \"... 17 more\" at the end)\n\ndoing some basic crunching of the \"tests-report.txt\" file from a recent run of all \"solr-core\" tests (that caused the above failure) leads to some pretty damn disconcerting numbers...\n\n\nhossman@frisbee:~/tmp$ wc -l tests-report.txt_suite-failure-due-to-sysout.txt\n1049177 tests-report.txt_suite-failure-due-to-sysout.txt\nhossman@frisbee:~/tmp$ grep \"Suite: org.apache.solr\" tests-report.txt_suite-failure-due-to-sysout.txt | wc -l\n465\nhossman@frisbee:~/tmp$ grep \"LeaseRenewer.run WARN Failed to renew lease\" tests-report.txt_suite-failure-due-to-sysout.txt | grep http://wiki.apache.org/hadoop/ConnectionRefused | wc -l\n1988\nhossman@frisbee:~/tmp$ calc\n1988 * 36\n71568\n\n\n\nSo running 465 Solr test suites, we got ~2 thousand of these \"Failed to renew lease\" WARNings.  Of the ~1 million total lines of log messages from all tests, ~70 thousand (~7%) are coming from these WARNing mesages \u2013 which can evidently be safetly ignored?\n\n\nSomething seems broken here.\n\nSomeone who understands this area of the code should either:\n\n\n\tinvestigate & fix the code/test not to have these lease renewal problems\n\ttweak our test logging configs to supress these WARN messages",
    "attachments": {
        "tests-report.txt_suite-failure-due-to-sysout.txt.zip": "https://issues.apache.org/jira/secure/attachment/12703527/tests-report.txt_suite-failure-due-to-sysout.txt.zip"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-03-09T23:03:40+0000",
            "author": "Hoss Man",
            "content": "ZIPed up copy of the tests-report.txt_suite-failure-due-to-sysout.txt i mentioned. ",
            "id": "comment-14353824"
        },
        {
            "date": "2015-03-10T08:03:13+0000",
            "author": "Dawid Weiss",
            "content": "I'm glad this test rule proved to be not just an annoyance but is useful for something  ",
            "id": "comment-14354496"
        },
        {
            "date": "2015-03-12T18:27:34+0000",
            "author": "Hoss Man",
            "content": "This is more jacked up then i thought \u2013 i just got one of the Suite failures from \"TestDocSet\" which directly extends LuceneTestCase and doesn't do ANY initialization of any Solr specific functionality (no CoreContainers, no SolrCores, no ZooKeeper)\n\nwhich means not only are these HDFS Client \"ConnectExceptions\" causing test failures due to too much logging \u2013 these threads appear to be leaking from the test suites and affecting other tests run in the same JVM EVEN WHEN WHATEVER TEST CREATED THESE THREADS PASSES ... The only failure i got was from TestDocSet. and yet it failed because of excessive logging from a thread created by some other test that had already passed.\n\n\nhossman@frisbee:~/lucene/dev/solr$ ant test\n...\n   [junit4] Suite: org.apache.solr.search.TestDocSet\n   [junit4]   2> 1460665 T5379 oahh.LeaseRenewer.run WARN Failed to renew lease for [DFSClient_NONMAPREDUCE_1277984620_5262] for 402 seconds.  Will retry shortly ... java.net.ConnectException: Call From frisbee/127.0.1.1 to localhost:47570 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n   [junit4]   2> \tat sun.reflect.GeneratedConstructorAccessor303.newInstance(Unknown Source)\n   [junit4]   2> \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n   [junit4]   2> \tat java.lang.reflect.Constructor.newInstance(Constructor.java:408)\n   [junit4]   2> \tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)\n   [junit4]   2> \tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)\n   [junit4]   2> \tat org.apache.hadoop.ipc.Client.call(Client.java:1410)\n   [junit4]   2> \tat org.apache.hadoop.ipc.Client.call(Client.java:1359)\n   [junit4]   2> \tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n   [junit4]   2> \tat com.sun.proxy.$Proxy43.renewLease(Unknown Source)\n   [junit4]   2> \tat sun.reflect.GeneratedMethodAccessor60.invoke(Unknown Source)\n   [junit4]   2> \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n   [junit4]   2> \tat java.lang.reflect.Method.invoke(Method.java:483)\n   [junit4]   2> \tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)\n   [junit4]   2> \tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n   [junit4]   2> \tat com.sun.proxy.$Proxy43.renewLease(Unknown Source)\n   [junit4]   2> \tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:519)\n   [junit4]   2> \tat org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:773)\n   [junit4]   2> \tat org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)\n   [junit4]   2> \tat org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)\n   [junit4]   2> \tat org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)\n   [junit4]   2> \tat org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)\n   [junit4]   2> \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]   2> Caused by: java.net.ConnectException: Connection refused\n   [junit4]   2> \tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n   [junit4]   2> \tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n   [junit4]   2> \tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n   [junit4]   2> \tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)\n   [junit4]   2> \tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)\n   [junit4]   2> \tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:601)\n   [junit4]   2> \tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:696)\n   [junit4]   2> \tat org.apache.hadoop.ipc.Client$Connection.access$2700(Client.java:367)\n   [junit4]   2> \tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1458)\n   [junit4]   2> \tat org.apache.hadoop.ipc.Client.call(Client.java:1377)\n   [junit4]   2> \t... 16 more\n   [junit4]   2> \n   [junit4]   2> 1460924 T8206 oahh.LeaseRenewer.run WARN Failed to renew lease for [DFSClient_NONMAPREDUCE_602751345_8088] for 91 seconds.  Will retry shortly ... java.net.ConnectException: Call From frisbee/127.0.1.1 to localhost:47687 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n   [junit4]   2> \tat sun.reflect.GeneratedConstructorAccessor303.newInstance(Unknown Source)\n   [junit4]   2> \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n   [junit4]   2> \tat java.lang.reflect.Constructor.newInstance(Constructor.java:408)\n   [junit4]   2> \tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)\n   [junit4]   2> \tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)\n   [junit4]   2> \tat org.apache.hadoop.ipc.Client.call(Client.java:1410)\n   [junit4]   2> \tat org.apache.hadoop.ipc.Client.call(Client.java:1359)\n   [junit4]   2> \tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)\n   [junit4]   2> \tat com.sun.proxy.$Proxy43.renewLease(Unknown Source)\n   [junit4]   2> \tat sun.reflect.GeneratedMethodAccessor60.invoke(Unknown Source)\n   [junit4]   2> \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n   [junit4]   2> \tat java.lang.reflect.Method.invoke(Method.java:483)\n   [junit4]   2> \tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)\n   [junit4]   2> \tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n   [junit4]   2> \tat com.sun.proxy.$Proxy43.renewLease(Unknown Source)\n   [junit4]   2> \tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewLease(ClientNamenodeProtocolTranslatorPB.java:519)\n   [junit4]   2> \tat org.apache.hadoop.hdfs.DFSClient.renewLease(DFSClient.java:773)\n   [junit4]   2> \tat org.apache.hadoop.hdfs.LeaseRenewer.renew(LeaseRenewer.java:417)\n   [junit4]   2> \tat org.apache.hadoop.hdfs.LeaseRenewer.run(LeaseRenewer.java:442)\n   [junit4]   2> \tat org.apache.hadoop.hdfs.LeaseRenewer.access$700(LeaseRenewer.java:71)\n   [junit4]   2> \tat org.apache.hadoop.hdfs.LeaseRenewer$1.run(LeaseRenewer.java:298)\n   [junit4]   2> \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]   2> Caused by: java.net.ConnectException: Connection refused\n   [junit4]   2> \tat sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n   [junit4]   2> \tat sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\n   [junit4]   2> \tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\n   [junit4]   2> \tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)\n   [junit4]   2> \tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)\n   [junit4]   2> \tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:601)\n   [junit4]   2> \tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:696)\n   [junit4]   2> \tat org.apache.hadoop.ipc.Client$Connection.access$2700(Client.java:367)\n   [junit4]   2> \tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1458)\n   [junit4]   2> \tat org.apache.hadoop.ipc.Client.call(Client.java:1377)\n   [junit4]   2> \t... 16 more\n   [junit4]   2> \n   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene50): {}, docValues:{}, sim=DefaultSimilarity, locale=es_CU, timezone=Asia/Dacca\n   [junit4]   2> NOTE: Linux 3.2.0-76-generic amd64/Oracle Corporation 1.8.0_31 (64-bit)/cpus=4,threads=1,free=75167392,total=524812288\n   [junit4]   2> NOTE: All tests run in this JVM: [TestShortCircuitedRequests, TestPHPSerializedResponseWriter, TestBinaryResponseWriter, ReplicationFactorTest, TestMiniSolrCloudCluster, TestManagedResource, BufferStoreTest, DistributedMLTComponentTest, TestExceedMaxTermLength, TestSchemaVersionResource, TestSimpleQParserPlugin, DirectUpdateHandlerTest, TestWriterPerf, DirectUpdateHandlerOptimizeTest, TestRTGBase, DistanceFunctionTest, EnumFieldTest, TestTrackingShardHandlerFactory, OverseerRolesTest, TestUtils, AnalysisErrorHandlingTest, SolrPluginUtilsTest, ShardSplitTest, TestDynamicLoading, ClusterStateTest, SOLR749Test, VMParamsZkACLAndCredentialsProvidersTest, AnalyticsMergeStrategyTest, TestJmxMonitoredMap, TestFaceting, TestAddFieldRealTimeGet, TestSchemaNameResource, TestOrdValues, DistributedFacetPivotSmallAdvancedTest, CopyFieldTest, BasicDistributedZkTest, AssignTest, TestRandomMergePolicy, DisMaxRequestHandlerTest, TestRequestStatusCollectionAPI, RecoveryAfterSoftCommitTest, PreAnalyzedUpdateProcessorTest, TestLuceneMatchVersion, SynonymTokenizerTest, RankQueryTest, DocValuesMultiTest, TestClassNameShortening, SolrInfoMBeanTest, CloudMLTQParserTest, TestManagedStopFilterFactory, StatelessScriptUpdateProcessorFactoryTest, TestSolrXml, DistributedFacetPivotSmallTest, TestLFUCache, TestNonDefinedSimilarityFactory, BadIndexSchemaTest, TestFiltering, TestCopyFieldCollectionResource, AlternateDirectoryTest, TestHashPartitioner, RollingRestartTest, TestRebalanceLeaders, DocValuesTest, TestStressVersions, TestSchemaSimilarityResource, TestSolr4Spatial, CircularListTest, TestCollationField, HdfsBasicDistributedZk2Test, TestFieldCollectionResource, AutoCommitTest, SuggestComponentTest, DistanceUnitsTest, TestSearchPerf, StandardRequestHandlerTest, TestComplexPhraseQParserPlugin, TestDefaultStatsCache, TestDistributedSearch, MultiThreadedOCPTest, TestCloudSchemaless, UnloadDistributedZkTest, StressHdfsTest, TestSolrConfigHandlerCloud, CloudExitableDirectoryReaderTest, SaslZkACLProviderTest, TestCryptoKeys, TestCloudManagedSchema, TriLevelCompositeIdRoutingTest, SpellCheckComponentTest, DistributedIntervalFacetingTest, DistributedFacetPivotLargeTest, TestRandomDVFaceting, TestStressRecovery, ExitableDirectoryReaderTest, DistributedTermsComponentTest, TestExactSharedStatsCache, HdfsDirectoryFactoryTest, DistributedFacetPivotWhiteBoxTest, TestHighlightDedupGrouping, DistributedQueryComponentCustomSortTest, ZkCLITest, TestStressUserVersions, SolrCmdDistributorTest, TestBadConfig, DistributedQueryElevationComponentTest, SolrIndexSplitterTest, FacetPivotSmallTest, LukeRequestHandlerTest, TestAnalyzedSuggestions, AddSchemaFieldsUpdateProcessorFactoryTest, TestExtendedDismaxParser, TestDynamicFieldCollectionResource, TestCSVLoader, TestStressLucene, TestFreeTextSuggestions, DateRangeFieldTest, DocumentBuilderTest, ZkStateWriterTest, JsonLoaderTest, TestCollapseQParserPlugin, TestQueryTypes, TestCoreDiscovery, TermVectorComponentTest, CacheHeaderTest, TestOmitPositions, TestFieldTypeResource, TestDynamicFieldResource, TestExpandComponent, TestJmxIntegration, ReturnFieldsTest, FieldAnalysisRequestHandlerTest, InfoHandlerTest, SegmentsInfoRequestHandlerTest, RequiredFieldsTest, DateFieldTest, NotRequiredUniqueKeyTest, SchemaVersionSpecificBehaviorTest, DefaultValueUpdateProcessorTest, TestHashQParserPlugin, TestDocSet]\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestDocSet -Dtests.seed=31CA503F5F477364 -Dtests.slow=true -Dtests.locale=es_CU -Dtests.timezone=Asia/Dacca -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] ERROR   0.00s J1 | TestDocSet (suite) <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: The test or suite printed 10988 bytes to stdout and stderr, even though the limit was set to 8192 bytes. Increase the limit with @Limit, ignore it completely with @SuppressSysoutChecks or run with -Dtests.verbose=true\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([31CA503F5F477364]:0)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4] Completed on J1 in 0.67s, 2 tests, 1 failure <<< FAILURES!\n...\n   [junit4] \n   [junit4] Tests with failures:\n   [junit4]   - org.apache.solr.search.TestDocSet (suite)\n   [junit4] \n   [junit4] \n   [junit4] JVM J0:     1.03 ..  1470.41 =  1469.38s\n   [junit4] JVM J1:     0.84 ..  1470.20 =  1469.36s\n   [junit4] JVM J2:     1.02 ..  1470.36 =  1469.33s\n   [junit4] Execution time total: 24 minutes 30 seconds\n   [junit4] Tests summary: 473 suites, 1882 tests, 1 suite-level error, 31 ignored (21 assumptions)\n\n ",
            "id": "comment-14359123"
        },
        {
            "date": "2015-03-12T19:34:47+0000",
            "author": "Dawid Weiss",
            "content": "Uncomment the ThreadLeakFilters, Hoss. Nothing should get through. SolrIgnoredThreadsFilter has way too many exclusions \u2013 these have to be shut down and cleaned properly, not ignored (leading to errors like this one):\n\n    /*\n     * IMPORTANT! IMPORTANT!\n     * \n     * Any threads added here should have ABSOLUTELY NO SIDE EFFECTS\n     * (should be stateless). This includes no references to cores or other\n     * test-dependent information.\n     */\n\n    String threadName = t.getName();\n    if (threadName.equals(TimerThread.THREAD_NAME)) {\n      return true;\n    }\n\n    if (threadName.startsWith(\"facetExecutor-\") || \n        threadName.startsWith(\"cmdDistribExecutor-\") ||\n        threadName.startsWith(\"httpShardExecutor-\")) {\n      return true;\n    }\n    \n    // This is a bug in ZooKeeper where they call System.exit(11) when\n    // this thread receives an interrupt signal.\n    if (threadName.startsWith(\"SyncThread\")) {\n      return true;\n    }\n\n    // THESE ARE LIKELY BUGS - these threads should be closed!\n    if (threadName.startsWith(\"Overseer-\") ||\n        threadName.startsWith(\"aliveCheckExecutor-\") ||\n        threadName.startsWith(\"concurrentUpdateScheduler-\")) {\n      return true;\n    }\n\n    return false;\n\n ",
            "id": "comment-14359227"
        }
    ]
}