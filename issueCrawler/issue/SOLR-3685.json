{
    "id": "SOLR-3685",
    "title": "Solr Cloud sometimes skipped peersync attempt and replicated instead due to tlog flags not being cleared when no updates were buffered during a previous replication.",
    "details": {
        "affect_versions": "4.0-ALPHA",
        "status": "Closed",
        "fix_versions": [
            "4.0",
            "6.0"
        ],
        "components": [
            "replication (java)",
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Critical",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "There's a serious problem with restarting nodes, not cleaning old or unused index directories and sudden replication and Java being killed by the OS due to excessive memory allocation. Since SOLR-1781 was fixed index directories get cleaned up when a node is being restarted cleanly, however, old or unused index directories still pile up if Solr crashes or is being killed by the OS, happening here.\n\nWe have a six-node 64-bit Linux test cluster with each node having two shards. There's 512MB RAM available and no swap. Each index is roughly 27MB so about 50MB per node, this fits easily and works fine. However, if a node is being restarted, Solr will consistently crash because it immediately eats up all RAM. If swap is enabled Solr will eat an additional few 100MB's right after start up.\n\nThis cannot be solved by restarting Solr, it will just crash again and leave index directories in place until the disk is full. The only way i can restart a node safely is to delete the index directories and have it replicate from another node. If i then restart the node it will crash almost consistently.\n\nI'll attach a log of one of the nodes.",
    "attachments": {
        "oom-killer.log": "https://issues.apache.org/jira/secure/attachment/12541260/oom-killer.log",
        "info.log": "https://issues.apache.org/jira/secure/attachment/12538147/info.log",
        "pmap.log": "https://issues.apache.org/jira/secure/attachment/12541345/pmap.log"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Markus Jelsma",
            "id": "comment-13423781",
            "date": "2012-07-27T10:33:29+0000",
            "content": "Here's a log for a node where the Java process is being killed by the OS. I can reproduce this consistently. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13423782",
            "date": "2012-07-27T10:38:33+0000",
            "content": "I forgot to add that it doesn't matter if updates are sent to the cluster. A node will start to replicate on startup when it's update to date as well and crash subsequently. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13423785",
            "date": "2012-07-27T10:42:54+0000",
            "content": "How much heap do you assign to Solr's Java process (-Xmx)? 512 MB physical RAM is very few. The Jetty default is as far as I remember larger. If the OS kills processes in its OOM process killer, we cannot do so much, as those processes are killed with a hard sigkill (-9) not sigterm. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13423786",
            "date": "2012-07-27T10:45:16+0000",
            "content": "I should have added this. I allocate just 98MB to the heap and 32 to the permgen so there just 130MB allocated. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13423787",
            "date": "2012-07-27T10:46:13+0000",
            "content": "Is it 32 bit or 64 bit JVM? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13423794",
            "date": "2012-07-27T11:01:31+0000",
            "content": "Java 1.6.0-26 64bit, just as Linux.\n\nI should also note now that i made an error in the configuration. I thought i had reduced the DocumentCache size to 64 but the node it was testing on had a size of 1024 configured and redistributed the config over the cluster via config bootstrap.\n\nThis still leaves the problem that Solr itself should run out of memory and not the OS as the cache is part of the heap. It also should clean old index directories. So this issue may consist of multiple problems. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13423796",
            "date": "2012-07-27T11:09:06+0000",
            "content": "OK, I wanted to come back:\nFrom what I see, 96 MB of heap is very few for Solr. Tests are running with -Xmx512. But regarding memory consumtion (Java's heap OOMs), Mark Miller might know better.\n\nBut Solr will not use all available RAM, as you are on 64 bit Java, Solr defaults to MMapDirectory - I recommend to read: http://blog.thetaphi.de/2012/07/use-lucenes-mmapdirectory-on-64bit.html\n\nIt will allocate from the system only heap + what Java itsself needs. Everything else is only allocated as adress space to directly acces file system cache. So the real memory usage of Solr is not what \"top\" reports in column \"VIRT\" but in column \"RES\" (resident memory). VIRT can be much higher (multiples of system RAM) with MMapDirectoy, as it only shows virtual address space allocated. This cannot cause Kernel-OOM to get active and kill processes, if that happens you have too few RAM for kernel, Solr + tools, sorry. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13423801",
            "date": "2012-07-27T11:19:10+0000",
            "content": "Hi - i don't look at virtual memory but RESident memory. My Solr install here will eat up to 512MB RESIDENT MEMORY and is killed by the OS. The virtual memory will then be almost 800MB, while both indexes are just 27MB in size. This sounds a lot of VIRT and RES for a tiny index and tiny heap.\n\nAlso, Solr will run fine and fast with just 100MB of memory, the index is still very small.\n\nThanks "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13423821",
            "date": "2012-07-27T11:48:05+0000",
            "content": "I have no idea what libraries bundled by Solr do outside, but as you poroblem seems to be related to cloud, it might be another thing in JVMs: DirectMemory (allocated by ByteBuffer.allocateDirect()). By default the JVM allows up to the heap size to be allocated on this space external to heap, so your -Xmx is only half of the truth. Solr by itsself does not use direct memory (only mmapped memory, but that is not resident), but I am not sure about Zookeeper and all that cloud stuff (and maybe plugins like TIKA-extraction).\n\nYou can limit direct memory with: -XX:MaxDirectMemorySize=<size>\n\nThe VIRT column can contain aditionally 2-3 times your index size depending on pending commits, merges,...\n\nPlease report back what this changes! "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13423841",
            "date": "2012-07-27T12:41:27+0000",
            "content": "Ok, i have increased my DocumentCache again to reproduce the problem and configured from -XX:MaxDirectMemorySize=100m to 10m but RES is still climbing at the same rate as before so no change. We don't use Tika only Zookeeper.\n\nAbout virtual memory. That also climbes to ~800Mb which is many times more than the index size. There are no pending commits or merges right after start up.\n\nThere may be some cloud replication related process that eats the RAM.\n\nThanks "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13424026",
            "date": "2012-07-27T18:09:08+0000",
            "content": "Seems this is perhaps two or three issues here.\n\n1. The resource usage. This may just be because replication causes two searchers to be open at the same time briefly? I really don't have any guesses at the moment. \n\n2. On a non graceful shutdown, old index dirs may end up left behind. We could look at cleaning them up on startup, but that should be it's own issue.\n\n3. You claim you are replicating on startup even though the shards should be in sync. You should not be replicating in that case. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13424785",
            "date": "2012-07-30T10:07:49+0000",
            "content": "Hi,\n\n1. Yes, but we allow only one searcher at the same time to be warmed. This resource usage also belongs to the Java heap, it cannot cause 5x as much heap being allocated.\n\n2. Yes, i'll open a new issue and refer to this.\n\n3. Well, in some logs i clearly see a core is attempting to download and judging from the multiple index directories it's true. I am very sure no updates have been added to the cluster for a long time yet it still attempts to recover. Below is a core recovering.\n\n\n2012-07-30 09:48:36,970 INFO [solr.cloud.ZkController] - [main] - : We are http://nl2.index.openindex.io:8080/solr/openindex_a/ and leader is http://nl1.index.openindex.io:8080/solr/openindex_a/\n2012-07-30 09:48:36,970 INFO [solr.cloud.ZkController] - [main] - : No LogReplay needed for core=openindex_a baseURL=http://nl2.index.openindex.io:8080/solr\n2012-07-30 09:48:36,970 INFO [solr.cloud.ZkController] - [main] - : Core needs to recover:openindex_a\n\n\n\nSomething noteworthy may be that for some reasons the index versions of all cores and their replica's don't match. After a restart the generation of a core is also different while it shouldn't have changed. The size in bytes is also slightly different (~20 bytes).\n\nThe main thing that's concerning that Solr consumes 5x the allocated heap space in the RESident memory. Caches and such are in the heap and the MMapped index dir should be in VIRTual memory and not cause the kernel to kill the process. I'm not yet sure what's going on here. Also, according to Uwe virtual memory should not be more than 2-3 times index size. In our case we see ~800Mb virtual memory for two 26Mb cores right after start up.\n\nWe have only allocated 98Mb to the heap for now and this is enough for such a small index. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13428255",
            "date": "2012-08-03T17:43:51+0000",
            "content": "Is it 2 or 3 cores you have? One thing is that it won't be just one extra searcher and index - it will be that times the number of cores. All of them will attempt to recover at the same time. So you will see a bump in RAM reqs. You are talking about off heap RAM though - I don't think SolrCloud will have much to do with that.\n\nLooking at your logs, it appears that you are replicating because the transaction logs look suspect - probably because of a hard power down. If you shutdown gracefully, you would get a peer sync instead which should determine you are up to date.\n\nThe comment for the path you are taking says:\n\n\n        // last operation at the time of startup had the GAP flag set...\n        // this means we were previously doing a full index replication\n        // that probably didn't complete and buffering updates in the meantime. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13428310",
            "date": "2012-08-03T19:20:43+0000",
            "content": "Looking at your logs, it appears that you are replicating because the transaction logs look suspect - probably because of a hard power down. If you shutdown gracefully, you would get a peer sync instead which should determine you are up to date.\n\nAlright, I just saw a similar thing happen even shutting everyone down gracefully.\n\nI think it's likely our kind of un-orderly cluster shutdown. If you shutdown all the nodes at once, depending on some timing differences, some recoveries may trigger as the leader goes down. Then the replica would go down.\n\nIn my case though, I was working with a 3 shard 2 replica cluster - so I don't think that was likely the issue. If one node goes down, there is no one to recover from.\n\nWe need to investigate a bit more. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13429132",
            "date": "2012-08-06T13:11:57+0000",
            "content": "Each node has two cores and allow only one warming searcher at any time. The problem is triggered on start up after graceful shutdown as well as a hard power off. I've seen it happening not only when the whole cluster if restarted (i don't think i've ever done that) but just one node of the 6 shard 2 replica test cluster.\n\nThe attached log is of one node being restarted out of the whole cluster.\n\nCould the off-heap RAM be part of data being sent over the wire?\n\nWe've worked around the problem for now by getting more RAM. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13430400",
            "date": "2012-08-07T16:25:45+0000",
            "content": "I was off a bit - even a non graceful shutdown should not cause this - if you are not indexing when you shutdown, at worst nodes should sync - not replicate.\n\nIn my testing, I could easily replicate this though - replication recoveries when it should be a sync.\n\nYonik recently committed a fix to this on trunk. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13430438",
            "date": "2012-08-07T17:05:02+0000",
            "content": "When exactly? Do you have an issue? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13430448",
            "date": "2012-08-07T17:13:17+0000",
            "content": "It was tagged to this issue number:\n\n+* SOLR-3685: Solr Cloud sometimes skipped peersync attempt and replicated instead due\n+  to tlog flags not being cleared when no updates were buffered during a previous\n+  replication.  (Markus Jelsma, Mark Miller, yonik) "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13430450",
            "date": "2012-08-07T17:15:41+0000",
            "content": "I think we still need to make an issue for cleaning up replication directories on non graceful shutdown.\n\nI'll rename this issue to match the recovery issue.\n\nAnd we can create a new issue for the memory thing (I tried to spot that locally, but have not yet). "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13436182",
            "date": "2012-08-16T18:15:07+0000",
            "content": "Finally! Two nodes failed again and got killed by the OS. All nodes have a lot of off-heap RES memory, sometimes 3x higher than the heap which is a meager 256MB.\n\nGot a name suggestion for the memory issue? I'll open one tomorrow and link to this one. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13436187",
            "date": "2012-08-16T18:19:52+0000",
            "content": "Are there any crash dump files? I don't think I've seen a java process crash without seeing one of these. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13436194",
            "date": "2012-08-16T18:23:50+0000",
            "content": "One node also got rsyslogd killed but the other survived. I assume the OOMkiller output of Linux is what you refer to? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13436200",
            "date": "2012-08-16T18:31:11+0000",
            "content": "Here's the relevant part of syslog for a node where Tomcat is killed by the OS. There is 1G or available RAM, no configured swap and the heap size is 256MB. The node has two running cores.\n\nThe off-heap RES memory for the Java process sometimes gets so large that Linux decides to kill it. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13436209",
            "date": "2012-08-16T18:40:25+0000",
            "content": "May also want to try specifying NIOFSDirectoryFactory in solrconfig.xml to see if it's related to mmap? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13436214",
            "date": "2012-08-16T18:47:25+0000",
            "content": "We didn't think mmap could be the cause but nevertheless we tried that once on a smaller cluster and got a lot of memory consumption again, after which it got killed.\nI can see if i can run one or two of the nodes with NIOFS but let the other run with mmap. We don't automatically restart cores so it should run fine if we temporarily change the config in zookeeper and restart two nodes.\n\nedit: each core has a ~2.5GB index. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13436243",
            "date": "2012-08-16T19:13:18+0000",
            "content": "Hi,\nI also don't think MMap is the reason for this, but it's good that you test it. You are saying that this happened with NIOFS, too, so my only guess is:\n\nAs noted before (in my last comment), there seems to be something using off-heap memory (RES does not contain mmap, so if RES raises, its definitely not mmap), but other \"direct memory\". I am not sure about other components in solr, that might use direct memory. Maybe Zookeeper? Its hard to find those things in external libraries. Can you try to limit the -XX:MaxDirectMemorySize to zero and see if exceptions occur? Also it would be good to have the output of \"pmap <pid>\", this shows allocated and mapped memory, we should look at anonymous mappings and how many are there. Pmap is in procutils package. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13436648",
            "date": "2012-08-17T10:25:17+0000",
            "content": "Here's the pmap for one node. Heap Xmx is still 256M. I also just noticed this node still has OpenJDK 6 running instead of Sun Java 6 like the other nodes. Despite that difference the memory consumption is equal.\nI'll also restart a node with NIOFS but i still expect memory to increase as with mmap. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13437790",
            "date": "2012-08-20T11:19:25+0000",
            "content": "To my surprise the RES for all nodes except the NIOFS node increased slowly over the past three days and were still increasing today. The mmapped nodes used sometimes up to three times the Xmx and, for some reason, about 1/2 Xmx of shared memory. We just restarted all nodes with 9 using mmap and one using NIO, after restart the mmapped nodes immediately start to use a lot more RES than the NIO node. The NIO node also uses much less shared memory.\n\nPerhaps what i've seen before with NIO also crashing was due to some other issue.\n\nSo what we're seeing here is the mmapped nodes use more RES and SHR than the NIO node. VIRT is as expected. I'll change another node to NIO and keep them running again for the next few days and keep sending documents and firing queries.\n\nAll nodes are using august 20th trunk from now on.\n "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13439666",
            "date": "2012-08-22T16:40:56+0000",
            "content": "Hi,\nanother thing to take into account: Java allocates on Linux for every thread a thread-local amount of memory (you can see this in the pmap output as additional \"[ anon ]\" mappings with a fixed size - it is only strange that you pmap does not populate the first hunman-readable column; I think you used -x instead of no option). This sums up to a lot of memory!\n\nSee http://mail.openjdk.java.net/pipermail/hotspot-compiler-dev/2012-August/008270.html (I don't think it is 64 MB, but for sure 1 MB)\n\nOn my machine, the threads take each 1 MB. Depending on your Tomcat config and its thread pools this can take lot of memory, too. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13457914",
            "date": "2012-09-18T16:17:19+0000",
            "content": "Whats happening with this issue: is it still one? should it be critical/block 4.0? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13458679",
            "date": "2012-09-19T13:27:34+0000",
            "content": "So what we're seeing here is the mmapped nodes use more RES and SHR than the NIO node. VIRT is as expected. I'll change another node to NIO and keep them running again for the next few days and keep sending documents and firing queries.\n\nthere is still an issue with mmap and high RES opposed to NIOFS but the actual issue is already resolved. I'll open a new issue. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13654248",
            "date": "2013-05-10T10:34:32+0000",
            "content": "Closed after release. "
        }
    ]
}