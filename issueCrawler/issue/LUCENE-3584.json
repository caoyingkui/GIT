{
    "id": "LUCENE-3584",
    "title": "bulk postings should be codec private",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Task",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "In LUCENE-2723, a lot of work was done to speed up Lucene's bulk postings read API.\n\nThere were some upsides:\n\n\tyou could specify things like 'i dont care about frequency data up front'.\n  This made things like multitermquery->filter and other consumers that don't\n  care about freqs faster. But this is unrelated to 'bulkness' and we have a\n  separate patch now for this on LUCENE-2929.\n\tthe buffersize for standardcodec was increased to 128, increasing performance\n  for TermQueries, but this was unrelated too.\n\n\n\nBut there were serious downsides/nocommits:\n\n\tthe API was hairy because it tried to be 'one-size-fits-all'. This made consumer code crazy.\n\tthe API could not really be specialized to your codec: e.g. could never take advantage that e.g. docs and freqs are aligned.\n\tthe API forced codecs to implement delta encoding for things like documents and positions.\n  But this is totally up to the codec how it wants to encode! Some codecs might not use delta encoding.\n\tusing such an API for positions was only theoretical, it would have been super complicated and I doubt ever\n  performant or maintainable.\n\tthere was a regression with advance(), probably because the api forced you to do both a linear scan thru\n  the remaining buffer, then refill...\n\n\n\nI think a cleaner approach is to let codecs do whatever they want to implement the DISI\ncontract. This lets codecs have the freedom to implement whatever compression/buffering they want\nfor the best performance, and keeps consumers simple. If a codec uses delta encoding, or if it wants\nto defer this to the last possible minute or do it at decode time, thats its own business. Maybe a codec\ndoesn't want to do any buffering at all.",
    "attachments": {
        "LUCENE-3584.patch": "https://issues.apache.org/jira/secure/attachment/12504431/LUCENE-3584.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2011-11-20T16:24:08+0000",
            "content": "patch: nuking the bulk api and implementing buffering for standardcodec so we have the same performance. ",
            "author": "Robert Muir",
            "id": "comment-13153825"
        },
        {
            "date": "2011-11-20T16:32:13+0000",
            "content": "+1\n\nMaybe we should also add buffering to 3.x codec, but thats not soo important. ",
            "author": "Uwe Schindler",
            "id": "comment-13153827"
        },
        {
            "date": "2011-11-20T16:53:40+0000",
            "content": "+1  ",
            "author": "Simon Willnauer",
            "id": "comment-13153830"
        },
        {
            "date": "2011-11-20T18:02:57+0000",
            "content": "+1 ",
            "author": "Michael McCandless",
            "id": "comment-13153836"
        },
        {
            "date": "2011-12-04T00:32:55+0000",
            "content": "I tested Solr's faceting code (the enum method that steps over terms and uses the filterCache), with minDf set high enough so that the filterCache wouldn't be used (i.e.it directly uses DocsEnum to calculate the count for the term).  %increase when we were using the bulk API = r208282/trunk time (i.e. performance is measured as change in throughput... so going from 400ms to 200ms is expressed as 100% increase in throughput).\n\n\n\n\nnumber of terms\ndocuments per term\nbulk API performance increase\n\n\n10000000\n1\n2.1\n\n\n1000000\n10\n3.0\n\n\n1000\n10000\n8.9\n\n\n10\n1000000\n51.6\n\n\n\n\n\nSo when terms match many documents, we've had quite a drop-off due to the removal of the bulk API. ",
            "author": "Yonik Seeley",
            "id": "comment-13162246"
        },
        {
            "date": "2011-12-04T12:58:46+0000",
            "content": "Yonik, where is the code to your benchmark? I don't trust it.\nhotspot likes to change how it compiles readvint so be sure to use lots of jvm iterations.\n\nI tested this change with luceneutil (lots of iterations, takes an hour to run) and everything\nwas the same, with disjunction queries looking better every time I ran it.\n\nI think everything is just fine.\n\n\n\n\nTask\nQPS\ntrunkStdDev\ntrunk QPS\npatchStdDev\npatch Pct diff\n\n\nIntNRQ\n10.44\n0.69\n9.80\n0.88\n-19% - 9%\n\n\nWildcard\n24.93\n0.41\n24.23\n0.44\n-6% - 0%\n\n\nPrefix3\n48.83\n1.14\n47.45\n1.09\n-7% - 1%\n\n\nTermBGroup1M1P\n43.29\n1.08\n42.28\n1.31\n-7% - 3%\n\n\nPKLookup\n187.88\n4.49\n186.43\n5.07\n-5% - 4%\n\n\nAndHighHigh\n15.10\n0.25\n14.99\n0.54\n-5% - 4%\n\n\nSpanNear\n15.96\n0.43\n15.87\n0.43\n-5% - 4%\n\n\nTermBGroup1M\n32.30\n0.87\n32.14\n0.64\n-4% - 4%\n\n\nSloppyPhrase\n14.53\n0.50\n14.47\n0.55\n-7% - 7%\n\n\nTermGroup1M\n24.07\n0.54\n24.01\n0.48\n-4% - 4%\n\n\nRespell\n87.11\n3.74\n86.91\n4.05\n-8% - 9%\n\n\nFuzzy1\n94.79\n3.18\n94.58\n4.05\n-7% - 7%\n\n\nFuzzy2\n48.13\n1.92\n48.10\n2.45\n-8% - 9%\n\n\nPhrase\n9.10\n0.41\n9.11\n0.41\n-8% - 9%\n\n\nTerm\n135.52\n4.74\n137.26\n2.91\n-4% - 7%\n\n\nAndHighMed\n51.64\n0.92\n53.20\n1.90\n-2% - 8%\n\n\nOrHighHigh\n10.75\n0.62\n11.79\n0.60\n-1% - 22%\n\n\nOrHighMed\n12.20\n0.75\n13.40\n0.71\n-1% - 23%\n\n\n\n ",
            "author": "Robert Muir",
            "id": "comment-13162392"
        },
        {
            "date": "2011-12-04T14:12:16+0000",
            "content": "where is the code to your benchmark?  I don't trust it.\n\nI'm always skeptical of benchmarks too \n\nNo benchmark code this time, I just hit Solr directly from the browser, waiting for the times to stabilize and picking the lowest (and assuring that I can hit very near that low again and it wasn't a fluke.  Results are very repeatable though (and I killed the JVM and retried to make sure hotspot would do the same thing again)\n\nThe index is from a 10M row CSV file I generated years ago.  For example, the field with 10 terms is simply a single valued field with a random number between 0 and 9, padded out to 10 chars.\n\nOh, this is Linux on a Phenom II, JKD 1.6.0_29  ",
            "author": "Yonik Seeley",
            "id": "comment-13162400"
        },
        {
            "date": "2011-12-04T19:23:38+0000",
            "content": "my first question would be did you flush the FS caches and warm up your JVM? if you didn't flush caches it would be interesting what you ran first? Are those two indexes the same?  ",
            "author": "Simon Willnauer",
            "id": "comment-13162445"
        },
        {
            "date": "2011-12-14T14:35:53+0000",
            "content": "Yonik, can you check if you see the same thing with your benchmark if you apply LUCENE-3648 ",
            "author": "Simon Willnauer",
            "id": "comment-13169399"
        },
        {
            "date": "2011-12-14T20:14:51+0000",
            "content": "I'm traveling this week and don't have access to that box, but I should be able to get to it next week sometime. ",
            "author": "Yonik Seeley",
            "id": "comment-13169656"
        }
    ]
}