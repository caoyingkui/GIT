{
    "id": "LUCENE-8221",
    "title": "MoreLikeThis.setMaxDocFreqPct can easily int-overflow on larger indexes",
    "details": {
        "labels": "",
        "priority": "Minor",
        "resolution": "Fixed",
        "affect_versions": "None",
        "status": "Closed",
        "type": "Bug",
        "components": [],
        "fix_versions": [
            "7.4"
        ]
    },
    "description": "public void setMaxDocFreqPct(int maxPercentage) {\n    this.maxDocFreq = maxPercentage * ir.numDocs() / 100;\n  }\n\n\nThe above overflows integer range into negative numbers on even fairly small indexes (for maxPercentage = 75, it happens for just over 28 million documents.\n\nWe should make the computations on long range so that it doesn't overflow and have a more strict argument validation.",
    "attachments": {
        "LUCENE-8221.patch": "https://issues.apache.org/jira/secure/attachment/12916147/LUCENE-8221.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16413467",
            "date": "2018-03-26T07:02:17+0000",
            "content": "Its also confusing and inconsistent that it uses numDocs() in the percentage calculation, which excludes deleted documents, when docFreq() does not. It should really use maxDoc() here. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16413484",
            "date": "2018-03-26T07:13:04+0000",
            "content": "True, although from a logical standpoint I think numDocs makes more sense than maxDoc \u2013 I'd typically want those thresholds calculated based on the actual number of documents in the index at any given moment, without regard to how deleted documents are represented. It's the docFreq that should be altered/ catered for (in the future), not numDocs?\n ",
            "author": "Dawid Weiss"
        },
        {
            "id": "comment-16413492",
            "date": "2018-03-26T07:18:48+0000",
            "content": "I don't agree: thats inconsistent with how all other scoring in lucene works. And its not feasible at all for term's docFreq to reflect deleted documents.\n\nThe current inconsistency makes a to of stupid things impossible: for example numDocs() can easily be 0, or < docFreq().\n\nAny use of numDocs is flat out wrong... ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16413517",
            "date": "2018-03-26T07:38:33+0000",
            "content": "Here is what I think.\n\nFrom information retrieval point of view it's a fairly common threshold: a percentage of documents present in the collection at the time. I didn't add this method originally, but I understand its intent and it's clear to me. In the end, this method computes and sets an absolute document frequency cutoff value. It's simple and correlated with the number of documents present in the index at the time when you make the call.\n\nIf the number of documents is 0, nothing really happens. What I'd consider and odd behavior is if this method could fluctuate depending on how many deletes or merges you had up to the point of invoking it... It'd confuse me (and I guess others not familiar with Lucene indexing internals) a lot.\n\nFinally, this issue is about fixing the overflow, really, not changing how it works. If you'd like to change the implementation to maxDoc can we discuss this as part of another issue (and you can probably sense from the above paragraph I don't quite agree maxDoc is a better choice here, it seems counter-intuitive to me, even if more consistent with docFreq use later on). ",
            "author": "Dawid Weiss"
        },
        {
            "id": "comment-16413521",
            "date": "2018-03-26T07:44:00+0000",
            "content": "\n\nIf the number of documents is 0, nothing really happens. What I'd consider and odd behavior is if this method could fluctuate depending on how many deletes or merges you had up to the point of invoking it... It'd confuse me (and I guess others not familiar with Lucene indexing internals) a lot.\n\n\n\u00a0\n\nYou use docFreq and numDocs. Because you use docFreq, its still gonna fluctuate with merges.\n\nBut now in addition to that, you've got skew. Your argument fails.\n\nSorry, I don't think my concerns should be brushed aside here. this is definitely related to the issue. The formula is simply wrong and we should fix it. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16413525",
            "date": "2018-03-26T07:53:32+0000",
            "content": "Robert, there is no docFreq call in that method. I frankly don't understand why you're being so stubborn. \n\nBut now in addition to that, you've got skew. Your argument fails.\n\nWhat skew?  ",
            "author": "Dawid Weiss"
        },
        {
            "id": "comment-16414472",
            "date": "2018-03-26T20:23:02+0000",
            "content": "I tried to come up with the documentation of this method when maxDoc is used and failed (this propagates to Solr's MLT handlers and docs too). I don't know how to clearly convey what this method will actually calculate when maxDocs is involved and at the same time I know what I want (previous behavior, with the int-overflow gone).\n\nI would like to sustain my request to commit this simple bug fix in and create another issue for the change in behaviour you suggested, Robert. Or provide a suggestion on the wording of the documentation for the maxDoc version. Thanks. ",
            "author": "Dawid Weiss"
        },
        {
            "id": "comment-16428222",
            "date": "2018-04-06T11:30:29+0000",
            "content": "Using numDocs percentage to set maxDocFreq effectively lowers it, providing a stricter estimate and possibly discarding terms that were frequent originally, but could be eligible to pass the docFreq > maxDocFreq threshold after deletions are applied (so they're no longer frequent). Using maxDoc on segments with heavy deletions potentially passes through a lot of terms that could be discarded earlier without being inserted into the queue at all. \n\nWhat further complicates things here is that the term score similarity function in createQueue also uses numDocs as one of the arguments, so changing to maxDoc all across the board would definitely change the output for people \u2013 don't know whether it'd be better or worse. ",
            "author": "Dawid Weiss"
        },
        {
            "id": "comment-16428239",
            "date": "2018-04-06T12:02:42+0000",
            "content": "It is easy to document, and see the skew, because we explain it elsewhere in scoring\n\nhttps://github.com/apache/lucene-solr/blob/master/lucene/core/src/java/org/apache/lucene/search/CollectionStatistics.java#L43\nhttps://github.com/apache/lucene-solr/blob/master/lucene/core/src/java/org/apache/lucene/search/TermStatistics.java#L42\nhttps://github.com/apache/lucene-solr/blob/master/lucene/core/src/java/org/apache/lucene/search/similarities/BM25Similarity.java#L139-L142\n\n\nWhat further complicates things here is that the term score similarity function in createQueue also uses numDocs as one of the arguments, so changing to maxDoc all across the board would definitely change the output for people \u2013 don't know whether it'd be better or worse.\n\nThat is broken too, then.\n\nAnd so is the range check in your patch, because percentage can be larger than 100% with the broken numDocs formula used here. When a percentage can be bigger than 100, man that's your first sign that shit is wrong! ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16428670",
            "date": "2018-04-06T17:49:25+0000",
            "content": "I don't mind changing the formula (even if I disagree that catering for internal representation of deleted documents justifies this), but not as part of this issue. Changing the formula will change the results people get from MLT: this should go into a major release, not a point release; what I patched was a trivial overflow problem that doesn't touch any internals.\n\nAnd so is the range check in your patch, because percentage can be larger than 100% with the broken numDocs formula used here. When a percentage can be bigger than 100, man that's your first sign that shit is wrong!\n\nTo me the percentage remains within 0-100% with numDocs; you compute the threshold against the current state of your index (live documents). The computed value of the cutoff threshold is correct, it is the comparison against docFreq that isn't sound here because docFreq doesn't have deleted documents information. I don't quite understand the way you perceive only one of those as \"correct\" vs. \"utter shit\" and I don't think I want to explore this subject further.\n\nIs it ok if I apply the overflow fix against 7.x, master and create a new issue cutting over to maxDoc (everywhere in mlt) and apply it to master only? If no, speak up. ",
            "author": "Dawid Weiss"
        },
        {
            "id": "comment-16490515",
            "date": "2018-05-25T10:23:35+0000",
            "content": "Commit 300ab00b4c75810a591750c785d47fcba2d65ebb in lucene-solr's branch refs/heads/branch_7x from Dawid Weiss\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=300ab00 ]\n\nLUCENE-8221: MoreLikeThis.setMaxDocFreqPct can easily int-overflow on larger indexes. ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-16490516",
            "date": "2018-05-25T10:23:36+0000",
            "content": "Commit 719fce80269706cf5c6f091cc71e39620f7b6cb2 in lucene-solr's branch refs/heads/master from Dawid Weiss\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=719fce8 ]\n\nLUCENE-8221: MoreLikeThis.setMaxDocFreqPct can easily int-overflow on larger indexes. ",
            "author": "ASF subversion and git services"
        }
    ]
}