{
    "id": "SOLR-6305",
    "title": "Ability to set the replication factor for index files created by HDFSDirectoryFactory",
    "details": {
        "affect_versions": "None",
        "status": "Open",
        "fix_versions": [],
        "components": [
            "hdfs"
        ],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "HdfsFileWriter doesn't allow us to create files in HDFS with a different replication factor than the configured DFS default because it uses:     \nFsServerDefaults fsDefaults = fileSystem.getServerDefaults(path);\n\nSince we have two forms of replication going on when using HDFSDirectoryFactory, it would be nice to be able to set the HDFS replication factor for the Solr directories to a lower value than the default. I realize this might reduce the chance of data locality but since Solr cores each have their own path in HDFS, we should give operators the option to reduce it.\n\nMy original thinking was to just use Hadoop setrep to customize the replication factor, but that's a one-time shot and doesn't affect new files created. For instance, I did:\n\nhadoop fs -setrep -R 1 solr49/coll1\n\nMy default dfs replication is set to 3 ^^ I'm setting it to 1 just as an example\n\nThen added some more docs to the coll1 and did:\n\nhadoop fs -stat %r solr49/hdfs1/core_node1/data/index/segments_3\n\n3 <-- should be 1\n\nSo it looks like new files don't inherit the repfact from their parent directory.\n\nNot sure if we need to go as far as allowing different replication factor per collection but that should be considered if possible.\n\nI looked at the Hadoop 2.2.0 code to see if there was a way to work through this using the Configuration object but nothing jumped out at me ... and the implementation for getServerDefaults(path) is just:\n\n  public FsServerDefaults getServerDefaults(Path p) throws IOException \n{\n    return getServerDefaults();\n  }\n\nPath is ignored",
    "attachments": {
        "0001-OIQ-23224-SOLR-6305-Fixed-SOLR-6305-by-reading-the-r.patch": "https://issues.apache.org/jira/secure/attachment/12917451/0001-OIQ-23224-SOLR-6305-Fixed-SOLR-6305-by-reading-the-r.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-14081035",
            "date": "2014-07-31T16:02:06+0000",
            "content": "If I'm reading this right, you can change the replication factor by pointing to a hadoop config folder that has dfs.replication (or whatever it is called) set in the hdfs config file. Have to get on a call, but I'll post some more in a bit. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14081042",
            "date": "2014-07-31T16:06:16+0000",
            "content": "It's solr.hdfs.confidr for the HdfsDirectroyFactory. That's reads in standard hdfs config property file stuff. You can just configure whatever as you would a normal hdfs client. This is also how you can setup name node HA and such. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-14081186",
            "date": "2014-07-31T17:55:02+0000",
            "content": "Thanks Mark, nice approach to config overriding, unfortunately, looks like Hadoop doesn't want this to work for us \n\nI tried your suggestion and added some debug to the HdfsFileWriter and here's what I see:\n\nIn HdfsFileWriter, fileSystem.rep=4 but fsDefaults.getReplication=2\n\nIn the main etc/hadoop/hdfs-site.xml (cluster wide default), dfs.replication=2\n\nI added etc/solr/hdfs-site.xml with dfs.replication=4 and then -Dsolr.hdfs.confdir=/Users/thelabdude/dev/lw/tools/hadoop-2.2.0/etc/solr\n\nThe fileSystem and Configuration objects being used by Solr definitely see: dfs.replication=4. However, it is still coming out with the cluster-wide default of 2! Digging into the Hadoop code a little, it looks like the DistributedFileSystem implementation of FileSystem gets its FsServerDefaults from DFSClient, which uses: serverDefaults = namenode.getServerDefaults();\n\nThat looks to me like the server defaults are coming from the namenode (cluster-wide) and totally disregarding what's in the Configuration object setup by HdfsDirectoryFactory! Ugh. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14083004",
            "date": "2014-08-01T21:41:06+0000",
            "content": "We use it to do things like this, so I don't think it can be totally disregarded. Mainly we use it for configuring name node HA, not lowering replication though.\n\nBut, for example, in this example project I have on GitHub, I have to config it to dfs.replication=1 because there is only one data node and it would complain otherwise that it couldn't meet the replication factor. When I configured the client to dfs.replication=1, it no longer complains. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14083006",
            "date": "2014-08-01T21:41:28+0000",
            "content": "Example project link: https://github.com/markrmiller/solr-map-reduce-example "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14083016",
            "date": "2014-08-01T21:50:28+0000",
            "content": "Now if I don't set the replication factor to 1 and leave it at it's default of 3 and try to start Solr, I get errors like:\n\norg.apache.hadoop.ipc.RemoteException(java.io.IOException): file /solr_test/collection1/core_node3/data/index/org.apache.solr.store.hdfs.HdfsDirectory@28b3d48e lockFactory=org.apache.solr.store.hdfs.HdfsLockFactory@2dc3049a-write.lock on client 127.0.0.1.\nRequested replication 3 exceeds maximum 1 "
        },
        {
            "author": "Hari Sekhon",
            "id": "comment-14367192",
            "date": "2015-03-18T14:31:50+0000",
            "content": "I'm also having problems with this in Solr 4.10.3. I had tried creating a separate hadoop conf dir pointed to via solr.hdfs.confdir with hdfs dfs.replication=1, then restarted all Solr instances, deleted and recreated the collection and dataDir but found that it only set the write locks to rep factor 1 and still set the data/index/segments* to rep factor 2. Even setting dfs.replication cluster wide resulted in the same behaviour which is odd (I didn't bounce the NN + DNs since this should be hdfs client writer side config).\n\nNote sure if this is related to SOLR-6528. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15005142",
            "date": "2015-11-14T03:50:55+0000",
            "content": "I only looked at the lock file above. Perhaps something is preventing the same thing from working on index files - odd, but certainly possible. "
        },
        {
            "author": "Harsh J",
            "id": "comment-15996083",
            "date": "2017-05-04T02:53:07+0000",
            "content": "Timothy Potter is right here in the description BTW. Hadoop APIs let you pass any arbitrary replication value via the FileSystem.create API - this overrides the local default (dfs.replication config) when passed. In Solr, the API usage is effectively asking the NameNode what its default replication factor is, and then creates a file with that value, ignoring the local configuration. As a result, you cannot specifically control the replication factor of index files in Solr without changing the whole HDFS cluster's default. "
        },
        {
            "author": "Boris Pasko",
            "id": "comment-16424254",
            "date": "2018-04-03T16:29:50+0000",
            "content": "This bug is not fixed for years now. Solr 6.6.3 exhibits same behavior.\u00a0 "
        },
        {
            "author": "Boris Pasko",
            "id": "comment-16424704",
            "date": "2018-04-03T22:44:24+0000",
            "content": "Here is the patch 0001-OIQ-23224-SOLR-6305-Fixed-SOLR-6305-by-reading-the-r.patch for solr 6.6.3.\n\nIt is very simple. Instead of relying on server-provided default, reread the replication factor from DFS client config.\n\nprivate static final OutputStream getOutputStream(FileSystem fileSystem, Path path) throws IOException {\n\u00a0\u00a0\u00a0 Configuration conf = fileSystem.getConf();\n\u00a0\u00a0\u00a0 FsServerDefaults fsDefaults = fileSystem.getServerDefaults(path);\n+\u00a0\u00a0 short replication = fileSystem.getDefaultReplication(path);\n\u00a0\u00a0\u00a0 EnumSet<CreateFlag> flags = EnumSet.of(CreateFlag.CREATE,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CreateFlag.OVERWRITE);\n\u00a0\u00a0\u00a0 if (Boolean.getBoolean(HDFS_SYNC_BLOCK)) {\n\u00a0\u00a0\u00a0\u00a0\u00a0 flags.add(CreateFlag.SYNC_BLOCK);\n\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0 return fileSystem.create(path, FsPermission.getDefault()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .applyUMask(FsPermission.getUMask(conf)), flags, fsDefaults\n+\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .getFileBufferSize(), replication, fsDefaults\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .getBlockSize(), null);\n\u00a0 }\n\nI have tested this on real hardware cluster and it generates files with replication factor set in /etc/hbase/conf/hdfs-site.xml (provided in solrconfig.xml).\n\nI haven't found any HdfsFileWriter unit tests so haven't modified any. \n\nI'm running 'ant test' with the patch. "
        },
        {
            "author": "Boris Pasko",
            "id": "comment-16424712",
            "date": "2018-04-03T22:54:33+0000",
            "content": "I have run\n\nant test\n\nand I see 1 test failure:\n\n\u00a0\u00a0 [junit4] Tests with failures [seed: B470387B3BBAF803]:\n\u00a0\u00a0 [junit4]\u00a0\u00a0 - org.apache.solr.cloud.CollectionsAPIDistributedZkTest.testCollect\n\nHowever, when this test is run in Eclipse, no errors reported. I assume this test is irrelevant and failing randomly.\n\n\u00a0 "
        },
        {
            "author": "Hendrik Haddorp",
            "id": "comment-16425097",
            "date": "2018-04-04T07:09:31+0000",
            "content": "I had just tested Solr 7.2.1\u00a0with an HDFS setup\u00a0that had only one node. For that I also saw that some files got created with a replication factor of 3 as that is default on the client side if nothing else is configured while others got created with a factor of 1, as configured on the name node. I then started Solr with the system property\u00a0\"solr.hdfs.confdir\" pointing to a directory containing just the file \"hdfs-site.xml\". In that file I set \"dfs.replication\" to 1. After that I did not find any files in HDFS anymore that had a replication factor of 3.\nIt would however been nice if the replication factor could consistently being controlled by the client side. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-16429550",
            "date": "2018-04-07T22:44:55+0000",
            "content": "I've never used HDFS, so I might not be contributing anything useful to this discussion.\n\nIt was my understanding that if you configure HDFS to keep 3 copies, and then tell SolrCloud to use a replicationFactor of 3, that you would actually have nine copies \u2013 SolrCloud would make its replicas just like it would on standard filesystems, and then HDFS would replicate each of the files in those indexes.  Is that an incorrect understanding? "
        },
        {
            "author": "Hendrik Haddorp",
            "id": "comment-16429676",
            "date": "2018-04-08T08:28:26+0000",
            "content": "Shawn Heisey When you store a file in HDFS it ends up being stored in blocks and these blocks get replicated to multiple nodes for increased safety. You can configure a default block replication factor but you can also create files with a specific replication factor. The problem in Solr is that some parts take the default replication factor as it is defined on the HDFS name node and some take the default. The default is 3 unless you tell Solr that you have a local HDFS configuration (using solr.hdfs.confdir). So when you set the default HDFS replication factor (done on the name node) to 1 Solr still ends up creating files that want to have a replication factor of 3. In case you are using a small HDFS test setup that only has one node (data node to be exact) then your blocks are still being created but they are under recplicated.\n\nWhen you tell SolrCloud to use a replicationFactor of 3 then Solr creates 3 copies of the collection files in HDFS, just like it does in the local case. So yes, in your case one could saw that the data exists 9 times. One could also see Solr on HDFS like Solr on a shared RAID filesystem. In RAID the files are however all replicated in the same way while in HDFS the replication factor of the files can be different and changed dynamically.\n\nIn my opinion the only problem is that Solr does not create the files in HDFS in the same way. Some pick the replication factor as defined on the HDFS name node while others don't. "
        },
        {
            "author": "Boris Pasko",
            "id": "comment-16430770",
            "date": "2018-04-09T15:57:22+0000",
            "content": "Shawn Heisey you're absolutely right. For solr, HDFS is just another block storage. It does not use any HDFS-specific replication strategies, for example, copying files as Map/Reduce job. Therefore, having 3 solr replicas and default replication factor of 3 actually produces 3 data copies.\n\nThe storage space itself is typically less of the concern, the bigger concern is network traffic. Bytes that has to be moved to 3 datanodes instead of 1 datanode. "
        },
        {
            "author": "Boris Pasko",
            "id": "comment-16438178",
            "date": "2018-04-14T01:30:43+0000",
            "content": "Unfortunately, it seems that the patch I did does not cover all usecases. It might be that merging is still done with server-side replication factor:\n\n$ hadoop fs -du -h /solr/classified/core_node4/data70.0 G   209.9 G  /solr/classified/core_node4/data/index.20180411213103577\n70.0 G   209.9 G  /solr/classified/core_node4/data/index.20180411213103577\n78       78       /solr/classified/core_node4/data/index.properties\n210      210      /solr/classified/core_node4/data/replication.properties\n0        0        /solr/classified/core_node4/data/snapshot_metadata\n915.3 M  1.8 G    /solr/classified/core_node4/data/tlog\n\n\nand\n\n$ hadoop fs -ls /solr/classified/core_node2/data/index\n-rwxr-xr-x   1 solr solr        418 2018-04-13 21:25 /solr/classified/core_node2/data/index/_13ke.si\n-rwxr-xr-x   3 solr solr  663715968 2018-04-11 17:22 /solr/classified/core_node2/data/index/_3fp.fdt\n-rwxr-xr-x   3 solr solr     517308 2018-04-11 17:21 /solr/classified/core_node2/data/index/_3fp.fdx\n-rwxr-xr-x   3 solr solr       3638 2018-04-11 17:21 /solr/classified/core_node2/data/index/_3fp.fnm\n-rwxr-xr-x   3 solr solr   25644767 2018-04-11 17:21 /solr/classified/core_node2/data/index/_3fp.nvd\n-rwxr-xr-x   3 solr solr        178 2018-04-11 17:21 /solr/classified/core_node2/data/index/_3fp.nvm\n-rwxr-xr-x   3 solr solr        522 2018-04-11 17:22 /solr/classified/core_node2/data/index/_3fp.si\n-rwxr-xr-x   1 solr solr     356244 2018-04-12 08:14 /solr/classified/core_node2/data/index/_3fp_9v.liv\n-rwxr-xr-x   3 solr solr 1634072760 2018-04-11 17:21 /solr/classified/core_node2/data/index/_3fp_Lucene50_0.doc\n-rwxr-xr-x   3 solr solr 2698137408 2018-04-11 17:22 /solr/classified/core_node2/data/index/_3fp_Lucene50_0.pos\n-rwxr-xr-x   3 solr solr  365912676 2018-04-11 17:21 /solr/classified/core_node2/data/index/_3fp_Lucene50_0.tim\n-rwxr-xr-x   3 solr solr    6024240 2018-04-11 17:21 /solr/classified/core_node2/data/index/_3fp_Lucene50_0.tip\n-rwxr-xr-x   3 solr solr  596163565 2018-04-11 17:22 /solr/classified/core_node2/data/index/_5oi.fdt\n-rwxr-xr-x   3 solr solr     479765 2018-04-11 17:22 /solr/classified/core_node2/data/index/_5oi.fdx\n-rwxr-xr-x   3 solr solr       3638 2018-04-11 17:22 /solr/classified/core_node2/data/index/_5oi.fnm\n-rwxr-xr-x   3 solr solr   26688139 2018-04-11 17:22 /solr/classified/core_node2/data/index/_5oi.nvd\n-rwxr-xr-x   3 solr solr        178 2018-04-11 17:22 /solr/classified/core_node2/data/index/_5oi.nvm\n-rwxr-xr-x   3 solr solr        522 2018-04-11 17:22 /solr/classified/core_node2/data/index/_5oi.si\n-rwxr-xr-x   3 solr solr 1466093502 2018-04-11 17:22 /solr/classified/core_node2/data/index/_5oi_Lucene50_0.doc\n-rwxr-xr-x   3 solr solr 2374256964 2018-04-11 17:22 /solr/classified/core_node2/data/index/_5oi_Lucene50_0.pos\n-rwxr-xr-x   3 solr solr  345128291 2018-04-11 17:22 /solr/classified/core_node2/data/index/_5oi_Lucene50_0.tim\n-rwxr-xr-x   3 solr solr    5839414 2018-04-11 17:22 /solr/classified/core_node2/data/index/_5oi_Lucene50_0.tip\n-rwxr-xr-x   1 solr solr     333668 2018-04-12 04:11 /solr/classified/core_node2/data/index/_5oi_aj.liv\n\n "
        },
        {
            "author": "Boris Pasko",
            "id": "comment-16438181",
            "date": "2018-04-14T01:42:27+0000",
            "content": "Looks like when recovery is performed, the index is downloaded by some code that still uses server-side replication factor. "
        },
        {
            "author": "Boris Pasko",
            "id": "comment-16458007",
            "date": "2018-04-29T12:36:10+0000",
            "content": "I was wrong with my analysis. The replication patch works correctly. The files with replication factor 3 are just leftovers from the time when patch was not applied.\n\nI propose to include the patch into Solr codebase. "
        }
    ]
}