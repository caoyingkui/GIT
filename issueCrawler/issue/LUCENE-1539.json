{
    "id": "LUCENE-1539",
    "title": "Improve Benchmark",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/benchmark"
        ],
        "type": "Improvement",
        "fix_versions": [
            "2.9"
        ],
        "affect_versions": "2.4",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Benchmark can be improved by incorporating recent suggestions posted\non java-dev. M. McCandless' Python scripts that execute multiple\nrounds of tests can either be incorporated into the codebase or\nconverted to Java.",
    "attachments": {
        "LUCENE-1539.patch": "https://issues.apache.org/jira/secure/attachment/12400197/LUCENE-1539.patch",
        "sortBench2.py": "https://issues.apache.org/jira/secure/attachment/12400198/sortBench2.py",
        "sortCollate2.py": "https://issues.apache.org/jira/secure/attachment/12400199/sortCollate2.py"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-02-13T18:26:48+0000",
            "content": "The patch adds CreateWikiIndex which creates enwiki indexes with\nmultiple percentages of deletes. It probably needs to be made into a\ntask or multiple tasks along with an alg file. One goal is to evolve\nthis patch to enable concurrent indexing and searching. \n\nI can see the elegance of using Python scripts because it's easy to\nedit, and the pickling is nice. Equivalent Java code could be fairly\nlengthy. However since this is a Java project and we have a framework\nwith the .alg files for defining some level of external operations,\nit seems we may want to figure out a way to put the Python script\nfunctionality into tasks and defined by .alg files.  ",
            "author": "Jason Rutherglen",
            "id": "comment-12673337"
        },
        {
            "date": "2009-02-13T18:27:54+0000",
            "content": "Python scripts attached.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12673339"
        },
        {
            "date": "2009-02-15T10:56:27+0000",
            "content": "Jason, couldn't we create a .alg that makes multiple copies of a Wikipedia index w/ different pctg deletes, instead of a static main java tool?\n\nIn fact.... one cleaner way to achieve this would be to use multiple commits in the same index.  So instead of making a full copy of the wiki index for each pctg of deletes, make a new commit.  You end up w/ a single index that has N commits, one for each pctg you need to test.  Then we'd just need a way to tell an alg which commit to open.  Since a commit can contain an optional string commitUserData, we could use to tell the alg which commit to open. ",
            "author": "Michael McCandless",
            "id": "comment-12673596"
        },
        {
            "date": "2009-02-16T22:36:09+0000",
            "content": "couldn't we create a .alg that makes multiple copies of a Wikipedia index w/ different pctg deletes, instead of a static main java tool? \n\nWe'll need a new DeletesTask that deletes based on a percentage?  \n\nuse multiple commits in the same index\n\nThis sounds good.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12674075"
        },
        {
            "date": "2009-02-20T00:31:30+0000",
            "content": "In looking over the code, to do the multiple commits using IR we'll need to add a IR.flush(String userData) method? ",
            "author": "Jason Rutherglen",
            "id": "comment-12675196"
        },
        {
            "date": "2009-02-20T11:35:19+0000",
            "content": "In looking over the code, to do the multiple commits using IR we'll need to add a IR.flush(String userData) method?\n\nYes, we should.  Can you open a new issue + patch?\n\nWe also have to fix contrib/benchmark to allow specification of a Deletion Policy, and then allow openReader task to take a string (userData) to specific which commit to open.\n\nBut: it'd be best if, within a single alg, we could specify a series of commits to open, so that we can iterate over the different commit points.  I don't think a param to the task allows this?  (But I'm not sure).  If we made it a config option then I believe we could specify a sequence which each round would advance through. ",
            "author": "Michael McCandless",
            "id": "comment-12675336"
        },
        {
            "date": "2009-03-09T20:59:30+0000",
            "content": "For a performing simultaneous indexing and searching, how should we\nbest represent this in the .alg file? We have an example\nindexing-multithreaded.alg so I suppose we can simply spawn another\nset of threads after the \"[\n{ \"MAddDocs\" AddDoc }\n : 5000] : 4\" line\nthat performs searches? Just gathering opinions as I don't feel\ncompletely familiar with the benchmark suite yet. ",
            "author": "Jason Rutherglen",
            "id": "comment-12680276"
        },
        {
            "date": "2009-03-12T18:42:43+0000",
            "content": "\n\tAdded deletepercent.alg as an example of these tasks\n\tCommitIndexTask commits an IndexWriter using a commit name\n\tOpenReaderTask opens a specific commit point by name\n\tFlushReaderTask flushes a reader using a commit name\n\tDeleteByPercentTask a percentage of reader documents\n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12681437"
        },
        {
            "date": "2009-04-03T15:51:39+0000",
            "content": "This patch looks good \u2013 some questions:\n\n\n\tIs CreateWikiIndex intended to be committed?  I thought not?  Ie I\n    though the goal w/ this issue is add the necessary tasks so that\n    CreateWikiIndex would be done as an alg.\n\n\n\n\n\tI think we shouldn't bump to Java 1.5 \u2013 it's only CreateWikiIndex\n    that needs it anyway (in only 2 places).\n\n\n\n\n\tPrintReaderTask never closes the reader.\n\n\n\n\n\tNot sure why you needed to relax private -> protected in AddDocTask?\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12695462"
        },
        {
            "date": "2009-04-08T00:42:09+0000",
            "content": "Fixed the above mentioned problems.  When LUCENE-1516 is in should we add the near realtime benchmarks here? ",
            "author": "Jason Rutherglen",
            "id": "comment-12696848"
        },
        {
            "date": "2009-04-08T10:12:26+0000",
            "content": "I think DeleteByPercentTask.java is missing?\n\nAlso: I think you're missing the ability to set the deletion policy for the reader or writer?  Without that, only the last commit is retained. ",
            "author": "Michael McCandless",
            "id": "comment-12696971"
        },
        {
            "date": "2009-04-08T15:23:31+0000",
            "content": "Is it also interesting to add extensions to EnwikiDocMaker, WriteLineDoc and LineDocMaker which can read/write the content in a bzip format?\nI downloaded the latest Enwiki dump, 4.5 GB in bzip format. Extracted XML size is 17GB. I thought to myslef that I don't have a real reason to extract it - I can read the content directly from the bzip-type file.\n\nSo I looked around and found out that in ant.jar there are two classes which can read/write that format. Just to compare, I gzipped the XML file and the result was 5.1GB file (~13% larger). The general measurements on the web also show bzip is superior to gzip, although it probably runs a bit slower.\n\nI then ran the WriteLineDoc task, to produce the one-line-per-document text file, and stopped when it reache 228MB. Again, I zipped, gzipped and bzipped the file, and the bzip format was smaller by ~20%.\n\nSo I was wondering - besides the speed of writing from a compressed archive, which is slwoer than reading from a plain XML or TXT file, is there a reason why we don't use bzip/gzip when reading content? It will save a lot of space and I'm not sure that part of the indexing is what's most important.\nHowever, I'm aware that some people might find it better to read from plain files, so I suggest we just have extensions which can read/write the compressed format.\nThe question is, assuming you agree to it, should we use bzip (which requires external library) or gzip which is in the JDK, does not compress as good as bzip, but might have better performance (I can give it some measurements if needed, but the main question I have is whether we want to introduce a dependency on another library).\n\nIf this belongs in a separate issue, let me know. ",
            "author": "Shai Erera",
            "id": "comment-12697057"
        },
        {
            "date": "2009-04-08T16:45:46+0000",
            "content": "Enabling bzip compression sounds like a win; the added dependency to contrib/benchmark seems fine (it already has several external dependencies).\n\nCan you open a new issue? ",
            "author": "Michael McCandless",
            "id": "comment-12697088"
        },
        {
            "date": "2009-04-08T17:37:22+0000",
            "content": "Can you open a new issue?\n\nWill do. ",
            "author": "Shai Erera",
            "id": "comment-12697117"
        },
        {
            "date": "2009-04-08T21:24:44+0000",
            "content": "Above mentioned issues fixed.\n\nIt seems a bit awkward that DeleteByPercentTask needs to call\nIR.undeleteAll before executing the deletes. Also that\nsubsequent delete by percent calls in deletepercent.alg need to\nopen the latest version of the index rather than the original\n(which does not have deletes). This is due to\nDirectoryIndexReader.acquireWriteLock checking to insure the\nlatest version of the index is locked. Perhaps we can relax\nthis? I would rather be able to open a commit point and delete\nfrom the reader, then flush as the latest version.\n\nPerhaps in flexible indexing we can have more customizability\nwith the versioning?  ",
            "author": "Jason Rutherglen",
            "id": "comment-12697219"
        },
        {
            "date": "2009-04-09T10:16:52+0000",
            "content": "This patch still has some noise, eg the unused *Property additions to PerfRunData, the nocommit \"first\" logic in ReadTask.\n\nOn DeleteTaskByPercentTask: should it delete a pctg of the undeleted (numDocs()) docs or of the total (maxDoc()) doc space?  Right now its implementation is dangerous, eg, if I delete 5% of the index and then 10%, that 10% delete will do nothing, since the docs it deletes will fall onto the exact docs that the 5% had deleted.\n\n\nIt seems a bit awkward that DeleteByPercentTask needs to call\nIR.undeleteAll before executing the deletes.\n\nOh, I see.  I don't think it should do that?  I think it should mean \"delete XXX% of the remaining undeleted docs\"?\n\n\nAlso that\nsubsequent delete by percent calls in deletepercent.alg need to\nopen the latest version of the index rather than the original\n(which does not have deletes)\n\nThis seems correct?  Ie the purpose of this task is \"open the latest commit on the index, delete XXX% of its undeleted docs\".\n\n\nThis is due to\nDirectoryIndexReader.acquireWriteLock checking to insure the\nlatest version of the index is locked. Perhaps we can relax\nthis? I would rather be able to open a commit point and delete\nfrom the reader, then flush as the latest version.\nI don't think we can relax that.  This (single transaction (writer) open at once) is a core assumption in Lucene. ",
            "author": "Michael McCandless",
            "id": "comment-12697468"
        },
        {
            "date": "2009-04-23T00:48:41+0000",
            "content": "\nI think it should mean \"delete XXX% of the remaining\nundeleted docs\"?\n\nYeah? Ok. So the deleteDocsByPercent method needs to somehow\ntake into account whether it's deleted before by adjusting the\ndoc nums it's deleting?\n\n\nI don't think we can relax that. This (single transaction\n(writer) open at once) is a core assumption in Lucene.\n\nTrue, however doesn't mean we have to stick with it, especially\ninternally. Hopefully we can move to a more componentized model\nsomeone could change this if they wanted. Perhaps in the\nflexible indexing revamp?\n\n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12701768"
        },
        {
            "date": "2009-04-24T16:50:40+0000",
            "content": "\n\nYeah? Ok. So the deleteDocsByPercent method needs to somehow\ntake into account whether it's deleted before by adjusting the\ndoc nums it's deleting?\n\nHow about randomly choosing docs to delete instead of every N?  Then\nyou don't need to keep track?\n\n\n> I don't think we can relax that. This (single transaction\n> (writer) open at once) is a core assumption in Lucene.\n\nTrue, however doesn't mean we have to stick with it, especially\ninternally. Hopefully we can move to a more componentized model\nsomeone could change this if they wanted. Perhaps in the\nflexible indexing revamp\n\nWe'd need to figure out how to get multiple writers to properly\n\"cooperate\".  Actually Marvin is working on something like this (for\nKS/Lucy), where one \"lightweight\" writer can do adds/deletes/small\nmerges, and a separate \"heavyweight\" writer does large merges. ",
            "author": "Michael McCandless",
            "id": "comment-12702436"
        },
        {
            "date": "2009-06-11T12:58:25+0000",
            "content": "Jason this patch seems close... are you gonna have time/itch to finish this soonish? ",
            "author": "Michael McCandless",
            "id": "comment-12718440"
        },
        {
            "date": "2009-06-11T18:16:39+0000",
            "content": "It would be good to get done, we need the deletes to randomly delete, or maybe just delete only docs that aren't already deleted?  (i.e. the loop tries to delete at a pos, if it's already deleted, try the next spot, etc). ",
            "author": "Jason Rutherglen",
            "id": "comment-12718575"
        },
        {
            "date": "2009-06-11T18:30:26+0000",
            "content": "Right, I think deleteDocsByPercent should 1) determine how many docs to delete (deletePct * reader.numDocs()), and then 2) random select ones to delete, counting how many actually were deleted, and stopping when it reaches the target.  To avoid this taking excessively long when too many deletions are requested, you should probably invert if the %tg is > 50?  Ie, choose instead the docs NOT to delete, and then make a linear sweep to delete any docs not chosen? ",
            "author": "Michael McCandless",
            "id": "comment-12718578"
        },
        {
            "date": "2009-06-12T02:05:40+0000",
            "content": "Changed the deletes to be random, cleaned up the code.\n\nMultiple passes of deletePercent.alg fails, I may have time to figure out why, as is though the patch works. ",
            "author": "Jason Rutherglen",
            "id": "comment-12718697"
        },
        {
            "date": "2009-06-12T17:02:05+0000",
            "content": "Keeps previous deletes (doesn't call undeleteall).  When existing deletes are over 50%, we loop through termdocs instead. ",
            "author": "Jason Rutherglen",
            "id": "comment-12718897"
        },
        {
            "date": "2009-06-12T17:03:49+0000",
            "content": "Thanks Jason, getting close:\n\n\n\tCan you add contrib/benchmark/CHANGES entry?\n\n\n\n\n\tThe new source files need a copyright header\n\n\n\n\n\tCan you remove the undeleteAll?  I don't think the\n    DeleteByPercentTask should do that.\n\n\n\n\n\tCan you make its param a real percent, ie so DeleteByPercent(25)\n    deletes 25% of the remaining docs.\n\n\n\n\n\tThe random-pick is going to be too slow once too many docs are\n    deleted (I mentioned this above, too).  How to fix?\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12718899"
        },
        {
            "date": "2009-06-12T17:13:46+0000",
            "content": "When existing deletes are over 50%, we loop through termdocs instead.\n\nOK good, except it's deleting too aggressively when > 50% deletions are already present (using nextBoolean()).  Can you change that to target a certain deletion rate?  Ie if you need to delete 20%, then do random.nextDouble() < 0.20 to do the delete?  But then I guess put a floor on that rate so that it doesn't get too slow on the \"tail\"?  It won't be perfectly random when it hits that tail but I think that's OK. ",
            "author": "Michael McCandless",
            "id": "comment-12718907"
        },
        {
            "date": "2009-06-12T17:38:22+0000",
            "content": "Implemented the changes.  Wasn't sure how to floor it.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12718914"
        },
        {
            "date": "2009-06-12T19:48:24+0000",
            "content": "Updated patch:\n\n\n\tSwitched to TermDocs to pick the deletes; I think this is sufficient (no floor is needed)\n\n\n\n\n\tBeefed up CHANGES\n\n\n\n\n\tAdded a few more copyrights\n\n\n\nI think it's ready to commit!  I'll wait a day or two... ",
            "author": "Michael McCandless",
            "id": "comment-12718954"
        },
        {
            "date": "2009-06-12T19:58:46+0000",
            "content": "The only small thing that came to mind is if the user decides to\nsubsequently (in the .alg) delete a lesser percentage of docs\nthan the what exists in the reader. Does that mean we should\nundelete docs? ",
            "author": "Jason Rutherglen",
            "id": "comment-12718957"
        },
        {
            "date": "2009-06-12T20:04:18+0000",
            "content": "I'd say we don't allow that now.  EG one can easily save & open a past commit point, with less deletions?\n\nBut maybe we should throw an exception if you attempt this, so you don't falsely think it worked.  I'll make that change. ",
            "author": "Michael McCandless",
            "id": "comment-12718960"
        },
        {
            "date": "2009-06-12T20:05:20+0000",
            "content": "Or... maybe we should just do undeleteAll all that case?  I'll take that approach instead. ",
            "author": "Michael McCandless",
            "id": "comment-12718961"
        },
        {
            "date": "2009-06-12T20:47:52+0000",
            "content": "Added undelete all if you try to delete to an absolute pct less than the current deletions. ",
            "author": "Michael McCandless",
            "id": "comment-12718971"
        },
        {
            "date": "2009-06-14T10:58:44+0000",
            "content": "Attached new patch; fixed a bunch of silly issues (eg we had broken\nparsing of the readOnly option to OpenReaderTask; the\ndeletepercent.alg was opening readOnly readers to do the deletes; the\nreadOnly option was ignored if you specified userData; etc.).\n\nI also switched the default for autoCommit to false, when creating an\nIndexWriter.\n\nI think it's ready to commit... I'll commit soon. ",
            "author": "Michael McCandless",
            "id": "comment-12719248"
        },
        {
            "date": "2009-06-14T17:08:11+0000",
            "content": "Thanks Jason! ",
            "author": "Michael McCandless",
            "id": "comment-12719305"
        },
        {
            "date": "2009-06-19T00:39:11+0000",
            "content": "I think it would be convenient to allow passing in the data files' absolute path, instead of assuming they're in a relative path.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12721586"
        },
        {
            "date": "2009-06-19T09:29:41+0000",
            "content": "Where are we assuming/requiring the path be relative? ",
            "author": "Michael McCandless",
            "id": "comment-12721705"
        },
        {
            "date": "2009-06-22T18:16:59+0000",
            "content": "Took a look at ANT in Action at Borders and learned the -Dproperty passed in overrides what's in the build.xml.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12722738"
        }
    ]
}