{
    "id": "LUCENE-3080",
    "title": "cutover highlighter to BytesRef",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/highlighter"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Won't Fix",
        "status": "Closed"
    },
    "description": "Highlighter still uses char[] terms (consumes tokens from the analyzer as char[] not as BytesRef), which is causing problems for merging SOLR-2497 to trunk.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2011-05-07T12:16:12+0000",
            "content": "This issue blocks merging SOLR-2497, because as NumericFields have now a stored field content which is no longer binary in Solr, SolrDefaultHighlighter wants to \"highlight\" the field. For that it passes Document.getValues() [which are now \"casted\" string representations of the real NumericField for backwards reasons - see explanation about this in LUCENE-3065] to highlighter using the TokenStream that it gets from the SchemaField - which is a NumericTokenStream. Highlighter then fails with IllegalArggumentEx because numeric fields are binary only and dont support CharTermAttribute.\n\nI fixed this in the linked issue by adding some \"hack\" , so please remember to fix that correctly in Solr once this is fixed (maybe remove; not sure), DefaultSolrHighlighter line 402ff.:\n\n\nprivate void doHighlightingByHighlighter( Query query, SolrQueryRequest req, NamedList docSummaries,\n      int docId, Document doc, String fieldName ) throws IOException {\n    final SolrIndexSearcher searcher = req.getSearcher();\n    final IndexSchema schema = searcher.getSchema();\n    \n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n    ...\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-13030332"
        },
        {
            "date": "2011-05-07T12:20:00+0000",
            "content": "I'm sorry, I don't think we should change the highlighter to use BytesRef.\n\nNot unless offsets are changed to byte offsets to fit, otherwise this will make highlighting even more complicated for no good reason. ",
            "author": "Robert Muir",
            "id": "comment-13030333"
        },
        {
            "date": "2011-05-07T12:35:27+0000",
            "content": "In the past this was not an issue, because stored numeric fields in Solr were only binary, but now they are consumed by the highlighter component.\n\nIf we dont fix highlighter I have no problem. We should add some checks in highlighter to prevent NumericFields from beeing highlighted (e.g. if the TokenStream does not support CTA, it should stop highlighting). The above hack was just faster to implement in Solr to not slow down merging (because index formats from 3.x and trunk are incompatible now, so trunk needs to be committed ASAP).\n\nThis is why I opened this issue. Just to record that its a little bit inconsistent. But of course your response is true, it may make more problems. ",
            "author": "Uwe Schindler",
            "id": "comment-13030339"
        },
        {
            "date": "2011-05-07T12:44:43+0000",
            "content": "Right, because highlighters are designed for doing a lot of high-level things on text to bring out (hopefully good) snippets, e.g. various segmentation and fragmenting and whatever they do, I think it would be best if they continued to work on char[]. This makes it easier for people to extend the APIs.\n\nI agree with the idea of somehow adding a check or assert to prevent Numerics from being highlighted... I don't think it makes sense to highlight NumericFields. ",
            "author": "Robert Muir",
            "id": "comment-13030340"
        },
        {
            "date": "2011-06-22T13:22:55+0000",
            "content": "There could be a good reason though for using byte-offsets in highlighting. I have in mind an optimization that would pull in text from an external file or other source, enabling highlighting without stored fields.  For best performance the snippet should be pulled from the external source using random access to storage, but this requires byte offsets.  I think this might be a big win for large field values.\n\nThis could only be done if the highlighter doesn't need to perform any text manipulation itself, so it's not really appropriate for Highlighter, as Robert said, but in the case of FVH it might be possible to implement.  I'm looking at this, but wondering before I get too deep in if anyone can comment on the feasibility of using byte offsets - I'm unclear on what they get used for other than highlighting: would it cause problems to have a CharFilter that returns \"corrected\" offsets such that char positions in the analyzed text are translated into byte positions in the source text?  ",
            "author": "Mike Sokolov",
            "id": "comment-13053245"
        },
        {
            "date": "2011-06-22T13:41:18+0000",
            "content": "Mike, its an interesting idea, as I think the offsets are intended to be opaque to the app (so you should be able to use byte offsets if you want).\n\nThere are some problems though, especially tokenfilters that muck with offsets:\nNGramTokenFilter, WordDelimiterFilter, ...\n\nIn general there are assumptions here that offsets are utf16. ",
            "author": "Robert Muir",
            "id": "comment-13053254"
        },
        {
            "date": "2011-06-22T13:48:26+0000",
            "content": "not trying to scare you off from that byte-offset-converting-charfilter idea with my comment above either, I think this would be interesting to fix! maybe we just need to fix the situation so that tokenfilters can pass down the call to 'correctOffset' and then use it, I think it might only be defined on Tokenizer... ",
            "author": "Robert Muir",
            "id": "comment-13053260"
        },
        {
            "date": "2011-06-22T14:07:11+0000",
            "content": "It might be a bit more complicated?  Looks like WordDelimiterFilter, in generatePart and concatenate, eg, performs computation with the offsets.  So it would either need to know the units of the offsets it was passed, or be given more than just a correctOffset() method: rather it seems to require something like addCharsToOffset (offset, charOffsetIncr), where charOffsetIncr is a number of chars, but offset is in some unspecified unit. ",
            "author": "Mike Sokolov",
            "id": "comment-13053275"
        },
        {
            "date": "2011-06-22T14:16:33+0000",
            "content": "yes: in general I think it would be problematic, especially since most tests use only all-ascii data.\n\nAnother problem on this issue is that if you want to use bytes, but with the Tokenizer-analysis-chain, it only takes Reader, so you cannot assume anything about the original bytes or encoding (e.g. that its UTF-8 for example).\n\n\n ",
            "author": "Robert Muir",
            "id": "comment-13053281"
        },
        {
            "date": "2011-06-22T15:06:03+0000",
            "content": "Yeah I knew that at some point, but stuffed it away as something to think about later  There really is no way to pass byte streams into the analysis chain.  Maybe providing a character encoding to the filter could enable it to compute the needed byte offsets.  ",
            "author": "Mike Sokolov",
            "id": "comment-13053300"
        },
        {
            "date": "2011-06-22T15:49:56+0000",
            "content": "Well, personally i am hesitant to introduce any encodings or bytes into our current analysis chain, because its unnecessary complexity that will introduce bugs (at the moment, its the users responsibility to create the appropriate Reader etc).\n\nFurthermore, not all character sets can be 'corrected' with a linear conversion like this: for example some actually order the text in a different direction, and things like that... there are many quirks to non-unicode character sets.\n\nMaybe as a start, it would be useful to prototype some simple experiments with a \"binary analysis chain\" and hackup a highlighter to work with them? This way we would have an understanding of what the potential performance gain is.\n\nHere's some example code for a dead simple binary analysis chain that only uses bytes the whole way through, you could take these ideas and prototype one with just all ascii-terms and split on the space byte and such:\nhttp://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/src/test/org/apache/lucene/index/TestBinaryTerms.java\nhttp://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/src/test/org/apache/lucene/index/BinaryTokenStream.java  ",
            "author": "Robert Muir",
            "id": "comment-13053319"
        },
        {
            "date": "2011-06-22T20:08:44+0000",
            "content": "I agree it's necessary to prove there is some point to all this - I'm working on getting some numbers.  At the moment I'm just assuming ASCII encoding, but I'll take a look at the binary stuff too - thanks. ",
            "author": "Mike Sokolov",
            "id": "comment-13053432"
        },
        {
            "date": "2013-08-09T15:26:27+0000",
            "content": "Hmmm... there hasn't been any activity on this issue for over two years, but the code in branch_4x for org.apache.solr.highlight.DefaultSolrHighlighter#doHighlightingByHighlighter has this TODO comment and \"hack\" code:\n\n\n    // TODO: Currently in trunk highlighting numeric fields is broken (Lucene) -\n    // so we disable them until fixed (see LUCENE-3080)!\n    // BEGIN: Hack\n    final SchemaField schemaField = schema.getFieldOrNull(fieldName);\n    if (schemaField != null && (\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieField) ||\n      (schemaField.getType() instanceof org.apache.solr.schema.TrieDateField)\n    )) return;\n    // END: Hack\n\n\n\nAnd, there is no note in the Solr highlighting wiki/refguide concerning this released \"hack\". ",
            "author": "Jack Krupansky",
            "id": "comment-13734900"
        },
        {
            "date": "2014-12-12T12:07:54+0000",
            "content": "I asked a question about this in other issue: https://issues.apache.org/jira/browse/SOLR-2497?focusedCommentId=14244044&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14244044 ",
            "author": "Pawel Rog",
            "id": "comment-14244045"
        },
        {
            "date": "2016-11-25T15:17:00+0000",
            "content": "I'm tempted to close this as Won't-Fix.\nThe original Highlighter is what it is; it has only been updated to support different queries in WeightedSpanTermExtractor.\n\nThe notion of switching to byte based offsets in the analysis chain together with highlighting using that is interesting!  It could in part accelerate highlighting massive docs to avoid reading massive Strings into memory.  But I feel a separate issue should be filed for that, starting just with changes to the analysis chain.  If we do that, I suppose this would become metadata on the FieldInfo so it's understood what kind of offsets are stored, Java char offsets, or byte offsets.  That way the highlighter could validate this assumption up instead of having subtle errors.\n\nHighlighting numerics, IMO, should also be a separate issue.  It simply doesn't match the title of this issue, nor large parts of what is being discussed here.  It'd be cool to add support for highlighting numbers into the UnifiedHighlighter; I've thought through how to do that before. ",
            "author": "David Smiley",
            "id": "comment-15696088"
        }
    ]
}