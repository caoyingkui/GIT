{
    "id": "LUCENE-2745",
    "title": "ArabicAnalyzer - the ability to recognise email addresses host names and so on",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "2.9.2,                                            2.9.3,                                            3.0,                                            3.0.1,                                            3.0.2",
        "resolution": "Later",
        "status": "Closed"
    },
    "description": "The ArabicAnalyzer does not recognise email addresses, hostnames and so on. For example,\n\nadam@hotmail.com\n\nwill be tokenised to [adam] [hotmail] [com]\n\nIt would be great if the ArabicAnalyzer can tokenises this to [adam@hotmail.com]. The same applies to hostnames and so on.\n\nCan this be resolved? I hope so\n\nThanks\n\nMAA",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2010-11-07T16:16:45+0000",
            "content": "I think that ArabicLetterTokenizer, which is the tokenizer used by ArabicAnalyzer, is obsolete (as of version 3.1), since StandardTokenizer, which implements the Unicode word segmentation rules from UAX#29, should be able to properly tokenize Arabic.  StandardTokenizer recognizes email addresses, hostnames, and URLs, so your concern would be addressed.  (See LUCENE-2167, though, which was just reopened to turn off full URL output.)\n\nYou can test this by composing your own analyzer, if you're willing to try using using as-yet-unreleased branch_3X, from which 3.1 will be cut (hopefully fairly soon): just copy ArabicAnalyzer class and swap in StandardTokenizer for ArabicLetterTokenizer ",
            "author": "Steve Rowe",
            "id": "comment-12929362"
        },
        {
            "date": "2010-11-07T16:27:30+0000",
            "content": "Thanks Steven. I will give it a go and will share the outcome. ",
            "author": "M Alexander",
            "id": "comment-12929364"
        },
        {
            "date": "2010-11-07T16:35:31+0000",
            "content": "I agree with what Steven said here... since previously StandardTokenizer would break on diacritics (shadda, etc)\nit wasn't appropriate for arabic writing systems, so we added ArabicLetterTokenizer as a workaround.\n\nbut you can use a different tokenizer in your own Analyzer to meet your needs... and we should try to avoid \n(deprecate+remove) language-specific tokenizers if we can.\n\nthe only trick to deprecating this ArabicLetterTokenizer is the persian case, since i dont think UAX#29 will split on\nzero-width-non-joiner, so we need to do something to handle that case, otherwise we can default to a better tokenizer here. ",
            "author": "Robert Muir",
            "id": "comment-12929365"
        },
        {
            "date": "2010-11-07T16:42:35+0000",
            "content": "Yes Robert, I have faced the diacritics problem. I am trying to have an Analyzer that would not break on diacritics as well as recognising email addresses, hostnames and so on (which Arabic text may contain). This is why I asked the question to see if there is a way to have full Arabic analysis (including diacritics) along with recognising email addresses, hostnames, etc at the same Analyzer. I will try your suggestions and will share the output. Thanks Robert for your help ",
            "author": "M Alexander",
            "id": "comment-12929367"
        },
        {
            "date": "2010-11-07T19:20:54+0000",
            "content": "the only trick to deprecating this ArabicLetterTokenizer is the persian case, since i dont think UAX#29 will split on zero-width-non-joiner, so we need to do something to handle that case, otherwise we can default to a better tokenizer here.\n\nRobert, can you provide more detail?  AFAICT from this Wikipedia article, ZWNJs are used in Persian as display hints, not as word separators. ",
            "author": "Steve Rowe",
            "id": "comment-12929386"
        },
        {
            "date": "2010-11-07T19:46:19+0000",
            "content": "steven, check out the link at the bottom of that article.\nespecially the top... it explains the use in the language,\nparticularly to block cursive joining for prefixes, suffixes,\ncompounds. we split on this and the affixes are in the stoplist\n\nthis is how the whole analyzer works, more examples in\nthe tests... I can give you more refs later, when I have\nbetter bandwidth... but its specific to this language.\nwe shouldn't split on it in general... also often a real\nspace is used instead, so this approach is the simplest\nfor the language ",
            "author": "Robert Muir",
            "id": "comment-12929389"
        },
        {
            "date": "2010-11-07T20:14:17+0000",
            "content": "steven, check out the link at the bottom of that article.\n\nYup, did that.\n\nespecially the top... it explains the use in the language, particularly to block cursive joining for prefixes, suffixes, compounds. we split on this and the affixes are in the stoplist \n\nUm, like I said, Persian uses ZWNJs as display hints, not as word separators.\n\nAccording to the ICU web demo, ZWNJs have the \\p\n{Word_Break:Extend}\n property, so the Lucene UAX#29-based tokenizers will not split on this char.\n\nWhat am I not getting? ",
            "author": "Steve Rowe",
            "id": "comment-12929392"
        },
        {
            "date": "2010-11-07T21:06:26+0000",
            "content": "\nthis is how the whole analyzer works, more examples in\nthe tests... I can give you more refs later, when I have\nbetter bandwidth... but its specific to this language.\nwe shouldn't split on it in general... also often a real\nspace is used instead, so this approach is the simplest\nfor the language\n\nAFAICT, ArabicLetterTokenizer just adds non-spacing marks to the list of acceptable token characters, so they won't be used to split words.  However, ZWNJ (U+200C) has the \"Cf\" \u2013 Format \u2013 general category, not the \"Mn\" general category (non-spacing marks), so as far as I can tell, the current Lucene ArabicLetterTokenizer (and hence ArabicAnalyzer) splits on ZWNJ.\n\nNone of the tests in TestArabicLetterTokenizer nor in TestArabicAnalyzer contain ZWNJ (U+200C).\n\nMaybe what I'm not understanding is \"this approach\" in your quote above.  Can you describe \"this approach\"?\n\nWhen you wrote \"we split on this and the affixes are in the stoplist\" did you mean that ArabicLetterTokenizer intentionally breaks Persian words at ZWNJ?  And then throws away the affixes that result?  Hunh???? ",
            "author": "Steve Rowe",
            "id": "comment-12929396"
        },
        {
            "date": "2010-11-07T21:24:11+0000",
            "content": "Hunh????\n\nOkay, I think I get it now.  \n\nI did a search for U+200C in the whole Lucene project, and I found TestPersianAnalyzer.\n\nApparently, Robert, when you said \"the whole analyzer\" and \"this approach\" you meant PersianAnalyzer, rather than ArabicAnalyzer.  Sorry for the confusion.\n\nWhat do you think the approach should be for Persian?  Maybe a StandardTokenizer clone that excludes ZWNJ from the \\p\n{Word_Break:Extend}\n class that gets added to every rule?  I'll see if there is some way to compose a PersianTokenizer.jflex (using the %include directive maybe?) using StandardTokenizerImpl.jflex, so that we don't end up with code duplication. ",
            "author": "Steve Rowe",
            "id": "comment-12929400"
        },
        {
            "date": "2010-11-07T22:25:47+0000",
            "content": "yes, sorry for the confusion!\n\none solution, add a charfilter that maps zwnj to space for persiananalyzer?\n\nthis way, it could use uax29 and support numerics etc ",
            "author": "Robert Muir",
            "id": "comment-12929403"
        },
        {
            "date": "2010-11-07T22:40:18+0000",
            "content": "\none solution, add a charfilter that maps zwnj to space for persiananalyzer?\n\nthis way, it could use uax29 and support numerics etc\n\nI like it - it sounds better than my other idea: a configurable token splitting filter. ",
            "author": "Steve Rowe",
            "id": "comment-12929405"
        },
        {
            "date": "2010-11-08T12:30:11+0000",
            "content": "\nI think that ArabicLetterTokenizer, which is the tokenizer used by ArabicAnalyzer, is obsolete (as of version 3.1), since StandardTokenizer, which implements the Unicode word segmentation rules from UAX#29, should be able to properly tokenize Arabic. StandardTokenizer recognizes email addresses, hostnames, and URLs, so your concern would be addressed. (See LUCENE-2167, though, which was just reopened to turn off full URL output.) \nYou can test this by composing your own analyzer, if you're willing to try using using as-yet-unreleased branch_3X, from which 3.1 will be cut (hopefully fairly soon): just copy ArabicAnalyzer class and swap in StandardTokenizer for ArabicLetterTokenizer \n\nI tried to test this and failed (miserably). I think I struggled to patch LUCENE-2167 correctly through my eclipse. I might just wait for branch_3X release to make my life easier. I will then create my own Analyzer to perform Arabic Text Analysis and another one for Farsi Text Analysis. Both Analyzers will have the ability to handle diacritics as well as email addresses, hostnames and so on. I will colse this issue for now (will re-open in the future if needed).\n\nQuick question - any thoughts of handling Arabic email addresses and hostnames in the future?\n\nThanks to both of you for the time taken and I shall wait for the branch release to solve my issue. ",
            "author": "M Alexander",
            "id": "comment-12929556"
        },
        {
            "date": "2010-11-08T12:31:52+0000",
            "content": "Will wait for the relaese, which should have the solution within ",
            "author": "M Alexander",
            "id": "comment-12929557"
        },
        {
            "date": "2010-11-08T12:33:04+0000",
            "content": "Oh, do you have a rough timing of the branch_3X release date? ",
            "author": "M Alexander",
            "id": "comment-12929558"
        },
        {
            "date": "2010-11-08T13:34:13+0000",
            "content": "Oh, do you have a rough timing of the branch_3X release date? \n\nWild guess: January 2011 ",
            "author": "Steve Rowe",
            "id": "comment-12929566"
        },
        {
            "date": "2010-11-10T22:23:21+0000",
            "content": "Quick question - how difficult is it to make the new StandardTokenizer (branch_3X) with its new capabilities (including properly tokenizing Arabic as well as identifying email addresses, hostnames, etc) to work with version 2.9.2?\n\nIs it very difficult, or would it only require copying across few classes and minor tweaks? ",
            "author": "M Alexander",
            "id": "comment-12930814"
        },
        {
            "date": "2010-11-11T00:16:49+0000",
            "content": "how difficult is it to make the new StandardTokenizer (branch_3X) with its new capabilities (including properly tokenizing Arabic as well as identifying email addresses, hostnames, etc) to work with version 2.9.2? \n\nYou wouldn't be able to just drop the files in and compile, but backporting to 2.9.X would definitely be possible.\n\nHere are the things I found looking through CHANGES.txt on branch_3x that would require attention if you were to backport to 2.9.2:\n\n\n\tLUCENE-2302: TermAttribute -> CharTermAttribute\n\tLUCENE-2074: Java4 -> Java5 regeneration of StandardTokenizerImpl* from .jflex source; support for different behavior based on Lucene Version\n\n\n\nThere are probably some other things, not sure what.\n\nLikely LUCENE-2302 is the biggest issue (it will block compilation), but if I remember correctly, the change is fairly simple. ",
            "author": "Steve Rowe",
            "id": "comment-12930857"
        },
        {
            "date": "2010-11-11T06:23:27+0000",
            "content": "Likely LUCENE-2302 is the biggest issue (it will block compilation), but if I remember correctly, the change is fairly simple.\n\nSimply use only the now-deprecated TermAttribute instead of backporting this issue. For StandardTokenizer this should be simple, just replace some methods like copyBuffer() to setTermBuffer() and replace the addAttributes and remove Generics, but add casts. ",
            "author": "Uwe Schindler",
            "id": "comment-12930943"
        },
        {
            "date": "2010-11-11T15:46:52+0000",
            "content": "\nLikely LUCENE-2302 is the biggest issue (it will block compilation), but if I remember correctly, the change is fairly simple.\n\nSimply use only the now-deprecated TermAttribute instead of backporting this issue. For StandardTokenizer this should be simple, just replace some methods like copyBuffer() to setTermBuffer() and replace the addAttributes and remove Generics, but add casts.\n\nYes, sorry, I meant what Uwe is saying here - there is no (good) reason to backport LUCENE-2302 to 2.9.X. ",
            "author": "Steve Rowe",
            "id": "comment-12931046"
        },
        {
            "date": "2010-11-14T15:41:15+0000",
            "content": "I tried to follow Steve's first respons and ended up with the expected compilation issue. I then decided to simplify my approach and did pretty much what Uwe has suggested - all is working now. Thanks all for your prompt help - much appreciated ",
            "author": "M Alexander",
            "id": "comment-12931836"
        }
    ]
}