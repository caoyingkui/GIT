{
    "id": "LUCENE-1422",
    "title": "New TokenStream API",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "New Feature",
        "fix_versions": [
            "2.9"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "This is a very early version of the new TokenStream API that \nwe started to discuss here:\n\nhttp://www.gossamer-threads.com/lists/lucene/java-dev/66227\n\nThis implementation is a bit different from what I initially\nproposed in the thread above. I introduced a new class called\nAttributedToken, which contains the same termBuffer logic \nfrom Token. In addition it has a lazily-initialized map of\nClass<? extends Attribute> -> Attribute. Attribute is also a\nnew class in a new package, plus several implementations like\nPositionIncrementAttribute, PayloadAttribute, etc.\n\nSimilar to my initial proposal is the prototypeToken() method\nwhich the consumer (e. g. DocumentsWriter) needs to call.\nThe token is created by the tokenizer at the end of the chain\nand pushed through all filters to the end consumer. The \ntokenizer and also all filters can add Attributes to the \ntoken and can keep references to the actual types of the\nattributes that they need to read of modify. This way, when\nboolean nextToken() is called, no casting is necessary.\n\nI added a class called TestNewTokenStreamAPI which is not \nreally a test case yet, but has a static demo() method, which\ndemonstrates how to use the new API.\n\nThe reason to not merge Token and TokenStream into one class \nis that we might have caching (or tee/sink) filters in the \nchain that might want to store cloned copies of the tokens\nin a cache. I added a new class NewCachingTokenStream that\nshows how such a class could work. I also implemented a deep\nclone method in AttributedToken and a \ncopyFrom(AttributedToken) method, which is needed for the \ncaching. Both methods have to iterate over the list of \nattributes. The Attribute subclasses itself also have a\ncopyFrom(Attribute) method, which unfortunately has to down-\ncast to the actual type. I first thought that might be very\ninefficient, but it's not so bad. Well, if you add all\nAttributes to the AttributedToken that our old Token class\nhad (like offsets, payload, posIncr), then the performance\nof the caching is somewhat slower (~40%). However, if you \nadd less attributes, because not all might be needed, then\nthe performance is even slightly faster than with the old API.\nAlso the new API is flexible enough so that someone could\nimplement a custom caching filter that knows all attributes\nthe token can have, then the caching should be just as \nfast as with the old API.\n\n\nThis patch is not nearly ready, there are lot's of things \nmissing:\n\n\n\tunit tests\n\tchange DocumentsWriter to use new API\n  (in backwards-compatible fashion)\n\tpatch is currently java 1.5; need to change before\n  commiting to 2.9\n\tall TokenStreams and -Filters should be changed to use\n  new API\n\tjavadocs incorrect or missing\n\thashcode and equals methods missing in Attributes and\n  AttributedToken\n\n\n\nI wanted to submit it already for brave people to give me \nearly feedback before I spend more time working on this.",
    "attachments": {
        "lucene-1422.take2.patch": "https://issues.apache.org/jira/secure/attachment/12392178/lucene-1422.take2.patch",
        "lucene-1422.take3.patch": "https://issues.apache.org/jira/secure/attachment/12392549/lucene-1422.take3.patch",
        "lucene-1422-take4.patch": "https://issues.apache.org/jira/secure/attachment/12392966/lucene-1422-take4.patch",
        "lucene-1422-take5.patch": "https://issues.apache.org/jira/secure/attachment/12393760/lucene-1422-take5.patch",
        "lucene-1422.patch": "https://issues.apache.org/jira/secure/attachment/12392076/lucene-1422.patch",
        "lucene-1422-take6.patch": "https://issues.apache.org/jira/secure/attachment/12394026/lucene-1422-take6.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-10-14T17:16:09+0000",
            "content": "prototypeToken() and nextToken() seem not the most descriptive names here.  Perhaps getToken() and incrementToken() would be better?  Also, would it be better to make the prototypeToken/getToken implementations idempotent?  This might then look like, e.g.:\n\npublic AttributedToken getToken() throws IOException {\n  if (token == null) {\n    token = input.getToken();\n    positionIncrement = new PositionIncrementAttribute();\n    token.addAttribute(positionIncrement);\n  }\n  return token;\n}\n\n\nI'm also not fond of the name AttributedToken, but don't yet have a better suggestion...\n ",
            "author": "Doug Cutting",
            "id": "comment-12639495"
        },
        {
            "date": "2008-10-15T11:15:31+0000",
            "content": "Thanks for your suggestions, Doug. It makes perfect sense to make getToken() idempotent.\nAlso, addAttribute() should be idempotent, because a Token can have only one instance \nof the an attribute.\n\nI changed prototypeToken() to getToken(), nextToken() to incrementToken and actually \nadded the attribute logic to Token itself. That has some advantages, but also the \nfollowing disadvantage. If people want to use the new API before 3.0, i. e. before the\ndeprecated members of Token have been removed, and they want to use something like the\nCachingTokenFilter or Tee/Sink-TokenFilter, then caching is more expensive. The reson \nis that Token itself has members for positionIncrement, offsets, etc. but you then also \nhave to add the appropriate attributes to Token to use the new API. But I think this\ndrawback would be acceptable?\n\nI also changed the way to add attributes to a Token:\n\nprotected final void addTokenAttributes() {\n  posIncrAtt = reusableToken.addAttribute(PositionIncrementAttribute.class);\n}\n\n\n\nThe addTokenAttributes() method belongs to TokenStream and is called from getToken()\nonly when a new Token instance was created, i. e. in its first call.\n\nNote the signature of Token.addAttribute:\n\npublic <T extends Attribute> T addAttribute(Class<T> attClass);\n\n\n\nNow you don't pass in an actual instance of *Attribute, but its class. The method will\nthen create a new instance via reflection. This approach makes the addAttribute() \nmethod itself idempotent.\n\nI changed all core tokenizers and filters to have an implementation of the new API.\n\nFor backwards-compatibility I added a (deprecated) static setUseNewAPI() method to \nTokenStream. I also changed the DocumentsWriter to use the new API in case \nuseNewAPI==true;\n\nI still have to do several things, including javadocs, testcases, hashcode(), etc. ",
            "author": "Michael Busch",
            "id": "comment-12639811"
        },
        {
            "date": "2008-10-21T05:19:01+0000",
            "content": "I added several things in this new patch:\n\n\n\thashCode() and equals() now incorporate the attributes\n\tpatch compiles against Java 1.4\n\tall core tests pass with and without the new API turned\n   on (via TokenStream.setUseNewAPI(true))\n\tAdded setToken() method to InvertedDocConsumerPerField\n   and TermsHashConsumerPerField and updated the \n   implementing classes. I have actually a question here,\n   because I don't know these classes very well yet. Would\n   it be better to add the Token to the DocInverter.FieldInvertState?\n   I think I also have to review LUCENE-1426 to see if these\n   changes are not in conflict ( I think 1426 should be committed\n   first?)\n\n\n\nOutstanding:\n\n\tdedicated junits for new APIs, even though the existing tests\n   already cover a lot when setUseNewAPI(true)\n\tjavadocs\n\tcontrib streams and filters\n\n ",
            "author": "Michael Busch",
            "id": "comment-12641273"
        },
        {
            "date": "2008-10-21T08:48:18+0000",
            "content": "Michael I think you left out oal.analysis.tokenattributes.* from your patch?  (I'm trying to compile things). ",
            "author": "Michael McCandless",
            "id": "comment-12641309"
        },
        {
            "date": "2008-10-21T09:47:27+0000",
            "content": "Would it be better to add the Token to the DocInverter.FieldInvertState?\n\nI think it would be slightly better, since it'd save a the recursive method calls to setToken mirroring what the \"add(Token)\" is about to do anyway, but the problem is the token can be different for each Field being added.  EG, I could have a doc with 10 instances of field \"foo\", whereby each instance is doing a different analysis and so the reused token may change.  But the InvertedDocConsumer API doesn't have a start() method for each instance of a field, and so there's no clean place for each consumer to lookup the attribute(s) it needs to use?  Maybe you could add such a start method, and then switch to putting the Token into FieldInvertState?\n\n\nI think I also have to review LUCENE-1426 to see if these\nchanges are not in conflict ( I think 1426 should be committed\nfirst?)\nOK I will commit LUCENE-1426 first.  I think mostly there won't be conflicts, except in FreqProxTermsWriterPerThread which should be minor.   ",
            "author": "Michael McCandless",
            "id": "comment-12641322"
        },
        {
            "date": "2008-10-21T09:59:14+0000",
            "content": "Oups, sorry, this should work now.\n\nAnother question: Is it correct that the DocInverterPerField\nonly takes into account the endOffset(), not the startOffset()? ",
            "author": "Michael Busch",
            "id": "comment-12641329"
        },
        {
            "date": "2008-10-21T10:20:12+0000",
            "content": "\nAnother question: Is it correct that the DocInverterPerField\nonly takes into account the endOffset(), not the startOffset()?\n\nI think it's correct, if rather confusing: DocInverterPerField tracks endOffset only for shifting the offsets of Tokens when a document has more than one Fieldable instance per textual field.  (Other consumers, like FreqProxTermsWriter do need to pay attention to both the start & end offset). ",
            "author": "Michael McCandless",
            "id": "comment-12641334"
        },
        {
            "date": "2008-10-21T19:08:16+0000",
            "content": "\nI think it would be slightly better, since it'd save a the recursive method calls to setToken mirroring what the \"add(Token)\" is about to do anyway, but the problem is the token can be different for each Field being added. EG, I could have a doc with 10 instances of field \"foo\", whereby each instance is doing a different analysis and so the reused token may change. But the InvertedDocConsumer API doesn't have a start() method for each instance of a field, and so there's no clean place for each consumer to lookup the attribute(s) it needs to use? Maybe you could add such a start method, and then switch to putting the Token into FieldInvertState?\n\nYeah I agree, that's better. I should probably add a start(Fieldable) method? I currently don't need to access the Fieldable instance in the start method, but it can't hurt to pass it along? ",
            "author": "Michael Busch",
            "id": "comment-12641530"
        },
        {
            "date": "2008-10-21T19:29:58+0000",
            "content": "I should probably add a start(Fieldable) method? I currently don't need to access the Fieldable instance in the start method, but it can't hurt to pass it along?\n\nI think that's the right approach! ",
            "author": "Michael McCandless",
            "id": "comment-12641549"
        },
        {
            "date": "2008-10-21T20:15:39+0000",
            "content": "\nI think that's the right approach!\n\nOK done.  Works great...\n\nAnother question. Currently the members in Token, like positionIncrement are deprecated with a comment that they will be made private. We should be able to do so in the next release, i. e. 2.9, right? It's not a public or protected API, so there shouldn't be the need to wait for 3.0 based on our backwards-compatibility policy. Or did I miss a discussion behind this, maybe because Token is a class that many people extend in the o.a.l.a package and we agreed to make an exception here?\n\nThe reason why I'm asking is Grant's suggestion of putting all default Attributes into Token and having methods like getPositionIncrement() return the value from the corresponding PositionIncrementAttribute. It won't be possible to do that if I can't remove those mentioned members, unless I subclass Token or with a performance hit, if I do something like this for example:\n\npublic int getPositionIncrement() {\n  if (this.positionIncrementAttribute != null) {\n    return this.positionIncrementAttribute.getPositionIncrement();\n  else {\n    return this.positionIncrement;\n  }\n}\n\n\n\nSeems inefficient and messy.\n\nSo my question is if it's ok if I remove the deprecated members with this patch in the 2.9 release? Thoughts? ",
            "author": "Michael Busch",
            "id": "comment-12641567"
        },
        {
            "date": "2008-10-21T20:42:25+0000",
            "content": "Another question. Currently the members in Token, like positionIncrement are deprecated with a comment that they will be made private.\n\nThese members were being directly accessed in various contribs. To me that makes them part of the published API, whether intentional or not. All of the code in Lucene core and contrib now use accessors.\n\nI marked them as deprecated so that people would have time to clean up their code. My assumption was that even though it is package protected that someone might create their own \"contrib\" with their Token subclass in o.a.l.analysis. I did't think the backward compatibility policy discussed package protected and I wasn't eager to open that discussion.\n\nStrictly speaking, it does break backward compatibility.\n\nBut, it should have never had package protected members. For precisely this reason. ",
            "author": "DM Smith",
            "id": "comment-12641580"
        },
        {
            "date": "2008-10-21T20:49:07+0000",
            "content": "\nStrictly speaking, it does break backward compatibility.\n\nYes I agree. But my take here is that the package-private methods are expert methods for which we don't have to guarantee backwards-compatibility the same way we do for public and protected APIs (i. e. only break compatibility in a version change X.Y->(X+1).0). Of course we have to update all contribs. ",
            "author": "Michael Busch",
            "id": "comment-12641583"
        },
        {
            "date": "2008-10-21T21:14:09+0000",
            "content": "\nYes I agree. But my take here is that the package-private methods are expert methods for which we don't have to guarantee backwards-compatibility the same way we do for public and protected APIs (i. e. only break compatibility in a version change X.Y->(X+1).0). Of course we have to update all contribs.\n\nI don't know about that.  I don't think it has been decided one way or the other.  At one extreme, the policy says: \"That's to say, any code developed against X.0 should continue to run without alteration against all X.N releases. \"  Since, it is perfectly legal to access package methods by declaring oneself to be part of that package, and, since we don't sign our jars to prevent such a move, I can see that others may feel free to use them when they deem it beneficial.  On the other hand, convention/good programming style suggests it's not good to write programs that rely on package level APIs in libraries, so caveat emptor.\n\nI do note that we broke Luke once before when we changed something that was package scoped.  Maybe a shame on us, maybe a shame on Luke,  I don't know.\n\nThat being said, I doubt a lot of people are doing this, so...  \nI'd suggest a vote on it on dev if breaking it is decided on, like we did w/ Fieldable and that we explicitly state what is happening in CHANGES, etc.  \n\nSaid vote could either be specific to this problem, or it could attempt to address the question of whether package scoped things are subject to the back-compat. policy. ",
            "author": "Grant Ingersoll",
            "id": "comment-12641598"
        },
        {
            "date": "2008-10-21T21:18:42+0000",
            "content": "\nSaid vote could either be specific to this problem, or it could attempt to address the question of whether package scoped things are subject to the back-compat. policy.\n\nAgreed. I'd prefer the latter. I will call a vote on java-dev later today... ",
            "author": "Michael Busch",
            "id": "comment-12641600"
        },
        {
            "date": "2008-10-29T01:15:16+0000",
            "content": "Because mulitple people mentioned it would be better to merge\nTokenStream and Token into one class, I thought more about it\nand now I think I prefer that approach too. I implemented it\nand added a class TokenStreamState to capture a state of a stream\nwhich can be used for buffering of Tokens. The performance of the\nCachingTokenFilter is lower than before if all attributes that the\nToken had are used, but slightly better if fewer are used (which\nwas previously not possible). I also had some ideas of making \nbuffering of Tokens perform better, but this patch is now \nalready pretty long, so I decided to add better buffering with\na separate issue at a later point. Here is a sumamry of my \nchanges:\n\nChanges in the analysis package:\n\n\tAdded the tokenattributes subpackage, known from the previous\n  patches\n\tAdded the abstract class AttributeSource that owns the\n  attribute map and appropriate methods to get and add the\n  attributes. TokenStream now extends AttributeSource.\n\tDeprecated the Token class.\n\tAdded TokenStream#start(), TokenStream#initialize() and\n  TokenStream#incrementToken(). start() must be called by a \n  consumer before incrementToken() is called the first time.\n  start() calls initialize(), which can be used by TokenStreams\n  and TokenFilters to add or get attributes. I separated \n  start() and initialize() to enforce in TokenFilters that\n  input.start() is called. I think it would be a pitfall for\n  bugs (happened to me before I added initialize().\n\tAdded another subclass of AttributeSource called\n  TokenStreamState which can be used to capture a current state\n  of a TokenStream, e. g. for buffering purposes. I changed the\n  CachingTokenFilter and Sink/Tee-TokenFilter to make use of \n  this new class.\n\tChanged all core TokenStreams and TokenFilters to implement\n  the new methods and deprecated the next(Token) methods, but\n  left them for backwards compatibility.\n\n\n\nChanges in the indexer package:\n\n\n\tChanged DocInverterPerField.processFields to use the new API\n  if TokenStream.useNewAPI() is set to true. I also added an \n  inner class to DocInverterPerThread called \n  BackwardsCompatibilityStream that allows me to set a Token\n  and all Attributes just return the values from the token.\n  That allows me to change all consuemrs in the indexer \n  package to not use Token anymore at all, but only \n  TokenStream, without a performance hit.\n\tAdded start(Fieldable) method to InvertedDocConsumerPerField\n  and TermsHashConsumerPerField that is called to notify the \n  consumers that one field instance is now going to be \n  processed, so that they can get the attribute references\n  from DocInverter.FieldInvertState.attributeSource. Also \n  changed the signature of the add() method of the above \n  mentioned classes to not take a Token anymore.\n\n\n\nChanges in queryparser package:\n\n\tChanged QueryParser so that it uses a CachingTokenFilter\n  instead of a List to buffer tokens. \n\n\n\nTestcases:\n\n\n\tAdded class TokenStreamTestUtils to the analysis test\n  package which contains two inner helper classes:\n  BackwardsCompatibleStream and BackwardsCompatibleFilter.\n  Both overwrite TokenStream#next(Token) and call \n  incrementToken() and then copy all attribute values to the\n  Token to be returned to the caller of next(Token). That \n  simplifies it to make existing tests run in old and new API\n  mode.\n\n\n\n\nAll test cases pass with useNewAPI=true and false. I think \nthis patch is mostly done now (I just have to update \nanalysis/package.html and cleanup imports), unless we're not\nhappy with the APIs. \nPlease give me some feedback about this approach. ",
            "author": "Michael Busch",
            "id": "comment-12643394"
        },
        {
            "date": "2008-10-29T02:30:50+0000",
            "content": "\nBecause mulitple people mentioned it would be better to merge\nTokenStream and Token into one class,\n\nThey did?  I only see Doug's comment, is there another one?  Would be good to see the reasoning.  Not that I'm against what you've done, just curious.\n\n\nSome questions (I've only glanced at the patch, so I still need to apply the patch):\n\nShould useNewAPI be static?  Seems like I could want to use the new one in some cases, but not in others?????\n\nWhy the static \"constructors\" on TokenStreamState?  Why not just have constructors for that? ",
            "author": "Grant Ingersoll",
            "id": "comment-12643397"
        },
        {
            "date": "2008-10-29T02:56:01+0000",
            "content": "\nThey did? I only see Doug's comment, is there another one?\n\nMike did in LUCENE-1426. I also don't see a reason anymore to have Token as a separate class. I think the TokenStreamState for the buffering usecases is more elegant...\n\n\nShould useNewAPI be static? Seems like I could want to use the new one in some cases, but not in others?????\nwow, a lot of question marks \nGood point. I made it static so that I could set it to true or false in LuceneTestCase.java to run all tests in both modes, but you're right, it should be per TokenStream instance. So maybe a static method for the default setting, and a non-static that can be used to override it?\n\n\nWhy the static \"constructors\" on TokenStreamState? Why not just have constructors for that?\nThere is not really a reason. I thought TokenStreamState.capture(TokenStream) would look more intuitive. (the word capture indicates what happens) ",
            "author": "Michael Busch",
            "id": "comment-12643399"
        },
        {
            "date": "2008-10-29T11:23:49+0000",
            "content": "\nGood point. I made it static so that I could set it to true or false in LuceneTestCase.java to run all tests in both modes, but you're right, it should be per TokenStream instance. So maybe a static method for the default setting, and a non-static that can be used to override it?\n\nI don't think you need the static, just fold it into a constructor and have one that is a default, too.\n\n\nThere is not really a reason. I thought TokenStreamState.capture(TokenStream) would look more intuitive. (the word capture indicates what happens)\n\nConstructors are about as intuitive as you can get for constructing a new object \n\n\nMike did in LUCENE-1426. I also don't see a reason anymore to have Token as a separate class. I think the TokenStreamState for the buffering usecases is more elegant...\n\nThanks.  I see it now.  I think one thing that might help is to rename Attribute to be TokenAttribute.  For me, I'm just trying to think how to explain this stuff to new users and it doesn't feel as cut-and-dried as saying \"A TokenStream produces Tokens.  Tokens have attributes like offset, position, etc.\".  Now we are saying \"A TokenStream produces Attributes.  Attributes describe a token with offset, position, etc.\".  Not a big deal, though.  I like where this is heading.\n\nI'm want to check it out for Solr, too, to see if there are any effects.\n\nHave you done any performance testing?  I would think it would be faster.  Out of curiosity,   do you think this would allow us to implement something like described in the paper here: http://arnoldit.com/wordpress/2008/09/06/text-processing-why-servers-choke/ ",
            "author": "Grant Ingersoll",
            "id": "comment-12643468"
        },
        {
            "date": "2008-10-29T11:50:15+0000",
            "content": "\nI like this new \"idempotent\" approach!\n\nI reviewed the patch.  It looks good!:\n\n\n\tIt seems like AttributeSource is fairly generic \u2013 mabye it should\n    go into util?  It seems like we could eventually use the same\n    approach for extensibility to index writing/reading APIs?\n\n\n\n\n\tPerformance \u2013 have you compared before/after simple tokenization\n    speeds?  Eg, tokenize all docs in Wikipedia w/ different\n    analyzers.  contrib/benchmark makes this easy now.\n\n\n\n\n\tI assume we would statically default TokenStream.useNewAPI to\n    true?\n\n\n\n\n\tYou are using a Token (perThreadlocalToken) and a\n    BackwardsCompatibilityStream when indexing a NOT_ANALYZED Field,\n    in DocInverterPerField \u2013 can you fix that code to use\n    not-deprecated new stuff?  EG maybe make a SingleTokenTokenStream\n    or something that's re-used for this purpose?\n\n\n\n\n\tOnce we can set useNewAPI per TokenStream instance, what happens\n    if someone builds up an analyzer chain that mixes old & new\n    TokenStream/Filters?  Are all permutations OK?\n\n\n\n\n\tAre DocInverter.BackwardsCompatibilityStream and\n    TokenStreamTestUtils.BackwardsCompatibleStream basically the same\n    thing?  If so, can we merge them?  It seems like others may needs\n    this (when mixing old/new analyzer chains \u2013 the question above).\n\n\n\n\n\tCan you add back-compat-future-freedom warnings to these new APIs?\n    (Ie \"these are new & subject to change\")\n\n\n\n\n\tDo we really need the separate subclass TokenStreamState?  Should\n    we absorb its methods into AttributeSource?\n\n\n\n\n\tIn DocInverterPerField you check if the attrSource has posIncr &\n    offset attrs, instead of just looking them up and getting the\n    default instance if it wasn't already there.  This then requires\n    you to default posIncr & offsets in two places \u2013\n    DocInverterPerField & each of the attr classes.  Wouldn't it be\n    cleaner to always look them up, and let the default instance of\n    each be the one place that holds the default?\n\n\n\n\n\tSome files are missing copyright header.  Also, the comments at the\n    top of TermAttribute.java & TypeAttribute.java should be fixed\n    (they are about pos incr now).  Also there is a \"must should\"\n    in the javadocs in TokenStrea.java.\n\n\n\n\n\tOn the new TokenStream.start method: is a TokenStream allowed to\n    not allow more than 1 start invocation?  Meaning, it cannot repeat\n    itself.  Just like we are grappling with on DocIdset.iterator()\n    it'd be good to address this semantics up front.\n\n\n\n\n\tMany unit tests had to be changed with this patch.  Was this\n    entirely due to avoiding the newly deprecated APIs, or, are there\n    any tests that fail if they were not changed?  (This is a simple\n    check to run, actually \u2013 only apply your core changes, then run\n    all tests).  If it's the former (which it should be), maybe we\n    should leave a few tests using the deprecated APIs to ensure we\n    don't break them before 3.0?\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12643472"
        },
        {
            "date": "2008-11-04T02:36:10+0000",
            "content": "Thanks for the thorough review, Mike! I made most of the changes you suggested. However, I've some comments to some of your points:\n\n\nI assume we would statically default TokenStream.useNewAPI to\ntrue?\n\nI don't think it should default to true. How it currently works (and this answers also some of your other questions) is that one can't mix old and new streams and filters in the same chain. If someone enables the new API, then all streams and filters in one chain have to implement the new API. The reason is that you can't \"simulate\" the old API by calling the new methods in an efficient way. We would have to copy all values from the Token that next(Token) returns into the appropriate Attributes.\nThis would slow down the ingestion performance and I think would affect backwards-compatibility: We guarantee that you can update from 2.4 to 2.9 without getting compile and runtime errors, but I think the performance should also not decrease significantly? That's the main reason why I left two implementations in all core streams and filters, for next(Token) and incrementToken().\n\n\nAre DocInverter.BackwardsCompatibilityStream and\nTokenStreamTestUtils.BackwardsCompatibleStream basically the same\nthing? If so, can we merge them? It seems like others may needs\nthis (when mixing old/new analyzer chains - the question above).\n\nSorry for the silly naming of these classes. They are not the same. The one in DocInverter is basically a wrapper with Attributes around an old Token. This is used so that almost all consumers in the indexer package can already use the new API in an efficient way.\nThe one in the test package however is used, so that TokenStream and -Filter implementations in our testcases don't have to have implementations for both the old and new API. If the old next(Token) is called, then it calls incrementToken() and copies over the values from the Attributes to the Token. Since performance is not critical in testcases, I decided to take this approach to reduce code duplication in the tests.\n\n\nMany unit tests had to be changed with this patch. Was this entirely due to avoiding the newly deprecated APIs, or, are there any tests that fail if they were not changed? (This is a simple check to run, actually - only apply your core changes, then run all tests). If it's the former (which it should be), maybe we should leave a few tests using the deprecated APIs to ensure we don't break them before 3.0?\n\nThat's actually the reason why I want to keep the static setUseNewAPI() method. When I test this patch I run all tests twice, in both modes. That way I can be sure to not break backwards compatibility.\n\n\nOn the new TokenStream.start method: is a TokenStream allowed to\nnot allow more than 1 start invocation? Meaning, it cannot repeat\nitself. Just like we are grappling with on DocIdset.iterator()\nit'd be good to address this semantics up front.\n\nNot sure I follow you here. Could you elaborate? ",
            "author": "Michael Busch",
            "id": "comment-12644881"
        },
        {
            "date": "2008-11-04T20:05:08+0000",
            "content": "...one can't mix old and new streams and filters in the same chain\n\nAhh, I see.  So on upgrading if someone has a chain involving an\nexternal TokenFilter that does not implement the new API, then the\nwhole chain should be downgraded to the old API until that filter is\nupgraded.\n\nOK I agree we should default it to false, then in 3.0 hardwire it to\ntrue (and remove all the deprecated methods).\n\nWe guarantee that you can update from 2.4 to 2.9 without getting compile and runtime errors, but I think the performance should also not decrease significantly?\n\nI agree: I think this should be (is?) part of the back compat promise.\n\nThey are not the same.\n\nOK this makes sense!\n\nWhen I test this patch I run all tests twice, in both modes. That way I can be sure to not break backwards compatibility.\n\nBut: what if you revert all changes to all tests, and leave useNewAPI\nfalse?  Nothing should fail, right?  (This is the \"true\" back compat\ntest, I think).\n\nNot sure I follow you here. Could you elaborate?\n\nIn LUCENE-1427, we were discussing whether a DocIdSet should \"declare\"\nwhether it can be iterated over more than once, or not, so that things\nthat know they need to consume it more than once would know whether to\nuse an intermediate cache.  I'm not sure we'd need it, here, but since\nwe are changing the API now I thought we should think about it.\nBut... we can (and should) do this as a separate issue, if and when it\nproves out to a real use case, so let's take it off the table for now. ",
            "author": "Michael McCandless",
            "id": "comment-12645060"
        },
        {
            "date": "2008-11-06T20:24:14+0000",
            "content": "> But: what if you revert all changes to all tests [ ... ]\n> This is the \"true\" back compat test, I think\n\nPerhaps we could formalize this.  We could specify a subversion URL in build.xml that points to a release tag, the tag of the oldest release we need to be compatible with.  We add a test that retrieves the tests from this tag and compiles and runs them. ",
            "author": "Doug Cutting",
            "id": "comment-12645565"
        },
        {
            "date": "2008-11-06T20:37:13+0000",
            "content": "Perhaps we could formalize this. We could specify a subversion URL in build.xml that points to a release tag, the tag of the oldest release we need to be compatible with. We add a test that retrieves the tests from this tag and compiles and runs them.\n\n+1\n\nHopefully someone who understands ant/build.xml very well (not me!) will do this \n\nOnce we have that working, we could then make it part of the nightly build. ",
            "author": "Michael McCandless",
            "id": "comment-12645573"
        },
        {
            "date": "2008-11-07T17:48:01+0000",
            "content": "\nPerhaps we could formalize this. We could specify a subversion URL in build.xml that points to a release tag, the tag of the oldest release we need to be compatible with. We add a test that retrieves the tests from this tag and compiles and runs them.\n\nI like this approach too. The only concern I have is about the package-private APIs. As we recently discussed they can always change, so if any test cases use package-private APIs that changed in trunk since the last release, then the old testcases would not even compile.\n\nHowever, I worked on a patch for this and will open a new issue soon. ",
            "author": "Michael Busch",
            "id": "comment-12645836"
        },
        {
            "date": "2008-11-08T14:31:23+0000",
            "content": "Good idea, Doug, on the back-compat testing.  Something we should definitely add.\n\nMichael and I did some performance tests at ApacheCon on this, and it seems like it is as fast as the old way, but we should probably formalize this by adding to the Tokenize algorithm in the benchmarker so it is easier for people to verify it.\n\nI still don't see the point to the \n\npublic static TokenStreamState capture(TokenStream from) \n\n  Just use the constructor.  Wrapping a constructor in a static doesn't gain you anything except an extra function call. ",
            "author": "Grant Ingersoll",
            "id": "comment-12645986"
        },
        {
            "date": "2008-11-10T02:04:58+0000",
            "content": "\n\n\tIn DocInverterPerField you check if the attrSource has posIncr &\n      offset attrs, instead of just looking them up and getting the\n      default instance if it wasn't already there. This then requires\n      you to default posIncr & offsets in two places -\n      DocInverterPerField & each of the attr classes. Wouldn't it be\n      cleaner to always look them up, and let the default instance of\n      each be the one place that holds the default?\n\n\n\nSo here's one thing about the new API that is not entirely clear to me yet. Let's say the consumer, in this case DocInverterPerField, wants to use the offset. You are right, it's not desirable to define default values in two places (the consumer and the attribute). Now the other alternative is to call addAttribute(OffsetAttribute.class) in the consumer. If no offset attribute is present, this will just add a default instance and the consumer can use its default value. So far so good. But what if there is e. g. a TokenFilter in the chain that checks input.hasAttribute(OffsetAttribute.class) and does something it would not do if that check returned false? It seems \"something\" is not clearly defined here yet. I think the \"something\" is the difference between a producer (TokenStream/Filter) adding an attribute vs. a consumer. Thoughts?\n\nHmm or maybe in this case the consumer is not behaving correctly. If there is no offset attribute, then the consumer should not have to write any data structures that use an offset, right? That's why you currently have the start() method in TermVectorTermsWriterPerField that checks if offsets need to be written or not.\nI think the issue here is that one consumer, the DocInverterPerField, deals with one value, the offset, that multiple subconsumers might want to use. That's why we don't want to throw an exception in DocInverterPerField if OffsetAttribute is not present, because it can't know if this is really a misconfiguration or not. Only the actual subconsumer (in this case TermVectorTermsWriterPerField) is able to decide that. For example if someone writes a new custom consumer that can only work if OffsetAttribute is present, then we would probably want that consumer to throw an exception if it's not present. But if the DocInverterPerField adds the OffsetAttribute to be able to use it's default value, we would not get the exception. \n\nBut maybe all this is fine and we should just say we strictly separate the Attributes from the configuration, which means that the knowledge about the presence or absence of an Attribute may not be used by anybody (filter or consumer) to determine anything about the configuration. This is how the TermVectors currently work. The configuration (whether or not to store positions and/or offsets) is stored in the Field, the data is carried by the OffsetAttribute. If we decide to define the API this way, do we even need AttributeSource.hasAttribute() and getAttribute()? Or wouldn't addAttribute() be sufficient then? ",
            "author": "Michael Busch",
            "id": "comment-12646146"
        },
        {
            "date": "2008-11-12T01:35:24+0000",
            "content": "\nIt seems \"something\" is not clearly defined here yet.\n\nOK, I added this to the javadocs of TokenStream to make things clear:\n\n  The workflow of the new TokenStream API is as follows:\n\n\n\tInstantiation of TokenStream/TokenFilters\n\tThe consumer calls TokenStream#start() which triggers calls to #initialize() of the stream and all filters in the chain.\n\tFilters can use #initialize() to get or add attributes and store local references to the attribute.\n\tAfter #start() returns the consumer retrieves attributes from the stream and stores local references to all attributes it wants to access\n\tThe consumer calls #incrementToken() until it returns false and consumes the attributes after each call.\n\n\n\n  To make sure that filters and consumers know which attributes are available\n  the attributes must be added in the #initialize() call. Filters and \n  consumers are not required to check for availability of attributes in #incrementToken().\n ",
            "author": "Michael Busch",
            "id": "comment-12646773"
        },
        {
            "date": "2008-11-12T03:21:05+0000",
            "content": "Changes in this patch:\n\n\n\tupdated patch to current trunk\n\tmoved AttributeSource and Attribute to o.a.l.util\n\tremoved TokenStreamState and moved captureState() and restoreState() into AttributeSource\n\tadded non-static setUseAPI() method; added warning that mixing of old and new streams/filters is not supported\n\tDocInverterPerField now uses only non-deprecated classes for NOT_ANALYZED fields\n\tAdded warnings saying that the new APIs might change in the future\n\tDocInverterPerField now adds PositionIncrementAttribute and OffsetAttribute in case they are not present in the stream and thus uses the default values defined in the attributes\n\tadded missing license headers and fixed javadocs\n\tRemoved TokenStreamTestUtils and references to Token from all unit tests (except the now also deprecated TestToken); LuceneTestCase enables the new API in setup() for all tests, so the current tests only test the new API; backwards-compatibility is tested with \"ant test-tag\" (see LUCENE-1440)\n\tfixed some small bugs that I encountered after changing the unit test\n\n\n\nAll current and 2.4 tests pass. ",
            "author": "Michael Busch",
            "id": "comment-12646786"
        },
        {
            "date": "2008-11-13T17:18:54+0000",
            "content": "Looks good!\n\nI'm seeing this failure (in test I just committed this AM).  I think\nit's OK, because the new API is enabled for all tests and I'm using\nthe old API with that analyzer?\n\n\n    [junit] Testcase: testExclusiveLowerNull(org.apache.lucene.search.TestRangeQuery):\tCaused an ERROR\n    [junit] This token does not have the attribute 'class org.apache.lucene.analysis.tokenattributes.TermAttribute'.\n    [junit] java.lang.IllegalArgumentException: This token does not have the attribute 'class org.apache.lucene.analysis.tokenattributes.TermAttribute'.\n    [junit] \tat org.apache.lucene.util.AttributeSource.getAttribute(AttributeSource.java:124)\n    [junit] \tat org.apache.lucene.index.TermsHashPerField.start(TermsHashPerField.java:252)\n    [junit] \tat org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:144)\n    [junit] \tat org.apache.lucene.index.DocFieldConsumersPerField.processFields(DocFieldConsumersPerField.java:36)\n    [junit] \tat org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:234)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:760)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:738)\n    [junit] \tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2039)\n    [junit] \tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2013)\n    [junit] \tat org.apache.lucene.search.TestRangeQuery.insertDoc(TestRangeQuery.java:304)\n    [junit] \tat org.apache.lucene.search.TestRangeQuery.initializeIndex(TestRangeQuery.java:287)\n    [junit] \tat org.apache.lucene.search.TestRangeQuery.testExclusiveLowerNull(TestRangeQuery.java:315)\n\n\n\nSome other random questions:\n\n\n\tI'm a little confused by 'start' and 'initialize' in TokenStream.\n    DocInverterPerField (consumer of this API) calls\n    analyzer.reusableTokenStream to get a token stream.  Then it calls\n    stream.reset().  Then it calls stream.start() which then calls\n    stream.initialize().  Can we consolidate these (there are 4 places\n    that we call to \"start\" the stream now)?\n.\n    EG why can't analyzer.reusableTokenStream() do the init internally\n    in the new API?  (Also, in StopFilter, initialize() sets termAtt &\n    posIncrAtt, but I would think this only needs to happen once when\n    that TokenFilter is created?  BackCompatTokenStream add the attrs\n    in its ctor, which seems better.).\n\n\n\n\n\tBackCompatTokenStream is calling attributes.put directly but all\n    others use super's addAttribute.\n\n\n\n\n\tWhy is BackCompatTokenStream overriding so many methods?  EG\n    has/get/addAttribute \u2013 won't super do the same thing?\n\n\n\n\n\tMaybe add reasons to some of the asserts, eg StopFilter has\n    \"assert termAtt != null\", so maybe append to that \": initialize()\n    wasn't called\".\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12647337"
        },
        {
            "date": "2008-11-13T17:47:48+0000",
            "content": "\nLooks good! \nThanks for reviewing ... again!\n\n\nI'm a little confused by 'start' and 'initialize' in TokenStream.\nGood question. The only reason why I added two separate methods here was to enforce that TokenFilter#start() always calls input.start() first. I think otherwise this will be a pitfall for people who want to implement their own filters and override start(). (happened to me when I modified the tests a couple of times)\nOn the other hand I agree that Three methods (start(), initialize(), reset()) are confusing. I guess we can deprecate reset() in the future and have start() rewind the stream if supported by the stream. (I think you mentioned a boolean method could return true if rewind is supported?)\nSo I'd be ok with collapsing start/initialize into one method if you prefer that. I'd just have to add an explicit warning to the javadocs in TokenFilter.\n\n\nI'm seeing this failure (in test I just committed this AM). I think\nit's OK, because the new API is enabled for all tests and I'm using\nthe old API with that analyzer?\nYeah, I've to modify the new test to use the new API.\n\n\n\n\tBackCompatTokenStream is calling attributes.put directly but all\nothers use super's addAttribute.\n\tWhy is BackCompatTokenStream overriding so many methods? EG\nhas/get/addAttribute - won't super do the same thing?\n\n\nI had a different implementation of BCTS, so this is left-over. Of course you're right, I can simplify that class.\n\n\nMaybe add reasons to some of the asserts, eg StopFilter has\n\"assert termAtt != null\", so maybe append to that \": initialize()\nwasn't called\".\nYeah good idea, will do. ",
            "author": "Michael Busch",
            "id": "comment-12647346"
        },
        {
            "date": "2008-11-14T09:59:01+0000",
            "content": "So I'd be ok with collapsing start/initialize into one method if you prefer that.\n\nOr, could we just continue to use reset() for this purpose?  I think it's ok via Javadocs to state clearly that if you override reset you have to be sure to call super.reset().\n\nAnd then tokenizers like CharTokenizer should not addAttribute on every reset/initialize, right?  They should just do it once in their ctor? ",
            "author": "Michael McCandless",
            "id": "comment-12647557"
        },
        {
            "date": "2008-11-14T22:06:07+0000",
            "content": "\nOr, could we just continue to use reset() for this purpose? I think it's ok via Javadocs to state clearly that if you override reset you have to be sure to call super.reset().\n\nAnd then tokenizers like CharTokenizer should not addAttribute on every reset/initialize, right? They should just do it once in their ctor?\n\nOK I like that! Let's do it that way, it's simpler and less confusing.\n\nI'm making the change now, then hopefully the patch is ready for committing... ",
            "author": "Michael Busch",
            "id": "comment-12647737"
        },
        {
            "date": "2008-11-16T23:20:56+0000",
            "content": "I removed start() and initialize().\nI also fixed some javadocs and added a pretty long section to package.html in the analysis package, describing the new APIs.\n\nAll trunk and 2.4 tests pass.\n\nI guess I'll wait until LUCENE-1448 is committed (and update this patch again...  ). I'll try to review 1448 later today (need to leave for a few hours now...) ",
            "author": "Michael Busch",
            "id": "comment-12648047"
        },
        {
            "date": "2008-11-17T19:55:00+0000",
            "content": "OK we decided to commit this before LUCENE-1448.\n\nI'll wait another day and then commit this if nobody objects. ",
            "author": "Michael Busch",
            "id": "comment-12648291"
        },
        {
            "date": "2008-11-18T23:49:04+0000",
            "content": "Committed revision 718798.\n\nThanks everyone who helped here with reviews and comments! ",
            "author": "Michael Busch",
            "id": "comment-12648818"
        },
        {
            "date": "2008-11-24T01:34:44+0000",
            "content": "FWIW: In previous versions of Lucene, a class like this would have been legal...\n\n\npublic final class DummyStream extends TokenFilter {\n  int count = 5;\n  public DummyStream() {  super(null);  }\n  public final Token next() {\n    return (0 < count--) ? new Token(\"YES\", 0, 0) : null;\n  }\n  public void reset() throws IOException {count = 5}\n  public void close() throws IOException {}\n}\n\n\n\n...there wouldn't have been much reason to write a subclass like this (better to subclass TokenStream directly for this type of usecase), but it would have worked.  with the new AttributeSource class, TokenFilter actually cares about the TokenStream constructor arg during construction, and a constructor like this will cause a NullPointerException.\n\nIt's an esoteric enough possibility that we probably don't need to worry about it, but i'm documenting it for posterity.  (I only noticed because Solr had a test case where it was constructing a stock TokenFilter using a null \"input\" arg just to then test the object in other ways)\n\nif anyone does run into a problem with something like this, your best solution is probably to subclass TokenStream directly, it has a no arg constructor.\n\nfor searching...\n\n\njava.lang.NullPointerException\n\tat org.apache.lucene.util.AttributeSource.<init>(AttributeSource.java:69)\n\tat org.apache.lucene.analysis.TokenStream.<init>(TokenStream.java:91)\n\tat org.apache.lucene.analysis.TokenFilter.<init>(TokenFilter.java:42)\n\t...\n\n ",
            "author": "Hoss Man",
            "id": "comment-12650093"
        },
        {
            "date": "2008-12-11T13:40:20+0000",
            "content": "\n Outstanding:\n\n\tcontrib streams and filters\n\n\n\nWhat happened to fixing the contribs?  Seems incomplete without it. ",
            "author": "Grant Ingersoll",
            "id": "comment-12655636"
        },
        {
            "date": "2008-12-11T18:48:09+0000",
            "content": "See LUCENE-1460. ",
            "author": "Michael Busch",
            "id": "comment-12655735"
        }
    ]
}