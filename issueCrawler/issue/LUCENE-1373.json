{
    "id": "LUCENE-1373",
    "title": "Most of the contributed Analyzers suffer from invalid recognition of acronyms.",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "2.3.2",
        "resolution": "Duplicate",
        "status": "Resolved"
    },
    "description": "LUCENE-1068 describes a bug in StandardTokenizer whereby a string like \"www.apache.org.\" would be incorrectly tokenized as an acronym (note the dot at the end).\n\nUnfortunately, keeping the \"backward compatibility\" of a bug turns out to harm us.\nStandardTokenizer has a couple of ways to indicate \"fix this bug\", but unfortunately the default behaviour is still to be buggy.\nMost of the non-English analyzers provided in lucene-analyzers utilize the StandardTokenizer, and in v2.3.2 not one of these provides a way to get the non-buggy behaviour \n\nI refer to:\n\n\tBrazilianAnalyzer\n\tCzechAnalyzer\n\tDutchAnalyzer\n\tFrenchAnalyzer\n\tGermanAnalyzer\n\tGreekAnalyzer\n\tThaiAnalyzer",
    "attachments": {
        "LUCENE-1373.patch": "https://issues.apache.org/jira/secure/attachment/12389558/LUCENE-1373.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-09-03T00:57:48+0000",
            "content": "I would be willing to contribute a patch to make these Analyzers work in the next point release.\n\nI see two ways to do this:\n1) Introduce a static method to StandardTokenizerImpl, whereby you could set the \"default\" value of the replaceInvalidAcronym flag.\nOne could then call setDefaultForReplaceInvalidAcronym(true) one time from your code, and then whenever anyone uses the old Constructor, it would set replaceInvalidAcronym=true\n2) Add the replaceInvalidAcronym flag to all of the above Analyzers.\nSome of these have multiple constructors already, so I would probably just add a setter/getter to them.\n\nThe question is, which of the above would be preferred?\nPersonally, I think the first is the least amount of work to do, and also the easiest to back out when you move onto v3.x, and the \"deprecated\" behaviour is removed.\nHowever, doing 2) means the least disruption to core code.\n\nAlso, judging by the \"Fix Version/s\" field above, I am guessing that a v2.3.3 release is planned, therefore I guess I should provide a patch for the 2.3 branch as well as trunk which will end up as 2.4? ",
            "author": "Mark Lassau",
            "id": "comment-12627873"
        },
        {
            "date": "2008-09-03T00:59:34+0000",
            "content": "Causes JIRA issue JRA-15484. ",
            "author": "Mark Lassau",
            "id": "comment-12627875"
        },
        {
            "date": "2008-09-03T04:58:18+0000",
            "content": "Had a closer look at the code, including changes in StandardAnalyzer.\nThe static default idea would need a reworking of StandardAnalyzer.reusableTokenStream(), and so I think it is safer to just add the replaceInvalidAcronym flag to the affected Analyzers. ",
            "author": "Mark Lassau",
            "id": "comment-12627903"
        },
        {
            "date": "2008-09-03T13:13:07+0000",
            "content": "I think you should mirror what is done in StandardAnalyzer.  You probably could create an abstract class that all of them inherit to share the common code.\n\nOf course, it's still a bit weird, b/c in your case the type value is going to be set to ACRONYM, when your example is clearly not one.  This suggests to me that the grammar needs to be revisited, but that can wait until 3.0 I believe. ",
            "author": "Grant Ingersoll",
            "id": "comment-12627990"
        },
        {
            "date": "2008-09-05T06:13:11+0000",
            "content": "Just discovered LUCENE-1151, which attempts to make StandardAnalyzer NOT be buggy by default.\nI think if the changes made to StandardAnalyzer here where moved to StandardTokenizer instead, then we would fix this issue. ",
            "author": "Mark Lassau",
            "id": "comment-12628563"
        },
        {
            "date": "2008-09-05T08:28:20+0000",
            "content": "Added a draft patch to fix the default behaviour of StandardTokenizer.\nThis basically involved moving the logic of LUCENE-1151 from StandardAnalyzer to StandardTokenizer.\n\nI added a unit test for StandardTokenizer, but unfortunately don't have time to add tests for the language analyzers listed above (FrenchAnalyzer etc...).\n\nI will be away for 3 weeks, so if anyone else wants to pick up this issue, that would be great  ... otherwise I will come back and look at it then. ",
            "author": "Mark Lassau",
            "id": "comment-12628589"
        },
        {
            "date": "2009-07-01T08:49:49+0000",
            "content": "Is it possible that when a property has a value that ends on \"Type\" like \"InputFileType\" is not indexed when the OS language is Dutch due to the same bug? I have two installations of Alfresco 3 Labs with Lucene 2.1.0 autoinstalled and with exactly the same installation options (English as language for Alfresco) the main difference next to the Hardware is the OS language. In both cases XP with SP2 but one English and the other Dutch. In the installation on the Dutch OS three properties with values ending on Type could not be found whereas they are present in the English version. ",
            "author": "Rob ten Hove",
            "id": "comment-12725972"
        },
        {
            "date": "2009-07-06T04:44:12+0000",
            "content": "@Rob \nThis issue is about how Lucene parses ACRONYM tokens, which must contain a dot (eg \"I.B.M.\"), and so you problem is certainly not exactly the same.\n\nWhether it is related to some other issue with Lucene analysers for different languages is not clear.\nIt depends on the workings of your application, and I would suggest you contact the Alfresco developers with this question. ",
            "author": "Mark Lassau",
            "id": "comment-12727390"
        },
        {
            "date": "2009-07-09T11:07:46+0000",
            "content": "@Mark, thanks for your reply on my question. So far the developers that worked on the application I was talking about were able to find a workaround. One thing is certain: the token analyzer mistreats the content... whether the content is an acronym or just plain text... seems that it tries to interpret the content of database elements a bit too much rather than just treat it as plain content... ",
            "author": "Rob ten Hove",
            "id": "comment-12729181"
        },
        {
            "date": "2009-10-22T19:54:35+0000",
            "content": "Dup of LUCENE-2002. ",
            "author": "Michael McCandless",
            "id": "comment-12768844"
        }
    ]
}