{
    "id": "SOLR-2906",
    "title": "Implement LFU Cache",
    "details": {
        "affect_versions": "3.4",
        "status": "Closed",
        "fix_versions": [
            "3.6",
            "4.0-ALPHA"
        ],
        "components": [
            "search"
        ],
        "type": "Sub-task",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Implement an LFU (Least Frequently Used) cache as the first step towards a full ARC cache",
    "attachments": {
        "TestLFUCache.java": "https://issues.apache.org/jira/secure/attachment/12504546/TestLFUCache.java",
        "ConcurrentLFUCache.java": "https://issues.apache.org/jira/secure/attachment/12504544/ConcurrentLFUCache.java",
        "LFUCache.java": "https://issues.apache.org/jira/secure/attachment/12504545/LFUCache.java",
        "SOLR-2906.patch": "https://issues.apache.org/jira/secure/attachment/12504559/SOLR-2906.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Shawn Heisey",
            "id": "comment-13153001",
            "date": "2011-11-18T17:57:35+0000",
            "content": "Here is the first crack at an LFU implementation.  There's some weirdness with negative numbers in the statistics, and I'm not sure that eviction and warming are working right, but I am having trouble getting a test environment fully operational. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13153002",
            "date": "2011-11-18T17:58:04+0000",
            "content": "I used branch_3x to create the above files.  I haven't even looked at trunk. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13153080",
            "date": "2011-11-18T19:44:16+0000",
            "content": "Evictions definitely don't seem to be working right.  I finally got a benchmark script going.  I watched as the size of the filterCache climbed to the maximum size of 64.  On the next insert, it was suddenly only 7 entries, and the eviction counter had incremented from 290 to 348.  That seems really aggressive.  I seem to have done something wrong.\n\nOnce I got through with the benchmark script, I did a commit, at that moment the size was 50 out of 64.  After warming (autowarmCount 16), the cache size was 12, and both the hits and lookups were -12 (negative). "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13153082",
            "date": "2011-11-18T19:47:05+0000",
            "content": "I will fully admit that I built the new cache type from the old code without really understanding what the code was doing, and now I am out of my depth. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13153249",
            "date": "2011-11-19T00:00:38+0000",
            "content": "I've been trying to find my bug.  Looking back at the original LRU implementation, I have no idea how it's working.\n\nWhen a CacheEntry is created in the LRU code, one of the values sent in is an incremented stats.accessCounter, which gets called lastAccessed in the new object.  When it is later used in markAndSweep, it is used in simple math along with the number of items that we want to keep/remove.  This is very confusing, and I can't see how it could ever work.  It might be that part of the code is simply skipped because of the way the math happens to work out.\n\nWhen I changed it around to use hitcounts, those counts are also used in the previously mentioned simple math, and I believe that results in some very weird behavior, such as removing most (or possibly all) of the cache entries.\n\nIt appears that this idea is a lot more complicated than I originally thought, and that the current code needs to be at least partially rewritten. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13153258",
            "date": "2011-11-19T00:17:56+0000",
            "content": "I've been trying to find my bug. Looking back at the original LRU implementation, I have no idea how it's working.\n\nHeh... it is pretty complex, and I did try to add a ton of comments because of that.\nThe basic idea is that I wanted to avoid an O(n*log( n )) solution of sorting everything and then discarding the lowest.\nIn my testing, it seems to work, and we often just need to take a singe O( n ) pass to evict all the entries we need.\n\nThe comment at the top is the most important to understanding how it works:\n\n\n    // if we want to keep at least 1000 entries, then timestamps of\n    // current through current-1000 are guaranteed not to be the oldest (but that does\n    // not mean there are 1000 entries in that group... it's acutally anywhere between\n    // 1 and 1000).\n    // Also, if we want to remove 500 entries, then\n    // oldestEntry through oldestEntry+500 are guaranteed to be\n    // removed (however many there are there).\n\n\n\nBut really, it seems like you should disregard all the algorithmic stuff in LRU when implementing LFU.\nIf you think you see a bug in the existing LRU stuff, you're going to have to spell it out for me a bit more. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13153288",
            "date": "2011-11-19T00:57:53+0000",
            "content": "But really, it seems like you should disregard all the algorithmic stuff in LRU when implementing LFU. If you think you see a bug in the existing LRU stuff, you're going to have to spell it out for me a bit more.\n\nI can't actually say that there is a bug, but I have to say that I'm really confused (in the LRU code) by what lastAccessed(Copy) actually is and how it works, and what the following code pieces from markAndSweep are doing with it, since wantToKeep and wantToRemove are entry counts:\n\n\nlong thisEntry = ce.lastAccessedCopy;\n  <snip>\nif (thisEntry > newestEntry - wantToKeep) {\n  <snip>\n} else if (thisEntry < oldestEntry + wantToRemove) { // entry in bottom group?\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13153306",
            "date": "2011-11-19T01:19:13+0000",
            "content": "since wantToKeep and wantToRemove are entry counts\n\nYes, it seems like mixing units.  But the key to understanding it is that lastAccessed isn't a real timestamp, but just a counter that is incremented for every access.  This means that if the latest \"timestamp\" is 5000, and we know we want to keep at least 1000 entries, then if we run across any timestamps greater than 4000 that it's guaranteed to be in the top 1000 entries and we don't need to consider it further.  One can just reverse that logic to figure out that an entry is definitely in the bottom group and we should immediately discard it. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13153335",
            "date": "2011-11-19T02:07:01+0000",
            "content": "Would it be possible to adapt the lastAccessed shortcut to LFU, or would it be simply best to remove that section of code? "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13153394",
            "date": "2011-11-19T05:17:33+0000",
            "content": "Now markAndSweep builds a treeset of the least used items, then uses the treeset to evict entries.\n\nThis is almost guaranteed to be an inefficient way to do things.  If it works, I leave optimization to the experts. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13154338",
            "date": "2011-11-21T18:20:20+0000",
            "content": "I've renamed the user-facing class to LFUCache and created a test program based on the LRU version.  The tests are failing, though.  So far I can't figure out why. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13154344",
            "date": "2011-11-21T18:23:19+0000",
            "content": "I've re-added lastAccessed to the class, as a tiebreaker when hitcount is equal.\n\nThe test method prints out leastUsedItems and mostUsedItems.  Somehow, item number 50 is included in both. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13154465",
            "date": "2011-11-21T20:32:13+0000",
            "content": "All known bugs found and fixed, unit test looks correct and passes.  This was created against branch_3x, but trunk probably won't be much different.\n\nIMHO, ready for review and possible inclusion.  The javadoc and other comments were reviewed and modified, but not closely. "
        },
        {
            "author": "Simon Willnauer",
            "id": "comment-13154482",
            "date": "2011-11-21T20:43:33+0000",
            "content": "IMHO, ready for review and possible inclusion. The javadoc and other comments were reviewed and modified, but not closely.\n\nshawn, is it possible to upload a diff file (patch). If you are using a svn checkout make sure on add all new files (svn add) and then run svn diff > SOLR-2906.patch from the top level directory. This makes it easier to see what changed and we only have to apply a single file to test & review your changes. there is also a wiki for this: http://wiki.apache.org/lucene-java/HowToContribute (see How to create a patch) "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13154514",
            "date": "2011-11-21T21:11:15+0000",
            "content": "shawn, is it possible to upload a diff file (patch).\n\nThese are all new files, no files changed.  \"svn diff\" returns nothing. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13154528",
            "date": "2011-11-21T21:15:07+0000",
            "content": "I figured out what I did wrong.  You have to 'svn add' the files before you can 'svn diff'  "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13155258",
            "date": "2011-11-22T17:06:28+0000",
            "content": "Something odd happens with the filterCache.  When things are first starting off, the cache size and the number of inserts don't match up.  It's usually off by 10, with more in the inserts.  This doesn't seem to happen with the other cache types, also using LFU. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13155267",
            "date": "2011-11-22T17:14:58+0000",
            "content": "The cache size and the number of inserts don't match up. It's usually off by 10\n\nDo you have any type of warming (autowarming or statically configured)?  Statistics are counted differently during warming.\n "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13155280",
            "date": "2011-11-22T17:32:05+0000",
            "content": "The only static warming I have is an all docs search with a sort parameter (and no filter query), which precaches my sort.  I do have autowarming configured, but this behavior happens from initial solr startup.  Things seem to behave correctly with warming - size is autowarmCount, inserts are zero.\n\nFastLRUCache behavior:\nqueryResultCache: size is one higher than inserts\ndocumentCache: size is one higher than inserts\nfilterCache: size and inserts are identical\n\nLFUCache behavior:\nqueryResultCache: size is one higher than inserts\ndocumentCache: size is one higher than inserts\nfilterCache: inserts is higher than size by a variable amount\n\nI've seen 10 (on two different runs) and 2 (on the most recent run) as the difference between inserts and size.\n\nThe FastLRUCache behavior is seen on my production servers with production queries, the LFUCache behavior is on my test server with a benchmark script providing the queries.  I suppose there might be something weird about my canned queries that makes the filterCache behave differently, but the original source of the queries was a production Solr log at level INFO. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13155290",
            "date": "2011-11-22T17:42:56+0000",
            "content": "I might have figured out the problem, and if I have, the cache code is fine.  I just checked the log from my most recent run and have found that there are two errors from invalid filter queries.  I think this means that when a filter is invalid, inserts gets incremented but size doesn't. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13155296",
            "date": "2011-11-22T17:50:27+0000",
            "content": "I can't reproduce it with LFUCache or FastLRUCache by manually sending invalid queries, so that's the wrong idea. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13156273",
            "date": "2011-11-23T20:55:40+0000",
            "content": "Possibly false alarm.  Although I still do not know what causes the discrepancy between inserts and size on the filter cache, I can confirm that exactly the same thing happens when I change it to FastLRUCache, restart Solr, and fire up the benchmarking script. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13156755",
            "date": "2011-11-24T15:21:55+0000",
            "content": "Updated patch.  One of the bugs I had to fix was in the least/most items methods, so that I added new items to the TreeSet before removing old ones, because the one it just added might have the same hitcount as entries already present.  Without checking the new entry, I couldn't know which entry was the right one to remove.\n\nThis change reverts it to a remove then add when the hitCounts are different in the right direction.  When they are equal, it still does the add before the remove.  By reducing the size of the set before adding a new member whenever possible, there is a possibility it can go faster. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13166399",
            "date": "2011-12-09T18:25:48+0000",
            "content": "I finally got a chance to do some testing in production.  I have two distributed index chains, both running 3.5.0 with this patch and the one from SOLR-1972 applied.  The chains are updated independently, there is no replication.  It's not a truly definitive test, because my queries are load balanced between the two chains and I do have some hardware discrepancies.  I cannot create a valid test environment to compare the two caches as they should be compared, with identical queries going to two completely identical servers.\n\nOn average, commits made on chain A where filterCache is set to LFU and the servers have 48GB of RAM happen faster than those on chain B, with FastLRU and 64GB of RAM.  One of the two servers on chain A has slightly faster processors than its counterpart on chain B \u2013 2.83 GHz vs. 2.66 GHz.  The other two servers on both chains have 2.5 GHz processors.\n\nI suspect that most of the potential gains that the LFU algorithm might be able to provide are swallowed by the very inefficient implementation.\n\nIf anyone has some thoughts for me to pursue, I will be happy to do so, but I am out of my own ideas.  I hope the patch will be committed.  It could use a lot of optimization and there's probably cosmetic cleanup to do. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13166407",
            "date": "2011-12-09T18:30:36+0000",
            "content": "Some additional info: The index is composed of six large shard cores and a small one, running on two servers.  The total index size on each server (CentOS 6, java 1.6.0_29) is about 60GB.  Solr/Jetty has an 8GB heap. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13171641",
            "date": "2011-12-17T19:12:24+0000",
            "content": "What do the collective people who actually know the code think about this patch? From my perspective, the LFU is a classic alternative to LRU and seems like a fine addition. If there are no objections, I'll volunteer to commit this.\n\nShawn:\nThis is the kind of thing that could use two things:\n1> text for CHANGES.txt describing the new functionality. If you could attach that as a separate patch file (or just e-mail it to me) that would be great. That file changes rapidly enough that I like to add the text at check-in time, it seems easier.\n\n2> Adding a description to the Wiki since this is new functionality. Perhaps in http://wiki.apache.org/solr/SolrCaching? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13171903",
            "date": "2011-12-18T17:16:47+0000",
            "content": "IMO, this is appropriate for trunk.  If we want to commit to 3x, we should mark as experimental so we can change the default functionality if desired.\n\nOne concern I have with straight LFU is the lack of any kind of time sensitivity. For example, I ran some batch job that accessed the same filters a million times, and now they are stuck in the cache even though they haven't been used for hours or days.  Perhaps one way to handle this would be to do something like count>>=2 to everything when a cleaning out old entries.\n\nI also wonder if it's worth keeping lastAccessed.  It's only valuable for breaking ties and I don't know how much that's actually needed.\n\nAnyway, +1 to commit since this won't be used in the example solrconfig by default (and hence we can speed things up and tweak the algorithm before it possibly does get used by default). "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13172358",
            "date": "2011-12-19T15:56:42+0000",
            "content": "I have added some text to CHANGES.TXT under 3.6.  Like before, my patch is against branch_3x.\n\nYonik, you may be right about lastAccessed.  I was striving for correctness on this first pass, but perhaps it's not worthwhile to care about that too much, just let Java's default functionality figure out which ones to evict. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13172564",
            "date": "2011-12-19T20:06:13+0000",
            "content": "We need to keep lastAccessed, removing it introduces a bug. The CacheEntry.compareTo method looks like this:\n\n    public int compareTo(CacheEntry<K, V> that) {\n      if (this.hitsCopy == that.hitsCopy) {\n        if (this.lastAccessedCopy == that.lastAccessedCopy) \n{\n          return 0;\n        }\n        return this.lastAccessedCopy < that.lastAccessedCopy ? 1 : -1;\n      }\n      return this.hitsCopy < that.hitsCopy ? 1 : -1;\n    }\n\nwhich is guaranteed to return non-zero unless the the exact same object is being compared since lastAccessed(Copy) is monotonically increasing.\n\nIf we remove the lastAccessed, then any two elements having the same number of hits compare as equal. Which also means that tree insertions overwrite each other in methods like getMost/LeastUsedItems. I don't know of any cheaper amounts of overhead to carry along to prevent this.\n\nI've made a few, mostly cosmetic changes while trying to understand the code, I'll check it in shortly. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13172590",
            "date": "2011-12-19T20:34:41+0000",
            "content": "Mostly cosmetic changes:\n\nChanged acceptableLimit to acceptableSize to keep it named consistently\n\nFormatted all the files\n\nImplemented Yonik's aging suggestion (but no tests, there doesn't seem to be a clean way to implement a test here without creating debug-only code).\n\nI'm not wholly convinced that dividing by 4 is the right thing to do here; it'll tend to flatten all the entries making removal somewhat arbitrary as after a few passes anything with hits in the low range will collapse to zero. That said, though, since the little adventure with lastAccessed, all entries with the same number of hits will be treated as LRU so I guess it works.\n\nMarked code as experimental\n\nCommented out some debugging code "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13172605",
            "date": "2011-12-19T20:59:10+0000",
            "content": "Actually, I was thinking count >>= 1 (divide by 2).\nWe could make it optional (timeDecay=true), but my gut feeling says it should be on by default. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13173223",
            "date": "2011-12-20T14:28:33+0000",
            "content": "Updated patch that divides by 2 and adds a unit test for aging out.\n\nShawn:\n\nCould you add in the optional time decay as Yonik suggests? I agree that it seems like the right thing is to have this on by default. At that point, I think it'll be ready to check in. We can add documentation as we can.\n\nWe could  also check it in as is and raise another JIRA. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13173345",
            "date": "2011-12-20T17:45:04+0000",
            "content": "Could you add in the optional time decay as Yonik suggests? I agree that it seems like the right thing is to have this on by default. At that point, I think it'll be ready to check in. We can add documentation as we can.\n\nI've looked at what Yonik has said and cannot figure out what I'd have to do.  I'm not completely ignorant, but there is a lot that I don't know.  I am amazed I was able to get this put together at all. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13173378",
            "date": "2011-12-20T18:25:00+0000",
            "content": "Right, there's a lot of code to wrap your head around and it can be bewildering. And compared to what you've already done, this is easy....\n\nBut lots of things are surprisingly easy in Solr, except when they're really hard <G>. \n\nThis is just about allowing another parameter to be specified in the declaration in solrconfig.xml, similar to initialSize, autowarmCount, etc. cleanupThread is probably a good exemplar.\n\nTake a look at LFUCache.java.init for how all of the other ones are parsed.\n\nThen just pass that through to the ConcurrentLFUCache, conditionally doing the \"ce.hits.set(ce.hitsCopy >>> 1);\" line. Defaulting the timeDecay=true if not present.\n\nSo this should be relatively few lines of actual code, making some automated tests will actually take more time I suspect, as well as any Wiki documentation if you're feeling ambitious.... "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13174475",
            "date": "2011-12-21T23:05:15+0000",
            "content": "I must be dense.  I can figure out how to add the timeDecay option, but I can't figure out what section of code to enable/disable based on the value of timeDecay.  I've gone as far as doing a diff on my Nov 24th patch and the Dec 20th patch from Erick.  (doing diffs on diffs ... the world is going to explode!) The only differences I can see between the two is in whitespace/formatting. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13174512",
            "date": "2011-12-21T23:59:33+0000",
            "content": "Here's what I had in mind, at least I think this will do but all I've done is insured that the code compiles and the current LFU test suite runs.\n\nLook in the diff for \"timeDecay\".\n\nThis still needs some proof that the new parameter comes through from a schema file. Let me know if that presents a problem or if you can't get 'round to it, I might have some time over Christmas.\n\nI think maybe you were under the impression that this had already been done and were looking for it to be in the code already? "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13174928",
            "date": "2011-12-22T17:50:35+0000",
            "content": "You're right, I had thought it was already done.  Now that I see what you've done, and looked up what >>> does, it makes sense, but when I read what Yonik was saying I had no idea.\n\nAdded a note to CHANGES.txt, created what is hopefully the final diff. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13176062",
            "date": "2011-12-27T01:54:59+0000",
            "content": "This should be the final patch. Added the stuff to actually get the parameter from solrconfig \"timeDecay\" which ages out the cache entries as we've discussed. Added tests to insure that it gets through from the config file.\n\nShawn: If you'd add some data to the Wiki about this new parameter, that would be a good thing.\n\nIf nobody objects, I'll probably check this in in the next couple of days. Since they're all new files, the patch will apply to both trunk and 3x cleanly. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13185197",
            "date": "2012-01-12T20:07:41+0000",
            "content": "Fixed in 3x at r: 1230744\nFixed on trunk at r: 1230790 NOTE: r 1230741 was messed up.\n\nShawn:\nDid you ever update the Wiki with this new functionality? That'd be awesome.... "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13195148",
            "date": "2012-01-27T22:15:44+0000",
            "content": "Did you ever update the Wiki with this new functionality? That'd be awesome....\n\nYes, I added LFUCache and the timeDecay option to the SolrCaching Wiki page. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13195324",
            "date": "2012-01-28T01:31:02+0000",
            "content": "Cool!\n\nOn Fri, Jan 27, 2012 at 2:16 PM, Shawn Heisey (Commented) (JIRA) "
        }
    ]
}