{
    "id": "LUCENE-3932",
    "title": "Improve load time of .tii files",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "affect_versions": "3.5",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "We have a large 50 gig index which is optimized as one segment, with a 66 MEG .tii file.  This index has no norms, and no field cache.\n\nIt takes about 5 seconds to load this index, profiling reveals that 60% of the time is spent in GrowableWriter.set(index, value), and most of time in set(...) is spent resizing PackedInts.Mutatable current.\n\nIn the constructor for TermInfosReaderIndex, you initialize the writer with the line,\n\nGrowableWriter indexToTerms = new GrowableWriter(4, indexSize, false);\n\nFor our index using four as the bit estimate results in 27 resizes.\n\nThe last value in indexToTerms is going to be ~ tiiFileLength, and if instead you use,\n\nint bitEstimate = (int) Math.ceil(Math.log10(tiiFileLength) / Math.log10(2));\nGrowableWriter indexToTerms = new GrowableWriter(bitEstimate, indexSize, false);\n\nLoad time improves to ~ 2 seconds.",
    "attachments": {
        "perf.csv": "https://issues.apache.org/jira/secure/attachment/12520487/perf.csv",
        "LUCENE-3932.trunk.patch": "https://issues.apache.org/jira/secure/attachment/12520602/LUCENE-3932.trunk.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-03-29T16:55:31+0000",
            "content": "I agree net/net that change is good; we know the in-RAM image will be at least as large as the tii file so we should make a better guess up front.\n\n3.x is currently in code freeze (for the 3.6.0 release), but I'll commit to trunk's preflex codec.\n\nCan you describe more about your index...?  If your tii fils is 66 MB, how many terms do you have...?  5 seconds is also a long startup time... what's the IO system like? ",
            "author": "Michael McCandless",
            "id": "comment-13241386"
        },
        {
            "date": "2012-03-29T17:22:49+0000",
            "content": "I was doing tests on my local machine with an ssd, and loading is definitely cpu bound.\n\nOur index has 600,000,000 terms.  This is an index of 10,000,000 emails, with associated attachments.  We generate a lot of garbage terms when parsing, things like time stamps, malformed attachments which parse badly, etc.\n\nAfter the change the big time waste is converting the terms from utf8 to utf16 when reading from the .tii file, and then back to utf8 when writing to the in memory store. ",
            "author": "Sean Bridges",
            "id": "comment-13241405"
        },
        {
            "date": "2012-03-29T18:40:00+0000",
            "content": "Nice.  I'd love to know how trunk handles all these terms (we have a more memory efficient terms dict/index in 4.0).\n\nAfter the change the big time waste is converting the terms from utf8 to utf16 when reading from the .tii file, and then back to utf8 when writing to the in memory store.\n\nWhat %tg of the time is spent on the decode/encode (after fixing the initial bitEstimate)?\n\nThat is very silly... fixing that is a somewhat deeper change though.  I guess we'd need to read the .tii file directly (not use SegmentTermEnum), and then copy the UTF8 bytes straight without going through UTF16...\n\nDo you have comparisons with pre-3.5 (before we cutover to this more RAM-efficient (but CPU heavy on load) terms index)?  Probably that less CPU on init, but more RAM held for the lifetime of the reader...? ",
            "author": "Michael McCandless",
            "id": "comment-13241497"
        },
        {
            "date": "2012-03-29T18:56:51+0000",
            "content": "\nOur index has 600,000,000 terms. This is an index of 10,000,000 emails, with associated attachments. We generate a lot of garbage terms when parsing, things like time stamps, malformed attachments which parse badly, etc.\n\nFor an index like that, have you tried specifying termInfosIndexDivisor to your IndexReader as well?\nIf it works with ok performance, then you could remove it adjust termIndexInterval at write-time to have a smaller .tii ",
            "author": "Robert Muir",
            "id": "comment-13241511"
        },
        {
            "date": "2012-03-29T19:33:41+0000",
            "content": "What %tg of the time is spent on the decode/encode (after fixing the initial bitEstimate)?\n\nI've attached a csv of a profiling session with the bitEstimateFix.  The third column is the important one.\n\nutf8 -> utf 16 is 7% of the time\nutf 16 -> utf8 is 16% of the time\n\nwriting vlong's is also 16% of the time, \nTermBufer.read() is 17% of the time (24% if you include the call to utf8ToUtf16) ",
            "author": "Sean Bridges",
            "id": "comment-13241548"
        },
        {
            "date": "2012-03-29T19:55:01+0000",
            "content": "Do you have comparisons with pre-3.5 (before we cutover to this more RAM-efficient (but CPU heavy on load) terms index)? Probably that less CPU on init, but more RAM held for the lifetime of the reader...?\n\nTrying with 3.4 gives a 4 second load time, most of the time spent in SegmentTermEnum.next().\n\n\nFor an index like that, have you tried specifying termInfosIndexDivisor to your IndexReader as well?\nIf it works with ok performance, then you could remove it adjust termIndexInterval at write-time to have a smaller .tii\n\nThanks, I will try that. ",
            "author": "Sean Bridges",
            "id": "comment-13241577"
        },
        {
            "date": "2012-03-30T12:40:15+0000",
            "content": "Patch for trunk; I factored out the int-math-only log function to new static class oal.util.MathUtil, and re-used from one other place. ",
            "author": "Michael McCandless",
            "id": "comment-13242295"
        },
        {
            "date": "2012-03-30T16:49:24+0000",
            "content": "Using the patch on trunk, load time goes from ~5 to ~2 seconds. ",
            "author": "Sean Bridges",
            "id": "comment-13242527"
        },
        {
            "date": "2012-03-30T17:02:10+0000",
            "content": "\nutf8 -> utf 16 is 7% of the time\n utf 16 -> utf8 is 16% of the time\n\nwriting vlong's is also 16% of the time, \n TermBufer.read() is 17% of the time (24% if you include the call to utf8ToUtf16)\n\nSeems like if we made a direct \"decode tii file and write in-memory format\" (instead of going through SegmentTermEnum), we could get some of this back.  The vLongs unfortunately need to be decoded/re-encoded because they are deltas in the file but absolutes in memory.  But, eg the vInt docFreq could be a \"copyVInt\" method instead of readVInt then writeVInt, which should save a bit.\n\nTrying with 3.4 gives a 4 second load time, most of the time spent in SegmentTermEnum.next().\n\nOK, a bit faster than 3.5.  But presumably 3.4 uses much more RAM after startup...?\n\nUsing the patch on trunk, load time goes from ~5 to ~2 seconds.\n\nAwesome, thanks for testing! ",
            "author": "Michael McCandless",
            "id": "comment-13242541"
        },
        {
            "date": "2012-03-30T20:42:04+0000",
            "content": "Seems like if we made a direct \"decode tii file and write in-memory format\" (instead of going through SegmentTermEnum), we could get some of this back. The vLongs unfortunately need to be decoded/re-encoded because they are deltas in the file but absolutes in memory. But, eg the vInt docFreq could be a \"copyVInt\" method instead of readVInt then writeVInt, which should save a bit.\n\nIs the space savings of delta encoding worth the processing time?  You could write the .tii file to disk such that on open you could read it straight into a byte[]. As a test, reading a random 69 meg file into a byte[] takes ~250 ms. ",
            "author": "Sean Bridges",
            "id": "comment-13242721"
        },
        {
            "date": "2012-03-31T10:08:08+0000",
            "content": "Is the space savings of delta encoding worth the processing time? You could write the .tii file to disk such that on open you could read it straight into a byte[].\n\nThis is actually what we do in 4.0's default codec (the index is an FST).\n\nIt is tempting to do that in 3.x (if we were to do another 3.x release after 3.6) ... we'd need to alter other things as well, eg the term bytes are also delta-coded in the file but not in RAM.\n\nI'm curious how much larger it'd be if we stopped delta coding... for your case, how large is the byte[] in RAM (just call dataPagedBytes.getPointer(), just before we freeze it, and print that result) vs the tii on disk...? ",
            "author": "Michael McCandless",
            "id": "comment-13243102"
        },
        {
            "date": "2012-04-02T16:15:39+0000",
            "content": "I'm curious how much larger it'd be if we stopped delta coding... for your case, how large is the byte[] in RAM (just call dataPagedBytes.getPointer(), just before we freeze it, and print that result) vs the tii on disk...?\n\ndataPagedBytes.getPointer() == 124973970\n\nOn disk the .tii file is 69508193 bytes\n\nThe entire index is ~50 gigs. ",
            "author": "Sean Bridges",
            "id": "comment-13244290"
        },
        {
            "date": "2012-04-05T14:12:35+0000",
            "content": "OK I committed this to trunk (thanks Sean!).\n\n\ndataPagedBytes.getPointer() == 124973970\n\nOn disk the .tii file is 69508193 bytes\n\nOK, ~80% bigger... but in the overall index it's minor increase (~0.1%).\n\nBut I think we should hold off on any more 3.x work until/unless we decide to do another release off of it.... ",
            "author": "Michael McCandless",
            "id": "comment-13247239"
        },
        {
            "date": "2012-04-05T15:39:57+0000",
            "content": "Thanks! ",
            "author": "Sean Bridges",
            "id": "comment-13247297"
        },
        {
            "date": "2012-05-22T16:10:54+0000",
            "content": "Can this be ported to 3.6.1 ",
            "author": "Sean Bridges",
            "id": "comment-13281068"
        },
        {
            "date": "2012-05-22T23:52:27+0000",
            "content": "Can this be ported to 3.6.1\n\nI don't think so: it should only be bug fixes in 3.6.x series...\n\nIf we somehow did a 3.7 (which we're hoping not to: hopefully we get 4.0 alpha out instead) then this could be backported for that... ",
            "author": "Michael McCandless",
            "id": "comment-13281322"
        }
    ]
}