{
    "id": "LUCENE-3838",
    "title": "IndexWriter.maybeMerge() removes deleted documents from index (Lucene 3.1.0 to 3.5.0)",
    "details": {
        "labels": "",
        "priority": "Blocker",
        "components": [
            "core/index"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "3.1,                                            3.2,                                            3.3,                                            3.4,                                            3.5",
        "resolution": "Invalid",
        "status": "Resolved"
    },
    "description": "My company uses Lucene for high performance, heavy loaded farms of translation repositories with hundreds of simultaneous add/delete/update/search/retrieve threads. In order to support this complex architecture beside other things and tricks used here I rely on docId-s being unchanged until I ask that explicitly (using IndexWriter.optimize() - IndexWriter.forceMerge()).\n\nFor this behavior LogMergePolicy is used.\n\nThis worked fine until we raised the Lucene version from 3.0.2 to 3.5.0. Until version 3.1.0 merge triggerred by IndexWriter.addDocument() didn't expunge deleted documents ensuring that docId-s stayed unchanged and making some critical jobs possible without impact on index size. IndexWriter.optimize() did the actual deleted documents removal.\n\nFrom Lucene version 3.1.0 IndexWriter.maybeMerge() does the same thing as IndexWriter.forceMerge() regarding deleted documents. There is no difference. This leads to unpredictable internal index structure changes during simple document add (and possible delete) operations and in undefined point in time. I looked into the Lucene source code and can definitely confirm this.\n\nThis issue makes our Lucene client code totally unusable.\n\nSolution steps:\n\n1) add a flag somewhere that will control whether the deleted documents should be removed in maybeMerge(). Note that this is only a half of what we need here.\n2) make forceMerge() always remove deleted documents no matter if maybeMerge() removes them or not. Alternatively, there can be another parameter added to forceMerge() that will also tell if deleted documents should be removed from index or not.\n\nThe sample JUnit code that can replicate this issue is added below.\n\n\n\npublic class TempTest {\n\n    private Analyzer _analyzer = new KeywordAnalyzer();\n\n    @Test\n    public void testIndex() throws Exception {\n\tFile indexDir = new File(\"sample-index\");\n\tif (indexDir.exists()) \n{\n\t    indexDir.delete();\n\t}\n\n\tFSDirectory index = FSDirectory.open(indexDir);\n\n\tDocument doc;\n\n\tIndexWriter writer = createWriter(index, true);\n\ttry \n{\n\t    doc = new Document();\n\t    doc.add(new Field(\"field\", \"text0\", Field.Store.YES,\n\t\t    Field.Index.ANALYZED));\n\t    writer.addDocument(doc);\n\n\t    doc = new Document();\n\t    doc.add(new Field(\"field\", \"text1\", Field.Store.YES,\n\t\t    Field.Index.ANALYZED));\n\t    writer.addDocument(doc);\n\n\t    doc = new Document();\n\t    doc.add(new Field(\"field\", \"text2\", Field.Store.YES,\n\t\t    Field.Index.ANALYZED));\n\t    writer.addDocument(doc);\n\n\t    writer.commit();\n\t}\n finally \n{\n\t    writer.close();\n\t}\n\n\tIndexReader reader = IndexReader.open(index, false);\n\ttry {\n\t    reader.deleteDocument(1);\n\t} finally {\n\t    reader.close();\n\t}\n\n\twriter = createWriter(index, false);\n\ttry {\n\t    for (int i = 3; i < 100; i++) {\n\t\tdoc = new Document();\n\t\tdoc.add(new Field(\"field\", \"text\" + i, Field.Store.YES,\n\t\t\tField.Index.ANALYZED));\n\t\twriter.addDocument(doc);\n\n\t\twriter.commit();\n\t    }\n\t} finally {\t    writer.close();\t}\n\n\tboolean deleted;\n\tString text;\n\n\treader = IndexReader.open(index, true);\n\ttry \n{\n\t    deleted = reader.isDeleted(1);\n\t    text = reader.document(1).get(\"field\");\n\t}\n finally \n{\n\t    reader.close();\n\t}\n\n\tassertTrue(deleted); // This line breaks\n\tassertEquals(\"text1\", text);\n    }\n\n    private MergePolicy createEngineMergePolicy() \n{\n\tLogDocMergePolicy mergePolicy = new LogDocMergePolicy();\n\n\tmergePolicy.setCalibrateSizeByDeletes(false);\n\tmergePolicy.setUseCompoundFile(true);\n\tmergePolicy.setNoCFSRatio(1.0);\n\n\treturn mergePolicy;\n    }\n\n    private IndexWriter createWriter(Directory index, boolean create)\n\t    throws Exception \n{\n\tIndexWriterConfig iwConfig = new IndexWriterConfig(Version.LUCENE_35,\n\t\t_analyzer);\n\n\tiwConfig.setOpenMode(create ? IndexWriterConfig.OpenMode.CREATE\n\t\t: IndexWriterConfig.OpenMode.APPEND);\n\tiwConfig.setMergePolicy(createEngineMergePolicy());\n\tiwConfig.setMergeScheduler(new ConcurrentMergeScheduler());\n\n\treturn new IndexWriter(index, iwConfig);\n    }\n\n}",
    "attachments": {
        "TempTest.java": "https://issues.apache.org/jira/secure/attachment/12516825/TempTest.java"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-03-02T12:39:37+0000",
            "content": "Sorry for the test added in description not being formatted correctly. TempTest.java is attached also. ",
            "author": "Ivan Stojanovic",
            "id": "comment-13220877"
        },
        {
            "date": "2012-03-04T10:34:11+0000",
            "content": "Lucene's maybeMerge, even in 3.1.0, will merge away deleted documents; I'm not sure why you don't see that happening.\n\nReally, when Lucene reclaims deletions and renumbers its documents, is an internal implementation detail.  Applications should not rely on this behavior.  Can you add your own ID field to the index?  Or, alternatively, never delete documents but instead use a filter in the application to skip the documents.  Or, in 4.0 (trunk), you could perhaps make a custom codec that \"pretends\" there are no deletions when merging runs... ",
            "author": "Michael McCandless",
            "id": "comment-13221842"
        },
        {
            "date": "2012-03-05T13:45:14+0000",
            "content": "Hi Michael,\n\nof course this happens in version 3.1.0 as stated in the title (in parentheses).\n\nActually, it has never been stated that this is an internal implementation detail (if I can remember correctly). I'm very sure that we are not the only ones who were relying on this behavior. Also, this backward compatibility break wasn't stated in 3.1.0 changes log.\n\nAnyway, we already have an ID field but we can't rely on it for long running operations. Suppose that an index export is in progress while there is a bunch of add/delete/search operations. Or worse, suppose that a batch delete is in progress (driven by a filter criteria) at the same time. I have to say here that we are using only one searcher per index here and also we are working with farms of indexes with size of 3-5 millions of documents per index. I can't even imagine the use of more than one searcher per index here. One searcher per index also gives us the best performance which is our top concern. Another thing. When an admin performs optimization then the index is locked so no one can access it in order to avoid disk overuse.\n\nWe also have a deletes filter \nIt is used as a ram filter that buffers deletes using BitSet and occasionally flushing this buffer to index (deleting documents marked as deletes). This gives us the lightning performance related to both deleting documents and search operation relying on a custom Collector wrapping this filter. If we used an application filter to skip documents than the search would suffer a significant slow down because of communication with an application filter for each document retrieved from index. if we do this than our ultra fast Lucene driven application loses its sense.\n\nThe suggestion with a custom codec sounds very promising. I only don't understand if we will have to wait for a Lucene 4.0 release for a custom codec implementation (with maybe an API that allows that) or I need to implement it for Lucene trunk. If I need to implement it for trunk than can you please give me a starting point where to begin from? I must say that I haven't dived deep in Lucene merge functionality. Also, can this approach differentiate between maybeMerge() and forceMerge(). We need to support document removal in forceMerge(), of course.\n\nGreatest regards,\nIvan ",
            "author": "Ivan Stojanovic",
            "id": "comment-13222347"
        },
        {
            "date": "2012-03-07T17:17:20+0000",
            "content": "of course this happens in version 3.1.0 as stated in the title (in parentheses).\n\nSorry, what I meant was Lucene has, forever, removed deleted docs\nduring natural merges (as well as forced merges); I'm not sure why\nin 3.0.2 you're seeing otherwise...\n\nActually, it has never been stated that this is an internal implementation detail (if I can remember correctly).\n\nI don't know whether this is officially documented anywhere, but it\ncomes up every so often on the lists and the answer is always \"don't\nrely on Lucene's docID\"... or, rather \"rely on docID at your own risk\".\n\nAnyway, we already have an ID field but we can't rely on it for long running operations.\n\nI didn't fully understand why you can't use your ID field for long\nrunning operations...\n\nI only don't understand if we will have to wait for a Lucene 4.0 release for a custom codec implementation \n\nYou'd have to use Lucene trunk (not yet released) to work with codecs.\n\nIf I need to implement it for trunk than can you please give me a starting point where to begin from?\n\nStart here I think?\n\n  http://wiki.apache.org/lucene-java/HowToContribute\n\nEG, get a trunk checkout, browse trunk's javadocs, look @ the test cases, etc.\n\nAlso, can this approach differentiate between maybeMerge() and forceMerge().\n\nMaybe... eg a MergePolicy/Scheduler knows if a given merge request was\n\"forced\" or not. ",
            "author": "Michael McCandless",
            "id": "comment-13224511"
        },
        {
            "date": "2016-01-27T01:08:48+0000",
            "content": "Hmm, ran across this searching for something else, should we close\nMichael McCandless? ",
            "author": "Erick Erickson",
            "id": "comment-15118398"
        },
        {
            "date": "2016-01-27T01:19:39+0000",
            "content": "Yes, thanks for the reminder Erick Erickson. ",
            "author": "Michael McCandless",
            "id": "comment-15118411"
        }
    ]
}