{
    "id": "SOLR-3755",
    "title": "shard splitting",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.3",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "We can currently easily add replicas to handle increases in query volume, but we should also add a way to add additional shards dynamically by splitting existing shards.",
    "attachments": {
        "SOLR-3755-CoreAdmin.patch": "https://issues.apache.org/jira/secure/attachment/12568765/SOLR-3755-CoreAdmin.patch",
        "SOLR-3755.patch": "https://issues.apache.org/jira/secure/attachment/12542883/SOLR-3755.patch",
        "SOLR-3755-combinedWithReplication.patch": "https://issues.apache.org/jira/secure/attachment/12573279/SOLR-3755-combinedWithReplication.patch",
        "SOLR-3755-combined.patch": "https://issues.apache.org/jira/secure/attachment/12573116/SOLR-3755-combined.patch",
        "SOLR-3755-testSplitter.patch": "https://issues.apache.org/jira/secure/attachment/12566761/SOLR-3755-testSplitter.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Yonik Seeley",
            "id": "comment-13440698",
            "date": "2012-08-23T21:48:01+0000",
            "content": "We need to associate hash ranges with shards and allow overlapping shards (i.e. 1-10, 1-5,6-10)\n\nGeneral Strategy for splitting w/ no service interruptions:\n\n\tBring up 2 new cores on the same node, covering the new hash ranges\n\tBoth cores should go into recovery mode (i.e. leader should start\nforwarding updates)\n\tleaders either need to consider these new smaller shards as replicas, or they need to forward to the \"leader\" for the new smaller shard\n\tsearches should no longer go across all shards, but should just span the complete hash range\n\tleader does a hard commit and splits the index\n\tSmaller indexes are installed on the new cores\n\tOverseer should create new replicas for new shards\n\tMark old shard as \u201cretired\u201d \u2013 some mechanism to shut it down (after there is an acceptable amount of coverage of the new shards via replicas)\n\n\n\nFuture: allow splitting even with \u201ccustom\u201d shards "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13443746",
            "date": "2012-08-29T02:02:19+0000",
            "content": "Draft (unfinished) patch just to let people know where I'm going...  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13445337",
            "date": "2012-08-30T21:55:41+0000",
            "content": "OK, here's a patch with minimal functionality that seems to work (no tests yet though):\n\n\ncurl \"http://localhost:8983/solr/admin/cores?core=collection1&action=SPLIT&path=/tmp/1&path=/tmp/2\"\n\n\n\nThat command will split the index in two.  It will not consider the existing \"range\" of the current core or create new cores or anything like that yet... it only splits the index and writes them in separate directories.  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13445362",
            "date": "2012-08-30T22:35:09+0000",
            "content": "Since this doesn't change any existing func, I've committed what I have now to enable easier integration/modification by others. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13445371",
            "date": "2012-08-30T22:43:02+0000",
            "content": "Hi Yonik,\nlooks nice, similar to oal.index.MultiPassIndexSplitter / PKIndexSplitter in misc module just using a HashPartitioned LiveDocs. I am just confused, why does it not use FixedBitSet? The length is fixed and no (int) casts needed.\n\n+1 otherwise "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13445376",
            "date": "2012-08-30T22:46:25+0000",
            "content": "\n// TODO: will many deletes have been removed, or should we optimize?\n\n\n\nThe merged indexes will have no deletions at all, because it merges not copies. IndexWriter.addIndexes(IndexReader...) does the same like a standard Lucene merge, IndexWriter.addIndexes(Directory) just copies the segment files. This is a plain stupid merge of a segment that has additional, overlaid deletions. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13445391",
            "date": "2012-08-30T22:58:10+0000",
            "content": "I am just confused, why does it not use FixedBitSet? \n\nHabit... OpenBitSet is just the class I'm used to and my fingers automatically type.\n\nThe merged indexes will have no deletions at all, because it merges not copies.\n\nCool, thanks for the clarification - I'll update the comment in my local copy. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13446054",
            "date": "2012-08-31T15:37:27+0000",
            "content": "So we need to have new cores up and running, and then install the new indexes in them.\nWe could either do it like replication and use a new index directory (and use a property file to redirect to that latest index), or we could try and make sure that there is no open writer on the new core and then split directly into the normal core index directory.\nedit: we could also simply use the existing writers on the new cores to \"merge in\" the new indexes (we just need to ensure that they are empty) "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13447347",
            "date": "2012-09-03T16:42:32+0000",
            "content": "Just committed some more progress.\n\nhttp://svn.apache.org/viewvc?rev=1380287&view=rev\n\nI started up a ZK cluster with one shard, one node.\ncurl \"http://localhost:8983/solr/admin/cores?core=collection1&action=SPLIT\"\n\nThe cloud state after looks like\n\n{\"collection1\":{\n    \"shard1\":{\"Rogue:8983_solr_collection1\":{\n        \"shard\":\"shard1\",\n        \"roles\":null,\n        \"leader\":\"true\",\n        \"state\":\"active\",\n        \"core\":\"collection1\",\n        \"collection\":\"collection1\",\n        \"node_name\":\"Rogue:8983_solr\",\n        \"base_url\":\"http://Rogue:8983/solr\"}},\n    \"shard1_0\":{\"Rogue:8983_solr_collection1_0\":{\n        \"shard\":\"shard1_0\",\n        \"leader\":\"true\",\n        \"roles\":null,\n        \"state\":\"active\",\n        \"core\":\"collection1_0\",\n        \"collection\":\"collection1\",\n        \"node_name\":\"Rogue:8983_solr\",\n        \"base_url\":\"http://Rogue:8983/solr\"}},\n    \"shard1_1\":{\"Rogue:8983_solr_collection1_1\":{\n        \"shard\":\"shard1_1\",\n        \"roles\":null,\n        \"leader\":\"true\",\n        \"state\":\"active\",\n        \"core\":\"collection1_1\",\n        \"collection\":\"collection1\",\n        \"node_name\":\"Rogue:8983_solr\",\n        \"base_url\":\"http://Rogue:8983/solr\"}}}}\n\n\n\nThe original core had 32 docs.  After I did a manual commit on both of the new cores, the first showed 14 docs and the second 18. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13447400",
            "date": "2012-09-03T19:52:18+0000",
            "content": "It seems like we need logical shard parameters (i.e. Slice class), but we don't currently have a place for them.\nThese parameters would include:\n\n\tcollection (this is somewhat redundant, but belongs more on a slice than on a replica)\n\treplication factor (i.e. in time based sharding, one may want more replicas of recent shards to handle greater query throughput)\n\thash range(s) covered by the slice\n\tmaybe a pointer to the leader, rather than having to search through the nodes?\n\n\n\nYou can see the previous structure of cloudstate from my previous message.\n\nOne fix is to introduce a \"nodes\" or \"replicas\" level to contain the nodes and leave the other properties as top-level:\n\n\n  \"shard1\": {\n    \"replication_factor\" : 3,\n    \"range\" : \"00000000-3fffffff\",\n    \"nodes\" : {\n      \"Rogue:8983_solr_collection1\":{\n        \"state\" : \"active\"\n      }\n    }\n  }\n\n\n\nAnother way is to introduce a \"props\" to store properties:\n\n\n  \"shard1\": {\n    \"props\" : {\n      \"replication_factor\" : 3,\n      \"range\" : \"00000000-3fffffff\"\n    },\n    \"Rogue:8983_solr_collection1\":{\n      \"state\" : \"active\"\n    }\n  }\n\n\n\nThe first option feels more natural to me - properties are directly under the shard, and the nodes of a shard are simply another property. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13447738",
            "date": "2012-09-04T15:05:33+0000",
            "content": "Yeah, I like the first option as well. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-13447851",
            "date": "2012-09-04T17:48:54+0000",
            "content": "+1 on the first option.  I think it's considered good JSON practice to have key names not contain state, but I can't remember where I saw that. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13450033",
            "date": "2012-09-06T20:52:19+0000",
            "content": "I've run into a few impedance mismatch issues implementing the JSON above.\nInternally we seem to use ZkNodeProps which accepts Map<String,String>... but a JSON Map is better represented as a Map<String,Object>.\n\nI think I'll try going in the following direction:\n\n\tMake ZkNodeProps that accepts Map<String,Object> as properties, and can thus represent integers and more complex types.  This will be just like a Map, but add some convenience methods\n\tMake Slice subclass ZkNodeProps\n\tMake a new Replica class (instead of just representing it as a generic ZkNodeProps)\n\n\n\nIn general, to construct these classes from JSON, it seems like we should just pass the Map<String,Object> generated from the JSON parser and then the constructor can pull out key elements and construct sub-elements.\n\nThoughts? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13451408",
            "date": "2012-09-08T21:01:02+0000",
            "content": "Make Slice subclass ZkNodeProps\n\nAfter a lot of code modification, I've realized that \"ZkNodeProps\" was probably supposed to be the same as \"Replica\".  I was fooled by thinking it was generic properties in ZK on any type of node (slice, replica, or whatever), and that was reinforced by it's use in other context as generic properties (messages in the overseer queue use ZkNodeProps as general properties - Overseer.java:125)\n\nGiven that Node also has another meaning (A Node is a CoreContainer/JVM that can contain multiple cores), I'm leaning toward renaming ZkNodeProps to Replica, and making a truly generic class ZkProps that Replica, Slice, etc, can subclass from.\n\nHere's an example of the types of code changes I've been making to hopefully make things more readable:\n\n\n-    for (Map.Entry<String,Slice> entry : slices.entrySet()) {\n-      Slice slice = entry.getValue();\n-      Map<String,ZkNodeProps> shards = slice.getShards();\n-      Set<Map.Entry<String,ZkNodeProps>> shardEntries = shards.entrySet();\n-      for (Map.Entry<String,ZkNodeProps> shardEntry : shardEntries) {\n-        final ZkNodeProps node = shardEntry.getValue();\n-        if (clusterState.liveNodesContain(node.get(ZkStateReader.NODE_NAME_PROP))) {\n-          return new ZkCoreNodeProps(node).getCoreUrl();\n+    for (Slice slice : slices.values()) {\n+      for (Replica replica : slice.getReplicas()) {\n+        if (clusterState.liveNodesContain(replica.get(ZkStateReader.NODE_NAME_PROP))) {\n+          return new ZkCoreNodeProps(replica).getCoreUrl();\n\n\n\nUnfortunately, when I got all done, ZK related tests were no longer passing.\nI'm going to try and make another attempt and see if I can make more incremental changes (so that I can run tests periodically). "
        },
        {
            "author": "Radim Kolar",
            "id": "comment-13474931",
            "date": "2012-10-12T10:37:19+0000",
            "content": "Useful theory about rehashing http://en.wikipedia.org/wiki/Consistent_hashing "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13550477",
            "date": "2013-01-10T21:58:44+0000",
            "content": "This has a back compat break that we should address somehow or at least mention in changes - previously you could specify explicit shard ids and still get distributed updates - now if you do that, you won't get distrib updates as shards won't be assigned ranges. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13552307",
            "date": "2013-01-13T20:50:05+0000",
            "content": "OK, after chatting w/ Mark a bit, this seems to be his use-case: A pre-configured cluster w/ no information yet in ZK.\nCurrently implemented via:\n\n\tconfiguring the collection & shard of each core in solr.xml\n\tbring all of those cores up\n\tstart indexing  (and in 4.0 style, the correct shard is picked via hashing and splitting up the range according to the currently known shards)\n\n\n\nThis 4.0 behavior could be replicated via a \"lazyHash\" router that simply splits the hash range over currently know shards at the time of every request.  This is fragile and error prone for many users of course, so it would not be a default.  Additionally, we would need code to explicitly specify the router for a collection (assuming the collection had not already been created).\n\nSomewhat related: control naming of shards.  This could be applicable for both hashing based collections and custom sharding based collections.  shardNames=myshard1,myshard2,myshard3? "
        },
        {
            "author": "Dmitry Kan",
            "id": "comment-13553117",
            "date": "2013-01-14T21:10:54+0000",
            "content": "\"Somewhat related: control naming of shards. This could be applicable for both hashing based collections and custom sharding based collections. shardNames=myshard1,myshard2,myshard3?\"\n\nWould this suit to logical (e.g. date based) sharding as well? Do you plan to support such a sharding type in the current shard splitting implementation? Not sure, if this helps: we have implemented our own custom date based sharding (splitting and routing) for solr 3.x and found it to be the most logical way of sharding our data (both from the load balancing and use case point of view). The routing implementation is done via loading a custom shards config file that contains mapping of date ranges to shards. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13564188",
            "date": "2013-01-28T10:44:48+0000",
            "content": "I'm planning to split the \"split\" API into a high-level collections API and a low-level core admin API.\n\nThe low level core admin API may not be solr cloud aware but would work completely on a single node. This way, a non solr cloud aware node can also make use of a split index feature.\n\nThe Core Admin command may be invoked as:\n\nhttp://host:port/solr/admin/cores?core=<core_name>&action=SPLIT&path=/path/1&path=/path/2&path=/path/3\n\n\nor, as:\n\nhttp://host:port/solr/admin/cores?core=<core_name>&action=SPLIT&targetCore=core1&targetCore=core2\n\n\nThe \"path\" and \"targetCore\" parameter is multi-valued and the collection will be split into as many number of sub-shards as the number of \"path\" values.\n\nThe collections api may be invoked as follows:\n\nhttp://host:port/solr/admin/collections?action=SPLIT&shard=<shard_1>&shard=<shard_2>\n\n\nSometimes, shard names are automatically assigned by SolrCloud and it may be more convenient for users to specify shards by shard keys instead of shard names e.g.\n\nhttp://host:port/solr/admin/collections?action=SPLIT&shard.keys=shardKey1,shardKey2\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13564201",
            "date": "2013-01-28T11:20:50+0000",
            "content": "We need to introduce shard states into the design. SolrCloud shards are always \u201cactive\u201d i.e. no state information is associated with shards presently. I'm planning to add two new states viz. \u201cConstruction\u201d and \u201cRecovery\u201d besides the default \u201cActive\u201d state.\n\nA shard in \u201cConstruction\u201d state has the following properties:\n\n\tShard nodes receive no queries\n\tShard nodes receive no updates except those forwarded by leaders\n\tOverseer does not allocate nodes to such a shard automatically\n\tLeader election is disabled for such a shard\n\tShard nodes automatically go into recovering state (buffering update mode)\n\n\n\nA shard in \u201cRecovering\u201d phase is similar to a shard in \"Construction\" state except that shard nodes automatically go into recovering state (\u201cAPPLYING_BUFFERED\u201d mode and once completed into active state).\n\nWe could merge the two states together if necessary once we start implementing stuff. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13564207",
            "date": "2013-01-28T11:33:05+0000",
            "content": "Thinking more about the general strategy that Yonik's devised for this feature, here is a rough draft of how it may go.\n\nA split operation is triggered via collections API\n\n\tOverseer Collection Processor (CP) creates new sub-shard in ZK in \"Construction\" state s.t. first node to join the shard becomes the leader and thereafter leaders are not elected automatically. Replicas are not automatically created in the \u201cConstruction\u201d state\n\tCP creates new cores on leader using the core/cloud descriptors of parent core.\n\t\n\t\tSuch cores are automatically designated as leader for respective sub-shard\n\t\tThese new cores join sub-shards in buffering-update mode and keep themselves in that mode until the shard changes its state.\n\t\tDUPF on parent forwards only relevant updates to sub-shard core.\n\t\n\t\n\tCP calls CoreAdmin split on leader of shard\n\t\n\t\tA hard commit is called and index is split and written into correct cores\n\t\n\t\n\tCP puts shard into Recovery state\n\t\n\t\tSub-shard cores go into apply-buffered-updates mode.\n\t\tCP puts a watch on sub-shard cores status\n\t\n\t\n\tOnce sub-shard core status becomes active, Overseer creates replicas and watches their state\n\tOnce a number of replicas (ceil(numReplicas/2) is enough?) have recovered for all sub-shards, atomically set sub-shard active and parent shard in-active.\n\n\n\nSuggestions/comments welcome. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13564284",
            "date": "2013-01-28T13:46:44+0000",
            "content": "A simple test for SolrIndexSplitter is attached "
        },
        {
            "author": "Commit Tag Bot",
            "id": "comment-13575205",
            "date": "2013-02-09T17:20:11+0000",
            "content": "[trunk commit] Shalin Shekhar Mangar\nhttp://svn.apache.org/viewvc?view=revision&revision=1444397\n\nSOLR-3755: Test for SolrIndexSplitter "
        },
        {
            "author": "Commit Tag Bot",
            "id": "comment-13575207",
            "date": "2013-02-09T17:32:11+0000",
            "content": "[branch_4x commit] Shalin Shekhar Mangar\nhttp://svn.apache.org/viewvc?view=revision&revision=1444398\n\n\u0096SOLR-3755: Test for SolrIndexSplitter "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13575496",
            "date": "2013-02-10T19:02:32+0000",
            "content": "Change for CoreAdmin split to not create cores. It accepts core names to which split index should be merged. "
        },
        {
            "author": "Commit Tag Bot",
            "id": "comment-13582829",
            "date": "2013-02-21T02:42:36+0000",
            "content": "[branch_4x commit] Shalin Shekhar Mangar\nhttp://svn.apache.org/viewvc?view=revision&revision=1447517\n\nSOLR-3755: Do not create core on split action, use 'targetCore' param instead "
        },
        {
            "author": "Commit Tag Bot",
            "id": "comment-13582845",
            "date": "2013-02-21T02:42:49+0000",
            "content": "[trunk commit] Shalin Shekhar Mangar\nhttp://svn.apache.org/viewvc?view=revision&revision=1447516\n\nSOLR-3755: Do not create core on split action, use 'targetCore' param instead "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13585668",
            "date": "2013-02-25T06:56:28+0000",
            "content": "Any suggestions/feedback on the earlier comment about the Collections API would be good. Here's what the collections API call(s) would look like:\n\n\"The collections api may be invoked as follows:\nhttp://host:port/solr/admin/collections?action=SPLIT&shard=<shard_1>&shard=<shard_2>\n\nSometimes, shard names are automatically assigned by SolrCloud and it may be more convenient for users to specify shards by shard keys instead of shard names e.g.\nhttp://host:port/solr/admin/collections?action=SPLIT&shard.keys=shardKey1,shardKey2\" "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13588809",
            "date": "2013-02-27T21:41:53+0000",
            "content": "\n\"The collections api may be invoked as follows:\nhttp://host:port/solr/admin/collections?action=SPLIT&shard=<shard_1>&shard=<shard_2>\n\nOk, I assume this is for splitting more than one shard (i.e. both shard_1 and shard_2 in this example will be split?)\nHow do we know what collection?  I assume there will be a \"collection\" parameter?\n\nRelated: SOLR-4503 - we now have the capability to use restlet, and should consider doing so for new APIs like this.\n\n\nSometimes, shard names are automatically assigned by SolrCloud and it may be more convenient for users to specify shards by shard keys instead of shard names e.g.\nhttp://host:port/solr/admin/collections?action=SPLIT&shard.keys=shardKey1,shardKey2\"\n\nshard.keys is currently used in routing request (and the values are often not shard names), so we probably shouldn't overload it here.  After all, it may make sense in the future to be able to use shard.keys to specify which shard you want to split!\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13591412",
            "date": "2013-03-02T14:06:14+0000",
            "content": "How do we know what collection? I assume there will be a \"collection\" parameter?\n\nYes, a collection param will also be present.\n\nshard.keys is currently used in routing request (and the values are often not shard names), so we probably shouldn't overload it here. After all, it may make sense in the future to be able to use shard.keys to specify which shard you want to split!\n\nYes! That is exactly the thinking behind shard.keys here. It is not being overloaded but used to indicate which shard to split by specifying the key which resolves to a shard name.\n\nRelated: SOLR-4503 - we now have the capability to use restlet, and should consider doing so for new APIs like this.\n\nI'm not familiar with restlet. I'll take a look at it. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13596385",
            "date": "2013-03-07T21:19:12+0000",
            "content": "I'd like to suggest supporting only a single shard through this API. It may be called multiple times for more than one shards.\n\nIn the future however, we may want to have a split API call which splits all existing shards, but that could be a different thing (if required). "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13596853",
            "date": "2013-03-08T05:50:49+0000",
            "content": "I'd like to suggest supporting only a single shard through this API.\n\n+1 "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13598968",
            "date": "2013-03-11T16:45:21+0000",
            "content": "Attached is a first iteration of the patch. It is not working yet but a lot of the pieces required for this feature are coming together in this patch. Soon, I'd like to split it into smaller, committable issues/patches.\n\nThis is a very rough cut but any comments/suggestions on the general approach and/or the patch will be very helpful.\n\nChanges:\n\n\tOverseerCollectionProcessor has a new command called splitshard which executes the whole strategy\n\tImplicit support for creating new shards (by creating a core with a new shard in \"construction\" state)\n\t\"Construction\" state is introduced for shards\n\tCoreAdmin has a new APPLYBUFFEREDUPDATES task which, as the name says, instructs a core to apply all buffered updates\n\n\n\nTODO:\n\n\tCreate replicas for new sub-shards\n\tFigure out how and where to set sub-shards as active and divert traffic away from parent shard\n\tTests (both unit and ChaosMonkey based)\n\n\n\nRight now, I'm running into a bug where the first sub-shard creation goes through but the second sub-shard coreadmin \"CREATE\" command blocks in .ShardLeaderElectionContext.waitForReplicasToComeUp(ElectionContext.java:326)\n\nCan anyone tell me what I've overlooked? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13599007",
            "date": "2013-03-11T17:13:25+0000",
            "content": "ShardLeaderElectionContext.waitForReplicasToComeUp(ElectionContext.java:326)\n\nYou probably need to avoid this call in this case - it's really for starting up a cold previously started cluster. It's a little odd that it hits on the second sub shard and not the first. I might be able to take a closer look when I get some free time. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13599793",
            "date": "2013-03-12T07:18:06+0000",
            "content": "Added replica creation to the earlier 'combined' patch that Shalin had put up. This is yet to be tested as we're yet to fix the 2nd core creation issue. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13600489",
            "date": "2013-03-12T21:24:10+0000",
            "content": "I think collection might be a better param name than name for the shard split api "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13600749",
            "date": "2013-03-13T02:52:57+0000",
            "content": "Well that was a b**ch \n\nHere is a patch that should unblock you. I'll probably spin my changes into one or two other JIRA's.\n\nThere were some recent changes around what slices are returned when during some of the cluster state refactoring. It's the first time I've stumbled upon it, and I'm not sure what I think of it, it seems somewhat easy to screw up. Anyway, for now I've left it and added some changes to get this working better. It still fails in the split code, but it should get you to new ground. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13600774",
            "date": "2013-03-13T03:37:18+0000",
            "content": "I filed SOLR-4566 for the main issue. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13600853",
            "date": "2013-03-13T05:48:27+0000",
            "content": "We wouldn't want any shard assignment/replica addition normally to go to a non-active Slice. I think changing the AssignShard to use getAllSlices may do what we're trying to avoid here. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13601087",
            "date": "2013-03-13T12:57:17+0000",
            "content": "Here is another patch layered on top of the committed SOLR-4566 work. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13601090",
            "date": "2013-03-13T12:58:23+0000",
            "content": "We wouldn't want any shard assignment/replica addition normally to go to a non-active Slice. I think changing the AssignShard to use getAllSlices may do what we're trying to avoid here.\n\nNo, I think you are confusing slice/shard state. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13601143",
            "date": "2013-03-13T13:51:16+0000",
            "content": "Anshum caught me up in chat - I am actually the one that is confused - because slice state stuff has already been committed. I thought I was looking at pre shard splitting trunk code.\n\nThe real problem here is how the slice state is being handled in relation to clusterstate.json updates - you can lose inactive slices from the clusterstate which will cause havoc. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13601175",
            "date": "2013-03-13T14:19:01+0000",
            "content": "So to summarize - to fix this current problem, I think we want to rework the current slice state stuff in trunk - I left open SOLR-4566 for the moment.\n\nI think the cleanest thing for this API, and what will help keep the current issue from reoccuring, is if we change getSlices and getSliceMap to return all slices always.\n\nThen we add getActiveSlices and getActiveSliceMap, and appropriate calls are changed to that. Then there are likely to be less surprises when we try and copy/update the clusterstate. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13601184",
            "date": "2013-03-13T14:29:47+0000",
            "content": "Thanks for the suggestions on that one Mark. I'll put up a patch soon for SOLR-4566 on the lines of what we discussed and what you've mentioned above.\n\nThough again, as we're not really using the states anywhere but in the patch for ShardSplitting, it should have no impact.\nHowever, just as a note, any future use of getSlices would mean handing inactive slices (or calling getActiveSlices) so the behaviour would change a bit (as we start using more of Slice states). "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13601422",
            "date": "2013-03-13T17:49:34+0000",
            "content": "SOLR-4568 is another issue I found while working on this - I'll pull the fix from my patch to SOLR-4568. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13601432",
            "date": "2013-03-13T17:56:39+0000",
            "content": "SOLR-4569 is another small improvement issue I'll pull from my patch here. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13601439",
            "date": "2013-03-13T18:02:39+0000",
            "content": "SOLR-4570 filed as well - another issue with a solution in my patch. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13605353",
            "date": "2013-03-18T16:57:29+0000",
            "content": "Patch updated over Mark's changes.\n\nChanges:\n1. Merged to latest trunk\n2. New way of sub-shard replication in DistributedUpdatedProcessor\n3. New operation \"updateshardstate\" in Overseer to change logical state of a shard\n4. Many bug fixes\n5. A slightly better test \u2013 still ways to go\n\nAnshum and I have been collaborating on building this feature. Since the patch was getting difficult to maintain without overwriting each others' changes, we created a fork in github to develop this feature. The attached patch is a diff against revision 1457757 of trunk (created by git).\n\nThe test case fails right now because the number of documents in sub-shards are not correct (they are the same as the parent shard). Strangely, even if we disable the index split part of the code, the number of documents in parent and sub-shards are equal which makes me believe that there is a bug in core creation or the way the test is setup. I'm looking into that right now.\n\nThe switch to make the parent shard in-active and the sub-shards active is also not correct because it doesn't wait for the sub-shard replicas to recover completely. We need to figure out a way to wait for replicas to be up and running before we switch.\n\nThe way DUPF forwards updates to sub-shard replicas may also open us to race conditions when we switch shard states. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13605354",
            "date": "2013-03-18T16:58:15+0000",
            "content": "Btw, the github fork is at https://github.com/shalinmangar/lucene-solr "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13607756",
            "date": "2013-03-20T15:54:16+0000",
            "content": "Updated patch created by git on svn r1458857.\n\nThe test passes \u2013 I had forgotten to add shards or distrib=false param and of course the document count was same on parent and sub-shards  "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13609309",
            "date": "2013-03-21T18:59:08+0000",
            "content": "Changes:\n\n\n\tFixed off-by-one mistake in replica creation\n\tCommit before split\n\tBetter sub-shard node addition logic using shard ranges. Defensive checks in DUPF are modified to allow sub-shard replication. A new param distrib.from.shard is added to forwarded requests if node list contains sub-shard leaders.\n\tModified ShardSplitTest to index docs constantly (to check sub-shard replication).\n\n\n\nThe test fails currently because the sub-shards now have more docs than they should have. I'm investigating this.\n\nPatch is created on top of trunk r1459441 via git. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13614251",
            "date": "2013-03-26T16:30:34+0000",
            "content": "There'are more changes on the branch, including a ChaosMonkey test for the feature. Any feedback on the design/strategy would be good.\n\nAlso, I'm working on adding some more documentation on the general strategy somewhere in the code/package and improving the javadoc for the same as well. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13615249",
            "date": "2013-03-27T13:24:40+0000",
            "content": "Hope to take a look at what you guys have been up to again soon. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13617490",
            "date": "2013-03-29T16:30:54+0000",
            "content": "Changes:\n\n\n\tMake splitshard retryable by unloading cores part of a sub-shard and starting from scratch again. If a sub-shard is already in active state, splitshard will fail.\n\tUse PRERECOVERY core admin command to make the parent shard leader wait for sub shard cores to be active\n\tSimilar to point 2 above, use the PRERECOVERY command to wait for sub shard replicas to be available before switching the shard states\n\tThe mismatch between the expected and actual number of documents was because the SolrIndexSplitter does not take the type of the unique key field into account while calculating the hash. I changed it to use indexedToReadable to make it compute the correct hash. This needs to be reviewed for performance implications.\n\tShardSplitTest passes.\n\n\n\nI'll be working to add more tests in the coming days. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13618065",
            "date": "2013-03-30T12:52:51+0000",
            "content": "Okay, the test still fails sometimes. I'm looking into it. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13618464",
            "date": "2013-03-31T20:41:53+0000",
            "content": "Was trying to look into it but strangely, I haven't run into it over 15 consecutive runs. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13620192",
            "date": "2013-04-02T19:52:31+0000",
            "content": "Nice that this is on a git branch - no stale patches, and you can see the full history!\n\nDoes anyone know an easy way to generate a diff?\nI did the following:\n\ngit clone https://github.com/shalinmangar/lucene-solr.git lusolr_shardsplitting\ncd lusolr_shardsplitting\ngit remote add upstream git://git.apache.org/lucene-solr.git\ngit diff remotes/upstream/trunk remotes/origin/trunk\n\n\n\nBut this does a diff with the current state of the trunk vs the branch.  Any tips from the git wizards out there?\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13620207",
            "date": "2013-04-02T19:58:43+0000",
            "content": "AFAIK it's somewhat annoying - usually it involves doing a squash commit on a tmp branch and diffing with that if you want it nicely in one file/chunk. Otherwise git format-patch can go back n commits and make a diff for each one and you'd have to stitch them together. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13620208",
            "date": "2013-04-02T20:00:04+0000",
            "content": "Was trying to look into it but strangely, I haven't run into it over 15 consecutive runs.\n\nIt's very common for these types of tests to be sensitive to the exact env (hardware, OS, etc). A lot of times it's some timing issue.\n "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13620215",
            "date": "2013-04-02T20:06:09+0000",
            "content": "You'd need to do a git merge and then compare it with the current branch.\n git fetch upstream\n git merge upstream/trunk\n git diff --no-prefix upstream/trunk\n\nThis should show the diff. For now, I've just merged the current state of the trunk with this branch. Getting the diff now should be straight forward. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13620224",
            "date": "2013-04-02T20:13:29+0000",
            "content": "Mark, you're right, it seems like a timing issue. I don't think even Shalin has been able to to recreate it too often under the same environment. Not even with the same seed. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13621060",
            "date": "2013-04-03T17:02:07+0000",
            "content": "I'd like to commit the patch to 4x and trunk soon. We can then work on improving the features and tests via the regular route. If there are no objections, I'll commit it tomorrow.\n\nIt's very common for these types of tests to be sensitive to the exact env (hardware, OS, etc). A lot of times it's some timing issue.\n\nYeah, I'm still trying to reproduce the issue. I'll try to find a solution before I commit. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13622208",
            "date": "2013-04-04T13:45:21+0000",
            "content": "I ran into another bug. Adding mutable state in cloud descriptor (like shard state and range) is a bad idea.\n\nThe sub shard cores are created while the sub shard is in construction state therefore their cloud descriptor keeps \"construction\" as the shard state. If the sub shard leader goes down after the shard state has been changed to \"active\", it sets the shard state to \"construction\" once again while publishing itself as \"down\". "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13623537",
            "date": "2013-04-05T11:08:28+0000",
            "content": "I've run into a few more issues while trying to improve the error handling/reporting.\n\n1. Splitting an existing sub-shard gets stuck up. The new sub-sub shards stay in construction state forever.\n2. The replicas are generally always created on the same node. (Debugging/fixing that) "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13623541",
            "date": "2013-04-05T11:15:55+0000",
            "content": "The sub shard cores are created while the sub shard is in construction state therefore their cloud descriptor keeps \"construction\" as the shard state. If the sub shard leader goes down after the shard state has been changed to \"active\", it sets the shard state to \"construction\" once again while publishing itself as \"down\".\n\nI've fixed it in the git branch. Although I don't like the fix very much. In the git branch, I'm using the shardState and shardRange fields in CloudDescriptor for a one-time usage. They are set to null once the new sub shard core is registered (and the new sub shard is created in zk).\n\nMaybe shardState and shardRange should be a core property instead? "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13625013",
            "date": "2013-04-07T20:43:15+0000",
            "content": "All of the above mentioned issues (and more) are now fixed. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13625383",
            "date": "2013-04-08T13:59:33+0000",
            "content": "Patch updated to trunk.\n\nChanges:\n\n\tRouter and range for the current core is used during split\n\tChange sub-shard replication to check only sub shard range instead of state\n\tFixed replica allocation code such that replicas are not created on the same node always\n\tSplitting a sub-shard works\n\tSlice state and range kept in CloudDescriptor is used one-time only. DUPF and other places rely on state/range read from ZK.\n\tRemoved multiple debug statements, sleeps and nocommits\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13625455",
            "date": "2013-04-08T15:22:03+0000",
            "content": "Adding patch that I've committed to trunk and branch_4x.\n\nChanges:\n1. Changed param name from \"name\" to \"collection\" for splitshard api\n2. Added a comment warning not to use shardState and shardRange from CloudDescriptor "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13626753",
            "date": "2013-04-09T16:05:16+0000",
            "content": "Committed three changes:\n\n\tSet update log to buffering mode before it is published (fixes bug with extra doc count on sub-shard)\n\tUse deleteIndex=true while unloading sub-shard cores (if a sub-shard in construction state already exists at the start of the splitshard operation)\n\tMade ChaosMonkeyShardSplitTest consistent with ShardSplitTest \u2013 Use correct router and replica count, assert sub-shards are active, parent shards are inactive etc\n\n\n\nAnshum suggested over chat that we should think about combining ShardSplitTest and ChaosMonkeyShardSplit tests into one to avoid code duplication. I'll try to see if we can do that. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13629010",
            "date": "2013-04-11T15:24:59+0000",
            "content": "Set update log to buffering mode before it is published (fixes bug with extra doc count on sub-shard)\n\nRegarding those changes - I'd really like to find another way to do that.\n\nThe original change around this made preRegister start taking a core rather than a core descriptor. I'd like to work that out so it doesn't need to be the case. That is where the core will find out some of it's properties (shard id, core node name, perhaps more in the future). It would be nice if the core init code had access to this information - so it would be nice if we could call preRegister (or some refactored version) before actually creating the SolrCore. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13631384",
            "date": "2013-04-14T20:26:45+0000",
            "content": "Anshum suggested over chat that we should think about combining ShardSplitTest and ChaosMonkeyShardSplit tests into one to avoid code duplication. I'll try to see if we can do that.\nI've changed ChaosMonkeyShardSplitTest to extend ShardSplitTest so that we can share most of the code. The ChaosMonkey test is not completely correct and I intend to improve it.\n\nThe original change around this made preRegister start taking a core rather than a core descriptor. I'd like to work that out so it doesn't need to be the case.\n\nI'll revert the change to the preRegister method signature and find another way.\n\nI've found two kinds of test failures of (ChaosMonkey)ShardSplitTest.\n\nThe first is because of the following sequence of events:\n\n\n\tA doc addition fails (because of the kill leader jetty command), client throws an exception and therefore the docCount variable is not incremented inside the index thread.\n\tHowever, the doc addition is recorded in the update logs (of the proxy node?) and replayed on the new leader so in reality, the doc does get added to the shard\n\tSplit happens and we assert on docCounts being equal in the server which fails because the server has the document that we have not counted.\n\n\n\nThis happens mostly with Lucene-Solr-Tests-4.x-Java6 builds. The bug is in the tests and not in the split code. Following is the stack trace:\n\n\n[junit4:junit4]   1> ERROR - 2013-04-14 14:24:27.697; org.apache.solr.cloud.ChaosMonkeyShardSplitTest$1; Exception while adding doc\n[junit4:junit4]   1> org.apache.solr.client.solrj.SolrServerException: No live SolrServers available to handle this request:[http://127.0.0.1:34203/h/y/collection1, http://127.0.0.1:34304/h/y/collection1, http://127.0.0.1:34311/h/y/collection1, http://127.0.0.1:34270/h/y/collection1]\n[junit4:junit4]   1> \tat org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:333)\n[junit4:junit4]   1> \tat org.apache.solr.client.solrj.impl.CloudSolrServer.request(CloudSolrServer.java:306)\n[junit4:junit4]   1> \tat org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:117)\n[junit4:junit4]   1> \tat org.apache.solr.cloud.AbstractFullDistribZkTestBase.indexDoc(AbstractFullDistribZkTestBase.java:561)\n[junit4:junit4]   1> \tat org.apache.solr.cloud.ChaosMonkeyShardSplitTest.indexr(ChaosMonkeyShardSplitTest.java:434)\n[junit4:junit4]   1> \tat org.apache.solr.cloud.ChaosMonkeyShardSplitTest$1.run(ChaosMonkeyShardSplitTest.java:158)\n[junit4:junit4]   1> Caused by: org.apache.solr.common.SolrException: Server at http://127.0.0.1:34311/h/y/collection1 returned non ok status:503, message:Service Unavailable\n[junit4:junit4]   1> \tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:373)\n[junit4:junit4]   1> \tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:181)\n[junit4:junit4]   1> \tat org.apache.solr.client.solrj.impl.LBHttpSolrServer.request(LBHttpSolrServer.java:264)\n[junit4:junit4]   1> \t... 5 more\n\n\n\nPerhaps we should check the exception message and continue to count such a document?\n\nThe second kind of test failures are where a document add fails due to version conflict. This exception is always seen just after the \"updateshardstate\" is called to switch the shard states. Following is the relevant log:\n\n\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.861; org.apache.solr.cloud.Overseer$ClusterStateUpdater; Update shard state invoked for collection: collection1\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.861; org.apache.solr.cloud.Overseer$ClusterStateUpdater; Update shard state shard1 to inactive\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.861; org.apache.solr.cloud.Overseer$ClusterStateUpdater; Update shard state shard1_0 to active\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.861; org.apache.solr.cloud.Overseer$ClusterStateUpdater; Update shard state shard1_1 to active\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.873; org.apache.solr.update.processor.LogUpdateProcessor; [collection1] webapp= path=/update params={wt=javabin&version=2} {add=[169 (1432319507166134272)]} 0 2\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.877; org.apache.solr.common.cloud.ZkStateReader$2; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 5)\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.884; org.apache.solr.update.processor.LogUpdateProcessor; [collection1_shard1_1_replica1] webapp= path=/update params={distrib.from=http://127.0.0.1:41028/collection1/&update.distrib=FROMLEADER&wt=javabin&distrib.from.parent=shard1&version=2} {} 0 1\n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.885; org.apache.solr.update.processor.LogUpdateProcessor; [collection1] webapp= path=/update params={distrib.from=http://127.0.0.1:41028/collection1/&update.distrib=FROMLEADER&wt=javabin&distrib.from.parent=shard1&version=2} {add=[169 (1432319507173474304)]} 0 2\n[junit4:junit4]   1> ERROR - 2013-04-14 19:05:26.885; org.apache.solr.common.SolrException; shard update error StdNode: http://127.0.0.1:41028/collection1_shard1_1_replica1/:org.apache.solr.common.SolrException: version conflict for 169 expected=1432319507173474304 actual=-1\n[junit4:junit4]   1> \tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:404)\n[junit4:junit4]   1> \tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:181)\n[junit4:junit4]   1> \tat org.apache.solr.update.SolrCmdDistributor$1.call(SolrCmdDistributor.java:332)\n[junit4:junit4]   1> \tat org.apache.solr.update.SolrCmdDistributor$1.call(SolrCmdDistributor.java:306)\n[junit4:junit4]   1> \tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n[junit4:junit4]   1> \tat java.util.concurrent.FutureTask.run(FutureTask.java:166)\n[junit4:junit4]   1> \tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n[junit4:junit4]   1> \tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n[junit4:junit4]   1> \tat java.util.concurrent.FutureTask.run(FutureTask.java:166)\n[junit4:junit4]   1> \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)\n[junit4:junit4]   1> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n[junit4:junit4]   1> \tat java.lang.Thread.run(Thread.java:679)\n[junit4:junit4]   1> \n[junit4:junit4]   1> INFO  - 2013-04-14 19:05:26.886; org.apache.solr.update.processor.DistributedUpdateProcessor; try and ask http://127.0.0.1:41028 to recover\n\n\n\nI'm not sure yet why a version conflict will happen and why it follows an \"updateshardstate\" command. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13631405",
            "date": "2013-04-14T21:41:56+0000",
            "content": "I'll revert the change to the preRegister method signature and find another way.\n\nI'm happy to help on this - it might be easier to just create a new issue rather than reverting, and work on getting it nicer from there, up to you though. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-13631490",
            "date": "2013-04-15T04:21:00+0000",
            "content": "This happens mostly with Lucene-Solr-Tests-4.x-Java6 builds.\n\nIs this true for all the exceptions or just the one that follows this line? I wasn't able to reproduce this on my system running Java7.\nAlso, are these consistent failures? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13631511",
            "date": "2013-04-15T05:31:55+0000",
            "content": "Is this true for all the exceptions or just the one that follows this line? I wasn't able to reproduce this on my system running Java7.\n\nThe error with the failing add doc happens with Java6 \u2013 haven't seen it with any other version. I've seen the version conflict exception on java7 and java8.\n\nAlso, are these consistent failures?\n\nYes but only on jenkins! I've had ec2 boxes running these tests all night and I haven't seen a failure in over 500 runs. These failures are very environment and timing dependent. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13637505",
            "date": "2013-04-21T08:27:11+0000",
            "content": "I haven't seen the test failure due to extra document after increasing read timeout values in the test. Now that 4.3 is about to release with this feature, I'm going to mark this issue as resolved. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13637506",
            "date": "2013-04-21T08:28:44+0000",
            "content": "This feature will be released with Solr 4.3 "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13651172",
            "date": "2013-05-07T18:50:38+0000",
            "content": "I'll revert the change to the preRegister method signature and find another way.\n\nI'm trying to look at this now. I'm not sure how to go about solving in an 'easy' way. Currently, you have to start buffering those updates before publishing, but I want it to so that you publish as DOWN before creating the SolrCore - but you need the SolrCore to start buffering.\n\nI don't see the 'easy' fix unfortunately. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13651192",
            "date": "2013-05-07T19:12:40+0000",
            "content": "I don't see the 'easy' fix unfortunately.\n\nOkay, I think I found it - doing this stuff in the bottom of the SolrCore constructor rather than preRegister seems to work so far. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13651267",
            "date": "2013-05-07T20:31:26+0000",
            "content": "Yeah, that'll work. We have an issue open to track this: SOLR-4745 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13653940",
            "date": "2013-05-10T10:33:29+0000",
            "content": "Closed after release. "
        }
    ]
}