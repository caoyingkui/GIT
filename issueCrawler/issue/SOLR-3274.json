{
    "id": "SOLR-3274",
    "title": "ZooKeeper related SolrCloud problems",
    "details": {
        "affect_versions": "4.0-ALPHA",
        "status": "Open",
        "fix_versions": [],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "Same setup as in SOLR-3273. Well if I have to tell the entire truth we have 7 Solr servers, running 28 slices of the same collection (collA) - all slices have one replica (two shards all in all - leader + replica) - 56 cores all in all (8 shards on each solr instance). But anyways...\n\nBesides the problem reported in SOLR-3273, the system seems to run fine under high load for several hours, but eventually errors like the ones shown below start to occur. I might be wrong, but they all seem to indicate some kind of unstability in the collaboration between Solr and ZooKeeper. I have to say that I havnt been there to check ZooKeeper \"at the moment where those exception occur\", but basically I dont believe the exceptions occur because ZooKeeper is not running stable - at least when I go and check ZooKeeper through other \"channels\" (e.g. my eclipse ZK plugin) it is always accepting my connection and generally seems to be doing fine.\n\nException 1) Often the first error we see in solr.log is something like this\n\nMar 22, 2012 5:06:43 AM org.apache.solr.common.SolrException log\nSEVERE: org.apache.solr.common.SolrException: Cannot talk to ZooKeeper - Updates are disabled.\n        at org.apache.solr.update.processor.DistributedUpdateProcessor.zkCheck(DistributedUpdateProcessor.java:678)\n        at org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:250)\n        at org.apache.solr.handler.XMLLoader.processUpdate(XMLLoader.java:140)\n        at org.apache.solr.handler.XMLLoader.load(XMLLoader.java:80)\n        at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:59)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:129)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1540)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:407)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:256)\n        at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n        at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n        at org.mortbay.jetty.Server.handle(Server.java:326)\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n        at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:945)\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:756)\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n        at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\n\n\nI believe this error basically occurs because SolrZkClient.isConnected reports false, which means that its internal \"keeper.getState\" does not return ZooKeeper.States.CONNECTED. Im pretty sure that it has been CONNECTED for a long time, since this error starts occuring after several hours of processing without this problem showing. But why is it suddenly not connected anymore?!\n\nException 2) We also see errors like the following, and if Im not mistaken, they start occuring shortly after \"Exception 1)\" (above) shows for the fist time\n\nMar 22, 2012 5:07:26 AM org.apache.solr.common.SolrException log\nSEVERE: org.apache.solr.common.SolrException: no servers hosting shard: \n        at org.apache.solr.handler.component.HttpShardHandler$1.call(HttpShardHandler.java:149)\n        at org.apache.solr.handler.component.HttpShardHandler$1.call(HttpShardHandler.java:123)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n        at java.lang.Thread.run(Thread.java:662)\n\n\nPlease note that the exception says \"no servers hosting shard: <blank>\". Looking at the code a \"shard\"-string was actually supposed to be written at <blank>.  Basically this means that HttpShardHandler.submit was called with an empty \"shard\"-string parameter. But who does this? CoreAdminHandler.handleDistribUrlAction or SearchHandler.handleRequestBody or SyncStrategy or PeerSync or... I dont know, and maybe it is not that relevant, because I guess they all get the \"shard\"-string from ZooKeeper. Again something pointing in the direction of unstable collaboration between Solr and ZooKeeper.\n\nException 3) We also see exceptions like this\n\nMar 25, 2012 3:05:38 PM org.apache.solr.common.cloud.ZkStateReader$3 process\nWARNING: ZooKeeper watch triggered, but Solr cannot talk to ZK\nMar 25, 2012 3:05:38 PM org.apache.solr.cloud.LeaderElector$1 process\nWARNING: \norg.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /collections/collA/leader_elect/slice26/election\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:118)\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)\n        at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1249)\n        at org.apache.solr.common.cloud.SolrZkClient$6.execute(SolrZkClient.java:266)\n        at org.apache.solr.common.cloud.SolrZkClient$6.execute(SolrZkClient.java:263)\n        at org.apache.solr.common.cloud.ZkCmdExecutor.retryOperation(ZkCmdExecutor.java:65)\n        at org.apache.solr.common.cloud.SolrZkClient.getChildren(SolrZkClient.java:263)\n        at org.apache.solr.cloud.LeaderElector.checkIfIamLeader(LeaderElector.java:92)\n        at org.apache.solr.cloud.LeaderElector.access$000(LeaderElector.java:57)\n        at org.apache.solr.cloud.LeaderElector$1.process(LeaderElector.java:121)\n        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:531)\n        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:507)\n\n\n\nMaybe this will we usable for some bug-fixing or for making the code more stable. I know 4.0 is not stable/released yet, and that we therefore should expect this kind of errors at the moment. So this is not negative criticism - just reporting of issues observed when using SolrCloud features under high load for several days. Any feedback is more than welcome.\n\nRegards, Per Steffensen",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-13238381",
            "date": "2012-03-26T13:36:42+0000",
            "content": "This happens because the connection between solr and zookeeper is lost - perhaps because the load on the box is too high. I think we may default to a fairly low timeout that could be raised (by default and manually). "
        },
        {
            "author": "Per Steffensen",
            "id": "comment-13238391",
            "date": "2012-03-26T13:50:28+0000",
            "content": "Thanks a lot, Mark! \n\nCan all the exception be explained by \"connection loss between solr and zookeeper\"? \n\nIm not sure I totally buy the explanation because I believe that, even though there is a fairly high update/search-load on the machines in the cluster, the machines actually do not seem to be exhausted (CPU idle way above 0% (more like 50% in average), not very high IO-wait etc.). So I would expect plenty of resources to be available for ZK to respond fast. But lets see what happens if we set the timeout higher. Can you point me in the direction of how to set it manually?\n\nRegards, Per Steffensen "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13238404",
            "date": "2012-03-26T14:01:41+0000",
            "content": "Can all the exception be explained by \"connection loss between solr and zookeeper\"?\n\nSessionExpiredException\n\nThis indicates the connection with ZooKeeper was lost.\n\norg.apache.solr.common.SolrException: Cannot talk to ZooKeeper - Updates are disabled.\n\nIf there is no connection to ZooKeeper, you will see this if you send an update.\n\norg.apache.solr.common.SolrException: no servers hosting shard: \n\nSami Siren has a JIRA issue about improving this message I believe - but normally it means that the cluster does not see a single node hosting a given shard. Not sure if this is related to the above - not the same smoking gun.\n\nCan you point me in the direction of how to set it manually?\n\nThe default is only 10 seconds. I'd try 30 seconds perhaps? You don't want it too low, but you also don't want it too high if you can help it. I can't remember what the \"zookeeper\" default is, but I've seen it set as high as 60 seconds looking around some hbase usage...\n\nYou should be able to set it in solr.xml as a cores attribute: zkClientTimeout=\"30000\" or whatever.\n\nThat is:   <cores adminPath=\"/admin/cores\" zkClientTimeout=\"30000\"\n\nYou'd want to do it for each node. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13238409",
            "date": "2012-03-26T14:04:50+0000",
            "content": "not the same smoking gun.\n\nSorry - actually this does make sense with the other errors - if the zk connection is lost, that node is no longer considered live - if that happens to each node hosting a shard (say you have 1 replica and this happened to both nodes) then searches would fail with this. "
        },
        {
            "author": "Per Steffensen",
            "id": "comment-13238419",
            "date": "2012-03-26T14:16:06+0000",
            "content": "Uhhhh 10 secs is A LOT OF TIME. I really wouldnt want to set it higher that that. If ZK is not able to answer within 10 secs I need to correct something else in my setup. \n\nI still believe that Solr might end in this state (where it \"believes\" that the connection to ZK is lost) some other way than actually experiencing a 10+ sec response-time from ZK, but I cant prove it (yet). So for now I will just thank you for your kind help, and assume that it is correct. Then basically my options are to setup a more responsive ZK cluster or maybe raise the ZK timeout on Solr side. \n\nThanks, again.\n\nRegards, Per Steffensen "
        },
        {
            "author": "Per Steffensen",
            "id": "comment-13238428",
            "date": "2012-03-26T14:27:43+0000",
            "content": "But why not just try to reconnect if/when this situation has occured, so that Solr can continue doing its work? I guess Solr does not do that, because it seems like when this error has first established, there is no \"recovering\", and certainly (Im close to 100% positive) ZK will not continue doing 10+ secs response-times to all requests, even though it might do a 10+ sec response once in a while.\n\nRegards, Per Steffensen "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13238431",
            "date": "2012-03-26T14:29:17+0000",
            "content": "Uhhhh 10 secs is A LOT OF TIME\n\nIt really depends - I've seen that timeout broken on a heavily loaded machine more than a few times. Then you have to add in any network delays. But yeah, on a fast machine under normal to high load, I have not really run into a problem with this timeout.\n\nThen basically my options are to setup a more responsive ZK cluster or maybe raise the ZK timeout on Solr side.\n\nThat's all I can suggest. If the ZooKeeper client loses the connection, it has up to the session timeout to reconnect. Once it reconnects, if more than the session timeout has passed, you will get the SessionExpiredException. If that happens, the node will go into recovery. If it's in recovery, it won't serve search requests until recovery is finished - so that could also contribute to the \"no servers hosting shard\" issue.\n\nLet me know how it goes and if you can pin point any problems. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13238455",
            "date": "2012-03-26T14:59:18+0000",
            "content": "\nBut why not just try to reconnect if/when this situation has occured, so that Solr can continue doing its work? I guess Solr does not do that, because it seems like when this error has first established, there is no \"recovering\", and certainly (Im close to 100% positive) ZK will not continue doing 10+ secs response-times to all requests, even though it might do a 10+ sec response once in a while.\n\nSolr does try to reconnect - but there can be no recovering due to the other issue you posted - because you have changed the core admin url. "
        },
        {
            "author": "Per Steffensen",
            "id": "comment-13239208",
            "date": "2012-03-27T06:04:31+0000",
            "content": "Ahhh I see. So correcting the adminPath will allow it to allow it to reconnect AND recover and then get back to work. Of course, we didnt I conclude that myself. We will try to correct adminPath and run the performance test again and see how it goes. If we see any problems we will report here and try to pinpoint (and potentially fix). Thanks a lot for your help!\n\nRegards, Per Steffensen "
        },
        {
            "author": "Per Steffensen",
            "id": "comment-13241121",
            "date": "2012-03-29T09:56:18+0000",
            "content": "Our performance test (with adminPath=\"/admin/cores\") ran successfully for a while, but then simular errors started to occur, and it seems like whenever a SolrCloud-cluster (of some of the Solr instances in it) gets into \"no contant to ZK\"-state, it has a hard time getting out of this state again. The CPU is like 70-80 idle on the machines while this is going on, so I have a hard time recognizing that ZK is not responding within 10 secs. But basically I cannot really pinpoint what goes wrong yet, and we will shift focus for a while, so it will probably be a while before I might get back with solid proof or concrete cases.\n\nOnly thing I can do for now is try to encourage you guys at apache-solr to do your own stability/robustness/endurance tests where you run with a fairly high concurrent load (not so high that the machines get exhausted) for many days, and hopefully you will see the problems occur yourselves.\n\nThanks for your collaboration!\n\nRegards, Per Steffensen "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13241186",
            "date": "2012-03-29T12:42:57+0000",
            "content": "If you don't solve the issue of the zk expirations, this is no real surprise. The larger the index gets, the longer the recoveries can take - until you end up in a similar situation as you had. The key is understanding why the connection to zookeeper is dropping.  "
        },
        {
            "author": "Jay Hacker",
            "id": "comment-13494029",
            "date": "2012-11-09T14:55:32+0000",
            "content": "Not sure if it's the same problem, but I have seen similar issues with 4.0.0 release.  I get errors like:\n\n\nClusterState says we are the leader, but locally we don't think so\nThere was a problem finding the leader in zk\nforwarding update to http://solr83:4000/solr/main/ failed - retrying ...\nCannot open channel to 3 at election address solr84/X.X.X.X:5002\nSession 0x0 for server null, unexpected error, closing socket connection and attempting reconnect\n\n\n\nI'm running zookeeper embedded, and the problem turns out to be long garbage collection pauses.  During a stop-the-world collection, zookeeper times out.  It's especially bad if the system has to page in a bunch of memory from disk.  This would explain why things run fine for a while, until memory fills up and you need to do a big GC.  This is quite repeatable for us; just index until memory is pretty full, wait for a long GC or trigger one manually with VisualVM.\n\nFor us, the solution was to do an autoSoftCommit every million documents.  This essentially clears the heap, so doing it regularly prevents the heap from getting too big and causing long garbage collection pauses.  It also had the nice side benefit of speeding things up a bit.\n\nYou can also try different garbage collectors (I've had some luck with -XX:+UseConcMarkSweepGC ), running zookeeper in an independent JVM, and/or turning up the zookeeper timeouts. "
        },
        {
            "author": "Alexander S.",
            "id": "comment-14090519",
            "date": "2014-08-08T09:24:09+0000",
            "content": "Suffering from the same problem, happens during high load on the nodes.\n\nOur setup is pretty simple, 4 solr instances: 2 shards, 2 replicas and 3 zookeeper instances. Everything is running on 3 physical nodes:\n\n\t1st node \u2014 1 zookeeper instance\n\t2nd node \u2014 2 solr shards and 1 zookeeper\n\t3rd node \u2014 2 solr replicas and 1 zookeeper\n\n\n\nWe're running solr instances this way:\njava -Xms2G -Xmx16G -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=80 -DzkHost=zoo1.devops:2181,zoo2.devops:2181,zoo3.devops:2181 -Dcollection.configName=Carmen -Dbootstrap_confdir=./solr/conf -Dbootstrap_conf=true -DnumShards=2 -jar start.jar etc/jetty.xml\n\nAnd once loading increases we get:\n\norg.apache.solr.common.SolrException: Cannot talk to ZooKeeper - Updates are disabled.\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.zkCheck(DistributedUpdateProcessor.java:1306)\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.processDelete(DistributedUpdateProcessor.java:981)\n\tat org.apache.solr.update.processor.LogUpdateProcessor.processDelete(LogUpdateProcessorFactory.java:121)\n\tat org.apache.solr.handler.loader.XMLLoader.processDelete(XMLLoader.java:349)\n\tat org.apache.solr.handler.loader.XMLLoader.processUpdate(XMLLoader.java:278)\n\tat org.apache.solr.handler.loader.XMLLoader.load(XMLLoader.java:174)\n\tat org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:92)\n\tat org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:74)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1952)\n\tat org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:774)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:418)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:207)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1419)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:455)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1075)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:384)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1009)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:368)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:489)\n\tat org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:953)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1014)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:861)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)\n\tat org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)\n\tat org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:744)\n\n\n\nThat's simply impossible for all 3 zookeeper instances to get offline simultaneously. I understand that 2nd and 3rd nodes could be overloaded because of Solr, but 1st node runs just a single zookeeper instance and the load average on that node is close to zero.\n\nSince there's always at least 1 stable ZK node this seems like a communication/reliability bug in Solr. "
        },
        {
            "author": "Per Steffensen",
            "id": "comment-14092864",
            "date": "2014-08-11T15:15:02+0000",
            "content": "That's simply impossible for all 3 zookeeper instances to get offline simultaneously. \n\nWell you never know\n\nSince there's always at least 1 stable ZK node this seems like a communication/reliability bug in Solr.\n\nIn a 3-node ZK-cluster you need at least 2 healthy ZK-nodes connected with each other for the cluster to be operational. A majority of the nodes always need to agree for an operation to be carried out - this way you know that at any time only one set of ZK-nodes in a ZK-cluster can successfully carry out operations  - e.g. when there is no network connection between two sets of ZK-nodes (but connections internally between the nodes in each set are ok), only one set can contain a majority of the total number of ZK-nodes in the cluster. "
        },
        {
            "author": "Alexander S.",
            "id": "comment-14092884",
            "date": "2014-08-11T15:27:37+0000",
            "content": "Hi, thanks for the response.\n\nWell you never know\nI've checked nodes status, that 3rd node was online all the time and there were no any load on it.\n\nIn a 3-node ZK-cluster you need at least 2 healthy ZK-nodes connected with each other for the cluster to be operational.\nThat should be the problem since 2 other ZK instances might be (theoretically) unavailable because of heavy load (since they share same nodes with Solr instances). Both nodes have 16 CPU cores, 48G of memory and RAID 10 (SSD), I thought it would be hard to get performance issues there. Anyway, adding a separate node with 4th zookeeper instance might help, right? "
        },
        {
            "author": "Per Steffensen",
            "id": "comment-14094033",
            "date": "2014-08-12T12:57:54+0000",
            "content": "Both nodes have 16 CPU cores, 48G of memory and RAID 10 (SSD), I thought it would be hard to get performance issues there\n\nYes that should be hard. Well done! \n\nAnyway, adding a separate node with 4th zookeeper instance might help, right?\n\nA ZK cluster should always have an uneven number of nodes. So if you want to add additional ZK instances you should add two. I would rather move the two ZK instances running on Solr-machines to two machines not running Solr. So that you end up with 3 ZK instances where non of them run on machines also running Solr. We never run ZK on the same machines as Solr - we have bad experiences with that - loosing ZK connections all the time. You will still occasionally loose ZK connections from Solrs when they are under high load, but usually they reconnect fairly quickly (before session timeout) and you can continue immediately.\n\nI have been working on an optimized ZK where you do not loose ZK connections nearly as often, but currently it is not prioritized to finish the job. "
        },
        {
            "author": "Arcadius Ahouansou",
            "id": "comment-14605355",
            "date": "2015-06-29T09:42:02+0000",
            "content": "We have similar issue on Solr5.2.1 with the log entry below.\nWe have 8 Solr nodes and 5 ZK nodes\nThe 8 solr nodes are identical with only 1 collection, only 1 replica per node.\n\nThe error\n\nCannot talk to ZooKeeper - Updates are disabled\n\nis not too helpful IMHO.\n\nIt would be good to also log all ZK nodes that Solr tried to connect to before throwing the error.\n\n\nERROR - 2015-06-29 00:42:04.912; [collectionA-01 shard1 core_node8 collectionA-01_shard1_replica1] org.apache.solr.common.SolrException; org.apache.solr.common.SolrException: Cannot talk to ZooKeeper - Updates are disabled.\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.zkCheck(DistributedUpdateProcessor.java:1482)\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.processCommit(DistributedUpdateProcessor.java:1602)\n\tat org.apache.solr.update.processor.LogUpdateProcessor.processCommit(LogUpdateProcessorFactory.java:161)\n\tat org.apache.solr.handler.RequestHandlerUtils.handleCommit(RequestHandlerUtils.java:69)\n\tat org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:68)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:143)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:2064)\n\tat org.apache.solr.servlet.HttpSolrCall.execute(HttpSolrCall.java:654)\n\tat org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:450)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:227)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:196)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:577)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:223)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1127)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1061)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:215)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:110)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:497)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:310)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)\n\tat org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:540)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)\n\tat java.lang.Thread.run(Thread.java:745)\n\n "
        },
        {
            "author": "Alexander S.",
            "id": "comment-14738200",
            "date": "2015-09-10T05:35:46+0000",
            "content": "Hi, just wanted to let you know that adding 2 new ZK servers (so I have 5 running ZK instances) improved the situation a lot.\n\nBut I found one weird thing with the ZK:\n\njava.net.UnknownHostException: zoo5.devops\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:178)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:579)\n\tat org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:368)\n\tat org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:402)\n\tat org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:840)\n\tat org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:762)\n2015-09-10 01:13:21,235 - WARN  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2181:QuorumCnxManager@382] - Cannot open channel to 2 at election address zoo2.devops:3888\njava.net.UnknownHostException: zoo2.devops\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:178)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:579)\n\tat org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:368)\n\tat org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:402)\n\tat org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:840)\n\tat org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:762)\n2015-09-10 01:13:21,235 - WARN  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2181:QuorumCnxManager@382] - Cannot open channel to 1 at election address zoo1.devops:3888\njava.net.UnknownHostException: zoo1.devops\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:178)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:579)\n\tat org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:368)\n\tat org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:402)\n\tat org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:840)\n\tat org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:762)\n2015-09-10 01:13:21,236 - WARN  [QuorumPeer[myid=3]/0:0:0:0:0:0:0:0:2181:QuorumCnxManager@382] - Cannot open channel to 4 at election address zoo4.devops:3888\njava.net.UnknownHostException: zoo4.devops\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:178)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:579)\n\tat org.apache.zookeeper.server.quorum.QuorumCnxManager.connectOne(QuorumCnxManager.java:368)\n\tat org.apache.zookeeper.server.quorum.QuorumCnxManager.connectAll(QuorumCnxManager.java:402)\n\tat org.apache.zookeeper.server.quorum.FastLeaderElection.lookForLeader(FastLeaderElection.java:840)\n\tat org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:762)\n\n\n\nJust opened 2 ssh sessions to that server and was monitoring the log with tail. While ZK posted these errors I was able to ping zoo1/2/4/5.devops servers and was able to connect to ZK there with telnet. So it seems something could go wrong with ZK itself. At this time I seen these \"cannot talk to ZK\" errors in Solr.\n\nAnd eventually I've just restarted this broken ZK instance and everything is fine again. So I guess Solr tried to connect namely to this broken ZK instance (can't say for sure since it doesn't mention the instance it failed to connect to in its log).\n\nUPD: but still often see these errors in ZK logs:\n\n2015-09-10 01:31:28,804 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@197] - Accepted socket connection from /10.128.202.22:35990\n2015-09-10 01:31:28,847 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@357] - caught end of stream exception\nEndOfStreamException: Unable to read additional data from client sessionid 0x0, likely client has closed socket\n\tat org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:228)\n\tat org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:208)\n\tat java.lang.Thread.run(Thread.java:744)\n2015-09-10 01:31:28,847 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1007] - Closed socket connection for client /10.128.202.22:35990 (no session established for client)\n\n\nWhere 10.128.202.22 is my solr instance. "
        },
        {
            "author": "Greg Pendlebury",
            "id": "comment-14979672",
            "date": "2015-10-29T02:21:56+0000",
            "content": "FWIW we ran into this issue today as well, and nothing worked until ZK was restarted. I would love to think that Solr could detect this issue, but it smells like a ZK bug to me. "
        },
        {
            "author": "Dan Kogan",
            "id": "comment-15121836",
            "date": "2016-01-28T16:29:45+0000",
            "content": "It appears we also ran into this issue using SolrCloud 5.4.  This is the stack trace we saw:\n\n\n org.apache.solr.common.SolrException: Cannot talk to ZooKeeper - Updates are disabled.\n        at org.apache.solr.update.processor.DistributedUpdateProcessor.zkCheck(DistributedUpdateProcessor.java:1459)\n        at org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:660)\n        at org.apache.solr.update.processor.LogUpdateProcessorFactory$LogUpdateProcessor.processAdd(LogUpdateProcessorFactory.java:104)\n        at org.apache.solr.handler.dataimport.SolrWriter.upload(SolrWriter.java:74)\n        at org.apache.solr.handler.dataimport.DataImportHandler$1.upload(DataImportHandler.java:260)\n        at org.apache.solr.handler.dataimport.DocBuilder.buildDocument(DocBuilder.java:525)\n        at org.apache.solr.handler.dataimport.DocBuilder.buildDocument(DocBuilder.java:415)\n        at org.apache.solr.handler.dataimport.DocBuilder.doFullDump(DocBuilder.java:330)\n        at org.apache.solr.handler.dataimport.DocBuilder.execute(DocBuilder.java:233)\n        at org.apache.solr.handler.dataimport.DataImporter.doFullImport(DataImporter.java:417)\n        at org.apache.solr.handler.dataimport.DataImporter.runCmd(DataImporter.java:481)\n        at org.apache.solr.handler.dataimport.DataImportHandler.handleRequestBody(DataImportHandler.java:200)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:156)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:2073)\n        at org.apache.solr.servlet.HttpSolrCall.execute(HttpSolrCall.java:658)\n        at org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:457)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:222)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:181)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:577)\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:223)\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1127)\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1061)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n        at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:215)\n        at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:110)\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)\n        at org.eclipse.jetty.server.Server.handle(Server.java:499)\n        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:310)\n        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)\n        at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:540)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)\n        at java.lang.Thread.run(Thread.java:745)\n\n "
        },
        {
            "author": "Hal Deadman",
            "id": "comment-15121858",
            "date": "2016-01-28T16:44:18+0000",
            "content": "We are also running SolrCloud 5.4.0 and we are seeing this issue in production today. I think it happened in the past and cleared itself up (unless the servers were rebooted without my knowledge) but we are in the process of getting rights to restart zookeeper and see if that clears it up. \n\nWe are in an environment where they are probably running periodic vulnerability scans against our servers. I wonder if this zookeeper bug: https://issues.apache.org/jira/browse/ZOOKEEPER-2186 could be causing us problems. We do see evidence in the zookeeper logs that indicate a scan ran last night so I am going to try building zookeeper 3.4.7 from the tag and trying that out in our dev environment.  "
        },
        {
            "author": "Stephan Lagraulet",
            "id": "comment-15123736",
            "date": "2016-01-29T16:47:42+0000",
            "content": "I think that this message could be misleading according to what is happening to our servers.\n\nFirst we receive a disconnect event.\n\n2016-01-29 14:52:38.868 INFO  (zkCallback-3-thread-73-processing-n:solrnode027:8983_solr-EventThread) [   ] o.a.s.c.c.ConnectionManager Watcher org.apache.solr.common.cloud.ConnectionManager@478ca953 name:ZooKeeperConnection Watcher:solrnode013:2181,solrnode014:2181,solrnode015:2181 got event WatchedEvent state:Disconnected type:None path:null path:null type:None\n2016-01-29 14:52:38.868 INFO  (zkCallback-3-thread-73-processing-n:solrnode027:8983_solr-EventThread) [   ] o.a.s.c.c.ConnectionManager zkClient has disconnected\n\n\n\nThen we receive an \"Expired\" event:\n\n2016-01-29 14:52:40.028 INFO  (zkCallback-3-thread-73-processing-n:solrnode027:8983_solr-EventThread) [   ] o.a.s.c.c.ConnectionManager Watcher org.apache.solr.common.cloud.ConnectionManager@478ca953 name:ZooKeeperConnection Watcher:solrnode013:2181,solrnode014:2181,solrnode015:2181 got event WatchedEvent state:Expired type:None path:null path:null type:None\n2016-01-29 14:52:40.028 INFO  (zkCallback-3-thread-73-processing-n:solrnode027:8983_solr-EventThread) [   ] o.a.s.c.c.ConnectionManager Our previous ZooKeeper session was expired. Attempting to reconnect to recover relationship with ZooKeeper...\n2016-01-29 14:52:40.029 INFO  (zkCallback-3-thread-73-processing-n:solrnode027:8983_solr-EventThread) [   ] o.a.s.c.c.DefaultConnectionStrategy Connection expired - starting a new one...\n\n\n\n\nThen we have the message \"Cannot talk to ZooKeeper\"\n\n2016-01-29 14:52:40.034 ERROR (qtp1057941451-80095) [c:offers_full s:shard3 r:core_node7 x:offers_full_shard3_replica2] o.a.s.h.RequestHandlerBase org.apache.solr.common.SolrException: Cannot talk to ZooKeeper - Updates are disabled.\n        at org.apache.solr.update.processor.DistributedUpdateProcessor.zkCheck(DistributedUpdateProcessor.java:1459)\n        at org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:660)\n        at org.apache.solr.update.processor.LogUpdateProcessorFactory$LogUpdateProcessor.processAdd(LogUpdateProcessorFactory.java:104)\n        at org.apache.solr.handler.loader.JavabinLoader$1.update(JavabinLoader.java:98)\n        at org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readOuterMostDocIterator(JavaBinUpdateRequestCodec.java:179)\n        at org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readIterator(JavaBinUpdateRequestCodec.java:135)\n        at org.apache.solr.common.util.JavaBinCodec.readVal(JavaBinCodec.java:260)\n        at org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readNamedList(JavaBinUpdateRequestCodec.java:121)\n        at org.apache.solr.common.util.JavaBinCodec.readVal(JavaBinCodec.java:225)\n        at org.apache.solr.common.util.JavaBinCodec.unmarshal(JavaBinCodec.java:145)\n        at org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec.unmarshal(JavaBinUpdateRequestCodec.java:186)\n\n\n\nAfter this the connexion is established:\n\n2016-01-29 14:52:40.393 INFO  (zkCallback-3-thread-73-processing-n:solrnode027:8983_solr-EventThread) [   ] o.a.s.c.c.ConnectionManager Watcher org.apache.solr.common.cloud.ConnectionManager@478ca953 name:ZooKeeperConnection Watcher:solrnode013:2181,solrnode014:2181,solrnode015:2181 got event WatchedEvent state:SyncConnected type:None path:null path:null type:None\n2016-01-29 14:52:40.393 INFO  (zkCallback-3-thread-73-processing-n:solrnode027:8983_solr-EventThread) [   ] o.a.s.c.c.ConnectionManager Client is connected to ZooKeeper\n2016-01-29 14:52:40.393 INFO  (zkCallback-3-thread-73-processing-n:solrnode027:8983_solr-EventThread) [   ] o.a.s.c.c.ConnectionManager Connection with ZooKeeper reestablished.\n2016-01-29 14:52:40.393 INFO  (zkCallback-3-thread-73-processing-n:solrnode027:8983_solr-EventThread) [   ] o.a.s.c.ZkController ZooKeeper session re-connected ... refreshing core states after session expiration.\n\n\n\nIt is not that we cannot talk to ZK but instead we are currently establishing a new connection. We ought to wait until this connection is established before discarding these updates.\n\nThe other question is why do we get these events in the first place?\nThis server was the leader and this event happens whenever a replica starts a full recovery against this leader. "
        },
        {
            "author": "Nitin Sharma",
            "id": "comment-15153691",
            "date": "2016-02-19T04:24:26+0000",
            "content": "I also ran into this issue with solr 4.6.1. The main issue is that updates go through fine in solr but solr-zk interactions are broken and few shards are marked as down. They never recover/reconnect to zk unless force solr restart. I even tried bumping up zk ensemble to 7 and upto 9 nodes but did not help. \n\nAny suggestions on how to deal with this?  "
        },
        {
            "author": "Scott Blum",
            "id": "comment-15154297",
            "date": "2016-02-19T14:55:25+0000",
            "content": "Don't forget to rule out GC as the problem.  Long GC pauses in Solr can cause the ZK session to timeout.  There are probably some bugs in various places for getting back into a consistent state after that happens. "
        },
        {
            "author": "Dragos C",
            "id": "comment-15177771",
            "date": "2016-03-03T12:52:18+0000",
            "content": "It happened to me on Solr 5.5.0 with the default setup. I just unzipped the file, started solr in cloud mode (sharts: 1, replication factor: 1), with a limit of 6GB (out of 20), java 1.8.0_73 x64, Windows Server 2008 R2 Standard and pushed the core configuration files. I have one zookeeper and one solr behind. I added some documents, but, as  Per Steffensen mentioned, the processor was barely around 70% (with various spikes above this limit). After a while, I am always getting 503 http status and the reply from solr is \"Cannot talk to ZooKeeper - Updates are disabled.\".\n\nSolr log:\n\n2016-03-03 12:34:20.902 INFO  (qtp1450821318-4031) [c:CORE s:shard1 r:core_node1 x:CORE_shard1_replica1] o.a.s.u.p.LogUpdateProcessorFactory [CORE_shard1_replica1]  webapp=/solr path=/update params={}{} 0 0\n2016-03-03 12:34:20.902 ERROR (qtp1450821318-4031) [c:CORE s:shard1 r:core_node1 x:CORE_shard1_replica1] o.a.s.h.RequestHandlerBase org.apache.solr.common.SolrException: Cannot talk to ZooKeeper - Updates are disabled.\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.zkCheck(DistributedUpdateProcessor.java:1469)\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:667)\n\tat org.apache.solr.update.processor.LogUpdateProcessorFactory$LogUpdateProcessor.processAdd(LogUpdateProcessorFactory.java:103)\n\tat org.apache.solr.handler.loader.XMLLoader.processUpdate(XMLLoader.java:250)\n\tat org.apache.solr.handler.loader.XMLLoader.load(XMLLoader.java:177)\n\tat org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:94)\n\tat org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:69)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:155)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:2082)\n\tat org.apache.solr.servlet.HttpSolrCall.execute(HttpSolrCall.java:670)\n\tat org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:458)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:225)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:183)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:577)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:223)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1127)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1061)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:215)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:110)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:499)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:310)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)\n\tat org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:540)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)\n\tat java.lang.Thread.run(Unknown Source)\n\nI have an automated tool that generates the xml documents that need to be pushed. And after I receive this error, after a while, I receive 404.\n "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-15764336",
            "date": "2016-12-20T14:32:12+0000",
            "content": "I hitting this in 6.3.0 a lot and I don't know why, my TTL for zookeeper is 120s and I had no log into the gc log with pauses higher than 100ms\n\nExists some configuration to see the reason for the failure talking with ZooKeeper? like connection timeout or something else?\n\norg.apache.solr.common.SolrException: Cannot talk to ZooKeeper - Updates are disabled.\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.zkCheck(DistributedUpdateProcessor.java:1508)\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:696)\n\tat org.apache.solr.handler.loader.JavabinLoader$1.update(JavabinLoader.java:97)\n\tat org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readOuterMostDocIterator(JavaBinUpdateRequestCodec.java:179)\n\tat org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readIterator(JavaBinUpdateRequestCodec.java:135)\n\tat org.apache.solr.common.util.JavaBinCodec.readVal(JavaBinCodec.java:275)\n\tat org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readNamedList(JavaBinUpdateRequestCodec.java:121)\n\tat org.apache.solr.common.util.JavaBinCodec.readVal(JavaBinCodec.java:240)\n\tat org.apache.solr.common.util.JavaBinCodec.unmarshal(JavaBinCodec.java:158)\n\tat org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec.unmarshal(JavaBinUpdateRequestCodec.java:186)\n\tat org.apache.solr.handler.loader.JavabinLoader.parseAndLoadDocs(JavabinLoader.java:107)\n\tat org.apache.solr.handler.loader.JavabinLoader.load(JavabinLoader.java:54)\n\tat org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:97)\n\tat org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:68)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:153)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:2213)\n\tat org.apache.solr.servlet.HttpSolrCall.execute(HttpSolrCall.java:654)\n\tat org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:460)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:303)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:254)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1668)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:581)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1160)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:511)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1092)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\n\tat org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:169)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:518)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:308)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:244)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:273)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:95)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceAndRun(ExecuteProduceConsume.java:246)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:156)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:654)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)\n\tat java.lang.Thread.run(Thread.java:745) "
        },
        {
            "author": "Navneet Khanna",
            "id": "comment-16165782",
            "date": "2017-09-14T05:46:29+0000",
            "content": "Hi Guys,\n\nIs this issue fixed? I am getting this issue with solr 6.6.0 and zk 3.4.10. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-16166909",
            "date": "2017-09-14T20:07:47+0000",
            "content": "No, it's still marked as \"unresolved\". That have you read through the discussion tried the suggestions of increasing the ZK timeouts? "
        },
        {
            "author": "Gopalakrishnan B",
            "id": "comment-16353442",
            "date": "2018-02-06T06:12:41+0000",
            "content": "Hi Team,\n\nIs this issue resolved on the latest Solr 7.x (7.2.1)?\n\nThanks. "
        }
    ]
}