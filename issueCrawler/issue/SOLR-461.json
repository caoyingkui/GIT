{
    "id": "SOLR-461",
    "title": "Highlighting TokenStream Truncation capability",
    "details": {
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [],
        "components": [
            "highlighter"
        ],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Won't Fix"
    },
    "description": "It is sometimes the case when generating snippets that one need not fragment/analyze the whole document (especially for large documents) in order to show meaningful snippet highlights. \n\nPatch to follow that adds a counting TokenFilter that returns null after X number of Tokens have been seen.  This filter will then be hooked into the SolrHighlighter and configurable via solrconfig.xml.  The default value will be Integer.MAX_VALUE or, I suppose, it could be set to whatever Max Field Length is set to, as well.",
    "attachments": {
        "SOLR-461.patch": "https://issues.apache.org/jira/secure/attachment/12373428/SOLR-461.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mike Klaas",
            "id": "comment-12560053",
            "date": "2008-01-17T19:46:54+0000",
            "content": "Isn't this essentially the same thing as the hl.maxAnalyzedChars parameter?\n\nhttp://wiki.apache.org/solr/HighlightingParameters "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12560057",
            "date": "2008-01-17T19:58:15+0000",
            "content": "I suppose it is similar, but I don't find counting characters all that intuitive.  A token based approach doesn't cut off in the middle of a word and it isn't clear to me whether it is counting whitespace characters, etc.  Plus, it is analogous to Lucene's Max Field Length, which is token based as well. "
        },
        {
            "author": "Mike Klaas",
            "id": "comment-12560069",
            "date": "2008-01-17T20:20:38+0000",
            "content": "The max characters thing is directly from the lucene contrib highlighter.  It is based on token offset, so it counts whitespace, and doesn't cut of a token in the middle.\n\nIt also analogous to the RegexFragmenter's maxAnalyzedChars parameter, which can't be token-based.\n\nI'm not sure it is wise to add two apis with virtually the same functionality.  Anyone who wants to set a high limit will have to set both.\n\nHowever, it might be nice to make the token filter a pluggable component, so that users can insert this token filter if they want. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12560072",
            "date": "2008-01-17T20:25:44+0000",
            "content": "Well, here's the patch, although I didn't test for coordination with the maxChars piece. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12884806",
            "date": "2010-07-02T21:07:20+0000",
            "content": "The TokenFilter for limiting Token count is already in Lucene 3.x and trunk: LimitTokenCountFilter and a Wrapping Analyzer that adds this filter on top of any existing Analyzer. I think somebody already added it to Solr, too, dont know the issue, but have seen it. "
        }
    ]
}