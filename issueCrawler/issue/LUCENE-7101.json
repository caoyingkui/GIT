{
    "id": "LUCENE-7101",
    "title": "OfflineSorter's merging is O(N^2) cost for large sorts",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [
            "6.0",
            "6.1",
            "7.0"
        ],
        "priority": "Blocker",
        "status": "Closed",
        "type": "Bug"
    },
    "description": "Our OfflineSorter acts just like Lucene, writing small initial\nsegments of sorted values (from what it was able to sort at once in\nheap), periodically merging them when there are too many, and doing a\nforceMerge(1) in the end.\n\nBut the merge logic is too simplistic today, resulting in O(N^2)\ncost.  Smallish sorts usually won't hit it, because the default 128\nmerge factor is so high, but e.g. the new 2B points tests do hit the\nN^2 behavior.  I suspect the high merge factor hurts performance (OS\nstruggles to use what free RAM it has to read-ahead on 128 files), and\nalso risks file descriptor exhaustion.\n\nI think we should implement a simple log merge policy for it, and drop\nits default merge factor to 10.",
    "attachments": {
        "LUCENE-7101.patch": "https://issues.apache.org/jira/secure/attachment/12793380/LUCENE-7101.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15193359",
            "author": "Dawid Weiss",
            "date": "2016-03-14T14:23:25+0000",
            "content": "Sorry for being dim, what's the scenario of hitting N^2 complexity here? "
        },
        {
            "id": "comment-15193402",
            "author": "Michael McCandless",
            "date": "2016-03-14T14:54:50+0000",
            "content": "To get log(N) behavior you need to merge N segments that are \"close\" to the same size on each merge...\n\nE.g., say you are sorting \"plenty\" of data, with only 16 MB heap buffer to work with.\n\nToday, we will write 128 (default merge factor) 16 MB segments, and then merge those down to a single 2 GB (= 128 * 16 MB) segment.  Next we write another 127 16 MB segments, and merge the 2 GB segment, plus 127 16 MB segments, into a 3.98 GB segment.  Then another 127 16 MB segments, and merge that into a 5.97 GB segment, etc.\n\nSo the net number of MB copied by merging will be something like:\n\n  (128 + 0*127)*16 + (128 + 1*127)*16 + (128 + 2*127)*16 + (128 + 3*127)*16 + ...\n\nThe equation has a linear number of terms vs your input size (input data divided by 1.98 GB, or so?).\n\nBut this equation asymptotes towards O(N^2):\n\nIt rewrites to:\n\n  16 * 128 + 16 * (0*127 + 1*127 + 2*127 + 3*127 + ...)\n\nand rewrites to:\n\n  16 * 128 + 16 * 127 * (0 + 1 + 2 + 3 + ...)\n\nwhere the number of terms in that last part is linear vs your input data size.\n\nSo this bug only happens if you sort enough data, vs. your allowed heap usage, to require many merges, so the too-large 128 merge factor \"hides\" this problem for most use cases. "
        },
        {
            "id": "comment-15193990",
            "author": "Michael McCandless",
            "date": "2016-03-14T19:33:29+0000",
            "content": "Patch, adding a trivial \"log merge policy\" to OfflineSorter, and changing its default merge factor from 128 to 10.\n\nI also fixed the 2B points tests to not \"cheat\" by giving BKD more heap than the defaults, and improved BKDWriter's temp file naming\n\nI'm running Test2BBKDPoints.test2D now ... "
        },
        {
            "id": "comment-15194893",
            "author": "Dawid Weiss",
            "date": "2016-03-15T08:10:10+0000",
            "content": "Thanks Mike, this was detailed.  Patch looks good at first glance. "
        },
        {
            "id": "comment-15195051",
            "author": "Michael McCandless",
            "date": "2016-03-15T10:15:53+0000",
            "content": "I'm running Test2BBKDPoints.test2D now ...\n\nIt passed (after 12.2 hours, on spinning disk)! "
        },
        {
            "id": "comment-15195130",
            "author": "ASF subversion and git services",
            "date": "2016-03-15T11:35:05+0000",
            "content": "Commit 82c06190a35a8159288c2fb48d8d38d6d81dbbf2 in lucene-solr's branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=82c0619 ]\n\nLUCENE-7101: OfflineSorter had O(N^2) merge cost, and used too many temporary file descriptors, for large sorts "
        },
        {
            "id": "comment-15195136",
            "author": "ASF subversion and git services",
            "date": "2016-03-15T11:38:02+0000",
            "content": "Commit 287d6c8be6a75f43ff298483ef79e8226f60d138 in lucene-solr's branch refs/heads/branch_6x from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=287d6c8 ]\n\nLUCENE-7101: OfflineSorter had O(N^2) merge cost, and used too many temporary file descriptors, for large sorts "
        },
        {
            "id": "comment-15195139",
            "author": "ASF subversion and git services",
            "date": "2016-03-15T11:41:34+0000",
            "content": "Commit 6168fe1afb40286a7515b5909c3eb41db1ab6d00 in lucene-solr's branch refs/heads/branch_6_0 from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6168fe1 ]\n\nLUCENE-7101: OfflineSorter had O(N^2) merge cost, and used too many temporary file descriptors, for large sorts\n\nConflicts:\n\tlucene/core/src/java/org/apache/lucene/util/bkd/BKDWriter.java\n\tlucene/core/src/test/org/apache/lucene/util/bkd/Test2BBKDPoints.java "
        },
        {
            "id": "comment-15279249",
            "author": "Hoss Man",
            "date": "2016-05-10T23:47:16+0000",
            "content": "Manually correcting fixVersion per Step #S6 of LUCENE-7271 "
        }
    ]
}