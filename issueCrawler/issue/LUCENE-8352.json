{
    "id": "LUCENE-8352",
    "title": "Make TokenStreamComponents final",
    "details": {
        "components": [
            "modules/analysis"
        ],
        "status": "Resolved",
        "resolution": "Fixed",
        "fix_versions": [
            "master (8.0)"
        ],
        "affect_versions": "None",
        "labels": "",
        "priority": "Minor",
        "type": "Improvement"
    },
    "description": "The current design is a little trappy. Any specialised subclasses of TokenStreamComponents (see\u00a0StandardAnalyzer,\u00a0ClassicAnalyzer, UAX29URLEmailAnalyzer) are discarded by any subsequent Analyzers that wrap them (see LimitTokenCountAnalyzer, QueryAutoStopWordAnalyzer, ShingleAnalyzerWrapper and other examples in elasticsearch).\u00a0\n\nThe current design means each AnalyzerWrapper.wrapComponents() implementation discards any custom TokenStreamComponents and replaces it with one of its own choosing (a vanilla TokenStreamComponents class from examples I've seen).\n\nThis is a trap I fell into when writing a custom TokenStreamComponents with a custom setReader() and I wondered why it was not being triggered when wrapped\u00a0by other analyzers.\n\nIf AnalyzerWrapper is designed to encourage composition it's arguably a mistake to also\u00a0permit custom TokenStreamComponent subclasses\u00a0 - the composition process does not preserve the choice of custom classes and any behaviours they might add. For this reason we should not encourage extensions to TokenStreamComponents (or if TSC extensions are required we should somehow mark an Analyzer as \"unwrappable\" to prevent lossy compositions).",
    "attachments": {
        "LUCENE-8352.patch": "https://issues.apache.org/jira/secure/attachment/12940175/LUCENE-8352.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16509545",
            "author": "Mike Sokolov",
            "content": "I'm currently relying on a custom TokenStreamComponents, created in order to override setReader (that does seem like the only reason to override this class?). We don't use this Analyzer-wrapping pattern, so I guess we were lucky enough to avoid the trap you describe here. I'd be concerned if this were made private that some other extension mechanism be opened up to allow for cases when you want to take some action for each instance being indexed. In our case we pass some metadata along with the actual text to be analyzed that informs the analysis process.\n\nI'm having difficulty seeing how to add something that would pass additional metadata down into the analysis chain without some fairly major impact though.\n\nI think that overriding setReader provides some value currently that is challenging to achieve in any other way, so would be in favor of keeping it public, and looking into fixing the wrapping situation instead. For example, what if wrapComponents actually wrapped the original components instead of replacing it? Or, as you say, explore the idea of marking Analyzers as unwrappable. Perhaps AnalyzerWrapper should examine the type of TokenStreamComponents (or some new \"discardable\" method could be added) to decide whether it is safe? ",
            "date": "2018-06-12T12:07:43+0000"
        },
        {
            "id": "comment-16509601",
            "author": "Adrien Grand",
            "content": "Good catch Mark Harwood. I agree this is a bit trappy.\n\nFor this reason we should not encourage extensions to TokenStreamComponents\n\nIt looks like having the tokenizer in TokenStreamComponents is only useful for the default implementation of setReader. So maybe we could remove this setReader method, make TokenStreamComponents final, and replace the tokenizer field with a Consumer<Reader> that would be tokenizer::setReader by default? I think Mike Sokolov's use-case would still work. ",
            "date": "2018-06-12T12:54:21+0000"
        },
        {
            "id": "comment-16509635",
            "author": "Mark Harwood",
            "content": "My use case was a bit special. I had a custom reader that dealt with hyperlinked text\u00a0and stripped out the\u00a0hyperlink markup using a custom Reader before feeding the remaining plain-text into tokenisation. The tricky bit was the extracted URLs would not be thrown away but passed to a special TokenFilter at the end of the chain to inject at the appropriate positions in the\u00a0text token stream.\n\nThe workaround was a custom AnalyzerWrapper that overrode wrapReader (which is still invoked when wrapped) and then some ThreadLocal hackery to get my TokenFilter connected to the Reader's extracted urls.\u00a0\n\nI'm not sure how common this sort of analysis is but before I reached this solution there was quite a detour trying to figure out why a custom TokenStreamComponents was not working when wrapped.\n\n\u00a0 ",
            "date": "2018-06-12T13:42:33+0000"
        },
        {
            "id": "comment-16509726",
            "author": "Mike Sokolov",
            "content": "So maybe we could remove this setReader method, make TokenStreamComponents final, and replace the tokenizer field with a Consumer<Reader> that would be tokenizer::setReader by default?\n\nI think that would work for me, yes, and not too difficult either   ",
            "date": "2018-06-12T15:05:09+0000"
        },
        {
            "id": "comment-16618834",
            "author": "Adrien Grand",
            "content": "+1 Great to see that the patch is simpler than I expected. Let's add a note to MIGRATE.txt and keep the ctor that takes a Tokenizer on 7.x but make it deprecated? ",
            "date": "2018-09-18T10:10:14+0000"
        },
        {
            "id": "comment-16619000",
            "author": "Alan Woodward",
            "content": "Here's a patch implementing Adrien Grand's idea.\u00a0 I renamed\u00a0TokenStreamComponenents.getTokenizer to getSource as that seemed to make more sense, and added a test to AnalyzerWrapper to ensure that everything gets called correctly.\n\nMost of the patch is changes to a couple of suggester tests which were heavily abusing TokenStreamComponents.  I also added a method to PrefixTreeStrategy to create a special Analyzer for Shape methods, currently only used by Solr's analysis page, but possibly more generally useful. (cc David Smiley) ",
            "date": "2018-09-18T11:54:18+0000"
        },
        {
            "id": "comment-16619038",
            "author": "Alan Woodward",
            "content": ">  keep the ctor that takes a Tokenizer on 7.x but make it deprecated?\n\nI don't think it needs to be deprecated? It's still used in plenty of places. ",
            "date": "2018-09-18T12:19:07+0000"
        },
        {
            "id": "comment-16619046",
            "author": "Adrien Grand",
            "content": "Oops indeed. I was considering deprecating it to make wrapping less trappy, but now that I think about it again, TokenStreamComponents doesn't expose a Tokenizer anymore, so there is no risk of passing the tokenizer rather than the Consumer<Reader> source when wrapping. Unrelated: javadocs of TokenStreamComponents#getSource still say this method returns a tokenizer. ",
            "date": "2018-09-18T12:26:53+0000"
        },
        {
            "id": "comment-16619349",
            "author": "Alan Woodward",
            "content": "Patch with fixed javadocs.  I ended up having to change the way AbstractSpatialPrefixTreeFieldType created its Analyzer, but I think it's reasonably tidy. ",
            "date": "2018-09-18T16:13:21+0000"
        },
        {
            "id": "comment-16619493",
            "author": "David Smiley",
            "content": "Looks good Alan.  Thanks for the CC. ",
            "date": "2018-09-18T18:08:10+0000"
        },
        {
            "id": "comment-16620376",
            "author": "ASF subversion and git services",
            "content": "Commit c696cafc0d7fc0e133df20e0b188655fa020fe99 in lucene-solr's branch refs/heads/master from Alan Woodward\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c696caf ]\n\nLUCENE-8352: Make TokenStreamComponents final ",
            "date": "2018-09-19T09:35:42+0000"
        }
    ]
}