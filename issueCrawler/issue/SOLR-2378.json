{
    "id": "SOLR-2378",
    "title": "FST-based Lookup (suggestions) for prefix matches.",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "3.3",
            "4.0-ALPHA"
        ],
        "components": [
            "spellchecker"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Implement a subclass of Lookup based on finite state automata/ transducers (Lucene FST package). This issue is for implementing a relatively basic prefix matcher, we will handle infixes and other types of input matches gradually. Impl. phases:\n\n\n\twrite a DFA based suggester effectively identical to ternary tree based solution right now,\n\tbaseline benchmark against tern. tree (memory consumption, rebuilding speed, indexing speed; reuse Andrzej's benchmark code)\n\tmodify DFA to encode term weights directly in the automaton (optimize for onlyMostPopular case)\n\tbenchmark again\n\tbenchmark again\n\tmodify the tutorial on the wiki http://wiki.apache.org/solr/Suggester",
    "attachments": {
        "SOLR-2378.patch": "https://issues.apache.org/jira/secure/attachment/12476318/SOLR-2378.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Dawid Weiss",
            "id": "comment-12997702",
            "date": "2011-02-22T09:06:36+0000",
            "content": "Hi guys. I probably need those admin rights on SOLR's JIRA project as well because I can't assign this to myself. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12997703",
            "date": "2011-02-22T09:10:20+0000",
            "content": "This time I cannot help - I don't have admin rights in SOLR! In my opinion, we should merge the user rights of both projects. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12997786",
            "date": "2011-02-22T13:18:30+0000",
            "content": "Hi guys. I probably need those admin rights on SOLR's JIRA project as well because I can't assign this to myself\n\nYou should have a committer role now. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13013932",
            "date": "2011-03-31T11:32:57+0000",
            "content": "I didn't have time to take care of this until now, apologies. So, looking at Lookup#lookup(), I just wanted to clarify:\n\n\n  /**\n   * Look up a key and return possible completion for this key.\n   * @param key lookup key. Depending on the implementation this may be\n   * a prefix, misspelling, or even infix.\n   * @param onlyMorePopular return only more popular results\n   * @param num maximum number of results to return\n   * @return a list of possible completions, with their relative weight (e.g. popularity)\n   */\n  public abstract List<LookupResult> lookup(String key, boolean onlyMorePopular, int num);\n\n\n\nthe \"onlyMorePopular\" means more popular than... what? I see TSTLookup and JaspellLookup (Andrzej, will you confirm, please?) sorts matches in a priority queue by their associated value (frequency I guess). This makes sense, but onlyMorePopular is misleading \u2013 it should be called onlyMostPopular (those with the native knowledge of English subtlieties, speak up if I'm right here).\n\nI also see and wanted to confirm \u2013 the Dictionary can come from various sources, so we can't rely on the presence of the built-in Lucene automaton, can we? Even if I wanted to reuse it, there'd be no easy way to determine if it's a full automaton, or a partial one (because of the gaps/trimming)... I think I'll just implement the solution by building the automaton from whatever Dictionary comes in and serializing/ deserializing it similar to TSTLookup.\n\nSounds ok?\n\n\n "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13013933",
            "date": "2011-03-31T11:43:57+0000",
            "content": "I see TSTLookup and JaspellLookup (Andrzej, will you confirm, please?) sorts matches in a priority queue by their associated value (frequency I guess)\n\nCorrect. I agree that the name is so-so, I inherited it from the spellchecker API - feel free to change it.\n\nthe Dictionary can come from various sources, ...\n\nYes. This is again a legacy of the Lucene SpellChecker API. I tried to extend this API in Solr without changing it in Lucene (see the *IteratorWrapper classes and TermFreqIterator) but ultimately it would be better to refactor this. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13014508",
            "date": "2011-04-01T10:31:49+0000",
            "content": "The original suggester issue. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13015011",
            "date": "2011-04-02T11:53:01+0000",
            "content": "Soliciting feedback on the following questions:\n\n\tsuggesters currently have float weights associated with terms; can these floats be bucketed and returned in approximation or do they need to be exact copies of the input? For automata, bucketed weights (to, let's say 5-10 different values) provide terrific speed/size improvements, so if this is not a rigid requirement, I'd use them.\n\tis there any info on threading of Solr components? I am in particular looking for mutable object fields in the suggester (can a single suggester instance be accessed by multiple threads at the same time)?\n\n\n\nI've implemented preliminary FST-based lookup (without weights yet). Speed-wise it doesn't rock because data is converted to/from utf8 on input/output and sorted during construction, but it is still acceptable, even at this early stage I think:\n\n\nJaspellLookup        buildTime[ms]=112 lookupTime[ms]=288\nTSTLookup            buildTime[ms]=115 lookupTime[ms]=103\nFSTLookup            buildTime[ms]=464 lookupTime[ms]=145\n\n\n\nnow... that was speed only, check out the in-memory size \n\n\nJaspellLookup        size[B]=81,078,997\nTSTLookup            size[B]=53,453,696\nFSTLookup            size[B]=2,909,396\n\n\n\n(This benchmark stores very limited vocabulary items \u2013 long numbers only, so it is skewed from reality, but it's nice to see something like this, huh?). "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13015053",
            "date": "2011-04-02T17:09:30+0000",
            "content": "is there any info on threading of Solr components? I am in particular looking for mutable object fields in the suggester (can a single suggester instance be accessed by multiple threads at the same time)?\n\nComponents are initalized at startup and the same instance is used for every request (multi-threaded)\n\nIf you need to use the same obejects in prepare and process, you can either put then in the request context map, or add something to ResponseBuilder (perhaps better if this is widely used) "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13015305",
            "date": "2011-04-04T08:03:45+0000",
            "content": "Thanks Ryan. Well, I'll just allocate off the heap, hoping it'll fit in TLB.\n\nI will also need a reasonably large list of suggestions to test/benchmark. By realistic I mean something that folks actually use in production (terms & weights) would be great; a sample of a few hundred thousand terms would be probably good enough . I can come up with something of my own (based on wikipedia or Carrot2 data), but a real data set would be better. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13015627",
            "date": "2011-04-04T21:24:49+0000",
            "content": "I'm done for the day. Lots of benchmarking and tweaking. Conclusions:\n\n\tUse raw utf16 values of input strings as they are ready to use in the input Strings and don't need to be converted. The automaton is slightly bigger, but not that much (read: most likely because English ascii chars fall into single byte vint range anyway).\n\tI created a non-recursive and recursive version of the main tail-state traversal loop, but the gain is minimal.\n\tI have a relatively fast machine (i7, quad core) and parallel GC combined with tlabs makes local allocations nearly zero-cost. Locally reused structures speed up by 5%, but increase code complexity greatly.\n\tThe benchmark currently in the codebase is very skewed. I tried on real life data (that I cannot share, unfortunately) and the results are:\n\nTSTLookup            size[B]=12,598,794\nFSTLookup            size[B]=472,679\n* Running 10 iterations for TSTLookup ...\n  - warm-up 5 iterations...\n  - main iterations:\nTSTLookup            buildTime[ms]=19 lookupTime[ms]=4,813\n* Running 10 iterations for FSTLookup ...\n  - warm-up 5 iterations...\n  - main iterations:\nFSTLookup            buildTime[ms]=68 lookupTime[ms]=263\n\n\nIt looks as if I've made a mistake, but not really. With the actual data the fan-out rate is much higher than on long numbers. Also, real terms are shorter than longs and 75% of their prefix is usually a 2-3 letter combination resulting in lots of matches that need to be passed through the priority queue, etc. With the FST weights are part of the search structure (approximated weights, to be exact) and so the cost of building it is  higher, but the gain at runtime is substantial. The lookup time should be virtually constant (with very small deviation).\n\tJaspellLookup does not pass the tests if the input prefix has a space inside . TSTLookup and FSTLookup work just fine.\n\tI compared against automata API in Morfologik; the speedup is about 30% (only ints are passed, no Arc object decompression, etc.). But then \u2013 the API of Lucene is more flexible.\n\n\n\nI'll clean up the code and create a final patch for testing/ evaluation tomorrow. "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13015631",
            "date": "2011-04-04T21:31:10+0000",
            "content": "Wow those improvements are awesome \u2013 FST 26.7X smaller RAM footprint, 18X faster lookups, but build time is 3.6X slower.\n\nThis is built on a composite reader, right?  Does the build time include the time to enum the terms from MultiTermsEnum? "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13015634",
            "date": "2011-04-04T21:40:39+0000",
            "content": "The build time needs to sort the input again (and create it in the first place). Because Lookup API assumes suggestion keywords can come from a variety of sources there is no guarantee they will be sorted, so we need to sort them before we can build the automaton. \n\nStill, I think the numbers are acceptable... if you need on-line construction of these suggestions you'll pick TST (it can add new keywords to the structure dynamically); for a batch-load suggester you'd pick the FST one.\n\nIt is also very likely that I overlooked something that could bring those numbers down, I'll create a clean patch tomorrow, so everything will be out there for improving. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13015898",
            "date": "2011-04-05T12:50:28+0000",
            "content": "FST implementation of Lookup interface. Completely rewritten benchmark class utilizing real data. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13015902",
            "date": "2011-04-05T13:02:55+0000",
            "content": "This is a git patch (trim the first level to apply without git, should work out of the box) against the trunk that implements FST-based suggester. I've also added a more realistic benchmark that shows performance limitations of the current suggester implementations, especially in the \"onlyMorePopular\" mode. Some benchmarks from my machine:\n\nTime to create internal data structures out of 50.000 suggestion terms (692kB of UTF8):\n\nJaspellLookup   input: 50,000, time[ms]: 30 [+- 4.93]\nTSTLookup       input: 50,000, time[ms]: 39 [+- 2.56]\nFSTLookup       input: 50,000, time[ms]: 62 [+- 2.52]\n\n\n\nMemory used:\n\nJaspellLookup   size[B]:    7,868,863\nTSTLookup       size[B]:    7,914,144\nFSTLookup       size[B]:      300,148\n\n\n\nTraversal speed (randomized prefixes of input sequences). We start with full matches:\n\n-- prefixes: 100-200, num: 7, onlyMorePopular: true\nJaspellLookup   queries: 50000, time[ms]: 108 [+- 6.23], ~qps: 462\nTSTLookup       queries: 50000, time[ms]: 54 [+- 1.54], ~qps: 922\nFSTLookup       queries: 50000, time[ms]: 66 [+- 1.30], ~qps: 761\n\n\n\nThese results are overly optimistic because users rarely type in something in full; let's cut the prefix length to 6-9 chars:\n\n-- prefixes: 6-9, num: 7, onlyMorePopular: true\nJaspellLookup   queries: 50000, time[ms]: 155 [+- 4.20], ~qps: 322\nTSTLookup       queries: 50000, time[ms]: 208 [+- 3.99], ~qps: 241\nFSTLookup       queries: 50000, time[ms]: 71 [+- 1.36], ~qps: 700\n\n\n\nClearly, the FST is more resilient to the length of the input prefix... let's cut it to 2-4 characters:\n\n-- prefixes: 2-4, num: 7, onlyMorePopular: true\nJaspellLookup   queries: 50000, time[ms]: 420 [+- 5.37], ~qps: 119\nTSTLookup       queries: 50000, time[ms]: 1687 [+- 10.96], ~qps: 30\nFSTLookup       queries: 50000, time[ms]: 90 [+- 2.27], ~qps: 554\n\n\n\nI didn't have the time to look into it, but TST's result is surprisingly bad with such short prefixes. FST keeps nearly the same perf. level.\n\nIn the current implementation I throw an exception if somebody tries to get suggestions from FST without sorting by weights (this is equivalent to building the suggester with a single weight for all terms). This could be implemented, but at a small performance penalty \u2013 to be considered if you think it is useful. The above performance problems for short prefixes are interestingly present even with onlyMorePopular set to false:\n\n-- prefixes: 100-200, num: 7, onlyMorePopular: false\nJaspellLookup   queries: 50000, time[ms]: 94 [+- 6.45], ~qps: 532\nTSTLookup       queries: 50000, time[ms]: 46 [+- 0.98], ~qps: 1092\n-- prefixes: 6-9, num: 7, onlyMorePopular: false\nJaspellLookup   queries: 50000, time[ms]: 123 [+- 3.12], ~qps: 405\nTSTLookup       queries: 50000, time[ms]: 188 [+- 3.03], ~qps: 266\n-- prefixes: 2-4, num: 7, onlyMorePopular: false\nJaspellLookup   queries: 50000, time[ms]: 225 [+- 5.69], ~qps: 222\nTSTLookup       queries: 50000, time[ms]: 1523 [+- 16.69], ~qps: 33\n\n\n\nPeek at the code and let me know if you can think of any improvements/ modifications, especially wrt Solr infrastructure (I specifically didn't implement any real solr instance test case).  "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13015921",
            "date": "2011-04-05T13:44:50+0000",
            "content": "Updated empty input/ failing test case. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13016372",
            "date": "2011-04-06T13:44:58+0000",
            "content": "I've been waiting for somebody to look at this patch, guys, just to confirm that everything is fine with it. If so, I'd like to commit it in and move on to infix suggestions support maybe. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13016378",
            "date": "2011-04-06T13:55:27+0000",
            "content": "Took a quick look:\n\nBuilder.add(char[], int, int, ..) adds codepoints (Character.codePointAt/Character.charCount) [utf-32 order] but the comparator you use when building the automaton compares characters [utf-16 order]. so if someone has a term in the supplementary range in their index, the order will be inconsistent.\n\nSo I think the comparator should just compare codepoints (it should iterate with codePointAt/charCount too)? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13016381",
            "date": "2011-04-06T14:00:00+0000",
            "content": "If it causes too much of a lookup performance hit, the Builder could just build in utf-16 order too? "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13016384",
            "date": "2011-04-06T14:07:13+0000",
            "content": "I am referring to build-time, not runtime here.\n\nrun-time can handle supplementary characters wrong and I wouldn't object to committing it,\nbut currently if someone has terms > 0xFFFF in their index it will preventing the FST from\nbeing built at all and suggesting will not work? (as i think the FST will throw exc?)  "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13016481",
            "date": "2011-04-06T18:39:14+0000",
            "content": "Oh, right \u2013 I didn't peek at the inside of Builder.add(char[],...), but I will verify this by trying to add something that has multilingual stuff in it \u2013 will update the patch tomorrow, hopefully. I would also love to have somebody who actually uses suggestions to try to compile it and use it on a production data set to see if my benchmark was approximately right with respect to the speed differences between the different available implementations. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13016805",
            "date": "2011-04-07T10:56:38+0000",
            "content": "Well spotted, Robert \u2013 indeed, three-byte codepoints were throwing automaton exceptions. I've added a test for this. I also added \"exact match promotion\" to the top of the suggestions list, regardless of the score of the exact match. This is controlled by a final flag at the moment... maybe it should become a parameter, I don't know. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13016818",
            "date": "2011-04-07T11:39:57+0000",
            "content": "Updated patch:\n\n\tfixed a bug with unicode codepoints [rmuir]\n\tadded promotion of exact matches to the top of the list (weighted mode)\n\tadded a \"poor man's\" alphabetical lookup even if weights are used (still faster than the existing implementations, but suboptimal to the case when no weights are used at all).\n\tAdded a few test cases here and there, reorganized the code.\n\tTurned thresholds into real config parameters.\n\n "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13016820",
            "date": "2011-04-07T11:44:06+0000",
            "content": "Ok, updated patch. The only thing I would like to add is a real Solr handler test, much like SuggesterTest. Should I add a separate test class or simply add another handler to that config file and test methods to SuggesterTest?\n\nAlso, this one puzzled me:\n\n    threshold = config.get(THRESHOLD_TOKEN_FREQUENCY) == null ? 0.0f\n            : (Float)config.get(THRESHOLD_TOKEN_FREQUENCY);\n\n\nWhat are the conversion rules for NamedList that SolrSpellChecker gets in init()? I have an Integer parameter, but didn't check what is returned for, say, \"12\" (String, Float, Integer?). "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13019774",
            "date": "2011-04-14T10:41:53+0000",
            "content": "Adding Solr tests, removed the big queries file so that it doesn't bloat the patch (will commit it in directly). "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13019783",
            "date": "2011-04-14T11:15:46+0000",
            "content": "In trunk. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13019932",
            "date": "2011-04-14T17:38:48+0000",
            "content": "Just an idea: should we default to this implementation in trunk?\nIt seems to be a significant reduction in RAM. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13019991",
            "date": "2011-04-14T19:15:20+0000",
            "content": "We could... after all when 4.0 sees the light of day backwards compatibility does not need to be (strictly) enforced, right? I would love somebody who actually uses this piece of functionality to test it in a realistic setting first though. If nothing else, this would at least tell us if any performance gain is visible at all (I'm guessing the HTTP stack will be the bottleneck before we can see any differences between all the implemented methods). The big drop in RAM usage should be visible right away, so this is a bonus. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13042838",
            "date": "2011-06-02T16:02:01+0000",
            "content": "Note: this is now backported to branch_3x. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13058953",
            "date": "2011-07-02T02:43:13+0000",
            "content": "Bulk close for 3.3 "
        }
    ]
}