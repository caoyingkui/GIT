{
    "id": "LUCENE-1753",
    "title": "Make not yet final core/contrib TokenStream/Filter implementations final",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/analysis"
        ],
        "type": "Task",
        "fix_versions": [
            "3.0"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Lucene's analysis package is designed in a way, that you can plug different implementations of analysis in chains of TokenStreams and TokenFilters. An analyzer is build of several TokenStreams/Filters that do the tokenization of text. If you want to modify the behaviour of tokenization, you implement a new subclass of TokenStream/-Filter/Tokenizer.\n\nMost classes in the core are correctly implemented like that. They are itsself final or their implementation methods are final (CharTokenizer).\n\nA lot of problems with backwards-compatibility of LUCENE-1693 are some classes in Lucene's core/contrib not yet final:\n\n\tKeywordTokenizer should be declared final or its implementation methods should be final\n\tStandardTokenizer should be declared final or its implementation methods should be final\n\tISOLatin1Filter is deprecated, so it will be removed in 3.0, nothing to do.\n\n\n\nCharTokenizer is the abstract base class of several other classes. The design is correct: Child classes cannot override the implementation, they can only change the behaviour of this final implementation.\n\nContrib should be checked, that all implementation classes are at least final or they are designed in the same way like CharTokenizer.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2009-10-11T17:35:49+0000",
            "content": "Committed revision: 824116 ",
            "author": "Uwe Schindler",
            "id": "comment-12764498"
        }
    ]
}