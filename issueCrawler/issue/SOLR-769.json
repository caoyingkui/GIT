{
    "id": "SOLR-769",
    "title": "Support Document and Search Result clustering",
    "details": {
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [
            "1.4"
        ],
        "components": [
            "contrib - Clustering"
        ],
        "type": "New Feature",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Clustering is a useful tool for working with documents and search results, similar to the notion of dynamic faceting.  Carrot2 (http://project.carrot2.org/) is a nice, BSD-licensed, library for doing search results clustering.  Mahout (http://lucene.apache.org/mahout) is well suited for whole-corpus clustering.  \n\nThe patch I lays out a contrib module that starts off w/ an integration of a SearchComponent for doing clustering and an implementation using Carrot.  In search results mode, it will use the DocList as the input for the cluster.   While Carrot2 comes w/ a Solr input component, it is not the same as the SearchComponent that I have in that the Carrot example actually submits a query to Solr, whereas my SearchComponent is just chained into the Component list and uses the ResponseBuilder to add in the cluster results.\n\nWhile not fully fleshed out yet, the collection based mode will take in a list of ids or just use the whole collection and will produce clusters.  Since this is a longer, typically offline task, there will need to be some type of storage mechanism (and replication??????) for the clusters.  I may push this off to a separate JIRA issue, but I at least want to present the use case as part of the design of this component/contrib.  It may even make sense that we split this out, such that the building piece is something like an UpdateProcessor and then the SearchComponent just acts as a lookup mechanism.",
    "attachments": {
        "SOLR-769.patch": "https://issues.apache.org/jira/secure/attachment/12391945/SOLR-769.patch",
        "SOLR-769-analyzerClass.patch": "https://issues.apache.org/jira/secure/attachment/12408894/SOLR-769-analyzerClass.patch",
        "clustering-libs.tar": "https://issues.apache.org/jira/secure/attachment/12391944/clustering-libs.tar",
        "SOLR-769-lib.zip": "https://issues.apache.org/jira/secure/attachment/12402482/SOLR-769-lib.zip",
        "SOLR-769.zip": "https://issues.apache.org/jira/secure/attachment/12402688/SOLR-769.zip",
        "SOLR-769.tar": "https://issues.apache.org/jira/secure/attachment/12405886/SOLR-769.tar",
        "clustering-componet-shard.patch": "https://issues.apache.org/jira/secure/attachment/12409722/clustering-componet-shard.patch",
        "subcluster-flattening.patch": "https://issues.apache.org/jira/secure/attachment/12412899/subcluster-flattening.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Grant Ingersoll",
            "id": "comment-12638637",
            "date": "2008-10-10T20:00:37+0000",
            "content": "Starting docs at http://wiki.apache.org/solr/ClusteringComponent "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12638791",
            "date": "2008-10-11T19:29:34+0000",
            "content": "Patch soon, as a start.  I'm going to check in the basic directory structure and libs, and then provide a patch with the source that we can iterate on. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12638794",
            "date": "2008-10-11T19:54:00+0000",
            "content": "Clustering libs "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12638795",
            "date": "2008-10-11T20:05:39+0000",
            "content": "First draft of a patch.\n\nNotes:\n\n1. Carrot2 uses the snowball stemmers, but it shouldn't clash, b/c it actually slightly changes the names of them to be like englishStemmer (as opposed to EnglishStemmer).  I'm debating whether or not to just re-implement this so that it can use the same snowball stemmers we use in Solr.  Probably not a big deal.\n\n2. I haven't implemented document clustering yet.  To do this, I need to setup a background thread that will be spawned to do the clustering, since it is presumably going through some large set of documents and clustering them.  To do this, it will probably require term vectors.  This will introduce a dep. on Mahout, so I'll need a version of that library too.\n\n3. It would be really cool for the Carrot2 implementation to support using other clustering algs besides Lingo.  Basically, this just needs to be factored into the configuration and the jars included in the distribution.  This is not a high priority for me at the moment.\n\nTODO:\nMore tests.\nDecide on output format\nImplement doc. clustering framework part (i.e. spawning of threads, commands)\n???? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12638813",
            "date": "2008-10-12T02:27:44+0000",
            "content": "More updates, added example "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12638814",
            "date": "2008-10-12T02:34:10+0000",
            "content": "Still to do, more testing, get feedback, implement basics of doc. clustering.  This last piece will take some more design work.  Also need to validate some more that the results make sense for search results clustering, but my first look suggests they do. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12638875",
            "date": "2008-10-12T18:03:31+0000",
            "content": "FYI, Carrot2 does support a handful of different clustering algorithms (the ones I know of are Fuzzy Ants, KMeans and Suffix Tree, in addition to Lingo). "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12638924",
            "date": "2008-10-12T23:37:15+0000",
            "content": "Yeah, I probably will include the other jars and make it easy to include them.  For now, I wanted to get something basic working for a talk I'm giving on Wednesday night  "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12639831",
            "date": "2008-10-15T13:11:31+0000",
            "content": "Here's a patch that actually passes the tests.\n\nNote, there's still a little oddity with the Snowball program that needs to be worked out, thus I don't recommend running this patch in production yet.  The issue is that both Carrot and Solr have deps on Snowball, but on different versions, furthermore, Carrot2 goes one further and slightly modifies the names of Snowball.\n\nI will upload new libs in a minute. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12639833",
            "date": "2008-10-15T13:12:56+0000",
            "content": "Untar in contrib/clustering/lib. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12639835",
            "date": "2008-10-15T13:20:33+0000",
            "content": "Note, also, that even though I put in support for some of the other C2 (Carrot2) algorithms, I don't think they quite work yet.  I think they require passing in more parameters to set some algorithm properties (for instance, for Fuzzy Ants, I think you need to set a depth) and I haven't figured those out yet.  If you have C2 experience, insight would be appreciated.\n\nFor now, stick to Lingo. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12641119",
            "date": "2008-10-20T18:55:33+0000",
            "content": "Removed the alternate algorithm implementations, but left in some of the framework for adding them.  The Carrot2 maintainers are likely to remove Fuzzy Ants and some of the other implementations in 3.0, which is due out sometime soon.  Thus, I'd rather not support something that isn't recommended.\n\nI'm likely to commit this fairly soon.\n\n-Grant "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12641342",
            "date": "2008-10-21T10:41:59+0000",
            "content": "OK, here's a first scratch at the component side of document clustering.  There are no implementations of the DocumentClusteringEngine yet, so I am bit hesitant to even throw out a proposed API for that yet, but the current one is pretty generic, which is both good and bad.  I don't particularly like passing around something as open as SolrParams, but I don't think I can pin down a generic set of explicit parameters either. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12641377",
            "date": "2008-10-21T13:28:36+0000",
            "content": "How about a patch where the tests pass?    Here ya go... "
        },
        {
            "author": "Vaijanath N. Rao",
            "id": "comment-12641812",
            "date": "2008-10-22T13:12:47+0000",
            "content": "Hi Grant,\n\nFor just minor copying of .txt file I got this working without any problems. \n\nSo what would be the procedure to add some clustering code beyond carrot or other available libraries.\n\n--Thanks and Regards\nVaijanath "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12641823",
            "date": "2008-10-22T13:34:55+0000",
            "content": "So what would be the procedure to add some clustering code beyond carrot or other available libraries.\n\nEssentially, you need to implement either a SearchClusteringEngine or a DocumentClusteringEngine and then hook declare it in the SearchComponent configuration, as is done with the Carrot2 example here:\n\n<lst name=\"engine\">\n      <!-- The name, only one can be named \"default\" -->\n      <str name=\"name\">default</str>\n      <!-- Carrot2 specific parameters.  See the Carrot2 site for details on setting. -->\n      <!-- carrot.algorithm:   Optional.  Currently only\n      lingo is supported pending the release of Carrot2 3.0.  \n       -->\n      <str name=\"carrot.algorithm\">lingo</str>\n      <!-- Lingo specific -->\n      <float name=\"carrot.lingo.threshold.clusterAssignment\">0.150</float>\n      <float name=\"carrot.lingo.threshold.candidateClusterThreshold\">0.775</float>\n    </lst>\n\n\nor, in the mock setup:\n\n<lst name=\"engine\">\n      <!-- The name, only one can be named \"default\" -->\n      <str name=\"name\">docEngine</str>\n      <str name=\"classname\">org.apache.solr.handler.clustering.MockDocumentClusteringEngine</str>\n    </lst>\n\n\n\nIf you don't declare the classname value, then it assumes the Carrot implementation.\n\nNaturally, you need to take care of all the libraries being available to Solr, etc. just as you would for any plugin.\n\nSince you are interested in clustering, Vaijanath, it would be good to get your feedback on the APIs.  Are you doing full document clustering or just search snippet clustering?   Also, if you are using an open source clustering library that has acceptable licensing terms (i.e. not GPL or similar), perhaps consider contributing an implementation of the engine and then we can make it available to everyone. "
        },
        {
            "author": "Vaijanath N. Rao",
            "id": "comment-12641832",
            "date": "2008-10-22T13:53:18+0000",
            "content": "Hi Grant,\n\nTill now I have worked mostly with full document clustering. Had never thought of search snippet clustering.  I will definitely pitch in for clustering library. There are many libraries which have favourable/acceptable licensing terms which can be added to Solr.\n\n--Thanks and Regards\nVaijanath "
        },
        {
            "author": "Bruce Ritchie",
            "id": "comment-12642179",
            "date": "2008-10-23T15:09:57+0000",
            "content": "Grant,\n\nThis patch looks very promising, I can't wait to give it a try and find a way to incorporate it into a project I'm working on (when it's ready of course ... likely not till after Carrot2 3  is released though)\n\nCan you give a quick estimate as to the performance impact of enabling clustering in search results mode? In the example @ http://wiki.apache.org/solr/ClusteringFullResultsExample the query time seems pretty high and I was wondering if that was a result of this patch or something else?\n\nThanks,\n\nBruce Ritchie "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12642182",
            "date": "2008-10-23T15:14:25+0000",
            "content": "Bruce,\n\nFor performance of the clustering algorithm alone, please take a look at: http://project.carrot2.org/algorithms.html\nObviously, you'd need to add the overhead of fetching the snippets / documents from the index. Not sure how many are fetched and whether they come from Solr's cache or not, so not sure if clustering or fetching time is prevailing.\n\nCheers,\n\nStaszek "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12642187",
            "date": "2008-10-23T15:31:26+0000",
            "content": "Hi Bruce,\n\nI haven't done any perf. testing, as I've been focused on functionality first.  However, I'm not sure whether that query was the first one run, or not, so I don't know the status of the searcher, etc.  I'm pretty sure I don't have any warming queries, etc. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12663332",
            "date": "2009-01-13T13:53:51+0000",
            "content": "Updated to trunk.  See http://wiki.apache.org/solr/ClusteringComponent "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12672278",
            "date": "2009-02-10T14:43:30+0000",
            "content": "Here's a patch for Carrot2 3.0 that COMPILES ONLY.  \nYou will need to download the clustering-libs.tar.gz from http://people.apache.org/~gsingers/clustering-libs.tar.gz as it is too big to upload to JIRA.\n\nTODO:\n1. Tests passing and more tests\n2. Update NOTICE.txt and LICENSE.txt\n3. Get trimmed down Carrot2 library that doesn't have all the Document Source dependencies, and preferably the web services deps.  Solr doesn't need the Google, etc. API deps.  Preferably remove the LGPL deps too, but for now, they are downloaded via ANT from the Maven repositories.\n4. Update the Maven template\n5. Hook in the builds\n6. Make sure the example works "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12672281",
            "date": "2009-02-10T14:50:08+0000",
            "content": "Hi Grant,\n\nI've added a Carrot2 issue referring to point 3 on your TODO list: http://issues.carrot2.org/browse/CARROT-457. I'll be looking into this over the weekend.\n\nStaszek "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12680940",
            "date": "2009-03-11T17:37:59+0000",
            "content": "Yet another patch, this time with passing unit tests and working example. Will make some more comments in a sec. Please use SOLR-769-lib.zip libs with this patch. "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12680942",
            "date": "2009-03-11T17:43:45+0000",
            "content": "Hi All,\n\nI've just uploaded a patch that passes unit tests and has working example, but this is by no means a final version. A few outstanding questions / issues:\n\n1. Response structure.\n\nI was wondering \u2013 to we need to repeat the document contents in the 'clusters' response section? Assuming that each document in the index has a unique ID, we could reduce the size of the response by just referencing documents by IDs like this:\n\n\n<lst name=\"clusters\">\n <int name=\"numClusters\">3</int>\n <lst name=\"cluster\">\n  <lst name=\"labels\">\n    <str name=\"label\">GPU VPU Clocked</str>\n  </lst>\n  <lst name=\"docs\">\n    <str name=\"doc\">EN7800GTX/2DHTV/256M</str>\n    <str name=\"doc\">100-435805</str>\n  </lst>\n </lst>\n <lst name=\"cluster\">\n  <lst name=\"labels\">\n    <str name=\"label\">Hard Drive</str>\n  </lst>\n  <lst name=\"docs\">\n    <str name=\"doc\">6H500F0</str>\n    <str name=\"doc\">SP2514N</str>\n  </lst>\n </lst>\n <lst name=\"cluster\">\n  <lst name=\"labels\">\n    <str name=\"label\">Other Topics</str>\n  </lst>\n  <lst name=\"docs\">\n    <str name=\"doc\">9885A004</str>\n  </lst>\n </lst>\n\n\nActually, this is what I've implemented in the patch.\n\nAlso, in case of hierarchical clusters I've introduced a grouping entity called \"clusters\" so that the top- and sub-levels or the response are consistent (see unit tests). Please let me know if this makes sense.\n\n\n\n2. Build: compile warnings about missing SimpleXML\n\nSimpleXML is one of the problematic dependencies as it's GPL. Luckily, it's not needed at runtime, but generates warnings about missing dependencies during compile time. So the option is either to live with the warnings or to add SimpleXML (version 1.7.2) to get rid of the warnings.\n\n\n\n3. Build: copying of protowords.txt etc\n\nThe patch includes lexical files both in the contrib/clustering/src/java/test/resources/.... and in the examples dir. I'm not sure how this is handled though \u2013 do you keep copies in the repository or copy those somehow in the build?\n\n\n\n4. Highlighting\n\nThis is the bit I've not yet fully analyzed. In general, Carrot2 should fairly well handle full documents (up to say a few hundred kB each), it's just the number of documents that must be in the order of hundreds. Therefore, highlighting is not mandatory, but it may sometimes improve the quality of clusters.\n\nI was wondering, if highlighting is performed earlier in the Solr pipeline, could this be reused during clustering? One possible approach could be that clustering uses whatever is fed from the pipeline: if highlighting is enabled, clustering will be performed on the highlighted content, if there was no highlighting, we'd cluster full documents. Not sure if that's reasonable / possible to implement though.\n\n\n\n5. Documentation (wiki) updates\n\nOnce we stabilise the ideas, I'm happy to update the wiki with regard to the algorithms used (Lingo/STC) and passing additional parameters. "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12683008",
            "date": "2009-03-18T12:51:10+0000",
            "content": "Libs with Carrot2 v3.0.1 we've just released. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12683854",
            "date": "2009-03-20T11:56:10+0000",
            "content": "Marking for 1.4 release "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12684014",
            "date": "2009-03-20T20:09:41+0000",
            "content": "Further code clean-ups, support for passing intialization-time attributes to Carrot2 algorithms, some comments in the example configuration file. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12688132",
            "date": "2009-03-22T13:56:32+0000",
            "content": "Highlighting:\n\nHmm, that's an interesting thought.  We could check to see if highlighting is done first.\n\nAlso, you say C2 can handle full docs, is it feasible, then to implement it for the \"offline\" mode I have in mind, whereby you cluster the whole collection offline and then store the clusters for retrieval?  I haven't implemented this yet, but was thinking some people will be interested in full corpus clustering.  The nice thing, then, is that as new documents come in, they can be added to existing clusters (and maybe periodically, we re-cluster).  Just thinking outloud.\n\nRest of the stuff in that comment sounds good.  I will try out the patch. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12688141",
            "date": "2009-03-22T16:03:10+0000",
            "content": "Should the MockClusteringAlgorithm be under the test source tree and not the main one?  I moved it in the patch to follow\n\nI don't think we need to output the number of clusters, since that will be obvious from the list size.  I dropped it in the patch to follow\n\nAlso, on the response structure, we certainly could make it optional, although it means having to go do a lookup in the real doc list, which could be less than fun.\n\nPatch to follow "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12688171",
            "date": "2009-03-22T21:09:16+0000",
            "content": "Also, you say C2 can handle full docs, is it feasible, then to implement it for the \"offline\" mode I have in mind, whereby you cluster the whole collection offline and then store the clusters for retrieval? I haven't implemented this yet, but was thinking some people will be interested in full corpus clustering. The nice thing, then, is that as new documents come in, they can be added to existing clusters (and maybe periodically, we re-cluster). Just thinking outloud.\n\nWe have two variables here: the length of docs and the number of docs. Carrot2 is suitable for small numbers of docs (up to say 1000). If the docs are short (a paragraph or so), the clustering should be pretty fast, suitable for on-line processing (see: http://project.carrot2.org/algorithms.html). If the documents get longer, Carrot2 will still handle them, but will require some more time for processing, I'll try to do some measurements. But C2 is not useful for the \"whole collection\" case \u2013 it performs all processing in-memory and here we'd need a totally different class of algorithm, something along the lines of Mahout developments.\n\nHmm, that's an interesting thought. We could check to see if highlighting is done first.\n\nTo quickly summarise the pros and cons of relying on highlighting being done outside of the clustering component:\n\nPros:\n\n\n\twe avoid duplication of processing (highlighting being done twice)\n\tsimpler code of the clustering component, less configuration\n\n\n\nCons:\n\n\n\tif someone doesn't want highlighting in the search results, the clustering is likely to take more time (because it operates on full documents, and it's controlled globally)\n\tdepending on the highlighter, we may get some markup in the summaries, which may affect clustering (I'd need to check how Carrot2 handles that)\n\n\n\nShould the MockClusteringAlgorithm be under the test source tree and not the main one? I moved it in the patch to follow \n\nAbsolutely, it should be in the test source.\n\nI don't think we need to output the number of clusters, since that will be obvious from the list size. I dropped it in the patch to follow\n\nMakes sense, I kept it because the original version had it.\n\nAlso, on the response structure, we certainly could make it optional, although it means having to go do a lookup in the real doc list, which could be less than fun.\n\nBy \"lookup\" you mean the lookup in the XML response? Here again we have a trade off between the length of the response and ease of processing: if we repeat document titles / snippets in the clusters structure, we at least double the response size (at least because the same document may belong to many clusters), but can potentially save some lookups. But if we want to get some other fields of a document (other than we repeat in the clusters list), we'd still need a lookup. \n\nTo sum up, my intuition would be to avoid duplication and stick with document ids in cluster list (this is what we do in Carrot2 XMLs as well). Optionally, the clustering component could have a list of configurable fields to be repeated in the cluster list if that's really helpful in real-word use cases. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12695401",
            "date": "2009-04-03T14:28:40+0000",
            "content": "Hi Stanislaw,\n\nI'm going to commit soon and I was wondering if Carrot2 has a handy place where they keep all the licenses and notices so that I can fill out Solr's NOTICE.txt and LICENSE.txt.  If not, I will go collate them. "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12695463",
            "date": "2009-04-03T15:52:26+0000",
            "content": "Hi Grant,\n\nIf you download http://download.carrot2.org/stable/carrot2-java-api-3.0.1.zip, you'll find licenses in the lib/ folder of the distribution. That distribution contains slightly more JARs than needed for Solr (which uses carrot2-mini.jar), so you'd need to pick only those that are relevant.\n\nS. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12699908",
            "date": "2009-04-16T22:22:53+0000",
            "content": "Looks like we need to make the NNI JAR be a download, too, right?  It appears to be LGPL.  Where does that library come from, anyway?  I don't see it on Carrot trunk, but it is in the zip.  And a search for it doesn't reveal much.\n\n-Grant "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12700028",
            "date": "2009-04-17T06:36:22+0000",
            "content": "NNI JAR is indeed LGPL, it comes from MTJ: http://ressim.berlios.de/. It's also included in Carrot2 trunk, not in the main lib/ dir, but in /core/carrot2-util-matrix/lib.\n\nAt the time we integrated it with Carrot2 (a few years ago), it used to be distributed as a separate dependency for MTJ, wow it's included in MTJ JAR. As MTJ is quite big and we need literally two classes that are in nni.jar, I'd prefer to make the NNI JAR as it is a part of download, with a reference to the MTJ project. Would that make sense?\n\nS. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12700634",
            "date": "2009-04-20T01:56:59+0000",
            "content": "OK, I think this is ready to go, except I still need to double check how it works with release.   Since we can't distribute LGPL, this is going to have to be a source only release artifact and thus can never be in the WAR, unfortunately.\n\nThe tarball contains the JAR files that one needs, with the exception of the LGPL deps which are downloaded from the approp. places. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12709153",
            "date": "2009-05-13T21:57:15+0000",
            "content": "OK, I think all the ducks are in a row.  \n\nI intend to commit on Friday. "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12710087",
            "date": "2009-05-16T09:09:22+0000",
            "content": "Thanks Grant! Looking forward to seeing the code in the repo!\n\nS. "
        },
        {
            "author": "Allahbaksh Mohammedali",
            "id": "comment-12710093",
            "date": "2009-05-16T09:51:31+0000",
            "content": "Hi Grant,\nI am looking forward keenly to see this feature. I want to see it in action as soon as possible. When the Code will be comitted to repo? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12711137",
            "date": "2009-05-20T13:32:16+0000",
            "content": "Committed revision 776692.\n\nThanks to everyone who helped out, especially Carrot2 creators Dawid and Stanislaw. "
        },
        {
            "author": "Brad Giaccio",
            "id": "comment-12711883",
            "date": "2009-05-22T00:35:39+0000",
            "content": "This is a patch to add shard support to the ClusteringComponent.  \n\nMuch like the recently posted spell check shard patch it simply implements finishStage and stitches the response together.\n\nA second option would have been to move the body of the process method to finishStage.  This would have the benefit of only needing to do the clustering on the final set of responses. After the QueryComponent does its job of creating the final result set.   This would also not make finishStage be so dependent on what is happening in the engines when they create their cluster response.\n\nI'm still trying to wrap my head around TestDistributedSearch so see how I can provide test methods.\n\nIf option 2 that I laid out is preferred I should be able to provide a patch for that as well. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12712165",
            "date": "2009-05-22T17:03:35+0000",
            "content": "A second option would have been to move the body of the process method to finishStage. This would have the benefit of only needing to do the clustering on the final set of responses. After the QueryComponent does its job of creating the final result set. This would also not make finishStage be so dependent on what is happening in the engines when they create their cluster response\n\nI would say that this is actually the correct way to do this, as opposed to just stitching the results together.  For example, it may very well make sense that results from shard 1 belong in cluster A when clustered on the main node, whereas they belong to cluster B when only clustered on the shard.  \n\nIf you can make that change and then add some tests, I can commit.\n\nI'm still trying to wrap my head around TestDistributedSearch so see how I can provide test methods.\n\nPlease add any insight you have to http://wiki.apache.org/solr/WritingDistributedSearchComponents. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12712420",
            "date": "2009-05-23T13:29:44+0000",
            "content": "(snip off from http://www.nabble.com/questions-about-Clustering-tt23681134.html)\n\nI'd like to use this cool stuff on an environment other than English, e.g. Japanese.\n\nI've implemented Carrot2JapaneseAnalyzer (w/ Payload/ITokenType) for this purpose. It worked well with ClusteringDocumentList example, but didn't work with CarrotClusteringEngine.\n\nWhat I did is that I inserted the following lines('+') to CarrotClusteringEngine:\n\n\nattributes.put(AttributeNames.QUERY, query.toString());\n+ attributes.put(AttributeUtils.getKey(Tokenizer.class, \"analyzer\"),\n+ Carrot2JapaneseAnalyzer.class);\n\n\n\nThere is no runtime errors, but Carrot2 didn't use my analyzer, it just ignored and used ExtendedWhitespaceAnalyzer (confirmed via debugger).\n\nIs it classloader problem? I placed my jar in ${solr.solr.home}/lib . "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12712421",
            "date": "2009-05-23T13:35:07+0000",
            "content": "Pasting the comment I made on the list:\n\nThe catch with analyzer is that this specific attribute is an initialization-time attribute, so you need to add it to the initAttributes map in the init() method of CarrotClusteringEngine.\n\nPlease let me know if this solves the problem. If not, I'll investigate further. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12712442",
            "date": "2009-05-23T16:28:34+0000",
            "content": "The catch with analyzer is that this specific attribute is an initialization-time attribute, so you need to add it to the initAttributes map in the init() method of CarrotClusteringEngine.\n\nThis solves the problem. Thank you! "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12712482",
            "date": "2009-05-24T02:10:09+0000",
            "content": "patch for \"carrot.analyzerClass\" feature. "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12712534",
            "date": "2009-05-24T09:51:52+0000",
            "content": "In fact, you can set Carrot2 attributes (both init- and request-time) in the solr config file, this should work also without the patch. Just add:\n\n<str name=\"Tokenizer.analyzer\">fully.qualified.class.Name</str>\n\nto the search component element. See http://wiki.apache.org/solr/ClusteringComponent for some example. You'll find list of Carrot2 attributes, their ids and description at: http://download.carrot2.org/stable/manual/#chapter.components. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12712544",
            "date": "2009-05-24T11:31:04+0000",
            "content": "\nIn fact, you can set Carrot2 attributes (both init- and request-time) in the solr config file, this should work also without the patch. Just add:\n\n<str name=\"Tokenizer.analyzer\">fully.qualified.class.Name</str>\n\nHmm, I thought I need to assign Class<?> type (other than String) for the second argument of the attribute. I'll try it. "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12712545",
            "date": "2009-05-24T11:45:32+0000",
            "content": "Ah, I should have mentioned that up front \u2013 Carrot2 will try to convert the string into the type accepted by the attribute. In case of the class-types attributes, it will try to load the class using the current thread's context classloader. Conversions are also available for numeric, boolean and enum attributes (see: http://download.carrot2.org/head/javadoc/org/carrot2/util/attribute/AttributeBinder.AttributeTransformerFromString.html). Please let me know if that way works for you. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12712557",
            "date": "2009-05-24T14:33:08+0000",
            "content": "\n<str name=\"Tokenizer.analyzer\">fully.qualified.class.Name</str>\n\n\n\nThis works as expected w/o my patch. Thank you, Stanislaw! "
        },
        {
            "author": "Brad Giaccio",
            "id": "comment-12715779",
            "date": "2009-06-03T02:50:53+0000",
            "content": "Okay I've rewritten the patch, as I suggested.  Now the clustering happens in finishStage for distributed queries and it happens in process for non-distributed  both by calling the new method clusterResults .   To make this happen I had to convert the interfaces and supporting code to use SolrDocumentList rather than DocList.\n\nI've added a unit test which extends TestDistributedSearch,  I had to modify TestDistributedSearch and make a bunch of things protected.   This allowed me to write a very small test case (just had to override doTest)  and leave all the logic for creating shards, distributing docs, and comparing responses in TestDistributedSearch.  I felt this made for a very clean way to test a single distributed component. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12724847",
            "date": "2009-06-27T12:20:01+0000",
            "content": "The response structure is a bit funny (it's like normal XML, which we don't really use in Solr-land), and certainly not optimal for JSON responses:\n\n\n \"clusters\":[\n  \"cluster\",[\n\t\"labels\",[\n\t \"label\",\"DDR\"],\n\t\"docs\",[\n\t \"doc\",\"TWINX2048-3200PRO\",\n\t \"doc\",\"VS1GB400C3\",\n\t \"doc\",\"VDBDB1A16\"]],\n  \"cluster\",[\n\t\"labels\",[\n\t \"label\",\"Car Power Adapter\"],\n\t\"docs\",[\n\t \"doc\",\"F8V7067-APL-KIT\",\n\t \"doc\",\"IW-02\"]],\n[...]\n\n\n\nIs \"labels\"  is needed because there could be multiple labels per cluster in the future?  ( I assume yes)\nDo we need more per-doc information than just the id?  (I assume no)\nCould we want other per-cluster information in the future (I assume yes)\nWhat other possible information could be added in the future?\n\nGiven the assumptions above, \"clusters\", \"docs\", and \"labels\" should all be arrays instead of NamedLists (the names are just repeated redundant info).\nAll of the remaining NamedLists(just each \"cluster\") should be a SimpleOrderedMap since access by key is more important than order... that will give us something along the lines of:\n\n\n\"clusters\" : [\n    { \"labels\" : [\"DDR\"],\n\t\"docs\":[\"TWINX2048-3200PRO\",\"VS1GB400C3\",\"VDBDB1A16\"]\n    }\n    ,\n    { \"labels\" : [\"Car Power Adapter\"],\n\t\"docs\":[\"F8V7067-APL-KIT\",\"IW-02\"]\n    }\n]\n\n\n\nMake sense? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12724854",
            "date": "2009-06-27T13:45:09+0000",
            "content": "I hit an error trying to cluster some documents I added with solr cell - 400 unknown field \"Author\".\nSeems like it would be nice if we could handle unknown field types gracefully? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12725573",
            "date": "2009-06-30T10:46:35+0000",
            "content": "Is \"labels\" is needed because there could be multiple labels per cluster in the future? ( I assume yes)\n\nNot sure, but likely so\n\nDo we need more per-doc information than just the id? (I assume no)\n\nI think for other algorithms like k-Means, Canopy and others (Mahout) you could reasonable expect to return:\n1.  The centroid that the given document belongs to -  This can be captured as the label, but it is often represented as a vector and could thus be quite long.  For instance, in Mahout, we could return this as a JSON string (we're using GSON over there)\n2.  The distance from the centroid used in clustering.\n\n\nCould we want other per-cluster information in the future (I assume yes)\n\nSee #1 in the previous.\n\n\nWhat other possible information could be added in the future?\n\nHard to say, but the nature of this implementation is such that people will can plug in their own clustering algorithms which may have different outputs.  Until we have at least one other implementation, it will be difficult to \"harden\" the interfaces.   For now, though, you're proposed alterations to the format are fine with me.\n\nSeems like it would be nice if we could handle unknown field types gracefully?\n\nYes, that would be good.\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12725694",
            "date": "2009-06-30T16:52:42+0000",
            "content": "Now that I'm looking at some of the code, is there a reason why clustering doesn't use a SolrQueryRequest, but instead grabs a searcher directly from the core? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12725702",
            "date": "2009-06-30T17:05:50+0000",
            "content": "Now that I'm looking at some of the code, is there a reason why clustering doesn't use a SolrQueryRequest, but instead grabs a searcher directly from the core?\n\nBecause the clustering engine gets initialized during core initialization and thus doesn't have a SolrQueryRequest at that time.  Is there harm in the way it's being done?  I suppose it adds an extra reference, right, meaning it could keep a core open longer?  \n\nIn the case of document clustering, I think it could be a long running job.  It's not clear yet how that should work, but it is something to keep in mind.  I expect to implement that sometime this summer, likely after 1.4. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12725703",
            "date": "2009-06-30T17:07:16+0000",
            "content": "Also, some  implementations may need lower level interfaces than Searcher, it just seems easier to have core access. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12725730",
            "date": "2009-06-30T17:59:50+0000",
            "content": "I'm talking about the search results clustering, which is per-request.  RequestHandlers should pretty much always use the core/searcher associated with the SolrQueryRequest.  newSearcher/firstSearcher hooks set this themselves, hence it's a different searcher than one would get from getSearcher() (and could possibly even cause a deadlock).   Architecturally, there could be any number of reasons to use a different searcher in the future... the SolrQueryRequest says which searcher to use. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12725731",
            "date": "2009-06-30T18:04:41+0000",
            "content": "Makes sense, might need to refactor some of the initialization code and the abstract clustering engine, but no big deal. "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12725739",
            "date": "2009-06-30T18:38:06+0000",
            "content": "Is \"labels\" is needed because there could be multiple labels per cluster in the future? ( I assume yes)\n\nCorrect. Currently neither of Carrot2's algorithms creates clusters with multiple labels, but it's quite likely that there are other algorithms that can do that. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12726149",
            "date": "2009-07-01T17:47:14+0000",
            "content": "The attached patch implements the simpler JSON friendly format.\n\nexample:\n\n[...] \n\"clusters\":[\n  { \"labels\":[\"DDR\"],\n    \"docs\":[\"TWINX2048-3200PRO\",\"VS1GB400C3\",\"VDBDB1A16\"]\n  },\n  { \"labels\":[\"Car Power Adapter\"],\n    \"docs\":[\"F8V7067-APL-KIT\",\"IW-02\"]\n  },\n  { \"labels\":[\"Display\"],\n    \"docs\":[\"MA147LL/A\",\"VA902B\"]\n  }\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12726215",
            "date": "2009-07-01T20:27:21+0000",
            "content": "This fixes the SolrQueryRequest issue and also stopped the swallowing of an exception that I just happened to see.\n\nI'll commit shortly. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12726482",
            "date": "2009-07-02T13:52:06+0000",
            "content": "Anyone mind if I reformat the source files that currently use tabs? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12726485",
            "date": "2009-07-02T13:56:11+0000",
            "content": "Anyone mind if I reformat the source files that currently use tabs? \n\n+1 "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12727247",
            "date": "2009-07-04T15:41:35+0000",
            "content": "Of course, now that I've removed the clustering libs from the solr.war, the example no longer works for some reason... looks like all the jars are in example/clustering/solr/lib, so it's classloading issues I imagine.\n\nOn a related note, I'm not sure how useful it is to have a clustering component with multiple plugins itself... the extra level of plugins seems to just add more complexity.  Different plugins could always share utility classes, perhaps even base classes, and could strive for a common output format - all without going to an additional plugin model. "
        },
        {
            "author": "Brad Giaccio",
            "id": "comment-12728270",
            "date": "2009-07-07T18:13:56+0000",
            "content": "If you could , could my patch to handle shards be applied before you reformat so I don't have to piece it together again and resubmit? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12728271",
            "date": "2009-07-07T18:15:40+0000",
            "content": "Apologies Brad - I didn't realize there were pending patches or I would have not done the reformat. "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12728861",
            "date": "2009-07-08T19:23:43+0000",
            "content": "Hi,\n\nWhile configuring the clustering component for an algorithm that returns hierarchical clusters, it took me a while to debug why subclusters wouldn't appear on the output. It turned out that the default value for the carrot.outputSubClusters parameter is false, which was the opposite to what I assumed  Would it be a problem to change the default to true, so that other users avoid the same problem? \n\nAnother improvement worth making for the carrot.outputSubClusters = false case is \"flattening\" the clusters: returning all documents of the 1st level clusters, including those contained in the subclusters the user chose not to output. Without this improvement, many document-cluster assignments may be lost because some Carrot2 algorithms will assign documents only to the \"leaf\" (deepest in the hierarchy) clusters.\n\nI'm attaching a patch that implements both changes. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12732277",
            "date": "2009-07-17T00:52:46+0000",
            "content": "un-assigning myself since I'm not sure when I'll be able to get back to this.\nIssues remaining:\n\n\tclassloading issues after the hander was removed from solr.war\n\tpossible packaging issues that Grant brought up (the downloaded jars shouldn't be shipped)\n\tupdate the Wiki once classloading works and we can generate the new example output\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12735635",
            "date": "2009-07-27T15:59:57+0000",
            "content": "classloading issues after the hander was removed from solr.war\n\nI think the issue is that changes you made don't include the actual include the clustering code in Solr when running the example.   I think we just need to copy over the clustering JAR from the build directory into the lib, but that is a bit weird, IMO.  \n\nTo fix, I'm going to make the example target create a proper Solr home under contrib/clustering/example.  Which, of course, isn't much different from how it used to be.  I am also going to restore the downloads directory for packaging/release functionality. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12735638",
            "date": "2009-07-27T16:15:29+0000",
            "content": "Note, I believe there is also a classloading issue when trying to load the carrot algorithm, b/c it does not use the SolrResourceLoader "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12735643",
            "date": "2009-07-27T16:29:28+0000",
            "content": "OK, I have committed my changes and believe functionality is restored and is properly working with the SolrResourceLoader.  Also applied Stanislaw's patch.\n\nStill likely need to review how to distribute all of this.  My guess is that we should only include the source, including the build and instructions for installing, and not even package jars at all since we can't include the LGPL ones necessary for Carrot2. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12735739",
            "date": "2009-07-27T19:46:16+0000",
            "content": "This should be back to working and the example is not contained in the contrib/clustering, plus I re-instated the downloads directory. "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12736030",
            "date": "2009-07-28T10:57:05+0000",
            "content": "Hi Grant,\n\nThere's one more thing: we're planning to release version 3.1.0 of Carrot2 with certain bug fixes in clustering algorithm and better support for Chinese (using the new analyzer from Lucene). Our plan is to release after Lucene 2.9 is out, but before Solr 1.4, so that the latter would have a newer version of Carrot2 on board (should be just a matter of replacing Carrot2 JAR / upgrading version of the downloaded dependency). Would that make sense? Should I create a separate issue for it, or rather reopen this one?\n\nThanks,\n\nS. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12736037",
            "date": "2009-07-28T11:30:44+0000",
            "content": "Would that make sense? Should I create a separate issue for it, or rather reopen this one?\n\nYes, I think that makes sense.  Separate issue would be good, this one is long enough. "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12736039",
            "date": "2009-07-28T11:46:09+0000",
            "content": "Created: SOLR-1314. I'll attach a patch there as soon as Lucene 2.9 is released. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12970742",
            "date": "2010-12-13T08:29:19+0000",
            "content": "add component info "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12970758",
            "date": "2010-12-13T09:27:23+0000",
            "content": "Apologies Grant for quote your comment on 27/Jul/09:\n\nAlso applied Stanislaw's patch.\n\nI'm confused by this line:\n\n\nList<Document> docs = outputSubClusters ? outCluster.getDocuments() : outCluster.getAllDocuments();\n\n\n\nAccording to Carrot2 Javadoc:\n\nhttp://download.carrot2.org/stable/javadoc/org/carrot2/core/Cluster.html#getAllDocuments%28%29\n\nShould it be:\n\n\nList<Document> docs = outputSubClusters ? outCluster.getAllDocuments() : outCluster.getDocuments();\n\n\n\n? "
        },
        {
            "author": "Stanislaw Osinski",
            "id": "comment-12970760",
            "date": "2010-12-13T09:38:31+0000",
            "content": "Hi Koji,\n\nActually, the current code seems right: if we don't output subclusters, we need to include all documents of the cluster, including those from its subclusters, otherwise the subclusters' documents may not appear in the response at all. But if we do output subclusters, we add only the documents assigned specifically to the cluster because the subclusters with their documents will be included in the response too.\n\nS. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12970762",
            "date": "2010-12-13T09:57:31+0000",
            "content": "Uh, I needed to read the part of the recursive call. Thanks for explanation! "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-14629791",
            "date": "2015-07-16T14:12:14+0000",
            "content": "This was reopened by a spam bot I think (a long time ago). "
        }
    ]
}