{
    "id": "SOLR-2548",
    "title": "Multithreaded faceting",
    "details": {
        "affect_versions": "3.1",
        "status": "Closed",
        "fix_versions": [
            "4.5",
            "6.0"
        ],
        "components": [
            "search"
        ],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Add multithreading support for faceting.",
    "attachments": {
        "SOLR-2548_4.2.1.patch": "https://issues.apache.org/jira/secure/attachment/12596877/SOLR-2548_4.2.1.patch",
        "SOLR-2548.patch": "https://issues.apache.org/jira/secure/attachment/12480574/SOLR-2548.patch",
        "SOLR-2548_for_31x.patch": "https://issues.apache.org/jira/secure/attachment/12480575/SOLR-2548_for_31x.patch",
        "SOLR-2548_multithreaded_faceting,_dsmiley.patch": "https://issues.apache.org/jira/secure/attachment/12602210/SOLR-2548_multithreaded_faceting%2C_dsmiley.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Janne Majaranta",
            "id": "comment-13039878",
            "date": "2011-05-26T19:35:18+0000",
            "content": "Patch for TRUNK "
        },
        {
            "author": "Janne Majaranta",
            "id": "comment-13039879",
            "date": "2011-05-26T19:36:47+0000",
            "content": "Patch for 3.1 "
        },
        {
            "author": "Janne Majaranta",
            "id": "comment-13039883",
            "date": "2011-05-26T19:42:40+0000",
            "content": "The attached patch adds initial multithreading support for faceting. The patch simply wraps the facet counting methods per field into java.util.concurrent.Callable instances and executes these with a ExecutorService having a threadpool-size equal to the number of processors reported by the runtime.\n\nSeems like this adds quite nice speed boosts when faceting over multiple fields. In initial tests (with a patched SOLR 3.1 instance) the faceting speed was about 2x-8x faster with both faceting methods, enum and fc.\n "
        },
        {
            "author": "Adrien Grand",
            "id": "comment-13040124",
            "date": "2011-05-27T08:23:12+0000",
            "content": "Hello Janne,\n\nIs there any reason why you didn't make facet queries and facet ranges multi-threaded ?\n\nYou create a thread pool with ${nProcessors} worker threads locally, meaning that a server with 8 processors serving 10 concurrent requests would have to start and then run 80 threads simultaneously before stopping each of them. At this level, multi-threaded facet computation may be slower than single-threaded facet computation. I suggest instead to share a single thread pool for every facet computation with coreSize=0, a SynchronousQueue, and ThreadPoolExecutor.CallerRunsPolicy as a RejectExecutionHandler. This way you would avoid the overhead of starting and then stopping threads and have a better control over the total number of threads computing facets. "
        },
        {
            "author": "Janne Majaranta",
            "id": "comment-13040142",
            "date": "2011-05-27T09:00:11+0000",
            "content": "Hi Adrien,\n\nIt's a initial patch. I don't use facet queries / ranges, so I started with field counts.\nWhat I'm planning to do is to run some stress tests against a 500M documents index distributed over 54 cores (once my index-building process is finished).\nI will definitely take a look at your suggestion once I'm able to run the stress tests against the 500M docs index.\n\nIf the multi-threaded faceting version performs better under heavy load, I'll look into the possibility of making the facet queries + ranges multi-threaded. \n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13049341",
            "date": "2011-06-14T19:04:21+0000",
            "content": "Janne: thanks for the awesome patch!\n\nin general i think this type of functionality is a good idea \u2013 the real question is how it should be configured/controlled.\n\nadmins with many CPUs who expect low amounts of concurrent user traffic might be ok with spawning availableProcessors() threads per request, but admins with less CPUs then concurrent requests are going to prefer that individual requests stay single threaded and take a little longer.\n\nThe suggestion to use a thread pool based executorservice definitely seems like it makes more sense, then it just becomes a matter of asking the admin to configure a simple number determining the size of the threadpool, we just need to support a few sentinal values: NUM_CPUS, and NONE (always use callers thread).\n\nsince we'd want a threadpool that lives longer then a single request, this definitely shouldn't be an option specified via SolrParams (not to mention the risk involved if people don't lock it down with invariants).  That leaves the question of wether this should be an \"init\" param on the FacetComponent, or something more global.\n\nMy first thought was that we should bite the bullet and add a new top level config for a global thread pool executor service that any solr plugin could start using, but after thinking about it some more i think that would not only be premature, but perhaps even a bad idea in general \u2013 even if we assume something like DIH, UIMA, or Highlighting could also take advantage of a shared thread pool owned by solr, if you really care about parallelizing faceting, you probably wouldn't want some other intensive component starving out the thread pool (or vice versa) "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13049355",
            "date": "2011-06-14T19:26:46+0000",
            "content": "I think this should be configurable on a per-request basis (not the max size of the threadpool, but how many threads out of that to use concurrently).\nFor facet.method=fcs (per-segment faceting using the field cache), I did introduce a \"threads\" localParam.\nPerhaps we should have a \"threads\" or \"facet.threads\" request parameter? "
        },
        {
            "author": "Adrien Grand",
            "id": "comment-13049453",
            "date": "2011-06-14T21:48:10+0000",
            "content": "Hoss Man,\n\nRegarding the best value of the number of threads to spawn based on the number of CPUs and the traffic, one could imagine to decide whether to spawn a new thread or to run the task in the current thread based on the load of the server. This way, servers under high traffic would run every request in a single thread (maximizing throughput) whereas servers under low traffic would be able to use every processor in order to minimize response time. The load is easily retrievable in Java 6 using OperatingSystemMXBean, I don't know if it is possible in a non OS-specific way in Java 5.\n\nI don't really understand what you mean by \"if you really care about parallelizing faceting, you probably wouldn't want some other intensive component starving out the thread pool\". Do you mean that you would expect some requests to be run slower with every component using a global thread pool than with a single thread pool dedicated to facets?\n\nYonik, why would you want to limit the number of threads on a per-request basis, if enough CPUs are available? "
        },
        {
            "author": "Scott Lundgren",
            "id": "comment-13730365",
            "date": "2013-08-06T05:19:28+0000",
            "content": "my read on this patch is that there is no contention (not to say endorsement, but no contention) that faceting over multiple fields gets a speed boost.\n\nThe outstanding questions seem to center on the configuration and how to expose and control the functionality.\n\nGiven that two years has elapsed from the OP's original post - are some of the outstanding configuration/control questions now more easily resolved? In short, has SOLR moved forward such that answering these questions is now easier than it was the time of 3.1? "
        },
        {
            "author": "Gun Akkor",
            "id": "comment-13733690",
            "date": "2013-08-08T17:03:12+0000",
            "content": "I would like to revive this ticket, if possible. We have an index with about 10 fields that we regularly facet on. These fields are either multi-valued or are of type TextField, so facet code chooses FC as the facet method, and uses the UnInvertedField instances to count each facet field, which takes several seconds per field in our case. So, multi-thread execution of getTermCounts() reduces the overall facet time considerably.\n\nI started with the patch that was posted against 3.1 and modified it a little bit to take into account previous comments made by Yonik and Adrien. The new patch applies against 4.2.1, uses the already existing facetExecutor thread pool, and is configured per request via a facet.threads request param. If the param is not supplied, the code defaults to directExecutor and runs sequential as before. So, code should behave as is if user chooses not to submit number of threads to use.\n\nAlso in the process of testing, I noticed that UnInvertedField.getUnInvertedField() call was synchronized too early, before the call to new UnInvertedField(field, searcher) if the field is not in the field value cache. Because its init can take several seconds, synchronizing on the cache in that duration was effectively serializing the execution of the multiple threads.\nSo, I modified it (albeit inelegantly) to synchronize later (in our case cache hit ratio is low, so this makes a difference).\n\nThe patch is still incomplete, as it does not extend this framework to possibly other calls like ranges and dates, but it is a start. "
        },
        {
            "author": "Gun Akkor",
            "id": "comment-13733691",
            "date": "2013-08-08T17:03:54+0000",
            "content": "Patch against 4.2.1 "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13756089",
            "date": "2013-09-02T13:58:42+0000",
            "content": "See Gun's comments about UnInvertedField serializing the facet counts due to the placement of the new viz. the synch block. It's at the very end of the class....\n\nDo people think that the chance of uninverting the same field more than once and throwing away 2...N is frequent enough to guard against with a Future (or whatever?) It seems like this is an expensive enough operation that the complexity is reasonable. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13757323",
            "date": "2013-09-04T00:42:17+0000",
            "content": "This bit in SimpleFacets.getFacetFieldCounts bothers me:\n\n    int maxThreads = Integer.parseInt(req.getParams().get(FacetParams.FACET_THREADS, \"0\"));\n    Executor executor = maxThreads == 0 ? directExecutor : facetExecutor;\n    maxThreads = maxThreads <= 0? Integer.MAX_VALUE : maxThreads;\n\n\nIt seems like if the user doesn't specify anything for FACET_THREADS, they wind up spawning as many threads as there are facet fields specified. Probably not a real problem given this list will be fairly small, but it seems more true to the old default behavior if it's changed to something like\n\n    int maxThreads = Integer.parseInt(req.getParams().get(FacetParams.FACET_THREADS, \"1\"));\n    Executor executor = maxThreads == 1 ? directExecutor : facetExecutor;\n    maxThreads = maxThreads < 1 ? Integer.MAX_VALUE : maxThreads;\n\n\nOr am I seeing things that aren't there? "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13757367",
            "date": "2013-09-04T01:42:43+0000",
            "content": "Latest version that does two things:\n\n1> does the max thread change I commented on earlier.\n\n2> puts in some checking to insure that if multiple threads try to uninvert the same field at the same time, it'll only be loaded once. I used a simple wait/sleep loop here since this method is called from several places and it looks like a real pain to try to do a Future or whatever. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13757466",
            "date": "2013-09-04T04:59:04+0000",
            "content": "This bit in SimpleFacets.getFacetFieldCounts bothers me:\n...\nIt seems like if the user doesn't specify anything for FACET_THREADS, they wind up spawning as many threads as there are facet fields specified\n\nI haven't reviewed the patch, but based on the snippet you posted i suspect you are reading that bit correctly.\n\nIf FACET_THREADS isn't specified, or if it's specified and equals the default value of 0, then the directExecutor is used and no threads should be spawned at all \u2013 the value of maxThreads shouldn't matter at that point, instead the existing request thread should processes all of them sequentially.\n\nI'm guessing you should change the patch back.\n\nSide comments...\n\n1) Something sketchy probably does happen if a user passes in a negative value \u2013 it looks like that's the case when facetExecutor will be used with an unlimited number of threads ... that may actually have been intentional \u2013 that if you say facet.threads=-1 every facet.field should get it's own thread, no matter how many there are, but if that is intentional i'd love to see a comment there making that obvious. (and a test showing that it works).\n\n2) can you please fix that Integer.parseInt(...\"0\")) to just use params.getInt(...,0) ... that way the correct error message will be returned if it's not an int (and it's easier to read)\n "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13757681",
            "date": "2013-09-04T11:41:26+0000",
            "content": "Chris Hostetter (Unused) Thanks. Your comments made me look more carefully at directExecutor, it took me a bit to wrap my head around that one.\n\n1> Still checking on the implications of stacking up a bunch of directExecutors all through the CompletionService, not something I've used recently and the details are hazy.\n\nAs far as tests are concerned, I haven't gotten there yet, the original patch didn't have any... It should be easy to create tests with multiple field.facet clauses, TestFaceting does this so there are templates. Is there a decent way to check whether more than one thread was actually spawned? If so, can you point me at some code that actually does that? Otherwise I'll create tests that just get the right response for single and multiple facet.field specifications and a bit of walk-through with the debugger to insure we actually go through that code path.\n\n2> done.\n\nThanks again. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13757931",
            "date": "2013-09-04T16:46:42+0000",
            "content": "Still checking on the implications of stacking up a bunch of directExecutors all through the CompletionService, not something I've used recently and the details are hazy.\n\nunless i'm missing something, it should be a single directExecutor, and when a job is submitted to the CompletionService, nothing happens in the background at all \u2013 the thread that submitted the job then immediately executes the job.  Telling the COpmletionService to use the directExecutor is essentially a way of saying \"when someone asks you to do execute X, make them do it themselves\"\n\nIs there a decent way to check whether more than one thread was actually spawned?\n\nI doubt it ... but it would be nice to at least know the functionality succeeds w/o failure.\n\nThere might be a way to subclass & instrument the ThreadPoolExecutor (or the Queue it uses to manage jobs) so that you could make it keep track of the max number of live threads at any one time, or the max size of the queue at any one time, and then your test could reach in and inspect either of those values to know if the wrong thing happened (ie: too many threads spun up, or too many things enqueued w/o being handed to threads) ... but i'm not sure how hard that would be.\n\nAcctually \u2013 maybe a better thing to do would be to have the Callables record the thread id of whatever thread executed them, and include that in the debug info ... then the test could just confirm that all of the ids match and don't start with \"facetExecutor-\" in the directExecutor case, and that the number of unique ids seen is not greater then N in the facet.threads=N case.  (That debug info could theoretically be useful to end users as well, to see that multiple threads really are getting used) "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13758091",
            "date": "2013-09-04T18:11:26+0000",
            "content": "Hmm, the whole recording-thread-info is a little more ambitious than I want to be right now. For the nonce, I did some \"by hand\" debugging, added in a couple of (temporary) print message in the getUnInvertedField code and insured that when it's called it only executes once per field, so I think I'll call that good now.\n\nI did play around with the directExcecutor and now I get to add another bit of knowledge, that it's really kind of cool that it allows one to have code like this. No matter how many times you submit a job, it all just executes in the current thread. Arcane, but kind of cool.\n\nAs for the rest, I've added at least functional tests and one test that the caching code is working that's non-deterministic but might trip bad conditions at least some of the time.\n\nSo unless people object I'll be committing this probably tomorrow. It passes precommit and at least all the tests in TestFaceting, I'll be running the full suite in a minute. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13758164",
            "date": "2013-09-04T18:49:50+0000",
            "content": "I used a simple wait/sleep loop here\n\nUgh - please let's not do that for multi-threaded code.\n\nAlso, I see some stuff like this in the patch:\n\n-      counts = getGroupedCounts(searcher, docs, field, multiToken, offset,limit, mincount, missing, sort, prefix);\n+      counts = getGroupedCounts(searcher, base, field, multiToken, offset,limit, mincount, missing, sort, prefix);\n\n\nWas there a bug that these changes fixed? "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13758261",
            "date": "2013-09-04T19:46:16+0000",
            "content": "bq: Was there a bug that these changes fixed?\n\nNope, I thought it was a refactoring and didn't look closely. It appears to be useless complexity, perhaps a remnant from the original patch against 3.1. I took them out.\n\nbq: please let's not do that for multi-threaded code.\n\nI can always count on you to call me on sleeping, don't know why I even try to put a sleep in any more . OK, took it out and substituted a notifyAll. And added a test that gets into this code while actually doing the inverting rather than just pulls stuff from the cache.\n\nI'll attach a new patch in a few. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13758271",
            "date": "2013-09-04T19:59:10+0000",
            "content": "It appears to be useless complexity, perhaps a remnant from the original patch against 3.1. I took them out.\n\nActually, I see now (and it's absolutely needed \nThe base docset can change from one facet request to another (think excludes), hence if we go multi-threaded, we can't reference \"SimpleFacets.docs\" in anything that could be executed from a separate thread.  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13758314",
            "date": "2013-09-04T20:48:33+0000",
            "content": "One issue with a static \"pending\" set on UnInvertedField is that it will block different cores trying to un-invert the same field.\nThis should probably be implemented the same way the FieldCache does it (insertion of a placeholder). "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13758318",
            "date": "2013-09-04T20:50:59+0000",
            "content": "OK, maybe this time.\n\n1> put back the passing in base.\n2> took out the sleep.\n3> changed how exceptions are propagated up past the new threads which fixed another test that this code broke.\n4> Added a non-deterministic test that forces parallel uninverting of the fields to make sure we exercise the synchronize/notify code. This test can't guarantee to execute that code every time, but it did manage with some printlns.\n\nRunning tests again, precommit all that. Won't check in until at least tomorrow.\n\nAnd thank heaven for \"local history\" in IntelliJ  "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13758362",
            "date": "2013-09-04T21:17:29+0000",
            "content": "Still have a test error in TestDistributedGrouping, no clue why and can't look right now. It's certainly a result of the changes in UnInvertField since if I put that in a clean trunk the same problem occurs.\n\nMy guess is that I can't synchronize on cache for some reason, but not much in the way of evidence for that right now. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13759097",
            "date": "2013-09-05T14:11:10+0000",
            "content": "[~yonik] Don't quite see what you're getting at. I understand what you're saying about pending blocking threads loading the same field name in different cores, good point.\n\nBut how to put a placeholder in the cache? It needs an UnInverted field as an entry. I could add a bogus c'tor to make an degenerate UnInverted field and use that, then check every time we get something out of the cache for a field in order to see if it's the degenerate case. One could add a member var \"imFake\" or something. Really the question is how to distinguish between the cache returning null or returning something signaling \"Come back later and get the entry another thread loaded\". I like the idea of not having the spare pending set at all, one less thing to coordinate.\n\nOr I could just make the pending bits prepend the core name to the field?\n\nActually, I'm beginning to wonder whether adding all this junk in is really better than just throwing the UnInvertedFields 2-n on the floor like the original patch did... "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13759115",
            "date": "2013-09-05T14:37:37+0000",
            "content": "See FieldCacheImpl.get() - there is a CreationPlaceHolder object used. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13759186",
            "date": "2013-09-05T16:07:32+0000",
            "content": "The latest patch isn't thread-safe in UnInvertedField - you're synchronizing on \"cache\" (of which there will be multiple) for accessing the singleton \"pending\".  "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13759193",
            "date": "2013-09-05T16:17:51+0000",
            "content": "bq: you're synchronizing on \"cache\"\n\nYeah, I realized that on the way in to an appointment. Siiigh.\n "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13759226",
            "date": "2013-09-05T17:04:19+0000",
            "content": "bq: See FieldCacheImpl.get() - there is a CreationPlaceHolder object used.\n\nRight, but the map that that's placed in is a map<key, Object>. The UnInvertedField cache is map<key, UnInvertedField>. That's what was behind my question about making a dummy UnInvertedField to use similarly to how CreationPlaceHolder is used. I'm really not up for making the underlying UnInvertedField cache take an Object, seems like the tail wagging the dog.\n\nInterestingly I think it was what Yonik Seeley pointed out about synching on different objects than I thought I was that was the problem with TestDistributedGrouping, it passes now.\n\nThe attached patch implements creating a placeholder UnInvertedField, removes the pending map and passes the failing test from yesterday as well as precommit.\n\nI'll run the full suite soon. If that passes, I'll let it stew for a bit and commit tomorrow unless there are objections. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13760159",
            "date": "2013-09-06T12:03:28+0000",
            "content": "Final patch, including CHANGES.txt entry. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13760427",
            "date": "2013-09-06T17:53:41+0000",
            "content": "Commit 1520645 from Erick Erickson in branch 'dev/trunk'\n[ https://svn.apache.org/r1520645 ]\n\nSOLR-2548, Multithread faceting "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13760528",
            "date": "2013-09-06T19:16:29+0000",
            "content": "Commit 1520670 from Erick Erickson in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1520670 ]\n\nSOLR-2548, Multithread faceting "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13760531",
            "date": "2013-09-06T19:20:18+0000",
            "content": "Thanks Janne and Gun! "
        },
        {
            "author": "David Smiley",
            "id": "comment-13762313",
            "date": "2013-09-09T21:20:09+0000",
            "content": "This issue just got on my radar; I like working on threading problems.\n\nI commend the progress made but I think it can be improved:\n\n\tI think it's counter-intuitive that if a user supplies facet.threads=2 then 3 cpu cores will actually be used (assuming >2 fields to facet on)\n\tOnly the first facet.threads worth of facets are actually done concurrently; the rest are done serially.\n\tEven if the previous problem was solved, the use of the main calling thread to compute facets (beyond facet.threads) means that if by bad luck the main thread is computing the most intensive facets to compute, the other threads will sit idle once they are done when it would be better to have remaining work queued up.\n\tin the event of an exception in one worker; the rest should be cancelled\n\tExecutionException is a wrapping exception; you should unwrap it and wrap SolrException on its contents, not the ExecutionException itself.\n\n\n\nThe attached patch fixes all these problems, keeps it no more complex and perhaps simpler (IMO), and without increasing the lines-of-code count. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13762321",
            "date": "2013-09-09T21:25:25+0000",
            "content": "Its a bad idea to call Future.cancel here.\n\nIf any of the faceting methods are blocked on IO (e.g. docvalues faceting), this will close file descriptors with NIO/MMAP directory implementations: see the documentation in org.apache.lucene.store for more information. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13762425",
            "date": "2013-09-09T22:55:08+0000",
            "content": "David Smiley\n\nSee below...\n\nI'm not seeing points 1-3. I think you might be missing the distinction between adding fields to the pending queue and actually doing the faceting:\n\n(1) I don't think so. If facet.threads == 2, the third time around the counter is -1 so the field gets added to the pending structure, it's not executed on at all until one of the other threads completes.\n\n(2) I'm not seeing it. Every time a task completes, another is started from the pending list. The main thread is just sitting around waiting for the child threads to complete. Mostly this is for my edification, I have no objection to the semaphore approach. In fact it's a little cleaner, the second \"for (String f : facetFs) {\" loop is somewhat loosely coupled.\n\n(3) Not quite sure about this either. I don't see where the main thread is used to compute any facets. Well, except in the intentionally serial case when the directExecutor is used and the old behavior is desired. Items are just added to the pending queue once you exceed facet.threads. That queue is consumed to submit other tasks to new threads via \"completionService.submit(pending.removeFirst());\" in the second loop. The main thread never computes facets. Or I'm just blind to it.\n\n(4) That makes sense, although I'll defer to Robert.\n\n(5) OK. I did have some trouble in the tests though, some of them were expecting 400 response code and the SERVER_ERROR is 500 as I remember so don't be surprised if there's an issue there when you run the full test suite if you haven't already. I made some effort to give back the same errors as the tests expected which may account for some of the weirdness you saw in the exception handling.\n\nYou'll notice I punted on Adrien's comment \"Is there any reason why you didn't make facet queries and facet ranges multi-threaded\"... feel free . "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13762537",
            "date": "2013-09-10T00:51:26+0000",
            "content": "Only the first facet.threads worth of facets are actually done concurrently; the rest are done serially.\n\nI remember that being my initial reaction too - but then when you think a little about it, you realize that it's not the case. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13762543",
            "date": "2013-09-10T00:58:46+0000",
            "content": "in the event of an exception in one worker; the rest should be cancelled\n\nIn addition to Robert's comment that points out why we never want to use cancel on anything that does IO, we shouldn't add complexity trying to optimize an error case.\n\nExecutionException is a wrapping exception; you should unwrap it and wrap SolrException on its contents, not the ExecutionException itself.\n\nWe should definitely strive to make the multi-threading as transparent as possible (i.e. exceptions should be as close as possible to the non-threaded case). "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13762630",
            "date": "2013-09-10T02:12:20+0000",
            "content": "Just as a (likely controversial) suggestion in general here, its hard to \"see\" the single-threaded case (which is the most common case).\n\nI think its a little too sneaky here and would be actually a lot easier long-term if the single-threaded case was explicitly separate from the multi-threaded one. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13762654",
            "date": "2013-09-10T02:29:40+0000",
            "content": "bq: Just as a (likely controversial) suggestion in general here, its hard to \"see\" the single-threaded case (which is the most common case).\n\nNo, not controversial at all. I had to look at that pretty hard to see that it was a single-threaded case, I tried to add a comment, mostly so I wouldn't have to try to figure it out again next time I was in that code \n\nI'm all in favor of a little more verbosity here, just didn't do it... "
        },
        {
            "author": "David Smiley",
            "id": "comment-13762750",
            "date": "2013-09-10T05:30:40+0000",
            "content": "If any of the faceting methods are blocked on IO (e.g. docvalues faceting), this will close file descriptors with NIO/MMAP directory implementations: see the documentation in org.apache.lucene.store for more information.\n\nOk; I'll look into that later.\n\n\n> Only the first facet.threads worth of facets are actually done concurrently; the rest are done serially.\n\nI remember that being my initial reaction too - but then when you think a little about it, you realize that it's not the case.\n\nAha; now I see it!  This is confusing code \u2013 adding to the completionService/executor in two different loops; and the 2nd loop is particularly un-obvious to me.\n\nJust as a (likely controversial) suggestion in general here, its hard to \"see\" the single-threaded case (which is the most common case).\n\n+0 not controversial to me "
        },
        {
            "author": "David Smiley",
            "id": "comment-13762775",
            "date": "2013-09-10T06:22:57+0000",
            "content": "BTW sorry for raising all these supposed shortfalls when the more serious ones have turned out to be invalid.  I guess it just underscores what we all know \u2013 multithreaded code is confusing.  All the more reason to try to document it better and/or to try to code it clearly. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13762949",
            "date": "2013-09-10T10:58:58+0000",
            "content": "bq:  sorry for raising all these supposed... \n\nNP. I would far, far rather have someone look at the code and raise any issues they see, even if they can be explained away than have code get into the wild and have to track it down later....\n\nSo far the structure of the code hasn't been obvious to anybody on first reading (you, Yonik and me first 2-3 times I looked at the patch). I probably won't remember it next week and would have to work laboriously through it again. It sure sounds like something that could stand some clarification/simplification.\n "
        },
        {
            "author": "David Smiley",
            "id": "comment-13767923",
            "date": "2013-09-15T21:43:38+0000",
            "content": "The attached patch improves on my previous one a little \u2013 a few more comments, a variable rename for clarity, an assertion.  And of course I removed the future.cancel() loop.\n\nI think this code is pretty clear as far as multithreaded code goes:  One loop that submits tasks, and a follow-on loop that consumes the results of those tasks, and a semaphore to ensure no more than the desired number of threads are computing the facets.\n\nIt'd be cool to eventually extend multithreading across all the faceting types. I'll look into that next week.  "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13767941",
            "date": "2013-09-15T22:49:43+0000",
            "content": "So maybe just commit this when you think it's ready? I'll probably get a chance to look it over Tuesday on the airplane, but if you're happy with it feel free. We can always put the other faceting types into a new JIRA? "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13768358",
            "date": "2013-09-16T14:34:04+0000",
            "content": "Commit 1523677 from David Smiley in branch 'dev/trunk'\n[ https://svn.apache.org/r1523677 ]\n\nSOLR-2548: Simplified multi-threading of facet.threads logic "
        },
        {
            "author": "David Smiley",
            "id": "comment-13768362",
            "date": "2013-09-16T14:35:27+0000",
            "content": "I just committed to trunk; I'll wait a day just in case and for any more feedback before applying to 4x. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13768518",
            "date": "2013-09-16T17:29:07+0000",
            "content": "David: i'm not suggesting we rush this \u2013 but if your changes aren't going to make it into 4.5, we should track them in a new issue that can have it's own record in CHANGES.txt so it's clear what versions of Solr have what version of the code. "
        },
        {
            "author": "David Smiley",
            "id": "comment-13768528",
            "date": "2013-09-16T17:37:59+0000",
            "content": "I thought about that. I figure that if I'm cautious about this such as by committing to trunk first, as I did, then there shouldn't be consternation about porting this to branch_45.  Besides, I have more confidence in understanding the code that I committed vs. what it replaced.  But I take your point that if for some reason it doesn't go to v4.5 then, sure, use another issue. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13769554",
            "date": "2013-09-17T14:30:41+0000",
            "content": "Commit 1524066 from David Smiley in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1524066 ]\n\nSOLR-2548: Simplified multi-threading of facet.threads logic "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13769575",
            "date": "2013-09-17T14:53:49+0000",
            "content": "Commit 1524080 from David Smiley in branch 'dev/branches/lucene_solr_4_5'\n[ https://svn.apache.org/r1524080 ]\n\nSOLR-2548: Simplified multi-threading of facet.threads logic "
        },
        {
            "author": "Adrien Grand",
            "id": "comment-13786995",
            "date": "2013-10-05T10:18:38+0000",
            "content": "4.5 release -> bulk close "
        },
        {
            "author": "Steven Bower",
            "id": "comment-13788556",
            "date": "2013-10-07T20:53:30+0000",
            "content": "Was just checking this out, very cool... One issue I see is that there is no way to limit the maximum number of threads specified at query time. This is configured statically in code to Integer.MAX_VALUE... this seems a bit scary to me.. especially when you don't have control over the types of queries being executed against the engine. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13788574",
            "date": "2013-10-07T21:10:29+0000",
            "content": "facet.threads is a new parameter to address exactly this concern, see:\n\nhttp://wiki.apache.org/solr/SimpleFacetParameters#facet.threads\n "
        },
        {
            "author": "Bill Bell",
            "id": "comment-13790041",
            "date": "2013-10-09T05:25:03+0000",
            "content": "When using facet.threads=1000 I am not seeing any better performance.\n\n1. Does it work on facet.query as well as facet.field?\n\n2. If I only have 1 facet.field - adding threads will it do anything?\n\n3. Will it help more on multiple facet.field?\n\n4. Does it help with facet.method=fc/fcs/ or enum? "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13790257",
            "date": "2013-10-09T11:18:11+0000",
            "content": "1. no. Could be extended to I think if you have the energy.\n2. no\n3. yes\n4 all "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13790425",
            "date": "2013-10-09T14:59:38+0000",
            "content": "I'm having a hard time measuring performance differenes without and with facet.threads. On my development machine, there are no differences on warmed indexes, both measure around 1ms. They're also almost identical after stop/start of Jetty with no warm up queries, around 40ms, after that, fast again. We're facetting on four fields this time, there are also four threads. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13790435",
            "date": "2013-10-09T15:04:37+0000",
            "content": "Alright, i took another index and facetted on much more fields and now i see a small improvement after start up of about 12%. It is not much, perhaps this machine is too fast in this case. "
        },
        {
            "author": "David Smiley",
            "id": "comment-13790544",
            "date": "2013-10-09T15:55:47+0000",
            "content": "Multithreaded faceting is useful when your CPU core count is much greater than the number of Solr cores you have, and you have a ton of data and need to facet on multiple fields.  You could theoretically get similar results by sharding more but you should limit sharding based on disk IO capabilities (especially when there's so much it won't get in RAM), which isn't necessary one-for-one with the CPU count. "
        },
        {
            "author": "Alexey Kozhemiakin",
            "id": "comment-13794018",
            "date": "2013-10-14T09:46:38+0000",
            "content": "We observed 4x speedup when calculating 14 facets in 6 threads for 200mln index. Thanks everybody!\nhttps://twitter.com/AlexKozhemiakin/status/389688204309196800 "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13794064",
            "date": "2013-10-14T11:21:10+0000",
            "content": "Alexey:\n\nThanks for the feedback. This is one of those things that's very dependent on faceting on more then one field/query and the facets being fairly complex/expensive. But when those conditions are met, it's very noticeable. "
        }
    ]
}