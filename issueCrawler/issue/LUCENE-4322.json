{
    "id": "LUCENE-4322",
    "title": "Can we make oal.util.packed.BulkOperation* smaller?",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Bug",
        "fix_versions": [
            "4.0",
            "6.0"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "These source files add up to a lot of sources ... it caused problems when compiling under Maven and InteliJ.\n\nI committed a change to make separates files, but in aggregate this is still a lot ...\n\nEG maybe we don't need to specialize encode?",
    "attachments": {
        "LUCENE-4322-2.patch": "https://issues.apache.org/jira/secure/attachment/12543269/LUCENE-4322-2.patch",
        "LUCENE-4322.patch": "https://issues.apache.org/jira/secure/attachment/12542782/LUCENE-4322.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-08-23T08:56:10+0000",
            "content": "Having a specialized encode can be important for GrowableWriter performance (this method is used every time the number of bits per value needs to be changed).\n\nNow that BulkOperation.java has been splitted, I think everything compiles under Maven and Intellij . Why do we still need to make these files smaller?\n\nIf we really need to make these classes smaller, maybe we could first stop specializing encode for int[] values when bitsPerValue > 32. I can't think of a use-case this method could be useful to. ",
            "author": "Adrien Grand",
            "id": "comment-13440157"
        },
        {
            "date": "2012-08-23T09:51:12+0000",
            "content": "I looked into those files briefly so this may not be applicable but if possible make them use loops instead of manually unrolling the code in Java. JIT will typically unroll loops anyway depending on the environment and those unrolled JIT loops are faster than java-unrolled loops (because certain bounds checks can be eliminated). ",
            "author": "Dawid Weiss",
            "id": "comment-13440179"
        },
        {
            "date": "2012-08-23T12:44:26+0000",
            "content": "Actually the size increase wasn't as bad as I thought ... Lucene core JAR is 2.3 MB in 4.0 Beta and now it's 2.7 MB.\n\nSo I agree the immediate problem (can't compile in some envs) is fixed ... so making these smaller isn't really important.\n\nStill if we have pointless code (int[] values with bpv > 32) we should remove it.\n\nAnd Dawid's idea sounds compelling if it could make things faster! ",
            "author": "Michael McCandless",
            "id": "comment-13440246"
        },
        {
            "date": "2012-08-23T13:35:32+0000",
            "content": "It shouldn't make anything slower, really. There are several reasons \u2013 loop unrolling at jit time is one, but then there are also jit codegen limits (too big methods won't even jit, ever), cpu cache considerations (jitted code will be larger than a loop), etc. ",
            "author": "Dawid Weiss",
            "id": "comment-13440282"
        },
        {
            "date": "2012-08-23T13:45:09+0000",
            "content": "\nActually the size increase wasn't as bad as I thought ... Lucene core JAR is 2.3 MB in 4.0 Beta and now it's 2.7 MB.\n\nthis is surprising, it used to be like 1MB. Its scary to me its 2.7MB even though we pulled out large generated code like queryparser and standardtokenizer. I think we need to investigate what happened here. ",
            "author": "Robert Muir",
            "id": "comment-13440290"
        },
        {
            "date": "2012-08-23T13:48:34+0000",
            "content": "I think we need to investigate what happened here.\n\n+1\n\nIn 3.6.1 it's 1.5M.  For the longest time we were under 1M!  That was impressive  ",
            "author": "Michael McCandless",
            "id": "comment-13440293"
        },
        {
            "date": "2012-08-23T13:53:08+0000",
            "content": "\nIn 3.6.1 it's 1.5M.\n\nA lot of that is because it supports several grammars of StandardTokenizer/UAX29URLEMailTokenizer/ClassicTokenizer and still has queryparser.\n\nAll of this was removed in core. 4.0 should be 1MB.\n\nIf we have megabytes of generated specialized code, we should remove all of this and replace it with a simple loop. Then each optimization should be re-introduced one by one based on its space/time tradeoff.\n\nWe certainly dont need optimizations for bits per value > anything like 4 or 5 I think. 32 is outlandish, just use an int[] ",
            "author": "Robert Muir",
            "id": "comment-13440297"
        },
        {
            "date": "2012-08-23T14:36:56+0000",
            "content": "All of this was removed in core. 4.0 should be 1MB.\n\nEven when removing the whole oal.util.packed package, the JAR size is still 2.1MB.\n\nWe certainly dont need optimizations for bits per value > anything like 4 or 5 I think. 32 is outlandish, just use an int[].\n\nThese classes are not only used to store large int arrays in memory but also to perform encoding/decoding of short sequences, such as in BlockPF. If we want BlockPF to remain fast, 5 is probably too low. Mike tested BlockPF with an unspecialized decoder and it showed a great performance loss : https://issues.apache.org/jira/browse/LUCENE-3892?focusedCommentId=13431491&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13431491 ",
            "author": "Adrien Grand",
            "id": "comment-13440335"
        },
        {
            "date": "2012-08-23T14:45:48+0000",
            "content": "\nEven when removing the whole oal.util.packed package, the JAR size is still 2.1MB.\n\nRight, I don't mean to complain about the packed package or single it out (though I have concerns about the massive specialization),\nI was pointing out the larger issue of bloat. There are definitely other problems too.\n\n\nThese classes are not only used to store large int arrays in memory but also to perform encoding/decoding of short sequences, such as in BlockPF. If we want BlockPF to remain fast, 5 is probably too low. Mike tested BlockPF with an unspecialized decoder and it showed a great performance loss : https://issues.apache.org/jira/browse/LUCENE-3892?focusedCommentId=13431491&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13431491\n\nBut I don't think an unspecialized decoder is necessarily fair. I think we could optimize the low bpv that we would find in freqs/positions and then have a unspecialized fallback or whatever.\n\nI have concerns that specializing every bpv just means that nothing is even getting JITd and actually makes things worse. ",
            "author": "Robert Muir",
            "id": "comment-13440346"
        },
        {
            "date": "2012-08-24T17:47:02+0000",
            "content": "I swept the full Wikipedia (05/12 export) index to count how many blocks we see for each bitsPerValue:\n\n\n 0 bits:  0.8% blocks=540300\n 1 bits:  0.0% blocks=38\n 2 bits:  6.9% blocks=4614597\n 3 bits: 13.5% blocks=9069600\n 4 bits:  6.7% blocks=4480753\n 5 bits:  2.8% blocks=1875920\n 6 bits:  2.8% blocks=1867222\n 7 bits:  8.0% blocks=5394210\n 8 bits: 39.4% blocks=26435409\n 9 bits:  3.4% blocks=2271263\n10 bits:  3.2% blocks=2114455\n11 bits:  2.8% blocks=1910650\n12 bits:  2.5% blocks=1670494\n13 bits:  2.1% blocks=1383284\n14 bits:  1.6% blocks=1091507\n15 bits:  1.3% blocks=855881\n16 bits:  1.0% blocks=643360\n17 bits:  0.6% blocks=421664\n18 bits:  0.4% blocks=256929\n19 bits:  0.2% blocks=132227\n20 bits:  0.0% blocks=31852\n21 bits:  0.0% blocks=11753\n22 bits:  0.0% blocks=7176\n23 bits:  0.0% blocks=1168\n\n\n\nThe 0 bits case means all values are the same (eg all 1s). ",
            "author": "Michael McCandless",
            "id": "comment-13441331"
        },
        {
            "date": "2012-08-24T22:57:26+0000",
            "content": "Results on the full Meme index (http://snap.stanford.edu/data/memetracker9.html), 211M docs:\n\n\n0 bits:  1.9% blocks=1350821\n1 bits:  0.0% blocks=649\n2 bits: 18.5% blocks=13112234\n3 bits:  9.1% blocks=6487129\n4 bits:  2.6% blocks=1865845\n5 bits:  2.8% blocks=2006701\n6 bits:  8.9% blocks=6344910\n7 bits: 17.1% blocks=12133598\n8 bits: 12.4% blocks=8811848\n9 bits:  6.5% blocks=4646619\n10 bits:  3.8% blocks=2696347\n11 bits:  3.0% blocks=2162407\n12 bits:  2.8% blocks=1965635\n13 bits:  2.5% blocks=1758592\n14 bits:  2.1% blocks=1469763\n15 bits:  1.7% blocks=1204291\n16 bits:  1.3% blocks=950651\n17 bits:  0.9% blocks=647747\n18 bits:  0.6% blocks=461223\n19 bits:  0.5% blocks=345580\n20 bits:  0.4% blocks=252739\n21 bits:  0.3% blocks=180809\n22 bits:  0.2% blocks=120015\n23 bits:  0.1% blocks=56358\n24 bits:  0.0% blocks=12463\n25 bits:  0.0% blocks=4242\n26 bits:  0.0% blocks=2077\n27 bits:  0.0% blocks=374\n\n ",
            "author": "Michael McCandless",
            "id": "comment-13441578"
        },
        {
            "date": "2012-08-26T12:49:30+0000",
            "content": "based on this, can we just specialize 2, 3, 6, 7, 8 or something like that? ",
            "author": "Robert Muir",
            "id": "comment-13442093"
        },
        {
            "date": "2012-08-26T23:47:43+0000",
            "content": "These numbers depend on so many factors (docFreq distribution, order in which documents have been indexed, etc.) that it sounds strange to me to only pick a few bits per value we would like to keep specialized based on these benchmarks. I think it would make more sense to specialize a full range?\n\nI have concerns that specializing every bpv just means that nothing is even getting JITd and actually makes things worse.\n\nI have the same concerns but on the other hand if we pick too few numbers of bits per value, BlockFor might show very disappointing performance with different datasets.\n\nMaybe something more conservative would be to specialize the [1-24] range. It would already make the JAR ~350kb smaller (if we removed all specialized impls, the JAR would be ~500kb smaller). Removing all encoder specializations would probably help us save another 50kb. ",
            "author": "Adrien Grand",
            "id": "comment-13442224"
        },
        {
            "date": "2012-08-28T16:28:28+0000",
            "content": "It looks like one of the reasons of the size increase of lucene-core.jar is its number of classes. Although some code has been moved to modules, its number of classes has increased from 972 in 3.6.1 to 1471 in trunk. ",
            "author": "Adrien Grand",
            "id": "comment-13443246"
        },
        {
            "date": "2012-08-28T16:35:12+0000",
            "content": "Patch that tries to reduce the JAR size:\n\n\tunspecialized encode methods,\n\tspecialized decode methods only when 0 < bitsPerValue <= 24.\n\n\n\nOverall, it makes the core jar 361kb bytes smaller (2700542 bytes before applying the patch, 2330514 after).\n\nI ran a quick run of lucene-util in debug mode with blockPostingsFormat=For and it showed no performance difference. ",
            "author": "Adrien Grand",
            "id": "comment-13443253"
        },
        {
            "date": "2012-08-28T16:53:09+0000",
            "content": "Patch looks good!  That's a nice reduction.\n\nToo bad we have to duplicate decode 4 times (from long[]/byte[] to long[]/int[]).\n\nWe could still shrink things further by doing less loop unrolling ourselves?  Eg for BulkOperationPacked2, when decoding from byte[], that code is replicated 8 times but could be done as just 8* iters. ",
            "author": "Michael McCandless",
            "id": "comment-13443267"
        },
        {
            "date": "2012-08-28T17:14:14+0000",
            "content": "We could still shrink things further by doing less loop unrolling ourselves? Eg for BulkOperationPacked2, when decoding from byte[], that code is replicated 8 times but could be done as just 8* iters.\n\nYes, I had planned to work on it too! Unless someone doesn't like my last patch, I'll commit it and will start working on this loop unrolling issue soon... ",
            "author": "Adrien Grand",
            "id": "comment-13443284"
        },
        {
            "date": "2012-08-28T17:31:45+0000",
            "content": "+1 for the first iteration. ",
            "author": "Robert Muir",
            "id": "comment-13443297"
        },
        {
            "date": "2012-08-28T17:53:17+0000",
            "content": "+1 for the first iteration. ",
            "author": "Michael McCandless",
            "id": "comment-13443313"
        },
        {
            "date": "2012-08-28T18:22:26+0000",
            "content": "Good idea, Andrien. More classes are definitely worse than more static methods, this is a normal result of how ZIP format works (each file is encoded individually, compression dictionaries are inefficient for many small files). ",
            "author": "Dawid Weiss",
            "id": "comment-13443343"
        },
        {
            "date": "2012-08-28T18:53:34+0000",
            "content": "+1!! Smaller, smaller, smaller  ",
            "author": "Uwe Schindler",
            "id": "comment-13443388"
        },
        {
            "date": "2012-08-31T13:34:37+0000",
            "content": "New iteration.\n\nThis patch makes the JAR 84kB smaller (now 2243513 bytes) by :\n 1.removing the specializations for BulkOperationPackedSingleBlock,\n 2. rolling back some loops,\n 3. removing the assertions on the buffer sizes (that were rather useless).\n\nI removed the specializations for BulkOperationPackedSingleBlock because the default version is already rather fast (it has no conditions). For example, for bitsPerValue=4, BulkOperationPackedSingleBlock is only ~20-30% slower than BulkOperationPacked4 and BulkOperationPackedSingleBlock4, while BulkOperationPacked is ~300% slower.\n\nMoreover I rolled back some loops (especially the decode(byte[],...) are now 8 times shorter).\n\nI ran a benchmark between this patch and current trunk to make sure these changes don't hurt the Block postings format performance ('local' means patch applied):\n\n                Task   QPS trunkStdDev trunk   QPS localStdDev local      Pct diff\n         LowSpanNear       28.22        0.81       27.11        1.64  -12% -    4%\n         MedSpanNear        8.86        0.30        8.63        0.47  -11% -    6%\n        HighSpanNear        7.59        0.24        7.42        0.42  -10% -    6%\n             MedTerm      965.02       26.52      947.64       22.67   -6% -    3%\n            HighTerm      265.84        9.09      262.46        5.01   -6% -    4%\n          HighPhrase        9.43        1.17        9.41        1.23  -22% -   28%\n            PKLookup      249.61       10.58      250.46        8.97   -7% -    8%\n          OrHighHigh       58.76        2.22       59.09        3.13   -8% -   10%\n           LowPhrase       76.16        2.73       76.82        3.24   -6% -    9%\n           OrHighLow      140.83        5.06      142.33        7.41   -7% -   10%\n           MedPhrase       60.21        2.81       60.92        3.28   -8% -   11%\n           OrHighMed      136.16        4.81      138.12        7.21   -7% -   10%\n     LowSloppyPhrase       81.25        2.17       82.80        2.82   -4% -    8%\n             Respell       91.72        4.07       93.47        2.21   -4% -    9%\n             Prefix3      251.63        8.73      256.48        6.61   -4% -    8%\n         AndHighHigh      136.17        2.20      138.95        2.85   -1% -    5%\n    HighSloppyPhrase       14.17        0.66       14.46        0.74   -7% -   12%\n            Wildcard      185.86        5.38      190.23        3.38   -2% -    7%\n              IntNRQ       63.10        6.93       64.65        2.58  -11% -   19%\n     MedSloppyPhrase       25.48        1.05       26.27        1.17   -5% -   12%\n              Fuzzy1      106.76        4.83      110.17        2.44   -3% -   10%\n          AndHighLow     2650.35       53.66     2739.95       58.80    0% -    7%\n              Fuzzy2       35.45        1.95       36.75        0.99   -4% -   12%\n          AndHighMed      357.48        5.31      370.88        6.77    0% -    7%\n             LowTerm     2211.45       89.59     2298.23      104.62   -4% -   13%\n\n\n\nThis looks good to me.\n\nI think there are maybe a few things we could still do to reduce the JAR size but I'm worried that there would be little gain compared to the increased complexity of these classes.\n\nI just tried to remove all BulkOperation* specializations, and it only made the JAR 52kb smaller compared to this patch, so maybe we should stop here? There's already been a lot of progress! ",
            "author": "Adrien Grand",
            "id": "comment-13445924"
        },
        {
            "date": "2012-08-31T14:34:37+0000",
            "content": "+1 to commit. Thanks for doing all of this Adrien! ",
            "author": "Robert Muir",
            "id": "comment-13445998"
        },
        {
            "date": "2012-08-31T16:09:45+0000",
            "content": "NP Robert, I'm happy to help, thanks for reviewing!\n\nI just committed (r1379479 and r1379489 on trunk, 1379491 on branch 4.x). Do we agree on closing this issue now? ",
            "author": "Adrien Grand",
            "id": "comment-13446080"
        },
        {
            "date": "2012-08-31T16:11:50+0000",
            "content": "+1 ",
            "author": "Robert Muir",
            "id": "comment-13446082"
        },
        {
            "date": "2013-05-10T10:32:45+0000",
            "content": "Closed after release. ",
            "author": "Uwe Schindler",
            "id": "comment-13653710"
        }
    ]
}