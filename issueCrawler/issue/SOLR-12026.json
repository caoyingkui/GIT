{
    "id": "SOLR-12026",
    "title": "SimplePostTool with robots.txt",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "SimplePostTool"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "7.2",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "[First issue here, apologies in advance for missteps.]\n\nThree things which could improve working with robots.txt:\n\n\tWhen fetching the corresponding robots.txt for a URL, the port is ignored and so it defaults to :80.\u00a0 If nothing is listening :80, it fetches the page.\u00a0 isDisallowedByRobots() could include the url.getPort() when constructing strRobot.\u00a0 This helps when testing your robots on a non-standard port, such as during development.\n\tDisallow directives are applied regardless of User-agent.\u00a0 parseRobotsTxt() could override a Disallow which specifies SimplePostTool-crawler.\u00a0 This would help when indexing your own site which you've explicitly allowed for indexing by SimplePostTool.\u00a0 I don't know if that's a good practice, but it would help in testing.\n\tThe User-agent header when fetching robots.txt is not \"SimplePostTool-crawler\" but shows as \"Java/<version>\".\u00a0 The code which sets the header correctly from readPageFromUrl() could be reused in isDisallowedByRobots().",
    "attachments": {},
    "issue_links": {},
    "comments": []
}