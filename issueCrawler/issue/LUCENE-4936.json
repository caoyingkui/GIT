{
    "id": "LUCENE-4936",
    "title": "docvalues date compression",
    "details": {
        "components": [
            "core/index"
        ],
        "fix_versions": [
            "4.4"
        ],
        "affect_versions": "None",
        "priority": "Major",
        "labels": "",
        "type": "Improvement",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "DocValues fields can be very wasteful if you are storing dates (like solr's TrieDateField does if you enable docvalues) and don't actually need all the precision: e.g. \"date-only\" fields like date of birth with no time component, time fields without milliseconds precision, and so on.\n\nIdeally we'd compute GCD of all the values to save space (numberOfTrailingZeros is not really enough here), but i think we should at least look for values like 86400000, 3600000, and 1000 to be practical.",
    "attachments": {
        "LUCENE-4936.patch": "https://issues.apache.org/jira/secure/attachment/12578935/LUCENE-4936.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2013-04-16T12:43:22+0000",
            "content": "here's my hack patch... I think we should do something for DiskDV too though... and of course add tests  ",
            "author": "Robert Muir",
            "id": "comment-13632801"
        },
        {
            "date": "2013-04-18T15:58:40+0000",
            "content": "Patch:\n\n\n\tAdds MathUtil.gcd(long, long)\n\n\n\n\n\tAdds \"GCD compression\" to Lucene42, Disk and CheapBastard.\n\n\n\n\n\tImproves BaseDocValuesFormatTest which almost only tested \"TABLE_COMPRESSED\" with Lucene42DVF\n\n\n\n\n\tNo more attempts to compress storage when the values are known to be dense, such as SORTED ords.\n\n\n\nI measured how slower doc values indexing is with these new checks, and it is completely unnoticeable with random or dense values since the GCD quickly reaches 1. When the GCD is larger, it only made indexing 2% slower (every doc has a single field which is a NumericDocValuesField). So I think it's fine. ",
            "author": "Adrien Grand",
            "id": "comment-13635270"
        },
        {
            "date": "2013-04-18T16:30:04+0000",
            "content": "Looks great! I'm glad you were able to make this fast.\n\nA few ideas:\n\n\tI like the switch with corruption-check on DiskDV. Can we easily integrate this into Lucene42?\n\tCan we update the file format docs (we attempt to describe the numerics strategies succinctly here)\n\n\n\nI can do a more thorough review and some additional testing later, but this looks awesome.\n\nLater we should think about a place (maybe in codec file format docs, maybe even NumericDocValuesField?) to add some practical general guidelines to users, that might not otherwise be intuitive: Stuff like if you are putting Dates in NumericDV, zero out portions you dont care about (e.g. milliseconds, time, etc) to save space, indexing as UTC will be a little more efficient than with local offset, etc.\n\n\nImproves BaseDocValuesFormatTest which almost only tested \"TABLE_COMPRESSED\" with Lucene42DVF\n\nYeah this is a good catch! We should also maybe open an issue to review DiskDV and try to make it more efficient. Optimizations like TABLE_COMPRESSED don't exist there I think: it could be handy if someone wants e.g. smallfloat scoring factor. Its nice this patch provides back compat for DiskDV but its not totally necessary in the future, if we want to review and rewrite it. In general that codec was just done very quickly and hasn't seen much benchmarking or anything: could use some work. ",
            "author": "Robert Muir",
            "id": "comment-13635310"
        },
        {
            "date": "2013-04-18T16:44:01+0000",
            "content": "\nindexing as UTC will be a little more efficient than with local offset, etc.\n\nWe could probably solve issues like that too (maybe in something like cheap-bastard codec), if we did a first pass to compute min/max\nand then did GCD only on the deltas from min... right? ",
            "author": "Robert Muir",
            "id": "comment-13635329"
        },
        {
            "date": "2013-04-18T17:25:22+0000",
            "content": "indexing as UTC will be a little more efficient than with local offset, etc.\n\nIf you use a NumericField and store the long epoch in it, it is UTC as no localization involved. ",
            "author": "Uwe Schindler",
            "id": "comment-13635374"
        },
        {
            "date": "2013-04-18T17:43:59+0000",
            "content": "But NumericField is totally unrelated to docvalues!\n\nBesides, delta+GCD has other applications than just GMT offset, e.g. solr's Currencyfield (at least in the US, people love prices like 199, 299, 399...): in that case it would save 9 bits per value where it would do nothing with the current patch.\n\nI'm not arguing the extra pass should be the in the default codec, i just said it might be interesting for cheap-bastard or something. ",
            "author": "Robert Muir",
            "id": "comment-13635406"
        },
        {
            "date": "2013-04-18T17:51:36+0000",
            "content": "Sorry i was thinking about car prices when i said 9bpv, but you get the drift  ",
            "author": "Robert Muir",
            "id": "comment-13635423"
        },
        {
            "date": "2013-04-18T18:00:32+0000",
            "content": "But NumericField is totally unrelated to docvalues!\n\nThats clear. I just said, if you use a LONG docvalues field and store the long epoch its always timezone-less. That was what I wanted to say. This applies to Solr, too. ",
            "author": "Uwe Schindler",
            "id": "comment-13635444"
        },
        {
            "date": "2013-04-19T13:29:32+0000",
            "content": "New patch:\n\n\n\tComputes the GCD based on deltas in order to be able to compress non-UTC dates.\n\n\n\n\n\tAdds support for TABLE_COMPRESSED to DiskDVF.\n\n\n\n\n\tAdds tests that ensure that these new compression methods are actually used whenever applicable.\n\n\n\n\n\tAdds a quick description of the compression method to Lucene42DVF javadocs.\n\n ",
            "author": "Adrien Grand",
            "id": "comment-13636361"
        },
        {
            "date": "2013-04-19T14:00:18+0000",
            "content": "This is really cool! I did not yet completely reviewed it, but i like that it is included in default codec, so user does not need to take care. ",
            "author": "Uwe Schindler",
            "id": "comment-13636390"
        },
        {
            "date": "2013-04-19T14:15:29+0000",
            "content": "Thank you Uwe! Unfortunately, I just figured out that the patch is broken when v - minValue overflows (in Consumer.addNumericField). I need to think about a way to fix it... ",
            "author": "Adrien Grand",
            "id": "comment-13636406"
        },
        {
            "date": "2013-04-19T15:30:47+0000",
            "content": "Here is a work-around for the issue: the consumer stops trying to perform GCD compression as soon as it encounters a value outside the [ -MAX_VALUE/2 , MAX_VALE/2 ] range. This prevents overflows from happening and I can't think of a reasonable use-case that would benefit from GCD compression and have values outside of this range? ",
            "author": "Adrien Grand",
            "id": "comment-13636488"
        },
        {
            "date": "2013-04-19T15:41:07+0000",
            "content": "Yeah, i think those are ridiculously large numbers \n\nI'm gonna try to help review and play with the patch. I think this is great though. ",
            "author": "Robert Muir",
            "id": "comment-13636500"
        },
        {
            "date": "2013-04-19T15:44:47+0000",
            "content": "Thank you Robert, I'd love to have a review to make sure the patch is correct, especially for MathUtil.gcd and the DVConsumer.addNumericField logic. ",
            "author": "Adrien Grand",
            "id": "comment-13636503"
        },
        {
            "date": "2013-04-19T15:51:43+0000",
            "content": "First thing that sticks out is maybe to remove the extra pass? Even though it just pulls\nthe first value...\n\nFor the DiskDV codec i feel its simple, just remove the loop and look for 'count == 0'.\nFor Lucene42, its probably best to just add 'count' for the same reason?\n\nBut if it makes things more confusing, maybe just leave it the way it is. Its a little tricky either way  ",
            "author": "Robert Muir",
            "id": "comment-13636508"
        },
        {
            "date": "2013-04-19T16:14:20+0000",
            "content": "Simple ideas are often the best ones, the new patch has a single loop! Thanks Robert! ",
            "author": "Adrien Grand",
            "id": "comment-13636537"
        },
        {
            "date": "2013-04-19T16:27:00+0000",
            "content": "Yeah the duplicate loop was strange much better now.\n\nOne small thing:\n\n\nif (v < - Long.MAX_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n\n\n\nshould maybe changed to, looks more logical to me...\n\n\nif (v < Long.MIN_VALUE / 2 || v > Long.MAX_VALUE / 2) {\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-13636556"
        },
        {
            "date": "2013-04-20T13:51:04+0000",
            "content": "Same patch: I just uploaded a test.\n\nFor DiskDVProducer, I think the compression tables and so on should be structured to be in the metadata section rather than the data section, to minimize the per-thread instance overhead.\n\ntoday its:\n\n    final IndexInput data = this.data.clone();\n    data.seek(entry.offset);\n\n    final BlockPackedReader reader = new BlockPackedReader(data, entry.packedIntsVersion, entry.blockSize, entry.count, true);\n    return new LongNumericDocValues() {\n      @Override\n      public long get(long id) {\n        return reader.get(id);\n      }\n    };\n\n\n\nI'll try to cut this stuff over... ",
            "author": "Robert Muir",
            "id": "comment-13637252"
        },
        {
            "date": "2013-04-20T14:50:17+0000",
            "content": "Additionally in all cases we can move the corruption check to where we read the metadata. this means we detect problems earlier, rather than when someone asks for the field the first time. I'll do this too. ",
            "author": "Robert Muir",
            "id": "comment-13637273"
        },
        {
            "date": "2013-04-20T15:17:31+0000",
            "content": "OK: patch with these changes: we write all this stuff in the metadata for diskdv (to reduce overhead), and also stronger/earlier checks on startup (lucene42, too). in getNumeric() i changed it to AssertionError since we already checked it at open time.\n\nadditionally for all corrumption checks, i ensured we did:\n\n\nif (something) {\n  throw new CorruptIndexException(\"some message, input=\" + meta);\n}\n\n\n\npassing the indexinput (meta/data) is very useful, in case someone hits the problem, they get a file name.\n\nI'll do some more review later... ",
            "author": "Robert Muir",
            "id": "comment-13637281"
        },
        {
            "date": "2013-04-20T15:25:08+0000",
            "content": "I will generate backwards indexes from 4.2.0 now and commit them to branch_4x/trunk.\n\nIt hurts nothing even if we don't commit this issue  ",
            "author": "Robert Muir",
            "id": "comment-13637285"
        },
        {
            "date": "2013-04-20T15:28:10+0000",
            "content": "Oh... i checked, i already did this. so when we commit this one, we can just generate 4.4 indexes.\n\nI think i generated these whenever we changed the format initially so we detect problems earlier than later... ",
            "author": "Robert Muir",
            "id": "comment-13637287"
        },
        {
            "date": "2013-04-21T19:37:12+0000",
            "content": "+1 to the proposed changes!\n\nHere is an updated patch that fixes the DVProducer constructors to open the data file and check the header in a try/finally block (so that data files are closed even if the header check fails). ",
            "author": "Adrien Grand",
            "id": "comment-13637628"
        },
        {
            "date": "2013-04-22T13:47:51+0000",
            "content": "+1: nice catch adrien.\n\nWhy do we have this logic for TABLE_COMPRESSED in diskdv consumer?\n\n    if (uniqueValues != null\n        && ((maxValue - minValue) < 0L || (maxValue - minValue) > 256)\n        && count <= Integer.MAX_VALUE) {\n\n\n\nShouldn't this just be:\n\n    if (uniqueValues != null && count <= Integer.MAX_VALUE) {\n\n\n\nWe only care about the number of unique values, but in this case it does not matter what they actually are. ",
            "author": "Robert Muir",
            "id": "comment-13638005"
        },
        {
            "date": "2013-04-22T15:14:40+0000",
            "content": "I guess the point was to avoid one level of indirection in case all values can be stored using a single byte. Maybe \"(maxValue - minValue) > 256\" should be replaced with \"(maxValue - minValue) >= uniqueValues.size()\"? This would ensure that table compression isn't used if values are alreadu dense? ",
            "author": "Adrien Grand",
            "id": "comment-13638090"
        },
        {
            "date": "2013-04-22T15:29:01+0000",
            "content": "I see. In this case should we just take bitsRequired on both sides? ",
            "author": "Robert Muir",
            "id": "comment-13638102"
        },
        {
            "date": "2013-04-22T15:41:44+0000",
            "content": "One advantage of DELTA_COMPRESSED is that it uses different numbers of bits per value per block. Even if max-min=200, it could still happen that most blocks only require 6 or 7 bits per value. If there are many blocks, this could save substantial disk/memory. ",
            "author": "Adrien Grand",
            "id": "comment-13638114"
        },
        {
            "date": "2013-04-22T15:45:04+0000",
            "content": "Only in the case of outliers... but TABLE could do this too, if it sorted its table by increasing frequency of occurrence. ",
            "author": "Robert Muir",
            "id": "comment-13638115"
        },
        {
            "date": "2013-04-22T15:45:26+0000",
            "content": "In this case should we just take bitsRequired on both sides?\n\nYes, this makes sense ! ",
            "author": "Adrien Grand",
            "id": "comment-13638117"
        },
        {
            "date": "2013-04-22T15:45:51+0000",
            "content": "(decreasing rather) ",
            "author": "Robert Muir",
            "id": "comment-13638118"
        },
        {
            "date": "2013-04-22T18:19:22+0000",
            "content": "OK i added the bitsRequired check in this patch... I think we are good to go here.\n\n+1 to commit ",
            "author": "Robert Muir",
            "id": "comment-13638270"
        },
        {
            "date": "2013-04-23T13:38:59+0000",
            "content": "[trunk commit] jpountz\nhttp://svn.apache.org/viewvc?view=revision&revision=1470948\n\nLUCENE-4936: Improve numeric doc values compression (for dates especially). ",
            "author": "Commit Tag Bot",
            "id": "comment-13639053"
        },
        {
            "date": "2013-04-23T13:44:47+0000",
            "content": "[branch_4x commit] jpountz\nhttp://svn.apache.org/viewvc?view=revision&revision=1470953\n\nLUCENE-4936: Improve numeric doc values compression (for dates especially). ",
            "author": "Commit Tag Bot",
            "id": "comment-13639058"
        },
        {
            "date": "2013-07-23T18:37:03+0000",
            "content": "Bulk close resolved 4.4 issues ",
            "author": "Steve Rowe",
            "id": "comment-13716732"
        }
    ]
}