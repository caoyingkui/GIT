{
    "id": "SOLR-3393",
    "title": "Implement an optimized LFUCache",
    "details": {
        "affect_versions": "3.6,                                            4.0-ALPHA",
        "status": "Open",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "components": [
            "search"
        ],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "SOLR-2906 gave us an inefficient LFU cache modeled on FastLRUCache/ConcurrentLRUCache.  It could use some serious improvement.  The following project includes an Apache 2.0 licensed O(1) implementation.  The second link is the paper (PDF warning) it was based on:\n\nhttps://github.com/chirino/hawtdb\nhttp://dhruvbird.com/lfu.pdf\n\nUsing this project and paper, I will attempt to make a new O(1) cache called FastLFUCache that is modeled on LRUCache.java.  This will (for now) leave the existing LFUCache/ConcurrentLFUCache implementation in place.",
    "attachments": {
        "SOLR-3393-4x-withdecay.patch": "https://issues.apache.org/jira/secure/attachment/12562724/SOLR-3393-4x-withdecay.patch",
        "SOLR-3393-trunk-withdecay.patch": "https://issues.apache.org/jira/secure/attachment/12562725/SOLR-3393-trunk-withdecay.patch",
        "SOLR-3393.patch": "https://issues.apache.org/jira/secure/attachment/12524015/SOLR-3393.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Shawn Heisey",
            "id": "comment-13259301",
            "date": "2012-04-22T23:25:52+0000",
            "content": "After all the bugs are worked out of the implementation covered by this issue, I can see a future scenario where the existing (slow) LFUCache is scrapped, FastLFUCache is renamed LFUCache, and a new FastLFUCache implementation that uses a Concurrent class is created.  If the committers would prefer that I go ahead and scrap LFUCache now and just use that name, let me know.\n\nI have no plans to work on SOLR-2889, so it may be best to just close that issue.  I don't know if SOLR-2906 should be changed from a subtask to a full task. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13259609",
            "date": "2012-04-23T13:58:57+0000",
            "content": "I've been working on this.  I've come to realize that I don't completely understand how CacheRegenerator works.  I suspect that it is geared around LRU caches and that the new cache won't have any of the frequency information from the old one, it will just put the entries into the cache as if they were new.  Can anyone confirm this?  If I am right, I think my SOLR-2906 implementation is incorrect at warm time.\n\nAfter the new cache is regenerated, should I go through the new cache, grab the frequency information from the old cache with each key, and fix the new cache up?\n\nYonik, you were the one that came up with the timeDecay option for SOLR-2906.  It was added to the markAndSweep routine (which evicts old entries).  Should it also be in the warm routine, or possibly only exist in the warm routine? "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13260750",
            "date": "2012-04-24T17:51:48+0000",
            "content": "Patch for new LFUCache implementation.  It should be pretty close to O(1).  ConcurrentLFUCache is removed.  TestLFUCache and SolrInfoMBeanTest have been updated as well.  All tests pass. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13260828",
            "date": "2012-04-24T18:58:19+0000",
            "content": "New patch that does not touch SolrInfoMBeanTest.  I figured out what FastLRUCache does to avoid failing this test and did the same.  The getStatistics method now immediately returns an empty NamedList if the cache has not been initialized.  This could be done for LRUCache as well. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13261098",
            "date": "2012-04-24T22:34:30+0000",
            "content": "Further work, but no patch yet because it requires a bunch of test tweaks: I have split timeDecay into evictDecay and warmDecay.  The timeDecay option still works, and sets both to true.  In the absence of the timeDecay option, I've made evictDecay default to false and warmDecay default to true.\n\nIn order to cause elements to decay over time, every entry in the cache must be removed from one LinkedHashSet and added to another.  This is probably pretty fast for this implementation, but it does still require time.  My use case scenario will have commits relatively often, up to once a minute.  For me, doing the decay at warm time is enough, and results in fewer items to touch.  To have it happen at eviction time is overhead I don't need, but it would be correct for some use cases.  What are some other people's opinions about the default settings? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13261197",
            "date": "2012-04-25T01:09:50+0000",
            "content": "I will attempt to make a new O(1) cache called FastLFUCache\n\n#OhDearGodPleaseNotAnotherClassWithFastInTheName\n\nPlease, please, please lets end the madness of subjective adjectives in class names ... if it's an LFU cache wrapped around a \"hawtdb\" why don't we just call it \"HawtDbLFUCache\" ?\n\nI've been working on this. I've come to realize that I don't completely understand how CacheRegenerator works. I suspect that it is geared around LRU caches and that the new cache won't have any of the frequency information from the old one, it will just put the entries into the cache as if they were new. Can anyone confirm this?\n\nThe idea behind the CacheRegenerator API is to be as simple as possible and agnostic to:\n\n\tthe Cache Impl (ie: LRUCache vs LFUCache vs HawtDbLFUCache)\n\tthe cache usage (ie: Query->DocSets vs Query->DocList vs String->MyCustomClass)\n\tthe means of generating values from keys (ie: how do you know which MyCustomClass should be cached for which String)\n\n\n\n... so you can have a custom (named) cache instance declared in your solrconfig.xml with your own MySpecialCacheRegenerator that knows about your usecase and might do something special with the keys/values (like: short-circut part of the generation if it can see the data hasn't changed, or read from authoritative data files outside of solr, etc...) and then use any Cache impl class that you're heart desires, and things will still work right.\n\nAfter the new cache is regenerated, should I go through the new cache, grab the frequency information from the old cache with each key, and fix the new cache up?\n\nyou certainly could \u2013 when (new HawtDbLFUCache(...)).warm(...) is called, it needs to delegate to the regenerator for pulling values from the \"old\" cache, but that doesn't mean it can't also directly ask the \"old\" cache instance for stats about each of the old keys as it loops over them \u2013 remember: the \"new\" cache is the one inspecting the \"old\" cache and deciding what things to ask the regenerator to generate.\n\nBut i question whether you really want any sort of stats from the \"old\" cache copied over to the \"new\" cache.  it is, after all, a completely new cache \u2013 with new usage.  should the stats really be preserved forever?  regardless of how popular an object was in the \"old\" cache instance, should we automatically assume it's equally popular in the \"new\" cache instance? "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13261203",
            "date": "2012-04-25T01:30:21+0000",
            "content": "Hoss, thanks for your comments.\n\nSince I was the one who wrote the previous version of this, I just opted to completely replace LFUCache rather than go with a \"Fast\" appendage.  I hadn't considered the naming problem in quite the same light as \"yet another fast*\" name, but it did seem like a bad idea.\n\nYonik had the same concern about stats being preserved forever on SOLR-2906, and he helped with a decay option to deal with that.  I think the decay is a good idea.  There was only one kind of decay before, applied to all elements anytime there were evictions, defaulted to on.\n\nIn a new version of the patch for this issue (which I have not yet uploaded) I have now included two kinds of decay.  There is the kind applied at eviction, now defaulting to off, and one applied at warming, defaulting to on.  I will expand the documentation on the Wiki, making it clear that turning off the decay option will probably lead to an undesirable cache state.  Currently the decay is implemented with a bit shift (>>> 1), I may make another option available that just subtracts one, and we can bikeshed about which option should be default. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13261666",
            "date": "2012-04-25T14:19:56+0000",
            "content": "Patch with separate options for evictDecay and warmDecay.  The timeDecay option sets the other two options to true, for compatibility with 3.6.0.  The tests aren't yet updated, so one of them fails.  I need to change the tests so they verify operation of both the new options. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13263425",
            "date": "2012-04-27T07:25:41+0000",
            "content": "Updated patch.  Tests pass and should cover all current functionality.  It defaults to a keepPercentage of 80.  I'm not sure I like 'keepPercentage' as an option name, feel free to bikeshed over that and all the defaults I have chosen.\n\nIt might be ready to commit now, if nobody finds fault with it. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13263445",
            "date": "2012-04-27T07:37:08+0000",
            "content": "Is this too big a change to be included in 3.6.1?  I will likely be trying this out with 3.6, once I clear my plate a little bit.  Possibly futile justification: It's a bug fix!  The bug it fixes is \"poor performance.\"  "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13264231",
            "date": "2012-04-28T06:19:29+0000",
            "content": "The commit for SOLR-1893 means that my patch needs more work.  I have a business trip next week that takes priority, though. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13412072",
            "date": "2012-07-11T22:25:54+0000",
            "content": "bulk fixing the version info for 4.0-ALPHA and 4.0 all affected issues have \"hoss20120711-bulk-40-change\" in comment "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13429883",
            "date": "2012-08-07T03:43:59+0000",
            "content": "rmuir20120906-bulk-40-change "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13448423",
            "date": "2012-09-05T03:12:52+0000",
            "content": "i manually wrangled Shawn's latest patch so that it works on trunk - tests pass, but i don't know if i'm comfortable committing for 4.0 w/o...\n\n1) a second pair of eyeballs sanity checking that my manual beating on the patch resulted in the correct final code.\n\n2) some timing data indicating that this change really is faster \u2013 particularly since it replaces the internals of LFUCache, so it will affect upgrading users even if they don't change their configs. "
        },
        {
            "author": "Adrien Grand",
            "id": "comment-13449665",
            "date": "2012-09-06T13:42:30+0000",
            "content": "Hi hoss, thanks for bringing this up!\n\n\n\tput should return the previous value instead of returning the value that has been put into the cache.\n\tI don't like the evictDecay option, I assume it is here to prevent the most frequently used entry from being evicted in case all entries have a frequency >= maxFreq but on the other hand it makes every put operation run in O( n ) so maybe we should just remove it and add a message in the class javadocs to warn users that entries that have a frequency >= maxFreq are evicted according to a LRU policy.\n\tMaybe we should remove warmDecay as well, I understand it is here to try to prevent cache pollution but it makes the cache behave differently in case there are commits: if an entry is retrieved 5 times before a commit and 5 times after, it will be considered less frequently used than an entry that has been retrieved 8 times after the commit, this is counterintuitive.\n\tI think Entry.value and Entry.frequency don't need to be volatile?\n\tmaxCacheSize - 1 is probably a too high default value for maxFreq. It can make doEviction (and consequently put) run in O( n ). Maybe we should make it configurable with something like log( n ) or 10 as a default value? Moreover, lower values of maxFreq are less prone to cache pollution.\n\n\n\nRegarding your (2), I personally don't mind if it is a little slower on average. I would expect the get method to be slower with this impl but on the other hand ConcurrentLFUCache seems to provide no warranty that it will be able to evict entries as fast as they get inserted into the cache  so I think this new impl is better. "
        },
        {
            "author": "Adrien Grand",
            "id": "comment-13449871",
            "date": "2012-09-06T18:04:31+0000",
            "content": "Chris, I tried to rework your patch in order to remove the decay options, make maxFreq configurable (with 10 as a default value) and share the statistics code with LRUCache (by adding a common MapCacheBase superclass).\n\nWhat do you think? "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13450096",
            "date": "2012-09-06T22:07:07+0000",
            "content": "Adrien, thanks for looking at it and making it better.  This is early in my experience with Java - I can still count the number of projects I've built myself on one hand.  Also, there have been a number of changes to the entire cache system since I wrote the first patch, changes that I have not had a chance to review.\n\nI definitely like doing the decay only at warm time.  I'm perfectly happy to have evictDecay yanked out.  I didn't think of the decay at all, that was Yonik on SOLR-2906.  I agreed with his reasons.  I wonder if there might be a way to have the decay happen much less often \u2013 say after a configurable number of commits rather than for every commit.  Also, I can't remember whether I kept the bitshift decay (dividing by two) or changed it to subtract one from the frequency.  IMHO subtracting one would be better.\n\nI don't understand your first note about the put, and I can't take the time to re-read the code right now.  On whether things should be volatile or not \u2013 I based all that on SOLR-2906, and I based SOLR-2906 on existing stuff.  I don't completely understand what the implications are.  If you do, awesome.\n\nOn the default for maxFreq and how it might affect performance \u2013 again, I expect you've got more experience and can make a better determination.\n\nHoss, I would be very surprised to learn than anyone was actually using the current implementation in 3.6.0 or the 4.0 alpha/beta.  I still haven't had a chance to give it a serious trial in my own setup, and I wrote it!  I think about that first attempt as similar to the first sort algorithm you ever get introduced to in a programming class, before they introduce recursion and tell you about quicksort.   "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13450114",
            "date": "2012-09-06T22:31:01+0000",
            "content": "Adrien,\n\nI've been looking at your patch, especially the warming code.  I can't see anything in there that maintains the frequency values from the old cache to the new cache.\n\nWith maxFreq of 10 and a cache size much larger (200, 1000, etc), there's no difference from the cache's perspective between something that has been requested 50 times versus something that has been requested 100 times.  How did the maxFreq being related to the cache size make it slower? "
        },
        {
            "author": "Adrien Grand",
            "id": "comment-13450494",
            "date": "2012-09-07T09:52:18+0000",
            "content": "I agreed with his reasons. I wonder if there might be a way to have the decay happen much less often \u2013 say after a configurable number of commits rather than for every commit.\n\nI not comfortable with applying evictDecay based on the commit rate while cache usage depends on the query rate: a read-only index would never benefit from it.\n\nI don't understand your first note about the put\n\nThe SolrCache.put(key, value) method should return the previous value associated with key or null if key was not in the cache. Instead, LFUCache.put returned the value that has just been added to the cache.\n\nOn whether things should be volatile or not \u2013 I based all that on SOLR-2906, and I based SOLR-2906 on existing stuff. I don't completely understand what the implications are. If you do, awesome.\n\nhttp://en.wikipedia.org/wiki/Volatile_variable#In_Java\n\nOne of the use-cases of the volatile keyword is to make sure that you are actually reading the most up-to-date value of a variable. By not using this keyword, it could happen that two CPUs don't think that a variable has the same value (because of their caches). Brian Goetz has written a nice article on the volatile keyword in case you are interested in the features of volatile variables http://www.ibm.com/developerworks/java/library/j-jtp06197/index.html.\n\nWe don't need it here because everything happens in synchronized blocks, which already ensure that you are reading the most up-to-date value.\n "
        },
        {
            "author": "Adrien Grand",
            "id": "comment-13450580",
            "date": "2012-09-07T13:02:27+0000",
            "content": "I can't see anything in there that maintains the frequency values from the old cache to the new cache.\n\nYou're right, I forgot to add a nocommit about it! This should be fixed with this new patch.\n\nWith maxFreq of 10 and a cache size much larger (200, 1000, etc), there's no difference from the cache's perspective between something that has been requested 50 times versus something that has been requested 100 times.\n\nTheir frequency is considered equal but they are sorted according to their access order, so only the least recently used has a chance to be evicted.\n\nHow did the maxFreq being related to the cache size make it slower?\n\nThis was because of the call to findLowestFrequency in doEviction. If one element has frequency=0 and all other ones have frequency=maxFreq, findLowestFrequency must inspect every slot. But this should be avoidable: since doEviction is only called inside put, there is no need to compute lowestFreq inside doEviction, put will always set lowestFreq=0 in the end.\n\nI also modified the patch to rename the previous LFUCache impl to ConcurrentLFUCache (similarly to FastLRUCache, I just didn't want to prefix with \"Fast\" since I think this is a bit misleading). "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13450611",
            "date": "2012-09-07T13:24:10+0000",
            "content": "ConcurrentLFUCache seems to provide no warranty that it will be able to evict entries as fast as they get inserted into the cache \n\nCorrect, but the context in which this cache is used (things like filter caches, query caches, etc) ensure that this isn't really a problem in practice.  And since it's a cache, holding an extra entry briefly really doesn't have any negative effects.\n\nI do think some sort of decay option is important, regardless of how it's implemented (and I never got a chance to look at the implementation here). "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13450667",
            "date": "2012-09-07T14:39:01+0000",
            "content": "This was because of the call to findLowestFrequency in doEviction. If one element has frequency=0 and all other ones have frequency=maxFreq, findLowestFrequency must inspect every slot. But this should be avoidable: since doEviction is only called inside put, there is no need to compute lowestFreq inside doEviction, put will always set lowestFreq=0 in the end.\n\nIf indeed I only used doEviction inside put, then lowestFreq would always be 0.  Thinking generally, in case doEviction did get called anywhere else, perhaps that value should be tracked rather than computed every time.  There's probably a way to quickly figure out the new value if the data structure connected to that frequency gets reduced to empty.\n\nIf we eliminate the need to cycle through every unused frequency one by one, we should be able to keep maxFreq at the max cache size minus one, allowing for the greatest granularity under the current implementation.\n\nLike Yonik, I think having decay is important, but the last implementation was way too aggressive.  My current thought: Only do it at warm time, and only do it after a configurable time period has passed from the previous decay or initial cache creation.  Offer an additional option where it would happen after X commits.  Default the decay to true and the time period to one hour.  Apply the decay by subtracting one rather than doing a bitshift.  This should keep things fairly predictable in the short term while also keeping the cache clean over the long term.\n\nUnit tests for the decay would probably use values <= 5000 milliseconds for the time period. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13450993",
            "date": "2012-09-07T21:24:54+0000",
            "content": "assigning to myself to follow up with this for 4.1 \u2013 if someone wants to review/profile in time for 4.0 we can certainly still consider it. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13527262",
            "date": "2012-12-08T21:15:34+0000",
            "content": "Followup while going through all my old issues:\n\nI like Adrien's changes to my patch in general, but I still think a slow default decay (subtract 1 from each frequency) on autowarm is a good idea.  In the interests of making sure it doesn't affect performance much, require a minimum time period to elapse before decaying again.\n\nIMHO my previous LFU implementation (the one that actually got committed) is total crap and this should just completely replace it. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13541106",
            "date": "2012-12-30T17:04:34+0000",
            "content": "A recent commit adding a large number of @Override annotations resulted in a lot of manual work to apply this patch to a 4x checkout.  I have done this work, and I have also added a \"minDecayIntervalMs\" option, defaulting to 300000, or five minutes. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13541116",
            "date": "2012-12-30T17:50:40+0000",
            "content": "New patches against recent trunk and 4x checkouts that also implement a slow decay.  Because two of the files have a getSource method that SVN changes on checkout, applying the patch with standard linux patch tools is problematic.\n\nSVN aware patch utilities (I tried with TortoiseSVN) seem to apply with no problems. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13541144",
            "date": "2012-12-30T19:34:32+0000",
            "content": "I would like to deprecate ConcurrentLFUCache (patch renames old LFUCache implementation to ConcurrentLFUCache) in this patch for 4.x and eliminate it entirely for trunk, but I will leave that decision up to the committer who takes this on.  The existing patches do not do this.  I also realized that I have not included CHANGES.TXT.  I have the following suggestion for that:\n\n4x:\nSOLR-3393: New LFUCache implementation with much better performance.  Deprecate old implementation and rename it to ConcurrentLFUCache.\n\ntrunk:\nSOLR-3393: New LFUCache implementation with much better performance.  Removed old implementation.\n "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13541153",
            "date": "2012-12-30T20:20:41+0000",
            "content": "When I built the 4x patch, I accidentally checked out a specific old revision instead of the newest.  The patch will apply successfully to the most recent revision as long as the SVN URL glitch is dealt with first, or you use svn-aware tools. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13541157",
            "date": "2012-12-30T20:53:24+0000",
            "content": "N.B.: subversion appears to have gotten the 'patch' subcommand in version 1.7 - CentOS 6 has v1.6.  I always find that Redhat's stable offerings are quite outdated. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13636626",
            "date": "2013-04-19T17:20:06+0000",
            "content": "I'd like to see this committed, my prior implementation is just terrible.  Hoss, what do you think about my suggestion to put a slow decay back in?  Would you like to continue on this issue, or should I take it over? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13636797",
            "date": "2013-04-19T19:52:48+0000",
            "content": "Shit, i'm sorry man i totally droped the ball on this.\n\ni haven't looked at your latest patch, but catching up on the comments you general API compatibility approach sounds fine to me \u2013 you may want to follow up with Adrien or Yonik to sanity check your decay code, because i didn't fully understand what you guys were talking about the first time it came up. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13717329",
            "date": "2013-07-23T18:47:53+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13970967",
            "date": "2014-04-16T12:56:46+0000",
            "content": "Move issue to Solr 4.9. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-14248265",
            "date": "2014-12-16T14:00:05+0000",
            "content": "I really need to finish this and see if I can get it into 5.0. "
        },
        {
            "author": "Bill Bell",
            "id": "comment-14616110",
            "date": "2015-07-07T03:09:07+0000",
            "content": "Let's get this done. What is remaining? O(1) sounds great. "
        }
    ]
}