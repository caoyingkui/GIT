{
    "id": "SOLR-1144",
    "title": "replication hang",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "1.4"
        ],
        "components": [],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "It seems that replication can sometimes hang.\nhttp://www.lucidimagination.com/search/document/403305a3fda18599",
    "attachments": {
        "stacktrace-slave-2.txt": "https://issues.apache.org/jira/secure/attachment/12448566/stacktrace-slave-2.txt",
        "stacktrace-slave-1.txt": "https://issues.apache.org/jira/secure/attachment/12448565/stacktrace-slave-1.txt",
        "stacktrace-master.txt": "https://issues.apache.org/jira/secure/attachment/12448564/stacktrace-master.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Noble Paul",
            "id": "comment-12705891",
            "date": "2009-05-05T04:10:19+0000",
            "content": "isn't this same as SOLR-1096 ? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12706199",
            "date": "2009-05-05T21:22:09+0000",
            "content": "Hmmm, I had trouble finding SOLR-1096 before.\nBut it looks like it was used mainly for adding a timeout.  There's still an underlying bug somewhere, right? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12706302",
            "date": "2009-05-06T04:54:59+0000",
            "content": "the stacktrace http://markmail.org/message/ecr6m4rf4iy2d652 . \n\nI suspect the following two threads are blocked\n\n\n'NioBlockingSelector.BlockPoller-2' Id=10, RUNNABLE on lock=, total cpu\ntime=5580.0000ms user time=2120.0000ms\nat sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\nat sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:215)\nat sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)\nat sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)\nat sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)\nat\norg.apache.tomcat.util.net.NioBlockingSelector$BlockPoller.run(NioBlockingSe\nlector.java:305)\n'NioBlockingSelector.BlockPoller-1' Id=9, RUNNABLE on lock=, total cpu\ntime=333280.0000ms user time=107520.0000ms\nat sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\nat sun.nio.ch.EPollrrayWrapper.poll(EPollArrayWrapper.java:215)\nat sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)\nat sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)\nat sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)\nat\norg.apache.tomcat.util.net.NioBlockingSelector$BlockPoller.run(NioBlockingSe\nlector.java:305)\n\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12706857",
            "date": "2009-05-07T12:59:27+0000",
            "content": "I don't see a thread trace from the replication handler.... which one should if it was causing the hang, right? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12706868",
            "date": "2009-05-07T13:13:21+0000",
            "content": "ReplicationHandler does not cause the hang on the master. On the slave the SnapPuller was waiting forever which I hope would have fixed with SOLR-1096 "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12707405",
            "date": "2009-05-08T16:31:37+0000",
            "content": "ReplicationHandler does not cause the hang on the master.\n\nThe slave is waiting forever, but it could be due to a bug on either the master or the slave, and it could be due to the replication handler.  It could also be another Solr bug somewhere, or it could be a Tomcat bug.\n\nWhat is apparent is that since there is no replication stack trace on the master, it thinks it finished the file send (either that or got an exception), but the slave is still expecting more for some reason.  Perhaps if we used non-persistent connections for replication, the master would close the connection when it thought it had sent everything? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12707421",
            "date": "2009-05-08T17:05:51+0000",
            "content": "The master closes the connection if everything is written.  if the download of a file is complete slave also closes the stream . The fact that the slave continued to wait means the file has not been downloaded completely.  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12707425",
            "date": "2009-05-08T17:14:29+0000",
            "content": "The master closes the connection if everything is written. \n\nHmmm, that doesn't jive with the slave hanging on a read though... seems like the only way read() should block is if there is no more data to read currently and the socket is still open. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12707435",
            "date": "2009-05-08T17:31:35+0000",
            "content": "The master closes the connection if everything is written. \nactually , it just closes the stream. the underlying connection could be open\n\nseems like the only way read() should block is if there is no more data to read currently and the socket is still open\n\nyes. I'm wondering what would have caused this scenario\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12716520",
            "date": "2009-06-05T06:33:36+0000",
            "content": "resolving for the time being. We can reopen if the issue is reported again\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12775729",
            "date": "2009-11-10T15:52:04+0000",
            "content": "Bulk close for Solr 1.4 "
        },
        {
            "author": "Toby Cole",
            "id": "comment-12884644",
            "date": "2010-07-02T11:32:58+0000",
            "content": "Just over a year since it was first spotted, I'm consistently getting the same symptoms as this bug.\nWe've got a single master, with two slaves polling it, both slaves have stalled at exactly the same point in the replication.\n\nHere's the relevent section of the replication handler's 'details' response:\nNode A\n\n      <str name=\"numFilesDownloaded\">18</str>\n      <str name=\"replicationStartTime\">Fri Jul 02 10:40:00 BST 2010</str>\n      <str name=\"timeElapsed\">6683s</str>\n      <str name=\"currentFile\">_9du.prx</str>\n      <str name=\"currentFileSize\">8.17 MB</str>\n      <str name=\"currentFileSizeDownloaded\">8.17 MB</str>\n      <str name=\"currentFileSizePercent\">100.0</str>\n      <str name=\"bytesDownloaded\">40.55 MB</str>\n      <str name=\"totalPercent\">0.0</str>\n      <str name=\"timeRemaining\">8290722s</str>\n      <str name=\"downloadSpeed\">6.21 KB</str>\n\n\n\nNode B\n\n      <str name=\"numFilesDownloaded\">18</str>\n      <str name=\"replicationStartTime\">Fri Jul 02 10:40:00 BST 2010</str>\n      <str name=\"timeElapsed\">6752s</str>\n      <str name=\"currentFile\">_9du.prx</str>\n      <str name=\"currentFileSize\">8.17 MB</str>\n      <str name=\"currentFileSizeDownloaded\">8.17 MB</str>\n      <str name=\"currentFileSizePercent\">100.0</str>\n      <str name=\"bytesDownloaded\">40.55 MB</str>\n      <str name=\"totalPercent\">0.0</str>\n      <str name=\"timeRemaining\">8376322s</str>\n      <str name=\"downloadSpeed\">6.15 KB</str>\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12884653",
            "date": "2010-07-02T12:24:52+0000",
            "content": "do you have the threaddump "
        },
        {
            "author": "Toby Cole",
            "id": "comment-12884672",
            "date": "2010-07-02T13:35:54+0000",
            "content": "Adding stacktraces for both slave instances and the master instance.\nThese stack traces are from a reproduction of the original problem, so the timestamps will not matchup with the XML from the replication-handler previously posted. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12884690",
            "date": "2010-07-02T14:24:46+0000",
            "content": "Thanks for the stack traces Toby!\n\nInteresting... seems like the commit in the slave blocked...\n\n\tat org.apache.solr.common.util.ConcurrentLRUCache.getLatestAccessedItems(ConcurrentLRUCache.java:276)\n\n\n\nSo perhaps another thread locked, but didn't unlock the lock?\n\nSOLR-1538 did fix something that could possibly lead to a deadlock, but it's super unlikely (a very small object allocation would have to fail at just the right spot).  Still, if this is easy enough to reproduce, could you try Solr 1.4.1 and see if it's fixed?  (and if it hangs again, be sure to get stack traces... they are super helpful!) "
        },
        {
            "author": "Toby Cole",
            "id": "comment-12884693",
            "date": "2010-07-02T14:32:01+0000",
            "content": "Oh yes, should have mentioned... we're already on Solr 1.4.1 in production as of yesterday (we don't hang about y'know  ). "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12884717",
            "date": "2010-07-02T15:29:27+0000",
            "content": "The odd thing is that the line numbers in the stack traces don't match up for either 1.4.0 or 1.4.1\nSpecifically ConcurrentLRUCache.java:276 is in the middle of markAndSweep() in both versions (as opposed to getLatestAccessedItems() which your stack trace would suggest).\n\nAre these stack traces from stock 1.4.0 or 1.41?  If so, does anyone have a clue why the line numbers would be off? "
        },
        {
            "author": "Toby Cole",
            "id": "comment-12884719",
            "date": "2010-07-02T15:35:22+0000",
            "content": "I know exactly why the line numbers would be off. I just remembered we're using a custom war package so we can add our own plugins in (yes, I know we can use solr.home/lib, but we've not got round to that yet).\n\nThe only classes we're overriding from solr are ConcurrentLRUCache and FastLRUCache. This was from pre solr 1.4, when the cache implementations were slowing faceting right down.\nI have a feeling if I remove those overridden classes and use the new (bug-free) ones, the hang may stop.\n\nI'll give it a go now, sorry in advance if it was my oversight that is causing this bug to re-appear.\nT "
        },
        {
            "author": "Vadim Kisselmann",
            "id": "comment-13012047",
            "date": "2011-03-28T13:28:30+0000",
            "content": "I have Solr running on one master and two slaves (load balanced) via\nSolr 1.4.1 native replication.\n\nIf the load is low, both slaves replicate with around 100MB/s from master.\nAfter a couple of hours the replication slows down to 100KB/s.\nSo the problem is still there.\n\nI tested it with both Jetty and Tomcat.\nIt looks like that aggressive JVM-Options can delay the problem, but then it starts anyway.\n\nMy Index is about 100GB, i use 10GB for JVM, 24GB total.\nThe slaves polls every 5 minutes. "
        },
        {
            "author": "Vadim Kisselmann",
            "id": "comment-13012407",
            "date": "2011-03-29T09:28:43+0000",
            "content": "After a few tests, I think I've located the problem. It's probably the Solr caches.\n\nIf I deactivate the caches in solrconfig.xml, replication works fine. But if any of them are active, the replication slows down.\n\nDisabling the caches isn't an option for me since the query times gets way too long. "
        }
    ]
}