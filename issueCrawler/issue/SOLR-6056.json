{
    "id": "SOLR-6056",
    "title": "Zookeeper crash JVM stack OOM because of recover strategy",
    "details": {
        "affect_versions": "4.6",
        "status": "Closed",
        "fix_versions": [
            "5.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Critical",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Some errors\"org.apache.solr.common.SolrException: Error opening new searcher. exceeded limit of maxWarmingSearchers=2, try again later\", that occur distributedupdateprocessor trig the core admin recover process.\nThat means every update request will send the core admin recover request.\n(see the code DistributedUpdateProcessor.java doFinish())\n\nThe terrible thing is CoreAdminHandler will start a new thread to publish the recover status and start recovery. Threads increase very quickly, and stack OOM , Overseer can't handle a lot of status update , zookeeper node for  /overseer/queue/qn-0000125553 increase more than 40 thousand in two minutes.\n\nAt the last zookeeper crash. \nThe worse thing is queue has too much nodes in the zookeeper, the cluster can't publish the right status because only one overseer work, I have to start three threads to clear the queue nodes. The cluster doesn't work normal near 30 minutes...",
    "attachments": {
        "patch-6056.txt": "https://issues.apache.org/jira/secure/attachment/12644376/patch-6056.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Raintung Li",
            "id": "comment-13994821",
            "date": "2014-05-12T05:34:30+0000",
            "content": "1. Move report the status from the  coreadminhandle to the doRecovery method, only one thread will report this status.\n2.  While find the thread is working for recovery, the other recovery thread will quit except set the parameter to enforce recovery. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13995618",
            "date": "2014-05-12T20:58:05+0000",
            "content": "Good find! Both are terrible bugs. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13996036",
            "date": "2014-05-13T04:24:52+0000",
            "content": "+1, good stuff. \n\nRegarding 2, we have to think carefully before making this change. \n\nWhat about the sequence where a replica is recovering, and an update to be buffered randomly fails - what ensures the replica fully recovers? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13996039",
            "date": "2014-05-13T04:27:45+0000",
            "content": "Just  reviewing by iphone, but I think the move in 1 is likely unnecessary as it some what defeats the purpose of the quick publish comment. It's really just a a best effort thing, and we can likely fall back to the natural publish that running recovery does.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14027970",
            "date": "2014-06-11T16:22:09+0000",
            "content": "I think we have to think through 2 more, but 1 is really nasty and we should just fix it by taking out the quick publish attempt I think. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14028033",
            "date": "2014-06-11T17:12:20+0000",
            "content": "I agree, we should remove the publish in CoreAdminHandler. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14029029",
            "date": "2014-06-12T11:18:34+0000",
            "content": "Commit 1602123 from shalin@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1602123 ]\n\nSOLR-6056: Don't publish recovery state until recovery runs to avoid overwhelming the overseer state queue "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14029055",
            "date": "2014-06-12T11:31:44+0000",
            "content": "I don't understand #2 because cancelRecovery will make sure that an old recovery thread joins and so at any given point there should be only one thread doing the recovery. If that is true, the while (recoveryRunning) block is also redundant. Am I missing something?\n\nOverall this doesn't seem like the best strategy because when each recovery request from DUPF.finish() will create a recovery thread and the next such request will cancel the last recovery and start a new recovery thread and so on. Also, if a long running recovery is in progress and there are additional request recovery threads coming in then the core will rapidly block up threads increasing the chances of deadlock. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14029516",
            "date": "2014-06-12T17:59:59+0000",
            "content": "Yes, only one thread should be running recovery at any time.\n\nOverall this doesn't seem like the best strategy \n\nThere is a better strategy - this one was just the simplest that also provides for the promises we need for proper recovery. It's 'evolved' a bit over time though, and it's certainly possible there is code in some spots that is no longer necessary. It's been a long time since I have been through it.\n\nCertainly it's an improvement to have no blocked threads, but any such improvement still has to work properly with recovery. I assumed Raintung had supplied at an attempt at such an improvement as a fix for #2. I have not had a chance to look closely at it. The reason that I punted on such a solution initially was that this was the easy way to be compliant with recovery.\n\nThere are many parts of SolrCloud like this - few initial builders means a lot of quick impls and leaves a lot of low hanging fruit improvements. "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14031601",
            "date": "2014-06-14T15:23:40+0000",
            "content": "SOLR-5593 was one more place where we had debated taking out the quick publish, but decided against it as you increase the chance of an unhealthy replica becoming the leader and losing updates. That argument probably still holds, if the leader is sending tons of recovery requests, couldn't either the leader or the replica not send/accept one while a recovery is in progress? "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14035947",
            "date": "2014-06-18T17:02:02+0000",
            "content": "Commit 1603527 from shalin@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1603527 ]\n\nRemove SOLR-6056 from 4.9 section "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15674255",
            "date": "2016-11-17T17:08:38+0000",
            "content": "It seems #1 was committed here and #2 was dealt with at SOLR-8371. Shalin Shekhar Mangar, Mark Miller, can we link SOLR-8371 as a related issue, and close this? "
        },
        {
            "author": "Cassandra Targett",
            "id": "comment-15843399",
            "date": "2017-01-27T19:58:39+0000",
            "content": "Per Ishan's last comment, it seems part of this issue was committed, and the other part was fixed in SOLR-8371. "
        }
    ]
}