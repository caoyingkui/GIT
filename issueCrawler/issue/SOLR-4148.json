{
    "id": "SOLR-4148",
    "title": "WhiteSpaceTokenizer splits long no-whitespace sequences every 255 characters",
    "details": {
        "affect_versions": "3.5,                                            4.0",
        "status": "Closed",
        "fix_versions": [],
        "components": [
            "Schema and Analysis"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Won't Fix"
    },
    "description": "I have the following text input in a field with an analysis chain that starts with WhiteSpaceTokenizerFactory.\n\nAuto,DEU,Drogendezernat,Drogenfahnder,Drogenfahndung,Drug,Duesseldorf,Ermittler,Ermittlung,Fahnder,Fahndung,Fahrzeug,Germany,Innere,Kriminalitaet,Kriminalpolizei,Kriminalpolizisten,Kripo,Oeffentliche,Ordnung,Polizei,Polizeikelle,Polizist,Polizistin,Rauschgift,Rauschgiftdezernat,Sicherheit,Sicherheitsbehoerden,Sicherheitskraefte,Uniform,Verbrechen,Waffen,Zivilbeamte,Zivilfahnder,Zivilpolizisten,anhalten,car,crime,criminal,drug,enforcement,investigators,law,officers,police,policeman,policemen,policeofficer,public,saftey,security,squad,stop,stoppen,stopping,suspect,suspicious,ueberpruefen,verdaechtig\n\nThis input has no whitespace.  WhiteSpaceTokenizer is taking this and breaking it into tokens after every 255th character.  It happens on both 3.5 and a recent branch_4x checkout.  I am using ICUTokenizerFactory on some fields in branch_4x, it doesn't have this problem.  I have not tried the ICUTokenizerFactory on 3.5.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Robert Muir",
            "id": "comment-13510830",
            "date": "2012-12-05T22:09:57+0000",
            "content": "This isn't so much a bug, all these simple tokenizers have limits like this. Its just not configurable in any way. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13510855",
            "date": "2012-12-05T22:31:38+0000",
            "content": "Thanks Robert.  As I was putting the issue together, I did wonder if it was a known limitation.  On 4x I'm using ICUTokenizerFactory on most fields, only one field left with the whitespace tokenizer.  That field is for mime-type, which should never have long unstructured data in it.\n\nIf nobody objects, this issue can hang around until someone decides to take a crack at fixing known limitations. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13510867",
            "date": "2012-12-05T22:46:11+0000",
            "content": "We can probably give reasonable options (e.g. you can increase it up to 4k).\n\nwarning: ICUTokenizer has a limit really. its just larger (i think 4K). "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13636751",
            "date": "2013-04-19T19:20:27+0000",
            "content": "This isn't a bug, as Robert pointed out.  Closing this issue.  If I get ambitious later, I may create an issue and patch that allows configuration of the max token length on all tokenizers\n\nThis is part of an effort to close old issues that I have reported.  Search tag: elyograg2013springclean "
        }
    ]
}