{
    "id": "LUCENE-92",
    "title": "maxFieldLength design flaw: large documents silently truncated",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "I have a large document (more than one megabyte) and terms appearing\nin the end of the document are not appearing in the index.\n\nThe IndexWriter class has a field \"maxFieldLength\". From the documentation:\n\nThe maximum number of terms that will be indexed for a single field in\na document. This limits the amount of memory required for indexing, so\nthat collections with very large files will not crash the indexing\nprocess by running out of memory.\n\nBy default, no more than 10,000 terms will be indexed for a field.\nThis means that Lucene will silently discard all tokens after it has\nindexed 10,000 in a particular document.\n\nThe solution is to remove this limitation. We have had success with\nwriter.maxFieldLength = Integer.MAX_VALUE.\n\nIn our opinion, this is a very dangerous design flaw, especially since\nLucene will truncate large documents without any warning, and the\nsearches will appear to work correctly, since some terms will work. It\nwas only by chance that a customer noticed and reported that some\ndocuments were not appearing in the search results.\n\nI'm sure there are many people using Lucene on large documents who\nthink that it's working correctly, yet their products are buggy.  This\nis what's most bothersome to me: that the default mode of usage\nencourages a serious bug (for that subset of users who have large \u2013\nbut not absurdly large! \u2013 documents).\n\nThe default should be to index all tokens via Integer.MAX_VALUE, and\nif a user requires memory efficiency, she should be able to set a\nlower threshold explicitly.\n\nSaying \"RTFM\" is not a useful response, since this is critical enough\nthat it should perform correctly upon first use. Moreover, the documentation\nis buried (it wasn't even in a FAQ or (as others have pointed out) in\na setter method \u2013 it just the JavaDoc for a random public instance\nvariable).\n\nSaying \"it's for performance reasons\" is even less useful.  Either\nIndexWriter should be rewritten to perfom gracefully under low memory\nconditions, or it should loudly announce (via an OutOfMemoryError,\nwhich is the expeected way of doing things, or a new descriptive runtime \nexception like TooManyTermsException, or a message logged to stderr or to the \ndefault logging target) its failure and suggest to\nthe developer that they may actively and consciously lower the threshold\nwith the knowledge that it will potentially introduce a defect into\ntheir product.  It's better to get a failure report for which there are known \nsolutions (increase your VM heap size or lower the field length) than for it to \nfail in a silent and insidious way.\n\nTo put it another way, the only people who would ever even see this \nhypothetical OutOfMemoryError are those who are attempting to index large \ndocuments; would they prefer to be informed or not informed that their attempt \nhas failed?\n\nThanks for a great product!  I hope this bug report helps make it even\nbetter.\n\n(Reported against both 1.3rc1 and 1.2)",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2003-04-09T03:00:11+0000",
            "content": "This is fairly common in search engines.  For example, Google silently truncates\npages whose HTML is longer than 100kB, around the same point where Lucene\ntruncates.  The problem is that crawlers and file system walkers would otherwise\nattempt to index things like gigantic log files, binaries, etc.\n\nI see your point though that for some classes of use, when the set of documents\nis tightly controlled and it is a requirement that every single word is indexed,\nthis is a problem.  The workaround is simple, although perhaps not obvious.\n\nMy concern with changing the default is that it would break all those folks who\ndepend on the current setting to keep their indexing from blowing up. ",
            "author": "cutting@apache.org",
            "id": "comment-12321228"
        },
        {
            "date": "2003-04-09T03:34:12+0000",
            "content": "Yeah, that \"for some classes of use\" is a killer, especially for a general-\npurpose library like a search engine.  I totally buy your reasoning that you \nshouldn't break existing crawlers.  \n\nAt the same time it's disturbing that it's silent.  It caused the team I was \nworking with to spend many hours isolating and tracking down the bug until \nsomeone carefully re-read the documentation for all the Lucene classes we were \nusing...\n\nEven if you don't change the implementation for 1.3, it would be excellent to \ndocument it more clearly in both the field and the addDocument method.\n\nTo the Javadoc for IndexWriter.maxFieldLength, I would add \"Note that this \neffectively truncates large documents, excluding from the index terms that \noccur late in the document.  If you know your source documents are large, be \nsure to set this value high enough to accomodate the expected size.  If you set \nit to Integer.MAX_VALUE, then the only limit is your memory, but you should \nanticipate an OutOfMemoryError.\"\n\nTo the JavaDoc for IndexWriter.addDocument, I would add \"If the document \ncontains more than \n{@link #maxFieldLength}\n terms for a given field, the \nremainder are discarded.\"\n\nAlso, the truncating when it happens could be logged somehow. Would it be \nappropriate for Lucene to support (or include) Commons Logging?\n\nThanks for your attention! ",
            "author": "Alex Chaffee",
            "id": "comment-12321229"
        },
        {
            "date": "2003-04-09T03:49:14+0000",
            "content": "+1 on the javadoc changes.\n\nI'm all for the logging too, if someone wants to submit diffs.  Where else would\nLucene benefit from logging?  Would it be better at this point to wait for JDK\n1.4 to get in broad distribution and then just use the java.util.logging stuff?\n Or use log4j?  Probably a logging discussion should be in a separate thread... ",
            "author": "cutting@apache.org",
            "id": "comment-12321230"
        },
        {
            "date": "2003-04-09T09:32:43+0000",
            "content": "I have added your Javadoc contribution, thank you!\nAs far as logging goes, yes, I think this should be discussed on lucene-dev.\nOne thing I really like about Lucene is that it has no run-time dependencies,\nand only depends on JavaCC for generating some classes at build time.\nI will close this bug for now, and if choose to add logging we can re-open this\nbug, or open a new one. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12321231"
        }
    ]
}