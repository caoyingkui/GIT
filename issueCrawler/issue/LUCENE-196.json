{
    "id": "LUCENE-196",
    "title": "[PATCH] Added support for segmented field data files and cached directories",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Duplicate",
        "status": "Resolved"
    },
    "description": "Hello, \n\nI would like to contribute the following enhancement, hoping that it would be \nas useful for you as it is for me. \n\nFor one of my applications, it was necessary to reprocess the Documents \nreturned by a search in a Lucene index according to some Field values (for \napplying an \"edit distance\" function on unindexed fields, in my case). \n\nBecause Lucene has to load every possibly relevant document (all fields, \nincluding the ones which are irrelevant for the algorithm) from disk into \nmemory for this operation - doing so is extensively time-consuming. \n\nAs far as I can see, currently, there is no satisfying solution to improve \nthis situation except buffering all data in RAM using a RAMDirectory. \n\nBut what if the field data is just too big to fit in RAM? \n\nMy patch will handle this by splitting the monolithic \"*.fdt\"-Field data file \ninto several \"data store\" files .fdt, .fd1, .fd2 and so on. \n\nThese \"data store\" files are connected as a linked-list which permits you to \nload only the part of the field data that is relevant for the current \noperation. \n\nSo, you can load all field data (as in the current implementation), or the \nfields from a specific interval [0;n] of data stores. Store 0 represents the \ndata in the \".fdt\" file, all data stores with ids > 0 are represented by files \n\".fd1\", \".fd2\", and so on. \n\nIn my case, I would then simply cache the \".fdt\" (data store 0) file in RAM \n(using a symbolic link to shm-/tmp), but leave all other .fd* files on \nharddisk. The .fdt file only contains the relevant field for my algorithm \n(which therefore remains quite small); all the other fields are stored in the \nrather big \".fd0\" file. So, accessing Fields in .fdt requires no disk I/O, \nwhich speeds up things remarkably. \n\nYou can compare this feature with having multiple tables in a relational \ndatabase that are linked with 1..1 cardinality instead of having one big \ntable. \n\nMy proposed enhancement requires some API additions, which I try to explain \nnow. \n\nTo specify the desired data store for a Field, simply call the new method \n\"Field setDataStore(int)\" (docstore 0 is the default): \ndoc.add(Field.Keyword(\"fieldA\", \"this is in docstore 0\")); \ndoc.add(Field.Keyword(\"fieldB\", \"this is in docstore 1\").setDataStore(1)); \n\nIn this example, fieldA would be stored in \".fdt\"; fieldB in \".fd1\". \n\nWhen you retrieve the Document object (example docId = 123) using an \nIndexReader, you have the following options: \n\"indexReader.document(123)\" would load all fields from all data stores. \n\"indexReader.document(123, 0)\" would load only the fields from data store 0. \n\"indexReader.document(123, 1)\" would explictly load only the fields from data \nstores 0 and 1. \n\nThe method \"IndexReader.document(int n, int k)\" is defined to fetch all fields \nfrom all data stores at least up to ID k. That way, existing IndexReader \nsubclasses do not have to be modified, as I provide an overridable method in \nIndexReader which simply calls document(int n). \n\nA more concrete example is attached to this feature request as a \nJUnit-Testcase, as well as the patch itself. \n\nHave fun with it! \n\n\nBest regards, \n\nChristian Kohlschuetter",
    "attachments": {
        "ASF.LICENSE.NOT.GRANTED--newDocStore-patch.txt": "https://issues.apache.org/jira/secure/attachment/12312321/ASF.LICENSE.NOT.GRANTED--newDocStore-patch.txt",
        "ASF.LICENSE.NOT.GRANTED--newDocStore-test-patch.txt": "https://issues.apache.org/jira/secure/attachment/12312322/ASF.LICENSE.NOT.GRANTED--newDocStore-test-patch.txt",
        "ASF.LICENSE.NOT.GRANTED--docStore-patch.txt": "https://issues.apache.org/jira/secure/attachment/12312317/ASF.LICENSE.NOT.GRANTED--docStore-patch.txt",
        "ASF.LICENSE.NOT.GRANTED--docStore-test-patch.txt": "https://issues.apache.org/jira/secure/attachment/12312318/ASF.LICENSE.NOT.GRANTED--docStore-test-patch.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2004-03-17T22:57:32+0000",
            "content": "Created an attachment (id=10828)\nThe patch diff ",
            "author": "Christian Kohlsch\u00c3\u00bctter",
            "id": "comment-12321613"
        },
        {
            "date": "2004-03-17T22:57:58+0000",
            "content": "Created an attachment (id=10829)\nThe JUnit testcase (as diff) ",
            "author": "Christian Kohlsch\u00c3\u00bctter",
            "id": "comment-12321614"
        },
        {
            "date": "2004-03-17T23:18:15+0000",
            "content": "I should also mention that this enhancement does not break \nbackwards-compatibility to existing indexes. \n\n\nChristian Kohlschuetter \n ",
            "author": "Christian Kohlsch\u00c3\u00bctter",
            "id": "comment-12321615"
        },
        {
            "date": "2004-03-18T00:36:52+0000",
            "content": "Thank you for your contribution.\n\nDoes your patch work with both the original index format (the one with .fdt file\nthat you describe), as well as with the newer compound index format? ",
            "author": "Otis Gospodnetic",
            "id": "comment-12321616"
        },
        {
            "date": "2004-03-18T00:56:51+0000",
            "content": "Otis, \n\nit should, at least. I have modified the relevant parts in SegmentMerger, but, \nfrankly, I have not tested it yet - I will try that tomorrow. \n\nKeep in mind that using this feature with compound indexes makes things \ncomplicated again - because then you have an even bigger, monolithic file, \nwhich cannot simply be linked into SHM-/tmp. To overcome this, I was thinking \nabout having a wrapping Directory that keeps files with specific suffixes in a \nRAMDirectory, all the other files in another one (probably FSDirectory)... \n\nEverybody interested in this patch is invited, of course, to apply the relevant \nchanges to the attached JUnit test class to prove its compound-format \ncompatibility. \n\n\nChristian \n ",
            "author": "Christian Kohlsch\u00c3\u00bctter",
            "id": "comment-12321617"
        },
        {
            "date": "2004-03-18T15:58:48+0000",
            "content": "Created an attachment (id=10842)\nModified JUnit testcase also proving compound-format compatiblity ",
            "author": "Christian Kohlsch\u00c3\u00bctter",
            "id": "comment-12321618"
        },
        {
            "date": "2004-03-18T16:12:22+0000",
            "content": "Created an attachment (id=10843)\nAn even more picky version of the JUnit testcase, now also checks CompoundFileReader for directory contents ",
            "author": "Christian Kohlsch\u00c3\u00bctter",
            "id": "comment-12321619"
        },
        {
            "date": "2004-03-18T20:09:07+0000",
            "content": "Another addition, which is especially useful in combination with this patch, \nbut its use not limited to that. \n\nMy \"CachedDirectory\" enables you to put specific files of a Directory into a  \nRAMDirectory, whereas all the others are kept in an FSDirectory, for example.  \nThis speeds up access to certain, smaller files, whereas the bigger files \nremain on disk. \n\nThe decision whether to cache a file or not may be made using a  \n\"FileFilter\" (interface provided with this patch), or for all files (cache  \nall).  \n\nThis makes using the Field-data-store patch very comfortable, as no \nsystem-specific symlinks have to be created (should now also work on Windows). \nTo keep the \".fdt\" file in RAM, you would only need a FileFilter that returns \n\"true\" for all files ending with \".fdt\" and \"false\" otherwise. \n\nThe CachedDirectory also works with compound indexes. \n\nThe problem is that there is no public Directory implementation for them, only \nan internal \"CompoundFileReader\", which does not provide access to the \n\"deletable\" and \"segment\" files - they are kept outside the compound file. I \nhave implemented a preliminary \"CompoundDirectoryWrapper\" which provides \ntransparent access to a directory containing an arbitrary number of compound \nfiles as well as unpacked files like \"deletable\" and \"segment\". \n\nAlthough the CachedDirectory can be used quite independently from the \nmultiple-field-data patch, I think it's ok to attach it here as well. \n\nI have also written a new method (read \"stolen from RAMDirectory\")  \n\"Directory.copyFrom(Directory source, String sourceName, String targetName)\" \nto make copying files between two Directories easier. \n\nIn addition to the documented classes, there are also two JUnit test cases, \nwhich could be used as a starting point. \n\nJust tell me what you think. \n\n\nChristian \n ",
            "author": "Christian Kohlsch\u00c3\u00bctter",
            "id": "comment-12321620"
        },
        {
            "date": "2004-03-18T20:10:06+0000",
            "content": "Created an attachment (id=10847)\nnew patch diff ",
            "author": "Christian Kohlsch\u00c3\u00bctter",
            "id": "comment-12321621"
        },
        {
            "date": "2004-03-18T20:10:33+0000",
            "content": "Created an attachment (id=10848)\nnew testcases diff ",
            "author": "Christian Kohlsch\u00c3\u00bctter",
            "id": "comment-12321622"
        },
        {
            "date": "2004-03-19T02:32:02+0000",
            "content": "The goal of this patch is to improve performance when all values, in all\ndocuments, of a field are required.  The solution proposed is to split the\nstored field data, so that it can be more efficiently read and cached, in effect\noptimizing IndexReader.document().\n\nAn alternate approach might be to instead index the field.  Then one can use a\nTermEnum to enumerate all values of the field, and a TermDocs to enumerate the\ndocuments which have those values.  This is considerably more efficient, as both\nthe sequence of terms and of documents are compressed, while field data accessed\nthrough IndexReader.document() is not compressed.  All the required data is also\nstored contiguously on disk, minimizing seeks and maximizing it's cacheability.\n\nHave you benchmarked your approach versus something like this?  Your change\nintroduces a fair amount of new code and API complexity.  I prefer to keep\nLucene's API and code as simple and small as possible, so I am reluctant to add\na feature like you propose unless there is no viable alternative. ",
            "author": "cutting@apache.org",
            "id": "comment-12321623"
        },
        {
            "date": "2004-03-19T05:07:53+0000",
            "content": "Doug, \n\nthanks for your reply. I think that I should explain some background of this \npatch. \n\nThe main reason for writing this patch was to provide support for applying \nfunctions on field values that are independent on an upstream index but \ndependent on the entered query. \n\nIn my application, I do use an index (access through TermEnum/TermDocs) to \nreduce the number of returned documents K to a fraction of all documents N. The \nreturned set (probably multiple terms per document) needs to be reprocessed \nagainst the entered query (which may consist of multiple terms as well). After \nreprocessing, the resulting set R is much smaller than the set of the initially \nreturned documents K (|R| << |K|), whereas R is a subset of K. \n\nThis procedere can be compared to something like this in the SQL world: \nSELECT TextValue FROM table1 WHERE IndexValue = \"FOOBAR\" AND \nDISTANCE_FUNCTION(TextValue, \"Query String\") < 0.4 \n\nThere would be an index-based solution if DISTANCE_FUNCTION had only \ndependencies on stored columns (\"functional indexes\", as in PostgreSQL), but in \nthis case, I see no other way than applying some function on every returned \ndocument (something like O(k)) \n\nUnfortunately, my initial dataset (the monolithic .FDT file) was far too big \n(gigabytes) to fit into a RAMDirectory. So seeking and reading from harddisk \nmust be included in the calculations. \n\nSo, I came up with the idea of \"partitioning\" the field data file: Partition \n(\"dataStore\") 0 would be small enough to fit into RAM (having no seek time when \nskipping from one document to another one). Partition 1 will only fit on my \nslow harddisk, but from this partition, I only need the data belonging to the \ndocuments in R, not of all in K. \n\nThat way, I am still in linear costs, but without seeking (and this makes an \nremarkably speedup in search time when you, for example, have to look at \n200,000 entries K of 5,000,000 entries N, just to get some 100 entries R as the \nfinal result, and this per user - with lots of simultaenous requests). \n\nPerhaps something like this could also be implemented using the new TermVector \nsupport somehow,  but I have not thought about it in detail, yet. \n\nRegarding compression issues, I would say that there would be no benefit of \ncompressing the field data values, as they are not traversed sequentially. \n\n\nHowever, another application for \"partitioned\" field data files would simply be \nthe shared storage of document information. You could have partition 0 on a \nRAMDirectory, partition 1 on a local FSDirectory and partition 2 on an \nNFS-mounted FSDirectory, for example. Partition would then only be accessed if \nnecessary (if the user wants more detailed information about a document - for \nexample the original HTML document along with all the other information as in \nGoogle Cache). \n\nCurrently, I would store that data outside lucene and have a filename or URI as \na field value pointing to the data (\"CLOB\"). With the new feature, a simple \nindexReader.document(docNo, 2).get(\"FieldName\") would be enough.  ",
            "author": "Christian Kohlsch\u00c3\u00bctter",
            "id": "comment-12321624"
        },
        {
            "date": "2005-06-02T07:53:02+0000",
            "content": "Wow... this is an awesome patch.  We're having the same problem.  \n\nI actually dived into FieldsReader to see if I could just jump over the fields\nbut the .fdt file being one monolithic beast prevents that because you have to\nseek() over the regions which is just as slow.\n\nThe only downside is that I'd have to reburn indexes which I'm not too excited\nabout. \n\nStill would like to see this patch to mainline.   ",
            "author": "c2ru048",
            "id": "comment-12321625"
        },
        {
            "date": "2005-06-02T07:59:29+0000",
            "content": "Also.. once this patch is accepted is there any reason to NOT store the data in\ndedicated files?\n\nThis only seems to be an advantage...  ",
            "author": "c2ru048",
            "id": "comment-12321626"
        },
        {
            "date": "2006-06-16T02:28:03+0000",
            "content": "Thanks Christian.  I think LUCENE-545 provided the solution to selective field loading now. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12416383"
        }
    ]
}