{
    "id": "LUCENE-7622",
    "title": "Should BaseTokenStreamTestCase catch analyzers that create duplicate tokens?",
    "details": {
        "labels": "",
        "priority": "Major",
        "resolution": "Unresolved",
        "affect_versions": "None",
        "status": "Open",
        "type": "Improvement",
        "components": [],
        "fix_versions": []
    },
    "description": "The change to BTSTC is quite simple, to catch any case where the same term text spans from the same position with the same position length. Such duplicate tokens are silly to add to the index, or to search at search time.\n\nYet, this change produced many failures, and I looked briefly at them, and they are cases that I think are actually OK, e.g. PatternCaptureGroupTokenFilter capturing (..)(..) on the string ktkt will create a duplicate token.\n\nOther cases looked more dubious, e.g. WordDelimiterFilter.",
    "attachments": {
        "LUCENE-7622.patch": "https://issues.apache.org/jira/secure/attachment/12846172/LUCENE-7622.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15807262",
            "date": "2017-01-07T10:38:37+0000",
            "content": "Here's a simple patch ... but I don't plan to pursuing this further now ... I think it's maybe too anal to insist on this from all analyzers ... so I'm posting the patch here in case anyone else gets itchy! ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-15807274",
            "date": "2017-01-07T10:46:59+0000",
            "content": "I agree that by default TokenStreams should not produce duplicate tokens, but there are use cases (boosting) where you might want to do this. E.g., if you want to raise the boost of a term in a document (e.g., if its inside a <em> HTML tag and should have emphasis), you can duplicate the token to increase its frequency (with same position). The alternative would be payloads and payload query, but this is cheap to do.\n\nAlso: If you use ASCIIFoldingFilter or stemming and add the folded/stemmed terms together with the original ones to the index, those terms with no folding/stemming applied would get duplicated. But If you don't do this the statistics would be wrong. I agree, for this case it would be better to have a separate field, but some people like to have it in the same. ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-15807277",
            "date": "2017-01-07T10:49:36+0000",
            "content": "For the above boosting use cases, it would be better to have an additional attribute in TokenStreams that default to 1, but returns a \"frequency\" or \"boost\" if used. Then you could stop cloning the tokens. FYI: I know that BM25 makes this type of boosting harder, but you can still add emphasis on tokens in a text by duplicating them ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-15807508",
            "date": "2017-01-07T13:31:25+0000",
            "content": "BM25 does not make this harder. It just normalizes term frequency in a way that isn't as brain dead as sqrt. And unlike Crappy^H^H^H^HDefaultSimilarity, its totally tunable without modifying source code, e.g. adjust k1 parameter to your needs.\n\nSorry, you are wrong: it only makes this kind of thing way easier. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-15807640",
            "date": "2017-01-07T15:12:32+0000",
            "content": "Hi Robert. I know that you can tune. Maybe I was a bit unclear. I wanted to say that unlike with stupid CrappyDefaultSim it's no longer possible to boost terms more or less unlimited (like a document with 10000 times the same term no longer beats all others). So to repeat terms at same position with a repeater token filter is still useful, but no longer so drastic. So sorry for being unclear. \ud83e\udd13 Maybe I change or remove the last sentence in my comment to remove the misunderstanding. ",
            "author": "Uwe Schindler"
        }
    ]
}