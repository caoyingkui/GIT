{
    "id": "SOLR-314",
    "title": "Store Analyzed token text from an incoming SolrInputDocument",
    "details": {
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [],
        "components": [
            "update"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Won't Fix"
    },
    "description": "This is an UpdateRequestProcessor that runs incoming fields through a Field Analyzer and stores the output of each token as a field value.\n\nFor Example.  If you have a field type defined:\n\n  <fieldType name=\"text_ws\" class=\"solr.TextField\" >\n      <analyzer>\n        <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n      </analyzer>\n  </fieldType>\n\nAnd send a request:\n/update?store.analysis=true&f.feature.analysis=text_ws\n<add> <doc>\n <field name=\"feature\">aaa bbb ccc</field>\n</doc></add>\n\nThe returned document will look like:\n<doc>\n <arr name=\"feature\">\n  <str>aaa</str>\n  <str>bbb</str>\n  <str>ccc</str>\n </arr>\n</doc>",
    "attachments": {
        "SOLR-314-StoreAnalysis.patch": "https://issues.apache.org/jira/secure/attachment/12362291/SOLR-314-StoreAnalysis.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Ryan McKinley",
            "id": "comment-12514428",
            "date": "2007-07-21T18:17:43+0000",
            "content": "This adds the StoreAnalysisProcessor to the default chain.  It is skipped unless the request includes a parameter \"store.analysis=true\"\n\nIt chooses the field type based on a field param: f.fieldname.analyze=FieldTypeName\n\nI'm not totally happy with the field names.  suggestions?\n\n\n\t- - - -\n\n\n\nThe one big issue I'm not sure how to deal with is stitching a multi-valued reqeust into a single TokenStream.\n\nConsider the input \n<add> <doc>\n <field name=\"feature\">aaa bbb ccc</field>\n <field name=\"feature\">bbb ccc ddd</field>\n</doc></add> \n\nAs is, If the FieldType has a 'RemoveDuplicates' filter, that won't remove the duplicates between the fields because each input field gets its own Reader\n\nAny ideas for a way around this?\n\nCan I extract the Tokenizer explicitly?\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12514429",
            "date": "2007-07-21T18:19:55+0000",
            "content": "I think we need to be very careful misleading people into thinking they need something like this\nto search for separate components of a field.  Most people will be best either with normal analysis, or with creating multiple fields themselves if that's what they really desire. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12514431",
            "date": "2007-07-21T18:30:11+0000",
            "content": "> This adds the StoreAnalysisProcessor to the default chain\n\nBased on my previous comments, I think I'd be against adding it to the default chain.\nI still see this as a very rare need.  The norm for stored fields should be \"what you put in, you get back out\". "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12514432",
            "date": "2007-07-21T18:49:09+0000",
            "content": "Right, the point of this is to process stored fields.  Any documentation for this would make the purpose clear and suggest that you will have more flexibility doing the processing on the client side.\n\nI need to find a user configurable way to  have someone process incoming fields.  In some cases that is splitting them into multiple tokens, but in others it is doing things like 'toLowerCase' and remove duplicates.  Rather then build my own interface for this, It would be great to use the existing configurable analyzer framework.\n\nIf this is something that ought to stay of of core, I'm fine with that.  But it does feel generally useful.\n "
        },
        {
            "author": "J.J. Larrea",
            "id": "comment-12514541",
            "date": "2007-07-23T02:04:06+0000",
            "content": "I agree that a stored-field pre-processor would be quite useful, but I'm not sure the proposed scheme is the best way to define and control it... in particular,  f.<field>.analysis=<fieldType> to pull the analyzer definition out of a different fieldType seems like a fragile and hacky construct.  And it blurs what I see as separate concerns, (1) having pre-storage processing part of how a field is handled, versus (2) dynamically changing the handling of a field.   Another valid concern you raise (3) is how to handle duplicate indexed values, but that should apply whether the duplicates arose from tokenization or separate <field>...</field> values.  \n\nI wonder if a more robust implementation of the pre-processing concern would simply be to add another analyzer type \"store\" to the current set \"index\" and \"query\" which can be defined on a fieldType; naturally it wouldn't be in the default set.\n\nFor your example, \n\n  <fieldType name=\"text_ws\" class=\"solr.TextField\" >\n      <analyzer type=\"store,index,query\">\n        <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n      </analyzer>\n  </fieldType>\n\nwould ws-tokenize \"aaa bbb ccc\" and store 3 separate strings.\n\nYou raise the question of how to control the catenation of tokens.  Simple enough to create an UnTokenize token filter which can be added to the tail of any analyzer chain.  It could take arguments for the separator strings to use based on whether tokens are overlapping or not, or better yet, printf formats for both cases.\n\nThat would extend the store analyzer to quite different use-cases... for example, semicolon-delimited author strings can be split, with each author run through your CapitalizationFilter for storage, while for indexing punctuation would be stripped and it would be lower-cased:\n\n\t<fieldType name=\"text_ws\" class=\"solr.TextField\" >\n\t\t<analyzer type=\"store\">\n\t\t\t<tokenizer class=\"solr.PatternTokenizerFactory\" pattern=\";\\s+\"/>\n\t\t\t<filter class=\"solr.CapitalizationFilterFactory\"\n\t\t\t\tonlyFirstWord=\"false\"\n\t\t\t\tkeep=\"and or the is my of for de\"\n\t\t\t\tokPrefix=\"McK\"\n\t\t\t\tforceFirstLetter=\"true\" />\n\t\t\t<filter class=\"solr.UnTokenizerFilterFactory\" adjacent=\"; \"/>\n\t\t</analyzer>\n\t\t<analyzer type=\"index,query\">  <!-- type=\"index,query\" is optional -->\n\t\t\t<tokenizer class=\"solr.PatternTokenizerFactory\" pattern=\"[,;|\\s]+\"/>\n                       ...\n\t\t\t<filter class=\"solr.LowerCaseFilterFactory\"/>\n\t\t</analyzer>\n\t</fieldType>\n\nIn a similar example, stored values could be run through the HyphenatedWordsFilterFactory (and then untokenized) so they reflect what is actually being indexed.\n\nOne could even store the result of analysis (perhaps in a CopyField) as a visual token mapping to help diagnose indexing/analysis problems, concatenated with something on the order of <filter class=\"solr.UnTokenizerFilterFactory\" adjacent=\" \" overlap=\" / \" missing=\"<null>\" /> e.g. \"<null> quick / fast dog / canine jumped ...\"\n\nThen to address the other concern (2) of allowing user-control of field types, one solution would be to recast the StoreAnalysisProcessor as say DynamicFieldTypeProcessor, allowing f.<field>.type=<fieldType> when it is inserted in the chain... e.g. for language-specific analysis, etc.\n\n(It's late, I hope this all makes sense...)\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12514860",
            "date": "2007-07-24T06:07:32+0000",
            "content": "I'm on the same page with the first part of JJs comments, the API seems a awkward and forced.  adding a new analyzer type would be one way to go if we wanted to change things at the schema/doc-processing level \u2013 the approach i was thinking about was just anew FieldType that used it's index analyzer for the stored values as well as the indexed values.\n\ni'm not really understanding most of the dicsussion about concatenating and how that would work \u2013 but i see it as being largely unrelated to the main point of the issue (a way to tokenize and process an input string) because people may want to use an option like that even when sending discrete values \u2013 we should tackle the issues seperately "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12543413",
            "date": "2007-11-18T21:32:19+0000",
            "content": "The functionality I was trying to get can now be achieved with a custom UpdateReqeustProcessor (SOLR-269) \n\nFor now, I don't think we want/need to bake this into the core "
        }
    ]
}