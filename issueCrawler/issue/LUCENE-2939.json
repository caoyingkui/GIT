{
    "id": "LUCENE-2939",
    "title": "Highlighter should try and use maxDocCharsToAnalyze in WeightedSpanTermExtractor when adding a new field to MemoryIndex as well as when using CachingTokenStream",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/highlighter"
        ],
        "type": "Bug",
        "fix_versions": [
            "3.2",
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "huge documents can be drastically slower than need be because the entire field is added to the memory index\nthis cost can be greatly reduced in many cases if we try and respect maxDocCharsToAnalyze\n\nthings can be improved even further by respecting this setting with CachingTokenStream",
    "attachments": {
        "LUCENE-2939.patch": "https://issues.apache.org/jira/secure/attachment/12472081/LUCENE-2939.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2011-02-27T01:18:58+0000",
            "content": "I'm a little rusty on the new tokenstream api, but here is a little test patch I popped out real quick ",
            "author": "Mark Miller",
            "id": "comment-12999859"
        },
        {
            "date": "2011-02-27T01:42:15+0000",
            "content": "hmmm...didn't quite get it right yet I think...\n\nq=java man news th*\n\n\n\njava.lang.NullPointerException\n\tat org.apache.lucene.util.CharacterUtils$Java5CharacterUtils.fill(CharacterUtils.java:181)\n\tat org.apache.lucene.analysis.CharTokenizer.incrementToken(CharTokenizer.java:150)\n\tat org.apache.lucene.analysis.miscellaneous.WordDelimiterFilter.incrementToken(WordDelimiterFilter.java:224)\n\tat org.apache.lucene.analysis.core.LowerCaseFilter.incrementToken(LowerCaseFilter.java:54)\n\tat org.apache.lucene.analysis.miscellaneous.ASCIIFoldingFilter.incrementToken(ASCIIFoldingFilter.java:71)\n\tat com.ACME.analysis.ACMEPluralStemFilter.incrementToken(ACMEPluralStemFilter.java:56)\n\tat org.apache.solr.highlight.TokenOrderingFilter.incrementToken(DefaultSolrHighlighter.java:575)\n\tat org.apache.lucene.analysis.CachingTokenFilter.fillCache(CachingTokenFilter.java:78)\n\tat org.apache.lucene.analysis.CachingTokenFilter.incrementToken(CachingTokenFilter.java:50)\n\tat org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:220)\n\n\n ",
            "author": "Mark Miller",
            "id": "comment-12999863"
        },
        {
            "date": "2011-02-27T02:10:21+0000",
            "content": "only seems to happen when maxDocCharsToAnalyze is absurdly low - like 5 ",
            "author": "Mark Miller",
            "id": "comment-12999866"
        },
        {
            "date": "2011-02-27T04:44:04+0000",
            "content": "i don't know why you get this null pointer exception (maybe you triggered a bug), but...\n\njust a quick glance:\n\n\twhy use offsets for this calculation? This seems a bit dangerous versus other approaches.\n\teither way, the reset() method should clear any state such as counters in the tokenstream.\n\n\n\nAs far as what i meant above... the whole maxDocCharsToAnalyze seems like the wrong measure.\nWhy not specify this just as max tokens, and use LimitTokenCountAnalyzer, which is already implemented.\n\nusing arbitrary chars and offsets is going to create fake tokens (e.g. truncate words) and other problems.\nbesides, its not unicode safe since a codepoint might span multiple chars. ",
            "author": "Robert Muir",
            "id": "comment-12999880"
        },
        {
            "date": "2011-02-27T05:06:09+0000",
            "content": "Because this is already a setting on the Highlighter that appears to work by offset? ",
            "author": "Mark Miller",
            "id": "comment-12999882"
        },
        {
            "date": "2011-02-27T05:13:39+0000",
            "content": "Maybe the setting is already there, but I think we should remove it: I don't think its the best measure.\n\nWe can instead replace it with a max # tokens setting, which is more intuitive, easier to implement,\nand consistent with how other things are limited (e.g. the old IW setting and the new\nLimitTokenCountFilter). ",
            "author": "Robert Muir",
            "id": "comment-12999883"
        },
        {
            "date": "2011-02-27T05:17:26+0000",
            "content": "Fair enough (this setting is well before my time AFAIK) - but not my intent with this issue - which is just to fix this little perf bug. ",
            "author": "Mark Miller",
            "id": "comment-12999884"
        },
        {
            "date": "2011-02-27T17:42:56+0000",
            "content": "The other problem was that CachingTokenFilter was exhausting the entire stream eagerly - which could be a spin through a very large TokenStream - uselessly if a user has set the maxDocCharOffset setting.\n\nThis and adding the whole stream to the MemoryIndex was a very large performance bug in the span highlighter for some time now.\n\nIn my test case, using Solr's DEFAULT_MAX_CHARS_TO_ANALYZE = 50*1024, highlighting 10 very large PDF docs I have dropped from 20 some seconds to 300ms.\n\nNew patch with some fixes and cleanup. I don't see the above error with a more correct TokenFilter impl. ",
            "author": "Mark Miller",
            "id": "comment-13000010"
        },
        {
            "date": "2011-03-03T23:51:14+0000",
            "content": "My last patch is missing a couple required test compile changes - I excluded that class cause I had some test code in it.\n\nI'll put up a new patch as soon as I get a chance with the test class changes (Scorer init method gets a new param and there are a couple anonymous impls in test) ",
            "author": "Mark Miller",
            "id": "comment-13002334"
        },
        {
            "date": "2011-03-03T23:54:00+0000",
            "content": "Honestly, if I was not so busy, I'd say we should really get this in for 3.1.\n\nIf you are doing something like desktop search, this can be a really cruel highlighter perf problem. ",
            "author": "Mark Miller",
            "id": "comment-13002335"
        },
        {
            "date": "2011-03-03T23:58:41+0000",
            "content": "P.S. One that is really a bad bug in my mind - we switched this to be the default and the old Highlighter did not suffer like this in these situations.\n\nLooking back over the email archives, it bit more than a few people. I'm pretty sure this bug was the impetus of the Fast Vector Highlighter (which is still valuable if you really do want to highlight over every token in your 3 billion word PDF file  ).\n\nYou pay this huge perf penalty for no gain and no reason. If you are talking wikipedia size docs, it won't affect you - but for long documents, doing 10 snippets can be prohibitive, with no workaround. That is not a friendly neighborhood highlighter. ",
            "author": "Mark Miller",
            "id": "comment-13002340"
        },
        {
            "date": "2011-03-04T00:57:58+0000",
            "content": "i think the offsetLength calculation needs to be inside the incrementToken?\n\nHonestly, if I was not so busy, I'd say we should really get this in for 3.1.\n\nyeah, performance bugs are bugs too. ",
            "author": "Robert Muir",
            "id": "comment-13002377"
        },
        {
            "date": "2011-03-04T01:47:17+0000",
            "content": "I can backport if you want. ",
            "author": "Grant Ingersoll",
            "id": "comment-13002394"
        },
        {
            "date": "2011-03-04T02:07:26+0000",
            "content": "i think the offsetLength calculation needs to be inside the incrementToken?\n\nI do not follow ... incrementToken is:\n\n+  @Override\n+  public boolean incrementToken() throws IOException {\n+    int offsetLength = offsetAttrib.endOffset() - offsetAttrib.startOffset();\n+    if (offsetCount < offsetLimit && input.incrementToken()) \n{\n+      offsetCount += offsetLength;\n+      return true;\n+    }\n+    return false;\n+  } ",
            "author": "Mark Miller",
            "id": "comment-13002400"
        },
        {
            "date": "2011-03-04T02:11:48+0000",
            "content": "Exactly, so what is the attributes values before calling input.incrementToken() ?\n\nI don't think this is good practice to work with the uninitialized values. ",
            "author": "Robert Muir",
            "id": "comment-13002402"
        },
        {
            "date": "2011-03-04T02:19:51+0000",
            "content": "This includes the change to the test to make it compile.\n\nStill no Changes entry.\n\nThe compile change to the test is a back compat break. The Scorer needs to know the maxCharsToAnalyze setting.\n\nHave not had time to consider further yet. ",
            "author": "Mark Miller",
            "id": "comment-13002406"
        },
        {
            "date": "2011-03-04T02:25:53+0000",
            "content": "I can backport if you want.\n\n+1\n\nI don't think this is good practice to work with the uninitialized values.\n\nI see what you mean now - though I still don't understand your previous comment.\nI assume that it's just defaulting to 0 - 0 now?\n\nYeah, that could be changed. ",
            "author": "Mark Miller",
            "id": "comment-13002407"
        },
        {
            "date": "2011-03-04T03:29:29+0000",
            "content": "\nI see what you mean now - though I still don't understand your previous comment.\nI assume that it's just defaulting to 0 - 0 now?\n\nOnly the first time.\n\nBut imagine you try to reuse this tokenstream (maybe its not being reused now, but in the future)... the values for the last token of the previous doc are say 10 - 5... the consumer calls reset(Reader) with new document and reset(), which clears your accumulator, but this attribute is still 10 - 5 until input.incrementToken()... only then does the tokenizer update the values. ",
            "author": "Robert Muir",
            "id": "comment-13002433"
        },
        {
            "date": "2011-03-04T12:20:50+0000",
            "content": "Given the back compat breaks in the API, are we sure we should try to shove this into 3.1?\n\nI am sympathetic to performance bugs, BUT it seems that one could use TermVectors and FastVectorHighlighter for these large documents, the user is hardly left without options.\n\nAs a safer alternative we can document the issue in CHANGES.txt and recommend that users take that approach for large documents, and take our time and fix for 3.2 ",
            "author": "Robert Muir",
            "id": "comment-13002562"
        },
        {
            "date": "2011-03-04T12:54:45+0000",
            "content": "Given the back compat breaks in the API, are we sure we should try to shove this into 3.1?\n\nI won't do the work, so whatever form my perspective...\n\nthe user is hardly left without options.\n\nDepends on what you mean by options - FastVectorHighlighter cannot highlight half our queries (multi-term last I knew, or Span) - trade one bug for anther. ",
            "author": "Mark Miller",
            "id": "comment-13002576"
        },
        {
            "date": "2011-03-04T12:58:59+0000",
            "content": "I'm OK either way, but it does seem like a pretty big performance bug. ",
            "author": "Grant Ingersoll",
            "id": "comment-13002579"
        },
        {
            "date": "2011-03-04T12:59:59+0000",
            "content": "\nDepends on what you mean by options - FastVectorHighlighter cannot highlight half our queries (multi-term last I knew, or Span) - trade one bug for anther.\n\nRight, but you can use term vectors with this highlighter too right?\nThis issue only seems to refer to the case where you have no term vectors and analyze the text at runtime...\n\nI don't think its too much to say 'index your content according to what you are going to need' ",
            "author": "Robert Muir",
            "id": "comment-13002580"
        },
        {
            "date": "2011-03-04T13:02:31+0000",
            "content": "Grant: in terms of the back compat issue - I'm not really worried about it myself since this is contrib and we have changed these interfaces before with no complaint -\n\nbut another tmp option is to special case and do an instanceOf check on the Scorer - and if its our QueryScorer, cast and set the max chars to analyze.\n\nIt's not as pretty, but it avoids the method sig change. ",
            "author": "Mark Miller",
            "id": "comment-13002582"
        },
        {
            "date": "2011-03-04T13:03:18+0000",
            "content": " Right, but you can use term vectors with this highlighter too right?\nThis issue only seems to refer to the case where you have no term vectors and analyze the text at runtime...\n\nNope, that's not true. If you turn on term vectors, that does NOT solve this bug. ",
            "author": "Mark Miller",
            "id": "comment-13002583"
        },
        {
            "date": "2011-03-04T13:06:47+0000",
            "content": "Anyhow, all I have time for on this today.\n\nI'll leave it up to you guys...err, I mean robert, to decide what to do here. ",
            "author": "Mark Miller",
            "id": "comment-13002585"
        },
        {
            "date": "2011-03-04T13:08:06+0000",
            "content": "\nI'll leave it up to you guys...err, I mean robert, to decide what to do here.\n\nSorry you feel this way... everyone says they want faster releases but doesn't want to take the appropriate steps to move towards a model that supports that.\n\nIn order to release more often we have to stop this cycle of shoving things in at the last minute. ",
            "author": "Robert Muir",
            "id": "comment-13002588"
        },
        {
            "date": "2011-03-04T13:13:21+0000",
            "content": "In order to release more often we have to stop this cycle of shoving things in at the last minute.\n\nAs always in Lucene land, these things should be taken case by case depending on the facts - the severity of the bug and its affect on the release. Tese things can often be discussed by more than a single person.\n\nNot ramrodded by someone being a bit of an asshole. ",
            "author": "Mark Miller",
            "id": "comment-13002594"
        },
        {
            "date": "2011-03-04T13:18:44+0000",
            "content": "Not ramrodded by someone being a bit of an asshole.\n\nTese things can often be discussed by more than a single person.\n\nYes, anyone can can produce a release candidate of Lucene. But if its going to be me doing it, i've already set aside time (and coordinated with others) to make RC builds. So I'm going to push back on shoving in last minute changes.\n\nWell you can call it that, or someone trying to be a release manager that will actually get out a release in the next year.\n\nBottom line: if you feel this change is really important, I respect your decision on that. But you should set the issue to blocker and be aware that the tradeoff likely means delaying the RC for a few weeks (unless someone else steps up to volunteer to produce an RC, which is fine!)\n\n ",
            "author": "Robert Muir",
            "id": "comment-13002600"
        },
        {
            "date": "2011-03-04T13:26:01+0000",
            "content": "What you don't seem to get is that I don't mind if you push back. I don't mind your position.\n\nI mind your attitude. Changing the issue target 2 seconds after Grant with no discussion. Declaring on your own that it won't get in. Not trying to get to a real conversation about the issue (which you clearly don't fully understand if you think storing term vectors will help). These things are my issue, not any so called push back.\n\nWell man, you need us on your team too. Performance bug is a technical valid reason for a -1 on a release. I'm not threatening that - but I'm pointing out that everyone needs to be on board - not just the RM. Taking the time for fair discussion is not a waste of time. ",
            "author": "Mark Miller",
            "id": "comment-13002601"
        },
        {
            "date": "2011-03-04T13:33:20+0000",
            "content": "\nI mind your attitude. Changing the issue target 2 seconds after Grant with no discussion. Declaring on your own that it won't get in. Not trying to get to a real conversation about the issue (which you clearly don't fully understand if you think storing term vectors will help). These things are my issue, not any so called push back.\n\nIts not an attitude, and its not personal. Its trying to stop last minute stuff from being shoved into the release right before the RC, especially if its not fully-formed patches ready to be committed.\n\n\nWell man, you need us on your team too. Performance bug is a technical valid reason for a -1 on a release. I'm not threatening that - but I'm pointing out that everyone needs to be on board - not just the RM. Taking the time for fair discussion is not a waste of time.\n\nI totally agree with you here. But some people might say, if the bug has been aroudn since say 2.4 or 2.9 that its not critical that it be fixed in 3.1 at the last minute, and still +1 the release.\n\nAs i stated earlier on this issue, I'm sympathetic to performance bugs: performance bugs are bugs too. But we need to evaluate risk-reward here.\n\nJust don't forget that there are other performance problems with large documents in lucene (some have been around a while) and we aren't trying to shove any last minute fixes for those in.\n\nSo, here are my questions:\n\n\tWhat version of Lucene was this performance bug introduced in? Is it something we introduced in version 3.1? If this is the case its more serious than if its something thats been around since 2.9.\n\tWhy is fast-vector highlighter with TVs \"ok\", but highlighter with TVs slow?\n\n ",
            "author": "Robert Muir",
            "id": "comment-13002602"
        },
        {
            "date": "2011-03-04T13:39:18+0000",
            "content": "I think Robert's right, we should not have shoved this in at the last minute, even though it is a pretty big issue for those doing highlighting of larger documents.  I'd say we just mark it as 3.1.1 or 3.2. ",
            "author": "Grant Ingersoll",
            "id": "comment-13002604"
        },
        {
            "date": "2011-03-04T13:51:03+0000",
            "content": "I think 3.2 is a good tradeoff, unless we introduced this slowdown in 3.1 (my earlier question).\n\nIf we are introducing this slowdown in the 3.1 release, then I think its much more serious, and I would instead suggest we set the issue to blocker.\n\nRegardless I think there are some technical steps that can be taken to easy my mind about the patch, for example the TokenFilter here can be tested independently with BaseTokenStreamTestCase (this is good at catching reuse bugs like the one I hinted at). ",
            "author": "Robert Muir",
            "id": "comment-13002607"
        },
        {
            "date": "2011-03-04T13:57:27+0000",
            "content": "we should not have shoved this in at the last minute,\n\nWe didn't? Marking something as 3.1 is the best way to get it considered for last minute inclusion, blocker or not. It certainly doesn't mean its not going to be pushed back out after discussion.\n\nIn any case, if you are not for it, that decides it - I'm not willing to do the work right now.\n\nSo, here are my questions:\n\n1. I don't remember 2.9 probably.\n2. Because it's a completely different approach.\n\nIt's been around for a while. I saw one guy that stayed on Solr 1.3 over 1.4 because of it. Most people will try fast vector and say oh nice, it's fast - but it doesn't highlight wildcard queries or these queries, etc. They either accept one bug over the other, or stick with an older version. Honestly, if that continues for another release, it's no skin off my nose. But neither are most bugs.\n ",
            "author": "Mark Miller",
            "id": "comment-13002609"
        },
        {
            "date": "2011-03-04T17:03:17+0000",
            "content": "In order to release more often we have to stop this cycle of shoving things in at the last minute\n\n+1\n\nDev is a constant thing around here and we keep holding back a release\nfor the one-more-issue to get in we will never release. \n\nOur lack-of-release reflects badly on Lucene/Solr \u2013 the outside world\nuses this as the proxy for our health and we know we get bad marks.\n\nWorse, this whole situation (people getting angry at the RM for doing\nprecisely what the RM is supposed to do) is a disincentive for\nfuture RMs to volunteer doing releases, thus causing even less\nfrequent releases.  It's already hard enough for us to get a release\nout as it is.\n\nThe RM is supposed to be an asshole (not that Robert has acted like\none, here, imho).  S/he has full authority to draw the line, crack the\nwhip, do whatever it takes to get the release out.  We all cannot\nquestion that, unless we want to step up and be the RM because it is\nNOT an easy job.\n\nI think this issue should wait for 3.2. ",
            "author": "Michael McCandless",
            "id": "comment-13002716"
        },
        {
            "date": "2011-03-04T17:29:13+0000",
            "content": "\nWorse, this whole situation (people getting angry at the RM for doing\nprecisely what the RM is supposed to do) is a disincentive for\nfuture RMs to volunteer doing releases, thus causing even less\nfrequent releases. It's already hard enough for us to get a release\nout as it is.\n\nI'm not sure I agree. Declaring that this will not get in is beyond the scope of an RM in my opinion. Putting pressure is fine, but just because being an RM is hard is not a King for a day pass IMO. It's up to the RM to build the release candidate from whatever issues he wants - does that mean he needs to man handle JIRA?\n\n\n\nThe RM is supposed to be an asshole (not that Robert has acted like\none, here, imho).\n\nI don't think he is? Too strong a word. I didn't even use it full to refer to the 2 actions I was commenting on. First, I said a acting like a bit of an asshole and I separated it a distance as theoretical. It was weak sauce commentary on the two actions I've pointed out:\n\n1. Just reverting a change another respected committer made immediately and without discussion - without too much investigation - because that issue was subsumed by another issue that had already been moved for 3.1 consideration and we where still discussing.\n\n2. Declaring that this will not make it in over ongoing discussion.\n\nS/he has full authority to draw the line, crack the\nwhip, do whatever it takes to get the release out.\n\nDo whatever it takes? Come on...\n\nWe all cannot question that, unless we want to step up and be the RM because it is NOT an easy job.\n\nI do question it. I have stepped up to be the RM, I know it's not an easy job, and I'll volunteer to do it again sometime.\n\nRobert is great at it - in general he has all my support in the world. I certainly understand the difficulties hes facing trying to point this release and he has my sympathies - and I wish he had more of my help. But that doesn't change my reaction to his actions. I feel the same way and I have the same responses to it. Meanwhile, I still like and respect Robert. ",
            "author": "Mark Miller",
            "id": "comment-13002730"
        },
        {
            "date": "2011-03-04T22:02:01+0000",
            "content": "\nPutting pressure is fine, but just because being an RM is hard is not a King for a day pass IMO\n\nPutting pressure is precisely what Robert has done here (and on\nSOLR-2390)?\n\nHe's acting just like an RM should act, as far as I can tell.  Moving\nan issue out, stating that an issue won't make the RC,\nis fair game.  These are the normal \"tools\" of the RM...\n\nMy point is, we all must expect/allow/not-get-upset-about this\n\"pressure\" from the RM \u2013 it comes with the territory, and it's the\nRM's right to be very aggressive in order to get the release done.\n\nElse releases will not get done, and we'll all keep one-more-thing'ing\nthe release, and that's bad for all of us.\n\nWe gotta remove the barriers to doing releases around here, not add to\nthem.  In fact, we should scrutinize our scary ReleaseTodo and pare it\nback to the bare minimum... it's gotta become a push button process. ",
            "author": "Michael McCandless",
            "id": "comment-13002838"
        },
        {
            "date": "2011-03-05T00:01:02+0000",
            "content": "Moving\nan issue out, stating that an issue won't make the RC,\nis fair game. These are the normal \"tools\" of the RM...\n\nNot in my experience. \n\nRegardless, I'm not sure this is the same as changing a JIRA issue right after someone else changes it with no discussion and apparent lack of understanding of the issue. That's a statement if you ask me. If you look at how the culture of Lucene has worked, this is unusual - and I'll push to make it remain so.\n\nPutting pressure is precisely what Robert has done here (and onSOLR-2390)?\n\nSOLR-2390 was this issue - this patch spans lucene and solr and covers both of them. This issue was still marked 3.1 and we where discussing it when this happened with SOLR-2390 - this is how I know little thought went into shoving it out - SOLR-2390 should be in lock step with this issue. It's not a new or another issue. It's just where I am tracking this from a Solr user bug perspective - it's easier to have one patch.\n\nMy point is, we all must expect/allow/not-get-upset-about this\n\"pressure\" from the RM \u2013 it comes with the territory, and it's the\nRM's right to be very aggressive in order to get the release done.\n\nDepends - I've seen a lot of RM's before - and I have been one. Personally I've never seen things done this way. Nor do I think it was necessary. We would have come to the same conclusion in either case. The history and culture of Lucene has been to not be forceful in JIRA - that's something I'll argue to maintain.\n\n\nWe gotta remove the barriers to doing releases around here, not add to\nthem.\n\nRelease at all costs is just not an excuse IMO. We release as often as someone is willing to put in the somewhat massive effort really. \n\nIn fact, we should scrutinize our scary ReleaseTodo and pare it\nback to the bare minimum... it's gotta become a push button process.\n\nI think we all agree with that. I'm still not aboard with the scary RM theory. ",
            "author": "Mark Miller",
            "id": "comment-13002877"
        },
        {
            "date": "2011-03-05T00:07:13+0000",
            "content": "I will also note, there was never any strong argument to include this issue.\n\nThere was never any danger of this needing to be strong armed out of 3.1.\n\nI've already said I wouldn't do it - and Grant had volunteered, but never argued for it either. ",
            "author": "Mark Miller",
            "id": "comment-13002879"
        },
        {
            "date": "2011-03-05T00:41:46+0000",
            "content": "Sorry if I missed it in this thread, which branch was this patch made against?  It doesn't apply cleanly against branch_3x. ",
            "author": "Trey Hyde",
            "id": "comment-13002883"
        },
        {
            "date": "2011-03-05T00:54:44+0000",
            "content": "Sorry if I missed it in this thread, which branch was this patch made against? It doesn't apply cleanly against branch_3x.\n\nThis patch is against trunk - still needs a fairly simple back port to 3x. ",
            "author": "Mark Miller",
            "id": "comment-13002893"
        },
        {
            "date": "2011-03-05T02:34:24+0000",
            "content": "The patch needs more than a simple back port.\n\nThe patch needs to be fixed. It has tokenstream reuse bugs that cause offsets from last token of the previous document to be applied to the calculations of the next document, because it reads dirty attributes.\n\nIts not just release manager being an asshole here, there are technical problems that need to be fixed. ",
            "author": "Robert Muir",
            "id": "comment-13002910"
        },
        {
            "date": "2011-03-05T03:15:34+0000",
            "content": "The patch needs more than a simple back port. The patch needs to be fixed.\n\nAnd that is simple too if you follow the above comments.\n\nYou should pop the offset calculation into the if statement - \n\nI'm not convinced it's a problem in this situation (especially for someone wanting to try a patch), because this works one document at a time.\n\nIts also simple not to break the api as I mention above.\n\nI have done all of these things in my own work earlier (and added a test for the new filter) - took about 2 minutes.\n\nEventually I will post another trunk patch.\n\nDoing a solid review and back port of this patch would not take long - it's fairly simple. I won't likely get to it for 3.X for a while though. ",
            "author": "Mark Miller",
            "id": "comment-13002917"
        },
        {
            "date": "2011-03-05T07:36:25+0000",
            "content": "here is a more up to date version of the patch for trunk - good for testing performance difference of this issue ",
            "author": "Mark Miller",
            "id": "comment-13002946"
        },
        {
            "date": "2011-04-11T12:29:16+0000",
            "content": "Mark,\n\nSeems like we can move forward with this now that the release is out.  Do you have time or do you want me to take it? ",
            "author": "Grant Ingersoll",
            "id": "comment-13018319"
        },
        {
            "date": "2011-04-13T16:31:14+0000",
            "content": "Okay - I'm going to commit to trunk shortly. ",
            "author": "Mark Miller",
            "id": "comment-13019421"
        },
        {
            "date": "2011-06-03T16:37:16+0000",
            "content": "Bulk closing for 3.2 ",
            "author": "Robert Muir",
            "id": "comment-13043498"
        }
    ]
}