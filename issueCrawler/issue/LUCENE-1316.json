{
    "id": "LUCENE-1316",
    "title": "Avoidable synchronization bottleneck in MatchAlldocsQuery$MatchAllScorer",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/query/scoring"
        ],
        "type": "Bug",
        "fix_versions": [
            "2.9"
        ],
        "affect_versions": "2.3",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "The isDeleted() method on IndexReader has been mentioned a number of times as a potential synchronization bottleneck. However, the reason this  bottleneck occurs is actually at a higher level that wasn't focused on (at least in the threads I read).\n\nIn every case I saw where a stack trace was provided to show the lock/block, higher in the stack you see the MatchAllScorer.next() method. In Solr paricularly, this scorer is used for \"NOT\" queries. We saw incredibly poor performance (order of magnitude) on our load tests for NOT queries, due to this bottleneck. The problem is that every single document is run through this isDeleted() method, which is synchronized. Having an optimized index exacerbates this issues, as there is only a single SegmentReader to synchronize on, causing a major thread pileup waiting for the lock.\n\nBy simply having the MatchAllScorer see if there have been any deletions in the reader, much of this can be avoided. Especially in a read-only environment for production where you have slaves doing all the high load searching.\n\nI modified line 67 in the MatchAllDocsQuery\nFROM:\n  if (!reader.isDeleted(id)) {\nTO:\n  if (!reader.hasDeletions() || !reader.isDeleted(id)) {\n\nIn our micro load test for NOT queries only, this was a major performance improvement.  We also got the same query results. I don't believe this will improve the situation for indexes that have deletions. \n\nPlease consider making this adjustment for a future bug fix release.",
    "attachments": {
        "LUCENE_1316.patch": "https://issues.apache.org/jira/secure/attachment/12384773/LUCENE_1316.patch",
        "LUCENE-1316.patch": "https://issues.apache.org/jira/secure/attachment/12397819/LUCENE-1316.patch",
        "MatchAllDocsQuery.java": "https://issues.apache.org/jira/secure/attachment/12384679/MatchAllDocsQuery.java"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-06-25T15:58:56+0000",
            "content": "My version of MatchAlldocsQuery.java which has the modification in it. ",
            "author": "Todd Feak",
            "id": "comment-12608088"
        },
        {
            "date": "2008-06-25T16:43:09+0000",
            "content": "Further investigation indicates that the ValueSourceQuery$ValueSourceScorer may suffer from the same issue and benefit from a similar optimization. ",
            "author": "Todd Feak",
            "id": "comment-12608107"
        },
        {
            "date": "2008-06-25T18:00:47+0000",
            "content": "Although this doesn't solve the general problem, this probably still makes sense to do now for the no-deletes case.\nTodd, can you produce a patch?  See http://wiki.apache.org/lucene-java/HowToContribute ",
            "author": "Yonik Seeley",
            "id": "comment-12608128"
        },
        {
            "date": "2008-06-25T18:01:23+0000",
            "content": "rather then attempting localized optimizations of individual Query classes, a more generalized improvements would probably be to change SegmentReader.isDeleted so that instead of the entire method being synchronized, it first checks if the segment has any deletions, and if not then enters a synchronized block to check deletedDocs.get. ",
            "author": "Hoss Man",
            "id": "comment-12608129"
        },
        {
            "date": "2008-06-25T18:08:01+0000",
            "content": "I like Hoss' suggestion better. I'll try that fix locally and if it provides the same improvement, I will submit a patch for you. ",
            "author": "Todd Feak",
            "id": "comment-12608132"
        },
        {
            "date": "2008-06-25T18:13:51+0000",
            "content": "> a more generalized improvements would probably be to change SegmentReader.isDeleted so that instead of the entire method being synchronized\n\nRight, but that's not totally back compatible.  Code that depended on deletes being instantly visible across threads would no longer be guaranteed. ",
            "author": "Yonik Seeley",
            "id": "comment-12608134"
        },
        {
            "date": "2008-06-25T18:23:26+0000",
            "content": "Code that depended on deletes being instantly visible across threads would no longer be guaranteed.\n\nyou lost me there ... why would deletes be stop being instantly visible if we changed this...\n\n\n  public synchronized boolean isDeleted(int n) {\n    return (deletedDocs != null && deletedDocs.get(n));\n  }\n\n\n\n...to this...\n\n\n  public boolean isDeleted(int n) {\n    if (null == deletedDocs) return false;\n    synchronized (this) { return (deletedDocs.get(n)); }\n  }\n\n\n\n? ",
            "author": "Hoss Man",
            "id": "comment-12608137"
        },
        {
            "date": "2008-06-25T18:42:37+0000",
            "content": "According to the java memory model, hasDeletions() would need to be synchronized as well , since if another thread did perform a deletion, it would need to be up to date.\n\nThis might work in later JVMs by declaring the deletedDocs variable volatile, but no guarantees.\n\nSeems better to ALLOW this behavior, that a reader might not see up to date deletions made during a query, and do a single synchronized check of deletions at the start.\n ",
            "author": "robert engels",
            "id": "comment-12608146"
        },
        {
            "date": "2008-06-25T18:42:52+0000",
            "content": "why would deletes be stop being instantly visible\n\nIt's minor, but before, if thread A deleted a document, and then thread B checked if it was deleted, thread B was guaranteed to see that it was in fact deleted.\n\nIf the check for deletedDocs == null were moved outside of the synchronized, there's no guarantee when thread B will see (if ever) that deletedDocs has been set (no memory barrier).\n\nIt's not a major issue since client code shouldn't be written that way IMO, but it's worth factoring into the decision. ",
            "author": "Yonik Seeley",
            "id": "comment-12608147"
        },
        {
            "date": "2008-06-25T18:44:54+0000",
            "content": "According to\n\nhttp://www.ibm.com/developerworks/java/library/j-jtp06197.html\n\ndeclaring the deletedDocs volatile should do the trick. ",
            "author": "robert engels",
            "id": "comment-12608148"
        },
        {
            "date": "2008-06-25T18:49:20+0000",
            "content": "The Pattern#5 referenced (cheap read-write lock) is exactly what is trying to be accomplished. ",
            "author": "robert engels",
            "id": "comment-12608149"
        },
        {
            "date": "2008-06-25T19:23:20+0000",
            "content": "declaring the deletedDocs volatile should do the trick.\n\nRight... that would be cheaper when no docs were deleted.  But would it be more expensive when there were deleted docs (a volatile + a synchronized?)  I don't know if lock coarsening could do anything with this case... ",
            "author": "Yonik Seeley",
            "id": "comment-12608160"
        },
        {
            "date": "2008-06-25T19:28:55+0000",
            "content": "If I remember correctly, volatile does not work correctly until java 1.5. At best I think it was implementation dependent with the old memory model.\n\nedit\n\nmaybe its ok under certain circumstances:\n\nhttp://www.ibm.com/developerworks/library/j-jtp02244.html\n\nProblem #2: Reordering volatile and nonvolatile stores ",
            "author": "Mark Miller",
            "id": "comment-12608162"
        },
        {
            "date": "2008-06-25T20:49:30+0000",
            "content": "if thread A deleted a document, and then thread B checked if it was deleted, thread B was guaranteed to see that it was in fact deleted.\n\nHmmm.... i'll take your word for it, but i don't follow the rational: the current synchronization just ensured that either the isDeleted() call will complete before the delete() call started or vice versa \u2013 but you have no guarantee that thread B would run after thread A and get true.   .... unless... is your point that without synchronization on the null check there's no garuntee that B will ever see the change to deletedDocs even if it does execute after delete() ?\n\neither way: robert's point about hasDeletions() needing to be synchronized seems like a bigger issue \u2013 isn't that a bug in the current implementation?  assuming we fix that then it seems like the original issue is back to square one: synchro bottlenecks when there are no deletions.\n\n\n ",
            "author": "Hoss Man",
            "id": "comment-12608183"
        },
        {
            "date": "2008-06-25T21:01:42+0000",
            "content": "Hoss, that is indeed the case, another thread would see deletedDocs as null, even though another thread has set it\n\nhasDeletions does not need to be synchronized if deletedDocs is volatile ",
            "author": "robert engels",
            "id": "comment-12608187"
        },
        {
            "date": "2008-06-25T21:03:11+0000",
            "content": "is your point that without synchronization on the null check there's no garuntee that B will ever see the change to deletedDocs even if it does execute after delete()\n\nRight... it's about the memory barrier.\n\nThe reality is that there is normally a need for higher level synchronization anyway.  That's why it was always silly for things like StringBuffer to be synchronized.\n\nassuming we fix that then it seems like the original issue is back to square one: synchro bottlenecks when there are no deletions.\n\nA scorer could just check once when initialized... there's never been any guarantees about in-flight queries immediately seeing deleted docs changes - now that really doesn't make sense.  TermScorer grabs the whole bit vector at the start and never checks again. ",
            "author": "Yonik Seeley",
            "id": "comment-12608189"
        },
        {
            "date": "2008-06-26T15:25:34+0000",
            "content": "I wanted to share my micro load test results with you, to make sure you all understand scale of the bottleneck as we are experiencing it.\n\nFor an optimized index with 4700+ documents (ie small), a NOT query varies by a factor of 35 under heavy load. Using 2.3.0 release I got 20 tps. With the volatile/synchronized fix suggested, I got 700 tps. The limiting factor on the 700 tps was the CPU on the computer throwing load, so this may be even worse. Furthermore, the more documents that exist in the index, the worse this may get, as it synchonizes on every single iteration through the loop.\n\nAn argument can be made that this is just a necessary evil, and that we must synchronize on this for the possibility of updates during reads. I have 2 questions regarding that.\n\n1. What is the cost of a dirty read in that case? Is it stale data, incorrect data, or a corrupted system?\n2. What is more prevalent in a production system. Indexes with no deletes, indexes with some deletes, or indexes with frequent deletes?\n\nDo we need to have 1 class that does it all, or should we consider 2 different implementation for 2 different uses. What about a read-only SegmentReader for read-only slaves in production environments?\n\n ",
            "author": "Todd Feak",
            "id": "comment-12608469"
        },
        {
            "date": "2008-06-26T15:43:54+0000",
            "content": "20tps (queries per second?) for 4700 docs is very slow... how many threads were you testing with? ",
            "author": "Yonik Seeley",
            "id": "comment-12608472"
        },
        {
            "date": "2008-06-26T15:54:32+0000",
            "content": "10 threads in JMeter throwing load at the Tomcat as fast as possible. The Tomcat was on a separate machine with more then 10 worker threads, though only 10 were in use at any one time.\n\nTo eliminate any differences, the tests were run back to back. The only difference was the lucene-core JAR and a Tomcat bounce between the tests. Otherwise, the same exact test is run in both cases. What you have is threads all trying to synchronize on isDeleted() 4700+ times per request. Lock contention goes through the roof. At any point during the test I can take a thread stack snapshot and they are all blocked waiting for isDeleted(). ",
            "author": "Todd Feak",
            "id": "comment-12608475"
        },
        {
            "date": "2008-06-26T17:44:18+0000",
            "content": "Attaching prototype patch (needs javadoc + tests if approach is acceptable) that avoids all synchronization when iterating over all documents.\n\nIf IndexReader.termDocs(Term term) is called with null, a TermDocs implementation is returned that matches all documents.  This is the same approach used by TermScorer via SegmentTermDocs to avoid synchronization by grabbing the BitVector of deleted docs at instantiation.\n\nThis patch also updates MatchAllDocuments to use this TermDocs to iterate over all documents.\n\nAdvantages:\n\n\tadds no new methods or interfaces, simply adds extra semantics to an existing method\n\tworks from the bottom-up... no need to instantiate a big BitVector\n\texposes the functionality to expert users for use in custom queries\n\tavoids a binary search to find the correct IndexReader in a MultiReader for each call (it leverages all the TermDocs code in all IndexReader implementations such as MultiTermDocs).\n\n\n\nDisadvantages:\n\n\tTermDocs instance returned cannot be used to seek to a different term.  However, this is minor and not a back compatibility concern since \"null\" was not previously a supported value.\n\n\n\nOn balance, I think it's 10% hack, 90% useful.  Thoughts? ",
            "author": "Yonik Seeley",
            "id": "comment-12608521"
        },
        {
            "date": "2008-06-26T18:21:50+0000",
            "content": "I applied that patch locally to a 2.3.0 build. Test results show this solution performs equally as the other solution did. I'd be happy with it for that reason alone. I cannot argue as to the quality of the proposed solution. I will leave that to those more knowledgeable on Lucene itself. Thank you for the time you guys have put into this issue for me. ",
            "author": "Todd Feak",
            "id": "comment-12608537"
        },
        {
            "date": "2008-06-26T18:42:26+0000",
            "content": "Test results show this solution performs equally as the other solution did.\n\nThat's good news (as I assume this was for an optimized index).\nWould it be possible for you to try the same test on a non-optimized index (multi-segment) with a couple deletions? ",
            "author": "Yonik Seeley",
            "id": "comment-12608542"
        },
        {
            "date": "2008-06-27T16:53:57+0000",
            "content": "I don't think that patch provides correct functionality. I went to run the load tests this morning against an un-optimized index and the query results do not match what an unpatched version does. Simply swapping the JAR and restarting returns different results for the same query. Specifically, empty (incorrect) results. ",
            "author": "Todd Feak",
            "id": "comment-12608835"
        },
        {
            "date": "2008-06-27T17:00:19+0000",
            "content": "I'll look into it (that's why it was marked as \"prototype\") ",
            "author": "Yonik Seeley",
            "id": "comment-12608841"
        },
        {
            "date": "2008-06-27T17:06:36+0000",
            "content": "I saw and fixed a bug that would affect multireaders.... patch attached.\nI've not yet written tests, so no guarantees.\nEdit: I reproduced... it still doesn't work, so hold off using this. ",
            "author": "Yonik Seeley",
            "id": "comment-12608844"
        },
        {
            "date": "2008-06-27T17:25:12+0000",
            "content": "Sigh... the problem is that things are done in a two step process by default.\nA TermDocs is created, and then seek is called (by which time the impl is set already). \n\n\n  public TermDocs termDocs(Term term) throws IOException {\n    ensureOpen();\n    TermDocs termDocs = termDocs();\n    termDocs.seek(term);\n    return termDocs;\n  }\n\n  public abstract TermDocs termDocs() throws IOException;\n\n\n\n So a full solution down this road would be slightly more involved  (overriding termDocs(Term) in all the sublcasses rather than just termDocs())\n ",
            "author": "Yonik Seeley",
            "id": "comment-12608849"
        },
        {
            "date": "2008-06-27T19:47:49+0000",
            "content": "Here's an updated patch that should work correctly. ",
            "author": "Yonik Seeley",
            "id": "comment-12608897"
        },
        {
            "date": "2008-06-27T20:22:17+0000",
            "content": "I applied the patch to the 2.3.0 file. I ran against an optimized and non-optimized (12 segment) index with 4700 entries.\n\n2.3.0 non-optimized index  104 tps\n2.3.0 patched non-optimized index 482 tps\n\n2.3.0 optimized index 21 tps\n2.3.0 patched optimized index 718 tps\n\nThe patch provided improvements in both optimized and unoptimized indexes. Thanks again Yonik. ",
            "author": "Todd Feak",
            "id": "comment-12608907"
        },
        {
            "date": "2008-06-28T00:56:56+0000",
            "content": "TermDocs instance returned cannot be used to seek to a different term. However, this is minor and not a back compatibility concern since \"null\" was not previously a supported value.\n\nso essentially this approach only improves MatchAllDocsQuery correct? .... Other use cases where lots of termDoc enumeration take place (RangeFilter and PrefixFilter type code) wouldn't' benefit from this at all.\n\nAssuming the genuinely eliminating the synchronization is infeasible, the other approach that occurred to me along the lines of a \"read only\" IndexReader is that if we started providing a public method for getting the list of all deleted docs (either as a BitVector or as a DocIdSet or something) then it would be easy to implement a SnapshotFilteredIndexReader that on construction cached the current list of all deleted docs in the IndexReader it's wrapping, used it for all isDeleted() methods, and proxied all other methods to the underlying reader.\n\nthen things like MatchAllDocs, RangeFilter, and PrefixFilter could (optionally?) construct one of those prior to their big iteration loops, and use it in place of the original IndexReader.  Trade the syncro bottleneck for deletion data as of the moment the query was started (a fair trade for most people) ",
            "author": "Hoss Man",
            "id": "comment-12608956"
        },
        {
            "date": "2008-07-03T01:25:29+0000",
            "content": "so essentially this approach only improves MatchAllDocsQuery correct?\n\nIt would essentially simulate a term indexed for every document, and improve any query that could use that (i.e. that needed to iterate over all documents).  For things like MatchAllDocuments and FunctionQuery on a MultiReader, it should actually be superior to the elimination of all synchronization on isDeleted() since it also removes  the binary search to find the correct reader for a document.\n\nOther use cases where lots of termDoc enumeration take place (RangeFilter and PrefixFilter type code) wouldn't' benefit from this at all.\n\nRight, but using TermDocs, they already have the same style of optimization and hence suffer no synchronization overhead.\n\nthe other approach that occurred to me along the lines of a \"read only\" IndexReader is that if we started providing a public method for getting the list of all deleted docs\n\nRight... that could be more useful for someone that needs random access to isDeleted(), at the cost of greater setup cost, and greater memory usage.  However the TermDocs approach solves the use-cases we have today in lucene-core. ",
            "author": "Yonik Seeley",
            "id": "comment-12610101"
        },
        {
            "date": "2008-07-03T04:48:38+0000",
            "content": "\nRight, but using TermDocs, they already have the same style of optimization and hence suffer no synchronization overhead.\n\nArggg.... sorry, my bad: i was thinking reader.isDeleted was used in those\ncases as well \u2013 I was totally missing that SegmentTermDocs takes care of\nit directly.\n\nbut ... thinking about how SegmentTermDocs are constructed for a moment,\nisn't the (unsynchronized) usage of deletedDocs in SegmentTermDocs.next\nprone to the same concern you had about removing (or reducing) the\nsynchronization in SegmentReader.isDeleted ... \"deletes aren't instantly\nvisible across threads\" ... are they?\n\nIs SegmentTermDocs.next too lax in how it deals with deletedDocs, or is\nSegmentReader.isDeleted to paranoid?\n ",
            "author": "Hoss Man",
            "id": "comment-12610123"
        },
        {
            "date": "2008-07-03T13:34:55+0000",
            "content": "but ... thinking about how SegmentTermDocs are constructed for a moment,\nisn't the (unsynchronized) usage of deletedDocs in SegmentTermDocs.next\nprone to the same concern you had about removing (or reducing) the\nsynchronization in SegmentReader.isDeleted ... \"deletes aren't instantly\nvisible across threads\" ... are they?\n\nNo, deletes aren't instantly visible across threads (when one thread has started a query and the other thread does a delete).  AFAIK it's always been that way, so I think it's probably acceptable.  On a practical level, seeking on a TermDocs crosses synchronization points, so deletes won't go unrecognized by other threads forever either.\n\nBut there is a thread-safety issue I've been mulling over since I wrote this patch.\nSegmentTermDocs.next() is fine... unsynchronized reads look benign on the BitVector class since writes just change a byte at a time.  \"count\" could be off, but that's OK too... it will simply be a stale value since updates to it are atomic.\n\nThe issue is the possibility of grabbing a partially constructed BitVector object to begin with.  Notice how I synchronize to avoid this in AllTermDocs:\n\n  protected AllTermDocs(SegmentReader parent) {\n    synchronized (parent) {\n      this.deletedDocs = parent.deletedDocs;\n    }\n    this.maxDoc = parent.maxDoc();\n  }\n\n\n\nThat should probably be done in SegmentTermDocs too.  Without it, a null pointer exception or an array out of bounds exception could result when checking the BitVector. ",
            "author": "Yonik Seeley",
            "id": "comment-12610228"
        },
        {
            "date": "2008-10-06T19:22:17+0000",
            "content": "we use MatchAllDocs query also. In addition to what is described here we got very nice gains by overriding the Scorer.score() methods that take a HitCollector.\n\nThis seems pretty dumb but I guess since it has to score every doc the method call overhead actually matters?  ",
            "author": "Robert Muir",
            "id": "comment-12637186"
        },
        {
            "date": "2008-11-10T02:45:55+0000",
            "content": "The new ReadOnly IndexReader option resolves this issue, correct? ",
            "author": "Mark Miller",
            "id": "comment-12646152"
        },
        {
            "date": "2008-11-20T00:05:09+0000",
            "content": "I looked at 2.4 and am surprised this patch did not make it in.  Read only IndexReader shouldn't be necessary to solve this issue.  Were there any problems in the unit tests for this patch or something? ",
            "author": "Jason Rutherglen",
            "id": "comment-12649219"
        },
        {
            "date": "2008-12-16T10:38:50+0000",
            "content": "Yonik is your patch here (creating an AllTermDocs) ready to commit, once you fix SegmentTermDocs to also synchronize on parent when grabbing the deletedDocs? ",
            "author": "Michael McCandless",
            "id": "comment-12656944"
        },
        {
            "date": "2009-01-13T21:43:01+0000",
            "content": "Added synchronized on parent when obtaining the deleted docs in SegmentTermDocs.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12663484"
        },
        {
            "date": "2009-01-15T21:35:05+0000",
            "content": "I think at least AllTermDocs.java is missing from the latest patch here.  When you apply a patch, make a change, and then \"svn diff\" in order to post a new patch, you have to [frustratingly] remember to first \"svn add\" any new files in the first patch.\n\nI think the Subversion team is working on an \"svn patch\" command that would do this (and apply properties, and maybe deal with a 3-way merge, etc) but last I checked it's not done yet.  So until then we have to workaround... ",
            "author": "Michael McCandless",
            "id": "comment-12664276"
        },
        {
            "date": "2009-01-16T04:37:18+0000",
            "content": "In a way, this is a mirror image of Jason's request in LUCENE-1476 for a getDeletedDocs() that returned a DocIdSet... provided it also worked on a MultiReader, etc.  MatchAllDocs could be efficiently implemented with that.\n\nIt does seem like having some sort of iterator over existing docs is useful to avoid the binary search cost of associating ids with segments, but there was never any feedback on what the API should be.  Instead of adding new functionality to termDocs(), we could also add a getAllDocs() that returns either a DocIdSet or an interator.\n ",
            "author": "Yonik Seeley",
            "id": "comment-12664404"
        },
        {
            "date": "2009-01-16T16:53:05+0000",
            "content": "Patch includes AllTermDocs.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12664562"
        },
        {
            "date": "2009-01-16T17:09:04+0000",
            "content": "\nIn a way, this is a mirror image of Jason's request in\nLUCENE-1476 for a getDeletedDocs() that returned a DocIdSet...\nprovided it also worked on a MultiReader, etc. MatchAllDocs could be\nefficiently implemented with that. \n\nI think the API of reader.termDocs(null) is fine. If we move to a\nDocIdSet for deleteddocs we can change the all docs implementation if\nneeded.  ",
            "author": "Jason Rutherglen",
            "id": "comment-12664567"
        },
        {
            "date": "2009-01-25T12:24:43+0000",
            "content": "OK, new patch attached:\n\n\n\tAdded more tests of *Reader.termDocs(null)\n\n\n\n\n\tChanged IndexReader.termDocs javadoc to explain term=null case\n\n\n\n\n\tFixed MemoryIndex.termDocs(null) to return freq=1\n\n\n\n\n\tAdded CHANGES.txt entry\n\n\n\nI think it's ready to commit.  I'll wait a day or two. ",
            "author": "Michael McCandless",
            "id": "comment-12667077"
        },
        {
            "date": "2009-01-25T13:19:20+0000",
            "content": "+1 to the latest patch.\nMuch thanks for picking this up! (I've been completely swamped.) ",
            "author": "Yonik Seeley",
            "id": "comment-12667083"
        },
        {
            "date": "2009-01-25T14:35:23+0000",
            "content": "OK, no problem Yonik \u2013 thanks for reviewing.  I'll commit shortly. ",
            "author": "Michael McCandless",
            "id": "comment-12667088"
        },
        {
            "date": "2009-01-25T14:39:03+0000",
            "content": "Committed revision 737513.  Thanks everyone! ",
            "author": "Michael McCandless",
            "id": "comment-12667089"
        }
    ]
}