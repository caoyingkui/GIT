{
    "id": "SOLR-12094",
    "title": "JsonRecordReader ignores root record fields after the split point",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "SolrJ"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "master (8.0)",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "JsonRecordReader, when configured with other than top-level split, ignores all top-level JSON nodes after the split ends, for example:\n\n\n{\n  \"first\": \"John\",\n  \"last\": \"Doe\",\n  \"grade\": 8,\n  \"exams\": [\n    {\n        \"subject\": \"Maths\",\n        \"test\": \"term1\",\n        \"marks\": 90\n    },\n    {\n        \"subject\": \"Biology\",\n        \"test\": \"term1\",\n        \"marks\": 86\n    }\n  ],\n  \"after\": \"456\"\n}\n\n\n\nNode \"after\" won't be visible in SolrInputDocument constructed from /update/json/docs.",
    "attachments": {
        "SOLR-12094.patch": "https://issues.apache.org/jira/secure/attachment/12914784/SOLR-12094.patch",
        "json-record-reader-bug.patch": "https://issues.apache.org/jira/secure/attachment/12914527/json-record-reader-bug.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2018-03-15T18:27:29+0000",
            "content": "There is something I don't understand. In your patch this node is outside of the /exams path:\n\n  public void testRecursiveWildcard2() throws Exception {\n    String json = \"{\\n\" +\n        \"  \\\"first\\\": \\\"John\\\",\\n\" +\n        \"  \\\"last\\\": \\\"Doe\\\",\\n\" +\n        \"  \\\"grade\\\": 8,\\n\" +\n        \"  \\\"exams\\\": [\\n\" +\n        \"      {\\n\" +\n        \"        \\\"subject\\\": \\\"Maths\\\",\\n\" +\n        \"        \\\"test\\\"   : \\\"term1\\\",\\n\" +\n        \"        \\\"marks\\\":90},\\n\" +\n        \"        {\\n\" +\n        \"         \\\"subject\\\": \\\"Biology\\\",\\n\" +\n        \"         \\\"test\\\"   : \\\"term1\\\",\\n\" +\n        \"         \\\"marks\\\":86}\\n\" +\n        \"      ],\\n\" +\n        \"  \\\"after\\\": \\\"456\\\"\\n\" +\n        \"}\";\n\n    JsonRecordReader streamer;\n    List<Map<String, Object>> records;\n\n    streamer = JsonRecordReader.getInst(\"/exams\", Collections.singletonList(\"/**\"));\n    records = streamer.getAllRecords(new StringReader(json));\n    assertEquals(2, records.size());\n    for (Map<String, Object> record : records) {\n      assertEquals(7, record.size());\n      assertTrue(record.containsKey(\"subject\"));\n      assertTrue(record.containsKey(\"test\"));\n      assertTrue(record.containsKey(\"marks\"));\n      assertTrue(record.containsKey(\"first\"));\n      assertTrue(record.containsKey(\"last\"));\n      assertTrue(record.containsKey(\"grade\"));\n      assertTrue(record.containsKey(\"after\"));\n    }\n\n\n\nSo it fails for a reason \u2013 after shouldn't be there and that's the correct behavior? ",
            "author": "Dawid Weiss",
            "id": "comment-16400902"
        },
        {
            "date": "2018-03-15T18:29:39+0000",
            "content": "Uh, sorry. I didn't get what \"split\" does. I think it's a bug indeed. ",
            "author": "Dawid Weiss",
            "id": "comment-16400907"
        },
        {
            "date": "2018-03-15T19:31:44+0000",
            "content": "Thank you Dawid for fixing\u00a0my poor\u00a0issue\u00a0formatting.\n\nYes, it\u00a0is a bug:\u00a0visibility of root fields in output depends on their order\u00a0within input\u00a0document, so functionally indifferent JSON with \"exams\" as a first field\u00a0would have all root-level fields skipped from resulting document. ",
            "author": "Przemys\u0142aw Szeremiota",
            "id": "comment-16401005"
        },
        {
            "date": "2018-03-15T21:07:42+0000",
            "content": "Hi, I have added a patch file \u00a0SOLR-12094.patch\u00a0in which I buffer extracting records to fill in missing fields from the json top level. ",
            "author": "Andrzej Wislowski",
            "id": "comment-16401117"
        },
        {
            "date": "2018-03-15T21:40:53+0000",
            "content": "Thanks for the patch! I'll be away for a week and I am not familiar with this code at all, but if nobody picks it up I'll try to dig in, review it and commit it. If you could run the test suite a few times (and precommit) to ensure the patch passes, it'd be great.\n\nThere are similar issues with DIH that I reported a while ago, but never got to fixing too. Sigh. ",
            "author": "Dawid Weiss",
            "id": "comment-16401146"
        },
        {
            "date": "2018-03-19T12:31:11+0000",
            "content": "Hi Dawid Weiss,\u00a0\n\ntests passes, precommit succeeds.\n\n\u00a0\n\n\u00a0 ",
            "author": "Andrzej Wislowski",
            "id": "comment-16404733"
        },
        {
            "date": "2018-03-19T19:41:13+0000",
            "content": "Thanks! I'll take a look at it next week (vacation). Here's another one you may want to take a look at (not related, but similar type of issue): SOLR-10012  ",
            "author": "Dawid Weiss",
            "id": "comment-16405329"
        },
        {
            "date": "2018-03-27T19:36:28+0000",
            "content": "I looked at the code of that streaming parser, it is quite complex; seems like all this node copying and record trickery could be avoided, but it'd be a significantly more complex patch then. Noble Paul - you seem to be involved much more in the parser development, would you like to take a look before I commit it in? ",
            "author": "Dawid Weiss",
            "id": "comment-16416136"
        },
        {
            "date": "2018-03-27T23:40:52+0000",
            "content": "before going into the patch, I can see that it is not designed to work like that . The reason is that JsonRecordReader is a streaming parser. To include the 'after' in the document, It must hold all the data in the 'exams in memory.  Think if there are a million docs in 'exams' and keeping all of them in memory before  it can read the value of 'after'. So, it is going to seriously affect the performance of the parser for the normal use case.  ",
            "author": "Noble Paul",
            "id": "comment-16416475"
        },
        {
            "date": "2018-03-28T07:58:44+0000",
            "content": "I understand, but I also believe it's really likely that people have such nested JSONs and will want to use them. Now it quietly just discards those trailing entries and I don't think that's good either: it should either signal an exception (probably pointing at a non-streaming solution, if there is any) or work correctly. What do you think? ",
            "author": "Dawid Weiss",
            "id": "comment-16417003"
        },
        {
            "date": "2018-03-28T08:03:15+0000",
            "content": "You are right, it's not a good idea to ignore this. It should probably throw an exception of it encounters such a json.\u00a0\n\n\u00a0\n\nIt's possible to implement a non streaming solution. User may pass an optional parameter to switch to that mode ",
            "author": "Noble Paul",
            "id": "comment-16417008"
        },
        {
            "date": "2018-04-06T10:26:40+0000",
            "content": "I understand the concept of \"streaming\" imports, but this just seems wrong to me here. An analogy here would be XSLT or other technologies where the implementation permits efficient \"streaming\" mode in certain cases, unless the input makes it impossible. \n\nI perceive a similar situation here: the parser should be able to handle the input efficiently if possible, but should also give the possibility for processing any type of input, even such that cannot be processed without bookkeeping of some history. Sure, an abuse case of millions of split nodes awaiting a single attribute is possible, but even then it'd be simpler to just say \"yeah, buffer up until you can emit the output\" than modify the structure of such a json (write a converter so that the nested nodes are always placed at the end of the parent).\n\nAndrzej Wislowski do you think you'd be able to modify the patch so that it accepts an argument and switches between the 'strict streaming' mode and 'relaxed' mode? In 'strict streaming' mode there should be no buffering and the parser should complain with an exception if it encounters extra nodes after the split. In the 'relaxed mode' the parser should buffer up the information until it's complete and can be emitted. ",
            "author": "Dawid Weiss",
            "id": "comment-16428179"
        },
        {
            "date": "2018-04-06T10:54:30+0000",
            "content": "I agree that we should be able to handle this use case as well. But, the primary objective is to handle streaming input well. Non streaming parsing should be optional ",
            "author": "Noble Paul",
            "id": "comment-16428202"
        },
        {
            "date": "2018-04-06T10:58:30+0000",
            "content": "Dawid Weiss I think it is a good idea. I\u00a0will take a look at this code and\u00a0try to create such patch. ",
            "author": "Andrzej Wislowski",
            "id": "comment-16428205"
        }
    ]
}