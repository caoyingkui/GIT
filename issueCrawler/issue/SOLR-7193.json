{
    "id": "SOLR-7193",
    "title": "Concatenate words from token stream",
    "details": {
        "components": [
            "Schema and Analysis"
        ],
        "type": "New Feature",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Resolved",
        "resolution": "Duplicate",
        "priority": "Major"
    },
    "description": "The user entered data often don't have proper spacing between words and words spelling and format also varies from data like business names, address etc. After tokenizing data, we might perform pattern replacement, stop word filtering etc. Later we want to concatenate all the tokens and generate n-grams token for indexing business name and perform the fuzzy match.",
    "attachments": {
        "concatenate_words.patch": "https://issues.apache.org/jira/secure/attachment/12702717/concatenate_words.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-03-05T06:00:28+0000",
            "author": "Abhishek Bafna",
            "content": "Please review and let me know further improvement if required. ",
            "id": "comment-14348205"
        },
        {
            "date": "2015-03-05T06:55:28+0000",
            "author": "chengyunyun",
            "content": "pressure test environment:\n\u2022\tClient used Spring restful,\n\u2022\tUser:500\n\u2022\tTotal data:3 million\n\u2022\tProblem: solr log search time is only 30ms; but searchClient execute server.query(SolrQuery query) need more time,even 30s;time gap is very large.\nis it related to CPU or memory? ",
            "id": "comment-14348272"
        },
        {
            "date": "2015-03-12T06:45:34+0000",
            "author": "Abhishek Bafna",
            "content": "The ConcatenateWordsFilter takes all the input token (words) and generate a single token. The CPU time and memory depends on the number and size of the tokens coming in the stream. The use case for this filter, when input stream contains business name, address, etc., which usually have a small number of tokens. I am guessing, here (test environment) input data containing long paragraphs or documents and that might be causing the issue. ",
            "id": "comment-14358173"
        },
        {
            "date": "2015-03-13T04:55:26+0000",
            "author": "Abhishek Bafna",
            "content": "chengyunyun Did you get the point I tried to convey. Can you please provide your further comment for the patch. ",
            "id": "comment-14359939"
        },
        {
            "date": "2016-05-08T08:03:29+0000",
            "author": "Mikhail Khludnev",
            "content": "linking org.opensextant.solrtexttagger.ConcatenateFilterFactory ",
            "id": "comment-15275510"
        },
        {
            "date": "2018-06-11T21:41:08+0000",
            "author": "Alexandre Rafalovitch",
            "content": "This seems to be satisfied by LUCENE-8332 and SOLR-12376, both coming in 7.4. ",
            "id": "comment-16508806"
        }
    ]
}