{
    "id": "SOLR-659",
    "title": "Explicitly set start and rows per shard for more efficient bulk queries across distributed Solr",
    "details": {
        "affect_versions": "1.3",
        "status": "Closed",
        "fix_versions": [
            "1.4"
        ],
        "components": [
            "search"
        ],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "The default behavior of setting start and rows on distributed solr (SOLR-303) is to set start at 0 across all shards and set rows to start+rows across each shard. This ensures all results are returned for any arbitrary start and rows setting, but during \"bulk queries\" (where start is incrementally increased and rows is kept consistent) the client would need finer control of the per-shard start and rows parameter as retrieving many thousands of documents becomes intractable as start grows higher.\n\nAttaching a patch that creates a &shards.start and &shards.rows parameter. If used, the logic that sets rows to start+rows per shard is overridden and each shard gets the exact start and rows set in shards.start and shards.rows. The client will receive up to shards.rows * nShards results and should set rows accordingly. This makes bulk queries across distributed solr possible.",
    "attachments": {
        "SOLR-659.patch": "https://issues.apache.org/jira/secure/attachment/12399776/SOLR-659.patch",
        "shards.start_rows.patch": "https://issues.apache.org/jira/secure/attachment/12386884/shards.start_rows.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Brian Whitman",
            "id": "comment-12616897",
            "date": "2008-07-25T14:07:20+0000",
            "content": "Attaching patch. "
        },
        {
            "author": "Brian Whitman",
            "id": "comment-12616903",
            "date": "2008-07-25T14:17:24+0000",
            "content": "An example of a bulk query using this patch. Without this patch such bulk queries will eventually time out or cause exceptions in the server as too much data is passed back and forth.\n\n\npublic SolrDocumentList blockQuery(SolrQuery q, int blockSize, int maxResults) {\n    SolrDocumentList allResults = new SolrDocumentList();\n    if(blockSize > maxResults) { blockSize = maxResults;  }\n    for(int i=0; i<maxResults; i=i+blockSize) {\n      // Sets rows of this query to the most results that could ever come back - the blockSize * the number of shards\n      q.setRows(blockSize * getNumberOfHosts());\n      // Don't set a start on the main query\n      q.setStart(0);\n      // But do set start and rows on the individual shards. \n      q.set(\"shards.start\", String.valueOf(i));\n      q.set(\"shards.rows\", String.valueOf(blockSize));\n      // Perform the query.\n      QueryResponse sub = query(q);\n      // For each returned document (up to blockSize*numberOfHosts() of them), append them to the main result\n      for(SolrDocument s : sub.getResults()) {\n        allResults.add(s);\n        // Break if we've reached our requested limit\n        if(allResults.size() > maxResults) { break; }\n      }\n      if(allResults.size() > maxResults) { break; }\n    }\n    return allResults;\n  }\n\n "
        },
        {
            "author": "Mike Klaas",
            "id": "comment-12618785",
            "date": "2008-07-31T18:09:37+0000",
            "content": "IMO it is too late in the release process for new features. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12629176",
            "date": "2008-09-08T14:49:28+0000",
            "content": "This looks simple enough.  I haven't tried it.  Brian, do you have a unit test you could attach?\n\nOr would it make more sense to have a custom QueryComponent for something like this? (I don't know yet) "
        },
        {
            "author": "Brian Whitman",
            "id": "comment-12671655",
            "date": "2009-02-08T17:53:03+0000",
            "content": "New patch syncs w/ trunk "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12683803",
            "date": "2009-03-20T09:02:13+0000",
            "content": "If I understand this correctly, it makes bulk queries cheaper at the expense of less precise scoring. But if I'm paging through some results and you modify the shard.start and shard.rows then I'll get inconsistent results. Is that correct?\n\nThe client will receive up to shards.rows * nShards results and should set rows accordingly. This makes bulk queries across distributed solr possible.\n\nI do not understand that. Why will the client get more than rows? Or by client, did you mean the solr server to which the initial request is sent? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12748986",
            "date": "2009-08-28T21:27:59+0000",
            "content": "I agree this makes sense to enable efficient bulk operations, and also fits in with a past idea I had about mapping shards.param=foo to param=foo during a sub-request.\n\nI'll give it a couple of days and commit if there are no objections. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12752235",
            "date": "2009-09-07T18:30:11+0000",
            "content": "Thanks Brian, I just committed this. "
        },
        {
            "author": "johnson.hong",
            "id": "comment-12769114",
            "date": "2009-10-23T07:00:22+0000",
            "content": "This is really helpful to bulk  queries ,but how to handle the pagination of query results.\ne.g.at the first query,I set  shards.start to 0 and set shards.rows to 30,it  may return 50 documents,and i get 30 documents to show ,the other 20 documents is discarded ;then how to get the next 30 documents ? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12775519",
            "date": "2009-11-10T15:51:43+0000",
            "content": "Bulk close for Solr 1.4 "
        }
    ]
}