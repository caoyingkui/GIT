{
    "id": "LUCENE-5030",
    "title": "FuzzySuggester has to operate FSTs of Unicode-letters, not UTF-8, to work correctly for 1-byte (like English) and multi-byte (non-Latin) letters",
    "details": {
        "components": [],
        "fix_versions": [
            "4.5",
            "6.0"
        ],
        "affect_versions": "4.3",
        "priority": "Major",
        "labels": "",
        "type": "Bug",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "There is a limitation in the current FuzzySuggester implementation: it computes edits in UTF-8 space instead of Unicode character (code point) space. \n\nThis should be fixable: we'd need to fix TokenStreamToAutomaton to work in Unicode character space, then fix FuzzySuggester to do the same steps that FuzzyQuery does: do the LevN expansion in Unicode character space, then convert that automaton to UTF-8, then intersect with the suggest FST.\n\nSee the discussion here: http://lucene.472066.n3.nabble.com/minFuzzyLength-in-FuzzySuggester-behaves-differently-for-English-and-Russian-td4067018.html#none",
    "attachments": {
        "nonlatin_fuzzySuggester_combo.patch": "https://issues.apache.org/jira/secure/attachment/12589383/nonlatin_fuzzySuggester_combo.patch",
        "run-suggest-benchmark.patch": "https://issues.apache.org/jira/secure/attachment/12588832/run-suggest-benchmark.patch",
        "benchmark-INFO_SEP.txt": "https://issues.apache.org/jira/secure/attachment/12589016/benchmark-INFO_SEP.txt",
        "nonlatin_fuzzySuggester_combo1.patch": "https://issues.apache.org/jira/secure/attachment/12589720/nonlatin_fuzzySuggester_combo1.patch",
        "nonlatin_fuzzySuggester.patch": "https://issues.apache.org/jira/secure/attachment/12587603/nonlatin_fuzzySuggester.patch",
        "nonlatin_fuzzySuggester1.patch": "https://issues.apache.org/jira/secure/attachment/12588115/nonlatin_fuzzySuggester1.patch",
        "nonlatin_fuzzySuggester3.patch": "https://issues.apache.org/jira/secure/attachment/12588144/nonlatin_fuzzySuggester3.patch",
        "benchmark-old.txt": "https://issues.apache.org/jira/secure/attachment/12589017/benchmark-old.txt",
        "LUCENE-5030.patch": "https://issues.apache.org/jira/secure/attachment/12589877/LUCENE-5030.patch",
        "nonlatin_fuzzySuggester_combo2.patch": "https://issues.apache.org/jira/secure/attachment/12589721/nonlatin_fuzzySuggester_combo2.patch",
        "benchmark-wo_convertion.txt": "https://issues.apache.org/jira/secure/attachment/12589018/benchmark-wo_convertion.txt",
        "nonlatin_fuzzySuggester4.patch": "https://issues.apache.org/jira/secure/attachment/12588305/nonlatin_fuzzySuggester4.patch",
        "nonlatin_fuzzySuggester2.patch": "https://issues.apache.org/jira/secure/attachment/12588140/nonlatin_fuzzySuggester2.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2013-06-13T11:30:53+0000",
            "content": "I've added a test, which demonstrates the bug. I have fixed TokenStreamToAutomaton, but I have no idea, how to update AnalyzingSuggester, which wants bytes instead of chars (ints, which cannot fit a byte). ",
            "author": "Artem Lukanin",
            "id": "comment-13682132"
        },
        {
            "date": "2013-06-13T12:00:30+0000",
            "content": "Thanks Artem!\n\nI think we need to change POS_SEP and HOLE?  256/267 are no longer safe to use?\n\nAlso, I think TokenStreamToAutomaton should operate with Unicode code points, not code units (java's char)?  Probably we should fork TS2A to a new class (TokenStreamToUnicodeAutomaton or something?).\n\nThen, in FuzzySuggester, after calling toLevenshteinAutomata and before calling FSTUtil.intersectPrefixPaths, we need to insert a call to UTF32ToUTF8().convert(automaton), just like in CompiledAutomaton.java.  That call will translate the fuzzed-up unicode Automaton into the UTF8 equivalent, and then we should be able to pass that on to intersect.\n\nAlso, I think you'll need to fix that 255 in FuzzySuggester.toLevenshteinAutomata ... in fact, just remove it, so we use the ctor that passes max unicode code point. ",
            "author": "Michael McCandless",
            "id": "comment-13682162"
        },
        {
            "date": "2013-06-17T08:51:38+0000",
            "content": "Now all the tests pass except testRandom when preserveSep is true.\n\nMichael, can you explain me, how this preserve separator feature works? ",
            "author": "Artem Lukanin",
            "id": "comment-13685107"
        },
        {
            "date": "2013-06-17T11:38:11+0000",
            "content": "Hmm POS_SEP and HOLE are still eating into Unicode's space (the last 2\ncode points)?  Maybe, we should just use Integer.MAX_VALUE and\nMAX_VALUE-1?\n\nIn fact I think you'll need to make POS_SEP and HOLE consistent across\nboth of the TS2A classes.  I think you should just define them in\nTokenStreamToAutomaton.java, and then reference those constants from\nTokenStreamToUnicodeAutomaton.java?\n\nThere's a lot of whitespace noise here ... can you remove those\nchanges so we can more easily see the \"real\" changes?  Thanks.\n\nThe preserveSep option means when finding a match we must \"respect\"\nthe boundaries between tokens.  It could be you're seeing tests fail\nbecause POS_SEP was different between the two classes? ",
            "author": "Michael McCandless",
            "id": "comment-13685469"
        },
        {
            "date": "2013-06-17T12:07:03+0000",
            "content": "Sorry for autoformatting, I will upload the patch without it.\nSince we use Integer.MAX_VALUE we do not need EscapingTokenStreamToUnicodeAutomaton any more, because the text will not have such a code point? ",
            "author": "Artem Lukanin",
            "id": "comment-13685481"
        },
        {
            "date": "2013-06-17T13:02:06+0000",
            "content": "BTW, if I replace it with Integer.MAX_VALUE, UTF32ToUTF8().convert(unicodeAutomaton) will not work any more. I mean it will not convert Integer.MAX_VALUE into a sequence of bytes (it converts it into 1fff bf bf bf). So I guess, we still need EscapingTokenStreamToUnicodeAutomaton. ",
            "author": "Artem Lukanin",
            "id": "comment-13685516"
        },
        {
            "date": "2013-06-17T13:21:03+0000",
            "content": "the patch without autoformatting ",
            "author": "Artem Lukanin",
            "id": "comment-13685543"
        },
        {
            "date": "2013-06-17T13:30:02+0000",
            "content": "I see, the patch still has autoformatting of some spaces. Sorry, I guess I cannot stop IntelliJ IDEA doing it  ",
            "author": "Artem Lukanin",
            "id": "comment-13685551"
        },
        {
            "date": "2013-06-17T13:42:07+0000",
            "content": "with untouched trailing spaces ",
            "author": "Artem Lukanin",
            "id": "comment-13685558"
        },
        {
            "date": "2013-06-17T13:55:20+0000",
            "content": "Oh, right, we can't just use MAX_VALUE: it must be a valid unicode char since we will send it through UTF32toUTF8.\n\nAlso, it's easiest if that char survives to UTF8 as a single byte to keep replaceSep [relatively] simple.\n\nMaybe we \"steal\" two unicode chars?  Maybe INFO_SEP (U+001F) and INFO_SEP2 (U+001E), and we document that these chars are not allowed on the input?  (We could also try, maybe later as a separate issue, to escape them when they occur, like EscapingTokenStreamToUnicodeAutomaton now does if it sees 0xFF on the input). ",
            "author": "Michael McCandless",
            "id": "comment-13685567"
        },
        {
            "date": "2013-06-18T06:50:18+0000",
            "content": "you already have\nprivate static final int PAYLOAD_SEP = '\\u001f';\nin AnalyzingSuggester ",
            "author": "Artem Lukanin",
            "id": "comment-13686447"
        },
        {
            "date": "2013-06-18T07:58:34+0000",
            "content": "I have fixed testRandom, which repeats the logic of FuzzySuggester.\nNow all the tests pass.\nPlease, review. ",
            "author": "Artem Lukanin",
            "id": "comment-13686482"
        },
        {
            "date": "2013-06-19T10:02:53+0000",
            "content": "I see, that some tests in AnalyzingSuggesterTest fail, so I have to look what's wrong... ",
            "author": "Artem Lukanin",
            "id": "comment-13687822"
        },
        {
            "date": "2013-06-19T11:54:40+0000",
            "content": "now tests in FuzzySuggesterTest and AnalyzingSuggesterTest pass, except for AnalyzingSuggesterTest.testRandom (when preserveSep = true).\n\nIf I enable VERBOSE, I see, that suggestions are correct. I guess, there is a bug in the test, but I cannot find it.\n\nCan you please review? ",
            "author": "Artem Lukanin",
            "id": "comment-13687899"
        },
        {
            "date": "2013-06-19T12:04:12+0000",
            "content": "I dont think changing SEP_LABEL from a single byte to 4 bytes is necessarily a good idea.\n\nI think benchmarks (size and speed) should be run on this change before we jump into it, I'm also concerned about the determinization and shit being in the middle of an autosuggest request... this seems like it would be way way too slow. ",
            "author": "Robert Muir",
            "id": "comment-13687902"
        },
        {
            "date": "2013-06-19T12:18:50+0000",
            "content": "Possibly we should change it to INFO_SEP2 (U+001E) as Michael suggested for TokenStreamToAutomaton?\nDo you like 0x10ffff and 0x10fffe separators in TokenStreamToAutomaton? Won't they slow down the process?\nI guess, Michael is the man, who runs benchmarks regularly? I don't know, how to do it... ",
            "author": "Artem Lukanin",
            "id": "comment-13687917"
        },
        {
            "date": "2013-06-19T17:28:14+0000",
            "content": "The easy performance tester to run is\nlucene/suggest/src/test/org/apache/lucene/search/suggest/LookupBenchmarkTest.java\n... we should test that first I think?  I can also run one based on\nFreeDB ... the sources are in luceneutil\n(https://code.google.com/a/apache-extras.org/p/luceneutil/ ).\n\nIf the perf hit is too much then one option would be to make it\noptional (whether we count edits in Unicode space UTF-8 space), or\nmaybe just another suggester class (FuzzyUnicodeSuggester?).\n\nI think we can use INFO_SEP: yes, this is used for PAYLOAD_SEP, but\nthat only means the incoming surfaceForm cannot contain this char, I\nthink?  So ... I think we are free to use it in the analyzed form?  Or\ndid something go wrong when you tried?\n\nWhichever chars we use (steal), we should add checks that these chars do not\noccur in the input... ",
            "author": "Michael McCandless",
            "id": "comment-13688185"
        },
        {
            "date": "2013-06-20T06:36:35+0000",
            "content": "I ran this command:\n\nant -Dtestcase=LookupBenchmarkTest clean test\n\n\nand got the same results for the patched and the original version:\n\n[junit4:junit4] Tests summary: 1 suite, 0 tests\n     [echo] 5 slowest tests:\n[junit4:tophints]  22.95s | org.apache.lucene.search.spell.TestSpellChecker\n[junit4:tophints]  22.70s | org.apache.lucene.search.suggest.analyzing.AnalyzingSuggesterTest\n[junit4:tophints]  15.08s | org.apache.lucene.search.suggest.fst.TestSort\n[junit4:tophints]  11.84s | org.apache.lucene.search.suggest.fst.FSTCompletionTest\n[junit4:tophints]  11.24s | org.apache.lucene.search.suggest.analyzing.FuzzySuggesterTest\n\n ",
            "author": "Artem Lukanin",
            "id": "comment-13688924"
        },
        {
            "date": "2013-06-20T09:52:36+0000",
            "content": "I used INFO_SEP and INFO_SEP2 for separators and holes. All the tests pass (I have fixed AnalyzingSuggesterTest.testStolenBytes). The benchmark is improved:\n\n\n[junit4:junit4] Suite: org.apache.lucene.search.suggest.LookupBenchmarkTest\n[junit4:junit4] Completed in 0.04s, 0 tests\n[junit4:junit4] \n[junit4:junit4] JVM J0:     1.64 ..     2.34 =     0.71s\n[junit4:junit4] Execution time total: 2.36 sec.\n[junit4:junit4] Tests summary: 1 suite, 0 tests\n     [echo] 5 slowest tests:\n[junit4:tophints]  22.95s | org.apache.lucene.search.spell.TestSpellChecker\n[junit4:tophints]  15.08s | org.apache.lucene.search.suggest.fst.TestSort\n[junit4:tophints]  13.41s | org.apache.lucene.search.suggest.analyzing.AnalyzingSuggesterTest\n[junit4:tophints]  11.84s | org.apache.lucene.search.suggest.fst.FSTCompletionTest\n[junit4:tophints]  10.78s | org.apache.lucene.search.suggest.analyzing.FuzzySuggesterTest\n\n ",
            "author": "Artem Lukanin",
            "id": "comment-13689073"
        },
        {
            "date": "2013-06-20T11:22:54+0000",
            "content": "Hi Artem,\n\nSorry, running the LookupBenchmarkTest is tricky ... you need to make temporary changes in 3 places.  I'm attaching a patch that should let you run it by just doing \"ant test -Dtestcase=LookupBenchmarkTest\". ",
            "author": "Michael McCandless",
            "id": "comment-13689155"
        },
        {
            "date": "2013-06-20T13:21:06+0000",
            "content": "OK, in general the performance is worse twice.\nbefore my patch: Total time: 9 minutes 20 seconds\nafter my patch with INFO_SEP and INFO_SEP2: Total time: 18 minutes 31 seconds\n\nI guess, the reason is UTF32ToUTF8().convert(unicodeAutomaton), so it is better to add correct transitions on the fly... but possibly, you can postpone this for another issue? ",
            "author": "Artem Lukanin",
            "id": "comment-13689231"
        },
        {
            "date": "2013-06-20T14:13:26+0000",
            "content": "Hmm can you post the full output of the benchmark?  It measures different things in each test case.\n\nGiven this, I guess we should make a separate fuzzy suggester class (that measures edit distance in Unicode code point space)?  Or make it a boolean option on the current class ...\n\nCan you also post your last patch (cutting over to INFO_SEP/2 for the stolen chars)? ",
            "author": "Michael McCandless",
            "id": "comment-13689269"
        },
        {
            "date": "2013-06-20T14:49:12+0000",
            "content": "The last patch with INFO_SEP/2 was posted today 20/Jun/13 at 10:52 ",
            "author": "Artem Lukanin",
            "id": "comment-13689296"
        },
        {
            "date": "2013-06-20T15:18:00+0000",
            "content": "Oh, woops, I missed it.  Thanks. ",
            "author": "Michael McCandless",
            "id": "comment-13689323"
        },
        {
            "date": "2013-06-21T07:47:29+0000",
            "content": "I'm uploading 3 results of benchmarking:\n1) benchmark-old.txt - before the patch\n2) benchmark-INFO_SEP.txt - after the patch\n3) benchmark-wo_convertion.txt - after the patch, but with commented lines of convertion in several places, like\n\n      //Automaton utf8lookupAutomaton = new UTF32ToUTF8().convert(lookupAutomaton);\n      //BasicOperations.determinize(utf8lookupAutomaton);\n\nbecause they are useless for words consisting only latin letters.\n\nAs you can see the conversion takes too much time. ",
            "author": "Artem Lukanin",
            "id": "comment-13690102"
        },
        {
            "date": "2013-06-21T08:21:58+0000",
            "content": "OK, I will add a new option UNICODE_AWARE = 4, which will switch conversion ON. ",
            "author": "Artem Lukanin",
            "id": "comment-13690124"
        },
        {
            "date": "2013-06-21T10:30:46+0000",
            "content": "I'm a little confused by the results.  E.g., how come AnalyzingSuggester changes so much between old (57 kQPS) and INFO_SEP (27 kQPS) and wo_conversion (41 kQPS), at the 2-4 prefixes len?  And why are the unrelated impls (TSS, FST, WFST, Jaspell) changing so much?  Maybe this is just horrible Hotspot noise? ",
            "author": "Michael McCandless",
            "id": "comment-13690183"
        },
        {
            "date": "2013-06-21T19:15:19+0000",
            "content": "Oh, duh, the conversion from Unicode -> UTF8, and the determinization, are in AnalyzingSuggester ... so it makes sense that it got slower.\n\nI agree we should add a UNICODE_AWARE option. ",
            "author": "Michael McCandless",
            "id": "comment-13690614"
        },
        {
            "date": "2013-06-24T07:10:08+0000",
            "content": "I have added UNICODE_AWARE option in Lucene and Solr. Should I create a separate Solr issue for Solr updated files or can I upload a patch for all updated files? ",
            "author": "Artem Lukanin",
            "id": "comment-13691744"
        },
        {
            "date": "2013-06-24T09:21:17+0000",
            "content": "I have uploaded a lucene/solr combo patch with new UNICODE_AWARE option ",
            "author": "Artem Lukanin",
            "id": "comment-13691817"
        },
        {
            "date": "2013-06-25T18:14:29+0000",
            "content": "Thanks Artem!\n\nI don't understand why we needed to change\nAnalyzingSuggesterTest.testStolenBytes?  That implies something is\nwrong w/ the escaping I think?  (Ie, results in that test should not\nchange whether SEP is preserved or not).  So I'm confused what\nchanged...\n\nAlso, I think we don't need the check for SEP_LABEL in\nAnalyzingSuggester.lookup (that throws IllegalArgumentException)?  (We\n\"escape\" this char).  But we should check for HOLE and throw\nIllegalArgumentException, since we don't escape it.  And could you add\na test confirming you get that exc if you try to add HOLE?  Thanks. ",
            "author": "Michael McCandless",
            "id": "comment-13693227"
        },
        {
            "date": "2013-06-26T08:50:56+0000",
            "content": "Sorry, I don't understand, why testStolenBytes worked before. I have restored it and now it fails. Can you please suggest, what wrong I did?\n\nAs I understood, if we do not preserve the separator, 1 token with a separator and 2 tokens (which is actually 1 string with a separator) equals after removing the separator in  replaceSep, so we should get 2 results instead of 1 when we do a lookup. No?\n\nI've added a test for IllegalArgumentException. ",
            "author": "Artem Lukanin",
            "id": "comment-13693840"
        },
        {
            "date": "2013-06-26T09:33:28+0000",
            "content": "I have restored testStolenBytes completely and now all the tests pass (see nonlatin_fuzzySuggester_combo2.patch).\n\nBut I'm not sure, what did you mean by 0xff byte in \n\ntoken(new BytesRef(new byte[] {0x61, (byte) 0xff, 0x61}))\n\n? Letter \u00ff or SEP_LABEL?\n\nNow it is treated as letter \u00ff, but in the previous modification of the test I treated it as SEP_LABEL. ",
            "author": "Artem Lukanin",
            "id": "comment-13693867"
        },
        {
            "date": "2013-06-26T12:04:50+0000",
            "content": "Hmm, testStolenBytes should be using the 0x1f byte ... the intention\nof the test is to ensure than an incoming token that contains\nSEP_LABEL still works correctly (i.e., that the escaping we do is\nworking).\n\nWhen I change the 0xff in the patch back to 0x1f I indeed see the\n(unexpected) failure without the PRESERVE_SEP option, which is curious\nbecause we do no escaping without PRESERVE_SEP.\n\nOK I see the issue: before, when POS_SEP was 256 and the input space\nwas a byte, replaceSep always worked correctly because there was no\nway for any byte input to be confused with POS_SEP.  But now that we\nare increasing the input space to all unicode chars, there is not\n\"safe\" value for POS_SEP.\n\nOK given all this I think we should stop trying to not-steal the byte:\nI think we should simply declare we steal both 0x1e and 0x1f.  This\nmeans we can remove the escaping code, put back your previous code\nthat I had asked you to remove (sorry) that threw IAE on 0x1f (and now\nalso 0x1e), remove testStolenBytes, and then improve your new\ntestIllegalLookupArgument to also verify 0x1f gets the\nIllegalArgumentException?\n\nAlso, we could maybe eliminate some code dup here, e.g. the two\ntoFiniteStrings ... maybe by having TS2A and TS2UA share a base class\n/ interface.  Hmm, maybe we should just merge TS2UA back into TS2A,\nand add a unicodeAware option to it? ",
            "author": "Michael McCandless",
            "id": "comment-13693931"
        },
        {
            "date": "2013-06-27T10:45:09+0000",
            "content": "Done. Please, review LUCENE-5030.patch ",
            "author": "Artem Lukanin",
            "id": "comment-13694609"
        },
        {
            "date": "2013-06-27T10:49:58+0000",
            "content": "BTW, for your \n\n// TODO: is there a Reader from a CharSequence?\n\n in AnalyzingSuggester.toLookupAutomaton\nThere is org.apache.commons.io.input.CharSequenceReader ",
            "author": "Artem Lukanin",
            "id": "comment-13694610"
        },
        {
            "date": "2013-06-27T15:36:43+0000",
            "content": "Thanks Artem, new patch looks good.\n\nThanks for the tip about org.apache.commons.io.input.CharSequenceReader!  I'll update the TODO with this information, but I don't think we should pull in a dep on commons for this. ",
            "author": "Michael McCandless",
            "id": "comment-13694801"
        },
        {
            "date": "2013-07-01T21:03:02+0000",
            "content": "I plan to commit the last patch soon ... thanks Artem! ",
            "author": "Michael McCandless",
            "id": "comment-13697173"
        },
        {
            "date": "2013-07-02T05:55:07+0000",
            "content": "Cool! ",
            "author": "Artem Lukanin",
            "id": "comment-13697537"
        },
        {
            "date": "2013-07-02T11:43:24+0000",
            "content": "\n+  /** Include this flag in the options parameter to {@link\n+   *  #AnalyzingSuggester(Analyzer,Analyzer,int,int,int)} if\n+   *  you want your suggester to operate non-ASCII letters. */\n+  public static final int UNICODE_AWARE = 4;\n\n\n\nErrr... this implies that there is something wrong with AnalyzingSuggester, or that this option actually even does anything at all for AnalyzingSuggester, when in fact it only changes the behavior of FuzzySuggester.\n\nCan we fix this? ",
            "author": "Robert Muir",
            "id": "comment-13697704"
        },
        {
            "date": "2013-07-02T13:25:41+0000",
            "content": "The javadocs are fixed. ",
            "author": "Artem Lukanin",
            "id": "comment-13697778"
        },
        {
            "date": "2013-07-03T13:27:02+0000",
            "content": "Maybe we should rename UNICODE_AWARE to FUZZY_UNICODE_AWARE?  (Because AnalyzingSuggester itself is already unicode aware... so this flag only impacts FuzzySuggester.) ",
            "author": "Michael McCandless",
            "id": "comment-13698951"
        },
        {
            "date": "2013-07-03T15:16:14+0000",
            "content": "Hmm also \"ant precommit\" is failing ... ",
            "author": "Michael McCandless",
            "id": "comment-13699074"
        },
        {
            "date": "2013-07-03T18:44:20+0000",
            "content": "in ant precommit I get this error:\nextra-target.xml:68: javax.script.ScriptException: javax.script.ScriptException: org.tmatesoft.svn.core.SVNException: svn: E155007: 'lucene-solr' is not a working copy\n\nI use git, not SVN, so I'm a bit confused...\n\nWhat error do you get? ",
            "author": "Artem Lukanin",
            "id": "comment-13699304"
        },
        {
            "date": "2013-07-03T19:04:01+0000",
            "content": "OK no problem, I can fix it.  The javadocs linter is angry that isUnicodeAware has no text (only an @return) ... ",
            "author": "Michael McCandless",
            "id": "comment-13699329"
        },
        {
            "date": "2013-07-03T21:06:17+0000",
            "content": "New patch, fixing the linter error, renaming UNICODE_AWARE -> FUZZY_UNICODE_AWARE, and fixing one compilation warning ... I think it's ready. ",
            "author": "Michael McCandless",
            "id": "comment-13699443"
        },
        {
            "date": "2013-07-04T07:05:18+0000",
            "content": "I have renamed the variables in comments and tests for consistency ",
            "author": "Artem Lukanin",
            "id": "comment-13699812"
        },
        {
            "date": "2013-07-16T11:55:41+0000",
            "content": "Sorry for the long delay here ...\n\nJust to verify: there is no point to passing FUZZY_UNICODE_AWARE to AnalyzingSuggester, right?\n\nIn which case, I think we the AnalyzingLookupFactory should not be changed?\n\nBut, furthermore, I think we can isolate the changes to FuzzySuggester?  E.g., move the FUZZY_UNICODE_AWARE flag down to FuzzySuggester, fix its ctor to strip that option when calling super() and move the isFuzzyUnicodeAware down as well, and then override toLookupAutomaton to do the utf8 conversion + det?\n\nThis way it's not even possible to send the fuzzy flag to AnalyzingSuggester. ",
            "author": "Michael McCandless",
            "id": "comment-13709695"
        },
        {
            "date": "2013-07-17T06:44:39+0000",
            "content": "Then I have to override (and copy a lot of code) getTokenStreamToAutomaton, lookup, toFiniteStrings and make a lot of private variables protected.\nI think, this is not a good idea. ",
            "author": "Artem Lukanin",
            "id": "comment-13710797"
        },
        {
            "date": "2013-07-17T07:49:45+0000",
            "content": "Moved the parameter from AnalyzingLookupFactory to FuzzyLookupFactory ",
            "author": "Artem Lukanin",
            "id": "comment-13710830"
        },
        {
            "date": "2013-07-17T09:06:06+0000",
            "content": "Michael, I got your idea. I will refactor the code not to use FUZZY_UNICODE_AWARE in AnalyzingSuggester. ",
            "author": "Artem Lukanin",
            "id": "comment-13710889"
        },
        {
            "date": "2013-07-17T09:44:14+0000",
            "content": "The code is refactored not to touch AnalyzingSuggester. Please, review. ",
            "author": "Artem Lukanin",
            "id": "comment-13710905"
        },
        {
            "date": "2013-07-17T13:08:17+0000",
            "content": "Patch looks great!  Thanks Artem.  No more mixing in of fuzzy-ness\ninto AnalyzingSuggester.\n\nIt looks like we are doing the utf8 conversion + det twice per lookup,\nonce in convertAutomaton and once in getFullPrefixPaths.  But, I think\nthis is inevitable: the first conversion is on the \"straight\"\nautomaton, for exactFirst match, and the second one is on the lev\nautomaton, for non-exactFirst.\n\nReally we should only do the first convertAutomaton if exactFirst is\ntrue ... but this is an optimization so we don't need to fix it now. ",
            "author": "Michael McCandless",
            "id": "comment-13711050"
        },
        {
            "date": "2013-07-18T14:36:35+0000",
            "content": "Commit 1504490 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1504490 ]\n\nLUCENE-5030: FuzzySuggester can optionally measure edits in Unicode code points instead of UTF8 bytes ",
            "author": "ASF subversion and git services",
            "id": "comment-13712377"
        },
        {
            "date": "2013-07-18T14:42:10+0000",
            "content": "Commit 1504492 from Michael McCandless in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1504492 ]\n\nLUCENE-5030: FuzzySuggester can optionally measure edits in Unicode code points instead of UTF8 bytes ",
            "author": "ASF subversion and git services",
            "id": "comment-13712380"
        },
        {
            "date": "2013-07-18T14:43:11+0000",
            "content": "OK I committed the last patch with a few small fixes:\n\n\n\tAdded @lucene.experimental to FuzzySuggester\n\n\n\n\n\tRemoved the added ctor (so we have just two ctors: the easy one,\n    which uses all defaults, and the expert one, where you specify\n    everything)\n\n\n\n\n\tRemoved System.out.printlns from the test\n\n\n\nThanks Artem! ",
            "author": "Michael McCandless",
            "id": "comment-13712381"
        },
        {
            "date": "2013-07-18T15:21:12+0000",
            "content": "JUHUUUU!  Thanks for heavy committing - it took a long time, but now it is good! Many thanks, Uwe ",
            "author": "Uwe Schindler",
            "id": "comment-13712408"
        },
        {
            "date": "2013-07-18T16:57:09+0000",
            "content": "Great! Thanks for reviewing. ",
            "author": "Artem Lukanin",
            "id": "comment-13712488"
        },
        {
            "date": "2013-10-05T10:19:11+0000",
            "content": "4.5 release -> bulk close ",
            "author": "Adrien Grand",
            "id": "comment-13787095"
        }
    ]
}