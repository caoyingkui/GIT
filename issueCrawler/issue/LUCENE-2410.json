{
    "id": "LUCENE-2410",
    "title": "Optimize PhraseQuery",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/search"
        ],
        "type": "Improvement",
        "fix_versions": [
            "3.1",
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Looking the scorers for PhraseQuery, I think there are some speedups\nwe could do:\n\n\n\tThe AND part of the scorer (which advances to the next doc that\n    has all the terms), in PhraseScorer.doNext, should do the same\n    optimizing as BooleanQuery's ConjunctionScorer, ie sort terms from\n    rarest to most frequent.  I don't think it should use a linked\n    list/firstToLast() that it does today.\n\n\n\n\n\tWe do way too much work now when .score() is not called, because\n    we go and find all occurrences of the phrase in the doc, whereas\n    we should stop only after finding the first and then go and count\n    the rest if .score() is called.\n\n\n\n\n\tFor the exact case, I think we can use two int arrays to find the\n    matches.  The first array holds the count of how many times a term\n    in the phrase \"matched\" a phrase starting at that position.  When\n    that count == the number of terms in the phrase, it's a match.\n    The 2nd is a \"gen\" array (holds docID when that count was last\n    touched), to avoid clearing.  Ie when incrementing the count, if\n    the docID != gen, we reset count to 0.  I think this'd be faster\n    than the PQ we now use.  Downside of this is if you have immense\n    docs (position gets very large) we'd need 2 immense arrays.\n\n\n\nIt'd be great to do LUCENE-1252 along with this, ie factor\nPhraseScorer into two AND'd sub-scorers (LUCENE-1252 is open for\nthis).  The first one should be ConjunctionScorer, and the 2nd one\nchecks the positions (ie, either the exact or sloppy scorers).  This\nwould mean if the PhraseQuery is AND'd w/ other clauses (or, a filter\nis applied) we would save CPU by not checking the positions for a doc\nunless all other AND'd clauses accepted the doc.",
    "attachments": {
        "LUCENE-2410_rewrite.patch": "https://issues.apache.org/jira/secure/attachment/12444271/LUCENE-2410_rewrite.patch",
        "LUCENE-2410.patch": "https://issues.apache.org/jira/secure/attachment/12447251/LUCENE-2410.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-05-11T20:11:33+0000",
            "content": "Another thing we should fix \u2013 PhraseQuery of a single term should rewrite to TermQuery. ",
            "author": "Michael McCandless",
            "id": "comment-12866305"
        },
        {
            "date": "2010-05-12T03:13:00+0000",
            "content": "just doing the easy part here, here's the rewrite patch.\nI checked, MultiPhraseQuery already has it. ",
            "author": "Robert Muir",
            "id": "comment-12866437"
        },
        {
            "date": "2010-05-12T09:37:04+0000",
            "content": "Looks great Robert \u2013 I think you should go ahead & commit that and we'll work on the rest of these optos later. ",
            "author": "Michael McCandless",
            "id": "comment-12866526"
        },
        {
            "date": "2010-05-12T13:59:15+0000",
            "content": "Committed revisions 943493 (trunk), 943499 (3x) ",
            "author": "Robert Muir",
            "id": "comment-12866572"
        },
        {
            "date": "2010-06-16T18:38:37+0000",
            "content": "Attached initial rough patch, doing the 1st and 3rd bullets above.\nStill many nocommits, but all tests pass.\n\nI only did this for the exact case (I don't understand the sloppy\ncase!), so I modified ExactPhraseScorer to no longer subclass\nPhraseScorer and instead do everything on its own.\n\nI tested on a 20M doc Wikipedia index, best of 10 runs:\n\n\n\n\nQuery\nNo. hits\nTrunk QPS\nPatch QPS\nSpeedup\n\n\nUnited States\n314K\n4.29\n11.04\n2.6X faster\n\n\nUnited Kingdom Parliament\n7K\n20.33\n58.57\n2.9X faster\n\n\n\n\n\nThe speedup is great \n\nHowever, there's one problem w/ the patch that I must fix (and will\nbring these gains down), which is it requires 2 int arrays sized to\nthe max position encountered during the search (which for a large doc\ncould be very large).  I think to make this committable I'd have to\nswitch to processing the positions in chunks (like BooleanScorer). ",
            "author": "Michael McCandless",
            "id": "comment-12879443"
        },
        {
            "date": "2010-06-17T09:48:47+0000",
            "content": "New patch attached, switches over to chunking (processing the positions 4096 at once).\n\nUnited States is now 10.66 QPS (2.5X speedup) and United Kingdom Parliament is now 54.93 QPS (2.7X speedup).\n\nI think it's ready to commit... I'll wait a few days. ",
            "author": "Michael McCandless",
            "id": "comment-12879725"
        },
        {
            "date": "2010-06-17T10:38:44+0000",
            "content": "Attached patch w/ another optimization: if the freq of the 2 rarest terms in the phrase are \"closish\", then just use .nextDoc() instead of .advance() when ANDing.  This buys another 15% speedup (12.30 QPS, net 2.9X faster than trunk) on United States phrase query.\n\nAlso, I fixed MultiPhraseQuery to sort its clauses by approx docFreq; the optimization is approx in this case because we can't efficiently compute the docFreq of a position that's unioning > 1 term. ",
            "author": "Michael McCandless",
            "id": "comment-12879741"
        },
        {
            "date": "2010-06-17T16:17:37+0000",
            "content": "New patch \u2013 makes the \"useAdvance\" per-Term, and, adds a safety fallback to .advance if too many .nextDocs are used. ",
            "author": "Michael McCandless",
            "id": "comment-12879828"
        },
        {
            "date": "2010-06-17T16:46:32+0000",
            "content": "Fantastic!  Phrase queries have often been a bottleneck. ",
            "author": "Yonik Seeley",
            "id": "comment-12879838"
        },
        {
            "date": "2010-06-17T17:55:33+0000",
            "content": "Very nice, Mike!\n\nAnother improvement we could make for positional queries (phrases, span queries) would be skip lists on the positions, maybe in a different codec?  This would probably be a nice speedup for large docs. ",
            "author": "Michael Busch",
            "id": "comment-12879870"
        },
        {
            "date": "2010-07-05T22:35:57+0000",
            "content": "Alas.... I think I somehow screwed up my performance tests above.\n\nI'm testing search perf (working on LUCENE-2504), and in comparing search perf from 2.9.x -> 3.x, I only saw a ~20% speedup on the phrase query \"united states\", for a 5M doc Wikipedia index.  And, re-running the test on trunk pre and post this commit, I still see only ~20% gain.... still not sure what I did wrong.\n\nI'll update CHANGES.  Two steps forward, one step back... sigh. ",
            "author": "Michael McCandless",
            "id": "comment-12885359"
        },
        {
            "date": "2011-03-30T15:50:08+0000",
            "content": "Bulk close for 3.1 ",
            "author": "Grant Ingersoll",
            "id": "comment-13013376"
        }
    ]
}