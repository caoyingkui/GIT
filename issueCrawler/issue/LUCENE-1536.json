{
    "id": "LUCENE-1536",
    "title": "if a filter can support random access API, we should use it",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/search"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "affect_versions": "2.4",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "I ran some performance tests, comparing applying a filter via\nrandom-access API instead of current trunk's iterator API.\n\nThis was inspired by LUCENE-1476, where we realized deletions should\nreally be implemented just like a filter, but then in testing found\nthat switching deletions to iterator was a very sizable performance\nhit.\n\nSome notes on the test:\n\n\n\tIndex is first 2M docs of Wikipedia.  Test machine is Mac OS X\n    10.5.6, quad core Intel CPU, 6 GB RAM, java 1.6.0_07-b06-153.\n\n\n\n\n\tI test across multiple queries.  1-X means an OR query, eg 1-4\n    means 1 OR 2 OR 3 OR 4, whereas +1-4 is an AND query, ie 1 AND 2\n    AND 3 AND 4.  \"u s\" means \"united states\" (phrase search).\n\n\n\n\n\tI test with multiple filter densities (0, 1, 2, 5, 10, 25, 75, 90,\n    95, 98, 99, 99.99999 (filter is non-null but all bits are set),\n    100 (filter=null, control)).\n\n\n\n\n\tMethod high means I use random-access filter API in\n    IndexSearcher's main loop.  Method low means I use random-access\n    filter API down in SegmentTermDocs (just like deleted docs\n    today).\n\n\n\n\n\tBaseline (QPS) is current trunk, where filter is applied as iterator up\n    \"high\" (ie in IndexSearcher's search loop).",
    "attachments": {
        "luceneutil.patch": "https://issues.apache.org/jira/secure/attachment/12498229/luceneutil.patch",
        "LUCENE-1536-rewrite.patch": "https://issues.apache.org/jira/secure/attachment/12497980/LUCENE-1536-rewrite.patch",
        "LUCENE-1536_hack.patch": "https://issues.apache.org/jira/secure/attachment/12498440/LUCENE-1536_hack.patch",
        "CachedFilterIndexReader.java": "https://issues.apache.org/jira/secure/attachment/12454467/CachedFilterIndexReader.java",
        "changes-yonik-uwe.patch": "https://issues.apache.org/jira/secure/attachment/12498289/changes-yonik-uwe.patch",
        "LUCENE-1536.patch": "https://issues.apache.org/jira/secure/attachment/12399482/LUCENE-1536.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-02-04T20:30:14+0000",
            "content": "Test results:\n\n\n\n\n%tg Filter\nQuery\nMethod\nHits\nQPS\nQPSNew\n%tg change\n\n\n0%\n1\nlow\n      0\n18992.3\n 142.6\n-99.2%\n\n\n0%\n1\nhigh\n      0\n18992.3\n 109.6\n-99.4%\n\n\n1%\n1\nlow\n   3863\n 133.7\n 135.3\n  1.2%\n\n\n1%\n1\nhigh\n   3863\n 133.7\n  99.7\n-25.4%\n\n\n2%\n1\nlow\n   7714\n 108.2\n 133.7\n 23.6%\n\n\n2%\n1\nhigh\n   7714\n 108.2\n 100.5\n -7.1%\n\n\n5%\n1\nlow\n  19333\n  76.9\n 128.6\n 67.2%\n\n\n5%\n1\nhigh\n  19333\n  76.9\n  97.2\n 26.4%\n\n\n10%\n1\nlow\n  38673\n  62.5\n 119.1\n 90.6%\n\n\n10%\n1\nhigh\n  38673\n  62.5\n  92.0\n 47.2%\n\n\n25%\n1\nlow\n  96670\n  47.3\n 102.3\n116.3%\n\n\n25%\n1\nhigh\n  96670\n  47.3\n  90.5\n 91.3%\n\n\n50%\n1\nlow\n 193098\n  40.0\n  85.6\n114.0%\n\n\n50%\n1\nhigh\n 193098\n  40.0\n  79.6\n 99.0%\n\n\n75%\n1\nlow\n 289765\n  38.0\n  82.8\n117.9%\n\n\n75%\n1\nhigh\n 289765\n  38.0\n  79.0\n107.9%\n\n\n90%\n1\nlow\n 347762\n  37.2\n  82.7\n122.3%\n\n\n90%\n1\nhigh\n 347762\n  37.2\n  72.8\n 95.7%\n\n\n95%\n1\nlow\n 367102\n  36.5\n  82.9\n127.1%\n\n\n95%\n1\nhigh\n 367102\n  36.5\n  73.1\n100.3%\n\n\n98%\n1\nlow\n 378721\n  37.3\n  81.8\n119.3%\n\n\n98%\n1\nhigh\n 378721\n  37.3\n  73.0\n 95.7%\n\n\n99%\n1\nlow\n 382572\n  36.6\n  83.3\n127.6%\n\n\n99%\n1\nhigh\n 382572\n  36.6\n  71.8\n 96.2%\n\n\n99.99999%\n1\nlow\n 386435\n  38.0\n  83.8\n120.5%\n\n\n99.99999%\n1\nhigh\n 386435\n  38.0\n  70.9\n 86.6%\n\n\n100%\n1\nlow\n 386435\n  88.0\n  89.1\n  1.2%\n\n\n100%\n1\nhigh\n 386435\n  88.0\n  89.5\n  1.7%\n\n\n0%\n1-2\nlow\n      0\n18808.1\n  71.5\n-99.6%\n\n\n0%\n1-2\nhigh\n      0\n18808.1\n  22.4\n-99.9%\n\n\n1%\n1-2\nlow\n   5363\n  46.8\n  65.2\n 39.3%\n\n\n1%\n1-2\nhigh\n   5363\n  46.8\n  22.5\n-51.9%\n\n\n2%\n1-2\nlow\n  10675\n  37.6\n  61.4\n 63.3%\n\n\n2%\n1-2\nhigh\n  10675\n  37.6\n  22.4\n-40.4%\n\n\n5%\n1-2\nlow\n  26880\n  28.8\n  53.4\n 85.4%\n\n\n5%\n1-2\nhigh\n  26880\n  28.8\n  22.3\n-22.6%\n\n\n10%\n1-2\nlow\n  53673\n  23.7\n  48.1\n103.0%\n\n\n10%\n1-2\nhigh\n  53673\n  23.7\n  21.8\n -8.0%\n\n\n25%\n1-2\nlow\n 133988\n  19.9\n  37.2\n 86.9%\n\n\n25%\n1-2\nhigh\n 133988\n  19.9\n  21.0\n  5.5%\n\n\n50%\n1-2\nlow\n 267757\n  17.2\n  27.4\n 59.3%\n\n\n50%\n1-2\nhigh\n 267757\n  17.2\n  20.3\n 18.0%\n\n\n75%\n1-2\nlow\n 401596\n  16.9\n  23.1\n 36.7%\n\n\n75%\n1-2\nhigh\n 401596\n  16.9\n  20.3\n 20.1%\n\n\n90%\n1-2\nlow\n 481911\n  17.0\n  21.2\n 24.7%\n\n\n90%\n1-2\nhigh\n 481911\n  17.0\n  20.6\n 21.2%\n\n\n95%\n1-2\nlow\n 508704\n  17.1\n  20.7\n 21.1%\n\n\n95%\n1-2\nhigh\n 508704\n  17.1\n  20.7\n 21.1%\n\n\n98%\n1-2\nlow\n 524909\n  17.3\n  20.7\n 19.7%\n\n\n98%\n1-2\nhigh\n 524909\n  17.3\n  20.7\n 19.7%\n\n\n99%\n1-2\nlow\n 530221\n  17.4\n  20.5\n 17.8%\n\n\n99%\n1-2\nhigh\n 530221\n  17.4\n  20.8\n 19.5%\n\n\n99.99999%\n1-2\nlow\n 535584\n  17.1\n  20.3\n 18.7%\n\n\n99.99999%\n1-2\nhigh\n 535584\n  17.1\n  20.3\n 18.7%\n\n\n100%\n1-2\nlow\n 535584\n  21.0\n  20.7\n -1.4%\n\n\n100%\n1-2\nhigh\n 535584\n  21.0\n  20.8\n -1.0%\n\n\n0%\n1-4\nlow\n      0\n17961.7\n  42.2\n-99.8%\n\n\n0%\n1-4\nhigh\n      0\n17961.7\n  11.9\n-99.9%\n\n\n1%\n1-4\nlow\n   6544\n  27.1\n  38.4\n 41.7%\n\n\n1%\n1-4\nhigh\n   6544\n  27.1\n  12.0\n-55.7%\n\n\n2%\n1-4\nlow\n  13062\n  21.4\n  36.0\n 68.2%\n\n\n2%\n1-4\nhigh\n  13062\n  21.4\n  11.9\n-44.4%\n\n\n5%\n1-4\nlow\n  32815\n  16.1\n  31.3\n 94.4%\n\n\n5%\n1-4\nhigh\n  32815\n  16.1\n  11.8\n-26.7%\n\n\n10%\n1-4\nlow\n  65491\n  13.3\n  27.8\n109.0%\n\n\n10%\n1-4\nhigh\n  65491\n  13.3\n  11.7\n-12.0%\n\n\n25%\n1-4\nlow\n 163600\n  10.5\n  21.0\n100.0%\n\n\n25%\n1-4\nhigh\n 163600\n  10.5\n  11.5\n  9.5%\n\n\n50%\n1-4\nlow\n 327302\n   9.9\n  15.3\n 54.5%\n\n\n50%\n1-4\nhigh\n 327302\n   9.9\n  11.2\n 13.1%\n\n\n75%\n1-4\nlow\n 490881\n   9.7\n  12.6\n 29.9%\n\n\n75%\n1-4\nhigh\n 490881\n   9.7\n  11.1\n 14.4%\n\n\n90%\n1-4\nlow\n 588990\n   9.6\n  11.6\n 20.8%\n\n\n90%\n1-4\nhigh\n 588990\n   9.6\n  11.1\n 15.6%\n\n\n95%\n1-4\nlow\n 621666\n   9.6\n  11.3\n 17.7%\n\n\n95%\n1-4\nhigh\n 621666\n   9.6\n  11.2\n 16.7%\n\n\n98%\n1-4\nlow\n 641419\n   9.4\n  11.1\n 18.1%\n\n\n98%\n1-4\nhigh\n 641419\n   9.4\n  11.2\n 19.1%\n\n\n99%\n1-4\nlow\n 647937\n   9.8\n  11.1\n 13.3%\n\n\n99%\n1-4\nhigh\n 647937\n   9.8\n  11.2\n 14.3%\n\n\n99.99999%\n1-4\nlow\n 654481\n  10.0\n  11.1\n 11.0%\n\n\n99.99999%\n1-4\nhigh\n 654481\n  10.0\n  11.2\n 12.0%\n\n\n100%\n1-4\nlow\n 654481\n  11.3\n  11.3\n  0.0%\n\n\n100%\n1-4\nhigh\n 654481\n  11.3\n  11.2\n -0.9%\n\n\n0%\n1-10\nlow\n      0\n15990.0\n  22.5\n-99.9%\n\n\n0%\n1-10\nhigh\n      0\n15990.0\n   5.8\n-100.0%\n\n\n1%\n1-10\nlow\n   8406\n  13.1\n  20.2\n 54.2%\n\n\n1%\n1-10\nhigh\n   8406\n  13.1\n   5.8\n-55.7%\n\n\n2%\n1-10\nlow\n  16756\n  10.2\n  18.9\n 85.3%\n\n\n2%\n1-10\nhigh\n  16756\n  10.2\n   5.8\n-43.1%\n\n\n5%\n1-10\nlow\n  41937\n   7.7\n  16.5\n114.3%\n\n\n5%\n1-10\nhigh\n  41937\n   7.7\n   5.7\n-26.0%\n\n\n10%\n1-10\nlow\n  83828\n   6.3\n  14.5\n130.2%\n\n\n10%\n1-10\nhigh\n  83828\n   6.3\n   5.7\n -9.5%\n\n\n25%\n1-10\nlow\n 209328\n   5.3\n  10.9\n105.7%\n\n\n25%\n1-10\nhigh\n 209328\n   5.3\n   5.7\n  7.5%\n\n\n50%\n1-10\nlow\n 418668\n   4.9\n   7.8\n 59.2%\n\n\n50%\n1-10\nhigh\n 418668\n   4.9\n   5.5\n 12.2%\n\n\n75%\n1-10\nlow\n 628338\n   4.9\n   6.4\n 30.6%\n\n\n75%\n1-10\nhigh\n 628338\n   4.9\n   5.5\n 12.2%\n\n\n90%\n1-10\nlow\n 753838\n   4.9\n   5.9\n 20.4%\n\n\n90%\n1-10\nhigh\n 753838\n   4.9\n   5.5\n 12.2%\n\n\n95%\n1-10\nlow\n 795729\n   5.0\n   5.7\n 14.0%\n\n\n95%\n1-10\nhigh\n 795729\n   5.0\n   5.6\n 12.0%\n\n\n98%\n1-10\nlow\n 820910\n   5.1\n   5.6\n  9.8%\n\n\n98%\n1-10\nhigh\n 820910\n   5.1\n   5.6\n  9.8%\n\n\n99%\n1-10\nlow\n 829260\n   5.1\n   5.6\n  9.8%\n\n\n99%\n1-10\nhigh\n 829260\n   5.1\n   5.6\n  9.8%\n\n\n99.99999%\n1-10\nlow\n 837666\n   5.2\n   5.6\n  7.7%\n\n\n99.99999%\n1-10\nhigh\n 837666\n   5.2\n   5.6\n  7.7%\n\n\n100%\n1-10\nlow\n 837666\n   5.7\n   5.7\n  0.0%\n\n\n100%\n1-10\nhigh\n 837666\n   5.7\n   5.6\n -1.8%\n\n\n0%\n+1-2\nlow\n      0\n18848.4\n 138.4\n-99.3%\n\n\n0%\n+1-2\nhigh\n      0\n18848.4\n  27.4\n-99.9%\n\n\n1%\n+1-2\nlow\n   2308\n  63.1\n  77.0\n 22.0%\n\n\n1%\n+1-2\nhigh\n   2308\n  63.1\n  27.1\n-57.1%\n\n\n2%\n+1-2\nlow\n   4621\n  50.3\n  69.7\n 38.6%\n\n\n2%\n+1-2\nhigh\n   4621\n  50.3\n  26.8\n-46.7%\n\n\n5%\n+1-2\nlow\n  11706\n  36.1\n  56.9\n 57.6%\n\n\n5%\n+1-2\nhigh\n  11706\n  36.1\n  26.5\n-26.6%\n\n\n10%\n+1-2\nlow\n  23272\n  28.4\n  48.6\n 71.1%\n\n\n10%\n+1-2\nhigh\n  23272\n  28.4\n  26.2\n -7.7%\n\n\n25%\n+1-2\nlow\n  58401\n  23.7\n  36.4\n 53.6%\n\n\n25%\n+1-2\nhigh\n  58401\n  23.7\n  24.9\n  5.1%\n\n\n50%\n+1-2\nlow\n 117083\n  20.9\n  28.2\n 34.9%\n\n\n50%\n+1-2\nhigh\n 117083\n  20.9\n  23.9\n 14.4%\n\n\n75%\n+1-2\nlow\n 176233\n  19.3\n  24.4\n 26.4%\n\n\n75%\n+1-2\nhigh\n 176233\n  19.3\n  22.8\n 18.1%\n\n\n90%\n+1-2\nlow\n 211362\n  18.6\n  22.9\n 23.1%\n\n\n90%\n+1-2\nhigh\n 211362\n  18.6\n  22.5\n 21.0%\n\n\n95%\n+1-2\nlow\n 222928\n  18.5\n  22.5\n 21.6%\n\n\n95%\n+1-2\nhigh\n 222928\n  18.5\n  22.5\n 21.6%\n\n\n98%\n+1-2\nlow\n 230013\n  18.3\n  22.0\n 20.2%\n\n\n98%\n+1-2\nhigh\n 230013\n  18.3\n  22.4\n 22.4%\n\n\n99%\n+1-2\nlow\n 232326\n  18.3\n  22.1\n 20.8%\n\n\n99%\n+1-2\nhigh\n 232326\n  18.3\n  22.3\n 21.9%\n\n\n99.99999%\n+1-2\nlow\n 234634\n  17.8\n  21.9\n 23.0%\n\n\n99.99999%\n+1-2\nhigh\n 234634\n  17.8\n  22.2\n 24.7%\n\n\n100%\n+1-2\nlow\n 234634\n  22.9\n  22.7\n -0.9%\n\n\n100%\n+1-2\nhigh\n 234634\n  22.9\n  22.6\n -1.3%\n\n\n0%\n+1-4\nlow\n      0\n17987.0\n 137.9\n-99.2%\n\n\n0%\n+1-4\nhigh\n      0\n17987.0\n  18.0\n-99.9%\n\n\n1%\n+1-4\nlow\n    923\n  34.5\n  58.4\n 69.3%\n\n\n1%\n+1-4\nhigh\n    923\n  34.5\n  17.9\n-48.1%\n\n\n2%\n+1-4\nlow\n   1849\n  28.7\n  51.9\n 80.8%\n\n\n2%\n+1-4\nhigh\n   1849\n  28.7\n  17.9\n-37.6%\n\n\n5%\n+1-4\nlow\n   4794\n  22.1\n  39.0\n 76.5%\n\n\n5%\n+1-4\nhigh\n   4794\n  22.1\n  17.8\n-19.5%\n\n\n10%\n+1-4\nlow\n   9595\n  19.9\n  35.3\n 77.4%\n\n\n10%\n+1-4\nhigh\n   9595\n  19.9\n  17.5\n-12.1%\n\n\n25%\n+1-4\nlow\n  24136\n  17.3\n  25.7\n 48.6%\n\n\n25%\n+1-4\nhigh\n  24136\n  17.3\n  17.2\n -0.6%\n\n\n50%\n+1-4\nlow\n  48328\n  16.1\n  19.5\n 21.1%\n\n\n50%\n+1-4\nhigh\n  48328\n  16.1\n  16.8\n  4.3%\n\n\n75%\n+1-4\nlow\n  72718\n  15.8\n  17.0\n  7.6%\n\n\n75%\n+1-4\nhigh\n  72718\n  15.8\n  16.6\n  5.1%\n\n\n90%\n+1-4\nlow\n  87259\n  15.3\n  16.3\n  6.5%\n\n\n90%\n+1-4\nhigh\n  87259\n  15.3\n  16.4\n  7.2%\n\n\n95%\n+1-4\nlow\n  92060\n  15.4\n  16.0\n  3.9%\n\n\n95%\n+1-4\nhigh\n  92060\n  15.4\n  16.4\n  6.5%\n\n\n98%\n+1-4\nlow\n  95005\n  15.4\n  15.8\n  2.6%\n\n\n98%\n+1-4\nhigh\n  95005\n  15.4\n  16.4\n  6.5%\n\n\n99%\n+1-4\nlow\n  95931\n  15.1\n  15.7\n  4.0%\n\n\n99%\n+1-4\nhigh\n  95931\n  15.1\n  16.4\n  8.6%\n\n\n99.99999%\n+1-4\nlow\n  96854\n  14.3\n  15.9\n 11.2%\n\n\n99.99999%\n+1-4\nhigh\n  96854\n  14.3\n  16.2\n 13.3%\n\n\n100%\n+1-4\nlow\n  96854\n  16.6\n  16.6\n  0.0%\n\n\n100%\n+1-4\nhigh\n  96854\n  16.6\n  16.6\n  0.0%\n\n\n0%\n\"u s\"\nlow\n      0\n19123.0\n 124.9\n-99.3%\n\n\n0%\n\"u s\"\nhigh\n      0\n19123.0\n   7.0\n-100.0%\n\n\n1%\n\"u s\"\nlow\n   3192\n  23.5\n  27.3\n 16.2%\n\n\n1%\n\"u s\"\nhigh\n   3192\n  23.5\n   7.1\n-69.8%\n\n\n2%\n\"u s\"\nlow\n   6179\n  17.8\n  24.3\n 36.5%\n\n\n2%\n\"u s\"\nhigh\n   6179\n  17.8\n   7.0\n-60.7%\n\n\n5%\n\"u s\"\nlow\n  15446\n  12.7\n  20.3\n 59.8%\n\n\n5%\n\"u s\"\nhigh\n  15446\n  12.7\n   7.0\n-44.9%\n\n\n10%\n\"u s\"\nlow\n  30858\n  10.1\n  16.1\n 59.4%\n\n\n10%\n\"u s\"\nhigh\n  30858\n  10.1\n   6.8\n-32.7%\n\n\n25%\n\"u s\"\nlow\n  77138\n   7.7\n  13.0\n 68.8%\n\n\n25%\n\"u s\"\nhigh\n  77138\n   7.7\n   6.8\n-11.7%\n\n\n50%\n\"u s\"\nlow\n 154331\n   6.7\n   9.9\n 47.8%\n\n\n50%\n\"u s\"\nhigh\n 154331\n   6.7\n   7.0\n  4.5%\n\n\n75%\n\"u s\"\nlow\n 231412\n   6.3\n   8.4\n 33.3%\n\n\n75%\n\"u s\"\nhigh\n 231412\n   6.3\n   7.0\n 11.1%\n\n\n90%\n\"u s\"\nlow\n 277692\n   5.8\n   7.2\n 24.1%\n\n\n90%\n\"u s\"\nhigh\n 277692\n   5.8\n   7.0\n 20.7%\n\n\n95%\n\"u s\"\nlow\n 293104\n   5.8\n   7.1\n 22.4%\n\n\n95%\n\"u s\"\nhigh\n 293104\n   5.8\n   7.0\n 20.7%\n\n\n98%\n\"u s\"\nlow\n 302371\n   5.8\n   7.0\n 20.7%\n\n\n98%\n\"u s\"\nhigh\n 302371\n   5.8\n   6.9\n 19.0%\n\n\n99%\n\"u s\"\nlow\n 305358\n   5.8\n   6.9\n 19.0%\n\n\n99%\n\"u s\"\nhigh\n 305358\n   5.8\n   6.9\n 19.0%\n\n\n99.99999%\n\"u s\"\nlow\n 308550\n   5.8\n   6.8\n 17.2%\n\n\n99.99999%\n\"u s\"\nhigh\n 308550\n   5.8\n   6.9\n 19.0%\n\n\n100%\n\"u s\"\nlow\n 308550\n   7.0\n   6.9\n -1.4%\n\n\n100%\n\"u s\"\nhigh\n 308550\n   7.0\n   6.9\n -1.4%\n\n\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12670449"
        },
        {
            "date": "2009-02-04T20:34:30+0000",
            "content": "Attaching patch I'm using to run tests.  NOTE: this is nowhere near\ncommittable.  It's just a hack to allow testing the different ways of\napplying filters. ",
            "author": "Michael McCandless",
            "id": "comment-12670451"
        },
        {
            "date": "2009-02-04T20:37:17+0000",
            "content": "It's a ridiculous amount of data to digest, but here are some initial\nobservations/thoughts:\n\n\n\tThere are very sizable gains here by switching to random-access\n    low (ie, handling top-level filter the way we now handle deletes).\n    I'm especially interested in gains in the slowest queries.  EG the\n    phrase query \"united states\" sees QPS gains from 16%-69%.  The\n    10-clause OR query 1-10 sees QPS gains between 8% and 130%.\n\n\n\n\n\tResults are consistent with LUCENE-1476: random-access low gives\n    the best performance when filter density is >= 1%.\n\n\n\n\n\tHigh is worse than trunk up until ~25% density, which makes sense\n    since we are asking Scorer to do alot of work producing docIDs\n    that we then nix with the filter.\n\n\n\n\n\tLow is consistently better than high, though as filter density\n    gets higher the gap between them narrows.  I'll drop high from\n    future tests.\n\n\n\n\n\tThe gains are generally strongest in the \"moderate\" density range,\n    5-25%.\n\n\n\n\n\tThe degenerate 0% case is clearly far far worse, which is expected\n    since the iterator scans the bits, finds none set, and quickly\n    ends the search.  For very low density filters we should continue\n    to use iterator.\n\n\n\n\n\tThe \"control\" 100% case (where filter is null) is about the same,\n    which is expected.\n\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12670452"
        },
        {
            "date": "2009-02-04T20:43:04+0000",
            "content": "Some ideas / further things to explore:\n\n\n\tDeletions, top-level filters, and BooleanQuery that \"factors\" to a\n    toplevel AND really should be handled by the same code.\n\n\n\n\n\tEven an AND'd filter on a sub-clause of a BooleanQuery can be\n    pushed down to the TermDocs under that tree.\n\n\n\n\n\tThat common code should send a top-level filter down to the lowest\n    level, used by random access API, if the filter supports random\n    access (not all do) and it's not super sparse.\n\n\n\n\n\tI think one thing slowing down trunk is the lack of a\n    Scorer.skipToButNotNext API.  We now ask the filter for its\n    next(), which gives us a filterDocID.  Then we call\n    Scorer.skipTo(filterDocID).  If the scorer does not match that\n    filterDocID, it internally does next(), which for an expensive\n    scorer is alot of likely wasted work: it advances to a docID that\n    the filter may not accept.  If we had a \"skipToButNotNext\" API we\n    could avoid that wasted work.  I'm curious what gains this change\n    alone would provide.\n\n\n\n\n\tI'm thinking (but haven't tested this) if the filter is relatively\n    sparse compared to the other iterators, it'd be better to convert\n    it to a sparse repr (eg SortedVIntList) and drive the search by\n    iteration through the filter, after fixing the above skipTo issue.\n    Maybe a \"low iterator\" access.\n\n\n\n\n\tWe may need a \"filter optimizer\" utility class somewhere, somehow.\n    For filters you do not plan to re-use, you would not bother with\n    this.  But for filters that will be re-used, you should 1) convert\n    them to sparse or non-sparse repr depending on their density, 2)\n    maybe invert them and make sparse if they are close to 100%\n    density, 3) maybe factor in deletions to the filter so there is\n    only a single top-level filter to apply.\n\n\n\n\n\tI'm not yet sure how to make this change cleanly to the APIs...\n\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12670455"
        },
        {
            "date": "2009-02-09T20:51:14+0000",
            "content": "\nOK I tested a different approach for matching when the filter is\nrelatively sparse filter (<= 10% of index size), by implementing a\nsimplistic prototype \"random access\" scorer API (JumpScorer), which\nonly exposes the method \"boolean jump(int docID)\" that returns true if\nthat doc matches, else false (and no next() under the hood).\n\nI only implemented JumpScorer for pure AND/OR queries (ie no excluded\nterms), so I only test for these queries.\n\nI convert the filter to SortedVIntList up front, so iteration is fast.\nThen during matching I iterate through each doc in the filter, and ask\nthe random-access scoring API to test whether it accepts the doc, and\ncollect it if so.\n\nThis only performs better when the filter is sparse relative to the\nquery.  Once we merge Query/Filter, I think this optimization can more\ngenerally be used whenever one sub-query is very restrictive compared\nto the rest of the sub-queries.\n\nThis is basically the reverse of what I first tested, which was to\ntake a filter that can support random access API and \"distribute\" it\ndown to each TermQuery, which gives very good gains especially when\nfilter is in the middle of the sparse/dense range.  Whereas this test\nkeeps the iterator API on the filter, but switches to a random access\nAPI on the scorer.\n\nResults:\n\n\n\n\n%tg Filter\nQuery\nHits\nQPS\nQPSNew\n%tg change\n\n\n1%\n1-2\n   5363\n  47.4\n  66.7\n 40.7%\n\n\n2%\n1-2\n  10675\n  37.6\n  50.6\n 34.6%\n\n\n5%\n1-2\n  26880\n  28.6\n  37.0\n 29.4%\n\n\n10%\n1-2\n  53673\n  23.8\n  26.2\n 10.1%\n\n\n1%\n1-4\n   6544\n  26.9\n  37.2\n 38.3%\n\n\n2%\n1-4\n  13062\n  21.2\n  29.2\n 37.7%\n\n\n5%\n1-4\n  32815\n  16.1\n  21.2\n 31.7%\n\n\n10%\n1-4\n  65491\n  13.2\n  15.2\n 15.2%\n\n\n1%\n1-10\n   8406\n  13.0\n  17.6\n 35.4%\n\n\n2%\n1-10\n  16756\n  10.2\n  14.2\n 39.2%\n\n\n5%\n1-10\n  41937\n   7.7\n  10.3\n 33.8%\n\n\n10%\n1-10\n  83828\n   6.3\n   7.8\n 23.8%\n\n\n1%\n+1-2\n   2308\n  63.6\n  82.9\n 30.3%\n\n\n2%\n+1-2\n   4621\n  49.9\n  60.7\n 21.6%\n\n\n5%\n+1-2\n  11706\n  35.8\n  47.6\n 33.0%\n\n\n10%\n+1-2\n  23272\n  28.3\n  35.5\n 25.4%\n\n\n1%\n+1-4\n    923\n  34.4\n  58.0\n 68.6%\n\n\n2%\n+1-4\n   1849\n  28.5\n  44.9\n 57.5%\n\n\n5%\n+1-4\n   4794\n  22.0\n  33.7\n 53.2%\n\n\n10%\n+1-4\n   9595\n  19.8\n  25.4\n 28.3%\n\n\n1%\n+1-10\n    292\n  17.0\n  36.6\n115.3%\n\n\n2%\n+1-10\n    579\n  15.2\n  30.2\n 98.7%\n\n\n5%\n+1-10\n   1517\n  13.5\n  22.2\n 64.4%\n\n\n10%\n+1-10\n   2999\n  12.4\n  17.4\n 40.3%\n\n\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12672005"
        },
        {
            "date": "2009-03-06T17:46:10+0000",
            "content": "For the skipToButNotNext() did you mean sth like LUCENE-1252  ? ",
            "author": "Paul Elschot",
            "id": "comment-12679665"
        },
        {
            "date": "2009-03-06T19:04:35+0000",
            "content": "\nAhh, that's actually something different but also a neat idea to\nexplore.\n\nI want a way to skipTo(docID) without having it then internally do a\nnext() if the docID was not accepted.  Basically a random-access\n\"accepts(int docID)\" API, that's called only on increasing docIDs.\nImplementing \"accepts\" for queries is often alot simpler than\nimplementing next/skipTo.\n\nLUCENE-1252 wants a way to expose access to the two constraints within\na single query separately.  EG a phrase search 1) must have all N\nterms, and 2) must have them in the right positions.  But if you could\ncheck only 1), and if it passes next check the filter on the search,\nand if it still passes go back and check 2), then that could give\nbetter search performance.\n\nI think there's decent room for improving search performance of\ncomplex queries. ",
            "author": "Michael McCandless",
            "id": "comment-12679690"
        },
        {
            "date": "2009-04-14T12:31:10+0000",
            "content": "Given the sizable performance gains, I think we should try to do this for 2.9, if possible. ",
            "author": "Michael McCandless",
            "id": "comment-12698755"
        },
        {
            "date": "2009-04-16T07:18:31+0000",
            "content": "How about DocIdSet adds a\n\nboolean isRandomAccess() { return false; }\n\n\nThat is implemented to return false in the default abstract class for backwards compatibility.\nIf a DocIdSet is random access (backed by OpenBitSet or is the empty iterator), isRandomAccess() is overridden to return true and an additional method in DocIdSet is implemented, the default would be:\n\nboolean acceptDoc(int docid) { throw new UnsupportedOperationException(); }\n\n\nBoth changes are backwards compatible, but filters using OpenBitSet would automatically be random access and support acceptDoc(). ",
            "author": "Uwe Schindler",
            "id": "comment-12699558"
        },
        {
            "date": "2009-04-16T07:53:36+0000",
            "content": "The empty docidset instance should not be random access , so the only change would affect OpenBitSet to overwrite these two new methods from the default abstract class:\n\nboolean isRandomAccess() { return true; }\nboolean acceptDoc(int docid) { return fastGet(docid); /* possibly inlined */ }\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-12699571"
        },
        {
            "date": "2009-04-16T07:57:04+0000",
            "content": "And the switch for different densities:\nOpenBitSet could calculate its density in isRandomAccess() and return true or false depending on the density factors above. The search code then would only check initially isRandomAccess() (before starting filtering) and then switch between iterator or random acess api. ",
            "author": "Uwe Schindler",
            "id": "comment-12699573"
        },
        {
            "date": "2009-04-16T12:20:04+0000",
            "content": "I like this approach!\n\nBut should we somehow decouple the density check vs the is random access check?  Ie, isRandomAccess should return true or false based on the underlying datastructure.  Then, somehow, I think the search code should determine whether a given docIdSet should be randomly accessed vs iterated?  (I'm not sure how yet!)\n\nAlso, we somehow need the mechanism to \"denormalize\" the application of the filter from top to bottom, ie, each leaf TermQuery involved in the full query needs to know to apply the random access filter just like it applies deletes. ",
            "author": "Michael McCandless",
            "id": "comment-12699669"
        },
        {
            "date": "2009-04-16T12:34:27+0000",
            "content": "I coupled the density check inside the OpenBitSet, because the internals of OpenBitset are responsible for determining how fast a sequential vs. random approach is. Maybe someone invents an new hyper-bitset that can faster do sequential accesses even in sparingly filled bitsets (e.g. fragmented bitset, bitset with RDBMS-like \"index\"). In this case, it has the responsibility to say: if density is between this and this i would use sequential. ",
            "author": "Uwe Schindler",
            "id": "comment-12699675"
        },
        {
            "date": "2009-04-16T12:43:34+0000",
            "content": "OK, if we do choose to couple, maybe we should name it \"useRandomAccess()\"?\n\nAnother filter optimization that'd be nice to get in is to somehow \"know\" that a filter has pre-incorporated deleted documents.  This way, once we have a solution for the \"push filter down to all TermScorers\", we could have it only check the filter and not also deleted docs.  (This is one of the optimizations in LUCENE-1594).\n\nWe might eventually want/need some sort of external FilterManager that would handle this (ie, convert a filter to sparse vs random-access as appropriate, multiply in deleted docs, handle caching, etc). ",
            "author": "Michael McCandless",
            "id": "comment-12699680"
        },
        {
            "date": "2009-04-16T21:52:23+0000",
            "content": "I thought we are going to get LUCENE-1518 working to compare the performance against passing the filter into TermDocs?  ",
            "author": "Jason Rutherglen",
            "id": "comment-12699892"
        },
        {
            "date": "2009-04-17T00:18:11+0000",
            "content": "Ahh right, we should re-test performance of this after LUCENE-1518 is done. ",
            "author": "Michael McCandless",
            "id": "comment-12699939"
        },
        {
            "date": "2009-04-20T23:26:19+0000",
            "content": "Perhaps we can go ahead with this patch given we're not sure how\nto do an optimized version of LUCENE-1518 yet. This patch\nentails passing the RandomAccessFilter to TermScorer, what's a\ngood way to do this without rewriting too much of the Lucene API?\n\n\n\tTermQuery.createWeight -> TermWeight.scorer instantiates the\nTermScorer which is where we need to pass in the filter? So we\ncould somehow pass the filter in via multiple constructors? I\ndidn't see a clean API way though.\n\n\n\n\n\tOr we can add a new method to Scorer, something like\ngetSequentialSubScorers? Which we then iterate over and if one\nis a TermScorer set the filter(s). This setting of the RAF would\nhappen in IndexSearcher.doSearch. \n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12700984"
        },
        {
            "date": "2009-04-21T02:48:11+0000",
            "content": "The patch is a start at something that will work within the API\n(I think). \n\n\n\tCreated a RandomAccessDocIdSet class that BitVector and\nOpenBitSet implement. \n\n\n\n\n\tSegmentTermDocs has a setter for RandomAccessDocIdSet with the\noption of including the deletedDocs\n\n\n\n\n\tIndexSearcher.doSearch does the main work of setting the\nRandomAccessDocIdSet on the TermScorers\n\n\n\n\n\tBooleanScorer2 and other classes implement\ngetSequentialSubScorers\n\n\n\n\n\tAndRandomAccessDocIdSet iterates over an array of\nRandomAccessDocIdSets\n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12701024"
        },
        {
            "date": "2009-04-21T17:45:23+0000",
            "content": "\n\tFilter has a parameter for included deletes. IndexSearcher\nuses it for setting the RandomAccessDocIdSet on the Scorers\n\n\n\n\n\tMatchAllDocsScorer in addition to TermScorer properly supports\nsetting a RandomAccessDocIdSet \n\n\n\n\n\tAdded more test cases\n\n\n\n\n\tAndRandomAccessDocIdSet.iterator() and BitVector.iterator()\nneeds to be implemented \n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12701216"
        },
        {
            "date": "2009-04-30T11:42:05+0000",
            "content": "Interesting stuff!\nHas anyone tested if this results in a performance degradation on SegmentTermDocs?\nThis is very inner loop stuff, and it's replacing a \"non virtual\" BitVector.get() which can be easily inlined with two dispatches through base classes.  Hopefully hotspot could handle it, but it's tough to figure out, esp in a real system where sometimes a user RAF will be used and sometimes not. ",
            "author": "Yonik Seeley",
            "id": "comment-12704590"
        },
        {
            "date": "2009-05-01T14:46:25+0000",
            "content": "Has anyone tested if this results in a performance degradation on SegmentTermDocs?\n\nI haven't, and I'm also nervous.\n\nOne thing we could do is require a concrete impl (eg OpenBitSet) in order to use this optimization?\n\nI also think we should require pre-multiplication of the deleted docs, and possibly inversion of the filter, rather than doing it on the fly per docID.  Also, we shouldn't negate the deleted docs per doc when there's no RAF.  We may want/need to switch to OpenBitSet for deleted docs, in order to not make 2 copies of the STD code. ",
            "author": "Michael McCandless",
            "id": "comment-12705008"
        },
        {
            "date": "2009-05-01T14:56:58+0000",
            "content": "I don't think it should be the caller's job to getSequentialSubScorers\nand push down a RAF?  Rather, I think when requesting a scorer we\nshould pass in a RAF, requiring that the returned scorer factors it\nin (passing on to its own sub-scorers if needed). ",
            "author": "Michael McCandless",
            "id": "comment-12705012"
        },
        {
            "date": "2009-05-01T14:58:31+0000",
            "content": "\nI don't think it should be the caller's job to getSequentialSubScorers\nand push down a RAF?  Rather, I think when requesting a scorer we\nshould pass in a RAF, requiring that the returned scorer factors it\nin (passing on to its own sub-scorers if needed).\n\nOne thing we could do is require a concrete impl (eg OpenBitSet) in order to use this optimization?\n\nThis may be too restrictive, because another use case (touched on\nalready in LUCENE-1593, but actually much more similar to this issue)\nis when sorting by field.\n\nEG say we are sorting by int ascending, and from the\nFieldValueHitQueue we know the bottom value is 17.  Then, within\nTermScorer if we see a docID we should check if its value is greater\nthan 17 and skip it if so.\n\nVery likely this will be a sizable performance gain, but it would be a\nmajor change because you can only do this if you do not need the\nprecise totalHits back.\n\nSo... maybe we need to allow an abstract RandomAccesDocIdSet, to allow\nthis use case.  But perhaps we should negate its API?  Ie it exposes\n\"boolean reject(int docID)\". ",
            "author": "Michael McCandless",
            "id": "comment-12705013"
        },
        {
            "date": "2009-05-01T15:50:22+0000",
            "content": "> it would be amajor change because you can only do this if \n> you do not need the precise totalHits back.\n\nEarly termination (pruning) also messes with totalHits.  I think \nit would be good to get away from the idea that totalHits is \nreliable side effect. ",
            "author": "Marvin Humphrey",
            "id": "comment-12705029"
        },
        {
            "date": "2009-05-02T11:01:55+0000",
            "content": "\nEarly termination (pruning) also messes with totalHits. I think \nit would be good to get away from the idea that totalHits is \nreliable side effect.\n\nAgreed.  Does KS/Lucy not guarantee accurate totalHits returned?  Lucene today always returns the precise total hits.\n\nIf we can move away from that (optionally) then we can gain sizable performance.\n\nBut: we'd then presumably need to return an approx hit count. ",
            "author": "Michael McCandless",
            "id": "comment-12705257"
        },
        {
            "date": "2009-06-11T12:48:29+0000",
            "content": "Moving out. ",
            "author": "Michael McCandless",
            "id": "comment-12718430"
        },
        {
            "date": "2009-06-22T03:41:28+0000",
            "content": " I don't think it should be the caller's job to\ngetSequentialSubScorers and push down a RAF? Rather, I think\nwhen requesting a scorer we should pass in a RAF, requiring that\nthe returned scorer factors it in (passing on to its own\nsub-scorers if needed). \n\nTrue, however that requires adding a scorer(IndexReader, RAF)\nmethod to the Weight interface? Which means adding it to I think\n20 classes or so. Which is fine, however is that definitely what\nwe want to do? \n\nAlso I forget if we created a patch or benchmarked AND NOT\ndeleteDocs? ",
            "author": "Jason Rutherglen",
            "id": "comment-12722472"
        },
        {
            "date": "2009-06-22T09:47:29+0000",
            "content": "that requires adding a scorer(IndexReader, RAF) method to the Weight interface?\n\nWell, to QueryWeight (abstract class, not interface) that we are migrating to with LUCENE-1630.  We'd make a default impl, probably a static method somewhere, so that any external QueryWeight's \"out there\" would just work.\n\nThough, since QueryWeight is new in 2.9, we are free to make this an abstract method, and put the default only in QueryWeightWrapper, if we could get this issue done for 2.9.\n\nI think we'd also want control on whether the Scorer should apply deletes or not, so that for filters that are often re-used, we have the option to pre-fold deletes in. ",
            "author": "Michael McCandless",
            "id": "comment-12722538"
        },
        {
            "date": "2009-06-22T18:54:26+0000",
            "content": " since QueryWeight is new in 2.9, we are free to make\nthis an abstract method, and put the default only in\nQueryWeightWrapper, \n\nSounds good.\n\n we'd also want control on whether the Scorer should\napply deletes or not \n\nFilter.hasDeletes can be used know whether to apply deletes or\nnot. \n\nmigrating to with LUCENE-1630\n\nLooks this this will be committed soon, so I'll wait for that\nthen make the changes, then benchmark. ",
            "author": "Jason Rutherglen",
            "id": "comment-12722761"
        },
        {
            "date": "2009-09-15T22:14:53+0000",
            "content": "\n\tAdded a IndexReader.termDocs(term, filter) method which\n  culminates in passing a RandomAccessDocIdSet to the scorers.\n  ConstantScoreQuery and FilterQuery need to AND the filters\n  together. \n\n\n\n\n\tDoes explain need the filter?\n\n\n\n\n\tSpans aren't supported yet\n\n\n\n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12755738"
        },
        {
            "date": "2009-09-16T06:25:33+0000",
            "content": "\n\tThe test case includes testConstantCoreQuery\n\n\n\n\n\tIf LUCENE-1632 were updated to use the new DocIdSetIterator API,\n  it could be useful for the iterators of AndRandomAccessDocIdSet\n  and NotRandomAccessDocIdSet.\n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12755900"
        },
        {
            "date": "2010-04-07T17:36:00+0000",
            "content": "With flex, you can now get the deleted docs from a reader (returns a Bits, yet another interface for bitsets), and, Docs/AndPositionsEnums require that you pass in the skipDocs.  Ie they no longer skip deleted docs by default.\n\nI think this makes this issue quite a bit simpler?  EG OpenBitSets implements Bits (as of flex landing), so... we just need a way to pass this Bits down to the low level scorers that actually pull a postings list.  But, we should only do this if the filter is not sparse.\n\nAlso: the filter must be inverted, and, ORd with the deleted docs.\n\nThis can result in enormous perf gains for searches doing filtering where the filter is relatively static (can be cached & shared across searches). ",
            "author": "Michael McCandless",
            "id": "comment-12854612"
        },
        {
            "date": "2010-09-13T18:44:58+0000",
            "content": "By subclassing FilterIndexReader, and taking advantage of how the flex\nAPIs now let you pass a custom skipDocs when pulling the postings, I\ncreated a prototype class (attached, named CachedFilterIndexReader) that\nup-front compiles the deleted docs for each segment with the negation\nof a Filter (that you provide), and returns a reader that applies that\nfilter.\n\nThis is nice because it's fully external to Lucene, and it gives\nawesome gains in many cases (see\nhttp://chbits.blogspot.com/2010/09/fast-search-filters-using-flex.html).\n\nI don't think we should commit this class \u2013 we should instead fix\nFilters correctly!  But it's a nice workaround until we do that. ",
            "author": "Michael McCandless",
            "id": "comment-12908914"
        },
        {
            "date": "2010-11-05T22:49:44+0000",
            "content": "Hi Mike,\n\n   Wondering what are your thoughts on fixing filters correctly are? I think that the initial thought of getting filters all the way down to postings enumeration if they support random access is a great one. A random access doc id set can be added (interface), and if a filter returns it (can be checked using instanceof), then the that doc set can be passed all the way to the enumeration (and intersected per doc with the deleted docs).\n\n   I think that any type of solution should support the great feature of Lucene queries, for example, FilteredQuery should use that, allowing to build complex query expressions without having the mentioned optimization only applied on the top level search.\n\n   As most filters results do support random access, either because they use OpenBitSet, or because they are built on top of FieldCache functionality, I think this feature will give great speed improvements to the query execution time. ",
            "author": "Shay Banon",
            "id": "comment-12928859"
        },
        {
            "date": "2010-11-06T10:29:47+0000",
            "content": "Wondering what are your thoughts on fixing filters correctly are?\n\nI think the approach you outlined is the right one!\n\nWe already have the APIs in flex (Bits interface for random access, postings APIs take a Bits skipDocs); in backporting to 3.x I think we'd just port Bits back.\n\nThere are some challenges though:\n\n\n\tWe should add a method to Filter to ask it if its already folded in deleted docs or not.  So eg if a Filter is random access but doesn't factor in del docs we'd have to wrap it so that every random access check also checks del docs (\"AND NOT deleted.get(docID)\").\n\n\n\n\n\tWe need a coarse heuristic in IndexSearcher to decide when a filter \"merits\" down low application.  Ie, even if a filter is random access, if it's rather sparse (< 1% or 2% or something) it's better to apply it the way we do today (\"up high\").  In the current patch it's too coarse (it's either globally on or off); it should be based on the filter instead, or maybe the filter provides a method and that method defaults to the 1/2% threshold check.\n\n\n\n\n\tI suspect we should invert the \"Bits skipDocs\" now passed to the flex APIs, to be \"Bits acceptDocs\" instead, so that we don't have to invert every filter.  This'd also mean changing IndexReader.getDeletedDocs to IndexReader.getNotDeleteDocs.\n\n\n\nThen I think we simply pass the Bits filter into the Weight.scorer API.\n\n\nI think that any type of solution should support the great feature of Lucene queries, for example, FilteredQuery should use that, allowing to build complex query expressions without having the mentioned optimization only applied on the top level search.\nGood point \u2013 FilteredQuery should use this same low level API if its filter is random access and \"dense enough\".\n\n\nAs most filters results do support random access, either because they use OpenBitSet, or because they are built on top of FieldCache functionality, I think this feature will give great speed improvements to the query execution time.\n\nRight, the speed gains are often awesome! ",
            "author": "Michael McCandless",
            "id": "comment-12928948"
        },
        {
            "date": "2011-06-26T18:27:14+0000",
            "content": "Initial patch for trunk... lots of nocommits, but tests all pass and I\nthink this is [roughly] the approach we should take to get fast(er)\nFilter perf.\n\nConceptually, this change is fairly easy, because the flex APIs all\naccept a Bits to apply low-level filtering.  However, this Bits is\ninverted vs the Filter that callers pass to IndexSearcher (skipDocs vs\nkeepDocs), so, my patch inverts 1) the meaning of this first arg to\nthe Docs/AndPositions enums (it becomes an acceptDocs instead of\nskipDocs), and 2) deleted docs coming back from IndexReaders (renames\nIR.getDeletedDocs -> IR.getNotDeletedDocs).\n\nThat change (inverting the Bits to be keepDocs not skipDocs) is the\nvast majority of the patch.\n\nThe \"real\" change is to add DocIdSet.getRandomAccessBits and\nbitsIncludesDeletedDocs, which IndexSearcher then consults to figure\nout whether to push the filter \"low\" instead of \"high\".  I then fixed\nOpenBitSet to return this from getRandomAccessBits, and fixed\nCachingWrapperFilter to turn this on/off as well as state whether\ndeleted docs were folded into the filter.\n\nThis means filters cached with CachingWrapperFilter will apply \"low\",\nand if it's DeletesMode.RECACHE then it's a single filter that's\napplied (else I wrap with an AND NOT deleted check per docID), but\ncustom filters are also free to impl these methods to have their\nfilters applied \"low\". ",
            "author": "Michael McCandless",
            "id": "comment-13055143"
        },
        {
            "date": "2011-06-26T20:41:01+0000",
            "content": "Hi Mike,\nnicae patch, only little bit big. I reviewed the essential parts like applying the filter in IndexSearcher, real cool. Also CachingWrapperFilter looks fine (not closely reviewed).\n\nMy question: Do we really need to make the delDocs inverse in this issue? The IndexSearcher impl can also be done using a simple OrNotBits(delDocs, filterDocs) wrapper (instead AndBits) implementation and NotBits (if no delDocs available)? The patch is unreadable because of that. In general, reversing the delDocs might be a good idea, but we should do it separate and hard (not allow both variants implemented by IndexReader & Co.). The method name getNotDeletedDocs() should also be getVisibleDocs() or similar [I don't like double negation].\n\nAbout the filters: I like the new API (it is as discussed before), so the DocIdSet is extended by an optional getBits() method, defaulting to null.\n\nAbout the impls: FieldCacheRangeFilter can also implement getBits() directly as FieldCache is random access. It should just return an own Bits impl for the DocIdSet that checks the filtering in get(index). ",
            "author": "Uwe Schindler",
            "id": "comment-13055169"
        },
        {
            "date": "2011-06-26T21:16:40+0000",
            "content": "One more comment about DocIdSet.bitsIncludesDeletedDocs(). I think the default in DocIdSet and of course OpenBitSet should be true, because current filters always respect deleted docs (this was a requirement: MTQ uses deleted docs, FCRF explicitely ands it in). So the default is fine here. Of course CachingWrapperFilter sets this to false if the SegmentReader got new deletes. ",
            "author": "Uwe Schindler",
            "id": "comment-13055181"
        },
        {
            "date": "2011-06-27T13:38:38+0000",
            "content": "My question: Do we really need to make the delDocs inverse in this issue?\n\nI agree, let's break this (inverting delDocs/skipDocs) into a new issue and do it first, then come back to this issue.  There's still more work to do here, eg the bits should be stored inverted too (and the sparse encoding \"flipped\").\n\nThe method name getNotDeletedDocs() should also be getVisibleDocs() or similar [I don't like double negation].\n\n+1 for getVisibleDocs \u2013 I also don't like double negation!\n\nIn general, reversing the delDocs might be a good idea, but we should do it separate and hard (not allow both variants implemented by IndexReader & Co.).\n\nI agree it must be hard cutover \u2013 no more getDelDocs, and getVisibleDocs is abstract in IR.\n\nAbout the impls: FieldCacheRangeFilter can also implement getBits() directly as FieldCache is random access. It should just return an own Bits impl for the DocIdSet that checks the filtering in get(index).\n\nAhh, right: FCRF has no trouble being random access, and it can re-use the already created matchDoc in the subclasses. ",
            "author": "Michael McCandless",
            "id": "comment-13055544"
        },
        {
            "date": "2011-06-27T14:38:25+0000",
            "content": "\n+1 for getVisibleDocs \u2013 I also don't like double negation!\n\nI agree... getVisibleDocs() or another alternative would be getLiveDocs() ",
            "author": "Robert Muir",
            "id": "comment-13055581"
        },
        {
            "date": "2011-07-09T14:53:02+0000",
            "content": "Patch.  Lots of nocommits still but tests pass. ",
            "author": "Michael McCandless",
            "id": "comment-13062392"
        },
        {
            "date": "2011-09-22T14:21:11+0000",
            "content": "I think I have managed to update this to trunk.  The only dated aspect is the use of OpenBitSet, which if I understand correctly, has been replaced by FixedBitSet for most cases. ",
            "author": "Chris Male",
            "id": "comment-13112611"
        },
        {
            "date": "2011-09-22T14:35:37+0000",
            "content": "The only dated aspect is the use of OpenBitSet, which if I understand correctly, has been replaced by FixedBitSet for most cases.\n\nYES! ",
            "author": "Uwe Schindler",
            "id": "comment-13112621"
        },
        {
            "date": "2011-09-22T18:16:48+0000",
            "content": "Looks good Chris; thanks for bringing up to date, and feel free to fix the nocommits too \n\nI think a couple trunk changes got lost in your merging \u2013 eg MultiPhraseQuery.rewrite lost the \"if (termsArray.isEmpty())\" case?  (Also PhraseQuery.rewrite: I didn't mean to remove that; I think that was committed after my last patch?).  And PayloadTermQuery lost an \"if (includeSpanScore)\"?\n\nWe should certainly cutover to FixedBitSet! ",
            "author": "Michael McCandless",
            "id": "comment-13112782"
        },
        {
            "date": "2011-09-23T05:33:43+0000",
            "content": "New patch which includes the missed merging, some small tidy-ups and converting over to FixedBitSet. ",
            "author": "Chris Male",
            "id": "comment-13113165"
        },
        {
            "date": "2011-09-23T05:44:22+0000",
            "content": "Few questions:\n\n\n\tWould it make more sense to use the 'LiveDocs' notion in this API too? So rather than it being bitsIncludeDeletedDocs, it would be liveDocsOnly.\n\tIn what situations would a DocIdSet return a different Bits implementation for getRandomAccessBits? If the DocIdSet impl doesn't support random access, then converting to a random-access Bits implementation could be intensive, right? which would probably out-weigh the benefits.  Therefore could we simply check if the DocIdSet is also a Bits impl?  If it isn't, then we do the traditional filtering approach.\n\tWhen would a Bits implementation not want to support random-access? It seems its interface is implicitly random-access.  If so, could we cut the setter from FixedBitSet (I'm moved it from OpenBitSet).\n\n ",
            "author": "Chris Male",
            "id": "comment-13113167"
        },
        {
            "date": "2011-09-23T16:40:16+0000",
            "content": "\nWould it make more sense to use the 'LiveDocs' notion in this API too? So rather than it being bitsIncludeDeletedDocs, it would be liveDocsOnly.\n\n+1\n\nIn what situations would a DocIdSet return a different Bits implementation for getRandomAccessBits? If the DocIdSet impl doesn't support random access, then converting to a random-access Bits implementation could be intensive, right? which would probably out-weigh the benefits. Therefore could we simply check if the DocIdSet is also a Bits impl? If it isn't, then we do the traditional filtering approach.\n\nYou mean instanceof check instead of getRandomAccessBits?  I think\nthat's good?\n\nWhen would a Bits implementation not want to support random-access? It seems its interface is implicitly random-access. If so, could we cut the setter from FixedBitSet (I'm moved it from OpenBitSet).\n\nRandom access is actually slower when the number of docs that match\nthe filter is tiny (like less than 1% of the index), so I think if\nit's a Bits instance we still need a way to force it to use the\nold way? ",
            "author": "Michael McCandless",
            "id": "comment-13113542"
        },
        {
            "date": "2011-09-23T16:52:05+0000",
            "content": "You mean instanceof check instead of getRandomAccessBits? I think that's good?\n\nThats exactly what I mean.\n\n\nRandom access is actually slower when the number of docs that match\nthe filter is tiny (like less than 1% of the index), so I think if\nit's a Bits instance we still need a way to force it to use the\nold way?\n\nWhat if we were to expose cardinality through the Bits interface? We could then define some point in IndexSearcher where it decides which Query + Filter execution Strategy to use.     ",
            "author": "Chris Male",
            "id": "comment-13113552"
        },
        {
            "date": "2011-09-23T17:41:40+0000",
            "content": "What if we were to expose cardinality through the Bits interface? We could then define some point in IndexSearcher where it decides which Query + Filter execution Strategy to use.\n\nHmm that makes me nervous, since computing cardinality isn't that cheap.\n\nIe, it's better if this is done once up front and recorded on the DocIdSet, like CachingWrapperFilter does when the filter is cached. ",
            "author": "Michael McCandless",
            "id": "comment-13113598"
        },
        {
            "date": "2011-09-24T05:38:48+0000",
            "content": "Hmm that makes me nervous, since computing cardinality isn't that cheap.\n\nOkay, good point.\n\nAnother alternative (just throwing out ideas) is to keep the getRandomAccessBits on DocIdSet, but pass into it the Bits liveDocs.  Then it'd be up to the implementation to incorporate the liveDocs Bits (if it hasn't already).  If it hadn't, it could use the AndBits like the patch currently does in IndexSearcher.  IS would then become a little cleaner.\n\nIf the DocIdSet didn't want random-access, then it would return null. ",
            "author": "Chris Male",
            "id": "comment-13113899"
        },
        {
            "date": "2011-09-24T06:34:25+0000",
            "content": "Actually, the more I look at the nocommits, the less I like what I've suggested.  I think having getRandomAccessBits as it is in the patch is fine.  But I like we should maybe make setLiveDocsOnly and setAllowRandomAccessFiltering 1st class features of the Bits interface. ",
            "author": "Chris Male",
            "id": "comment-13113912"
        },
        {
            "date": "2011-09-24T12:47:12+0000",
            "content": "But I like we should maybe make setLiveDocsOnly and setAllowRandomAccessFiltering 1st class features of the Bits interface.\n\nHmm... that also makes me a bit nervous  Bits is too low-level for\nthese concepts?  Ie whether a filter/DIS \"folded in\" live docs\nalready, and whether the filter/DIS is best applied by iteration vs by\nrandom access, are higher level filter concepts, not low level Bits\nconcepts, I think?\n\nAlso, Bits by definition is random-access so I don't think it should\nhave set/getAllowRandomAccessFiltering. ",
            "author": "Michael McCandless",
            "id": "comment-13113965"
        },
        {
            "date": "2011-09-24T18:39:53+0000",
            "content": "I didnt look too hard here at whats going on, but maybe we could use the RandomAccess marker interface from the jdk? ",
            "author": "Robert Muir",
            "id": "comment-13114041"
        },
        {
            "date": "2011-09-27T10:51:10+0000",
            "content": "New patch which adds greater control over the random-access to DocIdSet.\n\nAlso fixes most of the nocommits. ",
            "author": "Chris Male",
            "id": "comment-13115395"
        },
        {
            "date": "2011-09-27T18:30:47+0000",
            "content": "New patch, fixing a few failing tests, adding a couple comments.  I\ndowngraded the 2 nocommits in LTC to TODOs, and removed the other one\n(false is correct default for those two?).\n\nFor DocIdSet, can we nuke getRandomAccessBits?  Ie, if\nsupportRandomAccess() returns true, then we can cast the instance to\nBits?  Maybe we should rename supportRandomAccess to useRandomAccess?\n(Ie, it may support it, but we only want to use random access when the\nfilter is dense enough).\n\nI changed MTQWF's and CachingWrapperFilter's threshold to a double\npercent (of the reader's maxDoc) instead, default 1.0%.\n\nHmm FieldCacheRangeFilter never enables random access... not sure\nhow/when we should compute that.\n\nI think we are getting close! ",
            "author": "Michael McCandless",
            "id": "comment-13115783"
        },
        {
            "date": "2011-09-27T19:01:24+0000",
            "content": "Wouldn't it make sense for FCRF to always return random access=true? This filter has a very ineffective DISI, as it has to check each doc using the match method, so random access is much better for this one (maps to current matchDoc-method, which should be renamed to a simple Bits.get(int docId)). What's the sense to use a DISI for this one? ",
            "author": "Uwe Schindler",
            "id": "comment-13115806"
        },
        {
            "date": "2011-09-27T19:05:08+0000",
            "content": "Why does CachingWrapperFilter refers to OpenBitSetDISI again? It should only use FixedBitSet as it already has al DISI methods? ",
            "author": "Uwe Schindler",
            "id": "comment-13115811"
        },
        {
            "date": "2011-09-28T04:16:06+0000",
            "content": "I haven't had a chance to look at the latest patch, but:\n\n\nFor DocIdSet, can we nuke getRandomAccessBits? Ie, if\nsupportRandomAccess() returns true, then we can cast the instance to\nBits? Maybe we should rename supportRandomAccess to useRandomAccess?\n(Ie, it may support it, but we only want to use random access when the\nfilter is dense enough).\n\nI'm definitely +1 to useRandomAccess() but I think there is a usability question mark around removing getRandomAccessBits().  If we assume that if DocIdSet.useRandomAccess() returns true then the DocIdSet must be also be a Bits implementation, then we need to prevent non-Bits implementations from returning true, or setting true in setUseRandomAccess.  If we don't, we're likely to confuse even expert users because this all comes together in a method deep inside IndexSearcher.\n\nBut if we're going to constrain useRandomAccess to only Bits implementations, then I once again feel these should be on Bits.  What if we added to Bits allowRandomAccessFiltering() or something like that? So even though Bits is inherently random-access, we control whether the Bits should be used to do filtering.\n\nAlternatively we keep getRandomAccessBits() and see DocIdSet as a random-access Bits factory which currently just returns itself in most cases, but potentially might not in the future? ",
            "author": "Chris Male",
            "id": "comment-13116123"
        },
        {
            "date": "2011-09-28T12:24:37+0000",
            "content": "Wouldn't it make sense for FCRF to always return random access=true? This filter has a very ineffective DISI, as it has to check each doc using the match method, so random access is much better for this one (maps to current matchDoc-method, which should be renamed to a simple Bits.get(int docId)). What's the sense to use a DISI for this one?\n\n+1 let's do that!\n\nWhy does CachingWrapperFilter refers to OpenBitSetDISI again? It should only use FixedBitSet as it already has al DISI methods?\n\nHmm I thought I fixed this in my last patch?  Oh I see, I fixed the code but not the jdocs... I'll fix the jdocs. ",
            "author": "Michael McCandless",
            "id": "comment-13116371"
        },
        {
            "date": "2011-09-28T12:39:50+0000",
            "content": "I'm definitely +1 to useRandomAccess() but I think there is a usability question mark around removing getRandomAccessBits(). If we assume that if DocIdSet.useRandomAccess() returns true then the DocIdSet must be also be a Bits implementation, then we need to prevent non-Bits implementations from returning true, or setting true in setUseRandomAccess. If we don't, we're likely to confuse even expert users because this all comes together in a method deep inside IndexSearcher.\n\nWell it's quite expert to implement your own random-access filter?\n\nWe can fix IS so that if the DIS fails to cast you get a clear exception stating that useRandomAccess returned true yet the DIS isn't a Bits?  Or it could be silent and fall back to iteration, if the cast fails... but I prefer the former (so you know your \"true\" isn't working).\n\nI really don't think we should pollute Bits w/ useRandomAccess method; random-access is the entire point of the Bits interface.  It's too low-level to push it down there.\n\nAnd actually having a getter (getRandomAccessBits) is trappy I think, because the user may not realize it's called for every search and may do something crazy like allocate a new FixedBitSet every time. ",
            "author": "Michael McCandless",
            "id": "comment-13116378"
        },
        {
            "date": "2011-09-28T12:41:08+0000",
            "content": "New patch folding in Uwe's ideas \u2013 FCRF always uses random access; remove matchDoc and just override Bits.get; fixed CWF jdocs to not reference OpenBitSetDISI. ",
            "author": "Michael McCandless",
            "id": "comment-13116380"
        },
        {
            "date": "2011-09-28T12:58:13+0000",
            "content": "I really don't think we should pollute Bits w/ useRandomAccess method; random-access is the entire point of the Bits interface. It's too low-level to push it down there.\n\nIts not about marking whether random-access should be used or not, my idea was to mark whether it should be used to filter or not. ",
            "author": "Chris Male",
            "id": "comment-13116407"
        },
        {
            "date": "2011-09-28T18:07:20+0000",
            "content": "As a first (committable) step towards this, I think we should break out a new issue\nwhere we remove the hardcoded calls to IR.getLiveDocs() in all the Scorers?\n\nThen deleted docs could be supplied via the ScorerContext with something like acceptDocs.\n\nThis seems cleaner and more flexible to me regardless of what we do. ",
            "author": "Robert Muir",
            "id": "comment-13116645"
        },
        {
            "date": "2011-09-28T18:11:04+0000",
            "content": "+1, that change is a rote cutover.\n\nThen separately we can figure out how a Filter/DIS/Bits conveys the \"strategy\" to IS.  Really this is a part of the wider question of how Lucene should do query optimization in general... ",
            "author": "Michael McCandless",
            "id": "comment-13116651"
        },
        {
            "date": "2011-09-28T19:09:28+0000",
            "content": "\nThen separately we can figure out how a Filter/DIS/Bits conveys the \"strategy\" to IS. Really this is a part of the wider question of how Lucene should do query optimization in general...\n\nI think once we do this, we should just add \"Bits acceptDocs\" parameter to IndexSearcher's searchWithFilter method?\nAnd I think a simple (protected) heuristic to IndexSearcher whether it should execute the filter 'via acceptDocs': the default just be instanceof Bits && firstSetBit < X, like what BQ does?\nThen we might add optional boolean method to Filter for whether it already 'incorporates deletes'.\n\nThen we can optimize for the 4 cases:\n\n\tsparse/non-random-access filter, doesn't incorporate deletes: same logic as today\n\tsparse/non-random-access filter, incorporates deletes: Here we might pass null as 'acceptDocs' to the scorercontext, so we no longer redundantly check deleted docs.\n\theavy random-access filter, doesn't incorporate deletes: no filter instead with acceptDocs=AndBits(filter, liveDocs)\n\theavy random-access filter, incorporates deletes: no filter instead with acceptDocs=filter (don't need to use AndBits since we know it incorporates liveDocs).\n\n\n ",
            "author": "Robert Muir",
            "id": "comment-13116702"
        },
        {
            "date": "2011-09-29T12:30:31+0000",
            "content": "And I think a simple (protected) heuristic to IndexSearcher whether it should execute the filter 'via acceptDocs': the default just be instanceof Bits && firstSetBit < X, like what BQ does?\n\nThat's a neat idea!  Then we can sidestep this whole question about who/where/what \"computes\" whether the Bits is very sparse (< 1%) and so we should apply \"up high\", or not and so we should apply \"down low\".\n\nSo this logic would run after we have a Bits... we could actually go and test every Mth bit (not just first N), to hopefully reduce false negatives (ie, a dense filter that appeared sparse just because it's first N docs were not set).\n\nIt'd still be a heuristic and thus make mistakes sometimes... (vs actually calling .cardinality and making the \"right\" decision), but most of the time it should work.\n\nSeparately we still need someone to declare whether the filter AND'd live docs already; maybe we put this on the Filter class, since it would not normally vary per-segment? ",
            "author": "Michael McCandless",
            "id": "comment-13117242"
        },
        {
            "date": "2011-09-29T12:36:19+0000",
            "content": "+1 to this approach.\n\nSeparately we still need someone to declare whether the filter AND'd live docs already; maybe we put this on the Filter class, since it would not normally vary per-segment?\n\nI kind of like how Filter doesn't have any properties associated with it.  So perhaps we can keep the liveDocsOnly on DocIdSet? ",
            "author": "Chris Male",
            "id": "comment-13117246"
        },
        {
            "date": "2011-09-29T13:22:14+0000",
            "content": "OK, DocIdSet seems good too?  It already has isCacheable, so there's a precent of putting such \"hints\" on it.\n\nMaybe .containsOnlyLiveDocs()? ",
            "author": "Michael McCandless",
            "id": "comment-13117288"
        },
        {
            "date": "2011-09-29T13:24:46+0000",
            "content": "Maybe .containsOnlyLiveDocs()?\n\n+1 ",
            "author": "Chris Male",
            "id": "comment-13117291"
        },
        {
            "date": "2011-10-02T12:25:15+0000",
            "content": "Patch updated following the changes in LUCENE-3474.  (Note, this patch also fixes a missed change in LUCENE-3474, in MultiPhraseQuery.UnionDocsAndPositionsEnum).\n\nNew patch features:\n\n\n\tliveDocsOnly() -> containsOnlyLiveDocs()\n\tgetRandomAccessBits() and useRandomAccess() are gone, replaced by some additional logic in IS.  Now we check if the DocIdSet is a Bits impl, and then we consult a protected method in IS called useRandomAccess().\n\tAt the moment useRandomAccess() just returns true, while we discuss what heuristics are best to apply here.\n\n ",
            "author": "Chris Male",
            "id": "comment-13119000"
        },
        {
            "date": "2011-10-02T13:03:27+0000",
            "content": "I don't understand the de-optimizations to termquery in the patch. ",
            "author": "Robert Muir",
            "id": "comment-13119005"
        },
        {
            "date": "2011-10-02T13:05:25+0000",
            "content": "I used what has been in the patches so far.  If its out of date, just tell me. ",
            "author": "Chris Male",
            "id": "comment-13119006"
        },
        {
            "date": "2011-10-02T13:07:22+0000",
            "content": "it doesnt have anything to do with this issue. ",
            "author": "Robert Muir",
            "id": "comment-13119007"
        },
        {
            "date": "2011-10-04T16:27:07+0000",
            "content": "What a tiny patch this has turned into!\n\nIt looks great.\n\nI agree the TQ changes look like a merge mistake?  We should just keep\ntrunk here?\n\nCan we name it DocIdSet.containsOnlyLiveDocs?  I don't think we need\nthe \"is\" prefix?  Can you add @lucene.experimental to it?\n\nFor the default IS heuristic, how about testing 100 evenly spaced\nbits?  If more than 1 is set, and we can early-exit during the\ntesting, we use random access?  We can make the two values (100 and 1)\nsettable in expert IS ctors? ",
            "author": "Michael McCandless",
            "id": "comment-13120252"
        },
        {
            "date": "2011-10-04T16:37:42+0000",
            "content": "\nFor the default IS heuristic, how about testing 100 evenly spaced\nbits? \n\nHow about grab an iterator like today, pull the first bit, if this takes a long time because the filter is sparse, \nwe dive into the conjunction alg we do today anyway, no wasted effort.\n\nOtherwise if its < 100, use it as liveDocs.\n\nwe can nuke the 'boolean' protected method and figure out if/how to add abstractions later. ",
            "author": "Robert Muir",
            "id": "comment-13120264"
        },
        {
            "date": "2011-10-04T17:18:12+0000",
            "content": "Also, when we pass filter \"down low\" as liveDocs, we should ensure we set the appropriate inOrder/topLevel params so we can get BooleanScorer,\nsince we won't need to advance() it. ",
            "author": "Robert Muir",
            "id": "comment-13120293"
        },
        {
            "date": "2011-10-04T18:22:21+0000",
            "content": "and current patch is missing the optimization for case 2 described above:\n\n2. sparse/non-random-access filter, incorporates deletes: Here we might pass null as 'acceptDocs' to the scorercontext, so we no longer redundantly check deleted docs.\n\nI'll update the patch to support BooleanScorer and optimize this case. ",
            "author": "Robert Muir",
            "id": "comment-13120356"
        },
        {
            "date": "2011-10-04T18:31:14+0000",
            "content": "\nHow about grab an iterator like today, pull the first bit, if this takes a long time because the filter is sparse, \nwe dive into the conjunction alg we do today anyway, no wasted effort.\n\nOtherwise if its < 100, use it as liveDocs.\n\n+1, I like that.\n\nBut, < maxDoc()/100; and maybe we make expert setter for that 100.\n\n\nAlso, when we pass filter \"down low\" as liveDocs, we should ensure we set the appropriate inOrder/topLevel params so we can get BooleanScorer,\nsince we won't need to advance() it.\n\nGood catch, yes! ",
            "author": "Michael McCandless",
            "id": "comment-13120362"
        },
        {
            "date": "2011-10-04T18:43:27+0000",
            "content": "Duh, nevermind: I think it is just a < 100 check (no maxDoc involved).  It's as if we are [approximately] checking how many set bits in first 100 docs, and if we hit at least 1 such but we assume that means filter is > 1% dense. ",
            "author": "Michael McCandless",
            "id": "comment-13120372"
        },
        {
            "date": "2011-10-04T19:03:03+0000",
            "content": "updated patch:\n\n\timplements heuristic with getter/setter (defaults to random access if we estimate filter accepts > 1% of documents)\n\tsets the inOrder/topLevel stuff correct when we use random access <-- but we should add a test that we get BS1 here!\n\twhen the filter is sparse, but contains only live docs, we pass null as liveDocs.\n\n ",
            "author": "Robert Muir",
            "id": "comment-13120383"
        },
        {
            "date": "2011-10-04T19:10:11+0000",
            "content": "Patch looks great! ",
            "author": "Michael McCandless",
            "id": "comment-13120388"
        },
        {
            "date": "2011-10-04T19:55:46+0000",
            "content": "just a code style tweak, a test to make sure we get BS1 with random access filters, and randomization of the parameter value in LuceneTestCase. ",
            "author": "Robert Muir",
            "id": "comment-13120421"
        },
        {
            "date": "2011-10-05T02:17:03+0000",
            "content": "Small improvements:\n\n\n\tChanged isContainsOnlyLiveDocs to containsOnlyLiveDocs\n\tChanged visibility of containsOnlyLiveDocs field since with the getter/setters it doesn't need to be protected.\n\tTidied the documentation on the getter/setters of the threshold in IS\n\n\n\nI think we're good to go? ",
            "author": "Chris Male",
            "id": "comment-13120649"
        },
        {
            "date": "2011-10-05T07:30:20+0000",
            "content": "Hey, briefly checked the patch, and wondering out loud if it make sense to have similar random access logic in FilteredQuery? ",
            "author": "Shay Banon",
            "id": "comment-13120733"
        },
        {
            "date": "2011-10-05T08:11:51+0000",
            "content": "It certainly shares alot of similarities with the logic in IS so I think there would be benefit yeah. ",
            "author": "Chris Male",
            "id": "comment-13120747"
        },
        {
            "date": "2011-10-05T09:58:04+0000",
            "content": "maybe for FilteredQuery just open a separate issue? In this case I think\nwe should just give it another Scorer impl. ",
            "author": "Robert Muir",
            "id": "comment-13120784"
        },
        {
            "date": "2011-10-05T17:09:09+0000",
            "content": "+1 for new issue for FilteredQuery\n\nPatch looks great Chris \u2013 I think it's ready!  Finally \n\nTiny fix to the jdocs for IS.setFilterRandomAccessThreshold \u2013 \"Threshold use to heuristics\" -> \"Threshold used in heuristic\".\n\nI think we should run before/after benchmarks before committing... I'll try to do this soon and post back. ",
            "author": "Michael McCandless",
            "id": "comment-13121154"
        },
        {
            "date": "2011-10-05T17:29:07+0000",
            "content": "Can you only remove the unchecked exception here (FieldCacheRangeFilter):\n\n\npublic abstract boolean get(int doc) throws ArrayIndexOutOfBoundsException;\n\n\nThis exception is only used internally to ignore range checks, but we dont need to declare (and impl classes no longer declare it at all).\n\nAlso FieldCacheDocIdSet does not implement Bits completely? ",
            "author": "Uwe Schindler",
            "id": "comment-13121177"
        },
        {
            "date": "2011-10-05T18:17:43+0000",
            "content": "New patch:\n\n\tFixed FieldCacheRangeFilterDocIdSet to implement bits\n\tRemoved deleted docs handling in this filter. We explicitely pass false. No the iterator and get(bits) may return deleted docs.\n\tMade AndBits final (speed and no need to subclass)\n\n\n\nThe question here: I see no filter that explicitely returns true for the deletedDocs handling. But e.g. most filters actually dont have deleted docs, like MultiTermQueryWF,... ",
            "author": "Uwe Schindler",
            "id": "comment-13121316"
        },
        {
            "date": "2011-10-05T18:21:04+0000",
            "content": "I still disagree with the setter in DocIdSet to make it respect setContainsOnlyLiveDocs(). This is borken and only specific to FCRangeFilter. This method should not be there, instead the DocIdSet should return this info, e.g. FieldCacheRangeFilterDocIdSet should implement this method and return false. ",
            "author": "Uwe Schindler",
            "id": "comment-13121326"
        },
        {
            "date": "2011-10-05T18:34:04+0000",
            "content": "I modified my patch for FieldCacheRangeFilter to enforce that it does not respect deletes (it throws UOE on the setter).\n\nI checked some combinations, setContainsOnlyLiveDocs() cannot go in like that!!!\n\nCachingWrapperFilter should have its own handling of this and return the correct setting, but not modify an already created bitset/whatever DocIdSet. My above patch does no longer respect deleted docs in FieldCacheRangeFilter and returns false from the beginning. But if you cache this filter, it suddenly tries to set the boolean to true -> test fail\n\nAlso all Filters that internally use deleted docs, should return true from the beginning. We can maybe add the setter to FixedBitSet, so filter impls can easily set this bit, but it should not be a setter in DocIdSet! ",
            "author": "Uwe Schindler",
            "id": "comment-13121334"
        },
        {
            "date": "2011-10-05T20:37:15+0000",
            "content": "Further investigations showed more problems:\n\n\tFilteredDocIdSet does never implement Bits, but it should if the wrapped filter implements Bits. This cannot be done as two different implementation would be needed. I have no idea how to solve this.\n\n\n\nI uploaded a new patch that fixes the problems from before:\n\n\tCachingWrapperFilter now only set the flag for containsOnlyLiveDocs to true, if it was true before, too. If the orginal filter returned a DocIdSet without that flag, the cached filter cannot suddenly set it to true\n\tCachingWrapperFilter also copies the liveDocs when it copies to FixedBitSet (e.g. QueryWrapperFilter).\n\tThe default for containsOnlyLiveDocs is true, as all current filters were always resepcting this (exept FieldCacheRangeFilter since the rewrite). All filters in Lucene use liveDocs, because this was a requirement in older Lucene versions!\n\tQueryWrapperFilter may ignore liveDocs and simply return false for the flag.\n\n\n\nIn general I would like it more to rip the deleted docs handling in CachingWrapperFilter, as it no longer needs to take care. CWF should simply return containsOnlyLiveDocs=false if the deleted docs need to be merged in. There is no need to and them in using FilteredDocIdSet (which slows down for the random access case, see above) ",
            "author": "Uwe Schindler",
            "id": "comment-13121449"
        },
        {
            "date": "2011-10-05T22:45:16+0000",
            "content": "More problems (I am currently rewriting the whole liveDocs stuff):\n\n\tDocIdBitSet does not implement Bits, but its random access.\n\n\n\nAnother idea I had:\nWe could do Filter.getDocIdSet like Weight.scorer: Pass the live docs down. This could improve e.g. ChainedFilter as it can simply pass the results of filter down the chain, if its random access. Also QueryWrapperFilter could directly pass the incoming bits downto the wrapped Query (see comment in current code).\n\nAnd finally we would not need the containsOnlyLiveDocs flag at all. If IndexSearcher does not need deleted docs to be handled in the filter, it could pass down null, if it passes liveDocs down to the filter, the filter is required to respect them (like scorers).\n\nCachingWrapperFilter would use AndBits/FilteredDocIdSet in all cases to combine the cached result (which it caches always without liveDocs). This is still faster than the current approcah where deldocs are handled quite oftenh multiple times in the query execution. ",
            "author": "Uwe Schindler",
            "id": "comment-13121560"
        },
        {
            "date": "2011-10-06T02:11:23+0000",
            "content": "That is a lot to take in.  Let me see if I understand:\n\n\n\tBasically you're saying that allowing external code to setContainsOnlyLiveDocs is broken in CachingWrapperFilter since it makes a decision that a Filter contains only live docs, when in fact the Filter might not.\n\tYou're also arguing that most Filters take liveDocs into consideration anyway.\n\tYou suggest we pass liveDocs into Filter.getDocIdSet requiring the Filter to uses them (mirroring the Weight/Scorer API).  I kind of like this idea.\n\n\n\nI don't think its a crisis if some DocIdSet impls don't implement Bits.  I personally see this issue as providing the framework for doing random-access filtering, not necessarily making it happen with every DocIdSet.   ",
            "author": "Chris Male",
            "id": "comment-13121677"
        },
        {
            "date": "2011-10-06T02:24:19+0000",
            "content": "Thanks Uwe for the reviewing/policing!\n\nChris, I do agree its not a crisis, but i think its best if we set everything up to work nicely.\n\nI do think its really bad news if a filter claims it containsOnlyLiveDocs but in fact it does not... this is a big blocker for committing anything, because it will only create bugs. ",
            "author": "Robert Muir",
            "id": "comment-13121680"
        },
        {
            "date": "2011-10-06T03:12:40+0000",
            "content": "What do you think of the idea of passing liveDocs into Filter.getDocIdSet? ",
            "author": "Chris Male",
            "id": "comment-13121693"
        },
        {
            "date": "2011-10-06T07:52:14+0000",
            "content": "Hi Chris, hi Male,\n\nI was going to bed after my last post. I had a crisis with two facts in the new API, that do no play nicely together. I thought the whole night about it again and I also started to recode some details last evening, but all was not so fine (but I found lots of problems, so it's a good thing that I started to code - especially on several filters that are not so basic like those which only use FixedFitSet/OpenBitSet):\n\n\n\tthe hidden implementation of Bits is a nice idea, but has one big problem: Java is a strongly-typed language. If a DocIdSet implements Bits, but you want to wrap it using FilteredDocIdSet, this interface implementation  might suddenly go away, because the wrapper class does not implement Bits. If we make FilteredDocIdSet implement Bits, its also wrong, as it might wrap another DocIdSet that is not random access. So I tend to keep DocIdSet abstrcat and let it only expose functions that return a Bits interface. The same is that DocIdSet does not directly implement DocIdSetIterator, it can just return one. So I would strongly recommend to add a method like iterator() that returns a impl and not rely on \"marker interfaces\". I would favor \"Bits DocIdSet.bits()\" - would be in line with the iterator method. If the implementing class like FixedBitSet implements it itsself and returns \"this\" is an implementation detail. If DocIdSet does not allow random access it should expose with an exception thrown by bits or if it returns null. Does not really matter to me. - In general a wrapper like FilteredDocIdSet can do this in one class, wrapping bits() would check if bits() returns non-null, and then wrap another wrapper around bits() that uses match() to filter. The impl of this class is fast and supports both (iterator and bits, if available).\n\tthe other thing, I dont like, is the setContainsOnlyLiveDocs setter on DocIdSet. It allows anybody to change the DocIdSet (which should have an API that exposes only read-access). Only classes like FixedBitSet that implement this read-only interface might be able to change it from their own API (means the setter might be in the various DocIdSet implementations in oal.util). A consumer of the filter should not be able to change the DocIdSet behaviour from outside using a public API. I started to rewrite this yesterday and only left the getter in DocIdSet, but added the setter to FixedBitSet, OpenBitSet, DocIdBitSet,... The setter in the abstract base class also violates unmodifiable of EMPTY_DOCIDSET. This impl should be \"containsOnlyLiveDocs=true\") and this must be unchangeable fixed.\n\tAlso DocIdSet is a class not really related solely to Filters, e.g. Scorer extends DocIdSetIterator or DocsEnum extends DocIdSetIterator, Solr Facetting uses DocIdSet. DocIdSet is just a holder class for a bunch of documents exposing a iterator (and a Bits API - this is why I want two getter methods and no interface magic)). The existence of live docs is outside it's scope. I therefore would like a similar API like for scorers, so IndexSearcher can ask the Filter for a DocIdSet based on the given liveDocs (like the scorer method in Weights). The returned DocIdSet would not know if it only has live Docs or not (as the Scorer itsself also does not expose this information). CachingWrapperFilter is little bit special, but this one would always ask the wrapped Filter for a DocidSet without deletions and cache that one, but always return a FilteredDocIdSet bringing the liveDocs passed from IndexSearcher in. The cache would then always be without LiveDocs and easier to maintain. Reopening segments would never need to reload cache. CachingWrapperFilter would just decide on the fact if IndexSearcher passes a liveDocs BitSet or not, if it needs to use it or not (in its own getDocIdSet method). If we have a query and only filter some documents, IndexSearcher already knows about liveDocs from the main scorer and would pass null to the filter. This would remove lots of additional checks to liveDocs. Only the main scorer would know about them, the filter will ignore them (so there is no overhead in CachingWrapperFilter, as it can return the cached filter directly to IndexSearcher, without wrapping). QueryWrapperFilter could pass the liveDocs through the wrapped filter, too.\n\n\n\nI may have time today to implement some parts of this, should not be to difficult. ",
            "author": "Uwe Schindler",
            "id": "comment-13121779"
        },
        {
            "date": "2011-10-06T08:01:07+0000",
            "content": "Okay thats alot to take in again.\n\nYou've made a good case for dropping setContainsOnlyLiveDocs, I totally agree.  I really do like the idea of adding the acceptDocs to Filter.getDocIdSet.\n\nI'm also comfortable with adding .bits() to DocIdSet to address the typing problem.\n\nShould we bash out a quick patch making these changes and see how it looks? ",
            "author": "Chris Male",
            "id": "comment-13121786"
        },
        {
            "date": "2011-10-06T08:14:28+0000",
            "content": "+1, I have to revert here a lot again because I was trying to move the setLiveDocsOnly/liveDocsOnly down to FixedBitSet & Co, but this is too complicated.\n\nShould I start to hack something together? The biuggest change will be in all filter impls to add the parameter to getDocIdSet(). ",
            "author": "Uwe Schindler",
            "id": "comment-13121790"
        },
        {
            "date": "2011-10-06T09:08:06+0000",
            "content": "Yes please put something together and then we'll review / iterate. ",
            "author": "Chris Male",
            "id": "comment-13121807"
        },
        {
            "date": "2011-10-06T12:13:39+0000",
            "content": "A first rewrite of Lucene core to pass acceptDocs down to Filter.getDocIdSet:\n\n\toptimized and simpliefied CachingWrapper* - no deletesmode anymore\n\tFieldCacheTermsFilter has optimized DocIdSet\n\tAdded bits() to all DocIdSet\n\tIndexSearcher.searchWithFilter was rewritten to pass liveDocs down.\n\tAndBits is no longer needed\n\n\n\nThe tests are not yet rewritten, still 55 compile errors.... This patch is just for review ",
            "author": "Uwe Schindler",
            "id": "comment-13121869"
        },
        {
            "date": "2011-10-06T12:29:17+0000",
            "content": "\nI therefore would like a similar API like for scorers, so IndexSearcher can ask the Filter for a DocIdSet based on the given liveDocs (like the scorer method in Weights).\n\n\n\nIf this is the case, then in the !randomAccess path of indexsearcher.java please pass null as liveDocs. ",
            "author": "Robert Muir",
            "id": "comment-13121877"
        },
        {
            "date": "2011-10-06T12:33:53+0000",
            "content": "adding back this optimization, again.\n\nbefore committing please give me time to write tests to ensure we aren't losing these optimizations. ",
            "author": "Robert Muir",
            "id": "comment-13121882"
        },
        {
            "date": "2011-10-06T12:35:29+0000",
            "content": "Robert, thanks!\n\nI missed this line:\n\nBits acceptDocs = filterContainsLiveDocs ? null : context.reader.getLiveDocs();\n\n\n\nAs we now always use live docs in filter this would always be null! ",
            "author": "Uwe Schindler",
            "id": "comment-13121885"
        },
        {
            "date": "2011-10-06T14:26:57+0000",
            "content": "Here a patch with almost all core tests rewritten (I left out the CachingWrapper tests, as I nuked DeletesMode). Its just for demonstartion.\n\nSome tests have really stupid filters and work only with optimized indexes. I added asserts in those filters (except one), that acceptDocs==null. The remaining one uses QueryUtils and I have no idea whats going on there, that the acceptDocs!=null.\n\nWhen looking at the code in IndexSearcher, I would propose to remove all Filter special handling in IndexSaercher and move all code over to FilteredQuery (with all our optimizations). If you call IS.search(query, filter,...), IndexSearcher would simply wrap with FilteredQuery and we would have no code duplication and much easier maintainability in IS. ",
            "author": "Uwe Schindler",
            "id": "comment-13121975"
        },
        {
            "date": "2011-10-06T14:29:34+0000",
            "content": "\nWhen looking at the code in IndexSearcher, I would propose to remove all Filter special handling in IndexSaercher and move all code over to FilteredQuery (with all our optimizations). If you call IS.search(query, filter,...), IndexSearcher would simply wrap with FilteredQuery and we would have no code duplication and much easier maintainability in IS.\n\n+1\n\nAlso, we can nuke AndBits.java now? ",
            "author": "Robert Muir",
            "id": "comment-13121976"
        },
        {
            "date": "2011-10-06T14:32:48+0000",
            "content": "Also, we can nuke AndBits.java now?\n\nIt was nuked here, but still made it into the patch  ",
            "author": "Uwe Schindler",
            "id": "comment-13121978"
        },
        {
            "date": "2011-10-06T22:04:48+0000",
            "content": "New patch (still only Lucene Core, no contrib/modules/solr modified):\n\n\tNuked Filter handling completely from IndexSearcher. Algorithms and Random access optimizations were added to FilteredQuery. IS.search(Query, Filter,...) now only wraps the query with the Filter, if filter!=null (small helper method).\n\tThe random access threshhold is still in IndexSearcher.setFilterRandomAccessThreshold(), FilteredQuery gets it in it's weight from IndexSearcher. This is maybe not the best solutions, we can also add a setter to FilteredQuery and IS passes it to FilteredQuery.\n\n\n\nWhat do you think? Mike: Can you do perf tests? ",
            "author": "Uwe Schindler",
            "id": "comment-13122349"
        },
        {
            "date": "2011-10-06T22:16:54+0000",
            "content": "I will do perf tests!  Working on getting luceneutil to do random filters... but could be a few days (I'm offline for the next 3 days) unless I can commit to luceneutil and someone else can run the tests... ",
            "author": "Michael McCandless",
            "id": "comment-13122355"
        },
        {
            "date": "2011-10-06T22:17:10+0000",
            "content": "I will add further tests tomorrow, to test all code paths in FilteredQuery. There is a short-circuit (it implements Scorer.score(Collector) for fast top-scorer as it existed in IndexSearcher.searchWithFilter before. To test the standard scorer behavior (nextDoc/advance), a test should be added that adds FilteredQuery as clause with others to a BQ, so ConjunctionScorer tries nextDoc/advance. \n\nSomebody else might look at the scorer and double check. I had to rewrite FilteredQuery#Weight#Scorer, as the filterIter is already advanced to first doc (to check the random access threshold). ",
            "author": "Uwe Schindler",
            "id": "comment-13122356"
        },
        {
            "date": "2011-10-06T23:12:29+0000",
            "content": "New patch:\n\n\tFixed the FilteredQuery-Scorer's advance by logic change. Its now much easier to understand. The corresponding tests are in TestFilteredQuery: All tests are executed 2 times: as random access filter and as iterator filter. Also FilteredQuery is added to BQ, so the conventional scorer (nextDoc/advance) is tested.\n\n\n\nThe tests for CachingWrapper* are still disabled, have to rewrite them tomorrow. Then we can change contrib and Solr. ",
            "author": "Uwe Schindler",
            "id": "comment-13122398"
        },
        {
            "date": "2011-10-07T11:05:47+0000",
            "content": "Improved patch with modules and contrib fixed, Solr still on TODO:\n\n\tBooleanFilter and ChainedFilter do not apply acceptDocs to their Filter clauses. Instead they apply the acceptDocs on the final DocIdSet using BitsFilteredDocIdSet (see below). This improves filter performance, as the deleted documents are not applied on each clause.\n\tNew helper class BitsFilteredDocIdSet, which supplies a wrap method, that can apply a Bits (e.g. acceptDocs) to a DocIdSet. This is useful for Filters, that build DocIdSets without respecting the acceptDocs parameter and only want to apply the deletions live.\n\tCachingWrapperFilter and CachingSpanFilter now also use the acceptDocs wrapper, as the filters are cached without acceptDocs.\n\tFixed Javadocs, small changes to IndexSearcher\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-13122691"
        },
        {
            "date": "2011-10-07T12:20:32+0000",
            "content": "This looks awesome! ",
            "author": "Robert Muir",
            "id": "comment-13122729"
        },
        {
            "date": "2011-10-07T13:04:46+0000",
            "content": "updated patch: i fixed solr to the api changes and simply disabled the optimization in SolrIndexSearcher.\n\nI think this is the most conservative way to proceed, we can then open a followup issue to make whatever changes are necessary to Solr APIs so it can use the optimization (looks complex) ",
            "author": "Robert Muir",
            "id": "comment-13122755"
        },
        {
            "date": "2011-10-07T13:08:10+0000",
            "content": "Awesome idea Robert, I was staring at the Solr code a little bewildered about how to integrate the optimization. ",
            "author": "Chris Male",
            "id": "comment-13122760"
        },
        {
            "date": "2011-10-07T13:21:16+0000",
            "content": "Apart from Mike's benchmarks, are we waiting on any further changes to the patch? ",
            "author": "Chris Male",
            "id": "comment-13122771"
        },
        {
            "date": "2011-10-07T13:38:13+0000",
            "content": "Just until the policeman says 'final patch'  ",
            "author": "Robert Muir",
            "id": "comment-13122799"
        },
        {
            "date": "2011-10-07T13:46:25+0000",
            "content": "Chris: I have to fix the CachingWrapper tests (soon). And add some acceptDocs to solr, which Robert simply ignored. ",
            "author": "Uwe Schindler",
            "id": "comment-13122802"
        },
        {
            "date": "2011-10-07T14:07:28+0000",
            "content": "Patch with Solr fixes by Robert improved. Now usage of acceptDocs is correct. There is room to optimize, but this is the correct way to solve (as Filters are required to respect deleted docs in trunk).\n\nNow the remaining caching wrapper changes, then we are ready. ",
            "author": "Uwe Schindler",
            "id": "comment-13122815"
        },
        {
            "date": "2011-10-07T15:06:14+0000",
            "content": "At what levels of bitset sparseness does it make sense to use random access?  I ask because sometimes Solr actually knows the sparseness of it's sets. ",
            "author": "Yonik Seeley",
            "id": "comment-13122851"
        },
        {
            "date": "2011-10-07T15:12:31+0000",
            "content": "Mike suggested earlier in the issue anything denser than 1% sees benefits from random-access. ",
            "author": "Chris Male",
            "id": "comment-13122854"
        },
        {
            "date": "2011-10-07T15:15:33+0000",
            "content": "OK, thanks Chris.  I'll take a shot at optimizing this patch for Solr a bit more. ",
            "author": "Yonik Seeley",
            "id": "comment-13122856"
        },
        {
            "date": "2011-10-07T15:17:15+0000",
            "content": "+1 ",
            "author": "Jason Rutherglen",
            "id": "comment-13122858"
        },
        {
            "date": "2011-10-07T15:25:38+0000",
            "content": "Final patch.\n\nThe test for CachingWrapperFilter/CachingSpanFilter were simplified to only check that they actually respect deletions.\n\nI think thats ready to commit after some perf testing.\n\nRobert: If you have time? ",
            "author": "Uwe Schindler",
            "id": "comment-13122866"
        },
        {
            "date": "2011-10-07T15:34:23+0000",
            "content": "OK, I'll start again from the final patch. ",
            "author": "Yonik Seeley",
            "id": "comment-13122875"
        },
        {
            "date": "2011-10-07T15:51:31+0000",
            "content": "Sorry for interrupting you. I only changed Lucene classes, if that helps. With TortoiseSVN you can partly apply patches. ",
            "author": "Uwe Schindler",
            "id": "comment-13122893"
        },
        {
            "date": "2011-10-07T16:38:48+0000",
            "content": "\nRobert: If you have time?\n\nI'm attempting to benchmark the patch now with the example tasks mike added to luceneutil early this morning:\nhttp://code.google.com/a/apache-extras.org/p/luceneutil/source/browse/eg.filter.tasks?spec=svn3ea6dafca66a00e1dbf4563d1098b7418e386cbf&r=3ea6dafca66a00e1dbf4563d1098b7418e386cbf\n\nI'll report back if i'm able to get results... takes a few hours here. ",
            "author": "Robert Muir",
            "id": "comment-13122941"
        },
        {
            "date": "2011-10-07T19:03:06+0000",
            "content": "Here's the results... F0.1 for example means filter accepting a random 0.1% of documents.\n\n\n                Task   QPS trunkStdDev trunk   QPS patchStdDev patch      Pct diff\n          PhraseF0.1       67.61        1.89       29.85        2.52  -60% -  -50%\n          PhraseF0.5       20.08        0.72       13.09        1.11  -42% -  -26%\n          PhraseF1.0       12.37        0.46        8.84        0.88  -37% -  -18%\n      OrHighHighF0.1       78.84        1.19       59.96        2.87  -28% -  -19%\n            TermF0.5      133.27        4.80      125.91        7.29  -14% -    3%\n          OrHighHigh       12.73        0.45       12.13        0.92  -14% -    6%\n              Fuzzy1       57.63        1.70       56.62        2.33   -8% -    5%\n              Fuzzy2       96.92        2.25       96.19        2.63   -5% -    4%\n   AndHighHighF100.0       16.99        0.50       16.92        1.38  -11% -   10%\n    AndHighHighF99.0       17.00        0.48       16.94        1.37  -10% -   10%\n    AndHighHighF95.0       17.00        0.48       16.98        1.35  -10% -   10%\n          Fuzzy2F0.1      107.24        2.74      107.29        2.68   -4% -    5%\n    AndHighHighF90.0       17.04        0.47       17.13        1.36   -9% -   11%\n          Fuzzy1F0.1       74.60        1.58       75.03        1.55   -3% -    4%\n  SloppyPhraseF100.0        7.82        0.16        7.89        0.24   -4% -    6%\n   SloppyPhraseF99.0        7.82        0.16        7.92        0.23   -3% -    6%\n        Fuzzy2F100.0       97.16        2.31       98.43        2.19   -3% -    6%\n            PKLookup      171.71        6.83      174.15        7.28   -6% -   10%\n        WildcardF0.1       67.96        1.06       69.08        1.95   -2% -    6%\n            Wildcard       43.40        0.89       44.13        0.92   -2% -    5%\n         Fuzzy2F99.0       96.83        2.46       98.49        2.21   -3% -    6%\n         Fuzzy2F95.0       97.01        2.47       98.79        2.18   -2% -    6%\n      SpanNearF100.0        3.11        0.04        3.18        0.09   -1% -    6%\n    AndHighHighF75.0       17.13        0.48       17.57        1.36   -7% -   13%\n         Fuzzy2F90.0       97.01        2.53       99.49        2.10   -2% -    7%\n      OrHighHighF0.5       31.57        0.45       32.41        1.07   -2% -    7%\n   SloppyPhraseF95.0        7.82        0.18        8.03        0.25   -2% -    8%\n       SpanNearF99.0        3.11        0.04        3.20        0.09   -1% -    7%\n     AndHighHighF0.1      136.96        3.21      140.94        5.15   -3% -    9%\n    SloppyPhraseF0.1       56.27        0.88       57.97        1.47   -1% -    7%\n          Fuzzy2F0.5      100.39        2.48      103.57        2.47   -1% -    8%\n          PhraseF2.0        7.95        0.31        8.20        0.65   -8% -   15%\n         AndHighHigh       17.97        0.46       18.55        0.84   -3% -   10%\n            TermF0.1      351.76        9.38      363.42       16.25   -3% -   10%\n        SloppyPhrase        7.90        0.16        8.19        0.19    0% -    8%\n              Phrase        3.69        0.12        3.83        0.13   -3% -   10%\n        WildcardF0.5       62.57        0.88       65.31        2.07    0% -    9%\n   SloppyPhraseF90.0        7.83        0.16        8.18        0.24    0% -    9%\n         Fuzzy2F75.0       96.77        2.46      101.14        2.41    0% -    9%\n            SpanNear        3.15        0.04        3.30        0.07    1% -    8%\n                Term       71.54        4.98       74.98        5.61   -9% -   21%\n       SpanNearF95.0        3.11        0.05        3.26        0.09    0% -    9%\n        PhraseF100.0        3.49        0.13        3.68        0.15   -2% -   14%\n         PhraseF99.0        3.49        0.12        3.69        0.15   -2% -   14%\n        SpanNearF0.1       31.54        0.48       33.49        0.73    2% -   10%\n         PhraseF95.0        3.49        0.12        3.72        0.16   -1% -   15%\n       SpanNearF90.0        3.12        0.04        3.35        0.09    3% -   11%\n         Fuzzy2F50.0       97.08        2.32      104.79        2.66    2% -   13%\n         PhraseF90.0        3.49        0.13        3.78        0.16    0% -   17%\n        Fuzzy1F100.0       47.68        1.41       52.27        1.08    4% -   15%\n         Fuzzy1F99.0       47.57        1.49       52.28        1.19    4% -   16%\n    AndHighHighF50.0       17.30        0.48       19.12        1.47    0% -   22%\n        WildcardF1.0       58.03        0.81       64.32        2.40    5% -   16%\n         Fuzzy1F95.0       47.59        1.50       52.84        1.17    5% -   17%\n   SloppyPhraseF75.0        7.85        0.15        8.73        0.24    6% -   16%\n          Fuzzy2F1.0       98.59        2.36      110.12        2.89    6% -   17%\n         Fuzzy1F90.0       47.51        1.40       53.54        1.09    7% -   18%\n         PhraseF75.0        3.51        0.13        3.98        0.18    4% -   22%\n            TermF1.0       92.28        3.05      104.56        7.44    1% -   25%\n       WildcardF99.0       36.01        0.76       40.88        1.16    8% -   19%\n          Fuzzy1F0.5       59.00        1.10       67.10        1.36    9% -   18%\n      WildcardF100.0       35.92        0.79       40.86        1.19    8% -   19%\n       WildcardF95.0       36.01        0.75       41.02        1.19    8% -   19%\n       WildcardF90.0       36.06        0.70       41.14        1.20    8% -   19%\n         Fuzzy2F20.0       98.32        2.34      112.69        2.91    9% -   20%\n       WildcardF75.0       36.19        0.62       41.69        1.15   10% -   20%\n     AndHighHighF0.5       49.93        1.37       57.85        4.13    4% -   27%\n         Fuzzy1F75.0       47.25        1.50       55.55        1.11   11% -   23%\n         Fuzzy2F10.0       98.47        2.46      116.18        3.00   12% -   24%\n       WildcardF50.0       36.77        0.55       43.44        1.29   12% -   23%\n      OrHighHighF1.0       24.37        0.38       28.99        1.90    9% -   28%\n          Fuzzy1F2.0       52.64        1.05       63.12        1.32   15% -   24%\n       SpanNearF75.0        3.11        0.04        3.74        0.10   15% -   24%\n          Fuzzy2F5.0       97.96        2.31      118.02        3.48   14% -   27%\n          Fuzzy2F2.0       98.02        2.22      119.13        3.42   15% -   27%\n    OrHighHighF100.0        7.70        0.34        9.51        0.34   13% -   33%\n     OrHighHighF99.0        7.70        0.36        9.56        0.34   14% -   34%\n         Fuzzy1F50.0       47.46        1.24       59.15        1.18   19% -   30%\n         PhraseF50.0        3.57        0.12        4.45        0.23   14% -   35%\n     OrHighHighF95.0        7.73        0.35        9.73        0.35   16% -   36%\n   SloppyPhraseF50.0        7.92        0.16       10.09        0.28   21% -   33%\n        WildcardF2.0       53.32        0.69       68.29        3.44   20% -   36%\n     OrHighHighF90.0        7.77        0.35        9.97        0.35   18% -   39%\n       WildcardF20.0       41.13        0.60       54.63        2.12   25% -   39%\n     OrHighHighF75.0        7.91        0.32       10.73        0.36   26% -   45%\n        WildcardF5.0       47.44        0.57       65.42        3.11   29% -   46%\n       WildcardF10.0       44.01        0.53       61.16        2.61   31% -   46%\n         Fuzzy1F20.0       49.57        1.20       69.49        1.70   33% -   47%\n          Fuzzy1F1.0       54.39        1.07       76.95        2.03   35% -   48%\n     AndHighHighF1.0       34.63        1.07       50.01        4.02   28% -   60%\n          PhraseF5.0        5.16        0.20        7.61        0.75   27% -   68%\n         Fuzzy1F10.0       50.23        1.07       75.36        2.11   42% -   57%\n     OrHighHighF50.0        8.36        0.29       12.58        0.48   39% -   61%\n      OrHighHighF2.0       19.65        0.34       29.58        2.27   36% -   65%\n       SpanNearF50.0        3.11        0.04        4.76        0.12   47% -   58%\n            TermF2.0       68.99        2.38      106.22        8.65   36% -   72%\n          Fuzzy1F5.0       50.74        1.06       79.90        2.38   49% -   65%\n         PhraseF20.0        3.81        0.13        6.10        0.45   43% -   78%\n           TermF50.0       42.19        1.41       67.96        4.63   45% -   77%\n           TermF75.0       41.36        1.46       67.47        5.30   45% -   82%\n           TermF90.0       41.05        1.47       68.08        5.85   46% -   86%\n           TermF95.0       41.03        1.49       68.08        6.14   45% -   87%\n         PhraseF10.0        4.22        0.16        7.02        0.62   46% -   87%\n           TermF99.0       40.99        1.56       68.31        6.21   45% -   89%\n          TermF100.0       40.88        1.61       68.28        6.32   45% -   89%\n    SloppyPhraseF0.5       18.81        0.30       31.53        0.96   59% -   75%\n    AndHighHighF20.0       17.62        0.52       30.63        2.79   53% -   95%\n      OrHighHighF5.0       14.99        0.29       27.44        1.98   66% -  100%\n        SpanNearF0.5        9.17        0.12       17.12        0.42   79% -   93%\n           TermF20.0       45.25        1.50       84.63        6.04   68% -  107%\n     OrHighHighF20.0       10.35        0.25       19.60        1.08   74% -  104%\n            TermF5.0       52.49        1.71       99.90        8.02   69% -  112%\n     AndHighHighF2.0       25.97        0.81       50.45        4.72   70% -  119%\n     OrHighHighF10.0       12.36        0.22       24.25        1.56   80% -  112%\n           TermF10.0       46.97        1.47       92.60        7.08   76% -  119%\n   SloppyPhraseF20.0        8.18        0.16       16.35        0.58   89% -  111%\n        SpanNearF1.0        6.05        0.09       12.21        0.28   94% -  109%\n    AndHighHighF10.0       18.44        0.55       40.77        4.15   92% -  151%\n     AndHighHighF5.0       20.34        0.63       50.83        5.67  115% -  186%\n   SloppyPhraseF10.0        8.52        0.17       22.79        0.96  151% -  184%\n       SpanNearF20.0        3.15        0.05        9.03        0.24  174% -  198%\n    SloppyPhraseF1.0       13.62        0.23       42.77        2.29  192% -  236%\n        SpanNearF2.0        4.45        0.06       14.31        0.37  209% -  234%\n    SloppyPhraseF5.0        9.12        0.17       29.98        1.41  207% -  250%\n    SloppyPhraseF2.0       10.85        0.19       38.31        2.00  229% -  278%\n       SpanNearF10.0        3.25        0.05       13.71        0.39  303% -  339%\n        SpanNearF5.0        3.52        0.05       19.51        0.67  428% -  481%\n\n ",
            "author": "Robert Muir",
            "id": "comment-13123072"
        },
        {
            "date": "2011-10-07T19:23:46+0000",
            "content": "by the way, luceneutil noticed some problems:\n\nTraceback (most recent call last):\n  File \"localrun.py\", line 46, in <module>\n    comp.benchmark(\"trunk_vs_patch\")\n  File \"/home/rmuir/workspace/util/competition.py\", line 194, in benchmark\n    search=self.benchSearch, index=self.benchIndex, debugs=self._debug, debug=self._debug, verifyScores=self._verifyScores)\n  File \"/home/rmuir/workspace/util/searchBench.py\", line 130, in run\n    raise RuntimeError('results differ: %s' % str(cmpDiffs))\nRuntimeError: results differ: ([], ['query=body:changer~1.0 filter=CachingWrapperFilter(PreComputedRandomFilter(pctAccept=95.0)): hit 2 has wrong id/s [8684145] vs [6260043, 8684145]', 'query=body:changer~1.0 filter=CachingWrapperFilter(PreComputedRandomFilter(pctAccept=75.0)): wrong collapsed hit count: 4 vs 5', 'query=body:changer~1.0 filter=CachingWrapperFilter(PreComputedRandomFilter(pctAccept=99.0)): hit 2 has wrong id/s [8684145] vs [8043795]'])\n\n\n\nI have no idea whats going on, but i'll upload my modifications to these filters to make them work with the patch (maybe i jacked it up). ",
            "author": "Robert Muir",
            "id": "comment-13123095"
        },
        {
            "date": "2011-10-07T20:49:14+0000",
            "content": "I nice improvements!\n\nI dont understand the errors luceneutil prints, sorry. The patch looks correct, I see no issues. acceptDocs are applied consistent and correct. Maybe Mike can help what the messages mean. The question is: How does Luceneutil verifies the hits? ",
            "author": "Uwe Schindler",
            "id": "comment-13123172"
        },
        {
            "date": "2011-10-07T21:27:01+0000",
            "content": "robert, how do you create the index? do you have two different indices for benchmarking or one? if you have two indices it could happen that one contains doc X < doc Y while the other has doc Y < doc X (doc ID wise). if both have the same score you get different order and luceneutil might fail? Just an idea... ",
            "author": "Simon Willnauer",
            "id": "comment-13123209"
        },
        {
            "date": "2011-10-07T21:54:55+0000",
            "content": "The strange thing that I see is - two docids for one hit instead of one: \"hit 2 has wrong id/s [8684145] vs [6260043, 8684145]\" and a little bit later: \"wrong collapsed hit count: 4 vs 5\" - maybe an unrelated issue with grouping module? Was grouping also enabled during benchmarking? Otherwise I cannot explain those results.\n\nSimon: Robert said, both tests used the same, \u00e4hm, identical index created before. ",
            "author": "Uwe Schindler",
            "id": "comment-13123239"
        },
        {
            "date": "2011-10-07T22:54:17+0000",
            "content": "Here's an update that passes null where appropriate to prevent extra checking and implements bits() as appropriate.\n\nSort of an open question if some of the function query stuff should be changed (ValueSourceScorer, getRangeScorer, etc).  That's a more extensive change and can be in a diff issue if necessary. ",
            "author": "Yonik Seeley",
            "id": "comment-13123275"
        },
        {
            "date": "2011-10-08T07:23:04+0000",
            "content": "Hi Yonik, thanks!\n\nJust to note, if you implement something with \"new DocIdSet() \n{....}\n\", there is no need to override bits() to return null, as the default always returns null. Only OpenBitSet/FixedBitSet and some FieldCache filters in Lucene core automatically implement bits() if directly used as DocIdSet.\n\nSo the major changes in your patch are BitDocSet to implement random access and passing null as acceptDocs at some places (see comments)? I will merge all your changes into my checkout. ",
            "author": "Uwe Schindler",
            "id": "comment-13123400"
        },
        {
            "date": "2011-10-08T09:31:24+0000",
            "content": "Attached you will find a new patch LUCENE-1536.patch, incorporating Yonik's changes plus some minor improvements:\n\n\tchanged Javadocs of DIS.bits() to explain what you should do/not do.\n\tAdded another early exit condition in FilteredQuery#Weight.scorer(): As we already get the first matching doc of the filter iterator before looking at bits or creating the query scorer, we should erly exit, if the first matching doc is Disi.NO_MORE_DOCS. This   saves us from creating the Query Scorer.\n\tI removed Robert's safety TODO in SolrIndexSearcher. It no longer disabled random access completely. After Yoniks changes, all places in Solr that are not random access secure are disabled - e.g. SolrIndexSearcher.FilterImpl (not sure what this class does, maybe it should also implement bits()?) - we should do that in a Solr specific optimization issue.\n\n\n\nSome other cool thing with filters is ANDing filters without ChainedFilter (this approach is is very effective with random access as it does not allocate additional BitsSet). If you want to AND together several filters and apply them to a Query, do the following:\n\n\nIS.search(new FilteredQuery(query,filter2), filter1,...);\n\n\n\nYou can chain even more filters in by adding more FilteredQueries. What this does:\nIS will automatically create another FilteredQuery to apply the filter and get the Weight of the top-level FilteredQuery. The scorer of this one will be top-level, get the filter and if it is random access, it will execute the filter with acceptDocs==liveDocs. The result bits of this filter will be passed to Weight.scorer of the second FilteredQuery as acceptDocs. This one will pass the acceptDocs (which are already filtered) to its Filter and if again random access pass those as acceptDocs to the inner Query's scorer. Finally the top-level IS will execute scorer.score(Collector), which in fact is the inner Query's scorer (no wrappers!) with all filtering applied in acceptDocs. This is incredible cool \n\nOne thing about large patches in an issue:\nIf you are working on an issue and have you local changes in your checkout and posted a patch to an issue and somebody else, posted an updated patch to an issue, it is often nice to see the diff between those patches. I wanted to see what Yonik changed, but a 140 K patch is not easy to handle. The trick is \"interdiff\" from patchutils package: You can call \"interdiff LUCENE-1536-original.patch LUCENE-1536-yonik.patch\" and you get a patch of only changes applied by Yonik. This patch can even be applied to your local already patched checkout.\n\nThe changes-yonik-uwe.patch was generated that way and shows, what changes I did in my last patch in contrast to Yoniks original. ",
            "author": "Uwe Schindler",
            "id": "comment-13123418"
        },
        {
            "date": "2011-10-08T09:47:43+0000",
            "content": "I really think we should commit this to trunk (assuming all tests are passing) as soon as possible.  The patch is massive and contains a lot of changes.  Any further optimizations can then be done in small chunks. ",
            "author": "Chris Male",
            "id": "comment-13123421"
        },
        {
            "date": "2011-10-08T11:13:54+0000",
            "content": "i dont think we should do that chris.\n\nLuceneutil hints that something is possibly wrong, I want to know what is going on there before any committing\n ",
            "author": "Robert Muir",
            "id": "comment-13123433"
        },
        {
            "date": "2011-10-08T11:47:44+0000",
            "content": "I agree, I count that as a test failing  ",
            "author": "Chris Male",
            "id": "comment-13123438"
        },
        {
            "date": "2011-10-08T13:00:20+0000",
            "content": "Just to note, if you implement something with \"new DocIdSet() {....}\", there is no need to override bits() to return null, as the default always returns null.\n\nRight - but I felt more comfortable being explicit about what sets would definitely not be using random access.  A comment would have served in those cases too. ",
            "author": "Yonik Seeley",
            "id": "comment-13123474"
        },
        {
            "date": "2011-10-08T13:03:21+0000",
            "content": "The changes-yonik-uwe.patch was generated that way and shows, what changes I did in my last patch in contrast to Yoniks original.\n\nThanks for the diff-diff.  It is a pain trying to review differences between patches. ",
            "author": "Yonik Seeley",
            "id": "comment-13123475"
        },
        {
            "date": "2011-10-08T13:15:42+0000",
            "content": "\nI agree, I count that as a test failing\n\nYeah a 2 hour long test!\n\nHere's what I'll do: I'll run the benchmark again, against two clean checkouts of trunk.\n\nIf it gives the same error message then I think we should chalk it up as some luceneutil problem... otherwise we should dig into it. ",
            "author": "Robert Muir",
            "id": "comment-13123479"
        },
        {
            "date": "2011-10-08T13:46:08+0000",
            "content": "\nI ask because sometimes Solr actually knows the sparseness of it's sets.\n\nYonik, it knows this on a per-filter basis? \n\nOne idea now that the heuristic is actually in FilteredQuery would be to add a (clearly marked expert/internal!) hook\nto filteredquery to override the default heuristic for cases where someone \"knows\" for sure the density of the filter.\n\nSo instead of IS.search(query, filter, ...) which will just wrap with FilteredQuery,\nan expert could just do IS.search(new FilteredQuery(query, filter) {  \n  @Override\n  boolean whatever() \n{\n    return true;\n  }\n}); ",
            "author": "Robert Muir",
            "id": "comment-13123499"
        },
        {
            "date": "2011-10-08T14:01:47+0000",
            "content": "Hmmm, so solr passes null for acceptDocs where it can... but other methods like IndexSearcher.search still pass liveDocs (and filters derived from Solr's DocSets always respect liveDocs).   Perhaps we should check if (acceptDocs==null || acceptDocs==liveDocs) and not wrap in that case. ",
            "author": "Yonik Seeley",
            "id": "comment-13123501"
        },
        {
            "date": "2011-10-08T14:02:05+0000",
            "content": "Sounds like a good idea to me Robert.  \n\nCould we also possibly add another template method to return the threshold value used in the current heuristic (so it could be removed from IndexSearcher).  That way if anybody wanted to toy with either just the threshold or the full heuristic, they could just override the appropriate method.  Doing either are very expert. ",
            "author": "Chris Male",
            "id": "comment-13123502"
        },
        {
            "date": "2011-10-08T14:12:10+0000",
            "content": "> I ask because sometimes Solr actually knows the sparseness of it's sets.\n\nYonik, it knows this on a per-filter basis?\n\nA per-filter-implementation basis.\nIt's the filters that are derived from DocSets (DocSets always have liveDocs baked in).\n\nRight now, at the point of calling IS.search(), we no longer know if the filter is of that type (since we also support filters that are not derived from DocSets), so it would seem easiest to just solve in the specific getDocIdSet() implementations to avoid wrapping if passed liveDocs. ",
            "author": "Yonik Seeley",
            "id": "comment-13123504"
        },
        {
            "date": "2011-10-08T14:19:24+0000",
            "content": "Yonik, ok thanks.\n\nI still want to rework it this way because I don't like adding the strange parameters to IndexSearcher, it clutters it up with internal details.\n\nin my local, i removed this stuff entirely and just did this in FilteredQuery.\n\n\n  /**\n   * Expert: decides if a filter should be executed as \"random-access\" or not.\n   * random-access means the filter \"filters\" in a similar way as deleted docs are filtered\n   * in lucene. This is faster when the filter accepts many documents.\n   * However, when the filter is very sparse, it can be faster to execute the query+filter\n   * as a conjunction in some cases.\n   * \n   * The default implementation returns true if the first document accepted by the\n   * filter is < 100.\n   * \n   * @lucene.internal\n   */\n  protected boolean useRandomAccess(Bits bits, int firstFilterDoc) {\n    return firstFilterDoc < 100;\n  }\n\n\n\npatch coming after tests finish. ",
            "author": "Robert Muir",
            "id": "comment-13123507"
        },
        {
            "date": "2011-10-08T14:25:22+0000",
            "content": "Yeah, I agree there should be a way to control on a per-search/filter basis, regardless of how we end up solving solr's issues. ",
            "author": "Yonik Seeley",
            "id": "comment-13123508"
        },
        {
            "date": "2011-10-08T14:25:53+0000",
            "content": "patch moving the heuristic to FilteredQuery, AssertingIndexSearcher overrides wrapFilter() and returns random.nextBoolean().\n\nIn some tests we explicitly tested on/off, i fixed these to still do this, however i think its a little overkill (since newSearcher randomizes this anyway), but i left them working the same way. ",
            "author": "Robert Muir",
            "id": "comment-13123509"
        },
        {
            "date": "2011-10-08T15:17:12+0000",
            "content": "Hmmm, so solr passes null for acceptDocs where it can... but other methods like IndexSearcher.search still pass liveDocs (and filters derived from Solr's DocSets always respect liveDocs). Perhaps we should check if (acceptDocs==null || acceptDocs==liveDocs) and not wrap in that case.\n\nYonik: You can do this, but thats out of the scope of this issue. In my original Solr patches I added the BitsFilteredDocIdSet everywhere in Solr, as the Lucene trunk requirements are to 100% respect deleted docs / accept docs in Filter.getDocIdSet(). This was not needed before, as deleted docs were applied after the filters, so a Filter that contained deleted docs, was not a problem at all.\n\nNow: If you have a Filter that magically gets the deleted docs from outside like Solr's DocSet filters, then you can safely ignore the acceptDocs given to getDocIdSet(), but only if they are == getLiveDocs(). So simply add a check for that.\n\nIf the acceptDocs Unable to render embedded object: File (= liveDocs, you have to respect them, otherwise your Filter may return wrong docs. An exaple is the chained filter case I explained above [new FilterQuery(new FilterQuery(query, filter2), filter1)]. In that case, the inner filter2 would get different acceptDocs) not found. ",
            "author": "Uwe Schindler",
            "id": "comment-13123515"
        },
        {
            "date": "2011-10-08T15:20:39+0000",
            "content": "\nHere's what I'll do: I'll run the benchmark again, against two clean checkouts of trunk.\n\nIf it gives the same error message then I think we should chalk it up as some luceneutil problem... otherwise we should dig into it.\n\nI ran the luceneutil benchmark against two clean checkouts, no errors from grouping.\n\nThis doesn't mean there is a bug in the patch, it could be a bug in grouping or a false warning or bug in the benchmark itself,\nbut still i think we need to get to the bottom of this. ",
            "author": "Robert Muir",
            "id": "comment-13123518"
        },
        {
            "date": "2011-10-08T15:21:54+0000",
            "content": "Now: If you have a Filter that magically gets the deleted docs from outside like Solr's DocSet filters, then you can safely ignore the acceptDocs given to getDocIdSet(), but only if they are == getLiveDocs(). So simply add a check for that.\n\nYes, that's exactly what I was saying. ",
            "author": "Yonik Seeley",
            "id": "comment-13123519"
        },
        {
            "date": "2011-10-08T15:51:11+0000",
            "content": "Roberts patch with TestFilteredQuery cleaned up a little bit (not thousands of anonymous subclasses) ",
            "author": "Uwe Schindler",
            "id": "comment-13123524"
        },
        {
            "date": "2011-10-08T19:04:10+0000",
            "content": "Improved the test for random access with BucktScorer to also chain two filters. This should pass the acceptDocs down so both filters are ANDed together.\n\nAlso made AssertingIndexSearcher also use the autodetection. In half of all cases it uses autodetection from parent class, in the other half it decides between random access and iterator access. ",
            "author": "Uwe Schindler",
            "id": "comment-13123554"
        },
        {
            "date": "2011-10-08T21:04:58+0000",
            "content": "Robert and me were talking about causes of the problems in the luceneutil runs. One idea would be an until-now hidden failure in grouping: FilteredQuery does not pass liveDocs down the scorer-chain (if iterative filtering is used), in the case it knows, that the Filter already uses it. This may confuse grouping!\n\nWe should also check grouping with filters and old-style-iterator collecting. ",
            "author": "Uwe Schindler",
            "id": "comment-13123563"
        },
        {
            "date": "2011-10-09T11:53:08+0000",
            "content": "Are these errors in the luceneutil runs repeatable? Are we to get more information about them to possibly replicate in a smaller test? ",
            "author": "Chris Male",
            "id": "comment-13123666"
        },
        {
            "date": "2011-10-09T12:04:12+0000",
            "content": "I don't have the hardware or time to run this intensive thing over and over (takes hours here).  ",
            "author": "Robert Muir",
            "id": "comment-13123669"
        },
        {
            "date": "2011-10-09T16:09:29+0000",
            "content": "I don't have the hardware or time to run this intensive thing over and over (takes hours here).\nI will see if I can help here on monday! I will report back once I find something. ",
            "author": "Simon Willnauer",
            "id": "comment-13123715"
        },
        {
            "date": "2011-10-10T00:37:45+0000",
            "content": "Hmm, with the latest patch have we lost the RECACHE option for CachingWrapperFilter?  Ie, to re-AND the filter bits with the latest live docs and cache that.  This is useful if you only periodically reopen the reader (and it has new deletes), so you don't have to AND the deletes in for every query. ",
            "author": "Michael McCandless",
            "id": "comment-13123818"
        },
        {
            "date": "2011-10-10T00:51:04+0000",
            "content": "+1 to keep this option nuked, otherwise it limits acceptDocs to liveDocs. ",
            "author": "Robert Muir",
            "id": "comment-13123826"
        },
        {
            "date": "2011-10-10T01:07:47+0000",
            "content": "sounds like we should split out the 'add acceptDocs to getDocIdSet' as a separate issue, just like we did for weight.scorer. ",
            "author": "Robert Muir",
            "id": "comment-13123828"
        },
        {
            "date": "2011-10-10T02:21:23+0000",
            "content": "Isn't that what this issue has basically become? plus some magic in FilteredQuery. ",
            "author": "Chris Male",
            "id": "comment-13123839"
        },
        {
            "date": "2011-10-10T02:27:20+0000",
            "content": "except that api change could actually be committed... this issue can't because it grows too complex and has a bug. ",
            "author": "Robert Muir",
            "id": "comment-13123845"
        },
        {
            "date": "2011-10-10T02:49:40+0000",
            "content": "I agree this has grown too big and complex.  Go for it. ",
            "author": "Chris Male",
            "id": "comment-13123849"
        },
        {
            "date": "2011-10-10T04:44:20+0000",
            "content": "Hmm, with the latest patch have we lost the RECACHE option for CachingWrapperFilter? Ie, to re-AND the filter bits with the latest live docs and cache that. This is useful if you only periodically reopen the reader (and it has new deletes), so you don't have to AND the deletes in for every query.\n\nMike: We can no longer do this, as the acceptDocs passed to the getDocIdSet() are no longer always liveDocs, they can be everything. Simple example is two chained FilteredQuery. At least you would need to add the liveDocs instance as key to the cache.\n\nEven without recache we are still faster than before for a query, as the liveDocs are now only applied once! Before they were not applied in the filter, but everywhere else in the query. Now they are applied once per query. Putting them into the cache is stupid and wrong and would only save us one AND. The complexity in CachingWrapperFilter does not rectify this.\n\nAbout splitting that in two patches: I have no time to do it, I am on a business trip this week. ",
            "author": "Uwe Schindler",
            "id": "comment-13123889"
        },
        {
            "date": "2011-10-10T05:15:06+0000",
            "content": "Before they were not applied in the filter, but everywhere else in the query. Now they are applied once per query\n\nSorry this is only correct for the iterator based advancing. For the filter-down-low approach they are of-course still applied. But still we should show benchmarks, that this really hurts. Because caching acceptDocs (not liveDocs!!!!) is very hard to do. Of course a chained FilteredQuery with lots of chanined filters could be simplier (only one static BitSet cached for the whole filter chain).\n\nRobert and me had more ideas how to optimize the always appliying acceptDocs case in every scorer: E.g. ConjunctionTermScorer could pass null down for all but one sub-scorer. Ideally the one that gets the liveDocs should be the one with lowest docFreq. The others don't need liveDocs, as the lowDocFreq scorer already applied them and they can never be appear in hits, because the other scorers would then advance over it. We should open new issues for those optimizations. ",
            "author": "Uwe Schindler",
            "id": "comment-13123900"
        },
        {
            "date": "2011-10-10T11:01:01+0000",
            "content": "On the diff that luceneutil hits, it looks like there's an float iota\ndifference:\n\nOn trunk we get these results:\n\n\nTASK: cat=Fuzzy1F90.0 q=body:changer~1.0 s=null f=CachingWrapperFilter(PreComputedRandomFilter(pctAccept=90.0)) group=null hits=198715\n  32.160243 msec\n  thread 5\n  doc=6199951 score=40.27584\n  doc=6199960 score=40.27584\n  doc=6200023 score=40.27584\n  doc=7580697 score=40.27584\n  doc=7995191 score=33.34529\n  doc=8684145 score=31.100195\n  doc=6260043 score=31.100193\n  doc=7320778 score=31.100193\n  doc=7454704 score=31.100193\n  doc=7979518 score=26.333052\n  50 expanded terms\n\n\n\nWith the patch we get this:\n\n\nTASK: cat=Fuzzy1F90.0 q=body:changer~1.0 s=null f=CachingWrapperFilter(PreComputedRandomFilter(pctAccept=90.0)) group=null hits=198715\n  19.300811 msec\n  thread 4\n  doc=6199951 score=40.27584\n  doc=6199960 score=40.27584\n  doc=6200023 score=40.27584\n  doc=7580697 score=40.27584\n  doc=7995191 score=33.34529\n  doc=6260043 score=31.100195\n  doc=7454704 score=31.100195\n  doc=7320778 score=31.100193\n  doc=8684145 score=31.100193\n  doc=7979518 score=26.333052\n  50 expanded terms\n\n\n\nseveral of the docs with score 31.100193 or 31.100195 flipped around. ",
            "author": "Michael McCandless",
            "id": "comment-13123998"
        },
        {
            "date": "2011-10-10T11:09:03+0000",
            "content": "So where does this leave us? ",
            "author": "Chris Male",
            "id": "comment-13124001"
        },
        {
            "date": "2011-10-10T11:26:02+0000",
            "content": "this patch shouldn't be changing scores, I think even a small difference could be indicative of a larger problem: we need to understand what is causing this. ",
            "author": "Robert Muir",
            "id": "comment-13124003"
        },
        {
            "date": "2011-10-10T11:31:50+0000",
            "content": "Are any deletes made in the above benchmarking? Might try to simulate the same change in a small unit test. ",
            "author": "Chris Male",
            "id": "comment-13124004"
        },
        {
            "date": "2011-10-10T11:36:04+0000",
            "content": "deletes dont affect scoring. ",
            "author": "Robert Muir",
            "id": "comment-13124006"
        },
        {
            "date": "2011-10-10T11:39:09+0000",
            "content": "I realise that, I was just wanting to replicate the same conditions. ",
            "author": "Chris Male",
            "id": "comment-13124007"
        },
        {
            "date": "2011-10-10T13:17:20+0000",
            "content": "well, i think you are on the right path.\n\nif our unit tests pass but luceneutil 'fails' i think thats a bad sign of the quality of our tests... it sounds like\nwe need to improve the tests to have more coverage for filters & deletions? ",
            "author": "Robert Muir",
            "id": "comment-13124080"
        },
        {
            "date": "2011-10-10T13:54:12+0000",
            "content": "one bug is that FilteredQuery in the patch runs some heuristics per segment which determine how the booleans get set that drive the BS1 versus B2 decision.\n\nThis means that some segments could get BS1, and others get BS2, meaning we will rank some documents arbitrarily higher than others when they actually have the same underlying index statistics... this is bad!\n\nSo I think at least the parameters to subscorer (topLevel/inOrder) must be consistently applied to all segments from that Weight. ",
            "author": "Robert Muir",
            "id": "comment-13124109"
        },
        {
            "date": "2011-10-10T14:33:30+0000",
            "content": "Hmm, another bug is: we are never using BS1 when the filter is applied 'down low'; this is because FilteredQuery's Weight impl does not override scoresDocsOutOfOrder.  I think it should do so?  And if the filter will be applied 'down low', it should delegate to the wrapped Weight? ",
            "author": "Michael McCandless",
            "id": "comment-13124129"
        },
        {
            "date": "2011-10-10T14:39:07+0000",
            "content": "Mike: That could be the reason for the problems: Currently it delegates to the wrapped Weight, but id does not wrap all methods. ",
            "author": "Uwe Schindler",
            "id": "comment-13124135"
        },
        {
            "date": "2011-10-10T14:45:05+0000",
            "content": "one bug is that FilteredQuery in the patch runs some heuristics per segment which determine how the booleans get set that drive the BS1 versus B2 decision.\n\nHow can BS1 and BS2 return different scores, this would be a bug? Theoretically it should be possible to have one segment with BS1 the other one with BS2.\n\nBy the way: That was not different without FilteredQuery in the older patches.\n\nOf course the selection of the right scorer based on out of order should be done based on scoresDocOutOfOrder returned by the weight. This is a bug in FilteredQuery#Weight. But easy to fix.\n\nBy the way: This was also not different without FilteredQuery in the older patches. ",
            "author": "Uwe Schindler",
            "id": "comment-13124138"
        },
        {
            "date": "2011-10-10T14:49:44+0000",
            "content": "\nHow can BS1 and BS2 return different scores, this would be a bug? Theoretically it should be possible to have one segment with BS1 the other one with BS2.\n\nWell they are different Scorer.java's ? I think its bad to use different code to score different segments, in this case two different algorithms\ncould cause floating point operations to be done in different order?\n\nIts also a bug that the scoresDocsOutOfOrder is wrong: and this is really the whole bug. Somehow FilteredQuery#Weight needs to determine what its gonna do\nup front so that collector specialization is working, so that we use BS1 or BS2 consistently across all segments, etc. ",
            "author": "Robert Muir",
            "id": "comment-13124144"
        },
        {
            "date": "2011-10-10T17:02:12+0000",
            "content": "hack patch that computes the heuristic up front in weight init, so it scores all segments consistently and returns the proper scoresDocsOutOfOrder for BS1.\n\nUwe's new test (the nestedFilterQuery) doesnt pass yet, don't know why.\n\nI recomputed the benchmarks:\n\n                Task   QPS trunkStdDev trunk   QPS patchStdDev patch      Pct diff\n          PhraseF1.0       11.99        0.20        7.79        0.23  -37% -  -31%\n            TermF0.5      135.14        7.62      116.57        0.36  -18% -   -8%\n   AndHighHighF100.0       17.34        0.78       15.44        0.15  -15% -   -5%\n    AndHighHighF95.0       17.28        0.66       15.48        0.17  -14% -   -5%\n    AndHighHighF90.0       17.31        0.76       15.58        0.19  -14% -   -4%\n    AndHighHighF99.0       17.05        1.02       15.45        0.17  -15% -   -2%\n    AndHighHighF75.0       17.47        0.78       16.03        0.15  -12% -   -3%\n     AndHighHighF5.0       20.69        0.95       19.78        0.23   -9% -    1%\n     AndHighHighF1.0       35.11        1.46       33.64        0.36   -8% -    1%\n     AndHighHighF0.1      136.04        3.70      132.00        1.41   -6% -    0%\n         AndHighHigh       18.25        0.70       17.74        0.20   -7% -    2%\n     AndHighHighF0.5       49.84        1.72       48.58        0.49   -6% -    1%\n            TermF0.1      351.18       11.01      345.85        1.73   -4% -    2%\n        Fuzzy2F100.0       95.52        4.21       94.33        2.07   -7% -    5%\n  SloppyPhraseF100.0        8.01        0.28        7.91        0.09   -5% -    3%\n         Fuzzy2F90.0       95.42        3.86       94.51        1.74   -6% -    5%\n         Fuzzy2F95.0       95.20        4.86       94.33        1.83   -7% -    6%\n          Fuzzy1F1.0       54.02        1.67       53.56        1.07   -5% -    4%\n          PhraseF2.0        7.73        0.07        7.68        0.18   -3% -    2%\n   SloppyPhraseF99.0        7.99        0.23        7.95        0.10   -4% -    3%\n    AndHighHighF50.0       17.54        0.79       17.46        0.12   -5% -    4%\n          Fuzzy2F0.1      105.39        3.93      105.34        3.74   -7% -    7%\n      SpanNearF100.0        3.16        0.06        3.16        0.04   -2% -    2%\n         Fuzzy2F99.0       94.02        6.86       94.21        1.97   -8% -   10%\n         Fuzzy2F75.0       95.56        3.51       95.76        2.02   -5% -    6%\n        WildcardF2.0       52.79        0.27       53.05        0.57   -1% -    2%\n          Fuzzy1F0.5       58.12        1.83       58.43        1.22   -4% -    5%\n          PhraseF0.1       66.34        0.78       66.73        1.68   -3% -    4%\n    SloppyPhraseF0.1       56.15        1.52       56.79        0.64   -2% -    5%\n        SloppyPhrase        8.08        0.26        8.18        0.08   -2% -    5%\n            PKLookup      176.59        5.07      178.96        5.71   -4% -    7%\n        SpanNearF0.1       32.36        0.56       32.83        0.54   -1% -    4%\n      OrHighHighF0.1       78.20        0.52       79.44        0.74    0% -    3%\n   SloppyPhraseF95.0        7.91        0.08        8.05        0.09    0% -    3%\n              Fuzzy2       94.87        3.72       96.49        1.62   -3% -    7%\n      OrHighHighF0.5       31.41        0.47       31.96        0.33    0% -    4%\n       SpanNearF99.0        3.12        0.06        3.18        0.03    0% -    4%\n        WildcardF0.5       61.97        0.56       63.28        0.82    0% -    4%\n          PhraseF0.5       19.78        0.26       20.29        0.31    0% -    5%\n            SpanNear        3.19        0.08        3.27        0.05   -1% -    6%\n        WildcardF0.1       67.45        0.64       69.24        0.89    0% -    4%\n   SloppyPhraseF90.0        8.00        0.29        8.21        0.12   -2% -    8%\n       SpanNearF95.0        3.13        0.04        3.23        0.03    1% -    5%\n            Wildcard       43.19        0.34       44.64        1.40    0% -    7%\n         Fuzzy2F50.0       95.12        4.22       98.69        2.28   -2% -   11%\n              Fuzzy1       55.28        4.53       57.68        0.76   -4% -   15%\n          OrHighHigh       12.13        0.99       12.71        0.43   -6% -   18%\n              Phrase        3.60        0.04        3.81        0.04    3% -    7%\n       SpanNearF90.0        3.15        0.05        3.35        0.04    3% -    9%\n                Term       71.69        0.40       76.53        4.13    0% -   13%\n         PhraseF99.0        3.43        0.03        3.68        0.04    5% -    9%\n        PhraseF100.0        3.39        0.05        3.67        0.04    5% -   10%\n   SloppyPhraseF75.0        8.04        0.26        8.74        0.12    3% -   13%\n         Fuzzy2F20.0       97.38        4.03      106.17        2.88    1% -   16%\n         PhraseF95.0        3.38        0.03        3.70        0.05    6% -   11%\n         PhraseF90.0        3.42        0.02        3.76        0.03    8% -   11%\n         Fuzzy2F10.0       97.19        3.69      109.27        3.23    5% -   20%\n         PhraseF75.0        3.44        0.02        3.94        0.04   12% -   16%\n          Fuzzy2F5.0       96.77        4.17      112.60        3.30    8% -   25%\n          Fuzzy1F0.1       73.61        2.43       86.22        2.79    9% -   25%\n      WildcardF100.0       35.49        0.33       41.92        1.05   14% -   22%\n       SpanNearF75.0        3.15        0.07        3.72        0.04   14% -   22%\n       WildcardF95.0       35.43        0.24       41.90        0.99   14% -   21%\n       WildcardF90.0       35.59        0.32       42.11        1.11   14% -   22%\n       WildcardF99.0       35.43        0.34       41.94        1.09   14% -   22%\n         Fuzzy1F99.0       47.41        1.79       56.45        0.78   13% -   25%\n       WildcardF75.0       35.64        0.29       42.51        0.87   15% -   22%\n        Fuzzy1F100.0       46.85        1.83       56.42        0.55   14% -   26%\n          Fuzzy2F1.0       96.75        3.75      116.85        4.32   11% -   30%\n         Fuzzy1F95.0       46.91        1.37       56.69        0.69   15% -   25%\n          Fuzzy2F0.5       97.33        4.15      117.64        4.17   11% -   30%\n          Fuzzy2F2.0       95.51        3.65      115.66        3.95   12% -   30%\n         Fuzzy1F90.0       46.84        1.95       56.83        0.78   14% -   28%\n       WildcardF50.0       36.28        0.23       44.23        0.58   19% -   24%\n            TermF1.0       93.99        4.90      114.60        0.49   15% -   29%\n         Fuzzy1F75.0       47.12        1.68       58.11        0.82   17% -   29%\n        WildcardF1.0       56.94        0.80       71.15        0.80   21% -   28%\n         PhraseF50.0        3.49        0.01        4.39        0.04   24% -   27%\n   SloppyPhraseF50.0        8.03        0.28       10.12        0.13   20% -   32%\n         Fuzzy1F50.0       46.64        2.22       60.78        0.95   22% -   38%\n      OrHighHighF1.0       24.15        0.35       32.46        0.25   31% -   37%\n       WildcardF20.0       40.55        0.30       55.72        0.73   34% -   40%\n         Fuzzy1F20.0       49.29        1.52       69.91        1.26   35% -   48%\n        WildcardF5.0       47.02        0.33       67.11        0.81   40% -   45%\n          PhraseF5.0        5.03        0.06        7.29        0.13   40% -   49%\n       WildcardF10.0       43.04        0.57       62.68        0.69   42% -   49%\n       SpanNearF50.0        3.16        0.07        4.77        0.06   45% -   56%\n    AndHighHighF20.0       17.76        0.64       27.44        0.25   47% -   61%\n         Fuzzy1F10.0       48.53        2.47       75.31        1.60   44% -   66%\n          Fuzzy1F5.0       50.45        1.70       79.01        2.08   47% -   66%\n          Fuzzy1F2.0       52.03        1.54       82.41        2.46   49% -   68%\n    OrHighHighF100.0        7.69        0.20       12.19        0.33   50% -   67%\n     OrHighHighF99.0        7.72        0.35       12.25        0.34   47% -   70%\n         PhraseF20.0        3.74        0.03        5.95        0.05   56% -   61%\n     OrHighHighF95.0        7.79        0.28       12.43        0.33   49% -   69%\n      OrHighHighF2.0       19.60        0.24       31.28        0.14   56% -   62%\n            TermF2.0       70.16        3.78      112.18        0.53   51% -   69%\n     OrHighHighF90.0        7.83        0.23       12.56        0.33   51% -   69%\n         PhraseF10.0        4.14        0.04        6.76        0.09   59% -   66%\n           TermF50.0       42.57        1.77       70.63        1.25   56% -   76%\n           TermF75.0       41.43        1.61       70.40        2.44   57% -   82%\n     OrHighHighF75.0        7.81        0.22       13.33        0.36   61% -   80%\n           TermF95.0       41.07        1.65       70.96        3.30   58% -   88%\n           TermF99.0       41.12        1.57       71.14        3.41   58% -   88%\n           TermF90.0       40.92        1.58       70.81        3.00   59% -   87%\n          TermF100.0       40.01        0.73       71.10        3.36   66% -   89%\n     OrHighHighF50.0        8.39        0.24       14.92        0.29   69% -   86%\n    AndHighHighF10.0       18.68        0.61       36.17        0.23   86% -  101%\n           TermF20.0       45.66        1.98       88.55        0.52   84% -  103%\n      OrHighHighF5.0       14.89        0.30       29.12        0.20   90% -  100%\n   SloppyPhraseF20.0        8.34        0.29       16.36        0.26   86% -  106%\n            TermF5.0       52.71        1.88      105.42        0.46   92% -  108%\n           TermF10.0       47.53        1.99       97.62        0.51   96% -  115%\n     AndHighHighF2.0       26.32        1.16       54.83        0.27   98% -  119%\n     OrHighHighF20.0       10.36        0.19       22.12        0.22  107% -  119%\n     OrHighHighF10.0       12.31        0.35       26.43        0.23  106% -  122%\n   SloppyPhraseF10.0        8.73        0.28       22.70        0.38  147% -  172%\n    SloppyPhraseF0.5       19.21        0.58       52.13        0.61  160% -  183%\n       SpanNearF20.0        3.20        0.05        8.98        0.16  171% -  189%\n    SloppyPhraseF5.0        9.30        0.33       30.17        0.44  208% -  241%\n    SloppyPhraseF1.0       13.84        0.44       46.77        0.64  223% -  253%\n    SloppyPhraseF2.0       11.00        0.36       39.85        0.54  246% -  279%\n       SpanNearF10.0        3.31        0.07       13.57        0.23  294% -  325%\n        SpanNearF0.5        9.25        0.14       39.86        0.39  320% -  341%\n        SpanNearF5.0        3.54        0.07       19.35        0.38  425% -  468%\n        SpanNearF1.0        6.15        0.11       34.27        0.48  439% -  474%\n        SpanNearF2.0        4.52        0.09       28.11        0.43  500% -  543%\n\n ",
            "author": "Robert Muir",
            "id": "comment-13124293"
        },
        {
            "date": "2011-10-10T17:23:10+0000",
            "content": "I opened LUCENE-3503 for the score diff issue; it's a pre-existing bug. ",
            "author": "Michael McCandless",
            "id": "comment-13124317"
        },
        {
            "date": "2011-10-10T18:45:53+0000",
            "content": "I also bench'd Robert's patch (turned off verifyScores in lucenebench because of LUCENE-3503); results look very similar:\n\n                Task    QPS base StdDev baseQPS filterlowStdDev filterlow      Pct diff\n          PhraseF0.5       20.18        0.65        8.05        0.56  -64% -  -55%\n          PhraseF1.0       12.26        0.33        7.96        0.54  -41% -  -28%\n    AndHighHighF95.0       16.56        0.13       15.98        1.09  -10% -    3%\n         Fuzzy2F99.0       80.52        4.67       77.72        2.34  -11% -    5%\n    AndHighHighF99.0       16.55        0.12       15.97        1.05  -10% -    3%\n   AndHighHighF100.0       16.54        0.13       15.98        1.06  -10% -    3%\n        Fuzzy2F100.0       80.32        4.60       77.64        2.34  -11% -    5%\n         Fuzzy2F90.0       80.80        5.17       78.19        2.77  -12% -    7%\n    AndHighHighF90.0       16.57        0.15       16.05        1.13  -10% -    4%\n      OrHighHighF0.1       72.17        3.60       70.11        3.69  -12% -    7%\n      OrHighHighF0.5       29.26        1.23       28.44        1.50  -11% -    6%\n         Fuzzy2F95.0       79.95        4.49       77.86        2.10  -10% -    5%\n        WildcardF0.1       59.21        4.21       58.01        3.42  -13% -   11%\n        WildcardF0.5       54.94        3.78       53.88        3.08  -13% -   11%\n        WildcardF1.0       51.31        3.31       50.35        2.44  -12% -    9%\n        WildcardF2.0       46.99        2.93       46.13        2.15  -11% -    9%\n            Wildcard       38.73        1.94       38.14        1.78  -10% -    8%\n         Fuzzy2F75.0       80.57        5.03       79.38        2.04   -9% -    7%\n    AndHighHighF75.0       16.63        0.14       16.41        1.21   -9% -    6%\n  SloppyPhraseF100.0        7.73        0.15        7.64        0.25   -6% -    4%\n   SloppyPhraseF99.0        7.74        0.15        7.66        0.26   -6% -    4%\n            TermF0.1      328.10       15.20      325.54       16.82  -10% -    9%\n          OrHighHigh       10.68        1.11       10.61        0.75  -16% -   18%\n            TermF0.5      127.55        3.70      126.88        6.02   -7% -    7%\n          PhraseF0.1       63.93        2.25       63.62        2.87   -8% -    7%\n          PhraseF2.0        7.88        0.19        7.86        0.31   -6% -    6%\n     AndHighHighF0.1      129.64        5.02      129.28        6.98   -9% -    9%\n    SloppyPhraseF0.1       53.80        0.79       53.86        1.84   -4% -    5%\n   SloppyPhraseF95.0        7.74        0.15        7.75        0.27   -5% -    5%\n    SloppyPhraseF0.5       18.44        0.31       18.47        0.64   -4% -    5%\n    SloppyPhraseF1.0       13.10        0.23       13.13        0.47   -5% -    5%\n        SloppyPhrase        7.81        0.10        7.83        0.30   -4% -    5%\n     AndHighHighF0.5       47.61        1.00       47.76        2.33   -6% -    7%\n          Fuzzy2F1.0       81.49        4.85       81.96        0.96   -6% -    8%\n              Fuzzy1       47.97        3.71       48.35        1.94  -10% -   13%\n          Fuzzy1F0.1       64.31        3.56       64.82        0.83   -5% -    8%\n              Fuzzy2       80.93        6.15       81.61        1.74   -8% -   11%\n              Phrase        3.58        0.10        3.63        0.18   -6% -    9%\n      SpanNearF100.0        2.98        0.10        3.03        0.12   -5% -    9%\n   SloppyPhraseF90.0        7.74        0.15        7.87        0.28   -3% -    7%\n         AndHighHigh       17.31        0.24       17.62        0.64   -3% -    6%\n          Fuzzy2F0.1       89.54        5.78       91.38        1.44   -5% -   10%\n       SpanNearF99.0        2.98        0.09        3.04        0.13   -5% -    9%\n                Term       58.94        6.06       60.38        4.40  -13% -   22%\n        SpanNearF0.1       29.91        1.07       30.70        1.43   -5% -   11%\n        SpanNearF0.5        8.73        0.30        8.98        0.41   -5% -   11%\n        SpanNearF5.0        3.33        0.11        3.42        0.16   -5% -   11%\n         Fuzzy2F50.0       80.90        5.19       83.29        2.28   -5% -   13%\n            SpanNear        3.01        0.10        3.10        0.14   -4% -   11%\n            TermF1.0       87.07        2.01       89.92        6.38   -6% -   13%\n       SpanNearF95.0        2.98        0.10        3.10        0.13   -3% -   12%\n        PhraseF100.0        3.37        0.06        3.51        0.17   -2% -   11%\n         PhraseF99.0        3.37        0.05        3.52        0.17   -2% -   11%\n         PhraseF95.0        3.37        0.06        3.56        0.18   -1% -   12%\n            PKLookup      126.08        5.73      133.37        1.97    0% -   12%\n       SpanNearF90.0        2.98        0.10        3.18        0.14   -1% -   15%\n         PhraseF90.0        3.38        0.06        3.61        0.18    0% -   14%\n      WildcardF100.0       32.22        1.76       34.59        1.43   -2% -   18%\n       WildcardF99.0       32.23        1.79       34.61        1.39   -2% -   18%\n   SloppyPhraseF75.0        7.74        0.16        8.32        0.33    1% -   14%\n       WildcardF95.0       32.15        1.83       34.72        1.37   -1% -   19%\n       WildcardF90.0       32.10        1.82       34.90        1.29    0% -   19%\n        Fuzzy1F100.0       42.36        1.85       46.19        2.07    0% -   19%\n    AndHighHighF50.0       16.76        0.10       18.30        1.44    0% -   18%\n         Fuzzy1F99.0       42.21        1.84       46.21        1.96    0% -   19%\n         Fuzzy2F20.0       81.24        5.06       88.97        2.11    0% -   19%\n         Fuzzy1F95.0       42.25        1.85       46.54        2.10    0% -   20%\n       WildcardF75.0       31.98        1.81       35.49        1.32    1% -   22%\n         Fuzzy1F90.0       42.15        1.77       46.84        1.91    2% -   20%\n         PhraseF75.0        3.39        0.06        3.82        0.20    4% -   20%\n         Fuzzy1F75.0       42.01        1.55       47.63        1.98    4% -   22%\n      OrHighHighF1.0       22.54        0.94       25.87        1.81    2% -   28%\n         Fuzzy2F10.0       81.10        5.01       93.81        2.75    5% -   26%\n       WildcardF50.0       32.66        1.88       37.81        1.33    5% -   27%\n         Fuzzy1F50.0       42.25        1.68       49.68        1.91    8% -   27%\n       SpanNearF75.0        2.98        0.10        3.51        0.16    9% -   27%\n          Fuzzy2F5.0       80.39        4.38       96.21        2.19   10% -   29%\n          Fuzzy2F0.5       83.14        4.67       99.73        1.75   11% -   29%\n          Fuzzy2F2.0       80.95        4.92       98.00        1.76   12% -   31%\n   SloppyPhraseF50.0        7.78        0.16        9.62        0.43   15% -   31%\n         PhraseF50.0        3.45        0.06        4.36        0.24   17% -   35%\n       WildcardF20.0       35.76        2.01       45.85        1.89   16% -   41%\n        WildcardF5.0       41.47        2.41       53.54        2.38   16% -   43%\n         Fuzzy1F20.0       43.60        1.76       57.50        2.00   22% -   42%\n       WildcardF10.0       38.26        2.17       50.63        2.11   20% -   46%\n           TermF99.0       40.49        1.22       54.84        4.93   19% -   52%\n          TermF100.0       40.51        1.29       54.99        4.92   19% -   52%\n           TermF95.0       40.44        1.19       54.95        4.82   20% -   52%\n           TermF90.0       40.34        1.08       55.00        4.58   21% -   51%\n      OrHighHighF2.0       18.15        0.69       24.94        1.69   23% -   52%\n            TermF2.0       63.47        1.48       87.39        5.94   25% -   50%\n           TermF75.0       40.05        0.92       55.28        4.38   24% -   52%\n          Fuzzy1F0.5       51.14        2.45       71.30        1.82   29% -   50%\n    OrHighHighF100.0        7.05        0.15        9.96        0.73   28% -   54%\n     OrHighHighF99.0        7.04        0.15        9.97        0.72   28% -   55%\n           TermF50.0       40.94        0.70       58.33        4.05   30% -   55%\n         Fuzzy1F10.0       43.92        1.78       62.74        1.47   34% -   52%\n     OrHighHighF95.0        7.08        0.14       10.12        0.70   30% -   56%\n     OrHighHighF90.0        7.10        0.15       10.31        0.71   32% -   58%\n          PhraseF5.0        5.02        0.10        7.33        0.48   33% -   58%\n          Fuzzy1F1.0       47.45        2.15       70.55        1.95   38% -   60%\n          Fuzzy1F5.0       44.47        1.99       66.38        1.89   38% -   60%\n          Fuzzy1F2.0       46.09        1.98       69.35        1.65   40% -   60%\n       SpanNearF50.0        2.98        0.10        4.51        0.23   39% -   64%\n     OrHighHighF75.0        7.20        0.15       10.97        0.73   39% -   65%\n    AndHighHighF20.0       16.92        0.14       26.80        2.77   40% -   76%\n         PhraseF20.0        3.69        0.06        5.86        0.36   46% -   71%\n           TermF20.0       42.65        0.76       69.54        4.48   49% -   76%\n         PhraseF10.0        4.10        0.07        6.74        0.43   51% -   78%\n     OrHighHighF50.0        7.61        0.17       12.77        0.76   54% -   81%\n      OrHighHighF5.0       13.68        0.48       23.13        1.55   52% -   86%\n            TermF5.0       47.37        1.30       81.16        5.35   55% -   87%\n           TermF10.0       43.07        0.95       74.83        4.64   59% -   88%\n     AndHighHighF1.0       32.98        0.48       59.25        8.46   51% -  108%\n   SloppyPhraseF20.0        8.00        0.16       14.72        0.84   70% -   98%\n     OrHighHighF10.0       11.20        0.34       21.25        1.38   72% -  108%\n     OrHighHighF20.0        9.32        0.22       18.10        1.12   77% -  111%\n    AndHighHighF10.0       17.54        0.16       35.08        4.05   75% -  125%\n     AndHighHighF2.0       24.58        0.27       52.49        7.16   82% -  145%\n     AndHighHighF5.0       19.26        0.17       43.11        5.37   94% -  154%\n   SloppyPhraseF10.0        8.24        0.16       19.96        1.29  122% -  162%\n       SpanNearF20.0        3.01        0.10        8.24        0.48  149% -  199%\n    SloppyPhraseF5.0        8.75        0.17       26.13        1.80  172% -  225%\n    SloppyPhraseF2.0       10.35        0.20       33.95        2.51  198% -  259%\n       SpanNearF10.0        3.09        0.10       12.23        0.76  259% -  334%\n        SpanNearF1.0        5.75        0.19       30.48        2.42  372% -  492%\n        SpanNearF2.0        4.21        0.13       24.77        1.80  428% -  551%\n\n ",
            "author": "Michael McCandless",
            "id": "comment-13124385"
        },
        {
            "date": "2011-10-10T19:33:59+0000",
            "content": "Mike: We can no longer do this, as the acceptDocs passed to the getDocIdSet() are no longer always liveDocs, they can be everything.\n\nBut CWF's job is still the same with this patch?\n\nIt's just that the cache key is now a reader + acceptDocs (instead of\njust reader), and the \"policy\" must be more careful not to cache just\nany acceptDocs.\n\nIe, we could easily add back the RECACHE option (maybe just a boolean\n\"cacheLiveDocs\" or something)?  Or am I missing something?\n\nThe IGNORE option must go away, since no filter impl is allowed to ignore\nthe incoming acceptDocs.  The DYNAMIC option is what the patch now\nhardwires.\n\nI think this use case (app using CWF, doing deletes and reopening\nperiodically) is important.  For this use case we should do the AND w/\nliveDocs only once on each reopen, and cache & reuse that, instead of\nre-ANDing over and over for every query. ",
            "author": "Michael McCandless",
            "id": "comment-13124414"
        },
        {
            "date": "2011-10-11T05:55:15+0000",
            "content": "\nhack patch that computes the heuristic up front in weight init, so it scores all segments consistently and returns the proper scoresDocsOutOfOrder for BS1.\n\nUwe's new test (the nestedFilterQuery) doesnt pass yet, don't know why.\n\nVery easy to explain: Because it's a hack! The problem is simple: The new test explicitely checks that acceptDocs are correctly handled by the query, which is not the case for your modifications. In createWeight you get the first segemnt and create the filter's docidset on it, passing liveDocs (because you have nothing else). You cache this first DocIdSet (to not need to execute getDocIdSet for the first filter 2 times) and by that miss the real acceptDocs (which are != liveDocs in this test). The firts segment therefore returns more documents that it should.\n\nAlltogether, the hack is of course uncommitable and the source of outr problem only lies in the out of order setting. The fix in your patch is fine, but too much. The scoresDocsOutOfOrder method should simply return, what the inner weight returns, because it may return docs out of order. It can still retun them in order (if a filter needs to be applied using iterator). This is not different to behaviour before. So the fix is easy: Do the same like in ConstantScoreQuery, where we return the setting from the inner weight.\n\nBeing consistent in selecting scorer implementations between segments is not an issue of this special case, it's a general problem and cannot be solved by a hack. The selection of Scorer for BooleanQuery can be different even without FilteredQuery, as BooleanWeight might return different different scorer, too (so the problem is BooleanScorer that does selection of its Scorer per-segment). To fix this, BooleanWeight must do all the scorer descisions in it's ctor, so we would need to pass also scoreInOrder and other parameters to the Weight's ctor.\n\nPlease remove the hack, and only correctly implement scoresDocsOutOfOrder (which is the reason for the problem, as it suddenly returns documents in a different order). We can still get the documents with that patch in different order if we have random access enabled together with the filter but the old IndexSearcher used DocIdSetIterator (in-order). We should ignore those differences in document order, if score is identical (and Mike's output shows scores are equal). If we want to check that the results are identical, the benchmark test must explicitely request docs-in-order on trunk vs. patch to be consistent. But then it's no longer a benchmark.\n\nConclusion: In general we explained the differences between the patches and I think, my original patch is fine except the Weight.scoresDocsOutOfOrder, which should return the inner Weight's setting (like CSQ does) - no magic needed. Our patch does not return wrong documents, just the order of equal-scoring documents is different, which is perfectly fine. ",
            "author": "Uwe Schindler",
            "id": "comment-13124720"
        },
        {
            "date": "2011-10-11T06:41:37+0000",
            "content": "Patch that fixes the Weight.scoreDocsOutOfOrder method to return the inner weight's setting. The scorer can still return docs in order, but that was identical behaviour in previous unpatched trunk (IS looked at the out-of- order setting of the weight and uses correct collector, but once a filter was applied, the documents came in order). My patch only missed to pass this setting to our wrapper query.\n\nMike: If you have time, can you check this? We may need a test, that uses a larger index and tests FilteredQuery on top of it, the current indexes used for filtering are simply too small and in most cases have only one segment \n\nThere is no need for Robert's hack (that does not work correctly with aceptDocs != liveDocs), if different BooleanScorers return significant different scores, it as a bug, not a problem in FilteredQuery. Slight score changes and therefor different order in results is not a problem at all - this is just my opinion.\n\nIf we want to check that the results are identical, the benchmark test must explicitely request docs-in-order on trunk vs. patch to be consistent. But then it's no longer a benchmark.\n\nThis is of course untrue, sorry. If the weight returns that docs may come out of order, the collector should handle this. ",
            "author": "Uwe Schindler",
            "id": "comment-13124748"
        },
        {
            "date": "2011-10-11T07:40:04+0000",
            "content": "\nThere is no need for Robert's hack (that does not work correctly with aceptDocs != liveDocs), if different BooleanScorers return significant different scores, it as a bug, not a problem in FilteredQuery. Slight score changes and therefor different order in results is not a problem at all - this is just my opinion.\n\n\n\nUwe, you are very confused.\n\nBooleanWeight always returns BS1 or BS2, and BS2 always returns the same subscorer hierarchy, the decisions are based all on nothing IR-dependent.\nWe cannot do as your patch does, it is incorrect.\n\nHere is my standing -1 against this patch that returns different scorer implementations for different segments, it totally breaks scoring for\nfiltered queries in lucene. This is unacceptable, as filters should not affect the score.\n ",
            "author": "Robert Muir",
            "id": "comment-13124778"
        },
        {
            "date": "2011-10-11T07:54:41+0000",
            "content": "Its not even just the specific patch here thats broken, the design is broken too.\n\nBecause things like whether a filter can be accessible random access or not are not per-segment things (since it must be scored in a consistent way: same scorer, across all segments).\n\ncurrently a filter could return non-null Bits for one segment and null for another. ",
            "author": "Robert Muir",
            "id": "comment-13124787"
        },
        {
            "date": "2011-10-11T08:11:33+0000",
            "content": "currently a filter could return non-null Bits for one segment and null for another.\n\nAnd this also affects choosing scorers (and does also in current Lucene 3.x: because once a filter returns non-null or non-EMPTY_DOC_ID_SET for one segment, it will score in order). So you bring up another issue that has nothing to do with this patch. The current scorer design is that the Weight decides per segment which scorer to use. To make that consisten, we have to change the way how scorers are created. This has nothing to do with this patch.\n\nThe \"bug\" (which is none in my opinion) is definitely not in this filter, it was there since the beginning.\n\nI still disagree with you: BooleanScorer1 and 2 should return same scores. PERIOD. If they don't they are buggy. ",
            "author": "Uwe Schindler",
            "id": "comment-13124793"
        },
        {
            "date": "2011-10-11T08:23:16+0000",
            "content": "\nAnd this also affects choosing scorers (and does also in current Lucene 3.x: because once a filter returns non-null or non-EMPTY_DOC_ID_SET for one segment, it will score in order).\n\nThats a good point, ill open a bug for this. \n\nWe need to wrangle all these scoring bugs, get them under control, and add tests for this stuff. ",
            "author": "Robert Muir",
            "id": "comment-13124799"
        },
        {
            "date": "2011-10-11T11:29:55+0000",
            "content": "Thats a good point, ill open a bug for this.\n\nWe don't need for that case: if filter returns null, no docs are scored  So it doe snot matter which scorer is used.\n\nBut still the Weight API is confusing an should be improved, I agree with you. BooleanWeight should ensure that it always returns the same score impl, not our filter handling is responsible. The problem with in-order/out-of order + autodetection of sparseness exists in all previous patches on this issue (since the addition of the autodetection); it's not my fault! ",
            "author": "Uwe Schindler",
            "id": "comment-13124943"
        },
        {
            "date": "2011-10-11T11:41:20+0000",
            "content": "Sorry last patch upload was somehow corrupted, maybe because of JIRA issues. This is the one only implementing Weight.scoreDocsOutOfOrder(), no hacks - to have a clean start. ",
            "author": "Uwe Schindler",
            "id": "comment-13124945"
        },
        {
            "date": "2011-10-11T11:45:43+0000",
            "content": "Strange things going on. With Google Chrome, uploading patch files corrupts the file. With MSIE it worked. Sorry for the noise. Yesterday it worked normally... ",
            "author": "Uwe Schindler",
            "id": "comment-13124947"
        },
        {
            "date": "2011-10-11T12:06:19+0000",
            "content": "you still misunderstand my patch, btw:\n\n\n. You cache this first DocIdSet (to not need to execute getDocIdSet for the first filter 2 times) and by that miss the real acceptDocs (which are != liveDocs in this test).\n\n\n+        // try to reuse from our previous heuristic sampling\n+        if (context == plan.firstLeaf && acceptDocs == plan.liveDocs) {\n\n\n\nSo how is it != liveDocs? We only re-use the cache if this 'if' is true... ",
            "author": "Robert Muir",
            "id": "comment-13124959"
        },
        {
            "date": "2011-10-11T12:53:51+0000",
            "content": "Sorry Robert, if that works its fine, but test is still failing, so something is wrong.\n\nMy problem with the patch here is more, that for most filters, the call to getDocIdSet() is the most expensive one. So you are right with caching the result. But we actually calculate the DocIdSet twice (unless we use CachingWrapperFilter), if the acceptDocs != liveDocs. And as the first segment is generally the largest one, this is even worse.\n\nIn my opinion, the whole approach of looking into the sparseness of the DocIdSet is broken for this case, as we can correctly do this only per segment, but we later require all segments to use the same scorer implementation. I have no idea, how to solve this. It would not even be enough like Chris/Mikes orginal approaches to support something like DocIdSet.useBits()/isSparse() whatever, as this is also by segment.\n\nThere is also a second problem: It might happen that one filter returns a DocIdSet that does not support bits() for one segment, but another one for other segments? How to handle that? There is one case where this happens (DocIdSet.EMPTY_DOCIDSET always returns null for bits) - but this one is grafefully handled by an early exit condition, so we won't get NPE.\n\nThe only possible solution is to make Filters always request in-order scoring, but this would limit our optimization possibilities.\n\nFinally I still think we should fix BS1 and BS2 to return identical scores (and write a test for that which compares scores). Second, in Mike's document/score listing above with/wo patch, I see no score differences, only order of docs is different (which is caused by out-of-order missing), so where is the problem? ",
            "author": "Uwe Schindler",
            "id": "comment-13124993"
        },
        {
            "date": "2011-10-11T13:02:59+0000",
            "content": "\nThere is also a second problem: It might happen that one filter returns a DocIdSet that does not support bits() for one segment, but another one for other segments? How to handle that? \n\nThis is the most serious problem I think.\n\nOne solution: we can always JUST only ever use BS2. This is all\nfilters/filteredquery does today so its no regression, just an optimization we leave on\nthe table until it can be implemented cleanly.\n\nIf we decide to do that, we are still left with the random-access-or-not-heuristic. But we could safely do this per-segment because BS2 is going to return the same set of scorers with or without bits (we know this ourselves, so its safe to do). \n\nAnd I already committed LUCENE-3503 so it should be consistent with itself  ",
            "author": "Robert Muir",
            "id": "comment-13125000"
        },
        {
            "date": "2011-10-11T14:16:45+0000",
            "content": "Also is there a reason why indexsearcher/filteredquery wouldnt just use ReqExclScorer when its not random-access? ",
            "author": "Robert Muir",
            "id": "comment-13125056"
        },
        {
            "date": "2011-10-11T14:25:44+0000",
            "content": "i see, we would have to negate the bits, maybe we should pull out FilteredQ's anon scorer into a separate .java file for consistency. ",
            "author": "Robert Muir",
            "id": "comment-13125063"
        },
        {
            "date": "2011-10-11T14:31:46+0000",
            "content": "Doing that (pulling the anon scorer out) would make the classes more readable IMO too. ",
            "author": "Chris Male",
            "id": "comment-13125068"
        },
        {
            "date": "2011-10-11T14:57:21+0000",
            "content": "Uwe, there are tiny (iota) score diffs for a few docs... eg 8684145 has score 31.100195 in one case but 31.100193 in the other (last digit differs). ",
            "author": "Michael McCandless",
            "id": "comment-13125092"
        },
        {
            "date": "2011-10-11T15:27:56+0000",
            "content": "\nIsn't it strange to ask each filter impl to do ANDing for us?\n\nLike shouldn't we AND \"on top\" ourselves?  Are there compelling\nperformance gains by requiring every filter impl to do this for us?\nIf the filter impl just delegates to BitsFilteredDocIdSet.wrap then we\nmay as well just do that on top instead?\n\nWith the scorer API, it is compelling to do this since our enum impls\nalready take a liveDocs, so it's easy for scorers to respect this and\nwe get enormous perf gains by pushing the AND inside the scorers.\n\nSo I'm not sure this API change makes sense... unless there really is\nno other (cleaner) way for us to note that a filter already contains\nonly live docs.\n\nBut then implementing your own filter is rather expert so maybe it's\nnot such a big deal to ask you to AND this other bitset we hand you.\nAnd this solution (I think!  see comment above) will still allow for\nCachingWrapperFilter to re-AND liveDocs only once on reopen and then\ncache and reuse that. ",
            "author": "Michael McCandless",
            "id": "comment-13125115"
        },
        {
            "date": "2011-10-13T10:15:19+0000",
            "content": "Patch that for now (until BooleanWeight is fixed) rewuires in-order scoring when a filter is applied, as suggested by Robert.\n\nThere was also a silly bug in CachingWrapperFilter that made it not to apply acceptDocs on cached results. This may also be a reason for result differences.\n\nMike: \"Normal Filters\" always respect acceptDocs, as they generally use the acceptDocs to pass them to IndexReader. E.g. MTQWF or all other filters. Only some special filters e.g. working on FieldCache have to respect acceptDocs. So there is no slowdown at all, it all exactly as before (pre-acceptDocs Filters always used getLiveDocs() - although they were not required to do so). The silly thing was that we applied accept docs simply at too many places, so we decided to make the filters do it themselves (as they can, because in previous patch they always called IR.getLiveDocs(); except FiledCacheFilters).\n\nThe optimizations for CachingWrapperFilter to also cache acceptDocs should maybe done in a separate issue. We have currently no slowdown, as its still faster than before. Improvements can come later.\n\nI agree, we can make the pair (IR, acceptDocs) a key into the cache map, the only problem is that accpetDocs are not required to implement equals/hashCode - and if they do, like in FixedBitSet, it's slow. So the cache should do this using systemHashCode/IdentityHashMap-like algorithm (so only compare the Bits pointer, not contents). ",
            "author": "Uwe Schindler",
            "id": "comment-13126469"
        },
        {
            "date": "2011-10-13T14:14:33+0000",
            "content": "\nPatch that for now (until BooleanWeight is fixed) rewuires in-order scoring when a filter is applied, as suggested by Robert.\n\nI'm still not sure BooleanWeight needs to be fixed: given the same parameters it will always return the same scorers.\n\nIt seems to me these parameters (topLevel/scoresInOrder) really shouldn't be parameters to weight.scorer()! ",
            "author": "Robert Muir",
            "id": "comment-13126603"
        },
        {
            "date": "2011-10-15T14:46:55+0000",
            "content": "Mike: \"Normal Filters\" always respect acceptDocs, as they generally use the acceptDocs to pass them to IndexReader.\n\nOK this makes sense (that many filter impls will need to pass through the accept docs down to eventual enums).\n\nSo I think the API change is good!\n\nThe optimizations for CachingWrapperFilter to also cache acceptDocs should maybe done in a separate issue. We have currently no slowdown, as its still faster than before. Improvements can come later.\n\nOK we can add it back under a new issue after committing this; but I\nthink it's important to not lose this (CachingWrapperFilter today is\nable to pre-AND the liveDocs and cache that).\n\nIt sounds like the cache key just has to become a pair of reader and\nidentity(acceptDocs), but we should only cache when acceptDocs ==\nreader.getLiveDocs, else we can easily over-cache if in the future we\npass \"more interesting\" acceptDocs down.\n\nGreat! ",
            "author": "Michael McCandless",
            "id": "comment-13128209"
        },
        {
            "date": "2011-10-16T14:19:57+0000",
            "content": "Do we have a conclusion about the current patch, so we can commit and work in other issues to improve? I also want to open issues to remove broken SpanFilter from core and move to sandbox. ",
            "author": "Uwe Schindler",
            "id": "comment-13128406"
        },
        {
            "date": "2011-10-17T09:24:21+0000",
            "content": "Is the only question mark remaining around the BooleanWeight work? If so, I think its definitely worth examining that in a wider separate issue after this is committed. ",
            "author": "Chris Male",
            "id": "comment-13128744"
        },
        {
            "date": "2011-10-17T09:49:09+0000",
            "content": "Is the only question mark remaining around the BooleanWeight work? If so, I think its definitely worth examining that in a wider separate issue after this is committed.\n\nThe patch requests scorer always in order for now, so BooleanWeight is not mixed up for different segments. This is not different as in current trunk, as Scorers are always requested in order if filters are used. The optimization in the future would be to use out-of-order scoring if random access bits are used. ",
            "author": "Uwe Schindler",
            "id": "comment-13128749"
        },
        {
            "date": "2011-10-17T09:55:57+0000",
            "content": "I was more referring to Robert's comment: \n\nIt seems to me these parameters (topLevel/scoresInOrder) really shouldn't be parameters to weight.scorer()!\n ",
            "author": "Chris Male",
            "id": "comment-13128754"
        },
        {
            "date": "2011-10-17T10:16:15+0000",
            "content": "Yes, that should be sorted out in another issue. We have a working fix, the rest is optimization and unrelated api changes. ",
            "author": "Uwe Schindler",
            "id": "comment-13128767"
        },
        {
            "date": "2011-10-24T11:33:20+0000",
            "content": "I will commit this tomorrow, if nobody objects and we will work on further issues to improve Weight.scorer() API, CachingWrapperFilter,... There is no slowdown, only speedups with room to improve. ",
            "author": "Uwe Schindler",
            "id": "comment-13133990"
        },
        {
            "date": "2011-10-24T11:35:47+0000",
            "content": "+1, lets commit this one and make progress here. ",
            "author": "Robert Muir",
            "id": "comment-13133994"
        },
        {
            "date": "2011-10-24T12:59:09+0000",
            "content": "Here the updated patch after some changes in trunk. It also adds missCount checks back to Caching*Filters, I lost then during cleanup. ",
            "author": "Uwe Schindler",
            "id": "comment-13134039"
        },
        {
            "date": "2011-10-25T12:09:38+0000",
            "content": "Committed trunk revision: 1188624\n\nThanks to all! ",
            "author": "Uwe Schindler",
            "id": "comment-13134964"
        },
        {
            "date": "2011-10-25T12:12:52+0000",
            "content": "Yay! ",
            "author": "Chris Male",
            "id": "comment-13134967"
        }
    ]
}