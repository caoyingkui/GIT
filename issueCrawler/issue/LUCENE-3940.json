{
    "id": "LUCENE-3940",
    "title": "When Japanese (Kuromoji) tokenizer removes a punctuation token it should leave a hole",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [],
        "type": "Bug",
        "fix_versions": [
            "4.0-ALPHA",
            "3.6.1"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "I modified BaseTokenStreamTestCase to assert that the start/end\noffsets match for graph (posLen > 1) tokens, and this caught a bug in\nKuromoji when the decompounding of a compound token has a punctuation\ntoken that's dropped.\n\nIn this case we should leave hole(s) so that the graph is intact, ie,\nthe graph should look the same as if the punctuation tokens were not\ninitially removed, but then a StopFilter had removed them.\n\nThis also affects tokens that have no compound over them, ie we fail\nto leave a hole today when we remove the punctuation tokens.\n\nI'm not sure this is serious enough to warrant fixing in 3.6 at the\nlast minute...",
    "attachments": {
        "LUCENE-3940.patch": "https://issues.apache.org/jira/secure/attachment/12520702/LUCENE-3940.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-03-31T00:19:32+0000",
            "content": "Patch; I think it's ready, except, I need to make a simpler test case than that new curious string.... ",
            "author": "Michael McCandless",
            "id": "comment-13242885"
        },
        {
            "date": "2012-03-31T09:56:05+0000",
            "content": "New patch, fixing a bug in the last one, and adding a few more test cases.  I also made the \"print curious string on exception\" from BTSTC more ascii-friendly.\n\nI think it's ready. ",
            "author": "Michael McCandless",
            "id": "comment-13243100"
        },
        {
            "date": "2012-03-31T12:41:58+0000",
            "content": "I dont think we should do this. StandardTokenizer doesnt leave holes when it drops punctuation,\nI think holes should only be real 'words' for the most part ",
            "author": "Robert Muir",
            "id": "comment-13243126"
        },
        {
            "date": "2012-03-31T21:39:08+0000",
            "content": "StandardTokenizer doesnt leave holes when it drops punctuation,\n\nBut is that really good?\n\nThis means a PhraseQuery will match across end-of-sentence (.),\nsemicolon, colon, comma, etc. (English examples..).\n\nI think tokenizers should throw away as little information as\npossible... we can always filter out such tokens in a later stage?\n\nFor example, if a tokenizer created punct tokens (instead of silently\ndiscarding them), other token filters could make use of them in the\nmean time, eg a synonym rule for \"u.s.a. -> usa\" or maybe a dedicated\nEnglish \"acronyms\" filter.  We could then later filter them out, even\nnot leaving holes, and have the same behavior that we have now?\n\nAre there non-English examples where you would want the PhraseQuery to\nmatch over punctuation...?  EG, for Japanese, I assume we don't want\nPhraseQuery applying across periods/commas, like it will now? (Not\nsure about middle dot...?  Others...?). ",
            "author": "Michael McCandless",
            "id": "comment-13243299"
        },
        {
            "date": "2012-04-01T14:01:09+0000",
            "content": "New test-only patch, breaking out the non-controversial (I think!)\npart of the patch.\n\nWith this new patch, Kuromoji still silently discards punctuation\n(just like StandardAnalyzer), but at least we get better test coverage\nin BTSTC to verify graph tokens are not messing up their offsets.\n\nI had to turn it off when testing Kuromoji w/ punctuation\nremoval... but it's still tested w/o punctuation removal, so I think\nit'd likely catch any bugs in how Kuromoji sets offsets of the\ncompound tokens... at least it's better than not checking at all\n(ie, today).\n\nThe only non-tests-only change is I uncommented an assert in Kuromoji;\nI think it's a valid assert. ",
            "author": "Michael McCandless",
            "id": "comment-13243730"
        },
        {
            "date": "2012-04-01T15:07:56+0000",
            "content": "I think kuromoji has the problem: here it creates 'fake' intermediate 'tokens' and then deletes them and this somehow screws up the decompounding graph???\n\nit should never create these tokens in the first place! I think its well accepted that words carry the information content of a doc, punctuation has no information content really here, it doesn't tell me what the doc is about, and I don't think this is controversial, I just think your view on this is extreme... ",
            "author": "Robert Muir",
            "id": "comment-13243744"
        },
        {
            "date": "2012-04-01T16:52:55+0000",
            "content": "Here's an example where we create a compound token with punctuation.\n\nI got this from the Japanese Wikipedia export, with our MockCharFilter\nsometimes doubling characters: we are at a position that the\ncharacters \u3007\u3007'''\u3001''' after it... that \u3007 is this Unicode character\nhttp://www.fileformat.info/info/unicode/char/3007/index.htm\n\nWhen Kuromoji extends from this position, both \u3007 and \u3007\u3007 are KNOWN,\nbut then we also extend by unknown \u3007\u3007'''\u3001''' (ie, \u3007\u3007 plus only\npunctuation):\n\nNote that \u3007 is not considered punctuation by Kuromoji's isPunctuation\nmethod...\n\n\n  + UNKNOWN word \u3007\u3007'''\u3001''' toPos=41 cost=21223 penalty=3400 toPos.idx=0\n  + KNOWN word \u3007\u3007 toPos=34 cost=9895 penalty=0 toPos.idx=0\n  + KNOWN word \u3007 toPos=33 cost=2766 penalty=0 toPos.idx=0\n  + KNOWN word \u3007 toPos=33 cost=5256 penalty=0 toPos.idx=1\n\n\n\nAnd then on backtrace we make a compound token (UNKNOWN) for all of\n\"\u3007\u3007'''\u3001'''\", while the decompounded path keeps two separate \"\u3007\"\ntokens but drops the '''\u3001''' since it's all punctuation, thus\ncreating inconsistent offsets. ",
            "author": "Michael McCandless",
            "id": "comment-13243782"
        },
        {
            "date": "2012-04-01T16:56:44+0000",
            "content": "I think its well accepted that words carry the information content of a doc, punctuation has no information content really here, it doesn't tell me what the doc is about, and I don't think this is controversial, I just think your view on this is extreme...\n\nI disagree with you, Robert.  (If punctuation has no information content, why does it exist?)  IMHO Mike's examples are not at all extreme, e.g. some punctuation tokens could be used to trigger position increment gaps.\n\nStandardTokenizer doesnt leave holes when it drops punctuation, I think holes should only be real 'words' for the most part\n\n\"Standard\"Tokenizer is drawn from Unicode UAX#29, which only describes word boundaries.  Lucene has grafted onto these boundary rules an assumption that only alphanumeric \"words\" should be tokens - this assumption does not exist in the standard itself.\n\nMy opinion is that we should have both types of things: a tokenizer that discards non-alphanumeric characters between word boundaries; and different type of analysis component that discards nothing.  I think of the discard-nothing process as segmentation rather than tokenization, and I've argued for it previously. ",
            "author": "Steve Rowe",
            "id": "comment-13243784"
        },
        {
            "date": "2012-04-01T16:57:57+0000",
            "content": "OK here's one possible fix...\n\nRight now, when we are glomming up an UNKNOWN token, we glom only as long as the character class of each character is the same as the first character.\n\nWhat if we also require that isPunct-ness is the same?  That way we would never create an UNKNOWN token mixing punct and non-punct...\n\nI implemented that and the tests seem to pass w/ offset checking fully turned on again... ",
            "author": "Michael McCandless",
            "id": "comment-13243785"
        },
        {
            "date": "2012-04-01T17:28:28+0000",
            "content": "New patch, implementing the idea above.  I think it's ready...\n\nIt also means we can unconditionally check for correct offsets in\ngraph tokens, which is nice. ",
            "author": "Michael McCandless",
            "id": "comment-13243789"
        },
        {
            "date": "2012-04-01T18:01:35+0000",
            "content": "\nI disagree with you, Robert. (If punctuation has no information content, why does it exist?) IMHO Mike's examples are not at all extreme, e.g. some punctuation tokens could be used to trigger position increment gaps.\n\nPunctuation simply doesn't tell you anything about the document: this is fact. if we start indexing punctuation we just create useless terms that go to every document\n\nBecause of this, nobody wastes their time trying to figure out how index \"punctuation tokens\". Mike's problem is basically the fact he is creating a compound token of '??' \n\nFurthermore, the idea that 'if we don't leave a hole for anything removed, we are losing formation' is totally arbitrary, confusing, and inconsistent anyway. How come we leave holes for definitiveness in english but not for plurals in english, but in arabic or bulgarian we don't leave holes for definiteness, because it happens to be attached to the word and stemmed? ",
            "author": "Robert Muir",
            "id": "comment-13243799"
        },
        {
            "date": "2012-04-02T10:31:22+0000",
            "content": "I'm not familiar with the various considerations that were made with StandardTokenizer, but please allow me to share some comments anyway.\n\nPerhaps it's useful to distinguish between analysis for information retrieval and analysis for information extraction here?\n\nI like Michael's and Steven's idea of doing tokenization that doesn't discard any information.  This is certainly useful in the case of information extraction.  For example, if we'd like to extract noun-phrases based on part-of-speech tags, we don't want to conjoin tokens in case there's a punctuation character between two nouns (unless that punctuation character is a middle dot).\n\nRobert is of course correct that we generally don't want to index punctuation characters that occur in every document, so from an information retrieval point of view, we'd like punctuation characters removed.\n\nIf there's an established convention that Tokenizer variants discards punctuation and produces the terms that are meant to be directly searchable, it sounds like a good idea that we stick to the convention here as well.\n\nIf there's no established convention, it seems useful that a Tokenizer would provide as much details as possible with text being input and leave downstream Filters/Analyzers  to remove whatever is suitable based on a particular processing purpose.  We can provide common ready-to-use Analyzers with reasonable defaults that users can look to, i.e. to process a specific language or do another common high-level task with text.  \n\nHence, perhaps each Tokenizer can decide what makes the most sense to do based on that particular tokenizer's scope of processing?\n\nTo Roberts point, this would leave processing totally arbitrary and consistent, but this would be by design as it wouldn't be Tokenizer's role to enforce any overall consistency \u2013 i.e. with regards to punctuation \u2013 higher level Analyzers would provide that.\n\nThoughts? ",
            "author": "Christian Moen",
            "id": "comment-13244111"
        },
        {
            "date": "2012-04-02T11:00:57+0000",
            "content": "\nPerhaps it's useful to distinguish between analysis for information retrieval and analysis for information extraction here?\n\nYes, since we are an information retrieval library, then there is no sense in adding traps and complexity to our analysis API.\n\n\nI like Michael's and Steven's idea of doing tokenization that doesn't discard any information.\n\nFor IR, this is definitely not information... calling it data is a stretch.\n\n\nIf there's an established convention that Tokenizer variants discards punctuation and produces the terms that are meant to be directly searchable, it sounds like a good idea that we stick to the convention here as well.\n\nThats what the tokenizers do today, they find tokens (In the IR sense). So yeah, there is an established convention already. Changing\nthis would be a monster trap because suddenly tons of people would be indexing tons of useless punctuation. I would strongly\noppose such a change.\n\n\n ",
            "author": "Robert Muir",
            "id": "comment-13244118"
        },
        {
            "date": "2012-04-02T11:14:39+0000",
            "content": "\nThis is certainly useful in the case of information extraction. For example, if we'd like to extract noun-phrases based on part-of-speech tags, we don't want to conjoin tokens in case there's a punctuation character between two nouns (unless that punctuation character is a middle dot).\n\nThe option still exists in kuromoji (discardPunctuation=false) if you want to use it for this.\nI added this option because it originally kept the punctuation (for downstream filters to remove).\n\nlucene-gosen worked the same way, and after some time i saw numerous examples across the internet\nwhere people simply configured the tokenizer without any filters, which means huge amounts of \npunctuation being indexed by default. People don't pay attention to documentation or details,\nso all these people were getting bad performance.\n\nBased on this experience, I didn't want keeping punctuation to be the default, nor even achievable\nvia things like solr factories here. But i added the (expert) option to Kuromoji because its really \na more general purpose things for japanese analysis, because its already being used for other things,\nand because allowing a boolean was not expensive or complex to support.\n\nBut I don't consider this a bonafied option from the lucene apis, i would be strongly against adding\nthis to the solr factories, or as an option to KuromojiAnalyzer. And, I don't think we should add such\na thing to other tokenizers either. \n\nOur general mission is search, if we want to decide we are expanding our analysis API to be generally\nuseful outside of information retrieval, thats a much bigger decision with more complex tradeoffs that\neveryone should vote on (e.g. moving analyzers outside of lucene.apache.org and everything).\n ",
            "author": "Robert Muir",
            "id": "comment-13244124"
        },
        {
            "date": "2012-07-22T16:04:56+0000",
            "content": "Bulk close for 3.6.1 ",
            "author": "Uwe Schindler",
            "id": "comment-13420209"
        }
    ]
}