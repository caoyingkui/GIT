{
    "id": "SOLR-9361",
    "title": "Concept of replica state being \"down\" is confusing and missleading (especially w/DELETEREPLICA)",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "In this thread on solr-user, Jerome Yang pointed out some really confusing behavior regarding a \"down\" node and DELETEREPLICA's behavior when a node is not shutdown cleanly...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-solr-user/201607.mbox/%3CCA+8Dz=26QuB5qNogG_GNXUU7Ru2JQQ94oH5qJvfztPvn+h=2yw@mail.gmail.com%3E\n\nI'll post a comment in a momment with a detailed walk through of how confusing the \"state\" of a node/replica can be when a machine crashes, but the SUmmary highlights are...\n\n\n\tAdmin UI & CLUSTERSTATUS API use diff terminology to describe replicas hoted on machines that can't be reached\n\t\n\t\tCLUSTERSTATUS API lists the status as \"down\"\n\t\tthe Admin UI displays them as \"Gone\" (even though it also has an option for \"Down\" which never seems to be used)\n\t\n\t\n\tNeither Admin UI & CLUSTERSTATUS API distinguish replicas that on nodes that were shutdown cleanly vs replicas on nodes that just vanished from the cluster (ie: catastrophic failure / network partitioning)\n\tDELETEREPLICA w/ onlyIfDown=true only works if a replica was shutdown cleanly\n\t\n\t\tFor a replica that was on a node that had catastrophic failure, Using onlyIfDown=true causes an error that the replica state is 'active'\n\t\t\n\t\t\tThis in spite of the fact that CLUSTERSTATUS API explicitly says \"state\":\"down\" for that replica\n\t\t\n\t\t\n\t\n\t\n\tDELETEREPLICA on any replica that was hosted on a node that is no longer up (either because it was cleanly shutdown using & using onlyIfDown=true or down for any reason and using onlyIfDown=false generates a failure that \"Server refused connection\"\n\t\n\t\tThis in spite of the fact that the DELETEREPLICA otherwise appears to have succeded\n\t\n\t\n\n\n\n\n...there are probably multiple underlying bugs here that are exponentially worse in the context of eachother.  We should spin off new issues as needed to track them once they are concretely identified, but i wanted to open this \"ubser issue\" to capture the overall experience.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2016-07-29T23:08:44+0000",
            "author": "Hoss Man",
            "content": "\nSteps to \"reproduce\" the various confusion/problems...\n\n\n\tUse bin/solr -e cloud to create a cluster & collection with the following properties:\n\t\n\t\t3 nodes\n\t\taccept default port numbers for all 3 nodes (8983, 7574, 8984)\n\t\tgettingstarted collection with 1 shard & 3 replicas using default data_driven_schema_configs\n\t\n\t\n\n\n\n\n\tObserve that the Cloud Graph UI should say you have 3 active nodes\n\t\n\t\thttp://localhost:8983/solr/#/~cloud\n\t\n\t\n\n\n\n\n\tObserve that the CLUSTERSTATUS API should also agree that you have 3 live nodes and all 3 replicas of your (single) shard with a state=\"active\" ...\n\n$ curl 'http://localhost:8983/solr/admin/collections?action=CLUSTERSTATUS&wt=json&indent=true'\n{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":10},\n  \"cluster\":{\n    \"collections\":{\n      \"gettingstarted\":{\n        \"replicationFactor\":\"3\",\n        \"shards\":{\"shard1\":{\n            \"range\":\"80000000-7fffffff\",\n            \"state\":\"active\",\n            \"replicas\":{\n              \"core_node1\":{\n                \"core\":\"gettingstarted_shard1_replica2\",\n                \"base_url\":\"http://127.0.1.1:8983/solr\",\n                \"node_name\":\"127.0.1.1:8983_solr\",\n                \"state\":\"active\"},\n              \"core_node2\":{\n                \"core\":\"gettingstarted_shard1_replica1\",\n                \"base_url\":\"http://127.0.1.1:7574/solr\",\n                \"node_name\":\"127.0.1.1:7574_solr\",\n                \"state\":\"active\",\n                \"leader\":\"true\"},\n              \"core_node3\":{\n                \"core\":\"gettingstarted_shard1_replica3\",\n                \"base_url\":\"http://127.0.1.1:8984/solr\",\n                \"node_name\":\"127.0.1.1:8984_solr\",\n                \"state\":\"active\"}}}},\n        \"router\":{\"name\":\"compositeId\"},\n        \"maxShardsPerNode\":\"1\",\n        \"autoAddReplicas\":\"false\",\n        \"znodeVersion\":8,\n        \"configName\":\"gettingstarted\"}},\n    \"live_nodes\":[\"127.0.1.1:8984_solr\",\n      \"127.0.1.1:8983_solr\",\n      \"127.0.1.1:7574_solr\"]}}\n\n\n\n\n\n\n\tNow pick a port# that is not 8983 (since that's running embedded ZK) and do an orderly shutdown: \n\n$ bin/solr stop -p 7574\nSending stop command to Solr running on port 7574 ... waiting 5 seconds to allow Jetty process 4214 to stop gracefully.\n\n\n\n\n\n\n\tIf you reload the Cloud UI screen, you should now see the node you shutdown listed in light-grey \u2013 which according to the key means \"Gone\" (as opposed to \"Down\" which the UI key says should be in an orange color)\n\t\n\t\thttp://localhost:8983/solr/#/~cloud\n\t\n\t\n\n\n\n\n\tIf you check the CLUSTERSTATUS API again it should now say you have 2 live nodes and 2 replicas with a state=\"active\" while 1 replica has a state=\"down\" ...\n\n$ curl 'http://localhost:8983/solr/admin/collections?action=CLUSTERSTATUS&wt=json&indent=true'\n{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":1},\n  \"cluster\":{\n    \"collections\":{\n      \"gettingstarted\":{\n        \"replicationFactor\":\"3\",\n        \"shards\":{\"shard1\":{\n            \"range\":\"80000000-7fffffff\",\n            \"state\":\"active\",\n            \"replicas\":{\n              \"core_node1\":{\n                \"core\":\"gettingstarted_shard1_replica2\",\n                \"base_url\":\"http://127.0.1.1:8983/solr\",\n                \"node_name\":\"127.0.1.1:8983_solr\",\n                \"state\":\"active\",\n                \"leader\":\"true\"},\n              \"core_node2\":{\n                \"core\":\"gettingstarted_shard1_replica1\",\n                \"base_url\":\"http://127.0.1.1:7574/solr\",\n                \"node_name\":\"127.0.1.1:7574_solr\",\n                \"state\":\"down\"},\n              \"core_node3\":{\n                \"core\":\"gettingstarted_shard1_replica3\",\n                \"base_url\":\"http://127.0.1.1:8984/solr\",\n                \"node_name\":\"127.0.1.1:8984_solr\",\n                \"state\":\"active\"}}}},\n        \"router\":{\"name\":\"compositeId\"},\n        \"maxShardsPerNode\":\"1\",\n        \"autoAddReplicas\":\"false\",\n        \"znodeVersion\":11,\n        \"configName\":\"gettingstarted\"}},\n    \"live_nodes\":[\"127.0.1.1:8984_solr\",\n      \"127.0.1.1:8983_solr\"]}}\n\n\n\n\n\n\n\tOur first point of confusion for most users: the Terminology used in the Cloud Admin UI screens disagress with the state values returned by the CLUSTERSTATUS API\n\n\n\n\n\tNow pick the remaining port# that is not 8983 (since that's still running embedded ZK) and simulate a \"hard crash\" of the process and/or machine:\n\n$ cat bin/solr-8984.pid\n4386\n$ kill -9 4386\n\n\n\n\n\n\n\tIf you reload the Cloud UI screen, you should now see tha port 8983 is the only \"Active\" node, and both of the nodes we have shutdown/killed are listed in light-grey \u2013 which as a reminder: according to the key means \"Gone\" (as opposed to \"Down\" which the UI key says should be in an orange color)\n\t\n\t\thttp://localhost:8983/solr/#/~cloud\n\t\n\t\n\n\n\n\n\tOur second potential point of confusion for users: no distinction in the Admin UI between a node that has been orderly shutdown (ex: for maintence) and a node that unexpectedly vanished from the cluster\n\n\n\n\n\tIf you check the CLUSTERSTATUS API again it should now say you have 1 live node and 1 replica with a state=\"active\" while 2 replicas have a state=\"down\" ...\n\n$ curl 'http://localhost:8983/solr/admin/collections?action=CLUSTERSTATUS&wt=json&indent=true'\n{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":2},\n  \"cluster\":{\n    \"collections\":{\n      \"gettingstarted\":{\n        \"replicationFactor\":\"3\",\n        \"shards\":{\"shard1\":{\n            \"range\":\"80000000-7fffffff\",\n            \"state\":\"active\",\n            \"replicas\":{\n              \"core_node1\":{\n                \"core\":\"gettingstarted_shard1_replica2\",\n                \"base_url\":\"http://127.0.1.1:8983/solr\",\n                \"node_name\":\"127.0.1.1:8983_solr\",\n                \"state\":\"active\",\n                \"leader\":\"true\"},\n              \"core_node2\":{\n                \"core\":\"gettingstarted_shard1_replica1\",\n                \"base_url\":\"http://127.0.1.1:7574/solr\",\n                \"node_name\":\"127.0.1.1:7574_solr\",\n                \"state\":\"down\"},\n              \"core_node3\":{\n                \"core\":\"gettingstarted_shard1_replica3\",\n                \"base_url\":\"http://127.0.1.1:8984/solr\",\n                \"node_name\":\"127.0.1.1:8984_solr\",\n                \"state\":\"down\"}}}},\n        \"router\":{\"name\":\"compositeId\"},\n        \"maxShardsPerNode\":\"1\",\n        \"autoAddReplicas\":\"false\",\n        \"znodeVersion\":11,\n        \"configName\":\"gettingstarted\"}},\n    \"live_nodes\":[\"127.0.1.1:8983_solr\"]}}\n\n\n\n\n\n\n\tAgain: potential points of confusion for users:\n\t\n\t\tthe Terminology used in the Cloud Admin UI screens disagress with the state values returned by the CLUSTERSTATUS API\n\t\tNo distinction in the CLUSTERSTATUS response between a replica that has been orderly shutdown (ex: for maintence) vs unexpectedly vanished from the cluster\n\t\n\t\n\n\n\n\n\tLet's assume the user is not concerned about either of the \"down\" replicas\n\t\n\t\texample: one of the machines had a hardware failure and is never coming back.  After being alerted to the crash by a monitoring system, it was realized that this cluster was overprovisioned anyway, and a second node was shutdown to repurpose the hardware\n\t\n\t\n\n\n\n\n\tnow the user wants to \"clean up\" the cluster state and remove these replicas\n\t\n\t\tbut since they've never done this before, they want to me careful not to accidently delete the only active replica, so they plan to set onlyIfDown=true when issuing their DELETEREPLICA command\n\t\n\t\n\n\n\n\n\tFirst they issue the DELETEREPLICA command for the replica that was on the node that was shutdown cleanly (7574 / core_node2 in my example above) ...\n\n$ curl 'http://localhost:8983/solr/admin/collections?action=DELETEREPLICA&onlyIfDown=true&collection=gettingstarted&shard=shard1&replica=core_node2&wt=json&indent=true'\n{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":5133},\n  \"failure\":{\n    \"127.0.1.1:7574_solr\":\"org.apache.solr.client.solrj.SolrServerException:Server refused connection at: http://127.0.1.1:7574/solr\"}}\n\n\n\n\n\n\n\tNext point of confusion: why did they get a \"Server refused connection\" failure message? of course you can't connect, the server is down \u2013 that's why the replica is being removed.\n\n\n\n\n\tNow in a confused panic that maybe they screwed something up, the user checks the Cloud Admin UI & CLUSTERSTATUS\n\t\n\t\tAdmin UI now longer shows the removed replica \u2013 so hopefully the failure can be ignored?\n\t\t\n\t\t\thttp://localhost:8983/solr/#/~cloud\n\t\t\n\t\t\n\t\tCLUSTERSTATUS API also seems \"ok\" ? ... \n\n$ curl 'http://localhost:8983/solr/admin/collections?action=CLUSTERSTATUS&wt=json&indent=true'{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":2},\n  \"cluster\":{\n    \"collections\":{\n      \"gettingstarted\":{\n        \"replicationFactor\":\"3\",\n        \"shards\":{\"shard1\":{\n            \"range\":\"80000000-7fffffff\",\n            \"state\":\"active\",\n            \"replicas\":{\n              \"core_node1\":{\n                \"core\":\"gettingstarted_shard1_replica2\",\n                \"base_url\":\"http://127.0.1.1:8983/solr\",\n                \"node_name\":\"127.0.1.1:8983_solr\",\n                \"state\":\"active\",\n                \"leader\":\"true\"},\n              \"core_node3\":{\n                \"core\":\"gettingstarted_shard1_replica3\",\n                \"base_url\":\"http://127.0.1.1:8984/solr\",\n                \"node_name\":\"127.0.1.1:8984_solr\",\n                \"state\":\"down\"}}}},\n        \"router\":{\"name\":\"compositeId\"},\n        \"maxShardsPerNode\":\"1\",\n        \"autoAddReplicas\":\"false\",\n        \"znodeVersion\":12,\n        \"configName\":\"gettingstarted\"}},\n    \"live_nodes\":[\"127.0.1.1:8983_solr\"]}}\n\n\n\t\n\t\n\n\n\n\n\tFingers crossed that everything is actually ok, they issue the DELETEREPLICA command for the replica that was on the node that had a catastrophic failure (8984 / core_node3 in my example above) ...\n\n$ curl 'http://localhost:8983/solr/admin/collections?action=DELETEREPLICA&onlyIfDown=true&collection=gettingstarted&shard=shard1&replica=core_node3&wt=json&indent=true'\n{\n  \"responseHeader\":{\n    \"status\":400,\n    \"QTime\":26},\n  \"Operation deletereplica caused exception:\":\"org.apache.solr.common.SolrException:org.apache.solr.common.SolrException: Attempted to remove replica : gettingstarted/shard1/core_node3 with onlyIfDown='true', but state is 'active'\",\n  \"exception\":{\n    \"msg\":\"Attempted to remove replica : gettingstarted/shard1/core_node3 with onlyIfDown='true', but state is 'active'\",\n    \"rspCode\":400},\n  \"error\":{\n    \"metadata\":[\n      \"error-class\",\"org.apache.solr.common.SolrException\",\n      \"root-error-class\",\"org.apache.solr.common.SolrException\"],\n    \"msg\":\"Attempted to remove replica : gettingstarted/shard1/core_node3 with onlyIfDown='true', but state is 'active'\",\n    \"code\":400}}\n\n\n\n\n\n\n\tNow the user is completley baffled\n\t\n\t\twhy is Solr complaining that gettingstarted/shard1/core_node3 can't be removed with onlyIfDown='true' because state is 'active' ???\n\t\tNeither the UI or the CLUSTERSTATUS API said the replica was up \u2013 CLUSTERSTATUS explicitly said it was DOWN!\n\t\n\t\n\n\n\n\n\tFrustrated, the user tries again \u2013 this time with onlyIfDown=false assuming that that's the best option given the error message they recieved...\n\n$ curl 'http://localhost:8983/solr/admin/collections?action=DELETEREPLICA&onlyIfDown=false&collection=gettingstarted&shard=shard1&replica=core_node3&wt=json&indent=true'\n{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":5131},\n  \"failure\":{\n    \"127.0.1.1:8984_solr\":\"org.apache.solr.client.solrj.SolrServerException:Server refused connection at: http://127.0.1.1:8984/solr\"}}\n\n\n\n\n\n\n\tAnother confusing \"Server refused connection\" failure message \u2013 but at least now the Admin UI & CLUSTERSTATUS API agree that they don't know anything about either replica we wanted to remove...\n\t\n\t\thttp://localhost:8983/solr/#/~cloud\n\t\t\n$ curl 'http://localhost:8983/solr/admin/collections?action=CLUSTERSTATUS&wt=json&indent=true'{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":2},\n  \"cluster\":{\n    \"collections\":{\n      \"gettingstarted\":{\n        \"replicationFactor\":\"3\",\n        \"shards\":{\"shard1\":{\n            \"range\":\"80000000-7fffffff\",\n            \"state\":\"active\",\n            \"replicas\":{\n              \"core_node1\":{\n                \"core\":\"gettingstarted_shard1_replica2\",\n                \"base_url\":\"http://127.0.1.1:8983/solr\",\n                \"node_name\":\"127.0.1.1:8983_solr\",\n                \"state\":\"active\",\n                \"leader\":\"true\"},\n              \"core_node3\":{\n                \"core\":\"gettingstarted_shard1_replica3\",\n                \"base_url\":\"http://127.0.1.1:8984/solr\",\n                \"node_name\":\"127.0.1.1:8984_solr\",\n                \"state\":\"down\"}}}},\n        \"router\":{\"name\":\"compositeId\"},\n        \"maxShardsPerNode\":\"1\",\n        \"autoAddReplicas\":\"false\",\n        \"znodeVersion\":12,\n        \"configName\":\"gettingstarted\"}},\n    \"live_nodes\":[\"127.0.1.1:8983_solr\"]}}\n\n\n\t\n\t\n\n ",
            "id": "comment-15400199"
        },
        {
            "date": "2016-07-30T05:01:21+0000",
            "author": "Mark Miller",
            "content": "Historical info. Gone means no live node in zookeeper. Down should mean, either stale state and no zk connection or connected to zk and working to move from down to recovering.  ",
            "id": "comment-15400449"
        }
    ]
}