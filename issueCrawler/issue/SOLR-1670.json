{
    "id": "SOLR-1670",
    "title": "synonymfilter/map repeat bug",
    "details": {
        "affect_versions": "1.4",
        "status": "Closed",
        "fix_versions": [],
        "components": [
            "Schema and Analysis"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Cannot Reproduce"
    },
    "description": "as part of converting tests for SOLR-1657, I ran into a problem with synonymfilter\n\nthe test for 'repeats' has a flaw, it uses this assertTokEqual construct which does not really validate that two lists of token are equal, it just stops at the shorted one.\n\n    // repeats\n    map.add(strings(\"a b\"), tokens(\"ab\"), orig, merge);\n    map.add(strings(\"a b\"), tokens(\"ab\"), orig, merge);\n    assertTokEqual(getTokList(map,\"a b\",false), tokens(\"ab\"));\n    /* in reality the result from getTokList is ab ab ab!!!!! */\n\n\n\nwhen converted to assertTokenStreamContents this problem surfaced. attached is an additional assertion to the existing testcase.",
    "attachments": {
        "SOLR-1670.patch": "https://issues.apache.org/jira/secure/attachment/12428668/SOLR-1670.patch",
        "SOLR-1670_test.patch": "https://issues.apache.org/jira/secure/attachment/12428403/SOLR-1670_test.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Robert Muir",
            "id": "comment-12792363",
            "date": "2009-12-18T06:12:22+0000",
            "content": "btw i don't know how to fix this repeat problem (which is why i only uploaded a test, sorry). I am going to get some sleep and look at it tomorrow, its the last tokenstream to go thru I promise. \n\nif anyone knows this code better please help "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12792456",
            "date": "2009-12-18T12:36:10+0000",
            "content": "note: i have found more bugs in this filter by reworking the tests.\n\nit is impossible to fix the real bugs in the synonyms filter the way the tests are written now, as they do not compare correctly.\nI will upload the patch to SOLR-1657 fix all the analysis tests first, so then we have tests that work and can actually start debugging this.\nin the comments i will put what the old test \"thought\" it was testing as a FIXME "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12792920",
            "date": "2009-12-19T22:37:47+0000",
            "content": "the test for 'repeats' has a flaw, it uses this assertTokEqual construct which does not really validate that two lists of token are equal, it just stops at the shorted one.\n\nI agree with you regarding this part. But I'm not sure that the following size() should be 1 in your patch:\n\n\n+    assertEquals(1, getTokList(map,\"a b\",false).size());\n\n\n\nIf what \"repeats\" implies is repeating same term intentionally, I think it can boost tf. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12792922",
            "date": "2009-12-19T22:40:50+0000",
            "content": "Hi Koji, I don't really understand what you are saying here?\n\nIf it is a feature of synonymfilter that it should repeat to boost TF, then the test should have looked like this:\n\nassertTokEqual(getTokList(map,\"a b\",false), tokens(\"ab ab ab\"));\n\nbut it does not, the test looks like this:\nassertTokEqual(getTokList(map,\"a b\",false), tokens(\"ab\"));\n "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12792923",
            "date": "2009-12-19T22:48:31+0000",
            "content": "Koji, in my opinion the first step towards fixing this would be to resolve SOLR-1674.\n\nThis fixes the tests so that they reveal the true current behavior.\nBecause of the many problems with the current tests, I do not understand what the desired behavior of this filter should even be.\nThe only thing I know for sure is what the current behavior is.\n\nhere is another example:\n\n    // test that generated tokens start at the same offset as the original\n    map.add(strings(\"a\"), tokens(\"aa\"), orig, merge);\n    assertTokEqual(getTokList(map,\"a,5\",false), tokens(\"aa,5\")); /* NOTE: This position increment is really 1, but the test passes!!!! */\n\n\n\nAll the tests similar to the above test that position increments are the same as the original token.\nBut this is not actually what is happening, in fact synonymfilter is generating 'aa' with a position increment of 1.\n\nThere are several examples like this in SOLR-1674. What I did was write the assertions so that the tests pass, revealing the true behavior.\nBut I put comments above these assertions that say what i thought the existing test was trying to show, i.e. the desired behavior. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12792928",
            "date": "2009-12-19T23:27:22+0000",
            "content": "Robert, sorry, I wanted to say I agree with you regarding \"the test for 'repeats' has a flaw\". Then \"boost TF\" was just an input, though I don't know it is intentional feature or side effect.\n\nWhy don't you fix the flaws in SynonymFilter test in this ticket first, then fix SOLR-1674? (I've not looked into SOLR-1674 yet.) "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12792930",
            "date": "2009-12-19T23:44:04+0000",
            "content": "Why don't you fix the flaws in SynonymFilter test in this ticket first, then fix SOLR-1674? (I've not looked into SOLR-1674 yet.)\n\nBecause how would you know I properly fixed it? In fact, if anyone else submitted a patch to fix this issue, I don't really care how many tests they write, I would post a comment voting against the patch, because the tests mean nothing.\n\nThe analysis tests themselves (things like this assertTokEqual) are broken, so SOLR-1674 fixes the analysis tests so they actually work.\n\nI think we have to make the tests work correctly, before we can think of fixing nontrivial bugs? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12792940",
            "date": "2009-12-20T02:12:32+0000",
            "content": "This is my mess that predates solr even going open source... I may have a few neurons that can still recognize what it was trying to do.\n\nThe repeats test was testing that repeated rules didn't mess anything up.  In the code you quote, the rule a  b -> ab is added twice.  The rule merging code should handle it (the merging code handles merging all common prefixes).\n\nRobert, can you tell if the flaw is in the test code or the SynonymFilter?  I'm pretty sure that such a bug (in the filter) wasn't originally there... so my money would be on the test - perhaps getTokList, but I haven't had a chance to look yet. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12792980",
            "date": "2009-12-20T10:22:52+0000",
            "content": "Robert, can you tell if the flaw is in the test code or the SynonymFilter? I'm pretty sure that such a bug (in the filter) wasn't originally there... so my money would be on the test - perhaps getTokList, but I haven't had a chance to look yet.\n\nI am pretty sure there is a problem (both the existing test, and the synonymMap merging code).\nThis is why i think there is some confusion.\nThe existing test implies that the output will be \"ab\", which makes me think the SynonymMap should merge these duplicate definitions.\nThe real output is \"ab ab ab\", but the test as written (expect \"ab\") passes with the existing test setup, the problem is assertTokEqual().\n\nthe rewritten test in SOLR-1674 looks like: \n\n\nassertTokenizesTo(map, \"a b\", new String[] { \"ab\", \"ab\", \"ab\"  });\n\n\n\nwhere assertTokenizesTo is just\n\n\nstatic void assertTokenizesTo(SynonymMap dict, String input,\n      String expected[]) throws IOException {\n    Tokenizer tokenizer = new WhitespaceTokenizer(new StringReader(input));\n    SynonymFilter stream = new SynonymFilter(tokenizer, dict);\n    assertTokenStreamContents(stream, expected);\n  }\n\n\n\nand assertTokenStreamContents is the one copied from lucene.\n\nedit: i think i resolved the additional problems "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12793213",
            "date": "2009-12-21T14:30:29+0000",
            "content": "Yep, must be a bug in the merging code, never caught because of a bug in assertTokEqual.  I'll look into it. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12793423",
            "date": "2009-12-21T23:49:34+0000",
            "content": "Here's a patch that removes duplicate tokens at the same position.\nThis isn't really a serious bug, as the repeated tokens were at the same position.\n\nI don't think assertTokEqual really has a bug - it's written more to match lucene queries and indexes, not to exactly compare one token stream with another.   So a singe ab token matches multiple ab tokens at the same position. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12793529",
            "date": "2009-12-22T06:17:21+0000",
            "content": "Yonik, thanks, I am glad it is not a serious bug!\n\nI don't think assertTokEqual really has a bug - it's written more to match lucene queries and indexes, not to exactly compare one token stream with another.\n\nI see your point about assertTokEqual, but it was being used to test tokenstreams... so I am glad to see it go. \n\nMainly want to make sure we don't break anything trying to move this stuff to the new tokenstream API, I am sure this will involve more tests, but for now having well-defined behavior makes it a lot easier. Thanks again "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12793655",
            "date": "2009-12-22T16:23:31+0000",
            "content": "I don't think assertTokEqual really has a bug - it's written more to match lucene queries and indexes, not to exactly compare one token stream with another. So a singe ab token matches multiple ab tokens at the same position.\n\nSeriously, I think if you want to test these things, then the assertQ etc should be used (actually run queries and test results) instead.\n\nBut this is a bug, because aa is not the same as aa,aa(pos=0),aa(pos=0), not even for \"queries and indexes\".\nThis is because the latter will affect the score of the search.\n\nI think this is right in line with what you are saying in SOLR-1674, that you have somehow lost some flexibility: This is not true\n\n\tthese things are different, the tokenstream output is different, things such as score change\n\tif you don't like this, and instead want to test queries, then do just that, instead of examining tokenstreams.\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12793684",
            "date": "2009-12-22T17:34:55+0000",
            "content": "Duplicated tokens aside (that should have been tested another way), I prefer to test the intended semantics rather than the exact behavior (which often ends up testing the exact implementation and ends up being too fragile... valid changes to the implementation break the tests).\n\nThe description of the synonym filter never specifies the order of overlapping tokens, thus the tests should accept either.  But as a practical matter, I don't really care about the restrictive token ordering since it's localized to a single method that can be changed as needed later if the tests break. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12793690",
            "date": "2009-12-22T17:48:19+0000",
            "content": "Duplicated tokens aside (that should have been tested another way), I prefer to test the intended semantics rather than the exact behavior\n\nthen it would be better to use actual queries and such to test this filter, like (some of the) WordDelimiterFilter tests do with assertQ and friends.\n\nBut I think things like the order of tokens in the stream is still something i would like to preserve when migrating to the new tokenstream API. Maybe it doesn't seem important but it would still be a change in behavior (probably only break you if you ran positionfilter after, or something like that).\n\nI guess in my opinion, overall its better for the tests to be overly strict, and if in the future we make a valid change to the implementation that breaks a test, we can discuss it during said change, and people can comment on whether this behavior was actually important: for example the aa/a versus a/aa i would probably say not a big deal, but the aa versus aa/aa/aa thing to me is a big deal.\n\nThe alternative, being overly lax, presents the possibility of introducing incorrect behavior without being caught, and I think this is very dangerous. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-12806791",
            "date": "2010-01-31T08:06:32+0000",
            "content": "New version of the patch, introducing the possibility to test token streams that are allowed to vary the order of their overlapping tokens.  All tests pass for me.\n\nYonik wrote:\nThe description of the synonym filter never specifies the order of overlapping tokens, thus the tests should accept either.\n\nYonik, can you check that the attached patch does what you are suggesting?\n\nRobert wrote:\n\nI guess in my opinion, overall its better for the tests to be overly strict, and if in the future we make a valid change to the implementation that breaks a test, we can discuss it during said change, and people can comment on whether this behavior was actually important: for example the aa/a versus a/aa i would probably say not a big deal, but the aa versus aa/aa/aa thing to me is a big deal.\n\nThe alternative, being overly lax, presents the possibility of introducing incorrect behavior without being caught, and I think this is very dangerous.\n\nRobert, do you think that the attached patch crosses over into overly lax land? "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12806833",
            "date": "2010-01-31T14:35:54+0000",
            "content": "Steven, i don't have a problem with your patch (I do not wish to be in the way of anyone trying to work on SynonymFilter)\n\nBut i want to explain some of where i was coming from.\n\nThe main reason i got myself into this mess was to try to add wordnet support to solr. However, this is currently not possible without duplicating a lot of code.\nWe need to be really careful about allowing any order, it does matter in some situations.\nFor example, in Lucene's synonymfilter (with wordnet support), it has an option to limit the number of expansions (so its like a top-N synonym expansion).\nSolr doesnt currently have this, so its N/A for now, but just an example where the order suddenly becomes important.\n\nonly slightly related: we added some improvements to this assertion in lucene recently and found a lot of bugs, better checking for clearAttribute() and end()\nat some I would like to port these test improvements over to solr, too.  "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-12828077",
            "date": "2010-02-01T12:36:16+0000",
            "content": "We need to be really careful about allowing any order, it does matter in some situations.\n\nI left in place the existing test method, which requires the specified order.\n\n\nonly slightly related: we added some improvements to this assertion in lucene recently and found a lot of bugs, better checking for clearAttribute() and end()\nat some I would like to port these test improvements over to solr, too. \n\n+1 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12828204",
            "date": "2010-02-01T18:19:33+0000",
            "content": "I left in place the existing test method, which requires the specified order.\n\nis it possible to only expose the 'unsorted' one to synonyms test (such as in the synonyms test file itself, rather than base token stream test case?)\n\ni can't think of another situation where it would make sense, more likely to be abused instead "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-12829079",
            "date": "2010-02-03T14:04:13+0000",
            "content": "is it possible to expose the 'unsorted' one to synonyms test (such as in the synonyms test file itself, rather than base token stream test case?)\n\nI wrote it the way I did to reduce code duplication/maintenance effort, e.g. the upcoming sync with Lucene's changes in this area.\n\nI'm thinking it should be possible to refactor the current method to serve both the sorted case and an external unsorted case.  I'll work on it. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12829088",
            "date": "2010-02-03T14:27:30+0000",
            "content": "is it possible to only expose the 'unsorted' one to synonyms test (such as in the synonyms test file itself, rather than base token stream test case?)\n\nOrder of overlapping tokens is unimportant in every TokenFilter used in Solr that I know about. Order-sensitivity is the exception, no? "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12829092",
            "date": "2010-02-03T14:40:39+0000",
            "content": "Order of overlapping tokens is unimportant in every TokenFilter used in Solr that I know about. Order-sensitivity is the exception, no?\n\nI guess all along my problem is that tokenstreams are ordered by definition. if this order does not matter, a test that uses actual queries would make more sense.\n\nThe problem was that the test constructions previously used by this filter were used in other places where they really shouldn't have been, and the laxness hid real bugs (such as this very issue itself!!!).\nThis is all I am trying to avoid. There is nothing wrong with Steven's patch/test construction, just trying to err on the side of caution. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12829094",
            "date": "2010-02-03T14:49:30+0000",
            "content": "I guess all along my problem is that tokenstreams are ordered by definition. \n\nNot at the semantic level (for overlapping tokens).  For instance, it would be incorrect for someone to rely on a specific ordering for overlapping tokens - order has never been guaranteed by any TokenFilters that do produce overlapping tokens, and in fact the ordering for some has changed in the past as the implementation changed. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12829097",
            "date": "2010-02-03T14:57:15+0000",
            "content": "Not at the semantic level (for overlapping tokens).\n\nAnother way to look at it is that a tokenstream is just a sequence of tokens, and posInc is just another attribute.\n\nyour description of semantics makes sense in terms of how it is used by the indexer, but the order of these tokens can matter if someone uses a custom tokenfilter, it might matter for some custom attributes, and it might matter for a different consumer, its different behavior. i have made an effort to preserve all the behavior of all these  tokenstreams when converting to the new api. I really don't want to break anything. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12829399",
            "date": "2010-02-04T02:28:11+0000",
            "content": "Not at the semantic level (for overlapping tokens). For instance, it would be incorrect for someone to rely on a specific ordering for overlapping tokens - order has never been guaranteed by any TokenFilters that do produce overlapping tokens, and in fact the ordering for some has changed in the past as the implementation changed.\n\nIs this part of the TokenFilter contract documented somewhere? "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-12829765",
            "date": "2010-02-04T20:35:57+0000",
            "content": "I like Yonik's \"overlapping tokens\" term better than the term I used in my patch (\"collocated tokens\" - ambiguous, since this is used to refer to phrases rather than shared-position tokens), so I've replaced \"collocated\" with \"overlapping\" in variable names in this version of the patch.\n\nAlso, I had forgotten to test for the case where more token stream tokens are present at the same position than are expected.  This is now fixed.\n\nI have not made any other changes - in particular, I have not moved the overlapping-token-order-insensitive test capability out of BaseTokenTestCase.\n\nAll tests pass for me. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-14661705",
            "date": "2015-08-07T11:45:59+0000",
            "content": "Is this bug stil present 5 years later?  or can we close? "
        },
        {
            "author": "Alexandre Rafalovitch",
            "id": "comment-15552465",
            "date": "2016-10-06T16:51:15+0000",
            "content": "The suggestion to close this case is more than a year old. I am assuming, it is safe to close. If the issue occurs with the implementation, a new issue can be created with reference to this one. "
        }
    ]
}