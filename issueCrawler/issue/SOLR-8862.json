{
    "id": "SOLR-8862",
    "title": "/live_nodes is populated too early to be very useful for clients -- CloudSolrClient (and MiniSolrCloudCluster.createCollection) need some other ephemeral zk node to knowwhich servers are \"ready\"",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "/live_nodes is populated surprisingly early (and multiple times) in the life cycle of a sole node startup, and as a result probably shouldn't be used by CloudSolrClient (or other \"smart\" clients) for deciding what servers are fair game for requests.\n\nwe should either fix /live_nodes to be created later in the lifecycle, or add some new ZK node for this purpose.\n\noriginal bug report\nI haven't been able to make sense of this yet, but what i'm seeing in a new SolrCloudTestCase subclass i'm writing is that the code below, which (reasonably) attempts to create a collection immediately after configuring the MiniSolrCloudCluster gets a \"SolrServerException: No live SolrServers available to handle this request\" \u2013 in spite of the fact, that (as far as i can tell at first glance) MiniSolrCloudCluster's constructor is suppose to block until all the servers are live..\n\n\n    configureCluster(numServers)\n      .addConfig(configName, configDir.toPath())\n      .configure();\n    Map<String, String> collectionProperties = ...;\n    assertNotNull(cluster.createCollection(COLLECTION_NAME, numShards, repFactor,\n                                           configName, null, null, collectionProperties));",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2016-03-16T23:21:53+0000",
            "author": "ASF subversion and git services",
            "content": "Commit a0d48f873c21ca0ab5ba02748c1659a983aad886 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a0d48f8 ]\n\nSOLR-445: start of a new randomized/chaosmonkey test, currently blocked by SOLR-8862 (no monkey yet) ",
            "id": "comment-15198397"
        },
        {
            "date": "2016-03-16T23:23:13+0000",
            "author": "Hoss Man",
            "content": "The above mentioned commit a0d48f873c21ca0ab5ba02748c1659a983aad886 on the jira/SOLR-445 branch includes the test i was refering to.\n\non that branch, this seed fails100% of the time for me...\n\n\n   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene60), sim=ClassicSimilarity, locale=tr-TR, timezone=America/Manaus\n   [junit4]   2> NOTE: Linux 3.19.0-51-generic amd64/Oracle Corporation 1.8.0_74 (64-bit)/cpus=4,threads=1,free=214617808,total=276299776\n   [junit4]   2> NOTE: All tests run in this JVM: [TestTolerantUpdateProcessorChaosMonkey]\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestTolerantUpdateProcessorChaosMonkey -Dtests.seed=E73756F21DF21ECC -Dtests.slow=true -Dtests.locale=tr-TR -Dtests.timezone=America/Manaus -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] ERROR   0.00s | TestTolerantUpdateProcessorChaosMonkey (suite) <<<\n   [junit4]    > Throwable #1: org.apache.solr.client.solrj.SolrServerException: No live SolrServers available to handle this request:[http://127.0.0.1:45715/solr, http://127.0.0.1:37319/solr, http://127.0.0.1:60667/solr, http://127.0.0.1:57740/solr, http://127.0.0.1:35626/solr]\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([E73756F21DF21ECC]:0)\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.LBHttpSolrClient.request(LBHttpSolrClient.java:352)\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.CloudSolrClient.sendRequest(CloudSolrClient.java:1157)\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:927)\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.CloudSolrClient.request(CloudSolrClient.java:863)\n   [junit4]    > \tat org.apache.solr.client.solrj.SolrClient.request(SolrClient.java:1219)\n   [junit4]    > \tat org.apache.solr.cloud.MiniSolrCloudCluster.makeCollectionsRequest(MiniSolrCloudCluster.java:415)\n   [junit4]    > \tat org.apache.solr.cloud.MiniSolrCloudCluster.createCollection(MiniSolrCloudCluster.java:399)\n   [junit4]    > \tat org.apache.solr.cloud.TestTolerantUpdateProcessorChaosMonkey.createMiniSolrCloudCluster(TestTolerantUpdateProcessorChaosMonkey.java:125)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]    > Caused by: org.apache.solr.client.solrj.SolrServerException: IOException occured when talking to server at: http://127.0.0.1:35626/solr\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:591)\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:241)\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:230)\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.LBHttpSolrClient.doRequest(LBHttpSolrClient.java:372)\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.LBHttpSolrClient.request(LBHttpSolrClient.java:325)\n   [junit4]    > \t... 31 more\n   [junit4]    > Caused by: java.net.SocketException: Connection reset\n   [junit4]    > \tat java.net.SocketInputStream.read(SocketInputStream.java:209)\n   [junit4]    > \tat java.net.SocketInputStream.read(SocketInputStream.java:141)\n   [junit4]    > \tat org.apache.http.impl.io.AbstractSessionInputBuffer.fillBuffer(AbstractSessionInputBuffer.java:160)\n   [junit4]    > \tat org.apache.http.impl.io.SocketInputBuffer.fillBuffer(SocketInputBuffer.java:84)\n   [junit4]    > \tat org.apache.http.impl.io.AbstractSessionInputBuffer.readLine(AbstractSessionInputBuffer.java:273)\n   [junit4]    > \tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:140)\n   [junit4]    > \tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:57)\n   [junit4]    > \tat org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:261)\n   [junit4]    > \tat org.apache.http.impl.AbstractHttpClientConnection.receiveResponseHeader(AbstractHttpClientConnection.java:283)\n   [junit4]    > \tat org.apache.http.impl.conn.DefaultClientConnection.receiveResponseHeader(DefaultClientConnection.java:251)\n   [junit4]    > \tat org.apache.http.impl.conn.ManagedClientConnectionImpl.receiveResponseHeader(ManagedClientConnectionImpl.java:197)\n   [junit4]    > \tat org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:272)\n   [junit4]    > \tat org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:124)\n   [junit4]    > \tat org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:685)\n   [junit4]    > \tat org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:487)\n   [junit4]    > \tat org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:882)\n   [junit4]    > \tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n   [junit4]    > \tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:107)\n   [junit4]    > \tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:482)\n   [junit4]    > \t... 35 more\n\n\n ",
            "id": "comment-15198400"
        },
        {
            "date": "2016-03-17T19:18:55+0000",
            "author": "Hoss Man",
            "content": "Ok, so here's what i've found so far...\n\n\n\tJust adding a single line of logging to my test after configureCluster and before cluster.createCollection was enough to make the seed start passing fairly reliably.\n\t\n\t\tso clearly a finicky timing problem\n\t\n\t\n\tMiniSolrCloudCluster's constructor has logic that waits for /live_nodes have numServer children before returning\n\t\n\t\tthis was added in SOLR-7146 precisely because of problems like the one i'm seeing\n\t\tif there aren't the expected number of /live_nodes the first time it checks, then it sleeps in 1 second increments until there are.\n\t\n\t\n\t/live_nodes get's populated by ZkController.createEphemeralLiveNode\n\t\n\t\tTHIS METHOD IS SUSPICIOUSLY CALLED IN TWO DIFF PLACES:\n\t\t\n\t\t\tEDIT: this is actualy part of an OnReconnect handler that I misconstrued as something that would be called on the initial connect. fairly early in the ZkController constructor...\n\n// we have to register as live first to pick up docs in the buffer\ncreateEphemeralLiveNode();\n\n\n\t\t\tagain as the very last thing in ZkControlle.init...\n\n// Do this last to signal we're up.\ncreateEphemeralLiveNode();\n\n\n...this line+comment added in recently in SOLR-8696 when it replaced another previously existing call to createEphemeralLiveNode that was earlier in the init method (see https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;a=commitdiff;h=8ac4fdd;hp=7d32456efa4ade0130c3ed0ae677aa47b29355a9 )\n\t\t\n\t\t\n\t\n\t\n\tEven if /live_nodes were only populated as the very last line in ZkController.init, that's far from the last thing that happens when a solr node starts up. Things that happen after ZkController is initialized but before CoreContainer.createAndLoad returns and the SolrDispatchFilter starts accepting requests:\n\t\n\t\tZkContainer.initZooKeeper...\n\t\t\n\t\t\twhatever the hell this is suppose to do...\n\nif (zkRun != null && zkServer.getServers().size() > 1 && confDir == null && boostrapConf == false) {\n  // we are part of an ensemble and we are not uploading the config - pause to give the config time\n  // to get up\n  Thread.sleep(10000);\n}\n\n\n\t\t\tany node that has a confDir uploads it to zk: configManager.uploadConfigDir(configPath, confName); (even if it's not bootstrapping???)\n\t\t\tany node that IS doing bootstrap does that: ZkController.bootstrapConf(zkController.getZkClient(), cc, solrHome);\n\t\t\n\t\t\n\t\tCoreContainer.load()...\n\t\t\n\t\t\tAuthentication plugins are initialized\n\t\t\tcore * collection & configset & container handlers are initialized\n\t\t\tCoreDescriptor FOR EACH CORE DIR ON DISK ARE LOADED\n\t\t\t\n\t\t\t\twhich of course means opening transaction logs, opening indexwriters, open searchers, newSearcher event listeners, etc...\n\t\t\t\n\t\t\t\n\t\t\tZkController.checkOverseerDesignate() is called (no idea what that does)\n\t\t\n\t\t\n\t\n\t\n\n\n\n\nWhich all leads me to the following conclusions...\n\n\n\twhen using MiniSolrCloudCluster, if you are lucky, there will be at least one node not yet in /live_nodes} when it does it's first check, and then it will sleep 1 second giving those nodes time to actually startup & load their cores, and hopefully at least one of them will be completley finished by the time you actaully try to use a {{CloudSolrClient pointed at that ZK /live_nodes data.\n\tunless there is some other \"i'm alive\" data in ZK that MiniSolrCloudCluster should be consulting, it seems like it's doing the best it can to ensure that all the nodes are live before returning to the caller\n\tThis does not seem like a probably that only affects tests.  This seems like a real world problem we shoudl address \u2013 CloudSolrClient should be able to consult some info in ZK to know when a node is really alive and ready for requests.\n\t\n\t\tif there is a reason why the /live_nodes entry needs to be created as early as it is (ie: // we have to register as live first to pick up docs in the buffer) then it should only be created that one time and some other ephemeral node should be used\n\t\twhatever ephemeral node is used should be created by a very explicit very special method call made as the very last thing in SolrDispatchFilter\n\t\n\t\n\n ",
            "id": "comment-15200194"
        },
        {
            "date": "2016-03-17T19:46:09+0000",
            "author": "Noble Paul",
            "content": "ZkController.checkOverseerDesignate() is called (no idea what that does)\n\nI probably should add a comment there. If an overseer designate is down and comes back up, it should be pushed ahead of non designates . So it sends a message to overseer to put it in the front of the overseer election queue ",
            "id": "comment-15200233"
        },
        {
            "date": "2016-03-17T21:38:30+0000",
            "author": "Scott Blum",
            "content": "I think I can comment on this just a bit.  The first call to createEphemeralLiveNode() is not actually called from the constructor; it's called from the OnReconnect handler much later, if you lose your ZK session and have to create a new one.  At least, that's the theory.  Are you seeing it actually get called early?\n\nMore generally, the important race being resolved here is the call to registerAllCoresAsDown() happening before createEphemeralLiveNode().  Any client is supposed to join the cluster state (ie, is a core marked ACTIVE) with the live_nodes list.  So the idea is, mark everything as DOWN, then put in the live_nodes child, then go mark things ACTIVE as they actually come up.  This works reasonably well for things like routing search requests.  I can see how it might fall over if you're depending on live_nodes for doing cluster level operations. ",
            "id": "comment-15200439"
        },
        {
            "date": "2016-03-17T22:00:10+0000",
            "author": "Hoss Man",
            "content": "The first call to createEphemeralLiveNode() is not actually called from the constructor; it's called from the OnReconnect handler much later, if you lose your ZK session and have to create a new one. At least, that's the theory. Are you seeing it actually get called early?\n\nAh ... ok ... i'm probably wrong then \u2013 i thought the \"OnReconnect\" handler was also used on the initial connect as well.\n\nI'll edit my other comment to reduce confusion\n\nThis works reasonably well for things like routing search requests. I can see how it might fall over if you're depending on live_nodes for doing cluster level operations.\n\nthat's my concern \u2013 CloudSolrClient consults /live_nodes (via ClusterState.getLiveNodes()) to decide which nodes are up for any requests that aren't explicitly routable updates \u2013 in my particular case i'm getting burned by collection API calls...\n\nI guess I see your point though ... for any request involving specific collection(s) clients can use the replica state to see if they are ACTIVE (or if they are a LEADER for update situations) .. and CloudsolrClient does that even for searchers.  So I guess the \"practical\" impacts of this aren't as severe as i initially thought ... \n\nbut I still feel like we need something per-node in ZK that isn't set to  \"true\" until that node is actually listening on it's port.  ",
            "id": "comment-15200470"
        },
        {
            "date": "2016-03-20T03:27:34+0000",
            "author": "David Smiley",
            "content": "I hope this can get improved/resolved.  I didn't chase it down as far but I too had frustrations developing a test using MiniSolrCloudCluster that simply wanted the collection to be searchable (in SOLR-5750). ",
            "id": "comment-15203069"
        },
        {
            "date": "2016-03-21T00:43:33+0000",
            "author": "ASF subversion and git services",
            "content": "Commit aeda8dc4ae881c4ec405d70dcbf1d0b2c30871b7 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=aeda8dc ]\n\nSOLR-445: fix test bugs, and put in a stupid work around for SOLR-8862 ",
            "id": "comment-15203595"
        },
        {
            "date": "2016-03-21T22:03:07+0000",
            "author": "Alan Woodward",
            "content": "I've tried to dig a bit and see when everything here is run within the Jetty lifecycle, and it turns out that... it's complicated!\n\n\tIn a normal Solr setup, running using the Jetty start.jar, the SolrDispatchFilter is instantiated during startup (Jetty instantiates its Filters, and then its Servlets), and it won't serve any requests until all filters and servlets are fully constructed and have finished initialising.  So there could be a significant gap between registering the live_nodes znode and requests actually being served, particularly if there are other servlets within the container that take their time in starting up.\n\tIn JettySolrRunner, the SDF is instantiated within a jetty LifecycleListener (of which more below), which is called after Jetty has started listening on its port.  Requests won't be served via the filter until it has finished instantiating, but the gap here is smaller.\n\n\n\nIn both cases we have a race.  Ideally, we want to instatiate the filters, and only register ourselves with the cluster once we know we're serving requests, so we need a way to be notified that everything is ready to go:\n\n\tThe standard servlet API exposes ServletContextListeners, but these only get called before startup and shutdown, so these aren't any use.  We need to be notified after startup.\n\tJetty allows you to register LifecycleListeners that get called before and after startup and shutdown, which is exactly what we want.  Hurrah!\n\n\n\nSo what we really need to do here is to separate out CoreContainer construction, loading of cores, and creation of the live_nodes znode.  The container should be constructed and load up during server startup, and then register itself in a LifecycleListener.\n\nIt's not ideal that we have two different code paths here, one for 'proper' solr running using start.jar and xml configuration, and one programmatically, but I guess we can live with that for a while.\n\nOn a separate note, SOLR-8323 should help with waiting for collections to be searchable. ",
            "id": "comment-15205251"
        },
        {
            "date": "2016-03-22T16:40:53+0000",
            "author": "ASF subversion and git services",
            "content": "Commit b6be74f2182c46a10f861556ea81d3ed1a79a308 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b6be74f ]\n\nSOLR-8862 work around.  Maybe something like this should be promoted into MiniSolrCloudCluster's start() method? or SolrCloudTestCase's configureCluster? ",
            "id": "comment-15206745"
        }
    ]
}