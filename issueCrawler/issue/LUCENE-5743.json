{
    "id": "LUCENE-5743",
    "title": "new 4.9 norms format",
    "details": {
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed",
        "components": [],
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [
            "4.9",
            "6.0"
        ]
    },
    "description": "Norms can eat up a lot of RAM, since by default its 8 bits per field per document. We rely upon users to omit them to not blow up RAM, but its a constant trap.\n\nPreviously in 4.2, I tried to compress these by default, but it was too slow. My mistakes were:\n\n\tallowing slow bits per value like bpv=5 that are implemented with expensive operations.\n\ttrying to wedge norms into the generalized docvalues numeric case\n\tnot handling \"simple\" degraded cases like \"constant norm\" the same norm value for every document.\n\n\n\nInstead, we can just have a separate norms format that is very careful about what it does, since we understand in general the patterns in the data:\n\n\tuses CONSTANT compression (just writes the single value to metadata) when all values are the same.\n\tonly compresses to bitsPerValue = 1,2,4 (this also happens often, for very short text fields like person names and other stuff in structured data)\n\totherwise, if you would need 5,6,7,8 bits per value, we just continue to do what we do today, encode as byte[]. Maybe we can improve this later, but this ensures we don't have a performance impact.",
    "attachments": {
        "LUCENE-5743.patch": "https://issues.apache.org/jira/secure/attachment/12648814/LUCENE-5743.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14020815",
            "author": "Robert Muir",
            "content": "Patch.\n\nAs a simple test, I indexed geonames (its 8M documents):\n\nTrunk: 158,279,213 bytes RAM\nPatch:  36,446,880 bytes RAM ",
            "date": "2014-06-07T14:07:49+0000"
        },
        {
            "id": "comment-14021333",
            "author": "Adrien Grand",
            "content": "+1 ",
            "date": "2014-06-08T21:04:40+0000"
        },
        {
            "id": "comment-14021341",
            "author": "Adrien Grand",
            "content": "I'm wondering if we could have another format that would handle the case when there is a long tail of rare norm values. Eg. if there are 100 unique values but 95% of documents that have only 3 unique values: we could store norm values for these 95% documents using TABLE_COMPRESSED (2 bits per value including 1 special value saying that the norm is not there) and the other ones on disk? ",
            "date": "2014-06-08T21:16:21+0000"
        },
        {
            "id": "comment-14021403",
            "author": "Robert Muir",
            "content": "Adrien, its a good idea, basically a generalization  of the sparse case.  I wanted to tackle this, but decided against it here, the idea is to just improve lucenes defaults. This patch handles sparsity to some extent via low bPV and constant compression. Nothing sophisticated but I think effective enough as a step. ",
            "date": "2014-06-08T21:47:52+0000"
        },
        {
            "id": "comment-14021468",
            "author": "Michael McCandless",
            "content": "+1 ",
            "date": "2014-06-08T22:17:18+0000"
        },
        {
            "id": "comment-14025394",
            "author": "Ryan Ernst",
            "content": "This looks great!\n\nOne concern: the uniqueValues.toArray() call doesn't guarantee any order right? It doesn't look like it matters for correctness, but I would expect idempotence from the format, at least for reproducibility of tests. ",
            "date": "2014-06-09T17:26:56+0000"
        },
        {
            "id": "comment-14025449",
            "author": "Robert Muir",
            "content": "Its not a property we guarantee (e.g. SegmentInfo.files() set, FieldInfos.attributes(), various other places in the index write unordered sets where it does not matter), but we can add an Arrays.sort, this array is always <= 256 elements. ",
            "date": "2014-06-09T17:56:41+0000"
        },
        {
            "id": "comment-14026350",
            "author": "ASF subversion and git services",
            "content": "Commit 1601606 from Robert Muir in branch 'dev/trunk'\n[ https://svn.apache.org/r1601606 ]\n\nLUCENE-5743: Add Lucene49NormsFormat ",
            "date": "2014-06-10T11:35:49+0000"
        },
        {
            "id": "comment-14026400",
            "author": "ASF subversion and git services",
            "content": "Commit 1601625 from Robert Muir in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1601625 ]\n\nLUCENE-5743: Add Lucene49NormsFormat ",
            "date": "2014-06-10T12:48:05+0000"
        },
        {
            "id": "comment-14026404",
            "author": "Robert Muir",
            "content": "I added the Arrays.sort(), also a step towards a BaseNormsFormatTestCase. I've always been concerned that we didnt have enough stuff testing the norms directly...   ",
            "date": "2014-06-10T12:49:33+0000"
        }
    ]
}