{
    "id": "SOLR-4619",
    "title": "Improve PreAnalyzedField query analysis",
    "details": {
        "affect_versions": "4.0,                                            4.1,                                            4.2,                                            4.2.1,                                            6.0",
        "status": "Closed",
        "fix_versions": [
            "5.5",
            "6.0"
        ],
        "components": [
            "Schema and Analysis"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "PreAnalyzed field extends plain FieldType and mistakenly uses the DefaultAnalyzer as query analyzer, and doesn't allow for customization via <analyzer> schema elements.\n\nInstead it should extend TextField and support all query analysis supported by that type.",
    "attachments": {
        "SOLR-4619.patch": "https://issues.apache.org/jira/secure/attachment/12574600/SOLR-4619.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13608048",
            "date": "2013-03-20T19:20:07+0000",
            "content": "Comments by John Berryman, copied from SOLR-1535:\n\n\nAh, I see. This is a bit lower level than I was thinking. Still useful, but different. I was thinking about having PreAnalyzedField extend directly from TextField rather than from FieldType, and then be able to build up whatever analysis chain that you want in the usual TextField sense. Query analysis would proceed as with a normal TextField, but index analysis would smart detect whether this input was already parsed or not. If the input was not parsed, then it would go through the normal analysis. On the other hand, if the input was already parsed, then the token stream would go straight into the index (the assumption being that someone upstream understands what they're doing).\n\nThis way, in the SolrJ client you could build up some extra functionality so that the PreAnalyzedTextFields would be parsed client side and sent to Solr. In my current application, we have one Solr and N-indexers on different machines. The setup described here would take a big load off of Solr. The other benefit of this setup is that query analysis proceeds as it always does. I don't understand how someone would search over a PreAnalyzed field as it currently stands, without a bit of extra work/custom code on the client.\n\nOne pitfall to my idea is that you'd have to create a similar PreAnalyzedIntField, PreAnalyzedLocationField, PreAnalyzedDateField etc. I wish Java had mixins or multiple inheritance. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13608055",
            "date": "2013-03-20T19:23:40+0000",
            "content": "having PreAnalyzedField extend directly from TextField rather than from FieldType\n\nDone in the patch above. This means you can specify query analyzers.\n\nindex analysis would smart detect whether this input was already parsed or not\n\nWell, this should be possible with the pluggable parsers that this field supports. Since the formats can be vastly different, depending on the parserImpl, I'm not sure if we could come up with a generic detection mechanism... "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13608062",
            "date": "2013-03-20T19:33:46+0000",
            "content": "FYI, Andrzej, I see you have set 4.2.1 as the fix version - Mark Miller said on #lucene earlier today about cutting a 4.2.1 release: \"i may go tonight or tomorrow depending\". "
        },
        {
            "author": "John Berryman",
            "id": "comment-13608093",
            "date": "2013-03-20T19:58:18+0000",
            "content": "Wow... thanks for being so proactive here. I'll need to look at this in more detail later. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13608191",
            "date": "2013-03-20T20:55:45+0000",
            "content": "Removing 4.2.1 from Fix version - this apparently needs more discussion. "
        },
        {
            "author": "David Smiley",
            "id": "comment-13609012",
            "date": "2013-03-21T15:02:23+0000",
            "content": "Instead of having a special pre-analyzed field type, I think it makes more sense to introduce smarts into Solr's DocumentBuilder that sees a pre-analyzed field type value (e.g. a \"Field\" or some TokenStream of some sort) and instead of calling createField() on the field type, it simply populates the document.  There are details to be worked out for sure, but I think this is superior to having a special field type that is pre-analyzed when other field types are not.  Arguably all fields should be able to be pre-analyzed. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13610058",
            "date": "2013-03-22T09:13:12+0000",
            "content": "David: hmm, I'm not sure about that ... this would complicate the processing of all field types in order to support a use case that is very specific. If you throw in the ability to support different serializations then the detection of whether a field content is in a pre-analyzed format or not is not that simple anymore. Do you have a use case in mind that would require switching on the fly between the regular and pre-analyzed formats?\n\nJohn: please test the patch that I attached. If this is what you had in mind then I'd like to commit it soon, as it's a clear improvement over the current functionality. "
        },
        {
            "author": "David Smiley",
            "id": "comment-13611191",
            "date": "2013-03-22T20:28:22+0000",
            "content": "I guess we see this very differently.  I'm not arguing for any \"switching on the fly\" of anything, even if incidentally what I describe makes that possible.  Your design couples the ability to pre-analyze with the choice of field type, and I think that coupling need not exist.  It's already creating problems like this very issue (SOLR-4619).  Query analysis wouldn't need any improving, it shouldn't have anything to do with wether the field data was analyzed for indexing before it got to Solr or not.\n\nthis would complicate the processing of all field types\n\nSure, but it's not much.  I've already been poring over DocumentBuilder (as part of SOLR-4329) and I think I know what's involved.  It needs to examine the field value: if it's \"Field\" then it gets added right to the document.  A Solr UpdateRequestProcessor could convert the external serialization of the token stream from JSON/Avro/whatever to Field. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13611435",
            "date": "2013-03-22T23:41:59+0000",
            "content": "I like David Smileys generalized approach. Could you perhaps throw up a patch for discussion? "
        },
        {
            "author": "David Smiley",
            "id": "comment-13611625",
            "date": "2013-03-23T05:17:31+0000",
            "content": "Code matters more than words, for sure.  I already have more work than I can handle at the moment and I'm sorry I can't contribute the implementation I describe right now. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13611660",
            "date": "2013-03-23T08:11:29+0000",
            "content": "David, thanks for explaining - I see your point, and I like it too.\n\nIt needs to examine the field value: if it's \"Field\" then it gets added right to the document.\n\nEven if we push this functionality to an UpdateRequestProcessor it still needs to know when (not) to convert the input String to a Field. Do you have some thoughts about this?\n\nIn UpdateRequestProcessor we deal with SolrInputDocument-s in the context of available schema and solrconfig - so there are a few options how to determine the need for conversion.\n\n\n\tIt could be based on the document itself (tricky, in the light of multiple serialization formats).\n\tOR, we could extend SolrInputField (and the document serializations) to support additional per-field flags to indicate this, but that would complicate matters even more.\n\tOR, if this decision is going to be based on schema we would have to extend schema to pass additional flags to mark fields as preanalyzed - also tricky.\n\tAnd finally, we could put the list of fields to always convert in the init args of this UpdateRequestProcessor in solrconfig ... but that's a bit ugly, mixing schema and solrconfig.\n\n "
        },
        {
            "author": "David Smiley",
            "id": "comment-13611964",
            "date": "2013-03-24T04:37:11+0000",
            "content": "1. It could be based on the document itself (tricky, in the light of multiple serialization formats).\n\nI don't understand.\n\n2. OR, we could extend SolrInputField (and the document serializations) to support additional per-field flags to indicate this, but that would complicate matters even more.\n\nIf it were generalized, i.e. per-field map of metadata, then I rather like it.  Though it'd take a fair amount of work I think to fully realize this (e.g. update SolrJ & XML & JSON input formats), and it might also make URP's into more of a full-fledged pipeline but I think such things are better done external to Solr.\n\n3. OR, if this decision is going to be based on schema we would have to extend schema to pass additional flags to mark fields as preanalyzed - also tricky.\n\nI don't like this, as it ties the choice of pre-analysis to the schema which I think is unnecessary coupling just as it is to the field type.\n\n4. And finally, we could put the list of fields to always convert in the init args of this UpdateRequestProcessor in solrconfig ... but that's a bit ugly, mixing schema and solrconfig.\n\nI don't see it as \"mixing schema\" unless you simply mean to say that fields are referred to outside of the schema.  But heck, that's inevitable as fields are already referred to all over solrconfig.xml.  It's unrealistic to expect the names of fields in one's schema to not exist outside of schema.xml \u2013 the app needs to know too \n\nOne option would be to pass in a pseudo field _pre-analyzed_ (leading and trailing underscore) with a list of field names that are pre-analyzed.  The sender of the data is certainly aware of which fields are pre-analyzed as it had to pre-analyze them, so it can simply communicate that. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13612533",
            "date": "2013-03-25T10:12:49+0000",
            "content": "So it looks to me like the least controversial option is to put the list of preanalyzed fields in solrconfig in the specification of the URP. The trick with a \"magic\" field name sounds useful too - it would allow overriding the list of fields on a per-document basis. This could be also achieved by passing the list of fields via SolrParams - although it would affect all documents in a given update request.\n\nAnyway, I think these are good ideas worth trying. I'll start working on a patch. Thanks for the comments! "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13615508",
            "date": "2013-03-27T17:15:22+0000",
            "content": "I came to conclusion that this is really a different issue, worth pursuing on its own, so I created SOLR-4648. The improvements in analysis that I mentioned in the original description, and the patch, should be applied regardless of SOLR-4648. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-15097417",
            "date": "2016-01-14T01:31:16+0000",
            "content": "Patch that brings Andrzej's patch up to date with trunk, and adds tests for query-time functionality.\n\nI had assumed that PreAnalyzedField-s would use the PreAnalyzedTokenizer at query time, but that is not (currently) the case: instead FieldType.DefaultAnalyzer is used.  This patch changes the behavior when no analyzer is specified to instead use PreAnalyzedTokenizer.\n\nHowever, there is a chicken-and-egg interaction between PreAnalyzedTokenizer and QueryBuilder.createFieldQuery(), which aborts before performing any tokenization if the supplied analyzer's attribute factory doesn't contain a TermToBytesRefAttribute.  But PreAnalyzedTokenizer doesn't have any attributes defined until the input stream is consumed, in reset(). Robert Muir added a comment as part of LUCENE-5388 to PreAnalyzedTokenizer's ctor, where AttributeFactory.DEFAULT_ATTRIBUTE_FACTORY is set as the attribute factory rather than the default packed implementation: \"we don't pack attributes: since we are used for (de)serialization and dont want bloat.\"\n\nThis patch moves the stream.reset() call in QueryBuilder.createFieldQuery() in front of the TermToBytesRefAttribute check, so that PreAnalyzedTokenizer (and other tokenizers that don't have a pre-added set of attributes) has a chance to populate its attributes, and also moves the addAttribute(PositionIncrementAttribute.class) call to after the TermToBytesRefAttribute check, since that won't be needed if no tokenization will be performed.\n\nAn alternate approach to fix the chicken-and-egg problem might be to have PreAnalyzedTokenizer always include a dummy TermToBytesRefAttribute implementation, and then remove it when reset() is called, but that seems hackish.\n\nI haven't run the full tests yet with this patch, but the included query-time PreAnalyzedField tests succeed.\n\nI welcome feedback. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-15097440",
            "date": "2016-01-14T01:48:35+0000",
            "content": "\n But PreAnalyzedTokenizer doesn't have any attributes defined until the input stream is consumed, in reset()\n\nRight, thats a bug really. According to TokenStream's class javadocs:\n\n\n The workflow of the new TokenStream API is as follows:\n\n 1.  Instantiation of TokenStream/TokenFilters which add/get attributes to/from the AttributeSource.\n 2.  The consumer calls reset().\n 3.  The consumer retrieves attributes from the stream and stores local references to all attributes it wants to access. \n\nSo we have consumers (such as QueryBuilder) doing stuff out of order: if they do step 3 before they do step 2.\n\nMy question is, can we detect this in tests? If MockAnalyzer can enforce it, it is easier to fix it consistently everywhere. One idea is if MockTokenizer deferred initializing its attributes until reset()? Its not going to be the best (we need to tie it into its state machine logic somehow for that), but it might be an easy step. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-15097446",
            "date": "2016-01-14T01:53:57+0000",
            "content": "Also, majority of TokenFilters (which basically also serve as consumers too), are doing step 3 before step 2 today. Most of them are just assigning to final variables in their constructor.\n\nSo something is off: we gotta go one of two ways. Either fix the documentation to swap step 3 before step 2, and fix this TokenStream to somehow provide attributes before reset(), or we make a massive change to tons of tokenizers (making them more complex and less efficient).\n\nBut I think we have to do something, at least we should fix the docs to be clear, they need to reflect reality. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-15098237",
            "date": "2016-01-14T15:15:33+0000",
            "content": "Massive change doesn't seem warranted.\n\n\nBut PreAnalyzedTokenizer doesn't have any attributes defined until the input stream is consumed, in reset()\nRight, thats a bug really.\nfix this TokenStream to somehow provide attributes before reset()\n\nSince the input reader must be consumed before the attributes can be provided, the tokenizer must somehow have access to the input reader prior to reset().  The most likely place is setReader(), but Tokenizer.setReader() is final.\n\nA new analyzer class employing PreAnalyzedTokenizer could override initReader() or setReader().  I'll try with setReader(), since the docs for initReader() are focused on reader conditioning via char filters. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-15098245",
            "date": "2016-01-14T15:23:38+0000",
            "content": "A new analyzer class employing PreAnalyzedTokenizer could override initReader() or setReader(). I'll try with setReader(), since the docs for initReader() are focused on reader conditioning via char filters.\n\nI was referring to TokenStreamComponents.setReader() here, which is called as part of Analyzer.tokenStream():  A subclass created in the new analyzer's overridden createComponents() could call a new method on PreAnalyzedTokenizer to consume the input reader and in so doing provide the attributes. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-15107788",
            "date": "2016-01-20T01:20:47+0000",
            "content": "\nA new analyzer class employing PreAnalyzedTokenizer could override initReader() or setReader(). I'll try with setReader(), since the docs for initReader() are focused on reader conditioning via char filters.\n\nI was referring to TokenStreamComponents.setReader() here, which is called as part of Analyzer.tokenStream(): A subclass created in the new analyzer's overridden createComponents() could call a new method on PreAnalyzedTokenizer to consume the input reader and in so doing provide the attributes.\n\nPatch implementing the idea, splitting reader consumption out from reset() into its own method: decodeInput().  This method first removes all attributes from PreAnalyzedTokenizer's AttributeSource, then adds needed ones as a side effect of parsing the input.\n\nThere is a kludge here: because TokenStreamComponents.setReader() doesn't throw an exception, PreAnalyzedAnalyzer overrides createComponents() to create a TokenStreamComponents instance that catches and stores exceptions encountered during reader consumption with the stream's PreAnalyzedTokenizer instance, whose reset() method will then throw the stored exception, if any.\n\nWith this patch, PreAnalyzedAnalyzer can be reused; previously PreAnalyzedTokenizer reuse would ignore new input and re-emit tokens deserialized from the initial input.\n\nWith this patch, PreAnalyzedField analysis works like this: \n\n\tIf a query analyzer is specified in the schema then it will be used at query time.\n\tIf an analyzer is specified in the schema with no type (i.e., it is neither of \"index\" nor \"query\" type), then this analyzer will be used for query parsing, but will be ignored at index time.\n\tIf only an analyzer of \"index\" type is specified in the schema, then this analyzer will be used for query parsing, but will be ignored at index time.\n\n\n\nThis patch adds a new method removeAllAttributes() to AttributeSource, to support reuse of token streams with variable attributes, like PreAnalyzedTokenizer.\n\nI think it's ready to go. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-15107809",
            "date": "2016-01-20T01:48:39+0000",
            "content": "Updated patch, fixes PreAnalyzedFieldTest.testInvalidJson() to properly initialize its PreAnalyzedField, and to call reset() on the token streams created with the invalid JSON snippets, so that the exceptions triggered by the invalid JSON and stored during token stream creation are appropriately thrown.  Also tests that PreAnalyzedAnalyzer can be reused with valid input after having been fed invalid input.  "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-15108838",
            "date": "2016-01-20T16:27:21+0000",
            "content": "I plan on committing this later today if there are no objections. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15110110",
            "date": "2016-01-21T05:51:37+0000",
            "content": "Commit 1725869 from Steve Rowe in branch 'dev/trunk'\n[ https://svn.apache.org/r1725869 ]\n\nSOLR-4619: Improve PreAnalyzedField query analysis "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15110121",
            "date": "2016-01-21T06:06:18+0000",
            "content": "Commit 1725871 from Steve Rowe in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1725871 ]\n\nSOLR-4619: Improve PreAnalyzedField query analysis (merged trunk r1725869) "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-15110139",
            "date": "2016-01-21T06:25:44+0000",
            "content": "Committed to trunk and branch_5x.\n\nI opened LUCENE-6987 to address the TokenStream workflow documentation. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15111111",
            "date": "2016-01-21T19:08:43+0000",
            "content": "Commit 1726069 from Steve Rowe in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1726069 ]\n\nSOLR-4619: Fix Java7-only javadoc warnings by removing \n{@link}\n to private static nested class PreAnalyzedTokenizer "
        }
    ]
}