{
    "id": "LUCENE-6968",
    "title": "LSH Filter",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [
            "modules/analysis"
        ],
        "labels": "",
        "fix_versions": [
            "6.2",
            "7.0"
        ],
        "priority": "Major",
        "status": "Closed",
        "type": "Improvement"
    },
    "description": "I'm planning to implement LSH. Which support query like this\n\nFind similar documents that have 0.8 or higher similar score with a given document. Similarity measurement can be cosine, jaccard, euclid..\nFor example. Given following corpus\n\n1. Solr is an open source search engine based on Lucene\n2. Solr is an open source enterprise search engine based on Lucene\n3. Solr is an popular open source enterprise search engine based on Lucene\n4. Apache Lucene is a high-performance, full-featured text search engine library written entirely in Java\nWe wanna find documents that have 0.6 score in jaccard measurement with this doc\n\nSolr is an open source search engine\nIt will return only docs 1,2 and 3 (MoreLikeThis will also return doc 4)",
    "attachments": {
        "LUCENE-6968.patch": "https://issues.apache.org/jira/secure/attachment/12781160/LUCENE-6968.patch",
        "LUCENE-6968.6.patch": "https://issues.apache.org/jira/secure/attachment/12809479/LUCENE-6968.6.patch",
        "LUCENE-6968.5.patch": "https://issues.apache.org/jira/secure/attachment/12802736/LUCENE-6968.5.patch",
        "LUCENE-6968.4.patch": "https://issues.apache.org/jira/secure/attachment/12801204/LUCENE-6968.4.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15088886",
            "author": "Cao Manh Dat",
            "date": "2016-01-08T08:29:59+0000",
            "content": "Initial patch. Can find similar documents based on Jaccard similarity. "
        },
        {
            "id": "comment-15091661",
            "author": "Cao Manh Dat",
            "date": "2016-01-11T09:24:19+0000",
            "content": "This new patch include :\n\n\tChange IntHash to MultiplyShiftHash (faster universal hashing)\n\tCosineHashFilter support finding similar documents based on cosine similarity.\n\tLSH#createSlowQuery + LSHSimilarity which support finding similar documents on arbitrary similarity and similar score results.\n\ndoc1 : \"Solr is an open source search engine based on Lucene\"\ndoc2 : \"Solr is an open source enterprise search engine based on Lucene\"\ndoc3 : \"Solr is an open source enterprise search engine based on Lucene\"\n\nWhen we search for similarity of doc1. The result will contain:\n\ndoc1 : score = 100\ndoc2 : score = 90\ndoc3 : score = 87\n\n "
        },
        {
            "id": "comment-15091901",
            "author": "Joel Bernstein",
            "date": "2016-01-11T13:36:36+0000",
            "content": "Cao Manh Dat, interestiing ticket!\n\nCan you describe some of the potential uses for LSH. For example will this be helpful in approximating K nearest neighbor? "
        },
        {
            "id": "comment-15093091",
            "author": "Cao Manh Dat",
            "date": "2016-01-12T01:35:01+0000",
            "content": "Yes, It kinda like finding K nearest neighbor. But there a different here:\n\n\tIn K nearest neighbor, we use K as parameter which decide how many nearest neighbor we wanna retrieve.\n\tIn LSH we use a radius as parameter. The radius define minimum distance between center doc and other docs.  (LSH is also far faster than K nearest neighbor)\n\n\n\nIn both case, the result will be rank by distance to the center doc. "
        },
        {
            "id": "comment-15147204",
            "author": "Tommaso Teofili",
            "date": "2016-02-15T10:52:17+0000",
            "content": "since this has also been mentioned in SOLR-7739 maybe the LSH filter can be used to create a Lucene nearest neighbour Classifier which uses such filter (instead of the existing k nearest neighbour one). "
        },
        {
            "id": "comment-15195272",
            "author": "Tommaso Teofili",
            "date": "2016-03-15T13:11:39+0000",
            "content": "I was having a second look at this patch, why is LSHSimilarity returning 1 in all its methods ? Is that intended or does it mean it still needs to be implemented? "
        },
        {
            "id": "comment-15252065",
            "author": "Tommaso Teofili",
            "date": "2016-04-21T15:35:02+0000",
            "content": "Cao Manh Dat, I'd like to commit this patch shortly, my only concern is around the similarity implementation, having all methods returning 1 doesn't seem correct to me, or am I missing something ? "
        },
        {
            "id": "comment-15252122",
            "author": "Joel Bernstein",
            "date": "2016-04-21T16:08:50+0000",
            "content": "HI, I believe there will be some work forthcoming from Alfresco on this ticket. We may want to hold off until those patches go up. Hopefully I will have an update on this soon.\n\nTommaso Teofili, if you feel strongly though that this is ready to commit, then go for it. We can always make adjustments after the committal as well. "
        },
        {
            "id": "comment-15252743",
            "author": "Andy Hind",
            "date": "2016-04-21T21:07:12+0000",
            "content": "Hi\n\nIt would be quite common to use min hashing after shingling. At this point the number of possible word combinations vs the size of the hash is important. With shingles of 5 words from 100,000 that is 10e25 combinations. Some naive processing of the ~500k  Enron emails (splitting on white space, case folding and 5 word shingles) gives ~52M  combinations. So a long hash would be better at 1.8e19. I have not yet looked at a larger corpus.\n\nThe LSH query is neat. However the logic can give banding where the last band is uneven. In the patch I think the last band would be dropped unless bands * rows in band  = # of hashes\n\nThe underlying state of the source filter may also be lost (if using shingling)\n\nI do not believe the similarity is required at all. I think you can get Jaccard distance using constant score queries and disabling coordination on the boolean query. \n\nI went for 128-bit hashes, or a 32 bit hash identifier + 96 bit hash with a bit more flexibility allowing a minimum set of hash values for a bunch of hashes. There is clearly some trade off for speed of hashing and over representing short documents. The minimum set may be a solution to this.  I think there is some interesting research there. \n\nI will add my patch inspired by the original .... and apologise for the mixed formatting in advance ...... "
        },
        {
            "id": "comment-15255434",
            "author": "Cao Manh Dat",
            "date": "2016-04-24T00:52:11+0000",
            "content": "What's a wonderful patch. The code is optimized, sure that the the index will be much smaller!\n\nBut the patch keep some lowest values for each position, did it affect the formula \n\n Pr(h(s1) = h(s2)) = Jaccard(s1,s2) \n "
        },
        {
            "id": "comment-15256114",
            "author": "Andy Hind",
            "date": "2016-04-25T09:09:05+0000",
            "content": "The argument here says it is pretty much the same.\n\n\nhttps://en.wikipedia.org/wiki/MinHash\n\n\n\nThe plan was to offer both options.\n\nWith respect to banding and finding docs related to some start document, the number of hashes may depend on the start document. \n\nLet's start with 5 word shingles, one hash and keep the minimum 100 hash values. For a five word document we get one hash. For a 100 word doc where all the shingles/words are the same we get one hash. For all different shingles we get 96 hashes.\n\nIf we have 100 different hashes and keep the lowest one all the above cases end up with 100 hashes.\n\nSo back to banding. With minimum sets, you need to look and see how many hashes you really got and then do the banding. Comparing a small documents/snippet (where we get 10 hashes in the fingerprint)with a much larger document (where we get 100 hashes) is an interesting case to consider. Starting with the small document there are fewer bits to match in the generated query. With 100 hashes from the small document I think you end up in the roughly same place, except for small snippets. Any given band is more likely to have the same shingle hashed different ways.\n\nWith a 100 hash fingerprint, sampling for 100 words is great but not so great for 100,000 words. With a minimum set we have the option to generate a finger print related to the document length and other features.\n\nThere is also an argument for a winnowing approach. \n "
        },
        {
            "id": "comment-15257732",
            "author": "Cao Manh Dat",
            "date": "2016-04-26T08:26:25+0000",
            "content": "Thanks for the link. I totally agree that keeping some lowest values for single hash function would be better. \n\nBut in the wiki doc. It pointed out that the estimator formulation for \"variant with a single hash function\" is not same as the estimator formulation for \"variant with many hash function\". So the generated query must be different for each case.\n\nFor example, in case we use single hash function and keep some lowest values :\n\n\tWe have doc A = [1, 2, 5, 6, 7], doc B = [3, 4, 5, 6, 7]\n\tSo jaccard(A,B) = size( hk(A U B) \u2229 hk(A) \u2229 hk(B) ) / k =  size( [5] ) / k = 0.2\n\n "
        },
        {
            "id": "comment-15259924",
            "author": "Andy Hind",
            "date": "2016-04-27T10:15:36+0000",
            "content": "This comes down to \"what is a good estimate of |A U B|\" and do we need it for the use case.\n\nFor query, the main use case is finding documents like one source document. So we are comparing Doc A with all other documents. What we need is a measure that is fair for A -> B, A -> C. We probably do not care about B -> C. If we take the fingerprint from A and just OR the bits together into a big query we have a consistent measure of similarity of A with any other document. This particular measure is biased. For a start Sim(A, B) is not equal to Sim(B, A). But for this use case that may not matter. This measure contains both inclusion and duplication which may be a good thing. It is also pretty intuitive what it means. This is (A \u2229 B)/|A|.\n\nIf we want Sim(A, B) = Sim(B, A) then we need some consistent measure/sample of |A U B| to normalise our measure/estimate of A \u2229 B. This could be (|A| + |B| - |A \u2229 B|), or some similar estimate. We could use the size of the fingerprint sets. We could keep the full ordered set of hashes and have extra statistics like the total number of hashes and total number of unique hashes. \n\nFor two short documents, where there are fewer fingerprints than the maximum, we have the full sets.\nFor two larger docs we have an estimate of these based in the min hash sets.  You can argue \"min of many hashes\"  is a random sample with replacement and \"min set of one hash\" is a random sample without replacement; if your hash is good. If the sample is small compared with the set of all hashes the arguments converge. \n\nIf we were doing arbitrary comparisons between any pair of documents then we would have to use an unbiased estimator. Finding candidate pairs, moving onto clustering, ... "
        },
        {
            "id": "comment-15261480",
            "author": "Cao Manh Dat",
            "date": "2016-04-28T03:35:08+0000",
            "content": "Thanks, that make sense! "
        },
        {
            "id": "comment-15261879",
            "author": "Tommaso Teofili",
            "date": "2016-04-28T09:41:04+0000",
            "content": "attaching a slightly modified version of the last patch:\n\n\tadded service loader binding for MinHashFilterFactory\n\tadded IntelliJ required dependencies\n\tminor fixes to javadoc (and code style to be consistent with rest of the codebase)\n\n\n\nI've noticed though that the filter doesn't perfectly align the end offset attribute (being beyond the input length), in fact if I run all tests the TestFactories one fails with the following:\n\nSuite: org.apache.lucene.analysis.core.TestFactories\n   [junit4]   2> TEST FAIL: useCharFilter=true text='uuzfmo'\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestFactories -Dtests.method=test -Dtests.seed=9CF9D39BDAB31A80 -Dtests.slow=true -Dtests.locale=sv -Dtests.timezone=Asia/Choibalsan -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] FAILURE 13.5s J3 | TestFactories.test <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: endOffset must be <= finalOffset: got endOffset=7 vs finalOffset=6\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([9CF9D39BDAB31A80:14ADEC41744F7778]:0)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:211)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:300)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:304)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:828)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:627)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:525)\n   [junit4]    > \tat org.apache.lucene.analysis.core.TestFactories.doTestTokenFilter(TestFactories.java:105)\n   [junit4]    > \tat org.apache.lucene.analysis.core.TestFactories.test(TestFactories.java:58)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]   2> NOTE: test params are: codec=Lucene60, sim=RandomSimilarity(queryNorm=true,coord=no): {}, locale=sv, timezone=Asia/Choibalsan\n   [junit4]   2> NOTE: Mac OS X 10.11.3 x86_64/Oracle Corporation 1.8.0_45 (64-bit)/cpus=8,threads=1,free=197816752,total=324534272\n   [junit4]   2> NOTE: All tests run in this JVM: [TestPatternCaptureGroupTokenFilter, TestSnowballPorterFilterFactory, TestBulgarianStemFilterFactory, TestFactories]\n\n "
        },
        {
            "id": "comment-15261955",
            "author": "Robert Muir",
            "date": "2016-04-28T10:57:58+0000",
            "content": "analysis-common library cannot have any external dependencies. \n\nso If this filter is going to depend on guava, it needs to be its own analysis module. Just like all other analysis modules that have any third party dependencies.\n\nI would try to remove the guava dependency at all costs anyway. It causes hell for developers to depend on guava. "
        },
        {
            "id": "comment-15261961",
            "author": "Robert Muir",
            "date": "2016-04-28T11:02:05+0000",
            "content": "also, these analyzers should not be tested with queries. Instead, just use the asserts in BaseTokenStreamTestCase to check that the correct tokens are output.\n\nI see some solr code was copied so that things can be tested with queries. I know that solr likes to test this way, but nobody wants to debug a test failure that tests an analyzer this way.\n\nJust keep in mind we have a ton of analyzers, and when things go wrong (and they do), people have to debug them or even do refactorings across hundreds of them. That is why we have a consistent test class (BaseTokenStreamTestCase). "
        },
        {
            "id": "comment-15261990",
            "author": "Tommaso Teofili",
            "date": "2016-04-28T11:23:22+0000",
            "content": "thanks Robert, I agree on all of your suggestions.\nThe error reported by the TestFactories witnesses what you say, I can work on those more appropriate tests; however I would also like to keep the \"query\" ones as to keep an eye on whether one of the use case for this filter is supported as expected (a sort of integration test).\nRemoving Guava might of course be possible (and I'd prefer to do that), although rewriting the hashcode stuff is likely to be a bit annoying. "
        },
        {
            "id": "comment-15261997",
            "author": "Robert Muir",
            "date": "2016-04-28T11:27:01+0000",
            "content": "We don't need any \"integration tests\" or \"end to end tests\".\n\n-1 to those. Just think about it. Tokenizers only output one thing, and that is tokens. And that is what we should test. "
        },
        {
            "id": "comment-15262169",
            "author": "Andy Hind",
            "date": "2016-04-28T13:55:41+0000",
            "content": "I agree a pure token stream test makes sense. The only concern I have is about testing token filters chained together. Chaining shingle generation with min hashing requires that the underlying token stream has its state reset correctly for reuse. As I missed this, I added a test to cover it. Is there somewhere else in the test framework that covers this case? Some randomised chaining of filters?? Perhaps chaining is more of a SOLR thing.\n\nI would prefer to stick with a 128/96 bit hash. The link below [1] \"suggests\" 5-shingles become well distributed. Link [2] says upto 2/3 of all possible trigrams have been seen in 30 years of news  articles. So it seems we can expect to see many of the possible 5-shingles. Some bioinformatic use cases may also require this.  \n\n\n[1] http://googleresearch.blogspot.co.uk/2006/08/all-our-n-gram-are-belong-to-you.html\n[2] http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf\n\nI was not that keen to add Guava! However, it was already there somewhere.\nI am happy if this moves off into a separate module. I will also look to see how this dependency could be removed.\n\nPerhaps we should have some time to consider how to include the fingerprint length (sum of the min set size over all hashes) to support an unbiased query. An unbiased query would be more difficult to build correctly. Some fingerprint/LSH query support and tests may make sense. Some other statistics may also be useful in generating faster queries that find similar documents using some threshold and probability of meeting that threshold.  "
        },
        {
            "id": "comment-15262918",
            "author": "Andy Hind",
            "date": "2016-04-28T20:26:35+0000",
            "content": "Yonik Seeley has murmurhash3_x64_128 here https://github.com/yonik/java_util/blob/master/src/util/hash/MurmurHash3.java "
        },
        {
            "id": "comment-15263867",
            "author": "Andy Hind",
            "date": "2016-04-29T10:17:59+0000",
            "content": "After a bit more digging, the single hash and keeping the minimum set can be improved.\n\nSee: \n[1] http://jmlr.org/proceedings/papers/v32/shrivastava14.pdf\n[2] http://www.auai.org/uai2014/proceedings/individuals/225.pdf\n\nIn summary: rather than keep the minimum set, split the hash space up into 500 buckets (for a 500 hash fingerprint) and keep the minimum for each bucket. To fill an empty bucket, take the minimum from the next non-empty bucket on the right with rotation.  "
        },
        {
            "id": "comment-15274697",
            "author": "Andy Hind",
            "date": "2016-05-06T20:42:55+0000",
            "content": "I have attached an updated patch.\n\nThis addresses the following issues\n\n\tSupport for single hash, split into ranges with a minimum for each range\n\tRemove end to end tests and beefed up unit tests\n\tRemove Guava in favour of Yonik's murmur hash implementation. (Some duplication here with SOLR)\n\tFixed alignment and \"evil\" test case issue\n\tTestFactories passes > 200 times (some Japanese Number tokenisation failures)\n\tFixed formatting\n\n\n\nThere were issue applying patch 4 on its own or over the previous patch. I believe I have included everything other then the IDE related bits. "
        },
        {
            "id": "comment-15275123",
            "author": "Tommaso Teofili",
            "date": "2016-05-07T06:34:35+0000",
            "content": "thanks a lot Andy, it generally looks much better, I will have a look at failures on point 5 (and eventually add the IDE fixes back). "
        },
        {
            "id": "comment-15324733",
            "author": "Tommaso Teofili",
            "date": "2016-06-10T16:22:27+0000",
            "content": "minor fixes to the last patch, apart from that I cannot see any error, even with Japanese locale explicitly set\n\nant clean test -Dtests.multiplier=1000000 -Dtestcase=MinHashFilterTest -Dtests.locale=ja\n\n\n\nAndy Hind do you have the seed of a failed execution ? "
        },
        {
            "id": "comment-15328054",
            "author": "Andy Hind",
            "date": "2016-06-13T19:49:42+0000",
            "content": "Hi Tommaso, the MinHashFilterTest was running fine. It was JapaneseNumberFilter that was failing intermittently. I think on one of the evil test cases.\n\nLongPair should implement equals (and probably hashCode if it will be reused) as it goes into a TreeSet. An over sight on my part.\n\nFWIW, as far as I can tell, the change in patch 6 was included in 5. "
        },
        {
            "id": "comment-15329205",
            "author": "ASF subversion and git services",
            "date": "2016-06-14T09:51:48+0000",
            "content": "Commit 82a9244193ba948142b834ec08e2de0d98cfba9f in lucene-solr's branch refs/heads/master from Tommaso Teofili\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=82a9244 ]\n\nLUCENE-6968 - MinHash filter, thanks to Andy Hind and Cao Manh Dat for patches "
        },
        {
            "id": "comment-15329206",
            "author": "Tommaso Teofili",
            "date": "2016-06-14T09:52:40+0000",
            "content": "I've committed this, thanks to Andy Hind and Cao Manh Dat for your patches! "
        },
        {
            "id": "comment-15330032",
            "author": "Andy Hind",
            "date": "2016-06-14T17:58:52+0000",
            "content": "Hi Tommaso - are you planning to merge this to 6.x? "
        },
        {
            "id": "comment-15331794",
            "author": "Tommaso Teofili",
            "date": "2016-06-15T14:02:34+0000",
            "content": "yes, I plan to merge it to 6.x, I wanted to have a few more runs on Jenkins before merging it back to make sure there're no token filtering level issues. "
        },
        {
            "id": "comment-15333646",
            "author": "ASF subversion and git services",
            "date": "2016-06-16T12:08:36+0000",
            "content": "Commit 82a9244193ba948142b834ec08e2de0d98cfba9f in lucene-solr's branch refs/heads/apiv2 from Tommaso Teofili\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=82a9244 ]\n\nLUCENE-6968 - MinHash filter, thanks to Andy Hind and Cao Manh Dat for patches "
        },
        {
            "id": "comment-15348023",
            "author": "ASF subversion and git services",
            "date": "2016-06-24T08:56:12+0000",
            "content": "Commit 96a5595fffd4a4a1c553a987382697cb8a92b354 in lucene-solr's branch refs/heads/branch_6x from Tommaso Teofili\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=96a5595 ]\n\nLUCENE-6968 - LSH Filter backported to branch 6.x "
        },
        {
            "id": "comment-15348025",
            "author": "Tommaso Teofili",
            "date": "2016-06-24T08:57:50+0000",
            "content": "I've backported this to branch 6.x for inclusion in next 6.2 release. "
        },
        {
            "id": "comment-15414895",
            "author": "Varun Thacker",
            "date": "2016-08-10T08:20:47+0000",
            "content": "Hi Tommaso,\n\nI think we need to fix the CHANGES entry to move the entry to the 6.2 section. Its under 7.0 section currently  "
        },
        {
            "id": "comment-15439037",
            "author": "Michael McCandless",
            "date": "2016-08-26T14:00:22+0000",
            "content": "Bulk close resolved issues after 6.2.0 release. "
        },
        {
            "id": "comment-15599783",
            "author": "Yonik Seeley",
            "date": "2016-10-23T14:34:55+0000",
            "content": "Is there a JIRA issue yet to expose (and test) this and MinHash in Solr? "
        },
        {
            "id": "comment-15601387",
            "author": "Tommaso Teofili",
            "date": "2016-10-24T08:53:49+0000",
            "content": "Yonik Seeley the MinHash filter can be already used in Solr just like any other TokenFilter\u00a0using its MinHashFilterFactory; regarding the test, there used to be one to test its end to end usage in one of the first patches, however it was decided to keep only the BaseTokenStreamTestCase ones as to check the filtering was working correctly. Perhaps we can put the old usage test in the context of Solr and add it within the Solr tests. "
        },
        {
            "id": "comment-15604803",
            "author": "Andy Hind",
            "date": "2016-10-25T09:41:57+0000",
            "content": "There are probably some more bits required to integrate minhash with SOLR but it depends on the use case.\nA SOLR specific ticket would make sense.\n\nFinding likely duplicates/clutering from the whole corpus LSH style I have not thought about.....\n\n\"Near duplicates of a document\" would make sense and overlaps a bit with MLT. First this requires storing the fingerprint, recovering it in a distributed way and building a query. It probably makes sense to support a big OR constant score query, a big OR constant score query with a minimum match, and a banded query that reflects the usual LSH stuff. An example was in the unit tests that was removed (it needed a bit of refinement to avoid a small bucket). Banding groups random fingerprints into AND queries and ORs these together. \n\nI can provide the query logic.\n\nFYI - adding a 500 term fingerprint increased the index size ~10%  "
        },
        {
            "id": "comment-16706571",
            "author": "Mayya Sharipova",
            "date": "2018-12-03T01:02:41+0000",
            "content": "Andy Hind\u00a0\u00a0Hello Andy! I have several questions about the implementation of the MinHashFilter, and was wondering if you\u00a0would be able to answer them. Thanks a lot in advance.\n\nThe implementation from 1st original patch where the minimum set is kept is very clear to me, and follows the classic\u00a0idea\u00a0of constructing MinHash signature and LSH search after it. But I am having a hard time understanding the final implementation for MinHashFilter.\n\n1) What constitutes the signature of a document? Are these all values stored in the hash table? Doesn't it make a signature too large? Can you please refer the paper that describes this way of\u00a0constructing minhash signatures.\n\n2) What is the use of withRotation parameter? What is the advantage of using withRotation=true? In the paper you cited: http://www.auai.org/uai2014/proceedings/individuals/225.pdf, they fill empty bins with \"value of the closest non-empty bin in the clockwise direction (circular right hand side) added with offset C\". In the MinHashFilter implementation values for empty buckets are just blindly copied from non-empty ones, so a lot of buckets with have the same value.\n\nHopefully the questions make sense. Thanks again in advance. "
        }
    ]
}