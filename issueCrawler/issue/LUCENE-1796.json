{
    "id": "LUCENE-1796",
    "title": "Speed up repeated TokenStream init",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [
            "2.9"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "by caching isMethodOverridden results",
    "attachments": {
        "after.png": "https://issues.apache.org/jira/secure/attachment/12416118/after.png",
        "LUCENE-1796.patch": "https://issues.apache.org/jira/secure/attachment/12416077/LUCENE-1796.patch",
        "afterAndLucene1796.png": "https://issues.apache.org/jira/secure/attachment/12416119/afterAndLucene1796.png",
        "before.png": "https://issues.apache.org/jira/secure/attachment/12416117/before.png"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-08-10T15:15:09+0000",
            "content": "I've got my test environ all setup, so I'll be happy to test ",
            "author": "Mark Miller",
            "id": "comment-12741360"
        },
        {
            "date": "2009-08-10T15:16:52+0000",
            "content": "me too (not a real benchmark but i think average sized docs) ",
            "author": "Robert Muir",
            "id": "comment-12741362"
        },
        {
            "date": "2009-08-10T15:35:38+0000",
            "content": "unrelated to TokenStream init, but what appears to be creating additional slowdown is this:\n\nTRACE 300197:\n\torg.apache.lucene.util.AttributeSource.getAttributeImplsIterator(AttributeSource.java:140)\n\torg.apache.lucene.util.AttributeSource.clearAttributes(AttributeSource.java:233)\n\torg.apache.lucene.analysis.CharTokenizer.incrementToken(CharTokenizer.java:56)\n\torg.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:189)\n\n\n\neven with reusableTokenStreams, I am still seeing a slowdown and with fairly granular profiling, this is consistently at the top.\nis there a way to optimize clear() in any way?\n ",
            "author": "Robert Muir",
            "id": "comment-12741373"
        },
        {
            "date": "2009-08-10T15:35:59+0000",
            "content": "Here a poatch. About the naming of the private internal class we can discuss, but it seems to work. No real performance tests until now.\n\nCould you please test and compare the results with before? ",
            "author": "Uwe Schindler",
            "id": "comment-12741374"
        },
        {
            "date": "2009-08-10T15:44:07+0000",
            "content": "is there a way to optimize clear() in any way?\n\nThe cost is creating the iterator. As it is a LinkedHashMap, there must be used an iterator (because direct access is slow), but for extra safety the iterator is made unmodifable, this wrapper around could be removed. ",
            "author": "Uwe Schindler",
            "id": "comment-12741382"
        },
        {
            "date": "2009-08-10T15:52:18+0000",
            "content": "New patch, that also removes the Collections.unmodifiableXxx() calls. If somebody removes elements from the iterator its not our problem  ",
            "author": "Uwe Schindler",
            "id": "comment-12741387"
        },
        {
            "date": "2009-08-10T15:57:30+0000",
            "content": "ok lemme restart my benchmark... the performance difference i was seeing with your first patch so far looked minor, but consistent.\n\nhere's some unscientific numbers from your first patch indexing hamshari (about 500MB corpus, 165k docs)\nanalyzer with no reusableTS() (without patch):\n52988 ms\n52902 ms\n53035 ms\n53116 ms\n52637 ms\n\nanalyzer with no reusableTS() (with patch):\n51916 ms\n51969 ms\n51872 ms\n52438 ms\n51710 ms ",
            "author": "Robert Muir",
            "id": "comment-12741391"
        },
        {
            "date": "2009-08-10T15:59:34+0000",
            "content": "Test on the first patch:\n\nAlmost brings things back to par with Yoniks short solr indexing test (previous results were like 90 some seconds):\n\n\nBefore TokenStream revamp:\niter=100000 time=44173 throughput=2263\niter=100000 time=44403 throughput=2252\n\nAfter TokenStream revamp:\niter=100000 time=46720 throughput=2140\niter=100000 time=47038 throughput=2125\n\n\nThat method inspection was like the second hotest method (see the profiling results), and this must just take it right out of there.\n\nBy the way, I havn't looked if it slows down the single use case at all or not. ",
            "author": "Mark Miller",
            "id": "comment-12741393"
        },
        {
            "date": "2009-08-10T16:03:09+0000",
            "content": "No noticeable diff in the second patch for me. ",
            "author": "Mark Miller",
            "id": "comment-12741395"
        },
        {
            "date": "2009-08-10T16:06:21+0000",
            "content": "here are my large doc numbers with the second patch, same setup, no reusableTS()\n\n51347ms\n49917ms\n50676ms\n50010ms\n49261ms\n\nseems to help a bit. i will turn back on reusableTS and see if the iterator still shows up in profiling. ",
            "author": "Robert Muir",
            "id": "comment-12741397"
        },
        {
            "date": "2009-08-10T16:09:02+0000",
            "content": "Mark: I do not understand your comment completely, do you mean with before/after revamp the comparison between old Lucene before-Attributes TokenStreams compared to the patched Attribute ones? And the 90 seconds is the unpatched Attributes case? (looks like half speed)\n\nIf so, the problem was really this reflection calls. ",
            "author": "Uwe Schindler",
            "id": "comment-12741400"
        },
        {
            "date": "2009-08-10T16:14:38+0000",
            "content": "Sorry - to be a bit more clear:\n\nLucene trunk without your patch was like 90 seconds - twice as slow as a previous version of Lucene on Yoniks short doc test.\n\nThe numbers reported above are for Lucene pre the TokenStream improvements, and post with your patch - so now the numbers are much closer - though its still a tad slower.\n\nYou second patch didnt change things for me from your first patch - same perf in this test.\n\nIf so, the problem was really this reflection calls.\n\nYes - which you can clearly see from the profiling results I got yesterday:\n\nhttp://myhardshadow.com/images/before.png\n\nhttp://myhardshadow.com/images/after.png\n\nisMethodOverriden was called way too often for its speed - its a highly inefficient call if you trace through it. ",
            "author": "Mark Miller",
            "id": "comment-12741402"
        },
        {
            "date": "2009-08-10T16:18:19+0000",
            "content": "If it is still a little bit slower (in my opinion its a little bit, 2 seconds of 45s is 4%), could it be, because the Solr TokenFilters only implement next(Token) and not incrementToken()? What is your distribution of TokenStreams/Filters in your test. Only new ones, only old ones, both? % ",
            "author": "Uwe Schindler",
            "id": "comment-12741404"
        },
        {
            "date": "2009-08-10T16:23:37+0000",
            "content": "Yes indeed, its very close now.\n\nThe filters are:\n\n          <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n          <filter class=\"solr.WordDelimiterFilterFactory\" generateWordParts=\"1\" generateNumberParts=\"0\" catenateWords=\"0\" catenateNumbers=\"0\" catenateAll=\"0\"/>\n          <filter class=\"solr.LowerCaseFilterFactory\"/>\n\nSo I guess its 1 and 1? Lowercase comes from Lucene and implements increment, but WordDelim just does next. The other hotspot was (though much less of one) TokenStream<init>(AttributeSource) - if implementing increment relieves that, it might be the same speed after. ",
            "author": "Mark Miller",
            "id": "comment-12741408"
        },
        {
            "date": "2009-08-10T16:24:28+0000",
            "content": "Uwe, your patch seems to help my large doc case, although as you can see, the numbers are still very different if i implement reusableTS() than if i do not.\n\nWith patch (reusableTS)\nTotal time:43373\nTotal time:43428\nTotal time:43536\nTotal time:42857\nTotal time:42835\n\nWithout patch (reusableTS)\nTotal time:44613\nTotal time:45720\nTotal time:45592\nTotal time:45445\nTotal time:45090\n\nAlso, with your patch the org.apache.lucene.util.AttributeSource.getAttributeImplsIterator from clearAttributes() is ranked 8 instead of 1 or 2 in profiling. ",
            "author": "Robert Muir",
            "id": "comment-12741411"
        },
        {
            "date": "2009-08-10T16:37:58+0000",
            "content": "Mark: The only hotspot could be initTokenWrapper(), which does some checks, but normally shortcuts in filters to the input TokenStream's TokenWrapper instance.\n\nUsing incrementToken() would not help here, only if you switch of using the old API complete using TokenStream.setOnlyUseNewAPI(true), which can only be done globally, so all Solr TokenStreams must implement incrementToken(). ",
            "author": "Uwe Schindler",
            "id": "comment-12741416"
        },
        {
            "date": "2009-08-10T17:07:17+0000",
            "content": "Attached is a patch that removes the clearAttributes() from CharTokenizer (see discussion with Yonik on java-dev) and also removes clear() calls where not really needed. ",
            "author": "Uwe Schindler",
            "id": "comment-12741426"
        },
        {
            "date": "2009-08-10T17:17:33+0000",
            "content": "Uwe, removal of the CharTokenizer clearAttributes() makes a difference for me (compare to my last numbers):\n\n38729ms\n40201ms\n40085ms\n40238ms\n40169ms ",
            "author": "Robert Muir",
            "id": "comment-12741430"
        },
        {
            "date": "2009-08-10T17:51:02+0000",
            "content": "Another good cache, Uwe! \n\nAttributeSource.clearAttributes() could use the State (which is also used for cloning) to iterate the AttributeImpls faster. ",
            "author": "Michael Busch",
            "id": "comment-12741444"
        },
        {
            "date": "2009-08-10T17:53:43+0000",
            "content": "But if you use the State and there is no state already created, it would have the cost of capturing the state (cloning!) for no real use... ",
            "author": "Uwe Schindler",
            "id": "comment-12741445"
        },
        {
            "date": "2009-08-10T17:59:09+0000",
            "content": "I have another idea:\nWhy not make the AttributeImpls itsself a linked list. So each AttributeImpl gets a member called \"nextAttributeImpl\" for the linked list. Whenever an AttributeImpl is added to an AttributeSource, the last added AttributeImpl's next is set to the added one. By this iterating over AttributeImpls is just a simple for-loop from the first attribute. The AttributeSource must hold a reference to the first and last Attribute, but only one-direction linkage from first to last.\n\nAs Attributes can only added to one AttributeSource and not to multiple ones, I see no problem with it. And AttributeImpls can also not removed, so no problem at all. ",
            "author": "Uwe Schindler",
            "id": "comment-12741447"
        },
        {
            "date": "2009-08-10T18:11:22+0000",
            "content": "You don't have to call captureState and clone. You just need to call computeCurrentState() one time to create the internal state. That is basically a linked list.\n\nIf then someone adds more attributes to the source the state is invalidated and you would have to call computeCurrentState() again. ",
            "author": "Michael Busch",
            "id": "comment-12741457"
        },
        {
            "date": "2009-08-10T18:14:23+0000",
            "content": "Ah, you are right! I will try this out. The Iterator returned by getAttributeImplsIterator() could then also be implemented using the State. This iterator would not be used internally by AttributeSource (there the State is iterated directly), but e.g. in TeeSinkTokenStream. So supplying an iterator is still needed, but internally the faster direct State iteration can be used.\n\nI will try this out and re-enable clearAttribute() in the Tokenizers again. ",
            "author": "Uwe Schindler",
            "id": "comment-12741459"
        },
        {
            "date": "2009-08-10T19:47:52+0000",
            "content": "New patch that optimizes the iteration over the AttributeImpls using the computed State linked list. This also adds the default buffer size to KeywordTokenizer, that got lost during the move to the new API.\n\nTo test performance, I reactivated the clearAttributes() call in CharTokenizer.\n\nIf this is now all ok, I would like to fix this issue as soon as possible to be able to do more perf testing with the optimized impls. The big hammer of isMethodOverridden is now removed and speed came back to the original one (with some small slowdown caused by mixing old and new TokenFilters together). ",
            "author": "Uwe Schindler",
            "id": "comment-12741511"
        },
        {
            "date": "2009-08-10T19:52:41+0000",
            "content": "Nice work Uwe! ",
            "author": "Mark Miller",
            "id": "comment-12741514"
        },
        {
            "date": "2009-08-10T19:58:53+0000",
            "content": "The latest patch appears to hurt the Solr use case a bit - went from 46-47 seconds to 51-52 (remember its 43-45 with the pre reflection stuff) ",
            "author": "Mark Miller",
            "id": "comment-12741520"
        },
        {
            "date": "2009-08-10T20:07:50+0000",
            "content": "Hm, and with the termAtt.clear() instead of clearAttributes? Was the 46-47 with the clearAttributes call or without? You always have the problem with very short TokenStreams that are not reused, that the initialization and State linked-list construction needs some time. In the reused case, it should be faster with the latest patch or without clearAttributes at all.\n\nWhere is the hotSpot? Do you have a figure like before the patch with method execution times? ",
            "author": "Uwe Schindler",
            "id": "comment-12741523"
        },
        {
            "date": "2009-08-10T20:08:58+0000",
            "content": "I was getting 46-47 with both of the first two patches. I can double check a little later though. ",
            "author": "Mark Miller",
            "id": "comment-12741524"
        },
        {
            "date": "2009-08-10T20:11:27+0000",
            "content": "uwe in my case the latest patch performs approx the same as your patch where CharTokenizer clearAttributes() was removed (avg 40893ms). Thanks. ",
            "author": "Robert Muir",
            "id": "comment-12741525"
        },
        {
            "date": "2009-08-10T20:30:33+0000",
            "content": "And mine was a misreport - sorry - a wine program was eating one of my 4 cores and I didn't notice - its testing at about 48 s now, so just about the same as the first 2 patches.  ",
            "author": "Mark Miller",
            "id": "comment-12741532"
        },
        {
            "date": "2009-08-10T20:42:47+0000",
            "content": "OK. Last patch, I only added a test in TestAttributeSource that verifies the correctness of the returned getAttributeImplsIterator() [important, because iterator logic is not so simple to understand...]. TestTeeSinkTokenFilter would also fail with wrong Iterator...\n\nI think I commit this shortly? Any complaints? ",
            "author": "Uwe Schindler",
            "id": "comment-12741534"
        },
        {
            "date": "2009-08-10T20:59:56+0000",
            "content": "Just to complete my report:\n\nThe tests I reported in this issue were done with a little more beef in the documents - I had added about 4 lines from a newspaper article. The result is that we are only about 4-5% slower using those documents now. However, with Yonik's original test, with very short docs:\n\n\n String[] fields = {\"text\",\"simple\"\n            ,\"text\",\"test\"\n            ,\"text\",\"how now brown cow\"\n            ,\"text\",\"what's that?\"\n            ,\"text\",\"radical!\"\n            ,\"text\",\"what's all this about, anyway?\"\n            ,\"text\",\"just how fast is this text indexing?\"\n\n\n\n...we are 10% behind. This is a mix of TokenStreams - you can see the tokenfilters used in the profile pics. This is huge improvement from before - that was 50-60% slower with this test.\n\nAll profiling pics are from Yoniks original small doc test with 100000 iterations.\n\nI'll attach:\n\nbefore the reflection token stream stuff\nafter (trunk)\nafter with this patch ",
            "author": "Mark Miller",
            "id": "comment-12741545"
        },
        {
            "date": "2009-08-10T21:30:50+0000",
            "content": "I just want to say I think that 10% test case might be a worst case: very short documents and no reusableTS.\n\nI ran a bunch of iterations on a corpus (regular sized docs though) and found this:\nLucene 2.4.1 (CzechAnalyzer, does not implement reusableTS) avg 48290.4ms\nHEAD+LUCENE-1796 (CzechAnalyzer, does not implement reusableTS) avg 49943.8ms <-- a bit slower\nHEAD+LUCENE-1796+LUCENE-1794 (CzechAnalyzer, implements reusableTS) avg 47846.1ms <-- a bit faster\n\nSo I think reusableTS in Solr combined with this patch can mitigate any remaining construction overhead, and maybe be faster overall compared to the last release. ",
            "author": "Robert Muir",
            "id": "comment-12741564"
        },
        {
            "date": "2009-08-10T21:32:07+0000",
            "content": "The shorter the text, the more the construction cost increases. This is what I exspect. For normal text length like abstracts, newspaper articles and so on, the speed is equal than before.\n\nI think I commit this now and leave all other things to the current discussion on java-dev. ",
            "author": "Uwe Schindler",
            "id": "comment-12741567"
        },
        {
            "date": "2009-08-10T21:35:35+0000",
            "content": "Committed revision: 802930\n(I only removed the clearAttributes() call again, which is unneeded for CharTokenizer) ",
            "author": "Uwe Schindler",
            "id": "comment-12741570"
        },
        {
            "date": "2009-08-10T21:39:40+0000",
            "content": "\nI think I commit this now and leave all other things to the current discussion on java-dev.\n\n+1.\nGood work Uwe! Thanks. ",
            "author": "Michael Busch",
            "id": "comment-12741571"
        },
        {
            "date": "2009-08-10T22:11:13+0000",
            "content": "(I only removed the clearAttributes() call again, which is unneeded for CharTokenizer) \n\nI thought that Tokenizers had to clear all attributes? ",
            "author": "Yonik Seeley",
            "id": "comment-12741590"
        },
        {
            "date": "2009-08-10T22:15:15+0000",
            "content": "We had no conclusion on this. I think we should create a new issue out of it and change all Tokenizers to clear all Attributes in incrementToken(). This requirement then should also be noted in the JavaDocs of Tokenizer.\n\nCurrently no Tokenizer clears the attributes... AttributeSource.clearAttributes is never called at the moment. ",
            "author": "Uwe Schindler",
            "id": "comment-12741594"
        },
        {
            "date": "2009-08-10T22:19:55+0000",
            "content": "I think Token.reset() wasn't called before either. So I think we always had this potential problem? ",
            "author": "Michael Busch",
            "id": "comment-12741595"
        },
        {
            "date": "2009-08-11T16:39:52+0000",
            "content": "Token.clear() used to be called by the consumer... but then it was switched to the producer here:\nhttps://issues.apache.org/jira/browse/LUCENE-1101\n\nI don't know if all of the Tokenizers in lucene were ever changed, but in any case it looks like at least some of these bugs were introduced with the switch to the attribute API - for example StandardTokenizer did clear it's reusableToken... and now it doesn't. ",
            "author": "Yonik Seeley",
            "id": "comment-12741952"
        },
        {
            "date": "2009-08-11T16:52:32+0000",
            "content": "I don't know if all of the Tokenizers in lucene were ever changed, but in any case it looks like at least some of these bugs were introduced with the switch to the attribute API - for example StandardTokenizer did clear it's reusableToken... and now it doesn't.\n\nNo one is calling clearAttributes() in trunk code, only some of them clear attributes before filling data in.\n\nOK, I open another issue later and change all Tokenizers in core and contrib to call clearAttributes() as first call inside incrementToken()?\n\nBut in principle we could also change the indexer to call clear before each incrementToken() removing the need to do it in every Tokenizer. ",
            "author": "Uwe Schindler",
            "id": "comment-12741957"
        },
        {
            "date": "2009-08-12T11:27:53+0000",
            "content": "But in principle we could also change the indexer to call clear before each incrementToken() removing the need to do it in every Tokenizer.\n\nDoron brought up a good reason for not doing that in LUCENE-1101.\nA tokenizer (or other token producer) could produce multiple tokens before one made it to the ultimate consumer (because of stop filters, etc).  So it looks like producers should do the clear. ",
            "author": "Yonik Seeley",
            "id": "comment-12742293"
        },
        {
            "date": "2009-08-12T23:08:33+0000",
            "content": "I opened LUCENE-1801 for that. A patch is available and will be committed soon. ",
            "author": "Uwe Schindler",
            "id": "comment-12742644"
        }
    ]
}