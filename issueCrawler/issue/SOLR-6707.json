{
    "id": "SOLR-6707",
    "title": "Recovery/election for invalid core results in rapid-fire re-attempts until /overseer/queue is clogged",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.2",
            "6.0"
        ],
        "affect_versions": "4.10",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "We experienced an issue the other day that brought a production solr server down, and this is what we found after investigating:\n\n\n\tRunning solr instance with two separate cores, one of which is perpetually down because it's configs are not yet completely updated for Solr-cloud. This was thought to be harmless since it's not currently in use.\n\tSolr experienced an \"internal server error\" supposedly because of \"No space left on device\" even though we appeared to have ~10GB free.\n\tSolr immediately went into recovery, and subsequent leader election for each shard of each core.\n\tOur primary core recovered immediately. Our additional core which was never active in the first place, attempted to recover but of course couldn't due to the improper configs.\n\tSolr then began rapid-fire reattempting recovery of said node, trying maybe 20-30 times per second.\n\tThis in turn bombarded zookeepers /overseer/queue into oblivion\n\tAt some point /overseer/queue becomes so backed up that normal cluster coordination can no longer play out, and Solr topples over.\n\n\n\nI know this is a bit of an unusual circumstance due to us keeping the dead core around, and our quick solution has been to remove said core. However I can see other potential scenarios that might cause the same issue to arise.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2014-11-05T19:56:14+0000",
            "author": "Erick Erickson",
            "content": "Hmm, thanks for the writeup.\n\nbq: Solr experienced an \"internal server error\" I believe due in part to a fairly new feature we are using, which seemingly caused all cores to go down\n\nAnything in particular about the new feature we should know about? Or does this happen with any generic internal server error?\n ",
            "id": "comment-14198963"
        },
        {
            "date": "2014-11-06T22:15:56+0000",
            "author": "James Hardwick",
            "content": "My assumption was wrong about the feature. Here is the initial error that kicked off the sequence:\n\n\n2014-11-03 11:13:37,734 [updateExecutor-1-thread-4] ERROR update.StreamingSolrServers  - error\norg.apache.solr.common.SolrException: Internal Server Error\n \n \n \nrequest: http://xxx.xxx.xxx.xxx:8081/app-search/appindex/update?update.chain=updateRequestProcessorChain&update.distrib=TOLEADER&distrib.from=http%3A%2F%2Fxxx.xxx.xxx.xxx%3A8081%2Fapp-search%2Fappindex%2F&wt=javabin&version=2\n        at org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrServer$Runner.run(ConcurrentUpdateSolrServer.java:240)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\n2014-11-03 11:13:38,056 [http-bio-8081-exec-336] WARN  processor.DistributedUpdateProcessor  - Error sending update\norg.apache.solr.common.SolrException: Internal Server Error\n \n \n \nrequest: http://xxx.xxx.xxx.xxx:8081/app-search/appindex/update?update.chain=updateRequestProcessorChain&update.distrib=TOLEADER&distrib.from=http%3A%2F%2Fxxx.xxx.xxx.xxx%3A8081%2Fapp-search%2Fappindex%2F&wt=javabin&version=2\n        at org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrServer$Runner.run(ConcurrentUpdateSolrServer.java:240)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\n2014-11-03 11:13:38,364 [http-bio-8081-exec-324] INFO  update.UpdateHandler  - start commit{,optimize=false,openSearcher=true,waitSearcher=true,expungeDeletes=false,softCommit=false,prepareCommit=false}\n2014-11-03 11:13:38,364 [http-bio-8081-exec-324] INFO  update.UpdateHandler  - No uncommitted changes. Skipping IW.commit.\n2014-11-03 11:13:38,365 [http-bio-8081-exec-324] INFO  search.SolrIndexSearcher  - Opening Searcher@60515a83[appindex] main\n2014-11-03 11:13:38,372 [http-bio-8081-exec-324] INFO  update.UpdateHandler  - end_commit_flush\n2014-11-03 11:13:38,373 [updateExecutor-1-thread-6] ERROR update.SolrCmdDistributor  - org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: No space left on device\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.executeMethod(HttpSolrServer.java:550)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:210)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:206)\n        at org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrServer.request(ConcurrentUpdateSolrServer.java:292)\n        at org.apache.solr.update.SolrCmdDistributor.doRequest(SolrCmdDistributor.java:296)\n        at org.apache.solr.update.SolrCmdDistributor.access$000(SolrCmdDistributor.java:53)\n        at org.apache.solr.update.SolrCmdDistributor$1.call(SolrCmdDistributor.java:283)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\n \n2014-11-03 11:13:40,812 [http-bio-8081-exec-336] WARN  processor.DistributedUpdateProcessor  - Error sending update\norg.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: No space left on device\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.executeMethod(HttpSolrServer.java:550)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:210)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:206)\n        at org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrServer.request(ConcurrentUpdateSolrServer.java:292)\n        at org.apache.solr.update.SolrCmdDistributor.doRequest(SolrCmdDistributor.java:296)\n        at org.apache.solr.update.SolrCmdDistributor.access$000(SolrCmdDistributor.java:53)\n        at org.apache.solr.update.SolrCmdDistributor$1.call(SolrCmdDistributor.java:283)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\n2014-11-03 11:13:40,814 [http-bio-8081-exec-336] INFO  cloud.SolrZkClient  - makePath: /collections/appindex/leader_initiated_recovery/shard1/core_node3\n2014-11-03 11:13:40,826 [http-bio-8081-exec-336] INFO  cloud.ZkController  - Wrote down to /collections/appindex/leader_initiated_recovery/shard1/core_node3\n2014-11-03 11:13:40,826 [http-bio-8081-exec-336] INFO  cloud.ZkController  - Put replica core= appindex coreNodeName=core_node3 on xxx.xxx.xxx.xxx:8081_app-search into leader-initiated recovery.\n2014-11-03 11:13:40,827 [http-bio-8081-exec-336] WARN  cloud.ZkController  - Leader is publishing core= appindex coreNodeName =core_node3 state=down on behalf of un-reachable replica http://xxx.xxx.xxx.xxx:8081/app-search/appindex/; forcePublishState? false\n2014-11-03 11:13:40,852 [http-bio-8081-exec-336] ERROR processor.DistributedUpdateProcessor  - Setting up to try to start recovery on replica http://xxx.xxx.xxx.xxx:8081/app-search/appindex/ after: org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: No space left on device\n2014-11-03 11:13:40,864 [updateExecutor-1-thread-5] INFO  cloud.LeaderInitiatedRecoveryThread  - LeaderInitiatedRecoveryThread-appindex started running to send REQUESTRECOVERY command to http://xxx.xxx.xxx.xxx:8081/app-search/appindex/; will try for a max of 600 secs\n2014-11-03 11:13:40,865 [updateExecutor-1-thread-5] INFO  cloud.LeaderInitiatedRecoveryThread  - Asking core=appindex coreNodeName=core_node3 on http://xxx.xxx.xxx.xxx:8081/app-search to recover\n2014-11-03 11:13:40,866 [http-bio-8081-exec-336] WARN  processor.DistributedUpdateProcessor  - Error sending update\norg.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: No space left on device\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.executeMethod(HttpSolrServer.java:550)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:210)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:206)\n        at org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrServer.request(ConcurrentUpdateSolrServer.java:292)\n        at org.apache.solr.update.SolrCmdDistributor.doRequest(SolrCmdDistributor.java:296)\n        at org.apache.solr.update.SolrCmdDistributor.access$000(SolrCmdDistributor.java:53)\n        at org.apache.solr.update.SolrCmdDistributor$1.call(SolrCmdDistributor.java:283)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\n2014-11-03 11:13:40,933 [updateExecutor-1-thread-5] INFO  cloud.LeaderInitiatedRecoveryThread  - Successfully sent REQUESTRECOVERY command to core=appindex coreNodeName=core_node3 on http://xxx.xxx.xxx.xxx:8081/app-search\n2014-11-03 11:13:40,949 [updateExecutor-1-thread-5] INFO  cloud.LeaderInitiatedRecoveryThread  - LeaderInitiatedRecoveryThread-appindex completed successfully after running for 0 secs\n\n\n\nDespite it claiming \"No space left on device\", we had ~10 GB free. Regardless, the subsequent recovery process for our dead core sent things into a tailspin.  ",
            "id": "comment-14201049"
        },
        {
            "date": "2014-11-07T03:03:35+0000",
            "author": "Mark Miller",
            "content": "Solr then began rapid-fire reattempting recovery of said node, trying maybe 20-30 times per second.\n\nDo you have logs or something for that? We should not be doing that. ",
            "id": "comment-14201501"
        },
        {
            "date": "2014-11-07T22:21:42+0000",
            "author": "James Hardwick",
            "content": "Interesting clusterstate.json in ZK. Why would we have null range/parent properties for an implicitly routed index that has never been split?\n\n\n{\n  \"appindex\":{\n    \"shards\":{\"shard1\":{\n        \"range\":null,\n        \"state\":\"active\",\n        \"parent\":null,\n        \"replicas\":{\n          \"core_node1\":{\n            \"state\":\"active\",\n            \"core\":\"appindex\",\n            \"node_name\":\"xxx.xxx.xxx.xxx:8081_app-search\",\n            \"base_url\":\"http://xxx.xxx.xxx.xxx:8081/app-search\"},\n          \"core_node2\":{\n            \"state\":\"active\",\n            \"core\":\"appindex\",\n            \"node_name\":\"xxx.xxx.xxx.xxx:8081_app-search\",\n            \"base_url\":\"http://xxx.xxx.xxx.xxx:8081/app-search\",\n            \"leader\":\"true\"},\n          \"core_node3\":{\n            \"state\":\"active\",\n            \"core\":\"appindex\",\n            \"node_name\":\"xxx.xxx.xxx.xxx:8081_app-search\",\n            \"base_url\":\"http://xxx.xxx.xxx.xxx:8081/app-search\"}}}},\n    \"router\":{\"name\":\"implicit\"}},\n  \"app-analytics\":{\n    \"shards\":{\"shard1\":{\n        \"range\":null,\n        \"state\":\"active\",\n        \"parent\":null,\n        \"replicas\":{\n          \"core_node1\":{\n            \"state\":\"recovery_failed\",\n            \"core\":\"app-analytics\",\n            \"node_name\":\"xxx.xxx.xxx.xxx:8081_app-search\",\n            \"base_url\":\"http://xxx.xxx.xxx.xxx:8081/app-search\"},\n          \"core_node2\":{\n            \"state\":\"recovery_failed\",\n            \"core\":\"app-analytics\",\n            \"node_name\":\"xxx.xxx.xxx.xxx:8081_app-search\",\n            \"base_url\":\"http://xxx.xxx.xxx.xxx:8081/app-search\"},\n          \"core_node3\":{\n            \"state\":\"down\",\n            \"core\":\"app-analytics\",\n            \"node_name\":\"xxx.xxx.xxx.xxx:8081_app-search\",\n            \"base_url\":\"http://xxx.xxx.xxx.xxx:8081/app-search\",\n            \"leader\":\"true\"}}}},\n    \"router\":{\"name\":\"implicit\"}}}\n\n ",
            "id": "comment-14202822"
        },
        {
            "date": "2014-11-07T22:32:33+0000",
            "author": "James Hardwick",
            "content": "This is an excerpt from the the logs immediately following the original exception (shown above)\n\n\n2014-11-03 11:13:58,488 [zkCallback-2-thread-86] INFO  cloud.ElectionContext  - I am going to be the leader xxx.xxx.xxx.109:8081_app-search\n2014-11-03 11:13:58,489 [zkCallback-2-thread-86] INFO  cloud.SolrZkClient  - makePath: /overseer_elect/leader\n2014-11-03 11:13:58,489 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - Running the leader process for shard shard1\n2014-11-03 11:13:58,489 [zkCallback-2-thread-85] INFO  cloud.ShardLeaderElectionContext  - Running the leader process for shard shard1\n2014-11-03 11:13:58,496 [zkCallback-2-thread-86] INFO  cloud.Overseer  - Overseer (id=92718232187174914-xxx.xxx.xxx.109:8081_app-search-n_0000000188) starting\n2014-11-03 11:13:58,499 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - Checking if I (core=app-analytics,coreNodeName=core_node1) should try and be the leader.\n2014-11-03 11:13:58,499 [zkCallback-2-thread-85] INFO  cloud.ShardLeaderElectionContext  - Checking if I (core=appindex,coreNodeName=core_node1) should try and be the leader.\n2014-11-03 11:13:58,499 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - My last published State was down, I won't be the leader.\n2014-11-03 11:13:58,499 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - There may be a better leader candidate than us - going back into recovery\n2014-11-03 11:13:58,499 [zkCallback-2-thread-88] INFO  cloud.ElectionContext  - canceling election /collections/app-analytics/leader_elect/shard1/election/92718232187174914-core_node1-n_0001746105\n2014-11-03 11:13:58,499 [zkCallback-2-thread-85] INFO  cloud.ShardLeaderElectionContext  - My last published State was Active, it's okay to be the leader.\n2014-11-03 11:13:58,499 [zkCallback-2-thread-85] INFO  cloud.ShardLeaderElectionContext  - I may be the new leader - try and sync\n2014-11-03 11:13:58,504 [zkCallback-2-thread-88] INFO  update.DefaultSolrCoreState  - Running recovery - first canceling any ongoing recovery\n2014-11-03 11:13:58,506 [RecoveryThread] INFO  cloud.RecoveryStrategy  - Starting recovery process.  core=app-analytics recoveringAfterStartup=true\n2014-11-03 11:13:58,507 [RecoveryThread] ERROR cloud.RecoveryStrategy  - No UpdateLog found - cannot recover. core=app-analytics\n2014-11-03 11:13:58,507 [RecoveryThread] ERROR cloud.RecoveryStrategy  - Recovery failed - I give up. core=app-analytics\n2014-11-03 11:13:58,507 [RecoveryThread] INFO  cloud.ZkController  - publishing core=app-analytics state=recovery_failed collection=app-analytics\n2014-11-03 11:13:58,508 [RecoveryThread] INFO  cloud.ZkController  - numShards not found on descriptor - reading it from system property\n2014-11-03 11:13:58,521 [RecoveryThread] WARN  cloud.RecoveryStrategy  - Stopping recovery for core=app-analytics coreNodeName=core_node1\n2014-11-03 11:13:58,560 [zkCallback-2-thread-86] INFO  cloud.OverseerAutoReplicaFailoverThread  - Starting OverseerAutoReplicaFailoverThread autoReplicaFailoverWorkLoopDelay=10000 autoReplicaFailoverWaitAfterExpiration=30000 autoReplicaFailoverBadNodeExpiration=60000\n2014-11-03 11:13:58,575 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - Running the leader process for shard shard1\n2014-11-03 11:13:58,580 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - Checking if I (core=app-analytics,coreNodeName=core_node1) should try and be the leader.\n2014-11-03 11:13:58,581 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - My last published State was recovery_failed, I won't be the leader.\n2014-11-03 11:13:58,581 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - There may be a better leader candidate than us - going back into recovery\n2014-11-03 11:13:58,581 [zkCallback-2-thread-88] INFO  cloud.ElectionContext  - canceling election /collections/app-analytics/leader_elect/shard1/election/92718232187174914-core_node1-n_0001746107\n2014-11-03 11:13:58,583 [zkCallback-2-thread-88] INFO  update.DefaultSolrCoreState  - Running recovery - first canceling any ongoing recovery\n2014-11-03 11:13:58,584 [RecoveryThread] INFO  cloud.RecoveryStrategy  - Starting recovery process.  core=app-analytics recoveringAfterStartup=false\n2014-11-03 11:13:58,584 [RecoveryThread] ERROR cloud.RecoveryStrategy  - No UpdateLog found - cannot recover. core=app-analytics\n2014-11-03 11:13:58,584 [RecoveryThread] ERROR cloud.RecoveryStrategy  - Recovery failed - I give up. core=app-analytics\n2014-11-03 11:13:58,584 [RecoveryThread] INFO  cloud.ZkController  - publishing core=app-analytics state=recovery_failed collection=app-analytics\n2014-11-03 11:13:58,585 [RecoveryThread] INFO  cloud.ZkController  - numShards not found on descriptor - reading it from system property\n2014-11-03 11:13:58,588 [RecoveryThread] WARN  cloud.RecoveryStrategy  - Stopping recovery for core=app-analytics coreNodeName=core_node1\n2014-11-03 11:13:58,599 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - Running the leader process for shard shard1\n2014-11-03 11:13:58,606 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - Checking if I (core=app-analytics,coreNodeName=core_node1) should try and be the leader.\n2014-11-03 11:13:58,606 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - My last published State was recovery_failed, I won't be the leader.\n2014-11-03 11:13:58,606 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - There may be a better leader candidate than us - going back into recovery\n2014-11-03 11:13:58,606 [zkCallback-2-thread-88] INFO  cloud.ElectionContext  - canceling election /collections/app-analytics/leader_elect/shard1/election/92718232187174914-core_node1-n_0001746109\n2014-11-03 11:13:58,609 [zkCallback-2-thread-88] INFO  update.DefaultSolrCoreState  - Running recovery - first canceling any ongoing recovery\n2014-11-03 11:13:58,609 [RecoveryThread] INFO  cloud.RecoveryStrategy  - Starting recovery process.  core=app-analytics recoveringAfterStartup=false\n2014-11-03 11:13:58,610 [RecoveryThread] ERROR cloud.RecoveryStrategy  - No UpdateLog found - cannot recover. core=app-analytics\n2014-11-03 11:13:58,610 [RecoveryThread] ERROR cloud.RecoveryStrategy  - Recovery failed - I give up. core=app-analytics\n2014-11-03 11:13:58,610 [RecoveryThread] INFO  cloud.ZkController  - publishing core=app-analytics state=recovery_failed collection=app-analytics\n2014-11-03 11:13:58,610 [RecoveryThread] INFO  cloud.ZkController  - numShards not found on descriptor - reading it from system property\n2014-11-03 11:13:58,615 [RecoveryThread] WARN  cloud.RecoveryStrategy  - Stopping recovery for core=app-analytics coreNodeName=core_node1\n2014-11-03 11:13:58,621 [OverseerCollectionProcessor-92718232187174914-xxx.xxx.xxx.109:8081_app-search-n_0000000188] INFO  cloud.OverseerCollectionProcessor  - Process current queue of collection creations\n2014-11-03 11:13:58,623 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - Running the leader process for shard shard1\n2014-11-03 11:13:58,626 [OverseerStateUpdate-92718232187174914-xxx.xxx.xxx.109:8081_app-search-n_0000000188] INFO  cloud.Overseer  - Starting to work on the main queue\n2014-11-03 11:13:58,630 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - Checking if I (core=app-analytics,coreNodeName=core_node1) should try and be the leader.\n2014-11-03 11:13:58,630 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - My last published State was recovery_failed, I won't be the leader.\n2014-11-03 11:13:58,630 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - There may be a better leader candidate than us - going back into recovery\n2014-11-03 11:13:58,630 [zkCallback-2-thread-88] INFO  cloud.ElectionContext  - canceling election /collections/app-analytics/leader_elect/shard1/election/92718232187174914-core_node1-n_0001746111\n2014-11-03 11:13:58,639 [zkCallback-2-thread-88] INFO  update.DefaultSolrCoreState  - Running recovery - first canceling any ongoing recovery\n2014-11-03 11:13:58,640 [RecoveryThread] INFO  cloud.RecoveryStrategy  - Starting recovery process.  core=app-analytics recoveringAfterStartup=false\n2014-11-03 11:13:58,641 [RecoveryThread] ERROR cloud.RecoveryStrategy  - No UpdateLog found - cannot recover. core=app-analytics\n2014-11-03 11:13:58,641 [RecoveryThread] ERROR cloud.RecoveryStrategy  - Recovery failed - I give up. core=app-analytics\n2014-11-03 11:13:58,641 [RecoveryThread] INFO  cloud.ZkController  - publishing core=app-analytics state=recovery_failed collection=app-analytics\n2014-11-03 11:13:58,641 [RecoveryThread] INFO  cloud.ZkController  - numShards not found on descriptor - reading it from system property\n2014-11-03 11:13:58,645 [OverseerStateUpdate-92718232187174914-xxx.xxx.xxx.109:8081_app-search-n_0000000188] INFO  cloud.Overseer  - Update state numShards=null message={\n  \"operation\":\"state\",\n  \"state\":\"down\",\n  \"base_url\":\"http://xxx.xxx.xxx.160:8081/app-search\",\n  \"core\":\"appindex\",\n  \"node_name\":\"xxx.xxx.xxx.160:8081_app-search\",\n  \"shard\":\"shard1\",\n  \"collection\":\"appindex\"}\n2014-11-03 11:13:58,645 [OverseerStateUpdate-92718232187174914-xxx.xxx.xxx.109:8081_app-search-n_0000000188] INFO  cloud.Overseer  - node=core_node3 is already registered\n2014-11-03 11:13:58,646 [RecoveryThread] WARN  cloud.RecoveryStrategy  - Stopping recovery for core=app-analytics coreNodeName=core_node1\n2014-11-03 11:13:58,646 [zkCallback-2-thread-86] INFO  cloud.DistributedQueue  - LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged\n2014-11-03 11:13:58,658 [zkCallback-2-thread-88] INFO  cloud.ZkStateReader  - A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 2)\n2014-11-03 11:13:58,660 [zkCallback-2-thread-86] INFO  cloud.ShardLeaderElectionContext  - Running the leader process for shard shard1\n2014-11-03 11:13:58,666 [zkCallback-2-thread-86] INFO  cloud.ShardLeaderElectionContext  - Checking if I (core=app-analytics,coreNodeName=core_node1) should try and be the leader.\n2014-11-03 11:13:58,666 [zkCallback-2-thread-86] INFO  cloud.ShardLeaderElectionContext  - My last published State was recovery_failed, I won't be the leader.\n2014-11-03 11:13:58,666 [zkCallback-2-thread-86] INFO  cloud.ShardLeaderElectionContext  - There may be a better leader candidate than us - going back into recovery\n2014-11-03 11:13:58,667 [zkCallback-2-thread-86] INFO  cloud.ElectionContext  - canceling election /collections/app-analytics/leader_elect/shard1/election/92718232187174914-core_node1-n_0001746113\n2014-11-03 11:13:58,669 [zkCallback-2-thread-86] INFO  update.DefaultSolrCoreState  - Running recovery - first canceling any ongoing recovery\n2014-11-03 11:13:58,670 [RecoveryThread] INFO  cloud.RecoveryStrategy  - Starting recovery process.  core=app-analytics recoveringAfterStartup=false\n2014-11-03 11:13:58,671 [RecoveryThread] ERROR cloud.RecoveryStrategy  - No UpdateLog found - cannot recover. core=app-analytics\n2014-11-03 11:13:58,671 [RecoveryThread] ERROR cloud.RecoveryStrategy  - Recovery failed - I give up. core=app-analytics\n2014-11-03 11:13:58,671 [RecoveryThread] INFO  cloud.ZkController  - publishing core=app-analytics state=recovery_failed collection=app-analytics\n2014-11-03 11:13:58,671 [RecoveryThread] INFO  cloud.ZkController  - numShards not found on descriptor - reading it from system property\n2014-11-03 11:13:58,676 [RecoveryThread] WARN  cloud.RecoveryStrategy  - Stopping recovery for core=app-analytics coreNodeName=core_node1\n2014-11-03 11:13:58,676 [zkCallback-2-thread-88] INFO  cloud.DistributedQueue  - LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged\n2014-11-03 11:13:58,687 [zkCallback-2-thread-88] INFO  cloud.DistributedQueue  - LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged\n2014-11-03 11:13:58,689 [OverseerStateUpdate-92718232187174914-xxx.xxx.xxx.109:8081_app-search-n_0000000188] INFO  cloud.Overseer  - Update state numShards=0 message={\n  \"operation\":\"state\",\n  \"core_node_name\":\"core_node1\",\n  \"numShards\":\"0\",\n  \"shard\":\"shard1\",\n  \"roles\":null,\n  \"state\":\"recovery_failed\",\n  \"core\":\"app-analytics\",\n  \"collection\":\"app-analytics\",\n  \"node_name\":\"xxx.xxx.xxx.109:8081_app-search\",\n  \"base_url\":\"http://xxx.xxx.xxx.109:8081/app-search\"}\n  2014-11-03 11:13:58,704 [zkCallback-2-thread-88] INFO  cloud.DistributedQueue  - LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged\n2014-11-03 11:13:58,705 [OverseerStateUpdate-92718232187174914-xxx.xxx.xxx.109:8081_app-search-n_0000000188] INFO  cloud.Overseer  - Update state numShards=0 message={\n  \"operation\":\"state\",\n  \"core_node_name\":\"core_node2\",\n  \"numShards\":\"0\",\n  \"shard\":\"shard1\",\n  \"roles\":null,\n  \"state\":\"recovery_failed\",\n  \"core\":\"app-analytics\",\n  \"collection\":\"app-analytics\",\n  \"node_name\":\"xxx.xxx.xxx.154:8081_app-search\",\n  \"base_url\":\"http://xxx.xxx.xxx.154:8081/app-search\"}\n2014-11-03 11:13:58,719 [zkCallback-2-thread-88] INFO  cloud.DistributedQueue  - LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged\n2014-11-03 11:13:58,720 [OverseerStateUpdate-92718232187174914-xxx.xxx.xxx.109:8081_app-search-n_0000000188] INFO  cloud.Overseer  - Update state numShards=0 message={\n  \"operation\":\"state\",\n  \"core_node_name\":\"core_node1\",\n  \"numShards\":\"0\",\n  \"shard\":\"shard1\",\n  \"roles\":null,\n  \"state\":\"recovery_failed\",\n  \"core\":\"app-analytics\",\n  \"collection\":\"app-analytics\",\n  \"node_name\":\"xxx.xxx.xxx.109:8081_app-search\",\n  \"base_url\":\"http://xxx.xxx.xxx.109:8081/app-search\"}\n2014-11-03 11:13:58,724 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - Running the leader process for shard shard1\n2014-11-03 11:13:58,732 [zkCallback-2-thread-86] INFO  cloud.DistributedQueue  - LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged\n2014-11-03 11:13:58,732 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - Checking if I (core=app-analytics,coreNodeName=core_node1) should try and be the leader.\n2014-11-03 11:13:58,732 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - My last published State was recovery_failed, I won't be the leader.\n2014-11-03 11:13:58,732 [zkCallback-2-thread-88] INFO  cloud.ShardLeaderElectionContext  - There may be a better leader candidate than us - going back into recovery\n2014-11-03 11:13:58,732 [zkCallback-2-thread-88] INFO  cloud.ElectionContext  - canceling election /collections/app-analytics/leader_elect/shard1/election/92718232187174914-core_node1-n_0001746115\n2014-11-03 11:13:58,734 [zkCallback-2-thread-88] INFO  update.DefaultSolrCoreState  - Running recovery - first canceling any ongoing recovery\n2014-11-03 11:13:58,735 [RecoveryThread] INFO  cloud.RecoveryStrategy  - Starting recovery process.  core=app-analytics recoveringAfterStartup=false\n2014-11-03 11:13:58,736 [RecoveryThread] ERROR cloud.RecoveryStrategy  - No UpdateLog found - cannot recover. core=app-analytics\n2014-11-03 11:13:58,736 [RecoveryThread] ERROR cloud.RecoveryStrategy  - Recovery failed - I give up. core=app-analytics\n2014-11-03 11:13:58,736 [RecoveryThread] INFO  cloud.ZkController  - publishing core=app-analytics state=recovery_failed collection=app-analytics\n2014-11-03 11:13:58,736 [RecoveryThread] INFO  cloud.ZkController  - numShards not found on descriptor - reading it from system property\n2014-11-03 11:13:58,741 [zkCallback-2-thread-86] INFO  cloud.DistributedQueue  - LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged\n\n ",
            "id": "comment-14202844"
        },
        {
            "date": "2014-11-07T22:36:14+0000",
            "author": "James Hardwick",
            "content": "Also FYI, the original exception may very well have been from lack of disk space, since we were also noticing Solr occasionally holding onto a Tlog that was absolutely massive (250GB at one point). ",
            "id": "comment-14202853"
        },
        {
            "date": "2015-01-22T15:03:46+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "This happens because even if the replica has run out of disk space, the 'requestrecovery' command sent from the leader is successful. The leader then puts that replica back into rotation which fails. This keeps repeating. We need better handling of such exceptions on the leader code path. ",
            "id": "comment-14287549"
        },
        {
            "date": "2015-01-26T03:23:14+0000",
            "author": "Mark Miller",
            "content": "ERROR cloud.RecoveryStrategy  - No UpdateLog found - cannot recover.\n\nYou need to have an UpdateLog defined in solrconfig.xml. ",
            "id": "comment-14291421"
        },
        {
            "date": "2015-01-26T03:58:24+0000",
            "author": "Mark Miller",
            "content": "Some of this is probably related to SOLR-7033. ",
            "id": "comment-14291437"
        },
        {
            "date": "2017-08-23T22:42:06+0000",
            "author": "Ben DeMott",
            "content": "We have experienced this multiple times.  We host inside AWS and Zookeeper is spread across different availability zones...\nThis means that the connection between ZK's has high latency once in awhile which ZK doesn't seem to like.  I wonder if anyone else is in this situation.\nWe've never had so many Zookeeper issues as we do now that we've moved our infrastructure inside AWS.\n\nWhat triggered a backed up overseer queue for us was a hung ephemeral node in Zookeeper which I discuss here:\nhttps://stackoverflow.com/questions/23743424/solr-issue-clusterstate-says-we-are-the-leader-but-locally-we-dont-think-so/42210844#42210844\n\nAs OP said, once this goes on for long enough Solr runs out of file-descriptors, and eventually brings down the whole cluster.\n\nThis bug in Zookeeper (appears) to be the cause of the hung ephemeral node:\nhttps://issues.apache.org/jira/browse/ZOOKEEPER-2355 ",
            "id": "comment-16139275"
        }
    ]
}