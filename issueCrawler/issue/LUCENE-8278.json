{
    "id": "LUCENE-8278",
    "title": "UAX29URLEmailTokenizer is not detecting some tokens as URL type",
    "details": {
        "components": [],
        "status": "Closed",
        "resolution": "Fixed",
        "fix_versions": [
            "7.4",
            "master (8.0)"
        ],
        "affect_versions": "None",
        "labels": "",
        "priority": "Minor",
        "type": "Bug"
    },
    "description": "We are using the UAX29URLEmailTokenizer so we can use the token types in our plugins.\n\nHowever, I noticed that the tokenizer is not detecting certain URLs as <URL> but <ALPHANUM> instead.\n\nExamples that are not working:\n\n\texample.com is <ALPHANUM>\n\texample.net is <ALPHANUM>\n\n\n\nBut:\n\n\thttps://example.com is <URL>\n\tas is https://example.net\n\n\n\nExamples that work:\n\n\texample.ch is <URL>\n\texample.co.uk is <URL>\n\texample.nl is <URL>\n\n\n\nI have checked this JIRA, and could not find an issue. I have tested this on Lucene (Solr) 6.4.1 and 7.3.\n\nCould someone confirm my findings and advise what I could do to (help) resolve this issue?",
    "attachments": {
        "patched.png": "https://issues.apache.org/jira/secure/attachment/12927373/patched.png",
        "unpatched.png": "https://issues.apache.org/jira/secure/attachment/12927374/unpatched.png",
        "LUCENE-8278.patch": "https://issues.apache.org/jira/secure/attachment/12925332/LUCENE-8278.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16459032",
            "author": "Steve Rowe",
            "content": "Confirming there is an issue, but I don't think the spellings of \"example.com\" and \"example.net\" are the problem though; more likely this is related to an end-of-input issue.  This test added to TestUAX29URLEmailTokenizer fails for me:\n\n\n  public void testExampleURLs() throws Exception {\n    Analyzer analyzer = new Analyzer() {\n      @Override protected TokenStreamComponents createComponents(String fieldName) {\n        return new TokenStreamComponents(new UAX29URLEmailTokenizer(newAttributeFactory()));\n      }};\n\n    // A trailing space allows these to succeed\n    BaseTokenStreamTestCase.assertAnalyzesTo(analyzer, \"example.com \", new String[]{\"example.com\"}, new String[]{\"<URL>\"});\n    BaseTokenStreamTestCase.assertAnalyzesTo(analyzer, \"example.net \", new String[]{\"example.net\"}, new String[]{\"<URL>\"});\n    \n    // These fail\n    BaseTokenStreamTestCase.assertAnalyzesTo(analyzer, \"example.com\", new String[]{\"example.com\"}, new String[]{\"<URL>\"});\n    BaseTokenStreamTestCase.assertAnalyzesTo(analyzer, \"example.net\", new String[]{\"example.net\"}, new String[]{\"<URL>\"});\n  }\n\n\n\nSo there is an issue here with no-scheme end-of-input URLs not being recognized as type <URL>. ",
            "date": "2018-04-30T20:40:28+0000"
        },
        {
            "id": "comment-16459037",
            "author": "Steve Rowe",
            "content": "Hmm, \"example.co\" and \"example.info\" in the above test succeed, so the problem here is somehow related to TLD spelling.  ",
            "date": "2018-04-30T20:45:08+0000"
        },
        {
            "id": "comment-16460629",
            "author": "Junte Zhang",
            "content": "Thank you for confirming this issue Steve. We run Lucene/Solr 6.6 on our production servers, and we also found this workaround to append a whitespace to the token to work on this version. However, this workaround is no longer working on Lucene 7.3.0. I'll see if I can fix this... ",
            "date": "2018-05-02T07:35:44+0000"
        },
        {
            "id": "comment-16489997",
            "author": "Steve Rowe",
            "content": "I ran a test to check all TLDs appended to \"example.\", and 169 out of 1543 possible TLDs have this problem:\n\n\n\"accountants\", \"ads\", \"aeg\", \"afl\", \"aig\", \"aol\", \"art\", \"audio\", \"autos\", \"aws\", \"axa\", \"bar\", \"bbc\", \"bet\",\n\"bid\", \"bingo\", \"bms\", \"bnl\", \"bom\", \"boo\", \"bot\", \"box\", \"bzh\", \"cab\", \"cal\", \"cam\", \"camp\", \"car\", \"care\", \n\"careers\", \"cat\", \"cfa\", \"citic\", \"com\", \"coupons\", \"crs\", \"cruises\", \"deals\", \"dev\", \"dog\", \"dot\", \"eco\", \n\"esq\", \"eus\", \"fans\", \"fit\", \"foo\", \"fox\", \"frl\", \"fund\", \"gal\", \"games\", \"gdn\", \"gea\", \"gifts\", \"gle\", \n\"gmo\", \"goog\", \"hkt\", \"htc\", \"ing\", \"int\", \"ist\", \"itv\", \"jmp\", \"jot\", \"kia\", \"kpn\", \"krd\", \"lat\", \"law\", \n\"loans\", \"ltd\", \"man\", \"map\", \"markets\", \"med\", \"men\", \"mlb\", \"mma\", \"moe\", \"mov\", \"msd\", \"mtn\", \"nab\", \n\"nec\", \"new\", \"news\", \"nfl\", \"ngo\", \"now\", \"nra\", \"pay\", \"pet\", \"phd\", \"photos\", \"ping\", \"pnc\", \"pro\", \n\"prof\", \"pru\", \"pwc\", \"red\", \"reisen\", \"ren\", \"reviews\", \"run\", \"rwe\", \"sap\", \"sas\", \"sbi\", \"sca\", \"ses\", \n\"sew\", \"ski\", \"soy\", \"srl\", \"stc\", \"taxi\", \"tci\", \"tdk\", \"thd\", \"tjx\", \"top\", \"trv\", \"tvs\", \"vet\", \"vig\", \n\"vin\", \"wine\", \"works\", \"aco\", \"aigo\", \"arte\", \"bbt\", \"bio\", \"biz\", \"bmw\", \"book\", \"call\", \"cars\", \"cfd\", \n\"food\", \"gap\", \"gmx\", \"ink\", \"joy\", \"kim\", \"ltda\", \"menu\", \"meo\", \"mls\", \"moi\", \"mom\", \"mtr\", \"net\", \"nrw\", \n\"pink\", \"prod\", \"rent\", \"sapo\", \"sbs\", \"scb\", \"sex\", \"sexy\", \"skin\", \"sky\", \"srt\", \"vip\"\n\n\n\nIn each of the above cases I've looked at, there is a TLD that is a prefix that is shorter by one letter (see the branch_7x TLD regex).  Not sure if all such TLDs have this problem; I'll look.\n\nAlso, on branch_7x anyway (from which 7.3.0 was cut a few months ago), appending a space to the input still works around the problem for me, so I can't reproduce the non-working workaround that you say is a problem with 7.3.0, Junte Zhang. ",
            "date": "2018-05-24T23:43:09+0000"
        },
        {
            "id": "comment-16490197",
            "author": "Steve Rowe",
            "content": "Not sure if all such TLDs have this problem; I'll look.\n\nYes, the problematic TLDs are exactly the set of those for which there exists a one-letter-shorter prefix. ",
            "date": "2018-05-25T04:06:01+0000"
        },
        {
            "id": "comment-16490211",
            "author": "Steve Rowe",
            "content": "I suspect this behavior is caused as a side-effect of the fix for LUCENE-5391. ",
            "date": "2018-05-25T04:26:10+0000"
        },
        {
            "id": "comment-16492160",
            "author": "Steve Rowe",
            "content": "I've attached a fully-regenerated patch (which is why it's so big...) against the master branch for a fix I cooked up.  In this change, the TLD macro generator partitions TLDs by whether they are prefixes of other TLDs, and by suffix length, and then the grammar tries the longest TLDs first, falling back one suffix char at a time.  Currently there are only 3 buckets: \n\n\n\tNone of the TLDs is a 1-character-shorter prefix of another TLD\n\tEach TLD is a prefix of another TLD by 1 character\n\tEach TLD is a prefix of another TLD by 2 characters\n\n\n\nThe TLD macro generator does not hard code the number of buckets, so it should be able to handle future TLD prefixes with suffixes of more than 2 characters. \n\nI've added a test for example.TLD URLs at end-of-input for all TLDs, and it passes, as do all other tests in the analyzers-common module.\n\nFYI, the fix here was complicated by the fact that JFlex doesn't support end-of-input assertion (like Java's \\z) as part of a lexical rule: the <<EOF>> rule can't be combined with a regex, and zero-length lookahead assertions must match at least one character.\n\nJunte Zhang, can you test this in your context? ",
            "date": "2018-05-27T21:10:53+0000"
        },
        {
            "id": "comment-16492161",
            "author": "Steve Rowe",
            "content": "Here's the output from the TLD macro generator (ant gen-tlds) with the patch:\n\n\ngen-tlds:\n     [java] Found 1541 TLDs in IANA Root Zone Database at http://www.internic.net/zones/root.zone\n     [java] Wrote TLD macros to '/Users/sarowe/git/lucene-solr-3/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ASCIITLD.jflex-macro':\n     [java]                       ASCIITLD: 1420 TLDs\n     [java]     ASCIITLDprefix_1CharSuffix:  109 TLDs\n     [java]     ASCIITLDprefix_2CharSuffix:   12 TLDs\n     [java]                          Total: 1541 TLDs\n\n ",
            "date": "2018-05-27T21:16:04+0000"
        },
        {
            "id": "comment-16495719",
            "author": "Steve Rowe",
            "content": "I plan on committing this tomorrow if I don't get any feedback before then. ",
            "date": "2018-05-30T21:38:43+0000"
        },
        {
            "id": "comment-16495777",
            "author": "Junte Zhang",
            "content": "Hi Steve, sorry for the late response. I will check this tomorrow. Thanks for picking up this bug report!  ",
            "date": "2018-05-30T22:10:42+0000"
        },
        {
            "id": "comment-16498167",
            "author": "Steve Rowe",
            "content": "I will check this tomorrow.\n\nAny luck Junte Zhang? ",
            "date": "2018-06-01T16:04:34+0000"
        },
        {
            "id": "comment-16498309",
            "author": "Junte Zhang",
            "content": "I think I have tested the patch:\n\npatch -p1 -i LUCENE-8278.patch \npatching file lucene/analysis/common/build.xml\npatching file lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/ASCIITLD.jflex-macro\npatching file lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java\npatching file lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex\npatching file lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestUAX29URLEmailTokenizer.java\npatching file lucene/analysis/common/src/tools/java/org/apache/lucene/analysis/standard/GenerateJflexTLDMacros.java\n\n\n\u00a0\n\nthen ant compile\n\nStarted Solr and created a core with a fieldType:\n\n<fieldType name=\"urlEmail\" class=\"solr.TextField\">\n\u00a0\u00a0\u00a0\u00a0\u00a0 <analyzer>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 <tokenizer class=\"solr.UAX29URLEmailTokenizerFactory\"/>\n\u00a0\u00a0\u00a0\u00a0\u00a0 </analyzer>\n</fieldType>\n\nThen tested in the Solr Admin but didn't see a difference, but perhaps I missed something. ",
            "date": "2018-06-01T17:37:06+0000"
        },
        {
            "id": "comment-16498335",
            "author": "Steve Rowe",
            "content": "Started Solr [...] Then tested in the Solr Admin but didn't see a difference\n\nHow did you start Solr?  You must first run ant server to put the modified lucene-analyzers-common jar into solr/server/, which will then be used when you run bin/solr start. ",
            "date": "2018-06-01T17:48:34+0000"
        },
        {
            "id": "comment-16508726",
            "author": "Steve Rowe",
            "content": "I made a Solr configset with an analyzer containing only a solr.UAX29URLEmailTokenizerFactory tokenizer, then ran Solr on master both without the patch:  and with the patch: .\n\nJunte Zhang: I think you're just having issues running the modified code.\n\nCommitting shortly. ",
            "date": "2018-06-11T20:57:21+0000"
        },
        {
            "id": "comment-16508766",
            "author": "ASF subversion and git services",
            "content": "Commit 6140d8be05d1b7aad565f53dd2ee66b984b9a379 in lucene-solr's branch refs/heads/branch_7x from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6140d8b ]\n\nLUCENE-8278: Some end-of-input no-scheme domain-only URL tokens are typed as <ALPHANUM> rather than <URL> ",
            "date": "2018-06-11T21:16:37+0000"
        },
        {
            "id": "comment-16508767",
            "author": "ASF subversion and git services",
            "content": "Commit ead05a10b1eff181ef24f64cf7feee91ed5a5155 in lucene-solr's branch refs/heads/master from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ead05a1 ]\n\nLUCENE-8278: Some end-of-input no-scheme domain-only URL tokens are typed as <ALPHANUM> rather than <URL> ",
            "date": "2018-06-11T21:16:38+0000"
        },
        {
            "id": "comment-16509628",
            "author": "ASF subversion and git services",
            "content": "Commit 3ed77ebd8dc85ebda2817033ec20df372735c650 in lucene-solr's branch refs/heads/branch_7_4 from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=3ed77eb ]\n\nLUCENE-8278: Some end-of-input no-scheme domain-only URL tokens are typed as <ALPHANUM> rather than <URL> ",
            "date": "2018-06-12T13:33:10+0000"
        },
        {
            "id": "comment-16509629",
            "author": "ASF subversion and git services",
            "content": "Commit cb30a2634c0579d5279ec148aa427745fb7f55ad in lucene-solr's branch refs/heads/branch_7x from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=cb30a26 ]\n\nLUCENE-8278: move CHANGES entry to 7.4 section ",
            "date": "2018-06-12T13:35:29+0000"
        },
        {
            "id": "comment-16509630",
            "author": "ASF subversion and git services",
            "content": "Commit 90e4eca9dbf622d6a9d053bdca4aaaca7add1558 in lucene-solr's branch refs/heads/master from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=90e4eca ]\n\nLUCENE-8278: move CHANGES entry to 7.4 section ",
            "date": "2018-06-12T13:35:31+0000"
        }
    ]
}