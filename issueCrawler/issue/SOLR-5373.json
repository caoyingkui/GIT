{
    "id": "SOLR-5373",
    "title": "Can't become leader due infinite recovery loop",
    "details": {
        "affect_versions": "4.2",
        "status": "Closed",
        "fix_versions": [
            "4.7"
        ],
        "components": [],
        "type": "Bug",
        "priority": "Minor",
        "labels": "",
        "resolution": "Not A Problem"
    },
    "description": "We found an issue while performing stability tests on SolrCloud. Under certain circumstances, a node will get in an endless loop trying to recover. I've seen this happen in a two node setup, by following these steps:\n\n1) Node A started\n2) Node B started\n3) Node B stopped\n4) Node B started, and immediately Node A stopped (normal graceful shutdown). \n\nAt this point node B will throw connection refused messages while trying to sync to node A. For some reason (not always) this leads to a corrupt state where node B enters an infinite loop trying to recover from node A (it still thinks the cluster has two nodes). I think the leader election process started just fine, but since recovery is running async, at some point node B published it state as recovery failed, hence causing leader election to fail.\n\nZookeeper /live_nodes has only one file.\n\nThis shows on the logs:\n    0:57:18,960 INFO INFO  [ShardLeaderElectionContext] (main-EventThread) Running the leader process.\n    10:57:19,068 INFO INFO  [ShardLeaderElectionContext] (main-EventThread) Checking if I should try and be the leader.\n    10:57:19,068 INFO INFO  [ShardLeaderElectionContext] (main-EventThread) My last published State was recovery_failed, I won't be the leader.\n    10:57:19,068 INFO INFO  [ShardLeaderElectionContext] (main-EventThread) There may be a better leader candidate than us - going back into recovery\n    10:57:19,118 INFO INFO  [DefaultSolrCoreState] (main-EventThread) Running recovery - first canceling any ongoing recovery\n    10:57:19,118 WARN WARN  [RecoveryStrategy] (main-EventThread) Stopping recovery for zkNodeName=10.50.100.30:8998_solr_myCollectioncore=myCollection\n    10:57:19,869 ERROR ERROR [RecoveryStrategy] (RecoveryThread) Error while trying to recover. core=myCollection:org.apache.solr.common.SolrException: No registered leader was found, collection:myCollection slice:shard1\n            at org.apache.solr.common.cloud.ZkStateReader.getLeaderRetry(ZkStateReader.java:484)\n            at org.apache.solr.common.cloud.ZkStateReader.getLeaderRetry(ZkStateReader.java:467)\n            at org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:321)\n            at org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:223)\n\n    10:57:19,869 ERROR ERROR [RecoveryStrategy] (RecoveryThread) Recovery failed - trying again... (0) core=myCollection\n    10:57:19,869 ERROR ERROR [RecoveryStrategy] (RecoveryThread) Recovery failed - interrupted. core=myCollection\n    10:57:19,869 ERROR ERROR [RecoveryStrategy] (RecoveryThread) Recovery failed - I give up. core=myCollection\n    10:57:19,869 INFO INFO  [ZkController] (RecoveryThread) publishing core=myCollection state=recovery_failed\n    10:57:19,869 INFO INFO  [ZkController] (RecoveryThread) numShards not found on descriptor - reading it from system property\n    10:57:19,902 WARN WARN  [RecoveryStrategy] (RecoveryThread) Stopping recovery for zkNodeName=10.50.100.30:8998_solr_myCollectioncore=myCollection\n    10:57:19,902 INFO INFO  [RecoveryStrategy] (RecoveryThread) Finished recovery process. core=myCollection\n    10:57:19,902 INFO INFO  [RecoveryStrategy] (RecoveryThread) Starting recovery process.  core=myCollection recoveringAfterStartup=false\n\nSolr Version: 4.2.1.2013.03.26.08.26.55\n\nOther references to the same issue:\n\n\n\thttps://support.lucidworks.com/entries/23553611-Solr-cluster-not-able-to-recover\n\thttp://mail-archives.apache.org/mod_mbox/lucene-solr-user/201306.mbox/%3C1371473296754-4070983.post@n3.nabble.com%3E",
    "attachments": {
        "stack2": "https://issues.apache.org/jira/secure/attachment/12609463/stack2",
        "stack7": "https://issues.apache.org/jira/secure/attachment/12609468/stack7",
        "stack1": "https://issues.apache.org/jira/secure/attachment/12609462/stack1",
        "stack3": "https://issues.apache.org/jira/secure/attachment/12609464/stack3",
        "stack5": "https://issues.apache.org/jira/secure/attachment/12609466/stack5",
        "stack6": "https://issues.apache.org/jira/secure/attachment/12609467/stack6",
        "SOLR-5373.patch": "https://issues.apache.org/jira/secure/attachment/12609960/SOLR-5373.patch",
        "stack4": "https://issues.apache.org/jira/secure/attachment/12609465/stack4"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Javier Mendez",
            "id": "comment-13800812",
            "date": "2013-10-21T17:08:41+0000",
            "content": "I've attached different thread dumps from Node B when it enters the loop. Each thread dump is 10-15 seconds separate. You can see how it calls 'rejoinLeaderElection' recursively. Stack trace continues to grow indefinitely. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13800827",
            "date": "2013-10-21T17:18:23+0000",
            "content": "There have been a lot of bug fixes in this area since 4.2. Solr 4.5.1 should be out very soon, could you possibly try with that release?\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13803573",
            "date": "2013-10-23T23:58:19+0000",
            "content": "I think this might be by design?\n\nIf you stop node a before b can recover from it, we can't know that b is up to date. So the cluster won't serve - it wants you to know that you should restart the cluster and make sure node a is involved in the startup if you can so that we don't miss any data. If you decide to restart with just node b, at least you have to make that choice explicitly.\n\nIf you wait for node b to recover and become leader before stopping node a, it should work fine. If you need to be able to survive this exact scenario, you need more replicas. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13803575",
            "date": "2013-10-24T00:02:18+0000",
            "content": "Here is an example unit test that allows you to play around with this.\n\nIf you remove the line that waits for node b to recover when it comes back up, the test will often fail because node a goes down before node b can become the leader. With the line, the test should pass. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13893138",
            "date": "2014-02-06T07:44:36+0000",
            "content": "As noted by Mark, this is by design. "
        }
    ]
}