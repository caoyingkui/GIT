{
    "id": "LUCENE-2553",
    "title": "IOException: read past EOF",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/search"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "3.0.2",
        "resolution": "Not A Problem",
        "status": "Resolved"
    },
    "description": "We have been getting an IOException with the following stack trace:\n\n\n\njava.io.IOException: read past EOF\n\tat org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)\n\tat org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)\n\tat org.apache.lucene.store.IndexInput.readInt(IndexInput.java:69)\n\tat org.apache.lucene.store.IndexInput.readLong(IndexInput.java:92)\n\tat org.apache.lucene.index.FieldsReader.doc(FieldsReader.java:218)\n\tat org.apache.lucene.index.SegmentReader.document(SegmentReader.java:901)\n\tat com.cargurus.search.IndexManager$AllHitsUnsortedCollector.collect(IndexManager.java:520)\n\tat org.apache.lucene.search.BooleanScorer2.score(BooleanScorer2.java:275)\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:212)\n\tat org.apache.lucene.search.Searcher.search(Searcher.java:67)\n        ...\n\n\n\n\nWe have implemented a basic custom collector that collects all hits in an unordered manner:\n\n\n    private class AllHitsUnsortedCollector extends Collector {\n\n        private Log logger = LogFactory.getLog(AllHitsUnsortedCollector.class); \n        private IndexReader reader;\n        private int baselineDocumentId;\n        private List<Document> matchingDocuments = new ArrayList<Document>();\n        \n        @Override\n        public boolean acceptsDocsOutOfOrder() {\n            return true;\n        }\n\n        @Override\n        public void collect(int docId) throws IOException {\n\n            int documentId = baselineDocumentId + docId;\n            Document document = reader.document(documentId, getFieldSelector());\n            \n            if (document == null) {\n                logger.info(\"Null document from search results!\");\n            } else {\n                matchingDocuments.add(document);\n            }\n        }\n\n        @Override\n        public void setNextReader(IndexReader segmentReader, int baseDocId) throws IOException {\n            this.reader = segmentReader;\n            this.baselineDocumentId = baseDocId;\n        }\n\n        @Override\n        public void setScorer(Scorer scorer) throws IOException {\n            // do nothing\n        }\n\n        public List<Document> getMatchingDocuments() {\n            return matchingDocuments;\n        }\n    }\n\n\n\n\nThe exception arises when users perform searches while indexing/optimization is occurring. Our IndexReader is read-only. From the documentation I have read, a read-only IndexReader instance should be immune from any uncommitted index changes and should return consistent results during indexing and optimization. As this exception occurs during indexing/optimization, it seems to me that the read-only IndexReader is somehow stumbling upon the uncommitted content? \n\nThe problem is difficult to replicate as it is sporadic in nature and so far has only occurred in Production.\n\nWe have rebuilt the indexes a number of times, but that does not seem to alleviate the issue.\n\nAny other information I can provide that will help isolate the issue? \n\nThe most likely other possibility is that the Collector we have written is doing something it shouldn't. Any pointers?",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2010-07-23T09:24:02+0000",
            "content": "I think the problem here is that you are re-basing the docId to documentId, but then passing that rebased documentId to the SegmentReader, which is wrong.\n\nInstead you should pass docId when loading documents from the segment reader.\n\nIt's fine (if not performant) to load docs from within a custom Collector.\n\nThough it's not great you get a cryptic \"read past EOF\" instead of \"docID is out of bounds for this IndexReader\". ",
            "author": "Michael McCandless",
            "id": "comment-12891531"
        },
        {
            "date": "2010-07-23T18:22:31+0000",
            "content": "Gotcha. Thanks for the info, I will make the changes to the docId and let you know if it comes up again. I do have some questions relating to your comments:\n\n\n\tYou say it's not performant (the documentation says the same but no explanation as to why). What I find unclear is that the API for IndexSearcher only provides doc(...) methods for pulling elements out one at a time. If I were to store the re-based ids and only load them after all the ids have been collected, I would expect there to be a batch doc(Set<Integer>) to which I would ascribe performance improvements over iterating over every collected document id. What exactly makes loading the document ids faster outside of the Collector? Perhaps is there the risk that the same rebased document id may be collected twice during a search?\n\tIt would be great if the documentation for Collector were to be enhanced to answer this question and provide some pointers to other people who may have needs for a bare-bones simple Collector like the one I mentioned above. Would you like me to create a JIRA task for this?\n\n\n\nAnyhoo, thanks for your help! ",
            "author": "Kyle L.",
            "id": "comment-12891707"
        },
        {
            "date": "2010-07-24T14:31:03+0000",
            "content": "The IndexReader/Searcher.document call, itself, isn't that performant, regardless of whether you call it inside a custom Collector or outside.  If you need random-access to certain field(s) across all docs it's best to use FieldCache.DEFAULT.getXXX instead. ",
            "author": "Michael McCandless",
            "id": "comment-12891984"
        },
        {
            "date": "2010-08-02T14:53:43+0000",
            "content": "Custom Collector was using the re-based id to load the document from the SegmentReader. When I switched over to using the raw id, the issue has not resurfaced. It would be nice to have a more descriptive error message though. ",
            "author": "Kyle L.",
            "id": "comment-12894561"
        }
    ]
}