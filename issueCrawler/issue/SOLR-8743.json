{
    "id": "SOLR-8743",
    "title": "Data loss on shard recovery and leader election",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "5.5",
        "status": "Reopened",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "SolrCloud with 3 external ZK nodes in quorum, 2 data nodes (NodeA and NodeB).  Single collection that has a single shard and a single replica (replication factor 2).  The primary shard is initially on NodeA and the replica on NodeB.\n\n1.  Index Doc1 via NodeA\n\n./solr/bin/post -c people -d '<add><doc><field name=\"id\">1</field></doc></add>'\n\n2.  Shutdown NodeA, NodeB becomes the primary. Doc1 is searchable.\n3.  Delete Doc1 and index Doc2 via NodeB.\n\n./solr/bin/post -c people -d \"<delete><id>1</id></delete>\"\n./solr/bin/post -c people -d '<add><doc><field name=\"id\">2</field></doc></add>'\n\n4.  Shutdown NodeB, no nodes online.  ZK still up.\n5.  Start NodeA, it remains in \"down\" status since NodeB is down and it was last seen as the primary.\n6.  Start NodeB, it comes online in  \"down\" status.  NodeA is elected leader and sync starts.  Doc2 is gone, Doc1 exists in both shards.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2016-02-28T00:12:58+0000",
            "author": "Jan H\u00f8ydahl",
            "content": "This is expected and by design\nWhat you have here is two failures in a row, first node A fails, then node B fails.\nIf your requirement is to handle one failed node, then two replicas is sufficient.\nBut if your requirement is to handle two failures at the same time, you need to design for N+2 redundancy, i.e. three replicas.\n\nSo the real bug here is not in Solr, but in the architect's translation of the customers requirements into a too weak SolrCloud cluster. ",
            "id": "comment-15170769"
        },
        {
            "date": "2016-02-28T00:32:31+0000",
            "author": "Jan H\u00f8ydahl",
            "content": "Marked this as \"Not a bug\" for now. But then I also recall a recent JIRA (which I cannot find now) that talked about waiting for some time to allow the last seen leader to come online and thus avoid data loss. You do not mention the timing between up/down here. That may perhaps have a say in whether B should be allowed to come up and resume as leader or not. Feel free to reopen if you still have reason to believe that this is a bug in the election. ",
            "id": "comment-15170773"
        },
        {
            "date": "2016-02-28T00:42:53+0000",
            "author": "Jan H\u00f8ydahl",
            "content": "Reopening myself, as I probably jumped to conclusions, not paying enough attention to the details in steps 5 & 6. Sorry for that. Will let someone with deeper detail insight in this area make the call for proper resolution. ",
            "id": "comment-15170775"
        },
        {
            "date": "2016-02-28T03:01:16+0000",
            "author": "Matt Weber",
            "content": "Thank you for reopening, I feel it is a bug in step 6.  In step 5 when NodeA comes back online, it stays down (I assume) because it knows NodeB should be the primary due to the state in ZK that never went down.  In step 6 when NodeB comes back online, it should pick NodeB as the primary and sync from there, however it appears to pick the first node that came online even if it has stale data.  If step 6 never happens, I think it should stay down forever or until the user forces it back online. ",
            "id": "comment-15170810"
        }
    ]
}