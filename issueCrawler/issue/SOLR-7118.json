{
    "id": "SOLR-7118",
    "title": "ChaosMonkeyNothingIsSafeTest fails with too many update fails",
    "details": {
        "components": [
            "SolrCloud",
            "Tests"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.3",
            "6.0"
        ],
        "affect_versions": "5.0",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "There are frequent failures on both trunk and branch_5x with the following message:\n\njava.lang.AssertionError: There were too many update fails - we expect it can happen, but shouldn't easily\n\tat __randomizedtesting.SeedInfo.seed([786DB0FD42626C16:F98B3EE5353D0C2A]:0)\n\tat org.junit.Assert.fail(Assert.java:93)\n\tat org.junit.Assert.assertTrue(Assert.java:43)\n\tat org.junit.Assert.assertFalse(Assert.java:68)\n\tat org.apache.solr.cloud.ChaosMonkeyNothingIsSafeTest.doTest(ChaosMonkeyNothingIsSafeTest.java:224)\n\tat org.apache.solr.BaseDistributedSearchTestCase.testDistribSearch(BaseDistributedSearchTestCase.java:878)",
    "attachments": {
        "SOLR-7118.patch": "https://issues.apache.org/jira/secure/attachment/12699219/SOLR-7118.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-02-16T13:53:03+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "A few of the latest failures:\nhttp://jenkins.thetaphi.de/job/Lucene-Solr-5.0-Linux/151/\nhttp://jenkins.thetaphi.de/job/Lucene-Solr-5.x-Windows/4375/\nhttp://jenkins.thetaphi.de/job/Lucene-Solr-5.x-MacOSX/1902/ ",
            "id": "comment-14322774"
        },
        {
            "date": "2015-02-16T15:41:10+0000",
            "author": "Mark Miller",
            "content": "Looks like they are due to: org.apache.solr.common.SolrException: Not enough nodes to handle the request ",
            "id": "comment-14322897"
        },
        {
            "date": "2015-02-16T15:50:25+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Yeah, I see many request failing due to stale state and then \"Not enough nodes to handle the request\". I don't think we should mark node(s) as zombies if we exceed max retries on stale state. ",
            "id": "comment-14322914"
        },
        {
            "date": "2015-02-17T07:53:04+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Some of the failures have the following:\n\n14999 T100 C3 P57272 oasc.SolrException.log ERROR org.apache.solr.common.SolrException: ERROR: [doc=ft1-17] unknown field 'a_i1'\n\t\tat org.apache.solr.update.DocumentBuilder.toDocument(DocumentBuilder.java:183)\n\t\tat org.apache.solr.update.AddUpdateCommand.getLuceneDocument(AddUpdateCommand.java:78)\n\t\tat org.apache.solr.update.DirectUpdateHandler2.addDoc0(DirectUpdateHandler2.java:237)\n\t\tat org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:163)\n\t\tat org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:69)\n\t\tat org.apache.solr.update.proce\n\n\n\nI don't know when it started happening but it looks like this test never adds a document at all? It happens because the test has a FullThrottleStopableIndexingThread class which extends StopableIndexingThread but the field i1 in StopableIndexingThread is private and the class actually ends up picking AbstractFullDistribZkTestBase.i1 instead. ",
            "id": "comment-14323789"
        },
        {
            "date": "2015-02-17T08:17:32+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "\n\tMake sure the test has at least 1 jetty to kill (similar to the fix we did to ChaosMonkeySafeLeaderTest)\n\tMake StopableIndexThread's static fields package private so that they are visible to the sub-class\n\tChanged a few log lines from System.err to log.info\n\n ",
            "id": "comment-14323909"
        },
        {
            "date": "2015-02-17T08:21:24+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1660313 from shalin@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1660313 ]\n\nSOLR-7118: Use the right schema fields to avoid spurious failures and other misc fixes ",
            "id": "comment-14323917"
        },
        {
            "date": "2015-02-17T08:23:56+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1660314 from shalin@apache.org in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1660314 ]\n\nSOLR-7118: Use the right schema fields to avoid spurious failures and other misc fixes ",
            "id": "comment-14323920"
        },
        {
            "date": "2015-02-17T14:14:55+0000",
            "author": "Mark Miller",
            "content": "But this test doesn't need at least one replica to kill like the safe leader test. It will kill leaders. ",
            "id": "comment-14324212"
        },
        {
            "date": "2015-02-17T15:27:23+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "But this test doesn't need at least one replica to kill like the safe leader test. It will kill leaders.\n\nLook at ChaosMonkey.getRandomJetty() which calls checkIfKillIsLegal. If the number of active shards in a slice is less than 2 then ChaosMonkey doesn't kill a node at all.\n\nI don't know when it started happening but it looks like this test never adds a document at all?\n\nAnswering myself, actually the FullThrottleStopableIndexingThread is only used sometimes so the test does work but not always.\n\nYeah, I see many request failing due to stale state and then \"Not enough nodes to handle the request\". I don't think we should mark node(s) as zombies if we exceed max retries on stale state.\n\nThis idea of mine was wrong. The node isn't being marked as a zombie i.e. it has nothing to do with LbHttpSolrClient. There's something else at play here. Still digging. ",
            "id": "comment-14324304"
        },
        {
            "date": "2015-02-17T19:20:10+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Okay, here's what's happening at http://jenkins.thetaphi.de/job/Lucene-Solr-5.0-Linux/151/consoleText\n\n\n\tcollection has numShards=1 and replicationFactor=2\n\tcollection1 has stateFormat=2 so CloudSolrClient will cache cluster state and update when the server returns a stale state error\n\tChaosMonkey kills the leader when state version = 7\n\tThe leader publishes itself as down and unsets the leader flag. Overseer writes new state(s) and new state version = 9\n\tA Query request returns stale state error. CloudSolrClient evicts cached state and retries after refreshing+caching the state from ZK\n\n   [junit4]   2> 1646950 T8921 oascsi.CloudSolrClient.requestWithRetryOnStaleState ERROR Request to collection collection1 failed due to (510) org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://127.0.0.1:38608/collection1: STATE STALE: collection1:7valid : false, retry? 0\n   [junit4]   2> 1646951 T8921 oascsi.CloudSolrClient.requestWithRetryOnStaleState WARN Re-trying request to  collection(s) collection1 after stale state error from server.\n   [junit4]   2> 1646954 T8903 C3533 P38608 oasc.SolrCore.execute [collection1] webapp= path=/select params={q=to+come&_stateVer_=collection1:9&wt=javabin&version=2} hits=0 status=0 QTime=1 \n\n\n\tAn indexing request comes in but there is no leader in the cached state. Every subsequent indexing request will continue to fail until the cache entry expires i.e. 60 seconds\n\n   [junit4]   2> 1647581 T8920 oascsi.CloudSolrClient.requestWithRetryOnStaleState ERROR Request to collection collection1 failed due to (510) org.apache.solr.common.SolrException: Not enough nodes to handle the request, retry? 0\n   [junit4]   2> 1647582 T8920 oascsi.CloudSolrClient.requestWithRetryOnStaleState WARN Re-trying request to  collection(s) collection1 after stale state error from server.\n   [junit4]   2> 1647583 T8920 oascsi.CloudSolrClient.requestWithRetryOnStaleState ERROR Request to collection collection1 failed due to (510) org.apache.solr.common.SolrException: Not enough nodes to handle the request, retry? 1\n   [junit4]   2> 1647583 T8920 oascsi.CloudSolrClient.requestWithRetryOnStaleState WARN Re-trying request to  collection(s) collection1 after stale state error from server.\n   [junit4]   2> 1647584 T8920 oascsi.CloudSolrClient.requestWithRetryOnStaleState ERROR Request to collection collection1 failed due to (510) org.apache.solr.common.SolrException: Not enough nodes to handle the request, retry? 2\n   [junit4]   2> 1647584 T8920 oascsi.CloudSolrClient.requestWithRetryOnStaleState WARN Re-trying request to  collection(s) collection1 after stale state error from server.\n   [junit4]   2> 1647585 T8920 oascsi.CloudSolrClient.requestWithRetryOnStaleState ERROR Request to collection collection1 failed due to (510) org.apache.solr.common.SolrException: Not enough nodes to handle the request, retry? 3\n   [junit4]   2> 1647585 T8920 oascsi.CloudSolrClient.requestWithRetryOnStaleState WARN Re-trying request to  collection(s) collection1 after stale state error from server.\n   [junit4]   2> 1647586 T8920 oascsi.CloudSolrClient.requestWithRetryOnStaleState ERROR Request to collection collection1 failed due to (510) org.apache.solr.common.SolrException: Not enough nodes to handle the request, retry? 4\n   [junit4]   2> 1647587 T8920 oascsi.CloudSolrClient.requestWithRetryOnStaleState WARN Re-trying request to  collection(s) collection1 after stale state error from server.\n   [junit4]   2> 1647588 T8920 oascsi.CloudSolrClient.requestWithRetryOnStaleState ERROR Request to collection collection1 failed due to (510) org.apache.solr.common.SolrException: Not enough nodes to handle the request, retry? 5\n   [junit4]   2> REQUEST FAILED for id=0-107\n   [junit4]   2> org.apache.solr.client.solrj.SolrServerException: org.apache.solr.common.SolrException: Not enough nodes to handle the request\n   [junit4]   2> \tat org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:873)\n   [junit4]   2> \tat org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:866)\n   [junit4]   2> \tat org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:866)\n   [junit4]   2> \tat org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:866)\n   [junit4]   2> \tat org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:866)\n   [junit4]   2> \tat org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:866)\n   [junit4]   2> \tat org.apache.solr.client.solrj.impl.CloudSolrClient.request(CloudSolrClient.java:738)\n   [junit4]   2> \tat org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:124)\n   [junit4]   2> \tat org.apache.solr.cloud.StopableIndexingThread.indexDoc(StopableIndexingThread.java:174)\n   [junit4]   2> \tat org.apache.solr.cloud.StopableIndexingThread.indexr(StopableIndexingThread.java:158)\n   [junit4]   2> \tat org.apache.solr.cloud.StopableIndexingThread.run(StopableIndexingThread.java:103)\n   [junit4]   2> Caused by: org.apache.solr.common.SolrException: Not enough nodes to handle the request\n   [junit4]   2> \tat org.apache.solr.client.solrj.impl.CloudSolrClient.sendRequest(CloudSolrClient.java:1002)\n   [junit4]   2> \tat org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:795)\n   [junit4]   2> \t... 10 more\n   [junit4]   2> ROOT CAUSE for id=0-107\n   [junit4]   2> org.apache.solr.common.SolrException: Not enough nodes to handle the request\n   [junit4]   2> \tat org.apache.solr.client.solrj.impl.CloudSolrClient.sendRequest(CloudSolrClient.java:1002)\n   [junit4]   2> \tat org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:795)\n   [junit4]   2> \tat org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:866)\n   [junit4]   2> \tat org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:866)\n   [junit4]   2> \tat org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:866)\n   [junit4]   2> \tat org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:866)\n   [junit4]   2> \tat org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:866)\n   [junit4]   2> \tat org.apache.solr.client.solrj.impl.CloudSolrClient.request(CloudSolrClient.java:738)\n   [junit4]   2> \tat org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:124)\n   [junit4]   2> \tat org.apache.solr.cloud.StopableIndexingThread.indexDoc(StopableIndexingThread.java:174)\n   [junit4]   2> \tat org.apache.solr.cloud.StopableIndexingThread.indexr(StopableIndexingThread.java:158)\n   [junit4]   2> \tat org.apache.solr.cloud.StopableIndexingThread.run(StopableIndexingThread.java:103)\n\n\n\n ",
            "id": "comment-14324726"
        },
        {
            "date": "2015-02-18T09:27:11+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "An indexing request comes in but there is no leader in the cached state. Every subsequent indexing request will continue to fail until the cache entry expires i.e. 60 seconds\n\nReading the code again I realized that when this condition happens, the cached state is evicted. So, the above statement isn't true. What's really happening here is that in the time it takes for leader election, many updates are rejected (as they should be) and the test fails saying too many updates failed. \n\nFrom the logs,\n\n\tThe leader election process logs the following at time=1646937:\n\n[junit4]   2> 1646937 T8923 oasc.ShardLeaderElectionContext.runLeaderProcess I may be the new leader - try and sync\n\n\n\tThen it goes to sleep for 2500 ms to wait for ongoing updates to finish (see ShardLeaderElectionContext.runLeaderProcess)\n\tBy the time it wakes up at time=1649437, the monkey is finished and the test is being teared down\n\n[junit4]   2> 1648644 T8922 oasc.ChaosMonkey.monkeyLog monkey: finished\n   [junit4]   2> 1648645 T8922 oasc.ChaosMonkey.monkeyLog monkey: I ran for 7.931sec. I stopped 1 and I started 0. I also expired 0 and caused 0 connection losses\n   [junit4]   2> added docs:123 with 24 fails deletes:44\n   [junit4]   2> num searches done:3 with 0 fails\n   [junit4]   2> ASYNC  NEW_CORE C3537 name=collection1 org.apache.solr.core.SolrCore@1f81ebc url=http://127.0.0.1:38608/collection1 node=127.0.0.1:38608_ C3537_STATE=coll:collection1 core:collection1 props:{core=collection1, base_url=http://127.0.0.1:38608, node_name=127.0.0.1:38608_, state=active}\n   [junit4]   2> 1649437 T8923 C3537 P38608 oasc.SyncStrategy.sync Sync replicas to http://127.0.0.1:38608/collection1/\n\n\n\n\n\nIn summary, there is no bug, just another spurious failure because the test is tolerant of an arbitrary number of failures. ",
            "id": "comment-14325641"
        },
        {
            "date": "2015-02-18T16:27:38+0000",
            "author": "Mark Miller",
            "content": "In summary, there is no bug, just another spurious failure because the test is tolerant of an arbitrary number of failures.\n\nThe point of that failure is to alert us that we are seeing more failures than before. It's an arbitrary threshold that at one time was enough to cover the failures we see.\n\nWe don't want that test to keep passing when all of sudden 90% of the updates fail. We want to know why all of a sudden there are more failures. If it's reasonable why things changed (is it?), then the threshold can be raised.\n ",
            "id": "comment-14326139"
        },
        {
            "date": "2015-02-18T17:33:04+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "\nThe point of that failure is to alert us that we are seeing more failures than before. It's an arbitrary threshold that at one time was enough to cover the failures we see.\n\nWe don't want that test to keep passing when all of sudden 90% of the updates fail. We want to know why all of a sudden there are more failures. If it's reasonable why things changed (is it?), then the threshold can be raised.\n\nNo, I totally get why the test is written like that. The failures due to the full throttle test do not count towards the indexing failures so I don't see why things should have changed. I'll add this issue number to the failure message for easier tracking.\n\nI'd love to find a better way to test this, though I can't think of one right now. Chasing such false negatives wastes too much time and energy. ",
            "id": "comment-14326233"
        },
        {
            "date": "2015-02-18T17:56:29+0000",
            "author": "Mark Miller",
            "content": "The failures due to the full throttle test \n\nI have some other improvements for this test in the issue around a down shard still being able to elect a leader. Not sure if they overlap. There are a couple problems (like the concurrent solr server shutting down the http connection pool early sometimes). I'll try and pull them out eventually.\n\nChasing such false negatives wastes too much time and energy.\n\nIt's part of making this test good unfortunately. You don't get good complicated tests without a lot of effort. And this is a very complicated integration test. Given the low effort that has gone into these tests, there is still a lot of time and energy that they will suck before they are good. They are the most critical tests we have in SolrCloud and also some of the least mature. ",
            "id": "comment-14326264"
        },
        {
            "date": "2015-03-30T00:37:10+0000",
            "author": "Ramkumar Aiyengar",
            "content": "This seems to be occurring all the more frequently since the Jenkins hardware update \u2013 perhaps due to more updates being pushed through during election time, or because of generally more test runs happening every day.. ",
            "id": "comment-14386051"
        },
        {
            "date": "2015-04-25T19:59:28+0000",
            "author": "Mark Miller",
            "content": "I'm just going to raise it for now. ",
            "id": "comment-14512679"
        },
        {
            "date": "2015-04-25T20:01:23+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1676062 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1676062 ]\n\nSOLR-7118: Raise fail tolerance. ",
            "id": "comment-14512680"
        },
        {
            "date": "2015-04-25T20:07:54+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1676063 from Mark Miller in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1676063 ]\n\nSOLR-7118: Raise fail tolerance. ",
            "id": "comment-14512682"
        },
        {
            "date": "2015-05-27T16:41:23+0000",
            "author": "Mark Miller",
            "content": "Saw another errant fail on jenkins cluster - much, much rarer at least - going to raise again. ",
            "id": "comment-14561241"
        },
        {
            "date": "2015-05-29T14:22:25+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1682483 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1682483 ]\n\nSOLR-7118: Raise fail tolerance. ",
            "id": "comment-14564881"
        },
        {
            "date": "2015-05-29T14:23:32+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1682485 from Mark Miller in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1682485 ]\n\nSOLR-7118: Raise fail tolerance. ",
            "id": "comment-14564883"
        },
        {
            "date": "2015-08-26T13:05:59+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Bulk close for 5.3.0 release ",
            "id": "comment-14713174"
        }
    ]
}