{
    "id": "SOLR-5944",
    "title": "Support updates of numeric DocValues",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "6.5",
            "7.0"
        ],
        "components": [],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "LUCENE-5189 introduced support for updates to numeric docvalues. It would be really nice to have Solr support this.",
    "attachments": {
        "TestStressInPlaceUpdates.eb044ac71.beast-167-failure.stdout.txt": "https://issues.apache.org/jira/secure/attachment/12804506/TestStressInPlaceUpdates.eb044ac71.beast-167-failure.stdout.txt",
        "hoss.62D328FA1DEA57FD.fail.txt": "https://issues.apache.org/jira/secure/attachment/12804560/hoss.62D328FA1DEA57FD.fail.txt",
        "TestStressInPlaceUpdates.eb044ac71.failures.tar.gz": "https://issues.apache.org/jira/secure/attachment/12804665/TestStressInPlaceUpdates.eb044ac71.failures.tar.gz",
        "defensive-checks.log.gz": "https://issues.apache.org/jira/secure/attachment/12827956/defensive-checks.log.gz",
        "hoss.62D328FA1DEA57FD.fail2.txt": "https://issues.apache.org/jira/secure/attachment/12804561/hoss.62D328FA1DEA57FD.fail2.txt",
        "SOLR-5944.patch": "https://issues.apache.org/jira/secure/attachment/12638550/SOLR-5944.patch",
        "DUP.patch": "https://issues.apache.org/jira/secure/attachment/12796129/DUP.patch",
        "master-vs-5944-regular-updates.png": "https://issues.apache.org/jira/secure/attachment/12849244/master-vs-5944-regular-updates.png",
        "hoss.D768DD9443A98DC.fail.txt": "https://issues.apache.org/jira/secure/attachment/12804559/hoss.D768DD9443A98DC.fail.txt",
        "hoss.62D328FA1DEA57FD.fail3.txt": "https://issues.apache.org/jira/secure/attachment/12804562/hoss.62D328FA1DEA57FD.fail3.txt",
        "TestStressInPlaceUpdates.eb044ac71.beast-587-failure.stdout.txt": "https://issues.apache.org/jira/secure/attachment/12804507/TestStressInPlaceUpdates.eb044ac71.beast-587-failure.stdout.txt",
        "regular-vs-dv-updates.png": "https://issues.apache.org/jira/secure/attachment/12849245/regular-vs-dv-updates.png",
        "hoss.D768DD9443A98DC.pass.txt": "https://issues.apache.org/jira/secure/attachment/12804558/hoss.D768DD9443A98DC.pass.txt",
        "demo-why-dynamic-fields-cannot-be-inplace-updated-first-time.patch": "https://issues.apache.org/jira/secure/attachment/12842536/demo-why-dynamic-fields-cannot-be-inplace-updated-first-time.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-13959156",
            "date": "2014-04-03T20:15:30+0000",
            "content": "Here's an early patch where I'm just attempting to modify DirectUpdateHandler2 to do an in-place update to a document for which the only fields to update are stored=false, indexed=false, docValues=true.\n\nHowever, I'm stuck with the unit test for this. Whenever I run the unit test (added in the patch), about 40% of the times it passes, 40% of the time it fails (the default docvalues are used, not the updated ones) and 20% of the time I'm hitting the lucene3x codecs issue (as noted by Mikhail [0] in LUCENE-5189), even though I've explicitly suppressed \"lucene3x\". When Solr runs as a server, I manually test that I'm able to update the NDVs via the update request handler's REST endpoint and updated values are immediately reflected for sorting queries.\n\nI am wondering if there is some fundamental issue with the approach taken in the patch, or if there's some issue with the test setup (or both). Help and feedback are most welcome! If I can get the tests to pass, I can begin to address various other corner cases and issues with replication etc.\n\n[0] - https://issues.apache.org/jira/browse/LUCENE-5189?focusedCommentId=13958205&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13958205 "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-13959858",
            "date": "2014-04-04T10:41:15+0000",
            "content": "Updating patch to trunk. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-13959903",
            "date": "2014-04-04T12:10:53+0000",
            "content": "Updating the patch, the test passes now! Thanks to Shalin Shekhar Mangar for pointing out that I wasn't suppressing the Lucene40, 41, .. 45 codecs in the test.\nI shall start working on addressing other issues with the patch. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-13961561",
            "date": "2014-04-06T23:02:03+0000",
            "content": "Updated the patch to cover cases for replication of the in-place update. However, the patch still doesn't have tests for replication; working on it. Manually verified: after in-place updating a document in leader, the same document in a replica reflects the in-place update.\n\nTODO: \n1) Write more tests\n2) Support update of non-numeric fields using binaryDocValues "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-13962582",
            "date": "2014-04-08T05:02:50+0000",
            "content": "Updated the patch to add a test for update of numeric docValues in a distributed setup (1 shard, many replicas). "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-13962714",
            "date": "2014-04-08T08:47:45+0000",
            "content": "Added support for the \"inc\" update operator (increment the existing docValue by given amount). "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-13963502",
            "date": "2014-04-08T21:57:39+0000",
            "content": "Updated the patch, added support for updating a docValue using a javascript expression which binds the old docValue, thus allowing users to use simple functions to compute the updated docValue. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13963663",
            "date": "2014-04-09T00:40:08+0000",
            "content": "added support for updating a docValue using a javascript expression which binds the old docValue, thus allowing users to use simple functions to compute the updated docValue\n\nOh, that's awesome, Ishan! I think that should be a general improvement i.e. non-updateable numeric fields can also use this feature. Why don't we separate this feature out into it's own issue? "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-13964694",
            "date": "2014-04-09T21:33:43+0000",
            "content": "Sure, that makes sense. Added SOLR-5979 to track this. \nWill remove the JavaScript part from the patch here and have a consolidated patch for both in SOLR-5979. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-13967718",
            "date": "2014-04-13T02:54:17+0000",
            "content": "Updated the patch with the following changes:\n1. Removed the script based updates\n2. Added support for incrementing docvalues based on uncommitted documents (from transaction log)\n3. Earlier, atomic updates were not carrying forward the non-stored, non-indexed docvalues to the new document. Added a fix for this.\n4. Added tests for 3 and 4. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14171179",
            "date": "2014-10-14T16:30:39+0000",
            "content": "Ishan Chattopadhyaya I checked the last patch. It seems well. Perhaps, it's possible to better integrate it with the current atomic updates. \nI wonder if introducing DISTRIB_INPLACE_UPDATE is really necessary? I just can't see its' usage in tests. \n\nCommitters would you mind to pay attention? It's a great alternative to clunky ExternalFileField.\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14171427",
            "date": "2014-10-14T19:57:34+0000",
            "content": "Hi Mikhail Khludnev, the replicas expect that the leader will send a fully expanded document and they can just add the document to the index as-is. That's not true when we're updating DocValues because we wouldn't want to lookup the stored fields at all if there were no non-updateable fields used in an atomic update request. This is why the DISTRIB_INPLACE_UPDATE is required.\n\nPerhaps, it's possible to better integrate it with the current atomic updates\n\nThat and performance is my only concern at this point. We can commit it if we can deal with the docid lookup in a better/cleaner way. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14171439",
            "date": "2014-10-14T20:04:05+0000",
            "content": "My main concern is correctness... does this patch handle reordered writes on a replica?  If so, how? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14171451",
            "date": "2014-10-14T20:13:20+0000",
            "content": "My main concern is correctness... does this patch handle reordered writes on a replica? If so, how?\n\nIt's been some time since I looked at the patch. If version is passed from leader to replica then should re-ordered updates require any special handling? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14171463",
            "date": "2014-10-14T20:21:24+0000",
            "content": "If version is passed from leader to replica then should re-ordered updates require any special handling?\n\nWell, of course it does, because we're updating docValues only and not the version field so passing it along to a replica also doesn't make a difference. Let me think more on this. Do you have a suggestion? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14171465",
            "date": "2014-10-14T20:24:20+0000",
            "content": "If version is passed from leader to replica then should re-ordered updates require any special handling?\n\nYes, given that real partial updates and reordered updates (as currently implemented) seem fundamentally incompatible.\n\nWhat we need is a test with 1 leader, multiple replicas, and many threads sending updates at the same time (a mix of full documents and partial updates).  My guess is that would fail unless this patch (that I haven't looked at yet) does something tricky that I hadn't thought of. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-14171511",
            "date": "2014-10-14T20:59:03+0000",
            "content": "Thanks for having a look at the patch!\n\nYonik Seeley I didn't tackle the reordered updates case in the patch, and can forsee a problem of correctness, as you pointed out. Since the version field could be a non-docvalue field, I didn't consider sending it in to the replicas as part of the partial document.\n\nDo you think enforcing the version field to be docvalue in order to use in place updates to other docvalue fields would work, whereby an updated the version field can be used in a replica to ensure consistency of reordered updates? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14216393",
            "date": "2014-11-18T16:42:23+0000",
            "content": "I was just chatting with Shalin while we were both at ApacheCon.  In addition to leader->replica reordering issues,\nwe also need to handle realtime-get in the single-node case.  The way to do this is just add the update to the tlog like normal (with some indication that it's a partial update and doesn't contain all the fields).   When /get is invoked and we find an update from the in-memory tlog map for that document, we need to go through the same logic as a soft commit (open a new realtime-searcher and clear the tlog map), and then use the realtime-searcher to get the latest document.\n\nOh, and version will need to use DocValues so it can be updated at the same time of course. "
        },
        {
            "author": "Gopal Patwa",
            "id": "comment-14258857",
            "date": "2014-12-25T21:48:09+0000",
            "content": "not sure if this patch is complete but it would be nice to have this in 5.0 "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-14318313",
            "date": "2015-02-12T14:58:57+0000",
            "content": "Bringing the last patch up to trunk, so that further work can be done on this issue. This still suffers from the potential consistency issue in replicas if updates are reordered, as Yonik mentioned. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-14381291",
            "date": "2015-03-26T03:25:26+0000",
            "content": "Is it possible to not deal with reordering if we put a big fat warning on requirements that the indexing program has to follow and work on other problems later?\n\nI know of a bunch of clients who would be ecstatic to have update-in-place capabilities for DocValues fields even if it required that the indexing program follow specific rules like \"only one update for a particular doc can be sent per indexing request (e.g. solr.add(doclist). It'd be nice for the value to be returned in get and in results list, but even that is secondary I think.\n\nWARNING: I haven't looked at the code so I'm just raising the idea in case people can run with it. It'd also keep poor Ishan from continually having to update the patch...\n\nNo chance at all of putting this in 5.1 of course, even if people like the idea. 5.2 at the earliest. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-14381306",
            "date": "2015-03-26T03:48:18+0000",
            "content": "Thanks for bringing this up. If we're fine with going ahead without dealing with reordered updates case, the current patch could be fine. However, it has the potential to leave the replicas out of sync, if the user is willing to take this risk / informed decision.\n\nI am planning to tackle this more correctly by:\n\n\tMaking _version_ as a dv field.\n\tUpdates that are distributed to the replicas having \"old version\" (if any) as well as the new version of the doc updated.\n\tIf, upon receiving the update on a replica, the doc version on index/tlog is not the \"old version\" (that means we've missed in update in between to the doc, because of reordering), then we can write this update to tlog (and mark it somehow as something we're waiting on) but not actually update the doc in the index until we receive the update whose update \"old version\" is what we are expecting. After doing this (for all pending updates for the doc), we could unmark the documents.\n\tIf we don't receive an update we're waiting for after a reasonable time period, we could mark the replica as down.\n\n\n\nAlso, I had another idea that I like lesser than the above:\nFor each dv field, we could have a dv version field, and update the document partially using just the dv field and its own version field.\nThere maybe some subtleties I haven't fully thought through with this approach. "
        },
        {
            "author": "Shai Erera",
            "id": "comment-14381335",
            "date": "2015-03-26T04:21:02+0000",
            "content": "Making version as a dv field.\n\nI think that's a good idea and can be handled separately, as a precursor to this issue?\n\nFor each dv field, we could have a dv version field, and update the document partially using just the dv field and its own version field.\n\nI think it complicates things. Now there is a _version_ field for the whole document, which isn't updated on NDV update, but a separate dvField_version... seems will get confusing really quickly. And on a whole document update operation, need to remember to reset the dvField_version for every NDV, whether it was updated in the past or not, which also adds unnecessary information to the index. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14381840",
            "date": "2015-03-26T13:07:08+0000",
            "content": "Is it possible to not deal with reordering if we put a big fat warning on requirements that the indexing program has to follow and work on other problems later?\n\nToo hard for us to try and figure out what can result in reordered docs, much less a user.   For example, I think maybe even peer-sync could cause trouble here. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14384245",
            "date": "2015-03-27T17:57:42+0000",
            "content": "I would recommend the solution where _version_ is a dv field and no separate version for each updateable dv field. \n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14384251",
            "date": "2015-03-27T17:59:35+0000",
            "content": "I would recommend the solution where version is a dv field and no separate version for each updateable dv field.\n\n+1 "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-14642875",
            "date": "2015-07-27T15:32:46+0000",
            "content": "Here's a WIP strawman patch.\n\n\tUsing version as docValue field.\n\tTransaction log items have a previous pointer: to point from a partial update document to its previous version of the document in the tlog.\n\tRealtime get works for partial updates.\n\tTODO: Adding reordered updates in a follower replica doesn't work yet. (many of the supporting pieces to do this are there).\n\tTODO: Lots of nocommits, which would require some careful treatment of docvalue types or need some refactoring.\n\tTODO: Tests.\n\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-14937692",
            "date": "2015-09-30T16:40:45+0000",
            "content": "WIP patch, updated to trunk. The logic to deal with reorders in a replica is still TODO. Also, another TODO: this doesn't deal with multivalued docvalues yet. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-14943523",
            "date": "2015-10-05T15:29:29+0000",
            "content": "Made some progress on this.\n\n\tReordered updates working.\n\tAdded test for reordered updates. (TODO: some more needed)\n\tFixed some issues with RealTimeGet of updates from my previous patch.\n\n\n\nNeed to clean up the nocommits, add more tests. Also, need to deal with multivalued docvalues fields: my initial thought is to not support in-place partial updates to multivalued docvalues fields. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-14943570",
            "date": "2015-10-05T15:57:05+0000",
            "content": "bq: my initial thought is to not support in-place partial updates to multivalued docvalues fields\n\n+1, \"progress not perfection\" and all that.\n\nSeriously, there would be sooooo many people thrilled with update-in-place for a non-multiValued field that I think waiting for that support isn't necessary. Of course people will clamor for multiValued support  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14943573",
            "date": "2015-10-05T15:59:23+0000",
            "content": "Whew... this seems really tricky.  I've been diving into the Chaos* fails recently, and at first blush it seems like this would add more complexity to recovery as well (log replays, peer sync, etc?)  What are the implications there?\n\n\nIf, upon receiving the update on a replica, the doc version on index/tlog is not the \"old version\" (that means we've missed in update in between to the doc, because of reordering), then we can write this update to tlog (and mark it somehow as something we're waiting on) but not actually update the doc in the index until we receive the update whose update \"old version\" is what we are expecting. After doing this (for all pending updates for the doc), we could unmark the documents.\nIt seems like we can't return success on an update until that update has actually been applied?\nAlso, what happens to this prevPointer you are writing to the tlog if there is a commit in-between?\n\nAnother approach would be to get rid of update reordering... i.e. ensure that updates are not reordered when sending from leader to replicas. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14943610",
            "date": "2015-10-05T16:24:56+0000",
            "content": "Another \"progress but not perfection\" approach would be to get single-node working and committed and open a new issue for cloud mode support. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-14943615",
            "date": "2015-10-05T16:32:17+0000",
            "content": "Thanks for looking at it.\nWhew... this seems really tricky. I've been diving into the Chaos* fails recently, and at first blush it seems like this would add more complexity to recovery as well (log replays, peer sync, etc?) What are the implications there?\nI need to do the due diligence and write some tests to verify that things will work with log replays and peer sync.\n\nActually, since the following comment, things changed a bit (maybe be simpler?):\nIf, upon receiving the update on a replica, the doc version on index/tlog is not the \"old version\" (that means we've missed in update in between to the doc, because of reordering), then we can write this update to tlog (and mark it somehow as something we're waiting on) but not actually update the doc in the index until we receive the update whose update \"old version\" is what we are expecting. After doing this (for all pending updates for the doc), we could unmark the documents.\n\nChanged the above to the following:\n\nIf, upon receiving the update on a replica, the last doc's __version__ in index/tlog is\n not the \"prevVersion\" of the update (that means we've missed one/more updates in \nbetween to the doc, because of reordering), then we write this in-place update to a temporary\n in-memory buffer and not actually write this to the tlog/index until we receive\n the update whose __version__ is what we are expecting as the prevVersion for the buffered\n update. As buffered updates get written to the tlog/index, they are removed\n from the in-memory buffer.\n\n\n\nThis ensures that the tlog entries are always exactly in the order in which the documents were written.\n\nIt seems like we can't return success on an update until that update has actually been applied?\nGood point, I haven't thought about this. Is it okay to return success if it was written to (at least) the in-memory buffer (which holds these reordered updates)? Of course, that would entail the risk of queries to this replica to return the updated document till before the point at which reordering started.\n\nAlso, what happens to this prevPointer you are writing to the tlog if there is a commit in-between?\nThis prevPointer is just used (in the patch) for RTGs. In the InPlaceUpdateDistribTest, I've introduced commits (with 1/3 probability) in between the re-ordered updates, and the RTG seems to work fine.\n\nAnother approach would be to get rid of update reordering... i.e. ensure that updates are not reordered when sending from leader to replicas.\nSounds interesting. How do you suggest can this be achieved? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14943734",
            "date": "2015-10-05T17:50:58+0000",
            "content": "I need to do the due diligence and write some tests to verify that things will work with log replays and peer sync.\n\nYeah, things are tricky enough in this area (distributed updates / recovery in general) that one can't really validate through tests.  Need to brainstorm and think through all of the different failure scenarios and then try to use tests to uncover scenarios you hadn't considered.\n\nThis prevPointer is just used (in the patch) for RTGs. In the InPlaceUpdateDistribTest, I've introduced commits (with 1/3 probability) in between the re-ordered updates, and the RTG seems to work fine.\n\nAh, that makes sense now (for standalone / leader at least).\n\nOff the top of my head, here's a possible issue:\n\n\treplica buffers a partial update in memory (because it was reordered)\n\ta commit comes in, and we roll over to a new tlog\n\tnode goes down and then comes back up, and the in-memory update is no longer in memory, and the old tlog won't be replayed.  It will look like we applied that update.\n\n\n\nIs it okay to return success if it was written to (at least) the in-memory buffer (which holds these reordered updates)?\n\nI don't think so... another scenario:\n\n\ta client does an in-place update\n\tthe replicas receive the update reordered, and buffer in memory.\n\tthe client gets the response\n\tthe leader goes down (and stays down... hard drive crash)\n\tone of the other replicas takes over as leader, but we've now lost data we confirmed as written (the in-place update was only ever applied on the leader), even though we only lost 1 server.\n\n\n\nAnd then there is the even simpler scenario I think you were alluding to: if an update is ack'd, then a RTG on any active replica should see that update (or a later one).  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14943754",
            "date": "2015-10-05T18:01:51+0000",
            "content": "> Another approach would be to get rid of update reordering... i.e. ensure that updates are not reordered when sending from leader to replicas.\nSounds interesting. How do you suggest can this be achieved?\n\nDon't reorder updates between leader and replicas:\n\n\tcreate a new ConcurrentUpdateSolrClient that uses a single channel and can return individual responses... perhaps this fits into HTTP/2 ?\n\thave only a single SolrClient on the leader talk to each replica\n\torder the udpates in _version_ order when sending\n\t\n\t\tprob multiple ways to achieve this... reserve a slot when getting the version, or change versions so that they are contiguous so we know if we are missing one.\n\t\n\t\n\n\n\nThe only additional reason to use multiple threads when sending is to increase indexing performance.  We can also implement multi-threading for increased parallelism on the server side.  This should also simplify clients (no more batching, multiple threads, etc), as well as make our general recovery system more robust. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-14946804",
            "date": "2015-10-07T12:58:58+0000",
            "content": "I was thinking on changing the approach to make sure such inconsistencies (during recovery, failures etc.) can be avoided. Essentially, I'm thinking of changing the approach from buffering the inplace updates (as in the last patch) to, instead, letting leader threads, carrying out of order updates, to wait till the dependent update has been applied on the replica before the replica writes the current inplace update to its tlog/index and only then getting an ACK.\nI think, though I may be missing something, the reordered updates would be a rare occurrence and the delay for the required update to arrive will be in order of milliseconds, and hence there wouldn't be too much of an overhead to waiting on those rare occasions.\nMy initial thought is that doing this will not add any more complexity to log replays as of today, and the replicas will stay in sync.\nYonik Seeley What do you think?\n\nDon't reorder updates between leader and replicas:\nProvided the above approach (or some simple variant thereof) works, maybe we don't absolutely need to do this for supporting inplace updates? I think we explore this anyway, irrespective of this current issue. I can open another issue for doing this. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-14946806",
            "date": "2015-10-07T12:59:45+0000",
            "content": "Here's the approach I'm talking about, in details (also for the benefit of someone who hasn't followed the previous approaches/discussions closely):\n\n\tMake _version_ as a docValues field, indexed=false, stored=false SOLR-6337.\n\tWhen an inplace update command (which syntactically looks like an atomic update) is received on the leader, the leader\n\t\n\t\tversions the update and applies it to its index,\n\t\tobtains the prevVersion as the _version_ of the document on which the update was applied to,\n\t\twrites the inplace update (only the partial document) in its tlog along with a prevPointer, corresponding to the tlog entry of the document with prevVersion or -1 if such an entry doesn't exist, of the update,\n\t\tforward the update to the other replicas, along with the prevVersion.\n\t\n\t\n\tAt a replica, upon receiving an inplace update, the last version (lets call it lastVersion) for the document id is obtained from the tlog/index.\nNow, \n\t\n\t\tIf this matches the prevVersion, then apply the update, write to tlog and return success to the leader.\n\t\tIf this doesn't match the prevVersion (case of an out of order update), wait (till a timeout) for the update with prevVersion to be written to tlog/index, possibly in other threads, and then apply the current update and return success to the leader. If a timeout has reached, but the dependent update hasn't arrived, a failure is sent back to the leader (which can then mark the replica in recovery).\n\t\n\t\n\tRealTimeGet for documents that have inplace updates can be resolved by successively following a prevPointer in the tlog backwards, either to a full update in tlog or to the index.\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14946831",
            "date": "2015-10-07T13:14:43+0000",
            "content": "If this doesn't match the prevVersion (case of an out of order update), wait (till a timeout) for the update with prevVersion to be written to tlog/index, possibly in other threads,\nLooks like this needs to be nailed in more detail. What exactly happens to other writes when an out of order update reaches a replica?. Will the proposed change have any impact on the performance for users who do not use this feature?\n\nDon't reorder updates between leader and replicas: create a new ConcurrentUpdateSolrClient that uses a single channel and can return individual responses...\n\nThis is probably beyond the scope of this ticket. If the solution proposed by Ishan Chattopadhyaya is fine (looks fine to me) we should go ahead with his solution and tackle the other stuff in a separate ticket.  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14946974",
            "date": "2015-10-07T14:52:50+0000",
            "content": "I think, though I may be missing something, the reordered updates would be a rare occurrence and the delay for the required update to arrive will be in order of milliseconds, and hence there wouldn't be too much of an overhead to waiting on those rare occasions.\n\nIt's not so rare if multiple clients are updating the same doc.  Milliseconds can be pushed to many seconds due to GC, etc.  Also, it gets very tricky if a replica goes down...  I haven't thought through that part.  What happens if the dependent update fails on one shard?\n\ninstead, letting leader threads, carrying out of order updates, to wait till the dependent update has been applied on the replica\n\n\"leader threads\" meaning they are on the leader?  If so, this is sort of where I was going before (handling more on the leader)... except instead of detecting and waiting for the dependent update, just send the dependent update first (i.e. don't reorder).  Seems easier?\nHmmm, but looking at your follow-up, it sounds more like you're blocking on the replica?\n\n\n> Don't reorder updates between leader and replicas\nThis is probably beyond the scope of this ticket.\n\nIt certainly is beyond the scope of this ticket, but it also would solve a lot of issues.  We need to continue to think about any and all approaches to make this feature bullet-proof.\n\nWill all \"atomic updates\" now be handled this way?  Seems desirable in general.\n\nScanning back over this issue, I happened to see:\n3. Earlier, atomic updates were not carrying forward the non-stored, non-indexed docvalues to the new document. Added a fix for this.\n\nIf it hasn't been done already, this could be broken out into it's own issue and fixed now? "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-14948839",
            "date": "2015-10-08T15:25:24+0000",
            "content": "What happens if the dependent update fails on one shard?\nIf the dependent update (or even the out of order update) failed on a replica (isn't this what you meant by \"shard\"?), I think the leader will put that replica into LIR.\n\n\"leader threads\" meaning they are on the leader? If so, this is sort of where I was going before (handling more on the leader)... except instead of detecting and waiting for the dependent update, just send the dependent update first (i.e. don't reorder). Seems easier? Hmmm, but looking at your follow-up, it sounds more like you're blocking on the replica?\n\nWhat I meant by \"leader threads\" was that since at the leader the DistributedUpdateProcessor distributes these commands to the replicas synchronously [0], the leader thread will remain blocked till the replica will not write and return success/failure. However, my intention was, as you point out, to actually do the wait/\"blocking on the replica\".\n\n[0] - DistributedUpdateProcessor\n\n        for (Node subShardLeader : subShardLeaders) {\n          cmdDistrib.distribAdd(cmd, Collections.singletonList(subShardLeader), params, true);\n        }\n\n\n(the last boolean being for synchronous)\n\nIt certainly is beyond the scope of this ticket, but it also would solve a lot of issues. We need to continue to think about any and all approaches to make this feature bullet-proof.\nTotally agree. I really want to evaluate doing this, even if not immediately (unless we absolutely need it for this feature). If there is not a significant performance loss to using a single channel client to each replica, then this approach would be really very neat.\n\nWill all \"atomic updates\" now be handled this way? Seems desirable in general.\nFor atomic updates, as they stand today, I think sending only partial document to the replicas would help if there are lots of fields as we save the cost of sending and maybe also storage/memory space at the tlog. However, if the other stored fields require very expensive analysis, it might be better to let the leader send the full document to the replicas? Is there something else too which I missed? If we can get in-place updates to work for both numeric and binary docvalues, I think we'll get one step closer to recommending users to do all updates this way.\n\nIf it hasn't been done already, this could be broken out into it's own issue and fixed now?\nSure, good catch. I'll create another issue for this. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-14948865",
            "date": "2015-10-08T15:40:12+0000",
            "content": "Looks like this needs to be nailed in more detail. What exactly happens to other writes when an out of order update reaches a replica?. \nI think those other writes (i.e. writes for other documents) would continue to  happen, since they are being performed using separate threads. As for other writes to the same document, I think the consequence would be that they'll keep getting blocked until the dependent update is written to the replica (after which they'll get unblocked one by one, unless they timeout).\n\nMilliseconds can be pushed to many seconds due to GC, etc. Also, it gets very tricky if a replica goes down...\nDo you guys think it would be reasonable to let user configure this, and document somewhere on the lines of: if you have lots of back of back updates happening on the same documents, then there will be a possibility of threads waiting for other threads, and hence this timeout can be increased, say, to seconds.\n\nWill the proposed change have any impact on the performance for users who do not use this feature?\nI think there will be no impact, since this wait would only affect in-place updates. Unless if we want to make sure there will be no reordering in the first place. "
        },
        {
            "author": "Shai Erera",
            "id": "comment-14948912",
            "date": "2015-10-08T16:07:16+0000",
            "content": "I'll admit that I haven't dug through all the comments on the issue, and I'm not very familiar w/ the internals of Solr document updates, but at least from a Lucene perspective, numeric DV updates occur in order with any other document updates. So, simplifying on purpose, if Solr's tlog already handles document updates in order, and it will now list the updates like so:\n\n\nDOC_UPDATE: { \"id: \"doc1\", \"fields\": {field1, field2 ....}}\nDOC_UPDATE: { \"id: \"doc2\", \"fields\": {field1, field2 ....}}\nNUMERIC_UPDATE: { \"term\" : { \"field\" : \"id\", \"value\" : \"doc1\"}, \"dvField\": 123 }\nDOC_UPDATE: { \"id: \"doc1\", \"fields\": {field1, field2 ....}}\n\n\n\nWhat would cause the second update to 'doc1' apply before the NUMERIC update of 'dvField' of 'doc1'? Also, if the numeric update affects a group of documents, this should be OK still (from a Lucene standpoint).\n\nOr ... is the problem you're discussing a numeric update can affect a group of documents that are located on different shards? If so, doesn't Solr already protect against that, versioning each change?\n\nIf I'm missing something fundamental in how Solr works, then I'd be happy go learn/read about it. I wanted to give the perspective of numeric DV updates from a Lucene standpoint. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14950320",
            "date": "2015-10-09T12:43:04+0000",
            "content": "What would cause the second update to 'doc1' apply before the NUMERIC update of 'dvField' of 'doc1'?\n\nThe fundamental issue is this: the leader can forward updates to a replica over different threads / connections, and can be handled my multiple threads on the replica side.  This leads to many opportunities for update reorders.\n\nSolr does currently detect (via versioning) and handle all of the reorder cases today.  When a document is sent to a replica, the whole thing is sent.  If a replica gets an older version of a document, it can simply drop it.  This type of handling doesn't work when you're talking about re-ordered partial / in-place updates... just dropping old versions doesn't work. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14951183",
            "date": "2015-10-09T20:59:51+0000",
            "content": "Stepping back and looking the high level view of Ishan's last proposal/approach, I think it looks fine.\nOnce we get into true partial udpates, we can't really allow reorders, so they either have to be ordered on the sender side, or the receiver side.  From a complexity POV, once we have to order certain types of updates, it's no more complex to order them all.\n\nIf I could wave a magic wand, we'd have the order-on-the-sender-side approach, because it should also solve known weaknesses like I appear to be hitting in SOLR-8129, but I recognize it would be a lot of work.  Of course, reordering on the receiver side is not going to be a picnic either - the synchronization involved may be quite difficult to get right.\n\nedit: some of the other parts of the design, like the prevPointer & prevVersion seem fine regardless of how other parts of this puzzle are solved. "
        },
        {
            "author": "Shai Erera",
            "id": "comment-14952290",
            "date": "2015-10-11T13:54:37+0000",
            "content": "Thanks Yonik for the clarification. The issue then is that the version field is not 'updateable' right? If it was e.g. a numeric DV field itself, then each update to a document could involve updating both the version field, as well the numeric DV field in question. And regular doc updates would work fine as well.\n\nI remember reading about making the version field a numeric DV for that (or similar) purpose, so apologies if I re-iterate someone else's idea... "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14952494",
            "date": "2015-10-12T00:04:54+0000",
            "content": "The issue then is that the version field is not 'updateable' right?\n\nWell, that was one issue we identified earlier and decided we would move to docvals for that field. It's a good idea in any case. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-14953060",
            "date": "2015-10-12T12:33:50+0000",
            "content": "I am close to a patch for the above proposal, and shall post it soon.\n\nOne place where I am somewhere stuck is in the log buffering/replaying part. Here's the problem:\nWhen a replica is put into recovery by the leader, it comes back up and tries to perform a peersync. This seems to be happening in a two phase process: buffering (where the updates, after being obtained from the leader's tlog, are played back and written to the replica's tlog but not its index/ulog) and replaying (where the tlog is replayed and the updates are written to ulog/index, but not into tlog again). The problem I'm facing is that during this buffering phase, the inplace updates can't find dependent updates if they are not in the index, since the updates are not written to ulog in the buffering phase.\n\nI have two choices at the moment to get around this:\n\n\tDuring a buffering phase, I can keep a separate map of all updates (id to tlog pointer) to be used during and discarded after the buffering phase. That map can help resolve inplace updates that follow. (Pro: fast, Con: memory)\n\tFor every inplace update, I traverse back into the tlog and linearly scan for the required dependent update. (Pro: no memory, Con: Slow / O)\n\n\n\nAt this point, I'm inclined to go for option 1, but I'm wondering if there are any serious downsides to doing this. Any suggestions, please?\nAlso, am I correct in my assumption that the no. of updates processed during this buffering phase will not be more than numUpdatesToKeep?\nIn case I sound confused/unclear, please let me know and I'll post the relevant failing test for this. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-14954891",
            "date": "2015-10-13T13:04:30+0000",
            "content": "Adding patch for the above approach.\n\n\tAdded a test that times out a reordered update, when it exceeds 1s (currently hardcoded), and puts the replica in LIR and subsequently performs a peer sync.\n\tUsed a bufferingMap in UpdateLog, and populated it at DistributedUpdateProcessor at a time when ulog is in buffering state.\n\n\n\nTODOs:\n\n\tRemove the nocommits, and some of the TODOs.\n\tAdd more tests.\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14954900",
            "date": "2015-10-13T13:09:12+0000",
            "content": "Also, am I correct in my assumption that the no. of updates processed during this buffering phase will not be more than numUpdatesToKeep?\n\nThat's a minimum, not a maximum.  There really is no maximum. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-14955031",
            "date": "2015-10-13T14:30:28+0000",
            "content": "Do we have any sort of conditional update yet (or something generic like update-by-script)?  That would be great to help test this stuff out (i.e. to make sure that partial updates aren't reordered with respect to each-other).\n\nIf you're going to be around the conference this week, It would be great to chat about some of this stuff in person... "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-14955620",
            "date": "2015-10-13T20:39:54+0000",
            "content": "Do we have any sort of conditional update yet (or something generic like update-by-script)? That would be great to help test this stuff out (i.e. to make sure that partial updates aren't reordered with respect to each-other).\n\"inc\" is supported, I can try writing a suite that indexes bunch of full documents, \"set\" updates and \"inc\" updates (perhaps along with _version_s for optimistic concurrency) and then test for consistency across replicas. By update-by-script, do you mean something like SOLR-5979 (Alas it is not there yet)?\n\nIf you're going to be around the conference this week, It would be great to chat about some of this stuff in person...\n+1. I'm planning to be around on 15th and 16th. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14956297",
            "date": "2015-10-14T05:25:37+0000",
            "content": "I guess yonik meant something like CAS. Anyway let's meet up during LSR "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-14962099",
            "date": "2015-10-17T22:28:45+0000",
            "content": "Had productive discussions around this at Revolution with Yonik Seeley and Noble Paul. Here's a list of TODO items I am going to work on:\nHere's a summary:\n\n\tExpand the current tests to do more updates, using more threads, something like stress reorder test does. [Yonik]\n\tBefore timing out (in a replica) after waiting for dependent update long enough, instead of throwing an error (which triggers LIR), request from replica to leader to pull a range of updates (the missing ones), i.e. from what last version we have to the version we expect. This can be easily done by using (a slightly modified version of) the RTG's getUpdates handler. [Noble]\n\tSince there are changes to the javabin format (for inplace updates), add tests for backcompat. [Noble]\n\tCode cleanup, fix the todos/nocommits etc.\n\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15033860",
            "date": "2015-12-01T15:23:59+0000",
            "content": "Updating patch, depends on SOLR-8220.\nIf waiting for pending/reordered dependent update fails, it tries to get the updates from the leader now (as per point 2 in above comment). Fixed a few nocommits.\nStill need to add the extensive stress tests, and maybe some more refactoring needed. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15093412",
            "date": "2016-01-12T06:48:39+0000",
            "content": "Updating to trunk. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15143788",
            "date": "2016-02-12T00:40:23+0000",
            "content": "Updating the patch to latest master.\n\n\n\tAdded stress test, TestStressInPlaceUpdates, similar to TestStressReorder\n\tHas lots and lots of unnecessary logging info, which I left there as I tried to reproduce a bug (LUCENE-7025) due ot which the stress test is failing.\n\tLots of nocommits due to formatting/refactoring/unnecessary logging etc.\n\n\n\nCurrently, in the stress test, RTG works, but committed model / main searcher based search seems to be inconsistent, possibly due to the LUCENE-7025 issue or something to do with that. Seems to me that whatever is being written through an IndexWriter is not exactly what I search via searcher at the time when parallel commits are happening. I am still investigating why that could be.\n\nIt seems the problem of reordered updates is solved (in terms of correctness). Also seems the RTG is working properly.\n\nI am currently looking into the searcher inconsistency while commit is happening. Once I get past that, I'll do a serious cleanup of the code and the logging messages I am printing out. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15193478",
            "date": "2016-03-14T15:28:45+0000",
            "content": "Updated the patch to trunk with the following:\n1. Some bug fixes.\n2. Stress test.\n3. Some cleanups.\n4. Fixed nocommits.\n\nNoble Paul, Yonik Seeley, could you please review? Thanks.\n\nThere is one very very rare situation whereby after a commit the recently reopened searcher doesn't see a dv update which was supposed to be in this newly opened searcher. There maybe some issue in having the updates flushed properly at the Lucene level; I have so far been unable to reproduce this during one off plain Lucene tests or Solr tests, but it is exposed only during the Stress tests (once in 2000 runs or so). I'll continue to investigate this, perhaps by writing a similar stress test for plain Lucene level DV updates. Apart from that, I think this patch is feature complete. "
        },
        {
            "author": "Gopal Patwa",
            "id": "comment-15194359",
            "date": "2016-03-14T23:12:42+0000",
            "content": "Great! to see progress and near to completion this feature. can this be part of 6.0 release? "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15213308",
            "date": "2016-03-27T03:29:35+0000",
            "content": "Hi Gopal, sorry to have missed your comment. I think this can land in Solr 6.1 or later, since 6.0 release branch has already been cut. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15213310",
            "date": "2016-03-27T03:32:40+0000",
            "content": "Updating the patch to master, since parts of this patch are now committed as part of SOLR-8831 and SOLR-8865. Would be nice if someone could please review. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-15215802",
            "date": "2016-03-29T10:12:08+0000",
            "content": "In DUP.waitForDependentUpdates() it is not wise to blindly wait for 10 milliseconds. What if the dependent update came right after it entered the sleep() . In that case we are unnecessarily waiting while the thread could proceed immediately  . We can use a wait()/notify() mechanism to avoid that   "
        },
        {
            "author": "Noble Paul",
            "id": "comment-15215960",
            "date": "2016-03-29T12:48:03+0000",
            "content": "use a wait()/notify() instead of sleep() "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15215975",
            "date": "2016-03-29T13:04:02+0000",
            "content": "Thanks Noble, I added your changes to the full patch. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-15219447",
            "date": "2016-03-31T06:26:32+0000",
            "content": "There are a lot of places where a toString() is done and then parsing is done. It much cheaper to do an instanceof check \nlook at the following block in AtomicUpdateDocumentMerger.doInPlaceUpdateMerge()\n\n if (oldValue instanceof Long) {\n                    result = ((Long) oldValue).longValue() + Long.parseLong(value.toString());\n                  } else if (oldValue instanceof Float) {\n                    result = ((Float) oldValue).floatValue() + Float.parseFloat(value.toString());\n                  } else if (oldValue instanceof Double) {\n                    result = ((Double) oldValue).doubleValue() + Double.parseDouble(value.toString());\n                  } else {\n                    // int, short, byte\n                    result = ((Integer) oldValue).intValue() + Integer.parseInt(value.toString());\n                  }\n\n \ncan be optimized as follows\n\n    if (oldValue instanceof Long) {\n                    result = (Long) oldValue + (value instanceof Number ? ((Number) value).longValue() : Long.parseLong(value.toString()));\n                  } else if (oldValue instanceof Float) {\n                    result = (Float) oldValue + (value instanceof Number ? ((Number) value).floatValue() : Float.parseFloat(value.toString()));\n                  } else if (oldValue instanceof Double) {\n                    result = (Double) oldValue + (value instanceof Number ? ((Number) value).doubleValue() : Double.parseDouble(value.toString()));\n                  } else {\n                    // int, short, byte\n                    result = (Integer) oldValue + (value instanceof Number ? ((Number) value).intValue() : Integer.parseInt(value.toString()));\n                  }\n\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15219869",
            "date": "2016-03-31T13:37:01+0000",
            "content": "Thanks Noble, I've added your suggestion and your DUP patch to the full patch. Also did some cleanup, javadocs, removed some unnecessary logging.\n\nThe corresponding change to the patch are: \nhttps://github.com/chatman/lucene-solr/commit/8f38aa663e57335afe5f136139acc2e7293d2a9d\nhttps://github.com/chatman/lucene-solr/commit/f203d770dbdcb36edf472544902bc62bbbc28794 "
        },
        {
            "author": "Noble Paul",
            "id": "comment-15220387",
            "date": "2016-03-31T18:25:05+0000",
            "content": "fixed logging.\n\n\n\tIt's not OK to cache the values of log.isDebugEnabled() etc. These can change in between\n\tif a string is constructed, always do conditional logging\n\tif a template is used upto two variables eg: log.info(\"v1:{}, v2:{}\", v1,v2 ) , it is OK not to do conditional logging, because no new objects are created\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-15220505",
            "date": "2016-03-31T19:17:57+0000",
            "content": "is there a testcase for partial update  as the first operation?  instead of a full insert (like set and inc) "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15220582",
            "date": "2016-03-31T20:00:59+0000",
            "content": "Thats a good point, Noble; there isn't a test, I'll add one. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-15223864",
            "date": "2016-04-04T09:20:41+0000",
            "content": "Ishan Chattopadhyaya I'm done with my review. I have done some minor changes to your private  branch https://github.com/chatman/lucene-solr and branch solr_5944 . We have an issue that causes some data loss very rarely. if that is fixed, I'm +1 to commit this  "
        },
        {
            "author": "Justin Deoliveira",
            "id": "comment-15252850",
            "date": "2016-04-21T22:03:54+0000",
            "content": "I've been following this patch for a while, and am super excited about recent progress. I just applied the latest patch locally and built with maven and it resulted in some forbidden api failures. I don't know of this helps but here is a minor patch that addresses them.  "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15284816",
            "date": "2016-05-16T16:48:35+0000",
            "content": "The only remaining issue is a test failure, which used to happen very intermittently for me with TestStressInPlaceUpdates. Here are the partial logs (from April):\n\n\n  [beaster]   2> NOTE: reproduce with: ant test  -Dtestcase=TestStressInPlaceUpdates -Dtests.method=stressTest -Dtests.seed=3E8431D13FA03373 -Dtests.slow=true -Dtests.locale=sr-Latn -Dtests.timezone=Antarctica/Syowa -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n  [beaster] [18:34:34.713] ERROR   51.8s | TestStressInPlaceUpdates.stressTest <<<\n  [beaster]    > Throwable #1: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=181, name=READER10, state=RUNNABLE, group=TGRP-TestStressInPlaceUpdates]\n  [beaster]    >        at __randomizedtesting.SeedInfo.seed([3E8431D13FA03373:55E2EE7C0175E789]:0)\n  [beaster]    > Caused by: java.lang.RuntimeException: java.lang.AssertionError: Vals are: 3, 2000000008, id=4\n  [beaster]    >        at __randomizedtesting.SeedInfo.seed([3E8431D13FA03373]:0)\n  [beaster]    >        at org.apache.solr.cloud.TestStressInPlaceUpdates$3.run(TestStressInPlaceUpdates.java:416)\n  [beaster]    > Caused by: java.lang.AssertionError: Vals are: 3, 2000000008, id=4\n  [beaster]    >        at org.junit.Assert.fail(Assert.java:93)\n  [beaster]    >        at org.junit.Assert.assertTrue(Assert.java:43)\n  [beaster]    >        at org.apache.solr.cloud.TestStressInPlaceUpdates$3.run(TestStressInPlaceUpdates.java:390)\n\n\n\nHowever, I ran a beast of the same test on a 24 core (48 thread) machine, 1000 rounds, 16 at a time (http://imgur.com/Y2929rI), and the beasting passed. Maybe I'll need to try harder to reproduce it over the week or the coming weekend. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15284838",
            "date": "2016-05-16T17:08:05+0000",
            "content": "Thanks for the forbidden API fixes. I shall incorporate them into the next patch. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15284905",
            "date": "2016-05-16T17:40:26+0000",
            "content": "\nNoble Paul added a comment - 04/Apr/16 02:20\nIshan Chattopadhyaya I'm done with my review. I have done some minor changes to your private branch https://github.com/chatman/lucene-solr and branch solr_5944 . We have an issue that causes some data loss very rarely. if that is fixed, I'm +1 to commit this\n\nThere are so many questions raised by this comment...\n\n\twhat are the minor changes?  This comment was posted long after the most recent patch attached this this issue (on Mar31), implying that whatever the the changes are they didn't make it into any attached patch.  The comments says the changes were made to a github repo on a \"solr_5944\" branch, but the linked repo doesn't have a branch with that name \u2013 it DOES have a branch named \"SOLR-5944\" but the most recent commit on that branch is from Feb \u2013  2 months before this comment was posted (and way older then the most recent patch attached to this issue)\n\twhat is the issue that causes data loss? can you describe the \"rarely\" situation? is there a test that demonstrates the problem? ... is this the same (non-reproducible) test failure that Ishan mentioned in in his comment this morning (May16) or is there some other bug we need to be worried about?\n\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15284971",
            "date": "2016-05-16T18:15:56+0000",
            "content": "this is the most recent patch (attached by Noble on Mar31) updated to master (some hunk offset shifts and one trivial conflict) with Justin's precommit fixes added in.\n\n(precommit & the new test classes pass, still running fulltests to ensure no obvious problems, have not actually reviewed anything personally) "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15285349",
            "date": "2016-05-16T21:28:19+0000",
            "content": "small update \u2013 the previous patch was breaking QueryEqualityTest because of how it abused ValueSourceParser.addParser (SOLR-9119) to add a ValueSource parser to the static list.  If this special ValueSource parser was critical to the test, it could have been registered in the solrconfig.xml used by the test, but it didn't appear to serve any purpose beyond what the DocIdAugmenterFactory already provided, so i replaced the existing usages with that transformer (if anything the old code could have suffered from false positives since the valuesource was returning the segment docId, but the transformer returns the id used by the top level SolrIndexSearcher( "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15285885",
            "date": "2016-05-17T02:14:12+0000",
            "content": "Thanks for looking into the issue, Hoss.\n\nThe comments says the changes were made to a github repo on a \"solr_5944\" branch, but the linked repo doesn't have a branch with that name\n\nhttps://github.com/chatman/lucene-solr/tree/solr_5944\nNoble's last commit is: 4572983839e3943b7dea52a8a2d55aa2b3b5ca3a\n\nwhat is the issue that causes data loss\nWhen it happens, my understanding is that some uncommitted in-place updates don't reflect after a commit in a re-opened searcher.\n\ncan you describe the \"rarely\" situation? is there a test that demonstrates the problem?\nTestStressInPlaceUpdates is the affected test. Used to happen once in around 2000 runs, on a 3Ghz 5960X CPU (I'm unable to reproduce, as I mentioned in my previous comment, on dual 2.0Ghz Xeon E5 2658 V3).\n\nis this the same (non-reproducible) test failure that Ishan mentioned in in his comment this morning (May16)\nhttps://issues.apache.org/jira/browse/SOLR-5944?focusedCommentId=15193478&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15193478\nI think Noble is referring to the same. It could be due to some underlying issue with either how Solr uses Lucene or how Lucene is flushing the dv updates during a commit. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15286986",
            "date": "2016-05-17T16:47:52+0000",
            "content": "https://github.com/chatman/lucene-solr/tree/solr_5944 Noble's last commit is: 4572983839e3943b7dea52a8a2d55aa2b3b5ca3a\n\nUgh ... somehow i completley missed seeing this comment yesterday,  I don't know why i couldn't find that branch on github. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15286987",
            "date": "2016-05-17T16:48:34+0000",
            "content": "oh ... wait ... i see now, it was posted after my other comments/attachments ... but using the \"Reply\" feature so it appears inline. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-15286995",
            "date": "2016-05-17T16:52:31+0000",
            "content": "but using the \"Reply\" feature so it appears inline.\n\nYeah, I've stopped using the Reply thing because of this - you can't find all the new posts at the bottom if people use this misfeature (as Muir called it). "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15287029",
            "date": "2016-05-17T17:11:20+0000",
            "content": "Combined Justin's fixes, Hoss' fixes and Noble's fixes (which were already there), updated to master and committed to the solr_5944 branch (https://github.com/chatman/lucene-solr/tree/solr_5944). Attached the patch for the same here. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15287052",
            "date": "2016-05-17T17:26:58+0000",
            "content": "Yeah, I've stopped using the Reply thing because of this\nI see.. I'll consider stopping the use of the reply feature now  Sorry for the confusion. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-15287487",
            "date": "2016-05-17T20:23:01+0000",
            "content": "I'm beasting 2000 iterations of TestStressInPlaceUpdates with Miller's beast script against https://github.com/chatman/lucene-solr/tree/solr_5944 at revision  eb044ac71 and have so far seen two failures, at iteration 167 and at iteration 587, the stdout from which I'm attaching here. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15287525",
            "date": "2016-05-17T20:45:35+0000",
            "content": "FWIW, I'm testing ishan's latest patch against lucene-solr master and the two \"reproduce\" lines from steve's logs (minus the linedocs path) fail 100% of the time for me on my box - although the specific doc listed in the failure message varies from run to run, presumably because of the parallel threads? ...\n\n\nant test  -Dtestcase=TestStressInPlaceUpdates -Dtests.method=stressTest -Dtests.seed=FFC46C473EC471E6 -Dtests.slow=true -Dtests.locale=sr-ME -Dtests.timezone=Europe/Riga -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n\nant test  -Dtestcase=TestStressInPlaceUpdates -Dtests.method=stressTest -Dtests.seed=15E180DC7142CBF3 -Dtests.slow=true -Dtests.locale=pt-BR -Dtests.timezone=Africa/Juba -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n\n\n "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-15287640",
            "date": "2016-05-17T21:40:30+0000",
            "content": "Both of those seeds (FFC46C473EC471E6 and 15E180DC7142CBF3) reproduce for me too (only tried each one once). \n\nA third beasting failure, run 783, does NOT reproduce for me (0 failures out of 4 runs):\n\n\nant test  -Dtestcase=TestStressInPlaceUpdates -Dtests.method=stressTest -Dtests.seed=CCB5FA74FA9BB974 -Dtests.slow=true -Dtests.locale=sr -Dtests.timezone=Africa/Gaborone -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15287984",
            "date": "2016-05-18T00:50:58+0000",
            "content": "I've been reviewing the logs from sarowe's failures \u2013 I won't pretend to understand half of what i'm looking at here (I'm still not up on most of the new code) but here's some interesting patterns i've noticed...\n\n\n\tin both failure logs posted, doc \"13\" was the doc having problems\n\tthe specific docId is probably just a coincidence, but it does mean that the same egrep command works on both log files to give you the particularly interesting bits realtive to the failure...\n\negrep add=\\\\[13\\|id=13\\|ids=13 TestStressInPlaceUpdates.eb044ac71.beast-167-failure.stdout.txt > beast-167.important.txt\negrep add=\\\\[13\\|id=13\\|ids=13 TestStressInPlaceUpdates.eb044ac71.beast-587-failure.stdout.txt > beast-587.important.txt\n\n\n\tlooking first at beast-587.important.txt:\n\t\n\t\tthe ERROR that failed the test was first logged by READER2 @ (timestamp) 34456:\n\n34456 ERROR (READER2) [    ] o.a.s.c.TestStressInPlaceUpdates Realtime=true, ERROR, id=13 found={response={numFound=1,start=0,docs=[SolrDocument{id=13, title_s=[title13], val1_i_dvo=3, val2_l_dvo=3000000006, _version_=1534607778351415296, ratings=0.0, price=0}]}} model=[1534607780231512064, 3, 3000000012]\n\n\n\t\tWorking backwards, that expected version 1534607780231512064 was logged by WRITER10 as being returned to a PARTIAL update @ 31219:\n\n31219 INFO  (WRITER10) [    ] o.a.s.c.TestStressInPlaceUpdates PARTIAL: Writing id=13, val=[3,3000000012], version=1534607779993485312, Prev was=[3,3000000009].  Returned version=1534607780231512064\n\n\n\t\t\n\t\t\tWRITER10's logging of this \"Returned version=1534607780231512064\" came after core_node1, core_node2, and core_node3 all logged it being written to their TLOG & reported it via LogUpdateProc:\n\n30557 INFO  (qtp2010985731-180) [n:127.0.0.1:37972__m c:collection1 s:shard1 r:core_node1 x:collection1] o.a.s.u.UpdateLog TLOG: added id 13(ver=1534607780231512064, prevVersion=1534607779993485312, prevPtr=2343) to tlog{file=/tmp/beast-tmp-output/587/J0/temp/solr.cloud.TestStressInPlaceUpdates_FFC46C473EC471E6-001/shard-1-001/cores/collection1/data/tlog/tlog.0000000000000000004 refcount=1} LogPtr(2396) map=1977112331, actual doc=SolrInputDocument(fields: [id=13, val2_l_dvo=3000000012, _version_=1534607780231512064, val1_i_dvo=3])\n30589 INFO  (qtp1755078679-232) [n:127.0.0.1:38407__m c:collection1 s:shard1 r:core_node2 x:collection1] o.a.s.u.UpdateLog TLOG: added id 13(ver=1534607780231512064, prevVersion=1534607779993485312, prevPtr=2343) to tlog{file=/tmp/beast-tmp-output/587/J0/temp/solr.cloud.TestStressInPlaceUpdates_FFC46C473EC471E6-001/shard-2-001/cores/collection1/data/tlog/tlog.0000000000000000002 refcount=1} LogPtr(2396) map=1630836284, actual doc=SolrInputDocument(fields: [id=13, val2_l_dvo=3000000012, _version_=1534607780231512064, val1_i_dvo=3])\n30589 INFO  (qtp1755078679-232) [n:127.0.0.1:38407__m c:collection1 s:shard1 r:core_node2 x:collection1] o.a.s.u.p.LogUpdateProcessorFactory [collection1]  webapp=/_m path=/update params={update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:37972/_m/collection1/&distrib.inplace.prevversion=1534607779993485312&wt=javabin&version=2&distrib.inplace.update=true}{add=[13 (1534607780231512064)]} 0 0\n31216 INFO  (qtp2143623462-144) [n:127.0.0.1:58295__m c:collection1 s:shard1 r:core_node3 x:collection1] o.a.s.u.UpdateLog TLOG: added id 13(ver=1534607780231512064, prevVersion=1534607779993485312, prevPtr=2343) to tlog{file=/tmp/beast-tmp-output/587/J0/temp/solr.cloud.TestStressInPlaceUpdates_FFC46C473EC471E6-001/shard-3-001/cores/collection1/data/tlog/tlog.0000000000000000002 refcount=1} LogPtr(2396) map=1500522809, actual doc=SolrInputDocument(fields: [id=13, val2_l_dvo=3000000012, _version_=1534607780231512064, val1_i_dvo=3])\n31216 INFO  (qtp2143623462-144) [n:127.0.0.1:58295__m c:collection1 s:shard1 r:core_node3 x:collection1] o.a.s.u.p.LogUpdateProcessorFactory [collection1]  webapp=/_m path=/update params={update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:37972/_m/collection1/&distrib.inplace.prevversion=1534607779993485312&wt=javabin&version=2&distrib.inplace.update=true}{add=[13 (1534607780231512064)]} 0 0\n31219 INFO  (qtp2010985731-180) [n:127.0.0.1:37972__m c:collection1 s:shard1 r:core_node1 x:collection1] o.a.s.u.p.LogUpdateProcessorFactory [collection1]  webapp=/_m path=/update params={versions=true&wt=javabin&version=2}{add=[13 (1534607780231512064)]} 0 662\n\n\n\t\t\n\t\t\n\t\tbut looking after the ERROR was first logged @ 34456, we see that before the test had a chance to shut down all the nodes, there was some suspicious looking logging from core_node2 regarding updates out of order, that refer to the expected 1534607780231512064 version:\n\n34976 INFO  (qtp1755078679-111) [n:127.0.0.1:38407__m c:collection1 s:shard1 r:core_node2 x:collection1] o.a.s.u.p.DistributedUpdateProcessor Fetched the update: add{_version_=1534607780231512064,id=13}\n35810 INFO  (qtp1755078679-111) [n:127.0.0.1:38407__m c:collection1 s:shard1 r:core_node2 x:collection1] o.a.s.u.p.DistributedUpdateProcessor Fetched the update: add{_version_=1534607779993485312,id=13}\n35811 INFO  (qtp1755078679-111) [n:127.0.0.1:38407__m c:collection1 s:shard1 r:core_node2 x:collection1] o.a.s.u.UpdateLog TLOG: added id 13(ver=1534607779993485312, prevVersion=1534607778351415296, prevPtr=-1) to tlog{file=/tmp/beast-tmp-output/587/J0/temp/solr.cloud.TestStressInPlaceUpdates_FFC46C473EC471E6-001/shard-2-001/cores/collection1/data/tlog/tlog.0000000000000000002 refcount=1} LogPtr(3955) map=748019145, actual doc=SolrInputDocument(fields: [id=13, val2_l_dvo=3000000009, _version_=1534607779993485312, val1_i_dvo=3])\n35811 INFO  (qtp1755078679-111) [n:127.0.0.1:38407__m c:collection1 s:shard1 r:core_node2 x:collection1] o.a.s.u.p.DistributedUpdateProcessor Fetched missing dependent updates from leader, which likely succeeded. Current update: add{_version_=1534607780231512064,id=13}, Missing update: add{_version_=1534607779993485312,id=13}\n35811 INFO  (qtp1755078679-111) [n:127.0.0.1:38407__m c:collection1 s:shard1 r:core_node2 x:collection1] o.a.s.u.UpdateLog TLOG: added id 13(ver=1534607780231512064, prevVersion=1534607779993485312, prevPtr=3955) to tlog{file=/tmp/beast-tmp-output/587/J0/temp/solr.cloud.TestStressInPlaceUpdates_FFC46C473EC471E6-001/shard-2-001/cores/collection1/data/tlog/tlog.0000000000000000002 refcount=1} LogPtr(4015) map=748019145, actual doc=SolrInputDocument(fields: [id=13, val2_l_dvo=3000000012, _version_=1534607780231512064, val1_i_dvo=3])\n35811 INFO  (qtp1755078679-111) [n:127.0.0.1:38407__m c:collection1 s:shard1 r:core_node2 x:collection1] o.a.s.u.p.DistributedUpdateProcessor Fetched missing dependent updates from leader, which likely succeeded. Current update: add{,id=13}, Missing update: add{_version_=1534607780231512064,id=13}\n35811 INFO  (qtp1755078679-111) [n:127.0.0.1:38407__m c:collection1 s:shard1 r:core_node2 x:collection1] o.a.s.u.UpdateLog TLOG: added id 13(ver=1534607783219953664, prevVersion=1534607780231512064, prevPtr=4015) to tlog{file=/tmp/beast-tmp-output/587/J0/temp/solr.cloud.TestStressInPlaceUpdates_FFC46C473EC471E6-001/shard-2-001/cores/collection1/data/tlog/tlog.0000000000000000002 refcount=1} LogPtr(4069) map=748019145, actual doc=SolrInputDocument(fields: [id=13, val2_l_dvo=3000000015, _version_=1534607783219953664, val1_i_dvo=3])\n35812 INFO  (qtp1755078679-111) [n:127.0.0.1:38407__m c:collection1 s:shard1 r:core_node2 x:collection1] o.a.s.u.p.LogUpdateProcessorFactory [collection1]  webapp=/_m path=/update params={update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:37972/_m/collection1/&distrib.inplace.prevversion=1534607780231512064&wt=javabin&version=2&distrib.inplace.update=true}{add=[13 (1534607783219953664)]} 0 1714\n\n\n\t\t\n\t\t\tthese might be unrelated to the failure, since (IIRC) realtime get is always routed to the leader (can somone confirm this?) so this replica needing to fetch these out of order updates from the leader may not actaully be an indication of a problem \u2013 but it did catch my eye.\n\t\t\n\t\t\n\t\tif we change our focus and direct it at the \"actual\" version 1534607778351415296 that was returned by the realtime get causing the test failure, we notice it being recorded by WRITER1 as a partial update @ 28769:\n\n28769 INFO  (WRITER11) [    ] o.a.s.c.TestStressInPlaceUpdates PARTIAL: Writing id=13, val=[3,3000000006], version=1534607778339880960, Prev was=[3,3000000003].  Returned version=1534607778351415296\n\n\n\t\t\n\t\t\tThis is subsequent to the expected TLOG and LogUpdateProc messages we would expect for this update\n\t\t\n\t\t\n\t\tStarting at timestamp 30330, we see the expected log messages for the update that should have replaced 1534607778351415296, with params indicating that 1534607778351415296 is the previous version:\n\n30330 INFO  (qtp2010985731-180) [n:127.0.0.1:37972__m c:collection1 s:shard1 r:core_node1 x:collection1] o.a.s.u.UpdateLog TLOG: added id 13(ver=1534607779993485312, prevVersion=1534607778351415296, prevPtr=636) to tlog{file=/tmp/beast-tmp-output/587/J0/temp/solr.cloud.TestStressInPlaceUpdates_FFC46C473EC471E6-001/shard-1-001/cores/collection1/data/tlog/tlog.0000000000000000004 refcount=1} LogPtr(2343) map=1977112331, actual doc=SolrInputDocument(fields: [id=13, val2_l_dvo=3000000009, _version_=1534607779993485312, val1_i_dvo=3])\n30331 INFO  (qtp2143623462-144) [n:127.0.0.1:58295__m c:collection1 s:shard1 r:core_node3 x:collection1] o.a.s.u.UpdateLog TLOG: added id 13(ver=1534607779993485312, prevVersion=1534607778351415296, prevPtr=636) to tlog{file=/tmp/beast-tmp-output/587/J0/temp/solr.cloud.TestStressInPlaceUpdates_FFC46C473EC471E6-001/shard-3-001/cores/collection1/data/tlog/tlog.0000000000000000002 refcount=1} LogPtr(2343) map=1500522809, actual doc=SolrInputDocument(fields: [id=13, val2_l_dvo=3000000009, _version_=1534607779993485312, val1_i_dvo=3])\n30332 INFO  (qtp2143623462-144) [n:127.0.0.1:58295__m c:collection1 s:shard1 r:core_node3 x:collection1] o.a.s.u.p.LogUpdateProcessorFactory [collection1]  webapp=/_m path=/update params={update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:37972/_m/collection1/&distrib.inplace.prevversion=1534607778351415296&wt=javabin&version=2&distrib.inplace.update=true}{add=[13 (1534607779993485312)]} 0 0\n30336 INFO  (qtp1755078679-241) [n:127.0.0.1:38407__m c:collection1 s:shard1 r:core_node2 x:collection1] o.a.s.u.UpdateLog TLOG: added id 13(ver=1534607779993485312, prevVersion=1534607778351415296, prevPtr=636) to tlog{file=/tmp/beast-tmp-output/587/J0/temp/solr.cloud.TestStressInPlaceUpdates_FFC46C473EC471E6-001/shard-2-001/cores/collection1/data/tlog/tlog.0000000000000000002 refcount=1} LogPtr(2343) map=1630836284, actual doc=SolrInputDocument(fields: [id=13, val2_l_dvo=3000000009, _version_=1534607779993485312, val1_i_dvo=3])\n30336 INFO  (qtp1755078679-241) [n:127.0.0.1:38407__m c:collection1 s:shard1 r:core_node2 x:collection1] o.a.s.u.p.LogUpdateProcessorFactory [collection1]  webapp=/_m path=/update params={update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:37972/_m/collection1/&distrib.inplace.prevversion=1534607778351415296&wt=javabin&version=2&distrib.inplace.update=true}{add=[13 (1534607779993485312)]} 0 0\n30396 INFO  (qtp2143623462-227) [n:127.0.0.1:58295__m c:collection1 s:shard1 r:core_node3 x:collection1] o.a.s.c.S.Request [collection1]  webapp=/_m path=/get params={qt=/get&ids=13&wt=javabin&version=2} status=0 QTime=0\n30441 INFO  (qtp1755078679-106) [n:127.0.0.1:38407__m c:collection1 s:shard1 r:core_node2 x:collection1] o.a.s.c.S.Request [collection1]  webapp=/_m path=/get params={qt=/get&ids=13&wt=javabin&version=2} status=0 QTime=0\n30469 INFO  (qtp2010985731-180) [n:127.0.0.1:37972__m c:collection1 s:shard1 r:core_node1 x:collection1] o.a.s.u.p.LogUpdateProcessorFactory [collection1]  webapp=/_m path=/update params={versions=true&wt=javabin&version=2}{add=[13 (1534607779993485312)]} 0 140\n30556 INFO  (WRITER10) [    ] o.a.s.c.TestStressInPlaceUpdates PARTIAL: Writing id=13, val=[3,3000000009], version=1534607778351415296, Prev was=[3,3000000006].  Returned version=1534607779993485312\n\n\n\t\tBUT! Just before this logging about replacing 1534607778351415296 w/ 1534607779993485312 there is a log message mentioning version 1534607778351415296 that jumped out at me @ 30329:\n\n30329 INFO  (qtp2010985731-180) [n:127.0.0.1:37972__m c:collection1 s:shard1 r:core_node1 x:collection1] o.a.s.u.p.AtomicUpdateDocumentMerger Uncommitted doc is: SolrInputDocument(fields: [id=13, val2_l_dvo=3000000006, _version_=1534607778351415296, val1_i_dvo=3])\n\n\n\t\t\n\t\t\tI don't know anything about this AtomicUpdateDocumentMerger class, but this logging smells particularly suspicious to me because it is the last time AtomicUpdateDocumentMerger logs anything about doc #13\n\t\t\tis this an indication that the \"state\" AtomicUpdateDocumentMerger is tracking for doc #13 isn't getting updated when the (subsequently expected) version 1534607780231512064 is added?\n\t\t\n\t\t\n\t\n\t\n\tswitching to beast-167.important.txt:\n\t\n\t\tthe ERROR that failed the test was first logged by READER0 @ 32666:\n\n32666 ERROR (READER0) [    ] o.a.s.c.TestStressInPlaceUpdates Realtime=true, ERROR, id=13 found={response={numFound=1,start=0,docs=[SolrDocument{id=13, title_s=[title13], val1_i_dvo=2, val2_l_dvo=2000000002, _version_=1534599975700267008, ratings=0.0, price=0}]}} model=[1534599977073901568, 2, 2000000004]\n\n\n\t\tthis test didn't linger long enough to see if any interesting \"out of order update\" messages were logged by replicas after the failure.\n\t\tthe expected version 1534599977073901568 is reported as the returned version by WRITER5 @ 32108:\n\n32108 INFO  (WRITER5) [    ] o.a.s.c.TestStressInPlaceUpdates PARTIAL: Writing id=13, val=[2,2000000004], version=1534599975700267008, Prev was=[2,2000000002].  Returned version=1534599977073901568\n\n\n\t\t\n\t\t\tThis is subsequent to the expected TLOG and LogUpdateProc messages we would expect for this update\n\t\t\n\t\t\n\t\tWe again see no logging from AtomicUpdateDocumentMerger regarding the expected version 1534599977073901568\n\t\tWe DO see logging from AtomicUpdateDocumentMerger regarding the actual version return in the request that failed the assertion:\n\n32082 INFO  (qtp498157202-244) [n:127.0.0.1:55136__nge%2Fr c:collection1 s:shard1 r:core_node1 x:collection1] o.a.s.u.p.AtomicUpdateDocumentMerger Uncommitted doc is: SolrInputDocument(fields: [id=13, val2_l_dvo=2000000002, _version_=1534599975700267008])\n\n\n\t\t\n\t\t\tThis is again the last (and only) log message from AtomicUpdateDocumentMerger regarding doc #13\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\nIn my own hammering of TestStressInPlaceUpdates, i've encountered a handful of failures that haven't reproduced 100% reliably, but most of the failures i've seen have followed this pattern \n{red}The most recent version logged by AtomicUpdateDocumentMerger matches the 'actual' version logged in an ERROR assertion failure message{red}\n\n\thoss.62D328FA1DEA57FD.fail.txt - follows pattern:\n\n23263 INFO  (qtp905061545-62) [n:127.0.0.1:43930_i%2Fop c:collection1 s:shard1 r:core_node1 x:collection1] o.a.s.u.p.AtomicUpdateDocumentMerger Uncommitted doc is: SolrInputDocument(fields: [id=4, val2_l_dvo=3000000009, _version_=1534612078380187648, val1_i_dvo=3])\n...\n23543 ERROR (READER3) [    ] o.a.s.c.TestStressInPlaceUpdates Realtime=true, ERROR, id=4 found={response={numFound=1,start=0,docs=[SolrDocument{id=4, title_s=[title4], val1_i_dvo=3, val2_l_dvo=3000000009, _version_=1534612078380187648, ratings=0.0, price=0}]}} model=[1534612078708391936, 3, 3000000012]\n\n\n\thoss.62D328FA1DEA57FD.fail2.txt - does NOT follow the pattern:\n\n32481 INFO  (qtp1749706169-67) [n:127.0.0.1:48694_i%2Fop c:collection1 s:shard1 r:core_node1 x:collection1] o.a.s.u.p.AtomicUpdateDocumentMerger Uncommitted doc is: SolrInputDocument(fields: [id=13, val2_l_dvo=23000000023, _version_=1534622450566823936, val1_i_dvo=23])\n...\n32581 INFO  (qtp1749706169-238) [n:127.0.0.1:48694_i%2Fop c:collection1 s:shard1 r:core_node1 x:collection1] o.a.s.u.p.AtomicUpdateDocumentMerger Uncommitted doc is: SolrInputDocument(fields: [id=13, val2_l_dvo=23000000046, _version_=1534622450634981376, val1_i_dvo=23])\n...\n32661 ERROR (READER7) [    ] o.a.s.c.TestStressInPlaceUpdates Realtime=false, ERROR, id=13 found={response={numFound=1,start=0,docs=[SolrDocument{id=13, title_s=[title13], val1_i_dvo=23, val2_l_dvo=23000000023, _version_=1534622450566823936, ratings=0.0, price=0}]}} model=[1534622450634981376, 23, 23000000046]\n\n\n\thoss.62D328FA1DEA57FD.fail3.txt - follows pattern:\n\n33869 INFO  (qtp1530691049-201) [n:127.0.0.1:33944_i%2Fop c:collection1 s:shard1 r:core_node1 x:collection1] o.a.s.u.p.AtomicUpdateDocumentMerger Uncommitted doc is: SolrInputDocument(fields: [id=1, val2_l_dvo=7000000035, _version_=1534622560585515008])\n...\n34070 ERROR (READER1) [    ] o.a.s.c.TestStressInPlaceUpdates Realtime=true, ERROR, id=1 found={response={numFound=1,start=0,docs=[SolrDocument{id=1, title_s=[title1], val1_i_dvo=7, val2_l_dvo=7000000035, _version_=1534622560585515008, ratings=0.0, price=0}]}} model=[1534622560698761216, 7, 7000000042]\n\n\n\thoss.D768DD9443A98DC.fail.txt - follows pattern:\n\n29666 INFO  (qtp2123501190-71) [n:127.0.0.1:45821_norsg%2Fy c:collection1 s:shard1 r:core_node1 x:collection1] o.a.s.u.p.AtomicUpdateDocumentMerger Uncommitted doc is: SolrInputDocument(fields: [id=16, val2_l_dvo=7000000014, _version_=1534604230269075456, val1_i_dvo=7])\n...\n31018 ERROR (READER9) [    ] o.a.s.c.TestStressInPlaceUpdates Realtime=true, ERROR, id=16 found={response={numFound=1,start=0,docs=[SolrDocument{id=16, title_s=[title16], val1_i_dvo=7, val2_l_dvo=7000000014, _version_=1534604230269075456, ratings=0.0, price=0}]}} model=[1534604230432653312, 7, 7000000021]\n\n\n\t\n\t\tNOTE: this failure didn't reproduce for me reliably, see hoss.D768DD9443A98DC.pass.txt\n\t\n\t\n\n\n\n\n... hoss.62D328FA1DEA57FD.fail2.txt is currently the odd duck out.\n\nmy best guess (having not yet looked into AtomicUpdateDocumentMerger and what exactly it does and it's \"Uncommitted doc is: ...\" logging and what that refers to) is that some state tracked/updated by AtomicUpdateDocumentMerger is happening asyncronously and in the failure cases isn't \"real\" at the moments the test failures are happening \u2013 and that hoss.62D328FA1DEA57FD.fail2.txt may represent a situation where AtomicUpdateDocumentMerger is logging the version info in the \"new\" state before it's made it \"real\" and available for RTG.\n\n...that's my theory anyway. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15288019",
            "date": "2016-05-18T01:19:14+0000",
            "content": "Thanks for your analysis, Hoss. I'll take a deeper look as soon as possible. A pattern I have observed with such failures (and these failures are the ones I was referring to in the past) that documents get in trouble immediately after or during a commit (i.e. between the commit start and end) happening in a parallel thread. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-15289050",
            "date": "2016-05-18T14:26:13+0000",
            "content": "Beasting found 6 more failures, for a total of 9.\n\nI'm attaching a tar.gz file containing all 9 runs' logs (including those that are separately attached).\n\nHere are the repro lines (without the linedocs sysprop) for the 6 failing runs not previously mentioned on this issue:\n\n\nrun 927:\n\nant test  -Dtestcase=TestStressInPlaceUpdates -Dtests.method=stressTest -Dtests.seed=D936BF4963826AEC -Dtests.slow=true -Dtests.locale=ja -Dtests.timezone=GMT -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1\n\nrun 1246:\n\nant test  -Dtestcase=TestStressInPlaceUpdates -Dtests.method=stressTest -Dtests.seed=9717476E3589BED0 -Dtests.slow=true -Dtests.locale=pl -Dtests.timezone=Europe/Samara -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n\nrun 1543:\n\nant test  -Dtestcase=TestStressInPlaceUpdates -Dtests.method=stressTest -Dtests.seed=9D53690B34363B1C -Dtests.slow=true -Dtests.locale=es-NI -Dtests.timezone=America/Argentina/San_Juan -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1\n\nrun 1558:\n\nant test  -Dtestcase=TestStressInPlaceUpdates -Dtests.method=stressTest -Dtests.seed=655C990D41E11FF7 -Dtests.slow=true -Dtests.locale=en-NZ -Dtests.timezone=MST -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n\nrun 1604:\n\nant test  -Dtestcase=TestStressInPlaceUpdates -Dtests.method=stressTest -Dtests.seed=DD4A15BAF279FCB9 -Dtests.slow=true -Dtests.locale=de-AT -Dtests.timezone=Pacific/Pitcairn -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n\nrun 1854:\n\nant test  -Dtestcase=TestStressInPlaceUpdates -Dtests.method=stressTest -Dtests.seed=9D68461E2404622 -Dtests.slow=true -Dtests.locale=pl -Dtests.timezone=Brazil/West -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n\n\n\n5 of the above 6 runs reproduced for me on the first try, but I could not reproduce run 1246 (seed 9717476E3589BED0) - it succeeded in 4/4 attempts. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15289163",
            "date": "2016-05-18T15:36:51+0000",
            "content": "In the 587 file,\nbetween 30557 (when the expected update was written):\n\n   [junit4]   2> 30557 INFO  (qtp2010985731-180) [n:127.0.0.1:37972__m c:collection1 s:shard1 r:core_node1 x:collection1] o.a.s.u.UpdateLog TLOG: added id 13(ver=1534607780231512064, prevVersion=1534607779993485312, prevPtr=2343) to tlog{file=/tmp/beast-tmp-output/587/J0/temp/solr.cloud.TestStressInPlaceUpdates_FFC46C473EC471E6-001/shard-1-001/cores/collection1/data/tlog/tlog.0000000000000000004 refcount=1} LogPtr(2396) map=1977112331, actual doc=SolrInputDocument(fields: [id=13, val2_l_dvo=3000000012, _version_=1534607780231512064, val1_i_dvo=3])\n\n\n and the ERROR line at 34456,\n\n   [junit4]   2> 34456 ERROR (READER2) [    ] o.a.s.c.TestStressInPlaceUpdates Realtime=true, ERROR, id=13 found={response={numFound=1,start=0,docs=[SolrDocument{id=13, title_s=[title13], val1_i_dvo=3, val2_l_dvo=3000000006, _version_=1534607778351415296, ratings=0.0, price=0}]}} model=[1534607780231512064, 3, 3000000012]\n\n\n,\nthere are two suspicious events:\n\n\tThere was some reordering of updates (34422).\n\tThere was a commit (start: 33306, 33363, 33398; end: 33871, 34209).\n\n\n\nI think either of those, or both, could be the reason for the inconsistency. \n\nI don't know anything about this AtomicUpdateDocumentMerger class, but this logging smells particularly suspicious to me because it is the last time AtomicUpdateDocumentMerger logs anything about doc #13\nThe reason this is the last time AUDM logged anything about the doc #13 could be because soon after that line, the commit happened. And doc #13 was no longer found in the tlog, and hence there was no \"uncommitted doc\" with id=13. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15292921",
            "date": "2016-05-20T07:35:02+0000",
            "content": "Hoss Man pointed me to SOLR-8733, due to which returned versions are 0 unless the versioning is done locally (i.e. the update is sent to the correct shard leader). I found out that the stress test was affected by this issue. I have committed a fix to the stress test so as to workaround this problem by only sending updates to the leader of the shard. After having done that, I cannot reproduce the failures that Steve Rowe reported. \n\nHere's the corresponding commit: https://github.com/chatman/lucene-solr/commit/d61c4fc520111f721468edb236930180bd91d7eb "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-15294456",
            "date": "2016-05-20T22:59:43+0000",
            "content": "I beasted 2000 iterations of `TestStressInPlaceUpdates` against https://github.com/chatman/lucene-solr/commit/d61c4fc520111f721468edb236930180bd91d7eb and there were zero failures. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-15295236",
            "date": "2016-05-21T19:58:39+0000",
            "content": "Awesome "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15296663",
            "date": "2016-05-23T17:11:59+0000",
            "content": "I ran the test all weekend and found the following failures which reproduce reliably:\n\n8CF844B8D2C14DFA\nAE673569D5853984\n\n\n\nIt seems that the common pattern in these failures is that they fail when one of the docValues fields is Lucene54 and one of the fields is Memory. Here's an example (from the latter seed):\n\n   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene62): {title_s=PostingsFormat(name=MockRandom), id=Lucene50(blocksize=128)}, docValues:{val1_i_dvo=DocValuesFormat(name=Direct), _version_=DocValuesFormat(name=Direct), val2_l_dvo=DocValuesFormat(name=Memory), ratings=DocValuesFormat(name=Memory), price=DocValuesFormat(name=Lucene54)}, maxPointsInLeafNode=132, maxMBSortInHeap=7.882796951749762, sim=RandomSimilarity(queryNorm=true,coord=crazy): {}, locale=fr-CA, timezone=Pacific/Pago_Pago\n\n\n\nI found that the same pattern was present in all previously failing tests for which I had logs.\n\nAs a logical next step, I suppressed both \"Lucene54\" and \"Memory\" codecs in the test and ran them. The failing tests passed, and so did lots and lots of other seeds. However, one of the tests failed after suppressing Lucene54 and Memory: seed F9C1398E563942D5 (this seed didn't fail before). Surprisingly, for this failing seed, I don't see the \"NOTE\" line mentioning per field codecs, but just the following info:\n\nNOTE: test params are: codec=FastCompressingStoredFields(storedFieldsFormat=CompressingStoredFieldsFormat(compressionMode=FAST, chunkSize=24740, maxDocsPerChunk=7, blockSize=236), termVectorsFormat=CompressingTermVectorsFormat(compressionMode=FAST, chunkSize=24740, blockSize=236)), sim=RandomSimilarity(queryNorm=true,coord=no): {}, locale=sv, timezone=America/Catamarca\n\n\n\nI'm looking into how to make this particular test fail in a more simple, reproducible test that does not require lots of threads etc. \n\nHoss Man FYI. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15300541",
            "date": "2016-05-25T18:04:00+0000",
            "content": "\n\nI'm still very slowly trying to get up to speed on this ... i started out by reviewing the tests ishan wrote specifically for this issue, but once i realized there really weren't any pre-existing, non trivial, \"distributed atomic updates\" I put that on the backburner to work on SOLR-9159 \u2013 now that that test is solid and running on master & 6x, it's helped uncover an NPE in the new code when the latest patch is applied...\n\n\n   [junit4]   2> 20685 ERROR (qtp1751829478-197) [n:127.0.0.1:58791_solr c:test_col s:shard1 r:core_node4 x:test_col_shard1_replica2] o.a.s.h.RequestHandlerBase org.apache.solr.common.SolrException: Exception writing document id 34 to the index; possible analysis error.\n   [junit4]   2> \tat org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:188)\n   [junit4]   2> \tat org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:68)\n   [junit4]   2> \tat org.apache.solr.update.processor.UpdateRequestProcessor.processAdd(UpdateRequestProcessor.java:48)\n   [junit4]   2> \tat org.apache.solr.update.processor.DistributedUpdateProcessor.doLocalAdd(DistributedUpdateProcessor.java:954)\n   [junit4]   2> \tat org.apache.solr.update.processor.DistributedUpdateProcessor.versionAdd(DistributedUpdateProcessor.java:1145)\n   [junit4]   2> \tat org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:729)\n   [junit4]   2> \tat org.apache.solr.update.processor.LogUpdateProcessorFactory$LogUpdateProcessor.processAdd(LogUpdateProcessorFactory.java:103)\n   [junit4]   2> \tat org.apache.solr.handler.loader.JavabinLoader$1.update(JavabinLoader.java:98)\n   [junit4]   2> \tat org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readOuterMostDocIterator(JavaBinUpdateRequestCodec.java:179)\n   [junit4]   2> \tat org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readIterator(JavaBinUpdateRequestCodec.java:135)\n   [junit4]   2> \tat org.apache.solr.common.util.JavaBinCodec.readVal(JavaBinCodec.java:274)\n   [junit4]   2> \tat org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readNamedList(JavaBinUpdateRequestCodec.java:121)\n   [junit4]   2> \tat org.apache.solr.common.util.JavaBinCodec.readVal(JavaBinCodec.java:239)\n   [junit4]   2> \tat org.apache.solr.common.util.JavaBinCodec.unmarshal(JavaBinCodec.java:157)\n   [junit4]   2> \tat org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec.unmarshal(JavaBinUpdateRequestCodec.java:186)\n   [junit4]   2> \tat org.apache.solr.handler.loader.JavabinLoader.parseAndLoadDocs(JavabinLoader.java:108)\n   [junit4]   2> \tat org.apache.solr.handler.loader.JavabinLoader.load(JavabinLoader.java:55)\n   [junit4]   2> \tat org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:97)\n   [junit4]   2> \tat org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:69)\n   [junit4]   2> \tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:155)\n   [junit4]   2> \tat org.apache.solr.core.SolrCore.execute(SolrCore.java:2036)\n   [junit4]   2> \tat org.apache.solr.servlet.HttpSolrCall.execute(HttpSolrCall.java:658)\n   [junit4]   2> \tat org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:465)\n   [junit4]   2> \tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:257)\n   [junit4]   2> \tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:208)\n   [junit4]   2> \tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1676)\n   [junit4]   2> \tat org.apache.solr.client.solrj.embedded.JettySolrRunner$DebugFilter.doFilter(JettySolrRunner.java:138)\n   [junit4]   2> \tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1676)\n   [junit4]   2> \tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:581)\n   [junit4]   2> \tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:224)\n   [junit4]   2> \tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1160)\n   [junit4]   2> \tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:511)\n   [junit4]   2> \tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n   [junit4]   2> \tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1092)\n   [junit4]   2> \tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n   [junit4]   2> \tat org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:399)\n   [junit4]   2> \tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n   [junit4]   2> \tat org.eclipse.jetty.server.Server.handle(Server.java:518)\n   [junit4]   2> \tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:308)\n   [junit4]   2> \tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:244)\n   [junit4]   2> \tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:273)\n   [junit4]   2> \tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:95)\n   [junit4]   2> \tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n   [junit4]   2> \tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceAndRun(ExecuteProduceConsume.java:246)\n   [junit4]   2> \tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:156)\n   [junit4]   2> \tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:654)\n   [junit4]   2> \tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)\n   [junit4]   2> \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]   2> Caused by: java.lang.NullPointerException\n   [junit4]   2> \tat org.apache.solr.update.DirectUpdateHandler2.doNormalUpdate(DirectUpdateHandler2.java:297)\n   [junit4]   2> \tat org.apache.solr.update.DirectUpdateHandler2.addDoc0(DirectUpdateHandler2.java:221)\n   [junit4]   2> \tat org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:176)\n   [junit4]   2> \t... 47 more\n   ...\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestStressCloudBlindAtomicUpdates -Dtests.method=test_dv -Dtests.seed=64C390303CA3F13A -Dtests.slow=true -Dtests.locale=el -Dtests.timezone=Indian/Cocos -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n\n\n\n...that sort of failure has happened every time i've tried running that new test with the current patch, so i don't think the seed matters.\n\n\n\nI also realized i had some notes from lsat week when i first started reviewing the tests that i aparently never posted here...\n\n\nMisc comments as i try to get up to wrap my head around some of these tests/code...\n\n\n\tthe ASL header should always be above the package & import statements\n\twe should do everything we can to avoid adding any more kitch sink schema files \u2013 if an existing schema file isn't good enough, include only what's necessary to satisfy the test preconditions\n\t\n\t\ttests should also verify their schema preconditions to ensure no one edits the schema to break assumptions (like docValues=true & stored=false)\n\t\t\n\t\t\tschema API can be used to check this in cloud tests, or you can grab the IndexSchema directly from the SolrCore in single core tests\n\t\t\n\t\t\n\t\n\t\n\tat first glance, SolrDocument.applyOlderUpdate looks like it's totally broken for SolrInputDocument objects that contain multiple values for a single field in the SolrInputDocument \u2013 is that usecase tested? where?\n\t\n\t\tunit tests of this method in solrj independent of the usage in UpdateLog seems really important\n\t\n\t\n\tthese empty catch blogs are really bad, and overlook the possibility of serious bugs (either in the current code, or possibly being introduced in the future) by assuming any failures encountered here are the type of failure the code expects, w/o actually verifying it...\n\ntry {\n  addAndGetVersion(sdoc(\"id\",\"20\", \"_version_\", 123456, \"ratings\", map(\"inc\", 1)), null);\n  fail();\n} catch (Exception ex) {}\n\n\n...it should be something like...\n\nSolrException exected = expectThrows(SolrException.class, () -> { addAndGetVersion(...)});\nassertEquals(/* something about expected */, expected.getSomething());\n\n\n\tgiven that the whole point of this functionality is that the docValues are suppose to be updated \"in place\" i'm really suprised at how little is done to try and actaully assert that..\n\t\n\t\tInPlaceUpdateDistribTest checks the internal lucene \"docid\", but i don't see similar checks in the other tests.\n\t\tit seems like every one of these tests should really be looking at the segments in the index/replicas (via the Segments API for cloud, or more easily by grabbing the IndexReader direct from the SolrCore in the single node test) to really be absolutely sure that we didn't get a new segment unless we expected one.  That would not only help prevent false positives in the doc values updating code, but could also help future proof if some settings/defaults change in the future that cause auto commits or something else to happen that this test doesn't expect.\n\t\t\n\t\t\tor am i missunderstanding something about how the docvalue updating code works? ... IIUC we can tell the diff between new segments and existing segments whose docvalues have been updated, is my understanding incorrect?\n\t\t\n\t\t\n\t\tany assertions that are not specific to checking docvalues are updated \"in place\" could be refactored so that we can test them for any field, not just pure DV fields.\n\t\t\n\t\t\tfor example: the exsiting bulkd of the test logic could be refactored into a helper method that takes in a field name \u2013 that helper method could then be called from different test methods that pass in a hardcoded field name.  the test method that passes in a field name which is DV only, could have additional asserts that the segment files on disk do not change after the test passes.\n\t\t\n\t\t\n\t\n\t\n\tit's hard to put a lot of faith in TestInPlaceUpdate and InPlaceUpdateDistribTest considering they use so few docs that they aren't likely to involve more then one index segment per shard (even in nightly or with test.multiplier) ... using things like \"atLeast()\" to ensure we have a non trivial number of docs in the index, and then indexing a random number of them before the docs you are are actually testing against (and the rest after) would help\n\t\n\t\tlikewise for TestStressInPlaceUpdates ... use atLeast for the ndocs (and maybe the num threads) so it's lower on basic runs, but higher for nightly runs or runs where tests.multiplier is set\n\t\n\t\n\tis there a new test showing combinations/sequences of pure-dv updates followed by atomic updates of multiple fields, followed by pure-dv updates? to sanity check that (older style) atomic updates in the middle read the correct \"updated\" docvalues?\n\n\n\n...the comment about refactoring the parts of the test that aren't specific to \"in place\" DV updates is also applicable to the new TestStressCloudBlindAtomicUpdates i added \u2013 in that class \"test_dv\" could be updated to assert that the segments on disk didn't change after the test is finished. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15300551",
            "date": "2016-05-25T18:07:15+0000",
            "content": "Attaching fresh patch from the https://github.com/chatman/lucene-solr/tree/solr_5944 branch.\n\nI've added a test based on SolrTestCaseJ4, which fails, despite suppressing the Lucene54 and Memory codecs. I am now trying to reproduce this issue in a LuceneTestCase test. \n\nhttps://github.com/chatman/lucene-solr/commit/b80be98d9e1a98b97bee670a2d775f6acc2182c7\n\nReproduce:\n\n-ea -Dtests.seed=1D9D6101E3D231FF -Dtests.verbose=true -Dtestcase=TestInPlaceUpdate\n\n\n\nCouple things I noticed that: (a) this happens when Compressing codec is used, along with (b) the \"crazy\" value of max buffered docs is used as per this part from the randomization in LuceneTestCase.java\n\n    if (r.nextBoolean()) {\n      if (rarely(r)) {\n        // crazy value\n        c.setMaxBufferedDocs(TestUtil.nextInt(r, 2, 15));\n      } else {\n        // reasonable value\n        c.setMaxBufferedDocs(TestUtil.nextInt(r, 16, 1000));\n      }\n    }\n\n\n\nHowever, I can't reproduce the failure based on just those two factors; there must be something else that I'm unable to pinpoint. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15300916",
            "date": "2016-05-25T21:44:19+0000",
            "content": "minor tweaks to ishan's last patch...\n\n\n\tupdated to master (some compliation failures due to other jiras getting commited)\n\trefactored TestInPlaceUpdate\n\t\n\t\ttestReplay3 & testReplay4 reproduce the previously mentioned failure that required using the external file.\n\t\tyou can still refer to external files i nteh same format via \"testReplayFromFile\" but only if a sys prop is set...\n\nant test -Dtestcase=TestInPlaceUpdate -Dtests.seed=1D9D6101E3D231FF -Dtests.verbose=true -Dtests.method=testReplayFromFile -Dargs=\"-Dtests.inplace.file=/home/hossman/lucene/dev/operations3.txt\"\n\n\n\t\n\t\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15301048",
            "date": "2016-05-25T22:52:25+0000",
            "content": "Ugh, testReplay4 in my last patch was a waste of time \u2013 somehoe i screwed something up with copy/paste.\n\nThe point was to try and check if using \"set\" operations had any different effect on the test compared to using \"inc\" operations (like in testReplay3)\n\nThis patch fixes that, so that both tests demonstrate the problem with the seed ishan mentioned (and aren't identical) ... but more interesting then that is the new testReplay5, testReplay6 & testReplay7 (still using hte same seed)...\n\n\n\ttestReplay5 - still uses \"inc\" for doc id=0, but uses \"set\" for every other doc in the index\n\t\n\t\tthis currently fails with an NPE in AtomicUpdateDocumentMerger.doInPlaceUpdateMerge(AtomicUpdateDocumentMerger.java:283)\n\t\n\t\n\ttestReplay6 was a quick hack to work around the NPE in testReplay5 - ensuring that the first time a doc is added, a regular \"add\" is done, but everytime after that \"set\" is used (except for id=0 where \"inc\" is still used)\n\t\n\t\tthis test currently passes \u2013 even though it should be effectively the same as testReplay3 and testReplay4\n\t\n\t\n\ttestReplay7 is the same as testReplay6 but (like testReplya4) does all the id=0 updates using \"set\" as well just to sanity check no diff in behavior\n\t\n\t\tthis also passes\n\t\n\t\n\n\n\n...so based on the fact that testReplay6/7 pass, but testReplay3/4 fail, any ishan's observations about the setMaxBufferedDocs my suspicion is that the underlying problem probably has something to do with how frequently docs are flushed and trying to read back \"updated\" DV values when other docs were added with regular DV values i nte hsame field just before/after the DV update.\n\n(looking at TestNumericDocValuesUpdates i don't see many tests that mix calls to addDocument with calls to updateNumericDocValue \u2013 except testSegmentMerges & testManyReopensAndFields which update the DV of every document added up to that point) "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15301210",
            "date": "2016-05-26T00:34:03+0000",
            "content": "(I'm pretty sure) I was able to reproduce the root cause of the randomized failures in LUCENE-7301. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15301985",
            "date": "2016-05-26T12:06:30+0000",
            "content": "(I'm pretty sure) I was able to reproduce the root cause of the randomized failures in LUCENE-7301.\nThanks Hoss for beating me to it!\n\n\ntestReplay5 - still uses \"inc\" for doc id=0, but uses \"set\" for every other doc in the index\n\n    this currently fails with an NPE in AtomicUpdateDocumentMerger.doInPlaceUpdateMerge(AtomicUpdateDocumentMerger.java:283)\nI think the problem there is that a \"set\" operation was attempted at a document that still doesn't exist in the index. I think such an operation works with atomic updates, but the underlying docValues API doesn't support updates of dv fields that don't exist yet. I will try to handle this better, instead of throwing NPE.\n\nI shall work on fixing your review comments regarding the tests, and increase their scope as you suggest. My idea behind the tests were (and naming could be improved): TestInPlaceUpdate just tests some basic cases in non-cloud mode, TestStressInPlaceUpdates tests lots of documents, lots of updates, lots of threads and cloud mode, InPlaceUpdateDistribTest for some basic operations/scenarios in cloud (including testing if same document was updated, or a new one was created). I was thinking that if we can get past the DV updates flushing issue (LUCENE-7301), we can focus well on improving scope of tests more. Thanks for your review! "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15303367",
            "date": "2016-05-27T02:36:14+0000",
            "content": "\nOk ... more in depth comments reviewing the latest patch (ignoring some of the general higher level stuff i've previously commented on).\n\n(So far i've still focused on reviewing the tests, because we should make sure they're rock solid before any discussion of refacoting/improving/changing the code)\n\n\n\n\n\tin general, all these tests seem to depend on autoCommit being disabled, and use a config that is setup that way, but don't actaully assert that it's true in case someone changes the configs in the future\n\t\n\t\tTestInPlaceUpdate can get direct access to the SolrCore verify that for certain to\n\t\tthe distrib tests might be able to use one of hte new cnfig APIs to check this (i don't know off the top of my head)\n\t\t\n\t\t\tat a minimum define a String constant for the config file name in TestInPlaceUpdate and refer to it in the other tests where the same config is expected with a comment explaining that we're assuming it has autoCommit disabled and that TestInPlaceUpdate will fail if it does not.\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tTestInPlaceUpdate\n\t\n\t\tSuppressCodecs should be removed\n\t\tshould at least have class level javadocs explaining what's being tested\n\t\ttestUpdatingDocValues\n\t\t\n\t\t\tfor addAndGetVersion calls where we don't care about the returned version, don't bother assigning it to a variable (distracting)\n\t\t\tfor addAndGetVersion calls where we do care about the returned version, we need check it for every update to that doc...\n\t\t\t\n\t\t\t\tcurrently version1 is compared to newVersion1 to assert that an update incrememnted the version, but in between those 2 updates are 4 other places where that document was updated \u2013 we have to assert it has the expected value (either the same as before, or new - and if new record it) after all of those addAndGetVersion calls, or we can't be sure where/why/how a bug exists if that existing comparison fails.\n\t\t\t\tideally we should be asserting the version of every doc when we query it right along side the assertion for it's updated \"ratings\" value\n\t\t\t\n\t\t\t\n\t\t\tmost of the use of \"field(ratings)\" can probbaly just be replaced with \"ratings\" now that DV are returnable \u2013 although it's nice to have both included in the test at least once to demo that both work, but when doing that there should be a comment making it clear why\n\t\t\n\t\t\n\t\ttestOnlyPartialUpdatesBetweenCommits\n\t\t\n\t\t\tditto comment about checking return value from addAndGetVersion\n\t\t\tthis also seems like a good place to to test if doing a redundent atomic update (either via set to the same value or via inc=0) returns a new version or not \u2013 should it?\n\t\t\n\t\t\n\t\tDocInfo should be a private static class and have some javadocs\n\t\tbased on how testing has gone so far, and the discover of LUCENE-7301 it seems clear that adding even single thread, single node, randomized testing of lots of diff types of add/update calls would be good to add\n\t\t\n\t\t\twe could refactor/improve the \"checkReplay\" function I added in the last patch to do more testing of a randomly generated Iterable of \"commands\" (commit, doc, doc+atomic mutation, etc...)\n\t\t\tand of course: improve checkReplay to verify RTG against hte uncommited model as well\n\t\t\ttestReplayFromFile and getSDdoc should probably go away once we have more structured tests for doing this\n\t\t\n\t\t\n\t\tcreateMap can be elimianted \u2013 callers can just use SolrTestCaseJ4.map(...)\n\t\tIn general the tests in this class should include more queries / sorting against the updated docvalues field after commits to ensure that the updated value is searchable & sortable\n\t\tLikewise the test methods in this class should probably have a lot more RTG checks \u2013 with filter queries that constrain against the updated docvalues field, and checks of the expected version field \u2013 to ensure that is all working properly.\n\t\n\t\n\n\n\n\n\tInPlaceUpdateDistribTest\n\t\n\t\tSuppressCodecs should be removed\n\t\tshould at least have class level javadocs explaining what's being tested\n\t\tOnce LUCENE-7301 is fixed and we can demonstate that this passes reliably all of the time, we should ideally refactor this to subclass SolrCloudTestCase\n\t\tin general, the \"pick a random client\" logic should be refactored so that sometimes it randomly picks a CloudSolrClient\n\t\tthere should almost certianly be some \"delete all docs and optimize\" cleanup in between all of these tests\n\t\t\n\t\t\teasy to do in an @Before method if we refactor to subclass SolrCloudTestCase\n\t\t\n\t\t\n\t\tdocValuesUpdateTest\n\t\t\n\t\t\tshould randomize numdocs\n\t\t\twe need to find away to eliminate the hardcoded \"Thread.sleep(500);\" calls...\n\t\t\t\n\t\t\t\tif initially no docs have a rating value, then make the (first) test query be for rating:[* TO *] and execute it in a rety loop until the numFound matches numDocs.\n\t\t\t\tlikewise if we ensure all ratings have a value such that abs(ratings) < X, then the second update can use an increment such that abs(inc) > X*3 and we can use -ratings:[-X TO X] as the query in a retry loop\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\tensureRtgWorksWithPartialUpdatesTest\n\t\t\n\t\t\teven if we're only going to test one doc, we should ensure there are a random num docs in the index (some before the doc we're editing, and some after)\n\t\t\tif we're testing RTG, then we should be testing the version returned from every /get call against the last version returned from every update\n\t\t\n\t\t\n\t\toutOfOrderUpdatesIndividualReplicaTest\n\t\t\n\t\t\tditto comments about only one doc\n\t\t\tditto comments about testing the expected version in RTG requests\n\t\t\tif we're sending updates direct to replicas to test how they handle out of order updates, then something better assert exactly where the leader is hosted and ensure we don't send to it by mistake\n\t\t\twhat's the point of using a threadpool and SendUpdateToReplicaTask here?  why not just send the updates in a (randdomly assigned) determinisitc order?\n\t\t\t\n\t\t\t\tif we are going to use an ExecutorService, then the result of awaitTermination has to be checked\n\t\t\t\t... and shutdown & awaitTermination have to be called in that order\n\t\t\t\n\t\t\t\n\t\t\tsince this tests puts replicas out of sync, a \"delete all docs and optimize\" followed up \"wait for recovers\" should happen at the end of this test (or just in between every test) .. especially if we refactor it (or to protect someone in the future who might refactor it)\n\t\t\n\t\t\n\t\tdelayedReorderingFetchesMissingUpdateFromLeaderTest\n\t\t\n\t\t\tditto previous comments about using a threadpool and SendUpdateToReplicaTask\n\t\t\t\n\t\t\t\teven more so considering the \"Thread.sleep(100);\" ... what's the pont of using a threadpool if we want the requests to be sequential?\n\t\t\t\n\t\t\t\n\t\t\tIs there no way we can programatically tell if LIR has kicked in? ... pehaps by setting a ZK watch? ... this \"Thread.sleep(500);\" is no garuntee and seens arbitrary.\n\t\t\t\n\t\t\t\tat a minimum polling in a loop for the expected results seems better then just a hardcoded sleep\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\tSendUpdateToReplicaTask\n\t\t\n\t\t\tbased on how it's used, i'm not really sure i see the point in this class, but assuming it continues to exist...\n\t\t\tconstructor takes in a Random, but method uses that global random() anyway.\n\t\t\t\n\t\t\t\tshould probably take in a seed, and construct it's own Random\n\t\t\t\trandom() ensures that each Thread gets it's own consistent Random instance \u2013 but in Callables like this each Thread having a consistent seed doesn't help the reproducibility since there's no garutee which Threed from an ThreadPool (Executor) will invoke call().\n\t\t\t\n\t\t\t\n\t\t\tinstead of returning true, this should be a Callable<UpdateResponse> and call() should return the results of the request so the caller can assert it was successful (via Future.get().getStatus())\n\t\t\n\t\t\n\t\tgetReplicaValue\n\t\t\n\t\t\tusing SolrTestCaseJ4.params(...) would make this method a lot shorter\n\t\t\tbased on where/how this method is used, i don't understand why it returns String instead of just Object\n\t\t\n\t\t\n\t\tassertReplicaValue\n\t\t\n\t\t\tshould take in some sort of assertion message and/or build/append an assertion message using the clientId\n\t\t\tif getReplicaValue returns an Object, this can take an \"Object expected\" param and eliminate abonch of toString & string concating throughout the test\n\t\t\n\t\t\n\t\tsimulatedUpdateRequest\n\t\t\n\t\t\tif this method is going to assume that the only shard you ever want to similulate an update to is SHARD1 then the method name should be \"simulatedUpdateRequestToShard1Replica\"\n\t\t\tbetter still - why not ask the DocRouter which shard this doc belongs in, and fetch the leader URL tha way?\n\t\t\n\t\t\n\t\tmost usage of \"addFields\" can just be replaced with a call to \"sdoc(...)\" to simplify code\n\t\treplace createMap usage with SolrTestCaseJ4.map\n\t\twhy override tearDown if we're just calling super?\n\t\tin general, i think this test would be a lot easier to read if there were well named variables for HttpSolrClient instances pointed at specific replicas (ie HttpSolrClient SHARD1_LEADER = ...; HttpSolrClient SHARD1_REPLICA1 = ...; etc...) and passed those around to the various methods instead of magic ints (ie: \"1\", \"2\") to refer to which index in the static clients list should be used for a given update.\n\t\n\t\n\n\n\n\n\tTestStressInPlaceUpdates\n\t\n\t\tditto comments from InPlaceUpdateDistribTest about regarding @SupressCodecs, javadocs, and extending SolrCloudTestCase once LUCENE-7301 is fixed and we're sure this test passes reliably.\n\t\t\n\t\t\talso: we should really make this test use multiple shards\n\t\t\n\t\t\n\t\tstressTest\n\t\t\n\t\t\tit would be a lot cleaner/clearer if we refactored these anonymous Thread classes into private static final (inner) classes and instantiated them like normal objects\n\t\t\t\n\t\t\t\tmakes it a lot easier to see what threads access/share what state\n\t\t\t\tbetter still would be implementing these \"workers\" as Callable instances and using an ExecutorService\n\t\t\t\n\t\t\t\n\t\t\t\"operations\" comment is bogus (it's not just for queries)\n\t\t\tmaxConcurrentCommits WTF?\n\t\t\t\n\t\t\t\thas a comment that it should be less the # warming threads, but does that even make sense i na distrib test?\n\t\t\t\tcurrently set to nWriteThreads \u2013 so what's the point, when/how could we ever possibly have more concurrent commits then the number of threads? doesn't that just mean that at the moment every write thread is allowed to commit if it wants to?\n\t\t\t\tif there is a reason for it, then replaceing \"numCommitting\" with a new Semaphore(maxConcurrentCommits) would probably make more sense\n\t\t\t\n\t\t\t\n\t\t\twhy is the \"hardCommit start\" logic calling both commit(); and clients.get(chosenClientIndex).commit(); ?\n\t\t\tI'm not convinced the \"synchronize {...}; commit stuff; syncrhonize { ... };\" sequence is actually thread safe...\n\t\t\t\n\t\t\t\tT-W1: commit sync block 1: newCommittedModel = copy(model), version = snapshotCount++;\n\t\t\t\tT-W2: updates a doc and adds it to model\n\t\t\t\tT-W1: commit\n\t\t\t\tT-W1: commit sync block 2: committedModel = newCommittedModel\n\t\t\t\tT-R3: read sync block: get info from committedModel\n\t\t\t\tT-R3: query for doc\n\t\t\t\t...\n\t\t\t\n\t\t\t\n\t\t\t... in the above sequence, query results seen by thread T-R3 won't match the model because the update from T-W2 made it into the index before the commit, but after the model was copied\n\t\t\t\n\t\t\t\ti guess it's not a huge problem because the query thread doesn't bother to assert anything unless the versions match \u2013 but that seems kind of risky ... we could theoretically never assert anything\n\t\t\t\n\t\t\t\n\t\t\thaving at least one pass over the model checking every doc at the end of the test seems like a good idea no matter what\n\t\t\tI'm certain the existing \"synchronized (model)\" block is not thread safe relative to the synchronized blocks that copy the model into commitedModel, because the \"model.put(...)\" calls can change the iterator and trigger a ConcurrentModificationException\n\t\t\tthere's a bunch of \"TODO\" blocks realted to deletes that still need implemented\n\t\t\tthe writer threads should construct the SolrInputDocument themselves, and log the whole document (not just the id) when they log things, so it's easier to tell from the logs what updates succeed and which were rejected because of version conflicts\n\t\t\twhy is //fail(\"Were were results: \"+response); commented out?\n\t\t\tthere's a lot of \"instanceof ArrayList\" checks that make no sense to me since the object came from getFirstValue\n\t\t\n\t\t\n\t\tDocInfo\n\t\t\n\t\t\tshould be a private static class and have some javadocs\n\t\t\tor sould be a public class in it's own file, w/javadocs, and re-used in the various tests that want ot reuse it\n\t\t\n\t\t\n\t\tverbose\n\t\t\n\t\t\twhy does this method exist? why aren't callers just using log.info(...) directly?\n\t\t\tor if callers really need to pass big sequences of stuff, they can use log.info(\"{}\", Arrays.asList(...))\n\t\t\tor worst case: this method can simplified greatly to do that internally\n\t\t\n\t\t\n\t\taddDocAndGetVersion\n\t\t\n\t\t\tusing SolrTestCaseJ4.sdoc and SolrTestCaseJ4.params will make this method a lot sorder\n\t\t\twhy are we synchronizing on cloudClient but updating with leaderClient?\n\t\t\t\n\t\t\t\tif the point is just to ensure all udpates happem synchronously regardless of client, then we should just define some public static final Object UPDATE_SYNC = new Object(\"sync lock for updates\"); and use that\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\tgetClientForLeader\n\t\t\n\t\t\ti know this method is currently just a workaround for SOLR-8733, noting that in the method javadocs seems important\n\t\t\tif we refactor this test to use multiple shards before SOLR-8733 gets resolved, this method can take in a uniqueKey, and consult the DocRouter to pick the correct shard/node.\n\t\t\n\t\t\n\t\treplace createMap usage with SolrTestCaseJ4.map\n\t\twhy override tearDown if we're just calling super?\n\t\n\t\n\n\n\n\n\n\tSolrDocument\n\t\n\t\tapplyOlderUpdate\n\t\t\n\t\t\tas mentioned before, i don't think this method is correct when it comes to multivalued fields, and should have more unit tests of various permutations to be sure\n\t\t\tthis functionality should probably be moved to a private helper method in UpdateLog (it doesn't do anything that requires it have access to the internals of the SolrDocument)\n\t\t\tno matter where it lives, it should have some javadocs\n\t\t\n\t\t\n\t\n\t\n\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15316736",
            "date": "2016-06-06T16:42:08+0000",
            "content": "Updated the patch. Fixed some issues from Hoss' comments. Some nocommits are remaining. I'll reply to Hoss' suggestions in-line shortly.\nCouple of error handling fixes, but mostly changes to test suites. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15316984",
            "date": "2016-06-06T18:54:39+0000",
            "content": "Fixed two bugs:\n1. Added a check while writing a document to exclude anything to do with the id field.\n2. Added an exception when a \"set\" or \"inc\" operation is attempted at a non-existent document.\n\nReview comments:\n\n* in general, all these tests seem to depend on autoCommit being disabled, and use a config that is setup that way, but don't actaully assert that it's true in case someone changes the configs in the future\nTODO\n\n* TestInPlaceUpdate\nRenamed this test to TestInPlaceUpdatesStandalone.\n\n** SuppressCodecs should be removed\nRemoved all non 3x, 4x codec suppression. They need to be suppressed as per a comment from Mikhail. https://issues.apache.org/jira/browse/LUCENE-5189?focusedCommentId=13958205&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13958205\n\n** should at least have class level javadocs explaining what's being tested\nTODO\n\n\n\t\n\t\n\t\ttestUpdatingDocValues\n*** for addAndGetVersion calls where we don't care about the returned version, don't bother assigning it to a variable (distracting)\nFixed\n*** for addAndGetVersion calls where we do care about the returned version, we need check it for every update to that doc...\nTODO\n**** currently version1 is compared to newVersion1 to assert that an update incrememnted the version, but in between those 2 updates are 4 other places where that document was updated \u2013 we have to assert it has the expected value (either the same as before, or new - and if new record it) after all of those addAndGetVersion calls, or we can't be sure where/why/how a bug exists if that existing comparison fails.\nTODO\n**** ideally we should be asserting the version of every doc when we query it right along side the assertion for it's updated \"ratings\" value\nTODO\n*** most of the use of \"field(ratings)\" can probbaly just be replaced with \"ratings\" now that DV are returnable \u2013 although it's nice to have both included in the test at least once to demo that both work, but when doing that there should be a comment making it clear why\nFixed\n\t\ttestOnlyPartialUpdatesBetweenCommits\n\t\t\n\t\t\tditto comment about checking return value from addAndGetVersion\n*** this also seems like a good place to to test if doing a redundent atomic update (either via set to the same value or via inc=0) returns a new version or not \u2013 should it?\nAs of now, both will generate a new version. I think \"inc\" 0 should be dropped, and \"set\" same value should be versioned. I'll check if behaviour in this patch is at par with regular atomic updates; and if so, will open a separate issue for this later.\n** DocInfo should be a private static class and have some javadocs\nFixed\n** based on how testing has gone so far, and the discover of LUCENE-7301 it seems clear that adding even single thread, single node, randomized testing of lots of diff types of add/update calls would be good to add\nI think we can do the same in TestStressInPlaceUpdates, by randomly setting number of writer threads to 1 sometimes.\n*** we could refactor/improve the \"checkReplay\" function I added in the last patch to do more testing of a randomly generated Iterable of \"commands\" (commit, doc, doc+atomic mutation, etc...)\nTODO\n*** and of course: improve checkReplay to verify RTG against hte uncommited model as well\nI couldn't find a way to do this for the TestInPlaceUpdate (now called TestInPlaceUpdatesStandalone in this patch). This is based on SolrTestCaseJ4.\n\t\t\n\t\t\n\t\n\t\n\n\n\n*** testReplayFromFile and getSDdoc should probably go away once we have more structured tests for doing this\nFixed\n\n** createMap can be elimianted \u2013 callers can just use SolrTestCaseJ4.map(...)\nFixed\n\n** In general the tests in this class should include more queries / sorting against the updated docvalues field after commits to ensure that the updated value is searchable & sortable\nTODO\n\n** Likewise the test methods in this class should probably have a lot more RTG checks \u2013 with filter queries that constrain against the updated docvalues field, and checks of the expected version field \u2013 to ensure that is all working properly.\nCouldn't figure out how to do RTGs with this test, but will check RTGs + filter queries in the TestInPlaceUpdatesDistrib test (which was formerly InPlaceUpdateDistribTest)\n\n* InPlaceUpdateDistribTest\nRenamed to TestInPlaceUpdatesDistrib now\n\n** SuppressCodecs should be removed\n3x and 4x codec suppressions cannot be removed.\n** should at least have class level javadocs explaining what's being tested\nTODO\n** Once LUCENE-7301 is fixed and we can demonstate that this passes reliably all of the time, we should ideally refactor this to subclass SolrCloudTestCase\nTODO\n** in general, the \"pick a random client\" logic should be refactored so that sometimes it randomly picks a CloudSolrClient\nTODO\n** there should almost certianly be some \"delete all docs and optimize\" cleanup in between all of these tests\nAdded this to the beginning of every test. I couldn't see an easy way to refactor to SolrCloudTestCase immediately, but I can look more later.\n\n\n\t\n\t\n\t\tdocValuesUpdateTest\n*** should randomize numdocs\nFixed\n*** we need to find away to eliminate the hardcoded \"Thread.sleep(500);\" calls...\nFixed (I think it is the correct fix, but I'll review it again, testing with a fast machine and beasting it).\n**** if initially no docs have a rating value, then make the (first) test query be for rating:[* TO *] and execute it in a rety loop until the numFound matches numDocs.\nTODO\n**** likewise if we ensure all ratings have a value such that abs(ratings) < X, then the second update can use an increment such that abs(inc) > X*3 and we can use -ratings:[-X TO X] as the query in a retry loop\nTODO\n\t\tensureRtgWorksWithPartialUpdatesTest\n*** even if we're only going to test one doc, we should ensure there are a random num docs in the index (some before the doc we're editing, and some after)\nTODO\n*** if we're testing RTG, then we should be testing the version returned from every /get call against the last version returned from every update\nTODO\n\t\toutOfOrderUpdatesIndividualReplicaTest\n*** ditto comments about only one doc\nTODO\n*** ditto comments about testing the expected version in RTG requests\nTODO\n*** if we're sending updates direct to replicas to test how they handle out of order updates, then something better assert exactly where the leader is hosted and ensure we don't send to it by mistake\nTODO\n*** what's the point of using a threadpool and SendUpdateToReplicaTask here?  why not just send the updates in a (randdomly assigned) determinisitc order?\nTODO\n**** if we are going to use an ExecutorService, then the result of awaitTermination has to be checked\nTODO\n\t\t\n\t\t\tsince this tests puts replicas out of sync, a \"delete all docs and optimize\" followed up \"wait for recovers\" should happen at the end of this test (or just in between every test) .. especially if we refactor it (or to protect someone in the future who might refactor it)\nTODO\n\t\t\n\t\t\n\t\tdelayedReorderingFetchesMissingUpdateFromLeaderTest\n*** ditto previous comments about using a threadpool and SendUpdateToReplicaTask\nTODO\n*** Is there no way we can programatically tell if LIR has kicked in? ... pehaps by setting a ZK watch? ... this \"Thread.sleep(500);\" is no garuntee and seens arbitrary.\nTODO.\n\t\tgetReplicaValue\n*** using SolrTestCaseJ4.params(...) would make this method a lot shorter\nFixed\n*** based on where/how this method is used, i don't understand why it returns String instead of just Object\nFixed\n** assertReplicaValue\n*** should take in some sort of assertion message and/or build/append an assertion message using the clientId\nFixed\n*** if getReplicaValue returns an Object, this can take an \"Object expected\" param and eliminate abonch of toString & string concating throughout the test\nFixed\n\t\tsimulatedUpdateRequest\n*** if this method is going to assume that the only shard you ever want to similulate an update to is SHARD1 then the method name should be \"simulatedUpdateRequestToShard1Replica\"\nTODO\n*** better still - why not ask the DocRouter which shard this doc belongs in, and fetch the leader URL tha way?\nTODO\n** most usage of \"addFields\" can just be replaced with a call to \"sdoc(...)\" to simplify code\nFixed\n** replace createMap usage with SolrTestCaseJ4.map\nFixed\n** why override tearDown if we're just calling super?\nFixed\n** in general, i think this test would be a lot easier to read if there were well named variables for HttpSolrClient instances pointed at specific replicas (ie HttpSolrClient SHARD1_LEADER = ...; HttpSolrClient SHARD1_REPLICA1 = ...; etc...) and passed those around to the various methods instead of magic ints (ie: \"1\", \"2\") to refer to which index in the static clients list should be used for a given update.\nFixed: LEADER and NON_LEADERS now\n\t\n\t\n\n\n\n\n\tTestStressInPlaceUpdates\nTODO\n\n\n\n\n\tSolrDocument\n** applyOlderUpdate\nFixed, moved it to UpdateLog. I think Noble had moved this from UpdateLog to SolrDocument; and if so, I'll check with him once (after I'm done with other changes here marked as TODO).\n*** as mentioned before, i don't think this method is correct when it comes to multivalued fields, and should have more unit tests of various permutations to be sure\nTODO\n*** no matter where it lives, it should have some javadocs\nFixed\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15317318",
            "date": "2016-06-06T21:42:19+0000",
            "content": "Removed all non 3x, 4x codec suppression. They need to be suppressed as per a comment from Mikhail. ...\n\nthat comment is over 2 years old, from a time when those codecs existed but did not support updating doc values.\n\nthose codecs no longer exist (on either master or branch_6x) \u2013 even if someone had na existing index with segments from those codecs, they would not be supported by any Solr 6.x version because they are more then 1 major version old \u2013 we only have to worry about Lucene5* codecs and higher.\n\nAs of now, both will generate a new version. I think \"inc\" 0 should be dropped, and \"set\" same value should be versioned. I'll check if behaviour in this patch is at par with regular atomic updates; and if so, will open a separate issue for this later.\n\nyeah, sorry \u2013 my point was: \"whatever the current, non-patched, behavior is for the version returned from these types of updates, we need to assert that behavior is true here.\" \u2013 we should not be changing any semantics here, absolutely open a distinct issue for that if you think it makes sense as a future improvement.\n\nI think we can do the same in TestStressInPlaceUpdates, by randomly setting number of writer threads to 1 sometimes.\n\nIsn't that still a cloud based test with multiple nodes/shards?  Even with only 1 writer thread it's going ot be harder to debug then doing more randomized testing in a single node test (via something like checkReplay as in my previous suggestion)\n\nI couldn't find a way to do this (check RTG against uncommitted model) for the TestInPlaceUpdate (now called TestInPlaceUpdatesStandalone in this patch). This is based on SolrTestCaseJ4.\n\nSolrTestCaseJ4.addAndGetVersion(...)\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15318961",
            "date": "2016-06-07T17:35:30+0000",
            "content": "New patch. Fixed all comments in TestInPlaceUpdatesDistrib (formerly InPlaceUpdateDistribTest).\n\nwhat's the point of using a threadpool and SendUpdateToReplicaTask here? why not just send the updates in a (randdomly assigned) determinisitc order? \nEssentially, I need a way to send three updates to the replica asynchronously. To achieve the effect of asynchronous updates, I used a threadpool here. Three updates sent one after the other, each being a blocking call, wouldn't have simulated the leader -> replica interaction sufficiently. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15320436",
            "date": "2016-06-08T12:19:42+0000",
            "content": "New patch. Fixes to TestInPlaceUpdatesStandalone (formerly TestInPlaceUpdate).\n\nTODO items in the test are:\n\n\tRandomize the replay based tests.\n\tAdd new tests that validate sorting, searching, filter queries involving the updated DV.\n\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15320867",
            "date": "2016-06-08T16:29:46+0000",
            "content": "New patch.\n\n\tIncreasing the timeout of the threadpool termination in TestInPlaceUpdatesDistrib from 2s to 5s, and added assertion message.\n\tFixing some review comments in the TestStressInPlaceUpdates (more remains to be fixed).\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15323045",
            "date": "2016-06-09T18:26:21+0000",
            "content": "\nI don't understand this comment \u2013 particularly in light of the changes you've made to the test since...\n\n\nwhat's the point of using a threadpool and SendUpdateToReplicaTask here? why not just send the updates in a (randdomly assigned) determinisitc order? \n\nEssentially, I need a way to send three updates to the replica asynchronously. To achieve the effect of asynchronous updates, I used a threadpool here. Three updates sent one after the other, each being a blocking call, wouldn't have simulated the leader -> replica interaction sufficiently.\n\nWhen i posted that particular question it was about outOfOrderUpdatesIndividualReplicaTest \u2013 the code in question at teh time looked like this...\n\n\n// re-order the updates for replica2\nList<UpdateRequest> reorderedUpdates = new ArrayList<>(updates);\nCollections.shuffle(reorderedUpdates, random());\nfor (UpdateRequest update : reorderedUpdates) {\n  SendUpdateToReplicaTask task = new SendUpdateToReplicaTask(update, clients.get(1), random());\n  threadpool.submit(task);\n}\n\n\n\n...My impression, based on the entirety of that method, was that the intent of the test was to bypass the normal distributed update logic and send carefully crafted \"simulated\" updates direct to each replica, such that one repliica got the (simulated from leader) updates \"in order\" and another replica got the (simulated from leader) updates \"out of order\"\n\n\n\tif the point was for replica2 to get the (simulated from leader) updates \"out of order\" then why shuffle them - why not explicitly put them in the \"wrong\" order?\n\tif the goal was send them asynchronously, and try to get them to happen as concurrently as possible (as you indicated above in your answer to my question) then what was the point of the \"shuffle\" ?\n\n\n\nLooking at the modified version of that code in your latest patch doesn't really help clarify things for me...\n\n\n// re-order the updates for NONLEADER 0\nList<UpdateRequest> reorderedUpdates = new ArrayList<>(updates);\nCollections.shuffle(reorderedUpdates, random());\nList<Future<UpdateResponse>> updateResponses = new ArrayList<>();\nfor (UpdateRequest update : reorderedUpdates) {\n  AsyncUpdateWithRandomCommit task = new AsyncUpdateWithRandomCommit(update, NONLEADERS.get(0), seed);\n  updateResponses.add(threadpool.submit(task));\n  // send the updates with a slight delay in between so that they arrive in the intended order\n  Thread.sleep(10);\n}\n\n\n\nIn the context of your answer, that it's intentional for the updates to be async...\n\n\n\twhy shuffle them?\n\twhy is there now a sleep call with an explicit comment \"...so that they arrive in the intended order\" ... if there is an \"intended\" order why would you want them to be async?\n\n\n\nthe other SendUpdateToReplicaTask/AsyncUpdateWithRandomCommit usages exhibit the same behavior of a \"sleep\" in between {{ threadpool.submit(task); }} calls with similar comments about wanting to \"...ensure requests are sequential...\" hence my question about why threadpools are being used at all. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15323101",
            "date": "2016-06-09T18:50:15+0000",
            "content": "When update2 (say a partial update) arrives before update1 (say a full update, on which update2 depends), then the call for indexing update2 is a blocking call (which finishes either after update1 is indexed, or timeout is reached).\n\nThe intention was to:\n\n\tshuffle the updates (so that the 3 updates are in one of the 6 possible permutations, one of those permutations being in-order)\n\tsend them out in sequence of the shuffle\n\thave them arrive at Solr in the intended order (as intended in steps 1 and 2). However, since an out of order update waits for the dependent update and blocks the call until such a dependent update arrives (or timeout is reached), the intention is to have these calls non-blocking.\n\n\n\nSo, I wanted to send updates out sequentially (deliberately re-ordered, through a shuffle), but asynchronously (so as to keep those calls non-blocking).\n\n...My impression, based on the entirety of that method, was that the intent of the test was to bypass the normal distributed update logic and send carefully crafted \"simulated\" updates direct to each replica, such that one repliica got the (simulated from leader) updates \"in order\" and another replica got the (simulated from leader) updates \"out of order\"\nThat is exactly my intention.\n\nif the point was for replica2 to get the (simulated from leader) updates \"out of order\" then why shuffle them - why not explicitly put them in the \"wrong\" order?\nThere could be possibly 6 permutations in terms of the mutual ordering of the 3 updates, so I used shuffle instead of choosing a particular \"wrong\" ordering. Of course, one of those 6 permutations is the \"right\" order, so that case is not consistent with the name of the test; I can make a fix to exclude that case.\n\nif the goal was send them asynchronously, and try to get them to happen as concurrently as possible (as you indicated above in your answer to my question) then what was the point of the \"shuffle\" ?\nI think I was trying: (a) asynchronously (so that out of order update doesn't block out the next update request that sends a dependent order), (b) intention was not really to test for race conditions (i.e. not really \"as concurrently as possible\", but maybe I don't understand the phrase correctly), but just to be concurrent enough so that a dependent update arrives before an out of order update times out. \n\nwhy is there now a sleep call with an explicit comment \"...so that they arrive in the intended order\" ... if there is an \"intended\" order why would you want them to be async?\n\nThe point of this was to avoid situations where the shuffled list (and intended order for that testcase) was, say, \"update1, update3, update2\", but it actually arrived at the Solr server in the order \"update1, update2, update3\" due to parallel threads sending the updates at nearly the same time.\n\nif there is an \"intended\" order why would you want them to be async?\nSo that the calls are non-blocking. The first out of order partial update request will block the call until timeout/dependent update is indexed.\n\nDo you think this makes sense? I am open to revise this entire logic if you suggest. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15323784",
            "date": "2016-06-10T03:05:18+0000",
            "content": "\nWhen update2 (say a partial update) arrives before update1 (say a full update, on which update2 depends), then the call for indexing update2 is a blocking call (which finishes either after update1 is indexed, or timeout is reached).\n\nAhhh... now it makes sense to me. The part I wasn't getting before was that update2 blocks on the replica until it sees the update1 it is dependent on.\n\nI feel like there is probably a way we could write a more sophisticate \"grey box\" type test for this leveraging callbacks in the DebugFilter, but I'm having trouble working out what that would really look like.\n\nI think the hueristic approach you're taking here is generall fine for now (as a way to try to run the updates in a given order even though we know there are no garuntees) but i have a few suggestions to improve things:\n\n\n\tlots more comments in the test code to make it clear that we use multiple threads because each update may block if it depends on another update\n\treplace the comments on the sleep calls to make it clear that while we can't garuntee/trust what order the updates are executed in since multiple threads are involved, we're trying to bias the thread scheduling to run them in the order submitted\n\t\n\t\t(the wording right now seems definitive and makes the code look clearly suspicious)\n\t\n\t\n\tcreate atLeast(3) updates instead of just a fixed set of \"3\" so we increase our odds of finding potential bugs when more then one update is out of order.\n\tloop over multiple (random) permutations of orderings of the updates\n\t\n\t\tdon't worry about wether a given ordering is actually correct, that's a valid random ordering for the purposes of the test\n\t\ta simple comment saying we know it's possible but it doesn't affect any assumptions/assertions in the test is fine\n\t\n\t\n\tfor each random permutation, execute it (and check the results) multiple times\n\t\n\t\tthis will help increase the odds that the thread scheduling actaully winds up running our updates in the order we were hoping for.\n\t\n\t\n\tessentially this should be a a micro \"stress test\" of updates in arbitrary order\n\n\n\nSomething like...\n\n\nfinal String ID = \"0\";\nfinal int numUpdates = atLeast(3);\nfinal int numPermutationTotest = atLeast(5);\nfor (int p = 0; p < numPermutationTotest; p++) {\n  del(\"*:*);\n  commit();\n  index(\"id\",ID, ...); // goes to all replicas\n  commit();\n  long version = assertExpectedValuesViaRTG(LEADER, ID, ...);\n  List<UpdateRequest> updates = makeListOfSequentialSimulatedUpdates(ID, version, numUpdates);\n  for (UpdateRequest req : updates) {\n    assertEquals(0, REPLICA_1.requets(req).getStatus());\n  }\n  Collections.shuffle(updates, random());\n  // this method is where you'd comment the hell out of why we use threads for this,\n  // and can be re-used in the other place where a threadpool is used...\n  assertSendUpdatesInThreadsWithDelay(REPLICA_0, updates, 100ms);\n  for (SolrClient client : NONLEADERS) [\n    // assert value on replica matches original value + numUpdates\n  }\n}\n\n\n\n\n\nAs a related matter \u2013 if we are expecting a replica to \"block & eventually time out\" when it sees an out of order update, then there should be a white box test asserting the expected failure situation as well \u2013 something like...\n\n\nfinal String ID = \"0\";\ndel(\"*:*);\ncommit();\nindex(\"id\",ID, ...);\nUpdateRequest req = simulatedUpdateRequest(version + 1, ID, ...);\nTimer timer = new Timer();\ntimer.start();\nSolrServerException e = expectThrows(() -> { REPLICA_0.request(req); });\ntimer.stop();\nassert( /* elapsed time of timer is at least the X that we expect it to block for */ )\nassert(e.getgetHttpStatusMesg().contains(\"something we expect it to say if the update was out of order\"))\nassertEquls(/* whatever we expect in this case */, e.getHttpStatusCode());\n\n "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-15323803",
            "date": "2016-06-10T03:23:49+0000",
            "content": "I had a question come up today that I wanted to ask for posterity. What gets returned between the time we update one of these and a commit occurs? The old value or the new one? I'd assumed the old one but wanted to be sure.\n\nAnd explicitly this only applies to fields for which indexed=false I see, which answers another of the questions that came up. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15323805",
            "date": "2016-06-10T03:29:41+0000",
            "content": "I had a question come up today that I wanted to ask for posterity. What gets returned between the time we update one of these and a commit occurs? The old value or the new one? I'd assumed the old one but wanted to be sure.\n\nin theory, it's exactly identical to the existing behavior of an atomic update: searching returns only the committed values, an RTG will return the \"new\" (uncommitted) value. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15330923",
            "date": "2016-06-15T00:28:36+0000",
            "content": "\n\n\nI know Ishan has been working on improving the tests based on my last batch of feedback \u2013 since then I've been reviewing the non test changes in the lastest patch.\n\nHere's my notes about specific class/methods as I reviewed them in individually....\n\n\nJettySolrRunner\n\n\tjavadocs, javadocs, javadocs\n\n\n\nXMLLoader + JavabinLoader\n\n\twhy is this param checks logic duplicated in these classes?\n\twhy not put this in DUP (which already has access to the request params) when it's doing it's \"FROMLEADER\" logic?\n\n\n\nAddUpdateCommand\n\n\tthese variables (like all variables) should have javadocs explaining what they are and what they mean\n\t\n\t\tpeople skimming a class shouldn't have to grep the code for a variable name to understand it's purpose\n\t\n\t\n\thaving 2 variables here seems like it might be error prone?  what does it mean if prevVersion < 0 && isInPlaceUpdate == true ? or 0 < prevVersion && isInPlaceUpdate == false ?\n\t\n\t\twould it make more sense to use a single long prevVersion variable and have a public boolean isInPlaceUpdate() that simply does {{return (0 < prevVersion); }} ?\n\t\n\t\n\n\n\nTransactionLog\n\n\tjavadocs for both the new write method and the existig write method\n\t\n\t\texplain what \"prevPointer\" means and note in the 2 arg method what the effective default \"prevPoint\" is.\n\t\n\t\n\twe should really have some \"int\" constants for refering to the List indexes involved in these records, so instead of code like entry.get(3) sprinkled in various classes like UpdateLog and PeerSync it can be smething more readable like entry.get(PREV_VERSION_IDX)\n\n\n\n\nUpdateLog\n\n\tjavadocs for both the new LogPtr constructure and the existing constructor\n\t\n\t\texplain what \"prevPointer\" means and note in the 2 arg constructure what the effective default \"prevPoint\" is.\n\t\n\t\n\tadd(AddUpdateCommand, boolean)\n\t\n\t\tthis new code for doing lookups in map, prevMap and preMap2 seems weird to me (but admitedly i'm not really an expert on UpdateLog in general and how these maps are used\n\t\twhat primarily concerns me is what the expected behavior is if the \"id\" isn't found in any of these maps \u2013 it looks like prevPointer defaults to \"-1\" regardless of whether this is an inplace update ... is that intentional? ... is it possible there are older records we will miss and need to flag that?\n\t\tie: do we need to worry about distinguising here between \"not an in place update, therefore prePointer=-1\" vs \"is an in place update, but we can't find the prevPointer\" ??\n\t\tassuming this code is correct, it might be a little easier to read if it were refactored into something like:\n\n// nocommit: jdocs\nprivate synchronized long getPrevPointerForUpdate(AddUpdateCommand cmd) {\n  // note: sync required to ensure maps aren't changed out form under us\n  if (cmd.isInPlaceUpdate) {\n    BytesRef indexedId = cmd.getIndexedId();\n    for (Map<BytesRef,TransactionLog> currentMap : Arrays.asList(map, prevMap, prevMap2)) {\n      LogPtr prevEntry = currentMap.get(indexedId);\n      if (null != prevEntry) {\n        return prevEntry.pointer;\n      }\n    }\n  }\n  return -1; // default when not inplace, or if we can't find a previous entry\n}\n\n\n\t\n\t\n\tapplyPartialUpdates\n\t\n\t\tit seems like this method would be a really good candidate for some direct unit testing?\n\t\t\n\t\t\tie: construct a synthetic UpdateLog, and confirm applyPartialUpdates does the right thing\n\t\t\n\t\t\n\t\tthe sync block in this method, and how the resulting lookupLogs list is used subsequently, doesn't seem safe to me \u2013 particularly the way getEntryFromTLog calls incref/decref on each TransactionLog as it loops over that list...\n\t\t\n\t\t\twhat prevents some other thread from decref'ing one of these TransactionLog objects (and possibly auto-closing it) in between the sync block and the incref in getEntryFromTLog?\n\t\t\t\n\t\t\t\t(most existing usages of TransactionLog.incref() seem to be in blocks that sync on the UpdateLog \u2013 and the ones that aren't in sync blocks look sketchy to me as well)\n\t\t\t\n\t\t\t\n\t\t\tin general i'm wondering if lookupLogs should be created outside of the while loop, so that there is a consistent set of \"logs\" for the duration of the method call ... what happens right now if some other thread changes tlog/prevMapLog/prevMapLog2 in between iterations of the while loop?\n\t\t\n\t\t\n\t\tshouldn't we make some sanity check assertions about the results of getEntryFromTLog? \u2013 there's an INVALID_STATE if it's not an ADD or a list of 5 elements, but what about actually asserting that it's either an ADD or an UPDATE_INPLACE? ... what about asserting the doc's uniqueKey matches?\n\t\t\n\t\t\t(because unless i'm missing something, it's possible for 2 docs to have the same version, so if there is a glitch in the pointer we can't just trust the version check can we?)\n\t\t\n\t\t\n\t\tpartialUpdateEntry seems like a missleading variable name ... can't it be either a full document, or partial update (not in place), or an UPDATE_INPLACE partial update?\n\t\tif there is only 1 way to break out of this while loop, then the method would probably be easier to make sense of if the applyOlderUpdate and return 0 calls replaced the break statement\n\t\tsemi-related: while (true) is generally a red flag: it seems like might be better if it was refactored inot a while (0 <= prevPointer) loop?\n\t\n\t\n\tgetEntryFromTLog\n\t\n\t\tI don't really see the point of using get(i) over and over .. why not a simple for (TransactionLog log : lookupLogs) ?\n\t\twhy is the Exception & Error handling here just a debug log? shouldn't that be a pretty hard fail?\n\t\tas mentioned above re: applyPartialUpdates, isn't is possible for 2 diff docs to have the same version? if we get unlucky and those 2 docs, with identical versions, at the same position in diff TransactionLog files then isn't a sanity check of the doc ID in applyPartialUpdates too late? ... if applyPartialUpdates tries again it's just going to keep getting the same (wrong) document.  It seems like this method actaully needs to know the ID it's looking for, and \"skip\" any entries thta don't match, checking the next (older) TransactionLog\n\t\n\t\n\tlookupPartialUpdates\n\t\n\t\twhat is the purpose/intent of this method? ... it seems to be unused.\n\t\n\t\n\tdoReplay\n\t\n\t\tswitch statement ordering...\n\t\t\n\t\t\tin theory, switch statement cases should be ordered from most likeley to least likely (it's a microoptimization, but in heavily executed loops it might be worth it)\n\t\t\tso i wouldn't inject UPDATE_INPLACE at the begining of the switch \u2013 it should definitely come after ADD, probably best to put it at the end of the list\n\t\t\n\t\t\n\t\twhy is entry.get(2) commented out? and why does it say \"idBytes\" ? ... isn't slot #2 the prevPointer?  copy paste confusion from \"ADD\" ?\n\t\t\n\t\t\tif slot#2 really isn't needed in this code, get rid of the missleading comment about idBytes and replace it with an explicit comment that prevVersion isn't needed for reply.\n\t\t\n\t\t\n\t\n\t\n\n\n\nPeerSync\n\n\tditto comments about switch statement ordering from above comments about UpdateLog\n\ta lot of code here looks duplicated straight from UpdateLog.doReplay\n\t\n\t\tI realize that's true for the other case values as well, but bad code shouldn't be emulated\n\t\tlets refactor this duplicated code into a new public static AddUpdateCommand updateCommandFromTlog(List tlogEntry) method in UpdateLog and re-use it here.\n\t\n\t\n\tlog.info looks wrong here ... especially inside of an if (debug) block ... pretty sure this should be log.debug like the other case blocks\n\n\n\nDirectUpdateHandler2\n\n\tI don't really understand why we need any schema/DocValue checks here?\n\tIf cmd.isInPlaceUpdate is true, then doesn't that mean the update is by definition an in place update that only contains values for DV fields? ... wasn't it the responsibility of the upstream code that set that value to true to ensure that?\n\t\n\t\tif not, why can't it be? (ie: why can't we move the schema checks upstream, and out of the sync block here?\n\t\n\t\n\tif that's not what cmd.isInPlaceUpdate means, then why isn't there any error handling here in the event that non-DV field/values are found in the luceneDocument? ... doesn't that mean we need to fall back to the original writer.updateDocument call?\n\tIf it is neccessary to do SchemaField validation here for some reason, then shouldn't it be the exact same validation done in AtomicUpdateDocumentMerger.isSupportedForInPlaceUpdate ?\n\n\n\nAtomicUpdateDocumentMerger\n\n\tisSupportedForInPlaceUpdate\n\t\n\t\tshouldn't this method either be static or final?  the rules don't change if someone subclasses AtomicUpdateDocumentMerger do they?\n\t\twhy isn't TrieDateField also valid? ... could this just be checking for instanceof TrieField ?\n\t\t\n\t\t\tparticularly suspicious since doInPlaceUpdateMerge does in fact check instanceof TrieField\n\t\t\n\t\t\n\t\tif the intent is truely to only support \"numerics\" then, why not instanceof NumericValueFieldType ?\n\t\tshouldn't we also check \"not-indexed\" and \"not-stored\" here as well?\n\t\n\t\n\tdoInPlaceUpdateMerge\n\t\n\t\twhy does this method have 3 args?  can't all of the neccessary info be deterined from the AddUpdateCommand ?\n\t\tthis method seems like another good candidate for some explicit unit testing...\n\t\t\n\t\t\tbuild up an index & tlog with some explicitly crated non trivial docs/updates, then call this method with a variety of inputs and assert the expected modifications to the AddUpdateCommand (or assert no modifications if they aren't viable in place update candaites\n\t\t\tthen hard commit everything in the tlog and assert that all the same calls return the exact same output/modifications.\n\t\t\n\t\t\n\t\twe should probably continue to assume the common case is not to need (in-place) updates ... just regular adds.  in which case anything we can do to short circut out faster \u2013 before any checks that require stuff like SchemaField \u2013 wouldn't reordering the checks in the loop to something like this be equivilent to what we have currently but faster in the common case? ...\n\nfor (SolrInputField f : sdoc) {\n  final String fname = f.getName();\n  if (idField.getName().equals(fname)) {\n    continue;\n  }\n  if (! f.getValue() instanceof Map) {\n    // doc contains field value that is not an update, therefore definitely not an inplace update\n    return false;\n  }\n  if (!isSupportedForInPlaceUpdate(schema.getField(fname))) {\n    // doc contains update to a field that does not support in place updates\n    return false;\n  }\n}\n\n\n\t\t\n\t\t\tEven if i've overloked something and that code isn't better, i think in general the \"is this sdoc a candidate for in place updating\" logic should be refactored into a public static helper method that has some unit tests.\n\t\t\n\t\t\n\t\tthis method calls RealTimeGetComponent.getInputDocumentFromTlog but doesn't check for the special DELETED return value...\n\t\t\n\t\t\tdefinitely smells like a bug ... there are many assumptions made about uncommittedDoc as long as it's non-null\n\t\t\tbased on this, now i really want to see more testing of in place updates mixed with document deletions:\n\t\t\t\n\t\t\t\tsome explicit single node testing of \"add docX, delete docX, do a 'set':'42' on docX\"\n\t\t\t\tintroduce some randomized deleteById calls into the randomized single/multi node tests\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\tthis method calls RealTimeGetComponent.getVersionFromTlog which has docs that say \"If null is returned, it could still be in the latest index.\"\n\t\t\n\t\t\ti don't see any code accounting for that possibility, cmd.prevVersion is just blindly assigned the null in that case ... which could lead to an NPE since it's declared as public long prevVersion\n\t\t\tthe fact that this hasn't caused an NPE in testing is a red flag that there is a code path not being tested here ... but at a glance i can't really tell what it is?\n\t\t\n\t\t\n\t\tIn general, I guess I don't really understand why this method is jumping through as many hoops as it is with the RTG code?\n\t\t\n\t\t\tit seems to duplicate a lot of functionality already in RealTimeGetComponent.getInputDocument ... why not just use that method?\n\t\t\tif the concern is avoiding the searcher.doc(docid) call to get all stored fields, perhaps RealTimeGetComponent.getInputDocument could be refactored to make it easier to re-use most of the functionality here? ... not sure what would make the most sense off the top of my head.\n\t\t\tat a minimum it seems like using SolrIndexSearcher.decorateDocValueFields(...) would make more sense then doing it ourselves as we loop over the fields \u2013 we can even pass in the explicit list of field names we know we care about based on the SolrInputDocument (or even just the field names we know use \"inc\" if that's all we really need)\n\t\t\t\n\t\t\t\t(Or is there something i'm missing about why using decorateDocValueFields would be a mistake here?)\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\tin the switch statement, shouldn't we be using the doSet and doInc methods to actaully cary out the operations?\n\t\t\n\t\t\tthat would simplify the \"inc\" case a lot\n\t\t\n\t\t\n\t\tthe default on the switch statement looks sketchy to me ... i understand that only \"inc\" and \"set\" are supported, but why does this method only warn if it sees something else? shouldn't this be a hard failure?\n\t\t\n\t\t\tfor that matter: shouldn't the instanceof Map check when looping over the fields at the begining of the method short circut out if the Map contains an operation that isn't one of the supported \"in place update\" operations?\n\t\t\tin fact: if we pre-checked the Maps only contained \"set\" and \"inc\", and used something like decorateDocValueFields (or did the equivilent ourselves in a smaller loop) then couldn't we also simplify this method a lot by just delegating to the existing merge(SolrInputDocument,SolrInputDocument) method?\n\t\t\n\t\t\n\t\tthese assumptions seem sketchy, if that's the only reason for these \"else\" blocks then let's include some asert fieldName.equals(...) calls to prove it...\n\n          } else { // for version field, which is a docvalue but there's no set/inc operation\n            ...\n          }\n        } else { // for id field\n          ...\n\n\n\t\t\n\t\t\tin particluar i'm curious about the VERSION_FIELD...\n\t\t\t\n\t\t\t\tthis method is only called from one place \u2013 DistributedUpdateProcessor.getUpdatedDocument \u2013 and in the existing code of that method, when a SolrInputDocument is fetched from RealTimeGetComponent, the VERSION_FIELD is explicitly removed from it before using it & returning.\n\t\t\t\tshould this method also be explicitly removing the VERSION_FIELD field?  and/or should the caller (getUpdatedDocument) be removing it consistently before returning?\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\n\t\n\n\n\n\nRealTimeGetComponent\n\n\tprocess\n\t\n\t\tI like this SearcherInfo refactoring, but a few suggestions:\n\t\t\n\t\t\tit should be promoted into a (private) static (inner) class ... no need for a new class instance every time RealTimeGetComponent.process is called.\n\t\t\tadd a one arg constructor and pass the SolrCore to that.\n\t\t\tjavadocs, javadocs, javadocs .... note that it's not thread safe\n\t\t\tlet's make searcherHolder and searcher private, and replace direct searcher access with:\n\npublic SolrIndexSearcher getSearcher() {\n  assert null != searcher : \"init not called!\";\n  return searcher;\n}\n\n\n\t\t\n\t\t\n\t\tin the switch statement, it seems like there is a lot of code duplicated between the ADD and UPDATE_INPLACE cases\n\t\t\n\t\t\twhy not consolidate those cases into one block of code using (a modified) resolveFullDocument which can start with a call to toSolrDoc(...) and then return immediately if the entry is UpdateLog.ADD ?\n\t\t\n\t\t\n\t\n\t\n\tresolveFullDocument\n\t\n\t\tsee comments above about modifying this method to call toSolrDocument itself rather then expecting as input, and return early if the entry is an UpdateLog.ADD\n\t\tlet's put an assert 0==lastPrevPointer in this else block in case someone improves/breaks ulog.applyPartialUpdates to return -2 in the future...\n\n    } else { // i.e. lastPrevPointer==0\n\n\n\t\tsince ulog.applyPartialUpdates(...) is a No-Op when prevPointer == -1, can't we remove the redundent calls to mergePartialDocWithFullDocFromIndex & reopenRealtimeSearcherAndGet before and after calling ulog.applyPartialUpdates(...) ... ie:\n\n    long prevPointer = (long) logEntry.get(2);\n    long prevVersion = (long) logEntry.get(3);\n    // this is a No-Op if prevPointer is already negative, otherwise...\n    // get the last full document from ulog\n    prevPointer = ulog.applyPartialUpdates(idBytes, prevPointer, prevVersion, partialDoc);\n    if (-1 == prevPointer) {\n      ...\n    } else if (0 < prevPointer) {\n      ...\n    } else {\n      assert 0 == prevPointer;\n      ...\n    }\n\n\n\t\t\n\t\t\tIf there is some reason i'm not seeing why it's important to call mergePartialDocWithFullDocFromIndex & reopenRealtimeSearcherAndGet before calling ulog.applyPartialUpdates, then perhaps we should at least refactor the \"if mergedDoc == null, return reopenRealtimeSearcherAndGet\" logic into mergePartialDocWithFullDocFromIndex since that's the only way it's ever used.\n\t\t\n\t\t\n\t\n\t\n\tmergePartialDocWithFullDocFromIndex\n\t\n\t\tsince this is a private method, what's the expected usage of docidFromIndex ? ... it's never used, so can we refactor it away?\n\t\tsee previous comment about refactoring this method to automatically return reopenRealtimeSearcherAndGet(...) when it would otherwise return null\n\t\n\t\n\treopenRealtimeSearcherAndGet\n\t\n\t\tjavadocs, javadocs, javadocs\n\t\tsince this method is only used in conjunction with mergePartialDocWithFullDocFromIndex, if the code is refactored so that mergePartialDocWithFullDocFromIndex calls this method directly (see suggestion above), we could make a small micro-optimization by changing the method sig to take in a Term to (re)use rather then passing in idBytes and calling core.getLatestSchema().getUniqueKeyField() twice.\n\t\tre: the INVALID_STATE ... is that really a fatal error, or should this method be accounting for the possibility of a doc that has been completley deleted (or was never in the index) in a diff way?\n\t\n\t\n\tgetInputDocumentFromTlog\n\t\n\t\tlets put an explicit comment here noting that we are intentionally falling through to the Updatelog.ADD case\n\t\n\t\n\tgetVersionFromTlog\n\t\n\t\tbased on it's usage, should this method be changed to return -1 instead of null?  ... not clear to me from the given caller usage ... if so should be be declared to return long instead of Long ?\n\t\n\t\n\n\n\n\nDistributedUpdateProcessor\n\n\tSimilar question from AddUpdateCommand: do we really need 2 distinct params here, or would it be cleaner / less error-prone to have a single distrib.inplace.prevversion which indicates we're doing an inplace update if it's a positive # ?\n\tversionAdd\n\t\n\t\t\"Something weird has happened\" ... perhaps waitForDependentUpdates should return the lastFoundVersion so we can use it in this error msg? ... \"Dependent version not found after waiting: ...\"\n\t\t\"TODO: Should we call waitForDependentUpdates() again?\" ... i don't see how that would help?  if we get to this point it definitely seems like a \"WTF?\" fail fast situation.\n\t\tthe way the existing code in this method has been refactored into an \"else\" block (see comment: // non inplace update, i.e. full document update) makes sense, but the way the Long lastVersion declaration was refactored out of that block to reuse with the \"if isInPlaceUpdate\" side of things is a bit confusing and doesn't seem to actaully simplify anything...\n\t\t\n\t\t\tIt's not used after the if/else block, so there's no reason to declare it before the \"if\" statement\n\t\t\tby definition it must be null in the \"else\" case, so if (lastVersion == null) will also be true in that code path\n\t\t\tit seems simpiler to just let both the \"if\" and \"else\" branches declare/define their own \"Long lastVersion\" and not risk any confusion about why that variable needs to be \"shared\"\n\t\t\n\t\t\n\t\n\t\n\twaitForDependentUpdates\n\t\n\t\tjavadocs, javadocs, javadocs\n\t\tfetchFromLeader is always true? why not eliminate arg?\n\t\tI'm not a concurrency expert, but is this control flow with the sync/wait actaully safe? ... my understanding was the conditional check you're waiting on should always be a loop inside the sync block?\n\t\teven if there are no spurious returns from wait, logging at \"info\" level every 100ms is excessive ... logging that often at trace seems excessive.\n\t\t\n\t\t\twhy don't we log an info once at the start of method (since our of order updates should be rare) and once at debug anytime lastFoundVersion changes? (diff log message if/when lastFoundVersion is == or > prev)\n\t\t\tthe \"Waiting attempt\" counter \"i\" doesn't seem like useful info to log given how wait(...) works\n\t\t\n\t\t\n\t\t\"...Dropping current update.\" - that log msg seems missleading, this method doesn't do anything to drop the current update, it just assumes the current update will be droped later\n\t\ti don't really see the point of the boolean foundDependentUpdate variable... why not change the only place where it's set to true to return immediately?\n\t\tfetchMissingUpdateFromLeader can return null, but that possibility isn't handled here.\n\t\tif (uc instanceof AddUpdateCommand) ... what if it's not?\n\t\t\n\t\t\tcurrently it's just silently ignored\n\t\t\tis this a viable scenerio that needs accounted for, or an exceptional scenerio that should have error checking?\n\t\t\tlooks like maybe it's just a confusing way to do a null check?\n\t\t\n\t\t\n\t\t((System.nanoTime()-startTime)/1000000 ) ... that's a missleading thing to include in the Exception\n\t\t\n\t\t\twe didn't wait that long, we waited at most 5 seconds \u2013 who knows how long we spent calling fetchMissingUpdateFromLeader & executing it.\n\t\t\n\t\t\n\t\n\t\n\tfetchMissingUpdateFromLeader\n\t\n\t\tjavadocs, javadocs, javadocs\n\t\tshould we really be constructing a new HttpSolrClient on the fly like this?\n\t\tis this on the fly SolrClient + GenericSolrRequest going to work if/when the security auth/acl features are in use in the solr cluster?\n\t\tthis seems to assume the node that forwarded us the current request (via get(DISTRIB_FROM) is still the leader \u2013 but what if there was a leader election?\n\t\t\n\t\t\tif the leader failed, and a new one was elected, isn't that a pretty viable/likeley reason why waitForDependentUpdates timed-out and needed to call fetchMissingUpdateFromLeader in the first place?\n\t\t\n\t\t\n\t\te.printStackTrace(); ... huge red flag that serious error handling is missing here.\n\t\tThis method seems like it expects the possibility that missingUpdates will contain more then one entry, and if it does contain more then one entry it will convert/copy all of them into the updates list \u2013 but then it will completley ignore all but the first one.\n\t\t\n\t\t\tif we don't expect more then 1, why not assert that?\n\t\t\tif we expect more then one, and they are all important, why don't we return List<UpdateCommand> ?\n\t\t\tif we expect more then one, but only care about one in particularly \u2013 why loop over all of them?\n\t\t\t\n\t\t\t\thow do we know for sure which one in the list is the one we care about?\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\tditto comments about PeerSync & UpdateLog and creating a public static AddUpdateCommand updateCommandFromTlog(List tlogEntry) somwhere that can be reused here as well\n\t\tswitch statement should have some sort of default case ... even if i'ts just to throw an error because anything but an ADD or UPDATE_INPLACE is impossible\n\t\tneed to future proof this code against the posibility of other stuff down the road\n\t\n\t\n\n\n\nAnd here are some general overall comments / impressions I got while reviewing the code and then edited up once i was all finished...\n\n\n\n\tGiven that this patch requires some non-trivial changes to the types of records that go in the update log, and requires corisponding changes to PeerSync, it seems like there should definitely be some very explicit testing of log reply and peer sync\n\t\n\t\tie: TestReplay and PeerSyncTest should be updated to include a variety of scenerios involving in-place updates\n\t\n\t\n\tafter seeing how complex & hairy some of the new code has to be around the diff handling of \"in-place atomic updates\", vs existing \"atomic updates\" (that aren't in place) It seems like we should definitely have more test code that mixes and matches diff types of \"updates\"\n\t\n\t\tstatic, non randomized, examples of explicit tests we should definitely have...\n\t\t\n\t\t\ta doc gets a sequence of atomic updates each containing multiple \"in place\" inc/set ops on diff fields\n\t\t\ta doc gets a sequence of atomic updates each containing multiple inc/set ops, where a single update may have a mix of \"in place\" vs \"not in place\" eligable ops (due to diff fields having diff docvalue/stored settings)\n\t\t\n\t\t\n\t\tour randomized (cloud and non-cloud) testing of in-place updates should also include updates to the canidate docs that may ocasionally not be viable \"in-place\" updates (because they involved updates to additional non-dv fields)\n\t\tin all of these tests we should be checking that both the RTG and regualr search results make sense\n\t\n\t\n\twe also need a lot more testing of various deleteById and deleteByQuery commands mixed with in-place atomic updates\n\t\n\t\tboth deleteByQuerys against the DV fields used in the in-place atomic updates as well as DBQs against other fields in the documents\n\t\ttest the results of (uncommited) RTG as well as searches when these deletes are intermixed with in-place atomic updates\n\t\ttest the results of peer sync and reply when deletes are mixed with in-place atomic updates.\n\t\ttest that we correctly get 409 error codes when trying to do in-place updates w/optimistic concurrency after a delete (and vice versa: DBQ/dID afte in-place update)\n\t\n\t\n\tall logging needs heavily audited\n\t\n\t\tthere's a lot of logging happening at the info and debug level that should probably be much lower.\n\t\tlikewise there may be a few existing info or debug msgs that might be good candidates for warn or error level msgs.\n\t\n\t\n\tuniqueKey and Exception/log msgs\n\t\n\t\tthere is a lot of log msgs or Exception msgs that cite a version# when reporting a problem, but don't include the uniqueKey of the document involved\n\t\tthese messages aren't going to be remotely useful to end users w/o also including the (human readable) uniqueKey of the document involved.\n\t\n\t\n\tit feels like we now have a really large number of methods involved in the merging/combining/converting of documents to apply atomic updates (\"in place\" or otherwise) ... either for use in RTG, or for use when writing updates to disk, or from reading from the tlog, etc...\n\t\n\t\tthe ones that jump out at me w/o digging very hard...\n\nRealTimeGetComponent.resolveFullDocument\nRealTimeGetComponent.toSolrDoc\nRealTimeGetComponent.mergePartialDocWithFullDocFromIndex\nUpdateLog.applyPartialUpdates\nUpdateLog.applyOlderUpdate\nAtomicUpdateDocumentMerger.merge\nAtomicUpdateDocumentMerger.doInPlaceUpdateMerge\n\n\n\t\ti can't help but wonder if there is room for consolidation?\n\t\tin many cases these \"merge\" methods actually delegate to other \"merge\" methods, before/after applying additional logic \u2013 in which case at a minimum using @link or @see tags in the javadocs to make this (intentional) relationship/similarity more obvious would be helpful.\n\t\tin cases where methods do not delegate to eachother, or have any relationship w/eachother, having @link mentions of eachother in the javadocs to compare/constrast why they are different would be equally helpful.\n\t\tand who knows \u2013 perhaps in the process of writing these docs we'll find good oportunities to refactor/consolidate\n\t\n\t\n\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15337711",
            "date": "2016-06-18T10:00:02+0000",
            "content": "As I was incorporating Hoss' suggestions, I wrote a test for DV updates with DBQ on updated values. This was failing if there was no commit between the update and the DBQ. I think this is due to LUCENE-7344. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15355424",
            "date": "2016-06-29T15:21:24+0000",
            "content": "New patch:\n\n\tAddressed most of code level comments from Hoss. I think the readability of the code has now improved and also more robust exception handling. (Thanks Hoss). I shall reply inline to all of your suggestions, perhaps along with my next patch.\n\tNocommits remain, I think these are all related to Javadocs.\n\tStress test now has DBQ and DBI. It seems to pass 1000 runs successfully.\n\tPeerSync, TestReplay etc. and a few more unit tests remain.\n\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15359956",
            "date": "2016-07-02T03:38:42+0000",
            "content": "\n\tAdded Javadocs, fixed some nocommits\n\tAddressed an issue due to which in-place updating of non-existing DVs was throwing exceptions. For this, it was needed to know which fields have already been added to the index, so that if an update is needed to non-existent DV, then we can resort to a traditional full document atomic update. This check could've been easy if access to IW.globalFieldNumberMap was possible publicly. Instead resorted to checking with the RT searcher's list of DVs, and if field not found there then getting the document from tlog (RTG) and checking if the field exists in that document.\n\tAdded some simple tests to PeerSyncTest and TestRecovery for in-place updates.\n\tTODO: A few more tests remain, few nocommits remain (mostly test code related).\n\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15361862",
            "date": "2016-07-05T00:42:59+0000",
            "content": "New patch fixing all nocommits. Still a few additional tests, which Hoss mentioned, are TODO. Here's a stab at replying to Hoss' comments (Maybe I'll keep updating this comment itself as and when I fix some of the TODO items here):\n\n JettySolrRunner\n\n\tjavadocs, javadocs, javadocs [FIXED]\n\n\n\nXMLLoader + JavabinLoader\n\n\twhy is this param checks logic duplicated in these classes? [Not sure what you mean here, I just set the prevVersion to the cmd here now]\n\twhy not put this in DUP (which already has access to the request params) when it's doing it's \"FROMLEADER\" logic? [Since commitWithin and overwrite was being set here, I thought this is an appropriate place to set the prevVersion to the cmd]\n\n\n\nAddUpdateCommand\n\n\tthese variables (like all variables) should have javadocs explaining what they are and what they mean [FIXED]\n\t\n\t\tpeople skimming a class shouldn't have to grep the code for a variable name to understand it's purpose\n\t\n\t\n\thaving 2 variables here seems like it might be error prone?  what does it mean if prevVersion < 0 && isInPlaceUpdate == true ? or 0 < prevVersion && isInPlaceUpdate == false ? [FIXED: Now just have one variable]\n\t\n\t\twould it make more sense to use a single long prevVersion variable and have a public boolean isInPlaceUpdate() that simply does {{return (0 < prevVersion); }} ? [FIXED]\n\t\n\t\n\n\n\nTransactionLog\n\n\tjavadocs for both the new write method and the existig write method  [FIXED]\n\t\n\t\texplain what \"prevPointer\" means and note in the 2 arg method what the effective default \"prevPoint\" is.\n\t\n\t\n\twe should really have some \"int\" constants for refering to the List indexes involved in these records, so instead of code like entry.get(3) sprinkled in various classes like UpdateLog and PeerSync it can be smething more readable like entry.get(PREV_VERSION_IDX)  [FIXED]\n\n\n\n\nUpdateLog\n\n\tjavadocs for both the new LogPtr constructure and the existing constructor [FIXED]\n\t\n\t\texplain what \"prevPointer\" means and note in the 2 arg constructure what the effective default \"prevPoint\" is.  [FIXED]\n\t\n\t\n\tadd(AddUpdateCommand, boolean)\n\t\n\t\tthis new code for doing lookups in map, prevMap and preMap2 seems weird to me (but admitedly i'm not really an expert on UpdateLog in general and how these maps are used\n\t\twhat primarily concerns me is what the expected behavior is if the \"id\" isn't found in any of these maps \u2013 it looks like prevPointer defaults to \"-1\" regardless of whether this is an inplace update ... is that intentional? ... is it possible there are older records we will miss and need to flag that?  [Yes, this was intentional, and I think it doesn't make any difference. If an \"id\" isn't found in any of these maps, it would mean that the previous update was committed and should be looked up in the index. ]\n\t\tie: do we need to worry about distinguising here between \"not an in place update, therefore prePointer=-1\" vs \"is an in place update, but we can't find the prevPointer\" ?? [I think we don't need to worry. Upon receiving a prevPointer=-1 by whoever reads this LogPtr, it should be clear why it was -1: if the command's {{flags|UpdateLog.UPDATE_INPLACE}} is set, then this command is an in-place update whose previous update is in the index and not in the tlog; if that flag is not set, it is not an in-place update at all, and don't bother about the prevPointer value at all (which is -1 as a dummy value).]\n\t\tassuming this code is correct, it might be a little easier to read if it were refactored into something like:\n\n// nocommit: jdocs\nprivate synchronized long getPrevPointerForUpdate(AddUpdateCommand cmd) {\n  // note: sync required to ensure maps aren't changed out form under us\n  if (cmd.isInPlaceUpdate) {\n    BytesRef indexedId = cmd.getIndexedId();\n    for (Map<BytesRef,TransactionLog> currentMap : Arrays.asList(map, prevMap, prevMap2)) {\n      LogPtr prevEntry = currentMap.get(indexedId);\n      if (null != prevEntry) {\n        return prevEntry.pointer;\n      }\n    }\n  }\n  return -1; // default when not inplace, or if we can't find a previous entry\n}\n\n\n [FIXED: Refactored into something similar to above]\n\t\n\t\n\tapplyPartialUpdates\n\t\n\t\tit seems like this method would be a really good candidate for some direct unit testing? [Added test to UpdateLogTest]\n\t\t\n\t\t\tie: construct a synthetic UpdateLog, and confirm applyPartialUpdates does the right thing\n\t\t\n\t\t\n\t\tthe sync block in this method, and how the resulting lookupLogs list is used subsequently, doesn't seem safe to me \u2013 particularly the way getEntryFromTLog calls incref/decref on each TransactionLog as it loops over that list...\n\t\t\n\t\t\twhat prevents some other thread from decref'ing one of these TransactionLog objects (and possibly auto-closing it) in between the sync block and the incref in getEntryFromTLog?\n\t\t\t\n\t\t\t\t(most existing usages of TransactionLog.incref() seem to be in blocks that sync on the UpdateLog \u2013 and the ones that aren't in sync blocks look sketchy to me as well)\n\t\t\t\n\t\t\t\n\t\t\tin general i'm wondering if lookupLogs should be created outside of the while loop, so that there is a consistent set of \"logs\" for the duration of the method call ... what happens right now if some other thread changes tlog/prevMapLog/prevMapLog2 in between iterations of the while loop?\n\t\t\n\t\t\n\t\tshouldn't we make some sanity check assertions about the results of getEntryFromTLog? \u2013 there's an INVALID_STATE if it's not an ADD or a list of 5 elements, but what about actually asserting that it's either an ADD or an UPDATE_INPLACE? ... [FIXED] what about asserting the doc's uniqueKey matches? [We could do that, but I think it is not necessary]\n\t\t\n\t\t\t(because unless i'm missing something, it's possible for 2 docs to have the same version, so if there is a glitch in the pointer we can't just trust the version check can we?) [I think we can trust a document to be of the same id if the version matches. It is possible for 2 docs to have same version, but then they would have to be from two different shards altogether. Since all of this processing is happening within a particular replica (which obviously belongs to only one shard), I think we can get away safely without asserting the id and just relying on the version.]\n\t\t\n\t\t\n\t\tpartialUpdateEntry seems like a missleading variable name ... can't it be either a full document, or partial update (not in place), or an UPDATE_INPLACE partial update? [FIXED: calling it entry now]\n\t\tif there is only 1 way to break out of this while loop, then the method would probably be easier to make sense of if the applyOlderUpdate and return 0 calls replaced the break statement [FIXED]\n\t\tsemi-related: while (true) is generally a red flag: it seems like might be better if it was refactored inot a while (0 <= prevPointer) loop? [FIXED]\n\t\n\t\n\tgetEntryFromTLog [FIXED]\n\t\n\t\tI don't really see the point of using get(i) over and over .. why not a simple for (TransactionLog log : lookupLogs) ? [FIXED]\n\t\twhy is the Exception & Error handling here just a debug log? shouldn't that be a pretty hard fail? [This shouldn't be a hard fail. This is basically a seek operation on a transaction log at the specified position. If the seek results in an exception due to unable to deserialize the tlog entry, we can ignore it, since it just means we were looking up the wrong tlog. I have added a comment to this effect in the catch block.]\n\t\tas mentioned above re: applyPartialUpdates, isn't is possible for 2 diff docs to have the same version? if we get unlucky and those 2 docs, with identical versions, at the same position in diff TransactionLog files then isn't a sanity check of the doc ID in applyPartialUpdates too late? ... if applyPartialUpdates tries again it's just going to keep getting the same (wrong) document.  It seems like this method actaully needs to know the ID it's looking for, and \"skip\" any entries thta don't match, checking the next (older) TransactionLog [I think this is not a likely scenario, since in a given replica, a version should be unique to a document (I think we have bigger problems if this assumption isn't true).]\n\t\n\t\n\tlookupPartialUpdates\n\t\n\t\twhat is the purpose/intent of this method? ... it seems to be unused. [FIXED: Removed]\n\t\n\t\n\tdoReplay\n\t\n\t\tswitch statement ordering...  [FIXED: I'll add it to my knowledge!]\n\t\t\n\t\t\tin theory, switch statement cases should be ordered from most likeley to least likely (it's a microoptimization, but in heavily executed loops it might be worth it)\n\t\t\tso i wouldn't inject UPDATE_INPLACE at the begining of the switch \u2013 it should definitely come after ADD, probably best to put it at the end of the list\n\t\t\n\t\t\n\t\twhy is entry.get(2) commented out? and why does it say \"idBytes\" ? ... isn't slot #2 the prevPointer?  copy paste confusion from \"ADD\" ?  [FIXED: True, it was copy-paste confusion from ADD. Removed the commented out line.]\n\t\t\n\t\t\tif slot#2 really isn't needed in this code, get rid of the missleading comment about idBytes and replace it with an explicit comment that prevVersion isn't needed for reply. [FIXED: I have removed the spurious commented out lines, refactored that part into a updateCommandFromTlog() method. Does it address your concern here?]\n\t\t\n\t\t\n\t\n\t\n\n\n\nPeerSync\n\n\tditto comments about switch statement ordering from above comments about UpdateLog [FIXED]\n\ta lot of code here looks duplicated straight from UpdateLog.doReplay\n\t\n\t\tI realize that's true for the other case values as well, but bad code shouldn't be emulated\n\t\tlets refactor this duplicated code into a new public static AddUpdateCommand updateCommandFromTlog(List tlogEntry) method in UpdateLog and re-use it here. [FIXED]\n\t\n\t\n\tlog.info looks wrong here ... especially inside of an if (debug) block ... pretty sure this should be log.debug like the other case blocks [FIXED]\n\n\n\nDirectUpdateHandler2\n\n\tI don't really understand why we need any schema/DocValue checks here? [This was unnecessary and I've removed it. I have done a somewhat related refactoring to the AddUpdateCommand.getLuceneDocument(boolean isInPlace) to now only generate a Lucene document that has docValues. This was needed because the lucene document that was originally being returned had copy fields targets of id field, default fields, multiple Field per field (due to FieldType.createFields()) etc., which are not needed for in-place updates.]\n\tIf cmd.isInPlaceUpdate is true, then doesn't that mean the update is by definition an in place update that only contains values for DV fields? ... wasn't it the responsibility of the upstream code that set that value to true to ensure that? [True, it was. However, copy field targets of id field, default fields etc. were added in this doNormalAdd() method itself due to cmd.getLuceneDocument(); I have overloaded that method to tackle the needs of in-place updates and filter out such unnecessary fields being added to the lucene doc]\n\t\n\t\tif not, why can't it be? (ie: why can't we move the schema checks upstream, and out of the sync block here?\n\t\n\t\n\tif that's not what cmd.isInPlaceUpdate means, then why isn't there any error handling here in the event that non-DV field/values are found in the luceneDocument? ... doesn't that mean we need to fall back to the original writer.updateDocument call?\n\tIf it is neccessary to do SchemaField validation here for some reason, then shouldn't it be the exact same validation done in AtomicUpdateDocumentMerger.isSupportedForInPlaceUpdate ? [FIXED: We should do all schema validation there only, I have removed everything from here, except for some filtering logic at cmd.getLuceneDocument()]\n\n\n\nAtomicUpdateDocumentMerger\n\n\tisSupportedForInPlaceUpdate\n\t\n\t\tshouldn't this method either be static or final?  the rules don't change if someone subclasses AtomicUpdateDocumentMerger do they?\n\t\twhy isn't TrieDateField also valid? ... could this just be checking for instanceof TrieField ? [I wasn't sure how to deal with inc for dates, so left dates out of this for simplicity for now]\n\t\t\n\t\t\tparticularly suspicious since doInPlaceUpdateMerge does in fact check instanceof TrieField [FIXED this and moved this check to isSupportedFieldForInPlaceUpdate()]\n\t\t\n\t\t\n\t\tif the intent is truely to only support \"numerics\" then, why not instanceof NumericValueFieldType ?\n\t\tshouldn't we also check \"not-indexed\" and \"not-stored\" here as well? [This logic was in doInPlaceUpdateMerge in the previous patch; refactored it into the isSupportedFieldForInPlaceUpdate() method now]\n\t\n\t\n\n\n\n\n\tdoInPlaceUpdateMerge\n\t\n\t\twhy does this method have 3 args?  can't all of the neccessary info be deterined from the AddUpdateCommand ? [FIXED]\n\t\tthis method seems like another good candidate for some explicit unit testing... [I've refactored this method to make it much simpler and to now call the original merge method. I don't think we need specific tests for this method any longer.]\n\t\t\n\t\t\tbuild up an index & tlog with some explicitly crated non trivial docs/updates, then call this method with a variety of inputs and assert the expected modifications to the AddUpdateCommand (or assert no modifications if they aren't viable in place update candaites\n\t\t\tthen hard commit everything in the tlog and assert that all the same calls return the exact same output/modifications.\n\t\t\n\t\t\n\t\twe should probably continue to assume the common case is not to need (in-place) updates ... just regular adds.  in which case anything we can do to short circut out faster \u2013 before any checks that require stuff like SchemaField \u2013 wouldn't reordering the checks in the loop to something like this be equivilent to what we have currently but faster in the common case? ...\n\nfor (SolrInputField f : sdoc) {\n  final String fname = f.getName();\n  if (idField.getName().equals(fname)) {\n    continue;\n  }\n  if (! f.getValue() instanceof Map) {\n    // doc contains field value that is not an update, therefore definitely not an inplace update\n    return false;\n  }\n  if (!isSupportedForInPlaceUpdate(schema.getField(fname))) {\n    // doc contains update to a field that does not support in place updates\n    return false;\n  }\n}\n\n\n [FIXED: Moved it to isInPlaceUpdate() method, though]\n\t\t\n\t\t\tEven if i've overloked something and that code isn't better, i think in general the \"is this sdoc a candidate for in place updating\" logic should be refactored into a public static helper method that has some unit tests. [FIXED, moved it to isInPlaceUpdate() method.] [Fixed, added test to TestInPlaceUpdatesCopyField]\n\t\t\n\t\t\n\t\tthis method calls RealTimeGetComponent.getInputDocumentFromTlog but doesn't check for the special DELETED return value... [FIXED]\n\t\t\n\t\t\tdefinitely smells like a bug ... there are many assumptions made about uncommittedDoc as long as it's non-null\n\t\t\tbased on this, now i really want to see more testing of in place updates mixed with document deletions:\n\t\t\t\n\t\t\t\tsome explicit single node testing of \"add docX, delete docX, do a 'set':'42' on docX\"\n\t\t\t\tintroduce some randomized deleteById calls into the randomized single/multi node tests\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\tthis method calls RealTimeGetComponent.getVersionFromTlog which has docs that say \"If null is returned, it could still be in the latest index.\" [The javadocs on that method was stale. The method actually returns from index if not found in tlog]\n\t\t\n\t\t\ti don't see any code accounting for that possibility, cmd.prevVersion is just blindly assigned the null in that case ... which could lead to an NPE since it's declared as public long prevVersion\n\t\t\tthe fact that this hasn't caused an NPE in testing is a red flag that there is a code path not being tested here ... but at a glance i can't really tell what it is?\n\t\t\n\t\t\n\t\tIn general, I guess I don't really understand why this method is jumping through as many hoops as it is with the RTG code? [FIXED: Refactored this to modify the RTGC.getInputDocument to now accept a param whether to avoid stored fields or not. This refactoring is a great improvement in terms of readability!]\n\t\t\n\t\t\tit seems to duplicate a lot of functionality already in RealTimeGetComponent.getInputDocument ... why not just use that method?\n\t\t\tif the concern is avoiding the searcher.doc(docid) call to get all stored fields, perhaps RealTimeGetComponent.getInputDocument could be refactored to make it easier to re-use most of the functionality here? ... not sure what would make the most sense off the top of my head.\n\t\t\tat a minimum it seems like using SolrIndexSearcher.decorateDocValueFields(...) would make more sense then doing it ourselves as we loop over the fields \u2013 we can even pass in the explicit list of field names we know we care about based on the SolrInputDocument (or even just the field names we know use \"inc\" if that's all we really need)\n\t\t\t\n\t\t\t\t(Or is there something i'm missing about why using decorateDocValueFields would be a mistake here?)\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\tin the switch statement, shouldn't we be using the doSet and doInc methods to actaully cary out the operations?\n\t\t\n\t\t\tthat would simplify the \"inc\" case a lot\n\t\t\n\t\t\n\t\tthe default on the switch statement looks sketchy to me ... i understand that only \"inc\" and \"set\" are supported, but why does this method only warn if it sees something else? shouldn't this be a hard failure? [FIXED]\n\t\t\n\t\t\tfor that matter: shouldn't the instanceof Map check when looping over the fields at the begining of the method short circut out if the Map contains an operation that isn't one of the supported \"in place update\" operations? [FIXED]\n\t\t\tin fact: if we pre-checked the Maps only contained \"set\" and \"inc\", and used something like decorateDocValueFields (or did the equivilent ourselves in a smaller loop) then couldn't we also simplify this method a lot by just delegating to the existing merge(SolrInputDocument,SolrInputDocument) method? [FIXED]\n\t\t\n\t\t\n\t\tthese assumptions seem sketchy, if that's the only reason for these \"else\" blocks then let's include some asert fieldName.equals(...) calls to prove it...\n\n          } else { // for version field, which is a docvalue but there's no set/inc operation\n            ...\n          }\n        } else { // for id field\n          ...\n\n\n [FIXED]\n\t\t\n\t\t\tin particluar i'm curious about the VERSION_FIELD... [FIXED: I've not added or removed any VERSION_FIELD. If it exists in the cmd.sdoc, it will be copied over to the cmd.partialDoc via the merge() call.]\n\t\t\t\n\t\t\t\tthis method is only called from one place \u2013 DistributedUpdateProcessor.getUpdatedDocument \u2013 and in the existing code of that method, when a SolrInputDocument is fetched from RealTimeGetComponent, the VERSION_FIELD is explicitly removed from it before using it & returning.\n\t\t\t\tshould this method also be explicitly removing the VERSION_FIELD field?  and/or should the caller (getUpdatedDocument) be removing it consistently before returning?\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\n\t\n\n\n\n\nRealTimeGetComponent\n\n\tprocess\n\t\n\t\tI like this SearcherInfo refactoring, but a few suggestions: [FIXED]\n\t\t\n\t\t\tit should be promoted into a (private) static (inner) class ... no need for a new class instance every time RealTimeGetComponent.process is called.\n\t\t\tadd a one arg constructor and pass the SolrCore to that.\n\t\t\tjavadocs, javadocs, javadocs .... note that it's not thread safe\n\t\t\tlet's make searcherHolder and searcher private, and replace direct searcher access with:\n\npublic SolrIndexSearcher getSearcher() {\n  assert null != searcher : \"init not called!\";\n  return searcher;\n}\n\n\n\t\t\n\t\t\n\t\tin the switch statement, it seems like there is a lot of code duplicated between the ADD and UPDATE_INPLACE cases\n\t\t\n\t\t\twhy not consolidate those cases into one block of code using (a modified) resolveFullDocument which can start with a call to toSolrDoc(...) and then return immediately if the entry is UpdateLog.ADD ? [FIXED: but kept the call to toSolrDoc() call outside the resolveFullDocument]\n\t\t\n\t\t\n\t\n\t\n\tresolveFullDocument\n\t\n\t\tsee comments above about modifying this method to call toSolrDocument itself rather then expecting as input, and return early if the entry is an UpdateLog.ADD\n\t\tlet's put an assert 0==lastPrevPointer in this else block in case someone improves/breaks ulog.applyPartialUpdates to return -2 in the future...\n\n    } else { // i.e. lastPrevPointer==0\n\n\n [FIXED]\n\t\tsince ulog.applyPartialUpdates(...) is a No-Op when prevPointer == -1, can't we remove the redundent calls to mergePartialDocWithFullDocFromIndex & reopenRealtimeSearcherAndGet before and after calling ulog.applyPartialUpdates(...) ... ie:\n\n    long prevPointer = (long) logEntry.get(2);\n    long prevVersion = (long) logEntry.get(3);\n    // this is a No-Op if prevPointer is already negative, otherwise...\n    // get the last full document from ulog\n    prevPointer = ulog.applyPartialUpdates(idBytes, prevPointer, prevVersion, partialDoc);\n    if (-1 == prevPointer) {\n      ...\n    } else if (0 < prevPointer) {\n      ...\n    } else {\n      assert 0 == prevPointer;\n      ...\n    }\n\n\n [FIXED]\n\t\t\n\t\t\tIf there is some reason i'm not seeing why it's important to call mergePartialDocWithFullDocFromIndex & reopenRealtimeSearcherAndGet before calling ulog.applyPartialUpdates, then perhaps we should at least refactor the \"if mergedDoc == null, return reopenRealtimeSearcherAndGet\" logic into mergePartialDocWithFullDocFromIndex since that's the only way it's ever used.\n\t\t\n\t\t\n\t\n\t\n\tmergePartialDocWithFullDocFromIndex\n\t\n\t\tsince this is a private method, what's the expected usage of docidFromIndex ? ... it's never used, so can we refactor it away? [FIXED, removed]\n\t\tsee previous comment about refactoring this method to automatically return reopenRealtimeSearcherAndGet(...) when it would otherwise return null [FIXED]\n\t\n\t\n\treopenRealtimeSearcherAndGet\n\t\n\t\tjavadocs, javadocs, javadocs [FIXED]\n\t\tsince this method is only used in conjunction with mergePartialDocWithFullDocFromIndex, if the code is refactored so that mergePartialDocWithFullDocFromIndex calls this method directly (see suggestion above), we could make a small micro-optimization by changing the method sig to take in a Term to (re)use rather then passing in idBytes and calling core.getLatestSchema().getUniqueKeyField() twice. [FIXED]\n\t\tre: the INVALID_STATE ... is that really a fatal error, or should this method be accounting for the possibility of a doc that has been completley deleted (or was never in the index) in a diff way? [I think this is fatal, since if the doc was deleted, then there shouldn't have been an attempt to resolve to a previous document by that id. I think this should never be triggered.]\n\t\n\t\n\tgetInputDocumentFromTlog\n\t\n\t\tlets put an explicit comment here noting that we are intentionally falling through to the Updatelog.ADD case [FIXED]\n\t\n\t\n\tgetVersionFromTlog\n\t\n\t\tbased on it's usage, should this method be changed to return -1 instead of null?  ... not clear to me from the given caller usage ... if so should be be declared to return long instead of Long ? [I'm inclined to keep it to Long/null instead of long/-1, since versionInfo.getVersionFromIndex() is also Long/null]\n\t\n\t\n\n\n\n\nDistributedUpdateProcessor\n\n\tSimilar question from AddUpdateCommand: do we really need 2 distinct params here, or would it be cleaner / less error-prone to have a single distrib.inplace.prevversion which indicates we're doing an inplace update if it's a positive # ? [FIXED]\n\tversionAdd\n\t\n\t\t\"Something weird has happened\" ... perhaps waitForDependentUpdates should return the lastFoundVersion so we can use it in this error msg? ... \"Dependent version not found after waiting: ...\" [FIXED]\n\t\t\"TODO: Should we call waitForDependentUpdates() again?\" ... i don't see how that would help?  if we get to this point it definitely seems like a \"WTF?\" fail fast situation. [FIXED, agreed there's no value to trying again. Removed the comment]\n\t\tthe way the existing code in this method has been refactored into an \"else\" block (see comment: // non inplace update, i.e. full document update) makes sense, but the way the Long lastVersion declaration was refactored out of that block to reuse with the \"if isInPlaceUpdate\" side of things is a bit confusing and doesn't seem to actaully simplify anything... [FIXED]\n\t\t\n\t\t\tIt's not used after the if/else block, so there's no reason to declare it before the \"if\" statement\n\t\t\tby definition it must be null in the \"else\" case, so if (lastVersion == null) will also be true in that code path\n\t\t\tit seems simpiler to just let both the \"if\" and \"else\" branches declare/define their own \"Long lastVersion\" and not risk any confusion about why that variable needs to be \"shared\"\n\t\t\n\t\t\n\t\n\t\n\twaitForDependentUpdates\n\t\n\t\tjavadocs, javadocs, javadocs [FIXED]\n\t\tfetchFromLeader is always true? why not eliminate arg? [FIXED, this was remnants from a previous patch where I was trying to make this configurable]\n\t\tI'm not a concurrency expert, but is this control flow with the sync/wait actaully safe? ... my understanding was the conditional check you're waiting on should always be a loop inside the sync block? [I consulted Noble on exactly this, and he suggested that this is fine]\n\t\teven if there are no spurious returns from wait, logging at \"info\" level every 100ms is excessive ... logging that often at trace seems excessive. [FIXED]\n\t\t\n\t\t\twhy don't we log an info once at the start of method (since our of order updates should be rare) and once at debug anytime lastFoundVersion changes? (diff log message if/when lastFoundVersion is == or > prev)\n\t\t\tthe \"Waiting attempt\" counter \"i\" doesn't seem like useful info to log given how wait(...) works\n\t\t\n\t\t\n\t\t\"...Dropping current update.\" - that log msg seems missleading, this method doesn't do anything to drop the current update, it just assumes the current update will be droped later [FIXED]\n\t\ti don't really see the point of the boolean foundDependentUpdate variable... why not change the only place where it's set to true to return immediately? [FIXED]\n\t\tfetchMissingUpdateFromLeader can return null, but that possibility isn't handled here. [FIXED]\n\t\tif (uc instanceof AddUpdateCommand) ... what if it's not? [FIXED]\n\t\t\n\t\t\tcurrently it's just silently ignored\n\t\t\tis this a viable scenerio that needs accounted for, or an exceptional scenerio that should have error checking?\n\t\t\tlooks like maybe it's just a confusing way to do a null check?\n\t\t\n\t\t\n\t\t((System.nanoTime()-startTime)/1000000 ) ... that's a missleading thing to include in the Exception [FIXED]\n\t\t\n\t\t\twe didn't wait that long, we waited at most 5 seconds \u2013 who knows how long we spent calling fetchMissingUpdateFromLeader & executing it.\n\t\t\n\t\t\n\t\n\t\n\tfetchMissingUpdateFromLeader\n\t\n\t\tjavadocs, javadocs, javadocs [FIXED]\n\t\tshould we really be constructing a new HttpSolrClient on the fly like this? [This should be fine, since the underlying HttpClient is created from HttpClientUtil.createClient, and hence security features should all work. However, I've changed this to now use the UpdateShardHandler's httpClient.]\n\t\tis this on the fly SolrClient + GenericSolrRequest going to work if/when the security auth/acl features are in use in the solr cluster?\n\t\tthis seems to assume the node that forwarded us the current request (via get(DISTRIB_FROM) is still the leader \u2013 but what if there was a leader election? [FIXED: I've now added a check to the cluster state and now fetching from current leader]\n\t\t\n\t\t\tif the leader failed, and a new one was elected, isn't that a pretty viable/likeley reason why waitForDependentUpdates timed-out and needed to call fetchMissingUpdateFromLeader in the first place?\n\t\t\n\t\t\n\t\te.printStackTrace(); ... huge red flag that serious error handling is missing here. [FIXED. This one was bad indeed.]\n\t\tThis method seems like it expects the possibility that missingUpdates will contain more then one entry, and if it does contain more then one entry it will convert/copy all of them into the updates list \u2013 but then it will completley ignore all but the first one. [FIXED. Initially, I wanted to fetch all missing updates, i.e. from what we have till what we want. Noble suggested that fetching only one at a time makes more sense.]\n\t\t\n\t\t\tif we don't expect more then 1, why not assert that?\n\t\t\tif we expect more then one, and they are all important, why don't we return List<UpdateCommand> ?\n\t\t\tif we expect more then one, but only care about one in particularly \u2013 why loop over all of them?\n\t\t\t\n\t\t\t\thow do we know for sure which one in the list is the one we care about?\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\tditto comments about PeerSync & UpdateLog and creating a public static AddUpdateCommand updateCommandFromTlog(List tlogEntry) somwhere that can be reused here as well [FIXED]\n\t\tswitch statement should have some sort of default case ... even if i'ts just to throw an error because anything but an ADD or UPDATE_INPLACE is impossible [FIXED]\n\t\tneed to future proof this code against the posibility of other stuff down the road [Not sure what needs to be done more here]\n\t\n\t\n\n\n\nAnd here are some general overall comments / impressions I got while reviewing the code and then edited up once i was all finished...\n\n\n\n\tGiven that this patch requires some non-trivial changes to the types of records that go in the update log, and requires corisponding changes to PeerSync, it seems like there should definitely be some very explicit testing of log reply and peer sync [FIXED]\n\t\n\t\tie: TestReplay and PeerSyncTest should be updated to include a variety of scenerios involving in-place updates\n\t\n\t\n\tafter seeing how complex & hairy some of the new code has to be around the diff handling of \"in-place atomic updates\", vs existing \"atomic updates\" (that aren't in place) It seems like we should definitely have more test code that mixes and matches diff types of \"updates\" [Added such a test to TestInPlaceUpdatesStandalone.]\n\t\n\t\tstatic, non randomized, examples of explicit tests we should definitely have...\n\t\t\n\t\t\ta doc gets a sequence of atomic updates each containing multiple \"in place\" inc/set ops on diff fields\n\t\t\ta doc gets a sequence of atomic updates each containing multiple inc/set ops, where a single update may have a mix of \"in place\" vs \"not in place\" eligable ops (due to diff fields having diff docvalue/stored settings)\n\t\t\n\t\t\n\t\tour randomized (cloud and non-cloud) testing of in-place updates should also include updates to the canidate docs that may ocasionally not be viable \"in-place\" updates (because they involved updates to additional non-dv fields)\n\t\tin all of these tests we should be checking that both the RTG and regualr search results make sense\n\t\n\t\n\twe also need a lot more testing of various deleteById and deleteByQuery commands mixed with in-place atomic updates [FIXED]\n\t\n\t\tboth deleteByQuerys against the DV fields used in the in-place atomic updates as well as DBQs against other fields in the documents\n\t\ttest the results of (uncommited) RTG as well as searches when these deletes are intermixed with in-place atomic updates\n\t\ttest the results of peer sync and reply when deletes are mixed with in-place atomic updates.\n\t\ttest that we correctly get 409 error codes when trying to do in-place updates w/optimistic concurrency after a delete (and vice versa: DBQ/dID afte in-place update)\n\t\n\t\n\tall logging needs heavily audited [FIXED: I've gone through one round of this, but I'll go through again]\n\t\n\t\tthere's a lot of logging happening at the info and debug level that should probably be much lower.\n\t\tlikewise there may be a few existing info or debug msgs that might be good candidates for warn or error level msgs.\n\t\n\t\n\tuniqueKey and Exception/log msgs [FIXED: I've added the id to many places now. I'll go through another round to see if some places are still missing the id field]\n\t\n\t\tthere is a lot of log msgs or Exception msgs that cite a version# when reporting a problem, but don't include the uniqueKey of the document involved\n\t\tthese messages aren't going to be remotely useful to end users w/o also including the (human readable) uniqueKey of the document involved.\n\t\n\t\n\tit feels like we now have a really large number of methods involved in the merging/combining/converting of documents to apply atomic updates (\"in place\" or otherwise) ... either for use in RTG, or for use when writing updates to disk, or from reading from the tlog, etc... [TODO: I think these methods are in their suitable places for now, but maybe there could be scope for consolidation.]\n\t\n\t\tthe ones that jump out at me w/o digging very hard...\n\nRealTimeGetComponent.resolveFullDocument\nRealTimeGetComponent.toSolrDoc\nRealTimeGetComponent.mergePartialDocWithFullDocFromIndex\nUpdateLog.applyPartialUpdates\nUpdateLog.applyOlderUpdate\nAtomicUpdateDocumentMerger.merge\nAtomicUpdateDocumentMerger.doInPlaceUpdateMerge\n\n\n\t\ti can't help but wonder if there is room for consolidation?\n\t\tin many cases these \"merge\" methods actually delegate to other \"merge\" methods, before/after applying additional logic \u2013 in which case at a minimum using @link or @see tags in the javadocs to make this (intentional) relationship/similarity more obvious would be helpful.\n\t\tin cases where methods do not delegate to eachother, or have any relationship w/eachother, having @link mentions of eachother in the javadocs to compare/constrast why they are different would be equally helpful.\n\t\tand who knows \u2013 perhaps in the process of writing these docs we'll find good oportunities to refactor/consolidate\n\t\n\t\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15368165",
            "date": "2016-07-08T18:44:44+0000",
            "content": "\nI've not had a chance to look at the latest patch, but here's some comment responses...\n\nSince commitWithin and overwrite was being set here, I thought this is an appropriate place to set the prevVersion to the cmd\n\nBut there's a fundemental difference betwen params like commitWithin and overwrite and the new prevVersion param...\n\ncommitWithin and overwrite are client specified options specific to the xml/javabin update format(s).  The fact that they can be specified as request params is an implementation detail of the xml/javabin formats that they happen to have in common, but are not exclusively specifyied as params \u2013 for example the XMLLoader only uses the params as defaults, they can be psecified on a per <add/> basis.\n\nThe new prevVersion param however is an implementation detail of DUP ... DUP is the only code that should have to know/care that prevVersion comes from a request param.\n\nYes, this was intentional, and I think it doesn't make any difference. If an \"id\" isn't found in any of these maps, it would mean that the previous update was committed and should be looked up in the index. \nI think we don't need to worry. Upon receiving a prevPointer=-1 by whoever reads this LogPtr, it should be clear why it was -1: if the command's flags|UpdateLog.UPDATE_INPLACE is set, then this command is an in-place update whose previous update is in the index and not in the tlog; if that flag is not set, it is not an in-place update at all, and don't bother about the prevPointer value at all (which is -1 as a dummy value).\n\nWe should have a comment to these affects (literally we could just paste that text directly into a comment) when declaring the prevPointer variable in this method.\n\n... This was needed because the lucene document that was originally being returned had copy fields targets of id field, default fields, multiple Field per field (due to FieldType.createFields()) etc., which are not needed for in-place updates.\n\nHmm... that makes me wonder \u2013 we should make sure we have a test case of doing atomic updates on numeric dv fields which have copyfields to other numeric fields.  ie: lets make sure our \"is this a candidate for inplace updates\" takes into acount that the value being updated might need copied to another field.\n\n(in theory if both the source & dest of the copy field are single valued dv only then we can still do the in place updated as long as the copyField happens, but even if we don't have that extra bit of logic we need a test that the udpates are happening consistently)\n\nI wasn't sure how to deal with inc for dates, so left dates out of this for simplicity for now\n\nHmmm... is that really the relevant question though?\n\nI'm not sure how the existing (non-inplace) atomic update code behaves if you try to \"inc\" a date, but why does it matter for the isSupportedForInPlaceUpdate method?\n\n\n\tif date \"inc\" is supported in the existing atomic update code, then whatever that code path looks like (to compute the new value) it should be the same for the new inplace update code.\n\tif date \"inc\" is not supported in the existing atomic update code, then whatever the error is should be the same in the new inplace update code\n\n\n\nEither way, I don't see why isSupportedForInPlaceUpdate should care \u2013 or if it is going to care, then it should care about the details (ie: return false for (dv only) date field w/ \"inc\", but true for (dv only) date field with \"set\")\n\nI think this is fatal, since if the doc was deleted, then there shouldn't have been an attempt to resolve to a previous document by that id. I think this should never be triggered.\n\nlet's put those details in a comment where this Exception is thrown ... or better yet, try to incorporate it into the Exception msg?\n\nI'm inclined to keep it to Long/null instead of long/-1, since versionInfo.getVersionFromIndex() is also Long/null\n\nAh, ok ... good point \u2013 can we go ahead and add some javadocs to that method as well making that clear?\n\n\n... I've changed this to now use the UpdateShardHandler's httpClient.\n\nOk, cool ... Yeah, that probably makes more sense in general.\n\nNot sure what needs to be done more here\n\nYeah, sorry \u2013 that was a vague comment that even i don't know what i ment by, was probably ment to be part of the note about the switch statemenet default.\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15368262",
            "date": "2016-07-08T19:26:33+0000",
            "content": "\nBut there's a fundemental difference betwen params like commitWithin and overwrite and the new prevVersion param...\n\ncommitWithin and overwrite are client specified options specific to the xml/javabin update format(s). The fact that they can be specified as request params is an implementation detail of the xml/javabin formats that they happen to have in common, but are not exclusively specifyied as params \u2013 for example the XMLLoader only uses the params as defaults, they can be psecified on a per <add/> basis.\n\nThe new prevVersion param however is an implementation detail of DUP ... DUP is the only code that should have to know/care that prevVersion comes from a request param.\nSure, it makes sense. I'll fix it.\n\nWe should have a comment to these affects (literally we could just paste that text directly into a comment) when declaring the prevPointer variable in this method.\nI had put this comment there:\n\n@return If cmd is an in-place update, then returns the pointer (in the tlog) of the previous update that the given update depends on. Returns -1 if this is not an in-place update, or if we can't find a previous entry in the tlog.\n\n \nBut now I have updated it to make it even more detailed:\n\n@return If cmd is an in-place update, then returns the pointer (in the tlog) of the previous update that the given update depends on.\n   *        Returns -1 if this is not an in-place update, or if we can't find a previous entry in the tlog. Upon receiving a -1, it \n   *        should be clear why it was -1: if the command's flags|UpdateLog.UPDATE_INPLACE is set, then this\n   *        command is an in-place update whose previous update is in the index and not in the tlog; if that flag is not set, it is not an in-place\n   *        update at all, and don't bother about the prevPointer value at all (which is -1 as a dummy value).)\n\n\n\n\nHmm... that makes me wonder \u2013 we should make sure we have a test case of doing atomic updates on numeric dv fields which have copyfields to other numeric fields. ie: lets make sure our \"is this a candidate for inplace updates\" takes into acount that the value being updated might need copied to another field.\n\n(in theory if both the source & dest of the copy field are single valued dv only then we can still do the in place updated as long as the copyField happens, but even if we don't have that extra bit of logic we need a test that the udpates are happening consistently)\nSure, I'll add such a test. The latest patch incorporates the behaviour you suggested: if any of the copy field targets is not a in-place updateable field, then the entire operation is not an in-place update (but a traditional atomic update instead). But, if copy field targets of an updated field is also supported for an updateable dv, then it is updated as well.\n\n\nHmmm... is that really the relevant question though?\n\nI'm not sure how the existing (non-inplace) atomic update code behaves if you try to \"inc\" a date, but why does it matter for the isSupportedForInPlaceUpdate method?\n\n    if date \"inc\" is supported in the existing atomic update code, then whatever that code path looks like (to compute the new value) it should be the same for the new inplace update code.\n    if date \"inc\" is not supported in the existing atomic update code, then whatever the error is should be the same in the new inplace update code\n\nEither way, I don't see why isSupportedForInPlaceUpdate should care \u2013 or if it is going to care, then it should care about the details (ie: return false for (dv only) date field w/ \"inc\", but true for (dv only) date field with \"set\")\n\nFor now I've removed date field totally out of scope of this patch. If there is a update to date that is needed, it falls back to traditional atomic update. I can try to deal with the trie date field, if you suggest.\n\nlet's put those details in a comment where this Exception is thrown ... or better yet, try to incorporate it into the Exception msg?\nI had put this exception in the patch: \nUnable to resolve the last full doc in tlog fully, and document not found in index even after opening new rt searcher. \nbut now I'll change it to: \nUnable to resolve the last full doc in tlog fully, and document not found in index even after opening new rt searcher. If the doc was deleted, then there shouldn't have been an attempt to resolve to a previous document by that id.\n\nAh, ok ... good point \u2013 can we go ahead and add some javadocs to that method as well making that clear?\nSure, I'll update the javadocs for that existing method as well. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15373475",
            "date": "2016-07-12T19:02:16+0000",
            "content": "Updated patch:\n\n\tRefactored the logic to add a previous version to an AddUpdateCommand, moved it to DUP from JavabinLoader/XMLLoader\n\tUpdated javadocs to make them more detailed. Added javadocs to some related existing methods in VersionInfo.\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-15382975",
            "date": "2016-07-18T20:14:40+0000",
            "content": "Patch updated to master after resolving conflicts with the SOLR-9285 commit. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15383064",
            "date": "2016-07-18T21:05:11+0000",
            "content": "Thanks Shalin. Updated the patch to include a test for the UpdateLog's applyPartialUpdates method. Also, removed a possibly erroneous optimization from DUP that I introduced in my last patch (an if check for lastDependentVersion == -1 inside the synchronization block). "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15384532",
            "date": "2016-07-19T17:17:32+0000",
            "content": "Updated the patch:\n\n\tFixed a bug with mixing atomic and in-place updates. Problem was that after in-place update, the RTGC.getInputDocument() got only the partial document, and hence further atomic updates on it failed. Changed this to return a \"resolved\" document for use during atomic update.\n\tAdded direct unit tests for AUDM.isInPlaceUpdate() at TestInPlaceUpdatesCopyField.java and UpdateLogTest. applyPartialUpdates() at UpdateLogTest.java.\n\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15384634",
            "date": "2016-07-19T18:26:01+0000",
            "content": "Another TODO item got missed out.\n\n\tRefactored calls like entry.get(1) etc. (for entries fetched from the ulog/tlog) to entry.get(UpdateLog.VERSION_IDX).\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-15384773",
            "date": "2016-07-19T20:03:47+0000",
            "content": "Thanks Ishan. This is great progress since the last time I reviewed this patch.\n\nI've only skimmed the latest patch but in particular I find a few problems in DistributedUpdateProcessor#waitForDependentUpdates:\n\n\tThis method doesn't look correct. All places which call vinfo.lookupVersion() do it after acquiring a read lock by calling vinfo.lockForUpdate(). Looking at the code for vinfo.lookupVersion() it accesses the map and prevMap in updateLog which can be modified by a writer thread while waitForDependentUpdates is reading their values. So first of all we need to ensure that we acquire and release the read lock. Acquiring this lock and then waiting on a different object (the \"bucket\") will not cause a deadlock condition because it is a read lock (which can be held by multiple threads).\n\tSecondly, this method can be made more efficient. It currently wakes up every 100ms and reads the new \"lastFoundVersion\" from the update log or index. This is wasteful. A better way would be to wait for the timeout period directly after calling vinfo.lookupVersion() inside the synchronized block.\n\tSimilar to #1 \u2013 calling vinfo.lookupVersion() after fetchMissingUpdateFromLeader should be done after acquiring a read lock.\n\tThere is no reason to synchronize on bucket when calling the versionAdd method again because it will acquire the monitor anyway.\n\tDistributedUpdateProcessor#waitForDependentUpdates uses wrong javadoc tag '@returns' instead of '@return'\n\tThe debug log message should be moved out of the loop instead of introducing a debugMessagePrinted boolean flag\n\tUse the org.apache.solr.util.TimeOut class for timed wait loops\n\tMethod can be made private\n\n\n\nI've attempted to write a better wait-loop here (warning: not tested):\n\nlong prev = cmd.prevVersion;\n    long lastFoundVersion = 0;\n\n\n    TimeOut timeOut = new TimeOut(5, TimeUnit.SECONDS);\n    vinfo.lockForUpdate();\n    try {\n      synchronized (bucket) {\n        lastFoundVersion = vinfo.lookupVersion(cmd.getIndexedId());\n        while (lastFoundVersion < prev && !timeOut.hasTimedOut())  {\n          if (log.isDebugEnabled()) {\n            log.debug(\"Re-ordered inplace update. version=\" + (cmd.getVersion() == 0 ? versionOnUpdate : cmd.getVersion()) +\n                \", prevVersion=\" + prev + \", lastVersion=\" + lastFoundVersion + \", replayOrPeerSync=\" + isReplayOrPeersync);\n          }\n          try {\n            bucket.wait(5000);\n          } catch (InterruptedException ie) {\n            throw new RuntimeException(ie);\n          }\n          lastFoundVersion = vinfo.lookupVersion(cmd.getIndexedId());\n        }\n      }\n    } finally {\n      vinfo.unlockForUpdate();\n    }\n\n// check lastFoundVersion against prev again and handle all conditions\n\n\n\nHowever I think that since the read lock and bucket monitor has to be acquired by this method anyway, it might be a good idea to just call it from inside versionAdd after acquiring those monitors. Then this method can focus on just waiting for dependent updates and nothing else.\n\nA random comment on the changes made to DebugFilter: The setDelay mechanism introduced here may be a good candidate for Mark's new TestInjection#injectUpdateRandomPause? "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15385091",
            "date": "2016-07-20T00:06:36+0000",
            "content": "Thanks Shalin, I've fixed it in my updated patch, mostly on the lines of what you suggested in that snippet. Can you please take a look?\n\nBtw, I've surrounded only the lookupVersion() calls with the acquiring and releasing of the lock, instead of surrounding the entire wait loop with the acquiring/releasing of the lock: I reasoned that while we are waiting in that wait loop, other threads need to have indexed the update that we're waiting on, and hence I released the lock as soon as it was not needed, only to re-acquire it after 100ms. Does that sound like a valid reason?\n\nSecondly, this method can be made more efficient. It currently wakes up every 100ms and reads the new \"lastFoundVersion\" from the update log or index. This is wasteful. A better way would be to wait for the timeout period directly after calling vinfo.lookupVersion() inside the synchronized block.\n\nSince this method enters the wait loop for every in-place update that has arrived out of order at a replica (an event, that I think is frequent under multithreaded load), I don't want every such update to be waiting for the full timeout period (5s here), but instead check back from time to time. In most of the cases, the dependent update would've been written (by another thread) within the first 100ms, after which we can bail out. Do you think that makes sense? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-15385215",
            "date": "2016-07-20T02:45:55+0000",
            "content": "Btw, I've surrounded only the lookupVersion() calls with the acquiring and releasing of the lock, instead of surrounding the entire wait loop with the acquiring/releasing of the lock: I reasoned that while we are waiting in that wait loop, other threads need to have indexed the update that we're waiting on, and hence I released the lock as soon as it was not needed, only to re-acquire it after 100ms. Does that sound like a valid reason?\n\nThe read lock is for safe publication of fields in update log and it is acquired by indexing threads who only want to read stuff from update log. Also read locks can be held by multiple readers. Therefore, acquiring this lock does not prevent other threads from indexing.\n\nAlso, please be very careful when changing the order of acquiring locks because it can result in deadlocks. It is good practice to do them in the same sequence as everywhere else in the code. So synchronizing on bucket before vinfo.lockForUpdate for a small optimization doesn't seem worthwhile to me.\n\nSince this method enters the wait loop for every in-place update that has arrived out of order at a replica (an event, that I think is frequent under multithreaded load), I don't want every such update to be waiting for the full timeout period (5s here), but instead check back from time to time. In most of the cases, the dependent update would've been written (by another thread) within the first 100ms, after which we can bail out. Do you think that makes sense?\n\nYou misunderstand. A wait(5000) does not mean that you are waiting the full 5 seconds. Any notifyAll() will wake up the waiting thread and when it does, it will check the lastFoundVersion and proceed accordingly. In practice wait(100) may not be so bad but if an update doesn't arrive for more than 100ms the thread will wake up and lookup the version needlessly with your current patch.\n\nA few more comments:\n\n\tIn your latest patch, acquiring the read lock to call versionAdd is not necessary \u2013 it will do that anyway. You can re-acquire it for reading the version after the method call returns.\n\tI don't think the case of vinfo.lookupVersion returning a negative value (for deletes) is handled here at all.\n\tWhat happens if the document has been deleted already (due to reordering on the replica) when you enter waitForDependentUpdates? i.e. what if re-ordering leads to new_doc (v1) -> del_doc (v3) -> dv_update (v2) on the replica?\n\tSimilarly, how do we handle the case when the doc has been deleted on the leader when you execute fetchMissingUpdateFromLeader. Does RTG return the requested version even if the doc has been deleted already? I suspect it does but be nice to confirm.\n\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15385943",
            "date": "2016-07-20T14:26:25+0000",
            "content": "it is acquired by indexing threads who only want to read stuff from update log\nAh, I see. That makes it clear for me; I've updated the patch now and have done the wait loop as per your suggestion.\n\nIn your latest patch, acquiring the read lock to call versionAdd is not necessary \u2013 it will do that anyway. You can re-acquire it for reading the version after the method call returns.\nDone, removed.\n\nI don't think the case of vinfo.lookupVersion returning a negative value (for deletes) is handled here at all.\nIndeed, I hadn't handled it! I've now handled it in this new patch.\n\nWhat happens if the document has been deleted already (due to reordering on the replica) when you enter waitForDependentUpdates? i.e. what if re-ordering leads to new_doc (v1) -> del_doc (v3) -> dv_update (v2) on the replica?\nI think it should now be fixed: v2 update would be dropped since abs(v3) > v2 (earlier I was just checking v3 > v2, which didn't consider negative versions). I'll add a test for this.\n\nSimilarly, how do we handle the case when the doc has been deleted on the leader when you execute fetchMissingUpdateFromLeader. Does RTG return the requested version even if the doc has been deleted already? I suspect it does but be nice to confirm.\nI think as long as a particular update for the given version is present at the leader's ulog/tlog, it will be returned. I will add a test for this scenario specifically anyway. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15386370",
            "date": "2016-07-20T18:22:38+0000",
            "content": "Added tests for the two scenarios described in the previous comment.\n\n\tTestInPlaceUpdatesDistrib.outOfOrderDeleteUpdatesIndividualReplicaTest() for the scenario where in-place update arrives after a delete.\n\tAdded another scenario to TestInPlaceUpdatesDistrib.delayedReorderingFetchesMissingUpdateFromLeaderTest() where a missing update needs to be fetched from leader, even though the document itself has been deleted at the leader.\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15399807",
            "date": "2016-07-29T18:22:08+0000",
            "content": "\nOk \u2013 it took a while, but here's my notes after reviewing the latest patch....\n\n\nDistributedUpdateProcessor\n\n\twaitForDependentUpdates\n\t\n\t\tI know you & shalin went back and forth a bit on the wait call (ie: wait(100) with max retries vs wait(5000)) but i think the way things settled out bucket.wait(waitTimeout.timeLeft(TimeUnit.MILLISECONDS)); would be better then a generic wait(5000)\n\t\t\n\t\t\tconsider the scenerio where: the dependent update is never going to come; a spurious notify/wake happens during the first \"wait\" call @ 4950ms; the lookupVersion call takes 45ms.  Now we've only got 5ms left on our original TimeOut, but we could wind up \"wait\"ing another full 5s (total of 10s) unless we get another spurrious notify/wake inthe mean time.\n\t\t\n\t\t\n\t\tlog.info(\"Fetched the update: \" + missingUpdate); that's a really good candidate for templating since the AddUpdateCommand.toString() could be expensive if log.info winds up being a no-op (ie: log.info(\"Fetched the update: {}\", missingUpdate);)\n\t\n\t\n\n\n\n\n\tfetchMissingUpdateFromLeader\n\t\n\t\tIn response to a previous question you said...\n[FIXED. Initially, I wanted to fetch all missing updates, i.e. from what we have till what we want. Noble suggested that fetching only one at a time makes more sense.] ... but from what i can tell skimming RTGC.processGetUpdates() it's still possible that multiple updates will be returned, notably in the case where: // Must return all delete-by-query commands that occur after the first add requested.  How is that possibility handled in the code paths that use fetchMissingUpdateFromLeader?\n\t\t\n\t\t\tthat seems like a scenerio that would be really easy to test for \u2013 similar to how outOfOrderDeleteUpdatesIndividualReplicaTest works\n\t\t\n\t\t\n\t\tassert ((List<List>) missingUpdates).size() == 1: \"More than 1 update ...\n\t\t\n\t\t\tbased on my skimming of the code, an empty list is just as possible, so the assertion is missleading (ideally it should say how many updates it got, or maybe toString() the whole List ?)\n\t\t\n\t\t\n\t\n\t\n\n\n\n\nAtomicUpdateDocumentMerger\n\n\tisSupportedFieldForInPlaceUpdate\n\t\n\t\tjavadocs\n\t\n\t\n\n\n\n\n\tgetFieldNamesFromIndex\n\t\n\t\tjavadocs\n\t\tmethod name seems VERY missleading considering what it does\n\t\n\t\n\n\n\n\n\tisInPlaceUpdate\n\t\n\t\tjavadocs should be clear what hapens to inPlaceUpdatedFields if result is false (even if answer is \"undefined\"\n\t\tbased on usage, wouldn't it be simplier if instead of returning a boolean, this method just returned a (new) Set of inplace update fields found, and if the set is empty that means it's not an in place update?\n\t\tisn't getFieldNamesFromIndex kind of an expensive method to call on every AddUpdateCommand ?\n\t\t\n\t\t\tcouldn't this list of fields be created by the caller and re-used at least for the entire request (ie: when adding multiple docs) ?\n\t\t\n\t\t\n\t\tif (indexFields.contains(fieldName) == false && schema.isDynamicField(fieldName))\n\t\t\n\t\t\twhy does it matter one way or the other if it's a dynamicField?\n\t\t\n\t\t\n\t\tthe special DELETED sentinal value still isn't being checked against the return value of getInputDocumentFromTlog\n\t\tthis method still seems like it could/should do \"cheaper\" validation (ie: not requiring SchemaField object creation, or tlog lookups) first.  (Ex: the set of supported atomic ops are checked after isSupportedFieldForInPlaceUpdate & a possible read from the tlog).\n\t\t\n\t\t\tMy suggested rewrite would be something like...\n\nSet<String> candidateResults = new HashSet<>();\n// first pass, check the things that are virtually free,\n// and bail out early if anything is obviously not a valid in-place update\nfor (String fieldName : sdoc.getFieldNames()) {\n  if (schema.getUniqueKeyField().getName().equals(fieldName)\n      || fieldName.equals(DistributedUpdateProcessor.VERSION_FIELD)) {\n    continue;\n  }\n  Object fieldValue = sdoc.getField(fieldName).getValue();\n  if (! (fieldValue instanceof Map) ) {\n    // not even an atomic update, definitely not an in-place update\n    return Collections.emptySet();\n  }\n  // else it's a atomic update map...\n  for (String op : ((Map<String, Object>)fieldValue).keySet()) {\n    if (!op.equals(\"set\") && !op.equals(\"inc\")) {\n      // not a supported in-place update op\n      return Collections.emptySet();\n    }\n  }\n  candidateResults.add(fieldName);\n}\nif (candidateResults.isEmpty()) {\n  return Collections.emptySet();\n}\n// now do the more expensive checks...\nSet<String> indexFields = getFieldNamesFromIndex(cmd.getReq().getCore());\nSolrInputDocument rtDoc = null; // will lazy read from tlog if needed\nfor (String fieldName : candidateResults) {\n  SchemaField schemaField = schema.getField(fieldName);\n  // TODO: check isSupportedFieldForInPlaceUpdate\n  // TODO: check copyfields\n  // TODO: check indexFields, if not there...\n     // TODO: init rtDoc if null\n     // TODO: check rtDoc\n  // ...if any of these checks fail, immediately return Collections.emptySet()\n}\nreturn candidateResults;\n\n\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tdoInPlaceUpdateMerge\n\t\n\t\tjdocs should make it clear that updatedFields will be modified, and that the caller should have already ensured they are valid acording to isSupportedFieldForInPlaceUpdate(...)\n\t\t\n\t\t\teither that, or make isInPlaceUpdate include VERSION_FIELD, and have this method assert that it's there.\n\t\t\n\t\t\n\t\tif (docid >= 0)\n\t\t\n\t\t\twhat if it's not? should that trigger an error? ... even if it's \"ok\" a comment as to why it's ok and what is expected to happen instead down the flow of the code would be helpful here.\n\t\t\n\t\t\n\t\t// Copy over all supported DVs from oldDocument to partialDoc\n\t\t\n\t\t\twhy are we copying all supported DVs over?\n\t\t\tcan't we loop over updatedFields and only copy the dv fields we need over?\n\t\t\tand if we do that, can't we skip the SchemaField creation & isSupportedFieldForInPlaceUpdate(...) check (since isInPlaceUpdate(...) should have already checked that)\n\t\t\n\t\t\n\t\tif (isSupportedFieldForInPlaceUpdate(schemaField) || fieldName.equals(schema.getUniqueKeyField().getName()))\n\t\t\n\t\t\tfetch the uniqueKey field name outside of the loop, so we're not making 2 method calls every iteration to get it\n\t\t\tif the isSupportedFieldForInPlaceUpdate(...) really is still neccessary: swap the order of the OR so we do the cheap equality test first.\n\t\t\n\t\t\n\t\tin response to a previous suggestion i made about adding explicit unit tests to this method, you said: \n[I've refactored this method to make it much simpler and to now call the original merge method. I don't think we need specific tests for this method any longer.]... but i very much disagree.  There's still enough complexity here (paticularly with copying old docvals, the possibility that a previous update from the tlog may not have included all the fields we need to udpate now) that i think some unit tests like i described (ahnd creating a sequence of udpates, checking the expected result of merging, and the 'committing' and checking we still get the same results) would still be very useful.\n\t\n\t\n\n\n\nRealTimeGetComponent\n\n\tSearcherInfo\n\t\n\t\tstill has no javadocs\n\t\n\t\n\n\n\n\n\tresolveFullDocument\n\t\n\t\tI made this suggestion regarding simplifying the callers of this method: \nhy not consolidate those cases into one block of code using (a modified) resolveFullDocument which can start with a call to toSolrDoc(...) and then return immediately if the entry is UpdateLog.ADD ?. your response was...[FIXED: but kept the call to toSolrDoc() call outside the resolveFullDocument]  I explaind why i thought this refactoring simplified the calling code, can you please elaborate / explain why you think the code is better w/o this refactoring?\n\t\n\t\n\n\n\n\n\treopenRealtimeSearcherAndGet\n\t\n\t\ti'm confused about the \"throw INVALID_STATE vs return null\" change to this method.\n\t\t\n\t\t\tin the previous version of this method, it would never return null - instead it threw INVALID_STATE exception.\n\t\t\tI asked you about this, and your response was that it was definitely an (unusual) fatal error.\n\t\t\tBut in the latest version of the patch, you've cahnged the method so that it does return null in these cases, and every place it's called is now responsible to checking the reuslt for null, and throwing INVALID_STATE\n\t\t\t\n\t\t\t\tThere's some subtle variation in the INVALID_STATE msgs, but it's not clear if that's actually intention, or a copy/paste oversight.\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\twhy make this change? why isn't reopenRealtimeSearcherAndGet still throwing the exception itself?\n\t\tregardless of who throws these exceptions, we have an idTerm to work with, so the new String(...) should probably just be replaced with idTerm.text()\n\t\n\t\n\n\n\n\n\tgetInputDocument\n\t\n\t\tI'm confused by the permutations of this method..\n\t\t\n\t\t\tthe meat of the implementation is now in a 5 arg method: getInputDocument(SolrCore core, BytesRef idBytes, boolean avoidRetrievingStoredFields,Set<String> onlyTheseNonStoredDVs, boolean resolveFullDocument\n\t\t\tyou added a 3 arg helper that hardcodes avoidRetrievingStoredFields=false and onlyTheseNonStoredDVs=null\n\t\t\tyou removed the existing 2 arg method which was getInputDocument(SolrCore core, BytesRef idBytes) and updated those callers to use the 3 arg helper\n\t\t\t\n\t\t\t\tin the process i think you broken DocBasedVersionConstraintsProcessorFactory, but i'll come back to that later.\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\twhy not leave the 2 arg helper for backcompat, and any new/modified code that needs more control can use the 5 arg method?\n\t\t\n\t\t\tthe 2 arg method should almost certainly delgate to getInputDocument(core,idByes,false,null,true ... see comments below regarding DocBasedVersionConstraintsProcessorFactory\n\t\t\n\t\t\n\t\n\t\n\n\n\n\nDocBasedVersionConstraintsProcessorFactory\n\n\treplacing getInputDocument(core,id with getInputDocument(core,id,false smells wrong here\n\t\n\t\tand ditto for getInputDocumentFromTlog(core,id -> getInputDocumentFromTlog(core,id,false\n\t\n\t\n\n\n\n\n\tprior to this patch, there was no such thing as an inplace update \u2013 any call to getInputDocument* would by definition return a \"full document\"\n\n\n\n\n\tnow that the concept of an inplace update exists, and these getInputDocument* methods take a new third arg, shouldn't all existing calls to these methods pass true for the third arg to ensure conistent behavior?\n\t\n\t\tunless of course, there is specific reason why we know some caller code doesn't care about resolving the full document \u2013 in which case there should be a comment on the call as to why false is hardcoded. (but that doesn't seem to be the case here from my skimming of the usage)\n\t\n\t\n\n\n\nDocumentBuilder\n\n\tmethod javadocs should explain how the forInPlaceUpdate param(s) will be used (ie: why does it matter?)\n\n\n\n\n\taddField\n\t\n\t\twhat if someone passes an IndexableField \"val\" that is not a NumericDocValuesField, and \"forInPlaceUpdate=true\" ... shouldn't that be invalid? (ie: trip an assert?)\n\t\twhy do we need to check uniqueKeyField != null && f.name().equals(uniqueKeyField.getName()) on every IndexableField ?\n\t\t\n\t\t\twhy not just short circut out at the begining of the method if forInPlaceUpdate && field.equals(uniqueKeyField) ?\n\t\t\tor better still: make the caller do it and eliminate the uniqueKeyField param (only 3 places this method is called, and in one of them we're garunteed false==forInPlaceUpdate so no check would be needed there anyway)\n\t\t\n\t\t\n\t\tif true==forInPlaceUpdate and createFields(...) returns anything that is not a NumericDocValuesField (or returns more then one) shouldn't that trip an assert or something? (ie: doesn't that mean this SchemaField isn't valid for using with an in place update, and the code shouldn't have gotten this far?)\n\t\tin general it seems like this method should just be something much simpler that like...\n\nprivate static void addField(Document doc, SchemaField field, Object val, float boost, boolean forInPlaceUpdate) {\n  if (val instanceof IndexableField) {\n    assert ( (val instanceof NumericDocValuesField) || ! forInPlaceUpdate)\n    // set boost to the calculated compound boost\n    ((Field)val).setBoost(boost);\n    doc.add((Field)val);\n    return;\n  }\n  int numvals = 0;\n  for (IndexableField f : field.getType().createFields(field, val, boost)) {\n    if (null == f) continue; // null fields are not added\n    assert ( (f instanceof NumericDocValuesField) || ! forInPlaceUpdate)\n    doc.add((Field) f);\n    numvals++;\n  }\n  assert ((numvals <= 1) || ! forInPlaceUpdate)\n}\n\n\n\t\n\t\n\n\n\n\n\ttoDocument\n\t\n\t\t// Do not copy this field if this document is to be used for an in-place update, ... and this is the uniqueKey field\n\t\t\n\t\t\twhy not?\n\t\t\ti'm guessing this is just a small optimization? because the uniqueKey can't change so no need to \"update\" the copyField?\n\t\t\tif so, the comment should make that reasoning clear\n\t\t\n\t\t\n\t\tWe don't need to do this if this document is to be used for in-place updates\n\t\t\n\t\t\tagain: comment should explain why we don't need to do this.\n\t\t\n\t\t\n\t\n\t\n\n\n\n\nAddUpdateCommand\n\n\tprevVersion\n\t\n\t\twhy can't this comment be a javadoc comment?\n\t\tcomment/javadoc should explain what special values (like -1) mean\n\t\n\t\n\n\n\n\n\tgetLuceneDocument(boolean)\n\t\n\t\tjavadocs\n\t\n\t\n\tgetLuceneDocument()\n\t\n\t\tjavadocs should be updated to indicate (default) behavior compared to one arg version\n\t\n\t\n\n\n\n\nSolrDocumentBase\n\n\twhy does containsKey need declared here? it's already part of Map interface\n\tshould be able to drop this file from patch\n\n\n\n\nXMLLoader\n\n\twhitespace only changes ... drop from patch.\n\n\n\n\nDirectUpdateHandler2\n\n\tWTF: printStackTrace\n\n\n\n\n\tsolrCoreState.getUpdateLock() usage\n\t\n\t\tI don't know why I didn't ask about this last time, but is this really the appropriate/only lock to use here?\n\t\tI'm looking at the other usage of this lock, notably the comments in DUH2.deleteByQuery(...) and it seems like using this lock may be neccessary to prevent a commit from happening in the middle of multiple in-place docvalue updates ... but is sufficient ?\n\t\tnotably: i'm wondering what happens if doc#X gets an in-place update of N fields and concurrently some oher client sends a full replacement of doc#X (because DUH2 doesn't wrap existing writer.updateDocument calls in any synchronization on the solrCoreState.getUpdateLock() ?\n\t\tSo imagine the following sequence of ops:\n\t\t\n\t\t\tThread #1 enters DUH2.doNormalUpdate to do an in-place update of doc#X with 2 docvalues fields\n\t\t\tThread #2 enters DUH2.doNormalUpdate to do a full replacement of doc#X\n\t\t\tThread #1 acquires updateLock\n\t\t\tThread #1 calls writer.updateNumericDocValue on 1st field\n\t\t\tThread #2 calls writer.updateDocument on doc#x\n\t\t\tThread #1 calls writer.updateNumericDocValue on 2nd field\n\t\t\tThread #1 releases updateLock\n\t\t\n\t\t\n\t\tDo we need to either wrap all writer.updateDocument calls in the updateLock, or use some sort of secondary fineer grained \"per-uniqueKey\" locking as well to prevent bad stuff from happening in sequences like this?\n\t\n\t\n\n\n\n\nUpdateLog\n\n\tPlease add some javadocs to the new *_IDX constants you added explaining what they are for \u2013 notably which ones are only relevant if FLAGS_IDX includes UPDATE_INPLACE\n\n\n\n\n\tapplyPartialUpdates\n\t\n\t\tI had some concerns in my last patch review about the sync block in this method, and how lookupLogs is used \u2013 but i don't see any response to those concerns in previous comments, or any relevant edits to this method?\n\t\n\t\n\n\n\n\n\tgetEntryFromTLog\n\t\n\t\tin repsonse to a question i had about the catch (Exception | Error ex) block, you said...\n..If the seek results in an exception due to unable to deserialize the tlog entry, we can ignore it, since it just means we were looking up the wrong tlog. I have added a comment to this effect in the catch block....but that debug msg doesn't make it clear that this is \"ok\" .... if an end user sees DEBUG: Exception reading the log... in their log file, that seems like something very bad ... what are they suppose to do with that information?\n\t\n\t\n\n\n\n\n\tupdateCommandFromTlog\n\t\n\t\tneeds javadocs\n\t\twhy can't this method conditionally set the prevVersion so that case UpdateLog.ADD code paths (in both this class, and in other switch/case statements) be refactored to also use this method?\n\t\tIf it's only going to be valid for UPDATE_INPLACE entries, that should be clear from the method name and asserted in case someone tries to miss-use it.\n\t\t\n\t\t\teven if it's valid for both ADD and UPDATE_INPLACE, that should be clear from the method name (ie: convertAddUpdateCommandFromTlog perhaps?) and asserted in case someone tries to miss-use it.\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tdoReplay\n\t\n\t\tthe case UpdateLog.UPDATE_INPLACE block includes log.debug(\"add {}\", cmd); .. \"add\" doesn't seem appropriate\n\t\t\n\t\t\tif \"add\" is appropriate, then that seems like even more justification for updateCommandFromTlog being refactored to work on both ADD and UPDATE_INPLACE log entries, so those 2 case statements can be combined completley (via fall through)\n\t\t\n\t\t\n\t\n\t\n\n\n\nTransactionLog\n\n\tjdocs for the two write methods seem backwards\n\t\n\t\tif i'm calling the 2 arg version the jdocs should make it clear this can not be used for in-place updates\n\t\tif i'm calling the 3 arg version, the jdocs should make it clear what prevPointer value i should use if it is not an in-place update\n\t\n\t\n\tseems like the 3 arg write method should have an assertion that looks something like...\n\t\n\t\tassert (-1 <= prevPointer && (cmd.isInPlaceUpdate() ^ (-1 == prevPointer)))\n\t\n\t\n\n\n\ntest-files: schema.xml & schema15.xml\n\n\tpresumably the main purpose of the _version_ field def changes in these files is because a docvalues only version field is required by the new/modified tests that use them to support in place updates\n\t\n\t\tbut i didn't see any of these tests assert that the _version_ field had the neccessary properties \u2013 so there is no protection against anybody down the road modifying these schema files again and breaking that assumption (so that only \"regular\" atomic updates happen ... maybe causing confusing test failures, or maybe just weakening the tests.\n\t\n\t\n\n\n\nschema-minimal-atomic-stress.xml\n\n\tI would prefer we not make most of the changes in this file.\n\t\n\t\tInstead I would strongly prefer you add a new, small, purpose built & well commented schema-inplace-atomic-updates.xml file just for the new InPlace tests \u2013 containing only the fields/dynamicFields needed\n\t\thaving smaller single purpose test configs makes it a lot easier for people reviewing tests to understand what the minimum expectations are for the test to function\n\t\tEven if there was some incredibly strong reason to keep all these tests usign the same file...\n\t\t\n\t\t\twe should keep all the field/fieldType/dynamicField naming conventions cosistent\n\t\t\tif the existing naming convention of \"long_dv\" won't work for the new fields needed (ie: \"float_dv\", \"int_dv\", etc...) because the new tests need more then one field of each type, then the existing field declarations should be removed and TestStressCloudBlindAtomicUpdates should be updated to use field names based on the new dynamicField conventions (ie: \"long_dv\" -> \"*_l_dvo\")\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tin general, regarldess of what schema file is used by all the new tests...\n\t\n\t\twe should stop using the ambiguious \"ratings\" and \"price\" field names\n\t\tinstead the tests should use field names that clearly convey what the expecation of the field is \u2013 either vie xplicit schema declaration, or via one of the dynamicFields you already defined (ie: \"inplace_updatable_x_long\" or \"fieldX_l_dvo\")\n\t\t\n\t\t\tprobably a good idea for one to be an explicitly declared <field/> and the other to come from a <dynamicField/> just to ensure good test coverate of both scenerios \u2013 but either way the name as seen in the test files should make it clear what matters about the field)\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\nUpdateLogTest\n\n\tthe changes forced on the CommitTracker should be done in the breforeClass method so they don't suprise the hell out of anyone who tries adding new test methods to this class later.\n\t\n\t\tuse a static UpdateLog variable so test methods like testApplyPartialUpdates can re-use it (and be confident it's the same one with the CommitTracker changes\n\t\n\t\n\n\n\n\n\tin general there are a lot of LocalSolrQueryRequest objects created in this class that are never closed\n\t\n\t\tthat may not be causing problems right now, but it's arecipe for confusing leaks later\n\t\tthe convinience methods that build up UpdateCommants (ulogCommit & getDeleteUpdate & getAddUpdate) sould be refactored to take in a SolrQueryRequest which should be closed when the logical bit of code related to testing that UpdateCommand is finished. something like...\n\ntry (SolrQueryRequest req = req()) {\n  ulog.add(getAddUpdate(req, ...));\n  // TODO: anything else that needs to be part of this same request for test purposes?\n}\n// TODO ... assert stuff about ulog\n...\n\n\n\t\n\t\n\n\n\n\n\tin general, it's confusing to me how exactly the commands being tested here represent \"in-place\" updates considering the sdocs genreated don't included any inc/set atomic ops at all\n\t\n\t\ti think what's happening here is that we're testing at a low enough level that the logic for actualy applying the in-place update has already happened, and here we're just testing the merging of that doc (after the inc/set has been applied) with the older versions?\n\t\tif my understanding is correct, some comments/javadocs in the helper methods making this more clear would be helpful.\n\t\tif i'm wrong, then please clarify...\n\t\n\t\n\n\n\n\n\ttestApplyPartialUpdates\n\t\n\t\tjavadocs: what is being tested here? use an @see tag if nothing else\n\t\tin general it feels like this should be split up into multipe discreet test methods (ie: testApplyPartialUpdatesOnMultipleInPlaceUpdatesInSequence, testApplyPartialUpdatesAfterMultipleCommits, etc...)\n\t\t// If an in-place update depends on a non-add, assert that an exception is thrown.\n\t\t\n\t\t\tuse expectThrows...\n\nSolrException ex = expectThrows(SolrException.class, () -> {\n  returnVal = ulog.applyPartialUpdates(cmd.getIndexedId(), prevPointer, prevVersion, partialDoc);\n});\n// TODO: assert stuff about ex.code, ex.getMessage()\n\n\n\t\t\n\t\t\n\t\twhat about testing DBQs in the ulog and how they affect partial updates?\n\t\n\t\n\n\n\n\n\tulogCommit\n\t\n\t\tjavadocs\n\t\twhy can't this be static?\n\t\tsee previous comment about method args vs un-closed SolrQueryRequest\n\t\n\t\n\n\n\n\n\tgetDeleteUpdate\n\t\n\t\tjavadocs\n\t\twhy can't this be static?\n\t\tsee previous comment about method args vs un-closed SolrQueryRequest\n\t\n\t\n\n\n\n\n\tgetAddUpdate\n\t\n\t\tjavadocs\n\t\tsee previous comment about method args vs un-closed SolrQueryRequest\n\t\ti would change this method to take in a SolrInputDocument\n\t\t\n\t\t\tgetAddUpdate(42, sdoc(....)); is just as easy to read as getAddUpdate(42, ....); but more versitile\n\t\t\n\t\t\n\t\taren't very bad things garunteed to happen unless cmd.setVersion is set?\n\t\t\n\t\t\ti would remove the conditional wrapping cmd.setVersion and instead just assert that there is a version in the solrDoc to copy\n\t\t\teither that, or have some usage/test cases where the doc being added doesn't include a version field\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\ttoSolrDoc (& toSolrDoc)\n\t\n\t\twhy are these methods being copied/pasted from RTGC?\n\t\tif they are useful for testing, don't clone them \u2013 change them to public, anotate them with @lucene.experimental and use them directly from the test.\n\t\t\n\t\t\tonly the 3 arg seems needed directly by the test, the 2 arg method can probably remain private.\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\n\nPeerSyncTest\n\n\tWTF: System.out.println() ???\n\n\n\n\n\t// lets add some in-place updates\n\t\n\t\tinPlaceParams is only used once, so there aren't \"some in-place updates\" there is exactly 1 \"in-place update\"\n\t\ti'm guessing you inteded to re-use inPlaceParams in all the add(...) calls but had a copy/paste glitch?\n\t\tif i'm wrong, please include a comment explaining what exactly you're testing .. what's the point of the subsequent adds that do not set an DISTRIB_INPLACE_PREVVERSION\n\t\n\t\n\n\n\n\n\tv++\n\t\n\t\tplease avoid these kind of \"postfix increment happens after variable is evaluated\" short cuts \u2013 especailly since every other place in this testclase uses prefix increment to precivesly to avoid this kind of confusion.\n\t\tthe test would be much more readable if you just use concrete hardcoded literal version values in place of \"v++\" in every line\n\t\t\n\t\t\tespecially since DISTRIB_INPLACE_PREVVERSION has a hard coded assumption about what the value of \"v\" was\n\t\t\tthe fact that the \"id\" in use here is also the same as the starting value of \"v\" makes the DISTRIB_INPLACE_PREVVERSION actaully seem wrong at first glance\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tyour new code is adding the docs to client0, but then also calling assertSync on client0, and passing shardsArr[0] for the syncWith param.\n\t\n\t\tI'm 99% certain you're just asking client0 to sync with itself.\n\t\tif you add some queryAndCompare() calls to ensure these syncs are doing what you think, i'm pretty sure you'll see client0 and client1 disagree\n\t\t\n\t\t\teven if i'm wrong, adding some queryAndCompare calls would be useful to ensure the peersync is actually doing something useful and not just silently returning \"true\"\n\t\t\tplease don't follow the bad example usgae of queryAndCompare earlier in this test class \u2013 actually look at the QueryResponse that method returns and check the numfound, and returned field values ,etc...)\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\twhat about mixing in-place updates with deletes (both by id and query?)\n\n\n\n\n\n\nTestRecovery\n\n\twhy is // assert that in-place update is retained at teh end of the test, instead of when the (first) log replay happens and *:* -> numFound=3 is asserted? (ie: before \"A2\" is added)\n\n\n\n\n\twhat about mixing in-place updates with deletes (both by id and query?)\n\n\n\n\n\nTestInPlaceUpdatesStandalone\n\n\ttest setup still doesn't seem to be doing anything to assert that autocommit is disabled\n\t\n\t\tthis is important to prevent future changes to configs/defaults from weaking the test or causing spurrious / confusing failures.\n\t\n\t\n\n\n\n\n\tclearIndex & softCommit\n\t\n\t\tthese appear at the begining of every test method\n\t\tinstead add an @Before method to ensure this happens automatically\n\t\n\t\n\n\n\n\n\ttestUpdatingDocValues\n\t\n\t\tchecking return values from addAndGetVersion / addAndAssertVersion\n\t\t\n\t\t\tin a previous review of these tests, i pointed out...\n\n\tfor addAndGetVersion calls where we don't care about the returned version, don't bother assigning it to a variable (distracting)\n\tfor addAndGetVersion calls where we do care about the returned version, we need check it for every update to that doc...\n\n... but in the new patch things are still very confusing.\n\t\t\tthis new patch now includes an addAndAssertVersion which looks like it was designed to ensure that all updates (even atomic, in place or otherwiser) produce a new version greater then the old version \u2013 but many usages of this method that i see just pass \"0\" for the \"expected\" version \u2014 even though the actual expected version is already known...\n\nlong version1 = addAndGetVersion(sdoc(\"id\", \"1\", \"title_s\", \"first\"), null);\n...\n// Check docValues were \"set\"\nversion1 = addAndAssertVersion(0, \"id\", \"1\", \"ratings\", map(\"set\", 200));\n\n\n\t\t\n\t\t\n\t\tthe docid# vars should have a comment making it clear that the reason we're fetching these is to validate that the subsequent updates are done in place and don't cause the docids to change\n\t\t// Check back to back \"inc\"s are working (off the transaction log)\n\t\t\n\t\t\twhy isn't the query assertion here also checking the docid?\n\t\t\n\t\t\n\t\t// Check optimistic concurrency works\n\t\t\n\t\t\tplease use expectThrows here, and actually validate that the Exception we get is a SolrException with code=409\n\t\t\t\n\t\t\t\twe don't want the test to silently overlook if some bug is introduced when using optimistic concurrency with in-place updates\n\t\t\t\n\t\t\t\n\t\t\tInstead of \"123456,\" (which might randomly be the actual version of the doc) use \"-1\" (which indicates yo uexpect the document to not exist)\n\t\t\tbefore committing & doing the assertQ to check the searchable results, we should also be checking the results of an RTG against the tlog to ensure they're returning the in-place updated values as well.\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\n\ttestUpdateTwoDifferedFields\n\t\n\t\tditto previous concerns about passing \"0\" to addAndAssertVersion in many cases\n\t\t// RTG}\n\t\t\n\t\t\tthis is ... bizare ... i don't understand how this is remotely suppose to demonstrate an RTG.\n\t\t\tplease replace with something like...\n\nassertQ(\"RTG check\",\n        req(\"qt\",\"/get\",\"id\",id, \"wt\",\"xml\", \"fl\",\"score,val_i\"),\n        \"//doc/float[@name='ratings'][.='202.0']\",\n        \"//doc/int[@name='price'][.='12']\",\n        \"//doc/long[@name='_version_'][.='\"+version1+\"']\",\n        \"//doc/int[@name='[docid]'][.='\"+docid1+\"']\"\n        );\n\n\n\t\t\ti also don't understand why this is only done conditionally based on a random boolean \u2013 it has no side effects, and it's not an alternative to checking a regular query \u2013 so why not do it in every test run before the softCommit + query?\n\t\t\n\t\t\n\t\ti was really confused that this test never actaully verified that you could do in-place updates on 2 diff fields in a single request ... let's add that please...\n\nversion1 = addAndAssertVersion(0, \"id\", \"1\", \"ratings\", map(\"inc\", 66), \"price\", map(\"inc\", 44));\n\n\n\t\n\t\n\n\n\n\n\ttestDVUpdatesWithDelete\n\t\n\t\t// Add the document back to index\n\t\t\n\t\t\tdoesn't this weaken the test?\n\t\t\tshouldn't an add with map(\"set\", 200) succeed even after a delete (w/o needing add the doc first)\n\t\t\tshouldn't the point here be that we don't pick up some already deleted doc (from index or tlog) and apply an in-place update to it?\n\t\t\tsomething like...\n\nfor (boolean postAddCommit : Arrays.asList(true, false)) {\n  for (boolean delById : Arrays.asList(true, false)) {\n    for (boolean postDelCommit : Arrays.asList(true, false)) {\n      addAndGetVersion(sdoc(\"id\", \"1\", \"title_s\", \"first\"), null);\n      if (postAddCommit) assertU(commit());\n      assertU(delById ? delI(\"1\") : delQ(\"id:1\"));\n      if (postDelCommit) assertU(commit());\n      version1 = addAndGetVersion(sdoc(\"id\", \"1\", \"ratings\", map(\"set\", 200)));\n      // TODO: assert current doc#1 doesn't have old value of \"title_s\"\n\n\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\taddAndAssertVersion\n\t\n\t\twe should renamed \"version\" arg to \"expectedCurrentVersion\" to be more clear what it's purpose is\n\t\tneed assert 0 < expectedCurrentVersion to prevent missuse (see above)\n\t\n\t\n\n\n\n\n\ttestUpdateOfNonExistentDVsShouldNotFail\n\t\n\t\ti think the idea here is that it's hte only method using \"val_i_dvo\" \u2013 so adding id=0 helps test when the field doesn't exist at all, and id=1 helps test when the field does't exist for a specified doc?\n\t\t\n\t\t\tneed a whitebox check of the actual index reader to assert that field doesn't exist (in case someone adds a new test w/o realing it makes this one useless\n\t\t\tideally use a more distinctive field name as well\n\t\t\n\t\t\n\t\tthis test should also assert the docid doesn't change (when we dont' expect it to)\n\t\n\t\n\n\n\n\n\ttestOnlyPartialUpdatesBetweenCommits\n\t\n\t\twhy not test \"inc\" here?\n\t\tshould be asserting expected versions in final query.\n\t\n\t\n\n\n\n\n\tgetFieldValue\n\t\n\t\tthis should be using RTG so it does't depend on a commit\n\t\n\t\n\n\n\n\n\tDocInfo\n\t\n\t\tis val1 used anywhere ?  can it be removed?\n\t\t\n\t\t\tif it can be removed: lets rename val2 just \"value\"\n\t\t\tif we need them both, let's rename val1 and val2 to intFieldValue and longFieldValue (or somethign like that) so it's not as easy to get them confused\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\ttestReplay3, testReplay4, testReplay6, testReplay7\n\t\n\t\tthese should have better names\n\t\tthe String constants should be replaced with sdoc calls (see comments on checkReplay below)\n\t\n\t\n\n\n\n\n\tcheckReplay\n\t\n\t\tsince we're no longer using this method to read from text files this method should be refactored...\n\t\t\n\t\t\tlet's replace the String[] param with an Object[] param\n\t\t\t\n\t\t\t\tif it's a SolrInputDoc add it\n\t\t\t\telse look for some \"private static final Object\" sentinal objects for hard commit & soft commit\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tgetSdoc\n\t\n\t\tonce checkReplay is refactored, this method can be killed\n\t\n\t\n\n\n\n\n\ttestMixedInPlaceAndNonInPlaceAtomicUpdates\n\t\n\t\tagain: this is not a good way to test RTG\n\t\tagain: there's no reason to randomize if we do an RTG check\n\t\tideally we should do an RTG check after every one of the addAndAssertVersion calls (and still do a commit & query check a the end of the method\n\t\n\t\n\n\n\n\n\n\tI'd still like to see some 100% randomized testing using checkReplay.  It's a really powerful helper method \u2013 we should be taking full advantage of it.\n\n\n\n\nTestStressInPlaceUpdates\n\n\tA really large number of comments i made the last time i reviewed this class still seem problematic, and i can't find any (jira) comments addressing them either...\n\n\n\t...(class) javadocs, and extending SolrCloudTestCase once LUCENE-7301 is fixed and we're sure this test passes reliably. ....\n\t\n\t\talso: we should really make this test use multiple shards\n\t\n\t\n\n\n\n\n\tit would be a lot cleaner/clearer if we refactored these anonymous Thread classes into private static final (inner) classes and instantiated them like normal objects\n\t\n\t\tmakes it a lot easier to see what threads access/share what state\n\t\tbetter still would be implementing these \"workers\" as Callable instances and using an ExecutorService\n\t\n\t\n\n\n\n\n\t\"operations\" comment is bogus (it's not just for queries)\n\n\n\n\n\tI'm not convinced the \"synchronize {...}; commit stuff; syncrhonize { ... };\" sequence is actually thread safe...\n\t\n\t\tT-W1: commit sync block 1: newCommittedModel = copy(model), version = snapshotCount++;\n\t\tT-W2: updates a doc and adds it to model\n\t\tT-W1: commit\n\t\tT-W1: commit sync block 2: committedModel = newCommittedModel\n\t\tT-R3: read sync block: get info from committedModel\n\t\tT-R3: query for doc\n\t\t...\n\t\n\t\n\n\n\n\n\t... in the above sequence, query results seen by thread T-R3 won't match the model because the update from T-W2 made it into the index before the commit, but after the model was copied\n\t\n\t\ti guess it's not a huge problem because the query thread doesn't bother to assert anything unless the versions match \u2013 but that seems kind of risky ... we could theoretically never assert anything\n\t\n\t\n\n\n\n\n\thaving at least one pass over the model checking every doc at the end of the test seems like a good idea no matter what\n\n\n\n\n\tI'm certain the existing \"synchronized (model)\" block is not thread safe relative to the synchronized blocks that copy the model into commitedModel, because the \"model.put(...)\" calls can change the iterator and trigger a ConcurrentModificationException\n\n\n\n\n\tthe writer threads should construct the SolrInputDocument themselves, and log the whole document (not just the id) when they log things, so it's easier to tell from the logs what updates succeed and which were rejected because of version conflicts\n\n\n\n\n\tthere's a lot of \"instanceof ArrayList\" checks that make no sense to me since the object came from getFirstValue\n\n\n\n\n\tverbose\n\t\n\t\twhy does this method exist? why aren't callers just using log.info(...) directly?\n\t\tor if callers really need to pass big sequences of stuff, they can use log.info(\"{}\", Arrays.asList(...))\n\t\tor worst case: this method can simplified greatly to do that internally\n\t\n\t\n\n\n\n\n\taddDocAndGetVersion\n\t\n\t\tusing SolrTestCaseJ4.sdoc and SolrTestCaseJ4.params will make this method a lot sorder shorter\n\t\n\t\n\n\n\n\n\n\n\n\tthis block reads very awkwardly...\n\nif (oper < commitPercent + deletePercent) { // deleteById\n  returnedVersion = deleteDocAndGetVersion(Integer.toString(id), params(\"_version_\",Long.toString(info.version)), false);\n} else { // deleteByQuery\n  returnedVersion = deleteDocAndGetVersion(Integer.toString(id), params(\"_version_\",Long.toString(info.version)), true);\n}\nlog.info((oper < commitPercent + deletePercent? \"DBI\": \"DBQ\")+\": Deleting id=\" + id + \"], version=\" + info.version \n    + \".  Returned version=\" + returnedVersion);\n\n\n\t\n\t\twouldn't something like this be functionally equivilent and easier to make sense of? \n\nfinal boolean dbq = (commitPercent + deletePercent <= oper);\nreturnedVersion = deleteDocAndGetVersion(Integer.toString(id), params(\"_version_\",Long.toString(info.version)), dbq);\nlog.info((dbq ? \"DBI\": \"DBQ\") +\": Deleting id=\" + id + \"], version=\" + info.version  + \".  Returned version=\" + returnedVersion);\n\n\n\t\n\t\n\n\n\n\n\tthis looks like dead code ... or is something actaully still missing and needs done? ... \n\n} else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n  // TODO\n} else {\n\n\n\n\n\n\n\n\tDocInfo\n\t\n\t\tstill needs javadocs\n\t\tlet's rename val1 and val2 to intFieldValue and longFieldValue (or somethign like that) so it's not as easy to get them confused\n\t\n\t\n\n\n\n\n\tsynchronized (leaderClient)\n\t\n\t\tI understand why we are only sending to leader (SOLR-8733) but i still don't udnerstand why the updates are synchronized\n\t\n\t\n\n\n\n\n\tit doesn't take long to get this test to fail ... here's some seeds that failed for me when hammering thetests, (but succeeded when i tried the reproduce line) ...\n\t\n\t\t\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestStressInPlaceUpdates -Dtests.method=stressTest -Dtests.seed=CE02ABC183DF7342 -Dtests.slow=true -Dtests.locale=ar-LB -Dtests.timezone=America/Manaus -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] ERROR   31.1s | TestStressInPlaceUpdates.stressTest <<<\n   [junit4]    > Throwable #1: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=192, name=READER2, state=RUNNABLE, group=TGRP-TestStressInPlaceUpdates]\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([CE02ABC183DF7342:A564746CBD0AA7B8]:0)\n   [junit4]    > Caused by: java.lang.RuntimeException: org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://127.0.0.1:44464/muf/l/collection1: Unable to resolve the last full doc in tlog fully, and document not found in index even after opening new rt searcher. id=1, partial document=SolrDocument{id=stored,indexed,omitNorms,indexOptions=DOCS<id:1>, val1_i_dvo=docValuesType=NUMERIC<val1_i_dvo:13>, val2_l_dvo=docValuesType=NUMERIC<val2_l_dvo:13000000039>, _version_=docValuesType=NUMERIC<_version_:1541124487571832832>}\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([CE02ABC183DF7342]:0)\n   [junit4]    > \tat org.apache.solr.cloud.TestStressInPlaceUpdates$2.run(TestStressInPlaceUpdates.java:376)\n   [junit4]    > Caused by: org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://127.0.0.1:44464/muf/l/collection1: Unable to resolve the last full doc in tlog fully, and document not found in index even after opening new rt searcher. id=1, partial document=SolrDocument{id=stored,indexed,omitNorms,indexOptions=DOCS<id:1>, val1_i_dvo=docValuesType=NUMERIC<val1_i_dvo:13>, val2_l_dvo=docValuesType=NUMERIC<val2_l_dvo:13000000039>, _version_=docValuesType=NUMERIC<_version_:1541124487571832832>}\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:606)\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:259)\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:248)\n   [junit4]    > \tat org.apache.solr.client.solrj.SolrRequest.process(SolrRequest.java:149)\n   [junit4]    > \tat org.apache.solr.client.solrj.SolrClient.query(SolrClient.java:942)\n   [junit4]    > \tat org.apache.solr.client.solrj.SolrClient.query(SolrClient.java:957)\n   [junit4]    > \tat org.apache.solr.cloud.TestStressInPlaceUpdates$2.run(TestStressInPlaceUpdates.java:343)\n\n\n\t\t\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestStressInPlaceUpdates -Dtests.method=stressTest -Dtests.seed=D3059BEABF12E831 -Dtests.slow=true -Dtests.locale=nl-BE -Dtests.timezone=Asia/Thimbu -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] ERROR   33.1s | TestStressInPlaceUpdates.stressTest <<<\n   [junit4]    > Throwable #1: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=186, name=READER6, state=RUNNABLE, group=TGRP-TestStressInPlaceUpdates]\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([D3059BEABF12E831:B863444781C73CCB]:0)\n   [junit4]    > Caused by: java.lang.RuntimeException: org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://127.0.0.1:37544/ct_/px/collection1: Unable to resolve the last full doc in tlog fully, and document not found in index even after opening new rt searcher. id=0, partial document=SolrDocument{val2_l_dvo=docValuesType=NUMERIC<val2_l_dvo:5000000010>, id=stored,indexed,omitNorms,indexOptions=DOCS<id:0>, _version_=docValuesType=NUMERIC<_version_:1541142011874115584>}\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([D3059BEABF12E831]:0)\n   [junit4]    > \tat org.apache.solr.cloud.TestStressInPlaceUpdates$2.run(TestStressInPlaceUpdates.java:376)\n   [junit4]    > Caused by: org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://127.0.0.1:37544/ct_/px/collection1: Unable to resolve the last full doc in tlog fully, and document not found in index even after opening new rt searcher. id=0, partial document=SolrDocument{val2_l_dvo=docValuesType=NUMERIC<val2_l_dvo:5000000010>, id=stored,indexed,omitNorms,indexOptions=DOCS<id:0>, _version_=docValuesType=NUMERIC<_version_:1541142011874115584>}\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:606)\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:259)\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:248)\n   [junit4]    > \tat org.apache.solr.client.solrj.SolrRequest.process(SolrRequest.java:149)\n   [junit4]    > \tat org.apache.solr.client.solrj.SolrClient.query(SolrClient.java:942)\n   [junit4]    > \tat org.apache.solr.client.solrj.SolrClient.query(SolrClient.java:957)\n   [junit4]    > \tat org.apache.solr.cloud.TestStressInPlaceUpdates$2.run(TestStressInPlaceUpdates.java:343)\n\n\n\t\n\t\n\n\n\n\n\nTestInPlaceUpdatesCopyFields\n\n\tWTF: TestInPlaceUpdatesCopyFields extends AbstractBadConfigTestBase\n\n\n\n\n\tthis test seems to jumpt through hoops to use a mutable managed schema \u2013 but as far as i can tell it doesn't actaully test anything that requires the schema to change during the course of the test...\n\t\n\t\tit would be a lot simpler & easier to read if it just started up with a simple schema containing all of the copyFields needed\n\t\tunless you want to change the test so it does things like \"assert in-place update of foo_i_dvo works; add a copyField from foo_i_dvo to foo_stored; assert update of foo_i_dvo is no longer in place\"\n\t\n\t\n\n\n\n\n\tthe name of this class seems a bit too narrow\n\t\n\t\tit's not just a test of in-place updates using copy fields, it's a lot of unit tests of AtomicUpdateDocumentMerger.isInPlaceUpdate\n\t\tsuggest: TestAtomicUpdateDocMergerIsInPlace\n\t\tor just make it a test method in TestInPlaceUpdatesStandalone\n\t\t\n\t\t\tespecially if you simplify it to use pre-declared copyFields and don't need the mutable schema.\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tI found multiple seeds that fails 100% of the time on the same assert ... i haven't looked into why...\n\t\n\t\t\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestInPlaceUpdatesCopyFields -Dtests.method=testIsInPlaceUpdate -Dtests.seed=54280A18530C3306 -Dtests.slow=true -Dtests.locale=en-ZA -Dtests.timezone=Europe/Tirane -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] FAILURE 0.10s J2 | TestInPlaceUpdatesCopyFields.testIsInPlaceUpdate <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([54280A18530C3306:99BC22C9123C0682]:0)\n   [junit4]    > \tat org.apache.solr.update.TestInPlaceUpdatesCopyFields.testIsInPlaceUpdate(TestInPlaceUpdatesCopyFields.java:118)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n\n\n\t\t\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestInPlaceUpdatesCopyFields -Dtests.method=testIsInPlaceUpdate -Dtests.seed=A4B7A0F71938C5FE -Dtests.slow=true -Dtests.locale=en -Dtests.timezone=America/Tijuana -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] FAILURE 2.47s J2 | TestInPlaceUpdatesCopyFields.testIsInPlaceUpdate <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([A4B7A0F71938C5FE:692388265808F07A]:0)\n   [junit4]    > \tat org.apache.solr.update.TestInPlaceUpdatesCopyFields.testIsInPlaceUpdate(TestInPlaceUpdatesCopyFields.java:118)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n\n\n\t\n\t\n\n\n\n\nTestInPlaceUpdatesDistrib\n\n\tagain A large number of comments i made the last time i reviewed this class still seem problematic, and i can't find any (jira) comments addressing them either...\n\n\n\tOnce LUCENE-7301 is fixed and we can demonstate that this passes reliably all of the time, we should ideally refactor this to subclass SolrCloudTestCase\n\n\n\n\n\tIn general, the \"pick a random client\" logic should be refactored so that sometimes it randomly picks a CloudSolrClient\n\n\n\n\n\tensureRtgWorksWithPartialUpdatesTest\n\t\n\t\teven if we're only going to test one a few doc, we should ensure there are a random num docs in the index (some before the doc we're editing, and some after)\n\t\t2 docs before/after is not a random number ... random means random: we need to test edge cases of first docid in index, last docid in index, first/last docid in segment, etc...\n\t\n\t\n\n\n\n\n\toutOfOrderUpdatesIndividualReplicaTest\n\t\n\t\tditto comments about only one doc\n\t\tif we are going to use an ExecutorService, then the result of awaitTermination has to be checked\n\t\t... and shutdown & awaitTermination have to be called in that order\n\t\tsince this tests puts replicas out of sync, a ... \"wait for recovers\" should happen at the end of this test (or just in between every test) .. especially if we refactor it (or to protect someone in the future who might refactor it)\n\t\n\t\n\n\n\n\n\toutOfOrderUpdatesIndividualReplicaTest  (followup comments)\n\t\n\t\tlots more comments in the test code to make it clear that we use multiple threads because each update may block if it depends on another update\n\t\tcreate atLeast(3) updates instead of just a fixed set of \"3\" so we increase our odds of finding potential bugs when more then one update is out of order.\n\t\tloop over multiple (random) permutations of orderings of the updates\n\t\t\n\t\t\tdon't worry about wether a given ordering is actually correct, that's a valid random ordering for the purposes of the test\n\t\t\ta simple comment saying we know it's possible but it doesn't affect any assumptions/assertions in the test is fine\n\t\t\n\t\t\n\t\tfor each random permutation, execute it (and check the results) multiple times\n\t\t\n\t\t\tthis will help increase the odds that the thread scheduling actaully winds up running our updates in the order we were hoping for.\n\t\t\n\t\t\n\t\tessentially this should be a a micro \"stress test\" of updates in arbitrary order. Something like... \n\nfinal String ID = \"0\";\nfinal int numUpdates = atLeast(3);\nfinal int numPermutationTotest = atLeast(5);\nfor (int p = 0; p < numPermutationTotest; p++) {\n  del(\"*:*);\n  commit();\n  index(\"id\",ID, ...); // goes to all replicas\n  commit();\n  long version = assertExpectedValuesViaRTG(LEADER, ID, ...);\n  List<UpdateRequest> updates = makeListOfSequentialSimulatedUpdates(ID, version, numUpdates);\n  for (UpdateRequest req : updates) {\n    assertEquals(0, REPLICA_1.requets(req).getStatus());\n  }\n  Collections.shuffle(updates, random());\n  // this method is where you'd comment the hell out of why we use threads for this,\n  // and can be re-used in the other place where a threadpool is used...\n  assertSendUpdatesInThreadsWithDelay(REPLICA_0, updates, 100ms);\n  for (SolrClient client : NONLEADERS) [\n    // assert value on replica matches original value + numUpdates\n  }\n}\n\n\n\t\tAs a related matter \u2013 if we are expecting a replica to \"block & eventually time out\" when it sees an out of order update, then there should be a white box test asserting the expected failure situation as well \u2013 something like... \n\nfinal String ID = \"0\";\ndel(\"*:*);\ncommit();\nindex(\"id\",ID, ...);\nUpdateRequest req = simulatedUpdateRequest(version + 1, ID, ...);\nTimer timer = new Timer();\ntimer.start();\nSolrServerException e = expectThrows(() -> { REPLICA_0.request(req); });\ntimer.stop();\nassert( /* elapsed time of timer is at least the X that we expect it to block for */ )\nassert(e.getgetHttpStatusMesg().contains(\"something we expect it to say if the update was out of order\"))\nassertEquls(/* whatever we expect in this case */, e.getHttpStatusCode());\n\n\n\t\n\t\n\n\n\n\n\n\tdelayedReorderingFetchesMissingUpdateFromLeaderTest\n\t\n\t\tIs there no way we can programatically tell if LIR has kicked in? ... pehaps by setting a ZK watch? ... this \"Thread.sleep(500);\" is no garuntee and seens arbitrary.\n\t\t\n\t\t\tat a minimum polling in a loop for the expected results seems better then just a hardcoded sleep\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\n\n\n\n\n\ttest()\n\t\n\t\t// assert that schema has autocommit disabled\n\t\t\n\t\t\tthis doesn't assert autocommit is disabled, all it does is assert that a sys property was set in beforeSuperClass\n\t\t\tnothing about this actually asserts that the configs/defaults in use don't have autocommit \u2013 use the Config API to be certain\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tdocValuesUpdateTest\n\t\n\t\tthe lower limit of numDocs=1 seems absurd ... why not atLeast(100) or something?\n\t\tplease don't use (for) loops w/o braces.\n\t\tI don't understand anything about the use of luceneDocids in this method...\n\t\t\n\t\t\tcorrectness...\n\t\t\t\n\t\t\t\tfor starters, matchResults seems completely broken \u2013 but i'll get to that later.  let's assume for now that it works...\n\t\t\t\tthe initial list comes from a randomly choosen client, and then later it's compared to the list from another randomly choosen client \u2013 how is this comparison safe?\n\t\t\t\tIf there is any sort of hicup during indexing that requires the leader to retry sending a doc, then the docids won't match up.\n\t\t\t\tit seems like these checkers are really just relying on the fact that if there is any discrepency between the replicas, we'll retry enough times that eventually we'll get lucky and query the first replica again.\n\t\t\t\n\t\t\t\n\t\t\teven if everything in the code as written is 100% fine:\n\t\t\t\n\t\t\t\tthe iniial list of luceneDocids is populated by a query that doesn't do any retry logic and will fail fast if the numDocs doesn't match the result.size()\n\t\t\t\tthis does nothing to address the problem noted in the older version of the test: waiting for the commit to propogate and open new searchers on all shards: Thread.sleep(500); // wait for the commit to be distributed\n\t\t\t\n\t\t\t\n\t\t\tIn general, this seems like a convoluted way to try and kill two birds with one stone: 1) make sure all replicas have opened searchers with the new docs; 2) give us something to compare to later to ensure the update was truely in place\n\t\t\t\n\t\t\t\ti really think the first problem should be addressed the way i suggested previously:\n\n\tif initially no docs have a rating value, then make the (first) test query be for rating:[* TO *] and execute it in a rety loop until the numFound matches numDocs.\n\tlikewise if we ensure all ratings have a value such that abs(ratings) < X, then the second update can use an increment such that abs(inc) > X*3 and we can use -ratings:[-X TO X] as the query in a retry loop\n\n\n\t\t\t\tthe second problem should be solved by either using the segments API, or by checking the docids on every replica (w/o any retries) ... after independently verifying the searcher has been re-opened.\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tmatchResults\n\t\n\t\tno javadocs\n\t\t\n\t\t\tI'm assuming the point is to return true if the (same ordered) luceneDocids & ratings match the results\n\t\t\n\t\t\n\t\n\t\n\treturns true if the number of results doesn't match the number of luceneDocids\n\t\n\t\tseems broken: if a shard hasn't re-opened a searcher yet (ie: 0==results.size()) ... implies results do match when they are grossly difference.\n\t\n\t\n\tint l = ... ... why is this variable named \"l\" ???\n\n\n\n\n\tensureRtgWorksWithPartialUpdatesTest\n\t\n\t\tinstead of log.info(\"FIRST: \" + sdoc); and log.info(\"SECOND: \" + sdoc); just put the sdoc.toString() in the assert messages...\n\nassertEquals(\"RTG tlog price: \" + sdoc, (int) 100, sdoc.get(\"price\"));\n\n\n\t\tnothing in this test asserts that the update is actually in place\n\t\t\n\t\t\tchecking fl=[docid] in cloud RTG is currently broken (SOLR-9289) but we can/should be checking via the segments API anyway (if we have a general helper method for comparing the segments API responses of multiple replicas betwen multiple calls, it could be re-used in every test in this class)\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\toutOfOrderUpdatesIndividualReplicaTest\n\t\n\t\tlong seed = random().nextLong(); // seed for randomization within the threads\n\t\t\n\t\t\teach AsyncUpdateWithRandomCommit task needs it's own seed value, otherwise they'll all make the exact same choices.\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\toutOfOrderDeleteUpdatesIndividualReplicaTest\n\t\n\t\tnew test, most of the comments i had about outOfOrderUpdatesIndividualReplicaTest (both the new comments, and the older comments that i don't see any updates/replies regarding) also apply here.\n\t\n\t\n\n\n\n\n\tdelayedReorderingFetchesMissingUpdateFromLeaderTest\n\t\n\t\tthis is sketchy brittle \u2013 just create a List<SolrClient> ALL_CLIENTS when creating LEADER and NONLEADERS...\n\nfor (SolrClient client: new SolrClient\\[\\] \\{LEADER, NONLEADERS.get(0), \n    NONLEADERS.get(1)}) { // nonleader 0 re-ordered replica, nonleader 1 well-ordered replica\n\n\n\t\n\t\n\n\n\n\n\tsimulatedUpdateRequest\n\t\n\t\ti have not looked into where the compiler is finding the id variable used in this method, but it's definitely not coming fro mthe method args, or the doc \u2013 so it's probably broken.\n\t\twhatever the fix is for the baseUrl init code, please refactor it into a static helper method so we don't have these 4 lines duplicated here and in simulatedDeleteRequest\n\t\n\t\n\n\n\n\n\taddDocAndGetVersion\n\t\n\t\tsynchronized (cloudClient)\n\t\t\n\t\t\twhy are we synchronized on cloud client but updating via LEADER?\n\t\t\twhy are we synchronized at all?\n\t\t\n\t\t\n\t\n\t\n\n\n\nJettySolrRunner\n\n\tuse MethodHandles for static logger init\n\n\n\n\n\tif there are multiple delays whose counters have \"triggered\", why only execute a single delay of the \"max\" time? ... shouldn't it be the sum?\n\n\n\n\n\tnow that an individual \"Delay\" can be \"delayed\" (ie: there's a count involved and the delay may not happen for a while) let's add a String reason param to addDelay and the Delay class and log that message as we loop over the delay objects\n\n\n\n\nGeneral Questions / Concerns\n\n\tprecommit was failing for me due to javadoc warnings\n\n\n\n\n\tLUCENE-7344\n\t\n\t\tprevious comment from Ishan...\nAs I was incorporating Hoss' suggestions, I wrote a test for DV updates with DBQ on updated values. This was failing if there was no commit between the update and the DBQ. I think this is due to LUCENE-7344.\n\t\tI was expecting to find this test code somewhere, but i did not\n\t\tWe still need some sort of solution to this problem \u2013 the suggested requirement/documentation workarround suggested by McCandless in that issue doesn't really fit with how all work on this jira to date has kept the  the decision of when/if to do an \"in-place\" update a low level implementation detail .... that would have to radically change if we wanted to \"pass the buck\" up to the user to say \"you an't use DBQ on a docvalue field that you also use for in place updates\"\n\t\tso what's our Solr solution / workaround?  do we have any?\n\t\n\t\n\n\n\n\n\tBlock join docs?\n\t\n\t\tI never really considered this before, but it jumped out at me when reviewing some of the code\n\t\tWhat happens today (w/o patch) if someone has block join docs and does an an atomic update that updates both parent and child?\n\t\t\n\t\t\tI'm assuming that currently \"works\" (ie: store fields of both docs are read, update applied to both, and then written back as a new block)\n\t\t\n\t\t\n\t\tWhat happens w/this patch in the same situation?\n\t\t\n\t\t\twhat if in both docs, the field being updated supports in-place updates? ... does it work and update the both docs in place?\n\t\t\twhat if only othe parent doc's update involves an in-place updatable field, but the child doc update is on a field that is stored/indexed?  .... does the \"isInPlaceUpdate\" style logic kick in correctly for all the docs in the hierarchy? (so that the entire block is updated as a \"regular\" atomic update?)\n\t\t\n\t\t\n\t\n\t\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-15409383",
            "date": "2016-08-05T12:23:24+0000",
            "content": "Do we need to either wrap all writer.updateDocument calls in the updateLock, or use some sort of secondary fineer grained \"per-uniqueKey\" locking as well to prevent bad stuff from happening in sequences like this?\n\nthe secondary fine-grained lock is the per-bucket lock. The full updates use that . So,it should take the lock on both to mitigate the concurrency issues. Eventually, when Lucene exposes DocumentsWriter.updateDocValues(DocValuesUpdate... updates) we probably should be able to get rid of the global lock?\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15412122",
            "date": "2016-08-08T17:31:59+0000",
            "content": "the secondary fine-grained lock is the per-bucket lock. The full updates use that . So,it should take the lock on both to mitigate the concurrency issues. Eventually, when Lucene exposes DocumentsWriter.updateDocValues(DocValuesUpdate... updates) we probably should be able to get rid of the global lock?\n\nIndeed, I realized that we can use IndexWriter#updateDocValues(Term term, Field[] updates) instead of trying to lock at Solr. I shall include that fix in the next patch. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15412124",
            "date": "2016-08-08T17:32:22+0000",
            "content": "Regarding LUCENE-7344,\n\n\nWe still need some sort of solution to this problem \u2013 the suggested requirement/documentation workarround suggested by McCandless in that issue doesn't really fit with how all work on this jira to date has kept the the decision of when/if to do an \"in-place\" update a low level implementation detail .... that would have to radically change if we wanted to \"pass the buck\" up to the user to say \"you an't use DBQ on a docvalue field that you also use for in place updates\"\nso what's our Solr solution / workaround? do we have any?\n\nI tried to look for a workaround in Solr, but couldn't figure out a viable workaround. Given that we're looking to have in-place updates replace the current atomic updates mechanism transparently when certain fields types are involved (i.e. non-stored, non-indexed, single valued docvalues), LUCENE-7344 is a blocker for that: i.e. if a user updates a dv field, and issues a DBQ trying to use that updated value without a commit in between, then the DBQ doesn't actually use the updated value, but uses the older (last committed) value.\n\nOne approach is to not have in-place updates be a transparent replacement to atomic updates and be enabled or disabled (by default) on certain fields. By doing this, we can document that enabling in-place updates has the downside that DBQs on the updated values wouldn't work unless there's an explicit or auto commit between the update and the dbq.\n\nShai Erera Would love to hear your thoughts on this, and your suggestions on how should we go about dealing with LUCENE-7344. "
        },
        {
            "author": "Shai Erera",
            "id": "comment-15413974",
            "date": "2016-08-09T18:29:34+0000",
            "content": "Ishan Chattopadhyaya, I've continued the discussion in LUCENE-7344. I am looking into fixing the bug, though it's a hairy one... "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15414049",
            "date": "2016-08-09T19:02:54+0000",
            "content": "\nIt seems like solutions for dealing with LUCENE-7344 in Solr could fall into 3 broad categories:\n\nIndexWriter changes\nThese are solutions that might involve changes to IndexWriter\n\n\n\tThe ideal solution would be to \"fix\" LUCENE-7344 at the IndexWriter level, so that it magically does hte correct thing \u2013 but I certainly have no idea how to do that, and McCandless didn't think it was viable.  So this is an unlikely solution unless/untill Shai has more time to work through his ideas.\n\n\n\n\n\tAn improvement on the overall situation might be if IndexWriter.deleteDocuments could at least detect when the Query involves a DocValues field which has non-committed updates, and in that case throw a distinct Exception (ie: CanNotDeleteOnUpdatedDocValues extends InvalidStateException\n\t\n\t\tSolr could then, at a minimum, propogate that exception up to the user, and documentation could make it clera that if you want to mix atomic updates of docvalue with DBQs on those docvalues, you need to commit or you will get this exception\n\t\n\t\n\n\n\n\nWorkaround's in Solr: Invisible To Users\nThis would be any type of solution that let's us maintain the current approach in the existing patches of making docvalue updates be a complete implementation detail that happens automatically behind the scenes when the field types & updates support it.\n\n\n\tOne possibility would be to track somewhere (at the solr level) if/when inplace updates have been used since the last commit, and do an implicit commit if/when a DBQ comes in.\n\t\n\t\tto prevent excessive unexpected commits, we could completley disable all inplace updates unless autocommit was enabled, since that should be the common case for most \"real time\" applications of solr anywhere, where inplace updates will be the most helpful\n\t\n\t\n\n\n\n\n\tThe previous idea could be combined with the idea of IndexWriter.deleteDocuments throwing a CanNotDeleteOnUpdatedDocValues exception;\n\t\n\t\tif/when Solr encounters a CanNotDeleteOnUpdatedDocValues, we could do an implicit commit and retry the DBQ\n\t\n\t\n\n\n\n\nWorkaround's in Solr: involving API Changes/Additions to current patch\nThese are solutions that would involved explicit changes to the options offered to users so they can explicitly indicate \"I want an inplace update\" with the understanding that tha may prevent DBQs from working properly\n\n\n\tAs ishan suggested, one possibility would be to require fields have explicit inplaceUpdates=\"true\" configuration in schema.xml to indicate that they may be updated inplace\n\t\n\t\tat a later date, we could always increase the default schema version and change the default value of this new field property to automatically be true based on the docValues and other field attributes\n\t\tthe biggest downside I see to this approach is that it doesn't really do anything to prevent the underlying problem:\n\t\t\n\t\t\tit doesn't help \"enforce\" that DBQs won't be used against these fields, and it doesn't give the user any clear indication of a problem if they try \u2013 Solr/Lucene will continue to silently delete the wrong docs.\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\n\tAlternatively, we could make this notion of requiring that inplaceupdates be explicitly requested be something that can be specified on a per request basis, along the lines of...\n\n{ \"id\": \"12345\",\n  \"price\": { \"inc\": 42,\n             \"inplace\": true }}\n\n\n\t\n\t\tagain, the default could be \"false\" and at a later date we could always change the default to be based on the docValues and other field attributes\n\t\tthis approach seems slightly better to me then a schema setting because:\n\t\t\n\t\t\tthe \"inplace\" directive is front and center for clients sending updates, so they are more likeley to be aware of the caveats around using it with DBQ (even if they haven't read the schema first hand)\n\t\t\tclients can mix and match inplace atomic updates with \"regular\" atomic updates on the same field (or diff fields definied by the same <dynamicField/>, if/when they do/don't care about being able to reliably do a DBQ on that field.\n\t\t\n\t\t\n\t\tBut ultimately this still has the same scary behavior: Solr can silently delete teh wrong docs w/o immediate/obvious reason why if the user doing the delete doesn't know/care if/when/why a doc may have had an inplace atomic update.\n\t\n\t\n\n\nMy current feeling is that a hybrid solution may be the best bet:\n\n\tRequire users to explicitly configure/request \"inplace\" updates in some way\n\tDocument that when inplace updates are configured/requested, hard commits may happen implicitly if/when a DBQ is executed to ensure correct documents are deleted\n\tOnly do the hard commit if neccessary because IndexWriter threw CanNotDeleteOnUpdatedDocValues the first time we tried the DBQ, then retry.\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-15415219",
            "date": "2016-08-10T12:45:22+0000",
            "content": "I hadn't had a chance to review the patch after Ishan incorporated my last few review comments. So I took a look again. \n\n\n\tHoss's suggestion of `bucket.wait(waitTimeout.timeLeft(TimeUnit.MILLISECONDS));` instead of `wait(5000)` is much better and should be implemented.\n\tUnder no circumstances should we we calling `forceUpdateCollection` in the indexing code path. It is just too dangerous in the face of high indexing rates. Instead we should do what the DUP is already doing i.e. calling getLeaderRetry to figure out the current leader. If the current replica is partitioned from the leader then we have other mechanisms for taking care of it and the replica has no business trying to determine this.\n\tI'd still suggest that instead of introducing yet another delay mechanism via the DebugFilter in JettySolrRunner, we should use the fault injection facilities that we already have in Solr.\n\tThe executor service in the new tests should be shutdown in a finally clause. You can use ExecutorUtil.shutdownAndAwaitTermination for that.\n\n\n\nHoss has given some excellent comments on the new tests that you added. Please incorporate them into your next patch. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-15415326",
            "date": "2016-08-10T14:14:52+0000",
            "content": "Document that when inplace updates are configured/requested, hard commits may happen\n\ns/hard/soft commits?\n\nThinking about solr-cloud mode only (and the DBQ issue w/ updates that don't change the internal doc id): solr already blocks updates, issues the DBQ, and then forces open a new realtime searcher.\nThe easiest correctness workaround would perhaps just be to force open a realtime searcher before the DBQ as well.\n\nBasically, in DUH2.deleteByQuery():\n\nsynchronized (solrCoreState.getUpdateLock()) {\n  ulog.openRealtimeSearcher();\n\n\n\nIf Lucene could tell us if there were any unresolved updates (of the variety that change the doc w/o changing it's internal docid), then we could conditionally call that.\n\nLucene throwing an exception may be a non-starter... at the point in time that a DBQ is issued, there may not be any numeric updates? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15415981",
            "date": "2016-08-10T20:48:41+0000",
            "content": "I'd still suggest that instead of introducing yet another delay mechanism via the DebugFilter in JettySolrRunner, we should use the fault injection facilities that we already have in Solr.\n\nI'm not sure how that would really work via TestInjection \u2013 what Ishan's patch currently adds to DebugFilter allows for targeting specific requests (by sequence in the future) on specific nodes with delays.  The way TestInjection is designed is to be used at very low levels in the (production) code as asserts and completely randomized based on static \"odds\" that test methods can set.\n\nI have no idea how you would tell TestInjection \"The HTTP request should be stalled for 10 seconds\" and have that only apply to a single specific node.\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15416893",
            "date": "2016-08-11T08:46:51+0000",
            "content": "Attaching a patch that is based on the previous patch, just adding a new test: TestInPlaceUpdatesStandalone#testDVUpdatesWithDBQofUpdatedValue() demonstrating the DBQ issue.\n\nMeanwhile, I'm working on another patch incorporating Hoss' and Shalin's review comments, and shall post it soon. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15417210",
            "date": "2016-08-11T13:18:39+0000",
            "content": "The easiest correctness workaround would perhaps just be to force open a realtime searcher before the DBQ as well.\nThis seems to be working correctly for the few test cases I've tried out. Seems like this is the easiest workaround for LUCENE-7344 for now. I'll include this fix, and the tests in my next patch. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15446146",
            "date": "2016-08-29T15:15:15+0000",
            "content": "New patch.\n\n\tFixed the DBQ by updated value, as per Yonik's suggestion. (Thus working around LUCENE-7344)\n\tWith reordered DBQs, Shalin Shekhar Mangar pointed out a scenario whereby a document that needs in-place update cannot be \"resurrected\" if it has been deleted by an out of order DBQ. TODO: I'm working on the fix and the test for this.\n\tTODO: Fix a few nocommits in this patch.\n\tTODO: In-line reply to Hoss' suggestions.\n\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15477270",
            "date": "2016-09-09T15:04:42+0000",
            "content": "Updated patch. Fixes the nocommits, test for reordered DBQs (which now can potentially throw a replica into LIR).\nThere are occasional failures with the TestStressInPlaceUpdates test, that are basically due to: \"ClusterState says we are the leader, but locally we don't think so. Request came from null\". I'm looking into them. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15477688",
            "date": "2016-09-09T17:37:48+0000",
            "content": "Here's more on the problem with the re-ordered DBQs that Shalin pointed out.\nWhen a document requiring in-place update cannot be \"resurrected\" when the original full indexed document has been deleted by an out of order DBQ. As per this patch, expected behaviour in this case is to throw the replica into LIR (this situation itself is rare). Here's an example of the situation:\n\n        ADD(id=x, val=5, ver=1)\n        UPD(id=x, val=10, ver = 2)\n        DBQ(q=val:10, ver=4)\n        DV(id=x, val=5, ver=3)\n\n\n\nHere ^ the version 3 update, when it arrives, will not have a document to update to in its index. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15482887",
            "date": "2016-09-12T03:02:28+0000",
            "content": "Here's a log for the failure that I am looking into. This happened around once in 200 runs of the stress test. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15547986",
            "date": "2016-10-05T07:49:42+0000",
            "content": "Updated patch, brought up to master. Here are my replies inline (not all of them, but I'll keep editing this comment to provide all replies).\n\n\nOk \u2013 it took a while, but here's my notes after reviewing the latest patch....\n\nDistributedUpdateProcessor\n\n\twaitForDependentUpdates FIXED\n\t\n\t\tI know you & shalin went back and forth a bit on the wait call (ie: wait(100) with max retries vs wait(5000)) but i think the way things settled out bucket.wait(waitTimeout.timeLeft(TimeUnit.MILLISECONDS)); would be better then a generic wait(5000)\n\t\t\n\t\t\tconsider the scenerio where: the dependent update is never going to come; a spurious notify/wake happens during the first \"wait\" call @ 4950ms; the lookupVersion call takes 45ms.  Now we've only got 5ms left on our original TimeOut, but we could wind up \"wait\"ing another full 5s (total of 10s) unless we get another spurrious notify/wake inthe mean time.\n\t\t\n\t\t\n\t\tlog.info(\"Fetched the update: \" + missingUpdate); that's a really good candidate for templating since the AddUpdateCommand.toString() could be expensive if log.info winds up being a no-op (ie: log.info(\"Fetched the update: {}\", missingUpdate);)\n\t\n\t\n\n\n\n\n\tfetchMissingUpdateFromLeader\n\t\n\t\tIn response to a previous question you said...\n[FIXED. Initially, I wanted to fetch all missing updates, i.e. from what we have till what we want. Noble suggested that fetching only one at a time makes more sense.] ... but from what i can tell skimming RTGC.processGetUpdates() it's still possible that multiple updates will be returned, notably in the case where: // Must return all delete-by-query commands that occur after the first add requested.  How is that possibility handled in the code paths that use fetchMissingUpdateFromLeader?\n\t\t\n\t\t\tthat seems like a scenerio that would be really easy to test for \u2013 similar to how outOfOrderDeleteUpdatesIndividualReplicaTest works\n\t\t\n\t\t\n\t\tassert ((List<List>) missingUpdates).size() == 1: \"More than 1 update ...\n\t\t\n\t\t\tbased on my skimming of the code, an empty list is just as possible, so the assertion is missleading (ideally it should say how many updates it got, or maybe toString() the whole List ?)\n\t\t\n\t\t\n\t\n\t\n\n\n\n\nAtomicUpdateDocumentMerger\n\n\tisSupportedFieldForInPlaceUpdate\n\t\n\t\tjavadocs\n\t\n\t\n\n\n\n\n\tgetFieldNamesFromIndex\n\t\n\t\tjavadocs\n\t\tmethod name seems VERY missleading considering what it does Changed it to getSearcherNonStoredDVs\n\t\n\t\n\n\n\n\n\tisInPlaceUpdate\n\t\n\t\tjavadocs should be clear what hapens to inPlaceUpdatedFields if result is false (even if answer is \"undefined\"\n\t\tbased on usage, wouldn't it be simplier if instead of returning a boolean, this method just returned a (new) Set of inplace update fields found, and if the set is empty that means it's not an in place update? FIXED\n\t\tisn't getFieldNamesFromIndex kind of an expensive method to call on every AddUpdateCommand ?\n\t\t\n\t\t\tcouldn't this list of fields be created by the caller and re-used at least for the entire request (ie: when adding multiple docs) ? The set returned is precomputed upon the opening of a searcher. The only cost I see is to create a new unmodifiableSet every time. I'd prefer to take up this optimization later, if needed.\n\t\t\n\t\t\n\t\tif (indexFields.contains(fieldName) == false && schema.isDynamicField(fieldName))\n\t\t\n\t\t\twhy does it matter one way or the other if it's a dynamicField? Changed the logic to check in the IW for presence of field. Added a comment: \"// if dynamic field and this field doesn't exist, DV update can't work\"\n\t\t\n\t\t\n\t\tthe special DELETED sentinal value still isn't being checked against the return value of getInputDocumentFromTlog Not using getInputDocumentFromTlog call anymore\n\t\tthis method still seems like it could/should do \"cheaper\" validation (ie: not requiring SchemaField object creation, or tlog lookups) first.  (Ex: the set of supported atomic ops are checked after isSupportedFieldForInPlaceUpdate & a possible read from the tlog). FIXED\n\t\t\n\t\t\tMy suggested rewrite would be something like...\n\nSet<String> candidateResults = new HashSet<>();\n// first pass, check the things that are virtually free,\n// and bail out early if anything is obviously not a valid in-place update\nfor (String fieldName : sdoc.getFieldNames()) {\n  if (schema.getUniqueKeyField().getName().equals(fieldName)\n      || fieldName.equals(DistributedUpdateProcessor.VERSION_FIELD)) {\n    continue;\n  }\n  Object fieldValue = sdoc.getField(fieldName).getValue();\n  if (! (fieldValue instanceof Map) ) {\n    // not even an atomic update, definitely not an in-place update\n    return Collections.emptySet();\n  }\n  // else it's a atomic update map...\n  for (String op : ((Map<String, Object>)fieldValue).keySet()) {\n    if (!op.equals(\"set\") && !op.equals(\"inc\")) {\n      // not a supported in-place update op\n      return Collections.emptySet();\n    }\n  }\n  candidateResults.add(fieldName);\n}\nif (candidateResults.isEmpty()) {\n  return Collections.emptySet();\n}\n// now do the more expensive checks...\nSet<String> indexFields = getFieldNamesFromIndex(cmd.getReq().getCore());\nSolrInputDocument rtDoc = null; // will lazy read from tlog if needed\nfor (String fieldName : candidateResults) {\n  SchemaField schemaField = schema.getField(fieldName);\n  // TODO: check isSupportedFieldForInPlaceUpdate\n  // TODO: check copyfields\n  // TODO: check indexFields, if not there...\n     // TODO: init rtDoc if null\n     // TODO: check rtDoc\n  // ...if any of these checks fail, immediately return Collections.emptySet()\n}\nreturn candidateResults;\n\n\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tdoInPlaceUpdateMerge\n\t\n\t\tjdocs should make it clear that updatedFields will be modified, and that the caller should have already ensured they are valid acording to isSupportedFieldForInPlaceUpdate(...) Added a note: \"updatedFields passed into the method can be changed, i.e. the version field can be added to the set.\"\n\t\t\n\t\t\teither that, or make isInPlaceUpdate include VERSION_FIELD, and have this method assert that it's there.\n\t\t\n\t\t\n\t\tif (docid >= 0) FIXED: Bailing out to do a full atomic update here\n\t\t\n\t\t\twhat if it's not? should that trigger an error? ... even if it's \"ok\" a comment as to why it's ok and what is expected to happen instead down the flow of the code would be helpful here.\n\t\t\n\t\t\n\t\t// Copy over all supported DVs from oldDocument to partialDoc\n\t\t\n\t\t\twhy are we copying all supported DVs over? If we update dv1 in one update, and then update dv2, and again update dv1 (without commits in between), the last update would fetch from the tlog the partial doc for dv2 update. If that doc doesn't copy over the previous updates to dv1, then a full resolution (by following previous pointers) would need to be done to calculate the dv1 value. Hence, I decided to copy over older DV updates.\n\t\t\tcan't we loop over updatedFields and only copy the dv fields we need over?\n\t\t\tand if we do that, can't we skip the SchemaField creation & isSupportedFieldForInPlaceUpdate(...) check (since isInPlaceUpdate(...) should have already checked that)\n\t\t\n\t\t\n\t\tif (isSupportedFieldForInPlaceUpdate(schemaField) || fieldName.equals(schema.getUniqueKeyField().getName()))\n\t\t\n\t\t\tfetch the uniqueKey field name outside of the loop, so we're not making 2 method calls every iteration to get it FIXED\n\t\t\tif the isSupportedFieldForInPlaceUpdate(...) really is still neccessary: swap the order of the OR so we do the cheap equality test first.\n\t\t\n\t\t\n\t\tin response to a previous suggestion i made about adding explicit unit tests to this method, you said: \n[I've refactored this method to make it much simpler and to now call the original merge method. I don't think we need specific tests for this method any longer.]... but i very much disagree.  There's still enough complexity here (paticularly with copying old docvals, the possibility that a previous update from the tlog may not have included all the fields we need to udpate now) that i think some unit tests like i described (ahnd creating a sequence of udpates, checking the expected result of merging, and the 'committing' and checking we still get the same results) would still be very useful. Added a test for this: TestInPlaceUpdatesStandalone.testDoInPlaceUpdateMerge\n\t\n\t\n\n\n\nRealTimeGetComponent\n\n\tSearcherInfo\n\t\n\t\tstill has no javadocs\n\t\n\t\n\n\n\n\n\tresolveFullDocument\n\t\n\t\tI made this suggestion regarding simplifying the callers of this method: \nhy not consolidate those cases into one block of code using (a modified) resolveFullDocument which can start with a call to toSolrDoc(...) and then return immediately if the entry is UpdateLog.ADD ?. your response was...[FIXED: but kept the call to toSolrDoc() call outside the resolveFullDocument]  I explaind why i thought this refactoring simplified the calling code, can you please elaborate / explain why you think the code is better w/o this refactoring? One of the callers of this resolveFullDocument() (RTGC.process()) is calling with a SolrDocument by calling toSolrDoc() on the tlog SID, another (getInputDocumentFromTlog()) is calling this with the tlog SID directly. It wasn't clear to me how to elegantly consolidate these two calls with a common method, and hence kept this SolrDocument vs SolrInputDocument conversion/obtaining outside this method, and used the SolrDocumentBase as a return type.\n\t\n\t\n\n\n\n\n\treopenRealtimeSearcherAndGet As I was tackling simultaneous deletions/DBQs, it became clear that this was not a fatal error that deserved an exception. When a doc isn't found during the resolution process, it must be a case of deletes happening. Hence, I decided to bail out from reopenRealtimeSearcherAndGet() with a null (not exception) and let the caller handle the situation.\n\t\n\t\ti'm confused about the \"throw INVALID_STATE vs return null\" change to this method.\n\t\t\n\t\t\tin the previous version of this method, it would never return null - instead it threw INVALID_STATE exception.\n\t\t\tI asked you about this, and your response was that it was definitely an (unusual) fatal error.\n\t\t\tBut in the latest version of the patch, you've cahnged the method so that it does return null in these cases, and every place it's called is now responsible to checking the reuslt for null, and throwing INVALID_STATE\n\t\t\t\n\t\t\t\tThere's some subtle variation in the INVALID_STATE msgs, but it's not clear if that's actually intention, or a copy/paste oversight.\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\twhy make this change? why isn't reopenRealtimeSearcherAndGet still throwing the exception itself?\n\t\tregardless of who throws these exceptions, we have an idTerm to work with, so the new String(...) should probably just be replaced with idTerm.text()\n\t\n\t\n\n\n\n\n\tgetInputDocument\n\t\n\t\tI'm confused by the permutations of this method..\n\t\t\n\t\t\tthe meat of the implementation is now in a 5 arg method: getInputDocument(SolrCore core, BytesRef idBytes, boolean avoidRetrievingStoredFields,Set<String> onlyTheseNonStoredDVs, boolean resolveFullDocument\n\t\t\tyou added a 3 arg helper that hardcodes avoidRetrievingStoredFields=false and onlyTheseNonStoredDVs=null\n\t\t\tyou removed the existing 2 arg method which was getInputDocument(SolrCore core, BytesRef idBytes) and updated those callers to use the 3 arg helper\n\t\t\t\n\t\t\t\tin the process i think you broken DocBasedVersionConstraintsProcessorFactory, but i'll come back to that later.\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\twhy not leave the 2 arg helper for backcompat, and any new/modified code that needs more control can use the 5 arg method? FIXED\n\t\t\n\t\t\tthe 2 arg method should almost certainly delgate to getInputDocument(core,idByes,false,null,true ... see comments below regarding DocBasedVersionConstraintsProcessorFactory FIXED\n\t\t\n\t\t\n\t\n\t\n\n\n\n\nDocBasedVersionConstraintsProcessorFactory\n\n\treplacing getInputDocument(core,id with getInputDocument(core,id,false smells wrong here FIXED: calling 2 arg ctor, that calls true\n\t\n\t\tand ditto for getInputDocumentFromTlog(core,id -> getInputDocumentFromTlog(core,id,false\n\t\n\t\n\n\n\n\n\tprior to this patch, there was no such thing as an inplace update \u2013 any call to getInputDocument* would by definition return a \"full document\"\n\n\n\n\n\tnow that the concept of an inplace update exists, and these getInputDocument* methods take a new third arg, shouldn't all existing calls to these methods pass true for the third arg to ensure conistent behavior?\n\t\n\t\tunless of course, there is specific reason why we know some caller code doesn't care about resolving the full document \u2013 in which case there should be a comment on the call as to why false is hardcoded. (but that doesn't seem to be the case here from my skimming of the usage)\n\t\n\t\n\n\n\nDocumentBuilder\n\n\tmethod javadocs should explain how the forInPlaceUpdate param(s) will be used (ie: why does it matter?) FIXED\n\n\n\n\n\taddField\n\t\n\t\twhat if someone passes an IndexableField \"val\" that is not a NumericDocValuesField, and \"forInPlaceUpdate=true\" ... shouldn't that be invalid? (ie: trip an assert?) FIXED\n\t\twhy do we need to check uniqueKeyField != null && f.name().equals(uniqueKeyField.getName()) on every IndexableField ?\n\t\t\n\t\t\twhy not just short circut out at the begining of the method if forInPlaceUpdate && field.equals(uniqueKeyField) ?\n\t\t\tor better still: make the caller do it and eliminate the uniqueKeyField param (only 3 places this method is called, and in one of them we're garunteed false==forInPlaceUpdate so no check would be needed there anyway) FIXED\n\t\t\n\t\t\n\t\tif true==forInPlaceUpdate and createFields(...) returns anything that is not a NumericDocValuesField (or returns more then one) shouldn't that trip an assert or something? (ie: doesn't that mean this SchemaField isn't valid for using with an in place update, and the code shouldn't have gotten this far?) This is fine, since createFields() generates both NDV and non NDV fields for an indexable field, and the intent is to drop the non NDV one. Added a comment to this effect\n\t\tin general it seems like this method should just be something much simpler that like...\n\nprivate static void addField(Document doc, SchemaField field, Object val, float boost, boolean forInPlaceUpdate) {\n  if (val instanceof IndexableField) {\n    assert ( (val instanceof NumericDocValuesField) || ! forInPlaceUpdate)\n    // set boost to the calculated compound boost\n    ((Field)val).setBoost(boost);\n    doc.add((Field)val);\n    return;\n  }\n  int numvals = 0;\n  for (IndexableField f : field.getType().createFields(field, val, boost)) {\n    if (null == f) continue; // null fields are not added\n    assert ( (f instanceof NumericDocValuesField) || ! forInPlaceUpdate)\n    doc.add((Field) f);\n    numvals++;\n  }\n  assert ((numvals <= 1) || ! forInPlaceUpdate)\n}\n\n\nDid a refactoring that seems simple, similar to your suggestion\n\t\n\t\n\n\n\n\n\ttoDocument\n\t\n\t\t// Do not copy this field if this document is to be used for an in-place update, ... and this is the uniqueKey field\n\t\t\n\t\t\twhy not?\n\t\t\ti'm guessing this is just a small optimization? because the uniqueKey can't change so no need to \"update\" the copyField?\n\t\t\tif so, the comment should make that reasoning clear FIXED: updated the comment\n\t\t\n\t\t\n\t\tWe don't need to do this if this document is to be used for in-place updates\n\t\t\n\t\t\tagain: comment should explain why we don't need to do this.\n\t\t\n\t\t\n\t\n\t\n\n\n\n\nAddUpdateCommand\n\n\tprevVersion\n\t\n\t\twhy can't this comment be a javadoc comment?\n\t\tcomment/javadoc should explain what special values (like -1) mean\nFIXED\n\t\n\t\n\n\n\n\n\tgetLuceneDocument(boolean)\n\t\n\t\tjavadocs FIXED\n\t\n\t\n\tgetLuceneDocument()\n\t\n\t\tjavadocs should be updated to indicate (default) behavior compared to one arg version FIXED\n\t\n\t\n\n\n\n\nSolrDocumentBase\n\n\twhy does containsKey need declared here? it's already part of Map interface FIXED\n\tshould be able to drop this file from patch\n\n\n\n\nXMLLoader\n\n\twhitespace only changes ... drop from patch.\n\n\n\n\nDirectUpdateHandler2\n\n\tWTF: printStackTrace FIXED\n\n\n\n\n\tsolrCoreState.getUpdateLock() usage\n\t\n\t\tI don't know why I didn't ask about this last time, but is this really the appropriate/only lock to use here? Removed the updateLock use here by calling the internally atomic writer.updateDocValues() method. I don't remember why I didn't use it in the first place, I think I couldn't find such a method back then, or could be oversight\n\t\tI'm looking at the other usage of this lock, notably the comments in DUH2.deleteByQuery(...) and it seems like using this lock may be neccessary to prevent a commit from happening in the middle of multiple in-place docvalue updates ... but is sufficient ?\n\t\tnotably: i'm wondering what happens if doc#X gets an in-place update of N fields and concurrently some oher client sends a full replacement of doc#X (because DUH2 doesn't wrap existing writer.updateDocument calls in any synchronization on the solrCoreState.getUpdateLock() ?\n\t\tSo imagine the following sequence of ops:\n\t\t\n\t\t\tThread #1 enters DUH2.doNormalUpdate to do an in-place update of doc#X with 2 docvalues fields\n\t\t\tThread #2 enters DUH2.doNormalUpdate to do a full replacement of doc#X\n\t\t\tThread #1 acquires updateLock\n\t\t\tThread #1 calls writer.updateNumericDocValue on 1st field\n\t\t\tThread #2 calls writer.updateDocument on doc#x\n\t\t\tThread #1 calls writer.updateNumericDocValue on 2nd field\n\t\t\tThread #1 releases updateLock\n\t\t\n\t\t\n\t\tDo we need to either wrap all writer.updateDocument calls in the updateLock, or use some sort of secondary fineer grained \"per-uniqueKey\" locking as well to prevent bad stuff from happening in sequences like this? There was writer.updateDocValues() that I overlooked in the beginning, or was not present back then.\n\t\n\t\n\n\n\n\nUpdateLog\n\n\tPlease add some javadocs to the new *_IDX constants you added explaining what they are for \u2013 notably which ones are only relevant if FLAGS_IDX includes UPDATE_INPLACE Fixed\n\n\n\n\n\tapplyPartialUpdates\n\t\n\t\tI had some concerns in my last patch review about the sync block in this method, and how lookupLogs is used \u2013 but i don't see any response to those concerns in previous comments, or any relevant edits to this method?\n\n\"the sync block in this method, and how the resulting lookupLogs list is used subsequently, doesn't seem safe to me \u2013 particularly the way getEntryFromTLog calls incref/decref on each TransactionLog as it loops over that list...\"\n\n    \"what prevents some other thread from decref'ing one of these TransactionLog objects (and possibly auto-closing it) in between the sync block and the incref in getEntryFromTLog?\n        (most existing usages of TransactionLog.incref() seem to be in blocks that sync on the UpdateLog \u2013 and the ones that aren't in sync blocks look sketchy to me as well)\" <-- I tried to use the tlogs as they are used elsewhere. It is my understanding is that due to the incref/decref being inside the synchronized(ulog) block, no other thread's action would affect whatever happens to these tlog once we've entered the block. If so, either the tlogs are null (already closed or never existed at all) or not closed, in either case we should be good.\n\n    \"in general i'm wondering if lookupLogs should be created outside of the while loop, so that there is a consistent set of \"logs\" for the duration of the method call ... what happens right now if some other thread changes tlog/prevMapLog/prevMapLog2 in between iterations of the while loop?\" <-- I added them inside the while block so that if there have been multiple commits during the course of iterating the while loop, those tlogs wouldn't have grown stale. Do you think we should pull it out outside the while loop?\n\t\n\t\n\n\n\n\n\n\tgetEntryFromTLog\n\t\n\t\tin repsonse to a question i had about the catch (Exception | Error ex) block, you said...\n..If the seek results in an exception due to unable to deserialize the tlog entry, we can ignore it, since it just means we were looking up the wrong tlog. I have added a comment to this effect in the catch block....but that debug msg doesn't make it clear that this is \"ok\" .... if an end user sees DEBUG: Exception reading the log... in their log file, that seems like something very bad ... what are they suppose to do with that information? Added a \"(this is expected, don't worry)\" to the exception message.\n\t\n\t\n\n\n\n\n\tupdateCommandFromTlog\n\t\n\t\tneeds javadocs\n\t\twhy can't this method conditionally set the prevVersion so that case UpdateLog.ADD code paths (in both this class, and in other switch/case statements) be refactored to also use this method? FIXED: Named it addUpdateCommandFromTlog()\n\t\tIf it's only going to be valid for UPDATE_INPLACE entries, that should be clear from the method name and asserted in case someone tries to miss-use it.\n\t\t\n\t\t\teven if it's valid for both ADD and UPDATE_INPLACE, that should be clear from the method name (ie: convertAddUpdateCommandFromTlog perhaps?) and asserted in case someone tries to miss-use it.\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tdoReplay\n\t\n\t\tthe case UpdateLog.UPDATE_INPLACE block includes log.debug(\"add {}\", cmd); .. \"add\" doesn't seem appropriate\n\t\t\n\t\t\tif \"add\" is appropriate, then that seems like even more justification for updateCommandFromTlog being refactored to work on both ADD and UPDATE_INPLACE log entries, so those 2 case statements can be combined completley (via fall through) FIXED: log.debug(oper == ADD? \"add \": \"update \" + cmd);\n\t\t\n\t\t\n\t\n\t\n\n\n\nTransactionLog\n\n\tjdocs for the two write methods seem backwards FIXED\n\t\n\t\tif i'm calling the 2 arg version the jdocs should make it clear this can not be used for in-place updates\n\t\tif i'm calling the 3 arg version, the jdocs should make it clear what prevPointer value i should use if it is not an in-place update\n\t\n\t\n\tseems like the 3 arg write method should have an assertion that looks something like...\n\t\n\t\tassert (-1 <= prevPointer && (cmd.isInPlaceUpdate() ^ (-1 == prevPointer)))  Added a modified version of this assert:\nassert (-1 <= prevPointer && (cmd.isInPlaceUpdate() || (-1 == prevPointer))), since it is possible that this is an in-place update \"AND\" prevPointer==-1, for the case when the previous document is in the index and not in the current tlogs. \n\t\n\t\n\n\n\ntest-files: schema.xml & schema15.xml\n\n\tpresumably the main purpose of the _version_ field def changes in these files is because a docvalues only version field is required by the new/modified tests that use them to support in place updates\n\t\n\t\tbut i didn't see any of these tests assert that the _version_ field had the neccessary properties \u2013 so there is no protection against anybody down the road modifying these schema files again and breaking that assumption (so that only \"regular\" atomic updates happen ... maybe causing confusing test failures, or maybe just weakening the tests.Added check for version in schema.xml for PeerSyncTest, in schema15.xml for TestRecovery\n\t\n\t\n\n\n\nschema-minimal-atomic-stress.xml\n\n\tI would prefer we not make most of the changes in this file. Reverted all changes to this file\n\t\n\t\tInstead I would strongly prefer you add a new, small, purpose built & well commented schema-inplace-atomic-updates.xml file just for the new InPlace tests \u2013 containing only the fields/dynamicFields needed schema-inplace-updates.xml added\n\t\thaving smaller single purpose test configs makes it a lot easier for people reviewing tests to understand what the minimum expectations are for the test to function\n\t\tEven if there was some incredibly strong reason to keep all these tests usign the same file...\n\t\t\n\t\t\twe should keep all the field/fieldType/dynamicField naming conventions cosistent\n\t\t\tif the existing naming convention of \"long_dv\" won't work for the new fields needed (ie: \"float_dv\", \"int_dv\", etc...) because the new tests need more then one field of each type, then the existing field declarations should be removed and TestStressCloudBlindAtomicUpdates should be updated to use field names based on the new dynamicField conventions (ie: \"long_dv\" -> \"*_l_dvo\")\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tin general, regarldess of what schema file is used by all the new tests...\n\t\n\t\twe should stop using the ambiguious \"ratings\" and \"price\" field names FIXED\n\t\tinstead the tests should use field names that clearly convey what the expecation of the field is \u2013 either vie xplicit schema declaration, or via one of the dynamicFields you already defined (ie: \"inplace_updatable_x_long\" or \"fieldX_l_dvo\")\n\t\t\n\t\t\tprobably a good idea for one to be an explicitly declared <field/> and the other to come from a <dynamicField/> just to ensure good test coverate of both scenerios \u2013 but either way the name as seen in the test files should make it clear what matters about the field)\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\nUpdateLogTest\n\n\tthe changes forced on the CommitTracker should be done in the breforeClass method so they don't suprise the hell out of anyone who tries adding new test methods to this class later. FIXED\n\t\n\t\tuse a static UpdateLog variable so test methods like testApplyPartialUpdates can re-use it (and be confident it's the same one with the CommitTracker changes FIXED\n\t\n\t\n\n\n\n\n\tin general there are a lot of LocalSolrQueryRequest objects created in this class that are never closed FIXED\n\t\n\t\tthat may not be causing problems right now, but it's arecipe for confusing leaks later\n\t\tthe convinience methods that build up UpdateCommants (ulogCommit & getDeleteUpdate & getAddUpdate) sould be refactored to take in a SolrQueryRequest which should be closed when the logical bit of code related to testing that UpdateCommand is finished. something like...\n\ntry (SolrQueryRequest req = req()) {\n  ulog.add(getAddUpdate(req, ...));\n  // TODO: anything else that needs to be part of this same request for test purposes?\n}\n// TODO ... assert stuff about ulog\n...\n\n\n\t\n\t\n\n\n\n\n\tin general, it's confusing to me how exactly the commands being tested here represent \"in-place\" updates considering the sdocs genreated don't included any inc/set atomic ops at all\n\t\n\t\ti think what's happening here is that we're testing at a low enough level that the logic for actualy applying the in-place update has already happened, and here we're just testing the merging of that doc (after the inc/set has been applied) with the older versions?\n\t\tif my understanding is correct, some comments/javadocs in the helper methods making this more clear would be helpful.Correct. Added comment: \"This method, when prevVersion is passed in (i.e. for in-place update), represents an  that has undergone the merge process and inc/set operations have now been converted into actual values that just need to be written.\"\n\t\tif i'm wrong, then please clarify...\n\t\n\t\n\n\n\n\n\ttestApplyPartialUpdates\n\t\n\t\tjavadocs: what is being tested here? use an @see tag if nothing else FIXED\n\t\tin general it feels like this should be split up into multipe discreet test methods (ie: testApplyPartialUpdatesOnMultipleInPlaceUpdatesInSequence, testApplyPartialUpdatesAfterMultipleCommits, etc...) Fixed.\n\t\t// If an in-place update depends on a non-add, assert that an exception is thrown.\n\t\t\n\t\t\tuse expectThrows...\n\n \nSolrException ex = expectThrows(SolrException.class, () -> {\n  returnVal = ulog.applyPartialUpdates(cmd.getIndexedId(), prevPointer, prevVersion, partialDoc);\n});\n// TODO: assert stuff about ex.code, ex.getMessage()\n\n\nCouldn't use expectThrows as suggested, since it required variables like prevPointer, prevVersion, partialDoc etc. to be final variables, and changing them to such would make things ugly. Maybe I missed something?\n\t\t\n\t\t\n\t\twhat about testing DBQs in the ulog and how they affect partial updates? Any DBQ causes ulog to be cleared after it (irrespective of in-place updates, irrespective of whether any documents were deleted or not). Only that effect can be tested, and I've added a test for it.\n\t\n\t\n\n\n\n\n\tulogCommit FIXED\n\t\n\t\tjavadocs\n\t\twhy can't this be static?\n\t\tsee previous comment about method args vs un-closed SolrQueryRequest\n\t\n\t\n\n\n\n\n\tgetDeleteUpdate FIXED\n\t\n\t\tjavadocs\n\t\twhy can't this be static?\n\t\tsee previous comment about method args vs un-closed SolrQueryRequest\n\t\n\t\n\n\n\n\n\tgetAddUpdate FIXED\n\t\n\t\tjavadocs\n\t\tsee previous comment about method args vs un-closed SolrQueryRequest\n\t\ti would change this method to take in a SolrInputDocument\n\t\t\n\t\t\tgetAddUpdate(42, sdoc(....)); is just as easy to read as getAddUpdate(42, ....); but more versitile\n\t\t\n\t\t\n\t\taren't very bad things garunteed to happen unless cmd.setVersion is set?\n\t\t\n\t\t\ti would remove the conditional wrapping cmd.setVersion and instead just assert that there is a version in the solrDoc to copy FIXED\n\t\t\teither that, or have some usage/test cases where the doc being added doesn't include a version field\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\ttoSolrDoc (& toSolrDoc) FIXED: Used the RTGC one\n\t\n\t\twhy are these methods being copied/pasted from RTGC?\n\t\tif they are useful for testing, don't clone them \u2013 change them to public, anotate them with @lucene.experimental and use them directly from the test.\n\t\t\n\t\t\tonly the 3 arg seems needed directly by the test, the 2 arg method can probably remain private.\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\n\nPeerSyncTest\n\n\tWTF: System.out.println() ??? FIXED\n\n\n\n\n\t// lets add some in-place updates FIXED: Added 3 in-place updates.\n\t\n\t\tinPlaceParams is only used once, so there aren't \"some in-place updates\" there is exactly 1 \"in-place update\"\n\t\ti'm guessing you inteded to re-use inPlaceParams in all the add(...) calls but had a copy/paste glitch?\n\t\tif i'm wrong, please include a comment explaining what exactly you're testing .. what's the point of the subsequent adds that do not set an DISTRIB_INPLACE_PREVVERSION\n\t\n\t\n\n\n\n\n\tv++\n\t\n\t\tplease avoid these kind of \"postfix increment happens after variable is evaluated\" short cuts \u2013 especailly since every other place in this testclase uses prefix increment to precivesly to avoid this kind of confusion.\n\t\tthe test would be much more readable if you just use concrete hardcoded literal version values in place of \"v++\" in every line FIXED\n\t\t\n\t\t\tespecially since DISTRIB_INPLACE_PREVVERSION has a hard coded assumption about what the value of \"v\" was\n\t\t\tthe fact that the \"id\" in use here is also the same as the starting value of \"v\" makes the DISTRIB_INPLACE_PREVVERSION actaully seem wrong at first glance\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tyour new code is adding the docs to client0, but then also calling assertSync on client0, and passing shardsArr[0] for the syncWith param. FIXED: I had misunderstood the whole test logic at first. Now adding to client0 and syncing/asserting with client1\n\t\n\t\tI'm 99% certain you're just asking client0 to sync with itself.\n\t\tif you add some queryAndCompare() calls to ensure these syncs are doing what you think, i'm pretty sure you'll see client0 and client1 disagree\n\t\t\n\t\t\teven if i'm wrong, adding some queryAndCompare calls would be useful to ensure the peersync is actually doing something useful and not just silently returning \"true\"\n\t\t\tplease don't follow the bad example usgae of queryAndCompare earlier in this test class \u2013 actually look at the QueryResponse that method returns and check the numfound, and returned field values ,etc...) Fixed\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\twhat about mixing in-place updates with deletes (both by id and query?) Fixed\n\n\n\n\n\n\nTestRecovery\n\n\twhy is // assert that in-place update is retained at teh end of the test, instead of when the (first) log replay happens and *:* -> numFound=3 is asserted? (ie: before \"A2\" is added) FIXED\n\n\n\n\n\twhat about mixing in-place updates with deletes (both by id and query?) FIXED\n\n\n\n\n\nTestInPlaceUpdatesStandalone\n\n\ttest setup still doesn't seem to be doing anything to assert that autocommit is disabled Fixed\n\t\n\t\tthis is important to prevent future changes to configs/defaults from weaking the test or causing spurrious / confusing failures.\n\t\n\t\n\n\n\n\n\tclearIndex & softCommit FIXED\n\t\n\t\tthese appear at the begining of every test method\n\t\tinstead add an @Before method to ensure this happens automatically\n\t\n\t\n\n\n\n\n\ttestUpdatingDocValues\n\t\n\t\tchecking return values from addAndGetVersion / addAndAssertVersion\n\t\t\n\t\t\tin a previous review of these tests, i pointed out...\n\n\tfor addAndGetVersion calls where we don't care about the returned version, don't bother assigning it to a variable (distracting)\n\tfor addAndGetVersion calls where we do care about the returned version, we need check it for every update to that doc...\n\n... but in the new patch things are still very confusing. FIXED\n\t\t\tthis new patch now includes an addAndAssertVersion which looks like it was designed to ensure that all updates (even atomic, in place or otherwiser) produce a new version greater then the old version \u2013 but many usages of this method that i see just pass \"0\" for the \"expected\" version \u2014 even though the actual expected version is already known...\n\nlong version1 = addAndGetVersion(sdoc(\"id\", \"1\", \"title_s\", \"first\"), null);\n...\n// Check docValues were \"set\"\nversion1 = addAndAssertVersion(0, \"id\", \"1\", \"ratings\", map(\"set\", 200));\n\n\n FIXED\n\t\t\n\t\t\n\t\tthe docid# vars should have a comment making it clear that the reason we're fetching these is to validate that the subsequent updates are done in place and don't cause the docids to change\n\t\t// Check back to back \"inc\"s are working (off the transaction log)\n\t\t\n\t\t\twhy isn't the query assertion here also checking the docid? FIXED\n\t\t\n\t\t\n\t\t// Check optimistic concurrency works\n\t\t\n\t\t\tplease use expectThrows here, and actually validate that the Exception we get is a SolrException with code=409 FIXED: checked message contains \"conflict\"\n\t\t\t\n\t\t\t\twe don't want the test to silently overlook if some bug is introduced when using optimistic concurrency with in-place updates\n\t\t\t\n\t\t\t\n\t\t\tInstead of \"123456,\" (which might randomly be the actual version of the doc) use \"-1\" (which indicates yo uexpect the document to not exist) FIXED\n\t\t\tbefore committing & doing the assertQ to check the searchable results, we should also be checking the results of an RTG against the tlog to ensure they're returning the in-place updated values as well. FIXED\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\n\ttestUpdateTwoDifferedFields\n\t\n\t\tditto previous concerns about passing \"0\" to addAndAssertVersion in many cases FIXED\n\t\t// RTG}\n\t\t\n\t\t\tthis is ... bizare ... i don't understand how this is remotely suppose to demonstrate an RTG. FIXED: Wasn't able to make it work with the xpath based assertQ() (i.e. the code that you suggested), so used assertJQ instead and it works now.\n\t\t\tplease replace with something like...\n\nassertQ(\"RTG check\",\n        req(\"qt\",\"/get\",\"id\",id, \"wt\",\"xml\", \"fl\",\"score,val_i\"),\n        \"//doc/float[@name='ratings'][.='202.0']\",\n        \"//doc/int[@name='price'][.='12']\",\n        \"//doc/long[@name='_version_'][.='\"+version1+\"']\",\n        \"//doc/int[@name='[docid]'][.='\"+docid1+\"']\"\n        );\n\n\n\t\t\ti also don't understand why this is only done conditionally based on a random boolean \u2013 it has no side effects, and it's not an alternative to checking a regular query \u2013 so why not do it in every test run before the softCommit + query? FIXED\n\t\t\n\t\t\n\t\ti was really confused that this test never actaully verified that you could do in-place updates on 2 diff fields in a single request ... let's add that please...\n\nversion1 = addAndAssertVersion(0, \"id\", \"1\", \"ratings\", map(\"inc\", 66), \"price\", map(\"inc\", 44));\n\n\n FIXED: see comment \"same update command, updating both the fields together\"\n\t\n\t\n\n\n\n\n\ttestDVUpdatesWithDelete\n\t\n\t\t// Add the document back to index\n\t\t\n\t\t\tdoesn't this weaken the test? FIXED: removed this\n\t\t\tshouldn't an add with map(\"set\", 200) succeed even after a delete (w/o needing add the doc first) Yes, for non existent docs/fields, a full atomic update will happen\n\t\t\tshouldn't the point here be that we don't pick up some already deleted doc (from index or tlog) and apply an in-place update to it?\n\t\t\tsomething like...\n\nfor (boolean postAddCommit : Arrays.asList(true, false)) {\n  for (boolean delById : Arrays.asList(true, false)) {\n    for (boolean postDelCommit : Arrays.asList(true, false)) {\n      addAndGetVersion(sdoc(\"id\", \"1\", \"title_s\", \"first\"), null);\n      if (postAddCommit) assertU(commit());\n      assertU(delById ? delI(\"1\") : delQ(\"id:1\"));\n      if (postDelCommit) assertU(commit());\n      version1 = addAndGetVersion(sdoc(\"id\", \"1\", \"ratings\", map(\"set\", 200)));\n      // TODO: assert current doc#1 doesn't have old value of \"title_s\"\n\n\n\t\t\n\t\t\n\t\n\t\n\n\n\nFIXED\n\n\n\taddAndAssertVersion\n\t\n\t\twe should renamed \"version\" arg to \"expectedCurrentVersion\" to be more clear what it's purpose is FIXED\n\t\tneed assert 0 < expectedCurrentVersion to prevent missuse (see above) FIXED\n\t\n\t\n\n\n\n\n\ttestUpdateOfNonExistentDVsShouldNotFail\n\t\n\t\ti think the idea here is that it's hte only method using \"val_i_dvo\" \u2013 so adding id=0 helps test when the field doesn't exist at all, and id=1 helps test when the field does't exist for a specified doc?\n\t\t\n\t\t\tneed a whitebox check of the actual index reader to assert that field doesn't exist (in case someone adds a new test w/o realing it makes this one useless Fixed, added a check based on FieldInfos from the leafreader\n\t\t\tideally use a more distinctive field name as well FIXED\n\t\t\n\t\t\n\t\tthis test should also assert the docid doesn't change (when we dont' expect it to) TODO\n\t\n\t\n\n\n\n\n\ttestOnlyPartialUpdatesBetweenCommits\n\t\n\t\twhy not test \"inc\" here? FIXED\n\t\tshould be asserting expected versions in final query. FIXED\n\t\n\t\n\n\n\n\n\tgetFieldValue\n\t\n\t\tthis should be using RTG so it does't depend on a commit FIXED\n\t\n\t\n\n\n\n\n\tDocInfo\n\t\n\t\tis val1 used anywhere ?  can it be removed? FIXED\n\t\t\n\t\t\tif it can be removed: lets rename val2 just \"value\"\n\t\t\tif we need them both, let's rename val1 and val2 to intFieldValue and longFieldValue (or somethign like that) so it's not as easy to get them confused\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\ttestReplay3, testReplay4, testReplay6, testReplay7\n\t\n\t\tthese should have better names Fixed\n\t\tthe String constants should be replaced with sdoc calls (see comments on checkReplay below) FIXED\n\t\n\t\n\n\n\n\n\tcheckReplay FIXED\n\t\n\t\tsince we're no longer using this method to read from text files this method should be refactored...\n\t\t\n\t\t\tlet's replace the String[] param with an Object[] param\n\t\t\t\n\t\t\t\tif it's a SolrInputDoc add it\n\t\t\t\telse look for some \"private static final Object\" sentinal objects for hard commit & soft commit\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tgetSdoc FIXED\n\t\n\t\tonce checkReplay is refactored, this method can be killed\n\t\n\t\n\n\n\n\n\ttestMixedInPlaceAndNonInPlaceAtomicUpdates FIXED\n\t\n\t\tagain: this is not a good way to test RTG\n\t\tagain: there's no reason to randomize if we do an RTG check\n\t\tideally we should do an RTG check after every one of the addAndAssertVersion calls (and still do a commit & query check a the end of the method\n\t\n\t\n\n\n\n\n\n\tI'd still like to see some 100% randomized testing using checkReplay.  It's a really powerful helper method \u2013 we should be taking full advantage of it. TODO: Can be added later\n\n\n\n\nTestStressInPlaceUpdates\n\n\tA really large number of comments i made the last time i reviewed this class still seem problematic, and i can't find any (jira) comments addressing them either...\n\n\n\t...(class) javadocs, and extending SolrCloudTestCase once LUCENE-7301 is fixed and we're sure this test passes reliably. ....\n\t\n\t\talso: we should really make this test use multiple shards TODO: can be done later\n\t\n\t\n\n\n\n\n\tit would be a lot cleaner/clearer if we refactored these anonymous Thread classes into private static final (inner) classes and instantiated them like normal objects\n\t\n\t\tmakes it a lot easier to see what threads access/share what state\n\t\tbetter still would be implementing these \"workers\" as Callable instances and using an ExecutorService\n\t\n\t\n\n\n\n\n\t\"operations\" comment is bogus (it's not just for queries) FIXED\n\n\n\n\n\tI'm not convinced the \"synchronize {...}; commit stuff; syncrhonize { ... };\" sequence is actually thread safe...\n\t\n\t\tT-W1: commit sync block 1: newCommittedModel = copy(model), version = snapshotCount++;\n\t\tT-W2: updates a doc and adds it to model\n\t\tT-W1: commit\n\t\tT-W1: commit sync block 2: committedModel = newCommittedModel\n\t\tT-R3: read sync block: get info from committedModel\n\t\tT-R3: query for doc\n\t\t...\n\t\n\t\n\n\n\nCopied the logic straight from TestStressReorder. Maybe we can revisit both together, later, in light of your concerns?\n\n\n\t... in the above sequence, query results seen by thread T-R3 won't match the model because the update from T-W2 made it into the index before the commit, but after the model was copied\n\t\n\t\ti guess it's not a huge problem because the query thread doesn't bother to assert anything unless the versions match \u2013 but that seems kind of risky ... we could theoretically never assert anything\n\t\n\t\n\n\n\n\n\thaving at least one pass over the model checking every doc at the end of the test seems like a good idea no matter what FIXED\n\n\n\n\n\tI'm certain the existing \"synchronized (model)\" block is not thread safe relative to the synchronized blocks that copy the model into commitedModel, because the \"model.put(...)\" calls can change the iterator and trigger a ConcurrentModificationException Never saw a CME occur in thousands of rounds of this test. I think if a CME were to be thrown ever, it would've been quite frequent.\n\n\n\n\n\tthe writer threads should construct the SolrInputDocument themselves, and log the whole document (not just the id) when they log things, so it's easier to tell from the logs what updates succeed and which were rejected because of version conflicts FIXED: Logging id, val1, val2, version of the logged document.\n\n\n\n\n\tthere's a lot of \"instanceof ArrayList\" checks that make no sense to me since the object came from getFirstValue FIXED: was working around some bug at the time of writing the test, no longer the case now\n\n\n\n\n\tverbose Copied the logic straight from TestStressReorder. Maybe we can revisit both together, later, in light of your concerns?\n\t\n\t\twhy does this method exist? why aren't callers just using log.info(...) directly?\n\t\tor if callers really need to pass big sequences of stuff, they can use log.info(\"{}\", Arrays.asList(...))\n\t\tor worst case: this method can simplified greatly to do that internally\n\t\n\t\n\n\n\n\n\taddDocAndGetVersion FIXED\n\t\n\t\tusing SolrTestCaseJ4.sdoc and SolrTestCaseJ4.params will make this method a lot sorder shorter\n\t\n\t\n\n\n\n\n\n\n\n\tthis block reads very awkwardly...\n\nif (oper < commitPercent + deletePercent) { // deleteById\n  returnedVersion = deleteDocAndGetVersion(Integer.toString(id), params(\"_version_\",Long.toString(info.version)), false);\n} else { // deleteByQuery\n  returnedVersion = deleteDocAndGetVersion(Integer.toString(id), params(\"_version_\",Long.toString(info.version)), true);\n}\nlog.info((oper < commitPercent + deletePercent? \"DBI\": \"DBQ\")+\": Deleting id=\" + id + \"], version=\" + info.version\n    + \".  Returned version=\" + returnedVersion);\n\n\n\t\n\t\twouldn't something like this be functionally equivilent and easier to make sense of? \n\nfinal boolean dbq = (commitPercent + deletePercent <= oper);\nreturnedVersion = deleteDocAndGetVersion(Integer.toString(id), params(\"_version_\",Long.toString(info.version)), dbq);\nlog.info((dbq ? \"DBI\": \"DBQ\") +\": Deleting id=\" + id + \"], version=\" + info.version  + \".  Returned version=\" + returnedVersion);\n\n\n\t\n\t\n\n\n\nFixed\n\n\n\tthis looks like dead code ... or is something actaully still missing and needs done? ... \n\n} else if (oper < commitPercent + deletePercent + deleteByQueryPercent) {\n  // TODO\n} else {\n\n\nFIXED\n\n\n\n\n\tDocInfo\n\t\n\t\tstill needs javadocs FIXED\n\t\tlet's rename val1 and val2 to intFieldValue and longFieldValue (or somethign like that) so it's not as easy to get them confused FIXED\n\t\n\t\n\n\n\n\n\tsynchronized (leaderClient)\n\t\n\t\tI understand why we are only sending to leader (SOLR-8733) but i still don't udnerstand why the updates are synchronized\nFixed, beasting is happy.\n\t\n\t\n\tit doesn't take long to get this test to fail ... here's some seeds that failed for me when hammering thetests, (but succeeded when i tried the reproduce line) ... FIXED: There was some silly bug with deletion, which I fixed. Don't remember what the fix was.\n\t\n\t\t\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestStressInPlaceUpdates -Dtests.method=stressTest -Dtests.seed=CE02ABC183DF7342 -Dtests.slow=true -Dtests.locale=ar-LB -Dtests.timezone=America/Manaus -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] ERROR   31.1s | TestStressInPlaceUpdates.stressTest <<<\n   [junit4]    > Throwable #1: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=192, name=READER2, state=RUNNABLE, group=TGRP-TestStressInPlaceUpdates]\n   [junit4]    >        at __randomizedtesting.SeedInfo.seed([CE02ABC183DF7342:A564746CBD0AA7B8]:0)\n   [junit4]    > Caused by: java.lang.RuntimeException: org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://127.0.0.1:44464/muf/l/collection1: Unable to resolve the last full doc in tlog fully, and document not found in index even after opening new rt searcher. id=1, partial document=SolrDocument{id=stored,indexed,omitNorms,indexOptions=DOCS<id:1>, val1_i_dvo=docValuesType=NUMERIC<val1_i_dvo:13>, val2_l_dvo=docValuesType=NUMERIC<val2_l_dvo:13000000039>, _version_=docValuesType=NUMERIC<_version_:1541124487571832832>}\n   [junit4]    >        at __randomizedtesting.SeedInfo.seed([CE02ABC183DF7342]:0)\n   [junit4]    >        at org.apache.solr.cloud.TestStressInPlaceUpdates$2.run(TestStressInPlaceUpdates.java:376)\n   [junit4]    > Caused by: org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://127.0.0.1:44464/muf/l/collection1: Unable to resolve the last full doc in tlog fully, and document not found in index even after opening new rt searcher. id=1, partial document=SolrDocument{id=stored,indexed,omitNorms,indexOptions=DOCS<id:1>, val1_i_dvo=docValuesType=NUMERIC<val1_i_dvo:13>, val2_l_dvo=docValuesType=NUMERIC<val2_l_dvo:13000000039>, _version_=docValuesType=NUMERIC<_version_:1541124487571832832>}\n   [junit4]    >        at org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:606)\n   [junit4]    >        at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:259)\n   [junit4]    >        at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:248)\n   [junit4]    >        at org.apache.solr.client.solrj.SolrRequest.process(SolrRequest.java:149)\n   [junit4]    >        at org.apache.solr.client.solrj.SolrClient.query(SolrClient.java:942)\n   [junit4]    >        at org.apache.solr.client.solrj.SolrClient.query(SolrClient.java:957)\n   [junit4]    >        at org.apache.solr.cloud.TestStressInPlaceUpdates$2.run(TestStressInPlaceUpdates.java:343)\n\n\n\t\t\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestStressInPlaceUpdates -Dtests.method=stressTest -Dtests.seed=D3059BEABF12E831 -Dtests.slow=true -Dtests.locale=nl-BE -Dtests.timezone=Asia/Thimbu -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] ERROR   33.1s | TestStressInPlaceUpdates.stressTest <<<\n   [junit4]    > Throwable #1: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=186, name=READER6, state=RUNNABLE, group=TGRP-TestStressInPlaceUpdates]\n   [junit4]    >        at __randomizedtesting.SeedInfo.seed([D3059BEABF12E831:B863444781C73CCB]:0)\n   [junit4]    > Caused by: java.lang.RuntimeException: org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://127.0.0.1:37544/ct_/px/collection1: Unable to resolve the last full doc in tlog fully, and document not found in index even after opening new rt searcher. id=0, partial document=SolrDocument{val2_l_dvo=docValuesType=NUMERIC<val2_l_dvo:5000000010>, id=stored,indexed,omitNorms,indexOptions=DOCS<id:0>, _version_=docValuesType=NUMERIC<_version_:1541142011874115584>}\n   [junit4]    >        at __randomizedtesting.SeedInfo.seed([D3059BEABF12E831]:0)\n   [junit4]    >        at org.apache.solr.cloud.TestStressInPlaceUpdates$2.run(TestStressInPlaceUpdates.java:376)\n   [junit4]    > Caused by: org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://127.0.0.1:37544/ct_/px/collection1: Unable to resolve the last full doc in tlog fully, and document not found in index even after opening new rt searcher. id=0, partial document=SolrDocument{val2_l_dvo=docValuesType=NUMERIC<val2_l_dvo:5000000010>, id=stored,indexed,omitNorms,indexOptions=DOCS<id:0>, _version_=docValuesType=NUMERIC<_version_:1541142011874115584>}\n   [junit4]    >        at org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:606)\n   [junit4]    >        at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:259)\n   [junit4]    >        at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:248)\n   [junit4]    >        at org.apache.solr.client.solrj.SolrRequest.process(SolrRequest.java:149)\n   [junit4]    >        at org.apache.solr.client.solrj.SolrClient.query(SolrClient.java:942)\n   [junit4]    >        at org.apache.solr.client.solrj.SolrClient.query(SolrClient.java:957)\n   [junit4]    >        at org.apache.solr.cloud.TestStressInPlaceUpdates$2.run(TestStressInPlaceUpdates.java:343)\n\n\n\t\n\t\n\n\n\n\n\nTestInPlaceUpdatesCopyFields\n\n\tWTF: TestInPlaceUpdatesCopyFields extends AbstractBadConfigTestBase FIXED: Had copied this over from SpatialRPTFieldTypeTest \n\n\n\n\n\tthis test seems to jumpt through hoops to use a mutable managed schema \u2013 but as far as i can tell it doesn't actaully test anything that requires the schema to change during the course of the test... Removed this test, added the tests here into TestInPlaceUpdatesStandalone\n\t\n\t\tit would be a lot simpler & easier to read if it just started up with a simple schema containing all of the copyFields needed\n\t\tunless you want to change the test so it does things like \"assert in-place update of foo_i_dvo works; add a copyField from foo_i_dvo to foo_stored; assert update of foo_i_dvo is no longer in place\"\n\t\n\t\n\n\n\n\n\tthe name of this class seems a bit too narrow\n\t\n\t\tit's not just a test of in-place updates using copy fields, it's a lot of unit tests of AtomicUpdateDocumentMerger.isInPlaceUpdate\n\t\tsuggest: TestAtomicUpdateDocMergerIsInPlace\n\t\tor just make it a test method in TestInPlaceUpdatesStandalone\n\t\t\n\t\t\tespecially if you simplify it to use pre-declared copyFields and don't need the mutable schema.\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tI found multiple seeds that fails 100% of the time on the same assert ... i haven't looked into why... FIXED: I hadn't reloaded the core. No longer applicable now.\n\t\n\t\t\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestInPlaceUpdatesCopyFields -Dtests.method=testIsInPlaceUpdate -Dtests.seed=54280A18530C3306 -Dtests.slow=true -Dtests.locale=en-ZA -Dtests.timezone=Europe/Tirane -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] FAILURE 0.10s J2 | TestInPlaceUpdatesCopyFields.testIsInPlaceUpdate <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError\n   [junit4]    >        at __randomizedtesting.SeedInfo.seed([54280A18530C3306:99BC22C9123C0682]:0)\n   [junit4]    >        at org.apache.solr.update.TestInPlaceUpdatesCopyFields.testIsInPlaceUpdate(TestInPlaceUpdatesCopyFields.java:118)\n   [junit4]    >        at java.lang.Thread.run(Thread.java:745)\n\n\n\t\t\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestInPlaceUpdatesCopyFields -Dtests.method=testIsInPlaceUpdate -Dtests.seed=A4B7A0F71938C5FE -Dtests.slow=true -Dtests.locale=en -Dtests.timezone=America/Tijuana -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] FAILURE 2.47s J2 | TestInPlaceUpdatesCopyFields.testIsInPlaceUpdate <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError\n   [junit4]    >        at __randomizedtesting.SeedInfo.seed([A4B7A0F71938C5FE:692388265808F07A]:0)\n   [junit4]    >        at org.apache.solr.update.TestInPlaceUpdatesCopyFields.testIsInPlaceUpdate(TestInPlaceUpdatesCopyFields.java:118)\n   [junit4]    >        at java.lang.Thread.run(Thread.java:745)\n\n\n\t\n\t\n\n\n\n\nTestInPlaceUpdatesDistrib\n\n\tagain A large number of comments i made the last time i reviewed this class still seem problematic, and i can't find any (jira) comments addressing them either...\n\n\n\tOnce LUCENE-7301 is fixed and we can demonstate that this passes reliably all of the time, we should ideally refactor this to subclass SolrCloudTestCase TODO: can be done later\n\n\n\n\n\tIn general, the \"pick a random client\" logic should be refactored so that sometimes it randomly picks a CloudSolrClient TODO\n\n\n\n\n\tensureRtgWorksWithPartialUpdatesTest\n\t\n\t\teven if we're only going to test one a few doc, we should ensure there are a random num docs in the index (some before the doc we're editing, and some after) FIXED\n\t\t2 docs before/after is not a random number ... random means random: we need to test edge cases of first docid in index, last docid in index, first/last docid in segment, etc...\n\t\n\t\n\n\n\n\n\toutOfOrderUpdatesIndividualReplicaTest\n\t\n\t\tditto comments about only one doc FIXED\n\t\tif we are going to use an ExecutorService, then the result of awaitTermination has to be checked FIXED\n\t\t... and shutdown & awaitTermination have to be called in that order\n\t\tsince this tests puts replicas out of sync, a ... \"wait for recovers\" should happen at the end of this test (or just in between every test) .. especially if we refactor it (or to protect someone in the future who might refactor it)\n\t\n\t\n\n\n\n\n\toutOfOrderUpdatesIndividualReplicaTest  (followup comments)\n\t\n\t\tlots more comments in the test code to make it clear that we use multiple threads because each update may block if it depends on another update Added comment: \"Reordering needs to happen using parallel threads, since some of these updates will be blocking calls, waiting for some previous updates to arrive on which it depends.\"\n\t\tcreate atLeast(3) updates instead of just a fixed set of \"3\" so we increase our odds of finding potential bugs when more then one update is out of order. FIXED\n\t\tloop over multiple (random) permutations of orderings of the updates\n\t\t\n\t\t\tdon't worry about wether a given ordering is actually correct, that's a valid random ordering for the purposes of the test\n\t\t\ta simple comment saying we know it's possible but it doesn't affect any assumptions/assertions in the test is fine\n\t\t\n\t\t\n\t\tfor each random permutation, execute it (and check the results) multiple times TODO: Every test run will have a random permutation of reordering. Haven't added a loop to try multiple of these permutations in the same test; will add later.\n\t\t\n\t\t\tthis will help increase the odds that the thread scheduling actaully winds up running our updates in the order we were hoping for.\n\t\t\n\t\t\n\t\tessentially this should be a a micro \"stress test\" of updates in arbitrary order. Something like... \n\nfinal String ID = \"0\";\nfinal int numUpdates = atLeast(3);\nfinal int numPermutationTotest = atLeast(5);\nfor (int p = 0; p < numPermutationTotest; p++) {\n  del(\"*:*);\n  commit();\n  index(\"id\",ID, ...); // goes to all replicas\n  commit();\n  long version = assertExpectedValuesViaRTG(LEADER, ID, ...);\n  List<UpdateRequest> updates = makeListOfSequentialSimulatedUpdates(ID, version, numUpdates);\n  for (UpdateRequest req : updates) {\n    assertEquals(0, REPLICA_1.requets(req).getStatus());\n  }\n  Collections.shuffle(updates, random());\n  // this method is where you'd comment the hell out of why we use threads for this,\n  // and can be re-used in the other place where a threadpool is used...\n  assertSendUpdatesInThreadsWithDelay(REPLICA_0, updates, 100ms);\n  for (SolrClient client : NONLEADERS) [\n    // assert value on replica matches original value + numUpdates\n  }\n}\n\n\n\t\tAs a related matter \u2013 if we are expecting a replica to \"block & eventually time out\" when it sees an out of order update, then there should be a white box test asserting the expected failure situation as well \u2013 something like... \n\nfinal String ID = \"0\";\ndel(\"*:*);\ncommit();\nindex(\"id\",ID, ...);\nUpdateRequest req = simulatedUpdateRequest(version + 1, ID, ...);\nTimer timer = new Timer();\ntimer.start();\nSolrServerException e = expectThrows(() -> { REPLICA_0.request(req); });\ntimer.stop();\nassert( /* elapsed time of timer is at least the X that we expect it to block for */ )\nassert(e.getgetHttpStatusMesg().contains(\"something we expect it to say if the update was out of order\"))\nassertEquls(/* whatever we expect in this case */, e.getHttpStatusCode());\n\n\n\t\n\t\n\n\n\nTODO: can be done later\n\n\n\tdelayedReorderingFetchesMissingUpdateFromLeaderTest\n\t\n\t\tIs there no way we can programatically tell if LIR has kicked in? ... pehaps by setting a ZK watch? ... this \"Thread.sleep(500);\" is no garuntee and seens arbitrary.\n\t\t\n\t\t\tat a minimum polling in a loop for the expected results seems better then just a hardcoded sleep FIXED: added a polling loop. Can try a LIR flag check later\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\n\n\n\n\n\ttest()\n\t\n\t\t// assert that schema has autocommit disabled Fixed\n\t\t\n\t\t\tthis doesn't assert autocommit is disabled, all it does is assert that a sys property was set in beforeSuperClass\n\t\t\tnothing about this actually asserts that the configs/defaults in use don't have autocommit \u2013 use the Config API to be certain\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tdocValuesUpdateTest\n\t\n\t\tthe lower limit of numDocs=1 seems absurd ... why not atLeast(100) or something? Fixed\n\t\tplease don't use (for) loops w/o braces. Fixed\n\t\tI don't understand anything about the use of luceneDocids in this method...\n\t\t\n\t\t\tcorrectness...\n\t\t\t\n\t\t\t\tfor starters, matchResults seems completely broken \u2013 but i'll get to that later.  let's assume for now that it works...\n\t\t\t\tthe initial list comes from a randomly choosen client, and then later it's compared to the list from another randomly choosen client \u2013 how is this comparison safe? Since documents were added in the same order, and there's no reordering from leader to replica (since indexing is happening through single thread), the docids on every replica would be same for the same documents.\n\t\t\t\tIf there is any sort of hicup during indexing that requires the leader to retry sending a doc, then the docids won't match up.\n\t\t\t\tit seems like these checkers are really just relying on the fact that if there is any discrepency between the replicas, we'll retry enough times that eventually we'll get lucky and query the first replica again.Good point. Fixed to choose different clients up front, instead of choosing a new client at random every time.\n\t\t\t\n\t\t\t\n\t\t\teven if everything in the code as written is 100% fine:\n\t\t\t\n\t\t\t\tthe iniial list of luceneDocids is populated by a query that doesn't do any retry logic and will fail fast if the numDocs doesn't match the result.size() Fixed, querying the LEADER to obtain the luceneids initially\n\t\t\t\tthis does nothing to address the problem noted in the older version of the test: waiting for the commit to propogate and open new searchers on all shards: Thread.sleep(500); // wait for the commit to be distributed There is a wait loop for commit to propagate\n\t\t\t\n\t\t\t\n\t\t\tIn general, this seems like a convoluted way to try and kill two birds with one stone: 1) make sure all replicas have opened searchers with the new docs; 2) give us something to compare to later to ensure the update was truely in place\n\t\t\t\n\t\t\t\ti really think the first problem should be addressed the way i suggested previously:Done\n\n\tif initially no docs have a rating value, then make the (first) test query be for rating:[* TO *] and execute it in a rety loop until the numFound matches numDocs.Done\n\tlikewise if we ensure all ratings have a value such that abs(ratings) < X, then the second update can use an increment such that abs(inc) > X*3 and we can use -ratings:[-X TO X] as the query in a retry loop Done\n\n\n\t\t\t\tthe second problem should be solved by either using the segments API, or by checking the docids on every replica (w/o any retries) ... after independently verifying the searcher has been re-opened.\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tmatchResults\n\t\n\t\tno javadocs Fixed\n\t\t\n\t\t\tI'm assuming the point is to return true if the (same ordered) luceneDocids & ratings match the results\n\t\t\n\t\t\n\t\n\t\n\treturns true if the number of results doesn't match the number of luceneDocids Fixed, returns false if mismatch in number of docs\n\t\n\t\tseems broken: if a shard hasn't re-opened a searcher yet (ie: 0==results.size()) ... implies results do match when they are grossly difference.\n\t\n\t\n\tint l = ... ... why is this variable named \"l\" ??? Renamed to docid. Was named as l for luceneid\n\n\n\n\n\tensureRtgWorksWithPartialUpdatesTest\n\t\n\t\tinstead of log.info(\"FIRST: \" + sdoc); and log.info(\"SECOND: \" + sdoc); just put the sdoc.toString() in the assert messages Fixed...\n\nassertEquals(\"RTG tlog price: \" + sdoc, (int) 100, sdoc.get(\"price\"));\n\n\n\t\tnothing in this test asserts that the update is actually in place\n\t\t\n\t\t\tchecking fl=[docid] in cloud RTG is currently broken (SOLR-9289) but we can/should be checking via the segments API anyway (if we have a general helper method for comparing the segments API responses of multiple replicas betwen multiple calls, it could be re-used in every test in this class)Done by opening a new RT searcher and getting the docid for it\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\toutOfOrderUpdatesIndividualReplicaTest\n\t\n\t\tlong seed = random().nextLong(); // seed for randomization within the threads\n\t\t\n\t\t\teach AsyncUpdateWithRandomCommit task needs it's own seed value, otherwise they'll all make the exact same choices.\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\toutOfOrderDeleteUpdatesIndividualReplicaTest\n\t\n\t\tnew test, most of the comments i had about outOfOrderUpdatesIndividualReplicaTest (both the new comments, and the older comments that i don't see any updates/replies regarding) also apply here. Haven't randomized the test with respect to number of updates, since there are a few deletes and adds mixed.\n\t\n\t\n\n\n\n\n\tdelayedReorderingFetchesMissingUpdateFromLeaderTest\n\t\n\t\tthis is sketchy brittle \u2013 just create a List<SolrClient> ALL_CLIENTS when creating LEADER and NONLEADERS...\n\nfor (SolrClient client: new SolrClient\\[\\] \\{LEADER, NONLEADERS.get(0),\n    NONLEADERS.get(1)}) { // nonleader 0 re-ordered replica, nonleader 1 well-ordered replica\n\n\n\t\n\t\n\n\n\n\n\tsimulatedUpdateRequest\n\t\n\t\ti have not looked into where the compiler is finding the id variable used in this method, but it's definitely not coming fro mthe method args, or the doc \u2013 so it's probably broken.\nFixed\n\t\twhatever the fix is for the baseUrl init code, please refactor it into a static helper method so we don't have these 4 lines duplicated here and in simulatedDeleteRequest\nFixed\n\t\n\t\n\n\n\n\n\taddDocAndGetVersion\n\t\n\t\tsynchronized (cloudClient)\n\t\t\n\t\t\twhy are we synchronized on cloud client but updating via LEADER?\n\t\t\twhy are we synchronized at all?\nFixed\n\t\t\n\t\t\n\t\n\t\n\n\n\nJettySolrRunner\n\n\tuse MethodHandles for static logger init\nDone\n\n\n\n\n\tif there are multiple delays whose counters have \"triggered\", why only execute a single delay of the \"max\" time? ... shouldn't it be the sum?\nDone\n\n\n\n\n\tnow that an individual \"Delay\" can be \"delayed\" (ie: there's a count involved and the delay may not happen for a while) let's add a String reason param to addDelay and the Delay class and log that message as we loop over the delay objects\nDone\n\n\n\n\nGeneral Questions / Concerns\n\n\tprecommit was failing for me due to javadoc warnings\n\n\n\n\n\tLUCENE-7344\n\t\n\t\tprevious comment from Ishan...\nAs I was incorporating Hoss' suggestions, I wrote a test for DV updates with DBQ on updated values. This was failing if there was no commit between the update and the DBQ. I think this is due to LUCENE-7344.\n\t\tI was expecting to find this test code somewhere, but i did not\nAdded a test for this: TestInPlaceUpdatesStandalone.testDVUpdatesWithDBQofUpdatedValue()\n\t\tWe still need some sort of solution to this problem \u2013 the suggested requirement/documentation workarround suggested by McCandless in that issue doesn't really fit with how all work on this jira to date has kept the  the decision of when/if to do an \"in-place\" update a low level implementation detail .... that would have to radically change if we wanted to \"pass the buck\" up to the user to say \"you an't use DBQ on a docvalue field that you also use for in place updates\"\n\t\tso what's our Solr solution / workaround?  do we have any?\nOur workaround is to reopen an RT searcher during every DBQ (in the DUH2), as per Yonik's suggestion.\n\t\n\t\n\n\n\n\n\n\tBlock join docs?\nAs of now, atomic updates doesn't work with block-join documents and the behaviour is undefined. So, even with this, block join documents aren't updated.\n\t\n\t\tI never really considered this before, but it jumped out at me when reviewing some of the code\n\t\tWhat happens today (w/o patch) if someone has block join docs and does an an atomic update that updates both parent and child?\n\t\t\n\t\t\tI'm assuming that currently \"works\" (ie: store fields of both docs are read, update applied to both, and then written back as a new block)\n\t\t\n\t\t\n\t\tWhat happens w/this patch in the same situation?\n\t\t\n\t\t\twhat if in both docs, the field being updated supports in-place updates? ... does it work and update the both docs in place?\n\t\t\twhat if only othe parent doc's update involves an in-place updatable field, but the child doc update is on a field that is stored/indexed?  .... does the \"isInPlaceUpdate\" style logic kick in correctly for all the docs in the hierarchy? (so that the entire block is updated as a \"regular\" atomic update?)\n\t\t\n\t\t\n\t\n\t\n\n\n[Edit: updated with replies to comments, 16th November] "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-15582028",
            "date": "2016-10-17T11:59:33+0000",
            "content": "Deprecate ExternalFileField when this is committed? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-15582054",
            "date": "2016-10-17T12:09:08+0000",
            "content": "It doesn't seem to buy us much to deprecate that, and it would impose the burden on users to changing in order to upgrade (making it less likely that a user of EFF would upgrade). "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15617617",
            "date": "2016-10-29T07:13:28+0000",
            "content": "Just discovered another, more common, problem with reordered DBQs and in-place updates working together. The earlier discussed problem, of resurrecting a document, is very similar. So, here's a description of both:\n\nSCENARIO 1:\n\nImagine, updates on leader are:\nADD\t(id=1, updateable_field=1, title=\"mydoc1\", version=100)\nINP-UPD\t(id=1, updateable_field=2, version=200, prevVersion=100)\nDBQ\t(q=\"updateable_field:1\", version=300)\n\nThe same on the replica (forwarded):\nADD\t(id=1, updateable_field=1, title=\"mydoc1\", version=100)\nDBQ\t(q=\"updateable_field:1\", version=300)\nINP-UPD\t(id=1, updateable_field=2, version=200, prevVersion=100)\n\nThe expected net effect is that no document is deleted, and id=1 document exists with updateable_field=2.\nHere, the DBQ was reordered. When they are executed on the replica, the version=200 update cannot be applied since there is no document with (id=1,prevVersion=100). What is required is a resurrection of the document that was deleted by the DBQ, so that other stored/indexed fields are not lost.\n\n\n\nSCENARIO 2:\n\nImagine, updates on leader are:\nADD\t(id=1, updateable_field=1, title=\"mydoc1\", version=100)\nINP-UPD\t(id=1, updateable_field=2, version=200, prevVersion=100)\nDBQ\t(q=\"id:1\", version=300)\n\nThe same on the replica (forwarded):\nADD\t(id=1, updateable_field=1, title=\"mydoc1\", version=100)\nDBQ\t(q=\"id:1\", version=300)\nINP-UPD\t(id=1, updateable_field=2, version=200, prevVersion=100)\n\nThe expected net effect is that the document with id=1 be deleted. But again, the DBQ is reordered. When executed on replica, update version=200 cannot be applied, since the id=1 document has been deleted. What is required is for this update (version=200) to be dropped silently.\n\n\n\nScenario 1 is rare, scenario 2 would be more common. At the point when the inplace update (version=200 in both cases) is applied, the replica has no way to know if the update requires a resurrection of the document, or requires to be dropped.\n\nTill now, I hadn't considered scenario 2, but for the rare scenario 1, I resorted to throwing an error so as to throw the replica in LIR. Clearly, in view of scenario 2, this looks like a bad idea. Here are two potential solutions that come to mind:\nSolution 1:\n\nIn a replica, while applying an in-place update, if the required prevVersion update cannot be found in tlog or index (due to these reordered DBQs), then fetch from the leader an update that contains the full document with the version (for which the update failed at replica). If it has been deleted on leader, just drop it on replica silently. Downside to this approach is that unstored/non-dv fields will get dropped (as is the case with regular atomic updates today).\n\n\nSolution 2:\n\nEnsure that DBQs are never reordered from leader -> replica. One approach can be SOLR-8148. Another could be to block, on the leader, all updates newer than a DBQ until the DBQ is processed on leader and all the replicas, and only then process the other updates. Also, block the DBQ and execute it only after all updates older than the DBQ have been processed on leader and all the replicas.\n\n\nSolution 1 seems easier to implement now than solution 2, but solution 2 (if implemented correctly) seems cleaner. Any thoughts?\n\nEdit: There's a third solution in the interim:\n\nHave a field definition flag, inplace-updateable=true, or a similar schema level property, to enable or disable this feature (of updating docValues). This feature can be turned off by default (and this default can be revisited in a later major release). But someone can turn it on, if he/she agrees to (a) ensure they don't issue DBQs on updated documents or, even if they do that, (b) they make sure their DBQs are not reordered.\n\n\nNot an ideal solution, but this could be in the spirit of \"progress, not perfection\". "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15654435",
            "date": "2016-11-10T16:15:39+0000",
            "content": "New patch.\n\n\tImplements the Solution 1 approach from my above comment on the DBQs issue (with a slight difference: instead of pulling the document with the given version from the leader, I am pulling the latest document that there is on the leader).\n\tFixing many TODO review comments from Hoss.\n\tTestStressInPlaceUpdates, TestInPlaceUpdatesDistrib and TestInPlaceUpdatesStandalone all seem to be passing regularly.\n\tFixed for precommit failures.\n\n\n\nHoss Man, can you please review? "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15664194",
            "date": "2016-11-14T15:27:34+0000",
            "content": "Some minor changes:\n1. Split the UpdateLogTest to smaller tests, more readable.\n2. Added a javadoc comment to DUP.\n3. Combined the testReplay1, 2, 3 into a single test. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15664306",
            "date": "2016-11-14T16:15:56+0000",
            "content": "Replied to review comments inline here:\nhttps://issues.apache.org/jira/browse/SOLR-5944?focusedCommentId=15547986&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15547986 "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15667874",
            "date": "2016-11-15T18:24:57+0000",
            "content": "Added another patch. The PeerSyncTest was failing, due to fingerprint caching issue. This patch now depends on SOLR-9506's \"SOLR-9506-combined-deletion-key.patch\" SOLR-9777 patch. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15670453",
            "date": "2016-11-16T13:49:12+0000",
            "content": "Added another test, a specific unit test for AUDM's doInPlaceUpdateMerge(). "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15713257",
            "date": "2016-12-01T22:20:59+0000",
            "content": "\nOk \u2013 i've been reviewing Ishan's latest patch.\n\nIn a conversation I had with him offline, he mentioned that he hadn't been doing any further work on this locally since his last update, so w/o any risk of collision I went ahead and made a bunch of small updates directly...\n\nSmall changes I made directly in latest patch\n\n\tmisc javadoc tweaks & improvements\n\n\n\n\n\tsome logging cleanup & template improvements\n\t\n\t\tAnytime we log anything that includes a complex object (that we expect to be toString()-ified) is a situation where we should definitely be using templated log messages to avoid potentially expensive calls to toString() in the case that the log level results in the method being a No-Op.\n\t\tI probably missed some, but i tried to catch as many as i could\n\t\n\t\n\n\n\n\n\tsome simple method renames:\n\t\n\t\tUpdateLog.addUpdateCommandFromTlog -> convertTlogEntryToAddUpdateCommand\n\t\t\n\t\t\tolder name was ambigiuous because of dual meaning of \"add\", now verb is clear.\n\t\t\n\t\t\n\t\tAtomicUpdateDocumentMerger.getSearcherNonStoredDVs -> getNonStoredDocValueFieldNamesFromSearcher\n\t\n\t\n\n\n\n\n\trefactored some duplicate code into new helper methods:\n\t\n\t\tDirectUpdateHandler2.updateDocOrDocValues\n\t\n\t\n\n\n\n\n\trefactor some loops where methods were being called unneccessarily in every iteration\n\t\n\t\tDocumentBuilder.toDocument\n\t\tAtomicUpdateDocumentMerger.isInPlaceUpdate (IndexSchema.getUniqueKeyField and IndexWriter.getFieldNames())\n\t\n\t\n\n\n\n\n\tadded some comments containing important details/reasoning that had only been captured in jira comments so far\n\t\n\t\tAtomicUpdateDocumentMerger.doInPlaceUpdateMerge\n\t\n\t\n\n\n\n\n\tincreased usage of expectThrows, beefed up asserts related to expected Exceptions\n\n\n\n\n\tadded some nocommit comments regarding some questionable lines of code (either new questions, or things I've provided feedback on before that keep slipping through the cracks)\n\n\n\n\n\tupdated to master/98f75723f3bc6a718f1a7b47a50b820c4fb408f6\n\n\n\n\nMore in-depth/concrete responses/comments to the previous patch are listed below \u2013 anything mentioned below that still seems problematic to me should also have nocommit comments in the code associated with them, but i wanted to elaborate a bit here in the jira comments.\n\n(The reverse is not neccessarily true however: I added nocommits for some small things that I didn't bother to add jira comments aboute)\n\n\nschema.xml\nFWIW: I was initially concerned that change all these existing dynamicFields from indexed=true to indexed=false might be weakening other tests, but I did a quick grep for their usage in other test methods (prior to this jira) and the only impacts i found were in AtomicUpdatesTest, where the only concern was docValues=true and stored=false.  So changing these to indexed=false doesn't impact that test at all (they are never queried against)\n\nBased on the \"dvo\" naming, (which i think a reasonable person would read as \"doc values only\") making these fields indexed=false seems like a good idea\n\n\n\nDistributedUpdateProcessor\n\n\tIn general, i think there's a gap in the DUP logic related to how the results of fetchFullUpdateFromLeader are used when replicas never get dependent updates\n\t\n\t\tsee detailed nocommit comments in the body of fetchFullUpdateFromLeader for details\n\t\t\n\t\t\tthey apply to both call stacks where the method is used, but i wnated ot write it once in one place\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tversionAdd\n\t\n\t\tif (fetchedFromLeader == null)\n\t\t\n\t\t\tI'm a little concerned/confused by the \"else\" block of this condition\n\t\t\t\n\t\t\t\tgrep code for \"nocommit: OUTER IF\", \"nocommit: OUTER ELSE\" & \"nocommit: INNER ELSE\"\n\t\t\t\n\t\t\t\n\t\t\tat this point, we've replaced our \"in-place\" update with a regular update using the full SolrInputDocument from the leader (by overwritting cmd.solrDoc, cmd.prevVersion, cmd.setVersion(...))\n\t\t\tbut this if/else is itself wrapped in a bigger if (cmd.isInPlaceUpdate()) whose else clause does some sanity checking / processing logic for \"// non inplace update, i.e. full document update\"\n\t\t\tshouldn't the logic in that outer else clause also be applied to our \"no longer an in-place\" update?\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tfetchFullUpdateFromLeader\n\t\n\t\tthis method still calls forceUpdateCollection ... note shalin's comments back in August...\nUnder no circumstances should we we calling `forceUpdateCollection` in the indexing code path. It is just too dangerous in the face of high indexing rates. Instead we should do what the DUP is already doing i.e. calling getLeaderRetry to figure out the current leader. If the current replica is partitioned from the leader then we have other mechanisms for taking care of it and the replica has no business trying to determine this.\n\t\n\t\n\n\n\n\nDirectUpdateHandler2\n\n\tI previously asked about LUCENE-7344...\n\n\tso what's our Solr solution / workaround?  do we have any?\nOur workaround is to reopen an RT searcher during every DBQ (in the DUH2), as per Yonik's suggestion.\n\n\n\t\n\t\tI gather, reading this patch and knowing the context of this issue, that the DUH2 line i added a \"nocommit: LUCENE-7344\" to is the line in question?\n\t\tin general, any code that exists purely to work around some other bug should have a have a big, bold, loud, comment explaining the purpose for it's existence and a link to the known bug\n\t\tspecifically: the fact that grep LUCENE-7344 SOLR-5944.patch didn't match ANY lines in the last patch was a big red flag to me\n\t\t\n\t\t\tthe code should have a comment making it clear where/how/why/what the work around is\n\t\t\tthe test(s) written to ensure the workaround works should also have comments refering to the issue at hand (offhand, i personally couldn't identify them)\n\t\t\n\t\t\n\t\n\t\n\n\n\n\nDocumentBuilder\n\n\taddField\n\t\n\t\tprevious feedback/response...\n\n\tif true==forInPlaceUpdate and createFields(...) returns anything that is not a NumericDocValuesField (or returns more then one) shouldn't that trip an assert or something? (ie: doesn't that mean this SchemaField isn't valid for using with an in place update, and the code shouldn't have gotten this far?)\n\t\n\t\tThis is fine, since createFields() generates both NDV and non NDV fields for an indexable field, and the intent is to drop the non NDV one. Added a comment to this effect\n\t\n\t\n\n\n\t\tI couldn't make sense of this until i realized what you're describing is evidently a bug in TrieField: SOLR-9809\n\t\tI've updated addField with a comment to make it clear we're working around that bug\n\t\tunless i'm missunderstanding something, fixing that bug could theoretically allow us to throw away most of the new DocumentBuilder logic in this patch (that's conditional on forInPlaceUpdate)\n\t\t\n\t\t\tnotable exception: optimizing away the default field construction\n\t\t\talso: leaving the new code in will be handy for the the purposes of asserting that we only have the expected NumericDocValuesField instances\n\t\t\n\t\t\n\t\n\t\n\n\n\n\nUpdateLog\n\n\tapplyPartialUpdates\n\t\n\t\tin reply to some previous questions/feedback...\n...It is my understanding is that due to the incref/decref being inside the synchronized(ulog) block, no other thread's action would affect whatever happens to these tlog once we've entered the block....\n\t\t\n\t\t\tthe problem in previous patches was that the incref/decref were NOT inside the sync block \u2013 which is why i asked the questions.  In this patch you moved the getEntryFromTLog call inside the same sync block where the List<TransactionLog> is created, so now as getEntryFromTLog loops over the TransactionLog, we're safe \u2013 but that wasn't happening before, hence the question.\n\t\t\t\n\t\t\t\tI think the way the code is writen right now is safe/valid, but i'd feel better about it if we could get some more eyeballs to review the sync usage.\n\t\t\t\n\t\t\t\n\t\t\tEven if it is currently correct/valid, I still think it would be good to make the getEntryFromTLog method itself declared as synchronized \u2013 it should have no impact on performance/correctness since the only existing usage is the synchronized (this) block, but it will protect us against future dev mistakes since hte only valid usage of that method is when you have a lock on this (otherwise some other thread might be decrefing/closing the TransactionLog instances used in that method.\n\t\t\n\t\t\n\t\tsome (different) feedback about this method from before...\n\"in general i'm wondering if lookupLogs should be created outside of the while loop, so that there is a consistent set of \"logs\" for the duration of the method call ... what happens right now if some other thread changes tlog/prevMapLog/prevMapLog2 in between iterations of the while loop?\" <-- I added them inside the while block so that if there have been multiple commits during the course of iterating the while loop, those tlogs wouldn't have grown stale. Do you think we should pull it out outside the while loop?\n\t\t\n\t\t\tThe concern i have is that AFAICT rebuilding the Arrays.asList(tlog, prevMapLog, prevMapLog2) in every iteration of the while loop seems to be at cross purposes with the reason why the while loop might have more then one iteration.\n\t\t\tas you said: if commits happen in other threads while the code in the loop is evaluating an entry to apply partial updates, then the next iteration of the loop (if any) will get the updated tlog with new data\n\t\t\t\n\t\t\t\tbut the reason there might be a \"next\" iteration of the loop is if the prevPointer of the our current IN_PLACE update identified an early (ancestor) IN_PLACE update \u2013 having newer commits on the \"next\" iteration of the loop doesn't help us. (does it?)\n\t\t\t\n\t\t\t\n\t\t\tin other words: the purpose of the while loop is to be able to iterate backwards through time in the tlogs when many IN_PLACE updates are applied on top of eachother, while the purpose you listed for defining lookupLogs inside the loop is to support moving forward in time as new tlogs are created \u2013 which actually decreases the likelihood that we'll find some ancestor entry we have a cascading dependency on.\n\t\t\tdoes that make sense?\n\t\t\t\n\t\t\t\tAm i missunderstanding/missing some possible benefit of having the \"newer\" tlogs available as the while loop iterates back in time over older and older prevPointers?\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\tassuming i'm correct about both of the above points...\n\t\t\n\t\t\tList<TransactionLog> lookupLogs should be created once, outside of any looping, and never redefined to ensure that even if subsequent commits occur in concurrent threads, we can still trace the chain of dependent updates back as far as possible using the oldest known tlogs available to us when the method was called.\n\t\t\tList<TransactionLog> lookupLogs and any calls to getEntryFromTLog that use that List MUST happen in the same sync block.\n\t\t\tergo the sync block must wrap the while loop ... ie: we might as well declare the entire method syncrhonized.\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\n\n\tisInPlaceUpdate\n\t\n\t\tregarding some previous feedback...\ncouldn't this list of fields be created by the caller and re-used at least for the entire request (ie: when adding multiple docs) ? The set returned is precomputed upon the opening of a searcher. The only cost I see is to create a new unmodifiableSet every time. I'd prefer to take up this optimization later, if needed.\n\t\t\n\t\t\tthe method you're talking about isn't even used in this patch \u2013 as you noted in the comment you made immediately after that one, you changed it to use IndexWriter.getFieldNames() instead.\n\t\t\tIndexWriter.getFieldNames() delegates to a syncronized method, which means there is definitely overhead in calling it over and over for every field name in every document in the request.\n\t\t\teven if you don't think it's worth refactoring this method so that IndexWriter.getFieldNames() only needs to be called once per SolrQueryRequest, it seems irresponsible not to at least call it once in this method (ie: once per doc) before looping over all the fields ... so i made that change as well.\n\t\t\n\t\t\n\t\tregarding some other previous feedback...\n\n\tif (indexFields.contains(fieldName) == false && schema.isDynamicField(fieldName))\n\t\n\t\twhy does it matter one way or the other if it's a dynamicField? Changed the logic to check in the IW for presence of field. Added a comment: \"// if dynamic field and this field doesn't exist, DV update can't work\"\n\t\n\t\n\n\n\t\t\n\t\t\tthat doesn't really answer my question at all: why does it matter if it's a dynamic field?\n\t\t\tDoesn't the same problematic situation happen if it's an explicitly defined <field .. /> in the schema.xml, but just doesn't happen to exist yet in any docs? \u2013 ie: if the very first update to the index is:\n\n[{\"id\":\"HOSS\",\"price\":{\"set\":42.10}}]\n\n\n???\n\t\t\teven if i'm wrong: consider this code (and the comment you added) from the point of view of someone completely unfamiliar with this issue.  if they come along, and see this code jumping through some hoops related to dynamicFields, and the only comment is (essential) \"doesn't work for dynamic fields\" that comment isn't going to help them make any sense of the code \u2013 it would be like if some code had a comment that said // set the value of the variable will_not_work to true, it doesn't explain why that line of code exists.\n\t\t\t\n\t\t\t\tI've been working on this issue with you for months, I'm intimately familiar with this patch, but reading this code now I can't recall any particular reason why \"DV update can't work\" specifically in the case of \"dynamic field and this field doesn't exist\"\n\t\t\t\ti thought the only known problem like this was just \"this field doesn't yet exist in the index\" in general? (and that we had a check for that problem at a higher level then this method)\n\t\t\t\n\t\t\t\n\t\t\tif i am wrong, and there is an important distinction between dynamicFields and \"static\" fields when dealing with the situation of in-place updates being attempted against an index that doesn't yet contain them, then that makes me very concerned that most of the existing testing depends solely on dynamicFields (or in some cases solely on static fields)\n\t\t\t\n\t\t\t\tan easy solution would be to replace schema-inplace-updates.xml with 2 variants: one that only has the <dynamicField/> declarations needed to match all the field names used in the tests, and one that has an explicit <field /> declaration for every field name used in the tests.  and then all the tests could randomize between schema-inplace-updates-dynamicfields.xml and schema-inplace-updates-staticfields.xml in their @BeforeClass\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\texplicitly throwing new RuntimeException is a big red flag.\n\t\t\n\t\t\tif we're going to catch IOException and wrap it in a RuntimeException we should at least use SolrException instead and use a message that provides context\n\t\t\tsince the only caller of this method (DUP.getUpdatedDocument) already declares that it throws IOException, why not just let this method propogate the IOException as is?\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tdoInPlaceUpdateMerge\n\t\n\t\ta previous question...\nwhy are we copying all supported DVs over? If we update dv1 in one update, and then update dv2, and again update dv1 (without commits in between), the last update would fetch from the tlog the partial doc for dv2 update. If that doc doesn't copy over the previous updates to dv1, then a full resolution (by following previous pointers) would need to be done to calculate the dv1 value. Hence, I decided to copy over older DV updates.\n\t\t\n\t\t\tthat makes perfect sense \u2013 that explanation should be part of the code comment so a future dev doesn't have the same mistaken impression i did that it was copying DVs that aren't needed \u2013 so i added it to the code as a new java comment.\n\t\t\n\t\t\n\t\n\t\n\n\n\nschema-inplace-updates.xml\n\n\tin general there was a lot of unneccessary stuff in this schema copy/pasted from schema-minimal-atomic-stress.xml\n\t\n\t\teven some comments that refered to test classes that were using schema-minimal-atomic-stress.xml and had nothing to do with this config\n\t\n\t\n\tmy previous suggestion was \"add a new, small, purpose built & well commented schema-inplace-atomic-updates.xml file just for the new InPlace tests \u2013 containing only the fields/dynamicFields needed\" ... it's not \"small and purpose build\" if it's got useless stuff copy/pasted from other files.\n\t\n\t\tleaving unused fields/types here defeats the entire point of my suggestion: \"having smaller single purpose test configs makes it a lot easier for people reviewing tests to understand what the minimum expectations are for the test to function\"\n\t\ti went ahead and cleaned up everything that wasn't needed for the new tests, and added a few comments as appropriate to make it clear to future readers why some stuff is the way it is.\n\t\n\t\n\n\n\n\n\tregarding the copyField testing & schema comment you had...\n\t\n\t\ti had to re-read the schema and the test involved at least 10 times before i could make sense of it\n\t\tthe crux of my confusion was that in the second copyField:\n\t\t\n\t\t\tthe src field named copyfield_not_updateable_src did support in place updates (in spite of it's name)\n\t\t\tthe dest field named copyfield_updatable_target_i did not support in place updates (in spite of it's name)\n\t\t\n\t\t\n\t\ti renamed the fields used by the test to try and be more clear what was going on\n\t\n\t\n\n\n\nTestRecovery\n\n\tprevious feedback...\n\n\twhy is // assert that in-place update is retained at teh end of the test, instead of when the (first) log replay happens and *:* -> numFound=3 is asserted? (ie: before \"A2\" is added) FIXED\n\twhat about mixing in-place updates with deletes (both by id and query?) FIXED\n\n\n\tneither of those responses made any sense to me.\n\t\n\t\tRE: // assert that in-place update is retained\n\t\t\n\t\t\tthe latest patch included an additional copy of the same assert \u2013 but it was being checked after several other updates\n\t\t\tI couldn't see any reason why that same assert (that the in-place update is retained) shouldn't happen as soon as recovery is finished, before doc \"A2\" was added?\n\t\t\t\n\t\t\t\tSo I added this \u2013 and everything seems to be working fine.\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\tRE: mixing in-place updates with deletes\n\t\t\n\t\t\ti don't understand how this is \"FIXED\" ... there's been no additions to this test since the last patch that relate to deletes at all ???\n\t\t\tas far as i can tell, there is still no tests actually verifying that log replay will do the correct when deletes are intermixed with in-place updates, ie...\n\t\t\t\n\t\t\t\tDBQs against a field that had in-place updates\n\t\t\t\ta delete by id of a document inbetween two \"atomic updates\" of that document (to ensure the replay recognizes that the second update can't be done in-place)\n\t\t\t\tetc...\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\n\t\n\n\n\nUpdateLogTest\n\n\tlifecycle of requests\n\t\n\t\tin my last feedback, i mentioned that there are a lot of LocalSolrQueryRequest that were created and never closed, and said...\nthe convinience methods that build up UpdateCommands (ulogCommit & getDeleteUpdate & getAddUpdate) sould be refactored to take in a SolrQueryRequest which should be closed when the logical bit of code related to testing that UpdateCommand is finished. something like... \n\ntry (SolrQueryRequest req = req()) {\n  ulog.add(getAddUpdate(req, ...));\n  // TODO: anything else that needs to be part of this same request for test purposes?\n}\n// TODO ... assert stuff about ulog\n...\n\n\n\t\tin your updated patch, the SolrQueryRequest's are getting auto-closed \u2013 but that's happening in the static helper methods (ie: getAddUpdate, getDeleteUpdate) that then return an UpdateCommand which still refrences & uses the SolrRequest.  So the lifecycle of the request is still violated, just in a diff way\n\t\t\n\t\t\tnow instead of not closing them, we're closing them before we're done using them\n\t\t\n\t\t\n\t\tany logic that's being tested that would normally be pat of the request \u2013 like adding an UpdateCommand to the ulog \u2013 needs to happen before the SolrQueryRequest is closed, or the test isn't valid \u2013 it could be hiding bugs that exist in the normal lifecycle (ie: related to re-opened searchers, etc...)\n\t\tbased on the current code, the easiest fix maybe...\n\t\t\n\t\t\trename getAddUpdate, getDeleteUpdate --> applyAddUpdate, applyDeleteUpdate\n\t\t\tmake both methods take in an UpdateLog param and call the appropirate ulog.foo(...) method inside their try blocks\n\t\t\tchange applyDeleteUpdate to return void\n\t\t\tchange applyAddUpdate to return cmd.getIndexedId() so it can be used in subsequent ulog.lookup(...) calls for assertions about the state of the ulog\n\t\t\n\t\t\n\t\tbut i'm not certain, because i think these static methods are also used in other tests?\n\t\t\n\t\t\tso perhaps my original suggestion is still the only valid one: caller needs to pass in the SolrQueryRequest ?\n\t\t\n\t\t\n\t\tgrep test for \"nocommit: req lifecycle bug\"\n\t\n\t\n\n\n\n\n\tregarding using expectThrows:\nCouldn't use expectThrows as suggested, since it required variables like prevPointer, prevVersion, partialDoc etc. to be final variables, and changing them to such would make things ugly. Maybe I missed something?\n\t\n\t\ti'm not sure what you tried, but variables used in lambda bodies must be effectively final, and in this case all of the variables you're refering to are effectively final at the point where we need to be using expectThrows.\n\t\t\n\t\t\tas a general rule of thumb: if you want to use expectThrows and the variables you need to assert things about aren't effectively final, that's a code smell that you should consider breaking up your unit test into more discreete test methods.\n\t\t\tperhaps this is exactly the situation you ran into? you tried using expectThrows before splitting up the method and had difficulties, but now that the test methods are more discrete I was able to switch it to use expectThrows() no problem.\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tregarding DBQ testing: \nwhat about testing DBQs in the ulog and how they affect partial updates? Any DBQ causes ulog to be cleared after it (irrespective of in-place updates, irrespective of whether any documents were deleted or not). Only that effect can be tested, and I've added a test for it.\n\t\n\t\ti ment doing DBQs against a field/value that has in-place updates already in the tlog, and verifing that subsequent ulog.lookup calls do/don't match on that docId depending on wether the DBQ matched the old/new value after the in-place update.\n\t\t\n\t\t\tthe test you added (testApplyPartialUpdatesWithDBQ) just does a DBQ against the uniqueKey field \u2013 in spite of it's name, there isn't a single partial update anywhere in the test!\n\t\t\n\t\t\n\t\tif there's really nothing we can do with an UpdateLog after adding any DBQ to it (ie: all lookups will fail), then that's not at all obvious from this test \u2013 instead:\n\t\t\n\t\t\tif the behavior of a DBQ is completley independent of any in-place updates the method should be renamed\n\t\t\tthe asserts should make it clear the ulog has been cleared \u2013 add more docs first and then check that all lookups fail, verify directly that a new RT searcher really has been opened (compare the currently used RT searcher to an instance from before the DBQ?), etc...\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\nTestInPlaceUpdatesStandalone\n\n\tsome previous feedback i gave on one particular test...\nplease use expectThrows here, and actually validate that the Exception we get is a SolrException with code=409 FIXED: checked message contains \"conflict\"\n\t\n\t\tthat's a really weak assertion considering it's more correct, less brittle, and less code, to just use something like assertEquals(409, exception.code()) like i suggested.\n\t\tI went ahead and audited the all the new test code to ensure all of the expectThrows usage includes an assert that it has the expected error code (400, 409, 500, etc...) in addition to what ever existing message assertions make sense.\n\t\n\t\n\n\n\n\n\n\tgetFieldValueRTG\n\t\n\t\tthis method seems to have a lot of complexity, and requires a lot of complexity in all of the caller code, to work around the fact that it's using really low level access and dealing directly with the StoredField/NumericDocValuesField/LegacyField type objects that exist in the SolrInputDocuments that exist in the tlog\n\t\tit seems like it would be MASSIVELY simpler to just initialize a static SolrCLient in the @BeforeClass method along the lines of: solrClient = new EmbeddedSolrServer(h.getCoreContainer(), h.coreName); and then just use SolrClient.getById when we need values from an RTG\n\t\t\n\t\t\tgrep other solr/core tests for EmbeddedSolrServer to see examples of this\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\ttestReplay\n\t\n\t\ti previously gave the following feedback...\ntestReplay3, testReplay4, testReplay6, testReplay7\n\n\tthese should have better names Fixed\n\n\n\t\tinstead of giving the methods descriptive names making it clear what they tested, you seem to have combined them into one big generic testReplay method\n\t\tthat's pretty much the opposite of what i suggested\n\t\tif testReplay failed, it won't be particularly clear what aspect of the functinality encoutered a problem w/o reading through all of the steps, and if it fails early in the test (ie: the first checkReplay call) we won't have any idea if only that particularly sequence failed, or if the other sequences futher down in the test would also fail\n\t\tthat's the main value add in having discrete, independent, tests: being able to see when more then one of them fail as a result of some change.\n\t\t\n\t\t\ttestReplay as written was weaker then the 4 previous individual tests \u2013 even if they didn't have particularly distinctive names.\n\t\t\tat least before, if something caused testReplay3 and testReplay6 to fail, but testReplay4 and testReplay7 still passed, that would have given us a starting point for investigating what those methods had in common, and how they differed.\n\t\t\n\t\t\n\t\tI went ahead and re-split this into 4 distinct test methods\n\t\n\t\n\n\n\n\n\tprevious comment...\nI'd still like to see some 100% randomized testing using checkReplay.  It's a really powerful helper method \u2013 we should be taking full advantage of it. TODO: Can be added later\n\t\n\t\tconsidering how many weird edge case bugs we've found (both in the new solr code and in the existing IndexWriter level DB updating code) over the lifespan of this jira, that required very particular and specific circumnstances to manifest, I'm in no way comfortable committing this patch as is with the level of testing it has currently\n\t\tthat's why i keep saying we need more (solid) randomized & reproducible (ie: not cloud based where node routing might introduce subtle non-reproducibility) testing, and checkReplay is the starting point of doing that \u2013 it already takes care of the hard work, we just need some scaffolding to generate a random sequence of updates to pass to it, and a loop to do lots of iterations with diff random sequences.\n\t\n\t\n\n\n\nTestStressInPlaceUpdates\n\n\tsome previously feedback & responses...\n\t\n\t\t\"operations\" comment is bogus (it's not just for queries) FIXED\n\t\t\n\t\t\thow was this fixed? the comment on the operations variable in the last patch still had the exact same comment indicating it's a number of queries \u2013 i corrected it\n\t\t\n\t\t\n\t\t\nI'm not convinced the \"synchronize {...}; commit stuff; syncrhonize { ... };\" sequence is actually thread safe...\n\n\tT-W1: commit sync block 1: newCommittedModel = copy(model), version = snapshotCount++;\n\tT-W2: updates a doc and adds it to model\n\tT-W1: commit\n\tT-W1: commit sync block 2: committedModel = newCommittedModel\n\tT-R3: read sync block: get info from committedModel\n\tT-R3: query for doc\n\t...\nCopied the logic straight from TestStressReorder. Maybe we can revisit both together, later, in light of your concerns?\n\n\n\t\t\n\t\t\tbroken code is broken code.  It's bad enough it exists at all, it shouldn't be copied if we know it's broken.\n\t\t\tI've described in detail a specific example of how the synchronization approach in the code is invalid acording to my understanding of the code and the docs for the relevant methods \u2013 if you can explain to me some way(s) in which my example is inaccurate, or involves some missunderstanding of the relevant documentation then great! \u2013 we can leave the code the way it is.  Otherwise it's broken and needs to be fixed.\n\t\t\tthere's no point in adding (more!) tests to Solr that we know for a fact are unreliable as designed.\n\t\t\n\t\t\n\t\t\nhaving at least one pass over the model checking every doc at the end of the test seems like a good idea no matter what FIXED\n\t\t\n\t\t\thow is this fixed?\n\t\t\tthe very last thing in the stressTest method is still the loop that joins on all the threads.\n\t\t\n\t\t\n\t\tI'm certain the existing \"synchronized (model)\" block is not thread safe relative to the synchronized blocks that copy the model into commitedModel, because the \"model.put(...)\" calls can change the iterator and trigger a ConcurrentModificationException Never saw a CME occur in thousands of rounds of this test. I think if a CME were to be thrown ever, it would've been quite frequent.\n\t\t\n\t\t\tFWIW: Just because you never saw a CME in your testing doesn't make the test valid \u2013 it could be more likely to trigger on diff OS/architectures\n\t\t\t\n\t\t\t\tYou can't disprove a concurrency bug in code just by saying \"i ran it a bunch and never saw a problem\"\n\t\t\t\tHell: I could write a test that does assertTrue(0 != random().nextInt() and run it for thousands of rounds w/o ever getting a failure \u2013 but that doesn't make the test correct.\n\t\t\t\n\t\t\t\n\t\t\tThat said: I reviewed this particular concern again just to be sure and I'm pretty sure my initial concern was invalid: I'm guessing i didn't notice before that model is a ConcurrentHashMap so HashMap's copy constructor will get a weakly consistent iterator and never throw CME.\n\t\t\t\n\t\t\t\tI added a comment making this obvious to the next person to make the same mistake i did.\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\n\t\n\n\n\n\nTestInPlaceUpdatesDistrib\n\n\tstarting with a random index to help test edge cases\n\t\n\t\ta key piece of feedback i gave about this test before was:\n\n\teven if we're only going to test one a few doc, we should ensure there are a random num docs in the index (some before the doc we're editing, and some after) FIXED\n\t2 docs before/after is not a random number ... random means random: we need to test edge cases of first docid in index, last docid in index, first/last docid in segment, etc...\n\n\n\t\tI mentioned this about multiple test methods, including outOfOrderUpdatesIndividualReplicaTest\n\t\tthis is how outOfOrderUpdatesIndividualReplicaTest started in the last patch...\n\n  private void outOfOrderUpdatesIndividualReplicaTest() throws Exception {\n    del(\"*:*\");\n    commit();\n\n    index(\"id\", 0, \"title_s\", \"title0\", \"id_i\", 0);\n    commit();\n\n    float ratings = 1;\n\n    // update doc, set\n    index(\"id\", 0, \"ratings\", map(\"set\", ratings));\n...\n\n\n\t\t\n\t\t\tthe index contained exactly one document: id=0\n\t\t\tthat document started w/o a value in the field we want to update in place\n\t\t\tthen we do an inplace update the document to set a value on it, and then \"...\" more testing of id=0\n\t\t\n\t\t\n\t\tthis is what the start of that method looks like in the current patch...\n\n  private void outOfOrderUpdatesIndividualReplicaTest() throws Exception {\n    del(\"*:*\");\n    commit();\n\n    index(\"id\", 0, \"title_s\", \"title0\", \"id_i\", 0);\n    commit();\n\n    float inplace_updatable_float = 1;\n\n    // Adding random number of docs before adding the doc to be tested\n    int numDocsBefore = random().nextInt(1000);\n    for (int i=0; i<numDocsBefore; i++) {\n      index(\"id\", 1000 + i, \"title_s\", \"title\" + (1000+i), \"id_i\", 1000 + i);\n    }\n\n    // update doc, set\n    index(\"id\", 0, \"inplace_updatable_float\", map(\"set\", inplace_updatable_float));\n\n    // Adding random number of docs after adding the doc to be tested\n    int numDocsAfter = random().nextInt(1000);\n    for (int i=0; i<numDocsAfter; i++) {\n      index(\"id\", 5000 + i, \"title_s\", \"title\" + (5000+i), \"id_i\", 5000 + i);\n    }\n...\n\n\n\t\t\n\t\t\tthe test still starts out by adding doc id=0 first before any other docs\n\t\t\tthat means that we're still only realy testing a in-place update of the first document in the index\n\t\t\twe're only getting marginal benefit out of the \"randomzied\" index \u2013 namely that the size of the index is random. but the doc getting updated in-place is always the first doc in the index.  we're never testing an in-place update of a doc in the second segment, or the last doc in a segment, or the last doc in an index, etc...\n\t\t\n\t\t\n\t\tas written, this basically defeats the entire point of why we should use a random index\n\t\tthat said: at least it gives us some randomization\n\t\tmost other test methods in this class still don't even have that\n\t\tensureRtgWorksWithPartialUpdatesTest does seem to have good initial initial index setup code:\n\t\t\n\t\t\tadding a random number of docs\n\t\t\tthen add a \"special\" doc to be tested (w/o the field to be updated inplace)\n\t\t\tthen add some more random documents\n\t\t\n\t\t\n\t\twhy not refactor that index creation code from ensureRtgWorksWithPartialUpdatesTest into a helper method and use it in all of the other test methods?\n\t\t\n\t\t\tthese tests all still uses an index containing exactly 1 doc and should also have a random index:\n\t\t\t\n\t\t\t\toutOfOrderDBQsTest\n\t\t\t\toutOfOrderDeleteUpdatesIndividualReplicaTest\n\t\t\t\treorderedDBQsWithInPlaceUpdatesShouldNotThrowReplicaInLIRTest\n\t\t\t\tdelayedReorderingFetchesMissingUpdateFromLeaderTest\n\t\t\t\n\t\t\t\n\t\t\tdocValuesUpdateTest could use the same index building helper method even if doesn't follow the same pattern of only testing a single \"special\" doc\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tExecutorService\n\t\n\t\tI feel like i've mentioned this every time i've given feedback: \n\n\tif we are going to use an ExecutorService, then the result of awaitTermination has to be checked FIXED\n\t... and shutdown & awaitTermination have to be called in that order\n\n\n\t\tthe latest patch had 2 places where threadpools were shutdown & awaitTermination correctly \u2013 but there were still 4 more that were broken.\n\t\tI went ahead and fixed them.\n\t\n\t\n\n\n\n\n\n\tdocValuesUpdateTest\n\t\n\t\tknowing when all the updates have propogated...\n\t\t\n\t\t\tI had previously (twice) made the following suggestion: \n\n\tIn general, this seems like a convoluted way to try and kill two birds with one stone: 1) make sure all replicas have opened searchers with the new docs; 2) give us something to compare to later to ensure the update was truely in place\n\t\n\t\ti really think the first problem should be addressed the way i suggested previously:Done\n* if initially no docs have a rating value, then make the (first) test query be for rating:[* TO *] and execute it in a rety loop until the numFound matches numDocs.Done\n* likewise if we ensure all ratings have a value such that abs(ratings) < X, then the second update can use an increment such that abs(inc) > X*3 and we can use -ratings:[-X TO X] as the query in a retry loop Done\n\t\tthe second problem should be solved by either using the segments API, or by checking the docids on every replica (w/o any retries) ... after independently verifying the searcher has been re-opened.\n\t\n\t\n\n\n\t\t\n\t\t\n\t\tin the current patch you updated the // Keep querying until the commit is distributed and expected number of results are obtained code to use a range query like i suggested, but: that code is useless because it assumes that those docs won't match the query until the updates propogate, which isn't true the way the test is currently written...\n\t\t\n\t\t\tthe index starts off with every document having a value in the field being queried against, so even if none of the in-place updates happen, the query will still succeed.\n\t\t\teither all of the docs need to have no-values in the field before the in-place updates are attempted, or the test query needs to be tightened to only match the docs if they contain the updated values\n\t\t\t\n\t\t\t\texample: currently all docs start with a value of 101.0 and the in-place updates set every doc to a value <= 5.0, so we can make the verification of the first update be \"inplace_updatable_float:[* TO 5.0]\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\tThe verification of the second update looks  seems mostly ok, but it seems a little more complicated then it needs to be and i think there is at least one subtle bug...\n\t\t\n\t\t\tMath.max(Collections.max(valuesList), Math.abs(Collections.min(valuesList))); ?\n\t\t\t\n\t\t\t\tCollections.max(valuesList) might also be negative, we need to take the abs of both before values before deciding which is bigger.\n\t\t\t\n\t\t\t\n\t\t\tin general it seems overly complicated to worry about determining X dynamically...\n\t\t\t\n\t\t\t\tthe code that does the original update (valuesList.add(r.nextFloat()*5.0f);) ensures that the values are all between 0 and 5.0 \u2013 let's assume we tweak that to randomly use -5.0 sometimes so our range is -5.0 to 5.0\n\t\t\t\tthe code that does the second update can just use inc = atLeast(10) * (r.nextBoolean() ? 1 : -1);\n\t\t\t\tthen the code verifying the second update propogates can just be \"+inplace_updatable_float:[* TO *] -inplace_updatable_float:[-15 TO 15]\"\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\tin both cases however, only executing the query against clientToTest is a bad idea.  We need to be running the verification against ALL the replicas, otherwise we can get false positives from the subsequent validation (ie: the thread scheduling might result in clientToTest being completely finished with all the updates and the commits, while some other replica is still working on it and hasn't committed yet when we do our validation queries.\n\t\t\n\t\t\tthere's also the problem that the clients list includes the LEADER, so 1/3 of the time these queries aren't even hitting a replica and are useless.\n\t\t\n\t\t\n\t\tthe second part of my suggestion doesn't seem to have been implemented, nor did it get any comment:\n\n\tthe second problem should be solved by either using the segments API, or by checking the docids on every replica (w/o any retries) ... after independently verifying the searcher has been re-opened.\n\n\n\t\t\n\t\t\tditto the problem of picking a random clients and 1/3 of the time geting hte leader making the matchResults() call useless.\n\t\t\teven if we switch the randomization to use NONLEADERS there's no good reason to be comparing with only one replica \u2013 looping over NONLEADERS is at worst \"not harder\" then picking a random replica, and makes the test twice as likely to catch any potential bugs that might cause a replica to get out of sync\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\tgetInternalDocIds\n\t\n\t\tthis looks like it exists because of this prior feedback for ensureRtgWorksWithPartialUpdatesTest...\nchecking fl=[docid] in cloud RTG is currently broken (SOLR-9289) but we can/should be checking via the segments API anyway (if we have a general helper method for comparing the segments API responses of multiple replicas betwen multiple calls, it could be re-used in every test in this class)Done by opening a new RT searcher and getting the docid for it\n\t\tbut SOLR-9289 has been fixed since July ... there's no reason to jump through hoops like this instead of just doing an RTG request with fl=[docid] ... it just makes the test more brittle to refactoring (into a true cloud test) later.\n\t\n\t\n\n\n\n\n\tensureRtgWorksWithPartialUpdatesTest\n\t\n\t\tas mentioned breviously: comparing results from LEADER with results from a random clients is useless 1/3 of the time since that list contains the LEADER\n\t\t\n\t\t\tthe test will be a lot stronger and no longer if instead it loops over the clients and does a getById & asserting the results inside the loop.\n\t\t\n\t\t\n\t\tsee comments above regarding getInternalDocIds - rather then fixing that method it might be easier to just do direct getById calls here\n\t\t\n\t\t\talready doing LEADER.getById(\"100\"); \u2013 just add fl=[docid] to those and add some new similar getById calls to the replicas to compare with the existing getById validation calls to them as well\n\t\t\talthough: shouldn't ALL the getById calls in this method be replaced with getReplicaValue or assertReplicaValue calls ... isn't that what those methods are for? don't they ensure that RTG calls to replicas aren't internally forwarded to the LEADER?\n\t\t\n\t\t\n\t\n\t\n\n\n\n\n\n\taddDocAndGetVersion...\nsynchronized (cloudClient)\n\n\twhy are we synchronized on cloud client but updating via LEADER?\n\twhy are we synchronized at all?\nFixed\n\n\n\t\n\t\tNothing was fixed about this \u2013 it's still synchronizing on cloudClient .. does it need sync at all?????\n\t\n\t\n\n\n\n\nI plan to keep working on this issue for at least a few more days \u2013 starting with some of the ranodmized test improvements I mentioned (above) being really concerned with.\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15723670",
            "date": "2016-12-05T23:08:17+0000",
            "content": "\nThe only diff between this patch and the previous one is some improvements to TestInPlaceUpdatesStandalone and some neccessary additions to schema-inplace-updates.xml to support them...\n\n\n\n\treplace hackish getFieldValueRTG with a regular SolrClient.getById simplifying most caller code\n\trefactor getFieldValueIndex / getDocId\n\t\n\t\tonly usaage of getFieldValueIndex was in getDocId or places that should have been calling getDocId instead\n\t\trefactored logic to use SolrClient.query instead of hackish low level access\n\t\n\t\n\tadded new randomized test methods leveraging checkReply\n\t\n\t\tthis helped uncover a few minor bugs in checkReplay which i fixed\n\t\tin the process I also cleaned up the datatypes used in the existing callers of checkReply to reduce a lot of String/Number parsing/formatting logic in checkReply\n\t\tthis uncovered a ClassCastException when inplace updates are mixed with non-inplace atomic updates (see below)\n\t\t\n\t\t\tadded testReplay_MixOfInplaceAndNonInPlaceAtomicUpdates to demonstrate this directly\n\t\t\n\t\t\n\t\n\t\n\tnew schema assertions warranted by fields added for the above changes\n\n\n\nHere's an example of the ClassCastException that shows up in the logs when running testReplay_MixOfInplaceAndNonInPlaceAtomicUpdates ...\n\n\n   [junit4]   2> 2514 ERROR (TEST-TestInPlaceUpdatesStandalone.testReplay_MixOfInplaceAndNonInPlaceAtomicUpdates-seed#[70DBFB363B6DA180]) [    ] o.a.s.h.RequestHandlerBase java.lang.ClassCastException: org.apache.solr.common.SolrDocument cannot be cast to org.apache.solr.common.SolrInputDocument\n   [junit4]   2> \tat org.apache.solr.handler.component.RealTimeGetComponent.getInputDocumentFromTlog(RealTimeGetComponent.java:512)\n   [junit4]   2> \tat org.apache.solr.handler.component.RealTimeGetComponent.getInputDocument(RealTimeGetComponent.java:568)\n   [junit4]   2> \tat org.apache.solr.handler.component.RealTimeGetComponent.getInputDocument(RealTimeGetComponent.java:546)\n   [junit4]   2> \tat org.apache.solr.update.processor.DistributedUpdateProcessor.getUpdatedDocument(DistributedUpdateProcessor.java:1424)\n   [junit4]   2> \tat org.apache.solr.update.processor.DistributedUpdateProcessor.versionAdd(DistributedUpdateProcessor.java:1072)\n   [junit4]   2> \tat org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:751)\n   [junit4]   2> \tat org.apache.solr.update.processor.LogUpdateProcessorFactory$LogUpdateProcessor.processAdd(LogUpdateProcessorFactory.java:103)\n   [junit4]   2> \tat org.apache.solr.handler.loader.JsonLoader$SingleThreadedJsonLoader.handleAdds(JsonLoader.java:492)\n   [junit4]   2> \tat org.apache.solr.handler.loader.JsonLoader$SingleThreadedJsonLoader.processUpdate(JsonLoader.java:139)\n   [junit4]   2> \tat org.apache.solr.handler.loader.JsonLoader$SingleThreadedJsonLoader.load(JsonLoader.java:115)\n   [junit4]   2> \tat org.apache.solr.handler.loader.JsonLoader.load(JsonLoader.java:78)\n   [junit4]   2> \tat org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:97)\n   [junit4]   2> \tat org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:68)\n   [junit4]   2> \tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:152)\n   [junit4]   2> \tat org.apache.solr.core.SolrCore.execute(SolrCore.java:2228)\n   [junit4]   2> \tat org.apache.solr.servlet.DirectSolrConnection.request(DirectSolrConnection.java:124)\n   [junit4]   2> \tat org.apache.solr.SolrTestCaseJ4.updateJ(SolrTestCaseJ4.java:1173)\n   [junit4]   2> \tat org.apache.solr.SolrTestCaseJ4.addAndGetVersion(SolrTestCaseJ4.java:1319)\n   [junit4]   2> \tat org.apache.solr.update.TestInPlaceUpdatesStandalone.checkReplay(TestInPlaceUpdatesStandalone.java:823)\n   [junit4]   2> \tat org.apache.solr.update.TestInPlaceUpdatesStandalone.testReplay_MixOfInplaceAndNonInPlaceAtomicUpdates(TestInPlaceUpdatesStandalone.java:570)\n\n\n\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15727635",
            "date": "2016-12-07T04:14:46+0000",
            "content": "Hoss Man, I've created a branch (jira/solr-5944) for this, and committed your latest patch to the branch. Also, I've fixed the ClassCastException bug that you saw and also fixed a javadoc bug and removed unused imports. (https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=8c7a2a6). However, I saw consistent failures on TestInPlaceUpdatesStandalone#testReplay_Random_FewDocsManyShortSequences which you recently added. I haven't looked deep into why it could be failing, but a preliminary look at the logs lead me to believe that it is a test problem.\n\nBtw, do you know how to enable commit notifications to show up here for the jira/solr-5944 branch? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15727716",
            "date": "2016-12-07T05:04:58+0000",
            "content": "I saw consistent failures on...\n\nI'm seeing consistent failures from most of the randomized tests.\n\nI haven't looked deep into why it could be failing, but a preliminary look at the logs lead me to believe that it is a test problem.\n\ncan you elaborate on what in the logs gave you that impression?\n\nIf it were a test bug \u2013 ie: a bug in tracking the model state compared to the inplace atomic updes \u2013 I would expect the failures to reproduce if you switched the test to use a regular (indexed+stored) long field instead of a DVO field \u2013 ie: use the classic atomic update code instead of the inplace update code.\n\nBut when i tried toggling the field used (see comments in checkRandomReplay) I couldn't reproduce any failures.\n\nI added some hackish logging to checkRandomReplay to get it to dump a short sequence that failed and turned that into a new test method (testReplay_nocommit) and then i distilled what seems to be the key problematic bits into an even shorter test: testReplay_SetOverriddenWithNoValueThenInc ...\n\n\n  public void testReplay_SetOverriddenWithNoValueThenInc() throws Exception {\n    final String inplaceField = \"inplace_l_dvo\"; \n    // final String inplaceField = \"inplace_nocommit_not_really_l\"; // nocommit: \"inplace_l_dvo\"\n    \n    checkReplay(inplaceField,\n                //\n                sdoc(\"id\", \"1\", inplaceField, map(\"set\", 555L)),\n                SOFTCOMMIT,\n                sdoc(\"id\", \"1\", \"regular_l\", 666L), // NOTE: no inplaceField, regular add w/overwrite \n                sdoc(\"id\", \"1\", inplaceField, map(\"inc\", -77)),\n                HARDCOMMIT);\n  }\n\n\n\n...all of that is now on the branch.\n\nIf you toggle the above code to use regular atomic updates, then it passes \u2013 but as written, so it uses the new inplace update code paths, it fails like so...\n\n\n   [junit4] FAILURE 0.54s | TestInPlaceUpdatesStandalone.testReplay_SetOverriddenWithNoValueThenInc <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: expected:<-77> but was:<478>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([9D6E895FCBA28315:6DDD2091B324AFF2]:0)\n   [junit4]    > \tat org.apache.solr.update.TestInPlaceUpdatesStandalone.checkReplay(TestInPlaceUpdatesStandalone.java:920)\n   [junit4]    > \tat org.apache.solr.update.TestInPlaceUpdatesStandalone.testReplay_SetOverriddenWithNoValueThenInc(TestInPlaceUpdatesStandalone.java:590)\n\n\n\n...looks like a genuine bug to me: when a regular update overwrites a doc that had a DVO field value, a subsequent \"inc\" operation on the DVO fields is picking up the old value instead of operating against an implicit default of 0.\n\n(This kind of corner case is what makes randomized testing totally worth the time and effort)\n\nBtw, do you know how to enable commit notifications to show up here for the jira/solr-5944 branch?\n\nIIRC comments about commits to jira/* branches are suppressed intentionally as noise, because it's expected that there will be lots of iteration on the branches, some of which might be thrown away, and for posterity what matters is only commits to main line dev branches "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15729698",
            "date": "2016-12-07T19:37:07+0000",
            "content": "...looks like a genuine bug to me: ...\n\nFWIW:\n\nI made a half assed attempt to quickly reproduce this with bin/solr -e schemaless and some curl commands to create fields and add docs \u2013 and i couldn't reproduce this.\n\nIt's possible I screwed something up with the commands, and or varried the input slightly in a way that didn't tickle the bug; or it's possible that with the default schemaless configs, some (default?) autocommit setting caued the tlogs to get flushed/committed in a way that bypasses the bug.\n\n\nIn any case, I then attempted to more systematically demo the bug using the same configs as the test \u2013 and was able to easily reproduce...\n\n\nmkdir -p /tmp/solr_test_home/cores/inplace_testing/conf\ncp server/solr/solr.xml /tmp/solr_test_home/\ntouch /tmp/solr_test_home/cores/inplace_testing/core.properties\ncp core/src/test-files/solr/collection1/conf/schema-inplace-updates.xml /tmp/solr_test_home/cores/inplace_testing/conf/schema.xml\ncp core/src/test-files/solr/collection1/conf/solrconfig-tlog.xml /tmp/solr_test_home/cores/inplace_testing/conf/solrconfig.xml\ncp core/src/test-files/solr/collection1/conf/solrconfig.snippet.randomindexconfig.xml /tmp/solr_test_home/cores/inplace_testing/conf/\n\n# normally these are randomized by the test harness\n# i just picked these values arbitrarily since it seemed to reproduce regardless of seed\nbin/solr start -f -s /tmp/solr_test_home/ -Dsolr.tests.maxBufferedDocs=1000 -Dsolr.tests.ramBufferSizeMB=100 -Dsolr.tests.mergeScheduler=org.apache.lucene.index.ConcurrentMergeScheduler -Dsolr.tests.useMergePolicy=false -Dsolr.tests.mergePolicyFactory=org.apache.solr.index.TieredMergePolicyFactory\n\ncurl -X POST -H 'Content-Type: application/json' --data-binary '[\n{ \"id\": \"1\",\"inplace_l_dvo\": { \"set\": 555 } },\n{ \"id\": \"2\",\"not_inplace_l\": { \"set\": 555 } }\n]' 'http://localhost:8983/solr/inplace_testing/update?commit=true&softCommit=true'\n\ncurl -X POST -H 'Content-Type: application/json' --data-binary '[\n{ \"id\": \"1\",\"regular_l\": 666 },\n{ \"id\": \"2\",\"regular_l\": 666 }\n]' 'http://localhost:8983/solr/inplace_testing/update'\n\ncurl -X POST -H 'Content-Type: application/json' --data-binary '[\n{ \"id\": \"1\",\"inplace_l_dvo\": { \"inc\": -77 } },\n{ \"id\": \"2\",\"not_inplace_l\": { \"inc\": -77 } }\n]' 'http://localhost:8983/solr/inplace_testing/update'\n\ncurl 'http://localhost:8983/solr/inplace_testing/get?wt=json&indent=true&ids=1,2'\n\n\n\nFinal RTG Output...\n\n\n{\n  \"response\":{\"numFound\":2,\"start\":0,\"docs\":[\n      {\n        \"id\":\"1\",\n        \"inplace_l_dvo\":478,\n        \"_version_\":1553084875040358400,\n        \"regular_l\":666},\n      {\n        \"id\":\"2\",\n        \"regular_l\":666,\n        \"not_inplace_l\":-77,\n        \"_version_\":1553084875042455552,\n        \"copyfield1_src__both_updateable\":0,\n        \"inplace_updatable_float\":0.0,\n        \"copyfield2_src__only_src_updatable\":0,\n        \"inplace_updatable_int\":0}]\n  }}\n\n\n\n...note that unlike in the test where we only deal with a single document, I used 2 docs here \u2013 giving them identical updates \u2013 to show that while the problem reproduces when using a DVO field that gets inplace updates (doc id #1) it doesn't reproduce when using a regular stored+indexed field that gets a classic atomic update (doc id #2).\n\n\n\nside note: i was confused by all those fields with a value of \"0\" in doc id#2, and thought that might be somehow related to the bug \u2013 ie: DVO fields getting added with the default 0 in some code path \u2013 but then i realized they all have defaultValues in the schema.xml \u2013 i'm not entirely sure why only doc#2 shows them in this RTG, but doc#1 gets those field values as well once a commit happens.\n\nI suspect there is some code path in the regular atomic update logic where docs read from the tlog get any default field values added to them, and that isn't happening in the inplace code path case? ... not certain ... might be another bug to look into?\n\nperhaps the // Note: We don't need to add default fields ... would've happened during the full indexing initially isn't necessarily true for something that so far only exists in the tlog? ... need to thin about this some more. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15729798",
            "date": "2016-12-07T20:15:39+0000",
            "content": "Hoss Man, I can confirm that this indeed is a genuine bug! I could trace the culprit to the region where I'm doing the following in AUDM:\n\n// If oldDocument doesn't have a field that is present in updatedFields, \n    // then fetch the field from RT searcher into oldDocument.\n    // This can happen if the oldDocument was fetched from tlog, but the DV field to be\n    // updated was not in that document.\n\n\n\nI think something like a full resolve (following documents using the prevPointer up to the last full document) is in order, and my trying to optimize there is the culprit in that case. Doing so without rewriting a bunch of code (that does resolve) and trying to maximize code reuse (resolveFullDocument()?) would be a challenge to get right, and I'll update the branch once I can do this. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15729839",
            "date": "2016-12-07T20:32:23+0000",
            "content": "\nIIRC comments about commits to jira/* branches are suppressed intentionally as noise, because it's expected that there will be lots of iteration on the branches, some of which might be thrown away, and for posterity what matters is only commits to main line dev branches\n\nI see. I was looking at SOLR-8029, and assumed that it is the norm for those notifications to make it to the JIRA issue by default, but I guess those may have been enabled explicitly for that issue.  "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15730122",
            "date": "2016-12-07T22:21:46+0000",
            "content": "I suspect there is some code path in the regular atomic update logic where docs read from the tlog get any default field values added to them, and that isn't happening in the inplace code path case? ... not certain ... might be another bug to look into?\n\nYup.\n\nI committed some new tests cases to AtomicUpdatesTest demonstrating 2 classes of problems:\n\n\ttestFieldsWithDefaultValuesWhenAtomicUpdatesAgainstTlog\n\t\n\t\tchecks that after doing an (inplace or regular) atomic update against an uncommited doc, that all the expected \"default\" field values are returned by RTG\n\t\tthis test passes fine on master, but on the SOLR-5944 branch the field that supports inplace updates breaks the test.\n\t\n\t\n\ttestAtomicUpdateOfFieldsWithDefaultValue\n\t\n\t\tattempts an \"inc\" against a field whose value comes from a schema default\n\t\tcurrently disabled completely because it's broken on master for regular atomic updates: SOLR-9838\n\t\n\t\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15733820",
            "date": "2016-12-09T00:22:08+0000",
            "content": "\nI've pushed an update to TestInPlaceUpdatesDistrib that refactors some randomized index building into a new buildRandomIndex helper method which is now used by most of the \"test\" methods in that class.\n\nIt's NOT currently used by docValuesUpdateTest() even though it was designed to be \u2013 I made several aborted attempts to try and switch that method to use buildRandomIndex knowing that many assertions in that test would need other tweaks to account for more docs in the index, but i kept running into weird failures that took me a while to explain.\n\nUltimately I realized the problem is that currently, schema-inplace-updates.xml is configured with inplace_updatable_float having a default=\"0\" setting \u2013 which (besides making most of our testing using hta field much weaker then i realized) means that the initial sanity checks in docValuesUpdateTest() are even less useful then i originally thought.\n\n\n\nIshan Chattopadhyaya: do you remember why this default is set on inplace_updatable_float (and inplace_updatable_int) schema-inplace-updates.xml ? ... i see TestInPlaceUpdatesDistrib doing a preliminary sanity check assertion that the defaults exists in the schema, but I don't see any test that seems to care/expect that default to work, and it seems to weaken our test coverage of the more common case...\n\nSpecifically: when I tried to remove it, I started seeing NPEs from SolrIndexSearcher.decorateDocValueFields in various tests:\n\n\n\n   [junit4] ERROR   0.05s J2 | TestInPlaceUpdatesStandalone.testUpdateTwoDifferentFields <<<\n   [junit4]    > Throwable #1: java.lang.NullPointerException\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([29D61963E75459C5:26F189B6F032D44A]:0)\n   [junit4]    > \tat org.apache.solr.search.SolrIndexSearcher.decorateDocValueFields(SolrIndexSearcher.java:810)\n   [junit4]    > \tat org.apache.solr.handler.component.RealTimeGetComponent.getInputDocument(RealTimeGetComponent.java:599)\n   [junit4]    > \tat org.apache.solr.update.processor.AtomicUpdateDocumentMerger.doInPlaceUpdateMerge(AtomicUpdateDocumentMerger.java:286)\n   [junit4]    > \tat org.apache.solr.update.processor.DistributedUpdateProcessor.getUpdatedDocument(DistributedUpdateProcessor.java:1414)\n   [junit4]    > \tat org.apache.solr.update.processor.DistributedUpdateProcessor.versionAdd(DistributedUpdateProcessor.java:1072)\n   [junit4]    > \tat org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:751)\n   [junit4]    > \tat org.apache.solr.update.processor.LogUpdateProcessorFactory$LogUpdateProcessor.processAdd(LogUpdateProcessorF\n\n\n\n\n...while it certainly makes sense to have some testing of inplace updates when there is a schema specified default that's non-zero (although see the previously mentioned SOLR-9838 for some issues with doing that currently), i'm now concerned about how much of the code may only be working because these fields have an explicit default=\"0\" ? "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15735034",
            "date": "2016-12-09T11:18:59+0000",
            "content": "\nthat doesn't really answer my question at all: why does it matter if it's a dynamic field?\n\nIf a document's dv field is updated for the first time (i.e. that field never existed before in the index before), then the in-place update succeeds if it is an explicit field but fails if it is a dynamic field.\nIt fails with this error: \n\nCaused by: java.lang.IllegalArgumentException: can only update existing docvalues fields! field=abc_f_dvo, type=NUMERIC\n\tat org.apache.lucene.index.IndexWriter.updateDocValues(IndexWriter.java:1715)\n\tat org.apache.solr.update.DirectUpdateHandler2.updateDocOrDocValues(DirectUpdateHandler2.java:875)\n\tat org.apache.solr.update.DirectUpdateHandler2.doNormalUpdate(DirectUpdateHandler2.java:279)\n\tat org.apache.solr.update.DirectUpdateHandler2.addDoc0(DirectUpdateHandler2.java:213)\n\tat org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:168)\n\t... 57 more\n\n\nThe reason for this is that when a document is created, DV fields for all explicit fields are created. But when a non-existent dynamic field is attempted to be updated subsequently, then the underlying DV field doesn't exist.\n\nAttached a patch to demonstrate this. To demonstrate this, I've disabled the schema.isDynamicField() check and also another check that aborts the in-place updating for non-existent fields. (Fyi, the latter check is actually erroneous and needs to be re-written as I mentioned in the comment https://issues.apache.org/jira/browse/SOLR-5944?focusedCommentId=15729798&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15729798) "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15735064",
            "date": "2016-12-09T11:28:37+0000",
            "content": "\nThe concern i have is that AFAICT rebuilding the Arrays.asList(tlog, prevMapLog, prevMapLog2) in every iteration of the while loop seems to be at cross purposes with the reason why the while loop might have more then one iteration.\nYour explanation makes complete sense. I think we should pull it out. I've yet to look deeply into your suggestion of having it all within the same synchronized block, but it makes sense to me. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15735091",
            "date": "2016-12-09T11:39:53+0000",
            "content": "\nIn general, i think there's a gap in the DUP logic related to how the results of fetchFullUpdateFromLeader are used when replicas never get dependent updates \n\n                  // nocommit: INNER ELSE \u2013 I'm concerned/confused by this code happening here...\n                  //\n                  // at this point, we're replacing the data in our existing \"in-place\" update (cmd) so it\n                  // becomes a normal \"add\" using the full SolrInputDocument fetched from the leader\n                  //\n                  // but this if/else is itself is wrapped in a bigger if (grep for \"OUTER IF\" above\n                  // and \"OUTER ELSE\" below) where the \"else\" clause does some sanity checking / processing\n                  // logic for \"// non inplace update, i.e. full document update\" \u2013 all of which is\n                  // skipped for our modified \"cmd\"\n                  //\n                  // shouldn't the logic in that outer else clause also be applied to our\n                  // \"no longer really an in-place\" update?\nI think that the code there should be refactored more nicely. However, as it is currently in the branch, I think the same checks within the outer else are fulfilled; with the exception of checkDeleteByQueries = true, which doesn't seem to be used anywhere. However, even if the current code (in the branch) works correctly, it is very confusing and complicated and I shall try to refactor it in such a way that the code in that outer else block gets explicitly executed somehow. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15735138",
            "date": "2016-12-09T11:55:26+0000",
            "content": "\nSpecifically: when I tried to remove it, I started seeing NPEs from SolrIndexSearcher.decorateDocValueFields in various tests:\n\nI tried to chase this bug down, and I think it has led me to some learnings regarding the explicit vs dynamic fields (why dynamic fields are any different) issue.\n\n\nThe reason for this is that when a document is created, DV fields for all explicit fields are created. But when a non-existent dynamic field is attempted to be updated subsequently, then the underlying DV field doesn't exist.\nI think I missed the fact that \"DV fields for all explicit fields are created\" only if they have a default value set on them. Otherwise, explicit fields are no different from dynamic fields (in that they are not created until indexed for the first time).\n\nSo, therefore, I think we should be checking if the field exists or not, irrespective of explicit or dynamic. And if the field doesn't exist, we should just delegate to a regular atomic update. Do you think it makes sense? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15735756",
            "date": "2016-12-09T16:47:25+0000",
            "content": "So, therefore, I think we should be checking if the field exists or not, irrespective of explicit or dynamic. And if the field doesn't exist, we should just delegate to a regular atomic update. Do you think it makes sense?\n\nIIUC, in a nutshell: all of my questions about why we're treating dynamicFields as special will no longer be relevent, because dynamicFields will no longer be considered special, because they were never really special to begin with, they just seemed that way because the only testing of non-dynamic fields assumed/required them to have a schema default.  we'll remove that assumption from both the tests and code, and treat all fields the same.\n\nIf my understanding is correct, then yes you're plan to move forward seems sound. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15742173",
            "date": "2016-12-12T14:41:25+0000",
            "content": "Hoss, I've fixed the test failure mentioned in the comment [0] in the commit [1]. Also, the presence or absence of a default value for a DV should not affect how it is dealt with. \n\n[0] - (the testReplay* failures) https://issues.apache.org/jira/browse/SOLR-5944?focusedCommentId=15727716&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15727716\n[1] - e528b46c6aa1824aa96f2a47a0d55f085967e02a "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15742777",
            "date": "2016-12-12T18:52:09+0000",
            "content": "\ne528b46c6aa1824aa96f2a47a0d55f085967e02a\n\n\n@@ -234,36 +233,27 @@ public class AtomicUpdateDocumentMerger {\n...\n+      if (! fieldNamesFromIndexWriter.contains(fieldName) ) {\n+        // nocommit: this comment is not usefull - doesn't explain *WHY*\n+        return Collections.emptySet(); // if dynamic field and this field doesn't exist, DV update can't work\n+      }\n\n\n\n...none of those comments make sense now that there's no dynamic field checking\n\n\n\n@@ -284,42 +274,29 @@ public class AtomicUpdateDocumentMerger {\n \n     updatedFields.add(DistributedUpdateProcessor.VERSION_FIELD); // add the version field so that it is fetched too\n     SolrInputDocument oldDocument = RealTimeGetComponent.getInputDocument(cmd.getReq().getCore(),\n-                                              idBytes, true, updatedFields, false); // avoid stored fields from index\n+                                              idBytes, true, updatedFields, true); // avoid stored fields from index\n\n\n\n...isn't that comment now bogus? aren't you explicitly requesting stored fields w/true ? "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15744004",
            "date": "2016-12-13T03:20:05+0000",
            "content": "\n...none of those comments make sense now that there's no dynamic field checking\nIndeed, removed in my latest commit.\n\n\n...isn't that comment now bogus? aren't you explicitly requesting stored fields w/true ?\nThat false -> true change was for the resolveFullDocument parameter, so the comment (meant for avoiding the retrieval of stored fields) remains valid. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15806457",
            "date": "2017-01-07T01:44:38+0000",
            "content": "while reviewing some of ishan's recent commits to the jira/solr-5944 branch I was confused by how exactly TestRecovery.testLogReplayWithInPlaceUpdatesAndDeletes works and what it was doing, and asked ishan about it offline \u2013 while he walked me through the test, we realized that at one point while writting it he had assumed some weird behavior he had seen relating to the update log replay was an artifact of the test harness, and he included this line to work around it...\n\n\n      // Clearing the updatelog as it would happen after a fresh node restart\n      h.getCore().getUpdateHandler().getUpdateLog().deleteAll();\n\n\n\n..but the more we talked about it the more it seemed like a legitimate bug \u2013 either in the new code, or in the existing log replay code.\n\nI've been investigating and it seems like this is intentional, but weird, code on master, see SOLR-9941.\n\nThe net result being that the test as original written (w/o that call to getUpdateLog().deleteAll(); really was finding a problematic situation with log recovery on the branch, because of how the existing DUH2 code tries to pre-emptively apply DBQs during log recovery.  We're either going to need to change things as part of SOLR-9941 first, or deal with out of order DBQs differently as part of this issue, because the current approach of \"re-fetch whole doc from leader\" won't work if the leader (or a single node install) is itself recovering from it's tlog.\n\n\n\nHere's some simple steps to demonstrate the problem as it stands on the jira/solr-5944 branch (as of 5db04fd)...\n\n\nbin/solr -e techproducts\n\ncurl -X POST http://localhost:8983/solr/techproducts/config -H 'Content-Type: application/json' --data-binary '{\"set-property\":{\"updateHandler.autoCommit.maxTime\":\"-1\"}}'\n\ncurl -X POST -H 'Content-type:application/json' --data-binary '{\n  \"add-field\":{\n     \"name\":\"foo_dvo\",\n     \"type\":\"int\",\n     \"stored\":false,\n     \"indexed\":false,\n     \"docValues\":true }\n}' http://localhost:8983/solr/techproducts/schema\n\ncurl -X POST http://localhost:8983/solr/techproducts/update -H 'Content-Type: application/json' --data-binary '{\n  \"add\":{\n    \"doc\": {\n      \"id\": \"DOCX\",\n      \"foo_dvo\": 41\n    }\n   },\n  \"delete\": { \"query\":\"foo_dvo:42\" },\n  \"delete\": { \"query\":\"foo_dvo:43\" },\n  \"add\":{\n     \"doc\": {\n      \"id\": \"DOCX\",\n      \"foo_dvo\": { \"inc\" : \"1\" }\n    }\n  },\n  \"delete\": { \"query\":\"foo_dvo:41\" },\n  \"delete\": { \"query\":\"foo_dvo:43\" },\n  \"add\":{\n    \"doc\": {\n      \"id\": \"DOCX\",\n      \"foo_dvo\": { \"inc\" : \"1\" }\n    }\n  },\n  \"delete\": { \"query\":\"foo_dvo:41\" },\n  \"delete\": { \"query\":\"foo_dvo:42\" }\n}'\n\n# verify the in-place atomic updates were applied correctly...\ncurl 'http://localhost:8983/solr/techproducts/get?wt=json&id=DOCX'\n{\n  \"doc\":\n  {\n    \"id\":\"DOCX\",\n    \"_version_\":1555823554278195200,\n    \"foo_dvo\":43}}\n\n# crash the node...\nkill -9 PID # use whatever PID you get from \"ps -ef | grep start.jar | grep techproducts\"\n\n# restart and let recovery from log replay happen...\nbin/solr start -s example/techproducts/solr/\n\n# because of how the DUH2/UpdateLog code interacts, the doc is deleted during one of the DBQs and the inplace update code can't recover it from anywhere else...\ncurl 'http://localhost:8983/solr/techproducts/get?wt=json&id=DOCX'\n{\n  \"doc\":null}\n\n\n\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15806566",
            "date": "2017-01-07T02:46:35+0000",
            "content": "\n We're either going to need to change things as part of SOLR-9941 first, or deal with out of order DBQs differently as part of this issue\nI have really tried to think of various ways to \"deal with out of order DBQs differently\", but haven't found anything other than the current fetch from leader logic. I've even looked at ways to \"undelete\" a recently DBQ'd document, but that didn't look so promising. There is likely no clean way, in a replica, to retro-actively decide to ditch the partial update and instead do a full update (since that decision has already been taken in the past by the leader, so only going back to the leader for a full update/document can suffice here). Hence, I would think we need to address SOLR-9941. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15824606",
            "date": "2017-01-16T21:26:29+0000",
            "content": "Here's the latest patch from the jira/solr-5944 branch. Steve's jenkins has been running all tests on the branch and they seem to pass fine [0].\n\nThis is very close now, and barring new issues / review comments / suggestions, this patch can be committed.\n\n[0] - http://jenkins.sarowe.net/job/Solr-tests-SOLR-5944/ "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15831170",
            "date": "2017-01-20T04:18:34+0000",
            "content": "There are some new failures on the jira/solr-5944 branch after SOLR-8396 was merged to master. One of the failures is due to SOLR-9996, but there seem to be some other issue as well. I am looking into them.  "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15831619",
            "date": "2017-01-20T12:06:25+0000",
            "content": "Linking another PointsField issue, SOLR-10011, which affects this branch. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15833193",
            "date": "2017-01-21T23:48:11+0000",
            "content": "Updating the patch (from jira/solr-5944 branch). Tested with the PointFields fixes for SOLR-9996 and SOLR-10011, which are both in master now.\n\nUnless someone has any suggestions / objections, I plan to commit this (or perhaps squash merge branch jira/solr-5944) to master soon. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15833574",
            "date": "2017-01-22T16:22:10+0000",
            "content": "Hoss did some initial single threaded benchmarks on the performance impact of doing in-place updates as compared to regular atomic updates.\n\nThe script [0] adds 20K docs containing a stored+indexed text field, a DVO long field, and a stored+indexed long field. It then does 50 iterations of 5K updates to each of the 2 long fields (500K updates total, 250K on the DVO field, 250K on the stored+indexed field), recording the cumulative total times for the udpates to each type of field.\n\n\n\nHere are some results (times are in seconds):\nHoss' run, as of c21d8a005387eae4d0f0adde4de7e4e465fb73c8, on his laptop:\n\n\n\nBranch\nAdds\nDVO Updates\nstored+indexed Updates\n\n\nmaster\n52\n531\n543\n\n\n5944\n51\n352\n503\n\n\n\n\n\nMy run, as of right now:\n\n\n\nBranch\nAdds\nDVO Updates\nstored+indexed Updates\n\n\nmaster\n40\n682\n663\n\n\n5944\n36\n295\n694\n\n\n\n\n\nSeems like Hoss observed a 30% speedup on 5944 branch (in-place update vs. regular update), and I observed a 57% speedup on the same usecase. More benchmarks need to done to analyse the performance in multi-threaded indexing, and also to ensure there's no performance regression for non-dv updates.\n\n [0] - https://gist.github.com/anonymous/ae8bf7db3c713bf9d45937ec0aa1cfae "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15833767",
            "date": "2017-01-23T00:19:34+0000",
            "content": "\nIn this version of the script [0], the exact same 20K documents are added, and the script still does \n50 iterations of 5K updates \u2013 but this time only to the stored+indexed long field...\n\n\n\nHoss' results, as of c21d8a005387eae4d0f0adde4de7e4e465fb73c8, on his laptop:\n\n\n\nBranch\nAdds\nstored+indexed Updates\n\n\nmaster\n49\n521\n\n\n5944\n50\n527\n\n\n\n\n\nMy results:\n\n\n\nBranch\nAdds\nstored+indexed Updates\n\n\nmaster\n37\n603\n\n\n5944\n38\n637\n\n\n\n\n\nSeems like a 1% (Hoss) and 5% (my) slowdown in the regular non-dv updates from master to 5944. Will try to dig deeper to see if this slowdown can be avoided/reduced.\n\n\nEdit: I just did 12 runs (3 on 5944, then 3 on master, then 3 on 5944, then 3 on master).\nHere are the results of the 12 runs, and average per branch:\n\n\n\n\nBranch\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nAvg\n\n\nmaster\n\u00a0\n\u00a0\n\u00a0\n652\n671\n676\n\u00a0\n\u00a0\n\u00a0\n701\n667\n628\n666\n\n\n5944\n656\n661\n676\n\u00a0\n\u00a0\n\u00a0\n656\n652\n664\n\u00a0\n\u00a0\n\u00a0\n661\n\n\n\n\n\nI think these differences are just due to statistical noise, and there is no significant speedup/slowdown for the non-dv updates.\n\n[0] - https://gist.github.com/chatman/c085af9d1ea9d9e2c56d037e0c6ab119 "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15837400",
            "date": "2017-01-25T09:02:12+0000",
            "content": "I did some multithreaded benchmarks on the jira/solr-5944 branch. Here are the two main experiments I performed:\n\nRegular update vs. In-Place updates on branch\n\nFirst add 100,000 documents. Each document contains an numeric id field, a numeric version field, a text field with around 1000 words (generated using lucene-test-framework's TestUtil.randomSimpleString()), a stored+indexed long field (called stored_l) and a non-stored, non-indexed long DV field (called inplace_dvo_l).\n\nThen, there were 10 iterations of 25,000 updates to each of the two long fields. That is, 25k updates to stored_l, then 25k to inplace_dvo_l, and repeat this 10 times. Used a CUSC for sending these updates, using a configurable thread count.\n\nRepeated this with different values of thread count to control the parallelism of requests. Recorded and plotted the cumulative times (in seconds) per field:\n\n\nOnly regular updates: master branch vs. 5944 branch\nTo evaluate any impact to regular updates, I performed the same experiment as above, but with the following change: only update the stored_l field in every iteration. Carried out this experiment on master as well as on jira/solr-5944 branch. (Indexing times are in seconds.)\n\n\nConclusion\n\n\tIt seems the in-place updates are much faster than regular updates, esp. when the document contains text fields. (Hypothesis: speed of in-place updates is not proportional to document size)\n\tIt seems that there is a very slight, but not significant, slowdown for regular updates (master vs branch).\n\n\n\nReproducing these results\nThe solr-upgrade-tests (SOLR-8581) seemed to be easy to extend for these benchmarks. It takes in a git commit sha, checks out the repository, builds a package, starts zookeeper and solr, performs the benchmarks, stops and cleans up.\n\nhttps://github.com/chatman/solr-upgrade-tests/blob/master/BENCHMARKS.md\n\nFor these tests, I used the following commits:\nmaster: ca50e5b61c2d8bfb703169cea2fb0ab20fd24c6b\njira/solr-5944: fcf71e34f20ea74f99933b80d5bd43cd487751f1\n\nFor the second experiment, I passed in an additional parameter -onlyRegularUpdates true.\n\nMy computer setup: Intel Core i7 5820K (6 cores, OC'd to 4.3 GHz), 32GB DDR4 RAM, Samsung 950 Pro NVMe SSD. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15838967",
            "date": "2017-01-26T01:23:40+0000",
            "content": "Commit 5375410807aecf3cc67f82ca1e9ee591f39d0ac7 in lucene-solr's branch refs/heads/master from Ishan Chattopadhyaya\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5375410 ]\n\nSOLR-5944: In-place updates of Numeric DocValues "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15838969",
            "date": "2017-01-26T01:24:44+0000",
            "content": "Planning to backport to 6x after SOLR-8396 is backported. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15839375",
            "date": "2017-01-26T07:34:36+0000",
            "content": "Commit 5375410807aecf3cc67f82ca1e9ee591f39d0ac7 in lucene-solr's branch refs/heads/apiv2 from Ishan Chattopadhyaya\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5375410 ]\n\nSOLR-5944: In-place updates of Numeric DocValues "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15855837",
            "date": "2017-02-07T11:58:24+0000",
            "content": "Commit 3574404e3d3d5758c0d07f73fda72decc97c07d2 in lucene-solr's branch refs/heads/master from Ishan Chattopadhyaya\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=3574404 ]\n\nSOLR-5944: Use SolrCmdDistributor's synchronous mode for in-place updates "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15855877",
            "date": "2017-02-07T12:23:19+0000",
            "content": "Commit 0d760aeddbc95f2de977dcaed781547f2cbf715f in lucene-solr's branch refs/heads/master from Ishan Chattopadhyaya\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0d760ae ]\n\nSOLR-5944: Reverting the previous fix for SolrCmdDistributor "
        },
        {
            "author": "Mano Kovacs",
            "id": "comment-15869724",
            "date": "2017-02-16T10:55:49+0000",
            "content": "Ishan Chattopadhyaya, may I ask if the commit 5375410 (SOLR-5944: In-place updates of Numeric DocValues) will be backported to 6x? I am interested because the patch for SOLR-10114 based on your changes, and I would create a backport if this this one will not be backported. Thanks! "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15869838",
            "date": "2017-02-16T12:36:58+0000",
            "content": "Mano Kovacs, I shall be able to backport this some time mid next week (perhaps by 23rd February). Once that is done, I can backport SOLR-10114. Otherwise, in case you'd like SOLR-10114 to be backported sooner, I can work around it while backporting SOLR-5944. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15873405",
            "date": "2017-02-19T02:20:19+0000",
            "content": "Commit d5ef026f844a22cf56e56c86956b1c2a11a0a751 in lucene-solr's branch refs/heads/master from Ishan Chattopadhyaya\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d5ef026 ]\n\nSOLR-5944: Use SolrCmdDistributor's synchronous mode for in-place updates "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15873445",
            "date": "2017-02-19T03:52:00+0000",
            "content": "Commit 6358afbea66239436a6c0c52e088eeecebac1f65 in lucene-solr's branch refs/heads/master from Ishan Chattopadhyaya\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6358afb ]\n\nSOLR-5944: Cleanup comments and logging, use NoMergePolicy instead of LogDocMergePolicy "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15873545",
            "date": "2017-02-19T08:34:52+0000",
            "content": "Commit e9d3bdd02ada23ad09bcc6fc7ff3661880dd45bc in lucene-solr's branch refs/heads/branch_6x from Ishan Chattopadhyaya\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=e9d3bdd ]\n\nSOLR-5944: In-place updates of Numeric DocValues\n\n Conflicts:\n\tsolr/CHANGES.txt\n\tsolr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java\n\tsolr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java\n\tsolr/core/src/test-files/solr/collection1/conf/schema.xml\n\tsolr/core/src/test/org/apache/solr/cloud/TestSegmentSorting.java "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15873548",
            "date": "2017-02-19T08:35:05+0000",
            "content": "Commit 4de140bf8b5e621ad70a07cb272ddc7135346eaf in lucene-solr's branch refs/heads/branch_6x from Ishan Chattopadhyaya\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=4de140b ]\n\nSOLR-5944: Use SolrCmdDistributor's synchronous mode for in-place updates "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15873549",
            "date": "2017-02-19T08:35:08+0000",
            "content": "Commit 476cea57e8e55832878c3b4c8efe1cf6f113b3c4 in lucene-solr's branch refs/heads/branch_6x from Ishan Chattopadhyaya\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=476cea5 ]\n\nSOLR-5944: Cleanup comments and logging, use NoMergePolicy instead of LogDocMergePolicy\n\n Conflicts:\n\tsolr/core/src/java/org/apache/solr/update/DirectUpdateHandler2.java "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15873553",
            "date": "2017-02-19T08:35:23+0000",
            "content": "Commit 0689b2a7fd05c86c7fc2f1d1adffdd631d671ba1 in lucene-solr's branch refs/heads/branch_6x from Ishan Chattopadhyaya\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0689b2a ]\n\nSOLR-5944: Suppress PointFields for TestSegmentSorting "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15873561",
            "date": "2017-02-19T08:42:53+0000",
            "content": "Thanks everyone! "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15873575",
            "date": "2017-02-19T09:00:51+0000",
            "content": "Commit ea19bf5101817bae5b7b133a7d9d40ab41aac6ec in lucene-solr's branch refs/heads/master from Ishan Chattopadhyaya\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ea19bf5 ]\n\nMove solr/CHANGES.txt entries to appropriate sections after backporting SOLR-5944 and SOLR-10114 "
        },
        {
            "author": "Varun Thacker",
            "id": "comment-15874986",
            "date": "2017-02-20T19:33:58+0000",
            "content": "Hi Ishan,\n\nWhile working on integrating points in the collapse query parser on SOLR-9994 , I was looking at how the existing tests were randomizing points /trie fields.\n\nMost tests use the \"solr.tests.intClass\" system property including how it's randomized in SolrTestCaseJ4\n\n In this feature we use the \"solr.tests.intClassName\" system property. Although it means the same thing a small improvement here will be to use the same property names to make our tests more consistent?\n\nI think we just need to rename these fields in schema-inplace-updates.xml and it's usages?\n\n\n  <fieldType name=\"string\" class=\"solr.StrField\" multiValued=\"false\" indexed=\"false\" stored=\"false\" docValues=\"false\" />\n  <fieldType name=\"long\" class=\"solr.${solr.tests.longClassName}\" multiValued=\"false\" indexed=\"false\" stored=\"false\" docValues=\"false\"/>\n  <fieldType name=\"float\" class=\"solr.${solr.tests.floatClassName}\" multiValued=\"false\" indexed=\"false\" stored=\"false\" docValues=\"false\"/>\n  <fieldType name=\"int\" class=\"solr.${solr.tests.intClassName}\" multiValued=\"false\" indexed=\"false\" stored=\"false\" docValues=\"false\"/>\n\n "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15875108",
            "date": "2017-02-20T22:35:11+0000",
            "content": "The other usages of were on a per field definition, whereas I wanted to use it on a per fieldType definition.\n\nThe other usages, e.g.\n\n<dynamicField name=\"*_fd\" type=\"${solr.tests.floatClass:pfloat}\" indexed=\"true\" stored=\"false\" docValues=\"true\"/>\n\n\nseemed like mis-named. \"pfloat\" here refers to a named fieldType definition, and not a \"floatClass\". Hence, I chose the other convention of \"FloatPointField\" being referred to as \"solr.tests.floatClassName\".\n\nTo standardize, I think we should rename these two as: (a) \"solr.tests.floatFieldType\" as pfloat or float, when used on a per field basis, or (b) \"solr.tests.floatClassName\" as FloatPointField, when used in a fieldType definition.\n\nWDYT? Added SOLR-10177. "
        }
    ]
}