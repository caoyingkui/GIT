{
    "id": "LUCENE-2504",
    "title": "sorting performance regression",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/search"
        ],
        "type": "Bug",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "affect_versions": "4.0-ALPHA",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "sorting can be much slower on trunk than branch_3x",
    "attachments": {
        "LUCENE-2504.patch": "https://issues.apache.org/jira/secure/attachment/12453588/LUCENE-2504.patch",
        "LUCENE-2504_SortMissingLast.patch": "https://issues.apache.org/jira/secure/attachment/12454593/LUCENE-2504_SortMissingLast.patch",
        "LUCENE-2504.zip": "https://issues.apache.org/jira/secure/attachment/12449178/LUCENE-2504.zip"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-06-19T19:57:58+0000",
            "content": "I'll dig. ",
            "author": "Michael McCandless",
            "id": "comment-12880539"
        },
        {
            "date": "2010-06-19T20:05:57+0000",
            "content": "My guess is that this is caused by LUCENE-2380, but I opened a separate issue since I'm not sure.\nThis is the same type of JVM performance issues reported by Mike in LUCENE-2143 and myself in LUCENE-2380.\n\nSetup:\n  Same test index I used to test faceting: 10M doc index with 5 fields:\n\n\tf100000_s:  a single valued string field with 100,000 unique values\n\tf10000_s:   a single valued field with 10,000 unique values\n\tf1000_s:   a single valued field with 1000 unique values\n\tf100_s:   a single valued field with 100 unique values\n\tf10_s:   a single valued field with 10 unique values\n\n\n\nURLs I tested against Solr are of the form:\nhttp://localhost:8983/solr/select?q=*:*&rows=1&sort=f100000_s+asc\n\nbranch_3x\n----------------------------------------------------------\n f100000_s sort only: 101 ms\nsort against random field: 101 ms\n\ntrunk:\n----------------------------------------------------------\n f100000_s sort only: 111 ms\nsort against random field: 158 ms\n\nThis is not due to garbage collection or cache effects.  After you sort against a mix of fields, the performance is worse forever... you can go back to sorting against  f100000_s only, and the performance never recovers.\n\nSystem: Ubuntu on Phenom II 4x3.0GHz, Java 1.6_20\n\nSo my guess is that this is caused by the ord lookup going through PagedBytes, and the JVM not optimizing away the indirection when there is a mix of implementations. ",
            "author": "Yonik Seeley",
            "id": "comment-12880540"
        },
        {
            "date": "2010-06-19T20:11:50+0000",
            "content": "More numbers:  Ubuntu, Java 1.7.0-ea-b98 (64 bit):\nf100000_s sort only: 126 ms\nsort against random field: 175 ms ",
            "author": "Yonik Seeley",
            "id": "comment-12880541"
        },
        {
            "date": "2010-06-19T20:18:22+0000",
            "content": "More numbers: Windows 7:\njava version \"1.6.0_17\"\nJava(TM) SE Runtime Environment (build 1.6.0_17-b04)\nJava HotSpot(TM) 64-Bit Server VM (build 14.3-b01, mixed mode)\n\nf100000_s sort only: 115 ms\nsort against random field: 162 ms  ",
            "author": "Yonik Seeley",
            "id": "comment-12880542"
        },
        {
            "date": "2010-06-22T15:16:15+0000",
            "content": "Hmmm, the way FieldComparator / FieldComparatorSource work now, it doesn't seem possible to specialize based on the underlying native array type.  In order to do this, a new FieldComparator would need to be returned for each segment.\n\nOne possibility: modify setNextReader to return a FieldComparator? ",
            "author": "Yonik Seeley",
            "id": "comment-12881225"
        },
        {
            "date": "2010-06-22T17:09:44+0000",
            "content": "One possibility: modify setNextReader to return a FieldComparator?\n\nDo you mean Collector's setNextReader()? That doesn't make sense. Most Collectors don't deal w/ FieldComparators at all ... ",
            "author": "Shai Erera",
            "id": "comment-12881251"
        },
        {
            "date": "2010-06-22T18:13:08+0000",
            "content": "Do you mean Collector's setNextReader()? \nNo, FieldComparator.setNextReader() ",
            "author": "Yonik Seeley",
            "id": "comment-12881277"
        },
        {
            "date": "2010-06-22T18:20:54+0000",
            "content": "Ahh ok. That makes sense then . ",
            "author": "Shai Erera",
            "id": "comment-12881281"
        },
        {
            "date": "2010-07-10T23:17:18+0000",
            "content": "\nDigging into this, finally...\n\nTo try to make a somewhat more realistic search test, I created a\nstandalone test (attached zip file), which runs different query types\n(term, phrase, OR of 2 terms, AND of 2 terms, prefix, phrase) sorting\nby score or by a string field (with increasing numbers of unique\nvalues: country (~250 values I think), and then\nunique10/100/1K/10K/100K/1M)).  I derive the unique fields by taking\nfirst N unique titles from wikipedia; the country field comes from the\nSortableSingleDocSource in contrib/benchmark.\n\nIt runs with 2 threads (machine has 2 cores), and each thread first\nshuffles the queries privately but deterministically, so that each\nmatching thread in the trunk & 3x tests are running query+sort in same\norder.\n\nI then created a Wikipedia index with first 5M docs, one optimized and\none not optimized (13 segments) and with 5% docs deleted, on trunk and\n3x.\n\nI sweep through all query+sorts 23 times (getting top 10 hits for\neach), using 2 threads, measuring wall clock time each time.  I\ndiscard first 3 results for each query+sort, and then take fastest\ntime of the remaining 20.\n\nJava is 1.6.0_17; I run with -server -Xmx1g -Xms1g (machine has 3G\nRAM); OS is Linux CentOS 5.5.\n\nNOTE: these results include the patch from LUCENE-2504, for both\ntrunk & 3.x!\n\nResults (pctg change in query time, going from 3x -> trunk) on\noptimized index:\n\nResults on optimized index:\n\n\n\n\nQuery\ncountry\nunique10\nunique100\nunique1K\nunique10K\nunique100K\nunique1M\nscore\n\n\n<all>\n40.5%\n40.6%\n41.0%\n40.5%\n40.7%\n1.6%\n1.8%\n2.8%\n\n\n+united +states\n6.1%\n6.0%\n6.0%\n6.6%\n6.3%\n0.4%\n1.4%\n1.7%\n\n\n\"united states\"\n8.4%\n8.5%\n8.2%\n8.1%\n8.1%\n9.2%\n9.3%\n8.7%\n\n\nstates\n20.3%\n20.4%\n20.9%\n22.5%\n22.5%\n8.0%\n8.1%\n0.1%\n\n\nunite*\n8.1%\n8.3%\n8.3%\n8.6%\n9.0%\n2.8%\n0.8%\n1.2%\n\n\nunited states\n1.3%\n1.9%\n2.5%\n1.8%\n2.2%\n2.3%\n1.3%\n2.2%\n\n\n\n\n\nResults on unoptimized index (w/ 5% deletions):\n\n\n\n\nQuery\ncountry\nunique10\nunique100\nunique1K\nunique10K\nunique100K\nunique1M\nscore\n\n\n<all>\n25.1%\n25.8%\n24.9%\n27.2%\n26.3%\n27.4%\n27.3%\n1.4%\n\n\n+united +states\n7.8%\n7.6%\n7.5%\n7.8%\n7.6%\n8.6%\n8.9%\n6.5%\n\n\n\"united states\"\n13.4%\n13.7%\n13.6%\n13.8%\n13.4%\n14.1%\n13.6%\n14.8%\n\n\nstates\n13.6%\n14.3%\n14.2%\n15.5%\n15.5%\n18.6%\n18.8%\n1.7%\n\n\nunite*\n5.8%\n5.3%\n5.0%\n5.7%\n5.3%\n6.9%\n6.9%\n2.4%\n\n\nunited states\n2.3%\n2.6%\n1.4%\n1.9%\n2.5%\n4.9%\n6.6%\n0.1%\n\n\n\n\n\nUnfortunately, the tests have highish variance (up to maybe +/- 10%),\nI think thanks to hotspot's unpredictability (\"java ghosts\").  EG if I\nchange the order in which the queries are run, the results change\nquite a bit.  If I run the exact same test, results change alot.  This\nof course makes conclusions nearly impossible... but still some rough\nobservations:\n\n\n\tTrunk is definitely slower when sorting by field; sorting by\n    score is roughly the same perf.\n\n\n\n\n\tFor some reason, the unoptimized index generally takes less perf\n    hit than the optimized index... odd.\n\n\n\n\n\tCurious that phrase query is faster across the board... not sure\n    why.  Maybe my recent optos to PhraseQuery somehow favor flex?\n\n\n\n\n\tPerf loss is in proportion to how \"easy\" the query is\n    (AllDocsQuery is the worst; TermQuery next), which makes sense\n    since the slowdown is in collection.\n\n\n\nEven though the results are noisy... I still think we should try to\nspecialize direct access to the native array for doc->ord lookup.\nI'll work on that next... ",
            "author": "Michael McCandless",
            "id": "comment-12887111"
        },
        {
            "date": "2010-09-01T12:20:37+0000",
            "content": "OK I implemented Yonik's suggestion here: the comparator may now\nreturn a new segment-specific FieldComparator on each call to\n.setNextReader.  I fixed all FieldComparators to simply \"return this\",\nexcept for the TermOrdValComparator which returns a comparator\nspecialized to the bit-width of the packed ints doc->ord mapping for\nthe fixed-array (8, 16, 32) cases.\n\nThis is all quite silly: we are only doing this to \"game\" hotspot into\nproperly inlining/compiling what is in fact an array lookup, just\ncurrently hidden behind method calls in the packed ints impls.  We\nreally \"shouldn't have to\" do this custom source code specialization.\n\nAnd, I think a more general framework for source-code specialization\nis a cleaner way to minimize hotspot unpredictability (LUCENE-1594),\nin the future.  Maybe once we cutover to that, we can remove these\ncases of custom specialization in Lucene's core (the 12 private\ninner Collector impls in TopFieldCollector is another example).\n\nHere are the results, comparing 3.x perf to trunk w/ the attached\npatch \u2013 all runs include the pending [separate] fix on LUCENE-2631:\n\nOptimized index:\n\n\n\n\nQuery\ncountry\nunique10\nunique100\nunique1K\nunique10K\nunique100K\nunique1M\nscore\n\n\n<all>\n8.5%\n8.5%\n8.4%\n8.7%\n8.7%\n8.4%\n9.4%\n10.7%\n\n\n+united +states\n1.8%\n0.6%\n0.3%\n0.4%\n0.9%\n0.7%\n2.1%\n2.9%\n\n\n\"united states\"\n5.2%\n5.5%\n5.7%\n5.2%\n5.2%\n4.8%\n6.9%\n7.1%\n\n\nstates\n4.6%\n4.8%\n4.1%\n5.2%\n5.1%\n7.0%\n3.8%\n1.8%\n\n\nunite*\n2.0%\n1.7%\n3.0%\n2.6%\n2.4%\n5.7%\n6.0%\n3.0%\n\n\nunited states\n0.5%\n0.4%\n2.8%\n2.6%\n3.1%\n2.1%\n1.1%\n2.0%\n\n\n\n\n\n\nMulti-segment index (5% deletions):\n\n\n\n\nQuery\ncountry\nunique10\nunique100\nunique1K\nunique10K\nunique100K\nunique1M\nscore\n\n\n<all>\n10.0%\n10.2%\n10.1%\n9.4%\n9.4%\n10.1%\n10.0%\n5.1%\n\n\n+united +states\n7.2%\n7.5%\n7.7%\n8.5%\n8.4%\n7.1%\n5.4%\n1.9%\n\n\n\"united states\"\n4.5%\n4.2%\n4.0%\n3.8%\n4.5%\n4.3%\n3.7%\n4.2%\n\n\nstates\n6.5%\n8.6%\n7.3%\n6.9%\n7.5%\n9.4%\n9.9%\n1.3%\n\n\nunite*\n4.5%\n5.3%\n4.3%\n3.9%\n4.5%\n4.7%\n4.7%\n0.4%\n\n\nunited states\n4.6%\n2.4%\n3.2%\n3.4%\n1.9%\n4.8%\n3.3%\n1.9%\n\n\n\n\n\nSo... this fix does make up much of the difference; we still seem to\nbe a bit (single digits) slower, but, I think this is acceptable given\nthe massive reduction in RAM required for the FieldCache entry. ",
            "author": "Michael McCandless",
            "id": "comment-12905008"
        },
        {
            "date": "2010-09-11T00:16:23+0000",
            "content": "Great!  Hopefully I'll get a change to test it out shortly on my system too.\nI just committed 2 places that the new returned comparator was ignored. ",
            "author": "Yonik Seeley",
            "id": "comment-12908251"
        },
        {
            "date": "2010-09-11T00:45:05+0000",
            "content": "I just committed 2 places that the new returned comparator was ignored.\n\nUrgh \u2013 thanks! ",
            "author": "Michael McCandless",
            "id": "comment-12908255"
        },
        {
            "date": "2010-09-11T16:29:16+0000",
            "content": "This is all quite silly: we are only doing this to \"game\" hotspot into properly inlining/compiling what is in fact an array lookup, just currently hidden behind method calls in the packed ints impls. We really \"shouldn't have to\" do this custom source code specialization.\n\nYeah, but this is the way hotspot currently works, and I don't know if there are any plans to change it.\nHotspot can be pretty aggressive at inlining, but then it deoptimizes when it turns out that the inline is no longer valid (because of a different implementation).\n\nIt's something worth keeping in mind for the rest of Lucene too - bothin benchmarking and design. Multiple implementations used from a single spot will not be inlined (if multiple implementations are actually used in the same run). ",
            "author": "Yonik Seeley",
            "id": "comment-12908336"
        },
        {
            "date": "2010-09-11T19:25:22+0000",
            "content": "I'm still seeing bad degredations in solr - I think it's because the default way for solr to sort strings is with MissingLastOrdComparator, which isn't specialized. I'll try and work up a patch based on Mike's work. ",
            "author": "Yonik Seeley",
            "id": "comment-12908357"
        },
        {
            "date": "2010-09-12T18:25:23+0000",
            "content": "Hmmm, turns out the sorting bug I just checked in (r996332) has been around a bit longer than I thought - since field cache was converted to bytes on 6/3/2010 (LUCENE-2380).\n\nSo anyone using trunk since then and sorting on string fields with null values may want to upgrade. ",
            "author": "Yonik Seeley",
            "id": "comment-12908519"
        },
        {
            "date": "2010-09-13T18:37:02+0000",
            "content": "OK, here's  a patch for solr's sort missing last.\nMedian response time in my tests drops from 160 to 102 ms. ",
            "author": "Yonik Seeley",
            "id": "comment-12908909"
        },
        {
            "date": "2010-09-13T18:48:23+0000",
            "content": "silly question, what does the bigString do?\n\n(just wondering if it should be U+10FFFF,U+10FFFF,... now that we use utf-8 order, depending what it does) ",
            "author": "Robert Muir",
            "id": "comment-12908918"
        },
        {
            "date": "2010-09-13T19:08:13+0000",
            "content": "silly question, what does the bigString do? \n\nIt's actually not currently used by Solr.... but it's basically to use as a proxy for a null if you want the Comparables returned by value() to match the sort order the Comparator actually used.\n\n(just wondering if it should be U+10FFFF,U+10FFFF,... now that we use utf-8 order, depending what it does)\n\nMaybe... if it is supposed to be just a string (I know that's the name, but maybe it should be called bigTerm I guess).  All of our terms are currently UTF8 - but I don't know if that will last? ",
            "author": "Yonik Seeley",
            "id": "comment-12908934"
        },
        {
            "date": "2010-09-13T19:17:36+0000",
            "content": "Maybe... if it is supposed to be just a string (I know that's the name, but maybe it should be called bigTerm I guess). All of our terms are currently UTF8 - but I don't know if that will last?\n\nwell you are right, for example collated terms for Locale-sensitive sort will hopefully use full byte range soon... \n\nwe can always safely use bytes of 0xff i think? ",
            "author": "Robert Muir",
            "id": "comment-12908947"
        },
        {
            "date": "2010-09-13T19:23:47+0000",
            "content": "we can always safely use bytes of 0xff i think?\n\nYep, should be fine.  Is there an upper bound on how long collated terms can be? ",
            "author": "Yonik Seeley",
            "id": "comment-12908953"
        },
        {
            "date": "2010-09-13T19:29:17+0000",
            "content": "Yep, should be fine. Is there an upper bound on how long collated terms can be?\n\nThere isn't, but...\n\nI can't promise (but i'll verify), i think actually a single 0xff might do, for the major encodings.\n\n\n\tits invalid in utf-8\n\tits technically valid, but unused (reset byte) in bocu-1\n\tcollation keys i understand are a modified bocu, likely unused there too.\n\n\n\nso its like a NaN sentinel, if someone is doing something very wierd, maybe it wont work,\nbut in general, i think it will work. ill check. ",
            "author": "Robert Muir",
            "id": "comment-12908956"
        },
        {
            "date": "2010-09-13T19:43:06+0000",
            "content": "OK, I've changed bigString to bigTerm and used 10 0xff bytes (to account for possible binary encoding of 8 byte numerics + other stuff like tags that trie encoding uses). ",
            "author": "Yonik Seeley",
            "id": "comment-12908963"
        },
        {
            "date": "2010-09-13T19:49:53+0000",
            "content": "Cool thanks (unfortunately i ran a bunch of collators and encountered what looks like 0xff bytes)\nI think this will help. ",
            "author": "Robert Muir",
            "id": "comment-12908965"
        },
        {
            "date": "2010-09-14T02:54:35+0000",
            "content": "Yonik, just curious, how do you know what HotSpot is doing?  Empirically based on performance numbers?  HotSpot code or documentation that spells out exactly when it inlines?  Or is there some tool or diagnostic capability to learn this information? ",
            "author": "David Smiley",
            "id": "comment-12909098"
        },
        {
            "date": "2010-09-14T04:04:27+0000",
            "content": "Yonik, just curious, how do you know what HotSpot is doing? Empirically based on performance numbers?\n\nYeah - it's a best guess based on what I see when performance testing, and matching that up with what I've read in the past.\nAs far as deoptmization is concerned, it's mentioned here: http://java.sun.com/products/hotspot/whitepaper.html, but I haven't read much elsewhere.\n\nSpecific to this issue, the whole optimization/deoptimization issue is extremely complex.\nRecall that I reported this: \"Median response time in my tests drops from 160 to 102 ms.\"\n\nFor simplicity, there are some details I left out:\nThose numbers were for randomly sorting on different fields (hopefully the most realistic scenario).\nIf you test differently, the results are far different.\n\nThe first and second test runs measured median time sorting on a single field 100 times in a row, then moving to the next field.\n\nTrunk before patch:\n\n\n\nunique terms in field\nmedian sort time in ms (first run)\nsecond run\n\n\n100000\n105\n168\n\n\n10000\n105\n169\n\n\n1000\n106\n164\n\n\n100\n127\n163\n\n\n10\n165\n197\n\n\n\n\n\nTrunk after patch:\n\n\n\nunique terms in field\nmedian sort time in ms (first run)\nsecond run\n\n\n100000\n85\n130\n\n\n10000\n92\n129\n\n\n1000\n92\n126\n\n\n100\n116\n127\n\n\n10\n117\n128\n\n\n\n\n\nbranch_3x\n\n\n\nunique terms in field\nmedian sort time in ms (first run)\nsecond run\n\n\n100000\n102\n102\n\n\n10000\n102\n103\n\n\n1000\n101\n103\n\n\n100\n103\n103\n\n\n10\n118\n118\n\n\n\n\n\nSo, it seems by running in batches (sorting on the same field over and over), we cause hotspot to overspecialize somehow, and then when we switch things up the resulting deoptimization puts us in a permanently worse condition).  branch_3x does not suffer from that, but trunk still does due to the increased amount of indirection.  I imagine the differences are also due to the boundaries that the compiler tries to inline/specialize for a certain class.\n\nIt certainly complicates performance testing, and we need to keep a sharp eye on how we actually test potential improvements. ",
            "author": "Yonik Seeley",
            "id": "comment-12909107"
        },
        {
            "date": "2010-09-14T10:19:52+0000",
            "content": "The fickleness of the hotspot compiler is just awful, and, frankely\nunacceptable.  Java (Oracle) really needs to do something to address\nthis.\n\nEG, see my post here:\n\n  http://forums.sun.com/thread.jspa?threadID=5420631  \n\nIn that standalone test, I can get drastically different search\nperformance depending on what code runs first.  Hotspot gets itself\ninto a state where it's \"stuck\" and is not able to re-optimize for the\ncode that's running.  When I disassembled the methods hotspot had\ncompiled, one thing I found was that readVInt (the hottest of hot in\nLucene today) was compiled very differently depending on what code ran\nfirst!\n\nThe changes we've had to make to Lucene/Solr in this issue to\nworkaround hotspot are here are horrible \u2013 we've introduced ugly code\ndup specializations so that hotspot properly detects given method\ncalls are in fact just an array lookup.  We've made similar\nspecializations elsewhere in Lucene...\n\nWeirdly, I've found that running java with -Xbatch gives far more\nrepeatable results.  This is bizarre because that option forces\ncompilation to run in the foreground; it's not supposed to alter which\nmethods hotspot chooses to optimize, and, how much (I think?).  Though\nmaybe because threads are paused awaiting compilation it alters\nhotspots targets?  However, -Xbatch doesn't always give the fastest\nresults.\n\nNot that we have a choice here... but I've often wondered whether .NET\nhas this same hotspot fickleness problem.\n\nI think this is a severe and growing problem for Lucene going forward\n\u2013 our search performance is crucial and we can't risk hotspot\nrandomly, substantially slowing things down by alot.  We're unable\nto do true performance tuning when hospot \"noise\" easily dwarfs the\neffects we're trying to measure.\n\nI think the only viable option going forward is to create a search\nframework that's able to generate its own specialized java code.  We'd\nuse this, statically, to generate pieces of the search executation\npath that we think are common enough to warrent up-front specialization,\nbut also expose it dynamically so apps can \"optimize\" for their query\npaths, either statically (pre-built/compiled in their apps) or\ndynamically (like how a JSP rewrites to java code and is then\ncompiled).  Of course we'd still retain the non-specialized code, as a\nfallback to handle those cases the specializer can't yet cover, or,\nfor apps where the net bytecode must be kept smallish.\n\nIn theory such a search autogen framework could also generate into\nC/C++, enabling us to choose a good point to wrap the result with JNI\n(eg, TopDocsCollector.topDocs), which'd be wonderful as it'd fully\nsidestep the hotspot fickleness. ",
            "author": "Michael McCandless",
            "id": "comment-12909177"
        },
        {
            "date": "2010-09-14T12:25:19+0000",
            "content": "\nJava (Oracle) really needs to do something to address this.\n\nI think we all owe it to ourselves to stop equating java with Sun/Oracle, if Java \nstays with Oracle its pretty obvious the language (is) will die anyway.\n\n\nI think this is a severe and growing problem for Lucene going forward\n\n\tour search performance is crucial and we can't risk hotspot\nrandomly, substantially slowing things down by alot.\n\n\n\nWhile I agree at the moment we should make efforts to work around issues like this,\nI don't think we should jump the gun and make real design/architectural\nchoices based on Oracle bugs.\n\nEspecially for trunk, by the time we release Lucene 4.0 some other company\nwill probably \"own\" Java anyway.\n\n\nNot that we have a choice here... but I've often wondered whether .NET\nhas this same hotspot fickleness problem\n\n.NET is not a choice but generating C/C++ code is?\n ",
            "author": "Robert Muir",
            "id": "comment-12909214"
        },
        {
            "date": "2010-09-14T13:22:37+0000",
            "content": "I think we all owe it to ourselves to stop equating java with Sun/Oracle, if Java stays with Oracle its pretty obvious the language (is) will die anyway.\nI agree with robert that we should stop comparing against sun jvms all the time and turn everything upside-down specializing code here and there or go one step further and generate C++ code. Dude who is gonna maintain the compatibility to Java-Only environments? I could imagine that we have something which is super special purpose like mike did with DirectNIOFSDirectory  to work around unexposed methods like fadvice. \n\nI think that code specializations of very \"hot\" part of lucene are ok and we should follow that way like we did at some places but it already make things very complicated to follow. Without the knowledge of a committer or a person actively following that development it is extremely difficult to comprehend design decisions.\n\nI would rather like the idea to put effort in stuff like harmony and make code we can control perform better that introducing a preprocessor which generates code for a JVM owned by a company. Would it make way more sense to push OSS JVMs than spending lots of time on investigating on .NET as an alternative or C/C++ code generator? Before I would go the C++ path I'd rather use Java to host a C core like lucy which brings you as close as it gets to the machine. \n\nEG, see my post here:\n\ninteresting papers - seems we are touching the limits of Java though.\n ",
            "author": "Simon Willnauer",
            "id": "comment-12909230"
        },
        {
            "date": "2010-09-14T14:50:04+0000",
            "content": "Looks like we're not using the correct comparators everywhere.\nI was trying a slightly different way to implement sort-missing-last, and my first comparator only implements setNextReader(), but I'm now getting many UnsupportedOperationExceptions (i.e. the search process is using older comparators after calling setNextReader())\n\nOne culprit is OneComparatorNonScoringCollector, and another is OneComparatorFieldValueHitQueue I think. ",
            "author": "Yonik Seeley",
            "id": "comment-12909272"
        },
        {
            "date": "2010-09-14T16:32:21+0000",
            "content": "I'm now getting many UnsupportedOperationExceptions (i.e. the search process is using older comparators after calling setNextReader())\n\nThat's no good!\n\nOne culprit is OneComparatorNonScoringCollector, and another is OneComparatorFieldValueHitQueue I think.\n\nHmm I don't see the problem \u2013 eg OneComparatorNonScoringCollector saves the returned comparator from comparator.setNextReader.\n\nCan you post the full exc? ",
            "author": "Michael McCandless",
            "id": "comment-12909312"
        },
        {
            "date": "2010-09-14T16:44:09+0000",
            "content": "Attaching a draft patch that seems to fix the issue (the ones I can find at least).\n\nHmm I don't see the problem - eg OneComparatorNonScoringCollector saves the returned comparator from comparator.setNextReader.\n\nYes, but FieldValueHitQueue has it's own list of comparators that never get updated. ",
            "author": "Yonik Seeley",
            "id": "comment-12909319"
        },
        {
            "date": "2010-09-14T17:10:58+0000",
            "content": "\nI think we all owe it to ourselves to stop equating java with Oracle, if Java \nstays with Oracle its pretty obvious the language (is) will die anyway.\n\nYeah I agree.\n\nThe open question is whether this hotspot fickleness is particular to\nOracle's java impl, or, is somehow endemic to bytecode VMs (.NET\nincluded).  It's really a hard, complex problem (JIT compilation from\nbytecode based on runtime data), so it wouldn't surprise me if it's\nthe latter, to varying degrees.\n\n.NET is not a choice but generating C/C++ code is?\n\nAs far as I know it's much easier to invoke C/C++ from java, than .NET\nfrom java.  C/C++ is also more portable than .NET, I think?  (There is\nMono \u2013 how mature is it by now?).\n\n\nI don't think we should jump the gun and make real design/architectural\nchoices based on Oracle bugs.\n\nI expect source code spec will also buy sizable perf gains\nirrespective of hotspot fickleness, and in non-Oracle java impls.\nGenerating a dedicated class, with one method doing all searching and\ncollecting, removes all kinds of barriers to the JIT compiler.  It\nmakes its job far easier.\n\nI agree with robert that we should stop comparing against sun jvms all the time and turn everything upside-down specializing code here and there or go one step further and generate C++ code. Dude who is gonna maintain the compatibility to Java-Only environments?\n\nIf we manage to pursue specialized code gen, it'll be a loooong time\ncoming!  My point about C/C++ is that if we do somehow manage to get a\nworking code gen framework online (for Java), the added cost to make\nit also target C/C++ will be \"relatively\" small.  Ie, it's nearly \"for\nfree\".\n\nIf we were do to this, that would not mean we'd abandon java, of\ncourse \u2013 the framework would fully support \"pure java\" as well.\n\nI think that code specializations of very \"hot\" part of lucene are ok and we should follow that way like we did at some places but it already make things very complicated to follow. \n\nYou mean manual specialization right (like this issue)?\n\nYes, I think we will have to keep manually specializing, going\nforward, until we can have code generator that\ndoes it more cleanly...\n\nWould it make way more sense to push OSS JVMs than spending lots of time on investigating on .NET as an alternative or C/C++ code generator?\n\nI think we should do both.\n\nBefore I would go the C++ path I'd rather use Java to host a C core like lucy which brings you as close as it gets to the machine.\n\nI think this (a Java wrapper for Lucy) is a great idea \u2013 we should explore that, too.\n\ninteresting papers - seems we are touching the limits of Java though.\n\nWell that's the big question \u2013 limits of Java or limit's of Sun/Oracle's impl.\n\nIt looks like harmony has a ways to go on absolute performance: I just\nran a very quick benchmark (TermQuery search on 10 M multi-segment\nwiki index w/ a 50% random filter) and Oracle java 1.6.0_21 gets 15.6\nQPS while Harmony 1.5.0-r946978 gets 9.5 QPS (Harmony 1.6.0-r946981\nalso gets 9.5 QPS).  I just ran java -server -Xms2g -Xmx2g; it's\npossible by tuning Harmony (it has many awesome looking command-line\nargs!) it'd get faster... ",
            "author": "Michael McCandless",
            "id": "comment-12909337"
        },
        {
            "date": "2010-09-14T17:13:29+0000",
            "content": "Yes, but FieldValueHitQueue has it's own list of comparators that never get updated.\n\nUgh, yes. ",
            "author": "Michael McCandless",
            "id": "comment-12909339"
        },
        {
            "date": "2010-09-14T19:40:45+0000",
            "content": "The open question is whether this hotspot fickleness is particular to Oracle's java impl, or, is somehow endemic to bytecode VMs (.NET included).\n\nI tried IBM's latest Java6 (SR8 FP1, 20100624)\nIt seems to have some of the same pitfalls as Oracle's JVM, just different.\nThe first run does not differ from the second run in the same JVM as it does with Oracle, but the first run itself has much more variation.  The worst case is worse, and just like the Oracle JVM, it gets stuck in it's worst case.\n\nEach run (of the complete set of fields) in a separate JVM since two runs in the same JVM didn't really differ as they did in the oracle JVM.\n\n\nbranch_3x:\n\n\n\nunique terms in field\nmedian sort time of 100 sorts in ms\nanother run\nanother run\nanother run\nanother run\nanother run\nanother run\n\n\n100000\n129\n128\n130\n109\n98\n128\n135\n\n\n10000\n128\n123\n127\n127\n98\n128\n135\n\n\n1000\n129\n130\n130\n128\n98\n130\n136\n\n\n100\n128\n133\n133\n130\n100\n132\n139\n\n\n10\n150\n153\n153\n154\n122\n153\n159\n\n\n\n\n\ntrunk:\n\n\n\nunique terms in field\nmedian sort time of 100 sorts in ms\nanother run\nanother run\nanother run\nanother run\nanother run\nanother run\n\n\n100000\n217\n81\n383\n99\n79\n78\n215\n\n\n10000\n254\n73\n346\n101\n106\n108\n267\n\n\n1000\n253\n74\n347\n99\n107\n108\n258\n\n\n100\n253\n107\n394\n98\n107\n102\n255\n\n\n10\n251\n107\n388\n99\n106\n98\n257\n\n\n\n\n\nThe second way of testing is to completely mix fields (no serial correlation between what field is sorted on).  This is the test that is very predictable with the Oracle JVM, but I still see wide variability with the IBM JVM.  Here is the list of different runs for the IBM JVM (ms):\n\nbranch_3x\n\n\n\n128\n129\n123\n120\n128\n100\n95\n74\n130\n91\n120\n\n\n\n\n\ntrunk\n\n\n\n106\n89\n168\n116\n155\n119\n108\n118\n112\n169\n165\n\n\n\n\n\nTo my eye, it looks like we have more variability in trunk, due to increased use of abstractions?\n\nedit: corrected the table description - all times in this message are for the IBM JVM. ",
            "author": "Yonik Seeley",
            "id": "comment-12909407"
        },
        {
            "date": "2010-09-14T21:09:02+0000",
            "content": "OK, I've committed the fix to always use the latest generation field comparator.\nNot sure if this is the best way to handle - but at least it's correct now and we can improve more later. ",
            "author": "Yonik Seeley",
            "id": "comment-12909456"
        },
        {
            "date": "2010-09-14T21:26:07+0000",
            "content": "This was a simple attempt to try and simplify the comparators.   Static classes are used instead of inner classes.  Unfortunately, it didn't help the JVMs from getting stuck in badly optimized code (it was a long shot for that), but it does result in a consistent 4% speedup.\n\nIt looks as simple as the previous version to my eye, so I'll commit if there are no objections. ",
            "author": "Yonik Seeley",
            "id": "comment-12909462"
        },
        {
            "date": "2012-05-23T18:44:48+0000",
            "content": "yonik, I see a bunch of commits on this issue, can we resolve this? ",
            "author": "Simon Willnauer",
            "id": "comment-13281794"
        },
        {
            "date": "2012-07-11T23:03:47+0000",
            "content": "bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment ",
            "author": "Hoss Man",
            "id": "comment-13412315"
        },
        {
            "date": "2012-08-07T03:41:25+0000",
            "content": "rmuir20120906-bulk-40-change ",
            "author": "Robert Muir",
            "id": "comment-13429706"
        }
    ]
}