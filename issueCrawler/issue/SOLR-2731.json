{
    "id": "SOLR-2731",
    "title": "CSVResponseWriter should optionally return numfound",
    "details": {
        "affect_versions": "3.1,                                            3.3,                                            4.0-ALPHA",
        "status": "Closed",
        "fix_versions": [
            "3.1.1"
        ],
        "components": [
            "Response Writers"
        ],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Won't Fix"
    },
    "description": "an optional parameter \"csv.numfound=true\" can be added to the request which causes the first line of the response to be the numfound.  This would have no impact on existing behavior, and those who are interested in that value can simply read off the first line before sending to their usual csv parser.",
    "attachments": {
        "SOLR-2731-R1.patch": "https://issues.apache.org/jira/secure/attachment/12492024/SOLR-2731-R1.patch",
        "SOLR-2731.patch": "https://issues.apache.org/jira/secure/attachment/12491649/SOLR-2731.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Yonik Seeley",
            "id": "comment-13091094",
            "date": "2011-08-25T16:08:26+0000",
            "content": "It seems like if we go down this road, it should somehow be a more generic mechanism (since others will then want values like maxScore, etc).\n\nHere are some alternatives:\n\nnumFound,maxScore,start\n2038,1.414,100\nid,score\ndoc1,1.3\ndoc2,1.1\ndoc3,1.05\n\nnumFound,2038,maxScore,1.414,start,100\nid,score\ndoc1,1.3\ndoc2,1.1\ndoc3,1.05\n\nnumFound=2038,maxScore=1.414,start=100\nid,score\ndoc1,1.3\ndoc2,1.1\ndoc3,1.05\n\n#numFound=2038,maxScore=1.414,start=100\nid,score\ndoc1,1.3\ndoc2,1.1\ndoc3,1.05\n\n\n\n\nPerhaps the \"numFound=2038,maxScore=1.414,start=100\" would be the most human readable (and maybe alternately commenting it if that's supported).\nBut the first option could be attractive since it's more in the spirit of CSV and might be desirable if imported into excel for example.\nThoughts? "
        },
        {
            "author": "Jon Hoffman",
            "id": "comment-13091111",
            "date": "2011-08-25T16:34:52+0000",
            "content": "I like maintaining consistency with the CSV format because you don't have to reinvent any parsing logic.   It should be pretty easy for the client developer to read off the first two lines and parse with the same tool that's used to parse the rest of the document.  Preferences around separator, newline, etc can be reused (except maybe this meta header should always have a column name header).\n\nWhat should the parameter be called?  csv.metaheader? "
        },
        {
            "author": "Jon Hoffman",
            "id": "comment-13091119",
            "date": "2011-08-25T16:42:21+0000",
            "content": "To be clear, I like the first option best. "
        },
        {
            "author": "Simon Rosenthal",
            "id": "comment-13091187",
            "date": "2011-08-25T18:14:04+0000",
            "content": "In addition to loading CSV results into a spreadsheet, I often use CSV as a quick-and-dirty way of dumping the contents of an index to be re-read into Solr, and adding lines which would need manual removal would be rather inconvenient.\n\nI'd go for option 4, with the comment symbol and result metadata on one line. org.apache.commons.csv has an option (which is not currently enabled in the CSVRequestHandler) to recognize and discard comment lines - adding a request parameter to the handler to recognize comment lines would be straightforward, and would at least solve my use case, though I admit not all others.\n\n\n\n "
        },
        {
            "author": "Jon Hoffman",
            "id": "comment-13091205",
            "date": "2011-08-25T18:40:24+0000",
            "content": "Simon,\n\nKeep in mind that this additional header would only appear if you asked for it via a request parameter like \"csv.metaheader=true\".  Existing behavior would remain unchanged.  Is that still a problem? "
        },
        {
            "author": "Simon Rosenthal",
            "id": "comment-13091227",
            "date": "2011-08-25T19:13:02+0000",
            "content": "good point. In that case I'm agnostic - 1) would be fine. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13091483",
            "date": "2011-08-26T01:04:23+0000",
            "content": "i think yonik's 1st example would be the best for people loading the data into a spreedsheet tool or parsing with conventional CSV tools.  (even better then #2 because it's easy to cut/paste that data into a different sheet and still have clean separation between headers/data. or parsing with conventional CSV tools)\n\nbut i would suggest that if we're at the point of thinking about having a \"metadata\" section and a \"results\" section we shouldn't limit ourselves to two sections.\n\ninstead of just including metadata about the main doclist, we could allow arbitrary sections or arbitrary lengths (like facet counts) ... i haven't thought hard about what the params should look like, but i would suggest that for easy output parsing a simple 1 row/column row count prefix value telling you the number of (csv) rows for each \"section\", followed by the (csv) rows of data (including a header row for each section if \"csv.header=true\") would be easy for people to parse (assuming they were expecting it because they asked for it)\n\nie...\n\n\n2\nnumFound,maxScore,start\n103,1.414,100\n4\nid,score\ndoc1,1.3\ndoc2,1.1\ndoc3,1.05\n\n\n\n..or if csv.header=false ...\n\n\n1\n103,1.414,100\n3\ndoc1,1.3\ndoc2,1.1\ndoc3,1.05\n\n\n\nWe can worry about what other \"sections\" might be supported later as long as the basic param syntax gets fleshed out ... i would suggest maybe something like:\n\n\n\tmultivalued \"csv.section\" param\n\tsections are written out in the order that they are passed as param\n\tdefault is \"csv.section=results\"\n\tif only one value is specified for csv.section, then no row count prefix is used for that section\n\tonly one other value for csv.section supported initially: \"csv.section=results.meta\"\n\t\n\t\tadds the numFound,maxScore,start for the results\n\t\n\t\n\n\n "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13091697",
            "date": "2011-08-26T09:53:38+0000",
            "content": "-1\n\n\n\tWhen you do the same query twice, the second time it usually takes 0ms. If it doesn't, turn on query caching.\n\tYou can code these variations with Velocity. I would stick with keeping the very simplest CSV output and then coding any additions yourself.\n\n\n\n "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-13091734",
            "date": "2011-08-26T12:29:38+0000",
            "content": "I'm mostly with Lance here, actually.  I want pure CSV.  So long as there there is always an option (which should be the default) to keep the output pure CSV then I'm ok with whatever extras folks want to add as options.\n\nWe really should get the response writer framework able to return custom HTTP headers though. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-13091735",
            "date": "2011-08-26T12:30:23+0000",
            "content": "Perhaps we could have an Excel response writer that could create a multi-sheet spreadsheet file? "
        },
        {
            "author": "Jon Hoffman",
            "id": "comment-13092589",
            "date": "2011-08-29T01:18:02+0000",
            "content": "This new patch includes the csv style metaheader with \"numFound,maxScore,start\" "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13717169",
            "date": "2013-07-23T18:47:20+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13971181",
            "date": "2014-04-16T12:57:23+0000",
            "content": "Move issue to Solr 4.9. "
        },
        {
            "author": "Alexandre Rafalovitch",
            "id": "comment-15538833",
            "date": "2016-10-01T17:10:45+0000",
            "content": "Is there still a desire in augmenting CSV output or have JSON/export handlers proved sufficient? "
        },
        {
            "author": "jmlucjav",
            "id": "comment-15539209",
            "date": "2016-10-01T21:45:17+0000",
            "content": "this would be still a nice addition for a very specific case (that I faced recently): you want to export a lot of data so you combine wt=csv with cursorMark feature, so you can reindex the output in another solr instance. I managed to do without, but this would have been a cleaner way. "
        },
        {
            "author": "Alexandre Rafalovitch",
            "id": "comment-15539412",
            "date": "2016-10-02T00:42:39+0000",
            "content": "For exporting significant amount of data, we now have /export handler. Specifically, for importing into another instance, there is DIH with SolrInputProcessor. Would either of those have fulfilled the need?\n\nThe output of this writer - as proposed - would not even be able to go back into the Solr, that would require updating a different component and additional, completely new discussion. "
        },
        {
            "author": "Alexandre Rafalovitch",
            "id": "comment-15557122",
            "date": "2016-10-08T04:16:18+0000",
            "content": "This feature request did not get any traction and its value is not clear given more recent functionalities. If somebody wants to pick it up and make an implementation, the case can be reopened. "
        }
    ]
}