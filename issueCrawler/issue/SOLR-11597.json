{
    "id": "SOLR-11597",
    "title": "Implement RankNet.",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "contrib - LTR"
        ],
        "type": "New Feature",
        "fix_versions": [
            "7.3",
            "master (8.0)"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Implement RankNet as described in this tutorial.",
    "attachments": {
        "SOLR-11597.patch": "https://issues.apache.org/jira/secure/attachment/12911431/SOLR-11597.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-11-02T18:22:53+0000",
            "content": "GitHub user airalcorn2 opened a pull request:\n\n    https://github.com/apache/lucene-solr/pull/270\n\n    Implement RankNet SOLR-11597.\n\n    As described in [this tutorial](https://github.com/airalcorn2/Solr-LTR).\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/airalcorn2/lucene-solr ranknet\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/lucene-solr/pull/270.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #270\n\n\ncommit 24f11aeb7f7f5ba6a2c9e96c959ffba87fff885a\nAuthor: Michael A. Alcorn <airalcorn2@gmail.com>\nDate:   2017-11-02T17:38:09Z\n\n    Implement RankNet.\n\ncommit fe1c9c99a455ca6928c637dc6b2c20341668b9ab\nAuthor: Michael A. Alcorn <airalcorn2@gmail.com>\nDate:   2017-11-02T17:39:45Z\n\n    Wording.\n\ncommit d752b243e4dad968e862d71d3f3b1bf34ab94aa2\nAuthor: Michael A. Alcorn <airalcorn2@gmail.com>\nDate:   2017-11-02T18:15:07Z\n\n    Fix explain.\n\n ",
            "author": "ASF GitHub Bot",
            "id": "comment-16236325"
        },
        {
            "date": "2017-11-03T21:22:03+0000",
            "content": "Hello Michael, thank you for opening this ticket with a pull request!\n\nI will add your tutorial and the icml_ranking.pdf paper to my reading list \n\nIn the meantime, here's some quick thoughts from a quick look at the pull request:\n\n\n\tGreat class-level javadocs. Let's also add the new class to the table in the Solr Reference Guide the source for which is now in the same git repo as the code i.e. https://github.com/apache/lucene-solr/blob/master/solr/solr-ref-guide/src/learning-to-rank.adoc\n\n\n\n\n\tCuriosity only at this point, not yet having read your tutorial or the paper:\n\n\"weights\" : [\n  \"1,2,3\\n4,5,6\\n7,8,9\\n10,11,12\",\n  \"13,14,15,16\\n17,18,19,20\",\n  \"21,22\"\n]\n\n\nrepresentation versus (say)\n\n\"weights\" : [\n  [ [ 1, 2, 3 ], [ 4, 5, 6 ], [ 7, 8, 9 ], [ 10, 11, 12 ] ],\n  [ [ 13, 14, 15, 16 ], [ 17, 18, 19, 20 ] ],\n  [ [ 21, 22 ] ]\n]\n\n\n\n\n\n\n\tHow about implementing/overriding the validate method? E.g. if for the non-linearity parameter someone mistypes \"sigmoidt\" instead of \"sigmoid\" then the validate method could right away through a model exception instead of the doNonlinearity method later on being a no-op which could be tricky to debug from a user's perspective.\n\n\n\n\n\tSomething probably already on your todo list and/or something which others in the community might be interested in collaborating on: test coverage of some sort, possibly but not necessarily similar to the existing TestLinearModel and TestMultipleAdditiveTreesModel tests\n\n\n\n\n\tminor thing: I'm pretty sure the String.format(\"...\", ...); in the explain method will be flagged up by the forbidden-apis checks in ant precommit pointing towards String.format(Locale.???, \"...\", ...) as the alternative. Related link: https://github.com/policeman-tools/forbidden-apis/blob/master/src/main/resources/de/thetaphi/forbiddenapis/signatures/jdk-unsafe-1.6.txt#L48\n\n ",
            "author": "Christine Poerschke",
            "id": "comment-16238411"
        },
        {
            "date": "2017-11-06T23:44:36+0000",
            "content": "Thanks for the feedback, Christine Poerschke.\n\nLet's also add the new class to the table in the Solr Reference Guide\n\nWill do.\n\nString vs. list of lists representation\n\nI agree the list representation is more intuitive, but I'm having trouble figuring out how to set that in Solr. The only thing I could make work was:\n\n\nfinal List<List<List<Float>>> matrixList = (List<List<List<Float>>>) weights;\n\n\n\nwhich is pretty ugly.\n\n\nfinal List<float[][]> matrixList = (List<float[][]>) weights;\n\n\n\nand:\n\n\nfinal List<Float[][]> matrixList = (List<Float[][]>) weights;\n\n\n\ndidn't seem to work for me, and I'm not sure why.\n\nHow about implementing/overriding the validate method?\n\nSounds good.\n\ntest coverage of some sort\n\nI can take a look at that.\n\nI'm pretty sure the String.format(\"...\", ...); in the explain method will be flagged up by the forbidden-apis\n\nI'll fix that. ",
            "author": "Michael A. Alcorn",
            "id": "comment-16241197"
        },
        {
            "date": "2017-11-15T22:05:49+0000",
            "content": "I've attempted to address all your comments in the latest commit, Christine Poerschke. ",
            "author": "Michael A. Alcorn",
            "id": "comment-16254292"
        },
        {
            "date": "2017-11-22T13:28:59+0000",
            "content": "Thanks for extending the pull request, Michael A. Alcorn.\n\nHaving the structured numeric weights instead of the [ \"1,2,3\\n4,5,6\\n7,8,9\\n10,11,12\", ... ] style string array really helps with understanding the javadocs example and the code I think.\n\nOnce it's converted from the params.weights JSON into a ArrayList<float[][]> weightMatrices class member, then looking at how the matrices are used I struggled a little a couple of times with [row][column] vs. [column][row] and the [matrixCols - 1] being not a weight but a bias and the last layer for some reason not having the non-linearity applied to it ... so i explored representational separation of weights and biases (the first commit on https://github.com/cpoerschke/lucene-solr/tree/master-solr-11597).\n\nWith the representations and calculations then being easier to understand, for me at least, after a little while 'the penny dropped' and the \"Can score() directly feed its argument to the neural network?\" question arose and because the answer seems to be \"no\" in the second commit on https://github.com/cpoerschke/lucene-solr/tree/master-solr-11597 i separated out the inputs and the output from the layers in representational terms, this also conveniently removes the 'no non-linearity for the last layer' special case and the need to validate that the last layer contains exactly one node.\n\nThe third commit on https://github.com/cpoerschke/lucene-solr/tree/master-solr-11597 is strictly speaking unrelated to the first two commits, it explores a functor-style non-linearity implementation so that no string comparisons are needed when calculating scores plus deriving classes could be created to support additional custom non-linearities.\n\nhttps://github.com/airalcorn2/lucene-solr/compare/ranknet...cpoerschke:master-solr-11597 is the combination of the three commits - what do you think? Could an approach like this work too or might the extra structure(s) result in an unacceptable incremental performance dent for the score() method which will be run many  times for each model (whereas the setters and the validate method run essentially just once)? ",
            "author": "Christine Poerschke",
            "id": "comment-16262511"
        },
        {
            "date": "2017-12-01T13:41:05+0000",
            "content": "Hi, Christine Poerschke. I don't think those changes would cause too much overhead (but I don't know for sure!), and they definitely seem to make things more readable. I'm used to thinking of neural network calculations in terms of dot products, but your changes would probably make things more maintainable for people less familiar with them. ",
            "author": "Michael A. Alcorn",
            "id": "comment-16274414"
        },
        {
            "date": "2017-12-19T14:40:48+0000",
            "content": "Hi, Christine Poerschke. Is there anything else I can help with on this? ",
            "author": "Michael A. Alcorn",
            "id": "comment-16296893"
        },
        {
            "date": "2017-12-22T17:58:58+0000",
            "content": "Hi Michael A. Alcorn, thanks for following up on this!\n\nI've just sent you a pull request to extend the TestNeuralNetworkModel.testLinearAlgebra coverage - what do you think?\n\nAs far as the weights representation is concerned, I think it would be good to have additional perspectives and opinions on this: Diego Ceccarelli, Michael Nilsson, Yuki Yano, everyone - would you have any thoughts on neural network / RankNet model representations in our Solr LTR plugin? ",
            "author": "Christine Poerschke",
            "id": "comment-16301737"
        },
        {
            "date": "2017-12-23T06:37:50+0000",
            "content": "Hi Christine Poerschke and Michael A. Alcorn.\n\nThank you for letting me know this issue. In my opinion, Christine's format looks good because of the readability.\n\nBy the way, there are some points which concern me.\n\n1. The name of \"nonlinearity\" is a bit curious for me because we call these functions as \"activation function\". How about using \"activation\" instead of ''nonlinearity\" to represent it?\n\n2. If we think to apply this to more complex neural networks in the future, we will need to support layers (ex. convolutional and pooling) other than the full connected layer (ex. ReLU). One possibility is that we add a \"type\" parameter to represent a role of each layer like the prototxt of Caffe's layer for future extensions.\n\n\n\"layers\" : [\n  [\n    { \"weights\" : [  1.0,  2.0,  3.0 ], \"bias\" :  4.0, \"type\" : \"relu\" },\n    { \"weights\" : [  5.0,  6.0,  7.0 ], \"bias\" :  8.0, \"type\" : \"relu\"  },\n    { \"weights\" : [  9.0, 10.0, 11.0 ], \"bias\" : 12.0, \"type\" : \"relu\"  },\n    { \"weights\" : [ 13.0, 14.0, 15.0 ], \"bias\" : 16.0, \"type\" : \"relu\"  }\n  ],\n  [\n    { \"weights\" : [ 13.0, 14.0, 15.0, 16.0 ], \"bias\" : 17.0, \"type\" : \"relu\"  },\n    { \"weights\" : [ 18.0, 19.0, 20.0, 21.0 ], \"bias\" : 22.0, \"type\" : \"relu\"  }\n  ]\n]\n\n\n\nOn the other hand, this representation is redundant at this time (like RankNet). What do you think? ",
            "author": "Yuki Yano",
            "id": "comment-16302235"
        },
        {
            "date": "2018-01-02T14:40:25+0000",
            "content": "The additional tests look good to me, Christine Poerschke.\n\nThanks for the feedback, Yuki Yano.\n\nThe name of \"nonlinearity\" is a bit curious for me because we call these functions as \"activation function\". How about using \"activation\" instead of ''nonlinearity\" to represent it?\n\n\"Nonlinearity\" and \"activation function\" are used more or less interchangeably when talking about neural networks. See, e.g., this Stanford course, \"In other words, each neuron performs a dot product with the input and its weights, adds the bias and applies the non-linearity (or activation function)\". Because the two terms are interchangeable, I'm OK with either being used.\n\nIf we think to apply this to more complex neural networks in the future, we will need to support layers (ex. convolutional and pooling) other than the full connected layer (ex. ReLU).\n\nIn my opinion, if this is a route Solr eventually wants to go, I think a better strategy would be to just add a dependency on Deeplearning4j. New types of architectural designs are constantly coming out (e.g., attention, which I believe Deeplearning4j still does not even support), and there are so many subtle ways to vary neural networks that it would be a ton of work to try and maintain what has effectively already been done in other libraries like Deeplearning4j. ",
            "author": "Michael A. Alcorn",
            "id": "comment-16308148"
        },
        {
            "date": "2018-01-09T08:25:57+0000",
            "content": "Thanks for your reply, Michael A. Alcorn.\n\n\"Nonlinearity\" and \"activation function\" are used more or less interchangeably when talking about neural networks. See, e.g., this Stanford course, \"In other words, each neuron performs a dot product with the input and its weights, adds the bias and applies the non-linearity (or activation function)\". Because the two terms are interchangeable, I'm OK with either being used.\n\nI see. Then, I think it is better to keep the name of \"nonlinearity\" for the simplicity.\n\nIn my opinion, if this is a route Solr eventually wants to go, I think a better strategy would be to just add a dependency on Deeplearning4j. \n\nThat's a great idea  ",
            "author": "Yuki Yano",
            "id": "comment-16318012"
        },
        {
            "date": "2018-01-09T21:08:17+0000",
            "content": "... add a dependency on Deeplearning4j ...\n\nAn intriguing idea indeed, thanks! Deeplearning4j looks to be Apache License 2.0 i.e. no concerns there (presumably) - created SOLR-11838 for exploring the idea further (complementary to the RankNet effort here). ",
            "author": "Christine Poerschke",
            "id": "comment-16319174"
        },
        {
            "date": "2018-01-12T22:59:33+0000",
            "content": "Returning to this after Deeplearning4J SOLR-11838 and Streaming Expressions SOLR-11852 diversions ...\n\nSo, would it be fair to assume that there is a use case for this type of neural network separate from the more complex neural networks that might in future be supported separately e.g. via Deeplearning4J integration?\n\nAssuming there is a use case, how would folks typically train such models? I'm wondering if that would help us move forward and decide on the weights representation question. ",
            "author": "Christine Poerschke",
            "id": "comment-16324703"
        },
        {
            "date": "2018-01-12T23:14:14+0000",
            "content": "Christine Poerschke - yes, they are separate. RankNet is specifically a learning to rank model whereas the other architectures being discussed are more for modeling language. I describe how to train a RankNet model using Keras here. The weights can be exported from Keras in any format the user wants. The original Solr representation I suggested made it easy to export the weights from Keras since the weights are contained in matrices. ",
            "author": "Michael A. Alcorn",
            "id": "comment-16324712"
        },
        {
            "date": "2018-02-02T15:56:52+0000",
            "content": "Hi, Christine Poerschke. Anything I can contribute? ",
            "author": "Michael A. Alcorn",
            "id": "comment-16350559"
        },
        {
            "date": "2018-02-02T19:35:47+0000",
            "content": "Thanks for the ping!\n\n... used to thinking of neural network calculations in terms of dot products ...\n\nI've been reading and learning more about neural networks recently and yes, with some familiarity with the domain the matrix representations become less of a code comprehension issue. So I think we can stick with the matrix approach here and support for more complex networks in future would likely be via integration with libraries like deeplearning4j (and others?).\n\n... The weights can be exported from Keras in any format the user wants. ...\n\nGood to know.\n\nOn the non-linearity vs. activation-function choice, let's go with activation or activation function.\n\nAnd can I suggest we take a function/functor style approach e.g. as sketched out in this commit (assuming java.util.function.Function Function<float,float> does not work) for three reasons:\n\n\tno string comparisons at document scoring time\n\tno need to ensure that the validate and point-of-use activation strings match\n\tcustom activation functions could be added by subclasses overriding the setActivation method\n\n\n\nAnd two things perhaps in the nice-to-have bucket for the explain function:\n\n\tMight it be helpful to include the input values somehow e.g. just the input numerics (that are fed to the network) or the input feature-plus-score details similar to how LinearModel.explain does it? Ideally with a test.\n\tUsing StringBuilder in explain should reduce the number of String objects being allocated and needing to be GC-ed.\n\n ",
            "author": "Christine Poerschke",
            "id": "comment-16350847"
        },
        {
            "date": "2018-02-06T14:32:15+0000",
            "content": "Hi, Christine Poerschke. Please see my latest commit here.\n\nSo I think we can stick with the matrix approach here...\n\nBased on this feedback and the rest of the discussion in this Jira, the model representation now takes the following form:\n\n\n        \"layers\" : [\n            {\n                \"matrix\" : [ [ 1.0, 2.0, 3.0 ],\n                             [ 4.0, 5.0, 6.0 ],\n                             [ 7.0, 8.0, 9.0 ],\n                             [ 10.0, 11.0, 12.0 ] ],\n                \"bias\" : [ 13.0, 14.0, 15.0, 16.0 ],\n                \"activation\" : \"relu\"\n            },\n            {\n                \"matrix\" : [ [ 17.0, 18.0, 19.0, 20.0 ],\n                             [ 21.0, 22.0, 23.0, 24.0 ] ],\n                \"bias\" : [ 25.0, 26.0 ],\n                \"activation\" : \"relu\"\n            },\n            {\n                \"matrix\" : [ [ 27.0, 28.0 ] ],\n                \"bias\" : [ 29.0 ],\n                \"activation\" : \"none\"\n            }\n        ]\n\n\n\nI've also added a Layer class that I believe makes the logic of the calculations clearer.\n\nOn the non-linearity vs. activation-function choice, let's go with activation or activation function.\n\nDone (\"activation\").\n\nAnd can I suggest we take a function/functor style approach...\n\nYep, agreed that's a better approach.\n\nMight it be helpful to include the input values somehow...\n\nAdded them to explain.\n\nUsing StringBuilder in explain...\n\nDone. ",
            "author": "Michael A. Alcorn",
            "id": "comment-16353937"
        },
        {
            "date": "2018-02-07T21:08:13+0000",
            "content": "Hi, Michael A. Alcorn.\n\n... I've also added a Layer class that I believe makes the logic of the calculations clearer. ...\n\nIt does indeed, nice.\n\nJust sent you a small pull request. Had noticed that the model's validate was not yet calling the layers' validate and so added that, and that the model's explain could delegate the describe-yourself to each layer, and then it was just one small extra step to the\n\n-  public class Layer {\n+  public interface Layer {\n+    public float[] calculateOutput(float[] inputVec);\n+    public int validate(int inputDim) throws ModelException;\n+    public String describe();\n+  }\n+\n+  public class DefaultLayer implements Layer { \n\n\nabstraction - what do you think? Combined with a protected instead of private createLayer method and that would even give a way for custom classes to add custom layers.\n\nNext step for me (not today), (re)run the (extended) tests, had issues with them earlier. ",
            "author": "Christine Poerschke",
            "id": "comment-16356085"
        },
        {
            "date": "2018-02-08T13:34:47+0000",
            "content": "Thanks, Christine Poerschke.\u00a0Those changes all look good to me. Let me know if there's anything else I can do. ",
            "author": "Michael A. Alcorn",
            "id": "comment-16356945"
        },
        {
            "date": "2018-02-21T21:44:21+0000",
            "content": "Thanks Michael A. Alcorn\u00a0for reviewing and merging in my tweaks to fix two test failures, 1x javadocs + 2x test tweaks and explain test, (custom) activation (bits) pull requests!\n\nAttached SOLR-11597.patch is a snapshot of the changes as of now on top of current master:\n\n\tant precommit passes\n\tant beast -Dtestcase=TestNeuralNetworkModel -Dbeast.iters=100 passes\n\tjavadocs outbound links manually checked\n\n\n\nI think we're good to go here and if no one has any additional comments, concerns, etc. then I'll proceed to commit the changes in around a week or so from today. ",
            "author": "Christine Poerschke",
            "id": "comment-16372062"
        },
        {
            "date": "2018-03-02T18:59:01+0000",
            "content": "Commit c5938f79e540f81b6d61560d324b150a5efd7011 in lucene-solr's branch refs/heads/master from Christine Poerschke\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c5938f7 ]\n\nSOLR-11597: Add contrib/ltr NeuralNetworkModel class.\n(Michael A. Alcorn, Yuki Yano, Christine Poerschke)\n\nCloses #270 ",
            "author": "ASF subversion and git services",
            "id": "comment-16383981"
        },
        {
            "date": "2018-03-02T19:20:07+0000",
            "content": "Commit caa43c55c4a27eeba94d59d574b996e41493f145 in lucene-solr's branch refs/heads/branch_7x from Christine Poerschke\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=caa43c5 ]\n\nSOLR-11597: Add contrib/ltr NeuralNetworkModel class.\n(Michael A. Alcorn, Yuki Yano, Christine Poerschke)\n\nCloses #270 ",
            "author": "ASF subversion and git services",
            "id": "comment-16384020"
        },
        {
            "date": "2018-03-02T19:23:13+0000",
            "content": "Thanks Michael A. Alcorn and Yuki Yano! ",
            "author": "Christine Poerschke",
            "id": "comment-16384023"
        }
    ]
}