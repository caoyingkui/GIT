{
    "id": "LUCENE-3318",
    "title": "Sketch out highlighting based on term positions / position iterators",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/highlighter"
        ],
        "type": "Sub-task",
        "fix_versions": [
            "Positions Branch"
        ],
        "affect_versions": "Positions Branch",
        "resolution": "Won't Fix",
        "status": "Closed"
    },
    "description": "Spinn off from LUCENE-2878. Since we have positions on a large number of queries already in the branch is worth looking at highlighting as a real consumer of the API. A prototype is already committed.",
    "attachments": {
        "LUCENE-3318.patch": "https://issues.apache.org/jira/secure/attachment/12487617/LUCENE-3318.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2011-07-15T20:34:47+0000",
            "content": "glancing at the branch, i would suggest dropping use of term vectors completely here, and building the prototype to store the start/end offsets into the payload for now. then the highlighter can translate positions into offsets via the payload.\n\ni think we should not use term vectors for highlighting... if the prototype gets going we can then look at encoding (optionally) this offsets information in the postings explicitly in parallel with positions more efficiently, e.g. in ways that exploit that fact that in a majority of cases endOffset - startOffset is going to be the same for every position of term X. ",
            "author": "Robert Muir",
            "id": "comment-13066188"
        },
        {
            "date": "2011-07-15T21:31:35+0000",
            "content": "That's been on my mind since you mentioned it earlier, Robert.  I'll take a whack at it next week if someone doesn't beat me to it   I do think it will be good to maintain the ability to highlight using whatever information is available, though.  ",
            "author": "Mike Sokolov",
            "id": "comment-13066224"
        },
        {
            "date": "2011-07-15T21:48:06+0000",
            "content": "\nI do think it will be good to maintain the ability to highlight using whatever information is available, though. \n\nWell, the traditional school of thought has been, to do this (e.g. highlighters/morelikethis today that can work with tvs or without them). Personally I disagree with this reasoning. I think the whole point of indexing your content is to make searching fast: and we should make highlighting first class and make it kick ass.\n\nI guess a compromise would be to fall back to existing TVs, but not to re-analyzing the document at runtime: I guess I think that providing \"slow options\" is not actually user-friendly but instead just causes confusion and performance problems... better to kick out an error and say 'you must index your content with XYZ for this to work at all'.\n ",
            "author": "Robert Muir",
            "id": "comment-13066232"
        },
        {
            "date": "2011-07-15T22:09:40+0000",
            "content": "The most useful fallback is being able to highlight via re-analyzing the text.  It's perfectly scalable and the right tradeoff for some people (say indexing a billion tweets).  Other people may not have as strict of time requirements, but may have more strict space requirements.\n\nIf termvector based highlighting isn't much faster than re-analysis, that's the one we should drop support for. ",
            "author": "Yonik Seeley",
            "id": "comment-13066247"
        },
        {
            "date": "2011-07-15T23:58:20+0000",
            "content": "the last thing we need is another highlighter that falls back to this: this already exists.\n\nif we want to break index back compat, default offsets to ON, and require reindexing to upgrade to 4.0, then, I'm ok with the fallback.\n\notherwise I am -1, as its just going to be a bad slow default. ",
            "author": "Robert Muir",
            "id": "comment-13066316"
        },
        {
            "date": "2011-07-16T01:24:45+0000",
            "content": "Re-analyzing is often really not that slow for many, many applications. I have no problem with it as a fallback anytime. ",
            "author": "Mark Miller",
            "id": "comment-13066336"
        },
        {
            "date": "2011-07-16T01:29:38+0000",
            "content": "Thinking further - I suppose you could argue you that you have to explicitly enable an option to make this happen, rather then getting the behavior through fall back. But outright blocking it just seems silly. ",
            "author": "Mark Miller",
            "id": "comment-13066340"
        },
        {
            "date": "2011-07-16T02:51:20+0000",
            "content": "I think rmuir's point is that at a Java class level, we already have a highlighter that can do this.\n\nIf we add a new XyzBasedHighlighter class that can take advantage of some new XYZ information that can be recorded at index time, it's better for that highlighter to fail fast if the Xyz info isn't actually in the index \u2013 if you want term vectors or re-analysis as a fall back, then make that decision in your code when you call XyzBasedHighlighter.getHighlights().\n\ni don't really know anything about the existing Highlighter impl, but i agree with that philosophy at the Java API level\n\n(At a Solr API level there are a lot more options.  the naive API is to make the user pick an impl and fail if it doesn't work, but alternately we could pick the impl based on what options are used on the fields being indexed, etc...) ",
            "author": "Hoss Man",
            "id": "comment-13066350"
        },
        {
            "date": "2011-07-16T10:53:11+0000",
            "content": "thanks hossman. thats exactly it.\n\nthere's no need for such fallbacks to be in lucene's library code: they are confusing and also can sometimes inadvertently create bad defaults. A user can implement this logic themselves. ",
            "author": "Robert Muir",
            "id": "comment-13066400"
        },
        {
            "date": "2011-07-16T15:43:05+0000",
            "content": "we already have a highlighter that can do this.\n\nI'm hoping that once we have a highlighter than can use this information, that can be a much better highlighter - the others can go away. If we can avoid it, I don't see why we would want to split effort over 3 different highlighters.\n\nthen make that decision in your code when you call \n\nAs I mentioned - I think it's fine to make it so that it has to be explicitly asked for in code. But I think it's silly to disallow that just because in some cases with huge documents and a giant maxCharsToAnalyze, it can be slow.\n\nand also can sometimes inadvertently create bad defaults.\n\nDefaults can always be bad in some cases - else they wouldn't be called defaults - they would just be the way it is. In this case, IMO, it's generally been a fine default for the old highlighter - it's the minority of use cases that have need something else - all you can expect from a good default. ",
            "author": "Mark Miller",
            "id": "comment-13066445"
        },
        {
            "date": "2011-07-16T15:54:42+0000",
            "content": "OK, thanks for the clarifications.  As long as we have the ability somehow/somewhere to do highlighting based on re-analysis, I'm good. ",
            "author": "Yonik Seeley",
            "id": "comment-13066449"
        },
        {
            "date": "2011-07-16T16:24:09+0000",
            "content": "\nDefaults can always be bad in some cases - else they wouldn't be called defaults - they would just be the way it is. In this case, IMO, it's generally been a fine default for the old highlighter - it's the minority of use cases that have need something else - all you can expect from a good default.\n\nThere doesn't need to be defaults at all: i think these differnt ways should be split into independent highlighting methods. we can re-use the same APIs (fragmenters or whatever) across the different implementations, if possible, but why not just force the user to use the algorithm they want? i dont think we need to have ABC+DEFHighlighter that does black magic across different algorithms, you should instead pick ABCHighlighter or DEFHighlighter.\n\nthis is a library after all, I think this would be much cleaner. ",
            "author": "Robert Muir",
            "id": "comment-13066457"
        },
        {
            "date": "2011-07-16T16:59:52+0000",
            "content": "but why not just force the user to use the algorithm they want? \n\nThat I don't mind - just that the option exists if it's possible. Whether you choose through a setter or a different sub class, I don't mind. If it's not possible to re-analyze without keeping the other Highlighters around too, I'm much less for keeping the option around (at the least, the old highlighters should be heavily de-emphasized compared to the new one). I have not looked at the new code yet though. ",
            "author": "Mark Miller",
            "id": "comment-13066465"
        },
        {
            "date": "2011-07-23T22:15:55+0000",
            "content": "Uploading a patch for this that builds a TokenStream using position intervals from the query (matches) and their offsets, and then uses the existing Highlighter to do fragmentation and markup.\n\nThis approach should make it easy to skip creating fragments for interstitial (non-matching) portions of large documents, but this issue doesn't cover that yet.\n\nThe patch provides two methods for mapping positions to offsets; one is based on term vectors; the other uses offsets stored in payloads.  And you can still use analysis. The payload version is about twice as fast as the term vector version, which is around 8x faster than reanalysis (comparable to FastVectorHighlighter).  The choice of which to use (or whether to re-analyze) is up to the user; there are no auto-fallback behaviors in here \n\nUsing these schemes makes fragmentation more difficult. The issue is that offsets and positions are not readily available for all tokens - only for those that matched the query.  This makes it harder to fragment the document in reasonable places, and to surround the hits with some appropriate text. However, the substantial speedup seems to make it worth the effort.\n\nSome TODO's:\n\nThere's currently no consistency-checking: if no offset-payloads were stored, and the user attempts to use them, they simply get no highlighting. I think there may be a hard fail if absent term vectors are requested though.\n\nFragmentation doesn't necessarily land on a good boundary; we should at least scan for whitespace in a default fragmenter.\n\nSimon: something a bit weird happens when collecting position intervals now; in some cases the same interval can be collected twice.  This happens w/ConjunctionPositionIterator - when PosCollector.collect(doc) calls advanceTo(doc), positions are collected, and then I iterate over more positions, and collect() them (which I have to do to get other cases to work); then during this latter iteration, the same intervals are reported again.  I've worked around this easily enough, but I think it would be easier to work with if it didn't happen?  Not sure how difficult that is to arrange.\n\nAlso: I made all the collect() methods throw IOException so I could report exceptions from processing payloads. ",
            "author": "Mike Sokolov",
            "id": "comment-13070066"
        },
        {
            "date": "2011-07-24T09:15:03+0000",
            "content": "Some note from the TokenmStream policeman: The TokenStream must call clearAttributes() always as first action unless it returns false (see docs), the positionIncrement is then also automatically 1. Also you should make the whole class PosTokenStream final, as you can/should never extend it. ",
            "author": "Uwe Schindler",
            "id": "comment-13070126"
        },
        {
            "date": "2011-07-24T10:33:16+0000",
            "content": "Hi,\n\n\nif (q instanceof MultiTermQuery) {\n  ((MultiTermQuery)q).setRewriteMethod (MultiTermQuery.CONSTANT_SCORE_BOOLEAN_QUERY_REWRITE);\n}\n\n\n\nThis changes the original query, so we should clone the query before (this is what the standard highlighter does when it rewrites queries) - but: maybe cloning was done before in other highlighter code. This one just looks wrong to me. Also for large indexes with many terms, this may easily throw TooManyClausesException, so we should catch this exception somehow and disable highlighting in that case.\n\nAbout the payloads encoding: I would not limit the offsets and positions and use the full 32bit ints, but instead encode them as vInt. The facetting package already has a method for this (it does similar thins, namely encoding ints into payloads), maybe we move those byte[] vInt encoders to core utils. ",
            "author": "Uwe Schindler",
            "id": "comment-13070139"
        },
        {
            "date": "2011-07-24T11:55:43+0000",
            "content": "\nAbout the payloads encoding: I would not limit the offsets and positions and use the full 32bit ints, but instead encode them as vInt. The facetting package already has a method for this (it does similar thins, namely encoding ints into payloads), maybe we move those byte[] vInt encoders to core utils.\n\nYou can re-use a bytearraydataoutput here. see the analysis.synonyms builder package for an example. ",
            "author": "Robert Muir",
            "id": "comment-13070149"
        },
        {
            "date": "2011-07-24T16:43:37+0000",
            "content": "Thanks for the pointers; I'll follow up soon.  \n\nRe: rewriting MultiTermQueries: the code that is in the patch is definitely just a starting point.  It has the problems you pointed out Uwe, and also won't work properly in the case where a MTQ is not the top-level query:  (foo* AND bar*) eg.  \n\nThis issue is dealt with in the current system by some complex handcrafted logic in WeightedSpanTermExtractor (via QueryScorer), which we are not using here. I hope we can do something simpler, but we do need to walk the query tree cloning MTQs at least - this seems to be what WeightedSpanTermExtractor does.\n\nAlso - I did run a quick test to check, and it seems that the existing HL may throw a TooManyClauses in the same situation we would here.  Perhaps HL consumers expect this?  I think I agree that it would be more friendly to catch internally, though - can the consumer recover in any other way?  I don't think so. ",
            "author": "Mike Sokolov",
            "id": "comment-13070207"
        },
        {
            "date": "2011-07-24T16:47:31+0000",
            "content": "\nAlso - I did run a quick test to check, and it seems that the existing HL may throw a TooManyClauses in the same situation we would here. Perhaps HL consumers expect this? I think I agree that it would be more friendly to catch internally, though - can the consumer recover in any other way? I don't think so.\n\nMaybe the rewritemethod used here for highlighting should be instead TopTermsRewrite. ",
            "author": "Robert Muir",
            "id": "comment-13070208"
        },
        {
            "date": "2011-07-24T16:54:06+0000",
            "content": "Also - I did run a quick test to check, and it seems that the existing HL may throw a TooManyClauses in the same situation we would here. Perhaps HL consumers expect this? I think I agree that it would be more friendly to catch internally, though - can the consumer recover in any other way? I don't think so.\n\nWith standard highlighter the query is executed against a MemoryIndex with only this one document. Its unlikely to fail, but it can for very large docs (of course). ",
            "author": "Uwe Schindler",
            "id": "comment-13070210"
        },
        {
            "date": "2011-07-24T17:02:02+0000",
            "content": "Maybe the rewritemethod used here for highlighting should be instead TopTermsRewrite.\n\nThat sounds right - we make a best effort.  The primary use case for HL, IMO, is just to show a few representative samples that explain why the document matched. ",
            "author": "Mike Sokolov",
            "id": "comment-13070211"
        },
        {
            "date": "2011-07-24T17:54:11+0000",
            "content": "The TokenStream must call clearAttributes() always as first action unless it returns false (see docs)\n\nthis makes sense, but I don't see anything in the javadocs of TokenStream or AttributeSource about it. ",
            "author": "Mike Sokolov",
            "id": "comment-13070223"
        },
        {
            "date": "2011-07-24T18:25:00+0000",
            "content": "updated patch encodes offset payloads as VInts, corrects TokenStream usage, and adds (ignored) test for complex query rewriting.\n\nNothing done yet about query rewriting yet - my questions is: is there a better way to walk a query tree than a big switch statement with instanceof?  It would be nice if Query provided List<Query> subs(), but I don't see it  ",
            "author": "Mike Sokolov",
            "id": "comment-13070227"
        },
        {
            "date": "2011-08-07T22:46:04+0000",
            "content": "Updated patch now handles MultiTermQuerys nested (however deeply) in BooleanQuerys, and does not modify the queries in order to rewrite them.\n\nThis approach can now be used for highlighting many types of queries (SpanQueries are a special case - they do their own MTQ rewriting), but still does require inspection of query types.  It might be nice in the future to provide a Query rewriting interface that allows the caller more control over the kind of rewriting to be done.  With this, it would be possible to expect future query types to implement a rewriting method that could support highlighting without the need for instanceof, etc.\n\nI added a simple white-space boundary detection that can be used to adjust snippet boundaries to fall between words.  The basic idea could be extended to do sentence boundary detection.\n\nI added a Highlighter parameter (maxFragsToScore) that limits the number of fragments considered when attempting to find the highest-scoring one(s). This gives some decent speedup if you just want to find the first fragment in a document. ",
            "author": "Mike Sokolov",
            "id": "comment-13080688"
        },
        {
            "date": "2018-08-29T21:55:17+0000",
            "content": "Alexandre Rafalovitch please feel free to resolve, as discussed on the mailing list. This issue was superseded by LUCENE-8229 (7 years later!) ",
            "author": "Mike Sokolov",
            "id": "comment-16596890"
        }
    ]
}