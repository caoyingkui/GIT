{
    "id": "SOLR-8173",
    "title": "CLONE - Leader recovery process can select the wrong leader if all replicas for a shard are down and trying to recover as well as lose updates that should have been recovered.",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Resolved",
        "resolution": "Duplicate",
        "priority": "Critical"
    },
    "description": "I'm doing this test\ncollection test is replicated on two solr nodes running on 8983, 8984\nusing external zk\ninitially both nodes are empty\n\n1)turn on solr 8983\n2)add,commit a doc x con solr 8983\n3)turn off solr 8983\n4)turn on solr 8984\n5)shortly after (leader still not elected) turn on solr 8983\n6)8984 is elected as leader\n7)doc x is present on 8983 but not on 8984 (check issuing a query)\n\nIn attachment are the solr.log files of both instances",
    "attachments": {
        "solr_8983.log": "https://issues.apache.org/jira/secure/attachment/12767617/solr_8983.log",
        "solr_8984.log": "https://issues.apache.org/jira/secure/attachment/12767618/solr_8984.log"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-10-22T13:03:48+0000",
            "author": "Mark Miller",
            "content": "You are doing this test with version 5.2.1? ",
            "id": "comment-14969121"
        },
        {
            "date": "2015-10-23T15:47:40+0000",
            "author": "Matteo Grolla",
            "content": "Yes,\nUnpacked zip\nCloned server folder\nStarted 2 node cluster using bin/solr script\nCreated 'schemaless' collection using bin/solr script and ran the described\ntest.\n ",
            "id": "comment-14971207"
        },
        {
            "date": "2015-10-24T20:53:14+0000",
            "author": "Mark Miller",
            "content": "From my testing with this type of use case, the problem is how we are dealing with an empty index.\n\nThe leader election needs to be smarter about knowing if an empty index is a good candidate to be leader.\n\nWe need something that is part of the sync up phase that checks if any participating replicas have any tlogs. If they do, a replica with no tlogs should not become leader. ",
            "id": "comment-14972854"
        },
        {
            "date": "2015-10-28T15:57:18+0000",
            "author": "Matteo Grolla",
            "content": "Hi Mark,\nto me the problem happens also with a nonempty index, to reproduce:\n\ninitially both nodes \n-are shut down\n-CONTAIN DOCUMENT X\n\n1)turn on solr 8983\n2)add,commit a doc y con solr 8983\n3)turn off solr 8983\n4)turn on solr 8984\n5)shortly after (leader still not elected) turn on solr 8983\n6)8984 is elected as leader\n7)doc y is present on 8983 but not on 8984 (check issuing a query), which only contains document x\n\nHow can I test a scenario where the default leaderVoteWait makes the right leader be elected?\n ",
            "id": "comment-14978644"
        },
        {
            "date": "2015-11-05T03:16:48+0000",
            "author": "Varun Thacker",
            "content": "I tried to reproduce this and I think there could be two bugs in play here:\n\n1. The bug Matteo mentioned . These were the steps I used to reproduce it\n\n\n./bin/solr start -e cloud -noprompt -z localhost:2181\n\nhttp://localhost:8983/solr/admin/collections?action=CREATE&name=test3&collection.configName=gettingstarted&numShards=1&replicationFactor=2\n\ncore_node1 = core_node2 = active\n\n./bin/solr stop -p 7574\n\ncore_node2 = down\n\ncurl http://127.0.0.1:8983/solr/test3/update?commit=true -H 'Content-type:application/json' -d '[{\"id\" : \"1\"}]'\n\n./bin/solr stop -p 8983\n\n./bin/solr start -c -z localhost:2181 -s example/cloud/node2/solr -p 7574; sleep 10; ./bin/solr start -c -z localhost:2181 -s example/cloud/node1/solr -p 8983\n\nAt this point both replicas are 'ACTIVE' , replica 2 becomes the leader and the collection has 0 documents.\n\n\n\n2. A slight variation of the test also leads to lost updates. These were the steps I used to reproduce it.\n\n\n./bin/solr start -e cloud -noprompt -z localhost:2181\n\nhttp://localhost:8983/solr/admin/collections?action=CREATE&name=test1&collection.configName=gettingstarted&numShards=1&replicationFactor=2\n\ncore_node1 = core_node2 = active\n\n./bin/solr stop -p 7574\n\ncore_node2 = down\n\ncurl http://127.0.0.1:8983/solr/test1/update?commit=true -H 'Content-type:application/json' -d '[{\"id\" : \"1\"}]'\n\n./bin/solr stop -p 8983\n\n./bin/solr start -c -z localhost:2181 -s example/cloud/node2/solr -p 7574\n\n\n\n\nReplica 2 does not take leadership till timeout. It stays in down state.\n\nINFO  - 2015-10-26 23:15:53.026; [c:test1 s:shard1 r:core_node2 x:test1_shard1_replica1] org.apache.solr.cloud.ShardLeaderElectionContext; Waiting until we see more replicas up for shard shard1: total=2 found=1 timeoutin=139681ms\n\nReplica 2 becomes leader after timeout\n\nINFO  - 2015-10-26 23:18:13.127; [c:test1 s:shard1 r:core_node2 x:test1_shard1_replica1] org.apache.solr.cloud.ShardLeaderElectionContext; Was waiting for replicas to come up, but they are taking too long - assuming they won't come back till later\nINFO  - 2015-10-26 23:18:13.128; [c:test1 s:shard1 r:core_node2 x:test1_shard1_replica1] org.apache.solr.cloud.ShardLeaderElectionContext; I may be the new leader - try and sync\nINFO  - 2015-10-26 23:18:13.129; [c:test1 s:shard1 r:core_node2 x:test1_shard1_replica1] org.apache.solr.cloud.SyncStrategy; Sync replicas to http://192.168.1.9:7574/solr/test1_shard1_replica1/\nINFO  - 2015-10-26 23:18:13.129; [c:test1 s:shard1 r:core_node2 x:test1_shard1_replica1] org.apache.solr.cloud.SyncStrategy; Sync Success - now sync replicas to me\nINFO  - 2015-10-26 23:18:13.130; [c:test1 s:shard1 r:core_node2 x:test1_shard1_replica1] org.apache.solr.cloud.SyncStrategy; http://192.168.1.9:7574/solr/test1_shard1_replica1/ has no replicas\nINFO  - 2015-10-26 23:18:13.131; [c:test1 s:shard1 r:core_node2 x:test1_shard1_replica1] org.apache.solr.cloud.ShardLeaderElectionContext; I am the new leader: http://192.168.1.9:7574/solr/test1_shard1_replica1/ shard1\n\n\n\nSo for the first case I am guessing that the znode at the head gets picked up as the leader when all replicas are active. If thats the case can we pick the replica which has the latest data ?\nIn the second case after the timeout a replica can become a leader. Thinking aloud should we mark the replica as recovery failed instead by default and have a parameter which when specified allows any replica to become the leader?  ",
            "id": "comment-14991039"
        },
        {
            "date": "2016-02-01T10:49:56+0000",
            "author": "Stephan Lagraulet",
            "content": "Can you remove Fix version 5.2.1 if this bug is not resolved? ",
            "id": "comment-15126098"
        },
        {
            "date": "2016-02-01T16:10:54+0000",
            "author": "Shawn Heisey",
            "content": "Stephan Lagraulet, I have removed 5.2.1 from the \"fix version\" list.\n\nTypically this field is meaningless if the issue is unresolved.  Some people do populate it when creating an issue, to indicate the version they think it should be fixed in, but it doesn't mean anything until a fix is committed and the issue is resolved as Fixed.  When I start working on an issue, I will usually blank out \"fix version\", unless I happen to know with complete certainty when I will finish the work and which version will contain the fix.  Knowing this with complete certainty is rare. ",
            "id": "comment-15126461"
        },
        {
            "date": "2016-02-02T13:31:54+0000",
            "author": "Stephan Lagraulet",
            "content": "Thanks, sounds like a good process. ",
            "id": "comment-15128249"
        },
        {
            "date": "2017-03-03T18:12:21+0000",
            "author": "Amrit Sarkar",
            "content": "Are we planning to resolve this any time soon? ",
            "id": "comment-15894789"
        },
        {
            "date": "2017-03-03T18:33:52+0000",
            "author": "Frank Kelly",
            "content": "I agree. \nThis is a critical problem when ZooKeeper and Solr disagree as who the leader there needs to be a winner rather stay in some unrecoverable state. Even if it just randomly picked one shard - a fully operational but slightly \"off\" search index is better than no index at all.\n ",
            "id": "comment-15894821"
        }
    ]
}