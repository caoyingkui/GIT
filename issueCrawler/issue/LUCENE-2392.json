{
    "id": "LUCENE-2392",
    "title": "Enable flexible scoring",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/search"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.0-ALPHA",
            "flexscoring branch"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "This is a first step (nowhere near committable!), implementing the\ndesign iterated to in the recent \"Baby steps towards making Lucene's\nscoring more flexible\" java-dev thread.\n\nThe idea is (if you turn it on for your Field; it's off by default) to\nstore full stats in the index, into a new _X.sts file, per doc (X\nfield) in the index.\n\nAnd then have FieldSimilarityProvider impls that compute doc's boost\nbytes (norms) from these stats.\n\nThe patch is able to index the stats, merge them when segments are\nmerged, and provides an iterator-only API.  It also has starting point\nfor per-field Sims that use the stats iterator API to compute boost\nbytes.  But it's not at all tied into actual searching!  There's still\ntons left to do, eg, how does one configure via Field/FieldType which\nstats one wants indexed.\n\nAll tests pass, and I added one new TestStats unit test.\n\nThe stats I record now are:\n\n\n\tfield's boost\n\n\n\n\n\tfield's unique term count (a b c a a b --> 3)\n\n\n\n\n\tfield's total term count (a b c a a b --> 6)\n\n\n\n\n\ttotal term count per-term (sum of total term count for all docs\n    that have this term)\n\n\n\nStill need at least the total term count for each field.",
    "attachments": {
        "ASF.LICENSE.NOT.GRANTED--LUCENE-2392.patch": "https://issues.apache.org/jira/secure/attachment/12441412/ASF.LICENSE.NOT.GRANTED--LUCENE-2392.patch",
        "LUCENE-2392_take2.patch": "https://issues.apache.org/jira/secure/attachment/12469301/LUCENE-2392_take2.patch",
        "LUCENE-2392.patch": "https://issues.apache.org/jira/secure/attachment/12457447/LUCENE-2392.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-04-11T17:30:11+0000",
            "content": "Rough first patch attached.... ",
            "author": "Michael McCandless",
            "id": "comment-12855757"
        },
        {
            "date": "2010-04-12T01:11:45+0000",
            "content": "Mike, I don't think overlapTermCount should really exist in the Stats.\nTrying to implement some concrete FieldSimProviders, it starts getting messy.\nWhen using term unique pivoted length norm, i don't want to count these positionIncrement=0 terms either...\nso do we need a uniqueOverlapTermCount?\nEven when using non-unique (BM25) pivoted length norm, we don't want to count these when summing to calculate \naverages either.\n\nSo i know you probably see this as 'baking something into the index' but I think positionIncrement=0 means \n\"doesn't contribute to the document length\" by definition, and the discountOverlaps=false (no longer the default)\nshould be considered deprecated compatibility behavior  ",
            "author": "Robert Muir",
            "id": "comment-12855802"
        },
        {
            "date": "2010-04-12T07:35:40+0000",
            "content": "Mike - it'll also be great if we can store the length of the document in a custom way. I think what I'm saying is that if we can open up the norms computation to custom code - that will do what I want, right? Maybe we can have a class like DocLengthProvider which apps can plug in if they want to customize how that length is computed. Wherever we write the norms, we'll call that impl, which by default will do what Lucene does today?\nI think though that it's not a field-level setting, but an IW one? ",
            "author": "Shai Erera",
            "id": "comment-12855875"
        },
        {
            "date": "2010-04-12T09:48:59+0000",
            "content": "Mike, I don't think overlapTermCount should really exist in the Stats.\n\nOK I will remove it \u2013 I was unsure whether it was overkill   So\nit's purely an index time decision, whether the posIncr 0 tokens\n\"count\".\n\nHmm, but... we have a problem, which is that these posIncr 0 tokens\nare now counted in the unique token count.  Have to mull how to avoid\nthat...hmm... to do it correctly, I think means \"count this token as\n+1 on the unique tokens for this doc if ever it has non-zero posIncr\"?\n\nReally, maybe somehow we should be using at attr about the token\nitself?  Instead of posIncr == 0?  I mean a broken synonym injector\ncould conceivably provide the synonyms first (w/ first one having\nposIncr 1), followed by the real term (w/ posIncr 0)?\n\nBTW the cost of storing the stats isn't that bad \u2013 it increases index\nsize by 1.5%, on a 10M wikipedia index where each doc is 1KB of text\n(~171 words per doc on avg). ",
            "author": "Michael McCandless",
            "id": "comment-12855905"
        },
        {
            "date": "2010-04-12T09:57:35+0000",
            "content": "I think what I'm saying is that if we can open up the norms computation to custom code - that will do what I want, right? \n\nI'm calling the norms \"boost bytes\"   This was Marvin's term.. I\nlike it.\n\nThis patch makes boost byte computation completely private to the\nsim (see the *FieldSimProvider).  Ie the sim providers walk the stats\nand do whatever they want to \"prepare\" for real searching.  EG if you\nhave the RAM maybe you want to use a true float[] not boost bytes.  Or\nif you really don't have the RAM maybe you use only 4 bits per-doc,\nnot 8.  The FieldSim just provides a \"float boost(int docID)\" so what\nit does under the hood is private.\n\nMaybe we can have a class like DocLengthProvider which apps can plug in if they want to customize how that length is computed.\n\nSo... I'm actually trying to avoid extensibility on the first go, here\n(this is the \"baby steps\" part of the original thread).\n\nIe, the IR world seems to have converged on a smallish set of \"stats\"\nthat are commonly required, so I'd like to make those initial stats\nwork well, for starters.  Commit that (it enables all sorts of state\nof the art scoring models), and perhaps cutover to the default Robert\ncreated in LUCENE-2187 (which needs stats to work correctly).  And\nthen (phase 2) work out plugability so you can put your own stats\nin....\n\nWherever we write the norms, we'll call that impl, which by default will do what Lucene does today?\n\nRight, this is the DefaultSimProvider in my current patch \u2013 it simply\ncomputes the same thing Lucene does today, but uses the stats at IR\nopen time (once it's hooked up) to do, instead of doing so during\nindexing.\n\nI think though that it's not a field-level setting, but an IW one?\n\nIt's field level now and I think we should keep it that way.  EG\nTerrier was apparently document oriented in the past but has now\ndeprecated that and moved to per-field.\n\nYou can always make a catch-all field if you \"really\" want aggregated\nstats across the entire doc? ",
            "author": "Michael McCandless",
            "id": "comment-12855906"
        },
        {
            "date": "2010-04-12T10:18:12+0000",
            "content": "I'd like to withdraw my request from above. I misunderstood that the stats I need are stored per-field per-doc. So that will allow me to compute the docLength as I want. ",
            "author": "Shai Erera",
            "id": "comment-12855913"
        },
        {
            "date": "2010-04-12T11:49:47+0000",
            "content": "\nReally, maybe somehow we should be using at attr about the token\nitself? Instead of posIncr == 0? I mean a broken synonym injector\ncould conceivably provide the synonyms first (w/ first one having\nposIncr 1), followed by the real term (w/ posIncr 0)?\n\nRight, its my opinion all along (others disagree with me) that since\nwe have this 'ordered (incrementToken) Attributesource' / Token*Stream* that \nthis sorta broken filter isn't a valid equivalent..., its definitely a different\nTokenStream,even if its treated in some ways today as the same... we gotta\nbreak away from this for reasons like this.\n\notherwise why have it ordered at all? ",
            "author": "Robert Muir",
            "id": "comment-12855934"
        },
        {
            "date": "2010-10-18T14:52:39+0000",
            "content": "i brought the patch up to trunk: didn't fix any problems \n(e.g. some tests still fail, things outside of core lucene won't even compile) ",
            "author": "Robert Muir",
            "id": "comment-12922091"
        },
        {
            "date": "2011-01-25T17:24:38+0000",
            "content": "here's a really really rough \"take 2\" at the problem.\n\nThe general idea is to take a smaller \"baby-step\" as Mike calls it, to the problem.\nReally we have been working our way towards this anyway, exposing additional statistics,\nmaking Similarity per-field, fixing up inconsistencies... and this is the way I prefer, as we\nget things actually committed and moving.\n\nSo whatever is in this patch (which is full of nocommits, but all tests pass and all queries work with it),\nwe could possibly then split up into other issues and continue slowly proceeding, or maybe\ncreate a branch, whatever.\n\nMy problem with the other patch is it requires a ton more work to make any progress on it...\nand things don't even compile with it, forget about tests.\n\nThe basics here are to:\n\n\tSplit the \"matching\" and \"scoring calculations\" of Scorer. All responsibility of calculations belongs\nin the Similarity, the Scorer should be matching positions, working docsEnums, etc etc.\n\tSimilarity as we know it now, gets a more low-level API, and TFIDFSimilarity implements this API,\nbut exposes its customizations via the tf(), idf(), etc we know now.\n\tThings like score-caching and specialization of calculations are the responsibility of the Similarity,\nas these depend upon the formula being used. For TFIDFSimilarity, i added some optimizations here,\nfor example it specializes its norms == null case away to remove the per-doc \"if\".\n\tSince all Weights create PerReaderTermState (<-- this one needs a new name), to separate the\nseeking/stats collection from the calculations, i also optimized PhraseQuery's Weight/Scorer construction\nto be single-pass. \n\n\n\nAlso I like to benchmark every step of the way, so we don't come up with\nthis design that won't be performant: here are the scores for lucene's default Sim with the patch:\n\n\n\nQuery\nQPS trunk\nQPS patch\nPct diff\n\n\nspanNear([unit, state], 10, true)\n3.04\n2.92\n-4.0%\n\n\ndoctitle:.[Uu]nited.\n4.00\n3.99\n-0.1%\n\n\n+unit +state\n8.11\n8.12\n0.2%\n\n\nunited~2.0\n4.36\n4.40\n1.0%\n\n\nunited~1.0\n18.70\n18.93\n1.2%\n\n\nunit~2.0\n8.54\n8.71\n2.1%\n\n\nspanFirst(unit, 5)\n11.35\n11.59\n2.2%\n\n\nunit~1.0\n8.69\n8.91\n2.6%\n\n\nunit state\n7.03\n7.23\n2.8%\n\n\n\"unit state\"~3\n3.74\n3.86\n3.2%\n\n\nu*d\n16.72\n17.30\n3.5%\n\n\nstate\n19.24\n20.04\n4.1%\n\n\nun*d\n49.42\n51.55\n4.3%\n\n\n\"unit state\"\n5.99\n6.31\n5.3%\n\n\n+nebraska +state\n140.74\n151.85\n7.9%\n\n\nuni*\n10.66\n11.55\n8.4%\n\n\nunit*\n18.77\n20.41\n8.7%\n\n\ndoctimesecnum:[10000 TO 60000]\n6.97\n7.70\n10.4%\n\n\n\n\n\nAll Lucene/Solr tests pass, but there are lots of nocommits, especially\n\n\tNo Javadocs\n\tExplains need to be fixed: in general the explanation of \"matching\" belongs where it is now,\nbut the explanation of \"score calculations\" belongs in the Similarity.\n\tneed to refactor more out of Weight, currently we pass it to the docscorer, but\nits the wrong object, as it can only \"hold\" a single float.\n\n\n\nAnyway, its gonna take some time to rough all this out I'm sure, but I wanted\nto show some progress/invite ideas, and also show we can do this stuff\nwithout losing performance.\n\nI have separate patches that need to be integrated/relevance tested e.g. \nfor average doc length... maybe i'll do that next so we can get some concrete\nalternate sims in here before going any further.\n ",
            "author": "Robert Muir",
            "id": "comment-12986531"
        },
        {
            "date": "2011-03-28T02:49:26+0000",
            "content": "Updated patch, i brought the patch to trunk, cleaned up, enabled some more of the stats in scoring (e.g. totalTermFreq/sumOfTotalTermFreq).\n\nIn src/test i added a MockLMSimilarity, that implements \"Bayesian smoothing using Dirichlet priors\" from http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.136.8113\n\nThis one is interesting, as its faster than lucene's scoring formula today \n\nI want to get some of this stuff in shape for David (or any other GSOC students) to be able to implement their algorithms, but there is a lot of refactoring (e.g. explains) to do.\n\nI'll create a branch under https://svn.apache.org/repos/asf/lucene/dev/branches/flexscoring with this infrastructure in a bit.\n\nTonight i'll see if i can get the avg doc length stuff in the branch too. ",
            "author": "Robert Muir",
            "id": "comment-13011900"
        },
        {
            "date": "2011-07-07T14:10:28+0000",
            "content": "I think we need to commit the refactoring portions (separating TF-IDF out) to trunk very soon, because its really difficult\nto keep this branch in sync with trunk, e.g. lots of activity and refactoring going on.\n\nI'd like to get this merged in as quickly as possible. I don't think the svn history is interesting, especially given\nall the frustrations I am having with merging... The easiest way will be to commit a patch, I'll get everything in shape\nand upload one soon, like, today. ",
            "author": "Robert Muir",
            "id": "comment-13061332"
        },
        {
            "date": "2011-07-07T16:32:20+0000",
            "content": "\nI'd like to get this merged in as quickly as possible. I don't think the svn history is interesting, especially given\nall the frustrations I am having with merging... The easiest way will be to commit a patch, I'll get everything in shape\nand upload one soon, like, today.\n\n+1 even if this is not entirely in shape we can still iterate on trunk.  ",
            "author": "Simon Willnauer",
            "id": "comment-13061420"
        },
        {
            "date": "2011-07-07T16:45:49+0000",
            "content": "Attached is a patch, with this CHANGES entry:\n\n\n* LUCENE-2392: Decoupled vector space scoring from Query/Weight/Scorer. If you\n  extended Similarity directly before, you should extend TFIDFSimilarity instead.\n  Similarity is now a lower-level API to implement other scoring algorithms.\n  See MIGRATE.txt for more details.\n\n\n\nI would like to commit this, and then proceed onward with issues such as LUCENE-3220 and LUCENE-3221 ",
            "author": "Robert Muir",
            "id": "comment-13061433"
        },
        {
            "date": "2011-07-08T05:08:44+0000",
            "content": "Committed revision 1144158. ",
            "author": "Robert Muir",
            "id": "comment-13061769"
        }
    ]
}