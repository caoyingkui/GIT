{
    "id": "SOLR-6184",
    "title": "Replication fetchLatestIndex always failed, that will occur the recovery error.",
    "details": {
        "affect_versions": "4.6,                                            4.6.1",
        "status": "Open",
        "fix_versions": [],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "Usually the copy full index 70G need 20 minutes at least, 100M read/write network or disk r/w.  If in the 20 minutes happen one hard commit, that means the copy full index snap pull will be failed, the temp folder will be removed because it is failed pull task. \nIn the production, update index will happen in every minute, redo pull task always failed because index always change. \n\nAnd also always redo the pull it will occur the network and disk usage keep the high level.\n\nFor my suggestion, the fetchLatestIndex can be do again in some frequency. Don't need remove the tmp folder, and copy the largest index at first. Redo the fetchLatestIndex don't download the same biggest file again, only will copy the commit index just now, at last the task will be easy success.",
    "attachments": {
        "Solr-6184.txt": "https://issues.apache.org/jira/secure/attachment/12651636/Solr-6184.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Shawn Heisey",
            "id": "comment-14039320",
            "date": "2014-06-20T20:36:48+0000",
            "content": "Would the commitReserveDuration parameter on the replication handler be useful in keeping Solr from deleting the commit point that is being replicated until after the replication is complete?  Normally it's not recommended to have any config parameters for replication, but if a very large index is having problems recovering when there is a lot of update activity, perhaps that would be an exception. "
        },
        {
            "author": "Raintung Li",
            "id": "comment-14040673",
            "date": "2014-06-23T12:20:53+0000",
            "content": "How to estimate the duration? You will keep the update in the memory, and need think to avoid the OOM case. "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-14193955",
            "date": "2014-11-02T18:50:56+0000",
            "content": "Raintung Li Did you try increasing the commitReserveDuration parameter? Reserving a commit point would ensure that the index files corresponding to the latest commit point being fetched won't be deleted (due to, for example, lucene segment merges). \n\nSince it takes ~20 minutes to fetch the index, could you try setting this to ~20-25 minutes, maybe? "
        }
    ]
}