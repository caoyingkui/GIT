{
    "id": "LUCENE-3849",
    "title": "position increments should be implemented by TokenStream.end()",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Bug",
        "fix_versions": [
            "4.5",
            "6.0"
        ],
        "affect_versions": "3.6,                                            4.0-ALPHA",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "if you have pages of a book as multivalued fields, with the default position increment gap\nof analyzer.java (0), phrase queries won't work across pages if one ends with stopword(s).\n\nThis is because the 'trailing holes' are not taken into account in end(). So I think in\nTokenStream.end(), subclasses of FilteringTokenFilter (e.g. stopfilter) should do:\n\n\nsuper.end();\nposIncAtt += skippedPositions;\n\n\n\nOne problem is that these filters need to 'add' to the posinc, but currently nothing clears\nthe attributes for end() [they are dirty, except offset which is set by the tokenizer].\n\nAlso the indexer should be changed to pull posIncAtt from end().",
    "attachments": {
        "LUCENE-3849.patch": "https://issues.apache.org/jira/secure/attachment/12541653/LUCENE-3849.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-08-20T21:18:20+0000",
            "content": "Here's a patch. There are some things I don't like about it though.\n\nAgain to explain the situation:\n\n\tbuggy today, we call end() and then read a dirty offsetAttribute. nothing clears atts before end().\n\twe need to support multiple removers in the chain, each applying their posInc just like they do in incrementToken\n\tbecause of that the atts cannot be dirty.\n\n\n\nSo in this patch, i just call clearAttributes() in tokenstream.end() by default instead of doing nothing. this works, except that means when IW consumes this 'final posinc' there is an OB1, because posIncAtt's default value is 1. I don't like that.\n\nalternatively we could have tokenstream explicitly set posIncrAtt to 0 in end() instead of clearAttributes()? I'm just wondering if thats any better really.\n\nOtherwise the patch is straightforward, with the exception of IW's built-in keywordtokenizer (StringField.java). that one is not actually setting end(), we were just relying upon dirty atts, so thats why i changed it. ",
            "author": "Robert Muir",
            "id": "comment-13438219"
        },
        {
            "date": "2012-08-20T21:25:33+0000",
            "content": "+1 to call a full clearAttributes(). After calling end(), we have a new \"state\" of the TokenStream and havin e.g. the CharTermAttribute or other atts. In addition we should also check all Tokenizers to set the the correct endOffset (end of stream). ",
            "author": "Uwe Schindler",
            "id": "comment-13438227"
        },
        {
            "date": "2012-08-20T21:28:43+0000",
            "content": "What about the \"off-by-one\" though? Because the problem is a full clearAttributes (which seems correct), sets the posIncAtt to a default of 1.\n\nThis makes consuming the tokenstream awkward because you have to subtract 1 in this case. should we do something like clearAttributes() + posIncAtt.setPositionIncrement(0) ? ",
            "author": "Robert Muir",
            "id": "comment-13438232"
        },
        {
            "date": "2012-08-20T21:30:05+0000",
            "content": "\nIn addition we should also check all Tokenizers to set the the correct endOffset (end of stream).\n\nThis is checked by BaseTokenStream test already. its just that currently offsetAtt is the only thing that we consume from end(), and all tokenizers effectively \"overwrite\" it with the correct values. So analyzers tests already pass, the only buggy one was the built-in keywordtokenizer in StringField. ",
            "author": "Robert Muir",
            "id": "comment-13438234"
        },
        {
            "date": "2012-08-20T21:36:46+0000",
            "content": "should we do something like clearAttributes() + posIncAtt.setPositionIncrement(0) ?\n\nwhere does the \"positionIncrementGap\" come in? because my naive impression is that the end() method is precisely where something like posIncAtt.setPositionIncrement(getPositionIncrementaGap()) should be called.\n ",
            "author": "Hoss Man",
            "id": "comment-13438237"
        },
        {
            "date": "2012-08-20T21:44:01+0000",
            "content": "\nwhere does the \"positionIncrementGap\" come in? because my naive impression is that the end() method is precisely where something like posIncAtt.setPositionIncrement(getPositionIncrementaGap()) should be called.\n\nPositionIncrementGap is not related to this bug, because its something done separately by indexwriter. Its always factored in correctly, its not buggy.\n\nThe \"bug\" is if a field instance ends with a stopword, that accumulated 'hole' is lost.\n\ndoc.add(new TextField(\"body\", \"just a\", Field.Store.NO));\ndoc.add(new TextField(\"body\", \"test of gaps\", Field.Store.NO));\niw.addDocument(doc);\n...\nPhraseQuery pq = new PhraseQuery();\npq.add(new Term(\"body\", \"just\"), 0);\npq.add(new Term(\"body\", \"test\"), 2);\n// body:\"just ? test\"\nassertEquals(1, is.search(pq, 5).totalHits); // FAIL!\n\n\n\nSo the problem is the first instance of the field loses its hole for \"a\" (a stopword that was removed),\nonly because it ended on a stopword, so incrementToken() returned false and there was no subsequent token\nto apply the 'hole' to.\n\nThis is the same problem as an instance of a field ending with say a space character and all the offsets being wrong,\nthis was why end() was added, so the indexer can pull 'end of stream state'. ",
            "author": "Robert Muir",
            "id": "comment-13438244"
        },
        {
            "date": "2012-08-20T23:23:28+0000",
            "content": "here is an updated patch, fixing all the analyzers and also removing the -1.\n\nAdditionally i fixed basetokenstreamtestcase to apply the checkClearAttributes etc before end() too, and fixed things like StandardTokenizer that sometimes act like stopfilters (remove too-long tokens) to apply these in end(), in case they happen to appear at the end of a multivalued field.\n\nThere are unrelated problems in facets/ it has lots of tokenstreams that never call clearAttributes. I will open an issue to clean that up, its not possible to proceed until those tokenstreams are fixed. ",
            "author": "Robert Muir",
            "id": "comment-13438300"
        },
        {
            "date": "2013-08-17T14:45:29+0000",
            "content": "New patch, syncing up to trunk, and fixing various tokenizers that had bugs ... tests pass. ",
            "author": "Michael McCandless",
            "id": "comment-13742938"
        },
        {
            "date": "2013-08-19T17:01:34+0000",
            "content": "New patch, resolving the nocommit and adding a test.\n\nI think it's ready. ",
            "author": "Michael McCandless",
            "id": "comment-13743997"
        },
        {
            "date": "2013-08-20T03:49:01+0000",
            "content": "Thanks for bringing this back to life Mike.\n\nHow did you deal with facets? Is this stuff out of date now that it no longer encodes with payloads? It was the big barrier to my previous patch: lots of tokenstreams never calling clearAtts at all (LUCENE-4318)\n\nIf its no longer a problem, lets mark that issue resolved.\n\nPatch looks good to me, though it would be good to temporarily make end() final or something in TokenStream.java and review all tokenstreams that have an impl to make sure its ok. \n\nI still have my reservations about the posInc=0 stuff (i wish this was cleaner), but I don't see a better way: and this is really a bug we should fix. ",
            "author": "Robert Muir",
            "id": "comment-13744671"
        },
        {
            "date": "2013-08-20T11:43:38+0000",
            "content": "I think because facet module cutover from payloads to DV, many of the problematic TokenStreams disappeared?  But there was still one, inside DirectoryTaxonomyWriter, that I fixed in the patch ... it now calls clearAttributes and sets each att on incrementToken.\n\nThat's a good idea on end(); I'll do that and check all impls.\n\nI don't see a better way than setting posInc to 0 in end ... and I agree this bug is bad.  It can also affects suggesters, e.g. if it uses ShingleFilter after StopFilter and the user's last word was a stop word. ",
            "author": "Michael McCandless",
            "id": "comment-13744879"
        },
        {
            "date": "2013-08-20T11:45:42+0000",
            "content": "\nThat's a good idea on end(); I'll do that and check all impls.\n\nI did this: i dont think there are any issues (or you fixed them in the patch).\n\nSo I think when this issue is committed we should resolve LUCENE-4318. Thank you! ",
            "author": "Robert Muir",
            "id": "comment-13744880"
        },
        {
            "date": "2013-08-20T12:12:20+0000",
            "content": "So I think when this issue is committed we should resolve LUCENE-4318.\n\nOK, will do! ",
            "author": "Michael McCandless",
            "id": "comment-13744909"
        },
        {
            "date": "2013-08-20T17:13:07+0000",
            "content": "Commit 1515887 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1515887 ]\n\nLUCENE-3849: end() now sets position increment, so any trailing holes are counted ",
            "author": "ASF subversion and git services",
            "id": "comment-13745142"
        },
        {
            "date": "2013-08-20T18:13:46+0000",
            "content": "Commit 1515909 from Michael McCandless in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1515909 ]\n\nLUCENE-3849: end() now sets position increment, so any trailing holes are counted ",
            "author": "ASF subversion and git services",
            "id": "comment-13745225"
        },
        {
            "date": "2013-08-20T22:27:19+0000",
            "content": "Commit 1515994 from Michael McCandless in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1515994 ]\n\nLUCENE-3849: fix some more test only TokenStreams ",
            "author": "ASF subversion and git services",
            "id": "comment-13745541"
        },
        {
            "date": "2013-08-20T22:39:55+0000",
            "content": "Commit 1516001 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1516001 ]\n\nLUCENE-3849: fix some more test only TokenStreams ",
            "author": "ASF subversion and git services",
            "id": "comment-13745563"
        },
        {
            "date": "2013-10-05T10:19:20+0000",
            "content": "4.5 release -> bulk close ",
            "author": "Adrien Grand",
            "id": "comment-13787123"
        }
    ]
}