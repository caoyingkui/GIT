{
    "id": "LUCENE-1032",
    "title": "CJKAnalyzer should convert half width katakana to full width katakana",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "2.0.0",
        "resolution": "Duplicate",
        "status": "Resolved"
    },
    "description": "Some of our Japanese customers are reporting errors when performing searches using half width characters.\nThe desired behavior is that a document containing half width characters should be returned when performing a search using full width equivalents or when searching by the half width character itself.\nCurrently, a search will not return any matches for half width characters.\n\nHere is a test case outlining desired behavior (this may require a new Analyzer).\n\n\n\n\npublic class TestJapaneseEncodings extends TestCase\n{\n\n    byte[] fullWidthKa = new byte[]{(byte) 0xE3, (byte) 0x82, (byte) 0xAB};\n    byte[] halfWidthKa = new byte[]{(byte) 0xEF, (byte) 0xBD, (byte) 0xB6};\n\n    public void testAnalyzerWithHalfWidth() throws IOException\n    {\n        Reader r1 = new StringReader(makeHalfWidthKa());\n        TokenStream stream = new CJKAnalyzer().tokenStream(\"foo\", r1);\n        assertNotNull(stream);\n        Token token = stream.next();\n        assertNotNull(token);\n        assertEquals(makeFullWidthKa(), token.termText());\n    }\n\n    public void testAnalyzerWithFullWidth() throws IOException\n    {\n        Reader r1 = new StringReader(makeFullWidthKa());\n        TokenStream stream = new CJKAnalyzer().tokenStream(\"foo\", r1);\n        assertEquals(makeFullWidthKa(), stream.next().termText());\n    }\n\n    private String makeFullWidthKa() throws UnsupportedEncodingException\n    {\n        return new String(fullWidthKa, \"UTF-8\");\n    }\n\n    private String makeHalfWidthKa() throws UnsupportedEncodingException\n    {\n        return new String(halfWidthKa, \"UTF-8\");\n    }\n}",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2008-03-11T05:30:32+0000",
            "content": "I think this feature should merged to https://issues.apache.org/jira/browse/LUCENE-1215\n\nUnicode compatibility decomposition will fix this issue.  ",
            "author": "Hiroaki Kawai",
            "id": "comment-12577309"
        },
        {
            "date": "2010-04-22T20:06:20+0000",
            "content": "Normalization is implemented in LUCENE-2399 ",
            "author": "Robert Muir",
            "id": "comment-12859986"
        }
    ]
}