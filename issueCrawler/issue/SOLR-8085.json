{
    "id": "SOLR-8085",
    "title": "Fix a variety of issues that can result in replicas getting out of sync.",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.4",
            "6.0"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Critical"
    },
    "description": "I've been discussing this fail I found with Yonik.\n\nThe problem seems to be that a replica tries to recover and publishes recovering - the attempt then fails, but docs are now coming in from the leader. The replica tries to recover again and has gotten enough docs to pass peery sync.\n\nI'm trying a possible solution now where we won't allow peer sync after a recovery that is not successful.",
    "attachments": {
        "SOLR-8085.patch": "https://issues.apache.org/jira/secure/attachment/12761905/SOLR-8085.patch",
        "fail.150922_130608": "https://issues.apache.org/jira/secure/attachment/12761685/fail.150922_130608",
        "fail.150922_125320": "https://issues.apache.org/jira/secure/attachment/12761682/fail.150922_125320"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-09-22T17:00:38+0000",
            "author": "Yonik Seeley",
            "content": "Here's a fail from HdfsChaosMonkeySafeLeaderTest on trunk (with some minor patches provided by Mark).\n\nThrowable #1: java.lang.AssertionError: shard3 is not consistent.  Got 658 from http://127.0.0.1:44649/collection1lastClient and got 330 from http://127.0.0.1:38249/collection1 ",
            "id": "comment-14902983"
        },
        {
            "date": "2015-09-22T17:18:25+0000",
            "author": "Yonik Seeley",
            "content": "Another one that has a smaller log file.\n\nThrowable #1: java.lang.AssertionError: shard3 is not consistent.  Got 375 from http://127.0.0.1:40940/collection1lastClient and got 230 from http://127.0.0.1:47239/collection1 ",
            "id": "comment-14903025"
        },
        {
            "date": "2015-09-22T21:33:52+0000",
            "author": "Mark Miller",
            "content": "I'm trying a possible solution now where we won't allow peer sync after a recovery that is not successful.\n\nThis seems to work out well. \n\nMost of these issues show up easier when running the more heavyweight hdfs tests though, so I am running those - because we truncate support in hdfs was not added until 2.7, when a replication fails, we still replay all the buffered docs. A badly timed server restart can then pass incorrectly on peer sync if this happens, regardless of my attempted fix that won't cross jetty restarts. ",
            "id": "comment-14903479"
        },
        {
            "date": "2015-09-23T14:42:57+0000",
            "author": "Yonik Seeley",
            "content": "OK, here's an analyisis of fail.150922_130608\n\nit looks like LIR happens during normal recovery after startup, and we finally end up doing recovery with recoverAfterStartup=false, which uses recent versions in peersync (and include docs that have been buffered while we were recovering) rather than the true startup versions.  This causes peersync to pass when it should not have.\n\n\n(add first appears, node 47239 appears to be coming up at the time)\n  2> 74317 INFO  (qtp324612161-378) [n:127.0.0.1:40940_ c:collection1 s:shard3 r:core_node6 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] webapp= path=/update params={update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:38911/collection1/&wt=javabin&version=2} {add=[0-333 (1513033816377131008)]} 0 19\n  2> 74317 INFO  (qtp324612161-378) [n:127.0.0.1:40940_ c:collection1 s:shard3 r:core_node6 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] webapp= path=/update params={update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:38911/collection1/&wt=javabin&version=2} {add=[0-333 (1513033816377131008)]} 0 19\n  2> 74317 INFO  (qtp661063741-234) [n:127.0.0.1:38911_ c:collection1 s:shard3 r:core_node2 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] webapp= path=/update params={wt=javabin&version=2} {add=[0-333 (1513033816377131008)]} 0 31\n  \n(node coming up)\n  2> 75064 INFO  (coreLoadExecutor-196-thread-1-processing-n:127.0.0.1:47239_) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.VersionInfo Refreshing highest value of _version_ for 256 version buckets from index\n  2> 75065 INFO  (coreLoadExecutor-196-thread-1-processing-n:127.0.0.1:47239_) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.UpdateLog Took 31.0ms to seed version buckets with highest version 1513033812159758336\n  2> 75120 INFO  (coreZkRegister-190-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.ZkController Replaying tlog for http://127.0.0.1:47239/collection1/ during startup... NOTE: This can take a while.\n  2> 75162 DEBUG (recoveryExecutor-199-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.UpdateLog add add{flags=a,_version_=1513033807303802880,id=1-3}\n  2> 75162 DEBUG (recoveryExecutor-199-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE add{flags=a,_version_=1513033807303802880,id=1-3} LocalSolrQueryRequest{update.distrib=FROMLEADER&log_replay=true}\n\n(replay finished)\n  2> 75280 DEBUG (recoveryExecutor-199-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE add{flags=a,_version_=1513033812159758336,id=1-132} LocalSolrQueryRequest{update.distrib=FROMLEADER&log_replay=true}\n \n(meanwhile, the leader is asking us to recover?)\n  2> 75458 WARN  (updateExecutor-14-thread-5-processing-x:collection1 r:core_node2 http:////127.0.0.1:47239//collection1// n:127.0.0.1:38911_ s:shard3 c:collection1) [n:127.0.0.1:38911_ c:collection1 s:shard3 r:core_node2 x:collection1] o.a.s.c.LeaderInitiatedRecoveryThread Asking core=collection1 coreNodeName=core_node10 on http://127.0.0.1:47239 to recover; unsuccessful after 2 of 120 attempts so far ...\n  \n(and we see the request to recover)\n  2> 75475 INFO  (qtp2087242119-1282) [n:127.0.0.1:47239_    ] o.a.s.h.a.CoreAdminHandler It has been requested that we recover: core=collection1\n\n(so we cancel the existing recovery)\n  2> 75478 INFO  (Thread-1246) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.DefaultSolrCoreState Running recovery - first canceling any ongoing recovery\n\n  2> 75552 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Starting recovery process. recoveringAfterStartup=true\n\n  2> 75610 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy ###### startupVersions=[1513033812159758336, [...]\n  2> 75611 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Publishing state of core collection1 as recovering, leader is http://127.0.0.1:38911/collection1/ and I am http://127.0.0.1:47239/collection1/\n  2> 75611 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.ZkController publishing state=recovering\n \n  2> 75642 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Sending prep recovery command to http://127.0.0.1:38911; WaitForState: action=PREPRECOVERY&core=collection1&nodeName=127.0.0.1%3A47239_&coreNodeName=core_node10&state=recovering&checkLive=true&onlyIfLeader=true&onlyIfLeaderActive=true\n\n(In the meantime, we still receive updates.  1-414 is one of the updates that we are missing!!!  I assume we are buffering these?)\n  2> 75782 DEBUG (qtp2087242119-1281) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE add{,id=1-414} {update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:38911/collection1/&wt=javabin&version=2}\n  2> 75828 INFO  (qtp2087242119-1281) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] webapp= path=/update params={update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:38911/collection1/&wt=javabin&version=2} {add=[1-414 (1513033817943703552)]} 0 45\n\n(what's this about?)\n  2> 75923 INFO  (recoveryExecutor-199-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.s.SolrIndexSearcher Opening Searcher@2ba72586[collection1] main\n\n(wait... replay? is this from when we started up and we never stopped it?  should we have?)\n  2> 76135 DEBUG (recoveryExecutor-199-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE FINISH LocalSolrQueryRequest{update.distrib=FROMLEADER&log_replay=true}\n\n\n(log replay finishes... the dbq *:* shows that this is the tlog from the beginning of this test.. i.e. we never got to do a commit at first)\n  2> 76181 INFO  (recoveryExecutor-199-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] {deleteByQuery=*:* (-1513033805984694272),add=[1-3 (1513033807303802880), 0-5 (1513033807434874880), 1-6 (1513033807628861440), 1-7 (1513033807699116032), 1-9 (1513033807797682176), 0-14 (1513033807941337088), 0-15 (1513033807976988672), 1-14 (1513033807991668736), 1-15 (1513033808035708928), 0-17 (1513033808075554816), ... (61 adds)]} 0 1060\n  2> 76182 WARN  (recoveryExecutor-199-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.UpdateLog Log replay finished. recoveryInfo=RecoveryInfo{adds=61 deletes=0 deleteByQuery=1 errors=0 positionOfStart=0}\n\n(we're canceling recoveryh again?)\n  2> 76182 INFO  (coreZkRegister-190-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.DefaultSolrCoreState Running recovery - first canceling any ongoing recovery\n  2> 76186 WARN  (coreZkRegister-190-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Stopping recovery for core=collection1 coreNodeName=core_node10\n  \n(did this fail because we canceled the recovery?)\n  2> 76194 ERROR (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Error while trying to recover.:java.util.concurrent.ExecutionException: org.apache.solr.client.solrj.SolrServerException: IOException occured when talking to server at: http://127.0.0.1:38911\n  2> \tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n  2> \tat java.util.concurrent.FutureTask.get(FutureTask.java:192)\n  2> \tat org.apache.solr.cloud.RecoveryStrategy.sendPrepRecoveryCmd(RecoveryStrategy.java:598)\n  2> \tat org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:361)\n  2> \tat org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:227)\n  2> Caused by: org.apache.solr.client.solrj.SolrServerException: IOException occured when talking to server at: http://127.0.0.1:38911\n  2> \tat org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:590)\n  2> \tat org.apache.solr.client.solrj.impl.HttpSolrClient$1.call(HttpSolrClient.java:285)\n  2> \tat org.apache.solr.client.solrj.impl.HttpSolrClient$1.call(HttpSolrClient.java:281)\n  2> \tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n  2> \tat org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor$1.run(ExecutorUtil.java:231)\n  2> \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n  2> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n  2> \tat java.lang.Thread.run(Thread.java:745)\n  2> Caused by: java.net.SocketException: Socket closed\n  2> 76202 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Finished recovery process.\n\n(note: this is the last we hear of this thread... not clear if that's OK or not)\n  2> 76202 INFO  (coreZkRegister-190-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.ActionThrottle The last recovery attempt started 711ms ago.\n  2> 76202 INFO  (coreZkRegister-190-thread-1-processing-n:127.0.0.1:47239_ x:collection1 s:shard3 c:collection1 r:core_node10) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.ActionThrottle Throttling recovery attempts - waiting for 9288ms\n  \n (still receiving updates, while we are waiting to do the next recovery attempt...  are these still buffering? how do we tell?)\n  2> 76252 DEBUG (qtp2087242119-1283) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE add{,id=0-432} {update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:38911/collection1/&wt=javabin&version=2}\n\n\n(OK, looks like we are finally trying the recovery process again... but note that \"recoveringAfterStartup\" is now false!!!!)\n  2> 85491 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Starting recovery process. recoveringAfterStartup=false\n  2> 85531 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Publishing state of core collection1 as recovering, leader is http://127.0.0.1:38911/collection1/ and I am http://127.0.0.1:47239/collection1/\n\n(Peersync.  Because recoveringAfterStartup is false, it will use most recent versions to compare, rather than startup versions)\n(As an example, version 1513033825996767232 is very recent, at timestamp 83465, while we were still not recovered)\n  2> 92554 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy Attempting to PeerSync from http://127.0.0.1:38911/collection1/ - recoveringAfterStartup=false\n  2> 92554 DEBUG (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.PeerSync PeerSync: core=collection1 url=http://127.0.0.1:47239 startingVersions=100 [1513033825996767232, [...]\n  2> 92646 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.PeerSync PeerSync: core=collection1 url=http://127.0.0.1:47239  Received 100 versions from http://127.0.0.1:38911/collection1/\n  2> 92647 DEBUG (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.PeerSync PeerSync: core=collection1 url=http://127.0.0.1:47239  sorted versions from http://127.0.0.1:38911/collection1/ = [1513033825996767232, ...\n  2> 92647 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.PeerSync PeerSync: core=collection1 url=http://127.0.0.1:47239  Our versions are newer. ourLowThreshold=1513033822028955648 otherHigh=1513033825172586496\n  2> 92647 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.u.PeerSync PeerSync: core=collection1 url=http://127.0.0.1:47239 DONE. sync succeeded\n\n(And we pass peersync when we should not)\n  2> 92796 INFO  (RecoveryThread-collection1) [n:127.0.0.1:47239_ c:collection1 s:shard3 r:core_node10 x:collection1] o.a.s.c.RecoveryStrategy PeerSync Recovery was successful - registering as Active.\n\n  2> ######shard3 is not consistent.  Got 375 from http://127.0.0.1:40940/collection1lastClient and got 230 from http://127.0.0.1:47239/collection1\n  2> ###### sizes=375,230\n  2> ###### Only in http://127.0.0.1:40940/collection1: [{_version_=1513033815669342208, id=0-295}, [...], {_version_=1513033816377131008, id=0-333}]\n \n\n ",
            "id": "comment-14904585"
        },
        {
            "date": "2015-09-23T15:18:40+0000",
            "author": "Yonik Seeley",
            "content": "OK, here's one possible patch I think. ",
            "id": "comment-14904639"
        },
        {
            "date": "2015-09-23T15:24:18+0000",
            "author": "Mark Miller",
            "content": "but note that \"recoveringAfterStartup\" is now false!!!!)\n\nYeah, lot's of ways to lose the class and start over - so if you really want a field to persist it has to be static. ",
            "id": "comment-14904653"
        },
        {
            "date": "2015-09-23T15:28:56+0000",
            "author": "Mark Miller",
            "content": "Looked at the patch - yeah, or put it on the core state  ",
            "id": "comment-14904662"
        },
        {
            "date": "2015-09-23T19:48:28+0000",
            "author": "Yonik Seeley",
            "content": "Yeah, It can't be static because each core needs it's own state.\nWe could also maintain it as a normal variable in RecoveryStrategy and either reuse RecoveryStrategy objects, or initialize future objects from past objects.  Thoughts on best approach? ",
            "id": "comment-14905123"
        },
        {
            "date": "2015-09-23T22:40:12+0000",
            "author": "Mark Miller",
            "content": "Could be a static map or something in RecoveryStrategy too - but seeing as we already store another variable like this in the default state, made a lot of sense to me.\n\nWith your patch and running on a patched version of 4.10.3, I was still only seeing one other type of fail.\n\nDocs that came in during recovery after publishing recovering and before buffering would end up interfering with and causing a false peer sync pass if enough of them came in.\n\nI seemed to have worked around this issue by buffering docs before peer sync and before publishing as RECOVERING (the signal for the leader to start sending updates).\n\nWith my current runs using no deletes, I have not yet found a fail after this on this version of the code. ",
            "id": "comment-14905435"
        },
        {
            "date": "2015-09-23T22:42:18+0000",
            "author": "Mark Miller",
            "content": "Should mention one other change as I am mostly testing Hdfs version of this chaosmonkey test - after talking to Yonik I also fixed an issue where because we don't have truncate support we were replaying buffered docs on fail to get past them - really we should not do that as it can lead to bad peer sync passes and I have a fix for that as well. I'll file a separate JIRA issue for that one. ",
            "id": "comment-14905439"
        },
        {
            "date": "2015-09-24T19:02:21+0000",
            "author": "Mark Miller",
            "content": "For the last comment I filed SOLR-8094. ",
            "id": "comment-14906838"
        },
        {
            "date": "2015-09-24T19:10:15+0000",
            "author": "Mark Miller",
            "content": "Here is a patch I just ported to trunk that starts buffering before peer sync and before we publish as RECOVERING. \n\nWith Yonik Seeley's fix and the couple I have, I've been able to hit 200 HdfsChaosMonkey test runs without replica inconsistency on my 4.10.3 + backports based code for the first time. ",
            "id": "comment-14906848"
        },
        {
            "date": "2015-09-28T16:22:45+0000",
            "author": "Mark Miller",
            "content": "Here is a patch I just ported to trunk that starts buffering before peer sync and before we publish as RECOVERING.\n\nIt looks like this may pose a problem for shard splitting. ",
            "id": "comment-14933521"
        },
        {
            "date": "2015-09-28T16:49:06+0000",
            "author": "Mark Miller",
            "content": "NM - the patch was missing a change. Here is a new patch that combines with Yonik's. ",
            "id": "comment-14933554"
        },
        {
            "date": "2015-10-02T14:40:15+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1706423 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1706423 ]\n\nSOLR-8085: Fix a variety of issues that can result in replicas getting out of sync. ",
            "id": "comment-14941212"
        },
        {
            "date": "2015-10-02T14:45:45+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1706424 from Mark Miller in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1706424 ]\n\nSOLR-8085: Fix a variety of issues that can result in replicas getting out of sync. ",
            "id": "comment-14941215"
        },
        {
            "date": "2015-10-02T14:46:08+0000",
            "author": "Mark Miller",
            "content": "I also have a test change we may need to prevent false fails - I'll spin that off into it's own issue. ",
            "id": "comment-14941216"
        },
        {
            "date": "2015-10-02T21:12:04+0000",
            "author": "Joel Bernstein",
            "content": "Mark Miller, I'm getting errors in the TestSQLHandler test cases, which extends AbstractFullDistribZkTestBase, following this commit. In the test case it looks like one of the shards is not having documents added to it. Still investigating... ",
            "id": "comment-14941771"
        },
        {
            "date": "2015-10-02T21:33:50+0000",
            "author": "Joel Bernstein",
            "content": "I think I'm using the wrong method to index documents and this ticket just exposed that.  ",
            "id": "comment-14941803"
        },
        {
            "date": "2015-10-02T21:56:36+0000",
            "author": "Joel Bernstein",
            "content": "Ok, the issue was that I wasn't calling: waitForRecoveriesToFinish(false);\n\nOnce I added this the test passes.  ",
            "id": "comment-14941841"
        },
        {
            "date": "2015-10-04T04:51:27+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Good catch guys! Thanks for fixing this. Special thanks to Yonik for analysing the failure logs. ",
            "id": "comment-14942554"
        },
        {
            "date": "2015-10-04T14:16:16+0000",
            "author": "Mark Miller",
            "content": "I also have a test change we may need to prevent false fails - I'll spin that off into it's own issue.\n\nSOLR-8121 ",
            "id": "comment-14942679"
        }
    ]
}