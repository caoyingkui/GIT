{
    "id": "SOLR-6136",
    "title": "ConcurrentUpdateSolrServer includes a Spin Lock",
    "details": {
        "affect_versions": "4.6,                                            4.6.1,                                            4.7,                                            4.7.1,                                            4.7.2,                                            4.8,                                            4.8.1",
        "status": "Resolved",
        "fix_versions": [
            "4.10",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Critical",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "ConcurrentUpdateSolrServer.blockUntilFinished() includes a Spin Lock. This causes an extremely high amount of CPU to be used on the Cloud Leader during indexing.\n\nHere is a summary of our system testing. \n\nImporting data on Solr4.5.0: \nThroughput gets as high as 240 documents per second.\n\n[tomcat@solr-stg01 logs]$ uptime \n09:53:50 up 310 days, 23:52, 1 user, load average: 3.33, 3.72, 5.43\n\nPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND \n9547 tomcat 21 0 6850m 1.2g 16m S 86.2 5.0 1:48.81 java\n\nImporting data on Solr4.7.0 with no replicas: \nThroughput peaks at 350 documents per second.\n\n[tomcat@solr-stg01 logs]$ uptime \n10:03:44 up 311 days, 2 min, 1 user, load average: 4.57, 2.55, 4.18\n\nPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND \n9728 tomcat 23 0 6859m 2.2g 28m S 62.3 9.0 2:20.20 java\n\nImporting data on Solr4.7.0 with replicas: \nThroughput peaks at 30 documents per second because the Solr machine is out of CPU.\n\n[tomcat@solr-stg01 logs]$ uptime \n09:40:04 up 310 days, 23:38, 1 user, load average: 30.54, 12.39, 4.79\n\nPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND \n9190 tomcat 17 0 7005m 397m 15m S 198.5 1.6 7:14.87 java",
    "attachments": {
        "wait___notify_all.patch": "https://issues.apache.org/jira/secure/attachment/12648349/wait___notify_all.patch",
        "SOLR-6136.patch": "https://issues.apache.org/jira/secure/attachment/12655220/SOLR-6136.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Brandon Chapman",
            "id": "comment-14017838",
            "date": "2014-06-04T16:41:19+0000",
            "content": "Applying the patch from the linked ticket to Solr 4.5 will cause the same issue to be present in Solr 4.5. "
        },
        {
            "author": "Brandon Chapman",
            "id": "comment-14017842",
            "date": "2014-06-04T16:44:15+0000",
            "content": "Attached patch for Solr 4.7.1 drastically improves performance. The patch is a workaround of the spin lock by using a simple wait / notify mechanism. It is not a suggestion on how to fix ConcurrentUpdateSolrServer for an official release. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-14017896",
            "date": "2014-06-04T17:23:34+0000",
            "content": "Thanks for the patch Brandon! I'll start working on this issue tomorrow unless someone can dig into it sooner. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14055104",
            "date": "2014-07-08T16:37:01+0000",
            "content": "I'll def help with any thoughts or review on this one - important issue to fix. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-14055636",
            "date": "2014-07-08T22:57:43+0000",
            "content": "Thanks Mark. My strategy is to use Brandon's wait/notify around the runners object, which seems clean to me. The only other thing his patch needs is to use your fix for SOLR-4260 (the check for non-empty queue and no runners). Unless you guys see a problem with using wait/notify on the runners object? \n\nMain thing is the testing ... I've yet to actually see this block of code show up as a CPU hotspot in my testing  I'm trying some things on a CPU constrained node with complex docs to see if I can get it to trigger. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14055839",
            "date": "2014-07-09T04:41:14+0000",
            "content": "Main thing is the testing ... I've yet to actually see this block of code show up as a CPU hotspot in my testing  I'm trying some things on a CPU constrained node with complex docs to see if I can get it to trigger.\n\nIt might be easier to trigger if you send docs individually at a high rate instead of a batching them. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-14056571",
            "date": "2014-07-09T18:33:57+0000",
            "content": "Thanks Shalin ... that helped a little. Here's where I'm at with this: using the current \"spin-lock\" code on trunk (without wait/notify), VisualVM shows some self CPU time in the blockUntilFinished, with Brandon's patch + the fix for SOLR-4260 added back in, the self CPU time is negligible, which aligns with what Brandon reported  So I think I'm convinced this is a good improvement and am now going to run some stress tests on this and then post a patch for review. Also, surprisingly, CUSS didn't have a unit test case for it, so I'm building that too. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-14056673",
            "date": "2014-07-09T19:55:24+0000",
            "content": "Mark - just curious - what was the thinking on using only 1 thread for the CUSS used by StreamingSolrServers? Specifically, I'm looking at this code in StreamingSolrServers:\n\n      server = new ConcurrentUpdateSolrServer(url, httpClient, 100, 1, updateExecutor, true) {\n\nIf I had to guess it was that you didn't want to over-complicate debugging when this code was new? Wondering if you see value in increasing the thread count a little? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14056681",
            "date": "2014-07-09T20:02:44+0000",
            "content": "I intended on making it configurable eventually, but I think it should default to 1.  We should be artful about spinning up too many threads by default for a single request I think. Once we fix distributed deadlock, I'd like it if you could largely control the number of threads proportionally just by using the container thread pool sizing config.  "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-14056790",
            "date": "2014-07-09T21:39:42+0000",
            "content": "Ok, makes sense ... I'm wondering if it doesn't make sense to be a little more flexible if the queue is filling up? Currently, the code only allocates a new Runner if:\n\n          if (runners.isEmpty() || (queue.remainingCapacity() < queue.size() && runners.size() < threadCount)) {\n\nMy thinking is to relax this a little to allow an additional runner if the queue is half full but otherwise just keep it at 1. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14057445",
            "date": "2014-07-10T13:16:25+0000",
            "content": "My thinking is to relax this a little to allow an additional runner if the queue is half full but otherwise just keep it at 1.\n\nThat sounds okay to me - my only worry is doing something that spins up too many threads for a small queue. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-14058855",
            "date": "2014-07-11T14:54:04+0000",
            "content": "Here's a patch based largely on Brandon's original patch, using wait / notifyAll instead of the spin lock in blockUntilFinished. As mentioned above, VisualVM shows good evidence of this improvement in that the amount of CPU spent in the block method is negligible with this patch (and very noticeable without it).\n\nI've also included the first cut at a unit test for CUSS. There's probably more things we can do to exercise the logic in CUSS, so let me know if you have any other ideas for the unit test.\n\nBrandon - please try this patch out in your environment if possible. I'll plan to commit this to trunk and backport to 4x branch in a few days after keeping on eye on things in Jenkins. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-14058858",
            "date": "2014-07-11T14:56:08+0000",
            "content": "btw - I decided to not mess with the threadCount stuff Mark and I were discussing here as that should be handled under another \"improvement\" ticket after doing some benchmarking to show if it even helps. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14058910",
            "date": "2014-07-11T15:31:04+0000",
            "content": "\n-            final UpdateRequest updateRequest = queue.poll(250,\n-                TimeUnit.MILLISECONDS);\n+            final UpdateRequest updateRequest = \n+                queue.poll(pollQueueTime, TimeUnit.MILLISECONDS);\n             if (updateRequest == null)\n\n\n\nKnow when that bug was introduced? If it went out in 4.9, that is a pretty severe performance bug if you are not streaming or batching big. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-14058916",
            "date": "2014-07-11T15:45:16+0000",
            "content": "not sure about that one ... just something I caught while working on this issue "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14059082",
            "date": "2014-07-11T17:46:25+0000",
            "content": "Ah, I see, it's a different poll call. I only had this take affect on one of the poll calls because that was enough to relieve the performance issue in my benchmarks. I think it makes sense to expand it to the this other poll call as well.\n\n+1 on the patch, looks okay to me, tests pass locally.\n\nI don't want to think about testing CUSS at the moment, but nice work on a new test for it. Will be great to have it grow - this has been a troublesome class to stabilize over the years. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14106810",
            "date": "2014-08-22T13:10:21+0000",
            "content": "I don't know if it's this or not, but right around when this went in I started seeing the rare hang at:\n\n [junit4]    >         at org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrServer.blockUntilFinished(ConcurrentUpdateSolrServer.java:374)\n   [junit4]    >         at org.apache.solr.update.StreamingSolrServers.blockUntilFinished(StreamingSolrServers.java:103)\n   [junit4]    >         at org.apache.solr.update.SolrCmdDistributor.blockAndDoRetries(SolrCmdDistributor.java:228)\n   [junit4]    >         at org.apache.solr.update.SolrCmdDistributor.finish(SolrCmdDistributor.java:89)\n\n "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-14106898",
            "date": "2014-08-22T14:34:07+0000",
            "content": "ugh ... weird though as the notifyAll call that would free up the hang at that spot is in a finally block you'd think it would be getting called correctly ... the only path I can see that notify wouldn't get called is the queue never empties.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14106907",
            "date": "2014-08-22T14:39:15+0000",
            "content": "I filed SOLR-6406 to track this. "
        }
    ]
}