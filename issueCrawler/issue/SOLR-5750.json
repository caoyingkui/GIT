{
    "id": "SOLR-5750",
    "title": "Backup/Restore API for SolrCloud",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "6.1"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Sub-task",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "We should have an easy way to do backups and restores in SolrCloud. The ReplicationHandler supports a backup command which can create snapshots of the index but that is too little.\n\nThe command should be able to backup:\n\n\tSnapshots of all indexes or indexes from the leader or the shards\n\tConfig set\n\tCluster state\n\tCluster properties\n\tAliases\n\tOverseer work queue?\n\n\n\nA restore should be able to completely restore the cloud i.e. no manual steps required other than bringing nodes back up or setting up a new cloud cluster.\n\nSOLR-5340 will be a part of this issue.",
    "attachments": {
        "SOLR-5750.patch": "https://issues.apache.org/jira/secure/attachment/12732576/SOLR-5750.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Noble Paul",
            "id": "comment-13907174",
            "date": "2014-02-20T16:54:12+0000",
            "content": "\n\tIt should also be able to take a backup of a specific collection only\n\tOption of compressing data which is backed up\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13910268",
            "date": "2014-02-24T13:04:09+0000",
            "content": "\nIt should also be able to take a backup of a specific collection only\nOption of compressing data which is backed up\n\n+1 "
        },
        {
            "author": "Robert Parker",
            "id": "comment-13937924",
            "date": "2014-03-17T15:32:51+0000",
            "content": "You should have the option of backing up/replicating a live searchable collection on SolrCloud A to a live searchable collection across a WAN on SolrCloud B, each with their own separate ZooKeeper ensemble.   You should also be able to rename the collection on the fly so that the live searchable collection on SolrCloud A is called \"collectionA\" and its live updated searchable replication copy is known as \"collectionB\" so as to allow a single remote instance of SolrCloud to be multi-homed to act as a replication target for multiple other Solr instances' collections, even if those collections happen to have the same name on each of their source instances.  Also, WAN compression/optimization would be helpful as well. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13971319",
            "date": "2014-04-16T12:57:46+0000",
            "content": "Move issue to Solr 4.9. "
        },
        {
            "author": "Mark Bennett",
            "id": "comment-14196367",
            "date": "2014-11-04T17:12:30+0000",
            "content": "At least 2 clients that I know of have recently asked about this.  Would be awesome! "
        },
        {
            "author": "Damien Kamerman",
            "id": "comment-14522594",
            "date": "2015-05-01T01:09:58+0000",
            "content": "Only snapshot if the index version has changed. "
        },
        {
            "author": "Varun Thacker",
            "id": "comment-14541921",
            "date": "2015-05-13T13:37:52+0000",
            "content": "First pass at the feature.\n\nBACKUP:\nRequired params - collection, name, location\n\nExample API: \n/admin/collections?action=backup&name=my_backup&location=/my_location&collection=techproducts\n\nIt will create a backup directory called my_location inside which it will store the following -\n\n/my_location\n /my_backup\n  /shard1\n  /shard2\n  /zk_backup\n   /zk_backup/configs/configName ( The config which was being used for the backup collection )\n   /zk_backup/collection_state.json ( Always store the cluster state for that collection in collection_state.json )\n   /backup.properties ( Metadata about the backup )\n\nIf you have setup any aliases or roles or any other special property then that will not be backed up. That might not be that useful to restore as the it could be restored in some other cluster. We can add it later if its required.\n\nBACKUPSTATUS:\nRequired params - name\n\nExample API: /admin/collections?action=backupstatus&name=my_backup\n\nRESTORE:\nRequired params - collection, name, location\n\nExample API: /admin/collections?action=restore&name=my_backup&location=/my_location&collection=techproducts_restored\n\nYou can't restore into an existing collection. Provide a collection name where you want to restore the index into. The restore process will create the collection similar to the backed up collection and restore the indexes.\n\nRestoring in the same collection would be simple to add. But in that case we should only restore the indexes.\n\nRESTORESTATUS:\nRequired params - name\n\nExample API: /admin/collections?action=restorestatus&name=my_backup\n\nWould appreciate a review on this. I'll work on adding more tests "
        },
        {
            "author": "Varun Thacker",
            "id": "comment-14548686",
            "date": "2015-05-18T19:49:31+0000",
            "content": "Updated patch to trunk and improved the existing test case to also check in scenarios when an implicit router is used. "
        },
        {
            "author": "Greg Solovyev",
            "id": "comment-14551169",
            "date": "2015-05-19T20:33:48+0000",
            "content": "Varun Thacker if I am understanding the code correctly, when a collection has multiple shards spread over multiple nodes, data from each shard will be backed up only on that shard's node, metadata from ZK will be saved on whichever node happens to handle the backup request. Am I missing something here? "
        },
        {
            "author": "Varun Thacker",
            "id": "comment-14601699",
            "date": "2015-06-25T18:40:31+0000",
            "content": "Updated patch to trunk. "
        },
        {
            "author": "Varun Thacker",
            "id": "comment-14601709",
            "date": "2015-06-25T18:44:26+0000",
            "content": "So the idea is the location that you give should be a shared file system so that all the replica backup along with the ZK information stay in one place. Then during restore the same location can be used. \n\nWe can then support storing to other locations such as s3, hdfs etc as separate Jiras then "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-14601771",
            "date": "2015-06-25T19:13:27+0000",
            "content": "See my comment on SOLR-7374 - we should have something pluggable here. "
        },
        {
            "author": "Varun Thacker",
            "id": "comment-14602533",
            "date": "2015-06-26T07:55:35+0000",
            "content": "Hi Jan,\n\nIf you look at the patch most of the backup/restore work is being offloaded to the ReplicationHandler. So to make the underlying storage pluggable we just need to make the changes there. The only change required with respect to this patch is the backup of zk files. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14611757",
            "date": "2015-07-02T10:16:53+0000",
            "content": "2 design suggestions\n\n1) Move the operations to CollectionsHandler . When the process starts add the backup name and the node that is processing the task to ZK\n2) Do not restore all the replicas at once . Just create one replica of a shard first and then do ADDREPLICA till you have enough replicas "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-15007546",
            "date": "2015-11-16T22:48:57+0000",
            "content": "Reviving this. What about getting a basic, even if unperfect, cloud backup command into the hands of users, then followup with improvements? Have not reviewed the patch but looks like it can add great value already as is. "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-15007595",
            "date": "2015-11-16T23:09:39+0000",
            "content": "I'm planning to take a look at getting something like that up for shared file systems (hdfs).  That's an easier problem in that the underlying storage mechanism already supports snapshots and taking a snapshot doesn't require going to every shard (assuming reasonable data dir settings). "
        },
        {
            "author": "Varun Thacker",
            "id": "comment-15009198",
            "date": "2015-11-17T18:32:06+0000",
            "content": "HI Jan,\n\nThanks for looking into this. Let me bring up the patch to date and post it again.  "
        },
        {
            "author": "Varun Thacker",
            "id": "comment-15013141",
            "date": "2015-11-19T08:30:01+0000",
            "content": "Patch which is updated to trunk.\n\nOne change is the way restore works . Folding in Noble's suggestion this is how a restore works \n\n\n\tcreates a coreless collection for the indexes to be restored into\n\tMarks all shards of it as CONSTRUCTION\n\tCreates one replica for each shard and restores the indexes\n\tCalls add replica for each shard to reach the replicationFactor\n\n\n\nIt still needs iterating to make the command async. "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-15014824",
            "date": "2015-11-20T00:11:45+0000",
            "content": "Did a cursory look at this.  Some questions/comments:\n\n1) I know we already have \"location\" via https://cwiki.apache.org/confluence/display/solr/Making+and+Restoring+Backups+of+SolrCores but it just seems needlessly risky / error prone.  What if a user purposefully or accidentally overwrites important data?  You are giving anyone making a snapshot call solr's permissions.  Beyond that, making \"location\" a required param is not the greatest interface.  Most of the time when I'm taking a snapshot I don't even care where it is, I expect the system to just do something sensible and let me interact with the API with some id (i.e. name).  HDFS and HBase snapshots work in this way, for example.  Why not just have a backup location specified in solr.xml with some sensible default?\n2) On the above point: \"I expect the system to just do something sensible and let me interact with the API with some id (i.e. name)\" \u2013 why do I pass in a location for RESTORE?  Can't the system just remember that from the backup call?\n3) There's no API for deleting a snapshot?\n4) There's no API for listing snapshots? (I don't think this needs to be in an initial version necessarily)\n5) From So the idea is the location that you give should be a shared file system so that all the replica backup along with the ZK information stay in one place. Then during restore the same location can be used.\nWe can then support storing to other locations such as s3, hdfs etc as separate Jiras then\n\nI'm not sure the shard-at-a-time makes sense for shared file systems.  For example, it's much more efficient to take an hdfs snapshot of the entire collection directory than of each individual shard.  I haven't fully thought through how we support both, e.g. we could do something different based on the underlying storage of the collection (though that wouldn't let you backup a local FS collection to a shared FS), or allow a \"snapshotType\" parameter or something.  I think we can just make whatever you have the default here, so I don't think we strictly need to do anything in this patch.  Just pointing that out. "
        },
        {
            "author": "Varun Thacker",
            "id": "comment-15014898",
            "date": "2015-11-20T00:49:23+0000",
            "content": "1) I know we already have \"location\" via https://cwiki.apache.org/confluence/display/solr/Making+and+Restoring+Backups+of+SolrCores but it just seems needlessly risky / error prone. What if a user purposefully or accidentally overwrites important data? You are giving anyone making a snapshot call solr's permissions. Beyond that, making \"location\" a required param is not the greatest interface. Most of the time when I'm taking a snapshot I don't even care where it is, I expect the system to just do something sensible and let me interact with the API with some id (i.e. name). HDFS and HBase snapshots work in this way, for example. Why not just have a backup location specified in solr.xml with some sensible default?\n2) On the above point: \"I expect the system to just do something sensible and let me interact with the API with some id (i.e. name)\" \u2013 why do I pass in a location for RESTORE? Can't the system just remember that from the backup call?\n\nNice idea. We could let users specify it within the solr.xml solrcloud tag . \nI don't think it should default to anything and if not specified snapshots should fail. The reason being users not on a shared filesystem need to specify a shared mount for this to work.\nAnd by specifying it within the solr.xml file , even the restore command wouldn't need the user to specify the location which will solve your concern in the 2nd point\n\n\n<solr>\n  <solrcloud>\n    <str name=\"snapshotDirectory\">${snapshotDirectory:}</str> <!-- the path specified here should be a shared mount accessible by all nodes in the cluster for backup/restore to work on non shared file systems -->\n  </solrcloud>\n</solr>\n\n\n "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-15014909",
            "date": "2015-11-20T00:55:09+0000",
            "content": "I don't think it should default to anything and if not specified snapshots should fail. The reason being users not on a shared filesystem need to specify a shared mount for this to work.\n\nWe can't default to some path relative to the shard's dataDir? "
        },
        {
            "author": "Varun Thacker",
            "id": "comment-15014918",
            "date": "2015-11-20T00:59:09+0000",
            "content": "We can't default to some path relative to the shard's dataDir?\n\nWhen we do a restore, the current impl. restores into a fresh collection. So the backed up data can be on another node not accessible to the restoring collection shard. "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-15014946",
            "date": "2015-11-20T01:11:18+0000",
            "content": "When we do a restore, the current impl. restores into a fresh collection. So the backed up data can be on another node not accessible to the restoring collection shard.\n\nHard times on a non-shared file system .  In that case, sure, require it to be set for non-shared file systems (can do it for all filesystems for now, we can change in the future).  Or, restore on the same node as the existing shard where you took the snapshot to remove the shared mount requirement? "
        },
        {
            "author": "Varun Thacker",
            "id": "comment-15014960",
            "date": "2015-11-20T01:16:07+0000",
            "content": "I think the best approach is to make sure \"snapshotDirectory\" is mandatory if DirectoryFactory.isShared() is false . If it is a shared drive we can have the path default to a directory relative to dataDir. "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-15014972",
            "date": "2015-11-20T01:20:03+0000",
            "content": "I think the best approach is to make sure \"snapshotDirectory\" is mandatory if DirectoryFactory.isShared() is false . If it is a shared drive we can have the path default to a directory relative to dataDir.\n\nsgtm, thanks. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15014991",
            "date": "2015-11-20T01:32:47+0000",
            "content": "Might be a bit hackey and certainly not necessary for first impl, but would be great if we could do hard snapshots if we detect a UNIX like OS.  "
        },
        {
            "author": "David Smiley",
            "id": "comment-15126396",
            "date": "2016-02-01T15:34:02+0000",
            "content": "What's left to do on this issue?  Does it actually work?  The feedback I've seen seem to be around improvements to make it better \u2013 i.e. using a shared filesystem resulting in no actual copying of files.  \n\nI looked at the patch.  On the restore side I noticed a loop of slices and then a loop of replicas starting with this comment: \"//Copy data from backed up index to each replica\".  Shouldn't there be just one replica per shard to restore, and then later the replicationFactor will expand to the desired level?\nAnother question is I wonder if any of these loops should be done in parallel or if they are issuing asynchronous requests so it isn't necessary.  It would help to document the pertinent loops with this information, and possibly do some in parallel if they should be done so. "
        },
        {
            "author": "Varun Thacker",
            "id": "comment-15179489",
            "date": "2016-03-04T07:11:21+0000",
            "content": "\n\tAdded SolrJ support for Backup and Restore Collection Admin actions\n\t2 API calls - Backup and Restore . Both support async and is recommended to use them for polling to see if the task completed. There are they BackupStatus and RestoreStatus commands like there were in previous patches.\n\n\n\nBackup:\nRequired Params - name and collection. \n\"location\" can be optionally set via the cluster prop api. If the query parameter does not have it we refer to the value set in the cluster prop api\n\nWhat it backs up in the location directory\n\n\tIndex data from the shard leaders\n\tcollection_state_backup.json ( the backed up collection state )\n\tbackup.properties ( meta-data information )\n\tconfigSet\n\n\n\nRestore:\nRequired Params - name and collection. \n\"location\" can be optionally set via the cluster prop api. If the query parameter does not have it we refer to the value set in the cluster prop api\n\nHow it works\n\n\tThe restore collection name should not be present . Restore will create it for you. You can use collection alias to use it once it has been restored. We purposely don\u2019t allow restoring into an existing collection since rolling back in a distributed setup would be tricky . Maybe in the future if we are confident we can allow this.\n\tCreates a core-less collection with the config set from the backup ( it appends a restore.configSetName to it for avoiding collissions )\n\tMarks the shards in \"construction\" state so that if someone is sending it documents they get buffered in the tlog . TODO don't do\n\tCreate one replica per shard and restore the data into this\n\tAdds the necessary replicas to meet the same replication factor\n\n\n\nAnother question is I wonder if any of these loops should be done in parallel or if they are issuing asynchronous requests so it isn't necessary. It would help to document the pertinent loops with this information, and possibly do some in parallel if they should be done so.\n\nYes that makes sense. We need to add this\n\nI looked at the patch. On the restore side I noticed a loop of slices and then a loop of replicas starting with this comment: \"//Copy data from backed up index to each replica\". Shouldn't there be just one replica per shard to restore, and then later the replicationFactor will expand to the desired level?\n\nYeah true. This patch has those changes.\n\nIt's still a work in progress. The restore needs hardening. "
        },
        {
            "author": "David Smiley",
            "id": "comment-15184501",
            "date": "2016-03-08T06:16:13+0000",
            "content": "Hello Varun.  I've been kicking the tires on this.  When I run it explicitly, it seems to work fine \u2013 no errors.  I also tested changing my IP address between backup & restore, simply by switching my network connection, and I'm glad to see the restored collection with the correct IPs.\n\nUnfortunately, the test has never succeeded for me.  Half the time I get an NPE from Overseer, the other half the time the test fails its asserts at the end but before then I connection failure problems that I have yet to look closer at.  The Overseer NPE is curious... clusterState.json is null causing an NPE when a log message is printed around ~line 214, log.info(\"processMessage: .....  I started to debug that a bit but moved on.  I plan to look further into these things but I wanted to report this progress.\n\nBTW I see no RestoreStatus in the patch.\n\nI was thinking maybe I'll throw up an updated patch in ReviewBoard, but I'm waiting on INFRA-11152 "
        },
        {
            "author": "Hrishikesh Gadre",
            "id": "comment-15200515",
            "date": "2016-03-17T22:25:59+0000",
            "content": "Varun Thacker Is it possible that when the backup command is sent, one of the shards (or specifically the shard leader) is in \"recovering\" state ? If yes what happens with the current implementation? "
        },
        {
            "author": "David Smiley",
            "id": "comment-15202486",
            "date": "2016-03-19T01:43:20+0000",
            "content": "I made a branch from master at the time of Varun's last patch and applied it, then merged in the latest from master, then did some development and committed and pushed it \"solr-5750\".  I also have an attached patch file.  The tests no pass consistently for me.  I know I fixed some bugs in this code.  I also added some nocommit comments, some related to this issue and some related to a bug I'm seeing in Overseer (which I'll file a separate issue for).\n\nSome nocommits:\n\n\tspecify what the restored config set name is; default to that of the original\n\toption of not restoring the config set; assume there's one there that is suitable\n\tshard hash ranges aren't restored; this error could be disasterous\n\tuser defined collection properties aren't restored, nor are snitches and perhaps some other props.  IMO it's better to copy everything over to the extent we can.  The user is free to edit the backup properties to their needs before restoring.\n\n "
        },
        {
            "author": "Hrishikesh Gadre",
            "id": "comment-15218993",
            "date": "2016-03-30T22:34:42+0000",
            "content": "Varun Thacker Is there a reason why we have introduced a core Admin API for Backup/restore instead of reusing the replication handler? "
        },
        {
            "author": "Varun Thacker",
            "id": "comment-15219600",
            "date": "2016-03-31T08:47:50+0000",
            "content": "Hi Hrishikesh,\n\nIs there a reason why we have introduced a core Admin API for Backup/restore instead of reusing the replication handler?\n\nWhen we issue a backup command and it's with an async param then the core admin handler already has the necessary hooks to deal with the async requests. Hence in my patch I added core admin api's for backup/restore . "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-15219645",
            "date": "2016-03-31T09:27:41+0000",
            "content": "Creates a core-less collection with the config set from the backup ( it appends a restore.configSetName to it for avoiding collissions )\nWhat if we backup two collections using the same config \"foo\"? After restore of the first collection, we will have a config \"restore.foo\", but what happens during restore of the second collection? Have not looked at the patch code. Should it not be a goal that a backup + nuke + restore leaves you with a state as close to the original as possible, i.e. with one config called \"foo\", used by both collections? Perhaps a restore option shareConfig=true which creates the config if missing, otherwise reuses what is there? "
        },
        {
            "author": "David Smiley",
            "id": "comment-15219901",
            "date": "2016-03-31T14:04:32+0000",
            "content": "Agreed; I noted this as a nocommit in my last comment. \"option of not restoring the config set; assume there's one there that is suitable\".  Or to be more precise, restore the configset but don't overwrite if it's already there. "
        },
        {
            "author": "Hrishikesh Gadre",
            "id": "comment-15220368",
            "date": "2016-03-31T18:19:41+0000",
            "content": "Varun Thacker Thanks for the info. That make sense.\n\nI am currently investigating Solr Backup/restore changes for HDFS. I found this old email thread which discusses different approaches,\nhttp://qnalist.com/questions/5433155/proper-way-to-backup-solr\n\nI think we settled on \"copying\" behavior for portability reason. I think we should make this a configurable behavior so that we can make the best use of capabilities of underlying file-system. May be we introduce a higher level API? I think the API should consider source and destination file-systems to figure out the \"correct\" and \"optimized\" behavior...\n\nAny thoughts?\n\nCC Shawn Heisey Mark Miller Yonik Seeley "
        },
        {
            "author": "Hrishikesh Gadre",
            "id": "comment-15227182",
            "date": "2016-04-05T21:41:27+0000",
            "content": "The current patch assumes that we are restoring on a cluster which does not contain the specified \"collection\". In case user wants to restore a snapshot on the same cluster, we would need to replace the index state (after verifying that the zk metadata state is valid). In this scenario, we would need to \"disable\" solr collection so that we can safely restore the snapshot. Please refer to SOLR-8950 for details. "
        },
        {
            "author": "David Smiley",
            "id": "comment-15227555",
            "date": "2016-04-06T01:57:38+0000",
            "content": "(I'm working moving this forward still; expect another update in a day or so)\n\nHrishikesh Gadre the current functionality in the patch allows one to specify a restored collection name.  In the event one is trying to restore without downtime, I think this can and should be accomplished with collection aliases.  So if you intend on needing that, you would create the collection at the outset with a sequence number 0. When restoring, use the next sequence number and then switch the alias to this new one, and then delete the old one.  I'll grant you that it's less ideal in that one has to plan for this in advance and do this little dance.   "
        },
        {
            "author": "Hrishikesh Gadre",
            "id": "comment-15227620",
            "date": "2016-04-06T02:59:10+0000",
            "content": "David Smiley Yes collection \"aliases\" is a nice option to satisfy above requirement. I have made few comments in SOLR-8950, mostly from usability perspective. Please share your thoughts there (just to keep all relevant discussion in one thread). "
        },
        {
            "author": "David Smiley",
            "id": "comment-15240649",
            "date": "2016-04-14T05:48:59+0000",
            "content": "I pushed some new commits to the branch: https://github.com/apache/lucene-solr/commits/solr-5750\nSome important bits:\n\n\tRestored conf name is configurable; won't overwrite existing. Defaults to original.\n\treplicationFactor and some other settings are customizable on restoration.\n\tShard/slice info is restored instead of reconstituted.  Thus shard hash ranges (from e.g. shard split) is restored.\n\n\n\nI still want to:\n\n\ttest asyncId\n\ttest property.customKey=customVal passes through\n\tresolve nocommits on how to pass config name & replicationFactor on the SolrJ side\n\n\n\nBeyond that there are just some nocommits of a cleanup/document nature.  I code review would be much appreciated.  Should we just use GitHub branches (do I need my own fork?) for that or ReviewBoard?\n\nPossibly in this issue but more likely in another, it would be great if collection restoration could honor CREATE_NODE_SET, snitches, and rules.  I need to see how much extra work that'd be but I don't want to delay this too much. "
        },
        {
            "author": "David Smiley",
            "id": "comment-15260749",
            "date": "2016-04-27T19:00:31+0000",
            "content": "I pushed changes to the branch and attached a patch.  The changes include:\n\n\ttest asyncId\n\trefactored some Snapshooter.create* logic which included fixing a bug in which a core backup wasn't reserving the IndexCommit\n\tsome small miscellaneous stuff to resolve nocommits\n\n\n\nI didn't add to the test that core properties made their wait to the restored core as I'm not sure exactly how to do that but I manually verified they got there.\n\nThere are just 2 nocommits to resolve:\n\n\tAre we sure we want the parameter \"name\" for backup & restore to be the backup/snapshot name and not the collection name, and furthermore are we sure we want the collection name to be the parameter \"collection\"?  I have no strong convictions but I see for other collection oriented commands we use \"name\" as the name of the collection.  The requests here extend CollectionSpecificAdminRequest which have a getParams that put the collection name into \"name\" but we override it... which gave me some pause to question if these parameter names are best.  Perhaps the backup name parameter could be \"snapshot\" or \"snapshotName\"?  (note that \"snapshotName\" shows up in some backup/snapshot related properties files).\n\tVarun Thacker I don't understand why Overseer.processMessage has case statements for RESTORE & BACKUP that do nothing.  At a minimum there should be comments there explaining why; it sure looks buggy the way it is.  I set breakpoints there and the test never hit it.  I don't understand the differentiation between Overseer.processMessage and OverseerCollectionMessageHandler.processMessage which seem remarkably similar and redundant.\n\n\n\nShalin Shekhar Mangar if you have time I would love a code review.\n\nOtherwise, I think it's committable.  Tests pass.  If I don't get a code review or further comments for that matter, I'll commit in a couple days.\n\nPropagating createNodeSet, snitch, and rule options can be a follow-on issue.  Using HDFS as a backup location can be another issue too. "
        },
        {
            "author": "Hrishikesh Gadre",
            "id": "comment-15260789",
            "date": "2016-04-27T19:26:16+0000",
            "content": "David Smiley Shalin Shekhar Mangar I am refactoring the the backup/restore implementation in the Overseer primarily to make it extensible. Although we can do that as part of a separate JIRA, it would be nice if we can get it done as part of this patch itself. I should be able to complete this in next day or two.\n\nAlso I filed SOLR-9038 to provide a lightweight alternative to the \"full\" backup solution implemented by this JIRA. Both these JIRAs need following functionality\n\n\tBacking up collection metadata\n\tRestore the collection (both index data and collection metadata).\n\n\n\nSince the APIs defined in this patch use the name \"snapshot\", I wonder how should we reconcile these two features? I could think of following options,\n\n\tprovide a way to override the behavior (e.g. as an API parameter OR some other global configuration)\n\trename the APIs here to indicate that its a \"full\" backup (e.g. CREATEBACKUP/DELETEBACKUP). This will be consistent with the current core level operations (BACKUP/RESTORE).\n\n\n\nPlease let me know your thoughts on this... "
        },
        {
            "author": "Hrishikesh Gadre",
            "id": "comment-15260892",
            "date": "2016-04-27T20:41:47+0000",
            "content": "Ok please ignore the last point regarding the API naming. I see that we are already using BACKUP/RESTORE as API names... "
        },
        {
            "author": "David Smiley",
            "id": "comment-15261542",
            "date": "2016-04-28T04:56:35+0000",
            "content": "I commented on SOLR-9038.  Should that issue come to pass, you're right that we need to differentiate a \"backup\" from a \"snapshot\" in our terminology, assuming we even want these names.  As it is, in some of the code I used \"snapshotName\" as a nod to the existing snapshooter code using such terminology. "
        },
        {
            "author": "David Smiley",
            "id": "comment-15264676",
            "date": "2016-04-29T20:22:09+0000",
            "content": "I'll give some more time for review, like maybe Monday unless there are further changes to be done from any review/feedback.  Some things I think I want to change (which I will do today):\n\n\tsimply remove the Overseer.processMessage case statements for RESTORE & BACKUP as they simply aren't used.  This resolves a nocommit.\n\tto avoid risk of confusion with \"snapshot\" possibly being a named commit (SOLR-9038) in the log statements and backup.properites I'll call it a backupName, not snapshotName.\n\n\n\nTentative CHANGES.txt is as follows:\n\n* SOLR-5750: Add /admin/collections?action=BACKUP and RESTORE assuming access to a shared file system.\n  (Varun Thacker, David Smiley)\n\n\n\nAbout the \"shared file system\" requirement, it occurred to me this isn't really tested; it'd be nice it if failed fast if not all shards can see the backup location's ZK backup export.  I'm working on ensuring the backup fails if all slices don't see the backup directory that should be created at the start of the backup process.  This seems a small matter of ensuring that SnapShooter.validateCreateSnapshot call mkdir (which will fail if the parent dir isn't there) and not mkdirs but I'm testing to ensure the replication handler's use of SnapShooter is fine with this; I think it is. "
        },
        {
            "author": "Hrishikesh Gadre",
            "id": "comment-15264704",
            "date": "2016-04-29T20:44:30+0000",
            "content": "David Smiley I am almost done with refactoring the patch. I will submit the patch in next couple of hours. \n\n>>to avoid risk of confusion with \"snapshot\" possibly being a named commit (SOLR-9038) in the log statements and backup.properites I'll call it a backupName, not snapshotName.\n\nI have already fixed this in my patch. "
        },
        {
            "author": "Hrishikesh Gadre",
            "id": "comment-15264910",
            "date": "2016-04-29T22:42:51+0000",
            "content": "David Smiley \n\nPlease take a look at following pull request https://github.com/apache/lucene-solr/pull/36\n\nUnfortunately I wasn't able to apply your latest patch. Hence I had to send a PR. I am still working on refactoring the \"restore\" API. But in the mean time any feedback would be great. "
        },
        {
            "author": "David Smiley",
            "id": "comment-15266098",
            "date": "2016-05-02T04:17:04+0000",
            "content": "I made some small updates to the branch like I said I would. I elected not to switch Snapshooter to use mkdir instead of mkdirs for back-compat concerns so I have the backup caller make this check instead.\n\nHrishikesh Gadre I took a look.  As this issue finally nears the end, I don't feel good about rushing into the multiple abstractions introduced in the P/R (not to say I don't like them).  Please file a new issue and add me as a Watcher.  I have some comments but I'll wait to put them there. "
        },
        {
            "author": "Hrishikesh Gadre",
            "id": "comment-15266708",
            "date": "2016-05-02T14:40:58+0000",
            "content": "David Smiley sure. I have filed SOLR-9055 and uploaded the patch. Please take a look and let me have your feedback. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15267951",
            "date": "2016-05-03T02:20:58+0000",
            "content": "Commit 70bcd562f98ede21dfc93a1ba002c61fac888b29 in lucene-solr's branch refs/heads/master from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=70bcd56 ]\n\nSOLR-5750: Add /admin/collections?action=BACKUP and RESTORE "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15268021",
            "date": "2016-05-03T03:28:27+0000",
            "content": "Commit dac044c94a33ebd655c1d5f5c628c83c75bf8697 in lucene-solr's branch refs/heads/branch_6x from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=dac044c ]\n\nSOLR-5750: Add /admin/collections?action=BACKUP and RESTORE\n(cherry picked from commit 70bcd56) "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-15270796",
            "date": "2016-05-04T15:13:29+0000",
            "content": "I am seeing a reproducible failure in TestCloudBackupRestore\n\n\n  2> NOTE: reproduce with: ant test  -Dtestcase=TestCloudBackupRestore -Dtests.method=test -Dtests.seed=4AD582D4894F28CF -Dtests.slow=true -Dtests.locale=fr-BE -Dtests.timezone=Europe/Simferopol -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1\n[19:07:22.801] ERROR   28.2s J0 | TestCloudBackupRestore.test <<<\n   > Throwable #1: org.apache.solr.client.solrj.SolrServerException: No collection param specified on request and no default collection has been set.\n   >    at __randomizedtesting.SeedInfo.seed([4AD582D4894F28CF:C281BD0E27B34537]:0)\n   >    at org.apache.solr.client.solrj.impl.CloudSolrClient.directUpdate(CloudSolrClient.java:590)\n   >    at org.apache.solr.client.solrj.impl.CloudSolrClient.sendRequest(CloudSolrClient.java:1073)\n   >    at org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:962)\n   >    at org.apache.solr.client.solrj.impl.CloudSolrClient.request(CloudSolrClient.java:898)\n   >    at org.apache.solr.client.solrj.SolrRequest.process(SolrRequest.java:149)\n   >    at org.apache.solr.client.solrj.SolrClient.commit(SolrClient.java:484)\n   >    at org.apache.solr.client.solrj.SolrClient.commit(SolrClient.java:463)\n   >    at org.apache.solr.cloud.TestCloudBackupRestore.test(TestCloudBackupRestore.java:105)\n\n "
        },
        {
            "author": "David Smiley",
            "id": "comment-15271173",
            "date": "2016-05-04T18:44:31+0000",
            "content": "Thanks for reporting; I'll dig. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15271339",
            "date": "2016-05-04T20:04:49+0000",
            "content": "Commit 13832b4f857bc6f726a4764aa446a487b847bfee in lucene-solr's branch refs/heads/branch_6x from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=13832b4 ]\n\nSOLR-5750: Fix test to specify the collection on commit\n(cherry picked from commit 0a20dd4) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15271341",
            "date": "2016-05-04T20:06:00+0000",
            "content": "Commit 0a20dd47d1abbf0036896fac03dc7d801ebcd5bd in lucene-solr's branch refs/heads/master from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0a20dd4 ]\n\nSOLR-5750: Fix test to specify the collection on commit "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15272334",
            "date": "2016-05-05T13:43:12+0000",
            "content": "Commit 18d933ee65320ab1cae92a79d6635996fee9e818 in lucene-solr's branch refs/heads/master from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=18d933e ]\n\nSOLR-5750: Fix test to specify the collection on add "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15272336",
            "date": "2016-05-05T13:43:51+0000",
            "content": "Commit 83640278860d58ac01a7233ea4a96a49a3b6843d in lucene-solr's branch refs/heads/branch_6x from David Smiley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=8364027 ]\n\nSOLR-5750: Fix test to specify the collection on add\n(cherry picked from commit 18d933e) "
        },
        {
            "author": "Varun Thacker",
            "id": "comment-15275476",
            "date": "2016-05-08T05:48:52+0000",
            "content": "Some docs on the feature . Any thoughts on this otherwise I'll add it over to the ref guide\n\nSolrCloud Backup and Restore of Collections\n\nBackup and Restore Solr collections and it's associated configurations to a shared filesystem - for example HDFS or a Network File System\n\nBackup command:\n\n/admin/collections?action=BACKUP&name=myBackupName&collection=myCollectionName&location=/path/to/my/shared/drive\n\nThe backup command will backup Solr indexes and configurations for a specified collection.\nThe backup command takes one copy from each shard for the indexes. For configurations it backs up the configSet that was associated with the collection and other meta information.\n\nkey/Type/Required/Default/Description\n\nname/string/yes/<empty>/The backup name\ncollection/string/yes/<empty>/The name of the collection that needs to be backed up\nlocation/string/no/The location on the shared drive for the backup command to write to. Alternately it can be set as a cluster property (hyperlink to CLUSTERPROP and document it as a supported property)\nasync ( copy over from existing docs)\n\nRestore command:\n\n/admin/collections?action=RESTORE&name=myBackupName&location=/path/to/my/sharded/drive&collection=myRestoredCollectionName\n\nRestores Solr indexes and associated configurations.\nThe restore operation will create a collection with the specified name from the collection parameter. You cannot restore into the same collection and the collection should not be present at the time of restoring the collection, Solr will create it for you. The collection created will be of the same number of shards and replicas as the original colleciton, preserving routing information etc. Optionally you can overide some parameters documented below. For restoring the associated configSet if a configSet with the same name exists in ZK then Solr will reuse that else it will upload the backed up configSet in ZooKeeper and use that for the restored collection.\n\nYou can use the Collection ALIAS (hyperlink) feature to make sure client's don't need to change the endpoint to query or index against the restored collection.\n\nkey/Type/Required/Default/Description\n\nname/string/yes/<empty>/The backup name that needs to be restored\ncollection/string/yes/<empty>/The collection where the indexes will be restored to.\nlocation/string/no/The location on the shared drive for the restore command to read from. Alternately it can be set as a cluster property (hyperlink to CLUSTERPROP and document it as a supported property)\n\n(copy over from existing docs)\nasync\ncollection.configName\nreplicationFactor\nmaxShardsPerNode\nautoAddReplicas\nproperty.Param\nstateFormat "
        },
        {
            "author": "Hrishikesh Gadre",
            "id": "comment-15276630",
            "date": "2016-05-09T17:08:11+0000",
            "content": "Varun Thacker Thanks for the documentation. I have few comments,\n\nBackup and Restore Solr collections and it's associated configurations to a shared filesystem - for example HDFS or a Network File System\n\nCurrently we don't support integration with HDFS directly. This is being done as part of SOLR-9055.\n\nlocation/string/no/The location on the shared drive for the backup command to write to. Alternately it can be set as a cluster property (hyperlink to CLUSTERPROP and document it as a supported property)\n\nI don't think we have added support for configuring location via CLUSTERPROP API. We have discussed an alternate approach of configuring file-systems (or Backup repositories) via solr.xml. So I think it would make more sense to configure \"default\" location as part of that change (instead of enabling it via CLUSTERPROP API). This is being done as part of SOLR-9055. "
        },
        {
            "author": "Tim Owen",
            "id": "comment-15585823",
            "date": "2016-10-18T15:56:20+0000",
            "content": "David Smiley you mentioned in the mailing list back in March that you'd fixed the situation where restored collections are created using the old stateFormat=1 but it still seems to be doing that ... did that fix not make it into this ticket before merging? We've been trying out the backup/restore and noticed it's putting the collection's state into the global clusterstate.json instead of where it should be. "
        },
        {
            "author": "Tim Owen",
            "id": "comment-15585826",
            "date": "2016-10-18T15:56:47+0000",
            "content": "David Smiley you mentioned in the mailing list back in March that you'd fixed the situation where restored collections are created using the old stateFormat=1 but it still seems to be doing that ... did that fix not make it into this ticket before merging? We've been trying out the backup/restore and noticed it's putting the collection's state into the global clusterstate.json instead of where it should be. "
        },
        {
            "author": "Hrishikesh Gadre",
            "id": "comment-15585936",
            "date": "2016-10-18T16:38:45+0000",
            "content": "Tim Owen David Smiley I think this is not yet implemented (due to some unit test failure ?).\n\nhttps://github.com/apache/lucene-solr/commit/70bcd562f98ede21dfc93a1ba002c61fac888b29#diff-e864a6be5b98b5340273c1db4f4677a6R107\n\nI am not sure why this problem exists just for restore operation (and not for create). "
        }
    ]
}