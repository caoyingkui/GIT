{
    "id": "SOLR-12785",
    "title": "Add test for activation functions in NeuralNetworkModel",
    "details": {
        "type": "Test",
        "status": "Open",
        "labels": "",
        "fix_versions": [],
        "components": [
            "contrib - LTR"
        ],
        "priority": "Minor",
        "resolution": "Unresolved",
        "affect_versions": "None"
    },
    "description": "",
    "attachments": {
        "SOLR-12785-A.patch": "https://issues.apache.org/jira/secure/attachment/12944863/SOLR-12785-A.patch",
        "SOLR-12785-B.patch": "https://issues.apache.org/jira/secure/attachment/12944864/SOLR-12785-B.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16622973",
            "content": "A couple of comments regarding the patch:\n\n\tI moved the default implementations to the Activation interface, and implemented in enums (this pattern is mentioned in \"Effective Java\", Item 3). I decided on the enum as activation is just a function; if another layer is created with the same activation as a previous layer, only one object will be created (i.e. if there are three layers with sigmoid activation, one object will be created, instead of three as it is now). Doing this also made it easier to test. If it is preferred that the implementations not be singletons or in an enum class, please let me know; any comments are appreciated.\n\tI also attempted to write the test for the activation functions without changing current implementation of DefaultLayer.setActivation(), however in doing so, I felt that I had to construct a NeuralNetworkModel object (with one input, one output, weight 1 and bias 0). See test-no-activation-change.txt, which I have attached.\n\n\n\nAny comments or advice would be greatly appreciated! ",
            "author": "Kamuela Lau",
            "date": "2018-09-21T02:26:36+0000"
        },
        {
            "id": "comment-16646707",
            "content": "If changes are accepted for additional activation functions (SOLR-12780), the test in the patch should be changed accordingly; if the tests (and edited implementation of default activation functions) is accepted here, the implementation of the activation functions in SOLR-12780 will have to change accordingly. ",
            "author": "Kamuela Lau",
            "date": "2018-10-11T16:17:06+0000"
        },
        {
            "id": "comment-16657890",
            "content": "Deleted old patches and added new ones to reflect the fact that Leaky Relu and TanH were added in SOLR-12780.\n\nA-patch adds tests without changing the implementation of default Activation implementations (via a network with exactly one input and one output node).\n\nB-patch changes implementation of default Activation implementations and adds tests for that. ",
            "author": "Kamuela Lau",
            "date": "2018-10-20T15:47:54+0000"
        }
    ]
}