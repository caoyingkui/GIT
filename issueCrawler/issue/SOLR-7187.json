{
    "id": "SOLR-7187",
    "title": "SolrCloud does not fully clean collection after delete",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "4.10.3",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "When I attempt to delete a collection using /admin/collections?action=DELETE&name=collection1 if I go into HDFS I will still see remnants from the collection. No files, but empty directories stick around.\n\n\n[root@solr1 ~]# sudo -u hdfs hdfs dfs -ls -R /solr/collection1\ndrwxr-xr-x   - solr solr          0 2015-03-03 15:42 /solr/collection1/core_node1\ndrwxr-xr-x   - solr solr          0 2015-03-03 15:42 /solr/collection1/core_node2\ndrwxr-xr-x   - solr solr          0 2015-03-03 15:42 /solr/collection1/core_node3\ndrwxr-xr-x   - solr solr          0 2015-03-03 15:42 /solr/collection1/core_node4\ndrwxr-xr-x   - solr solr          0 2015-03-03 15:42 /solr/collection1/core_node5\ndrwxr-xr-x   - solr solr          0 2015-03-03 15:42 /solr/collection1/core_node6\n\n\n\n(Edit: I had the wrong log portion here originally)\nIn the logs, after deleting all the data, I see:\n\n2015-03-03 16:15:14,762 INFO org.apache.solr.servlet.SolrDispatchFilter: [admin] webapp=null path=/admin/cores params={deleteInstanceDir=true&action=UNLOAD&core=collection1_shard5_replica1&wt=javabin&qt=/admin/cores&deleteDataDir=true&version=2} status=0 QTime=362 \n2015-03-03 16:15:14,787 INFO org.apache.solr.common.cloud.ZkStateReader: A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 1)\n2015-03-03 16:15:14,854 INFO org.apache.solr.cloud.DistributedQueue: LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged\n2015-03-03 16:15:14,879 INFO org.apache.solr.cloud.DistributedQueue: LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged\n2015-03-03 16:15:14,896 INFO org.apache.solr.cloud.DistributedQueue: LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged\n2015-03-03 16:15:14,920 INFO org.apache.solr.cloud.DistributedQueue: LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged\n2015-03-03 16:15:15,151 INFO org.apache.solr.cloud.DistributedQueue: LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged\n2015-03-03 16:15:15,170 INFO org.apache.solr.cloud.DistributedQueue: LatchChildWatcher fired on path: /overseer/queue state: SyncConnected type NodeChildrenChanged\n2015-03-03 16:15:15,279 INFO org.apache.solr.common.cloud.ZkStateReader: A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/clusterstate.json, has occurred - updating... (live nodes size: 1)\n2015-03-03 16:15:15,546 INFO org.apache.solr.cloud.DistributedQueue: LatchChildWatcher fired on path: /overseer/collection-queue-work/qnr-0000000016 state: SyncConnected type NodeDataChanged\n2015-03-03 16:15:15,562 INFO org.apache.solr.cloud.DistributedQueue: LatchChildWatcher fired on path: /overseer/collection-queue-work state: SyncConnected type NodeChildrenChanged\n2015-03-03 16:15:15,562 INFO org.apache.solr.cloud.OverseerCollectionProcessor: Overseer Collection Processor: Message id:/overseer/collection-queue-work/qn-0000000016 complete, response:{success={solr1.example.com:8983_solr={responseHeader={status=0,QTime=207}},solr1.example.com:8983_solr={responseHeader={status=0,QTime=243}},solr1.example.com:8983_solr={responseHeader={status=0,QTime=243}},solr1.example.com:8983_solr={responseHeader={status=0,QTime=342}},solr1.example.com:8983_solr={responseHeader={status=0,QTime=346}},solr1.example.com:8983_solr={responseHeader={status=0,QTime=362}}}}\n\n\n\nThis might be related to SOLR-5023, but I'm not sure.",
    "attachments": {
        "log.out.gz": "https://issues.apache.org/jira/secure/attachment/12702602/log.out.gz"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-03-04T06:38:16+0000",
            "author": "Ramkumar Aiyengar",
            "content": "Quite possibly different, but is the fix from SOLR-7011 applied? ",
            "id": "comment-14346485"
        },
        {
            "date": "2015-03-04T20:25:20+0000",
            "author": "Mike Drob",
            "content": "It was not originally applied, but I added it and see the same behavior. Also attaching DEBUG logs from the timeframe, in case they will help anything. ",
            "id": "comment-14347508"
        },
        {
            "date": "2015-03-04T23:19:03+0000",
            "author": "Mark Miller",
            "content": "I would guess the issue is:\n\nThere are a few options when deleting a core - remove instance dir and remove data dir being the two pertinent ones here.\n\nWhen not using hdfs, remove instance dir would deal with this higher level dir - but with hdfs, it wouldn't and only the data dir would be removed. ",
            "id": "comment-14347760"
        },
        {
            "date": "2015-03-06T13:59:29+0000",
            "author": "Mike Drob",
            "content": "The data dir is deleted by each shard individually when instructed to unload. This is good and makes sense.\n\nI'm trying to compare the cloud implementation in CollectionsHandler.handleDeleteAction with a non-cloud implementation, but I'm having trouble finding it. I do have a unit test that shows the same behavior on a non-hdfs SolrCloud, though. ",
            "id": "comment-14350353"
        },
        {
            "date": "2015-03-06T15:58:19+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "I'm trying to compare the cloud implementation in CollectionsHandler.handleDeleteAction with a non-cloud implementation, but I'm having trouble finding it.\n\nA non-cloud implementation would just call CoreAdminHandler.unload directly. The CollectionHandler.handleDeleteCollection eventually calls (via a Zookeeper queue) OverseerCollectionProcessor.deleteCollection which then calls CoreAdminHandler.unload. ",
            "id": "comment-14350503"
        },
        {
            "date": "2015-03-06T17:10:05+0000",
            "author": "Mike Drob",
            "content": "Yea, I traced the queues through from delete to unload. Based on your comments and running a few sample tests, it sounds like the difference in implementation is architectural, which gives me a little confusion to how we name things.\n\nData dir in HDFS: hdfs://localhost:44036/solr/delete_data_dir/core_node2/data\nInstance dir in HDFS: /tmp/solr.cloud.hdfs.StressHdfsTest-C14CE921F29EF7F3-001/tempDir-005/delete_data_dir_shard2_replica1\n\nData dir in non-HDFS: /tmp/solr.cloud.CreateDeleteCollectionTest-AD37514288D15339-001/tempDir-003/delete_data_dir_shard1_replica2/data/\nInstance dir in non-HDFS: /tmp/solr.cloud.CreateDeleteCollectionTest-AD37514288D15339-001/tempDir-003/delete_data_dir_shard1_replica2\n\nWhen we delete the instance dir, we are always looking at a local directory. I could wire up a patch to delete dataDir.getParent() when deleting the data direcotry if we are using HDFS, but that seems fragile. Maybe it makes the most sense to delete the entire collection dir as a post step, if we determine that we're on HDFS. My impression is that there is no common collection-wide local directory for non-HDFS use cases, even when multiple cores are hosted on the same server, which is why this wasn't seen outside of HDFS.\n\nIs the tempDir-003 part of the path a meaningful directory level or just an artifact of JUnit structuring; i.e. should we be worrying about deleting it when we delete a collection (we currently do not)? ",
            "id": "comment-14350586"
        },
        {
            "date": "2015-03-17T19:30:13+0000",
            "author": "Mike Drob",
            "content": "Took another crack at this, I haven't been able to find an elegant solution yet. The OverseerCollectionProcessor has no reference to the file system to do the directory clean-up. I was thinking that maybe we can add to the CoreAdminHandler as part of the unload command, if the parent directory is empty (i.e. we've deleted all the data in HDFS) then delete the core directory too. Following this logic, the handler delegates the deletes to the HdfsDirectoryFactory, but it's all actually implemented in CachingDirectoryFactory and only happens on close(). But by the time that we get to closing the directory factory, we've lost all information about whether we are deleting instance and data directories, we just have a list of things to remove and it's tought to extrapolate from there. So this approach might be a dead end.\n\nI think some of this will be made easier with zk=truth and it makes sense to revisit again at that point. ",
            "id": "comment-14365865"
        }
    ]
}