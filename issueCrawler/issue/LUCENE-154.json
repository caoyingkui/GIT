{
    "id": "LUCENE-154",
    "title": "[PATCH] Chinese Tokenizer, Analyzer, Filter",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Date: Thu, 29 Nov 2001 10:18:47 -0800 (PST)\nFrom: \"Yiyi Sun\" <yiyisun@yahoo.com>",
    "attachments": {
        "ASF.LICENSE.NOT.GRANTED--lucene-cn-analyzer-tokenizer-filter-src.zip": "https://issues.apache.org/jira/secure/attachment/12312272/ASF.LICENSE.NOT.GRANTED--lucene-cn-analyzer-tokenizer-filter-src.zip"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2003-10-13T23:18:28+0000",
            "content": "Created an attachment (id=8559)\nChinese Tokenizer, Analyzer, Filter (ZIP file) ",
            "author": "Otis Gospodnetic",
            "id": "comment-12321422"
        },
        {
            "date": "2003-12-24T02:56:42+0000",
            "content": "Added to Lucene Sandbox, although I don't yet understand how this is different\nfrom the implementation submitted in bug 23545 entry. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12321423"
        }
    ]
}