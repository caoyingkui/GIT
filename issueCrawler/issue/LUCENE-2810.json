{
    "id": "LUCENE-2810",
    "title": "Explore Alternate Stored Field approaches for highly redundant data",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/store"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Won't Fix",
        "status": "Resolved"
    },
    "description": "In some cases (logs, HTML pages w/ boilerplate, etc.), the stored fields for documents contain a lot of redundant information and end up wasting a lot of space across a large collection of documents.  For instance, simply compressing a typical log file often results in > 75% compression rates.  We should explore mechanisms for applying compression across all the documents for a field (or fields) while still maintaining relatively fast lookup (that being said, in most logging applications, fast retrieval of a given event is not always critical.)  For instance, perhaps it is possible to have a part of storage that contains the set of unique values for all the fields and the document field value simply contains a reference (could be as small as a few bits depending on the number of uniq. items) to that value instead of having a full copy.  Extending this, perhaps we can leverage some existing compression capabilities in Java to provide this as well.  \n\nIt may make sense to implement this as a Directory, but it might also make sense as a Codec, if and when we have support for changing storage Codecs.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2010-12-13T12:40:42+0000",
            "content": "For instance, perhaps it is possible to have a part of storage that contains the set of unique values for all the fields and the document field value simply contains a reference (could be as small as a few bits depending on the number of uniq. items) to that value instead of having a full copy. \n\nGrant, how would that be different to DocValues DerefVarBytes variant really. I don't think that the Stored Fields feature in lucene should be very much extended in its semantics. if somebody wants to use BDB or something else to store the values fine, other than that they should really use docValues and specialize for a certain usecase. Stored Fields support in Codec is far away IMO since we need to build quiet some API on top of the existing codec API to consume entire documents. Yet, that said - help is very welcome in the docValues branch....\n\n\nsimon ",
            "author": "Simon Willnauer",
            "id": "comment-12970796"
        },
        {
            "date": "2010-12-13T13:04:25+0000",
            "content": "I agree with Simon's words here 'specialize for a certain usecase'.\n\nI prefer for compression to not be inside lucene, but I think it belongs inside the app. \nHowever I think that we should make ensure we provide the APIs so that someone can implement compression in their app.\n\nThe problem to me is that I think its too tricky to have in lucene any compression that will be good 'in general' without this app-specific knowledge.\nJust like people use different compression for image files than they do ms-word documents, etc.\n\nProviding a \"general purpose\" compression with reasonable random access seems redundant, \nmodern filesystems will do this for you transparently (e.g. NTFS you just tell it that the .fdt should be compressed). ",
            "author": "Robert Muir",
            "id": "comment-12970800"
        },
        {
            "date": "2010-12-13T14:58:57+0000",
            "content": "We removed support for stupid GZIP compression support in stored fields in 3.0. What is the reason to reimplement that now?\n\nField compression can easily be done using binary fields and any compression you can think of. Do I miss something to understand the issue? ",
            "author": "Uwe Schindler",
            "id": "comment-12970833"
        },
        {
            "date": "2010-12-13T15:03:15+0000",
            "content": "I think Grant was looking for something that could compress across fields of different documents (i.e. where every document represents a log record). ",
            "author": "Yonik Seeley",
            "id": "comment-12970834"
        },
        {
            "date": "2010-12-13T15:10:03+0000",
            "content": "I think Grant was looking for something that could compress across fields of different documents (i.e. where every document represents a log record).\n\nright, I understand the reason for that issue and I think it makes sense to implement something like that on a codec basis as it has a whole bunch or limitations I guess. For instance how do you deal with partial field loading? I mean you have to decompress everything really unless you have a compression scheme that allows to do something like a block cipher though. Not sure if something like that is around so input is appreciated. Still unless we don' t have codec support for stored fields I think we should not do it though.\n\nsimon ",
            "author": "Simon Willnauer",
            "id": "comment-12970838"
        },
        {
            "date": "2010-12-13T15:32:23+0000",
            "content": "I think Grant was looking for something that could compress across fields of different documents (i.e. where every document represents a log record). \n\nYes, this is what I meant.  To the others, please go back and read what I wrote before you jump to conclusions.  I'm not talking about compressing a particular field or even a particular document.  I'm talking about alternate storage techniques for large quantities of repeated (or near repeated, potentially) documents.  It doesn't even have to be GZIP.  There are plenty of use cases for this and I believe it can be done effectively in Lucene with out messing up API's, etc.  And it does belong in Lucene b/c I don't want to have to introduce another storage technique.  It could be something as simple as a Directory implementation that handles stored fields differently underneath the hood, but all the APIs are the same.\n\nFor instance how do you deal with partial field loading?\n\nThat will depend, I haven't thought about implementation yet.  The simplest approach to this may be a shared area where all the unique docs live and then per document storage just contains a file offset pointer to the original doc.  Sure, it's not the highest compression one could get, but it could be pretty good without too much effort.  In that case, partial field loading would work just fine. ",
            "author": "Grant Ingersoll",
            "id": "comment-12970852"
        },
        {
            "date": "2010-12-13T15:38:08+0000",
            "content": "Grant i did read what you wrote, I'm not jumping to conclusions.\n\nAnd it does belong in Lucene b/c I don't want to have to introduce another storage technique.\n\nI don't think that we should add \"Compression 2.0\" just because of this reason. it might simply slow down other users who have high\nexpectations for compression. This is one of the reasons Compression 1.0 was removed right? Lots of users compressing things\nand only making things slower and worse.\n\nin any event, its useless to add any compression that doesn't beat what filesystems can already do on average. ",
            "author": "Robert Muir",
            "id": "comment-12970853"
        },
        {
            "date": "2010-12-13T15:56:00+0000",
            "content": "in any event, its useless to add any compression that doesn't beat what filesystems can already do on average.\n\nI'm not sure it's useless ... consider an application like Google Desktop Search developed on top of Lucene. You cannot force users to compress the installation folder, yet it'll still be valuable to have Lucene compress stuff on its own .. especially stuff that it chooses to store. Such applications are special in that they offer a service to the user, that's installed on his/her machine, and w/o control of the one that actually developed it. Therefore I find myself tuning my Lucene-based app as much as I can, and often don't rely on users enabling certain OS features (and who knows if one day those features will be gone?).\n\nToday I handle compressed fields by using Lucene's CompressionTools, and I'm generally happy with it. If however there will be a compressed-store that will improve the performance of my application by compressing the stored fields otherwise, achieving better compression ratio etc., it might be useful. Especially if its integration will be a no brainer.\n\nI think though we'd want to differentiate fields - not all of them should be compressed, because it means they'll need to be de-compressed, which might be expensive for some apps. ",
            "author": "Shai Erera",
            "id": "comment-12970860"
        },
        {
            "date": "2010-12-13T15:57:21+0000",
            "content": "Grant, I undestand what you are trying todo I just question if what you are proposing is something that really belongs into the core of if it should be done in a pluggable codec. If you can do it in a dir impl well then just go ahead and put it under misc. I don't know what should keep you from doing it. Yet, I think that seems much more like something for a codec and I think that support is needed desperately. If that is in place - we would not discuss that for long really....\n\nsimon ",
            "author": "Simon Willnauer",
            "id": "comment-12970861"
        },
        {
            "date": "2010-12-13T15:57:40+0000",
            "content": "Compression 1.0 was a different use case.  That was for compressing a single field and I agree it was a waste.  Where in my email did I say that users had to use it?  We have all kinds of alternate things.  And you are so hung up on the word compression.  I will change the name of this issue to something else without the word in it so you don't think this has to be some form of gzip, but instead is about alternate storage options that will benefit particular use cases. ",
            "author": "Grant Ingersoll",
            "id": "comment-12970862"
        },
        {
            "date": "2010-12-13T16:01:24+0000",
            "content": "I think though we'd want to differentiate fields - not all of them should be compressed, because it means they'll need to be de-compressed, which might be expensive for some apps.\n\nYes.\n\nYet, I think that seems much more like something for a codec and I think that support is needed desperately.\n\nAgreed.  And also agreed that this is something for contrib.  I never, ever thought it was something to be forced on the primary implementation. ",
            "author": "Grant Ingersoll",
            "id": "comment-12970864"
        },
        {
            "date": "2010-12-13T16:04:47+0000",
            "content": "\nProviding a \"general purpose\" compression with reasonable random access seems redundant,\nmodern filesystems will do this for you transparently (e.g. NTFS you just tell it that the .fdt should be compressed).\n\nThis may be valid for some people, but not everyone has the ability to tell their admin (or their downstream users) to turn on compression for a particular file. ",
            "author": "Grant Ingersoll",
            "id": "comment-12970868"
        },
        {
            "date": "2010-12-13T16:07:34+0000",
            "content": "Where in my email did I say that users had to use it?\n\nWe didnt force users to use the old compression either? But there are even emails on the userlists of someone asking 'where did compressed fields go'\nand we said the reasons why, and then sure enough they reported back that it only made their data larger and slower.\n\nSo, I'm not sure we should add something so app-dependent to lucene's core, as it depends very heavily on the content you are indexing.\nIf people see compression in the core APIs they are going to assume that it works well in the general purpose case, but I'm trying to say\nthats very tricky to do.\n\na trivial example, \ncase 1: perhaps your documents have many fields all redundant with each other.\ncase 2: This is very different from documents that have only 1 field thats heavy redundant and the rest are not, e.g. nearly unique metadata.\n\nFor these two use cases you need to implement the 'compression'/layout completely differently or you only introduce waste, in the case of many fields and wrong block size you just make things bigger and it acts like Compression 1.0 all over again. ",
            "author": "Robert Muir",
            "id": "comment-12970870"
        },
        {
            "date": "2010-12-13T16:09:46+0000",
            "content": "This may be valid for some people, but not everyone has the ability to tell their admin (or their downstream users) to turn on compression for a particular file.\n\nIn this case you can do it automatically via an API as a normal user (e.g. from your Directory) ",
            "author": "Robert Muir",
            "id": "comment-12970872"
        },
        {
            "date": "2010-12-13T16:20:02+0000",
            "content": "If people see compression in the core APIs\n\nLike I said, it can go in contrib.\n\nAgain, you seem to be hung up on the word compression, so let's stop using it.  I'm not necessarily talking about compression here, OK?  Compression is an example of an alternate storage technique, but it isn't the only way to solve this problem and as you point out it may not always be the best thing to do.  Having said that, I've seen enough applications from a very wide set of users over the years that I can see many use cases where going beyond our simple storage mechanisms would be useful and giving users alternate tools for storage is a good thing, especially since retrieving stored fields is almost always one of the biggest performance killers in real world applications. ",
            "author": "Grant Ingersoll",
            "id": "comment-12970876"
        },
        {
            "date": "2010-12-13T16:34:19+0000",
            "content": "Again, you seem to be hung up on the word compression, so let's stop using it. I'm not necessarily talking about compression here, OK\n\nTo me its compression either way, you can call it data deduplication if you want (modern filesystems do this too!).\n\nespecially since retrieving stored fields is almost always one of the biggest performance killers in real world applications.\n\nI haven't had this experience, please don't try to generalize for everyone. ",
            "author": "Robert Muir",
            "id": "comment-12970882"
        },
        {
            "date": "2010-12-13T16:40:12+0000",
            "content": "I haven't had this experience, please don't try to generalize for everyone.\n\nHence why I said \"almost always\".   I've seen a lot of places where it is one of the main problems (due to random disk access) and the ability to reorder/change storage would be beneficial. ",
            "author": "Grant Ingersoll",
            "id": "comment-12970885"
        },
        {
            "date": "2010-12-13T16:47:24+0000",
            "content": "and the ability to reorder/change storage would be beneficial.\n\nRight, i agree with the \"general ability\". What I am concerned with is any concrete implementation, as I believe that to be very app-specific.\n\nIn other words, we should make the storage flexible in general, definitely! This is completely unrelated to data redundancy, its just something we should do so that users can more easily do what makes sense for their app.\n\nBut I'm not certain we should even provide the fundamental building blocks for \"compression/duplication\". This gets complicated fast (e.g. patented algorithms and cryptographic hash functions), forget about some concrete implementation that puts these together in anything close to a general way.\n\nOther libraries likely provide this support better than we ever could, for lucene i think the focus shouldn't have anything to do with data redundancy in particular but just making the storage API in general so that everyone's needs are met, not just your log file needs. ",
            "author": "Robert Muir",
            "id": "comment-12970889"
        },
        {
            "date": "2010-12-13T16:52:17+0000",
            "content": "Other libraries likely provide this support better than we ever could, for lucene i think the focus shouldn't have anything to do with data redundancy in particular but just making the storage API in general so that everyone's needs are met, not just your log file needs.\n\nTotally agree.  I haven't talked too much on implementation yet.  Perhaps this alternate implementation is merely to bake in, via a contrib/module, one of these libraries underneath the hood.  As with all of this, documentation is the key.\n\nThis isn't just limited to log file needs, though, although that is probably one of the most extreme use cases where an alternate storage mechanism would be beneficial. ",
            "author": "Grant Ingersoll",
            "id": "comment-12970892"
        },
        {
            "date": "2012-08-22T04:35:13+0000",
            "content": "@Grant, I see this post is nearly two years old.\n\nI have exactly the same issue: Highly redundant data in the Document fields of an index.\n\nHas there been any solution to this? It would be great if you could share ",
            "author": "Sabbir Kumar Manandhar",
            "id": "comment-13439266"
        },
        {
            "date": "2012-08-22T10:46:11+0000",
            "content": "It's now possible, at least, via the Codecs.  I don't think there is any implementation yet, though. ",
            "author": "Grant Ingersoll",
            "id": "comment-13439398"
        },
        {
            "date": "2012-08-22T14:30:17+0000",
            "content": "any working example? ",
            "author": "Sabbir Kumar Manandhar",
            "id": "comment-13439566"
        },
        {
            "date": "2012-08-29T00:10:58+0000",
            "content": "Oops, I didn't know of this issue when I opened LUCENE-4226. It tries to solve a very similar issue I think! ",
            "author": "Adrien Grand",
            "id": "comment-13443679"
        },
        {
            "date": "2012-10-08T09:44:17+0000",
            "content": "Grant Ingersoll, I think we can close this issue given that LUCENE-4226 just got committed. Are you OK with that? ",
            "author": "Adrien Grand",
            "id": "comment-13471473"
        }
    ]
}