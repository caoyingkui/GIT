{
    "id": "LUCENE-4489",
    "title": "improve LimitTokenCountFilter and/or it's tests",
    "details": {
        "components": [],
        "fix_versions": [
            "4.1",
            "6.0"
        ],
        "affect_versions": "None",
        "priority": "Major",
        "labels": "",
        "type": "Improvement",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "spinning off a discussion about LimitTokenCountFilter  and it's tests from SOLR-3961 (which was about a specific bug in the LimitTokenCountFilterFactory)",
    "attachments": {
        "LUCENE-4489.patch": "https://issues.apache.org/jira/secure/attachment/12549612/LUCENE-4489.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-10-17T23:34:02+0000",
            "content": "In SOLR-3961 rmuir and i were discussing the tests for LimitTokenCountFilter.  it seems to me like there are at least 2 things that should be improved in the tests, and one possible improvement to the code itself...\n\nA) fix TestLimitTokenCountAnalyzer to use MockTokenizer...\n\n\n3) the closest thing i see to a test of LimitTokenCountFilter is TestLimitTokenCountAnalyzer - i realize now the reason it's testLimitTokenCountAnalyzer doesn't get the same failure is because it's wrapping WhitespaceAnalyzer, StandardAnalyzer - should those be changed to use MockTokenizer?\n\nI think we should always do this!\n\nB) testLimitTokenCountAnalyzer.testLimitTokenCountIndexWriter smells fishy to me, still discussing with rmuir...\n\n\n4) TestLimitTokenCountAnalyzer also has a testLimitTokenCountIndexWriter that uses MockAnalyzer w/o calling setEnableChecks(false) which seems like it should trigger the same failure i got since it uses MockTokenizer, but in general that test looks suspicious, as it seems to add the exact number of tokens that the limit is configured for, and then asserts that the last token is in the index - but never actaully triggers the limiting logic since exactly the allowed umber of tokens are used.\n\nThen thats fine, because when LimitTokenCountFilter consumes the whole stream, its a \"good consumer\". its only when it actually truncates that it breaks the tokenstream contract.\n\nMy concern wasn't that i thought that test was fishy just because it didn't call setEnableChecks(false) \u2013 my concern was that i don't see the point of the test at all as it's currently written, because it doesn't actually trigger the token limiting behavior (if it did it would have to call setEnableChecks(false) ... so what exactly is it suppose to be testing?\n\nC) going back to a comment rmuir made earlier in SOLR-3961...\n\nThats because LimitTokenCountFilter violates the workflow of the standard tokenstream API ... it cuts off its consumer and calls end() even while that consumer still has more tokens: this could easily cause unexpected side effects and bugs. ... But we knew about this anyway\n\nThis makes me wonder if it would be worth while to add an option to  LimitTokenCountFilter to make it always consume all the tokens from the stream it wraps \u2013 it could be off by default so it would be havle exactly like today, but if you are wrapping a TokenStream where it's really important to you that every token is consumed before end() is called, you could set it appropriately.\n\nwould that make sense? ",
            "author": "Hoss Man",
            "id": "comment-13478494"
        },
        {
            "date": "2012-10-18T00:22:58+0000",
            "content": "here's a quick patch i tried to throw together to demonstrate what i had in mind.\n\nboth tests in TestLimitTokenCountAnalyzer now fail for reasons that aren't really clear to me \u2013 not sure if it's because: i made a stupid mistake somewhere; i broke something i don't understand; MockAnalyzer behaves different enough from WhitespaceAnalyzer that the previous assertions need to be changed; there's some other bug under the covers i've exposed; some combination of the above. ",
            "author": "Hoss Man",
            "id": "comment-13478521"
        },
        {
            "date": "2012-10-18T04:53:43+0000",
            "content": "Some combination of the above, attached is an updated patch:\n\n\n\tprevious patch had a test bug where it reused the same directory in the loop, so the docFreq() would be wrong as it kept adding documents.\n\tthere was another test bug where it looped form 0..limit and then added \"x\" but checked docFreq=1, this won't happen as the limit was exceeded (I changed the loop to 1..limit).\n\tprevious assertions about finalOffset from end() had wrong values that depended upon implementation details: thats the whole bug here! so these assertions are correct now: if you pass consumeAll = true, the finalOffset is correct, highlighting on multivalued fields with limiting will work correctly and so on. If you pass consumeAll=false, its impl dependent (and likely will be wrong, just as before). p.s. would be better to still improve the test to actually not assert endOffset at all if consumeAll=false, because my \"fix\" still keeps the test fragile and dependent on MockTokenizer impl in the \"wrong\" case.\n\tprevious patch also had a real bug: if you passed consumeAllTokens=true, but the stream had less tokens than the limit, it would incorrectly call incrementToken() after it returned false.\n\tfixes for javadocs errors (see references needed #) and typos.\n\n ",
            "author": "Robert Muir",
            "id": "comment-13478666"
        },
        {
            "date": "2012-10-18T04:59:56+0000",
            "content": "same patch as the above, just passing null in the test for finalOffset when consumeAll is false.\n\nThis way the test isn't fragile, we know that if you specify false for consumeAll that its likely wrong and will screw up highlighting for multivalued fields, etc (since it doesnt consume the whole stream, it doesnt necessarily know the real \"end\"). ",
            "author": "Robert Muir",
            "id": "comment-13478669"
        },
        {
            "date": "2012-11-20T20:29:59+0000",
            "content": "Thanks rob, \n\n(I changed the loop to 1..limit).\n\n...i could not for the life of me make sense of this and how the existing test worked at all, until i realized it wasn't realy testing anything about the filter \u2013 was using 10^3 in one place and 10^4 in another \u2013 so the limit never even got hit.\n\nI've updated the patch to add this as an option in the factory, and include some docs explaining the trade off in using the new option and what the default is.\n\n(would appreciate sanity check of the wording in the javadocs) ",
            "author": "Hoss Man",
            "id": "comment-13501446"
        },
        {
            "date": "2012-11-20T21:05:43+0000",
            "content": "Wording looks good! Thanks Hoss! ",
            "author": "Robert Muir",
            "id": "comment-13501463"
        },
        {
            "date": "2012-11-21T07:23:00+0000",
            "content": "Committed revision 1411996. - trunk\nCommitted revision 1411997. - 4x ",
            "author": "Hoss Man",
            "id": "comment-13501732"
        },
        {
            "date": "2012-11-21T15:30:28+0000",
            "content": "[branch_4x commit] Chris M. Hostetter\nhttp://svn.apache.org/viewvc?view=revision&revision=1411997\n\nLUCENE-4489: Added consumeAllTokens option to LimitTokenCountFilter (merge r1411996)\n ",
            "author": "Commit Tag Bot",
            "id": "comment-13502045"
        },
        {
            "date": "2012-11-21T15:30:37+0000",
            "content": "[trunk commit] Chris M. Hostetter\nhttp://svn.apache.org/viewvc?view=revision&revision=1411996\n\nLUCENE-4489: Added consumeAllTokens option to LimitTokenCountFilter\n ",
            "author": "Commit Tag Bot",
            "id": "comment-13502051"
        },
        {
            "date": "2013-03-22T16:14:42+0000",
            "content": "[branch_4x commit] Chris M. Hostetter\nhttp://svn.apache.org/viewvc?view=revision&revision=1411997\n\nLUCENE-4489: Added consumeAllTokens option to LimitTokenCountFilter (merge r1411996) ",
            "author": "Commit Tag Bot",
            "id": "comment-13610516"
        }
    ]
}