{
    "id": "SOLR-9366",
    "title": "replicas in LIR seem to still be participating in search requests",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "Spinning this off from SOLR-9363 where sarowe's jenkins encountered a strange test failure when TestInjection is used causing replicas to return errors on some requests.\n\nReading over the logs it appears that when this happens, and the replicas are put into LIR they then ignore an explicit user requested commit (ie: Ignoring commit while not ACTIVE - state: BUFFERING replay: false) but still participate in queries \u2013 apparently before they finish recovery (or at the very least before / with-out doing the commit/openSearcher that they previously ignored.\n\nDetails and logs to follow...",
    "attachments": {
        "SOLR-9366.txt.gz": "https://issues.apache.org/jira/secure/attachment/12821474/SOLR-9366.txt.gz"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-08-02T00:35:35+0000",
            "author": "Hoss Man",
            "content": "\nsarowe's jenkins identified that this test fails fairly reliably on master (circa d86c369533)...\n\n\nant test  -Dtestcase=TestStressCloudBlindAtomicUpdates  -Dtests.seed=60C9AAFC94FD8921 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/lucene-data/enwiki.random.lines.txt -Dtests.locale=id -Dtests.timezone=America/Puerto_Rico -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n\n\n\nNOTE: test_dv_stored is the specific test method that fails, but including -Dtest.method in the reproduce line won't work due to an unrelated bug in TestInjection (jira link to follow once i get a chance to file it)\n\n\n\nThe attached (gzipped) SOLR-9366.txt.gz log file is from a run of that seed on my computer, pruned down to only the output from test_dv_stored.\n\nNote that MinSolrCluster collection has 5 shards with a repFactor of 2.  the test indexes 332 docs, and during the process encounters many errors caused by TestInjection \u2013 ultimately leading to all non-leader replicas to be in recovery.\n\nAfter all 332 docs are added, the test issues an explicit commit, which all of the replicas in recovery log that they are explicitly ignoring...\n\n\n   [junit4]   2> 96495 INFO  (qtp1014592735-82) [n:127.0.0.1:53326_solr c:test_col s:shard1 r:core_node9 x:test_col_shard1_replica2] o.a.s.u.p.DistributedU\npdateProcessor Ignoring commit while not ACTIVE - state: BUFFERING replay: false\n\n   [junit4]   2> 96497 INFO  (qtp463019022-84) [n:127.0.0.1:51890_solr c:test_col s:shard2 r:core_node4 x:test_col_shard2_replica1] o.a.s.u.p.DistributedUpdateProcessor Ignoring commit while not ACTIVE - state: BUFFERING replay: false\n\n   [junit4]   2> 96503 INFO  (qtp574107834-44) [n:127.0.0.1:45722_solr c:test_col s:shard4 r:core_node10 x:test_col_shard4_replica2] o.a.s.u.p.DistributedUpdateProcessor Ignoring commit while not ACTIVE - state: BUFFERING replay: false\n\n   [junit4]   2> 96504 INFO  (qtp1497478396-522) [n:127.0.0.1:60681_solr c:test_col s:shard3 r:core_node3 x:test_col_shard3_replica1] o.a.s.u.p.DistributedUpdateProcessor Ignoring commit while not ACTIVE - state: BUFFERING replay: false\n\n   [junit4]   2> 96510 INFO  (qtp1947867562-490) [n:127.0.0.1:35941_solr c:test_col s:shard5 r:core_node6 x:test_col_shard5_replica1] o.a.s.u.p.DistributedUpdateProcessor Ignoring commit while not ACTIVE - state: BUFFERING replay: false\n\n\n\n...meanwhile the 5 leaders are all committing & opening new searchers...\n\n\n   [junit4]   2> 96503 INFO  (searcherExecutor-70-thread-1-processing-n:127.0.0.1:50832_solr x:test_col_shard1_replica1 s:shard1 c:test_col r:core_node2) [n:127.0.0.1:50832_solr c:test_col s:shard1 r:core_node2 x:test_col_shard1_replica1] o.a.s.c.SolrCore [test_col_shard1_replica1] Registered new searcher Searcher@641bc113[test_col_shard1_replica1] main{ExitableDirectoryReader(UninvertingDirectoryReader(Uninverting(_y(7.0.0):C65)))}\n\n   [junit4]   2> 96508 INFO  (searcherExecutor-71-thread-1-processing-n:127.0.0.1:43905_solr x:test_col_shard2_replica2 s:shard2 c:test_col r:core_node8) [n:127.0.0.1:43905_solr c:test_col s:shard2 r:core_node8 x:test_col_shard2_replica2] o.a.s.c.SolrCore [test_col_shard2_replica2] Registered new searcher Searcher@2fd5ffdc[test_col_shard2_replica2] main{ExitableDirectoryReader(UninvertingDirectoryReader(Uninverting(_v(7.0.0):C65)))}\n\n   [junit4]   2> 96511 INFO  (searcherExecutor-68-thread-1-processing-n:127.0.0.1:56607_solr x:test_col_shard3_replica2 s:shard3 c:test_col r:core_node5) [n:127.0.0.1:56607_solr c:test_col s:shard3 r:core_node5 x:test_col_shard3_replica2] o.a.s.c.SolrCore [test_col_shard3_replica2] Registered new searcher Searcher@29273482[test_col_shard3_replica2] main{ExitableDirectoryReader(UninvertingDirectoryReader(Uninverting(_15(7.0.0):C75)))}\n\n   [junit4]   2> 96513 INFO  (searcherExecutor-69-thread-1-processing-n:127.0.0.1:36018_solr x:test_col_shard5_replica2 s:shard5 c:test_col r:core_node1) [n:127.0.0.1:36018_solr c:test_col s:shard5 r:core_node1 x:test_col_shard5_replica2] o.a.s.c.SolrCore [test_col_shard5_replica2] Registered new searcher Searcher@126ca96c[test_col_shard5_replica2] main{ExitableDirectoryReader(UninvertingDirectoryReader(Uninverting(_w(7.0.0):C68)))}\n\n   [junit4]   2> 96515 INFO  (searcherExecutor-72-thread-1-processing-n:127.0.0.1:45414_solr x:test_col_shard4_replica1 s:shard4 c:test_col r:core_node7) [n:127.0.0.1:45414_solr c:test_col s:shard4 r:core_node7 x:test_col_shard4_replica1] o.a.s.c.SolrCore [test_col_shard4_replica1] Registered new searcher Searcher@5204d7bc[test_col_shard4_replica1] main{ExitableDirectoryReader(UninvertingDirectoryReader(Uninverting(_q(7.0.0):C59)))}\n\n\n\nOnce the test gets a \"success\" status from the SolrCloudClient in response to the commit request, it attempts to do a *:* search to sanity check that the index contains the expected docs before proceeding with the actual stuff the test cares about \u2013 but this assertion fails, because some of those replicas that are in recovery are consulted as part of the search...\n\n\n   [junit4]   2> 96526 INFO  (qtp1665435692-109) [n:127.0.0.1:56607_solr c:test_col s:shard3 r:core_node5 x:test_col_shard3_replica2] o.a.s.c.S.Request [test_col_shard3_replica2]  webapp=/solr path=/select params={distrib=false&_stateVer_=test_col:9&fl=id&fl=score&shards.purpose=4&start=0&fsv=true&shard.url=http://127.0.0.1:56607/solr/test_col_shard3_replica2/&rows=10&version=2&q=*:*&NOW=1470079073107&isShard=true&wt=javabin} hits=75 status=0 QTime=0\n\n   [junit4]   2> 96527 INFO  (qtp831283616-488) [n:127.0.0.1:43905_solr c:test_col s:shard2 r:core_node8 x:test_col_shard2_replica2] o.a.s.c.S.Request [test_col_shard2_replica2]  webapp=/solr path=/select params={distrib=false&_stateVer_=test_col:9&fl=id&fl=score&shards.purpose=4&start=0&fsv=true&shard.url=http://127.0.0.1:43905/solr/test_col_shard2_replica2/&rows=10&version=2&q=*:*&NOW=1470079073107&isShard=true&wt=javabin} hits=65 status=0 QTime=0\n   [junit4]   2> 96530 INFO  (qtp574107834-44) [n:127.0.0.1:45722_solr c:test_col s:shard4 r:core_node10 x:test_col_shard4_replica2] o.a.s.c.S.Request [test_col_shard4_replica2]  webapp=/solr path=/select params={distrib=false&_stateVer_=test_col:9&fl=id&fl=score&shards.purpose=4&start=0&fsv=true&shard.url=http://127.0.0.1:45414/solr/test_col_shard4_replica1/|http://127.0.0.1:45722/solr/test_col_shard4_replica2/&rows=10&version=2&q=*:*&NOW=1470079073107&isShard=true&wt=javabin} hits=0 status=0 QTime=0\n   [junit4]   2> 96531 INFO  (qtp287931206-46) [n:127.0.0.1:50832_solr c:test_col s:shard1 r:core_node2 x:test_col_shard1_replica1] o.a.s.c.S.Request [test_col_shard1_replica1]  webapp=/solr path=/select params={distrib=false&_stateVer_=test_col:9&fl=id&fl=score&shards.purpose=4&start=0&fsv=true&shard.url=http://127.0.0.1:50832/solr/test_col_shard1_replica1/&rows=10&version=2&q=*:*&NOW=1470079073107&isShard=true&wt=javabin} hits=65 status=0 QTime=0\n   [junit4]   2> 96532 INFO  (qtp1947867562-96) [n:127.0.0.1:35941_solr c:test_col s:shard5 r:core_node6 x:test_col_shard5_replica1] o.a.s.c.S.Request [test_col_shard5_replica1]  webapp=/solr path=/select params={distrib=false&_stateVer_=test_col:9&fl=id&fl=score&shards.purpose=4&start=0&fsv=true&shard.url=http://127.0.0.1:36018/solr/test_col_shard5_replica2/|http://127.0.0.1:35941/solr/test_col_shard5_replica1/&rows=10&version=2&q=*:*&NOW=1470079073107&isShard=true&wt=javabin} hits=0 status=0 QTime=0\n   [junit4]   2> 96533 INFO  (qtp831283616-487) [n:127.0.0.1:43905_solr c:test_col s:shard2 r:core_node8 x:test_col_shard2_replica2] o.a.s.c.S.Request [test_col_shard2_replica2]  webapp=/solr path=/select params={q=*:*&distrib=false&_stateVer_=test_col:9&shards.purpose=64&NOW=1470079073107&ids=0,35,13,36,15,4,16,8,41,32&isShard=true&shard.url=http://127.0.0.1:43905/solr/test_col_shard2_replica2/&wt=javabin&version=2} status=0 QTime=0\n   [junit4]   2> 96534 INFO  (qtp287931206-461) [n:127.0.0.1:50832_solr c:test_col s:shard1 r:core_node2 x:test_col_shard1_replica1] o.a.s.c.S.Request [test_col_shard1_replica1]  webapp=/solr path=/select params={q=*:*&_stateVer_=test_col:9&wt=javabin&version=2} hits=205 status=0 QTime=13\n\n\n\n\n...and BOOM, the test fails before it ever really starts because 332 docs expected != the 205 numFound returned by the search.\n\n ",
            "id": "comment-15403104"
        },
        {
            "date": "2016-08-02T04:22:53+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Yes, this is to be expected. The reason is that the queries execute before the new state (i.e. 'down') is propagated to all the nodes.\n\nYou can see this in the logs (the following excerpt is for shard5 only):\n\n   [junit4]   2> 96432 INFO  (updateExecutor-29-thread-10-processing-x:test_col_shard5_replica2 r:core_node1 http:////127.0.0.1:35941//solr//test_col_shard5_replica1// n:127.0.0.1:36018_solr s:shard5 c:test_col) [n:127.0.0.1:36018_solr c:test_col s:shard5 r:core_node1 x:test_col_shard5_replica2] o.a.s.c.LeaderInitiatedRecoveryThread Asking core=test_col_shard5_replica1 coreNodeName=core_node6 on http://127.0.0.1:35941/solr to recover\n....\n   [junit4]   2> 96458 INFO  (qtp1947867562-51) [n:127.0.0.1:35941_solr    ] o.a.s.h.a.CoreAdminOperation It has been requested that we recover: core=test_col_shard5_replica1\n....\n   [junit4]   2> 96510 INFO  (OverseerStateUpdate-96343095816486940-127.0.0.1:36018_solr-n_0000000000) [n:127.0.0.1:36018_solr    ] o.a.s.c.o.ReplicaMutator Update state numShards=5 message={\n   [junit4]   2>   \"core\":\"test_col_shard5_replica1\",\n   [junit4]   2>   \"core_node_name\":\"core_node6\",\n   [junit4]   2>   \"roles\":null,\n   [junit4]   2>   \"base_url\":\"http://127.0.0.1:35941/solr\",\n   [junit4]   2>   \"node_name\":\"127.0.0.1:35941_solr\",\n   [junit4]   2>   \"numShards\":\"5\",\n   [junit4]   2>   \"state\":\"recovering\",\n   [junit4]   2>   \"shard\":\"shard5\",\n   [junit4]   2>   \"collection\":\"test_col\",\n   [junit4]   2>   \"operation\":\"state\"}\n....\n   [junit4]   2> 96527 INFO  (qtp623658064-525) [n:127.0.0.1:36018_solr    ] o.a.s.h.a.CoreAdminOperation Will wait a max of 183 seconds to see test_col_shard5_replica2 (shard5 of test_col) have state: recovering\n....\n   [junit4]   2> 96713 INFO  (OverseerStateUpdate-96343095816486940-127.0.0.1:36018_solr-n_0000000000) [n:127.0.0.1:36018_solr    ] o.a.s.c.o.ZkStateWriter going to update_collection /collections/test_col/state.json version: 14\n....\n[junit4]   2> 96715 INFO  (zkCallback-45-thread-2-processing-n:127.0.0.1:50832_solr) [n:127.0.0.1:50832_solr    ] o.a.s.c.c.ZkStateReader Updating data for [test_col] from [14] to [15]\n[junit4]   2> 96715 INFO  (zkCallback-50-thread-5-processing-n:127.0.0.1:36018_solr) [n:127.0.0.1:36018_solr    ] o.a.s.c.c.ZkStateReader Updating data for [test_col] from [14] to [15]\n\n\n\n\n\tTime: 96432 \u2013 The shard5 leader asks replica shard5_replica1 to recover.\n\tTime: 96458 \u2013 The replica shard5_replica1 receives the recovery request. At this point, it internally marks itself in recovering state.\n\tTime: 96510 \u2013 The message to publish shard5_replica1 as 'down' is received at overseer\n\tTime: 96527 \u2013 The shard5 leader i.e. shard5_replica2 is still waiting to see shard5_replica1 as down. Note that around this time, the query is being executed\n\tTime: 96713 \u2013 The overseer actually writes the new collection state to ZooKeeper. The delay is because the overseer buffers writes to ZK. The query has already executed by 96534.\n\tTime: 96715 \u2013 The new collection state is finally available at the relevant nodes.\n\n\n\nIn summary, state updates cannot be instantaneously propagated to the entire cluster and this information asymmetry will always exist unless we implement consensus for each update. Also note that this is a temporary issue because the replica in recovery will not be selected for shard requests once the state is propagated across the cluster.\n\nIt is a bad idea to use Solr as a consistent data store like this test does. The test can do one of the following:\n\n\tWait for recovery to complete before asserting doc counts,\n\tUse min_rf with updates and assert that all documents which made to both replicas are returned in the query results\n\tQuery each shard leader individually (distrib=false) and sum up the numFound on the client\n\n\n\nThat being said, we can improve a few things:\n\n\tIf the query happens to be aggregated by the leader, it can take LIR into account while selecting a replica for the non-distrib request\n\tThe overseer can help by flushing 'down' state updates to ZK faster and not perform buffering.\n\tProvide a way to query leaders only for full consistency\n\tUse LIR information from ZK in addition to live_nodes to select replica for non-distrib request \u2013 I am not sure we should do this. If we do, we should make sure we do not hit ZK in the fast path.\n\n\n\nNone of the above will completely avoid this problem but they will make it less probable. ",
            "id": "comment-15403321"
        },
        {
            "date": "2016-08-02T04:57:20+0000",
            "author": "Hoss Man",
            "content": "That being said, we can improve a few things:\n\nAdd to that list \"replicas which know they are not active can response with 503 Service Unavailable\" ... which is what I thought replicas were already suppose to be doing in situations like this.\n\nI understand that not every client can immediately/magically know that replicas which are in recovery shouldn't be queried \u2013 my concern is that replicas which do know for a fact that they are in recovery and are inconsistent with their leader are happily participating in queries knowing that they are returning incorrect results. ",
            "id": "comment-15403348"
        },
        {
            "date": "2016-08-02T16:01:50+0000",
            "author": "ASF subversion and git services",
            "content": "Commit c17605b4a9978311b6b231f202d70dd800e473e6 in lucene-solr's branch refs/heads/branch_6x from Chris Hostetter\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c17605b ]\n\nSOLR-9363: tweak test to work around SOLR-9366 + SOLR-9367 since those issues are not key to what's being tested here\n\n(cherry picked from commit 04321c401c6584395c76c509f8513c5e5e4730ee) ",
            "id": "comment-15404247"
        },
        {
            "date": "2016-08-02T16:01:55+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 04321c401c6584395c76c509f8513c5e5e4730ee in lucene-solr's branch refs/heads/master from Chris Hostetter\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=04321c4 ]\n\nSOLR-9363: tweak test to work around SOLR-9366 + SOLR-9367 since those issues are not key to what's being tested here ",
            "id": "comment-15404250"
        }
    ]
}