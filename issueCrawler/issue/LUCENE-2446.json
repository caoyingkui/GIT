{
    "id": "LUCENE-2446",
    "title": "Add checksums to Lucene segment files",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.8",
            "6.0"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "It would be useful for the different files in a Lucene index to include checksums. This would make it easy to spot corruption while copying index files around; the various cloud efforts assume many more data-copying operations than older single-index implementations.\n\nThis feature might be much easier to implement if all index files are created in a sequential fashion. This issue therefore depends on LUCENE-2373.",
    "attachments": {
        "LUCENE-2446.patch": "https://issues.apache.org/jira/secure/attachment/12637749/LUCENE-2446.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2014-03-30T23:09:42+0000",
            "content": "I think this is a pretty important issue: besides the case of distributed system copying files around, we have the issue that today there is no integrity mechanism to detect hardware issues (can cause developers to pull hair out trying to debug corruptions), and we have some optimized components doing bulk merge which can propagate corruptions to new segments over a long time.\n\nAlso in recent jvms, computing checksum is fast: e.g. in java8 CRC32 is intrinsic and uses clmul hardware instructions on x86 and so on.\n\nI created an initial patch: the last 8 bytes of every file is a zlib-crc32 checksum. We also write some additional metadata before it (its done via CodecUtil.writeFooter) so we can extend it more in the future if we need.\n\nFor small metadata files (e.g. .fnm, .si, .dvm, ...) we just verify when we open, because we are reading the file anyway. So this provides some extra safety.\n\nFor larger files this would be expensive: instead the patch adds AtomicReader.validate() which asks the codec (or filterreader, or whatever), to ensure everything is valid. This is called by e.g. checkindex before decoding.\n\nPatch adds an option (defaults to off) on IndexWriterConfig to call this before merging. Ideally we wouldnt need this and just validate-as-we-merge, but that requires some codec/merge API changes...\n\nFile format changes are backwards compatible. ",
            "author": "Robert Muir",
            "id": "comment-13954863"
        },
        {
            "date": "2014-03-31T08:13:29+0000",
            "content": "This looks really great! +1 ",
            "author": "Shai Erera",
            "id": "comment-13955022"
        },
        {
            "date": "2014-03-31T10:17:02+0000",
            "content": "+1, this looks wonderful; it gives us end-to-end checksums, like ZFS.\n\nThis means, when there is a checksum mis-match, we can be quite certain that there's a hardware problem (bit flipper) in the user's env, and not a Lucene bug. ",
            "author": "Michael McCandless",
            "id": "comment-13955073"
        },
        {
            "date": "2014-03-31T10:32:31+0000",
            "content": "this is awesome - robert do you think it would make sense to commit this to a branch and let CI run it for a bit. That way we can clean stuff up further if need as well? I just wonder though since it's a pretty big patch  ",
            "author": "Simon Willnauer",
            "id": "comment-13955080"
        },
        {
            "date": "2014-03-31T10:56:18+0000",
            "content": "I think the patch is quite solid as is ... we should just commit & let it bake for a while on trunk?  We can iterate on further improvements...\n\nI just ran distributed beasting from luceneutil (runs all Lucene core + modules tests, across 5 machines, 28 paralle, JVMs) ~150 times over and it didn't hit any test failures. ",
            "author": "Michael McCandless",
            "id": "comment-13955092"
        },
        {
            "date": "2014-03-31T11:19:36+0000",
            "content": "Is CRC32 intrinsic available and the same on all relevant platforms ?\n(I assume it is CRC32C version of Intel)\n\nIf not, what happens ? Does it still work ? Does it need to be cross-platform-compatible ?\nCould it be a concern for performance-related issue ? ",
            "author": "Bert Sanders",
            "id": "comment-13955103"
        },
        {
            "date": "2014-03-31T11:35:49+0000",
            "content": "java.util.zip.CRC32 has been used by lucene for the segments_N for quite some time.\nThis just applies it to other files. And while is nice that it is 3x faster in java 8, it is still 1GB/second with java 7 on my computer.\nBy the way, the instruction used in java8 is not CRC32C. it is PCLMULQDQ. If they broke this, it would be a big bug in java  ",
            "author": "Robert Muir",
            "id": "comment-13955111"
        },
        {
            "date": "2014-03-31T12:20:41+0000",
            "content": "I glanced at the patch again I think it looks pretty good. I didn't look close enough before! +1 to move that in and let it bake ",
            "author": "Simon Willnauer",
            "id": "comment-13955129"
        },
        {
            "date": "2014-03-31T12:37:08+0000",
            "content": "Great!\n\nI am not so happy about the method name validate(). Especially as it is in AtomicReader, people might think (because of name), its just some cheap check. Yes, I know there are Javadocs, but IDEs suggestions may lead to use it!\n\nI have no yet thought thoroghly about a better name, maybe checkIntegrity()? ",
            "author": "Uwe Schindler",
            "id": "comment-13955138"
        },
        {
            "date": "2014-03-31T12:40:53+0000",
            "content": "Is CRC32 intrinsic available and the same on all relevant platforms ?\n\nDoes not matter what the JVM does internally. The public API for CRC32 is there in the java.util.zip package and we use it since the early beginning of Lucene. If it would be suddenly return incorrect/different results, it would violate ZIP spec (which is indirectly an ISO standard - through OpenDocument ISO standard). ",
            "author": "Uwe Schindler",
            "id": "comment-13955141"
        },
        {
            "date": "2014-03-31T12:41:19+0000",
            "content": "bq: I am not so happy about the method name\n\nWhy not a method name that indicates the function used? Something like checkIntegrityUsingRobertsNiftyNewCRC32Code?\n\nHmmm, a little long, but what about something like\nvalidateCRC32\nor\ncheckIntegrityCRC32 ",
            "author": "Erick Erickson",
            "id": "comment-13955142"
        },
        {
            "date": "2014-03-31T12:42:57+0000",
            "content": "Although its currently only doing file integrity checks, we may extend this later! Maybe remove more parts of CheckIndex class and move it to Codec level?\n\nAnother idea: Move validate()/checkIntegrity() up to IndexReader, so it would also work on DirectoryReaders? This would help with cleaning up stuff from CheckIndex. ",
            "author": "Uwe Schindler",
            "id": "comment-13955143"
        },
        {
            "date": "2014-03-31T13:02:40+0000",
            "content": "I don't want to do this Uwe. I dont want all this checkindex stuff being run at merge. A big point here is to provide an option to prevent detection on merge.\n\nIf you read my description I am unhappy about the current situation because it cannot be enabled by default for performance reasons. I want to fix the codec apis so this is no longer the case. I don't want checkindex code moved into this method. ",
            "author": "Robert Muir",
            "id": "comment-13955154"
        },
        {
            "date": "2014-03-31T13:14:14+0000",
            "content": "Robert, sorry if I throwed some unrelated ideas in here: My idea here has not much to do with the stuffs here. It just came into my mind when I was seeing the validate() method which made me afraid.\n\nNow just throwing in something, which does not have to affect you while implementing checksumming - just think about it as a separate issue, inspired by this one:\n\nUwe's unrelated idea\nIn general CheckIndex as a separate class is wrong in my opinion. Especially if it does checks not valid for all codecs (using instanceof checks in CheckIndex code - horrible). A really good CheckIndex should work like fsck - implemented by the file system driver + filesystem code. So the checks currently done by CheckIndex should be done by the codecs (only general checks may work on top - like numDocs checks, etc.)\n\n\nNow back to this issue: To come back to the initial suggestion: validate => checkIntegrity. What do you think? ",
            "author": "Uwe Schindler",
            "id": "comment-13955162"
        },
        {
            "date": "2014-03-31T13:22:52+0000",
            "content": "I think checkIntegrity is too generic (i.e. it does not check the integrity of the posting lists, a'la what CheckIndex does. How about validateChecksums, since that's what it currently does? I don't mind if it's renamed in the future if the method validates more things, we shouldn't worry about back-compat with this API. ",
            "author": "Shai Erera",
            "id": "comment-13955170"
        },
        {
            "date": "2014-03-31T13:30:05+0000",
            "content": "You guys figure out the name for the method, i really dont care. I will wait on the issue until you guys bikeshed it out. The only thing i care about is that its not mixed up with other logic.\n\nIts really important when debugging a corrupt index that you can know that the file itself is corrupted (e.g. hardware) versus some bug in lucene. Its also really important that there is at least the option (like there is in this patch) to prevent this corruption from being propagated in merge. So lets please not mix unrelated stuff in here. \n\nThings like refactoring checkindex or whatever: please just open a separate issue for that  ",
            "author": "Robert Muir",
            "id": "comment-13955173"
        },
        {
            "date": "2014-04-01T06:44:59+0000",
            "content": "You guys figure out the name for the method, i really dont care. I will wait on the issue until you guys bikeshed it out.\n\nUwe, so what do you think: validateChecksums or checkIntegrity? \nLet's get this thing committed so Jenkins can bless it. ",
            "author": "Shai Erera",
            "id": "comment-13956167"
        },
        {
            "date": "2014-04-01T06:48:18+0000",
            "content": "checkIntegrity is fine with me. We can also rename it before releasing. \n\nI'll mark it internal for now... ",
            "author": "Robert Muir",
            "id": "comment-13956169"
        },
        {
            "date": "2014-04-01T06:48:53+0000",
            "content": "+1 ",
            "author": "Shai Erera",
            "id": "comment-13956170"
        },
        {
            "date": "2014-04-01T07:36:47+0000",
            "content": "Commit 1583550 from Robert Muir in branch 'dev/trunk'\n[ https://svn.apache.org/r1583550 ]\n\nLUCENE-2446: add checksums to index files ",
            "author": "ASF subversion and git services",
            "id": "comment-13956189"
        },
        {
            "date": "2014-04-01T07:49:08+0000",
            "content": "Thanks Robert!\ncheckIntegrity() is in my opinion the best we can have: most generic, but not to easy to misunderstand and run after every commitI was just afraid about the optimize()-loving people! \nUwe ",
            "author": "Uwe Schindler",
            "id": "comment-13956202"
        },
        {
            "date": "2014-04-01T07:56:09+0000",
            "content": "One idea to make the whole thing more hidden to the user:\nI am not sure if we need the abstract checkIntegrity() on the public AtomicReader abstract interface - because it only applies to disk-based indexes. Would it be not enough to have it on SegmentReader? I know we might need some instanceof checks while merging, but I think we do those already. By that it would not be public and we can hide it to the user. We would only not validate on FilterAtomicReader's merging (split / sort indexes).\n\nAny opinion about this? ",
            "author": "Uwe Schindler",
            "id": "comment-13956206"
        },
        {
            "date": "2014-04-01T08:13:14+0000",
            "content": "I don't think that its true it \"only applies to disk-based indexes\".\n\nFilterReader/SlowWrapper etc pass this down to their underlying readers so you still get the check on the underlying data e.g. if you are using SortingMergePolicy. ",
            "author": "Robert Muir",
            "id": "comment-13956218"
        },
        {
            "date": "2014-04-01T08:21:58+0000",
            "content": "Commit 1583565 from Robert Muir in branch 'dev/trunk'\n[ https://svn.apache.org/r1583565 ]\n\nLUCENE-2446: ensure we close file if we hit exception writing codec header ",
            "author": "ASF subversion and git services",
            "id": "comment-13956228"
        },
        {
            "date": "2014-04-02T03:20:23+0000",
            "content": "Commit 1583863 from Robert Muir in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1583863 ]\n\nLUCENE-2446: add checksums to index files ",
            "author": "ASF subversion and git services",
            "id": "comment-13957295"
        },
        {
            "date": "2014-04-02T03:20:56+0000",
            "content": "I will open followups for some of the ideas here. ",
            "author": "Robert Muir",
            "id": "comment-13957296"
        },
        {
            "date": "2014-04-04T23:31:46+0000",
            "content": "Hi Robert,\nI think we should add a Lucene 4.8 TestBackwardsCompatibility index to the test resources. Should I prepare one? ",
            "author": "Uwe Schindler",
            "id": "comment-13960792"
        },
        {
            "date": "2014-04-04T23:35:35+0000",
            "content": "Absolutely not! 4.8 is not released ",
            "author": "Robert Muir",
            "id": "comment-13960798"
        },
        {
            "date": "2014-04-05T11:07:04+0000",
            "content": "But we should take care of it, maybe open issue! so we add it once 4.8 is released? ",
            "author": "Uwe Schindler",
            "id": "comment-13961050"
        },
        {
            "date": "2014-04-05T11:12:05+0000",
            "content": "It's part of the release workflow. Come on, this is nothing new and no different from any other format change. No need to single out this issue. ",
            "author": "Robert Muir",
            "id": "comment-13961051"
        },
        {
            "date": "2014-04-27T23:26:03+0000",
            "content": "Close issue after release of 4.8.0 ",
            "author": "Uwe Schindler",
            "id": "comment-13982640"
        }
    ]
}