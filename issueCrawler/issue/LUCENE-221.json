{
    "id": "LUCENE-221",
    "title": "QueryParser treats CJK and English query strings differently",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/queryparser"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Incomplete",
        "status": "Closed"
    },
    "description": "Since 1.3 final, the Standard Analyzer returns strings of CJK characters as\nseparate tokens.  However, the generated QueryParser has its own grammer which\ndoesn't take account of this.  So we get the following behaviour:\n\nparse(\"one two three\", \"content\", new StandardAnalyzer()) returns 'content:one\ncontent:two content:three', searching for each term individually.\nparse(\"\\\"one two three\\\"\", \"content\", new StandardAnalyzer()) returns\n'content:\"one two three\"', searching for the phrase.\nparse(\"C1C2C3\", \"content\", new StandardAnalyzer()) where Cn is a Chinese\ncharacter returns 'content:\"C1 C2 C3\"', when it should really be 'content:C1\ncontent:C2 content:C3'.  This is inconsistent.\nparse(\"\\\"C1C2C3\\\"\", \"content\", new StandardAnalyzer())  also returns\n'content:\"C1 C2 C3\"', identical to the previous case.\n\nAlthough the string is separated out into the separate CJK tokens (indicated by\nthe spaces between them), the query parser builds a phrase search for them\nrather than individual token searches.  To get the desired query the user has to\ninstead enter \"C1 C2 C3\" as the query string (or I have to pre-process the query\nstring in my code to add the spaces), which is non-intuitive.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2004-09-19T18:09:15+0000",
            "content": "StandardAnalyzer is not supposed to work with CJK, please try the \"cjk\" \nanalyzer from Lucene's sandbox instead. \n ",
            "author": "Daniel Naber",
            "id": "comment-12321707"
        }
    ]
}