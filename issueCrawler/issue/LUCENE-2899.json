{
    "id": "LUCENE-2899",
    "title": "Add OpenNLP Analysis capabilities as a module",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "New Feature",
        "fix_versions": [
            "7.3",
            "master (8.0)"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Now that OpenNLP is an ASF project and has a nice license, it would be nice to have a submodule (under analysis) that exposed capabilities for it. Drew Farris, Tom Morton and I have code that does:\n\n\tSentence Detection as a Tokenizer (could also be a TokenFilter, although it would have to change slightly to buffer tokens)\n\tNamedEntity recognition as a TokenFilter\n\n\n\nWe are also planning a Tokenizer/TokenFilter that can put parts of speech as either payloads (PartOfSpeechAttribute?) on a token or at the same position.\n\nI'd propose it go under:\nmodules/analysis/opennlp",
    "attachments": {
        "OpenNLPFilter.java": "https://issues.apache.org/jira/secure/attachment/12547169/OpenNLPFilter.java",
        "OpenNLPTokenizer.java": "https://issues.apache.org/jira/secure/attachment/12547168/OpenNLPTokenizer.java",
        "LUCENE-2899-RJN.patch": "https://issues.apache.org/jira/secure/attachment/12571282/LUCENE-2899-RJN.patch",
        "LUCENE-2899.patch": "https://issues.apache.org/jira/secure/attachment/12611857/LUCENE-2899.patch",
        "LUCENE-2899-6.1.0.patch": "https://issues.apache.org/jira/secure/attachment/12817282/LUCENE-2899-6.1.0.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2011-05-19T11:44:16+0000",
            "content": "The first release is now out. I guess you will use maven for dependency management, you can find here how to add the released version as a dependency:\nhttp://incubator.apache.org/opennlp/maven-dependency.html ",
            "author": "Joern Kottmann",
            "id": "comment-13036124"
        },
        {
            "date": "2012-06-06T08:17:43+0000",
            "content": "This is a patch for the trunk (as of a few days ago) that supplies the OpenNLP Sentence Detector, Tokenizer, Parts-of-Speech, Chunking and Named Entity Recognition tools.\n\nThis has nothing to do with the code mentioned above.\n ",
            "author": "Lance Norskog",
            "id": "comment-13290007"
        },
        {
            "date": "2012-06-06T08:31:05+0000",
            "content": "Notes for a Wiki page:\n\nOpenNLP Integration\n\nWhat is the integration? The first integration is a Tokenizer and three Filters. \n\n\tThe OpenNLPTokenizer uses the OpenNLP SentenceDetector and Tokenizer tools instead of the standard Lucene Tokenizers.  This requires statistical model files. One quirk of these is that all punctuation is maintained.\n\tThe OpenNLPFilter implements Parts-of-Speech tagging, Chunking (finding noun/verb phrases), and Named Entity Recognition (tagging people, place names etc.). This filter will add all tags as payload attributes to the tokens.\n\tThe FilterPayloadsFilter removes tokens by checking the payloads. Given a list of payloads, it will either keep only tokens with one of those payloads, or remove only matching tokens and keep the rest. (This filter maintains position increments correctly.)\n\tThe StripPayloadsFilter removes payloads from Tokens.\n\n\n\nHow do I get going?\n\n\tpull the latest trunk\n\tapply the patch\n\tdownload these models to contrib/opennlp/src/test-* files/opennlp/solr/conf/opennlp/\n\t\n\t\thttp://opennlp.sourceforge.net/models-1.5/\n\t\tEverything that starts with 'en'\n\t\n\t\n\tdownload the OpenNLP distribution from http://opennlp.apache.org/cgi-bin/download.cgi\n\t\n\t\tCurrently it is apache-opennlp-1.5.2-incubating-bin.tar.gz\n\t\n\t\n\tunpack this and copy the jar files from lib/ to\nsolr/contrib/opennlp/lib\n\n\n\nNow, go to trunk-dir/solr and run 'ant test-contrib'. It compiles against the libraries and uses the model files. \nNext, run 'ant example', cd to the example directory and run 'java -Dsolr.solr.home=opennlp -jar start.jar'\nYou now should start without any Exceptions. At this point, go to the Schema analyzer, pick the 'text_opennlp_pos' field type, and post a sentence or two to the analyzer. You should get text tokenized with payloads. Unfortunately, the analysis page shows them as bytes instead of text. If you would like this, then go vote on SOLR-3493.\n ",
            "author": "Lance Norskog",
            "id": "comment-13290015"
        },
        {
            "date": "2012-06-06T08:35:46+0000",
            "content": "About the build-\n\n\tThis should be a Lucene module. I got lost trying to make the build work copying jars around, so it will ended up in Solr/contrib.\n\tDownloading the jars. I don't know how to put together license validation with the OpenNLP Maven build. I think it takes some upgrading in the OpenNLP project.\n\tWhy download the models from a separate place? The models are not Apache licensed. They are binaries derived from GNU- and otherwise licensed training data. The OpenNLP people archived them on Sourceforge.\n\n\n\n\n ",
            "author": "Lance Norskog",
            "id": "comment-13290020"
        },
        {
            "date": "2012-06-06T08:44:01+0000",
            "content": "I consider the code and feature set mostly cooked as a first release. The toolkit as is lets you do two things:\n\n\tDo named entity recognition and filter out names for an autosuggest dictionary\n\tPick nouns and verbs out of text and only index those. This gives you a field with a smaller more focused set of terms. MoreLikeThis might work better.\n\n\n\nPlease review it for design, bugs, code nits, whatever. ",
            "author": "Lance Norskog",
            "id": "comment-13290023"
        },
        {
            "date": "2012-06-06T09:08:36+0000",
            "content": "An explanation about the OpenNLPUtil factory class: the statistical models are several megabytes apiece. This class loads them and caches them by file name. It does not reload them across commits. \n\nThe models are immutable objects. The factory class creates another object that consults the model. There is one of these for each field analysis. \n\nThe models are large enough that if the different unit tests load them all at once, it needs more than the default ram. Therefore, the unit tests unload all models between tests, and only run single-threaded.\n ",
            "author": "Lance Norskog",
            "id": "comment-13290040"
        },
        {
            "date": "2012-06-09T03:24:47+0000",
            "content": "License-ready.\nIvy-ready.\nOpenNLP libraries available through Ivy.\nYou still have to download jwnl-1.3.3 from http://sourceforge.net/projects/jwordnet/files/\n\nAnd of course download the model files. But this is committable to the Solr side. ",
            "author": "Lance Norskog",
            "id": "comment-13292182"
        },
        {
            "date": "2012-06-12T11:36:32+0000",
            "content": "Very cool Lance.  The models are indeed tricky and I wonder how we can properly hook them into the tests, if at all.  I wonder how hard it would be to create much smaller ones based on training just a few things. ",
            "author": "Grant Ingersoll",
            "id": "comment-13293526"
        },
        {
            "date": "2012-06-12T11:40:48+0000",
            "content": "I wonder how hard it would be to create much smaller ones based on training just a few things.\nthere was the idea of using the OpenNLP CorpusServer with some wikinews articles to train them (back to OPENNLP-385) ",
            "author": "Tommaso Teofili",
            "id": "comment-13293529"
        },
        {
            "date": "2012-06-12T12:08:04+0000",
            "content": "I am using this mentioned Corpus Server together with the Apache UIMA Cas Editor for labeling projects. If someone wants to set something up to label data we (OpenNLP people) are happy to help with that! ",
            "author": "Joern Kottmann",
            "id": "comment-13293560"
        },
        {
            "date": "2012-06-12T12:13:12+0000",
            "content": "Cool!  \n\nI think if we could just get a very small model that can be checked in and used for testing purposes, that is all that would be needed.  We don't really need to test OpenNLP, we just need to test that the code properly interfaces with OpenNLP, so a really small model should be fine.   ",
            "author": "Grant Ingersoll",
            "id": "comment-13293562"
        },
        {
            "date": "2012-06-12T12:46:51+0000",
            "content": "This really should just be a part of the analysis modules (with the exception of the Solr example parts).  I don't know exactly how we are handling Solr examples anymore, but I seem to recall the general consensus was to not proliferate them.  Can we just expose the functionality in the main one?\n\nI'll update the patch to move this to the module for starters.  Not sure on what to do w/ the example part. ",
            "author": "Grant Ingersoll",
            "id": "comment-13293582"
        },
        {
            "date": "2012-06-12T13:03:36+0000",
            "content": "For a test you can run OpenNLP just over a piece of training data, even when trained on a tiny amount of data this will give good results. It does not test OpenNLP, but is sufficient for the desired interface testing. ",
            "author": "Joern Kottmann",
            "id": "comment-13293589"
        },
        {
            "date": "2012-06-13T03:28:53+0000",
            "content": "This really should just be a part of the analysis modules (with the exception of the Solr example parts). I don't know exactly how we are handling Solr examples anymore, but I seem to recall the general consensus was to not proliferate them. Can we just expose the functionality in the main one?\nA lot of Solr/Lucene features are only demoed in solrconfig/schema unit test files (DIH for example). That is fine.\nThe models are indeed tricky and I wonder how we can properly hook them into the tests, if at all.\nD'oh! Forgot about that. If we have tagged data in the project, it helps show the other parts of an NLP suite. It's hard to get a full picture of the jigsaw puzzle if you don't know NLP software.\n ",
            "author": "Lance Norskog",
            "id": "comment-13294113"
        },
        {
            "date": "2012-06-15T10:23:12+0000",
            "content": "Wiki page is up! http://wiki.apache.org/solr/OpenNLP\n\nAlso, the Solr fancy toolkits had no links from the Solr front page, so I added 'Advanced Tools' with links to UIMA and this. ",
            "author": "Lance Norskog",
            "id": "comment-13295570"
        },
        {
            "date": "2012-06-17T07:59:54+0000",
            "content": "The models are indeed tricky and I wonder how we can properly hook them into the tests, if at all.\n\nI have mini training data for sentence detection, tokenization, POS and chunking. The purpose is to make the matching unit tests pass. The data and build script are in a new (unattached) patch. \n\nNER is proving a tougher nut to crack. I tried annotating several hundred lines of Reuters but no go. \n\nHow would I make an NER dataset that will make OpenNLP spit out one or two tags? Is there a large NER dataset that is Apache-friendly? ",
            "author": "Lance Norskog",
            "id": "comment-13393485"
        },
        {
            "date": "2012-06-18T10:15:41+0000",
            "content": "For NER you should try the perceptron and a cutoff of zero. For NER with a cutoff of 5 you need otherwise much more training data. ",
            "author": "Joern Kottmann",
            "id": "comment-13395787"
        },
        {
            "date": "2012-06-22T10:08:49+0000",
            "content": "For NER you should try the perceptron and a cutoff of zero. \nThanks! This patch generates all models needed by tests, and the tests are rewritten to use the poor quality data from the models. To make the models, go to solr/contrib/opennlp/src/test-files/training and run bin/training.sh. This populates solr/contrib/opennlp/src/test-files/opennlp/conf/opennlp. I don't have windows anymore so I can't make a .bat version.\n ",
            "author": "Lance Norskog",
            "id": "comment-13399243"
        },
        {
            "date": "2012-06-22T10:10:55+0000",
            "content": "General status:\n\n\tAt this point you have to download 1 library (jwnl) and run a script to make the unit tests work.\n\tYou have to download several model files from sourceforge to do real work. There is no script to help.\n\tThe tokenizer and filter are in solr/ not lucene/\n\n\n\nWhat is missing to make this a full package:\n\n\tPayload handling\n\t\n\t\tTokenFilter to parse TAG/term or term_TAG into term/payload.\n\t\tOutput code in Solr for the reverse.\n\t\tPayload query for tags.\n\t\tSimilarity scoring algorithms for tags.\n\t\n\t\n\tTag handling\n\t\n\t\tThere is a universal set of 12 parts-of-speech tags, with mappings for many language tagsets (Treebank etc.) into 12 common tags. Multi-language sites would benefit from this. I persuaded the authors to switch from GNU to Apache licensing.\n\t\t\n\t\t\tA Universal Part-of-Speech Tagset\n\t\t\n\t\t\n\t\n\t\n\n\n\nWhat NLP apps would be useful for search? Coordinate expansion, for example. ",
            "author": "Lance Norskog",
            "id": "comment-13399245"
        },
        {
            "date": "2012-07-02T06:10:20+0000",
            "content": "This is about finished. The Tokenizer and TokenFilters are moved over into lucene/analysis/opennlp. They do not have unit tests in lucene/ because of the difficulty in supplying model data. They are unit-tested by the factories in solr/contrib/opennlp.\n\nThe solr/example/opennlp directory is gone, as per request. Possible field types are documented in the solrconfig.xml in the unit test resources.\n\nAll jars are downloaded via ivy. The jwnl library is one rev after what this was compiled with. It is only used in collocation, which is not exposed in this release.\n\nTo build, test and commit, there is a boostrap sequence. In the top-level directory:\n\n  ant clean compile\n\n\nThis downloads the OpenNLP jars\n\ncd solr/contrib/opennlp/test-files/training\nsh bin/training.sh\n\n\nThis creates low-quality model files in solr/contrib/opennlp/src/test-files/opennlp/solr/collection1/conf/opennlp. In the trunk/solr directory, run\n\n \nant example test-contrib\n\n\nYou now have committable binary models. They are small, and only there to run the OpenNLP unit tests. They generate results that are objectively bogus, but the unit tests are matched to the results. If you want real models, you have to download them from sourceforge. ",
            "author": "Lance Norskog",
            "id": "comment-13404899"
        },
        {
            "date": "2012-07-02T06:29:44+0000",
            "content": "Oops- remove solr/contrib/opennlp/src/test-files/opennlp/solr/collection1/conf/opennlp/.gitignore. This will prevent you from committing the models. ",
            "author": "Lance Norskog",
            "id": "comment-13404904"
        },
        {
            "date": "2012-07-02T07:24:03+0000",
            "content": "dev-tools needs updating. I don't have IntelliJ and don't feel comfortable making the right Eclipse files. \n\nThis patch works on both trunk and 4.x. I made a few changes in the build files where modules were out of alphabetic order. Also, the reams of copied code in module-build.xml had blocks out of order. I can't easily see where, but it seems like some of them are missing a few lines that others have. ",
            "author": "Lance Norskog",
            "id": "comment-13404915"
        },
        {
            "date": "2012-07-05T01:07:44+0000",
            "content": "The Wiki is updated for testing and committing this patch: http://wiki.apache.org/solr/OpenNLP. ",
            "author": "Lance Norskog",
            "id": "comment-13406804"
        },
        {
            "date": "2012-07-16T01:41:54+0000",
            "content": "There is a regression in Solr which causes this to not work in a Solr example: SOLR-3625. Until this is fixed, you have to copy the Lucene opennlp jar, the Solr opennlp jar, and the solr/contrib/opennlp/lib jars into the solr war.  ",
            "author": "Lance Norskog",
            "id": "comment-13414820"
        },
        {
            "date": "2012-07-21T03:29:54+0000",
            "content": "SOLR-3623 should give a final answer for how to build contribs and Lucene libraries and external dependencies. I've found it a little confusing. ",
            "author": "Lance Norskog",
            "id": "comment-13419732"
        },
        {
            "date": "2012-08-05T05:53:52+0000",
            "content": "New patch for current build system on trunk & 4.x. ",
            "author": "Lance Norskog",
            "id": "comment-13428808"
        },
        {
            "date": "2012-08-05T05:55:51+0000",
            "content": "As it turns out, building is still confused: solr/example/solr-webapps comes and goes. \n\nThis build parks the lucene-analyzer-opennlp jar in solr/contrib/opennlp/lucene-libs. example/..../solrconfig.xml includes a reference to ../....../contrib/opennlp/lib and lucene-libs and ../...../dist.\n\nA jar-of-jars or a fully repacked jar in dist/ is the best way to ship this. \n\nBug status: payloads added by this filter do not get written to the index!\n\nBuild-fiddling status: forbidden api checks fail. checksums and licenses validate. rat-sources validate. No dev-tools changes. \n\nIf you want this committed, I'm quite happy to do the last mile.  ",
            "author": "Lance Norskog",
            "id": "comment-13428809"
        },
        {
            "date": "2012-08-23T14:44:25+0000",
            "content": "Yes, please, it would be awesome if someone could make this last effort and commit this issue. Many thanks! ",
            "author": "Alexey Kozhemiakin",
            "id": "comment-13440345"
        },
        {
            "date": "2012-08-27T02:39:06+0000",
            "content": "Committable except for dev-tools/ and production builds. I've updated dev-tools/eclipse, I don't have IntelliJ. These dev-tools build files contain 'uima' and so need parallel work for 'opennlp':\n\ndev-tools/maven/lucene/analysis/pom.xml.template\ndev-tools/maven/lucene/analysis/uima/pom.xml.template\ndev-tools/maven/pom.xml.template\ndev-tools/maven/solr/contrib/pom.xml.template\ndev-tools/maven/solr/contrib/uima/pom.xml.template\ndev-tools/scripts/SOLR-2452.patch.hack.pl\n  - this one seems to be dead\n\n\n ",
            "author": "Lance Norskog",
            "id": "comment-13442250"
        },
        {
            "date": "2012-08-27T02:41:09+0000",
            "content": "The latest patch is tested fully and painfully in trunk. I'm sure it works as-is in 4.x, but it is not going into 4.0, so I'm not spending time on that ",
            "author": "Lance Norskog",
            "id": "comment-13442251"
        },
        {
            "date": "2012-09-30T13:34:31+0000",
            "content": "Could you please create a new Patch for the current Trunk? I had some problems on applying it to my working copy...\n\nI am not entirely sure whether its the Trunk or your Code, but it seems like your OpenNLP-code only works for the first request.\n\nAs far as I was able to debug, the create()-method of the TokenFilterFactory is only called every now and again (are created TokenFilters reused for longer than one call in Solr?).\n\nIf create() of your FilterFactory was called, everything works. However if the TokenFilter is somehow reused, it fails. \n\nIs this a bug of Solr or of your Patch? ",
            "author": "Em",
            "id": "comment-13466478"
        },
        {
            "date": "2012-09-30T17:20:31+0000",
            "content": "Some Attributes were not reset (i.e. \"first\"-Attribute in OpenNLPTokenizer and \"indexToken\" in OpenNLPFilter) correctly.\n\nSince I had trouble applying your patch, I'd like to provide the working source code. Please, create a patch for the current Trunk.  ",
            "author": "Em",
            "id": "comment-13466527"
        },
        {
            "date": "2012-09-30T20:41:31+0000",
            "content": "Thank you!\n\nThis worked when I posted it. There have been many changes in 4.x and trunk since then. For example, all of the tokenizer and filter factories moved to Lucene from Solr. I'm waiting until 4.0 is finished before I redo this patch. \n\n ",
            "author": "Lance Norskog",
            "id": "comment-13466565"
        },
        {
            "date": "2012-10-29T18:36:44+0000",
            "content": "Would there be a patch for 4.0 as it is released. ",
            "author": "Phani Vempaty",
            "id": "comment-13486242"
        },
        {
            "date": "2012-11-19T22:33:01+0000",
            "content": "Thanks for this patch!\n\nI'm able to get the posTagger working, yet I still have not found a way to incorporate either the Chunker or the NER Models into my Solr project.\n\nSetting posTagger by itself works, but when I add a link to the chunkerModel (or even just the chunkerModel by itself), I obtain only the tokenized text.\n\n\n<fieldType name=\"text_opennlp_pos\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n<analyzer>\n   <tokenizer class=\"solr.OpenNLPTokenizerFactory\"\n      tokenizerModel=\"opennlp/en-token.bin\" />\n   <filter class=\"solr.OpenNLPFilterFactory\" \n      chunkerModel=\"opennlp/en-chunking.bin\"/>\n</analyzer>\n</fieldType>\n\n\n\nI'm new to OpenNLP, so any pointers in the right direction would be greatly appreciated. ",
            "author": "Patricia Gorla",
            "id": "comment-13500698"
        },
        {
            "date": "2012-12-31T05:30:35+0000",
            "content": "Wow, someone tried it! I apologize for not noticing your question.\n\nI'm able to get the posTagger working, yet I still have not found a way to incorporate either the Chunker or the NER Models into my Solr project.\n\nThe schema.xml file includes samples for all of the models:\n\n/lusolr_4x_opennlp/solr/contrib/opennlp/src/test-files/opennlp/solr/collection1/conf/schema.xml\n\nThis is for the chunker. The chunker works from parts-of-speech tags, not the original words. The chunker needs a parts-of-speech model as well as a chunker model. This should throw an error if the parts-of-speech model is not there. I will fix this.\n\n\n <filter class=\"solr.OpenNLPFilterFactory\" \n          posTaggerModel=\"opennlp/en-test-pos-maxent.bin\"\n          chunkerModel=\"opennlp/en-test-chunker.bin\"\n        />\n\n\n\nIs the NER configuration still not working? ",
            "author": "Lance Norskog",
            "id": "comment-13541285"
        },
        {
            "date": "2013-01-31T13:05:39+0000",
            "content": "The patch seems to be a bit out of date.\nApplying it to branch_4x or trunk fails (build scripts). ",
            "author": "Kai G\u00fclzau",
            "id": "comment-13567605"
        },
        {
            "date": "2013-01-31T16:30:00+0000",
            "content": "End of OpenNLPTokenizer.fillBuffer() should be:\n\nwhile(length == size) {\n  offset += size;\n  fullText = Arrays.copyOf(fullText, offset + size);\n  length = input.read(fullText, offset, size);\n}\nif (length == -1) {\n  length = 0;\n}\nfullText = Arrays.copyOf(fullText, offset + length);\n\n ",
            "author": "Kai G\u00fclzau",
            "id": "comment-13567775"
        },
        {
            "date": "2013-01-31T21:50:09+0000",
            "content": "Thank you. Have you tried this on the trunk? The Solr components did not work, they could not find the OpenNLP jars. ",
            "author": "Lance Norskog",
            "id": "comment-13568150"
        },
        {
            "date": "2013-02-01T08:25:00+0000",
            "content": "I have applied the Patch to trunk, modified the build scripts manually (ignoring javadoc tasks) and built the opennlp jars.\nJars are running in a vanilla Solr 4.1 environment.\n\n\n\tsolr_server4.1\\solr\\lib\\opennlp\\\n\t\n\t\tjwnl-1.4_rc3.jar\n\t\tlucene-analyzers-opennlp-5.0-SNAPSHOT.jar (build with patch)\n\t\topennlp-maxent-3.0.2-incubating.jar\n\t\topennlp-tools-1.5.2-incubating.jar\n\t\tsolr-opennlp-5.0-SNAPSHOT.jar (build with patch)\n\t\n\t\n\n\n\nwith <lib dir=\"../lib/opennlp\" /> in solrconfig.xml\n\nWorks for me: http://mail-archives.apache.org/mod_mbox/lucene-solr-user/201301.mbox/%3CB65DA877C3F93B4FB39EA49A1A03C95CC27AB1%40email.novomind.com%3E\n\nedit: removed jwnl*.jar as stated by Joern ",
            "author": "Kai G\u00fclzau",
            "id": "comment-13568575"
        },
        {
            "date": "2013-02-01T10:32:09+0000",
            "content": "The jwnl library is only needed if you use the OpenNLP Coreference component, otherwise its safe to exclude it. The 1.4_rc3 version is not tested anyway and its likely that the Coreferencer does not probably run with it. ",
            "author": "Joern Kottmann",
            "id": "comment-13568640"
        },
        {
            "date": "2013-02-27T22:24:11+0000",
            "content": "New patch for both trunk and 4.1 stable. Tested on revision 1450998.\n\n\nant compile\ncd solr/contrib/src/test-files/training\nsh bin/trainall.sh\ncd ../../../../../../solr\nant example test-contrib\n\n \n\nHope this helps more people in testing OpenNLP integration with Solr.\n\nTODO: \n\n\tImplementing dev-tools\n\tInclude references to javadocs\n\n ",
            "author": "Rene Nederhand",
            "id": "comment-13588854"
        },
        {
            "date": "2013-02-27T22:24:44+0000",
            "content": "New patch for both trunk and 4.1 stable. Tested on revision 1450998.\n\n\nant compile\ncd solr/contrib/src/test-files/training\nsh bin/trainall.sh\ncd ../../../../../../solr\nant example test-contrib\n\n \n\nHope this helps more people in testing OpenNLP integration with Solr.\n\nTODO: \n\n\tImplementing dev-tools\n\tInclude references to javadocs\n\n ",
            "author": "Rene Nederhand",
            "id": "comment-13588855"
        },
        {
            "date": "2013-04-17T08:49:18+0000",
            "content": "why don't you prepare this as separate project that produces some jars and config files with instructions on how to add it in solr configuration instead of publishing all changes as patches to solr sources? I am interested in doing some tests with your library but setting all things up seems quite complicated and hard to maintain in future... it is just a thought. ",
            "author": "Maciej Lizewski",
            "id": "comment-13633907"
        },
        {
            "date": "2013-04-25T13:02:01+0000",
            "content": "Some information for those wanting to try this after fighting it for a day: the latest patch posted, LUCENE-2899-RJN.patch for 4.1 does not have Em's OpenNLPFilter.java and OpenNLPTokenizer.java fixed applied. So after applying the patch, make sure to replace those classes with Em's version or the bug that causes the NLP system to only be utilized on the first request will still be present. I was also able to successfully apply this patch to 4.2.1 with minor modification (mostly to the build/ivy xml files). ",
            "author": "Zack Zullick",
            "id": "comment-13641739"
        },
        {
            "date": "2013-04-25T17:20:17+0000",
            "content": "Maciej- This is a good point. This package needs changes in a lot of places and it might be easier to package it the way you say. \n\nZack- The \"churn\" in the APIs is a major problem in the Lucene code management. The original patch worked in the 4.x branch and trunk when it was posted. What Em fixed is in an area which is very very basic to Lucene. The API changed with no notice and no change in versions or method names. \n\nEveryone- It's great that this has gained some interest. Please create a new master patch with whatever changes are needed for the current code base.\n\nLucene grand masters- Please don't say \"hey kids, write plugins, they're cool!\" and then make subtle incompatible changes in APIs.  ",
            "author": "Lance Norskog",
            "id": "comment-13641968"
        },
        {
            "date": "2013-05-20T03:55:58+0000",
            "content": "I'm updating the patches for 4.x and trunk. Kai's fix works. The unit tests did not attempt to analyse text that is longer than the fixed size temp buffer, and thus the code for copying successive buffers was never exercised. Kai's fix handles this problem. I've added a unit test. \n\nEm: the Lucene Tokenizer lifecyle is that the Tokenizer is created with a Reader, and each call to incrementToken() walks the input. When incrementToken() returns false, that is all- the Tokenizer is finished. TokenStream can support a 'stateful' token stream: with OpenNLPFilter, you call incrementToken() until it returns false, and then you can call 'reset' and it will start over from the beginning. The unit tests include a check that reset() works. The changes you made support a feature that is not supported by Lucene. Also, the changes break most of the unit tests. Please create a unit test that shows the bug, and fix the existing unit tests. No unit test = no bug report.\n\nI'm posting a patch for the current 4.x and trunk. It includes some changes for TokenStream/TokenFilter method signatures, some refactoring in the unit tests, a little tightening in the Tokenizer & Filter, and Kai's fix. There are unit tests for the problem Kai found, and also a test that has TokenizerFactory create multiple Tokenizer streams. If there is a bug in this patch, please write a unit test which demonstrates it.\n\nThe patch is called LUCENE-2899-current.patch. It is tested against the current 4.x branch and the current trunk.\n\nThanks for your interest and hard work- I know it is really tedious to understand this code \n\nLance Norskog ",
            "author": "Lance Norskog",
            "id": "comment-13661758"
        },
        {
            "date": "2013-06-06T05:00:48+0000",
            "content": "I found the problem with multiple documents. The API for reusing Tokenizers changed something more sensible, but I only noticed and implemented part of the change. The result was than when you upload multiple documents, it just re-processed the first document.\n\nFile LUCENE-2899-x.patch has this fix. It applies against the 4.x branch and the trunk. It does not apply against Lucene 4.0, 4.1, 4.2 or 4.3. For all released Solr versions you want LUCENE-2899.patch from August 27, 2012. There are no new features since that release. ",
            "author": "Lance Norskog",
            "id": "comment-13676698"
        },
        {
            "date": "2013-06-06T07:26:36+0000",
            "content": "Lance, does the patch gets jwnl form our old SourceForge page? This page is often overloaded and probably makes your build unstable. To solve this issue (see OPENNLP-510) we moved jwnl for 1.5.3 to the central repo. Anyway as long as you don't use the coreference component you can exclude this dependency. ",
            "author": "Joern Kottmann",
            "id": "comment-13676795"
        },
        {
            "date": "2013-06-06T18:20:03+0000",
            "content": "Yup- upgrading to 1.5.3 is next on the list. ",
            "author": "Lance Norskog",
            "id": "comment-13677336"
        },
        {
            "date": "2013-06-10T04:15:57+0000",
            "content": "I did not make the right changes to OpenNLPFilter.java to handle the API changes. I have attached a fixed version of this to this issue. Please try it and see if it fixes what you see.\n\nA-a-a-a-a-a-n-n-n-n-d chunking is broken. Oy.\n ",
            "author": "Lance Norskog",
            "id": "comment-13679293"
        },
        {
            "date": "2013-06-16T22:56:24+0000",
            "content": "Fixed the Chunker problem. I switched to the new released version of the OpenNLP packages. The MaxEnt implementation (statistical modeling) for chunking changed slightly, and my test data now produces different noun&verb phrase chunks for the sample text.\n\nAt this point the only problems I know of are that the licenses are slightly wrong, and so \n'ant validate' fails.\n\nThese comments only apply to LUCENE-2899x.patch, which applies to the current 4.x and trunk codelines. LUCENE-2899.patch applies to the release 4.0>4.3 releases. It is not upgraded to the new OpenNLP release. ",
            "author": "Lance Norskog",
            "id": "comment-13684828"
        },
        {
            "date": "2013-07-23T18:44:27+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13716957"
        },
        {
            "date": "2013-07-31T01:54:46+0000",
            "content": "A little bit of a shameless plug, but we just wrote a blog post here about using the stanford library for NER as a processor factory / request handler for Solr. It seems applicable to the audience on this ticket, is it worth contributing it to the community via a patch of some sort? ",
            "author": "Andrew Janowczyk",
            "id": "comment-13724748"
        },
        {
            "date": "2013-08-01T00:50:14+0000",
            "content": "Yup! Another NER is always helpful.  But the big problem with NLP software is not the code but the models- do you have a good source of free models?  ",
            "author": "Lance Norskog",
            "id": "comment-13725911"
        },
        {
            "date": "2013-08-01T07:56:02+0000",
            "content": "Stanford NLP is licensed under GPLv2, this license is not compatible with the AL 2.0 and therefore such a component can't be contributed to an Apache project directly. ",
            "author": "Joern Kottmann",
            "id": "comment-13726191"
        },
        {
            "date": "2013-08-01T08:33:35+0000",
            "content": "ahhh thanks for the info. i found a relevant link discussing the licenses which clearly explains the details here. oh well, it was worth a try  ",
            "author": "Andrew Janowczyk",
            "id": "comment-13726221"
        },
        {
            "date": "2013-08-02T08:21:40+0000",
            "content": "@Lance Norskog we now have support in OpenNLP to train the name finder on a corpus in the Brat [1] data format, that makes it much easier to annotate custom data within a couple of days/weeks.\n\n[1] http://brat.nlplab.org/ ",
            "author": "Joern Kottmann",
            "id": "comment-13727478"
        },
        {
            "date": "2013-08-03T18:53:41+0000",
            "content": "Wow! Brat looks bitchin! Looking forward to using it. ",
            "author": "Lance Norskog",
            "id": "comment-13728617"
        },
        {
            "date": "2013-10-01T06:01:29+0000",
            "content": "Hi, \nI have applied this patch successfully on SOLR latest branch 4.x. But now I am not getting how to perform contextual searches on the data I have. I need to perform search on text field using some NLP process. I am new to NLP so need some help on how do I proceed further. How to train model using this integrated solr ? Do I need to study some thing else before moving ahead with this ?\n\nI designed a analyzer and tried indexing data. But the results are weird and inconsistent. Kindly provide some pointers to move ahead \n\nThanks in advance. ",
            "author": "rashi gandhi",
            "id": "comment-13782654"
        },
        {
            "date": "2013-10-03T07:01:41+0000",
            "content": "Hi,\n\nI designed an analyzer using OpenNLP filters and indexed some data on it.\n\n <fieldType name=\"text_opennlp_nvf\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer type=\"index\">\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n      </analyzer>\n\t  <analyzer type=\"query\">\n       <tokenizer class=\"solr.OpenNLPTokenizerFactory\" sentenceModel=\"opennlp/en-sent.bin\" tokenizerModel=\"opennlp/en-token.bin\"/>\n       <filter class=\"solr.OpenNLPFilterFactory\" posTaggerModel=\"opennlp/en-pos-maxent.bin\"/>\n        <filter class=\"solr.FilterPayloadsFilterFactory\" payloadList=\"NN,NNS,NNP,NNPS,VB,VBD,VBG,VBN,VBP,VBZ,FW\" keepPayloads=\"true\"/>\n\t\t<filter class=\"solr.StripPayloadsFilterFactory\"/>\n\t\t<filter class=\"solr.LowerCaseFilterFactory\"/>\n\t\t<filter class=\"solr.StopFilterFactory\" ignoreCase=\"true\" words=\"stopwords.txt\" enablePositionIncrements=\"true\" />\n      </analyzer>\n </fieldType>\n\n <field name=\"Detail_Nvf\" type=\"text_opennlp_nvf\" indexed=\"true\" stored=\"true\" omitNorms=\"true\" omitPositions=\"true\"/>\n\nMy problem is:While searching, SOLR sometimes return result and sometimes not ( but documents are there).\nfor example: if i search for Detail_Nvf:brett ,it returns a document\nand after sometime again if i fire the same query, it returns Zero document\n Iam not getting why SOLR results are unstable.\nPlease help me on this.\n\nThanks in Advance ",
            "author": "rashi gandhi",
            "id": "comment-13784869"
        },
        {
            "date": "2013-10-03T12:39:29+0000",
            "content": "I have seen this behavior before (look up to previous comments, especially from user Em and his previous fix) and I am experiencing similar results with the latest patch uploaded (Jun-16-2013) on 4.4/branch_4x. In my case, the OpenNLP system is only working when indexing the first document then no longer working thereafter. It seems you are having a similar issue, with the exception that yours is happening on the query end rather than the indexing. I sent out an email to Lance to see if he has any advice for us.  ",
            "author": "Zack Zullick",
            "id": "comment-13785043"
        },
        {
            "date": "2013-10-03T14:04:31+0000",
            "content": "Thanks Zack\n\nWaiting for a reply from Lance  ",
            "author": "rashi gandhi",
            "id": "comment-13785216"
        },
        {
            "date": "2013-10-23T09:24:32+0000",
            "content": "Hi,\n\nI'm new to Solr and Opennlp.\nI have followed the tutorial to install this patch. I have downloaded the branch_4x, then i download and apply the LUCENE-2899-current.patch. Then i do \"ant compile\".\n\nEverything works fine, but no opennlp folder in /solr/contrib/ is created.\n\nWhat I am doing wrong?\n\nThanks for your help  ",
            "author": "simon raphael",
            "id": "comment-13802735"
        },
        {
            "date": "2013-10-23T16:03:45+0000",
            "content": "Hi-\n\nThe latest patch is LUCENE-2899-x.patch, pls try that. Also, apply it with:\npatch -p0 < patchfile\n\nLance\n\n ",
            "author": "Lance Norskog",
            "id": "comment-13802982"
        },
        {
            "date": "2013-11-04T02:33:50+0000",
            "content": "This patch includes a fix for the problem where searching twice doesn't work. The file is LUCENE-2899.patch \nIt has been tested with trunk, branch_4x and the 4.5.1 release.\n\nI do not know of any outstanding issues. To avoid confusion, I have removed all old patches. ",
            "author": "Lance Norskog",
            "id": "comment-13812581"
        },
        {
            "date": "2013-11-06T09:55:47+0000",
            "content": "Hi, \n\nI have a problem after installing the patch. I can't launch Solr anymore. I've got the following error : \n\nPlugin init failure for [schema.xml] analyzer/tokenizer: Error loading class 'solr.OpenNLPTokenizerFactory'\n\nThough the opennlp*.jar files are correctly added : \n\nAdding 'file:/var/www/lucene_solr_4_5_1/solr/contrib/opennlp/lib/opennlp-tools-1.5.3.jar' to classloader\n5453 [coreLoadExecutor-3-thread-1] INFO  org.apache.solr.core.SolrResourceLoader  \u2013 Adding 'file:/var/www/lucene_solr_4_5_1/solr/contrib/opennlp/lib/opennlp-maxent-3.0.3.jar' to classloader\n\n\nAny idea of what I am doing wrong ?\n\nThank you  ",
            "author": "simon raphael",
            "id": "comment-13814753"
        },
        {
            "date": "2013-11-07T19:01:36+0000",
            "content": "The solrconfig.xml file should have these lines in the library set:\n\n  <lib dir=\"../../../contrib/opennlp/lib\" regex=\".*\\.jar\" />\n  <lib dir=\"../../../dist/\" regex=\"solr-opennlp-\\d.*\\.jar\" />\n\nAlso, you have to copy lucene/build/analysis/opennlp/lucene-analyzers-opennlp*.jar to {{solr/contrib/opennlp/lib/} .\n\nThis last problem was a mess. I have not followed these issues: SOLR-3664, LUCENE-5249, LUCENE-5257. I don't know if they handle the problem I described. Shipping this thing as a Lucene/Solr contrib module patch was a mistake- it intersects the build&code structure in too many places. ",
            "author": "Lance Norskog",
            "id": "comment-13816277"
        },
        {
            "date": "2013-11-08T15:15:19+0000",
            "content": "Hi - any change this is going to get committed some day? ",
            "author": "Markus Jelsma",
            "id": "comment-13817332"
        },
        {
            "date": "2013-11-12T03:35:29+0000",
            "content": "Hi Markus: I haven't looked at this patch. I'll review it now and give my thoughts. ",
            "author": "Robert Muir",
            "id": "comment-13819791"
        },
        {
            "date": "2013-11-12T04:20:59+0000",
            "content": "Just some thoughts:\n\nI think it would be best to split out the different functionality here into subtasks for each piece, and figure out how each should best be integrated.\n\nThe current patch does strange things to try to deal with some impedence mismatch due to the design here, such as the tokenfilter which consumes the entire analysis chain and then replays the whole thing back with POS or NER as payloads. Is it really necessary to give this thing more scope than a single setnence? typically such tagging models (at least the ones ive worked with) tend to be trained only within sentence scope. \n\nAlso payloads should not be used internally, instead things like TypeAttribute should be used for POSTags, if someone wants to filter out certain POS or maintain certain POS they can use already existing stuff like TypeTokenFilter, if they want to index Type as a payload, they can use TypeAsPayloadTokenFilter, and so on.\n\nWhile I can see this POS-tagging being useful inside the analysis chain: the NER case is much less clear, I think its more important to e.g. be integrated outside of the analysis chain so that named entities/mentions can be faceted on, added to separate fields for search (likely with a different analysis chain for that), etc. So for lucene that would be an easier way to add these as facets, for solr it probably makes more sense as UpdateProcessor than as analysis chain.\n\nFinally: I'm confused as to what benefit we get from using OpenNLP directly, versus integrating with it via opennlp-uima? Our UIMA integration at various levels (analysis chain/update processor) is already there, so I'm just wondering if thats a much shorter way path. ",
            "author": "Robert Muir",
            "id": "comment-13819813"
        },
        {
            "date": "2013-11-12T11:34:28+0000",
            "content": "I know of an NER model that looks at the entire text to bias towards consistent tagging of entities in larger units. However, I agree that crocks are bad. Perhaps this is an opportunity to think about how to expand the analysis protocol to support this sort of thing more smoothly?\n\nIt would be desirable if this integration were to start with a set of Token Attributes that could be used in any number of analysis components, inside or outside of Lucene, that were in a position to deliver similar items. I suppose I'm late to ask for this, as the UIMA component must pose the same question.\n\nIn some languages, NER is very clumsy as a token filter, because entities don't obey token boundaries very well. Also, in my experience, entities aren't useful as additional tokens in the same field as their source text, but rather in their own field (where they can be facetted upon, for example). Is there any appetite to look at Lucene support for a stream that delivers to more than one field? Or is there such a thing and I've missed it?\n\nI agree with Rob about UIMA because I think that Lucene analysis attributes are a weak data model for interconnecting NLP modules and flowing data through them \u2013 and one frequently needs to do that.\n ",
            "author": "Benson Margulies",
            "id": "comment-13820031"
        },
        {
            "date": "2013-11-12T13:29:59+0000",
            "content": "I don't think we should expand the analysis protocol: I think its actually already more complicated than it needs to be.\n\nIt doesnt need to work across multiple fields or support things like NER.\n\nI know people disagree, but i dont care (typically they dont do a lot of work to maintain this code). \n\nI'll fight it to the death: Lucene's analysis is about doing information retrieval (search and query), and its already overly complex. It should stay per-field, it should stay like a state machine it is.\n\nStuff like this NER should NOT be in the analysis chain. as i said, its more useful in the \"document build\" phase anyway. ",
            "author": "Robert Muir",
            "id": "comment-13820083"
        },
        {
            "date": "2013-11-12T13:34:57+0000",
            "content": "Fair enough. Solr URP's do this very well upstream of analysis. ES doesn't have the concept, perhaps it should. It clarifies the situation nicely to think of Lucene as serial token operations. ",
            "author": "Benson Margulies",
            "id": "comment-13820085"
        },
        {
            "date": "2013-11-12T13:43:14+0000",
            "content": "Stuff like this NER should NOT be in the analysis chain. as i said, its more useful in the \"document build\" phase anyway.\n\n+1\n\nBenson, as far as I understand, ES doesn't have the concept by design. ",
            "author": "Christian Moen",
            "id": "comment-13820093"
        },
        {
            "date": "2013-11-12T14:10:20+0000",
            "content": "UIMA based NLP pipelines can use components like Solrcas or Lucas to write their results to an index. This works really well in my experience. ",
            "author": "Joern Kottmann",
            "id": "comment-13820115"
        },
        {
            "date": "2013-12-24T06:47:19+0000",
            "content": "Hi,\n\nI have successfully applied LUCENE-2899.patch to SOLR-4.5.1 and its working properly.\nNow , my requirement is to combine OpenNLP with jwnl.\nIs it possible to combine OpenNLP with jwnl and what are the changes required in SOLR schema.xml for the same?\nKindly provide some pointers to move ahead.\n\nThanks in Advance ",
            "author": "rashi gandhi",
            "id": "comment-13856194"
        },
        {
            "date": "2013-12-30T19:18:46+0000",
            "content": "All fair criticisms. \n\nAbout UIMA: clearly it is much more advanced than this design, but I'm not smart enough to use it . I've tried to put together something useful (a few times) and each time was completely confused. I learn by example, and the examples are limited. Also there is very little traffic on the mailing lists etc. about UIMA.\n\nAbout payloads v.s. internal attributes: the examples don't use this feature, but payloads are stored in the index. This supports a question-answering system. Add PERSON payloads with all records, then search for \"word X AND 'payload PERSON anywhere'\" when someone says \"who is X\". This does the tagging during indexing, but not searching. A better design would be to add PERSON as a synonym rather than a payload. I also don't see much traffic about payloads. \n\nAbout doing this in the analysis pipeline v.s. upstream: yes, upstream request processors are the right place for this. In Solr. URPs don't exist in ES or just plain Lucene coding.  ",
            "author": "Lance Norskog",
            "id": "comment-13859006"
        },
        {
            "date": "2013-12-30T19:26:30+0000",
            "content": "JWNL is WordNet. Lucene has a WordNet parser for use as a synonym filter.\nhttp://lucene.apache.org/core/4_0_0/analyzers-common/index.html?org/apache/lucene/analysis/synonym/SynonymMap.html\n\nI don't know how to use this from a Solr filter factory. Please ask this on the Solr mailing list. ",
            "author": "Lance Norskog",
            "id": "comment-13859009"
        },
        {
            "date": "2013-12-31T13:06:56+0000",
            "content": "ok, thanks Lance. One more Question\nI wanted to design an analyzer that can support location containment relationship , \nFor example Europe->France->Paris\nMy requirement is like: when user search for any country , then results must have the documents having that country , as well as the documents having states and cities which comes under that country.\nBut , documents with country name must have high relevancy.\nIt must obeys containment relationship up to 4 levels .i.e. Continent->Country->State->City \nI wanted to know , is there any way in OpenNLP that can support this type of search.\nCan location tagger model can be used for this?\nPlease provide me some pointers to move ahead\n\nThanks in Advance ",
            "author": "rashi gandhi",
            "id": "comment-13859480"
        },
        {
            "date": "2014-04-16T12:54:36+0000",
            "content": "Move issue to Lucene 4.9. ",
            "author": "Uwe Schindler",
            "id": "comment-13970814"
        },
        {
            "date": "2014-05-23T13:16:50+0000",
            "content": "Hi,\n\n I have one running solr core with some data indexed on solr deployed on Tomcat. \nThis core  is designed to provide OpenNLP functionalities for indexing and searching.\nSo I have kept following binary models at this location: \\apache-tomcat-7.0.53\\solr\\collection1\\conf\\opennlp \n\u2022         en-sent.bin\n\u2022         en-token.bin\n\u2022         en-pos-maxent.bin\n\u2022         en-ner-person.bin\n\u2022         en-ner-location.bin\n\nMy Problem is: When I unload the running core, and try to delete conf directory from it.\nIt is not allowing me to delete directory with prompt that en-sent.bin and en-token.bin is in use.\nAll other files in conf directory are getting deleted except en-sent.bin and en-token.bin.\nIf I have unloaded core, then why it is not unlocking the connection with core?\nIs this a known issue with OpenNLP Binaries?\nHow can I release the connection between unloaded core and conf directory. (Specially binary models)\n\nPlease provide me some pointers on this.\nThanks in Advance ",
            "author": "rashi gandhi",
            "id": "comment-14007147"
        },
        {
            "date": "2014-06-04T06:01:30+0000",
            "content": "I followed this link to integrate https://wiki.apache.org/solr/OpenNLP to integrate\n\nInstallation\n\nFor English language testing: Until LUCENE-2899 is committed:\n\n    1.pull the latest trunk or 4.0 branch\n\n    2.apply the latest LUCENE-2899 patch\n    3.do 'ant compile'\n    cd solr/contrib/opennlp/src/test-files/training\n    .\n    .\n    . \ni followed first two steps but got the following error while executing 3rd point\n\ncommon.compile-core:\n    [javac] Compiling 10 source files to /home/biginfolabs/solrtest/solr-lucene-trunk3/lucene/build/analysis/opennlp/classes/java\n\n    [javac] warning: [path] bad path element \"/home/biginfolabs/solrtest/solr-lucene-trunk3/lucene/analysis/opennlp/lib/jwnl-1.3.3.jar\": no such file or directory\n\n    [javac] /home/biginfolabs/solrtest/solr-lucene-trunk3/lucene/analysis/opennlp/src/java/org/apache/lucene/analysis/opennlp/FilterPayloadsFilter.java:43: error: cannot find symbol\n\n    [javac]     super(Version.LUCENE_44, input);\n\n    [javac]                  ^\n    [javac]   symbol:   variable LUCENE_44\n    [javac]   location: class Version\n    [javac] /home/biginfolabs/solrtest/solr-lucene-trunk3/lucene/analysis/opennlp/src/java/org/apache/lucene/analysis/opennlp/OpenNLPTokenizer.java:56: error: no suitable constructor found for Tokenizer(Reader)\n    [javac]     super(input);\n    [javac]     ^\n    [javac]     constructor Tokenizer.Tokenizer(AttributeFactory) is not applicable\n    [javac]       (actual argument Reader cannot be converted to AttributeFactory by method invocation conversion)\n    [javac]     constructor Tokenizer.Tokenizer() is not applicable\n    [javac]       (actual and formal argument lists differ in length)\n    [javac] 2 errors\n    [javac] 1 warning\n\nIm really stuck how to passthough this step. I wasted my entire day to fix this but couldn't move a bit. Please someone help me..? ",
            "author": "vivek",
            "id": "comment-14017403"
        },
        {
            "date": "2015-05-18T21:30:11+0000",
            "content": "@vivek you can change the file and replace the super(Version.LUCENE_44, input) with super(input); ",
            "author": "Sameer Maggon",
            "id": "comment-14549267"
        },
        {
            "date": "2015-10-01T10:59:13+0000",
            "content": "I have tried this patch with the current trunk, it patches fine but when it goes to compile there are a lot of errors, some are easily fixed issues with java 8 being more strict but there are a few which are caused by method signatures being different.\n\nWill there be an updated patch to fix these issues?\n\nWhat is the status of this being fully integrated in to the trunk so a patch is not required? ",
            "author": "Alex Watson",
            "id": "comment-14939673"
        },
        {
            "date": "2015-10-10T22:51:47+0000",
            "content": "I don't work in this area anymore. Somebody else will have to make an up-to-date patch,  and you need to find a committer to be a champion for it.\n\nA tech report of a real-life deployment would be a great way to persuade someone.\n ",
            "author": "Lance Norskog",
            "id": "comment-14952086"
        },
        {
            "date": "2016-07-09T23:46:03+0000",
            "content": "Patch.  I took Lance Norskog's latest patch and did the following (among other modernization/cleanup stuff):\n\n\n\tUpgraded to the latest OpenNLP release version (1.6.0)\n\tMoved the analysis factories, along with their tests and test data, into the Lucene module at lucene/analysis/opennlp/\n\tRemoved the Solr contrib, since it only contained the analysis factories\n\tAdded SPI files for the analysis factories\n\tExtended BaseTokenStreamTestCase.assertTokenStreamContents() to test payload contents.\n\tConverted analysis tests to use the above method instead of the home-grown ones in tests in the previous patch.\n\tConverted the test model creation shell script into an Ant target named train-test-models.  I've run that target and included the binary models it produced in this patch.\n\tAdded IntelliJ and Maven config\n\tAdded license and checksum files for the two OpenNLP dependencies\n\tIncluded a dependency on the Lucene opennlp module in the Solr analysis-extras contrib, so that the module's jar and its dependencies be shipped with the distribution.\n\n\n\nAll the module's tests pass for me.\n\nI manually tested the Solr integration:\n\n\n\tbuilt the distribution and unpacked it\n\tcloned the data driven configs and modified solrconfig.xml to add <lib> elements for the two directories containing the necessary jars\n\tdownloaded English binary models from OpenNLP's sourceforge site\n\tvia the schema api, added a field type that performs sentence splitting, tokenization and POS tagging, and a field using it\n\tadded a simple doc with the opennlp-invoking field via curl and the update/json/docs handler\n\tsearched using the admin UI\n\trandom text pasted into the Admin UI's analysis pane shows payloads with POS tags (as hex bytes...)\n\n\n\nLeft to do prior to committability, IMHO, in no particular order:\n\n\n\tI think OpenNLPFilter should be broken up into a separate component for each of the things it can do.\n\tI agree with Robert Muir that the tagging functionality here should be converted to use token type instead of payloads.  Then the included FilterPayloadsFilter won't be necessary (since the TypeTokenFilter has the same functionality for token types), and probably the included StripPayloadsFilter won't be necessary either, since populating payloads would likely only be done as a final step in the analysis chain (e.g. via TypeAsPayloadTokenFilter).\n\tConvert the NER support to be a Solr update processor.\n\n\n\nNot sure it should prevent the current state from being committed, but: incorporating SegmentingTokenizerBase (extended by ThaiTokenizer and HMMChineseTokenizer) might be a useful improvement to the sentence-breaking/tokenization strategy currently used by OpenNLPTokenizer. ",
            "author": "Steve Rowe",
            "id": "comment-15369351"
        },
        {
            "date": "2016-07-10T11:35:21+0000",
            "content": "Patch, adds OpenNLPLemmatizerFilter, based on OpenNLP's SimpleLemmatizer (new in OpenNLP 1.6.0), which can lemmatize given a dictionary mapping surface form/part-of-speech => lemma. ",
            "author": "Steve Rowe",
            "id": "comment-15369594"
        },
        {
            "date": "2016-07-11T09:49:11+0000",
            "content": "Glad to see some update on this page. Hope this code gets committed soon.\nI am trying to use this patch to do some information extraction using SOLR and Open NLP.\nCan this patch be now used with latest version of SOLR . ",
            "author": "Ramesh Kumar Thirumalaisamy",
            "id": "comment-15370477"
        },
        {
            "date": "2016-07-11T13:45:24+0000",
            "content": "Steve Rowe per our conversation yesterday.. Would be interesting to store the PoS and entity information as stacked tokens vs (or in addition to the) payload... such that you could do \"bob @person\"~0 or \"house @verb\"~0 type queries.. or things like \"@person @ceo\"~10 ",
            "author": "Steven Bower",
            "id": "comment-15370786"
        },
        {
            "date": "2016-07-11T22:43:05+0000",
            "content": "Can this patch be now used with latest version of SOLR .\n\nThe patch named LUCENE-2899.patch is against the (unreleased) master branch of Lucene/Solr.\n\nI tried applying it to the 6.1.0 source code (from a git clone: git checkout releases/lucene-solr/6.1.0), and it failed in a couple places.\n\nSo I made a 6.1.0-specific patch named LUCENE-2899-6.1.0.patch and am attaching it to the issue.  Compilation and tests succeeded after I ran ant train-test-models from lucene/analysis/opennlp/. ",
            "author": "Steve Rowe",
            "id": "comment-15371827"
        },
        {
            "date": "2016-07-11T22:57:05+0000",
            "content": "Steve Rowe per our conversation yesterday.. Would be interesting to store the PoS and entity information as stacked tokens vs (or in addition to the) payload... such that you could do \"bob @person\"~0 or \"house @verb\"~0 type queries.. or things like \"@person @ceo\"~10\n\nSteven Bower, I agree, that possibility would be nice.  I checked for the existence of a token type->synonym filter, and don't see one, but I think it would be fairly easy to add.\n\nWhich reminds me: the lemmatization filter I added here should have the ability (like some stemmers, indirectly) to emit lemmas as synonyms - this is possible, as in the PorterStemmer implementaiton, simply by not processing any tokens with the keyword attribute set to true, and preceding with the KeywordRepeatFilter.  ",
            "author": "Steve Rowe",
            "id": "comment-15371843"
        },
        {
            "date": "2016-07-27T21:41:24+0000",
            "content": "Attaching another WIP patch with more progress:\n\n\n\tSwitched OpenNLPFilter to use TypeAttribute instead of PayloadAttribute to hold annotations from part-of-speech tagging, chunking and NER tagging.\n\tAdded a new TypeAsSynonymFilter to the analyzers-common module that  adds a token at the same position as a (presumably previously annotated) token, with the value of the TypeAttribute copied into its CharTermAttribute.  See Steven Bower's comment above for potential uses.\n\tRemoved the now unnecessary FilterPayloadsFilter and StripPayloadFilter that were present in previous iterations of the patch.\n\tAdded KeywordAttribute awareness to OpenNLPLemmatizationFilter, so that lemmatization won't be performed on tokens with isKeyword()==true.\n\tFixed the new payload-aware BaseTokenStreamTestCase.assertTokenStreamContents() to use BytesRef.equals() instead of directly comparing byte arrays and not referencing offset&length.\n\tAdded TypeAttribute awareness to CannedTokenStream.\n\n ",
            "author": "Steve Rowe",
            "id": "comment-15396443"
        },
        {
            "date": "2016-07-27T22:08:03+0000",
            "content": "Patch, only changes are fixes to the opennlp overview javadocs:\n\n\trefer to TypeAttribute instead of payloads\n\tremove mention of FilterPayloadTokenFilter and StripPayloadsFilter\n\trecommend TypeAsPayloadFilter and TypeAsSynonymFilter to make tags searchable\n\n ",
            "author": "Steve Rowe",
            "id": "comment-15396502"
        },
        {
            "date": "2016-07-28T00:55:12+0000",
            "content": "It's really cool to see someone with clout pick this up & modernize it.\n\nCheers,\n\nLance Norskog ",
            "author": "Lance Norskog",
            "id": "comment-15396746"
        },
        {
            "date": "2016-07-28T05:14:54+0000",
            "content": "Seconded, this is really useful stuff. ",
            "author": "Ryan Josal",
            "id": "comment-15396928"
        },
        {
            "date": "2016-08-09T10:08:22+0000",
            "content": "Indeed, great work! This would be most useful to have. ",
            "author": "Markus Jelsma",
            "id": "comment-15413325"
        },
        {
            "date": "2016-09-02T09:35:19+0000",
            "content": "sorry for the out of office reply, Exchange is not my favorite mail server :-\\ ",
            "author": "Kai G\u00fclzau",
            "id": "comment-15458062"
        },
        {
            "date": "2016-10-21T11:29:55+0000",
            "content": "The patch has this comment:\n\"EN POS tagger sometimes tags last word as a period if no period at the end\"\n\nDo you remove punctuation from sentence before it is passed to the POS Tagger, Chunker, or Name Finder?\n\nThe sourceforge models are trained with punctuation, but it shouldn't be difficult to retrain them if this is necessary.  ",
            "author": "Joern Kottmann",
            "id": "comment-15594847"
        },
        {
            "date": "2016-10-21T14:11:23+0000",
            "content": "IIRC a period is added to sentences that don't already have one.\n\nLance Norskog, since you're the author of the above-referenced comment, can you provide more detail here? ",
            "author": "Steve Rowe",
            "id": "comment-15595203"
        },
        {
            "date": "2016-10-22T01:22:55+0000",
            "content": "I don't remember if it's always or just seldom. It was just something I noticed when testing them. I'm not an NLP researcher, and I've been out of the Solr world for years. It sounds like Joern Kottman knows his way around this stuff. ",
            "author": "Lance Norskog",
            "id": "comment-15596885"
        },
        {
            "date": "2017-12-12T05:00:57+0000",
            "content": "Patch, lots of changes (see below), I think it's ready to go (precommit and all Lucene/Solr tests pass).  My plan is to wait a couple days for review, then commit if there are no objections.\n\nChanges since the last patch:\n\n\n\tCorrected some test model training data.\n\tSwitched OpenNLPTokenizer to require both the sentence and tokenizer models - I couldn't think of a reason to support specification of only one of them.\n\tOpenNLPTokenizer now extends SegmentingTokenizerBase, and uses an OpenNLP sentence segmenter via shim class OpenNLPSentenceBreakIterator.  End-of-sentence tokens are marked by setting a bit on the FlagsAttribute.  All OpenNLP-based filters operate on sentences.\n\tOpenNLPLemmatizerFilter now supports dictionary-based and/or model-based lemmatization; if both are configured, dictionary-based tokenization is performed first, and then out-of-vocabulary tokens are lemmatized using the model.\n\tEach analysis operation is now in its own component (previously OpenNLPFilter did multiple things).\n\tRemoved the end-of-sentence hack in OpenNLPPOSFilter (described in an earlier comment on this issue) - periods are no longer appended to sentences prior to pos tagging.\n\tNamed entity recognition is now handled in an update request processor in the analysis-extras Solr contrib (though the NER model loading machinery remains in OpenNLPOpsFactory in the lucene/analysis/opennlp module).\n\tAdded ref guide docs.\n\tAdded CHANGES.txt entries.\n\n ",
            "author": "Steve Rowe",
            "id": "comment-16287106"
        },
        {
            "date": "2017-12-13T13:25:40+0000",
            "content": "looks good to me, thanks Steve! ",
            "author": "Tommaso Teofili",
            "id": "comment-16289246"
        },
        {
            "date": "2017-12-15T16:25:56+0000",
            "content": "Commit b720e1ee3a524034fb8a8a6188b0b23bf17ff1cb in lucene-solr's branch refs/heads/branch_7x from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b720e1e ]\n\nLUCENE-2899: Add OpenNLP Analysis capabilities as a module ",
            "author": "ASF subversion and git services",
            "id": "comment-16292754"
        },
        {
            "date": "2017-12-15T16:25:59+0000",
            "content": "Commit 3e2f9e62d772218bf1fcae6d58542fad3ec43742 in lucene-solr's branch refs/heads/master from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=3e2f9e6 ]\n\nLUCENE-2899: Add OpenNLP Analysis capabilities as a module ",
            "author": "ASF subversion and git services",
            "id": "comment-16292755"
        },
        {
            "date": "2017-12-15T16:27:12+0000",
            "content": "Committed to master and branch_7x.  Thanks everybody! ",
            "author": "Steve Rowe",
            "id": "comment-16292757"
        },
        {
            "date": "2017-12-15T16:41:31+0000",
            "content": "Commit 464199293d169d7d2096f0428792d72a722cc927 in lucene-solr's branch refs/heads/branch_7x from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=4641992 ]\n\nLUCENE-2899: Fix hyperlink ",
            "author": "ASF subversion and git services",
            "id": "comment-16292775"
        },
        {
            "date": "2017-12-15T16:41:34+0000",
            "content": "Commit f5c4276163d1211f33dc0f27e947e7dc78aa0444 in lucene-solr's branch refs/heads/master from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f5c4276 ]\n\nLUCENE-2899: Fix hyperlink ",
            "author": "ASF subversion and git services",
            "id": "comment-16292776"
        },
        {
            "date": "2017-12-15T16:42:47+0000",
            "content": "Commit 46fa2e45f72a83bde83e2773a6e24673c73c7505 in lucene-solr's branch refs/heads/branch_7x from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=46fa2e4 ]\n\nLUCENE-2899: Fix hyperlink text ",
            "author": "ASF subversion and git services",
            "id": "comment-16292778"
        },
        {
            "date": "2017-12-15T16:42:49+0000",
            "content": "Commit 565d13c96d89064214f74a81739eaf6b9fb7be18 in lucene-solr's branch refs/heads/master from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=565d13c ]\n\nLUCENE-2899: Fix hyperlink text ",
            "author": "ASF subversion and git services",
            "id": "comment-16292779"
        },
        {
            "date": "2017-12-15T22:34:02+0000",
            "content": "My Jenkins found a reproducing seed on master for a TestOpenNLPPOSFilterFactory.testPos() failure:\n\n\n  [junit4] Suite: org.apache.lucene.analysis.opennlp.TestOpenNLPPOSFilterFactory\n  [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestOpenNLPPOSFilterFactory -Dtests.method=testPOS -Dtests.seed=9CB6DAAD9AB4A5C3 -Dtests.slow=true -Dtests.locale=lv-LV -Dtests.timezone=America/La_Paz -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n  [junit4] FAILURE 0.02s J2 | TestOpenNLPPOSFilterFactory.testPOS <<<\n  [junit4]    > Throwable #1: org.junit.ComparisonFailure: term 0 expected:<[Sentence]> but was:<[2]>\n  [junit4]    > \tat __randomizedtesting.SeedInfo.seed([9CB6DAAD9AB4A5C3:E04A51BF1852C7E]:0)\n  [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:201)\n  [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:325)\n  [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertTokenStreamContents(BaseTokenStreamTestCase.java:329)\n  [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:865)\n  [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:727)\n  [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.assertAnalyzesTo(BaseTokenStreamTestCase.java:390)\n  [junit4]    > \tat org.apache.lucene.analysis.opennlp.TestOpenNLPPOSFilterFactory.testPOS(TestOpenNLPPOSFilterFactory.java:75)\n  [junit4]    > \tat java.lang.Thread.run(Thread.java:748)\n  [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70), sim=Asserting(org.apache.lucene.search.similarities.AssertingSimilarity@4f758cba), locale=lv-LV, timezone=America/La_Paz\n  [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_151 (64-bit)/cpus=16,threads=1,free=412573592,total=514850816\n  [junit4]   2> NOTE: All tests run in this JVM: [TestOpenNLPPOSFilterFactory]\n\n ",
            "author": "Steve Rowe",
            "id": "comment-16293373"
        },
        {
            "date": "2017-12-19T00:04:36+0000",
            "content": "Commit 40ed963b362281c67b37efaca51a3dafca00762a in lucene-solr's branch refs/heads/branch_7x from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=40ed963 ]\n\nLUCENE-2899: tests: remove unused constants ",
            "author": "ASF subversion and git services",
            "id": "comment-16295903"
        },
        {
            "date": "2017-12-19T00:04:38+0000",
            "content": "Commit 42239ea51d9b1c34f20d273ab06821e22b421e54 in lucene-solr's branch refs/heads/branch_7x from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=42239ea ]\n\nLUCENE-2899: OpenNLPPOSFilter: fix reset() to fully reset ",
            "author": "ASF subversion and git services",
            "id": "comment-16295904"
        },
        {
            "date": "2017-12-19T00:04:41+0000",
            "content": "Commit 827595233751d97d8a2408e69be5dbaf004c7d55 in lucene-solr's branch refs/heads/master from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=8275952 ]\n\nLUCENE-2899: tests: remove unused constants ",
            "author": "ASF subversion and git services",
            "id": "comment-16295905"
        },
        {
            "date": "2017-12-19T00:04:43+0000",
            "content": "Commit f8fb13965612142e9ee91631c6ce80a7b255e348 in lucene-solr's branch refs/heads/master from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f8fb139 ]\n\nLUCENE-2899: OpenNLPPOSFilter: fix reset() to fully reset ",
            "author": "ASF subversion and git services",
            "id": "comment-16295906"
        },
        {
            "date": "2017-12-19T00:13:22+0000",
            "content": "My Jenkins found a reproducing seed on master for a TestOpenNLPPOSFilterFactory.testPos() failure\n\nI committed a fix for this and all other Jenkins failures I could find for this test suite: OpenNLPPosFilter.reset() wasn't working properly, resulting in state being inappropriately carried over from previous invocations. ",
            "author": "Steve Rowe",
            "id": "comment-16295915"
        },
        {
            "date": "2018-04-04T10:31:34+0000",
            "content": "Hi, do you plan to write documentation how to work with this feature?\n\nI have tried to install this and get it work on SolrCloud but I have no luck.\n\nThere no much answers or info on stackoverflow so it will be nice to have docs to start using this feature!\u00a0 ",
            "author": "Alexey Ponomarenko",
            "id": "comment-16425302"
        },
        {
            "date": "2018-04-04T13:19:28+0000",
            "content": "Hi, do you plan to write documentation how to work with this feature? I have tried to install this and get it work on SolrCloud but I have no luck.\n\nThis feature has not yet been released - Lucene/Solr 7.3 will include it when it's released, which will (very likely) happen today.\n\nThe 7.3 Solr reference guide, which is already online, includes some docs for the language analysis features added under this issue: http://lucene.apache.org/solr/guide/7_3/language-analysis.html#opennlp-integration, Here is the Solr 7.3 javadoc, also already online, for the NER update request processor: https://lucene.apache.org/solr/7_3_0/solr-analysis-extras/org/apache/solr/update/processor/OpenNLPExtractNamedEntitiesUpdateProcessorFactory.html ",
            "author": "Steve Rowe",
            "id": "comment-16425492"
        },
        {
            "date": "2018-04-04T14:34:09+0000",
            "content": "Thanks, but how can I upload model files to the Zookeeper?\n\nI have asked similar question to SO but nobody can answer me https://stackoverflow.com/questions/49515397/upload-filebinary-into-zookeeper-solrcloud\n\nWithout uploading model files I can not use OpenNLP this is a crucial point in installation.\u00a0 ",
            "author": "Alexey Ponomarenko",
            "id": "comment-16425619"
        },
        {
            "date": "2018-04-04T14:41:12+0000",
            "content": "how can I upload model files to the Zookeeper?\n\nHave you seen https://lucene.apache.org/solr/guide/7_3/using-zookeeper-to-manage-configuration-files.html and https://lucene.apache.org/solr/guide/7_3/command-line-utilities.html ?  In particular, from https://lucene.apache.org/solr/guide/7_3/command-line-utilities.html#put-a-local-file-into-a-new-zookeeper-file:\n\n\n./server/scripts/cloud-scripts/zkcli.sh -zkhost 127.0.0.1:9983 -cmd putfile /my_zk_file.txt /tmp/my_local_file.txt\n\n ",
            "author": "Steve Rowe",
            "id": "comment-16425628"
        },
        {
            "date": "2018-04-04T15:07:20+0000",
            "content": "Thanks, I saw this but I will try. But the point is that I am using windows and I am trying to use\u00a0solr zk\u00a0command but as far as I see this is wrong direction.\u00a0\n\nAnyway I will try\u00a0server/scripts/cloud-scripts/zkcli.bat .\n\nThanks a lot for pointing me into right direction.\u00a0 ",
            "author": "Alexey Ponomarenko",
            "id": "comment-16425667"
        },
        {
            "date": "2018-04-04T22:20:13+0000",
            "content": "The last time I read up on ZK, files are limited to 1mb. The ZK \"file system\" is intended for small configuration files. NLP models can be many megabytes. You might need an alternate path (scp) to distribute NLP models. On Windows, SMB file sharing. ",
            "author": "Lance Norskog",
            "id": "comment-16426251"
        },
        {
            "date": "2018-04-04T22:23:29+0000",
            "content": "I'm so cheered up that Steve Rowe\u00a0picked this up and added it to Solr! ",
            "author": "Lance Norskog",
            "id": "comment-16426256"
        },
        {
            "date": "2018-04-05T08:30:29+0000",
            "content": "Lance Norskog Thanks for advise. so I need to use SCP or SMB protocol to point to this model files somewhere in network?\u00a0\n\nWhat are network protocols supported by SolrCloud?\u00a0\n\nI don't event know that SolrCloud can download model files somewhere from network.\u00a0 ",
            "author": "Alexey Ponomarenko",
            "id": "comment-16426624"
        },
        {
            "date": "2018-04-05T10:14:00+0000",
            "content": "No, I think you may need to copy the model files to the right directory on\neach SolrCloud server via your own custom script.\nOr, have the files on a network share and then mount that share on each\nSolrCloud server, using the same letter on all servers.\n\nOn Thu, Apr 5, 2018 at 1:31 AM, Alexey Ponomarenko (JIRA) <jira@apache.org>\n\n\n\n\n\u2013 \nLance Norskog\nlance.norskog@gmail.com\nRedwood City, CA ",
            "author": "Lance Norskog",
            "id": "comment-16426706"
        },
        {
            "date": "2018-04-05T10:18:31+0000",
            "content": "Lance Norskog\u00a0No, I think you may need to copy the model files to the right directory on -\u00a0But what is this directory?\n\nI have tried some directories but I have no luck. ",
            "author": "Alexey Ponomarenko",
            "id": "comment-16426711"
        },
        {
            "date": "2018-04-06T13:25:07+0000",
            "content": "Lance Norskog One more question. How can I use SMB and\\or scp with SolrCloud correclty?\n\nEvent if I use someting like this:\u00a0\n\n// smb://DESKTOP-LMQI80K/opennlp/en-tokenizer.bin or \\\\DESKTOP-LMQI80K/opennlp/en-tokenizer.bin or file://DESKTOP-LMQI80K/opennlp/en-pos-maxent.bin\n\n\n\nSolr is throwing strange error:\n\n\n\norg.apache.solr.common.SolrException:org.apache.solr.common.SolrException: Could not load conf for core numberplate_shard2_replica_n6: Can't load schema managed-schema: java.io.IOException: Error opening /configs/numberplate/smb://DESKTOP-LMQI80K/opennlp/en-pos-maxent.bin\n\n\n\nIt seems that it \"want to find\" files inside of Zookeeper.   ",
            "author": "Alexey Ponomarenko",
            "id": "comment-16428310"
        },
        {
            "date": "2018-04-06T13:37:18+0000",
            "content": "BTW here is part of my management-schema config: \n\n\n\n\t<fieldType name=\"text_opennlp_pos\" class=\"solr.TextField\">\n\t  <analyzer>\n       <tokenizer class=\"solr.OpenNLPTokenizerFactory\"\n             sentenceModel=\"opennlp/en-sent.bin\"\n             tokenizerModel=\"opennlp/en-token.bin\"/>\n        <filter class=\"solr.OpenNLPPOSFilterFactory\" posTaggerModel=\"smb://DESKTOP-LMQI80K/opennlp/en-pos-maxent.bin\"/>\n        <filter class=\"solr.TypeAsPayloadFilterFactory\"/>\n       <!-- <filter class=\"solr.TypeTokenFilterFactory\" types=\"stop.pos.txt\"/> -->\n      </analyzer>\n\t</fieldType>\n\n ",
            "author": "Alexey Ponomarenko",
            "id": "comment-16428331"
        },
        {
            "date": "2018-04-06T13:49:45+0000",
            "content": "Note the workaround on SOLR-4793 for ZK resources larger than 1M; from the ZK admin manual:\n\n\nUnsafe Options\n\nThe following options can be useful, but be careful when you use them. The risk of each is explained along with the explanation of what the variable does.\n\n[...]\n\njute.maxbuffer:\n(Java system property: jute.maxbuffer)\nThis option can only be set as a Java system property. There is no zookeeper prefix on it. It specifies the maximum size of the data that can be stored in a znode. The default is 0xfffff, or just under 1M. If this option is changed, the system property must be set on all servers and clients otherwise problems will arise. This is really a sanity check. ZooKeeper is designed to store data on the order of kilobytes in size.\n\n\u00a0This is spelled out a little more here: https://www.shi-gmbh.com/tutorials/increase-file-size-zookeeper/ ",
            "author": "Steve Rowe",
            "id": "comment-16428348"
        },
        {
            "date": "2018-04-06T14:56:09+0000",
            "content": "Steve Rowe Thanks for pointing me into right direction. But maybe you know about putting model files somewhere in network. This is my prev question. Maybe you know something about this as Lance Norskog said about this? ",
            "author": "Alexey Ponomarenko",
            "id": "comment-16428420"
        },
        {
            "date": "2018-04-06T14:59:33+0000",
            "content": "Steve Rowe Thanks for pointing me into right direction. But maybe you know about putting model files somewhere in network. This is my prev question. Maybe you know something about this as Lance Norskog said about this?\n\nSorry, I haven't tested this, but I believe you'll have to use locally attached storage on each server, and specify an absolute path. ",
            "author": "Steve Rowe",
            "id": "comment-16428422"
        },
        {
            "date": "2018-04-06T15:06:55+0000",
            "content": "I should mention that the ideal\u00a0hosting location for OpenNLP models would\u00a0be the Blob Store, but that is not currently possible for schema-loaded classes - see SOLR-9175. ",
            "author": "Steve Rowe",
            "id": "comment-16428434"
        },
        {
            "date": "2018-04-06T15:38:49+0000",
            "content": "Steve Rowe Thanks, I will try your solution. But I will also wait for Lance Norskog about network solutions.  ",
            "author": "Alexey Ponomarenko",
            "id": "comment-16428471"
        },
        {
            "date": "2018-04-06T17:46:38+0000",
            "content": "I apologize, Alexey Ponomarenko, but I cannot help here. I have not worked with Solr for a few years.\n ",
            "author": "Lance Norskog",
            "id": "comment-16428665"
        },
        {
            "date": "2018-04-10T07:09:00+0000",
            "content": "Lance Norskog Ok, I will try Steve Rowe solution, but I think that we need to add something of we  have discussed to documentation. But unfortunately it seems to be that I have no enough rights to do that   ",
            "author": "Alexey Ponomarenko",
            "id": "comment-16431821"
        },
        {
            "date": "2018-04-17T08:54:34+0000",
            "content": "Hi once more I am trying to implement named entities extraction using this manual https://lucene.apache.org/solr/7_3_0//solr-analysis-extras/org/apache/solr/update/processor/OpenNLPExtractNamedEntitiesUpdateProcessorFactory.html \n\nI am modified solrconfig.xml like this: \n\n\n\n <updateRequestProcessorChain name=\"multiple-extract\">\n   <processor class=\"solr.OpenNLPExtractNamedEntitiesUpdateProcessorFactory\">\n     <str name=\"modelFile\">opennlp/en-ner-person.bin</str>\n     <str name=\"analyzerFieldType\">text_opennlp</str>\n     <str name=\"source\">description_en</str>\n     <str name=\"dest\">content</str>\n   </processor>\n </updateRequestProcessorChain>\n\n\n\nBut when I was trying to add data using:\n\nrequest: \n\nPOST http://localhost:8983/solr/numberplate/update?version=2.2&wt=xml&update.chain=multiple-extract\n\n\n<add><doc><field name=\"description_en\">This is Steve Jobs 2 </field><field name=\"content_pos\">This is text 2</field><field name=\"content\">This is text for content 2</field></doc></add>\n\n\n\nresponse\n\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response>\n    <lst name=\"responseHeader\">\n        <int name=\"status\">0</int>\n        <int name=\"QTime\">3</int>\n    </lst>\n</response>\n\n\n\nBut I don't see any data inserted to content field and in any other field. \n\nIf you need some additional data I can provide it. \n\nCan you help me? What have I done wrong? \n\n\n ",
            "author": "Alexey Ponomarenko",
            "id": "comment-16440624"
        }
    ]
}