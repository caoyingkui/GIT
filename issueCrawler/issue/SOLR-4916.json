{
    "id": "SOLR-4916",
    "title": "Add support to write and read Solr index files and transaction log files to and from HDFS.",
    "details": {
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [
            "4.4",
            "6.0"
        ],
        "components": [],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "",
    "attachments": {
        "SOLR-4916-move-MiniDfsCluster-deps-from-solr-test-framework-to-solr-core.patch": "https://issues.apache.org/jira/secure/attachment/12591764/SOLR-4916-move-MiniDfsCluster-deps-from-solr-test-framework-to-solr-core.patch",
        "SOLR-4916.patch": "https://issues.apache.org/jira/secure/attachment/12587702/SOLR-4916.patch",
        "SOLR-4916-ivy.patch": "https://issues.apache.org/jira/secure/attachment/12590914/SOLR-4916-ivy.patch",
        "SOLR-4916-nulloutput.patch": "https://issues.apache.org/jira/secure/attachment/12591815/SOLR-4916-nulloutput.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-13680854",
            "date": "2013-06-12T00:48:13+0000",
            "content": "I'm working on getting a first rev for this in line with trunk and 4x. Patrick Hunt and I did the bulk of the implementation while Greg Chanan added the support for talking to a kerberos'd hdfs.\n\nWe borrowed the HdfsDirectory from Apache Blur and contributed some code back to them. Initially, it's in Solr because that was easiest and satisfied my initial goals - however, someone that knows Lucene modules a bit better than me might want to take on moving it later on, with the idea that we might even collaborate with the Apache Blur guys on it in one location.\n\nI'm close to having an initial patch. I have to take care of 2 test fails that I think are related to the changes in SOLR-4655, and investigate a test fail in TestCloudManagedSchema. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13681339",
            "date": "2013-06-12T15:42:08+0000",
            "content": "The second issue is actually a fail in TestCloudManagedSchemaAddField (or it also can happen in TestCloudManagedSchema). It seems to depend on the luck of the classpath - both tests pass when run in eclipse for me.\n\nUnfortunately, the issue seems to be a jar hell issue. Some hdfs test classes we need require Jetty 6.1.26 on the test classpath. Previously, probably because of a lot of package name changes in Jetty from 6 to 8, non of the tests had a problem with both 6.1.26 and 8.1.10 on the classpath. It seems one or both of these tests do have a problem though.\n\nNot sure what the solution might be yet. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13681367",
            "date": "2013-06-12T16:34:52+0000",
            "content": "For the first issue (the 2 shard split test fails):\n\nThe change that I think actually caused to start failing is when Shalin made it so that if waitForState timed out, it failed the split. I was missing one small piece from SOLR-4655 where we set the correct coreNodeName for the subshard when we waitForState - with that change, the shard split tests are passing.\n\nFor initial issues, that only leaves problem one (jetty and the cp issue) to deal with. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13681887",
            "date": "2013-06-13T03:37:55+0000",
            "content": "I've figured out what was going on with the jetty issue. All tests are currently passing - I'll post my first patch tomorrow. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13682729",
            "date": "2013-06-13T21:02:19+0000",
            "content": "A first patch - more commentary to come. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13682781",
            "date": "2013-06-13T21:37:03+0000",
            "content": "FYI - this patch incorporates SOLR-4655 as mentioned above. "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13683219",
            "date": "2013-06-14T08:56:18+0000",
            "content": "Mark, this functionality looks very cool!\n\nRe. Hadoop dependencies: the patch adds a hard dependency on Hadoop and its dependencies directly to Solr core. I wonder if it's possible to refactor it so that it could be optional and the functionality itself moved to contrib/ - this way only users who want to use HdfsDirectory would need Hadoop deps.\n\nRe. Cache and BlockCache implementation - I did something similar in Luke's FsDirectory, where I decided to use Ehcache, although that implementation was read-only so it was much simpler. Performance improvements for repeated searches were of course dramatic, not so much for unique queries though. Do you have some preliminary benchmarks for this implementation, how much slower is the indexing / searching? Anyway, doing an Ehcache-based implementation of Cache with your patch seems straightforward, too.\n\nThere's very little javadoc / package docs for the new public classes and packages.\n\nWhat are HdfsDirectory.LF_EXT and getNormalNames() for? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13683270",
            "date": "2013-06-14T10:36:54+0000",
            "content": "Thanks for taking a look AB!\n\nRe. Hadoop dependencies: the patch adds a hard dependency on Hadoop and its dependencies directly to Solr core. I wonder if it's possible to refactor it so that it could be optional and the functionality itself moved to contrib/ - this way only users who want to use HdfsDirectory would need Hadoop deps.\n\nYeah, I don't really beleive in Solr contribs - they are not so useful IMO - it's a pain to actually pull them out and it has to be done after the fact. Given the size of the dependencies is such a small percentage of the current size, that we don't want to support the UpdateLog as actually pluggable, and that it would be nice that hdfs was supported out of the box just as local filesystem, I don't see being a contrib being much of a win. It saves a few megabytes when we are already well over 100 - and that's if you are willing to pull it apart after you download it. From what I've seen, even with the huge extract contrib, most people don't bother repackaging. It's hard to imagine they would for a few megabytes. \n\nCache and BlockCache imple\n\nWe have done some casual benchmarking - loading tweets at a high rate of speed while sending queries at a high rate of speed with 1 second NRT - essentially the worst case NRT scenerio. By and large, performance has been similiar to local filesystem performance. We will likely share some numbers when we have some less casual results. You do of course have to warm up the block cache before it really kicks in.\n\nIn terms of impl, as I mentioned, the orig HdfsDirectory comes from the Blur guys - we tried not to change it too much currently - not until we figure out if we might evolve it with them in the future - eg as a Lucene module or something.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13685642",
            "date": "2013-06-17T15:12:57+0000",
            "content": "\nThe Patch:\n\n\n\tAn HdfsDirectory implementation that uses a BlockDirectory to cache (read/write) hdfs blocks.\n\n\n\nThe default index codec currently supports append only filesystems, so impl is fairly straightforward and effective. It would be interesting if we could easily tell if a codec was append only.\n\n\n\tAn HdfsDirectoryFactory to hook this into Solr.\n\n\n\nNow that Directory is a first class citizen in Solr, allows pretty much everything to work on hdfs with few other tweaks, including Replication. \nAdds a new option to DirectoryFactory to have Searchers explicitly reserve commits points - no delete on last close like unix and no delete while in use fails like windows.\n\n\n\tAn HdfsUpdateLog that allows writing the transaction log to hdfs as well.\n\n\n\nI talked to Yonik a while back and I think we are in agreement that we don't want to currently support making a pluggable UpdateLog - so this one is built in and triggers on using an hdfs:// prefixed update log path.\n\n\n\tAn HdfsLockFactory.\n\n\n\nSimple impl to write lock files to hdfs rather than the local filesystem.\n\n\n\tSOLR-4655 Overseer should assign node names\n\n\n\nIncludes the work for SOLR-4566 - while a good general improvement, this is also important for this patch because we use the node name in hdfs paths - if a different machine takes over for that path, it's awkward to have the address for another machine as part of it.\n\n\n\tTests\n\n\n\nThere a few new tests specifically written for HDFS. There are also a bunch of new tests that simply run the current pertinent SolrCloud tests against hdfs. Because the SolrCloud tests are already so long, on a slower machine, this can greatly increase the test run time. It's actually almost no noticeable slow down on my 6 core machine, but it's pretty awful on my 2 core machine. To deal with this, in my patch, I have made the tests that are functionally equivalent to current tests but run against hdfs, only run nightly. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13687031",
            "date": "2013-06-18T18:19:33+0000",
            "content": "I think this is a pretty solid base to iterate on, so I'd like to commit before long to minimize the cost of keeping this set of changes in sync. I'll upload a patch updated to trunk in a bit. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13687051",
            "date": "2013-06-18T18:44:39+0000",
            "content": "Patch to trunk. "
        },
        {
            "author": "Jack Krupansky",
            "id": "comment-13687167",
            "date": "2013-06-18T20:48:46+0000",
            "content": "Is this intended to be a 5.0-only feature, or 4.x or maybe 4.5 or maybe even 4.4?\n\nAren't there a lot of different distributions of Hadoop? So, when Andrzej mentions this patch adding Hadoop as a core Solr dependency, what exactly will that dependency be? 1.2.0? Or, will the Hadoop release be pluggable/configurable?\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13688094",
            "date": "2013-06-19T16:00:43+0000",
            "content": "It doesn't greatly affect other parts of Solr, it's not some big experimental change, so I intend to first commit to 5x and see how jenkins likes things and then backport to 4.x.\n\nA lot of the core changes for this have slowly gone into 4.x long ago - including issues around making custom Directories first class in Solr and other little changes.\n\nThis builds to run against Apache Hadoop 2.0.5-alpha. I don't suspect that will be easily 'pluggable', but it will be easy enough to change the ivy files to point to another Hadoop distro, fix any compile time errors (if there are any), run the tests, and build Solr.\n\nBecause our dependency is on client code that talks to hdfs, I suspect that it will work fine as is with most distros based on the same version of Apache Hadoop - and probably other versions as well in many cases. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13688434",
            "date": "2013-06-19T20:48:16+0000",
            "content": "New Patch:\n\n\n\tMerged up to trunk.\n\tConsolidates hadoop versions in ivy files like is already done with jetty - set the version in one spot in the ivy file.\n\tWorks around random test fail caused by https://issues.apache.org/jira/browse/HADOOP-9643\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13694199",
            "date": "2013-06-26T19:55:15+0000",
            "content": "Okay, now that SOLR-4926 is squared away, I am ready to commit this.\n\nI'm sure that it's going to make the shadow maven build angry. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13694204",
            "date": "2013-06-26T19:59:18+0000",
            "content": "Who knows what evil lurks in the heart of Solr?  The shadow maven build knows... "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13694231",
            "date": "2013-06-26T20:47:35+0000",
            "content": "The commit-tag-bot user cannot currently log in - JIRA wants him to solve a captcha to log in, but doesn't seem to accept any of my answers. Bleh.\n\nCommitted:\n5x: 1497072 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13694265",
            "date": "2013-06-26T21:44:35+0000",
            "content": "Mark: You missed to add the new dependencies to NOTICE.txt. This is especially important for CDDL files. They have to be listed in NOTICE.txt\n\nIs there no solution for the jetty 1.6.26 classes? Can we add easymocks to work around the outdated jetty version, if its not actually used just referred to? The existence of jetty 1.6.26 is horrible, I would have -1 that if I would have seen earlier. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13694279",
            "date": "2013-06-26T21:53:46+0000",
            "content": "You missed to add the new dependencies to NOTICE.txt.\n\nYeah, I only added blur - I'll look at what else should be added.\n\nIs there no solution for the jetty 1.6.26 classes?\n\nI'm not really concerned about it atm - it's a dependency that the tests have for running the namenode - it's only a test framework dependency and all of the package names are different than in later version of jetty so they don't overlap. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13694300",
            "date": "2013-06-26T22:13:03+0000",
            "content": "5x: r1497133 Update NOTICE file and remove log4j from test-framework dependencies "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13694306",
            "date": "2013-06-26T22:19:29+0000",
            "content": "Thanks! "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13694371",
            "date": "2013-06-27T00:33:05+0000",
            "content": "5x: r1497159 add assume false to test for java 8 "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13694781",
            "date": "2013-06-27T15:08:44+0000",
            "content": "There are some jenkins/test issues to look into. "
        },
        {
            "author": "Jack Krupansky",
            "id": "comment-13694790",
            "date": "2013-06-27T15:23:15+0000",
            "content": "To be clear, this specific Jira is only about reading and writing of internal Solr files from HDFS, not indexing of user data from HDFS, correct?\n\nIn other words, this would not provide support for reading from HDFS by the \"stream.file\" and \"stream.url\" update parameters, correct?\n\nBut, external file fields would be covered, correct? As well as all other Solr \"conf\" configuration files (like stopwords)?\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13694850",
            "date": "2013-06-27T16:38:11+0000",
            "content": "It looks like we will have to ignore hdfs tests on Windows for now - running on Windows requires Cygwin - it seems the current Windows fails happen while trying to make 'df' shell calls from the NameNode. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13694866",
            "date": "2013-06-27T16:59:11+0000",
            "content": "It should not call \"df\" at all (also not on unix!). This is not good and platform independent at all. We can also not check that the shell call does not do any harm outside the java sandbox.\n\nI was about to remove execute permissions from the policy file. Currently this test is (fortunate) the only one calling shell commands! "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13694870",
            "date": "2013-06-27T17:05:03+0000",
            "content": "We can also not check that the shell call does not do any harm outside the java sandbox.\n\nWe simply use a java client when running in Solr. This is just for tests - to run an hdfs filesystem to test against. To run an hdfs env we have to run what the Apache Hadoop project makes.\n\nAs a side note, I think it's way overkill to ban shelling out in Lucene/Solr. Chill out policeman  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13694875",
            "date": "2013-06-27T17:11:49+0000",
            "content": "5x r1497451 Do not run hdfs tests on Windows as it requires cygwin\nURL: http://svn.apache.org/r1497451 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13694881",
            "date": "2013-06-27T17:13:55+0000",
            "content": "I would prefer to disable this functionality in Mini-Slowdoop-Cluster. There is for sure a config option, let me revisit the source code. There is no reason to run DF from Java, you can do all this with Java 7's Path API. And for tests, disk free is not a problem at all, so could be left out.\n\nThe execute permission is also enabled for the JRE crash tester  It spawns another JRE, so it needs execute permission (this is why it is there). The policy file is there to not allow any test to work outside the test sandbox and e.g. modify files in the checkout. If you can execute commands, this cannot be checked anymore - that was the reason why I wanted to disable this. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13694883",
            "date": "2013-06-27T17:16:31+0000",
            "content": "We have one more failing windows test, maybe some unclosed file:\n\n\n[junit4:junit4] Suite: org.apache.solr.store.blockcache.BlockDirectoryTest\n[junit4:junit4]   2> 544841 T1593 oassb.BlockDirectory.<init> Block cache on write is disabled\n[junit4:junit4]   2> 544842 T1593 oassb.BlockDirectory.<init> Block cache on read is disabled\n[junit4:junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=BlockDirectoryTest -Dtests.method=testEOF -Dtests.seed=D041A49D09140\n737 -Dtests.slow=true -Dtests.locale=cs_CZ -Dtests.timezone=US/Arizona -Dtests.file.encoding=Cp1252\n[junit4:junit4] ERROR   0.11s J2 | BlockDirectoryTest.testEOF <<<\n[junit4:junit4]    > Throwable #1: java.io.IOException: Unable to delete file: .\\org.apache.solr.store.hdfs.HdfsDirectory-1372353019\n215\\normal\\test.eof\n[junit4:junit4]    >    at __randomizedtesting.SeedInfo.seed([D041A49D09140737:412AE6954B30A14B]:0)\n[junit4:junit4]    >    at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:1919)\n[junit4:junit4]    >    at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1399)\n[junit4:junit4]    >    at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1331)\n[junit4:junit4]    >    at org.apache.commons.io.FileUtils.forceDelete(FileUtils.java:1910)\n[junit4:junit4]    >    at org.apache.commons.io.FileUtils.cleanDirectory(FileUtils.java:1399)\n[junit4:junit4]    >    at org.apache.commons.io.FileUtils.deleteDirectory(FileUtils.java:1331)\n[junit4:junit4]    >    at org.apache.solr.store.blockcache.BlockDirectoryTest.tearDown(BlockDirectoryTest.java:118)\n[junit4:junit4]    >    at java.lang.Thread.run(Thread.java:724)\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13694887",
            "date": "2013-06-27T17:19:18+0000",
            "content": "5x: r1497458 Do not run hdfs tests on FreeBSD because they do not play nice with blackhole\nURL: http://svn.apache.org/r1497458 "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13694891",
            "date": "2013-06-27T17:24:49+0000",
            "content": "Uwe, these shell commands are used because Hadoop has to run on Java 6. In addition to 'df' it uses 'whoami' and 'ls'.\n\nThere is for sure a config option, let me revisit the source code\n\nI wish it were so ... I use a set of AspectJ hacks to remove this dependency from Hadoop binaries to run tests on Windows. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13694894",
            "date": "2013-06-27T17:27:55+0000",
            "content": "5x: r1497468 Fix test to close properly\nURL: http://svn.apache.org/r1497468 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13694897",
            "date": "2013-06-27T17:33:05+0000",
            "content": "Uwe, these shell commands are used because Hadoop has to run on Java 6. In addition to 'df' it uses 'whoami' and 'ls'.\n\n\n\twhoami: System.getProperty(\"user.name\")\n\tls: WTF??\n\tdf: new File(path).getFreeSpace(), to get all mountpoints File#listRoots()\n\n "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13694907",
            "date": "2013-06-27T17:42:03+0000",
            "content": "I agree, this is crazy. with java6 you can implement all of these commands in pure java  "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-13694939",
            "date": "2013-06-27T18:31:58+0000",
            "content": "Don't shoot the messenger  I'm just reporting what's already there, and I agree it's somewhat crazy, but some information was not available in pure java < 7, for example file permissions and user groups. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13694945",
            "date": "2013-06-27T18:37:59+0000",
            "content": "permissions can be done in java6 too:\nFile.canRead/canExecute/canWrite/setReadable/setExecutable/setWritable\n\ni dont understand why user groups should be necessary. "
        },
        {
            "author": "The Heavy Commit Tag Bot",
            "id": "comment-13695049",
            "date": "2013-06-27T20:50:51+0000",
            "content": "[trunk commit] sarowe\nhttp://svn.apache.org/viewvc?view=revision&revision=1497563\n\nSOLR-4916: Maven configuration for the new HDFS deps "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13697213",
            "date": "2013-07-01T21:31:20+0000",
            "content": "Commit 1498702 from Mark Miller\n[ https://svn.apache.org/r1498702 ]\n\nSOLR-4916: Add support to write and read Solr index files and transaction log files to and from HDFS. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13697217",
            "date": "2013-07-01T21:35:47+0000",
            "content": "Commit 1498707 from Mark Miller\n[ https://svn.apache.org/r1498707 ]\n\nSOLR-4916: Update NOTICE file and remove log4j from test-framework dependencies "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13697218",
            "date": "2013-07-01T21:36:56+0000",
            "content": "Commit 1498710 from Mark Miller\n[ https://svn.apache.org/r1498710 ]\n\nSOLR-4916: add assume false to test for java 8 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13697223",
            "date": "2013-07-01T21:37:57+0000",
            "content": "Commit 1498711 from Mark Miller\n[ https://svn.apache.org/r1498711 ]\n\nSOLR-4916: Do not run hdfs tests on Windows as it requires cygwin "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13697225",
            "date": "2013-07-01T21:38:59+0000",
            "content": "Commit 1498712 from Mark Miller\n[ https://svn.apache.org/r1498712 ]\n\nSOLR-4916: Do not run hdfs tests on FreeBSD because they do not play nice with blackhole "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13697226",
            "date": "2013-07-01T21:39:58+0000",
            "content": "Commit 1498713 from Mark Miller\n[ https://svn.apache.org/r1498713 ]\n\nSOLR-4916: Fix test to close properly "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13697227",
            "date": "2013-07-01T21:41:02+0000",
            "content": "Commit 1498714 from Mark Miller\n[ https://svn.apache.org/r1498714 ]\n\nSOLR-4916: Maven configuration for the new HDFS deps "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13697231",
            "date": "2013-07-01T21:45:04+0000",
            "content": "I still need to toss together an initial bit of doc for this. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13699161",
            "date": "2013-07-03T16:49:05+0000",
            "content": "Commit 1499472 from Mark Miller\n[ https://svn.apache.org/r1499472 ]\n\nSOLR-4916: Merge out separate hdfs solrconfig.xml "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13699163",
            "date": "2013-07-03T16:50:22+0000",
            "content": "Commit 1499473 from Mark Miller\n[ https://svn.apache.org/r1499473 ]\n\nSOLR-4916: Merge out separate hdfs solrconfig.xml "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13700209",
            "date": "2013-07-04T16:52:09+0000",
            "content": "I don't really know ivy, but here is a patch that moves dfsminicluster dependencies from test-framework to core. I'm not really sure if the private conf stuff is working or not - I don't think we have another module that depends on core to check with... "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13700218",
            "date": "2013-07-04T17:03:13+0000",
            "content": "Duh, of course the contribs are also ivy modules that depends on core...I'll mess around and see if I can get this working nicely... "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13700259",
            "date": "2013-07-04T17:56:57+0000",
            "content": "I don't think this solution is very nice unless we have the \"conf\" really working. Currently all JARs are copied to lib folder ignoring the conf=\"...\" attribute and we have to filter them ourselves (see your patch where you exclude from WAR file).\n\nIn this case (without ivy:cachepatch/ivy:cachefileset) I would prefer the current solution.\n\nIMHO, the problem with the release smoker is more the fact that it \"checks too much\". The smoke tester should only deny javax classes in official lib folders and JAR files, not in test dependencies. I have really no problem with having the test dependencies in Solr's test-framework, of course not in Lucene's test-framework. In other lib dir we also have transient non-compile time dependencies.\n\nMy best idea would be to add a second lib folder (test-framework/runtime-libs) that is not packed into the binary ZIP file distribution. It's easy to add: We can add a separate resolve with another target folder. In Maven it should also definitely not be listed as dependency for runtime, too! "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13700262",
            "date": "2013-07-04T18:00:34+0000",
            "content": "Well that's no help - even if all of this is tied up to the classpaths used, it doesn't seem to be a mechanism for shielding modules from each other AFAICT. I guess the main use case is for downstream projects to have the ability to filter out these dependencies and avoid pulling down the test time dependencies - but it seems we would care about that in the maven shadow build, not here - we don't publish based on the ivy files right?\n\nIn that case, it would seem we should simply do the same thing as with some of the other jars in core that are excluded from the webapp - exclude them in the build.xml and have the maven build treat them as part of a test configuration?\n\nSteve Rowe, does any of that make any sense? "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13700275",
            "date": "2013-07-04T18:19:25+0000",
            "content": "I also don't want the files in the distribution ZIP, and currently they are listed in the distribution ZIP's test-framework folder (and this is why the smoke tester fails)! Because of that I proposed to have a separate lib folder that is never zipped (to WAR but also not to bin-tgz/bin-zip). "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13700300",
            "date": "2013-07-04T18:55:07+0000",
            "content": "Commit 1499842 from Mark Miller\n[ https://svn.apache.org/r1499842 ]\n\nSOLR-4916: Fix NOTICE - take Solr entry out of Lucene section "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13700320",
            "date": "2013-07-04T19:31:36+0000",
            "content": "My best idea would be to add a second lib folder (test-framework/runtime-libs) that is not packed into the binary ZIP file distribution. It's easy to add: We can add a separate resolve with another target folder. In Maven it should also definitely not be listed as dependency for runtime, too!\n\nI crossposted with this, so I had not read it yet. That's fine with me if Robert Muir is fine with it. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13700321",
            "date": "2013-07-04T19:33:42+0000",
            "content": "Commit 1499847 from Mark Miller\n[ https://svn.apache.org/r1499847 ]\n\nSOLR-4916: Fix NOTICE - take Solr entry out of Lucene section "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13700704",
            "date": "2013-07-05T14:16:28+0000",
            "content": "\nI guess the main use case is for downstream projects to have the ability to filter out these dependencies and avoid pulling down the test time dependencies - but it seems we would care about that in the maven shadow build, not here - we don't publish based on the ivy files right?\n\nIn that case, it would seem we should simply do the same thing as with some of the other jars in core that are excluded from the webapp - exclude them in the build.xml and have the maven build treat them as part of a test configuration?\n\nSteve Rowe, does any of that make any sense?\n\nYes, it does - if these deps were moved to solr-core and declared in the maven conf as test scope, they would not be pulled in as transitive deps by consumers of the solr-core artifact.\n\nMy best idea would be to add a second lib folder (test-framework/runtime-libs) that is not packed into the binary ZIP file distribution. It's easy to add: We can add a separate resolve with another target folder. In Maven it should also definitely not be listed as dependency for runtime, too!\n\nIf we leave the deps where they are now, on test-framework (which I don't think we should do, since these are really only solr-core deps), then they could be declared optional in the maven conf, but then all consumers that need these deps would need to declare them; so, at least in the maven config, there is zero point in keeping them as deps of test-framework.\n\nMy vote is to move the deps to solr-core. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13700741",
            "date": "2013-07-05T14:23:49+0000",
            "content": "My vote is to move the deps to solr-core.\n\n+1. Like test-only in Maven, for IVY, I would put them into a separate config and store in a separate directory, so they are not packaged: solr/core/test-libs "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13701116",
            "date": "2013-07-05T20:59:16+0000",
            "content": "Commit 1500135 from Steve Rowe\n[ https://svn.apache.org/r1500135 ]\n\nSOLR-4916: IntelliJ configuration (merged trunk r1497105) "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13705265",
            "date": "2013-07-11T00:15:41+0000",
            "content": "My best idea would be to add a second lib folder (test-framework/runtime-libs) that is not packed into the binary ZIP file distribution. It's easy to add: We can add a separate resolve with another target folder. In Maven it should also definitely not be listed as dependency for runtime, too!\n\n\nMy vote is to move the deps to solr-core.\n\n+1. Like test-only in Maven, for IVY, I would put them into a separate config and store in a separate directory, so they are not packaged: solr/core/test-libs\n\nThis patch moves the MiniDfsCluster dependencies from solr/test-framework/ivy.xml to solr/core/ivy.xml, using a separate Ivy configuration, and storing the deps in solr/core/test-lib/.  I also took the opportunity to store other test jars there: easymock and its deps.  As a result, we no longer need exceptions for these test-only deps when pulling from solr/core/lib/ to put into the war.\n\nThe patch also gives DIH the same treatment for easymock -> solr/contrib/dataimporthandler/test-lib/ - previously DIH got that test dep via solr/core/lib/.\n\nThe patch includes Ant/Ivy, IntelliJ, Maven, and Eclipse support for the dependency moves.  I successfully ran Solr tests under each of those, except Eclipse, which I don't use.\n\nI want to include this change in 4.4, so that we don't ship Maven config for solr test-framework with dependencies on solr-core-only test deps. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13705269",
            "date": "2013-07-11T00:19:30+0000",
            "content": "+1, I think it's a step forward. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13705443",
            "date": "2013-07-11T04:03:26+0000",
            "content": "Commit 1502105 from Steve Rowe\n[ https://svn.apache.org/r1502105 ]\n\nSOLR-4916: Move MiniDfsCluster test dependencies from solr test-framework to solr-core; download solr-core test dependencies to solr/core/test-lib/ instead of solr/core/lib/; download DIH test dependencies to solr/contrib/dataimporthandler/test-lib instead of [...]/lib/ "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13705453",
            "date": "2013-07-11T04:24:50+0000",
            "content": "Commit 1502113 from Steve Rowe\n[ https://svn.apache.org/r1502113 ]\n\nSOLR-4916: Move MiniDfsCluster test dependencies from solr test-framework to solr-core; download solr-core test dependencies to solr/core/test-lib/ instead of solr/core/lib/; download DIH test dependencies to solr/contrib/dataimporthandler/test-lib/ instead of [...]/lib/ (merged trunk r1502105) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13705454",
            "date": "2013-07-11T04:25:55+0000",
            "content": "Commit 1502114 from Steve Rowe\n[ https://svn.apache.org/r1502114 ]\n\nSOLR-4916: Move MiniDfsCluster test dependencies from solr test-framework to solr-core; download solr-core test dependencies to solr/core/test-lib/ instead of solr/core/lib/; download DIH test dependencies to solr/contrib/dataimporthandler/test-lib/ instead of [...]/lib/ (merged trunk r1502105) "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13705493",
            "date": "2013-07-11T05:55:57+0000",
            "content": "Cool, thanks! I am glad, easymock is gone, too \n\nThis is a good step forwards to better structure test dependencies in Solr! "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13705597",
            "date": "2013-07-11T08:31:13+0000",
            "content": "Commit 1502147 from Uwe Schindler\n[ https://svn.apache.org/r1502147 ]\n\nSOLR-4916: Re-add missing log=\"download-only\" "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13705598",
            "date": "2013-07-11T08:32:55+0000",
            "content": "Commit 1502148 from Uwe Schindler\n[ https://svn.apache.org/r1502148 ]\n\nMerged revision(s) 1502147 from lucene/dev/trunk:\nSOLR-4916: Re-add missing log=\"download-only\" "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13705600",
            "date": "2013-07-11T08:33:56+0000",
            "content": "Commit 1502150 from Uwe Schindler\n[ https://svn.apache.org/r1502150 ]\n\nMerged revision(s) 1502147 from lucene/dev/trunk:\nSOLR-4916: Re-add missing log=\"download-only\" "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13705634",
            "date": "2013-07-11T09:24:39+0000",
            "content": "Hi Mark,\nwhile reviewing the current committed stuff I found a \"bug\":\nHdfsDirectory has a special case to prevent the segments.gen file from being written: It redirecty the output to an NullIndexOutput. The bug is that HdfDirectory has a static instance of this NullIndexOutput, but the static instance has \"state\" (file-size, file position). When you return always the same instance this is keeping state and not safe if used by different threads in parallel - so it could cause bugs. openOutput must return a new instance (which costs nothing as its a small object on Eden Heap only).\n\nSee attached patch! "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13705636",
            "date": "2013-07-11T09:31:29+0000",
            "content": "Fixing another bug missing to update length. This is all not performance critical, as it affects segments.gen only.\n\nWill commit soon! "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13705639",
            "date": "2013-07-11T09:34:38+0000",
            "content": "Commit 1502167 from Uwe Schindler\n[ https://svn.apache.org/r1502167 ]\n\nSOLR-4916: Fix bugs & usage of NullIndexOutput "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13705641",
            "date": "2013-07-11T09:35:39+0000",
            "content": "Commit 1502168 from Uwe Schindler\n[ https://svn.apache.org/r1502168 ]\n\nMerged revision(s) 1502167 from lucene/dev/trunk:\nSOLR-4916: Fix bugs & usage of NullIndexOutput "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13705642",
            "date": "2013-07-11T09:36:30+0000",
            "content": "Commit 1502169 from Uwe Schindler\n[ https://svn.apache.org/r1502169 ]\n\nMerged revision(s) 1502167 from lucene/dev/trunk:\nSOLR-4916: Fix bugs & usage of NullIndexOutput "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13708015",
            "date": "2013-07-14T12:58:44+0000",
            "content": "Thanks Uwe - I'll relate these issues back to Apache Blur as well - perhaps it will help convince them of the benefits of collaborating on the HdfsDirectory in a Lucene module. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13717197",
            "date": "2013-07-23T18:47:26+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 "
        }
    ]
}