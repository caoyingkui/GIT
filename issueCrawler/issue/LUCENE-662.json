{
    "id": "LUCENE-662",
    "title": "Extendable writer and reader of field data",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/store"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Won't Fix",
        "status": "Resolved"
    },
    "description": "As discussed on the dev mailing list, I have modified Lucene to allow to define how the data of a field is writen and read in the index.\n\nBasically, I have introduced the notion of IndexFormat. It is in fact a factory of FieldsWriter and FieldsReader. So the IndexReader, the indexWriter and the SegmentMerger are using this factory and not doing a \"new FieldsReader/Writer()\".\n\nI have also introduced the notion of FieldData. It handles every data of a field, and also the writing and the reading in a stream. I have done this way because in the current design of Lucene, Fiedable is an interface, so methods with a protected or package visibility cannot be defined.\n\nA FieldsWriter just writes data into a stream via the FieldData of the field.\nA FieldsReader instanciates a FieldData depending on the field name. Then it use the field data to read the stream. And finnaly it instanciates a Field with the field data.\n\nAbout compatibility, I think it is kept, as I have writen a DefaultIndexFormat that provides some DefaultFieldsWriter and DefaultFieldsReader. These implementations do the exact job that is done today.\nTo acheive this modification, some classes and methods had to be moved from private and/or final to public or protected.\n\nAbout the lazy fields, I have implemented them in a more general way in the implementation of the abstract class FieldData, so it will be totally transparent for the Lucene user that will extends FieldData. The stream is kept in the fieldData and used as soon as the stringValue (or something else) is called. Implementing this way allowed me to handle the recently introduced LOAD_FOR_MERGE; it is just a lazy field data, and when read() is called on this lazy field data, the saved input stream is directly copied in the output stream.\n\nI have a last issue with this patch. The current design allow to read an index in an old format, and just do a writer.addIndexes() into a new format. With the new design, you cannot, because the writer will use the FieldData.write provided by the reader.\n\nenjoy !",
    "attachments": {
        "generic-fieldIO-3.patch": "https://issues.apache.org/jira/secure/attachment/12345430/generic-fieldIO-3.patch",
        "generic-fieldIO-2.patch": "https://issues.apache.org/jira/secure/attachment/12341499/generic-fieldIO-2.patch",
        "indexFormat.patch": "https://issues.apache.org/jira/secure/attachment/12352518/indexFormat.patch",
        "entrytable.patch": "https://issues.apache.org/jira/secure/attachment/12348433/entrytable.patch",
        "generic-fieldIO-5.patch": "https://issues.apache.org/jira/secure/attachment/12348432/generic-fieldIO-5.patch",
        "ASF.LICENSE.NOT.GRANTED--generic-fieldIO.patch": "https://issues.apache.org/jira/secure/attachment/12339330/ASF.LICENSE.NOT.GRANTED--generic-fieldIO.patch",
        "indexFormat-only.patch": "https://issues.apache.org/jira/secure/attachment/12354705/indexFormat-only.patch",
        "generic-fieldIO-4.patch": "https://issues.apache.org/jira/secure/attachment/12347765/generic-fieldIO-4.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2006-09-24T12:30:45+0000",
            "content": "I think I got it. What was disturbing on the last patch was the notion of FieldData I added. So I removed it. So let's summerize the diff between the trunk and my patch :\n\n\n\tThe concepts :\n\t\n\t\tan IndexFormat defines which FieldsWriter and FieldsReader to use\n\t\tan IndexFormat defines the used extensions, so the user can add it's own files\n\t\tthe format of an index is attached to the Directory\n\t\tthe whole index format isn't customizable, just a part of them. So some functions are private or \"default\", so the Lucene user won't have acess to them : it's Lucene internal stuff. Some others are public or protected : they can be redefined.\n\t\tLucene now provide an API to add some files which are tables of data, as the FieldInfos is\n\t\tit is to the FieldsWriter implementation to check if the field to write is of the same format (basically checking by a instanceof).\n\t\tthe user can add some information at the document level, and provide it's own implementation of Document\n\t\tthe user can define how data for a field is stored and retreived, and provide it's own implementation of Fieldable\n\t\tthe reading of field data is done in the Fieldable\n\t\tthe writting of the field is done in the FieldsWriter\n\t\n\t\n\n\n\n\n\tAPI change :\n\t\n\t\tThere are new constructors of the directory : contructors with specified IndexFormat\n\t\tnew Entry and EntryTable : generic API for managing a table of data in a file\n\t\tFieldInfos extends now EntryTable\n\t\n\t\n\n\n\n\n\tCode changes :\n\t\n\t\tAbstractField become Fieldable (Fieldable is no more an interface).\n\t\tthe FieldsWriter have been separated in the abstract class FieldsWriter and its default implementation DefaultFieldsWriter. Idem for FieldsReader and DefaultFieldsReader.\n\t\tthe lazy loading have been moved from FieldsReader to Fieldable\n\t\tIndexOuput can now write directly from an input stream\n\t\tIf a field was loaded lazily, the DefaultFieldsWriter directly copy the source input stream to the output stream\n\t\tthe IndexFileNameFilter take now it's list of known file extensions from the index format\n\t\teach time a temporary RAM directory is created, the index format have to be passed : see diff for CompoundFileReader or IndexWriter\n\t\tSome private and/or final have been moved to public\n\t\n\t\n\n\n\n\n\tLast worries :\n\t\n\t\tquite a big one in fact, but I don't know how to handle it : every RMI tests fails because of :\n\nerror unmarshalling return; nested exception is:\n    [junit]     java.io.InvalidClassException: org.apache.lucene.document.Field; no valid constructor\n    [junit] java.rmi.UnmarshalException: error unmarshalling return; nested exception is:\n    [junit]     java.io.InvalidClassException: org.apache.lucene.document.Field; no valid constructor\n    [junit]     at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:157)\n\n\n\t\ta function is public and it shouldn't : see Fieldable.setLazyData()\n\t\n\t\n\n\n\nI have added an exemple of implementation in the patch that use this future : look at org.apache.lucene.index.rdf\n\nI know this is a big patch but I think the API has not been broken, and I would appreciate comments on this. ",
            "author": "Nicolas Lalev\u00e9e",
            "id": "comment-12437202"
        },
        {
            "date": "2006-09-25T12:49:32+0000",
            "content": "Haven't looked fully at the patch, but one thing I am curious about is why remove the Fieldable interface? ",
            "author": "Grant Ingersoll",
            "id": "comment-12437543"
        },
        {
            "date": "2006-09-25T19:11:02+0000",
            "content": "It is due to lazy loading. A lazy field, when being retreived data, have to know how to read the stream. In the current trunk, a special implementation of Field is doing this. Here, we don't have control of the implemenation of Fieldable it will be. As I wanted to keep the lazy loading mechanism controlled internally in Lucene, being transparent to the user, I had to force every Fieldable implementation to know how about retreiving data lazily. So I switched the interface to an abstract class : in fact I have moved AbstractField to Fieldable.\nBut as I already raised, I still have an issue about it : the lazy loading mechanism isn't totally internal. The function Fieldable.setLazyData() shouldn't be public but default. ",
            "author": "Nicolas Lalev\u00e9e",
            "id": "comment-12437644"
        },
        {
            "date": "2006-10-20T09:44:09+0000",
            "content": "I just realized reading the recent discussing on lucene-dev (LazyField use of IndexInput not thread safe) that the implementation I have done isn't thread safe at all. The input is not cloned at all... ",
            "author": "Nicolas Lalev\u00e9e",
            "id": "comment-12443766"
        },
        {
            "date": "2006-11-21T16:10:20+0000",
            "content": "Here is an update of the patch:\n\n\tmerged with the last commit in trunk\n\tI have fixed the issue with stream cloning (just reusing the same way of cloning as it is done in the current trunk)\n\tthe FieldData is back. So the Fieldable is back too. And the worry I had about offering an internal function to public is gone.\n\tevery test passed.\n\tI have moved the bunch of classes that implement the FieldReader/FieldWriter in a RDF way into the tests. So there are some tests on this extension mechanism.\n\n ",
            "author": "Nicolas Lalev\u00e9e",
            "id": "comment-12451689"
        },
        {
            "date": "2006-12-12T13:36:44+0000",
            "content": "Hi Nicolas,\n\nHave you run any benchmarks on this?  Once I finish up some documentation stuff, my plan is to start digging into this. \n\n-Grant ",
            "author": "Grant Ingersoll",
            "id": "comment-12457713"
        },
        {
            "date": "2006-12-12T15:05:52+0000",
            "content": "Not at all.\n\nIn fact we don't use a lucene modified with my patch in our system. I only start working with lucene this year, and our search engine is a too critical component to play with a patched trunk. So I have even not tested it in real condition. ",
            "author": "Nicolas Lalev\u00e9e",
            "id": "comment-12457757"
        },
        {
            "date": "2006-12-22T17:16:35+0000",
            "content": "Patch synchronized with the trunk.\nI also tried to minimize the diff. And in fact I just realized that there are two patchs in one there : \n\n\tthe real object-oriented storage of field data.\n\tand some refactoring about the storage of the field infos : for reuse of the indexed binary storage of a table of String.\n\n\n\nI will try to seperate them. ",
            "author": "Nicolas Lalev\u00e9e",
            "id": "comment-12460548"
        },
        {
            "date": "2007-01-06T14:50:28+0000",
            "content": "here it is : I have synchronized with the current trunk, and I have splited the patch in two parts. ",
            "author": "Nicolas Lalev\u00e9e",
            "id": "comment-12462712"
        },
        {
            "date": "2007-03-03T20:46:27+0000",
            "content": "Patch update: synchronized with the trunk and new features.\n\n\n\tThe index format has now an ID which is serialized in a new file in the directory. This new file is managed by the SegmentInfos class. It has been pushed in a new file to keep me from breaking things, but it may be pushed in the segment file. This new feature will help to avoid opening index with wrong code. Like the index version, if the index format is not compatible, opening it fails. And it also fails while trying to use IndexWriter#addIndexes(). This compatibilities issues are managed by the implementations of the index format: an implementation have to implement the function canRead(String indexFmtID). But I think something is still missing in this design. Saying that a format is compatible is another one is OK, but I have to figured out if this is really possible to make a reader which handle two different formats.\n\n\n\n\n\tWhen synchronizing with the trunk, I had trouble with the new FieldSelectorResult : SIZE. This new feature expect the FieldsReader to know the size of the content of the field. With the generic FieldReader, the data is only a sequence of byte, so it cannot compute the size of the decoded data. I did a dumb implementation: it returns the size of the data in bytes. I know this is wrong, the associated tests fail (I let it fails in the patch). It has to be fixed, this may require some change in the API I have designed.\n\n\n\n\n\tThere was a discussion in java-dev about changing the order of the postings. Today in the .frq file, the document numbers are ordered by document number. The proposal was to order them by frequency. So I worked a little bit on the mechanism I have done to generify the field storing, and applied it to posting storing. This part of the patch proposed here is not well (nearly not at all) documented and is a draft. But it works (at least with the actual implementation), the mechanism allow to implement a custom PostingReader, PostingWritter :\n\n\n\npublic interface PostingWriter {\n  public void close() throws IOException;\n  public long[] getPointers();\n  public int getNbPointer();\n  public long writeSkip(RAMOutputStream skipBuffer) throws IOException;\n  public void write(int doc, int lastDoc, int nbPos, int[] positions) throws IOException;\n}\n\npublic interface PostingReader {\n  public void close() throws IOException ;\n  public TermDocs termDocs(BitVector deletedDocs, TermInfosReader tis, FieldInfos fieldInfos) throws IOException;\n  public TermPositions termPositions(BitVector deletedDocs, TermInfosReader tis, FieldInfos fieldInfos) throws IOException;\n}\n\nFurthermore this \"generification\" also allows an implementation invoked many times : http://wiki.apache.org/jakarta-lucene/FlexibleIndexing\nNote that it does not break the actual format. The .tis file is still managed internaly by Lucene and it holds pointers to some external files (managed by the indexFormat). The implementation of the PostingReader/PostingWriter specify how many pointers there are. The default one is 2 : .frq and .prx. The FlexibleIndexing would be 1.\n\n\n\tTo show that the default implementation of the index format can be changed, I have created a new package org.apache.lucene.index.impl which holds the actual index format :\n\n\n\tDefaultFieldData : the data part of Field\n\tDefaultFieldsReader : the non-generified part of the FieldsReader\n\tDefaultFieldsWriter : the non-generified part of the FieldsWriter\n\tDefaultIndexFormat : the factory of readers and writers\n\tDefaultPostringReader : just instanciate SegmentTermDocs and SegmentTermPositions\n\tDefaultPostringWriter : the posting writing part of DocumentWriter\n\tSegmentTermDocs : just moved\n\tSegmentTermPositions : just moved\n\n\n\n\n\tWhere I want to continue: I am mainly interested in the generic field storage, so I will continue to maintain it, I will try to fix the SIZE issue and will work about allowing readers being compatible with each other. I am also interested in some generic index storing for facetted search. But I figured out that the indexed data have to be stored at the document level. And this cannot be done with postings. So I don't think I will go further in playing with postings. I prefer look at LUCENE-584.\n\n ",
            "author": "Nicolas Lalev\u00e9e",
            "id": "comment-12477683"
        },
        {
            "date": "2007-03-04T07:38:48+0000",
            "content": "Nicolas,\n\nwow this looks like a lot of work!! There are good ideas in your patch. I have been and am currently very busy (moving to a new country...), so I probably won't have a chance to review it for another week or so. \n\nMichael ",
            "author": "Michael Busch",
            "id": "comment-12477758"
        },
        {
            "date": "2007-03-04T11:22:06+0000",
            "content": "Thanks Michael !\nI will appreciate a review and feedbacks, as it will open a lot the API, this will go even further than just make Document public (LUCENE-778). ",
            "author": "Nicolas Lalev\u00e9e",
            "id": "comment-12477774"
        },
        {
            "date": "2007-03-10T00:35:23+0000",
            "content": "Hi Nicolas,\n\nI tried applying indexFormat.patch and am getting:\ngrantingersoll@britta[1016]$ patch -p 0 -i ../patches/indexFormat.patch --dry-run\npatching file src/test/org/apache/lucene/store/IndexInputTest.java\npatching file src/test/org/apache/lucene/index/DocHelper.java\npatching file src/test/org/apache/lucene/index/TestIndexFormat.java\ncan't find file to patch at input line 369\nPerhaps you used the wrong -p or --strip option?\nThe text leading up to this was:\n--------------------------\n\n\n\n\n\nProperty changes on: src/test/org/apache/lucene/index/TestIndexFormat.java\n\n\n___________________________________________________________________\n\n\nName: svn:keywords\n\n\n   + Date Revision Author HeadURL Id\n\n\nName: svn:eol-style\n\n\n   + native\n\n\n\n\nIndex: src/test/org/apache/lucene/index/impl/TestSegmentTermDocs.java\n\n\n===================================================================\n\n\n\u2014 src/test/org/apache/lucene/index/impl/TestSegmentTermDocs.java     (revision 0)\n\n\n+++ src/test/org/apache/lucene/index/impl/TestSegmentTermDocs.java     (working copy)\n--------------------------\nFile to patch:\n\n\n\n\n\n--------\nMeaning, it doesn't know what to do with this diff.  From the looks of it, TestSegmentTermDocs.java did not get move to the impl directory from the directory it was in.\n\nI'm not sure how to handle this in SVN, but I suspect you have to do a local copy move first.  Perhaps try applying this patch to a clean checkout to let me know if it works for you.  Also, perhaps we can collaborate with Doron to write up some benchmarks or to at least make sure the existing benchmarks are covering this new way. ",
            "author": "Grant Ingersoll",
            "id": "comment-12479779"
        },
        {
            "date": "2007-03-10T15:05:35+0000",
            "content": "Hum... same here.... This is due to some svn mv, and I created the patch with svn diff.\nI can provide a patch with the complete diff, but you will loose the svn mv infos, so the svn history of the file will be lost.\nAny advise is welcomed. I will also ask on monday to my colleagues how they use to work with svn mv and patches. ",
            "author": "Nicolas Lalev\u00e9e",
            "id": "comment-12479843"
        },
        {
            "date": "2007-03-12T19:18:53+0000",
            "content": "Patch updated and synchornized with the trunk r517330.\nI have removed the \"svn mv\" I have done so now the patch is applying fine on a fresh trunk. The svn mv was just about creating the package impl. So everthing came back to o.a.l.index.\n\nNote about the last commit in trunk I have merged : lazy loading of the \"proxstream\". The feature is lost within this patch. I didn't took time to merge it properly. I think this is hightly feasable, but not just done. So a new item on the TODO list. ",
            "author": "Nicolas Lalev\u00e9e",
            "id": "comment-12480174"
        },
        {
            "date": "2007-04-01T19:26:30+0000",
            "content": "Synchronized with the trunk, so with the payload feature. It allowed me to refactor in one class the payload writing which is in two places today : it is now in the DefaultPostingWriter class.\n\nFrom my last update, the TODO list is still to do, nothing has been fixed. Furthermore there is a regression in the new patch : the ensureOpen() is not correctly handled for lazy loaded fields : a test fail. This is due to the fact that the FieldsReader doesn't handle it anymore in my patch. As the data struture can be customized, lazy loading is exported to the FieldData created by the FieldsReader. So the both instance have to communicate about the closing of the streams. So a new item in the TODO list.\n\nAs discussed in java-dev, here is a light patch with only the index format handling, without the possibility to redefine how data and postings are store/retreived. ",
            "author": "Nicolas Lalev\u00e9e",
            "id": "comment-12485913"
        },
        {
            "date": "2010-07-01T14:15:28+0000",
            "content": "Marking as won't fix, as I think the new flex indexing stuff takes care of this. ",
            "author": "Grant Ingersoll",
            "id": "comment-12884290"
        }
    ]
}