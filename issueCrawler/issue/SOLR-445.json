{
    "id": "SOLR-445",
    "title": "Update Handlers abort with bad documents",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "6.1",
            "7.0"
        ],
        "components": [
            "update"
        ],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "This issue adds a new TolerantUpdateProcessorFactory making it possible to configure solr updates so that they are \"tolerant\" of individual errors in an update request...\n\n\n  <processor class=\"solr.TolerantUpdateProcessorFactory\">\n    <int name=\"maxErrors\">10</int>\n  </processor>\n\n\n\nWhen a chain with this processor is used, but maxErrors isn't exceeded, here's what the response looks like...\n\n\n$ curl 'http://localhost:8983/solr/techproducts/update?update.chain=tolerant-chain&wt=json&indent=true&maxErrors=-1' -H \"Content-Type: application/json\" --data-binary '{\"add\" : { \"doc\":{\"id\":\"1\",\"foo_i\":\"bogus\"}}, \"delete\": {\"query\":\"malformed:[\"}}'\n{\n  \"responseHeader\":{\n    \"errors\":[{\n        \"type\":\"ADD\",\n        \"id\":\"1\",\n        \"message\":\"ERROR: [doc=1] Error adding field 'foo_i'='bogus' msg=For input string: \\\"bogus\\\"\"},\n      {\n        \"type\":\"DELQ\",\n        \"id\":\"malformed:[\",\n        \"message\":\"org.apache.solr.search.SyntaxError: Cannot parse 'malformed:[': Encountered \\\"<EOF>\\\" at line 1, column 11.\\nWas expecting one of:\\n    <RANGE_QUOTED> ...\\n    <RANGE_GOOP> ...\\n    \"}],\n    \"maxErrors\":-1,\n    \"status\":0,\n    \"QTime\":1}}\n\n\n\nNote in the above example that:\n\n\n\tmaxErrors can be overridden on a per-request basis\n\tan effective maxErrors==-1 (either from config, or request param) means \"unlimited\" (under the covers it's using Integer.MAX_VALUE)\n\n\n\nIf/When maxErrors is reached for a request, then the first exception that the processor caught is propagated back to the user, and metadata is set on that exception with all of the same details about all the tolerated errors.\n\nThis next example is the same as the previous except that instead of maxErrors=-1 the request param is now maxErrors=1...\n\n\n$ curl 'http://localhost:8983/solr/techproducts/update?update.chain=tolerant-chain&wt=json&indent=true&maxErrors=1' -H \"Content-Type: application/json\" --data-binary '{\"add\" : { \"doc\":{\"id\":\"1\",\"foo_i\":\"bogus\"}}, \"delete\": {\"query\":\"malformed:[\"}}'\n{\n  \"responseHeader\":{\n    \"errors\":[{\n        \"type\":\"ADD\",\n        \"id\":\"1\",\n        \"message\":\"ERROR: [doc=1] Error adding field 'foo_i'='bogus' msg=For input string: \\\"bogus\\\"\"},\n      {\n        \"type\":\"DELQ\",\n        \"id\":\"malformed:[\",\n        \"message\":\"org.apache.solr.search.SyntaxError: Cannot parse 'malformed:[': Encountered \\\"<EOF>\\\" at line 1, column 11.\\nWas expecting one of:\\n    <RANGE_QUOTED> ...\\n    <RANGE_GOOP> ...\\n    \"}],\n    \"maxErrors\":1,\n    \"status\":400,\n    \"QTime\":1},\n  \"error\":{\n    \"metadata\":[\n      \"org.apache.solr.common.ToleratedUpdateError--ADD:1\",\"ERROR: [doc=1] Error adding field 'foo_i'='bogus' msg=For input string: \\\"bogus\\\"\",\n      \"org.apache.solr.common.ToleratedUpdateError--DELQ:malformed:[\",\"org.apache.solr.search.SyntaxError: Cannot parse 'malformed:[': Encountered \\\"<EOF>\\\" at line 1, column 11.\\nWas expecting one of:\\n    <RANGE_QUOTED> ...\\n    <RANGE_GOOP> ...\\n    \",\n      \"error-class\",\"org.apache.solr.common.SolrException\",\n      \"root-error-class\",\"java.lang.NumberFormatException\"],\n    \"msg\":\"ERROR: [doc=1] Error adding field 'foo_i'='bogus' msg=For input string: \\\"bogus\\\"\",\n    \"code\":400}}\n\n\n\n...the added exception metadata ensures that even in client code like the various SolrJ SolrClient implementations, which throw a (client side) exception on non-200 responses, the end user can access info on all the tolerated errors that were ignored before the maxErrors threshold was reached.\n\n\n\n\nOriginal Jira Request\nHas anyone run into the problem of handling bad documents / failures mid batch.  Ie:\n\n<add>\n  <doc>\n    <field name=\"id\">1</field>\n  </doc>\n  <doc>\n    <field name=\"id\">2</field>\n    <field name=\"myDateField\">I_AM_A_BAD_DATE</field>\n  </doc>\n  <doc>\n    <field name=\"id\">3</field>\n  </doc>\n</add>\n\nRight now solr adds the first doc and then aborts.  It would seem like it should either fail the entire batch or log a message/return a code and then continue on to add doc 3.  Option 1 would seem to be much harder to accomplish and possibly require more memory while Option 2 would require more information to come back from the API.  I'm about to dig into this but I thought I'd ask to see if anyone had any suggestions, thoughts or comments.",
    "attachments": {
        "solr-445.xml": "https://issues.apache.org/jira/secure/attachment/12468219/solr-445.xml",
        "SOLR-445-alternative.patch": "https://issues.apache.org/jira/secure/attachment/12637960/SOLR-445-alternative.patch",
        "SOLR-445_3x.patch": "https://issues.apache.org/jira/secure/attachment/12469114/SOLR-445_3x.patch",
        "SOLR-445.patch": "https://issues.apache.org/jira/secure/attachment/12468218/SOLR-445.patch",
        "SOLR-445-3_x.patch": "https://issues.apache.org/jira/secure/attachment/12468644/SOLR-445-3_x.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Ryan McKinley",
            "id": "comment-12554452",
            "date": "2007-12-26T18:44:51+0000",
            "content": "yup... as is, it keeps anything it had before it fails.\n\nCheck http://wiki.apache.org/solr/UpdateRequestProcessor for a way to intercect the standard processing.  One could easily write an UpdateRequestProcessor to make sure all documents are valid before tryinig to commit them.  (that is queue everything up until finish())\n\nAnother approach may be to keep track of the docs that were added correctly, then remove them if something goes wrong.... "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12554466",
            "date": "2007-12-26T21:05:14+0000",
            "content": "Option 2 seems like a good approach... return a list of documents that failed, why they failed, etc, and that could be logged for a human to check later. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12556898",
            "date": "2008-01-08T13:52:56+0000",
            "content": "\nOption 2 seems like a good approach... return a list of documents that failed, why they failed, etc, and that could be logged for a human to check later.\n\n+1\n\nAnd it should not just be for the XML version, right?  Anything that can take a batch of documents should be able to return a list of those that failed along with the reason they failed. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12557681",
            "date": "2008-01-10T16:05:03+0000",
            "content": "Is it reasonable to use the AddUpdateCommand to communicate out of the UpdateHandler that a given document failed?  For instance, in the update handler, it could catch any exception, and then add that exception onto the command (the next reuse would have to reset it) and then the various RequestHandler (XML/CSV) can check to see if the exception is set, add it to a list of failed docs and then add the failed docs to the response, which can then be written out as needed by the writers? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12670777",
            "date": "2009-02-05T15:10:10+0000",
            "content": "Marking for 1.5 "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12872556",
            "date": "2010-05-27T22:07:45+0000",
            "content": "Bulk updating 240 Solr issues to set the Fix Version to \"next\" per the process outlined in this email...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E\n\nSelection criteria was \"Unresolved\" with a Fix Version of 1.5, 1.6, 3.1, or 4.0.  email notifications were suppressed.\n\nA unique token for finding these 240 issues in the future: hossversioncleanup20100527 "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-12981127",
            "date": "2011-01-13T04:22:09+0000",
            "content": "Here's a cut at an improvement at least.\n\nThe attached XML file contains an <add> packet with a number of documents illustrating a number of errors. The xml file can be POSTed Solr to index via the post.jar file so you can see the output.\n\nThis patch attempts to report back to the user the following for each document that failed:\n1> the ordinal position in the file where the error occurred (e.g. the first, second, etc <doc> tag).\n2> the <uniqueKey> if available.\n3> the error.\n\nThe general idea is to accrue the errors in a StringBuilder and eventually re-throw the error after processing as far as possible.\n\nIssues:\n1> the reported format in the log file is kind of hard to read. I pipe-delimited the various <doc> tags, but they run together in a Windows DOS window. What happens on Unix I'm not quite sure. Suggestions welcome.\n2> From the original post, rolling this back will be tricky. Very tricky. The autocommit feature makes it indeterminate what's been committed to the index, so I don't know how to even approach rolling back everything.\n3> The intent here is to give the user a clue where to start when figuring out what document(s) failed so they don't have to guess.\n4> Tests fail, but I have no clue why. I checked out a new copy of trunk and that fails as well, so I don't think that this patch is the cause of the errors. But let's not commit this until we can be sure.\n5> What do you think about limiting the number of docs that fail before quitting? One could imagine some ratio (say 10%) that have to fail before quitting (with some safeguards, like don't bother calculating the ratio until 20 docs had been processed or...). Or an absolute number. Should this be a parameter? Or hard-coded? The assumption here is that if 10 (or 100 or..) docs fail, there's something pretty fundamentally wrong and it's a waste to keep on. I don't have any strong feeling here, I can argue it either way....\n6> Sorry, all, but I reflexively hit the reformat keystrokes so the raw patch may be hard to read. But I'm pretty well in the camp that you have to reformat as you go or the code will be held hostage to the last person who didn't format properly. I'm pretty sure I'm using the right codestyle.xml file, but let me know if not.\n7> I doubt that this has any bearing on, say, SolrJ indexing. Should that be another bug (or is there one already)? Anybody got a clue where I'd look for that since I'm in the area anyway?\n\nErick "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12982997",
            "date": "2011-01-18T03:55:46+0000",
            "content": "2> From the original post, rolling this back will be tricky. Very tricky. The autocommit feature makes it indeterminate what's been committed to the index, so I don't know how to even approach rolling back everything.\nDon't allow autocommits during an update. Simple. Or, rather, all update requests block at the beginning during an autocommit. If an update request has too many documents, don't do so many documents in an update.\n "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-12983175",
            "date": "2011-01-18T12:58:22+0000",
            "content": "I think it's ready for review, both trunk and 3_x. Would someone look this over and commit it if they think it's ready?\n\nNote to self: do NOT call initCore in a test case just because you need a different schema.\n\nThe problem I was having with running tests was because I needed a schema file with a required field so I naively called initCore with schema11.xml in spite of the fact that @BeforeClass called it with just schema.xml. Which apparently does bad things with the state of something and caused other tests to fail... I can get TestDistributedSearch to fail on unchanged source code simply by calling initCore with schema11.xml and doing nothing else in a new test case in BasicFunctionalityTest. So I put my new tests that required schema11 in a new file instead.\n\nThe XML file attached is not intended to be committed, it is just a convenience for anyone checking out this patch to run against a Solr instance to see what is returned.\n\nThis seems to return the data in the SolrJ case as well.\n\nNOTE: This does change the behavior of Solr. Without this patch, the first document that is incorrect stops processing. Now, it continues merrily on adding documents as it can. Is this desirable behavior? It would be easy to abort on first error if that's the consensus, and I could take some tedious record-keeping out. I think there's no big problem with continuing on, since the state of committed documents is indeterminate already when errors occur so worrying about this should be part of a bigger issue. "
        },
        {
            "author": "Simon Rosenthal",
            "id": "comment-12983215",
            "date": "2011-01-18T15:04:07+0000",
            "content": "Don't allow autocommits during an update. Simple. Or, rather, all update requests block at the beginning during an autocommit. If an update request has too many documents, don't do so many documents in an update. (Lance)\nLance - How do you (dynamically ) disable autocommits during a specific update  ? That functionality would also be useful in other use cases, but that's another issue). \n\nNOTE: This does change the behavior of Solr. Without this patch, the first document that is incorrect stops processing. Now, it continues merrily on adding documents as it can. Is this desirable behavior? It would be easy to abort on first error if that's the consensus, and I could take some tedious record-keeping out. I think there's no big problem with continuing on, since the state of committed documents is indeterminate already when errors occur so worrying about this should be part of a bigger issue.\n\nI think it should be an option, if possible. I can see use cases where abort-on-first-error is desirable, but also situations where you know one or two documents may be erroneous, and its worth continuing on in order to index the other 99% "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-12985189",
            "date": "2011-01-22T19:47:31+0000",
            "content": "From several discussions, it sounds like my test issues are unrelated to the latest patch (I did fix the test case to not do bad things). So I guess if anyone wants to pick this up, it's ready for review and/or commit.\n\nUnless we want to make some parameter like <abortOnFirstIndexingFailure> or some such. If the consensus is to add such an option, it seems like it needs to go in one of the config files because this code is also reachable by SolrJ. Probably default to \"true\" to preserve present functionality?\n\nThoughts?\nErick "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12985213",
            "date": "2011-01-22T22:41:31+0000",
            "content": "Unless we want to make some parameter like <abortOnFirstIndexingFailure> or some such. \nYes. Yes. Yes. Fail Early, Fail Often is a common system design style. Meaning, if I have a data quality problem kick me in the head about it. For example, my online store should keep yesterday's product lineup rather than load a Halloween costume with no categories:\n\n<field ... required=\"true\" .../>\n\n\nis there for a reason.\n "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-12985363",
            "date": "2011-01-23T16:57:45+0000",
            "content": "Is there any particular preference where the tag for making this configurable should go? My first impulse would be SolrConfig.xml, <indexDefaults> but I wanted to ask first to see if I'm reading the intent correctly. Leaving the field out would be equivalent to <abortOnFirstBatchIndexError>true</...>\n\nOh My,! that is a clumsy name, any better suggestions? \nprocessAllBatchIndexDocsPossible (default to false)\ncontinueAddingDocsInBatchIfErrors (default to false)\n\n\nI kind of like the last one. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12985373",
            "date": "2011-01-23T17:42:01+0000",
            "content": "This is solely for the XML loader, right Erick?  If so, the setting really belongs with the /update handler configuration seems to me.   [this is reminiscent of the schema.xml setting for default operator and/or, which is awkwardly placed, when it really belongs with the qparser declaration]\n\nOn a related note - are there comparable issues with other batch indexers like the CSV one?\n "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-12985398",
            "date": "2011-01-23T19:05:21+0000",
            "content": "bq: If so, the setting really belongs with the /update handler configuration seems to me\nThanks, this is what I was looking for, I'll try to make it so unless there are objections. This goes in <updateHandler class=\"solr.DirectUpdateHandler2\">, right?\n\nbq: On a related note - are there comparable issues with other batch indexers like the CSV one?\nI assume so, I haven't looked. I'd like to raise that as a separate JIRA if we want to address it. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-12985500",
            "date": "2011-01-24T03:35:43+0000",
            "content": "OK, I think this is ready to go if someone wants to take a look and commit.\n\nThis patch includes the ability to turn on continuing to process documents after the first failure, as per Erik H's comments. The default is the old behavior of stopping upon the first error.\n\nChanged example solrconfig.xml to include the new parameter as false (mimicing old behavior) in both 3x and trunk.\n "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12985513",
            "date": "2011-01-24T04:37:37+0000",
            "content": "The general verb is 'update' not 'index', so... abortUpdateOnError default to true is my vote.\n\nThe extraction handler by nature takes one document. The DIH has its own system for this that works pretty well.\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12986581",
            "date": "2011-01-25T18:31:51+0000",
            "content": "This patch looks pretty reasonable from the details of the implementation, but I don't think it's quite ready for commit yet.\n\nFirst, we should be able to extend this to all that implement ContentStreamLoader (JSONLoader, CSVLoader) if they want it (it doesn't make sense for the SolrCell stuff).  \n\nAs I see it, we can do this by putting some base functionality into ContentStreamLoader which does what is done in this patch.\nI think we need two methods, one that handles the immediate error (takes in a StringBuilder and the info about the doc that failed) and decides whether to abort or buffer the error for later reporting depending on the configuration setting.  \n\nI don't think the configuration of the item belongs in the UpdateHandler.  Erik H. meant that it goes in the configuration of the /update RequestHandler in the config, not the DirectUpdateHandler2, as in \n\n<requestHandler name=\"/update\" class=\"solr.XmlUpdateRequestHandler\" />\n\n\nThis config could be a request param just like any other (such that one could even say they want to override it via a request via the defaults, appends, invariants).\n\nAlso, I know it is tempting to do so, but please don't reformat the code in the patch.  It slows down review significantly.  In general, I try to reformat right before committing as do most committers. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12986584",
            "date": "2011-01-25T18:33:00+0000",
            "content": "Oh, one other thing.  You don't need to produce a 3.x patch.  We can just do an SVN merge. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12988383",
            "date": "2011-01-29T04:01:51+0000",
            "content": "The SolrJ  StreamingUpdateSolrServer needs updating for this. It needs a lot of other updating also. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-12993035",
            "date": "2011-02-10T12:55:14+0000",
            "content": "Grant:\n\nThanks for the comments. I'd assumed there was a better, more general fix, but what about the near term? Does it make any sense to re-work this enough to put the parameter in the update handler and commit? I think there's value in the patch as-is that would provide better-but-not-comprehensive help to users, at least in the XML case, rather than defer any improvement at all into the indefinite future.\n\nAs always, though, up to you.\n\nErick "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13020888",
            "date": "2011-04-18T00:30:36+0000",
            "content": "So, Grant. How do you feel about refactorings <G>?\n\nI got bitten by this problem again so I decided to dust off the patch, and I re-created it. This one shouldn't have the gratuitous re-formatting. But, after I added the bookkeeping, the method got even more unwieldy, so I extracted some of the code to methods in XMLLoader. I also have the un-refactored version if this one is too painful.\n\nThis patch incorporates the changes you suggested months ago. I'm a little uncertain whether putting a constant in UpdateParams.java was the correct place, but it seemed like a pattern used for other parameters.\n\nOne minor issue: The behavior is the same here as it used to be if you don't start the packet with <add>. An NPE is thrown. That's because the addCmd variable isn't initialized until the <add> tag is encountered and the NPE is a result of using the addCmd variable later (I think I was seeing it at line 118). I think it would be better to fail if the first element wasn't an <add> element rather than because it just happens to cause an NPE.\n\nWhile I'm at it, though, what do you think about making this robust enough to ignore ?xml and/or !DOCTYPE entries? Or is that just not worth the bother?\n\nErick "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13024780",
            "date": "2011-04-25T12:13:41+0000",
            "content": "Anyone willing to pick this up? It's one of those things that's not technically very interesting, but makes end-users' lives easier...\n\nErick "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-13024806",
            "date": "2011-04-25T14:11:56+0000",
            "content": "What about addressing the other handlers?  Any progress on that? "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13024813",
            "date": "2011-04-25T14:23:02+0000",
            "content": "Nope, the framework is in for anything that derives from the base class, but only the XMLLoader uses it yet. \n\nDoes it make sense to open separate JIRAs for those? Which ones do you think are most useful next?\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-13024814",
            "date": "2011-04-25T14:29:35+0000",
            "content": "We should fix this for all the update handlers. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13027245",
            "date": "2011-04-30T00:13:43+0000",
            "content": "If the DIH semantics cover all of the use cases, please follow that model: behavior, names, etc. It will be much easier on users.  "
        },
        {
            "author": "Shinichiro Abe",
            "id": "comment-13027545",
            "date": "2011-05-02T04:17:32+0000",
            "content": "In Solr Cell, There is the same problem.\nIt aborts mid during posting the protected files(SOLR-2480).\nI hope that update handlers should be fixed by applying that model. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-13042813",
            "date": "2011-06-02T15:08:08+0000",
            "content": "Erick, feel free to take this one and iterate as you see fit "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043807",
            "date": "2011-06-03T16:47:10+0000",
            "content": "Bulk move 3.2 -> 3.3 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13106436",
            "date": "2011-09-16T14:51:06+0000",
            "content": "3.4 -> 3.5 "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13233357",
            "date": "2012-03-20T11:30:03+0000",
            "content": "Well, it's clear I won't get to this in the 3.6 time frame, so if someone else wants to pick it up feel free. However, I also wonder whether with 4.0 and SolrCloud we have to approach this differently to accomodate how documents are passed around there? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13291055",
            "date": "2012-06-07T15:02:43+0000",
            "content": "I imagine a maxErrors parameter might be useful (and more readable than abortOnFirstBatchIndexError)\n\nmaxErrors=0 (the current behavior - stop processing more updates when we hit an error)\nmaxErrors=10 (allow up to 10 documents to fail before aborting the update... useful for true bulk uploading where you want to allow for an isolated failure or two, but still want to stop if every single update is failing because something is configured wrong)\nmaxErrors=-1 (allow an unlimited number of documents to fail)\n\nMaking updates transactional seems really tough in cloud mode since we don't keep old versions of documents around... although it might be possible for a short time with the transaction log.  Anyway, that should definitely be a separate issue.\n\nA couple of other notes:\n\n\tstructured error responses were recently added in 4.0-dev that should make this issue easier in general.  Example:\n\n{\"responseHeader\":{\"status\":400,\"QTime\":0},\"error\":{\"msg\":\"ERROR: [doc=mydoc] unknown field 'foo'\",\"code\":400}}\n\n\n\tPer did some error handling work that's included in a patch attached to SOLR-3178\n\n "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13717183",
            "date": "2013-07-23T18:47:23+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 "
        },
        {
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-13955935",
            "date": "2014-04-01T00:26:07+0000",
            "content": "This is a different approach for this issue. The errors are managed by an UpdateRequestProcessor that must be added before other processors in the chain. It accepts maxErrors in the configuration as default or as a request parameter. If used, the default maxErrors value is Integer.MAX_VALUE, to get the current behavior one should set it to 0 (however, wouldn\u2019t make sense to add the processor to the chain in this case, unless it depends on the request parameter).\nThis would handle only bad documents, but not others mentioned in previous comments (like Tika parsing exceptions, etc).\nThe response will look something like: \n\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response>\n<lst name=\"responseHeader\">\n  <int name=\"numErrors\">10</int>\n  <lst name=\"errors\">\n    <lst name=\"1\">\n      <str name=\"message\">ERROR: [doc=1] Error adding field 'weight'='b' msg=For input string: \"b\"</str>\n    </lst>\n    <lst name=\"3\">\n      <str name=\"message\">ERROR: [doc=3] Error adding field 'weight'='b' msg=For input string: \"b\"</str>\n    </lst>\n...\n  <int name=\"status\">0</int>\n  <int name=\"QTime\">17</int>\n</lst>\n</response>\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13956720",
            "date": "2014-04-01T16:28:32+0000",
            "content": ". The errors are managed by an UpdateRequestProcessor that must be added before other processors in the chain.\n\nOff the cuff: this sounds like a great idea.\n\nThe on piece of feedback that occurred to me though would be to tweak the response format so that there is a 1-to-1 correspondence of documents in the initial request to statuses in the response \u2013 even if the schema doesn't use uniqueKey...\n\n\n<lst name=\"responseHeader\">\n  <int name=\"numErrors\">10</int>\n  <lst name=\"results\">\n    <!-- if schema has uniqueKeys, they are the names of the response -->\n    <lst name=\"42\" /> <!-- success so empty -->\n    <lst name=\"1\"> <!-- 2nd doc in update, with uniqueKey of 1 had this failure -->\n      <str name=\"message\">ERROR: [doc=1] Error adding field 'weight'='b' msg=For input string: \"b\"</str>\n    </lst>\n    <lst name=\"60\" /> <!-- success so empty -->\n    <lst name=\"3\"> <!-- 4th doc in update, with uniqueKey of 3 had this failure -->\n      <str name=\"message\">ERROR: [doc=3] Error adding field 'weight'='b' msg=For input string: \"b\"</str>\n    </lst>\n...\n  <int name=\"status\">0</int>\n  <int name=\"QTime\">17</int>\n</lst>\n\n\n\n\n? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13956748",
            "date": "2014-04-01T16:54:23+0000",
            "content": "even if the schema doesn't use uniqueKey...\n\nThat would lead to some huge responses.  I think instead the notion of not having a uniqueKey should essentially be deprecated. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13956767",
            "date": "2014-04-01T17:06:56+0000",
            "content": "That would lead to some huge responses. I think instead the notion of not having a uniqueKey should essentially be deprecated.\n\n+1 "
        },
        {
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-13956790",
            "date": "2014-04-01T17:24:26+0000",
            "content": "I think instead the notion of not having a uniqueKey should essentially be deprecated.\n+1\n\nThat would lead to some huge responses.\nDo you mean including the ids of good docs in the response too? I don't think that would be that big. Should be much smaller than the request "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13956798",
            "date": "2014-04-01T17:31:08+0000",
            "content": "Do you mean including the ids of good docs in the response too? I don't think that would be that big. Should be much smaller than the request\n\nSome people (including myself) send/load millions of docs per request - it's very unfriendly to get back megabytes of responses unless you explicitly ask.\nIf this processor is not in the default chain, then I guess it doesn't matter much.  But I could see adding this ability by default (regardless of if it's a separate processor or not) via a parameter like maxErrors or something. "
        },
        {
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-13956848",
            "date": "2014-04-01T18:23:10+0000",
            "content": "I see. Maybe I could add then just the \"numSucceed\" just as a confirmation that the rest of the docs made it in? "
        },
        {
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-13958056",
            "date": "2014-04-02T19:13:49+0000",
            "content": "added \"numAdds\" to header "
        },
        {
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-13967726",
            "date": "2014-04-13T04:13:58+0000",
            "content": "Any more thoughts on this patch? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13968996",
            "date": "2014-04-14T23:22:56+0000",
            "content": "Tomas: \n\n\n\twe need better class level javadocs for the TolerantUpdateProcessorFactory - basically everything currently in the TolerantUpdateProcessor's javadocs, plus some example configuration, plus a note about how \"maxErrors\" can be specified as a request param or as an init param and an explanation of the default behavior if \"maxErrors\" specified at all\n\twould you mind renaming \"tolerant-chain1\" and \"tolerant-chain2\" with more descriptive names to make the tests easier to read? perhaps \"tolerate-10-failures-chain\" and \"tolerate-unlimited-failures-chain\" ?\n\teven if \"maxErrors\" isn't reached, we should consider carefully whether or not it makes sense to be returning a \"200\" status code even if every update command that's executed for a request fails. (ie: if maxErrors defaults to Integer.MAX_VALUE, and i send 100 docs and all 100 fail, should i really get a 200 status code back?)\n\n "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13971298",
            "date": "2014-04-16T12:57:42+0000",
            "content": "Move issue to Solr 4.9. "
        },
        {
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-13974536",
            "date": "2014-04-18T21:55:34+0000",
            "content": "I uploaded a new patch with more javadocs and the test chains name changed. \n\neven if \"maxErrors\" isn't reached, we should consider carefully whether or not it makes sense to be returning a \"200\" status code even if every update command that's executed for a request fails. (ie: if maxErrors defaults to Integer.MAX_VALUE, and i send 100 docs and all 100 fail, should i really get a 200 status code back?)\nI think this would make it more confusing. Having this processor means that the client wants to manage failing docs on their side. If all the docs fail so be it, they\u2019ll know how to manage it on their side, I don\u2019t think that should be a special case. Plus, I think getting the 200 gives you more information, it tells you that Solr tried adding all the docs the client sent and it didn\u2019t abort somewhere in the middle, like it would happen if you get a 4XX/5XX\n\nI was also thinking that this processor won\u2019t work together with DistributedUpdateProcessor, it has its own error processing, plus the distribution would create multiple internal requests (chains too) right? Also, the ConcurrentUpdateSolrServer used in SolrCmdDistributor would batch docs in a non-deterministic way, right? Would be impossible to count errors at this level.  "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13974667",
            "date": "2014-04-19T00:45:42+0000",
            "content": "I think this would make it more confusing. Having this processor means that the client wants to manage failing docs on their side. If all the docs fail so be it.\n\nYeah, i'm not convinced you're wrong \u2013 I just wasn't sure how i felt about it and I wanted to make we considered.  Even if users configure this, they might be surprised if something like a a schema.xml mismatch with some update process they are using causes a 500 error on every individual udpate \u2013 but still results in a 200 coming back because of this component.\n\nBut I think you are right  \u2013 as long as the docs are clear that the status will allways be a 200, even if all docs fail, we're fine.\n\nI was also thinking that this processor won\u2019t work together with DistributedUpdateProcessor, it has its own error processing, plus the distribution would create multiple internal requests...\n\nAs long as this processor is configured before the DistributedUpdateProcessorFactory it should work fine:\n\n\twhen the requests get forwarded to other shards, they'll bypass this processor (and any other processors that come before DistributedUpdateProcessorFactory) so it won't break the cumulative error handling in DistributedUpdateProcessorFactory\n\tDistributedUpdateProcessorFactory still ultimately throws only one Exception per UpdateCommand when it forwards to multiple replicas, so your new processor will still get at most 1 error to track per doc when accumulating results to return to the client\n\n\n\nbut it's trivial to write a distributed version of your test case to prove that you get the results you expect \u2013 probably a good idea to write one to help future proof this processor against unforeseen future changes in the distributed update processing "
        },
        {
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-13981778",
            "date": "2014-04-25T23:54:49+0000",
            "content": "My simple test to use with SolrCloud fails (not 100% of the times, but very frequently). This is my understanding of the problem:\nIt works only in the case of the update arriving to the shard leader (as it would fail while adding the doc locally), but if the update needs to be forwarded to the leader, then it will not work. \nIf the request is forwarded to the leader it is done asynchronically and the DistributedUpdateProcessor tracks the errors internally. Finally, after all the docs where processed  the \u201cfinish\u201d method is called and the DistributedUpdateProcessor will add one of the exceptions to the response. This is a problem because \u201cprocessAdd\u201d never really fails as the TolerantUpdateProcessor is expecting. It also can\u2019t know the total number of errors, this is counted internally in the DistributedUpdateProcessor. \n\nAs a side note, this DistributedUpdateProcessor behavior makes it \u201ctolerant\u201d, but only in some cases? A request like this:\n\n<add>invalid-doc</add>\n<add>valid-doc</add>\n<add>valid-doc</add>\n\n\nwould leave Solr in a different state depending on who is receiving the request (the shard leader or a replica/follower). Is this expected? "
        },
        {
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-13982423",
            "date": "2014-04-27T18:48:47+0000",
            "content": "As a side note, this DistributedUpdateProcessor behavior makes it \u201ctolerant\u201d, but only in some cases? \nI have confirmed this. Depending on which node gets the initial update request and the position of the invalid doc in the batch, the docs that end up indexed will vary from 0 to all but the invalid doc.  "
        },
        {
            "author": "Denis Shishlyannikoc",
            "id": "comment-14110647",
            "date": "2014-08-26T12:26:16+0000",
            "content": "Question related to this JIRA.\nAfter failure to index one of documents with wrong date value (2014-03-18K18:15:13Z) solr kept this document in some queue and tried to reindex this document again (attempt per some 3-5 minutes, did not measure exact time of that), showing same (failed to parse date) exception in logs! After solr server restart issue is gone: no more tries to reindex problematic date document. \nLooks like not very correct actions. How can it be explained? How can I avoid such reindexing ? I don't care to lose some not correct documents, but I don't want solr to stuck on them after failure. "
        },
        {
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-14113986",
            "date": "2014-08-28T17:13:31+0000",
            "content": "Denis Shishlyannikoc Sorry I missed your comment. I understand this issue you are seeing is not with any of the patches in this Jira, right? If so, you should ask the question in the users list, you'll get much more eyes in your problem that way than posting here.  "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14269854",
            "date": "2015-01-08T18:52:22+0000",
            "content": "It works only in the case of the update arriving to the shard leader (as it would fail while adding the doc locally), but if the update needs to be forwarded to the leader, then it will not work. \n\n...i'm not sure if this will solve all of the problems Tomas ran into, but one thing that might help (and was added after the latest version of hte patch was written) is the \"UpdateRequestProcessorFactory.RunAlways\" marker interface.  it gives UpdateProcessorFactories a mechanism to say they want to be run as part of hte chain even if the \"update.distrib\" logic would normally skip them for already being run on a previous node (ie: the update has already been forwarded once)\n\nso that interface, combined with some basic checks of \"am i the leader?\" could allow this processor to ensure it was always/only executing some bits of logic on the leader.\n\n(there might still be some problems however in terms of accurately responding/reporting aggregate failures when batch updates involve docs that go to differnet leaders) "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-14632944",
            "date": "2015-07-19T22:30:30+0000",
            "content": "Working on more tests in the next patch and support/test for deletes. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-14633103",
            "date": "2015-07-20T06:52:18+0000",
            "content": "I'm seeing a few errors with the current patch and I think I know what's going on. I'll take a look at it and update the patch tomorrow. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14634464",
            "date": "2015-07-21T03:14:41+0000",
            "content": "I guess it would be better if we return the whole command instead of just the id to the user "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-14636504",
            "date": "2015-07-22T08:32:09+0000",
            "content": "Updated patch "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-14642407",
            "date": "2015-07-27T07:47:30+0000",
            "content": "Updated patch. I'm still working on getting the correct error count for the following case:\nClient -> Node1 -> Node2 (Leader for shard 1) -> Node3 (Non-leader Replica for shard 1)\n\nIn the above case, Node2 as of now return an HTTP OK and doesn't throw an exception, the StreamingSolrClient used but the Distributed Updated Processor doesn't realize the error that was consumed by the leader of shard 1.\nI'm making progress on that but have run into deadend with a few of the approaches I've tried so far.\n\nThe TolerantUpdateProcessor as of now kicks in when\n1. It's the leader node or,\n2. It's the node that receives the request from the client i.e. yet to forward the request to the leader shard. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15061218",
            "date": "2015-12-17T00:31:32+0000",
            "content": "\nI started playing arround with this patch a bit to see if I could help move it forward.  I'm a little out of my depth with a lot of the details of how distribute updates work, but the more I tried to make sense of it, the more convinced I was that there was a lot of things that just weren't very well accounted for in the existing tests (which were consistently failing, but the failures themselves weren't consistent between runs).\n\nHere's a summary of what's new/different in the patch i'm attaching...\n\n\n\n\tDistributedUpdateProcessor.DistribPhase\n\t\n\t\tnot sure why this enum was made non-static in earlier patches ... i reverted this unneeded change.\n\t\n\t\n\tTolerantUpdateProcessor\n\t\n\t\tprocessDelete\n\t\t\n\t\t\tMethod has a couple of glaringly obvious bugs, that aparently don't trip under the current tests\n\t\t\tadded several nocommits of things that jumpted out at me\n\t\t\n\t\t\n\t\n\t\n\tDistribTolerantUpdateProcessorTest\n\t\n\t\tbeefed up assertion msgs in assertUSucceedsWithErrors\n\t\tfixed testValidAdds so it's not dead code\n\t\ttestInvalidAdds\n\t\t\n\t\t\tsanity check code wasn't passing reliably\n\t\t\t\n\t\t\t\tdetails of what failed are lost depending on how update is routed (random seed)\n\t\t\t\trelaxed this check to be reliable with a nocommit comment to see if we can tighten it up\n\t\t\t\n\t\t\t\n\t\t\tassuming sanity check passes assertUSucceedsWithErrors (still) fails on some seeds w/null error list\n\t\t\t\n\t\t\t\tI'm Guessing this is what anshum alluded to in last comment: \"Node2 as of now return an HTTP OK and doesn't throw an exception, the StreamingSolrClient used but the Distributed Updated Processor doesn't realize the error that was consumed by the leader of shard 1\"\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\n\t\n\tTestTolerantUpdateProcessorCloud\n\t\n\t\tNew MiniSolrCloudCluster based test to try and demonstrate all the possible distrib code paths i could think of (see below)\n\t\n\t\n\n\n\nTestTolerantUpdateProcessorCloud is the real meat of what i've added here.  Starting with the basic behavior/assertions currently tested in TolerantUpdateProcessorTest, I built it up to try and exorcise every possible distribute update code path i could imagine (updates with docs all on one shard some of which fail, updates with docs for diff shards and some from each shard fail, updates with docs for diff shards but only one shard fails, etc...) \u2013 but only tested against a MinSolrCloud collection that actaully had 1 node, 1 shard, 1 replica and an HttpSolrClient talking directly to that node.  Once all those assertions were passing, then I changed it to use 5 nodes, 2 shards, 2 replicas and started testing all of those scenerios against 5 HttpSolrClients pointed at every individual node (one of which hosts no replicas) as well as a ZK aware CloudSolrClient.  All 6 tests against all 6 clients currently fail (reliably) at some point in these scenerios.\n\n\n\nIndependent of all the things i still need to make sense of in the existing code to try and help get these tests passing, I still have one big question about what the desired/epected behavior should be for clients when maxErrors is exceeded \u2013 at the moment, in single node setups, the client gets a 400 error with the top level \"error\" section corisponding with whatever error caused it to exceed the maxErrors, but the responseHeader is still populated with the individual errors and the appropraite numAdds & numErrors, for example...\n\n\n$ curl -v -X POST 'http://localhost:8983/solr/techproducts/update?indent=true&commit=true&update.chain=tolerant' -H 'Content-Type: application/json' --data-binary '[{\"id\":\"hoss1\",\"foo_i\":42},{\"id\":\"bogus1\",\"foo_i\":\"bogus\"},{\"id\":\"hoss2\",\"foo_i\":66},{\"id\":\"bogus2\",\"foo_i\":\"bogus\"},{\"id\":\"bogus3\",\"foo_i\":\"bogus\"},{\"id\":\"hoss3\",\"foo_i\":42}]'\n* Hostname was NOT found in DNS cache\n*   Trying 127.0.0.1...\n* Connected to localhost (127.0.0.1) port 8983 (#0)\n> POST /solr/techproducts/update?indent=true&commit=true&update.chain=tolerant HTTP/1.1\n> User-Agent: curl/7.38.0\n> Host: localhost:8983\n> Accept: */*\n> Content-Type: application/json\n> Content-Length: 175\n> \n* upload completely sent off: 175 out of 175 bytes\n< HTTP/1.1 400 Bad Request\n< Content-Type: text/plain;charset=utf-8\n< Transfer-Encoding: chunked\n< \n{\n  \"responseHeader\":{\n    \"numErrors\":3,\n    \"errors\":{\n      \"bogus1\":{\n        \"message\":\"ERROR: [doc=bogus1] Error adding field 'foo_i'='bogus' msg=For input string: \\\"bogus\\\"\"},\n      \"bogus2\":{\n        \"message\":\"ERROR: [doc=bogus2] Error adding field 'foo_i'='bogus' msg=For input string: \\\"bogus\\\"\"},\n      \"bogus3\":{\n        \"message\":\"ERROR: [doc=bogus3] Error adding field 'foo_i'='bogus' msg=For input string: \\\"bogus\\\"\"}},\n    \"numAdds\":2,\n    \"status\":400,\n    \"QTime\":4},\n  \"error\":{\n    \"msg\":\"ERROR: [doc=bogus3] Error adding field 'foo_i'='bogus' msg=For input string: \\\"bogus\\\"\",\n    \"code\":400}}\n* Connection #0 to host localhost left intact\n\n\n\n...but because this is a 400 error, that means that if you use HttpSolrClient, you're not going to get access to any of that detailed error information at all \u2013 you'll just get a RemoteSolrException with the bare details.\n\n\n\tShould the use of this processor force all \"error\" responses to be rewritten as HTTP 200s?\n\tShould the solrj clients be updated so that RemoteSolrException still provides an accessor to get the parsed/structured SolrResponse (assuming the HTTP response body can be parsed w/o any other errors?)\n\n\n\n? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15135274",
            "date": "2016-02-05T23:41:42+0000",
            "content": "Ok, i've been going around in circles on this issue for the past few weeks, but i've finally got something that feels like progress.\n\nListing everything new since the last patch would be fairely tedioius, so i'll focus on the broad strokes...\n\n\n\tpunting on deletes (for now)\n\t\n\t\tthe previuos processDelete method was totally broken in distributed queries (was doing equality comparisons of Strings directly to enum objects, made assumptions about isLeader pased on previous \"add\" commands in the same request, etc...)\n\t\ti was having enough time struggling with getting adds to work properly, that i just removed the delete code completely\n\t\tbased on the rest of the progress (see below) re-adding support for deletes should be straight forward, but will require some refactoring of how the errors are tracked & counted to distinguish between an \"add doc w/id=22\" that fails and a \"delete doc w/id=22\" that fails in the same request\n\t\n\t\n\tDealing with forward(ing/ed) requests and async distributed failures\n\t\n\t\tMost of the meat of the test failures in the last patch came from dealing with the async requests fired off my\nDistributedUpdateProcessor and how to deal with failures reported by other leaders.\n\t\tI started down the road of trying to do a bettter job in SolrCmdDistributor of tracking the UpdateCommand that corisponds with each Req so that when processing the Error we could know what failed remotely \u2013 this code is still in the patch (because i think it's cleaner then the cmdString tracking currently in Solr) but proved mostly useless because of how ConcurrentUpdateSolrClient can combine multiple \"requests\" together.\n\t\tThe next step, which mostly worked, was to improve the error handling in DUP's finish() so that instead of aborting with info about whatever (remote) Error happened to return first, it now returns a new DistributedUpdatesAsyncException which wraps & remembers the summary info of all the remote errors encountered \u2013 or at least, all of the errors that were previously candidates to tell the user about.  Stuff that was swallowed & logged before is still swallowed & logged.\n\t\t\n\t\t\tOne notable change her is that i switched DUP.finish() from directly calling SOlrQueryResponse.setException() and instead made it throw the exception.  Independent of this issue, the existing behavior seems like a bug / bad-form \u2013 what if the caller already caught some earlier exception it wants to return and finish() is just being called in finally?\n\t\t\tThis lead me to discover SOLR-8633 \u2013 the patch from that issue is currently including in this patch because it's so neccessary for hte current code.\n\t\t\tFWIW: if, for some reason, folks think calling SOlrQueryResponse.setException() is better for some wacko reason, then TolerantUpdateProcess.finish() could, in theory, go check SOlrQueryResponse.getException() and treat it the same way as exceptions it catches (see below) but that's a lot more tedious and (and error prone in the long run)\n\t\t\n\t\t\n\t\tOnce DUP.finish() started throwing DistributedUpdatesAsyncException, tracking all the various errors we care about from distributed requests became possible...\n\t\t\n\t\t\tTolerantUpdateProcessor now pays attention to when the request is a TOLEADER forwarded request, and if so:\n\t\t\t\n\t\t\t\tit acts tolerant of up to maxErrors failures (just like single node)\n\t\t\t\tin finish(), if any failures happened, return the first error \u2013 and annotate it with info about all the failures in this request, using the existing SolrException.getMetadata() map.\n\t\t\t\n\t\t\t\n\t\t\tConcurrentUpdateSolrClient already ensures that if a SolrException happens on the remote server, any \"metadata\" included in the response for that exception is copied into the local SolrException it constructs\n\t\t\tSo if/when TolerantUpdateProcessor.finish() catches a DistributedUpdatesAsyncException, it loops over the wrapped exceptions, and pulls out the metadata for each of those to update it's list of known failures\n\t\t\n\t\t\n\t\n\t\n\twhich error is returned if maxErrors exceeded: now the \"first\" one\n\t\n\t\ti mentioned this above in discussing how async errors from other leaders are handled, but i wanted to emphasis it and elaborate a bit\n\t\tin the original patch, exceptions were completley ignored until maxErrors were seen, at which point hte next exception was immediately (re)-thrown\n\t\tthat made sense for single node cases, but in a distributed case, with async exceptions, we can't always just \"rethrow\" an exception\n\t\t\n\t\t\tit's also ambiguious what the \"last\" exception really is in an async situation.\n\t\t\n\t\t\n\t\tso now, instead,  TolerantUpdateProcessor always remembers the first exception it caught, and if/when maxErrors is exceeded, it re-throws that first exception\n\t\t\n\t\t\tlater, in finish() that (remembered) exception is annotated with metadata about all the failures\n\t\t\t\n\t\t\t\tthis metadata is critical for forwarded requests, but should also be useful for SolrClient users who (should) see it copied into a RemoteSolrException even if they don't get the normal UpdateResponse object and it's getResponseHeader()\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\tthis, in my opinion, makes the behavior of using TolerantUpdateProcessor more useful (and consistent with how stock solr works) when there is a fundamental problem with your data \u2013 you get an error indicating the first problem document in your data, as opposed to seeing an error from the 11th, or 101th, or maxErrors+1 malformed document in your data.\n\t\tif folks disagree, we just need to re-work the FirstErrTracker class to be a LastErrTracker class...\n\t\t\n\t\t\tFirstErrTracker.throwFirst() becomes LastErrTracker.throwLast()\n\t\t\tinstead of checking null == first, LastErrorTracker.caught(Throwable) would ignore any additional exceptions once true == thrown\n\t\t\n\t\t\n\t\n\t\n\tthe problem with returning numAdds (and numDeletes if we want that)\n\t\n\t\tgetting numAdds to work correctly in a distributed request is really hard\n\t\tcurrently the code (specifically on the first node processing the AddUpdateCommand just requires that super.processAdd() succeeds to do numAdds++\n\t\t\n\t\t\tthis gives a missleading number when the failures aren't reported immediately because they were forwarded async to a diff leader and we only find out about problems in finish()\n\t\t\n\t\t\n\t\teven if we only did numAdds++ for docs where we know we are the leader, getting the count from other leaders later is tricky\n\t\t\n\t\t\tright now, the only way TolerantUpdateProcessor learns the results of any async requests to other leaders is if DUP.finish() throws an error \u2013 we can get numAdds from the metadata of those errors, but that doesn't help the case when requests succeed\n\t\t\tDUP doesn't currently do anything with successful responses, so there's no easy way to get the numAdds in that case\n\t\t\n\t\t\n\t\tpossible solutions...\n\t\t\n\t\t\teliminate the concept of numAdds from this feature, focus solely on being tolerant and reporting the errors (which we can do accurately)\n\t\t\t\n\t\t\t\tif people want to know how many succeeded, they can use features like versions=true explicitly \u2013 although even then, i'm not sure if it will work reliably today ... i don't see any indication that DUP merges the remote responses when that feature is used.\n\t\t\t\n\t\t\t\n\t\t\tmake TolerantUpdateProcessor detect when it's not a leader for something, and in that case implicitly add version=true to get the info from requests we forward to, and prune the response down to just a simple numAdds count in finish()\n\t\t\t\n\t\t\t\tsame problem as above if i'm correct about versions=true not currently handled correctly in forward(ing/ed) requests\n\t\t\t\n\t\t\t\n\t\t\treturn distinct numAddsAttempted and numAddsConfirmed values \u2013 probably not as useful to end clients, but more accurate representation of what we know...\n\t\t\t\n\t\t\t\ttrack a distinct \"numAddsAttempted\" for every shard we forward to (added together at end of request)\n\t\t\t\t\n\t\t\t\t\tkind of a sketchy pain in the ass to do\n\t\t\t\t\n\t\t\t\t\n\t\t\t\tassume numAddsConfirmed = numAddsAttempted for any other shard leader we dont see an exception from\n\t\t\t\tfor shard leaders we do get exceptions from, add to our numAddsConfirmed based on the metadata (ie: numAddsConfirmed + remoteException.getMetatata(NUM_ADDS_CONFIRMED)\n\t\t\t\n\t\t\t\n\t\t\ttreat it as a distinct feature in a new jira\n\t\t\t\n\t\t\t\tgeting numAdds (or numDeletes) count in the responseHeader really feels like it should be an orthoginal feature to being tolerant of maxErrors\n\t\t\t\twe should open a distinct issue to track adding that as a feature, and ensuring that DUP aggregates correctly from all the various leaders that requests get forwarded to\n\t\t\t\tthis is waht i personally think we should do \u2013 notably because it would make it trivial to know how many documents you added when streaming a bunch of data, even if you don't use this update processor.\n\t\t\t\n\t\t\t\n\t\t\n\t\t\n\t\n\t\n\tthe nuance of the maxErrors=N param\n\t\n\t\ti just want to point out \u2013 in a distributed cloud setup, there is no way to truely enforce a hard limit at maxErrors\n\t\tthe async processing means that if a request includes docs destined for diff shards, we can't ensure that only that max amount of errors will be hit \u2013 we might hit more errors in async threads before we notice and stop processing\n\t\ti don't think this is a problem, it's a feature of handling updates to diff shards/leaders in paralel, but it does mean we might want to rethink the param name ? (errorTolleranceThreshold=N perhaps?\n\t\n\t\n\n\n\n\nThere's still a lot of work todo, in no particular order...\n\n\n\tdeletes\n\t\n\t\tneed to refactor the way we track errors (both locally and stashed in the SolrException metadata) so that we can we can distinguish \"add doc w/id=22\" faiulres from \"delete doc w/id=22\" failures\n\t\tprobably need to rethink the responseHeader formatting for how errors are tracked as well in order to distinguish them\n\t\tonce we have that, adding a processDeletes(...) method that works similar to processAdd should be trivial\n\t\tneed to beef up the cloud testing to include delete checks\n\t\n\t\n\tCloudSolrClient\n\t\n\t\tall of the tests using CloudSolrClient currently fail because of how that client does it's own \"direct to leaders\" splitting/merging of docs destined for diff shards.\n\t\ta bunch of new code needs written there (may be able to refactor/share some stuff from the error list merging code in TolerantUpdateProcessor (see above about refactoring that for deletes)\n\t\n\t\n\tneed lots more randomized testing\n\ttons of nocommits that need cleaned up\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15154210",
            "date": "2016-02-19T13:30:55+0000",
            "content": "Wow, I've never seen this old beast issue.\n\nGoodness, with all this work if we could just fix the error handling too for 6.0...\n\nIf we had proper error responses, oh the things we could do. I don't know how you handle when 100,000 docs in your stream fail though. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15154220",
            "date": "2016-02-19T13:43:26+0000",
            "content": "One notable change her is that i switched DUP.finish() from directly calling SOlrQueryResponse.setException() and instead made it throw the exception. Independent of this issue, the existing behavior seems like a bug / bad-form \u2013 what if the caller already caught some earlier exception it wants to return and finish() is just being called in finally?\n\nCan we document that on the setException method?\n\nmaxErrors\n\nYeah, works for the end user. Internally, if we had a good way to track all the fails in some efficient manner (we learn about them as they happen or something), we could perhaps use a single ConcurrentUpdateSolrClient per replica and be much more connection efficient. Kind of beyond this issue, but my interest in this issue is that it seems to be the start of that path. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15154226",
            "date": "2016-02-19T13:49:33+0000",
            "content": "I have no issue with punting numAdds on this.\n\nIf we could get this out in a major version, I'd also love to make this standard behavior and not some optional update processor. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15154253",
            "date": "2016-02-19T14:09:29+0000",
            "content": "\n // nocommit: should this really be a top level exception?\n // nocommit: or should it be an HTTP:200 with the details of what faild in the body?\n\nFor the Collections API I went with HTTP:200. The overall request to the server succeeded, here are the individual update fails. I guess I lean that way a bit. If 1 update out 1000 fails, it seems kind of strange to fail the whole request with this new code.\n\n**\nOh wait a minute, are you only doing that when maxErrors is exceeded? In that case failing the request makes sense to me I guess. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15154266",
            "date": "2016-02-19T14:23:50+0000",
            "content": "\n      if (\"LeaderChanged\".equals(cause)) {\n        // let's just fail this request and let the client retry? or just call processAdd again?\n        log.error(\"On \"+cloudDesc.getCoreNodeName()+\", replica \"+replicaUrl+\n            \" now thinks it is the leader! Failing the request to let the client retry! \"+error.e);\n        errorsForClient.add(error);\n        break; // nocommit: why not continue?\n      }\n\n\n\nBeen thinking about this a little. Perhaps the idea is, we know the error is not from a forward request, we skip those here. Which means they must all be leader to replica and if the leader changed, we can't put anyone in LIR anyway.\nAnyhow, it does seems safer to me to not break and process all the errors. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15154493",
            "date": "2016-02-19T16:56:27+0000",
            "content": "Can we document that on the setException method?\n\nI'm not sure what exactly it should say, but that seems orthogonal to the current issue \u2013 feel free to add whatever you think makes sense as a distinct git commit, my point was just that the current behavior is very inconsistent with the way exceptions are normally processed in solr, and doesn't give \"up stream\" callers the chance to catch/handle the exception.\n\nIf we could get this out in a major version, I'd also love to make this standard behavior and not some optional update processor.\n\nAgreed - but for now, to minimize the invasiveness, I'd prefer to continue on the path of using a custom update processor & then later we can assess refactoring the code to make this a more integrated feature and change that update processor to a No-Op.\n\n(particularly since there is going to be some overhead in counting/tracking the errors)\n\nOh wait a minute, are you only doing that when maxErrors is exceeded? In that case failing the request makes sense to me I guess.\n\nyeah, that's the context of the question ... i'm leaning towards agreeing with you, particularly since (as things stand now) the caller can access the SolrJ exception metadata to see exactly what failed (but i really wish there was an easier way to access the full response body in those cases)\n\nAnyhow, it does seems safer to me to not break and process all the errors.\n\nYeah, that was my thinking.\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-15154525",
            "date": "2016-02-19T17:26:56+0000",
            "content": "I'd also love to make this standard behavior and not some optional update processor.\n\n+1 to that... big value in having it by default. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15154609",
            "date": "2016-02-19T18:34:08+0000",
            "content": "I'm not sure what exactly it should say, but that seems orthogonal to the current issue\n\nSeems like part of this issue to me. As you point out and fix in this issue, we really should not be doing setException explicitly, that should really be done in one place. So you fix it here, but best way to prevent this things from creeping back in is doc. Hardly worth another issue to add a comment to the effect of what you already wrote above though.\n\nSo something along the lines of: You should not generally add new calls to this method, you should throw exceptions and let the existing infra handle it. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15154637",
            "date": "2016-02-19T18:43:45+0000",
            "content": "Commit 3a9da7ae576f35e742ec54a72da2d4224066bb63 in lucene-solr's branch refs/heads/jira/SOLR-445 from markrmiller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=3a9da7a ]\n\nSOLR-445: hossman's feb 5 2016 patch "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15154638",
            "date": "2016-02-19T18:43:46+0000",
            "content": "Commit fd12a5b9f8d6319945d4445ac31e650bd1627dfc in lucene-solr's branch refs/heads/jira/SOLR-445 from markrmiller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=fd12a5b ]\n\nSOLR-445: some cleanup "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15154641",
            "date": "2016-02-19T18:45:00+0000",
            "content": "Pushed a branch of this patch with some minor cleanup I needed to remove all Eclipse errors and the comment I mentioned above. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15154647",
            "date": "2016-02-19T18:47:27+0000",
            "content": "and the comment I mentioned above.\n\nI still don't understand why that's part of this issue and not SOLR-8633 where the actual bad behavior needs fixed (and should be done indepenetnly, and prior to, trying to move forward here) "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15154775",
            "date": "2016-02-19T20:02:22+0000",
            "content": "Huh? What does SOLR-8633 have to do with calling setException?\n\nI'd say it fits right here. Here is where it's talked about, here is where it's changed in a patch... "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15154793",
            "date": "2016-02-19T20:08:31+0000",
            "content": "Let's not get too pedantic about adding comments to help future devs avoid bad decisions when we find bad decisions. Easier to just add the comment and make the code base a little easier to understand (which I've taken a stab at in the above branch). "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15155031",
            "date": "2016-02-19T22:40:42+0000",
            "content": "Huh? What does SOLR-8633 have to do with calling setException?\n\nSorry, nothing ... it's been a while since i looked at the specifics of that code and i spaced out on what we were actually talking about. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15166548",
            "date": "2016-02-25T01:57:26+0000",
            "content": "Commit a58ad2a6b11077a24040810b9c6b6d84f15b055d in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a58ad2a ]\n\nMerge branch 'master' into jira/SOLR-445 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15166549",
            "date": "2016-02-25T01:57:28+0000",
            "content": "Commit bc5dfeeff1a182630fc3b55be3cf2f4fe164d446 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=bc5dfee ]\n\nSOLR-445: play nice with SOLR-8674 test changes "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15166551",
            "date": "2016-02-25T01:57:31+0000",
            "content": "Commit 2e5c5b022ed2f185b95c745ccdbd0a142e3b5794 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=2e5c5b0 ]\n\nSOLR-445: prune out bogus (ie: technically infeasible to be accurate) 'numAdds' code "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15166552",
            "date": "2016-02-25T01:57:32+0000",
            "content": "Commit d5fd11999e17f52939ff0a2dd54572f0a95431ca in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d5fd119 ]\n\nSOLR-445: loop over all distributed errors "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15166553",
            "date": "2016-02-25T01:57:33+0000",
            "content": "Commit 98e8c344b81a169f20b09742e48f423f533837f6 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=98e8c34 ]\n\nSOLR-445: no need to track maxErrors explicitly, that's what errors.size() is for "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15166554",
            "date": "2016-02-25T01:57:35+0000",
            "content": "Commit 08bcb769bd1e896e719ebb0b4512208c993d9c38 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=08bcb76 ]\n\nSOLR-445: refactor metadata key+val parsing/formatting to use a new static inner helper class (KnownErr) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15166555",
            "date": "2016-02-25T01:57:36+0000",
            "content": "Commit 4ce376fa0f1e6acc84744582ddba6dfe9fd6f11a in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=4ce376f ]\n\nSOLR-445: replace errors map with List<KnownErr> and tweak public so we can differentiate errors of diff types\n\nfor example: an error on deleteById for docId1 vs an error on add for docId1 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15166556",
            "date": "2016-02-25T01:57:37+0000",
            "content": "Commit 0f0571928012c071c62fb928d2142d21fa183e2b in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0f05719 ]\n\nSOLR-445: basic support and tests for being tolerant of deletion failures\n\nthe tests that send deleteByQuerys to non shard leaders are failing because the resulting error details are null, need to re-review the DBQ logic in DUP to figure out where these failures are getting lost (see testVariousDeletesViaNoCollectionClient, testVariousDeletesViaShard1NonLeaderClient, testVariousDeletesViaShard2NonLeaderClient) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15174038",
            "date": "2016-03-01T17:06:53+0000",
            "content": "Commit 2401c9495319e1b5065b05ef3a36ee586f06b6d4 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=2401c94 ]\n\nSOLR-445 Merge branch 'master' into jira/SOLR-445 (pick up SOLR-8738 changes) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15174039",
            "date": "2016-03-01T17:06:54+0000",
            "content": "Commit 2401c9495319e1b5065b05ef3a36ee586f06b6d4 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=2401c94 ]\n\nSOLR-445 Merge branch 'master' into jira/SOLR-445 (pick up SOLR-8738 changes) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15191681",
            "date": "2016-03-12T01:20:16+0000",
            "content": "Commit 0bd817d19cadf7a6c0c522ed15f75110620029b3 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0bd817d ]\n\nSOLR-445: ensure maxErrors=-1 is treated as effectively unlimited "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15193606",
            "date": "2016-03-14T16:44:25+0000",
            "content": "(ment to post last friday but was blocked by the jira outage)\n\nOk ... i think things are looking pretty good on the jira/SOLR-445 branch \u2013 good enough that I'd really like some help reviewing the code & sanity checking the API (and internals for anyone who is up for it)...\n\n\n\nFor folks who haven't been following closely, here's what the configuration looks like (from the javadocs)...\n\n\n  <processor class=\"solr.TolerantUpdateProcessorFactory\">\n    <int name=\"maxErrors\">10</int>\n  </processor>\n\n\n\nWhen a chain with this processor is used, but maxErrors isn't exceeded, here's what the response looks like...\n\n\n$ curl 'http://localhost:8983/solr/techproducts/update?update.chain=tolerant-chain&wt=json&indent=true&maxErrors=-1' -H \"Content-Type: application/json\" --data-binary '{\"add\" : { \"doc\":{\"id\":\"1\",\"foo_i\":\"bogus\"}}, \"delete\": {\"query\":\"malformed:[\"}}'\n{\n  \"responseHeader\":{\n    \"errors\":[{\n        \"type\":\"ADD\",\n        \"id\":\"1\",\n        \"message\":\"ERROR: [doc=1] Error adding field 'foo_i'='bogus' msg=For input string: \\\"bogus\\\"\"},\n      {\n        \"type\":\"DELQ\",\n        \"id\":\"malformed:[\",\n        \"message\":\"org.apache.solr.search.SyntaxError: Cannot parse 'malformed:[': Encountered \\\"<EOF>\\\" at line 1, column 11.\\nWas expecting one of:\\n    <RANGE_QUOTED> ...\\n    <RANGE_GOOP> ...\\n    \"}],\n    \"maxErrors\":-1,\n    \"status\":0,\n    \"QTime\":1}}\n\n\n\nNote in the above example that:\n\n\n\tmaxErrors can be overridden on a per-request basis\n\tan effective maxErrors==-1 (either from config, or request param) means \"unlimited\" (under the covers it's using Integer.MAX_VALUE)\n\n\n\nIf/When maxErrors is reached for a request, then the first exception that the processor caught is propagated back to the user, and metadata is set on that exception with all of the same details about all the tolerated errors.\n\nThis next example is the same as the previous except that instead of maxErrors=-1 the request param is now maxErrors=1...\n\n\n$ curl 'http://localhost:8983/solr/techproducts/update?update.chain=tolerant-chain&wt=json&indent=true&maxErrors=1' -H \"Content-Type: application/json\" --data-binary '{\"add\" : { \"doc\":{\"id\":\"1\",\"foo_i\":\"bogus\"}}, \"delete\": {\"query\":\"malformed:[\"}}'\n{\n  \"responseHeader\":{\n    \"errors\":[{\n        \"type\":\"ADD\",\n        \"id\":\"1\",\n        \"message\":\"ERROR: [doc=1] Error adding field 'foo_i'='bogus' msg=For input string: \\\"bogus\\\"\"},\n      {\n        \"type\":\"DELQ\",\n        \"id\":\"malformed:[\",\n        \"message\":\"org.apache.solr.search.SyntaxError: Cannot parse 'malformed:[': Encountered \\\"<EOF>\\\" at line 1, column 11.\\nWas expecting one of:\\n    <RANGE_QUOTED> ...\\n    <RANGE_GOOP> ...\\n    \"}],\n    \"maxErrors\":1,\n    \"status\":400,\n    \"QTime\":1},\n  \"error\":{\n    \"metadata\":[\n      \"org.apache.solr.common.ToleratedUpdateError--ADD:1\",\"ERROR: [doc=1] Error adding field 'foo_i'='bogus' msg=For input string: \\\"bogus\\\"\",\n      \"org.apache.solr.common.ToleratedUpdateError--DELQ:malformed:[\",\"org.apache.solr.search.SyntaxError: Cannot parse 'malformed:[': Encountered \\\"<EOF>\\\" at line 1, column 11.\\nWas expecting one of:\\n    <RANGE_QUOTED> ...\\n    <RANGE_GOOP> ...\\n    \",\n      \"error-class\",\"org.apache.solr.common.SolrException\",\n      \"root-error-class\",\"java.lang.NumberFormatException\"],\n    \"msg\":\"ERROR: [doc=1] Error adding field 'foo_i'='bogus' msg=For input string: \\\"bogus\\\"\",\n    \"code\":400}}\n\n\n\n...the added exception metadata ensures that even in client code like the various SolrJ SolrClient implementations, which throw a (client side) exception on non-200 responses, the end user can access info on all the tolerated errors that were ignored before the maxErrors threshold was reached.\n\nCloudSolrClient in particular \u2013 which already has logic to split UpdateRequests; route individual commands to the appropraite leaders; and merge the responses \u2013 has been updated to handle merging these responses as well.\n\n(The ToleratedUpdateError class for modeling these types of errors has been added to solr-common, and has static utilities that client code can use to parse the data out of the responseHeader or out of any client side SolrException metadata)\n\n\n\nThere are still a bunch of nocommit comments, but they are almost all related to either:\n\n\tadding tests\n\tadding docs\n\trefactoring / hardening some internal APIs\n\tremoving suspected unneccessary \"isLeader\" code (once tests are final)\n\n\n\nI'll keep working on those, but I'd appreciate feedback from folks on how things currently stand.\n\nEven if you don't understand/care about the internals, thoughts on the user facing API would be appreciated. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-15194219",
            "date": "2016-03-14T21:49:21+0000",
            "content": "digging into this now, thanks Hoss Man "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-15194368",
            "date": "2016-03-14T23:18:40+0000",
            "content": "Thanks Hoss! I'll take a look at this tonight or tomorrow morning. "
        },
        {
            "author": "David Smiley",
            "id": "comment-15194699",
            "date": "2016-03-15T03:57:49+0000",
            "content": "The usage you explained looks nice Hoss!  This will be very useful.  I assume there are SolrJ hooks here.  I'll leave the internal review to others. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15195859",
            "date": "2016-03-15T18:18:37+0000",
            "content": "Commit 116ffaec6f6680c7312cb87680f8463df862f1f0 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=116ffae ]\n\nSOLR-445: test failures from adds mixed with deletes "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-15196019",
            "date": "2016-03-15T19:20:01+0000",
            "content": "Hoss Man what's the best way to look at the diff here?\nI tried the following but this gives a ton of unrelated stuff, which I assume is because this branch isn't up to date with the current master\n\ngit diff master\n\n\n\nI also tried this after Shawn Heisey suggested it on irc, but it's the same result:\n\n# after checking out and doing a pull for both, master and jira/SOLR-445\ngit diff refs/heads/master refs/heads/jira/SOLR-445\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15196063",
            "date": "2016-03-15T19:47:10+0000",
            "content": "git diff master...jira/SOLR-445 should be what you are looking for. (note: three dots)\n\nif you want to review hte list of individual commits, that's git log master..jira/SOLR-445 (note: two dots)\n\n(using two docs with diff, or three dots with log give you totally different and exciting and confusing behavior) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15198396",
            "date": "2016-03-16T23:21:51+0000",
            "content": "Commit a0d48f873c21ca0ab5ba02748c1659a983aad886 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a0d48f8 ]\n\nSOLR-445: start of a new randomized/chaosmonkey test, currently blocked by SOLR-8862 (no monkey yet) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15203592",
            "date": "2016-03-21T00:43:29+0000",
            "content": "Commit 8cc0a38453b389bdb031d78ad638b76dfa27f2d5 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=8cc0a38 ]\n\nSOLR-445: Merge branch 'master' into jira/SOLR-445 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15203593",
            "date": "2016-03-21T00:43:30+0000",
            "content": "Commit 8cc0a38453b389bdb031d78ad638b76dfa27f2d5 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=8cc0a38 ]\n\nSOLR-445: Merge branch 'master' into jira/SOLR-445 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15203594",
            "date": "2016-03-21T00:43:31+0000",
            "content": "Commit aeda8dc4ae881c4ec405d70dcbf1d0b2c30871b7 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=aeda8dc ]\n\nSOLR-445: fix test bugs, and put in a stupid work around for SOLR-8862 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15203596",
            "date": "2016-03-21T00:43:34+0000",
            "content": "Commit 1aa1ba3b3af69cad65b7a411ca88e120a418a598 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1aa1ba3 ]\n\nSOLR-445: harden & add logging to test\n\nalso rename since chaos monkey isn't going to be involved (due to SOLR-8872) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15203598",
            "date": "2016-03-21T00:43:37+0000",
            "content": "Commit 6ec8c635bf5853dfb229f89cb2818749c1cfe8ce in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6ec8c63 ]\n\nSOLR-445: cleanup some simple nocommits "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15203599",
            "date": "2016-03-21T00:43:38+0000",
            "content": "Commit 21c0fe690dc4e968e484ee906632a50bf0273786 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=21c0fe6 ]\n\nSOLR-445: hardent the ToleratedUpdateError API to hide implementation details "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-15205503",
            "date": "2016-03-22T01:07:36+0000",
            "content": "Thanks Hoss.\n\nI didn't look at the recent commits but from whatever I reviewed, this looks good. A bunch of nocommits but good stuff overall.\nI'll try and pitch in. "
        },
        {
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-15205787",
            "date": "2016-03-22T04:47:22+0000",
            "content": "Looks really good to me.\nWhat is the impact of many docs failing due to missing ID? Is there a test for that? I couldn't find one, but the diff is pretty big, I may have missed stuff.\nDon't know the answer to the \"isLeader\" question. I'd say the request would fail if leader changes in the middle of a request, but I'm not sure.  "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15206741",
            "date": "2016-03-22T16:40:48+0000",
            "content": "Commit fe54da0b58ed18a38f3dd436dd3f30fbee9acbbf in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=fe54da0 ]\n\nSOLR-445: remove nocommits related to OOM trapping since SOLR-8539 has concluded that this isn't a thing the java code actually needs to be defensive of "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15206743",
            "date": "2016-03-22T16:40:51+0000",
            "content": "Commit 5d93384e724b6f611270e212a4f9bd5b00c38e85 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5d93384 ]\n\nSOLR-445: fix exception msg when CloudSolrClient does async updates that (cumulatively) exceed maxErrors\n\nI initially thought it would make sense to refactor DistributedUpdatesAsyncException into solr-common and re-use it here, but when i started down that path i realized it didn't make any sense since there aren't actual exceptions to wrap client side. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15206746",
            "date": "2016-03-22T16:40:55+0000",
            "content": "Commit cc2cd23ca2537324dc7e4afe6a29605bbf9f1cb8 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=cc2cd23 ]\n\nSOLR-445: cloud test & bug fix for docs missing their uniqueKey field "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15206749",
            "date": "2016-03-22T16:42:47+0000",
            "content": "\nWhat is the impact of many docs failing due to missing ID? Is there a test for that? I couldn't find one, but the diff is pretty big, I may have missed stuff.\n\ngood question \u2013 there were checks of this in TolerantUpdateProcessorTest (from the early days of this patch) but i added some to TestTolerantUpdateProcessorCloud which uncovered a bug (now fixed) when checking isLeader \u2013 see: cc2cd23ca2537324dc7e4afe6a29605bbf9f1cb8\n\nDon't know the answer to the \"isLeader\" question. I'd say the request would fail if leader changes in the middle of a request, but I'm not sure.\n\nHmm... can you explain more what you think/expect could go wrong with the isLeader code removed that wouldn't go wrong with the code as it is today?   I mean ... theoretically, even with the isLeader check as we have it right now, the leader could change between the time we do the isLeader check and the call to super.processAdd (where DUP will do it's own isLeader check) ... or it could change (again) between the time super.processAdd/DUP.processAdd throws an exception and the time we make a decision wetherto only track it or track and immediately re-throw.\n\nI'm just not sure if that added code is really gaining us anything useful \u2013 but if someone can help me understand (or better still: demonstrate with a test) a concrete situation where the current code does the correct thing, but removeing the isLeader check is broken then i'll be convinced.\n\n\n\nWhere things currently stand:\n\n\n\tThe only remaining nocommits on the branch are questions about deleting the isLeader code, and questions about deleting DistribTolerantUpdateProcessorTest since we have other more robust cloud tests now.\n\tEven with the \"retry after giving serachers time to reopen\" logic in TestTolerantUpdateProcessorRandomCloud, i'm seeing a failure that reproduces consistently for me...\n\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestTolerantUpdateProcessorRandomCloud -Dtests.method=testRandomUpdates -Dtests.seed=ECFD2B9118A542E7 -Dtests.slow=true -Dtests.locale=bg -Dtests.timezone=Asia/Taipei -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] FAILURE 6.00s | TestTolerantUpdateProcessorRandomCloud.testRandomUpdates <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: cloud client doc count doesn't match bitself cardinality expected:<22> but was:<23>\n\n\n...so i'm currently working to improve the logging and trace through the test to understand that.\n\n\n "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15209461",
            "date": "2016-03-24T00:03:58+0000",
            "content": "Commit ae22181193dcb24707f7255f0132a2a0a85bf300 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ae22181 ]\n\nSOLR-445: fix silly test bug "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15209462",
            "date": "2016-03-24T00:03:59+0000",
            "content": "Commit 956d9a592a0a6e9c9d7c8244a4289f4cbf5d5012 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=956d9a5 ]\n\nSOLR-445: more testing of DBQ mixed with failures\n\n(trying to staticly recreate a random failure i haven't fully figured out yet) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15209463",
            "date": "2016-03-24T00:04:01+0000",
            "content": "Commit 2622eac2915ee210cfffd1969ef5dd8e2030e5cf in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=2622eac ]\n\nSOLR-445: harden checks in random test; add isoluated cloud test demonstrating bug random test found; add nocommit hack to DUP to work around test failure for now\n\n(SOLR-8890 to fix a better way) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15209465",
            "date": "2016-03-24T00:04:03+0000",
            "content": "Commit a4686553712a0d01dc2d6853038c4cca2caee63f in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a468655 ]\n\nSOLR-445: randomized testing of the 'doc missing unique key' code path "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15209467",
            "date": "2016-03-24T00:04:06+0000",
            "content": "Commit da3ea40e80189c7c2bbd8114a99c72a64262786b in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=da3ea40 ]\n\nSOLR-8890: generalized whitelist of param names DUP will use when forwarding requests, usage in SOLR-445 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15210779",
            "date": "2016-03-24T19:09:31+0000",
            "content": "Commit 39884c0b0c02b4090640d6268a45a1cf5f54f3e0 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=39884c0 ]\n\nSOLR-445: removing questionable isLeader check; beasting the tests w/o this code didn't demonstrate any problems "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15210780",
            "date": "2016-03-24T19:09:32+0000",
            "content": "Commit 1d8cdd27993a46ae17c4ac308504513a33f01a15 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1d8cdd2 ]\n\nSOLR-445: remove test - we have more complete coverage in TestTolerantUpdateProcessorCloud which uses the more robust SolrCloudTestCase model "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15210781",
            "date": "2016-03-24T19:09:33+0000",
            "content": "Commit b08c284b26b1779d03693a45e219db89839461d0 in lucene-solr's branch refs/heads/jira/SOLR-445 from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b08c284 ]\n\nSOLR-445: fix logger declaration to satisfy precommit "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15210784",
            "date": "2016-03-24T19:11:21+0000",
            "content": "I'm still beasting the tests a bit, but i think this is pretty solid and ready for master/branch_6x "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15210793",
            "date": "2016-03-24T19:15:45+0000",
            "content": "updated summary to reflect basic information about feature being added "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-15210827",
            "date": "2016-03-24T19:28:18+0000",
            "content": "LGTM +1 Nice test coverage of all this!  This will be very useful for streaming applications (such as from Spark and Storm) where re-trying individual docs after an error is less than ideal. Now we'll be able to pin-point exactly which docs had issues!\n\nI'd prefer this to be baked into the default chain but can understand the rationale for leaving it out for now too. So long as we put up an example of how to enable it using the Config API in the ref guide.\n "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15212179",
            "date": "2016-03-25T18:03:54+0000",
            "content": "Commit f051f56be96b12f1f3e35978ca4c840ae06a801f in lucene-solr's branch refs/heads/master from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f051f56 ]\n\nSOLR-445: new ToleranteUpdateProcessorFactory to support skipping update commands that cause failures when sending multiple updates in a single request.\n\nSOLR-8890: New static method in DistributedUpdateProcessorFactory to allow UpdateProcessorFactories to indicate request params that should be forwarded when DUP distributes updates.\n\nThis commit is a squashed merge from the jira/SOLR-445 branch (as of b08c284b26b1779d03693a45e219db89839461d0) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15212181",
            "date": "2016-03-25T18:03:57+0000",
            "content": "Commit f051f56be96b12f1f3e35978ca4c840ae06a801f in lucene-solr's branch refs/heads/master from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f051f56 ]\n\nSOLR-445: new ToleranteUpdateProcessorFactory to support skipping update commands that cause failures when sending multiple updates in a single request.\n\nSOLR-8890: New static method in DistributedUpdateProcessorFactory to allow UpdateProcessorFactories to indicate request params that should be forwarded when DUP distributes updates.\n\nThis commit is a squashed merge from the jira/SOLR-445 branch (as of b08c284b26b1779d03693a45e219db89839461d0) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15212415",
            "date": "2016-03-25T21:09:23+0000",
            "content": "Commit 5b6eacb80bca5815059cd50a1646fa4ecb146e43 in lucene-solr's branch refs/heads/branch_6x from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5b6eacb ]\n\nSOLR-445: new ToleranteUpdateProcessorFactory to support skipping update commands that cause failures when sending multiple updates in a single request.\n\nSOLR-8890: New static method in DistributedUpdateProcessorFactory to allow UpdateProcessorFactories to indicate request params that should be forwarded when DUP distributes updates.\n\nThis commit is a squashed merge from the jira/SOLR-445 branch (as of b08c284b26b1779d03693a45e219db89839461d0) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15212417",
            "date": "2016-03-25T21:09:26+0000",
            "content": "Commit 5b6eacb80bca5815059cd50a1646fa4ecb146e43 in lucene-solr's branch refs/heads/branch_6x from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=5b6eacb ]\n\nSOLR-445: new ToleranteUpdateProcessorFactory to support skipping update commands that cause failures when sending multiple updates in a single request.\n\nSOLR-8890: New static method in DistributedUpdateProcessorFactory to allow UpdateProcessorFactories to indicate request params that should be forwarded when DUP distributes updates.\n\nThis commit is a squashed merge from the jira/SOLR-445 branch (as of b08c284b26b1779d03693a45e219db89839461d0) "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15212418",
            "date": "2016-03-25T21:09:27+0000",
            "content": "Commit b8c0ff66f958e5e199874059b0427ea267778c3a in lucene-solr's branch refs/heads/branch_6x from Chris Hostetter (Unused)\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b8c0ff6 ]\n\nSOLR-445: Merge remote-tracking branch 'refs/remotes/origin/branch_6x' into branch_6x\n\n(picking up mid backport conflicts) "
        },
        {
            "author": "Hoss Man",
            "id": "comment-15278994",
            "date": "2016-05-10T21:34:11+0000",
            "content": "\nManually correcting fixVersion per Step #S5 of LUCENE-7271 "
        },
        {
            "author": "ASF GitHub Bot",
            "id": "comment-15327357",
            "date": "2016-06-13T13:26:31+0000",
            "content": "GitHub user arafalov opened a pull request:\n\n    https://github.com/apache/lucene-solr/pull/43\n\n    Trivial name spelling fix for SOLR-445\n\n    ToleranteUpdateProcessorFactory -> ToleranteUpdateProcessorFactory\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/arafalov/lucene-solr-1 patch-3\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/lucene-solr/pull/43.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #43\n\n\ncommit 6742355f93f0d2d03600fe408b542507ee89bf54\nAuthor: Alexandre Rafalovitch <arafalov@gmail.com>\nDate:   2016-06-13T13:19:25Z\n\n    Trivial Spelling fix \n\n    ToleranteUpdateProcessorFactory -> TolerantUpdateProcessorFactory\n\ncommit ebffa9aa2aebd689db53ba363d5022b893c7eeb0\nAuthor: Alexandre Rafalovitch <arafalov@gmail.com>\nDate:   2016-06-13T13:22:49Z\n\n    Trivial Spelling fix\n\n    ToleranteUpdateProcessorFactory -> TolerantUpdateProcessorFactory\n\n "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15352788",
            "date": "2016-06-28T10:53:49+0000",
            "content": "Commit adaabaf834964e1674236fca1d4a2801c6cad931 in lucene-solr's branch refs/heads/master from Shalin Shekhar Mangar\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=adaabaf ]\n\nTrivial name spelling fix for SOLR-445\nMerge branch 'patch-3' of https://github.com/arafalov/lucene-solr-1\n\nThis closes #43 "
        },
        {
            "author": "ASF GitHub Bot",
            "id": "comment-15352790",
            "date": "2016-06-28T10:54:28+0000",
            "content": "Github user asfgit closed the pull request at:\n\n    https://github.com/apache/lucene-solr/pull/43 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-15352798",
            "date": "2016-06-28T11:08:50+0000",
            "content": "Commit c8f9973a106c57075601d963f13b5e0997f14f7d in lucene-solr's branch refs/heads/branch_6x from Shalin Shekhar Mangar\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c8f9973 ]\n\nTrivial name spelling fix for SOLR-445. Cherry-picked 8c47d20 "
        }
    ]
}