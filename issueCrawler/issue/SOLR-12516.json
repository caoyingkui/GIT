{
    "id": "SOLR-12516",
    "title": "JSON \"range\" facets can incorrectly refine subfacets for buckets",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "Facet Module"
        ],
        "type": "Bug",
        "fix_versions": [
            "7.5",
            "master (8.0)"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "while simple type:range facets don't benefit from refinement, because every shard returns the same set of buckets, some bugs currently exist when a range facet contains sub facets that use refinement:\n\n\n\tthe optional other buckets (before/after/between) are not considered during refinement\n\twhen using the include option: if edge is specified, then the refinement of all range buckets mistakenly includes the lower bound of the range, regardless of whether lower was specified.\n\n\n\n\n\n#1 occurs because FacetRangeMerger extends FacetRequestSortedMerger<FacetRange> ... however FacetRangeMerger does not override getRefinement(...) which means only FacetRequestSortedMerger.buckets is evaluated and considered for refinement. The additional, special purpose, FacetBucket instances tracked in FacetRangeMerger are never considered for refinement.\n\n#2 exists because of a mistaken in the implementation of refineBucket and how it computes the start value.",
    "attachments": {
        "SOLR-12516.patch": "https://issues.apache.org/jira/secure/attachment/12929068/SOLR-12516.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2018-06-25T17:48:36+0000",
            "content": "I only discovered this because of a randomized failure from CurrencyRangeFacetCloudTest while testing out SOLR-12343 \u2013 that patch (currently) enforces that every shard which has at least one bucket for a field facet must \"contribute\" to every bucket (either in phase#1 or via refinement) to be returned \u2013 and it can cause the existing testJsonRangeFacetWithSubFacet to fail for some (randomized) shard count because of this bug.\n\nI'll try to work up a more isolated, generalized, test case that shows incorrect counts (on a simple/non-currency) for a range facet w/o SOLR-12343 in place ",
            "author": "Hoss Man",
            "id": "comment-16522598"
        },
        {
            "date": "2018-06-25T18:17:15+0000",
            "content": "Attached a trivial test patch that modifies the existing TestJsonFacetRefinement check of a range facet w/a single bucket to also specify other:all so that the between bucket should have the same count & sub-facet results as the existing single bucket \u2013 but it does not. ",
            "author": "Hoss Man",
            "id": "comment-16522632"
        },
        {
            "date": "2018-06-25T19:04:45+0000",
            "content": "One complexity in fixing this is the way the refinement requests are currently built \u2013 when building/parsing the _l (and _s + _p ) list(s) the code currently assume everything it finds is a bucket value (or a pair of bucket value and sub-facet refinement requests) ... there's no intermediate structure that says \"buckets\" like there is in the facet response, so there is no way to put \"before\"/\"between\"/\"after\" outside of the list of bucket values to indicate what sub-facets need refined \u2013 and putting them in the bucket list seems like a hack that could cause problems w/the existing code that expects everything in that list to be parsable by the Calc for this facet.\nPerhaps a new _other key should be added alongside the existing _l, _s, _p keys that then contains _l, _s, _p for each of the \"other\" buckets (by name) where a sub-facet needs refinement?\n\nSomething like...\n\n\n    doTestRefine(\"{top:{type:range, other:all, field:R, start:0, end:1, gap:1, facet:{x : {type:terms, field:X, limit:2, refine:true} } } }\",\n        \"{top: {buckets:[{val:0, count:2, x:{buckets:[{val:x1, count:5},{val:x2, count:3}]} } ]\" +\n        \"       before:{count:0},after:{count:0},\" +\n        \"       between:{count:2,x:{buckets:[{val:x1, count:5},{val:x2, count:3}]} } } }\",\n        \"{top: {buckets:[{val:0, count:1, x:{buckets:[{val:x2, count:4},{val:x3, count:2}]} } ]\" +\n        \"       before:{count:0},after:{count:0},\" +\n        \"       between:{count:1,x:{buckets:[{val:x2, count:4},{val:x3, count:2}]} } } }\",\n        null,\n        \"=={top: {\" +\n            \"_s:[  [0 , {x:{_l:[x1]}} ]  ]\" +\n            \"_other:{_s:[  ['between' , {x:{_l:[x1]}} ]  ] }\" +\n            \"    }  \" +\n            \"}\"\n    );\n\n\n\n? ",
            "author": "Hoss Man",
            "id": "comment-16522690"
        },
        {
            "date": "2018-06-28T23:32:09+0000",
            "content": "I started looking into the existing refinement code to think about the best way to refactor the existing code to deal with refining the \"special\" buckets...\n\nThe complexity being that the lower/upper bounds of the \"between\" and \"after\" buckets require finding the \"real end\" (of the ranges) based on \"end\" & \"hardend\" options, which normally requires using the field's Calc(ulator) to iteratively computing all the buckets until you read the end \u2013 but the exsiting refinement code doesn't do that, it just computes the buckets requested based on the (numeric) values of the buckets.\n\n...but while reviewing that code i realized there is another refinement bug: because of a typo (or copy paste mistake?) the refinement of every individual bucket assumes the \"start\" value is the same as the current bucket value, which means using include:edge causes the wrong value of boolean incLower for almost all buckets being refined.\n\nSo i set aside the code and have been focusing on writing some comprehensive tests of type:range faceting using multiple shards, and various request options (inlcuding a tye:terms subfacet) against a randomized index by comparing it with an in memory model of the data in the index.\n\nWith the updated patch, it's pretty trivial to find a seed that causes most of hte new tests to fail ... at a glance i'm pretty sure that these failures are all due to one of the 2 problems so far identified in this jira, but i'll dig into fixing them and revise the issue further if it turns out there are other problems. ",
            "author": "Hoss Man",
            "id": "comment-16526911"
        },
        {
            "date": "2018-06-28T23:40:48+0000",
            "content": "revising issue summary & description based on expanded findings of the error cases ",
            "author": "Hoss Man",
            "id": "comment-16526921"
        },
        {
            "date": "2018-07-03T02:04:50+0000",
            "content": "Updated patch that i think fixes these bugs completley.\n\n\u00a0\n\nThere are still a lot of nocommits, but they are almost enirely regarding more tests i want ot rwite, and/or cleanup/refactoring of the code \u2013 the existing FacetRange code already had a lot of duplication & dead code, i added more duplication in an effort to minimize changes until i could get the tests to pass... once i finish writting all the tests i've thought of i want to slash and burn a lot of the duplication.\n\nAs far as what the implementation looks like...\nThe complexity being that the lower/upper bounds of the \"between\" and \"after\" buckets require finding the \"real end\" (of the ranges) based on \"end\" & \"hardend\" options, which normally requires using the field's Calc(ulator) to iteratively computing all the buckets ...\n\n...i would up going with an approach similar to how stat refinement works: if we're a phase#1 shard request, we return an extra bit of \"actual_end\" metadata for hte merger to use (in this case sending back as part of the refinement requests if needed) that the end user never sees. This approach involves some redundency in the phase#1 responses for every shard, but saves us the need for any/all nodes to \"spin loop\" over every bucket to figure out the actual-end value to use if either the after/between buckets need refinement. ",
            "author": "Hoss Man",
            "id": "comment-16530701"
        },
        {
            "date": "2018-07-03T16:23:48+0000",
            "content": "attaching basically the same patch as yesterday \u2013 just updated to account for a few conflicts & test-tweaks needed because of the SOLR-12326 changes. ",
            "author": "Hoss Man",
            "id": "comment-16531632"
        },
        {
            "date": "2018-07-03T22:23:34+0000",
            "content": "Updated patch with all the tests i could think of ... moving on to cleaning up the cruft/duplication in the implementation ",
            "author": "Hoss Man",
            "id": "comment-16532021"
        },
        {
            "date": "2018-07-05T19:23:59+0000",
            "content": "updated patch that cleans up all the cruft & code duplication, and resolves the last few nocommits.\n\nprecommit seems to pass \u2013 still running & hammering on tests.\n\nI'll do another self-review tomorow, and if nohitng jumps out at me, or i don't hear any complains i'll go ahead and commit. ",
            "author": "Hoss Man",
            "id": "comment-16534041"
        },
        {
            "date": "2018-07-05T22:08:31+0000",
            "content": "Whoops... my cleanup had a dumb bug (variable not initialized) in how the \"other\" ranges were being refined \u2013 RangeFacetCloudTest caught it easily, but apparently the first time i ran that test it randomly picked single shard mode.\n\npatch updated with fix ... still hammering tests. ",
            "author": "Hoss Man",
            "id": "comment-16534213"
        },
        {
            "date": "2018-07-06T16:34:17+0000",
            "content": "Commit 7d8ef9e39d3321a5366fcfe1a358ec015fb7b8b1 in lucene-solr's branch refs/heads/master from Chris Hostetter\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7d8ef9e ]\n\nSOLR-12516: Fix some bugs in 'type:range' Facet refinement when sub-facets are combined with non default values for the 'other' and 'include' options.\n\n1) the optional other buckets (before/after/between) are not considered during refinement\n\n2) when using the include option: if edge is specified, then the refinement of all range buckets mistakenly includes the lower bound of the range, regardless of whether lower was specified. ",
            "author": "ASF subversion and git services",
            "id": "comment-16535086"
        },
        {
            "date": "2018-07-06T16:50:24+0000",
            "content": "Commit b3896b4eba8bb820f265205ddd05bccb98cdd801 in lucene-solr's branch refs/heads/branch_7x from Chris Hostetter\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b3896b4 ]\n\nSOLR-12516: Fix some bugs in 'type:range' Facet refinement when sub-facets are combined with non default values for the 'other' and 'include' options.\n\n1) the optional other buckets (before/after/between) are not considered during refinement\n\n2) when using the include option: if edge is specified, then the refinement of all range buckets mistakenly includes the lower bound of the range, regardless of whether lower was specified.\n\n(cherry picked from commit 7d8ef9e39d3321a5366fcfe1a358ec015fb7b8b1) ",
            "author": "ASF subversion and git services",
            "id": "comment-16535103"
        },
        {
            "date": "2018-07-06T22:59:15+0000",
            "content": "\n\n\n  -1 overall \n\n\n\n\n\n\n\n\n\n Vote \n Subsystem \n Runtime \n Comment \n\n\n -1 \n  patch  \n   0m  5s \n  SOLR-12516 does not apply to master. Rebase required? Wrong Branch? See https://wiki.apache.org/solr/HowToContribute#Creating_the_patch_file for help.  \n\n\n\n\n\n\n\n\n\n Subsystem \n Report/Notes \n\n\n JIRA Issue \n SOLR-12516 \n\n\n JIRA Patch URL \n https://issues.apache.org/jira/secure/attachment/12930455/SOLR-12516.patch \n\n\n Console output \n https://builds.apache.org/job/PreCommit-SOLR-Build/139/console \n\n\n Powered by \n Apache Yetus 0.7.0   http://yetus.apache.org \n\n\n\n\n\n\nThis message was automatically generated.\n ",
            "author": "Lucene/Solr QA",
            "id": "comment-16535456"
        }
    ]
}