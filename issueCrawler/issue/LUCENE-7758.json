{
    "id": "LUCENE-7758",
    "title": "EdgeNGramTokenFilter breaks highlighting by keeping end offsets of original tokens",
    "details": {
        "labels": "",
        "priority": "Major",
        "resolution": "Unresolved",
        "affect_versions": "6.4.1",
        "status": "Open",
        "type": "Bug",
        "components": [
            "modules/analysis"
        ],
        "fix_versions": []
    },
    "description": "When EdgeNGramTokenFilter produces new tokens, they inherit end positions from parent tokens. This behaviour is irrational and breaks highlighting: highlighted not matched pattern, but whole source tokens.\n\nSeems like similar problem was fixed in LUCENE-3642, but end offsets was broken again after LUCENE-3907.\n\nSome discussion was found in SOLR-7926:\nI agree this (highlighting of hits from tokens produced by\nEdgeNGramFilter) got worse with LUCENE-3907, but it's not clear how to\nfix it.\nThe stacking seems more correct: all these grams are logically\ninterchangeable with the original token, and were derived from it, so\ne.g. a phrase query involving them with adjacent tokens would work\ncorrectly.\nWe could perhaps remove the token graph requirement that tokens\nleaving from the same node have the same startOffset, and arriving to\nthe same node have the same endOffset. Lucene would still be able to\nindex such a graph, as long as all tokens leaving a given node are\nsorted according to their startOffset. But I'm not sure if there\nwould be other problems...\nOr we could maybe improve the token graph, at least for the non-edge\nNGramTokenFilter, so that the grams are linked up correctly, so that any\npath through the graph reconstructs the original characters.\nBut realistically it's not possible to innovate much with token graphs\nin Lucene today because of apparently severe back compat requirements:\ne.g. LUCENE-6664, which fixes the token graph bugs in the existing\nSynonymFilter so that proximity queries work correctly when using\nsearch-time synonyums, is blocked because of the back compat concerns\nfrom LUCENE-6721.\nI'm not sure what the path forward is...",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15948712",
            "date": "2017-03-30T09:06:44+0000",
            "content": "This behaviour is irrational\n\nWell, this is not exactly true. This is a token filter, meaning it can be applied on top on any set of other token filters. Now imagine that someone is applying edge n-grams on top of synonyms, this could generate broken offsets (going backwards for instance) so keeping the original offsets is the only safe move. A workaround to this issue is to use the (edge) n-gram tokenizers (as opposed to filters), which also have a protected isTokenChar method that can be overriden in case you want to eg. split on whitespaces. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-15948713",
            "date": "2017-03-30T09:06:44+0000",
            "content": "This behaviour is irrational\n\nWell, this is not exactly true. This is a token filter, meaning it can be applied on top on any set of other token filters. Now imagine that someone is applying edge n-grams on top of synonyms, this could generate broken offsets (going backwards for instance) so keeping the original offsets is the only safe move. A workaround to this issue is to use the (edge) n-gram tokenizers (as opposed to filters), which also have a protected isTokenChar method that can be overriden in case you want to eg. split on whitespaces. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-15948882",
            "date": "2017-03-30T11:29:38+0000",
            "content": "Now imagine that someone is applying edge n-grams on top of synonyms, this could generate broken offsets (going backwards for instance) so keeping the original offsets is the only safe move\nBut why one feature should break another? I don't use synonyms or something like that, but I have no possibility to use token filter with properly offsets.\n\nA workaround to this issue is to use the (edge) n-gram tokenizers (as opposed to filters)\nSuch workaround applicable only to cases when input text can be simple splitted on specified characters. In my case I want to use icu_tokenizer before edge_ngram for properly split by words. For example, imagine japan language. ",
            "author": "Mikhail Bystryantsev"
        },
        {
            "id": "comment-15948956",
            "date": "2017-03-30T12:21:40+0000",
            "content": "I agree with what you are saying, it would like to have the ability to modify offsets in the cases that it makes sense too. I just wanted to react to your comment that the current behaviour is irrational. Moreover, I would not be surprised that highlighting the entire token is a desired behaviour for some users.\n\nI haven't thought about it much but it feels to me that we would need a way to annotate token streams in order to know whether the content of the CharTermAttribute matches the original text between the offsets stored in the OffsetAttribute if we want to change offsets safely. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-15949023",
            "date": "2017-03-30T13:04:28+0000",
            "content": "I just wanted to react to your comment that the current behaviour is irrational\nOk, in other words: unexpected and may be confusing from the point of view of the average user.\n\nI would not be surprised that highlighting the entire token is a desired behaviour for some users.\nI think for such cases just should be implemented possibility to behaviour tuning, for example parameter like inherit_offsets: true | false. ",
            "author": "Mikhail Bystryantsev"
        },
        {
            "id": "comment-15949039",
            "date": "2017-03-30T13:18:36+0000",
            "content": "Moreover, I would not be surprised that highlighting the entire token is a desired behaviour for some users.\n\nThis is correct. Modifying offsets inside a TokenFilter is not going to be correct for highlighting for the reasons you are mentioning. This is a general issue with all token filters that are splitting tokens: The \"famous\" example is WordDelimiterFilter.\n\nAssigning offsets is the responsibility of tokenizers. Tokenfilters should just look at tokens and modify them, but not split them or change their offsets. \n\nIn addition, highlighting is not meant to produce \"exact\" explanations of every analysis step. It is more meant to allow highlighting whole tokens afterwards, so the user has an idea, which token was responsible for a hit. ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-15949095",
            "date": "2017-03-30T13:51:42+0000",
            "content": "Assigning offsets is the responsibility of tokenizers. Tokenfilters should just look at tokens and modify them, but not split them or change their offsets.\nBut tokenizer can be only one, so there is no way to get tokens different than produced by specific single tokenizer. No way to customize without writing your own tokenizers. It is possible to combine token filters, but not tokenizers.\n\nIn addition, highlighting is not meant to produce \"exact\" explanations of every analysis step. It is more meant to allow highlighting whole tokens afterwards, so the user has an idea, which token was responsible for a hit.\nI think this should be decided by Lucene users, not by anyone else. When you project your index and search behaviour, only you can decide how it should work based on your project requirements. ",
            "author": "Mikhail Bystryantsev"
        }
    ]
}