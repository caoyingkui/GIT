{
    "id": "SOLR-11891",
    "title": "DocsStreamer populates SolrDocument w/unnecessary fields",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "Response Writers"
        ],
        "type": "Improvement",
        "fix_versions": [
            "7.4",
            "master (8.0)"
        ],
        "affect_versions": "5.4,                                            6.4.2,                                            6.6.2",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "We observe that solr query time increases significantly with the number of rows requested, \u00a0even all we retrieve\u00a0for each document is just fl=id,score.\u00a0 Debugged a bit and see that most of the increased time was spent in BinaryResponseWriter,\u00a0 converting lucene document into SolrDocument.\u00a0 Inside convertLuceneDocToSolrDoc():\u00a0\u00a0\u00a0\n\nhttps://github.com/apache/lucene-solr/blob/df874432b9a17b547acb24a01d3491839e6a6b69/solr/core/src/java/org/apache/solr/response/DocsStreamer.java#L182\u00a0\n\nI am a bit puzzled why we need to iterate through all the fields in the document. Why can\u2019t we just iterate through the requested field list?\u00a0 \u00a0\u00a0\n\nhttps://github.com/apache/lucene-solr/blob/df874432b9a17b547acb24a01d3491839e6a6b69/solr/core/src/java/org/apache/solr/response/DocsStreamer.java#L156\u00a0\n\ne.g. when pass in the field list as\u00a0\n\nsdoc\u00a0=\u00a0convertLuceneDocToSolrDoc(doc, rctx.getSearcher().getSchema(), fnames)\n\nand just iterate through fnames, \u00a0there is a significant performance boost in our case.",
    "attachments": {
        "DocsStreamer.java.diff": "https://issues.apache.org/jira/secure/attachment/12909266/DocsStreamer.java.diff",
        "SOLR-11891.patch": "https://issues.apache.org/jira/secure/attachment/12914795/SOLR-11891.patch",
        "SOLR-11891.patch.BAD": "https://issues.apache.org/jira/secure/attachment/12914619/SOLR-11891.patch.BAD"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2018-01-24T06:20:50+0000",
            "content": "I believe that after [Document doc = docFetcher.doc(id, fnames);|https://github.com/apache/lucene-solr/blob/df874432b9a17b547acb24a01d3491839e6a6b69/solr/core/src/java/org/apache/solr/response/DocsStreamer.java#L155]\u00a0Lucene's Document contains only the requested fields.\n\nI'm not sure but this can be related somehow, when result [transformer] is used it may refer to any field, so it might impact fl filtering, also some remaining wildcard param fl=*\u00a0might trigger full loading.\u00a0 ",
            "author": "Mikhail Khludnev",
            "id": "comment-16336996"
        },
        {
            "date": "2018-01-24T18:08:57+0000",
            "content": "NOTE: This Jira was opened in response to a mailing list thread...\n\nhttps://lists.apache.org/thread.html/%3CCAL1=wh0UOo1J01D-FROath_Ea9cvN9Cm44N1cgJeRDqRTKUSng@mail.gmail.com%3E\n\nI believe that after Document doc = docFetcher.doc(id, fnames) Lucene's Document contains only the requested fields.\n\nMy comments from that thread requesting this jira...\n\n\nI have a hunch here -- but i haven't verified it.\n\nFirst of all: the specific code in question that you mention assumes it \ndoesn't *need* to filter out the result of \"doc.getFields()\" basd on the \n'fl' because at the point in the processing where the DocsStreamer is \nlooping over the result of \"doc.getFields()\" the \"Document\" object it's \ndealing with *should* only contain the specific (subset of stored) fields \nrequested by the fl param -- this is handled by RetrieveFieldsOptimizer & \nSolrDocumentFetcher that the DocsStreamer builds up acording to the \nresults of ResultContext.getReturnFields() when asking the \nSolrIndexSearcher to fetch the doc()\n\nBut i think what's happening here is that because of the documentCache, \nthere are cases where the SolrIndexSearcher is not actaully using \na SolrDocumentStoredFieldVisitor to limit what's requested from the \nIndexReader, and the resulting Document contains all fields -- which is \nthen compounded by code that loops over every field.  \n\nAt a quick glance, I'm a little fuzzy on how exactly \nenableLazyFieldLoading may/may-not be affecting things here, but either \nway I think you are correct -- we can/should make this overall stack of \ncode smarter about looping over fields we know we want, vs looping over \nall fields in the doc.\n\nCan you please file a jira for this?\n\n\n\n... I'm not sure but this can be related somehow, when result [transformer] is used it may refer to any field, so it might impact fl filtering, ...\n\nI don't think so \u2013 the DocTransformer API has getExtraRequestFields() explicitly for this purpose, so that the ReturnFields structure (used by DocStreamer & RetrieveFieldsOptimizer) should already know exactly which fields are needed \u2013 even by the transformers. ",
            "author": "Hoss Man",
            "id": "comment-16338009"
        },
        {
            "date": "2018-01-24T19:51:56+0000",
            "content": "I believe that after [Document doc = docFetcher.doc(id, fnames);|https://github.com/apache/lucene-solr/blob/df874432b9a17b547acb24a01d3491839e6a6b69/solr/core/src/java/org/apache/solr/response/DocsStreamer.java#L155]\u00a0Lucene's Document contains only the requested fields.\nThe lucene document is actually\u00a0created with all fields, but fields not requested are created as lazyfields. I think this is fine for the document cache & enableLazyLoading option.\u00a0 What I am puzzled is whether these lazyfields are needed when convert lucene document to solr document,\u00a0 as we are creating new solr document from scratch and it is not cached for future use.\u00a0\n\nhttps://github.com/apache/lucene-solr/blob/df874432b9a17b547acb24a01d3491839e6a6b69/solr/core/src/java/org/apache/solr/response/DocsStreamer.java#L182https://github.com/apache/lucene-solr/blob/df874432b9a17b547acb24a01d3491839e6a6b69/solr/core/src/java/org/apache/solr/response/DocsStreamer.java#L182\n\n\u00a0 ",
            "author": "wei wang",
            "id": "comment-16338136"
        },
        {
            "date": "2018-01-24T20:20:26+0000",
            "content": "convertLuceneDocToSolrDoc should be innocent enough; it merely iterates the IndexableField instances in a collection and potentially calls name() on them which is always a simple getter.  We can't avoid putting all fields on the SolrDocument because of the potential for a document transformer to need it \u2013 and that's not known by RetrieveFieldsOptimizer.  But it should be innocent enough because if no such transformer requests the value, then it shouldn't actually be loaded (it's lazy).\n\nSomething else must be going on.  I wonder who the caller is and what doc transformers you may have. ",
            "author": "David Smiley",
            "id": "comment-16338160"
        },
        {
            "date": "2018-01-24T20:39:36+0000",
            "content": "BTW there very well may be a bug in Solr related to this; I just doubt that it's in DocStreamer.convertLuceneDocToSolrDoc.  I did some basic debugging of this on master with a small modification to SolrExampleBinaryTest.testExampleConfig just now and I don't see a problem.  I also set a break point where the lazy loaded value is loaded and I only saw it occur in a couple tests in those in SolrExampleTests \u2013 one of them was testChildDoctransformer which doesn't surprise me.  I'm using master (roughly same as 7x).  There have been some changes since 6x I recall. ",
            "author": "David Smiley",
            "id": "comment-16338183"
        },
        {
            "date": "2018-01-24T21:42:29+0000",
            "content": "We can't avoid putting all fields on the SolrDocument because of the\u00a0potential\u00a0for a document transformer to need it \u2013 and that's not known by RetrieveFieldsOptimizer.\u00a0\nCan you explain more on what's the impact of not putting all the fields into SolrDocument?\u00a0 What we observed is that although convertLuceneDocToSolrDoc\u00a0merely iterates through the fields in doc, generating all the lazy fields in solr document makes a performance difference when we have 100+ fields in the doc vs only have fl= id, score.\u00a0\u00a0 ",
            "author": "wei wang",
            "id": "comment-16338273"
        },
        {
            "date": "2018-01-24T22:26:18+0000",
            "content": "convertLuceneDocToSolrDoc should be innocent enough; it merely iterates the IndexableField instances in a collection and potentially calls name() on them which is always a simple getter. ...\nThat iteration over all fields (real and lazy) is in and of itself the problem being reported \u2013 if the docs contain 1000 stored fields, but only 1 is requested in the 'fl' then the current code is looping over all 1000 fields in every doc even though it knows exactly which fields it needs \u2013 even if there are no disk reads involved for some lazy fields, it's still a waasteful iteration that's multiplicitive of the # of fields in the docs and the number of docs in the response, regardless of how small the fl is.\n...We can't avoid putting all fields on the SolrDocument because of the potential for a document transformer to need it \u2013 and that's not known by RetrieveFieldsOptimizer.\nIIUC we can avoid it and RetrieveFieldsOptimizer does know that \u2013 as i mentioned in my response to mk: that's the entire point of DocTransformer.getExtraRequestFields() (see the javadocs) which is used to build up the list returned by SolrReturnFields.getLuceneFieldNames()\nBut it should be innocent enough because if no such transformer requests the value, then it shouldn't actually be loaded (it's lazy).\nEven if the Document field values are lazy, the existing code that loops over all of them is still building up the SolrDocument that contains all of those (lazy) fields \u2013 wasting time and a small amount of space (and that assumes they are all lazy: it's an option, it may not be on for some people \u2013 if/when they're not lazy then that takes up even more time & space reading them from disk)\n\nI think the ideal \"fix\" is that the SolrReturnFields.getLuceneFieldNames() should get passed down all the way into convertLuceneDocToSolrDoc (or something we refactor it into) such that we do an runtime check of which list is smaller: SolrReturnFields.getLuceneFieldNames() or Document.getFields() \u2013 and then loop over that (smallest) list.\n\nRegardless of what changes we make: we should have a whitebox test of convertLuceneDocToSolrDoc (or something we refactor it into) confirming that:\n\n\tthe resulting SolrDocument doesn't contain any fields that aren't needed\n\tsome explicitly un-requested \"lazy\" IndexableFields in the input Document must still be \"lazy\" (ie: not \"actuallized\") when the method returns (ie: that we didn't do a disk read we didn't need)\n\n ",
            "author": "Hoss Man",
            "id": "comment-16338342"
        },
        {
            "date": "2018-01-25T03:03:11+0000",
            "content": "That iteration over all fields (real and lazy) is in and of itself the problem being reported\n\nI understand that now.\n\nDocTransformer API has getExtraRequestFields()\n\nOoooohhhh, okay I see, I didn't know about that.  Sorry for not reading the issue description thoroughly.\n\nOkay so I agree that we can populate the SolrDocument with only the fields needed in the request as defined by the RetrieveFieldsOptimizer (or something similar that Hoss suggests).  You could simply try that and run all tests and cross your fingers   \n\nwei wangi, by the way, you ought to enable docValues for your uniqueKey defined field.  Recent versions of Solr are smart enough to avoid the stored document altogether if all requested fields are docValues. ",
            "author": "David Smiley",
            "id": "comment-16338653"
        },
        {
            "date": "2018-01-26T18:50:17+0000",
            "content": "Thanks all.\u00a0\u00a0\nwei wang, by the way, you ought to enable docValues for your uniqueKey defined field. Recent versions of Solr are smart enough to avoid the stored document altogether if all requested fields are docValues.\n\u00a0Is there any performance implications to\u00a0enable docValues for the uniqueKey field, i.e retrieve via docValues vs stored field?\u00a0\n\n\u00a0 ",
            "author": "wei wang",
            "id": "comment-16341423"
        },
        {
            "date": "2018-01-26T20:33:23+0000",
            "content": "Is there any performance implications to enable docValues for the uniqueKey field, i.e retrieve via docValues vs stored field? \n\nGood ones   Retrieval should be much faster.  You'd have to re-index changing the schema in such a way.  And there is some extra storage for docValues of any field... but in the scheme of things it's not a big deal.  The optimization I'm referring to is SOLR-8344 which landed in 7.1.  The \"id\" field in the default configset ought to be docValues but it is not... I ought to file a JIRA for it. ",
            "author": "David Smiley",
            "id": "comment-16341547"
        },
        {
            "date": "2018-02-05T19:48:26+0000",
            "content": "FYI I attached the diff we made to DocsStreamer.\u00a0\u00a0 ",
            "author": "wei wang",
            "id": "comment-16352832"
        },
        {
            "date": "2018-03-15T02:21:17+0000",
            "content": "\n\nFYI I attached the diff we made to DocsStreamer.  \n\nThanks wei \u2013 unfortunately your patch breaks compilation of some other classes (on master) but also suffers from an NPE in the case where globs are used (ex: fl=*\n\nI started down the road of a more \"optimized\" patch with what i suggested above...\n\nI think the ideal \"fix\" is that the SolrReturnFields.getLuceneFieldNames() should get passed down all the way into convertLuceneDocToSolrDoc (or something we refactor it into) such that we do an runtime check of which list is smaller: SolrReturnFields.getLuceneFieldNames() or Document.getFields() \u2013 and then loop over that (smallest) list.\n\n...and i've currently got a patch which implements this along with a whitebox test to assert that the \"optimization\" is being used \u2013 but while working on it i realized this isn't actually an optimization...\n\n\nfor (String fname : returnFieldNames) {\n  for (IndexableField f : doc.getFields(fname)) {\n    // do stuff\n  }\n}\n\n\nThe problem is that Document isn't a Map \u2013 it doesn't have efficient lookup of the values associated with a fieldname.  In order to do the fieldname=>value[] lookup of doc.getFields(fname), it has to do an iterative scan all of the internal IndexableField (it can't even short circut out when it finds one because there could be multiples with the same name, and there's no garuntee they are in a predictible order)\n\nSo with this \"optimization\" we're actually introducing more loops over all the IndexableField instances.\n\nThe key reason wei was probably aple to see an improvement with hte change mentioned, is because at least when convertDocumentToSolrDocument is done, the final SolrDocumnet is as small as possible, so the subsequent scans in the ResponseWriter are faster.\n\nWe should be able to accomplish the same speed up \"safely\" by ensuring that when we do loop over the IndexableField instances, we check ReturnField.wantsField(fname)\n\nI'll work on a revised (and much simpler) patch tomorow.\n\n ",
            "author": "Hoss Man",
            "id": "comment-16399802"
        },
        {
            "date": "2018-03-15T23:35:43+0000",
            "content": "We should be able to accomplish the same speed up \"safely\" by ensuring that when we do loop over the IndexableField instances, we check ReturnField.wantsField(fname)\n\nIt turns out this is actually more nuanced...\n\n\n\twantsField(String) only returns true when the client wants the field \u2013 not any time the field is needed for other purposes (like transformers).  The correct logic is to do a hash lookup on the Set from ReturnFields.getLuceneFieldNames()\n\tOnce i made these changes and started doing more testing, i uncovered 2 independent bugs in some DocTransformers that are currently dependent on the existing sloppy code in convertLuceneDocToSolrDoc AND the documentCache being enabled in order to function properly...\n\t\n\t\tChildDocTransformerFactory assumes the uniqueKey field is always going to be available in converted SolrDocument w/o explicitly asking for them in a getExtraRequestFields() impl\n\t\tRawValueTransformerFactory assumes it doesn't need to do anything if the wt doesn't match it's configured type, and that those fields will implicitly be in the SolrDocument and get returned to the user automatically (as regular String values)\n\t\n\t\n\n\n\nI'm attaching a patch that fixes all of these, and creating new linked issue to track the sub-bugs.\n\nI'll do some more hammering on tests and aim to commit soon unless people have concerns. ",
            "author": "Hoss Man",
            "id": "comment-16401262"
        },
        {
            "date": "2018-03-17T15:34:25+0000",
            "content": "Nice thorough job hoss.\n\nOne little Q: \u00a0in convertLuceneDocToSolrDoc() (3 arg version), wouldn't it be simpler/clearer to remove the outer\u00a0if (null == fieldNamesNeeded) and instead guard the fieldNamesNeeded.contains call with fieldNamesNeeded == null || ....    ?  At least I think so but no big deal. ",
            "author": "David Smiley",
            "id": "comment-16403487"
        },
        {
            "date": "2018-03-17T23:41:53+0000",
            "content": "the two loops originally came from my earlier plan to loop over different lists in each case \u2013 but once i realized that wasn't an optimization i left the conditional in thinking that in the \"want all fields case\" (ie:\u00a0null == fieldNamesNeeded) it would be nice to optimize away the inner null check (and in the non-null case, nice to optimize away the null check) ... but you're probably right \u2013 it would probably be easier to undersatnd with a single loop, and since fieldNamesNeeded is final the JVM should be able to optimize it just as well.\n\nI'll change before committing ",
            "author": "Hoss Man",
            "id": "comment-16403795"
        },
        {
            "date": "2018-03-19T19:29:07+0000",
            "content": "Commit 8bd7e5c9d254c1d629a784e0b601885adea2f57b in lucene-solr's branch refs/heads/master from Chris Hostetter\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=8bd7e5c ]\n\nSOLR-11891: DocStreamer now respects the ReturnFields when populating a SolrDocument\nThis is an optimization that reduces the number of unneccessary fields a ResponseWriter will see if documentCache is used\n\nThis commit also includes fixes for SOLR-12107 & SOLR-12108 \u2013 two bugs that were previously dependent on the\nun-optimized behavior of DocStreamer in order to function properly.\n\n\n\tSOLR-12107: Fixed a error in [child] transformer that could ocur if documentCache was not used\n\tSOLR-12108: Fixed the fallback behavior of [raw] and [xml] transformers when an incompatble 'wt' was specified,\n  the field value was lost if documentCache was not used.\n\n ",
            "author": "ASF subversion and git services",
            "id": "comment-16405313"
        },
        {
            "date": "2018-03-19T21:31:16+0000",
            "content": "Commit 11af2144b66717f41e2fcb5c73c7059cf009a00a in lucene-solr's branch refs/heads/branch_7x from Chris Hostetter\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=11af214 ]\n\nSOLR-11891: DocStreamer now respects the ReturnFields when populating a SolrDocument\nThis is an optimization that reduces the number of unneccessary fields a ResponseWriter will see if documentCache is used\n\nThis commit also includes fixes for SOLR-12107 & SOLR-12108 \u2013 two bugs that were previously dependent on the\nun-optimized behavior of DocStreamer in order to function properly.\n\n\n\tSOLR-12107: Fixed a error in [child] transformer that could ocur if documentCache was not used\n\tSOLR-12108: Fixed the fallback behavior of [raw] and [xml] transformers when an incompatble 'wt' was specified,\n  the field value was lost if documentCache was not used.\n\n\n\n(cherry picked from commit 8bd7e5c9d254c1d629a784e0b601885adea2f57b) ",
            "author": "ASF subversion and git services",
            "id": "comment-16405484"
        },
        {
            "date": "2018-03-19T21:43:44+0000",
            "content": "Thanks Hoss!\u00a0 We will run a test with your patch to see the results.\u00a0\u00a0 ",
            "author": "wei wang",
            "id": "comment-16405497"
        }
    ]
}