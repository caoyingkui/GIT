{
    "id": "SOLR-4823",
    "title": "Split LBHttpSolrServer into two classes one for the solrj use case and one for the solr cloud use case",
    "details": {
        "affect_versions": "None",
        "status": "Open",
        "fix_versions": [],
        "components": [
            "SolrCloud"
        ],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "The LBHttpSolrServer has too many responsibilities. It could perhaps be broken into two classes, one in solrj to be used in the place of an external load balancer that balances across a known set of solr servers defined at construction time and one in solr core to be used by the solr cloud components that balances across servers dependant on the request.\n\nTo save code duplication, if much arises an abstract bass class could be introduced in to solrj.",
    "attachments": {
        "SOLR-4823.patch": "https://issues.apache.org/jira/secure/attachment/12583488/SOLR-4823.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13658700",
            "date": "2013-05-15T19:41:44+0000",
            "content": "Yea, load balancing should be handled separately for cloud.\n\nHow is LB for cloud handled today? Does it roundrobin between all noes in cluster, or does it intelligently load balance only between nodes which have a core in the collection? And what if you specify an explicit shard ID, will it load balance between all replicas in that shard only? "
        },
        {
            "author": "philip hoy",
            "id": "comment-13659365",
            "date": "2013-05-16T08:57:11+0000",
            "content": "The load balancer does indeed use round robin to pick a shard replica to forward the request on to. However it does not do the clever stuff to pick out what shards are possible candidates for a particular query. That role is fulfilled by the org.apache.solr.handler.component.HttpShardHandler. "
        },
        {
            "author": "philip hoy",
            "id": "comment-13659604",
            "date": "2013-05-16T14:53:56+0000",
            "content": "Here is a first stab at a refactoring, it is without any additional test coverage at present and may well be a bit too much to swallow. However I am happy to revisit it. \n\nInterestingly moving the cloud load balancing code out of LBHttpSolrServer did not affect any tests so perhaps that use case could do with some extra test coverage regardless of this patch. "
        },
        {
            "author": "philip hoy",
            "id": "comment-13660676",
            "date": "2013-05-17T12:33:16+0000",
            "content": "Added tests for the cloud load balancer. "
        },
        {
            "author": "Greg Pendlebury",
            "id": "comment-15642463",
            "date": "2016-11-06T21:44:06+0000",
            "content": "I know that my question will be off-topic for this particular issue, but it seems that it might be a viable launching point for a customization our team has been considering in-house. We were thinking of trying out the addition of one or more nodes in the cluster that had no allocated range hash in clusterstate (whether or not we needed to modify to code to achieve this we haven't looked yet).\n\nTheir purpose would be to act as search entry points for the cluster with more stable JVM performance (because they manage no lucene segments) as well as internalizing cluster security at the OS level. Right now, in a 200 replica cluster we need to let any/all SolrJ clients have access to the ZK ensemble as well as ports on every replica. It also makes managing threading (such as in the default http client thread pool) annoying to configure and test for performance.\n\nWith philip hoy's patch we could still make use of SolrJ, but just provide a small whitelist of our 'search nodes' and keep client-side requirements for searching very simple in terms of security and thread management. "
        }
    ]
}