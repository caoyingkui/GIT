{
    "id": "LUCENE-6664",
    "title": "Replace SynonymFilter with SynonymGraphFilter",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [
            "6.4",
            "7.0"
        ],
        "priority": "Major",
        "status": "Resolved",
        "type": "New Feature"
    },
    "description": "Spinoff from LUCENE-6582.\n\nI created a new SynonymGraphFilter (to replace the current buggy\nSynonymFilter), that produces correct graphs (does no \"graph\nflattening\" itself).  I think this makes it simpler.\n\nThis means you must add the FlattenGraphFilter yourself, if you are\napplying synonyms during indexing.\n\nIndex-time syn expansion is a necessarily \"lossy\" graph transformation\nwhen multi-token (input or output) synonyms are applied, because the\nindex does not store posLength, so there will always be phrase\nqueries that should match but do not, and then phrase queries that\nshould not match but do.\nhttp://blog.mikemccandless.com/2012/04/lucenes-tokenstreams-are-actually.html\ngoes into detail about this.\n\nHowever, with this new SynonymGraphFilter, if instead you do synonym\nexpansion at query time (and don't do the flattening), and you use\nTermAutomatonQuery (future: somehow integrated into a query parser),\nor maybe just \"enumerate all paths and make union of PhraseQuery\", you\nshould get 100% correct matches (not sure about \"proper\" scoring\nthough...).\n\nThis new syn filter still cannot consume an arbitrary graph.",
    "attachments": {
        "usa_flat.png": "https://issues.apache.org/jira/secure/attachment/12747248/usa_flat.png",
        "LUCENE-6664.patch": "https://issues.apache.org/jira/secure/attachment/12743916/LUCENE-6664.patch",
        "usa.png": "https://issues.apache.org/jira/secure/attachment/12747249/usa.png"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14616408",
            "author": "Michael McCandless",
            "date": "2015-07-07T09:19:45+0000",
            "content": "Patch, still work in progress.  It includes the FlattenGraphFilter\nfrom LUCENE-6638.\n\nI put everything in sandbox for now, so I could add a test case that\nTermAutomatonQuery works correctly for query-time syn expansion.  But\nthis added a dep from sandbox on analyzers ... I think I'll move the\nnew filters back to analyzers module and comment on the TAQ test case\nas an example. "
        },
        {
            "id": "comment-14642099",
            "author": "Michael McCandless",
            "date": "2015-07-26T21:30:54+0000",
            "content": "Example syn graph, and flattened version. "
        },
        {
            "id": "comment-14642104",
            "author": "Michael McCandless",
            "date": "2015-07-26T21:42:59+0000",
            "content": "New patch, fixing all nocommits, folding in all the nice test cases from LUCENE-6582 (thanks Ian Ribas!), fixing some offsets bugs.\n\nI think it's finally ready.  This issue absorbs LUCENE-6638.\n\nI also wrote a fun test method (toDot(TokenStream)) that converts a TokenStream to a dot file which you can then render with graphviz.  E.g. here's the un-flattened expansion for various syns of usa:\n\n\n\nand the corresponding flattened version:\n\n\n\n(red arcs are the inserted synonym tokens)\n\nWith SynonymGraphFilter, multi token synonyms can finally be correctly represented in the token stream, and using query-time synonyms with either TermAutomatonQuery or some other approach (e.g. expanding all paths and making OR of PhraseQuery), the correct results should be returned.  Index-time synonyms will still be incorrect (fail to match some phrase queries, and incorrectly match other phrase queries) since we don't index the PosLenAttribute. "
        },
        {
            "id": "comment-14642694",
            "author": "Robert Muir",
            "date": "2015-07-27T13:02:03+0000",
            "content": "So is positionlength still (ab)used as really a node ID? Should it use another attribute instead? "
        },
        {
            "id": "comment-14642761",
            "author": "Michael McCandless",
            "date": "2015-07-27T14:15:35+0000",
            "content": "Rob had a good idea here, to make both SynonymGraphFilter and FlattenGraphFilter package private, and only expose SynonymFilter which under the hood is appending both of these filters.\n\nThis is safer (you can't forget to append the FlatttenGraphFilter), but it means users can't play with query-time synonyms until we open this up again.  But I think it's a good baby step ... I'll explore it. "
        },
        {
            "id": "comment-14642765",
            "author": "Michael McCandless",
            "date": "2015-07-27T14:19:54+0000",
            "content": "So is positionlength still (ab)used as really a node ID?\n\nYeah: it \"generalizes\" position as a nodeID, like Kuromoji.\n\nShould it use another attribute instead?\n\nMaybe we could do that?  FromNodeAtt/ToNodeAtt?  And fix SausageFilter to only look at those?  Since we'll make both of these package private it would be completely safe, at least until we open them up for query-time synonyms... "
        },
        {
            "id": "comment-14642768",
            "author": "Michael McCandless",
            "date": "2015-07-27T14:22:09+0000",
            "content": "For query-time integration (a separate future issue!) I think we could use the existing TokenStreamToTermAutomatonQuery, either as-is (producing a TermAutomatonQuery by iterating all tokens from a TokenStream), or forked to instead produce union of PhraseQuery, or MultiPhraseQuery if the graph \"matches\". "
        },
        {
            "id": "comment-14642778",
            "author": "Robert Muir",
            "date": "2015-07-27T14:26:03+0000",
            "content": "\nYeah: it \"generalizes\" position as a nodeID, like Kuromoji.\n\nWait i'm confused, positions from kuromoji always seem normal to me. I think the difference is that its decompounding is a subset of the things that synonymfilter can do? "
        },
        {
            "id": "comment-14642842",
            "author": "Michael McCandless",
            "date": "2015-07-27T15:12:56+0000",
            "content": "I think the difference is that its decompounding is a subset of the things that synonymfilter can do?\n\nHmm you're right: when Kuromoji makes a \"side path\", it's always just a single token.\n\nI.e., all it really \"plays with\" is posLenAtt, I think.  So it also still only makes sausage graphs... "
        },
        {
            "id": "comment-14644039",
            "author": "Michael McCandless",
            "date": "2015-07-28T08:07:30+0000",
            "content": "New patch with Rob's idea: I made the new SynonymGraphFilter and\nSausageFilter package private, and replaced the old SynonymFilter with\nthese two filters.\n\nBut TestSynonymMapFilter (the existing unit test) fails, because there\nare some changes in behavior with the new filter:\n\n\n\tSyn output order is different: with the new syn filter, the syn\n    comes out before the original token.  This is necessary to ensure\n    offsets never go backwards...\n\n\n\n\n\tWhen there are more output tokens for a syn than input tokens,\n    then new syn filter makes new positions for the extra tokens, but\n    the old one didn't.\n\n\n\n\n\tThe new syn filter does more captureState() calls\n\n\n\nI think we need to keep the old behavior available, maybe using a\nVersion constant or a separate class (SynFilterPre53,\nLegacySynFilter) or something? "
        },
        {
            "id": "comment-14648907",
            "author": "Michael McCandless",
            "date": "2015-07-31T08:15:39+0000",
            "content": "Thinking about this more, I think it's sort of silly to only expose the \"always flattened synonym filter\" because that's really no better than today: you still can't search multi-token synonyms correctly, and there are small differences in how the graph corruption happens.\n\nSo I'd like to go back to the 2nd patch, where both filters are public.  I'll mark them experimental... "
        },
        {
            "id": "comment-14650672",
            "author": "Michael McCandless",
            "date": "2015-08-02T09:40:39+0000",
            "content": "New patch, making the new filters public and experimental again.\n\nI also improved the naming.\n\nRobert Muir is this OK?  Or do you think which attributes to use should block committing this?  I can also put this in sandbox? "
        },
        {
            "id": "comment-14650697",
            "author": "Robert Muir",
            "date": "2015-08-02T11:17:13+0000",
            "content": "I dont agree with changing the.meqning of these posinc/poslen atts in this way. So i dont understand the point of committjng it this way. \n\nThis isnt a contajned commit, it tries to alter their semantics. Where zometime its now this crazy nodeid. Positions are.now ununderstandable in our analysis api. Is this what we want? "
        },
        {
            "id": "comment-14651107",
            "author": "Michael McCandless",
            "date": "2015-08-02T17:59:42+0000",
            "content": "I dont agree with changing the.meqning of these posinc/poslen atts in this way.\n\nActually, I think this is a natural generalization of the existing posInc/posLen atts, and we should let graph filters use these attributes.\n\nI think making separate \"graph attributes\" is a short term hack that will just cause future problems.\n\nWon't it make interop b/w \"graph\" and \"non-graph\" filters a nightmare?  E.g. we'd need separate StopGraphFilter and a StopFilter, LengthGraphFilter/LengthFilter, etc.?\n\nSo i dont understand the point of committjng it this way.\n\nOK I won't commit this.\n\nReally until we have a complete query-time solution here, I suppose this patch is essentially pointless.\n\nBut I was hoping by making it available, other devs would see it and try to innovate on the query-time side of things.\n\nAre you also -1 on committing this to sandbox? "
        },
        {
            "id": "comment-14651158",
            "author": "Robert Muir",
            "date": "2015-08-02T19:55:28+0000",
            "content": "OK, if you say its a generalization, then I am ok.  But you are saying current code in queryparsers won't do the wrong thing?: I know its a tough one, since it already is somewhat wrong today!? I'm just asking because we dont want to make it worse or more confusing.\n "
        },
        {
            "id": "comment-14658267",
            "author": "Michael McCandless",
            "date": "2015-08-05T14:17:17+0000",
            "content": "I pushed this out to 5.4, we shouldn't shove it in to 5.3.\n\nThanks Robert Muir, I'll try to update the javadocs for posInc/posLenAtt.\n\nI think the current code in queryparsers simply ignores posLen, right?  (just like indexer)  Which should \"just\" mean positions get messed up in the same way as indexer, so some phrase queries will work but others won't... "
        },
        {
            "id": "comment-14658311",
            "author": "Robert Muir",
            "date": "2015-08-05T14:44:52+0000",
            "content": "but they use posInc! so i think its important not to break the semantics. I'm super-against changing this to \"nodeID\" because it will break code like that. We need a new attribute for that. "
        },
        {
            "id": "comment-14658347",
            "author": "Michael McCandless",
            "date": "2015-08-05T15:14:25+0000",
            "content": "I'm super-against changing this to \"nodeID\" because it will break code like that. We need a new attribute for that.\n\nOK I won't commit this.\n\nI'll open a new issue to figure out whether/how/should we can implement graph filters...\n\nReally, unless we can figure out how query parsers can make use of graph filters like this one, there's not much point in committing this. "
        },
        {
            "id": "comment-14658424",
            "author": "Michael McCandless",
            "date": "2015-08-05T15:56:05+0000",
            "content": "We can't move forward here until we figure out how to keep back-compat ... "
        },
        {
            "id": "comment-14934080",
            "author": "Matt Davis",
            "date": "2015-09-28T21:30:05+0000",
            "content": "Could this be a candidate for 6.0 (breaking back-compat)? "
        },
        {
            "id": "comment-14941139",
            "author": "Michael McCandless",
            "date": "2015-10-02T13:34:45+0000",
            "content": "Could this be a candidate for 6.0 (breaking back-compat)?\n\nProbably not ... I think Robert Muir's objections would still hold for 6.0 as well?\n\nThe patch is fairly standalone so you could just use this new token filter (SynonymGraphFilter) in your own world? "
        },
        {
            "id": "comment-14942187",
            "author": "Paul Elschot",
            "date": "2015-10-03T08:48:03+0000",
            "content": "From the SausageGraphFilter: Lucene cannot yet index an arbitrary token graph.\n\nPerhaps positional joins (LUCENE-5627) can help here.This indexes joins between non-decreasing positions of any field in the same document, and allows the joins to be queried. However I have the impression that these positional joins bring more complexity than what is needed here.\n\nOne basic mechanism for the positional joins is a non decreasing series of positions. (Currently these are in payloads, I'm considering docvalues). These are accessed by both index and value, and used at query time to jump for example from one field to another.\nAnother basic mechanism there is a hierarchy between the positions of a single field, for example for nested XML element names. This hierarchy is probably too restrictive here.\n\nHow arbitrary are the token graphs here?\n "
        },
        {
            "id": "comment-14942642",
            "author": "Michael McCandless",
            "date": "2015-10-04T11:17:09+0000",
            "content": "How arbitrary are the token graphs here?\n\nWell, the token graphs produced by SynonymGraphFilter (this patch) don't really have any more expressibility over what existing tokenizers can do (e.g. Kuromoji/JapaneseTokenizer).  They are acyclic, and are enumerated under similar constraints as the automaton API, i.e. you must add all tokens leaving a given position before moving to the next position.\n\nI think what must be controversial about this patch is that this new filter can create new positions, which is necessary to fix bugs in the old SynonymFilter to correctly handle syns that expand to more tokens than they consumed, e.g. \"dns -> domain name system\".  Because otherwise you cannot distinguish the output of SynonymGraphFilter from e.g. JapaneseTokenizer: they both produce graphs with side paths.\n\nProbably LUCENE-5012 is the only realistic way to move forward here: the synonym filter on that branch fixes the bugs that SynonymGraphFilter (this patch) also fixes, and then fixes additional bugs so that it can consume an incoming graph as well.  E.g. on that branch you could apply synonyms to the graph output from JapaneseTokenizer). "
        },
        {
            "id": "comment-14942650",
            "author": "Robert Muir",
            "date": "2015-10-04T11:53:49+0000",
            "content": "I thought I already explained my concerns well. I guess I will try yet again...\n\n\n\n\tI think its a huge, huge break to modify the semantics of existing token attributes like position increment, to mean something very different (nodeID), that seems like an awkward fit, wedged in.\n\tIf position increment/length no longer have meaning and are just confusing ways of storing node ID, that's a huge usability issue: lets use a clean slate of attributes instead of abusing in that way...\n\tI'm concerned about complexity: today the simple case (posInc + posLen) is hard enough to understand, but doable. I feel like going this route, pushes the complexity onto the simple case, and that worries me a lot. A lot of people just simply will not have graphs! Should all of our analysis components really be required to support them? Maybe we should really be putting the effort into the ability to use alternative analysis APIs, so that we can have a \"graph analysis\" api that just works totally different from the tokenstream one, and so on.\n\n "
        },
        {
            "id": "comment-14942686",
            "author": "Jack Krupansky",
            "date": "2015-10-04T15:30:51+0000",
            "content": "Hey Michael McCandless, don't get discouraged, this was a very valuable exercise. I am a solid proponent of getting multi-term synonyms working in a full and robust manner, but I recognize that they just don't fit in cleanly with the existing flat token stream architecture. That's life. In any case, don't give up on this long-term effort.\n\nMaybe the best thing for now is to retain the traditional flat synonym filter for compatibility, fully add the new SynonymGraphFilter, and then add the optional ability to enable graph support in the main Lucene query parser. (Alas, Solr, has its own fork of the Lucene query parser.) Support within phrase queries is the tricky part.\n\nIt would also be good to address the issue with non-phrase terms being analyzed separately - the query parser should recognize adjacent terms without operators are analyze as a group so that multi-token synonyms can be recognized. "
        },
        {
            "id": "comment-14942705",
            "author": "Michael McCandless",
            "date": "2015-10-04T16:35:27+0000",
            "content": "don't get discouraged,\n\nI'm not discouraged.\n\nMany issues have been blocked before because they are controversial.  I resolved this as \"later\" because that's the reality of what's happening: it will be a long time until we can fix these bugs in SynonymFilter.\n\nThis is just how Apache open source works: it's inherently a conservative / design by committee development model, and one veto to a change blocks it.  Only changes everyone agrees on are allowed.  The more successful the project, the more conservative its development becomes.\n\nThe few users who are affected by the buggy SynonymFilter we have today can always test SynonymGraphFilter in this patch with query-time synonyms to confirm it fixes their phrase query bugs (please report back!).\n\nBut users are definitely affected by these bugs today, e.g. see https://lucidworks.com/blog/solution-for-multi-term-synonyms-in-lucenesolr-using-the-auto-phrasing-tokenfilter where there's lots of exploration on how to work around the bugs that this patch in fact fixes correctly, if you are willing/able to use query-time synonyms.\n\nMy original blog post on this topic also explains the bugs: http://blog.mikemccandless.com/2012/04/lucenes-tokenstreams-are-actually.html\n\nI recognize that they just don't fit in cleanly with the existing flat token stream architecture.\n\nI disagree.\n\nThis is exactly why we added PosLengthAttribute originally, and e.g. Kuromoji makes use of that very well: it produces a graph token stream.  I think there is an overblown/irrational fear of graph tokenizers at work here...  maybe we should remove all graph tokenizers/token filters, along with PosLengthAttribute?  To arbitrarily declare that only tokenizers (not token filters) can create new positions makes no sense to me: the resulting output from the tokenizer or the token filter is indistinguishable.\n\nFurthermore, the graph flattening filter in this patch gives good index-time back compat if you apply your synonyms during indexing, while also enabling bug-free query time multi-token synonyms.\n\nenable graph support in the main Lucene query parser.\n\nRight, this is a missing part now.  We do have TermAutomatonQuery, to execute the full token graph correctly, but we still have to fix the query parser to somehow produce that query when it \"sees\" a graph when tokenizing a phrase query?  Maybe that's not so hard, e.g. we could always create a TermAutomatonQuery but fix that query to rewrite to a simple PhraseQuery or MultiPhraseQuery if it was an \"easy\" case?\n\n(Alas, Solr, has its own fork of the Lucene query parser.) \n\nHmm, why?  There are so many query parsers now ...\n\nIt would also be good to address the issue with non-phrase terms being analyzed separately \n\nHmm what does this mean?  I thought the query parsers analyze whole text chunks between operators, so they could already apply multi-token synonyms not inside a phrase query? "
        },
        {
            "id": "comment-14942765",
            "author": "Michael McCandless",
            "date": "2015-10-04T18:32:43+0000",
            "content": "Maybe that's not so hard, e.g. we could always create a TermAutomatonQuery but fix that query to rewrite to a simple PhraseQuery or MultiPhraseQuery if it was an \"easy\" case?\n\nI opened LUCENE-6824 for this ... seems like a low hanging fruit to make query parser integration easier. "
        },
        {
            "id": "comment-15627246",
            "author": "Steve Rowe",
            "date": "2016-11-02T01:00:43+0000",
            "content": "Michael McCandless, I think your repurposing of posincr/poslen on this issue (as node ids) is to enable non-lossy query parser interpretation of token streams, so that e.g. tokens from overlapping phrases aren't inappropriately interleaved in generated queries, like your wtf example on LUCENE-6582):\n\n\n if I have these synonyms:\n\nwtf --> what the fudge\nwtf --> wow that's funny\n\n\nAnd then I'm tokenizing this:\n\nwtf happened\n\n\nBefore this change (today) I get this crazy sausage incorrectly\nmatching phrases like \"wtf the fudge\" and \"wow happened funny\":\n\nBut after this change, the expanded synonyms become separate paths in\nthe graph right? So it will look like this?:\n\n\nAn alternative implementation idea I had, which would not change posincr/poslen semantics, is to add a new attribute encoding an entity ID.  Graph-aware producers would mark tokens that should be treated as a sequence with the same entity ID, and graph-aware consumers would use the entity ID to losslessly interpret the resulting graph.  Here's the wtf example using this scheme:\n\n\n\n\ntoken\nposInc\nposLen\nentityID\n\n\nwtf\n1\n3\n0\n\n\nwhat\n0\n1\n1\n\n\nwow\n0\n1\n2\n\n\nthe\n1\n1\n1\n\n\nthat's\n0\n1\n2\n\n\nfudge\n1\n1\n1\n\n\nfunny\n0\n1\n2\n\n\nhappened\n1\n1\n3\n\n\n\n\n\nNo flattening stage is required.  Non-graph-aware components aren't affected (I think).  And handling QueryParser.autoGeneratePhraseQueries() properly (see LUCENE-7533) would be easy: if more than one token has the same entityID, then it should be a phrase when autoGeneratePhraseQueries=true.\n\nI haven't written any code yet, so I'm not sure this idea is feasible.\n\nThoughts? "
        },
        {
            "id": "comment-15633726",
            "author": "Michael McCandless",
            "date": "2016-11-03T18:24:34+0000",
            "content": "This is a neat approach!\n\nI like that no explicit flattening stage is required, though I think it means graph-producing stages must do their own flattening, to resolve side-paths into equivalent flattened positions?  But maybe we could factor out a little thingy to share code.\n\nIf a side path spawns another side path it would get a new entity ID right?\n\nAnd then, a node ID is really just the tuple of (entityID, position), and you can losslessly reconstruct the graph. "
        },
        {
            "id": "comment-15633856",
            "author": "Steve Rowe",
            "date": "2016-11-03T19:03:12+0000",
            "content": "I think it means graph-producing stages must do their own flattening, to resolve side-paths into equivalent flattened positions? But maybe we could factor out a little thingy to share code.\n\n+1\n\nIf a side path spawns another side path it would get a new entity ID right?\n\nI think so, yes, though that points to a problem: new entity IDs would have to be produced without knowing which ones have already been used - otherwise a filter would have to consume the whole stream before it could assign a new one.  I was assuming it would be one-up integers, but that won't work in this case.  Hmm. "
        },
        {
            "id": "comment-15663544",
            "author": "ASF subversion and git services",
            "date": "2016-11-14T11:33:23+0000",
            "content": "Commit a86f807685403537c20aa697b7c7e06bd97cbdf9 in lucene-solr's branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a86f807 ]\n\nLUCENE-6664: add getter "
        },
        {
            "id": "comment-15663705",
            "author": "ASF subversion and git services",
            "date": "2016-11-14T11:40:02+0000",
            "content": "Commit 3046f86ce7344dda560ecd925eb22fc05b6d5f1f in lucene-solr's branch refs/heads/branch_6x from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=3046f86 ]\n\nLUCENE-6664: add getter "
        },
        {
            "id": "comment-15668108",
            "author": "Michael McCandless",
            "date": "2016-11-15T20:00:18+0000",
            "content": "I'm re-opening this issue: I think my original patch here is a good way to move forward.  It is a simple, backwards compatible way, for token streams to naturally produce graphs, and to empower token filters to create new positions.\n\nExisting token streams, that produce posInc=0 or posInc=1 and posLength=1 tokens, naturally work the way they do today with this change, producing \"sausage\" graphs.\n\nGraph-aware token streams, like the new SynonymGraphFilter here, the Kuromoji JapaneseTokenizer, and WordDelimiterFilter if we improve it, can produce correct graphs which can be used at query time to make accurate queries.\n\nToday, multi-word synonyms are buggy (see https://lucidworks.com/blog/2014/07/12/solution-for-multi-term-synonyms-in-lucenesolr-using-the-auto-phrasing-tokenfilter and http://blog.mikemccandless.com/2012/04/lucenes-tokenstreams-are-actually.html), missing hits that should match, and incorrectly returning hits that should not match, for queries that involve the synonyms.  With this change, if you use query time synonym expansion, along with separate improvements to query parser, it would fix the bug.  The required changes to query parsing are surprisingly contained ... see https://github.com/elastic/elasticsearch/pull/21517 as an example approach.\n\nI am not proposing, here, that the Lucene index format be changed to support indexing a position graph.  Instead, I'm proposing that we make it possible for query-time position graphs to work correctly, so multi-token synonyms are no longer buggy, and I think this is a good way to make that happen. "
        },
        {
            "id": "comment-15669749",
            "author": "Tommaso Teofili",
            "date": "2016-11-16T07:51:27+0000",
            "content": "\n I'm proposing that we make it possible for query-time position graphs to work correctly, so multi-token synonyms are no longer buggy, and I think this is a good way to make that happen.\n\n+1  "
        },
        {
            "id": "comment-15670466",
            "author": "David Smiley",
            "date": "2016-11-16T13:57:36+0000",
            "content": "+1.  This should be experimental for now as people begin to try this out.\n\nI don't think this patch abuses the semantics of posInc or posLen.  It's a shame most (all?) filters don't handle input posLen != 1 properly but that's a separate issue. "
        },
        {
            "id": "comment-15762891",
            "author": "Michael McCandless",
            "date": "2016-12-20T01:37:06+0000",
            "content": "Here's another patch, just modernizing the last one to apply to\ncurrent master, renaming SausageGraphFilter to\nFlattenGraphFilter and fixing a few javadocs.  I think it's\nready. "
        },
        {
            "id": "comment-15771034",
            "author": "ASF subversion and git services",
            "date": "2016-12-22T20:39:29+0000",
            "content": "Commit c0467bb929133605fca2bc63fe1ebba758332d41 in lucene-solr's branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c0467bb ]\n\nLUCENE-6664: add SynonymGraphFilter for correct multi-token synonym handling "
        },
        {
            "id": "comment-15771064",
            "author": "ASF subversion and git services",
            "date": "2016-12-22T20:55:08+0000",
            "content": "Commit 68db0334089b3cb052d660d143d06aaedd7b922c in lucene-solr's branch refs/heads/branch_6x from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=68db033 ]\n\nLUCENE-6664: add SynonymGraphFilter for correct multi-token synonym handling "
        },
        {
            "id": "comment-15778606",
            "author": "Steve Rowe",
            "date": "2016-12-26T16:28:57+0000",
            "content": "Looks like the new FlattenGraphFilter is implicated in this reproducing TestRandomChains failure from Policeman Jenkins https://builds.apache.org/job/Lucene-Solr-NightlyTests-master/1193/:\n\n\n   [junit4] Suite: org.apache.lucene.analysis.core.TestRandomChains\n   [junit4]   2> TEST FAIL: useCharFilter=true text='\\ud991\\udc33\\u0662 vb wlvvo \\ufe0f\\ufe04\\ufe01\\ufe05\\ufe00\\ufe07 ]u[{1,5 ntwwqlyvt \\ua4ed\\ua4d2\\ua4ff\\ua4fd\\ua4ef\\ua4db\\ua4e3\\ua4e4\\ua4db\\ua4e2\\ua4ea jlrzerz'\n   [junit4]   2> Exception from random analyzer: \n   [junit4]   2> charfilters=\n   [junit4]   2> tokenizer=\n   [junit4]   2>   org.apache.lucene.analysis.wikipedia.WikipediaTokenizer()\n   [junit4]   2> filters=\n   [junit4]   2>   org.apache.lucene.analysis.commongrams.CommonGramsFilter(ValidatingTokenFilter@68052231 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,flags=0, [wfo, i, ecngk, lntfhzycu, f])\n   [junit4]   2>   org.apache.lucene.analysis.miscellaneous.KeywordRepeatFilter(ValidatingTokenFilter@4c507013 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,flags=0,keyword=false)\n   [junit4]   2>   org.apache.lucene.analysis.synonym.FlattenGraphFilter(ValidatingTokenFilter@27c78e22 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,flags=0,keyword=false)\n   [junit4]   2> offsetsAreCorrect=false\n   [junit4]   2> NOTE: download the large Jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestRandomChains -Dtests.method=testRandomChainsWithLargeStrings -Dtests.seed=740BB1C4895371B0 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/jenkins-slave/workspace/Lucene-Solr-NightlyTests-master/test-data/enwiki.random.lines.txt -Dtests.locale=es-CO -Dtests.timezone=EET -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1\n   [junit4] ERROR   14.8s J0 | TestRandomChains.testRandomChainsWithLargeStrings <<<\n   [junit4]    > Throwable #1: java.lang.IllegalArgumentException: startOffset must be non-negative, and endOffset must be >= startOffset, startOffset=24,endOffset=22\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([740BB1C4895371B0:1E500ED5D01D5143]:0)\n   [junit4]    > \tat org.apache.lucene.analysis.tokenattributes.PackedTokenAttributeImpl.setOffset(PackedTokenAttributeImpl.java:107)\n   [junit4]    > \tat org.apache.lucene.analysis.synonym.FlattenGraphFilter.releaseBufferedToken(FlattenGraphFilter.java:237)\n   [junit4]    > \tat org.apache.lucene.analysis.synonym.FlattenGraphFilter.incrementToken(FlattenGraphFilter.java:264)\n   [junit4]    > \tat org.apache.lucene.analysis.ValidatingTokenFilter.incrementToken(ValidatingTokenFilter.java:67)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:724)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:635)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:533)\n   [junit4]    > \tat org.apache.lucene.analysis.core.TestRandomChains.testRandomChainsWithLargeStrings(TestRandomChains.java:869)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]   2> NOTE: leaving temporary files on disk at: /x1/jenkins/jenkins-slave/workspace/Lucene-Solr-NightlyTests-master/checkout/lucene/build/analysis/common/test/J0/temp/lucene.analysis.core.TestRandomChains_740BB1C4895371B0-001\n   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {dummy=PostingsFormat(name=LuceneVarGapFixedInterval)}, docValues:{}, maxPointsInLeafNode=1442, maxMBSortInHeap=6.705070576143851, sim=RandomSimilarity(queryNorm=true): {dummy=DFR I(n)L1}, locale=es-CO, timezone=EET\n   [junit4]   2> NOTE: Linux 3.13.0-85-generic amd64/Oracle Corporation 1.8.0_102 (64-bit)/cpus=4,threads=1,free=136682616,total=285736960\n   [junit4]   2> NOTE: All tests run in this JVM: [TestRussianLightStemFilter, TestEnglishAnalyzer, EdgeNGramTokenFilterTest, TestSwedishAnalyzer, TestHindiFilters, TestHindiNormalizer, TestHungarianLightStemFilterFactory, TestPorterStemFilter, TestCondition, TestTruncateTokenFilterFactory, TestCollationKeyAnalyzer, TestSpanishAnalyzer, TestHTMLStripCharFilterFactory, TestArabicFilters, TestFactories, TestIrishLowerCaseFilterFactory, TestBrazilianAnalyzer, TestLatvianAnalyzer, TestEscaped, TestPortugueseLightStemFilter, TestElisionFilterFactory, TestHungarianAnalyzer, TestGreekLowerCaseFilterFactory, TestElision, TestCustomAnalyzer, TestTurkishLowerCaseFilterFactory, TestFullStrip, TestSpanishLightStemFilterFactory, TestNorwegianMinimalStemFilterFactory, QueryAutoStopWordAnalyzerTest, NGramTokenizerTest, WikipediaTokenizerTest, TestIndonesianStemFilterFactory, TestBasqueAnalyzer, DateRecognizerFilterFactoryTest, TestNorwegianLightStemFilter, TestFrenchMinimalStemFilterFactory, TestCommonGramsFilterFactory, TestPersianNormalizationFilterFactory, TestTwoSuffixes, TestIndonesianStemmer, TypeAsPayloadTokenFilterTest, TestFrenchLightStemFilterFactory, TestThaiAnalyzer, TestCaseSensitive, TestRandomChains]\n   [junit4] Completed [133/275 (1!)] on J0 in 121.18s, 2 tests, 1 error <<< FAILURES!\n\n "
        },
        {
            "id": "comment-15779143",
            "author": "Michael McCandless",
            "date": "2016-12-26T22:59:30+0000",
            "content": "Thanks Steve Rowe, I'll look... "
        },
        {
            "id": "comment-15781557",
            "author": "Steve Rowe",
            "date": "2016-12-27T23:51:53+0000",
            "content": "Another TestRandomChains failure, from https://builds.apache.org/job/Lucene-Solr-NightlyTests-6.x/239/:\n\n\nChecking out Revision 9dde8a30303de4bce5da45189219dd6150252b29 (refs/remotes/origin/branch_6x)\n[...]\n   [junit4] Suite: org.apache.lucene.analysis.core.TestRandomChains\n   [junit4]   2> TEST FAIL: useCharFilter=true text='ivi[q.(k--r f\\u0002\\uf672o\\u983c'\n   [junit4]   2> Exception from random analyzer: \n   [junit4]   2> charfilters=\n   [junit4]   2>   org.apache.lucene.analysis.charfilter.HTMLStripCharFilter(java.io.StringReader@261ff7a0)\n   [junit4]   2> tokenizer=\n   [junit4]   2>   org.apache.lucene.analysis.wikipedia.WikipediaTokenizer()\n   [junit4]   2> filters=\n   [junit4]   2>   org.apache.lucene.analysis.StopFilter(ValidatingTokenFilter@62f3af70 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,flags=0, [hejalskyy, d, skap, nfd, nirasnsmg, hmdqqn])\n   [junit4]   2>   org.apache.lucene.analysis.synonym.FlattenGraphFilter(ValidatingTokenFilter@1a9001e5 term=,bytes=[],startOffset=0,endOffset=0,positionIncrement=1,positionLength=1,type=word,flags=0)\n   [junit4]   2> offsetsAreCorrect=false\n   [junit4]   2> NOTE: download the large Jenkins line-docs file by running 'ant get-jenkins-line-docs' in the lucene directory.\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestRandomChains -Dtests.method=testRandomChains -Dtests.seed=127E19CE02B54D17 -Dtests.multiplier=2 -Dtests.nightly=true -Dtests.slow=true -Dtests.linedocsfile=/home/jenkins/jenkins-slave/workspace/Lucene-Solr-NightlyTests-6.x/test-data/enwiki.random.lines.txt -Dtests.locale=zh-HK -Dtests.timezone=America/Virgin -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] ERROR   62.4s J1 | TestRandomChains.testRandomChains <<<\n   [junit4]    > Throwable #1: java.lang.IllegalArgumentException: startOffset must be non-negative, and endOffset must be >= startOffset, startOffset=4,endOffset=3\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([127E19CE02B54D17:2F9F30AF45A750D7]:0)\n   [junit4]    > \tat org.apache.lucene.analysis.tokenattributes.PackedTokenAttributeImpl.setOffset(PackedTokenAttributeImpl.java:107)\n   [junit4]    > \tat org.apache.lucene.analysis.synonym.FlattenGraphFilter.releaseBufferedToken(FlattenGraphFilter.java:237)\n   [junit4]    > \tat org.apache.lucene.analysis.synonym.FlattenGraphFilter.incrementToken(FlattenGraphFilter.java:264)\n   [junit4]    > \tat org.apache.lucene.analysis.ValidatingTokenFilter.incrementToken(ValidatingTokenFilter.java:67)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:723)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:634)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:532)\n   [junit4]    > \tat org.apache.lucene.analysis.core.TestRandomChains.testRandomChains(TestRandomChains.java:842)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]   2> NOTE: leaving temporary files on disk at: /x1/jenkins/jenkins-slave/workspace/Lucene-Solr-NightlyTests-6.x/checkout/lucene/build/analysis/common/test/J1/temp/lucene.analysis.core.TestRandomChains_127E19CE02B54D17-001\n   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene62): {dummy=PostingsFormat(name=Memory doPackFST= true)}, docValues:{}, maxPointsInLeafNode=772, maxMBSortInHeap=6.693205231616328, sim=RandomSimilarity(queryNorm=false,coord=yes): {dummy=DFR I(F)LZ(0.3)}, locale=zh-HK, timezone=America/Virgin\n   [junit4]   2> NOTE: Linux 3.13.0-85-generic amd64/Oracle Corporation 1.8.0_102 (64-bit)/cpus=4,threads=1,free=177327976,total=255852544\n   [junit4]   2> NOTE: All tests run in this JVM: [TestPatternTokenizerFactory, TestCircumfix, TestReverseStringFilterFactory, TestSnowball, TestIrishAnalyzer, TestBulgarianAnalyzer, TestHomonyms, TestKeywordRepeatFilter, TestPrefixAwareTokenFilter, CommonGramsFilterTest, TestHyphenationCompoundWordTokenFilterFactory, TestSoraniAnalyzer, TestGermanStemFilterFactory, TestEmptyTokenStream, TestIndicNormalizer, TestTurkishLowerCaseFilter, TestGalicianMinimalStemFilterFactory, TestDecimalDigitFilterFactory, TestLatvianStemmer, TestItalianLightStemFilter, TestKeepWordFilter, TestLithuanianStemming, TestKeepFilterFactory, TestPortugueseMinimalStemFilter, TestAnalyzers, TestAlternateCasing, TestSoraniStemFilter, TestApostropheFilterFactory, TestDictionary, TestCodepointCountFilterFactory, TestDanishAnalyzer, TestRomanianAnalyzer, TestPortugueseMinimalStemFilterFactory, TestArabicNormalizationFilter, TestLimitTokenOffsetFilterFactory, TestZeroAffix, DateRecognizerFilterTest, TestGermanLightStemFilter, TestCJKAnalyzer, TestMorphData, TestBulgarianStemFilterFactory, TestSynonymGraphFilter, TestGermanNormalizationFilterFactory, TestBulgarianStemmer, DelimitedPayloadTokenFilterTest, TestStrangeOvergeneration, TestFactories, TestRandomChains]\n   [junit4] Completed [141/275 (1!)] on J1 in 119.08s, 2 tests, 1 error <<< FAILURES!\n\n "
        },
        {
            "id": "comment-15782494",
            "author": "Michael McCandless",
            "date": "2016-12-28T09:32:14+0000",
            "content": "Thanks Steve Rowe, I'll have a look, but likely not until I'm back from vacation next year ... "
        },
        {
            "id": "comment-15794874",
            "author": "ASF subversion and git services",
            "date": "2017-01-03T11:47:59+0000",
            "content": "Commit f6fb6941bb62f8d47d653b2ed187ffa0107cd5c5 in lucene-solr's branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f6fb694 ]\n\nLUCENE-6664: be more robust to broken token stream offsets "
        },
        {
            "id": "comment-15794877",
            "author": "ASF subversion and git services",
            "date": "2017-01-03T11:49:10+0000",
            "content": "Commit c35fbbd328687f5e309fcb00acf6169122f2a009 in lucene-solr's branch refs/heads/branch_6x from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c35fbbd ]\n\nLUCENE-6664: be more robust to broken token stream offsets "
        },
        {
            "id": "comment-15794880",
            "author": "Michael McCandless",
            "date": "2017-01-03T11:51:08+0000",
            "content": "Steve Rowe, I pushed a fix for one of the above failures.  They happen when broken offsets are sent into FlattenGraphFilter. "
        },
        {
            "id": "comment-16386294",
            "author": "Shawn Heisey",
            "date": "2018-03-05T16:23:28+0000",
            "content": "Since multiple Graph filters cannot be used in an analysis chain, what is somebody running 8.0 supposed to do if they need both the WordDelimiter filter and Synonym filter in their analysis chain? "
        }
    ]
}