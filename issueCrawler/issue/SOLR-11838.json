{
    "id": "SOLR-11838",
    "title": "explore supporting Deeplearning4j NeuralNetwork models",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "New Feature",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "Yuki Yano wrote in SOLR-11597:\n... If we think to apply this to more complex neural networks in the future, we will need to support layers ...\n\nMichael A. Alcorn wrote in SOLR-11597:\n\n... In my opinion, if this is a route Solr eventually wants to go, I think a better strategy would be to just add a dependency on Deeplearning4j ...\n\nCreating this ticket for the idea to be explored further (if anyone is interested in exploring it), complimentary to and independent of the SOLR-11597 RankNet related effort.",
    "attachments": {
        "SOLR-11838.patch": "https://issues.apache.org/jira/secure/attachment/12905344/SOLR-11838.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2018-01-09T21:04:59+0000",
            "content": "Patch with ivy.xml dependencies added and sketching out on how an AdapterModel might be factored out from SOLR-11250's WrapperModel so that a MultiLayerNetwork object could be lazily loaded and initialised once all the model parameters are known. ",
            "author": "Christine Poerschke",
            "id": "comment-16319165"
        },
        {
            "date": "2018-01-09T21:48:00+0000",
            "content": "Have had some recent DL4J work and was thinking about something like this though hadn't thought about it in terms of LTR... was thinking in terms of streaming expressions etc. Is this meant to only support this for re-ranking or for a more generic/train/validate/predict over using the data that one already has in solr? ",
            "author": "Gus Heck",
            "id": "comment-16319237"
        },
        {
            "date": "2018-01-10T02:23:41+0000",
            "content": "Greetings from eclipse deeplearning4j! I just wanted to extend a hand out to anyone who needs help with this integration.\n\nPlease ask us question on the gihub:\nhttps://github.com/deeplearning4j/deeplearning4j\n\nPing me on pull request reviews for this as well. We worked with the tika community in a similar capacity as well.\nI'm  @agibsonccc on github if you ever need anything.\n\nThanks for doing this! ",
            "author": "Adam Gibson",
            "id": "comment-16319589"
        },
        {
            "date": "2018-01-10T22:23:57+0000",
            "content": "That's nice guys! I would like to help on this as well. I have lots of experience with Solr and Deeplearning4j. I already played with using Deeplearning4j in the Solr streaming API for applying models just like Gus mentioned. \n\nAny ideas on what kind of models should be supported or what kind of data it should use as input? As we can have bag of words and or features in simple feed forward networks or sequences data with LSTMs which require different formatted inputs.\nFurthermore, I can imagine users would like to have some influence on what kind of data will be used, sparse vectors, word vectors, etc, as these can have a huge impact on the performance of the network in both results and the system itself.\n\nThere's a lot of things to explore here.\n\nPersonally I would like to have something which is easy to use which increases the adoption of this feature. We could build further on the experiences of users.\n ",
            "author": "Jeroen Steggink",
            "id": "comment-16321244"
        },
        {
            "date": "2018-01-11T02:15:03+0000",
            "content": "One interesting thing to think about is that ND4J and thus DL4J makes extensive use of off heap memory, and require native drivers for GPU utilization. This will probably mean that folks might want to add GPU nodes to their existing cluster and define a collection that resides on only those nodes, then the ML expressions can target collections that  have GPU enabled nodes only... drawing data from the existing regular nodes... (I think that should be possible anyway  ).\n\nOther wide ranging thoughts I've had... Christine Poerschke, let me know if I'm way too far afield, I can go get my own jira(s)... or this could become a parent of several jiras...\n\n\n\tPre-trained models loaded into blob store, with a name that can be used to retrieve them and hydrate them with  ModelSerializer.restoreMultiLayerNetwork(InputStream) where it can serve as the guts of a generic predict() expression that can act as a tuple transformer (i.e. categorizing each tuple, etc)  - including loading keras models/transfer learning etc (something DL4J should be able to do for us).\n\tan expression that accepts the hyperparameters/dimensions of a layer that can be combined with other such expressions to create an untrained network.\n\texpressions for partitioning the data into test/train (or K-Folds) and iterating the training a model (I believe I've seen jiras go by that sound like something of the sort exits in streaming expressions already, I know Joel Bernstein talked about efficient sampling in his LSR talk)\n\twriting the trained model back to the blob store automatically on each epoch on a rolling basis (keeping last N copies) to enable early stopping, or selection of best model after K-folds.\n\ta solrj implementation of DataSetIterator that can use a query to specify a set of data to be used for training which then is streamed down, and potentially cached locally or re-streamed for training iteration.\n\n\n\nIn all cases the intent would be that Solr provides Data, environment and infrastructure and all ML heavy lifting would be DL4J based, and I would hope reusable in a LTR context at the very least by drawing trained models from blobstore. I notice LTR has a a model store, is that backed by the blob store, or are there 2 places to store content now? (sorry haven't had any real opportunity to use LTR yet)\n\nAnyway, that stuff's all been rattling around my head trying to get out, hope it's not too much for this ticket. ",
            "author": "Gus Heck",
            "id": "comment-16321567"
        },
        {
            "date": "2018-01-11T02:23:51+0000",
            "content": "Hi Gus,\n\nThere are 2 sources of off heap memory to consider. \nWe cover that here:\nhttp://deeplearning4j.org/memory\n For the GPU we have essentially our own GC. The thing to think about here if you want to have either cpu or gpu, is both are optional.\nI'm more than glad to answer questions on that if anyone has any concerns.\n\n Beyond that, Nd4j itself is very similar to slf4j. You pick the \"chip\" you want as a jar file.\nSo you could in theory have 2 class paths, one for cpu and one for gpu, picking one as the default.\n\nPretrain models can be either a computation graph or a multi layer network. We have a ModelGuesser that helps mitigate the various types.\n\nWe are introducing another type as well soonish that can directly import tensorflow and onnx as well (this will be a more flexible api similar to pytorch)\nwhich will also work. We will be releasing that within the next few weeks. Depending on the timelines for the release, we're happy to coordinate with folks interested\nin various pretrained models.\n\nThis is in top of our existing keras support.\n\nFor the untrained network/various hyper parameters, might I suggest allowing folks to upload a config of their choice? You can try to offer various kinds of \nsample architectures but we've found that the best way to handle this in practice is by just allowing folks to upload their own architectures.\n\nFor the datasetiterator: That is mainly used for minibatch training. You can also create datasets on the fly as well.\n\nFor inference purposes, dl4j makes no assumptions. You could technically just call network.output an on INDArray directly)\n\nThe solr project might be also interested in our alpha sparse support if you need to convert a document vector directly for inference purposes. ",
            "author": "Adam Gibson",
            "id": "comment-16321580"
        },
        {
            "date": "2018-01-11T19:36:34+0000",
            "content": "Hi, thanks everyone for your comments so far here, that's exactly and more than what I'd hoped for when putting 'explore' in the ticket title  I'll remove the 'in contrib/ltr' suffix to reflect the general scope.\n\n... Pre-trained models ... retrieve them and hydrate them with ModelSerializer.restoreMultiLayerNetwork(InputStream) ...\n\n\"Retrieve a pre-trained model, hydrate it and use it (for contrib/ltr).\" - that would nicely capture an aspect I'm personally particularly interested in. Thanks for the pointer to ModelSerializer.restoreMultiLayerNetwork(InputStream)!\n\nAt present the LTR models are stored in a ManagedModelStore/ManagedResource i.e. in ZooKeeper; blob store use has been explored but as yet it's not yet used. With SOLR-11250 there is now also provision for the model definition to be essentially outside the model store with the store stored content just naming the .json file that has the actual model: https://lucene.apache.org/solr/guide/7_2/learning-to-rank.html#using-large-models\n\nAlso on the subject of model storage/retrieval, on Tobias K\u00e4ssmann's SOLR-9887 (about reading KeepWordFilter etc. data from a JDBC source) David Smiley mentioned\n\n... it'd be cool if these might pull from a StreamingExpression, which in turn could be an expression that reads from a JDBC or Solr or who knows what else someone might plug in.\n\nand whilst word filter data and machine learning models are kind of different, the idea of Solr using a streaming expression to pull data from <somewhere> seems fairly generic and very appealing? ",
            "author": "Christine Poerschke",
            "id": "comment-16322824"
        },
        {
            "date": "2018-01-11T21:23:08+0000",
            "content": "using a streaming expression to pull data from <somewhere> seems fairly generic and very appealing?\n\nYes exactly  ",
            "author": "Gus Heck",
            "id": "comment-16322970"
        },
        {
            "date": "2018-01-12T22:57:34+0000",
            "content": "... using a streaming expression to pull data from <somewhere> ...\n\nI have not had an opportunity before to try out streaming expressions ... so had a go and hacked up a proof-of-concept this evening (SOLR-11852) - the connection to Deeplearning4j here could be for the pre-trained DL4J model definitions to be stored <somewhere> and for contrib/ltr to be able to access them from there via a streaming expression.\n\nThe pre-trained models would of course still have to be hydrated and for contrib/ltr purposes the LTRScoringModel.java base class to be implemented.\n ",
            "author": "Christine Poerschke",
            "id": "comment-16324700"
        },
        {
            "date": "2018-01-29T16:35:58+0000",
            "content": "I'm very excited to see this integration happening.\u00a0 Gus Heck has been working with me on some DL4j projects in particular training models and evaluating them for classification.\u00a0 I think at a high level there are 3 main integration patterns that we could / should consider in Solr.\n\n\tusing a model at ingest time to tag / annotate a record going into the index.\u00a0 (primary example would be something like sentiment analysis tagging.)\u00a0 This implies the model was trained and saved somewhere.\n\tusing a solr index (query) to generate a set of training test data so that DL4j can \"fit\" the model and train it.\u00a0 (there might even be a desire for some join functionality so you can join together 2 datasets to create adhoc training datasets.)\n\t(this is a bit more out there.)\u00a0 indexing each node of the multi layer network / computation graph as a document in the index, and use a query to evaluate the output of the model by traversing the documents in the index to ultimately come up with a set of relevancy scores for the documents that represent the output layer of the network.\n\n\n\nI think , perhaps, the most interesting use case is #2.\u00a0 So basically, the idea is you want to define a network\u00a0 (specify the layers, types of layers, activation function, etc..) and then specify a query that matches a set of documents in the index that would be used to train that model.\u00a0 Currently DL4j uses \"datavec\" to handle all the data normalization prior to going into the model for training.\u00a0 That exposes a DataSetIterator.\u00a0 The datasetiterator could be replaced with an iterator that sits ontop of the export handler or even just a raw search result.\u00a0 The general use cases here for pagination would be\u00a0\n\n\tto iterate the full result set\u00a0 (presumably multiple times as the model will make multiple passes over the data when training.)\n\tgenerate a random ordering of the dataset being returned\n\texcluding a random (but deterministic?) set of documents from the main query to provide a holdout testing dataset.\n\n\n\nKeeping in mind that typically in network training, you have both your training dataset and the testing dataset.\u00a0\u00a0\n\nThe final outcome of this would be a computationgraph/multilayernetwork which can be serialized by dl4j as a json file, and the other output could/should be the evaluation or accuracy scores of the model\u00a0 (F1, Accuracy, and confusion matrix.)\n\nAs per the comments about natives, yes, there are definitely platform dependent parts of DL4j, in particular the \"nd4j\" which can be gpu/cpu, but there are also other dependencies on javacv/javacpp.\u00a0 The javacv/javacpp stuff is really only used for image manipulation as it's the java binding to OpenCV.\u00a0 The dependency tree for DL4j is rather large, so I think we'll need to take care/caution that we're not injecting a bunch of conflicting jar files.\u00a0 Perhaps, if we identify the conflicting jar versions.\u00a0\n\n\u00a0 ",
            "author": "Kevin Watters",
            "id": "comment-16343602"
        },
        {
            "date": "2018-01-29T16:46:54+0000",
            "content": "Just a heads up. I plan on adding the self organizing map implementations (SOM) from Apache Commons Math in the very near future (probably 7.4). These are neural network unsupervised\u00a0clustering algorithms that I believe also are supported by DL4j. It sounds like people are mostly interested in the the supervised\u00a0deep learning models with DL4j, but the SOM's might be an interesting first step towards neural network classifiers.\n\nThe SOM's are part of larger clustering integration which includes kmeans, fuzzyKmeans, DBSCAN, Multivariate Gaussian (EM), PCA, LSA and SOM. ",
            "author": "Joel Bernstein",
            "id": "comment-16343620"
        },
        {
            "date": "2018-01-29T17:20:08+0000",
            "content": "As a start, I think applying models for LTR or classifying documents/fields when indexing would be most useful.\n\nOne thing we shouldn't underestimate is data structures for Neural Networks. Depending on the network structure a model may depend on a specific data structure. For example, timeseries-vectors are very different from other vectors. Are we doing just bag-of-words or do we keep the order of words? How many fields would your like as input? How many inputs can models have (preferably ComputationGraphs, as they are more flexible).\n\nFurthermore, we should think about what is actually going to work. Having one-hot encoding for all terms in an index could be problematic. There is already a\u00a0logistic regression implementation which works great for simple classification. If we're going to use DL4J it should add something more than Solr already offers.\n\nMaybe we can think of a few specific use cases to make a prototype for?\n\n\u00a0\n\nI think\u00a0we can make a DataVec record reader for Solr\u00a0(@Kevin Watters). But I guess this is something we can add to DataVec itself, instead of adding this to Solr. An alternative could be to use Solr's Streaming API to return data in a\u00a0format which is efficient and could be directly used by DataVec.\n\nAnother thing I'd like to mention is dependencies. Instead of relying on DL4J specifically, we could think about abstracting data input and output for machine learning and applying models in general. As a DL4J user I'm not very interested in running it on a Solr server. I have dedicated servers running DL4J models which I serve using REST APIs. The reason is that I have servers with GPUs and lot's of RAM dedicated for this type of process. Solr on the other hand can be very demanding in a different way.\n\n\u00a0 ",
            "author": "Jeroen Steggink",
            "id": "comment-16343656"
        },
        {
            "date": "2018-01-29T20:23:02+0000",
            "content": "A little bit with the dependency tree question in mind, but also generally, just throwing this out there ...\n\nFor the use cases of using a model trained and saved elsewhere for Learning To Rank or ClassificationUpdateProcessorFactory purposes - where could that code comfortably live, need it necessarily be within Apache Solr?\n\nAdam Gibson - I noticed there's a deeplearning4j-modelimport module, might there be scope for a deeplearning4j-modelexport or deeplearning4j-modelexport-solr module? (Happy to open a deeplearning4j issue or pull request if here is not the best place to pose that question.)\n\nAttached illustrative patch, Solr LTR itself would contain the generic AdapterModel and what is the sandbox/model-consumer/src/main/java/please/replace/me/MultiLayerNetworkLTRScoringModel.java in the patch in principle could (with javadocs and of course tests) live elsewhere. ",
            "author": "Christine Poerschke",
            "id": "comment-16343959"
        },
        {
            "date": "2018-01-29T22:10:10+0000",
            "content": "Christine Poerschke Yes we would be more than glad to accommodate you here.\u00a0\n\nThat model import module is mainly for keras import. In nd4j (our tensor library) we are working on supporting both the onnx file format (http://onnx.ai) as well as the tensorflow fromat as well.\u00a0\n\nI think Jeroen Steggink has the right idea re: dl4j. I can say that it has special requirements that are pretty atypical of infrastructure typical of running solr. This is especially true with batch inference.\n\n\u00a0\n\nJoel Bernstein SOMs are far from interesting in 2018. Dl4j has Variational AutoeEncoders which are both the cutting edge right before GANs but also more stable. Variational AutoEncoders , with their built in reconstruction mechanic could also be used to track anomalies or rank results right in solr. We use this workflow mainly for time series data but due to solr being a ranking engine it makes a ton of sense here.\n\nRe: Time series/LSTMs.The vectors themselves are 3d, but are created from a time dimension with a specific window length. We typically create this with what we call a SequenceRecordReader which can take varying file inputs and infer a time series length based on the input. For solar, a timestamp field could be used to create the proper 3d vectors pretty easily.\u00a0\n\n\u00a0\n\nRe: Datavec record reader. We are happy to take pull requests. It is definitely of interest thanks! ",
            "author": "Adam Gibson",
            "id": "comment-16344095"
        },
        {
            "date": "2018-02-15T21:51:19+0000",
            "content": "ticket cross-references:\n\n\tSOLR-11941 committed org.apache.solr.ltr.model.AdapterModel\n\tdeeplearning4j/pull/4662 request opened proposing to add a new deeplearning4j-modelexport-solr module for use with Apache Solr 7.3 (or higher):\n\t\n\t\tTrain a model in your 'usual' way and on your 'usual' infrastructure.\n\t\tSave a.k.a. serialize the model to a file.\n\t\tAdd the deeplearning4j-modelexport-solr jar (and any dependencies) to your Apache Solr installation.\n\t\tConfigure Solr, in the 'usual' way, to use the serialized model file e.g. for Learning To Rank purposes.\n\t\n\t\n\n ",
            "author": "Christine Poerschke",
            "id": "comment-16366306"
        },
        {
            "date": "2018-02-16T01:25:28+0000",
            "content": "I realize SOMs are no longer the cutting edge. I've been taking the approach with Streaming Expressions of starting with the basics: statistics, probability, curve fitting, regression, interpolation, derivatives/integrals, clustering etc... and building up to more complicated machine learning algorithms. SOMs are one step along the way towards more interesting neural network algorithms.\n\ndl4j looks like it has some awesome algorithms, looking forward to see how they can be integrated into Solr.\n\n\u00a0 ",
            "author": "Joel Bernstein",
            "id": "comment-16366511"
        },
        {
            "date": "2018-02-17T21:57:32+0000",
            "content": "Just of note if you use, deeplearning4j-nn, and the nd4j apis you don't actually need\n\ndeeplearning4j-core. Behind the scenes we've been working on modularizing core out quite a bit.\n\nWe may be able to help you here. What components of dl4j are you using?\n From the looks of it, it's just the configuration and some of the basic nd4j apis.\n\nYou can get away without using core. That should simplify things quite a bit.\n\n\u00a0\n\nEdit: Just checked the PR, it's already setup, so guava shouldn't be in there (it's only used in core) - but it indeed does seem to be a transitive dependency. Thanks for flagging this, let me see what we can do to minimize this on our side. It's being brought in by reflections, which we were planning on getting rid of anyways. We'll see what we can do to prioritize this and are also open to PRs there as well.\n\n\u00a0 ",
            "author": "Adam Gibson",
            "id": "comment-16368383"
        },
        {
            "date": "2018-02-19T07:33:43+0000",
            "content": "Hey folks: Follow up on the guava thing. Here is a roadmap for removing it (which would remove the guava 20 requirement):\nhttps://github.com/deeplearning4j/nd4j/issues/2648\n\nhttps://github.com/deeplearning4j/deeplearning4j/issues/4672 ",
            "author": "Adam Gibson",
            "id": "comment-16368840"
        },
        {
            "date": "2018-03-29T20:17:58+0000",
            "content": "I've been playing around some more with deeplearning4j and Solr streaming expressions ...\n\nnew pull request deeplearning4j/pull/4876 shares the results, proposing to add a DataSetIterator implementation (tentatively named TupleStreamDataSetIterator) which uses a streaming expression to fetch data from Solr and/or one of the sources (e.g. jdbc) supported as a stream source.\n\n... this is not specific to Learning-To-Rank and I have no specific real use case in mind as yet but would be curious to hear if anyone could think of scenarios where something other than fields are part of the streaming expression used in the iterator? ",
            "author": "Christine Poerschke",
            "id": "comment-16419681"
        },
        {
            "date": "2018-05-25T22:40:26+0000",
            "content": "I've been wondering some more about how deeplearning4j models might be 'exported to' and used with Solr ...\n\nSOLR-12402 shares a ModelTupleStream class outline. No specific real use case in mind as yet but i would be curious to hear people's thoughts \u2013 specifically if the stream being run through the model was something other than a plain search(...) that might get very interesting? ",
            "author": "Christine Poerschke",
            "id": "comment-16491347"
        },
        {
            "date": "2018-06-01T20:51:53+0000",
            "content": "ticket cross-reference: https://github.com/deeplearning4j/deeplearning4j/pull/5429 has started to turn the above mentioned ModelTupleStream class outline into an actual class. ",
            "author": "Christine Poerschke",
            "id": "comment-16498556"
        }
    ]
}