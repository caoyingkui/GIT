{
    "id": "SOLR-5426",
    "title": "org.apache.solr.common.SolrException; org.apache.solr.common.SolrException: org.apache.lucene.search.highlight.InvalidTokenOffsetsException: Token 0 exceeds length of provided text sized 840",
    "details": {
        "affect_versions": "4.4,                                            4.5.1",
        "status": "Resolved",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "components": [
            "highlighter"
        ],
        "type": "Bug",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Highlighter does not work correctly on test-data.\nI added index- and config- files (see attached highlighter.zip) for reproducing this issue.\nEverything works fine if I search without highlighting:\n\nhttp://localhost:8983/solr/global/select?q=aa&wt=json&indent=true\n\nBut if search with highlighting: \n\nhttp://localhost:8983/solr/global/select?q=aa&wt=json&indent=true&hl=true&hl.fl=*_stx&hl.simple.pre=<em>&hl.simple.post=<%2Fem>\n\nI'm get the error:\n\nERROR - 2013-11-07 10:17:15.797; org.apache.solr.common.SolrException; null:org.apache.solr.common.SolrException: org.apache.lucene.search.highlight.InvalidTokenOffsetsException: Token 0 exceeds length of provided text sized 840\n\tat org.apache.solr.highlight.DefaultSolrHighlighter.doHighlightingByHighlighter(DefaultSolrHighlighter.java:542)\n\tat org.apache.solr.highlight.DefaultSolrHighlighter.doHighlighting(DefaultSolrHighlighter.java:414)\n\tat org.apache.solr.handler.component.HighlightComponent.process(HighlightComponent.java:139)\n\tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:208)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1859)\n\tat org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:703)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:406)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:195)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1419)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:455)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1075)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:384)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1009)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:368)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:489)\n\tat org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:942)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1004)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:640)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\tat org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)\n\tat org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Unknown Source)\nCaused by: org.apache.lucene.search.highlight.InvalidTokenOffsetsException: Token 0 exceeds length of provided text sized 840\n\tat org.apache.lucene.search.highlight.Highlighter.getBestTextFragments(Highlighter.java:225)\n\tat org.apache.solr.highlight.DefaultSolrHighlighter.doHighlightingByHighlighter(DefaultSolrHighlighter.java:527)\n\t... 33 more",
    "attachments": {
        "highlighter.zip": "https://issues.apache.org/jira/secure/attachment/12612531/highlighter.zip",
        "SOLR-5426.patch": "https://issues.apache.org/jira/secure/attachment/12629221/SOLR-5426.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Nikolay",
            "id": "comment-13815638",
            "date": "2013-11-07T04:34:16+0000",
            "content": "data for reproduce the bug, folder \"global\" contains lucene index, also there are 2 config files (schema.xml, solrconfig.xml) "
        },
        {
            "author": "Arun Kumar",
            "id": "comment-13885364",
            "date": "2014-01-29T14:08:02+0000",
            "content": "I did investigate this issue and found that it is a desired behavior of the highlighter. The default chars to analyze for highlighter is hard coded and set to 51200 chars max.\n\nDEFAULT_MAX_CHARS_TO_ANALYZE = 50*1024;\n\nBut the document you indexed has more characters to analyze in one of the field value that causes this issue. But there is a way to increase this max chars to analyze by sending an additional query param like this hl.maxAnalyzedChars=52300\n\nSo if I fire a query as below then the error is not seen.\nhttp://localhost:8983/solr/global/select?q=aa&indent=true&hl=true&hl.fl=*_stx&hl.simple.pre=%3Cem%3E&hl.simple.post=%3C/em%3E&hl.maxAnalyzedChars=52300\n\nHope this helps.  "
        },
        {
            "author": "Arun Kumar",
            "id": "comment-13892043",
            "date": "2014-02-05T12:20:48+0000",
            "content": "disregard my previous comments, Even if a string exists the max offset limit it shouldn't blow up with an exception. On further investigation I found that an unused token of the larger string when it token count increase the max offset limit carries to the next available string in the loop. Attached a patch file for the fix, it resolves the issue. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13893706",
            "date": "2014-02-06T19:37:44+0000",
            "content": "Arun: Since you seem to have a grasp on the problem here, would it be possible for you to help write a unit test to recreate it? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13893709",
            "date": "2014-02-06T19:39:27+0000",
            "content": "Also: when submitting patches, it's really helpful if you can please generate the patch against the entire code base...\n\nhttps://wiki.apache.org/solr/HowToContribute#Generating_a_patch "
        },
        {
            "author": "Arun Kumar",
            "id": "comment-13894432",
            "date": "2014-02-07T11:41:19+0000",
            "content": "Patch generated at the project root level.  "
        },
        {
            "author": "Arun Kumar",
            "id": "comment-13894434",
            "date": "2014-02-07T11:44:51+0000",
            "content": "Hi Hoss,\n\nThanks for your review, I have updated the patch which is generated against the entire code base. I tried to create an unit test to recreate it but couldn't do that successfully as this is reproducible in combination of CachingTokenFilter along with OffsetLimitTokenFilter.\n\nThanks,\nArun "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13900481",
            "date": "2014-02-13T16:32:09+0000",
            "content": "Arun, what do you mean when you say the following?\n\nI tried to create an unit test to recreate it but couldn't do that successfully as this is reproducible in combination of CachingTokenFilter along with OffsetLimitTokenFilter.\n\nI don't understand why these token filters need to be involved?  I looked at the schema.xml in the .zip attachment, and the *_stx fields' type text_stx doesn't use those filters.\n\nThe patch you attached can't be committed without a unit test that fails without the patch and succeeds with it. "
        },
        {
            "author": "Arun Kumar",
            "id": "comment-13902433",
            "date": "2014-02-15T15:17:17+0000",
            "content": "Steve, thanks for reviewing my changes. All these token filters are used by the highlighter component. I have updated the patch with a unit test which helps us to reproduce the issue and the code change in the patch fixes it.  "
        },
        {
            "author": "Ahmet Arslan",
            "id": "comment-13988848",
            "date": "2014-05-03T23:40:25+0000",
            "content": "Bring failing test case to trunk. Simplify schema and add 5 test methods to isolate problem. Problematic field type is given below. Exception occurs for only stored and multiValued field. TestCase demonstrates this.\n\nAnother interesting thing is test passes\n\n\twhen WordDelimiterFilterFactory is removed from index analyzer\n\twhen ReversedWildcardFilterFactory is removed from index analyzer\nseparately. \n\n\n\n\n <fieldType name=\"text_stx\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer type=\"index\">\n        <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n        <filter class=\"solr.WordDelimiterFilterFactory\" generateWordParts=\"1\" generateNumberParts=\"1\" catenateWords=\"1\" catenateNumbers=\"1\" catenateAll=\"0\" splitOnCaseChange=\"0\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.ReversedWildcardFilterFactory\" withOriginal=\"true\"\n           maxPosAsterisk=\"3\" maxPosQuestion=\"2\" maxFractionAsterisk=\"0.33\"/>\n      </analyzer>\n      <analyzer type=\"query\">\n        <tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n        <filter class=\"solr.SynonymFilterFactory\" synonyms=\"synonyms.txt\" ignoreCase=\"true\" expand=\"true\"/>\n        <filter class=\"solr.WordDelimiterFilterFactory\" generateWordParts=\"1\" generateNumberParts=\"1\" catenateWords=\"0\" catenateNumbers=\"0\" catenateAll=\"0\" splitOnCaseChange=\"0\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n      </analyzer>\n    </fieldType>\n\n "
        },
        {
            "author": "Ahmet Arslan",
            "id": "comment-13988850",
            "date": "2014-05-03T23:43:36+0000",
            "content": "Does any know how can we add ReversedWildcardFilterFactory to TestRandomChains? "
        },
        {
            "author": "Ahmet Arslan",
            "id": "comment-13988858",
            "date": "2014-05-04T00:03:58+0000",
            "content": "I said/used stored by mistake. I was trying to figure out its relation to indexed property. This patch corrects this. Note that indexed=false fields can be highlighted if tokenizer defined for them.\n\nOnly failing method is testIndexedMultiValued. Other three combinations pass.\n\n\n <field name=\"indexed_multiValued\"       type=\"text_stx\" indexed=\"true\" stored=\"true\"  multiValued=\"true\"/>\n\n "
        },
        {
            "author": "Ahmet Arslan",
            "id": "comment-13988860",
            "date": "2014-05-04T00:13:18+0000",
            "content": "Hi Arun Kumar, I couldn't see your fix in your patch. Did you forget to add it accidentally? The patch you attached does not include OffsetLimitTokenFilter.java.patch. Can you re-attact it? "
        },
        {
            "author": "Arun Kumar",
            "id": "comment-14014641",
            "date": "2014-05-31T13:14:03+0000",
            "content": "In my last patch upload I accidentally missed out the main change, in this patch it is covered up.  "
        },
        {
            "author": "Ahmet Arslan",
            "id": "comment-14014731",
            "date": "2014-05-31T18:20:12+0000",
            "content": "Thanks! Arun, so this is the fix \n\n\n-    if (offsetCount < offsetLimit && input.incrementToken()) {\n+    if (input.incrementToken() && offsetCount < offsetLimit) {\n\n\n\nwith this test of SOLR-3193 passes too. Can you explain what is the magic here? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14031013",
            "date": "2014-06-13T19:04:32+0000",
            "content": "I've updated the patch to cleanup the test a bit...\n\n\n\trenamed new schema field & added comment about it's purpose\n\trefactored test to eliminate some duplication & move the test methods to the top\n\tadded firm assertions to the test to ensure highlighting is actually happening\n\t\n\t\tthis actually uncovered a bug in the test that it wasn't doing anything useful on the non-indexed fields because they didn't match any docs\n\t\n\t\n\n\n\n...the change to OffsetLimitTokenFilter definitely fixes the problem \u2013 but i'm honestly not sure if that's the right fix \u2013 it's not clear to me why consuming the token before checking hte limit is the \"correct\" behavior (it seems counter intuitive to me) and makes me wonder if this is actually masking some other \"real\" bug in ReversedWildcardFilter "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-14031042",
            "date": "2014-06-13T19:24:36+0000",
            "content": "Hi,\nthe issue is in ReversedWildcardTokenFilter: The TokenFilter does not correctly implement reset(). If the TokenStream is reused and it was not completely consumed before, the state is still active. In that case it restores the \"save\" state and so injects a buggy state from the previous usage as first token into the new one.\n\nThe fix is to make ReverseWildcardTokenFilter correctly implement reset() and NULL all state. The pseudo-fix in the highligters's tokenfilter just hides the bug.\n\nI will provide a patch in a minute! "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-14031054",
            "date": "2014-06-13T19:32:41+0000",
            "content": "This is the correct fix:\n\n\tAdded missing reset() in ReverseWildCardFilter\n\tmade fields correct final, so its obvious which is state and which is config\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14031067",
            "date": "2014-06-13T19:41:31+0000",
            "content": "Spoke to Uwe on IRC: he's AFK but asked me to go ahead and commit & backport on his behalf \u2013 will do as soon as full test&precommit finish. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14031182",
            "date": "2014-06-13T21:15:51+0000",
            "content": "Commit 1602525 from hossman@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1602525 ]\n\nSOLR-5426: Fixed a bug in ReverseWildCardFilter that could cause InvalidTokenOffsetsException when highlighting "
        },
        {
            "author": "Ahmet Arslan",
            "id": "comment-14031200",
            "date": "2014-06-13T21:30:18+0000",
            "content": "Thanks Uwe! and Hoss for bringing closure. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14031202",
            "date": "2014-06-13T21:31:40+0000",
            "content": "Commit 1602527 from hossman@apache.org in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1602527 ]\n\nSOLR-5426: Fixed a bug in ReverseWildCardFilter that could cause InvalidTokenOffsetsException when highlighting (merge r1602525) "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14031209",
            "date": "2014-06-13T21:36:07+0000",
            "content": "Thanks everybody! "
        }
    ]
}