{
    "id": "SOLR-8575",
    "title": "Fix HDFSLogReader replay status numbers, a performance bug where we can reopen FSDataInputStream much too often, and an hdfs tlog data integrity bug.",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "6.0"
        ],
        "affect_versions": "None",
        "status": "Resolved",
        "resolution": "Fixed",
        "priority": "Critical"
    },
    "description": "Patrick Dvorak noticed some funny transaction log replay status logging a while back:\n\nactive=true starting pos=444978 current pos=2855956 current size=16262 % read=17562\nactive=true starting pos=444978 current pos=5748869 current size=16262 % read=35352\n\n17562% read? Current size does not change as expected in this case?",
    "attachments": {
        "SOLR-8575.patch": "https://issues.apache.org/jira/secure/attachment/12783440/SOLR-8575.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-01-20T22:23:51+0000",
            "author": "Mark Miller",
            "content": "Patch that fixes a couple issues and adds an isolated test that tries to target tlog replay while buffering during recovery.\n\nWhen we recalculated the size of the tlog, it keeps coming back the same as the first size call, even if the tlog has grown. I think this has something to do with an open file with hdfs.\n\nWe were somewhat incorrectly using that size for tlog progress logging. We should having been using our internally tracked size.\n\nthe same as the first size call, even if the tlog has grown.\n\nAnd that kept a large performance issue in. When we opened the tlog at first, we could replay fairly fast, but if we buffer updates while replay, we were reopening the reader every update due to the stale size issue. ",
            "id": "comment-15109609"
        },
        {
            "date": "2016-01-20T22:45:53+0000",
            "author": "Mike Drob",
            "content": "I had talked to Andrew Wang about this maybe a month ago and he suggested that if you want to get the updated size from the file then you have to use hsync with the length update flag[1] using an HdfsOutputStream (not FSDataOutputStream like we use).\n\nUsing an internally stored length is probably better anyway, though.\n\n[1]: https://github.com/apache/hadoop/blob/2ec438e8f7cd77cb48fd1264781e60a48e331908/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/HdfsDataOutputStream.java#L105 ",
            "id": "comment-15109639"
        },
        {
            "date": "2016-01-20T23:03:08+0000",
            "author": "Mark Miller",
            "content": "Using an internally stored length is probably better anyway, though.\n\nThe problem is that our internal size does not correlate with what we can actually read, even after an hflush. (unless we reopen inputstreams)\n\nupdated size from the file then you have to use hsync with the length update flag[1] using an HdfsOutputStream \n\nAh, interesting, I'll poke around that a bit to see if we want to do anything different. ",
            "id": "comment-15109686"
        },
        {
            "date": "2016-02-04T22:52:45+0000",
            "author": "Mike Drob",
            "content": "Mark Miller - do you think we will be going with your current proposed approach or do you expect to redo this to use HdfsOutputStream? Not sure how much research you've already done so I don't want to duplicate effort, but would be interested in making sure this issue gets resolved. ",
            "id": "comment-15133219"
        },
        {
            "date": "2016-02-04T22:55:12+0000",
            "author": "Mark Miller",
            "content": "Yeah, I was about to commit what I have. Adding an hsync would be much slower than this and is not necessary with this approach in my testing. ",
            "id": "comment-15133252"
        },
        {
            "date": "2016-02-04T23:06:40+0000",
            "author": "ASF subversion and git services",
            "content": "Commit ec4c72310f3548b93139b25a12d6e9a16ac9e322 in lucene-solr's branch refs/heads/master from Mark Miller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ec4c723 ]\n\nSOLR-8575: Fix HDFSLogReader replay status numbers and a performance bug where we can reopen FSDataInputStream too often. ",
            "id": "comment-15133275"
        },
        {
            "date": "2016-02-04T23:54:42+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 482b40f841660820f633267a21e6df44aff55346 in lucene-solr's branch refs/heads/branch_5x from Mark Miller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=482b40f ]\n\nSOLR-8575: Fix HDFSLogReader replay status numbers and a performance bug where we can reopen FSDataInputStream too often. ",
            "id": "comment-15133361"
        },
        {
            "date": "2016-02-05T00:03:55+0000",
            "author": "Anshum Gupta",
            "content": "Hey Mark Miller is the commit supposed to be from the mark dot miller at oblivion dot ch id ? ",
            "id": "comment-15133372"
        },
        {
            "date": "2016-02-05T00:07:23+0000",
            "author": "Mark Miller",
            "content": "Strange...not what is in my gitconfig. Must be something to do with the INFRA tagging script? ",
            "id": "comment-15133375"
        },
        {
            "date": "2016-02-05T00:08:32+0000",
            "author": "Mark Miller",
            "content": "Probably because my name is not my username, just my email id's me. I'll switch the username. ",
            "id": "comment-15133377"
        },
        {
            "date": "2016-02-05T00:09:41+0000",
            "author": "Uwe Schindler",
            "content": "Where do you see this in this commit? It shows both for author and committer the same @apache.org ID. ",
            "id": "comment-15133382"
        },
        {
            "date": "2016-02-05T00:10:52+0000",
            "author": "Uwe Schindler",
            "content": "Ah the comment refers wrong JIRA user name! I think thats a bug and INFRA should take care. ",
            "id": "comment-15133383"
        },
        {
            "date": "2016-02-05T04:41:17+0000",
            "author": "Anshum Gupta",
            "content": "Thanks Uwe! \n\nDo you mean to say there's already an open issue? or do we need to open another one? ",
            "id": "comment-15133649"
        },
        {
            "date": "2016-02-05T07:08:18+0000",
            "author": "Uwe Schindler",
            "content": "I think we should contact them or open issue. Maybe they have a \"mapping\" table (ASF-ID -> JIRA-ID) somewhere. ",
            "id": "comment-15133772"
        },
        {
            "date": "2016-02-05T17:40:00+0000",
            "author": "ASF subversion and git services",
            "content": "Commit ec4c72310f3548b93139b25a12d6e9a16ac9e322 in lucene-solr's branch refs/heads/lucene-6835 from Mark Miller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ec4c723 ]\n\nSOLR-8575: Fix HDFSLogReader replay status numbers and a performance bug where we can reopen FSDataInputStream too often. ",
            "id": "comment-15134544"
        },
        {
            "date": "2016-02-11T04:08:10+0000",
            "author": "Yonik Seeley",
            "content": "I was going to reopen this issue, but it's still open anyway.\nI've changed to a blocker for 5.5 based on what I'm seeing in here:\nhttps://issues.apache.org/jira/browse/SOLR-8586?focusedCommentId=15142215&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15142215 ",
            "id": "comment-15142219"
        },
        {
            "date": "2016-02-11T10:25:03+0000",
            "author": "Michael McCandless",
            "content": "Hmm so it sounds like the changes committed for this issue caused the test failures you're seeing on SOLR-8586 Yonik Seeley?  Should we revert the change here until we can explain it? ",
            "id": "comment-15142521"
        },
        {
            "date": "2016-02-11T13:27:34+0000",
            "author": "Mark Miller",
            "content": "Yeah, I would just pull it out of 5.5 rather than try and address it. ",
            "id": "comment-15142718"
        },
        {
            "date": "2016-02-11T13:32:47+0000",
            "author": "ASF subversion and git services",
            "content": "Commit f6098148aed067c06e2459a3ab55abe2e66300b0 in lucene-solr's branch refs/heads/master from markrmiller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f609814 ]\n\nSOLR-8575: Revert while investigated. (reverted from commit ec4c72310f3548b93139b25a12d6e9a16ac9e322) ",
            "id": "comment-15142725"
        },
        {
            "date": "2016-02-11T13:36:39+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 68ba7a5e5275d4ad10e4e8f70e223f9b61d70b54 in lucene-solr's branch refs/heads/branch_5x from markrmiller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=68ba7a5 ]\n\nSOLR-8575: Revert while investigated. (reverted from commit 482b40f841660820f633267a21e6df44aff55346) ",
            "id": "comment-15142729"
        },
        {
            "date": "2016-02-11T13:40:45+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 51257d2ebe099a6c7029e7fd47ce25f4393cfb49 in lucene-solr's branch refs/heads/branch_5_5 from markrmiller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=51257d2 ]\n\nSOLR-8575: Revert while investigated. (reverted from commit 482b40f841660820f633267a21e6df44aff55346) ",
            "id": "comment-15142733"
        },
        {
            "date": "2016-02-11T14:29:07+0000",
            "author": "Michael McCandless",
            "content": "Thanks Mark Miller. ",
            "id": "comment-15142796"
        },
        {
            "date": "2016-02-11T18:55:18+0000",
            "author": "Yonik Seeley",
            "content": "Here's an interesting exception I found logged:\n\n  2> 84141 ERROR (recoveryExecutor-132-thread-2-processing-n:127.0.0.1:44435_ x:collection1 s:shard5 c:collection1 r:core_node4) [n:127.0.0.1:44435_ c:collection1 s:shard5 r:core_node4 x:collection1] o.a.s.u.UpdateLog java.io.EOFException\n  2> \tat org.apache.solr.common.util.FastInputStream.readByte(FastInputStream.java:207)\n  2> \tat org.apache.solr.common.util.JavaBinCodec.readVal(JavaBinCodec.java:207)\n  2> \tat org.apache.solr.update.HdfsTransactionLog$HDFSLogReader.next(HdfsTransactionLog.java:419)\n  2> \tat org.apache.solr.update.UpdateLog$LogReplayer.doReplay(UpdateLog.java:1333)\n  2> \tat org.apache.solr.update.UpdateLog$LogReplayer.run(UpdateLog.java:1255)\n\n\n\nAn important part of this patch was recording the amount of data we've written so far (as \"sz\"),\nand then a new input stream is opened.\n\nDoes HDFS guarantee that all data written will be readable if we open the file again (even if we haven't closed the file)?\nAnd does read() make the same guarantees about reading at least a single byte (or blocking) unless we've reached EOF? ",
            "id": "comment-15143262"
        },
        {
            "date": "2016-02-12T02:59:06+0000",
            "author": "Mark Miller",
            "content": "Here is the patch I'm currently playing with.\n\nOn opening the the Log reader, when we first open the input stream reader, we now hflush before that. In all the fails I was seeing, the starting position was 0 and we were hitting EOF pretty much right away.\n\nStill testing out, but I think I'm on the right path. ",
            "id": "comment-15143938"
        },
        {
            "date": "2016-02-12T13:45:40+0000",
            "author": "Mark Miller",
            "content": "I'm no longer seeing inconsistency fails with this patch. ",
            "id": "comment-15144592"
        },
        {
            "date": "2016-02-12T15:43:33+0000",
            "author": "Mark Miller",
            "content": "Should also note, latest patch does also include an additional fast stream flushbuffer so that the sz we get is accurate and works with the hflush. I had originally thought that was simply the issue, but still got these EOF fails in the same read method until also changing the constructor. ",
            "id": "comment-15144727"
        },
        {
            "date": "2016-02-12T16:17:41+0000",
            "author": "Yonik Seeley",
            "content": "I've started testing this morning with this patch... it will be a few hours at least before I know if it's fixed for me as well.\n\nOne of the error caused by premature EOF that I was seeing happened after the re-open, so the constructor changes should not matter in that specific fail.\nBut an important addition was made in this current patch, which calls fos.flushBuffer() in the reopen... that was missing in the previous patch.\n\nActually, it looks like this patch fixed more than just performance... that missing fos.flushBuffer() wasn't just missing from the previous patch, it was never there in the code to begin with!  This appears to mean that prior to this JIRA, buffering while replaying could sometimes prematurely abort (by getting an EOF) because a partial record was written.  Simply adding a flushBuffer would not have been sufficient though... by using the actual size of the file (unsynchronized) as the point to read up to, we can get premature EOFs as well.  Given we're using 64K write buffers, the odds of seeing issues due to this is related to the document size being indexed as well as the throughput. ",
            "id": "comment-15144779"
        },
        {
            "date": "2016-02-12T16:35:53+0000",
            "author": "Yonik Seeley",
            "content": "With the latest patch, the flushBuffer in this part of the code is redundant:\n\n    public Object next() throws IOException, InterruptedException {\n      long pos = fis.position();\n\n      synchronized (HdfsTransactionLog.this) {\n        if (trace) {\n          log.trace(\"Reading log record.  pos=\"+pos+\" currentSize=\"+fos.size());\n        }\n\n        if (pos >= fos.size()) {\n          return null;\n        }\n       \n        fos.flushBuffer();\n      }\n\n ",
            "id": "comment-15144808"
        },
        {
            "date": "2016-02-12T16:45:53+0000",
            "author": "Mark Miller",
            "content": "Actually, it looks like this patch fixed more than just performance\n\nRight, it's not just a performance fix or a 'status' numbers fix. The issue was the size hdfs was returning to us was wrong and we were going off the wrong size info. That made it so that when we had to open a new reader, we then did so every update. That seems to have hidden some of the issues here. There was no way to know if there was a bug users where hitting here beyond super, super slow replay while buffering performance though. For example, you were not seeing inconsistency fails with that code. It was obviously a bug no matter what flushing happened though, because we were basing our logic on file sizes that did not relate to reality (and did not generally change at all between calls).  ",
            "id": "comment-15144833"
        },
        {
            "date": "2016-02-12T17:17:08+0000",
            "author": "Yonik Seeley",
            "content": "Yeah, if HDFS had reported the correct length, the old code (prior to this JIRA) would have attempted to read partial records and get EOFs where it shouldn't.\n\nFor others following along... the key thing to the current patch is this:\n\n        synchronized (HdfsTransactionLog.this) {\n          fos.flushBuffer();\n          sz = fos.size();\n        }\n\n\n\nThe synchronization (which is the same monitor used to write records) means that our recorded \"sz\" represents a whole record and is hence safe to read up to. ",
            "id": "comment-15144882"
        },
        {
            "date": "2016-02-12T20:58:20+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 4cc844897e094ffc07f1825d88730ea975de3fde in lucene-solr's branch refs/heads/master from markrmiller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=4cc8448 ]\n\nSOLR-8575: Fix HDFSLogReader replay status numbers, a performance bug where we can reopen FSDataInputStream much too often, and an hdfs tlog data integrity bug. ",
            "id": "comment-15145286"
        },
        {
            "date": "2016-02-12T21:12:42+0000",
            "author": "Yonik Seeley",
            "content": "Yeah, everything looks good - no consistency fails after running all day! ",
            "id": "comment-15145307"
        },
        {
            "date": "2016-02-12T21:31:10+0000",
            "author": "Mark Miller",
            "content": "I'll spend a little time trying to get rid of some false chaos monkey test fails so we can keep a better track when things go bad. ",
            "id": "comment-15145344"
        },
        {
            "date": "2016-02-16T21:47:38+0000",
            "author": "Mike Drob",
            "content": "https://github.com/apache/lucene-solr/blob/0bba332549a11d5c381efc93a66087999b6de210/solr/core/src/java/org/apache/solr/update/UpdateLog.java#L1443\n\nIs that line supposed to have an assert on it? ",
            "id": "comment-15149375"
        },
        {
            "date": "2016-02-16T21:49:30+0000",
            "author": "Mark Miller",
            "content": "Yeah, good catch. ",
            "id": "comment-15149378"
        },
        {
            "date": "2016-02-21T01:35:53+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 2fd90cd4893952f5150e34ed70e86d3e85f61458 in lucene-solr's branch refs/heads/master from markrmiller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=2fd90cd ]\n\nSOLR-8575: Add missing assert. ",
            "id": "comment-15155839"
        }
    ]
}