{
    "id": "LUCENE-5066",
    "title": "TestFieldsReader fails in 4.x with OOM",
    "details": {
        "components": [],
        "fix_versions": [],
        "affect_versions": "None",
        "priority": "Major",
        "labels": "",
        "type": "Bug",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "Its FaultyIndexInput is broken (doesn't implement seek/clone correctly).\n\nThis causes it to read bogus data and try to allocate an enormous byte[] for a term.\n\nThe bug was previously hidden:\nFaultyDirectory doesnt override openSlice, so CFS must not be used at flush if you want to trigger the bug.\nFailtyIndexInput's clone is broken, it uses \"new\" but doesn't seek the clone to the right place. This causes a disaster with BufferedIndexInput (which it extends), because BufferedIndexInput (not just the delegate) must \"know\" its position since it has seek-within-block etc code...\n\nIt seems with this test (very simple one), that only 3.x codec triggers it because its term dict relies upon clone()'s being seek'd to right place. \n\nI'm not sure what other codecs rely upon this, but imo we should also add a low-level test for directories that does something like this to ensure its really tested:\n\n\ndir.createOutput(x);\ndir.openInput(x);\ninput.seek(somewhere);\nclone = input.clone();\nassertEquals(somewhere, clone.getFilePointer());",
    "attachments": {
        "LUCENE-5066.patch": "https://issues.apache.org/jira/secure/attachment/12588526/LUCENE-5066.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2013-06-19T03:28:54+0000",
            "content": "here's a patch against 4.x ",
            "author": "Robert Muir",
            "id": "comment-13687556"
        },
        {
            "date": "2013-06-19T04:10:21+0000",
            "content": "I opened LUCENE-5067 for a way to add tests for this (and other existing ones) for all of our directories.\n\nWe should also open another issue to improve codecs so that they don't OOM trying to allocate absurdly-huge datastructures on bugs, but instead trip a check or an assert.\n\nWe discussed some of these ideas at berlin buzzwords:\n\ninstead of:\n\nint size = readVint();\nbyte something[] = new byte[size]; // gives OOM if 'size' is corrupt: no chance for read past EOF\nreadBytes(something, size);\n\n\n\nwe can do:\n\nint size = readVint();\nassert size < MAX_SIZE; // for something like terms with a bounded limit\nassert getFilePointer() + size < length(); // for something unbounded, at least it must not exceed the file's length\nbyte something[] = new byte[size];\nreadBytes(something, size);\n\n\n ",
            "author": "Robert Muir",
            "id": "comment-13687581"
        },
        {
            "date": "2013-06-19T17:12:27+0000",
            "content": "+1 patch looks good\n\nMaybe we should pull out a public static final MAX_TERM_LENGTH_BYTES\nin IndexWriter?  And DWPT references that, and this added assert in\nTermBuffer.java uses it too?  Shai needed to use it recently as well... ",
            "author": "Michael McCandless",
            "id": "comment-13688168"
        },
        {
            "date": "2013-06-19T22:09:22+0000",
            "content": "I mentioned this in the email: should we do it here under this issue?\n\nre above: I think we should spin off an issue to improve the codec checks (so we get assert fails at least, rather than OOM), i imagine this would be part of that issue, but can do it here too. ",
            "author": "Robert Muir",
            "id": "comment-13688532"
        }
    ]
}