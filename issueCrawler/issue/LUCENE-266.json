{
    "id": "LUCENE-266",
    "title": "German Analyzer does not handle search terms with asterisks",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/search"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "1.4",
        "resolution": "Won't Fix",
        "status": "Closed"
    },
    "description": "I created a test set of text files with special German characters in them and in\nUTF-8 format, which I then indexed using GermanAnalyzer and an adapted Lucene\ndemo program - IndexFiles.java. But the QueryParser in the demo SearchFiles.java\nalways returns the search term containing the original German umlauts or sz\nletters, whenever I use the wildcard asterisk. It does not replace the\numlauts and sz letters as I would expect it to do before performing a search.\nExamples: Using the GermanAnalyzer, QueryParser returns these words in lower\ncase, but with the German umlaut letters unchanged in the parsed queries:\nB\u00c3\u00bcrger*, Schl\u00c3\u00bcssel*, W\u00c3\u00a4hr*, Stra\u00c3\u009f, herk\u00c3\u00b6mm, st\u00c3\u00a4dt, \u00c3\u0084, \u00c3\u0096, \u00c3\u009c*.\n(When the above words appear with broken letters, here they are in ASCII format:\nBuerger*, Schluessel*, Waehr*, Strasz*, herkoemm, staedt*, AE*, OE*, UE*).\nThis is leading to Lucene exceptions such as BooleanQuery.TooManyClauses in our\nsystem that uses Lucene.",
    "attachments": {
        "ASF.LICENSE.NOT.GRANTED--GermanAnalyzerTest.java": "https://issues.apache.org/jira/secure/attachment/12312384/ASF.LICENSE.NOT.GRANTED--GermanAnalyzerTest.java"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2004-08-22T02:52:33+0000",
            "content": "hi\n\nif your files are utf-8 encoded and you are using the lucene demo program, you\nhave to change the following line:\n\nReader reader = new BufferedReader(new InputStreamReader(is));\n\nto\n\nReader reader = new BufferedReader(new InputStreamReader(is, \"UTF-8\"));\n\nin FileDocument.java. If done, utf-8 encoded files can be indexed and all the\nsearchers for german special characters work.\n\nregards\nBernhard ",
            "author": "Bernhard Messer",
            "id": "comment-12321854"
        },
        {
            "date": "2004-08-22T18:47:16+0000",
            "content": "Created an attachment (id=12504)\nA JUnit test for creating a German word index and searches including wildcard. ",
            "author": "Kenneth Aitken",
            "id": "comment-12321855"
        },
        {
            "date": "2004-08-22T18:50:42+0000",
            "content": "I append two JUnit test cases in a Java file (UTF-8 format). The one which\nsearches for whole words succeeds. The test case which performs wildcard\nsearches fails. ",
            "author": "Kenneth Aitken",
            "id": "comment-12321856"
        },
        {
            "date": "2004-08-22T19:17:58+0000",
            "content": "Hi Bernhard\nPlease see my test case. It is done with strings within the methods, so no file\naccess is involved. Searches for whole words work without any problem. But\nsearches with wildcards with German special characters before the wildcard\nreturned no results.\nI had the same result with the adapted Lucene demo files, when I changed\nFileDocument.java as you suggested. ",
            "author": "Kenneth Aitken",
            "id": "comment-12321857"
        },
        {
            "date": "2004-08-22T19:20:21+0000",
            "content": "You're missing something very critical here - QueryParser does not analyze wildcard queries.  If you \nwish to augment how this works, one option is to subclass QueryParser and override getWildcardQuery \nand somehow use the designated analyzer.  But, this is tricky - what should the analyzer do when the \nquery is \"some*thing\"?  \n\nI'm going to mark this as WONTFIX as it is working as it is meant to, although I do understand the \ndilemma. ",
            "author": "Erik Hatcher",
            "id": "comment-12321858"
        },
        {
            "date": "2004-08-22T21:14:31+0000",
            "content": "Hi Erik\n\nThank you for your comments. I will think about your advice. However, I don't\nsee why \"some*thing\" might be a problem for the analyzer. Could you please\nexplain this?\nThanks. ",
            "author": "Kenneth Aitken",
            "id": "comment-12321859"
        },
        {
            "date": "2004-08-22T23:23:03+0000",
            "content": "The problem is that an analyzer would (generally) split \"some*thing\" into \"some\" and \"thing\" and you'd \nlose the wildcard character altogether.  This is most definitely a confusing situation.  QueryParser is \ndecent for general purpose use, but certainly lacks in situations like this where you do need some \nnormalization of the text.  Thankfully, though, QueryParser does have extension points allowing you to \ntweak how Query's are created. ",
            "author": "Erik Hatcher",
            "id": "comment-12321860"
        }
    ]
}