{
    "id": "LUCENE-2023",
    "title": "Improve performance of SmartChineseAnalyzer",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "I've noticed SmartChineseAnalyzer is a bit slow, compared to say CJKAnalyzer on chinese text.\n\nThis patch improves the internal hhmm implementation. \nTime to index my chinese corpus is 75% of the previous time.",
    "attachments": {
        "LUCENE-2023.patch": "https://issues.apache.org/jira/secure/attachment/12423694/LUCENE-2023.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-10-30T17:36:11+0000",
            "content": "If we have a 2.9.2 release, can this be there too? ",
            "author": "DM Smith",
            "id": "comment-12772003"
        },
        {
            "date": "2009-10-30T17:59:52+0000",
            "content": "DM, personally I don't see the slowness as a bug... though I am very aware word segmentation speed is really important to chinese users.\n\nbut if all the other devs jumped up and said yeah its really critical to be in a bugfix release, i'd backport it to java 1.4\n\nin my opinion its just an improvement. ",
            "author": "Robert Muir",
            "id": "comment-12772013"
        },
        {
            "date": "2009-10-30T18:20:03+0000",
            "content": "I fully understand that at some point, \"just say no.\"\n\nI don't think it warrants a new bug fix release, but if there is one then it would be a \"nice to have\" iff the backport is a trivial effort.\n\nThat said, a 25% improvement is substantial. ",
            "author": "DM Smith",
            "id": "comment-12772022"
        },
        {
            "date": "2009-10-30T18:20:35+0000",
            "content": "How long are you stuck on 1.4 DM  You guys are gonna upgrade those Macs someday aren't you? Or is it a matter of supporting that platform for your users?\n\nIMO, it wouldn't be horrible to stretch easily backported optimizations considering this is the last 1.4 release - as long as someone is willing to do it - and we have another release. ",
            "author": "Mark Miller",
            "id": "comment-12772023"
        },
        {
            "date": "2009-10-30T18:26:48+0000",
            "content": "well, i'm more than willing to do the backport, if will be used.\n\nfor now, maybe someone can take a look and double-check the patch for trunk, smartcn is always dangerous territory.  \n\n(for the record the big bottleneck was the hashmap usage in these graphs, its rather silly since its indexed by int, you know the number of values up front, etc) ",
            "author": "Robert Muir",
            "id": "comment-12772027"
        },
        {
            "date": "2009-10-30T18:37:02+0000",
            "content": "Thanks, Mark.\n\nI'm stuck with 1.4 for the near future.  I wish people would upgrade their MacOS! I'll have a release soon and then another in 4-6 months. The release after that will be Java 5 and will probably follow shortly after.\n\nI'm willing to do the work, especially if it is trivial and it is something I use Perhaps, it would be best to create a separate issue targeting 2.9.2 (or whatever the next release might be), linking it to the original(s), mark it as an \"Improvement\" and attach appropriate patch(es). Maybe one issue for all improvements, or one per \"parent\" issue? Then if there is a bugfix that necessitates such a release, the patches would be applied for that release. ",
            "author": "DM Smith",
            "id": "comment-12772034"
        },
        {
            "date": "2009-10-30T19:29:19+0000",
            "content": "Robert,\nYou have in BigramDictionary:\n\n  public boolean isToExist(int to) {\n    return to < tokenPairListTable.length && tokenPairListTable[to] != null;\n  }\n\n\nAnd you call it in:\n\n  public void addSegTokenPair(SegTokenPair tokenPair) {\n    final int to = tokenPair.to;\n    if (!isToExist(to)) {\n      ArrayList<SegTokenPair> newlist = new ArrayList<SegTokenPair>();\n      newlist.add(tokenPair);\n      tokenPairListTable[to] = newlist;\n      tableSize++;\n    } else {\n      List<SegTokenPair> tokenPairList = tokenPairListTable[to];\n      tokenPairList.add(tokenPair);\n    }\n  }\n\n\n\nThe check in addSegTokenPair assumes the isToExist(to) returns false when \"to\" is in bounds because \"tokenPairListTable[to]\" will throw an array bounds exception otherwise. Is it an invariant that tokenPair.to will always be in bounds?\n\nIn the same way the array in SegGraph, does the same thing.\n\nWith the former implementation, it did not have an issue.\n\nOther than that, it looks good. ",
            "author": "DM Smith",
            "id": "comment-12772050"
        },
        {
            "date": "2009-10-30T19:32:53+0000",
            "content": "hi DM,\n\ni think the bounds checks are redundant actually, \nWith both situations, the bounds are calculated up front in the constructor.\n\nIs it an invariant that tokenPair.to will always be in bounds?\n\nYes, in this case.\n\nThe reason I did this is for isToExist, etc is because those methods are public... but this stuff is pkg private anyway so maybe i should delete the bounds checks altogether??? ",
            "author": "Robert Muir",
            "id": "comment-12772054"
        },
        {
            "date": "2009-10-30T19:44:14+0000",
            "content": "change these redundant bounds checks to assertions as DM observed. ",
            "author": "Robert Muir",
            "id": "comment-12772058"
        },
        {
            "date": "2009-11-01T14:50:28+0000",
            "content": "updated patch, shaves off another 5%\n\nmy avg indexing throughput:\n\n\tcjkanalyzer: 3447k/s\n\torig smartcn: 1357k/s\n\tpatched smartcn: 1965k/s\n\n\n\nthere are serious memory consumption problems in the n^2 part of the algorithm (BiSegGraph), will see about improving it more. ",
            "author": "Robert Muir",
            "id": "comment-12772325"
        },
        {
            "date": "2009-11-01T16:45:18+0000",
            "content": "create a generic graph that is reusable, used by both SegGraph and BiSegGraph.\nThis cleans up the code a lot and prevents billions of arraylists from being created in n^2 style. ",
            "author": "Robert Muir",
            "id": "comment-12772341"
        },
        {
            "date": "2009-11-01T18:39:18+0000",
            "content": "in BiSegGraph, char[] was being created in n^2 fashion for each edge (SegTokenPair), even though its only used for weight calculation.\ninstead, add methods to BigramDictionary to get the frequency of a bigram: getFrequency(char left[], char right[]) without this silliness.\n\nnew figures are:\n\n\tcjkanalyzer: 3447k/s\n\torig smartcn: 1357k/s\n\tpatched smartcn: 2125k/s\n\n ",
            "author": "Robert Muir",
            "id": "comment-12772346"
        },
        {
            "date": "2009-11-01T18:47:09+0000",
            "content": "Question, the smartcn internals are pkg private (and marked experimental to boot),\nI'd like to keep this clean and theres some unused stuff that could now be deprecated or removed.\n\nshould this be 3.0 or 3.1? should i deprecate or clean house (since its experimental and pkg private)?\n\nThanks! ",
            "author": "Robert Muir",
            "id": "comment-12772348"
        },
        {
            "date": "2009-11-01T18:56:40+0000",
            "content": "Internals are internals. Anyone digging into smartcn's internals should be hog-tied and whipped. They can change at any moment and without warning. (IMHO) ",
            "author": "DM Smith",
            "id": "comment-12772350"
        },
        {
            "date": "2009-11-01T19:00:21+0000",
            "content": "Anyone digging into smartcn's internals should be hog-tied and whipped\n\noh no, I think I am in trouble then!  ",
            "author": "Robert Muir",
            "id": "comment-12772352"
        },
        {
            "date": "2009-11-01T19:14:07+0000",
            "content": "i think this BiSegGraph might be a lot better as a 2D array of weights instead of complex object graph.\ni'll try it and see what happens. ",
            "author": "Robert Muir",
            "id": "comment-12772356"
        },
        {
            "date": "2009-11-02T14:37:00+0000",
            "content": "latest iteration, gets rid of SegTokenPair/PathNode.\nBiSegGraph still isn't as simple or efficient as it should be,\nbut my indexing speed is up to 2400k/s  ",
            "author": "Robert Muir",
            "id": "comment-12772522"
        },
        {
            "date": "2009-11-02T17:43:53+0000",
            "content": "refactor a lot of this analyzer:\n\n\tmove hhmm specific stuff (like WordType, CharType, Utility) into hhmm package\n\tmove/remove tokenfilter specific stuff (like lowercasing, full-width conversion) out of hhmm package (uses LowerCaseFilter, adds FullWidthFilter)\n\tremove the stopwords list, it was full of various punctuation, all of which got converted by \"SegTokenFilter\" into a comma anyway. instead just don't emit punctuation.\n\n\n\nto me, this refactoring makes the analyzer easier to debug. it also happens to improve performance (up to 2500k/s now) ",
            "author": "Robert Muir",
            "id": "comment-12772569"
        },
        {
            "date": "2009-11-02T20:11:37+0000",
            "content": "fix WordTokenFilter to use Version, because if its not going to output delimiters thru stopFilter and then remove them, then it needs to adjust posInc (depending on version) ",
            "author": "Robert Muir",
            "id": "comment-12772638"
        },
        {
            "date": "2009-11-08T09:48:43+0000",
            "content": "if no one objects I'd rather work this into 3.1, along with some refactoring of this code.\n\nDM Smith, if this causes you a problem I would rather just upload a java 1.4 patch for you to improve your performance than slip it into 3.0.  there was already a bug in 2.9 in this analyzer so I don't want to introduce a new one without having a lot of time to play with this code. ",
            "author": "Robert Muir",
            "id": "comment-12774750"
        },
        {
            "date": "2009-12-20T21:21:46+0000",
            "content": "I think given that this package is brand new, and in contrib (no back compat promise unless explicitly stated), and Robert has some solid improvements, it should be fine to make non-back-compatible changes here.\n\nAlso I think we should only do this for 3.1?  This is a big change... and it breaks back compat.  It shouldn't be backported to past releases? ",
            "author": "Michael McCandless",
            "id": "comment-12793044"
        },
        {
            "date": "2013-07-23T18:44:48+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13717065"
        },
        {
            "date": "2014-04-16T12:54:27+0000",
            "content": "Move issue to Lucene 4.9. ",
            "author": "Uwe Schindler",
            "id": "comment-13970761"
        }
    ]
}