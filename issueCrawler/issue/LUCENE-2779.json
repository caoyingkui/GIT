{
    "id": "LUCENE-2779",
    "title": "Use ConcurrentHashMap in RAMDirectory",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/store"
        ],
        "type": "Improvement",
        "fix_versions": [
            "3.1",
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "RAMDirectory synchronizes on its instance in many places to protect access to map of RAMFiles, in addition to updating the sizeInBytes member. In many places the sync is done for 'read' purposes, while only in few places we need 'write' access. This looks like a perfect use case for ConcurrentHashMap\n\nAlso, syncing around sizeInBytes is unnecessary IMO, since it's an AtomicLong ...\n\nI'll post a patch shortly.",
    "attachments": {
        "TestCHM.java": "https://issues.apache.org/jira/secure/attachment/12464871/TestCHM.java",
        "LUCENE-2779-backwardsfix.patch": "https://issues.apache.org/jira/secure/attachment/12464905/LUCENE-2779-backwardsfix.patch",
        "LUCENE-2779.patch": "https://issues.apache.org/jira/secure/attachment/12464801/LUCENE-2779.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-11-25T17:43:23+0000",
            "content": "Perhaps we can eliminate synchronization entirely by using ConcurrentHashMap - I'll check that too. ",
            "author": "Shai Erera",
            "id": "comment-12935812"
        },
        {
            "date": "2010-11-25T17:49:54+0000",
            "content": "Using RWLock is needless.\nsynchronized performs pretty nice on modern JVMs\nto at least some get contention on RAMDir you need to index+reopen gazillion times a second - not a likely scenario. Usual searching doesn't hit any of these locks\nthat's premature optimization, imo ",
            "author": "Earwin Burrfoot",
            "id": "comment-12935813"
        },
        {
            "date": "2010-11-25T17:53:47+0000",
            "content": "In fact, uncontended synchronized block might perform faster than RWLock. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12935816"
        },
        {
            "date": "2010-11-28T06:10:57+0000",
            "content": "In my app, I need to server ~50 qps on a single JVM. And that JVM is dedicated entirely for reading the index (so the 'WriteLock' will in fact never be required. I don't think this will be a huge performance gain, but as one smart guy once said on the mailing list, if we keep adding 1 to 1, eventually it adds up to a big gain.\n\nSo I do think RWLock is in place. Also, I think CocurrentHashMap nearly perfectly matches here, except for the listAll() case where I still need to figure out if something will go wrong in case the map is modified in the middle of iteration. ",
            "author": "Shai Erera",
            "id": "comment-12964500"
        },
        {
            "date": "2010-11-28T08:10:43+0000",
            "content": "If you don't write, you don't care about locking in RAMDir at all. Your reader opens files once, and then never ever does it again.\nBelieve me, this is so pointless. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12964511"
        },
        {
            "date": "2010-11-28T08:19:40+0000",
            "content": "I mean, even if aquiring locks costed you one second each, you're not going to notice that, as in your case it is a startup-only cost.\n\nLook at this Directory impl I used for a while - https://gist.github.com/715617 - it uses synchronizedMap() over HashMap.\nI had around 100qps with it on 4/8-way boxes AND pretty frequent updates.\nI benchmarked a switch to RWLock and it yielded zero benefits. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12964512"
        },
        {
            "date": "2010-11-28T08:46:55+0000",
            "content": "Shai, I actually think Earwin is right with his claim that this is unneeded / pointless really. Performance on modern JVMs is very good for both RWLock and synchronized blocks and to make a big difference heavy contention is needed anyway. I would not expect any difference if you are on a Java 6 JVM at all even if you'd have heavy contention. I have looked into this too a while ago and came to the same conclusion as earwin, there seem to be no real gain in refactoring this to use RWLocks instead fo sync blocks.  ",
            "author": "Simon Willnauer",
            "id": "comment-12964514"
        },
        {
            "date": "2010-11-28T08:53:19+0000",
            "content": "My primary point was that you're not going to have contention in that place at all. Take these 100queries/s, turn them into 100reopens+commits/s, multiply by ten, and then maybe you start noticing something. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12964515"
        },
        {
            "date": "2010-11-28T09:16:21+0000",
            "content": "I've changed RAMDir to use ConcurrentHashMap and everything works (still need to run contrib and Solr tests) so far, w/o any synchronization (neither synchronize nor RWLock). While I don't expect any major perf gains from this, I think it simplifies the code, a bit. I'll post a patch shortly. ",
            "author": "Shai Erera",
            "id": "comment-12964516"
        },
        {
            "date": "2010-11-28T09:47:48+0000",
            "content": "Patch replaces HashMap w/ ConcurrentHashMap and removes synchronized blocks. All tests pass. ",
            "author": "Shai Erera",
            "id": "comment-12964521"
        },
        {
            "date": "2010-11-28T12:43:11+0000",
            "content": "Changed title and description to better match the issue's topic. ",
            "author": "Shai Erera",
            "id": "comment-12964543"
        },
        {
            "date": "2010-11-28T14:11:15+0000",
            "content": "I'm happy with CHM.\nThe only thing bothering me is RAMDir.listAll() method. It feels broken both in your directory and the one I posted. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12964556"
        },
        {
            "date": "2010-11-28T15:55:38+0000",
            "content": "The generic problem with optimizing something is the risk of introducing bugs (so hopefully we're optimizing where it counts).\n\nAt a quick glance, I see:\n\n\ta race in listAll() (as earwin indicated) that can cause an AIOB exception\n\ta race in createOutput that can lead to an incorrect directory size (multiple threads can subtract the size of the same file being overwritten).  This probably isn't an issue with how Lucene uses RAMDirectory.\n\n\n\nChanging how concurrency works is rarely easy  ",
            "author": "Yonik Seeley",
            "id": "comment-12964565"
        },
        {
            "date": "2010-11-28T16:39:09+0000",
            "content": "I checked CHM iterators and they are safe w.r.t the map being modified after the iterator was obtained. It's easily testable: I added 3 items to CHM and called keySet.iterator(). I then called next() once, removed an item, called next() again and added an item. The changes were no visible to the iterator and it returned 3 items only (those that I added first). So I think we're safe on that.\n\nAbout remove I'll have to look in the code (not near the comp now) - but I had a feeling that it is safe too, b/c the file is first removed, which is a blocking 'write' operation and only if it isn't null then the AtomicLong is updated. But I'll Check the code again, and maybe even add a concurrent test case.\n\nThanks for the review. If in the end this change will smell bad, I'll close the issue - introducing concurrency bugs is the last thing we need . ",
            "author": "Shai Erera",
            "id": "comment-12964570"
        },
        {
            "date": "2010-11-28T17:08:29+0000",
            "content": "I checked CHM iterators and they are safe w.r.t the map being modified after the iterator was obtained.\n\nYes, but that wasn't the problem.  The iterator for ketSet \"guarantees to traverse elements as they existed upon construction of the iterator\".\nThe real problem is that the String[] is created based on the size of the keySet, and then later, an iterator is created over the keySet.  If an addition to the set is done between those two events, the iterator will traverse more elements than there is room for - hence the AIOB exception.\n\nAbout remove I'll have to look in the code (not near the comp now) - but I had a feeling that it is safe too, b/c the file is first removed,\n\nThe createOutput issue is that get() was used (not remove) so multiple threads could overwrite the same file and all of them subtract the size. ",
            "author": "Yonik Seeley",
            "id": "comment-12964575"
        },
        {
            "date": "2010-11-28T17:21:02+0000",
            "content": "Anyway, you should be able to replace all the code in listAll with\n\n  return (String[])fileMap.keySet().toArray();\n\n\n\nThe toArray() code of collection is smart and can handle the number of elements changing.\n\nAnd for createOutput, replacing get() with remove() should be sufficient. ",
            "author": "Yonik Seeley",
            "id": "comment-12964576"
        },
        {
            "date": "2010-11-29T04:27:23+0000",
            "content": "Fixed createOutput and listAll as Yonik suggested. Thanks Yonik - it looks safer now . ",
            "author": "Shai Erera",
            "id": "comment-12964626"
        },
        {
            "date": "2010-11-29T15:20:07+0000",
            "content": "Committed revision 1040138 (trunk).\nCommitted revision 1040145 (3x). ",
            "author": "Shai Erera",
            "id": "comment-12964773"
        },
        {
            "date": "2010-11-29T16:04:53+0000",
            "content": "Note: there is an important change of semantics to listAll() - it can now \"lie\".\nPreviously, it always gave an accurate listing of files as they existed at some point in time.  Now, you can get back a list of files that never really existed together at any point in time (i.e. the lie).\n\nI'm OK with these new semantics because we have the same limitation already on FSDirectory.  This issue manifested in LUCENE-2585, so we should keep in mind that we could now also see this with RAMDirectory.\n\nHere's a test program that demonstrates the issue by having a single writer adding 5,-5 to a map before removing the old 4,-4, etc.  At any point in time, there will always be a pair of numbers.  The single reader looks for this and spits out an error if not found.\n\noutput from my box:\niterations=1000000 errors=4120 minLen=0 maxLen=10 ",
            "author": "Yonik Seeley",
            "id": "comment-12964789"
        },
        {
            "date": "2010-11-29T19:07:21+0000",
            "content": "I think we can still avoid this if we clone the keySet(). Since listAll I'd not a frequently called method, and since I believe RAMDirs do not hold hundreds of files, I believe It won't be expensive. What do you think? ",
            "author": "Shai Erera",
            "id": "comment-12964882"
        },
        {
            "date": "2010-11-29T20:23:12+0000",
            "content": "I don't believe cloning the keySet() will make it more \"snapshotty\". There's no way to get a completely consistent snapshot of some concurrent datastructure without locking it completely, or using a variant of copy-on-write approach. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12964920"
        },
        {
            "date": "2010-11-29T22:09:55+0000",
            "content": "This commit broke backwards compatibility in 3.x.\n\nPlease revert 3.x, we are no longer backwards compatible, because we changed type of a protected field!\n\n(and: is it so complicated to do ant test-backwards before committing such a change?) ",
            "author": "Uwe Schindler",
            "id": "comment-12964968"
        },
        {
            "date": "2010-11-29T22:10:44+0000",
            "content": "If we really want to break backwards compatibility, here would be the fix for 3.x backwards branch. ",
            "author": "Uwe Schindler",
            "id": "comment-12964971"
        },
        {
            "date": "2010-11-29T22:25:57+0000",
            "content": "Reverted 3.x commit in revision: 1040320\n\n(this change not even had a CHANGES.txt entry!) Please try to resolve this in a correct way! ",
            "author": "Uwe Schindler",
            "id": "comment-12964978"
        },
        {
            "date": "2010-11-29T22:27:12+0000",
            "content": "Maybe we should commit it to 4.0 only? Doesn't look like a really important patch, that just has to be backported. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12964982"
        },
        {
            "date": "2010-11-29T22:30:32+0000",
            "content": "Maybe we should commit it to 4.0 only? Doesn't look like a really important patch, that just has to be backported. \n\n+1; And please add a CHANGES.txt entry for trunk. ",
            "author": "Uwe Schindler",
            "id": "comment-12964987"
        },
        {
            "date": "2010-11-29T22:38:21+0000",
            "content": "In Shai's defense, a CHANGES entry for 4.0 is pretty questionable.  It's not going to be a noticeable enough speedup to call out as an end-user feature.  Any super-expert level people extending RAMDirectory and accessing the Map directly will get a compile error (a good thing) and instantly know what's changed.  IMO, this falls into the category of \"little internal cleanup\" and people can consult the SVN logs or mailing lists if they wish to know every single one of them  ",
            "author": "Yonik Seeley",
            "id": "comment-12964990"
        },
        {
            "date": "2010-11-29T22:55:30+0000",
            "content": "OK, but in 3.x its clearly a backwards break. So if we change it there, we have to put it into backwards break section and apply the attached patch to backwards/ ",
            "author": "Uwe Schindler",
            "id": "comment-12964993"
        },
        {
            "date": "2010-11-30T04:37:01+0000",
            "content": "I didn't add a CHANGES entry because I consider it an internal change. And as a user, I wouldn't know what to do w/ an entry that says \"RAMDirectory now uses CHM\".\n\nI don't believe cloning the keySet() will make it more \"snapshotty\".\n\nCloning the keySet() will be exactly the 'snapshotty' behavior we're looking for. Before I made the change, you could call listAll(), lock RAMDir, return the array and before/after that files could be added/removed. W/ the clone, we'll get the same behavior - files can be added/removed before the clone, clone would reflect those changes, whatever happens after the clone is invisible to the iterator - hence why I consider it snapshotty. ",
            "author": "Shai Erera",
            "id": "comment-12965086"
        },
        {
            "date": "2010-11-30T07:03:35+0000",
            "content": "Doesn't look like a really important patch, that just has to be backported. \n\nEarwin, I did not just backport it. This time I deviated from my usual work habit, which is changing 3x before trunk, because I did want to cause any mergeprops troubles. But in real life, I work on and improve 3x, and back/forward port to trunk \u2013 at least until trunk/4.0 release will be anywhere near sight. So for all intent and purposes, this improvement should have gone into 3x, as far as I'm concerned. That that we have an issue w/ our backwards tests layer (sent a separate email about it) is unrelated to the issue.\n\nAs a general practice though, I do not neglect 3x branch \u2013 if something that I do can fit there, I will put it there, whether it's a major performance improvement, or a minor house cleaning. ",
            "author": "Shai Erera",
            "id": "comment-12965130"
        },
        {
            "date": "2010-11-30T09:25:43+0000",
            "content": "Attached patch combines the changes to RAMDir and the changes to MockRAMDir under backwards.\n\nI also made the map final, as Uwe suggested, but it means that on close() it cannot be nullified, so instead I clear()-ed it. I think it achieves the same goal - clearing the references to RAMFiles.\n\nAlso, what do you know, I've hit an AIOB exception thrown from listAll() when it called toArray() . So I cloned the set of keys first, which protects against it. After the set is cloned, it's not affected by any changes to the map, and therefore toArray() works safely, and returns some point in time snapshot of the map. The \"point in time\" is not necessarily the one that existed when you called listAll(), but the cloned set becomes the \"point in time\" snapshot. I think it's ok.\n\nI've hit it when running backwards tests (from TestIndexWriterExceptions), but not from core. Perhaps it was just a threading issue.\n\nIf you're ok w/ that, I'll make the same changes to trunk as well (making the map final and cloning the set).\n\nAll tests pass (this time, including backwards ). ",
            "author": "Shai Erera",
            "id": "comment-12965167"
        },
        {
            "date": "2010-11-30T10:54:08+0000",
            "content": "Cloning the keySet() will be exactly the 'snapshotty' behavior we're looking for. Before I made the change, you could call listAll(), lock RAMDir, return the array and before/after that files could be added/removed. W/ the clone, we'll get the same behavior - files can be added/removed before the clone, clone would reflect those changes, whatever happens after the clone is invisible to the iterator - hence why I consider it snapshotty.\nThere are still weird cases, when file B was added after deleting A, but you see both in listAll(). These - remain, so it's not a \"point in time\" it's more like a \"span in time\".\nWhatever happened after toArray was invisible to array too, so the behaviour hasn't changed.\n\nAlso, what do you know, I've hit an AIOB exception thrown from listAll() when it called toArray() \nBut this fact is really interesting. toArray() with no parameters failed on CHM? Cloning has a meaning now \n\nEarwin, I did not just backport it. ........\nDidn't mean to offend anyone, sorry if I did. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12965194"
        },
        {
            "date": "2010-11-30T11:38:47+0000",
            "content": "toArray() with no parameters failed on CHM?\n\nI don't use toArray(). I use toArray(new String[0]) because I don't want to cast the returned array to String[]. According to the javadocs:\n\nReturns an array containing all of the elements in this collection; the runtime type of the returned array is that of the specified array. If the collection fits in the specified array, it is returned therein. Otherwise, a new array is allocated with the runtime type of the specified array and the size of this collection.\n\nThis method guarantees the right array size will be allocated. ",
            "author": "Shai Erera",
            "id": "comment-12965205"
        },
        {
            "date": "2010-11-30T14:35:45+0000",
            "content": "I plan to commit this tomorrow, so if you have comments, please share them. ",
            "author": "Shai Erera",
            "id": "comment-12965234"
        },
        {
            "date": "2010-11-30T14:39:58+0000",
            "content": "I am fine, there is only a typo copied from my patch in the RuntimeException  ",
            "author": "Uwe Schindler",
            "id": "comment-12965235"
        },
        {
            "date": "2010-11-30T14:47:34+0000",
            "content": "OK, a few points on the latest patch:\n\n\tcloning the map does not change the \"lie\" (i.e. it's still not a point-in-time snapshot)... the constructor for the new set must iterate over the items also, so consistency is not increased.  You've just changed where the iteration happens.  You can see this by trying out my test program, and making the following change:\n\n      Object[] vals = map.keySet().toArray(new Integer[0]);\n      Object[] vals = new HashSet<String>(map.keySet()).toArray(new Integer[0]);\n\n\n\ttoArray() or toArray(T[]) should be safe to call on ConcurrentHashMap.keySet().  It works fine on my JVM (Oracle Java6)\n  What JVM are you using?\n\n ",
            "author": "Yonik Seeley",
            "id": "comment-12965240"
        },
        {
            "date": "2010-11-30T14:55:00+0000",
            "content": "Yonik: You are right. Sun Java 5 creates an ArrayList and then uses the Iterator and adds each key to the ArrayList. After that it calls toArray on the ArrayList. This is safe, as the iterator is documented to be safe. Java 6 is safe as AbstractCollection.toArray() is oficially documented to behave correctly.\n\nIf Shai wants to keep this code, he should at least use ArrayList instead of HashSet for the keys, as cloning is much faster then. But as ArrayList(Collection) uses toArray() in its ctor it may also be broken.\n\nMaybe Shai uses IBM JRocket? Or Harmony (Harmony is currently broken, HARMONY-6681)? ",
            "author": "Uwe Schindler",
            "id": "comment-12965245"
        },
        {
            "date": "2010-11-30T17:06:49+0000",
            "content": "Ok I can change the code to: new ArrayList(map.keySet()).toArray(). How's that sound?\n\nI use IBM JVM and I'll check the KeySet.toArray later when I'm in front of the comp. But I think we should have a resilient code and the one above seems right to me. What do you think? ",
            "author": "Shai Erera",
            "id": "comment-12965293"
        },
        {
            "date": "2010-11-30T17:18:10+0000",
            "content": "Quoting Sun JDK 1.6:\n\n\npublic ArrayList(Collection<? extends E> c) {\n  elementData = c.toArray();\n  size = elementData.length;\n  // c.toArray might (incorrectly) not return Object[] (see 6260652)\n  if (elementData.getClass() != Object[].class)\n    elementData = Arrays.copyOf(elementData, size, Object[].class);\n}\n\n\n\nIt calls toArray() on collection provided. You might as well skip wrapping with ArrayList and use toArray directly  ",
            "author": "Earwin Burrfoot",
            "id": "comment-12965296"
        },
        {
            "date": "2010-11-30T17:31:04+0000",
            "content": "Earwin: I wanted to say the same. \n\nJRockit and Harmony are broken, Sun JVM is correct. So remove the code and only use toArray(). We cannot work around bugs in non-oficially-supported JVMs (IBM wants to go away from JRocket, too). ",
            "author": "Uwe Schindler",
            "id": "comment-12965300"
        },
        {
            "date": "2010-11-30T17:31:18+0000",
            "content": "Hi:\n\nplease don't post sun/oracle proprietary source code on apache jira issues! ",
            "author": "Robert Muir",
            "id": "comment-12965301"
        },
        {
            "date": "2010-11-30T17:42:07+0000",
            "content": "The only performant variant of that code that works around all these bugs is the code snippet in Java SE6 JavaDocs, which is published by Sun as \"works-as-if-implemented-as\" in their JavaDocs: \n\n\nSet<String> set = #####.keySet();\nList<String> list = new ArrayList<String>(set.size());\nfor (String s : set) list.add(s);\nreturn list.toArray(new String[0]);\n\n\n\nBut I still don't think we should do this, its a bug outside Lucene in a seldom used JVM! Sun Java 5 and Sun Java 6 work 100% correct and never throw exceptions (by using techniques like above). ",
            "author": "Uwe Schindler",
            "id": "comment-12965303"
        },
        {
            "date": "2010-11-30T19:23:31+0000",
            "content": "I did hit an AIOOBE though, and I use IBM's JDK, which its CHM apparently uses AbstractCollection's impl of toArray(Object[]). The question is, why should we rely on toArray() implemented one way or the other in which JDK? If we think the best impl would be to allocate an ArrayList and add all the elements to it, then why not do this explicitly? Remember that before this change, listAll() would do almost exactly that - it iterated on the keySet() and added to items to an array, only then it could rely on size().\n\nApparently, doing new ArrayList(fileMap.keySet()) is not safe either, as internally it allocates an array and calls the set's toArray. So I ended up writing the following code and comment:\n\n\n    // NOTE: due to different implementations of different JDKs, it's not safe\n    // to do either:\n    // 1. return fileMap.keySet().toArray(new String[0])\n    // 2. return new ArrayList<String>(fileMap.keySet()).toArray(new String[0])\n    // as both can result in ArrayIndexOutOfBoundException, in case the keySet()\n    // is modified while this method is executed.\n    // Therefore we must clone the set in the following manner, never relying on\n    // keySet() size (even though it's used, ArrayList grows as needed.\n    Set<String> fileNames = fileMap.keySet();\n    List<String> names = new ArrayList<String>(fileNames.size());\n    for (String name : fileNames) names.add(name);\n    return names.toArray(new String[names.size()]);\n\n ",
            "author": "Shai Erera",
            "id": "comment-12965359"
        },
        {
            "date": "2010-11-30T19:39:42+0000",
            "content": "But I still don't think we should do this, its a bug outside Lucene in a seldom used JVM! \n\nActually I think [sadly] it is our responsibility to sidestep JRE bugs when we can.  We want to maximize Lucene's portability. ",
            "author": "Michael McCandless",
            "id": "comment-12965376"
        },
        {
            "date": "2010-11-30T19:46:46+0000",
            "content": "So I ended up writing the following code and comment\nLooks good. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12965380"
        },
        {
            "date": "2010-11-30T19:53:57+0000",
            "content": "The question is, why should we rely on toArray() implemented one way or the other in which JDK?\n\nBoth Harmony and IBM's implementations are broken - it's not an issue of a different but still valid implementation.\n\nActually I think [sadly] it is our responsibility to sidestep JRE bugs when we can. We want to maximize Lucene's portability.\n\n+1 (within reason)\nIn this case, it's relatively straightforward to do so, and it's not in an inner-loop. ",
            "author": "Yonik Seeley",
            "id": "comment-12965390"
        },
        {
            "date": "2010-11-30T19:58:18+0000",
            "content": "Code looks good, I would only chnage the comment to simply say, that:\n\nreturn fileMap.keySet().toArray(new String[0]);\n\n\nonly works correct in Suns JVM. The thing with new ArrayList(Collection) or any other thing is implemented different in every JVM (even new HashSet(Collection) can be broken if it uses toArray() for some weird reason). But just to note, according to the JDK documentation the above code should work - period. So we should only document that and say, that this is broken. ",
            "author": "Uwe Schindler",
            "id": "comment-12965396"
        },
        {
            "date": "2010-12-01T04:51:30+0000",
            "content": "Patch w/ the latest code + a typo fix. I will commit it later today. ",
            "author": "Shai Erera",
            "id": "comment-12965571"
        },
        {
            "date": "2010-12-01T15:06:28+0000",
            "content": "Committed revision 1041019 (3x).\nCommitted revision 1041039 (trunk).\n\nThanks all for the review ! ",
            "author": "Shai Erera",
            "id": "comment-12965709"
        },
        {
            "date": "2010-12-03T01:15:33+0000",
            "content": "  If the assumption still stands that an IndexInput will not be opened on a \"writing\" / unclosed IndexOutput, then RAMFile can also be improved when it comes to concurrency. The RAMOutputStream can maintain its own list of buffers (simple array list, no need to sync), and only when it gets closed, initialize the respective RAMFile with the list. This means most of the synchronize aspects of RAMFile can be removed. Also, on RAMFile, lastModified can be made volatile, and remove the sync on its respective methods. ",
            "author": "Shay Banon",
            "id": "comment-12966376"
        },
        {
            "date": "2010-12-03T09:22:44+0000",
            "content": "If the assumption still stands that an IndexInput will not be opened on a \"writing\" / unclosed IndexOutput, then RAMFile can also be improved when it comes to concurrency. \n\nThis assumption does still stand, and our unit tests now assert this \u2013 MockDirWrapper throws an exception if we ever try to open an II when an IO is still open against that file.\n\nThat said... we are considering relaxing this, because RT search needs to be able to access the doc stores with an II even as IO is appending to it. ",
            "author": "Michael McCandless",
            "id": "comment-12966457"
        },
        {
            "date": "2011-03-30T15:50:08+0000",
            "content": "Bulk close for 3.1 ",
            "author": "Grant Ingersoll",
            "id": "comment-13013375"
        }
    ]
}