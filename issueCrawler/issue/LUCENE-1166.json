{
    "id": "LUCENE-1166",
    "title": "A tokenfilter to decompose compound words",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "New Feature",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "A tokenfilter to decompose compound words you find in many germanic languages (like German, Swedish, ...) into single tokens.\n\nAn example: Donaudampfschiff would be decomposed to Donau, dampf, schiff so that you can find the word even when you only enter \"Schiff\".\n\nI use the hyphenation code from the Apache XML project FOP (http://xmlgraphics.apache.org/fop/) to do the first step of decomposition. Currently I use the FOP jars directly. I only use a handful of classes from the FOP project.\n\nMy question now:\nWould it be OK to copy this classes over to the Lucene project (renaming the packages of course) or should I stick with the dependency to the FOP jars? The FOP code uses the ASF V2 license as well.\n\nWhat do you think?",
    "attachments": {
        "de.xml": "https://issues.apache.org/jira/secure/attachment/12374855/de.xml",
        "CompoundTokenFilter.patch": "https://issues.apache.org/jira/secure/attachment/12374854/CompoundTokenFilter.patch",
        "hyphenation.dtd": "https://issues.apache.org/jira/secure/attachment/12374856/hyphenation.dtd"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-02-06T11:09:00+0000",
            "content": "A preliminary version of the token filter. ",
            "author": "Thomas Peuss",
            "id": "comment-12566084"
        },
        {
            "date": "2008-02-06T11:10:44+0000",
            "content": "A hyphenation grammar. You can download them from: http://downloads.sourceforge.net/offo/offo-hyphenation.zip?modtime=1168687306&big_mirror=0 ",
            "author": "Thomas Peuss",
            "id": "comment-12566085"
        },
        {
            "date": "2008-02-06T11:11:12+0000",
            "content": "The DTD describing the hyphenation grammar XML files. ",
            "author": "Thomas Peuss",
            "id": "comment-12566086"
        },
        {
            "date": "2008-02-06T16:39:21+0000",
            "content": "Hi Thomas,\n\nLooking at http://offo.sourceforge.net/hyphenation/licenses.html, which seems to be the same information as in the off-hyphenation.zip file you attached to this issue, the license issue may be a problem - the hyphenation data is covered by different licenses on a per-language basis.  For example, there are two German data files, and both are licensed under a LaTeX license, as is the Danish file, and these two languages are the most likely targets for your TokenFilter.  IANAL, but unless Apache licenses can be secured for this data, I don't think the files can be incorporated directly into an Apache project.\n\nAlso, I don't see Swedish among the hyphenation data licenses - is it covered in some other way? ",
            "author": "Steve Rowe",
            "id": "comment-12566188"
        },
        {
            "date": "2008-02-06T17:33:46+0000",
            "content": "Looking at http://offo.sourceforge.net/hyphenation/licenses.html, which seems to be the same information as in the off-hyphenation.zip file you attached to this issue, the license issue may be a problem - the hyphenation data is covered by different licenses on a per-language basis. For example, there are two German data files, and both are licensed under a LaTeX license, as is the Danish file, and these two languages are the most likely targets for your TokenFilter. IANAL, but unless Apache licenses can be secured for this data, I don't think the files can be incorporated directly into an Apache project.\n\nThis is true. And that's why I uploaded the two files without the ASF license grant. The FOP project does not have the files in the code base as well because of the licensing problem.\n\nAlso, I don't see Swedish among the hyphenation data licenses - is it covered in some other way?\nOFFO has no Swedish grammar file. We can generate a Swedish grammar file out of the LaTeX grammar files. I have a look into this tonight.\n\nAll other hyphenation implementations I have found so far use them either directly or in an converted variant like the FOP code. What we can do of course is to ask the authors of the LaTeX files if they want to license their work under the ASF license as well. It is worth a try. But I suppose that many email addresses in the LaTeX files are not used anymore. I try to contact the authors of the German grammar files tomorrow.\n\nBTW: an example for those that don't want to try the patch:\nInput token stream:\nRindfleisch\u00fcberwachungsgesetz Drahtschere abba\n\nOutput token stream:\n(Rindfleisch\u00fcberwachungsgesetz,0,29)\n(Rind,0,4,posIncr=0)\n(fleisch,4,11,posIncr=0)\n(\u00fcberwachung,11,22,posIncr=0)\n(gesetz,23,29,posIncr=0)\n(Drahtschere,30,41)\n(Draht,30,35,posIncr=0)\n(schere,35,41,posIncr=0)\n(abba,42,46) ",
            "author": "Thomas Peuss",
            "id": "comment-12566220"
        },
        {
            "date": "2008-02-07T12:22:51+0000",
            "content": "A Swedish hyphenation grammar is available at http://www.peuss.de/node/64 ",
            "author": "Thomas Peuss",
            "id": "comment-12566568"
        },
        {
            "date": "2008-02-12T11:09:19+0000",
            "content": "Changes:\n\n\tadded unittest\n\tminor tweaks for getting the encoding of the XML files right\n\n ",
            "author": "Thomas Peuss",
            "id": "comment-12568052"
        },
        {
            "date": "2008-02-14T16:22:12+0000",
            "content": "Updated version:\n\n\tnew dumb decomposition filter\n\t\n\t\tuses a brute-force approach by generating substrings and checking them against the dictionary\n\t\tseems to work better for languages that have no patterns file with a lot of special cases\n\t\tIs roughly 3 times slower than the decomposition filter using hyphenation patterns\n\t\tNo licensing problems because of the hyphenation pattern files\n\t\n\t\n\tRefactoring to have all methods used by both decomposition filters in one place\n\tMinor performance improvements\n\n ",
            "author": "Thomas Peuss",
            "id": "comment-12568985"
        },
        {
            "date": "2008-02-17T05:48:12+0000",
            "content": "I haven't looked at the patch.\nBut I'm wondering if a similar approach could be used for, say, word segmentation in Chinese?\nThat is, iterate through a string of Chinese characters, buffering them and looking up the buffered string in a Chinese dictionary.  Once there is a dictionary match, and the addition of the following character results in a string that has no entry in the dictionary, that previous buffered string can be considered a word/token.\n\nI'm not sure if your patch does something like this, but if it does, I am wondering if it is general enough that what you did can be used (as the basis of) word segmentation for Chinese, and thus for a Chinese Analyzer that's not just a dump n-gram Analyzer (which is what we have today). ",
            "author": "Otis Gospodnetic",
            "id": "comment-12569641"
        },
        {
            "date": "2008-02-17T15:24:40+0000",
            "content": "But I'm wondering if a similar approach could be used for, say, word segmentation in Chinese? That is, iterate through a string of Chinese characters, buffering them and looking up the buffered string in a Chinese dictionary. Once there is a dictionary match, and the addition of the following character results in a string that has no entry in the dictionary, that previous buffered string can be considered a word/token. I'm not sure if your patch does something like this, but if it does, I am wondering if it is general enough that what you did can be used (as the basis of) word segmentation for Chinese, and thus for a Chinese Analyzer that's not just a dump n-gram Analyzer (which is what we have today).\n\nCurrently the code adds a token to the stream when an n-gram from the current token in the token stream matches a word in the dictionary (I am only speaking about the DumbCompoundWordTokenFilter because I doubt that there exist hyphenation patterns for Chinese languages). I don't know much about the structure of Chinese characters to answer this questions in detail. You can have a look at the test-case in the patch to see how the filters work.\n\n ",
            "author": "Thomas Peuss",
            "id": "comment-12569708"
        },
        {
            "date": "2008-02-28T23:11:14+0000",
            "content": "Thomas, I think that might work for Chinese - going through the \"string\" of Chinese characters, one at a time, and looking up a dictionary after each additional character.  One you find a dictionary match, you look at one more character.  If that matches a dictionary entry, keep doing that until you keep matching dictionary entries (in order to grab the longest dictionary-matching string of characters).  If the next character does not match, then the previous/last character was the end of the dictionary entry.\nThat would work, no?\n\nAs for the license info, I think you could take the approach where the required libraries are not included in the contribution in the ASF repo, but are downloaded on the fly, at build time, much like some other contributions.  Could you do that? ",
            "author": "Otis Gospodnetic",
            "id": "comment-12573508"
        },
        {
            "date": "2008-03-03T10:33:40+0000",
            "content": "Thomas, I think that might work for Chinese - going through the \"string\" of Chinese characters, one at a time, and looking up a dictionary after each additional character. One you find a dictionary match, you look at one more character. If that matches a dictionary entry, keep doing that until you keep matching dictionary entries (in order to grab the longest dictionary-matching string of characters). If the next character does not match, then the previous/last character was the end of the dictionary entry. That would work, no?\n\nI have started to look into this. I will add the constructor parameter \"onlyLongestMatch\" (default is false).\n\nAs for the license info, I think you could take the approach where the required libraries are not included in the contribution in the ASF repo, but are downloaded on the fly, at build time, much like some other contributions. Could you do that?\n\nI pull the grammar files for the tests already. But I don't know if it makes sense to pull them on build time because the end-user can easily download them. I need the XML versions now - so the jar-file from Sourceforge does not help anymore (I have included the needed classes from the FOP project - they use the ASF license as well). ",
            "author": "Thomas Peuss",
            "id": "comment-12574418"
        },
        {
            "date": "2008-03-03T16:35:35+0000",
            "content": "Updated patch according to Otis suggestions for longest match.\n\nNext step: move to contrib ",
            "author": "Thomas Peuss",
            "id": "comment-12574604"
        },
        {
            "date": "2008-03-25T12:43:39+0000",
            "content": "Moved the compound word tokenfilter stuff to contrib. ",
            "author": "Thomas Peuss",
            "id": "comment-12581898"
        },
        {
            "date": "2008-03-25T12:49:16+0000",
            "content": "Moved compound word token filter to contrib. ",
            "author": "Thomas Peuss",
            "id": "comment-12581900"
        },
        {
            "date": "2008-03-25T12:56:27+0000",
            "content": "Dropped Java5 dependencies. ",
            "author": "Thomas Peuss",
            "id": "comment-12581903"
        },
        {
            "date": "2008-03-29T11:04:16+0000",
            "content": "Fixed a compilation bug in the testcase. ",
            "author": "Thomas Peuss",
            "id": "comment-12583303"
        },
        {
            "date": "2008-04-24T01:14:00+0000",
            "content": "\nI pull the grammar files for the tests already. But I don't know if it makes sense to pull them on build time because the end-user can easily download them. I need the XML versions now - so the jar-file from Sourceforge does not help anymore (I have included the needed classes from the FOP project - they use the ASF license as well).\n\nI think they have to download automatically, otherwise the automated tests, etc. will not run.  I applied the patch and ran \"ant test\" and it fails b/c I didn't download the files.\n\nAlso, much of the code has author tags that are not you, I am assuming you got it from FOP per your comments above, but can you explicitly mark all the files as to there origin? ",
            "author": "Grant Ingersoll",
            "id": "comment-12591895"
        },
        {
            "date": "2008-04-24T06:13:16+0000",
            "content": "All files in the package org.apache.lucene.analysis.compound.hyphenation are from the FOP project (as well ASF licensed). Should I add a comment to them to state from where they are? All other files are from me. I have to check why it fails when you run \"ant test\" by downloading a fresh copy of Lucene-trunk. ",
            "author": "Thomas Peuss",
            "id": "comment-12591930"
        },
        {
            "date": "2008-04-24T06:51:40+0000",
            "content": "The error is\n\n    [junit] Testsuite: org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter\n    [junit] Tests run: 4, Failures: 0, Errors: 2, Time elapsed: 2,139 sec\n    [junit]\n    [junit] Testcase: testHyphenationCompoundWordsDE(org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter):  Caused an ERROR\n    [junit] File not found: /home/thomas/projects/lucene-trunk-compound/hyphenation.dtd (No such file or directory)\n    [junit] org.apache.lucene.analysis.compound.hyphenation.HyphenationException: File not found: /home/thomas/projects/lucene-trunk-compound/hyphenation.dtd (No such file or directory)\n    [junit]     at org.apache.lucene.analysis.compound.hyphenation.PatternParser.parse(PatternParser.java:123)\n    [junit]     at org.apache.lucene.analysis.compound.hyphenation.HyphenationTree.loadPatterns(HyphenationTree.java:138)\n    [junit]     at org.apache.lucene.analysis.compound.HyphenationCompoundWordTokenFilter.getHyphenationTree(HyphenationCompoundWordTokenFilter.java:142)\n    [junit]     at org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter.testHyphenationCompoundWordsDE(TestCompoundWordTokenFilter.java:70)\n    [junit]\n    [junit]\n    [junit] Testcase: testHyphenationCompoundWordsDELongestMatch(org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter):      Caused an ERROR\n    [junit] File not found: /home/thomas/projects/lucene-trunk-compound/hyphenation.dtd (No such file or directory)\n    [junit] org.apache.lucene.analysis.compound.hyphenation.HyphenationException: File not found: /home/thomas/projects/lucene-trunk-compound/hyphenation.dtd (No such file or directory)\n    [junit]     at org.apache.lucene.analysis.compound.hyphenation.PatternParser.parse(PatternParser.java:123)\n    [junit]     at org.apache.lucene.analysis.compound.hyphenation.HyphenationTree.loadPatterns(HyphenationTree.java:138)\n    [junit]     at org.apache.lucene.analysis.compound.HyphenationCompoundWordTokenFilter.getHyphenationTree(HyphenationCompoundWordTokenFilter.java:142)\n    [junit]     at org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter.testHyphenationCompoundWordsDELongestMatch(TestCompoundWordTokenFilter.java:96)\n    [junit]\n    [junit]\n    [junit] Test org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter FAILED\n\n\n\nSo it does not find the hyphenation.dtd. I have to investigate how I can make that DTD know to the parser without copying the hyphenation.dtd to Lucene's base directory. ",
            "author": "Thomas Peuss",
            "id": "comment-12591936"
        },
        {
            "date": "2008-04-24T10:11:00+0000",
            "content": "\n\tFixed the problem with the hyphenation.dtd file that was not found\n\tRemoved all @author tags\n\tAdded a note to all files I copied from the FOP project\n\tAdded package.html files (not very much in there - but credits for the FOP project)\n\n ",
            "author": "Thomas Peuss",
            "id": "comment-12591969"
        },
        {
            "date": "2008-04-30T01:12:00+0000",
            "content": "So, why would I ever want to use a \"Dumb\" compound filter?  Any suggestions for a better name?  No need for a patch, I can just make the change. ",
            "author": "Grant Ingersoll",
            "id": "comment-12593194"
        },
        {
            "date": "2008-04-30T01:20:44+0000",
            "content": "This looks pretty good Thomas.  I think the last bit that would be good is to add to the package docs an example of start to finish using it, kind of like in the test case.  You might want to explain a little bit about where to get the hyphenation files, etc. (if I am understanding them correctly). \n\nI think if we can finish that up, we can look to commit.\n\nThe other interesting thing here, as an aside, is the Ternary Tree might be worth pulling up to a \"util\" package (no need to do so now, just thinking out loud), as it could be used for other interesting things.  For instance, see http://www.javaworld.com/javaworld/jw-02-2001/jw-0216-ternary.html   The version we have needs a little work, but I have been thinking about how it might be used to improve spelling, etc. ",
            "author": "Grant Ingersoll",
            "id": "comment-12593198"
        },
        {
            "date": "2008-04-30T03:16:53+0000",
            "content": "Ah, TST was in there, lovely!  +1 to what Grant said about getting it into util later.\nNoticed a misspelling in javadoc while glancing at TST: hibrid -> hybrid ",
            "author": "Otis Gospodnetic",
            "id": "comment-12593212"
        },
        {
            "date": "2008-04-30T03:28:57+0000",
            "content": "\n\n\ncool.  It needs some work, IMO to add more features, per that article  \nI sent, but no biggie.\n\n\nI saw some of those, but purposely left in the FOP typos... There were  \nmore than just that one.\n\n\n--------------------------\nGrant Ingersoll\n\nLucene Helpful Hints:\nhttp://wiki.apache.org/lucene-java/BasicsOfPerformance\nhttp://wiki.apache.org/lucene-java/LuceneFAQ\n\n\n\n\n\n ",
            "author": "Grant Ingersoll",
            "id": "comment-12593216"
        },
        {
            "date": "2008-04-30T06:49:40+0000",
            "content": "So, why would I ever want to use a \"Dumb\" compound filter? Any suggestions for a better name? No need for a patch, I can just make the change.\n\nA better name would be DictionaryCompoundWordTokenFilter. I called it \"Dumb\" because it uses a brute-force approach. But DictionaryCompoundWordTokenFilter characterizes it better. ",
            "author": "Thomas Peuss",
            "id": "comment-12593238"
        },
        {
            "date": "2008-04-30T09:17:56+0000",
            "content": "\n\tRenamed DumbCompoundWordTokenFilter to DictionaryCompoundWordTokenFilter\n\tAdded more text to the package description file (package.html)\n\tRemoved some code that was necessary because of LUCENE-1163 (in HyphenationCompoundWordTokenFilter and DictionaryCompoundWordTokenFilter\n)\n\n ",
            "author": "Thomas Peuss",
            "id": "comment-12593275"
        },
        {
            "date": "2008-04-30T14:26:05+0000",
            "content": "\n\tMinor bugfix in DictionaryCompoundWordFilter: it was not using the maxSubwordSize parameter\n\tMajor performance improvement for the DictionaryCompoundWordTokenFilter: we now convert all dictionary strings to lower case before adding them to the CharArraySet and set the ignoreCase parameter of CharArraySet to false. The filter makes a lower case copy of the token before it starts working on it. This avoids many toLowerCase() calls in CharArraySet.\n\tMinor performance improvement for the HyphenationCompoundWordTokenFilter: see above\n\n ",
            "author": "Thomas Peuss",
            "id": "comment-12593338"
        },
        {
            "date": "2008-05-07T09:48:16+0000",
            "content": "Is there any plan of integrating this patch in the official lucene libraries in the short term ?  ",
            "author": "Fran\u00e7ois Terrier",
            "id": "comment-12594838"
        },
        {
            "date": "2008-05-07T10:42:57+0000",
            "content": "Yes.\n\n\n\n--------------------------\nGrant Ingersoll\n\nLucene Helpful Hints:\nhttp://wiki.apache.org/lucene-java/BasicsOfPerformance\nhttp://wiki.apache.org/lucene-java/LuceneFAQ\n\n\n\n\n\n ",
            "author": "Grant Ingersoll",
            "id": "comment-12594850"
        },
        {
            "date": "2008-05-14T10:43:50+0000",
            "content": "I'm now getting:\n ..../lucene/java/lucene-clean/contrib/analyzers/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java:60: warning: unmappable character for encoding utf-8\n    [javac]         \"Aufgabe\", \"\u00dcberwachung\" };\n\n\nCan you convert the classes in question to UTF-8 for the source? ",
            "author": "Grant Ingersoll",
            "id": "comment-12596717"
        },
        {
            "date": "2008-05-16T11:32:39+0000",
            "content": "UTF-8 problem fixed... ",
            "author": "Thomas Peuss",
            "id": "comment-12597426"
        },
        {
            "date": "2008-05-16T12:28:41+0000",
            "content": "Committed revision 657027. ",
            "author": "Grant Ingersoll",
            "id": "comment-12597446"
        }
    ]
}