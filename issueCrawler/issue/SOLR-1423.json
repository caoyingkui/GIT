{
    "id": "SOLR-1423",
    "title": "Lucene 2.9 RC4 may need some changes in Solr Analyzers using CharStream & others",
    "details": {
        "affect_versions": "1.4",
        "status": "Closed",
        "fix_versions": [
            "1.4"
        ],
        "components": [
            "Schema and Analysis"
        ],
        "type": "Task",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Because of some backwards compatibility problems (LUCENE-1906) we changed the CharStream/CharFilter API a little bit. Tokenizer now only has a input field of type java.io.Reader (as before the CharStream code). To correct offsets, it is now needed to call the Tokenizer.correctOffset(int) method, which delegates to the CharStream (if input is subclass of CharStream), else returns an uncorrected offset. Normally it is enough to change all occurences of input.correctOffset() to this.correctOffset() in Tokenizers. It should also be checked, if custom Tokenizers in Solr do correct their offsets.",
    "attachments": {
        "SOLR-1423-with-empty-tokens.patch": "https://issues.apache.org/jira/secure/attachment/12419613/SOLR-1423-with-empty-tokens.patch",
        "SOLR-1423-FieldType.patch": "https://issues.apache.org/jira/secure/attachment/12419493/SOLR-1423-FieldType.patch",
        "SOLR-1423.patch": "https://issues.apache.org/jira/secure/attachment/12419445/SOLR-1423.patch",
        "SOLR-1423-fix-empty-tokens.patch": "https://issues.apache.org/jira/secure/attachment/12419620/SOLR-1423-fix-empty-tokens.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12754152",
            "date": "2009-09-11T14:33:46+0000",
            "content": "I'd like to check it before 1.4 release. I'll look into it once RC4 is checked in Solr. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12754810",
            "date": "2009-09-14T02:25:23+0000",
            "content": "I thought I call tokenizer.correctOffset() in newToken() method, but I couldn't because the method is protected. In this patch, I converted the anonymous Tokenizer class to PatternTokenizer, and PatternTokenizer has the following:\n\n\n+    public int correct( int currentOffset ){                                   \n+      return correctOffset( currentOffset );                                   \n+    }                                                                          \n\n "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12754836",
            "date": "2009-09-14T06:02:22+0000",
            "content": "I have seen this PatternTokenizer, too. The method is protected, as the corectOffset should only be called from inside Tokenizer (e.g. in incrementToken), never from the outside. Why does the PatternTokenizer does not have the methods newToken and so on in its own class (by the way: This should be updated to new TokenStream API)? "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12754844",
            "date": "2009-09-14T06:41:22+0000",
            "content": "I searched for setOffset() in Solr source code and found one additional occurence of it without offset correcting in FieldType.java. This patch fixes this.\n\nI will provide a completely streaming PatternTokenizer not using ArrayLists soon. It will move the while(matcher.find()) loop into the incrementToken() enumeration and will also use the new TokenStream API. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12754901",
            "date": "2009-09-14T09:53:25+0000",
            "content": "This is a complete more effective rewrite of the whole Tokenizer (I would like to put this into, Lucene contrib, too!) using the new TokenStream API.\n\nWhen going through the code, I realized the following: This Tokenizer can return empty tokens, it only filters enpty tokens in split() mode. Is this exspected? If empty tokens should be omitted, the if (matcher.find()) should be replaced by while (match.find()) with if match.length==0 continue; - The logic behind the strange omit empty token at the end  will get very simple after this change.\n\nThis patch removes the whole split()/group() methods from the factory as not needed anymore. If this is a backwards break, replace them by not used dummies (e.g. initialize a Tokenizer and return the token's TermText).\n\nIn my opinion, one should never index empty tokens...\n\nA second thing: Lucene has a new BaseTokenStreamTest class for checking tokens without Token instances (which would no loger work, when Lucene 3.0 switches to Attributes only). Maybe you should update these test and use assertAnalyzesTo from the new base class instead. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12755245",
            "date": "2009-09-14T23:32:53+0000",
            "content": "The patch that is Uwe's one with replacing split()/group() methods.\n\nWhy does the PatternTokenizer does not have the methods newToken and so on in its own class\nYeah, I'd realized it immediately after posting the patch, but I was going to be out.\n\nAnd thank you for adapting it for new TokenStream API.\n\nI searched for setOffset() in Solr source code and found one additional occurence of it without offset correcting in FieldType.java. This patch fixes this.\nGood catch, Uwe! I slipped over it.\n\nI think the empty tokens is a bug and should be omitted in this patch.\n\nA second thing: Lucene has a new BaseTokenStreamTest class for checking tokens without Token instances (which would no loger work, when Lucene 3.0 switches to Attributes only). Maybe you should update these test and use assertAnalyzesTo from the new base class instead.\nVery nice! Can you open a separate ticket? "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12755409",
            "date": "2009-09-15T07:57:23+0000",
            "content": "I think the empty tokens is a bug and should be omitted in this patch.\n\nThe Javadocs say, that it works like String.split() which return empty tokens, but strips empty tokens at the end of the string. This functionality is provided by Solr before and with this patch.\nThe code would get simplier, if the Tokenizer would generally strip empty tokens, but it is a backwards break. I would tend to just commit and then open another issue.\n\nVery nice! Can you open a separate ticket?\n\nWill open one about Lucene's BaseTokenStreamTestCase  "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12755410",
            "date": "2009-09-15T08:00:23+0000",
            "content": "I foget: I would deprecate the unneeded methods in the factory! "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12755425",
            "date": "2009-09-15T08:58:05+0000",
            "content": "Some refactoring (I moved the PatternTokenizer to its own class, like PatternReplaceFilter). This patch is functionally identical to current trunk, but more effective and uses new TokenStream API and implements end() (which sets the offset to the end of the string).\n\nI will soon post a patch, which removes empty tokens. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12755444",
            "date": "2009-09-15T10:21:54+0000",
            "content": "This is a patch that fixes the empty tokens:\nThis Tokenizer is not backwards compatible, as it only return non-zero length tokens. Maybe we should have a switch somewhere to change this behaviour. It is currently for discussion only. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12755503",
            "date": "2009-09-15T14:25:20+0000",
            "content": "This Tokenizer is not backwards compatible, as it only return non-zero length tokens. Maybe we should have a switch somewhere to change this behaviour. \n\nPassing through only non-zero length tokens was probably always the intent, and the old behavior is a bug and isn't useful, so I don't think  we need a switch. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12755564",
            "date": "2009-09-15T16:41:55+0000",
            "content": "Then you could use SOLR-1423-fix-empty-tokens.patch it should work. The comparison with String.split() in one of the tests was commented out, because it does not work with the tokenizer (as empty tokens are not returned).\n\nI only wanted to check, that the offsets are calculated correctly. The second tests does this, but I want to be sure, that they are correct for both group=-1 and group>=0. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12755577",
            "date": "2009-09-15T17:04:22+0000",
            "content": "Attached a new patch with the empty token fix.\n\nIt has an additional test for the offsets, if group!=-1. It also is more optimized, as it uses setTermBuffer( string, offset, len) to copy the chars into the termbuffer, which is faster than allocating a new string with substring(). "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12756923",
            "date": "2009-09-18T02:58:32+0000",
            "content": "The patch looks good! Will commit shortly. "
        },
        {
            "author": "Koji Sekiguchi",
            "id": "comment-12757028",
            "date": "2009-09-18T07:38:22+0000",
            "content": "Committed revision 816502. Thanks, Uwe! "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12775869",
            "date": "2009-11-10T15:52:14+0000",
            "content": "Bulk close for Solr 1.4 "
        }
    ]
}