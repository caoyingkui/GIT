{
    "id": "LUCENE-3220",
    "title": "Implement various ranking models as Similarities",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/query/scoring",
            "core/search"
        ],
        "type": "Sub-task",
        "fix_versions": [
            "flexscoring branch"
        ],
        "affect_versions": "flexscoring branch",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "With LUCENE-3174 done, we can finally work on implementing the standard ranking models. Currently DFR, BM25 and LM are on the menu.\n\nDone:\n\n\tEasyStats: contains all statistics that might be relevant for a ranking algorithm\n\tEasySimilarity: the ancestor of all the other similarities. Hides the DocScorers and as much implementation detail as possible\n\tBM25: the current \"mock\" implementation might be OK\n\tLM\n\tDFR\n\tThe so-called Information-Based Models",
    "attachments": {
        "LUCENE-3220.patch": "https://issues.apache.org/jira/secure/attachment/12483155/LUCENE-3220.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2011-06-20T13:42:01+0000",
            "content": "EasyStats object added. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13051974"
        },
        {
            "date": "2011-06-20T15:52:20+0000",
            "content": "a few comments (it generally looks close to me):\n\n\tmaybe we should use 'numberOfDocuments' instead of 'docNo' and same with 'numberOfFieldTokens'? this might make the naming more clear\n\ti'm worried about 'uniqueTermCount', do you know of which implementations require this? this number is not accurate if the index has more than one segment.\n\n ",
            "author": "Robert Muir",
            "id": "comment-13052019"
        },
        {
            "date": "2011-06-20T16:01:26+0000",
            "content": "\n\tI was wondering about that too \u2013 actually docNo is a mistake, it should have been noDocs or noOfDocs anyway, but I guess I'll just go with numberOfDocuments.\n\tI'll put a nocommit there for the time being, and if no sims use it, I'll just remove it from the Stats. Terrier has it, though, so I guess there should be at least one method that depends on it.\n\n ",
            "author": "David Mark Nemeskey",
            "id": "comment-13052025"
        },
        {
            "date": "2011-06-20T16:07:18+0000",
            "content": "I'll put a nocommit there for the time being, and if no sims use it, I'll just remove it from the Stats. Terrier has it, though, so I guess there should be at least one method that depends on it.\n\nI've never seen one that did... I don't imagine us ever implementing this efficiently given that we support incremental indexing. ",
            "author": "Robert Muir",
            "id": "comment-13052029"
        },
        {
            "date": "2011-06-20T16:12:22+0000",
            "content": "oh two more nitpicky comments: \n\n\tcan you update the patch to use two-spaces instead of tabs? if you use eclipse, you can download this and configure this as your default codestyle: http://people.apache.org/~rmuir/Eclipse-Lucene-Codestyle.xml\n\tcan you also remove the @author? For legal reasons (i think actually for your protection!) we omit these from new files.\n\tit might be a good idea to use the tag @lucene.experimental also for new classes: this is a template that 'ant-javadocs' replaces with \"WARNING: This API is experimental and might change in incompatible ways in the next release.\" to tell users that its very new and not to expect precise backwards compatibility.\n\n ",
            "author": "Robert Muir",
            "id": "comment-13052032"
        },
        {
            "date": "2011-06-20T16:29:06+0000",
            "content": "Oh, sorry, how lame of me  Actually I am working now on a different machine than the one I usually do, so that's why I made those mistakes. Anyhow, I have fixed them. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13052044"
        },
        {
            "date": "2011-06-20T16:37:41+0000",
            "content": "one last thing, can we do 'numberOfFieldTokens' instead of noFieldTokens? \n\nthen I think we can commit this as a step, should make things a lot easier for experimentation, if you are new to lucene it will make life much easier. ",
            "author": "Robert Muir",
            "id": "comment-13052052"
        },
        {
            "date": "2011-06-21T11:48:17+0000",
            "content": "Done. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13052496"
        },
        {
            "date": "2011-06-22T15:27:52+0000",
            "content": "EasySimilarity added. Lots of questions and nocommit in the code. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13053308"
        },
        {
            "date": "2011-06-22T16:16:37+0000",
            "content": "Just took a look, a few things that might help:\n\n\n\tyes the maxdoc does not reflect deletions, but neither does things like totalTermFreq or docFreq either... so its best to not worry about deletions in the scoring and to be consistent and use the stats (e.g. maxDoc, not numDocs) that do not take deletions into account.\n\n\n\n\n\tfor the computeStats(TermContext... termContexts) its wierd to sum the DF across the different terms in the case? But i don't honestly have any suggestions here... maybe in this case we should make a EasyPhraseStats that computes the EasyStats for each term, so its not hiding anything or limiting anyone? and you could then do an instanceof check and have a different method like scorePhrase() that it forwards to in case its an EasyPhraseStats? In general i'm not sure how other ranking systems tend to handle this case, the phrase estimation for IDF in lucene's formula is done by summing the IDFs\n\n ",
            "author": "Robert Muir",
            "id": "comment-13053329"
        },
        {
            "date": "2011-06-25T17:53:51+0000",
            "content": "Implementation of the DFR framework added. Lots of nocommits, though. I things to think about:\n\n\tlots of (float) conversions. Maybe the inner API (BasicModel, etc.) could use doubles? According to my experience, double is faster anyway, at least on 64bit architectures\n\tI am not overly happy about the naming scheme, i.e. BasicModelBE, etc. Maybe we should do it the same way as in Terrier, with a basicmodel package and class names like BE?\n\tA regular SimilarityProvider implementation won't play well with DFRSimilarity, in case the user wants to use several different setups. Actually, this is a problem for all similarities that have parameters (e.g. BM25 with b and k).\n\n\n\nAlso, I think we need that NormConverter we talked earlier on irc, so that the Similarities can run on any index. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13054938"
        },
        {
            "date": "2011-06-26T13:00:00+0000",
            "content": "Made the signature of EasySimilarity.score() a bit saner. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13055070"
        },
        {
            "date": "2011-06-27T08:09:09+0000",
            "content": "Explanation-handling added to EasySimilarity and DFRSimilarity. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13055385"
        },
        {
            "date": "2011-06-27T15:09:05+0000",
            "content": "Information-based model framework due to Clinchant and Gaussier added. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13055594"
        },
        {
            "date": "2011-07-04T15:58:10+0000",
            "content": "Fixed a few things in MockBM25Similarity. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13059492"
        },
        {
            "date": "2011-07-05T15:29:23+0000",
            "content": "\n\tlog2() moved from DFRSimilarity to EasySimilarity,\n\tchanged DFRSimilarity so that it constructor does not use reflection.\n\n ",
            "author": "David Mark Nemeskey",
            "id": "comment-13059953"
        },
        {
            "date": "2011-07-06T16:46:38+0000",
            "content": "Hi David: I had some ideas on stats to simplify some of these sims:\n\n\tI think we can use an easier way to compute average document length: sumTotalTermFreq() / maxDoc(). This way the average is 'exact' and not skewed by index-time-boosts, smallfloat quantization, or anything like that.\n\tTo support pivoted unique normalization like lnu.ltc, I think we can solve this in a similar way: add sumDocFreq(), which is just a single long, and divide this by maxDoc. this gives us avg # of unique terms. I think terrier might have a similar stat (#postings or #pointers or something)?\n\n\n\nso i think this could make for nice simplifications: especially for switching norms completely over to docvalues: we should be able to do #1 immediately right now, change the way we compute avgdoclen for e.g. BM25 and DFR.\n\nthen in a separate issue i could revert this norm summation stuff to make the docvalues integration simpler, and open a new issue for sumDocFreq() ",
            "author": "Robert Muir",
            "id": "comment-13060680"
        },
        {
            "date": "2011-07-10T14:43:57+0000",
            "content": "\n\tFixed #1\n\tAdded a totalBoost to EasySimilarity, and a getter method \u2013 noone uses it yet\n\tAdded basic implementations for the Jelinek-Mercer and the Dirichlet LM methods.\n\n\n\nAs for the last one: the implementation is very basic now, I want to factor a few things out (e.g. p(w|C) to LMStats, possibly in a pluggable way so ppl can implement it however they want). It also doesn't seem right to have the same LM method implemented twice (both as MockLMSimilarity and here), so I'll take a look to see if I can merge those two. Finally, I am wondering whether I should implement the absolute discounting method, which, according to the paper, seems inferior to the Jelinek-Mercer and Dirichlet methods. Right now I am more on the \"no\" side. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13062742"
        },
        {
            "date": "2011-07-13T14:37:28+0000",
            "content": "Added LMSimilarity so that the two LM methods have a common parent. It also defines the CollectionModel interface which computes p(w|C) in a pluggable way (and only once per query, though I am not sure this improves performance as I need a cast in score()). ",
            "author": "David Mark Nemeskey",
            "id": "comment-13064600"
        },
        {
            "date": "2011-07-14T09:35:37+0000",
            "content": "Explanation added to LM models; query boost added. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13065142"
        },
        {
            "date": "2011-07-14T18:47:59+0000",
            "content": "Made the score() and explain() methods in Similarity components final. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13065453"
        },
        {
            "date": "2011-07-15T12:51:02+0000",
            "content": "Hi David, this is looking really good! The patch is quite large so what i did was:\n\n\tre-sync flexscoring branch to trunk\n\tcommit your patch as is (i did a tiny tweak for LUCENE-3299)\n\n\n\nI saw a couple things we should address (full review will really mean i have to take quite a bit of time for each model!)\nBut we can take care of some of this easy stuff first!\n\n\n\tnumberOfFieldTokens seems to be the same as sumOfTotalTF? we should only have one name for this stat i think\n\ti like the idea of NoAfterAffect/NoNormalization in DFR, maybe we should make these ordinary classes, and in DFR we just don't allow null for any of the components? just thought it might look cleaner.\n\tsome of the files in .similarities need apache license header.\n\tbecause we dont need the norm for averaging, maybe we should use lucene's encoding? we can pre-build the decode table like TF-IDF similarity, except our decode table is basically 1 / decode(float)^2 to give us the quantized doc length. from a practical perspective, this would mean someone could use this stuff with existing lucene indexes (once they upgrade their segments to 4.0's format), and easily switch between things without reindexing.\n\n\n\nif you want, you can do these things on this issue or open separate issues, whichever is easiest. but i think looking at smaller patches at this point will make iteration easier! ",
            "author": "Robert Muir",
            "id": "comment-13065896"
        },
        {
            "date": "2011-07-20T11:48:31+0000",
            "content": "Fixed two of the issues you mentioned:\n\n\tApache license header added to all files in the similarities package;\n\tcleaned up the constructor of DFRSimilarity and added a few new ones.\n\n\n\nI have not yet moved the NoNormalization and NoAfterEffect classes to their own files, because I feel a bit uncomfortable about the naming, since it's different from that of the other classes, e.g. NormalizationH2 vs NoNormalization.  ",
            "author": "David Mark Nemeskey",
            "id": "comment-13068301"
        },
        {
            "date": "2011-07-20T11:55:42+0000",
            "content": "Thanks David: i committed this. ",
            "author": "Robert Muir",
            "id": "comment-13068303"
        },
        {
            "date": "2011-07-25T18:09:37+0000",
            "content": "I think I realized what I wanted with numberOfFieldTokens. I was afraid that sumTotalTermFreq is affected by norms / index time boost / etc, and I wanted to make numberOfFieldTokens to unaffected by those (I don't know now how); only I forgot to do so.\n\nBut if sumTotalTermFreq is really just the number of tokens in the field, I will delete one of them. Not sure which, because for me numberOfFieldTokens seems a more descriptive name than sumTotalTermFreq, but the latter is used everywhere in Lucene. May I ask your opinion on this question? ",
            "author": "David Mark Nemeskey",
            "id": "comment-13070654"
        },
        {
            "date": "2011-07-25T18:15:59+0000",
            "content": "\nNot sure which, because for me numberOfFieldTokens seems a more descriptive name than sumTotalTermFreq\n\nI think I agree with you: in the context of stats for scoring this might be the way to go, as its easier to understand. ",
            "author": "Robert Muir",
            "id": "comment-13070656"
        },
        {
            "date": "2011-08-02T08:17:37+0000",
            "content": "Added norm decoding table to EasySimilarity, and removed sumTotalFreq. Sorry I could only upload this patch now but I didn't have time to work on Lucene the last week.\n\nAs I see, all the problems you mentioned have been corrected, so maybe we can go on with the review? ",
            "author": "David Mark Nemeskey",
            "id": "comment-13076096"
        },
        {
            "date": "2011-08-02T12:42:18+0000",
            "content": "Hi David, i was thinking for the norm, we could store it like DefaultSimilarity. this would make it especially convenient, as you could easily use these similarities with the same exact index as one using Lucene's default scoring. Also I think (not sure!) by using 1/sqrt we will get better quantization from smallfloat?\n\n\n  public byte computeNorm(FieldInvertState state) {\n    final int numTerms;\n    if (discountOverlaps)\n      numTerms = state.getLength() - state.getNumOverlap();\n    else\n      numTerms = state.getLength();\n    return encodeNormValue(state.getBoost() * ((float) (1.0 / Math.sqrt(numTerms))));\n  }\n\n\n\nfor computations, you have to 'undo' the sqrt() to get the quantized length, but thats ok since its only done up-front a single time and tableized, so it won't slow anything down. ",
            "author": "Robert Muir",
            "id": "comment-13076171"
        },
        {
            "date": "2011-08-02T20:04:31+0000",
            "content": "EasySimilarity now computes norms in the same way as DefaultSimilarity.\n\nActually not exactly the same way, as I have not yet added the discountOverlaps property. I think it would be a good idea for EasySimilarity as well (it is for phrases, right), what do you reckon?\n\nI also wrote a quick test to see which norm (length directly or 1/sqrt) is closer to the original value and it seems that the direct one is usually much closer (RMSE is 0.09689688608375747 vs 0.23787634482532286). Of course, I know it is much more important that the new Similarities can use existing indices. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13078403"
        },
        {
            "date": "2011-08-04T20:03:50+0000",
            "content": "Deleted the accidentally forgotten abstract modifier from the Distribution classes. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13079555"
        },
        {
            "date": "2011-08-04T20:12:10+0000",
            "content": "Removed reflection from IBSimilarity. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13079563"
        },
        {
            "date": "2011-08-05T13:19:05+0000",
            "content": "Thanks, I committed your latest patch, some ideas just perusing:\n\n\twe can move the calculations currently in decodeNormValue into the static table, this way we aren't doing these per-document multiplications and divisions... so decodeNormValue just returns the document length.\n\tshould easysim change its score method from score(Stats stats, float freq, byte norm) to score(Stats stats, float freq, int documentLength) ? then it could encapsulate this encoding/decoding.\n\tI think we should try to factor in the index-time boost in computeNorm here if we can, e.g. just divide the document length by it? So documents with a higher index-time boost have a shorter length.\n\n ",
            "author": "Robert Muir",
            "id": "comment-13079961"
        },
        {
            "date": "2011-08-06T11:14:54+0000",
            "content": "Done. Actually, I wanted to implement the norm table in the way you said, but somehow forgot about it.\n\nTwo questions remain on my side:\n\n\tthe one about discountOverlaps (see above)\n\twhat kind of index-time boosts do people usually use? Too big a boost might cause problems if we just divide the length with it. Maybe we should take the logarithm or sth like that?\n\n ",
            "author": "David Mark Nemeskey",
            "id": "comment-13080398"
        },
        {
            "date": "2011-08-06T11:30:15+0000",
            "content": "Added a short explanation on the parameter for the Jelinek-Mercer method. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13080399"
        },
        {
            "date": "2011-08-08T20:16:11+0000",
            "content": "Added discountOverlaps to EasySimilarity. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13081163"
        },
        {
            "date": "2011-08-09T20:23:31+0000",
            "content": "Got rid of all but one nocommits. ",
            "author": "David Mark Nemeskey",
            "id": "comment-13081890"
        },
        {
            "date": "2011-08-09T20:29:44+0000",
            "content": "Thanks David: I committed this. ",
            "author": "Robert Muir",
            "id": "comment-13081895"
        },
        {
            "date": "2011-08-12T13:10:02+0000",
            "content": "Robert: Since we use LUCENE-3357 for testing & bug fixing, I propose we close this issue. If we decide to implement other methods as well, we can do it under a new issue. Or do you have something else in mind (such as to rename EasySimilarity to SimilarityBase)? ",
            "author": "David Mark Nemeskey",
            "id": "comment-13084098"
        },
        {
            "date": "2011-08-12T13:22:13+0000",
            "content": "+1, I do think we should consider naming and stuff (I sorta like SimilarityBase but we can discuss it)... but we should just open separate issues for that after we have worked out all the technical details first, its easy to refactor naming.\n\nAnd we also want to at the same time move it into src/java, we can open a separate issue for all of this \"integrate new similarities\" or something. Let's close this one! ",
            "author": "Robert Muir",
            "id": "comment-13084106"
        },
        {
            "date": "2011-08-12T13:22:33+0000",
            "content": "Thanks David! Awesome work  ",
            "author": "Robert Muir",
            "id": "comment-13084107"
        },
        {
            "date": "2012-02-17T15:29:15+0000",
            "content": "Hi All,\n\npardon my ignorance, I'm new to this. What I need is the BM25 to implement in my current project (bachelor thesis), I'm using Lucene 3.0.2. \nCan you instruct me what do I need to do, so that I can add the bm25 to my project? Do I get a jar? or do I need to compile everything on my own?\nfurthermore, do I need to re-index sources in order to have BM25 working?\n\nbest,\n\n fiska ",
            "author": "Fis Ka",
            "id": "comment-13210328"
        }
    ]
}