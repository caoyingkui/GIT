{
    "id": "LUCENE-6983",
    "title": "IndexWriter.updateDocument on FSDirectory does not work Ver.5.4.0",
    "details": {
        "resolution": "Invalid",
        "affect_versions": "5.4",
        "components": [
            "core/index"
        ],
        "labels": "",
        "fix_versions": [],
        "priority": "Major",
        "status": "Resolved",
        "type": "Bug"
    },
    "description": "I try to create a simple index and then update documents (with IndexWriter.updateDocuments). When I use RAMDirectory it works fine (IndexWriter.numDocs returns correct value, IndexSearcher.search returns correct hits). However, when I use FSDirectory it creates duplicate documents, and IndexSearcher returns multiple documents with same key field values. Here is the code excerpt:\n//This code is in a thread and it updates documents with regular intervals, say 1 min.\nAnalyzer analyzer = new StandardAnalyzer();\ntry \n{\nPath path = Paths.get(indexDirectory);\nDirectory directory = FSDirectory.open(path);\nIndexWriterConfig config = new IndexWriterConfig(analyzer);\nIndexWriter iwriter = new IndexWriter(directory, config);\nSystem.out.println(\"Document count=\" + iwriter.numDocs());\nfor (Company newCompany : newCompanies)\n{ Document doc = new Document(); doc.add(new Field(\"entityid\", \"Company\" + newCompany.getId().toString(), TextField.TYPE_STORED)); doc.add(new Field(\"companyname\", newCompany.getName(), TextField.TYPE_NOT_STORED)); Term term = new Term(\"entityid\", \"Company\" + newCompany.getId().toString()); iwriter.updateDocument(term, doc);\t}\niwriter.flush();\niwriter.commit();\nSystem.out.println(\"Document count2=\" + iwriter.numDocs());\niwriter.close();\n} catch (IOException e1)\n{ e1.printStackTrace(); }",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15108195",
            "author": "Dawid Weiss",
            "date": "2016-01-20T08:09:30+0000",
            "content": "Updating a document means deleting the old one and inserting a new one. Documents are marked in the index until the next merge \u2013 they will be returned in numDocs.\n\nEverything is fine, in other words. "
        },
        {
            "id": "comment-15108199",
            "author": "uygar yuzsuren",
            "date": "2016-01-20T08:15:52+0000",
            "content": "Ok,\n\nThen how am I going to ensure that the search of the web client will not see duplicate entries?\n\nThanks a lot. "
        },
        {
            "id": "comment-15108200",
            "author": "Dawid Weiss",
            "date": "2016-01-20T08:18:37+0000",
            "content": "Please see more documentation. IndexSearcher does it for you, it collects only non-deleted documents. You can also do it yourself if you implement a custom collector, but I'd use the simpler methods that collect top-n matches for you. "
        },
        {
            "id": "comment-15108215",
            "author": "uygar yuzsuren",
            "date": "2016-01-20T08:37:08+0000",
            "content": "Ok, I will look more into the documentation.\n\nOn the other hand, I already made the test with a simple IndexSearcher code taken from the documentation and it returned duplicates. I think this should be one of the most basic functionalities of Lucene, which shouldn't require an additional configuration option, that is why I didn't go deeper into the docs.\n\npublic static void main(String[] args) {\n\t\ttry {\n\t\t\tAnalyzer analyzer = new StandardAnalyzer();\n\t\t\tPath path = Paths.get(\"C:\\\\LUCENEINDEX\");\n\t\t\tDirectory directory = FSDirectory.open(path);\n\t\t\t// Now search the index:\n\t\t\tDirectoryReader ireader = DirectoryReader.open(directory);\n\t\t\tIndexSearcher isearcher = new IndexSearcher(ireader);\n\t\t\t// Parse a simple query that searches for \"text\":\n\t\t\tQueryParser parser = new QueryParser(\"webaddress\", analyzer);\n\t\t\tQuery query = parser.parse(\"lucene\");\n\t\t\tScoreDoc[] hits = isearcher.search(query, 100).scoreDocs;\n\t\t\t// Iterate through the results:\n\t\t\tfor (int i = 0; i < hits.length; i++) \n{\n\t\t\t\tDocument hitDoc = isearcher.doc(hits[i].doc);\n\t\t\t\tSystem.out.println(hitDoc.get(\"entityid\") + \"/\" + hits[i].score);\n\t\t\t}\n\t\t\tireader.close();\n\t\t\tdirectory.close();\n\t\t} catch (IOException | ParseException e) \n{\n\t\t\t// TODO Auto-generated catch block\n\t\t\te.printStackTrace();\n\t\t}\n\t} "
        },
        {
            "id": "comment-15108622",
            "author": "uygar yuzsuren",
            "date": "2016-01-20T14:30:30+0000",
            "content": "I found a weird workaround (if it is a workaround at all, it is the expected behaviour actually).\n\nI changed deleteDocument(Term) to deleteDocument(Query) and now it works fine. Now I am wondering what is the functional difference between deleteDocument(Term) and deleteDocument(Query). There is not any query argument for updateDocument(), therefore I had to call delete and add consecutively.\n\nI doubt if there is a bug but these are unexpected behaviours for an intuitive developer though.   \n\nQueryParser parser = new QueryParser(\"entityid\", analyzer);\n\t\t\t\ttry \n{\n\t\t\t\t\tQuery query = parser.parse(\"Company_\" + newCompany.getId().toString());\n\t\t\t\t\tiwriter.deleteDocuments(query);\n\t\t\t\t\tiwriter.addDocument(doc);\n\t\t\t\t}\n catch (ParseException e) \n{\n\t\t\t\t\te.printStackTrace();\n\t\t\t\t}\n "
        }
    ]
}