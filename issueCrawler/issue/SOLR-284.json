{
    "id": "SOLR-284",
    "title": "Parsing Rich Document Types",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "1.4"
        ],
        "components": [
            "update"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "I have developed a RichDocumentRequestHandler based on the CSVRequestHandler that supports streaming a PDF, Word, Powerpoint, Excel, or PDF document into Solr.\n\n\nThere is a wiki page with information here: http://wiki.apache.org/solr/UpdateRichDocuments",
    "attachments": {
        "un-hardcode-id.diff": "https://issues.apache.org/jira/secure/attachment/12389188/un-hardcode-id.diff",
        "solr-word.pdf": "https://issues.apache.org/jira/secure/attachment/12394495/solr-word.pdf",
        "source.zip": "https://issues.apache.org/jira/secure/attachment/12365110/source.zip",
        "libs.zip": "https://issues.apache.org/jira/secure/attachment/12360970/libs.zip",
        "test.zip": "https://issues.apache.org/jira/secure/attachment/12365113/test.zip",
        "schema_update.patch": "https://issues.apache.org/jira/secure/attachment/12419229/schema_update.patch",
        "rich.patch": "https://issues.apache.org/jira/secure/attachment/12365107/rich.patch",
        "test-files.zip": "https://issues.apache.org/jira/secure/attachment/12360969/test-files.zip",
        "SOLR-284-no-key-gen.patch": "https://issues.apache.org/jira/secure/attachment/12399726/SOLR-284-no-key-gen.patch",
        "SOLR-284.patch": "https://issues.apache.org/jira/secure/attachment/12393987/SOLR-284.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Eric Pugh",
            "id": "comment-12509667",
            "date": "2007-07-02T21:01:14+0000",
            "content": "Patch file for adding new handler and test cases. "
        },
        {
            "author": "Eric Pugh",
            "id": "comment-12509673",
            "date": "2007-07-02T21:11:10+0000",
            "content": "test files to go in test/test-files for unit testing. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12509676",
            "date": "2007-07-02T21:13:12+0000",
            "content": "I haven't run this patch, but have a few questions...\n\nWhat is the general approach to extract a lucene document (list of fields) from a PDF? Word? Powerpoint?\n\nIs this just access to a few common fields like author, keywords, text, etc?  Is this something that realistically would need to be custom for each case?  \n\nPerhaps it makes sense to add a contrib section for this sort of stuff.  It seems weird to add 10 library dependencies to the core distribution.  How does nutch handle this?\n "
        },
        {
            "author": "Eric Pugh",
            "id": "comment-12509679",
            "date": "2007-07-02T21:15:41+0000",
            "content": "new jars to go in trunk/lib for pdf and office parsing... "
        },
        {
            "author": "Eric Pugh",
            "id": "comment-12509683",
            "date": "2007-07-02T21:29:32+0000",
            "content": "So, I was not attempting to \"boil the ocean\" and provide the ultimate solution.  Our need was just to take all the raw text and index it in a field, and pass in a bunch of other data fields to be indexed.  \n\nWe are parsing a large number of unstructured documents, that may or may not have common fields populated, but fortunately we don't really need them.  Our users aren't searching by author, but by content.  \n\nI think there are only 5 additional libraries, and one (poi-scratchpad) may be able to be removed...\n\nYonik also mentioned using Tika, as a framework for creating a common interface to these types of rich documents, but Tika is still in incubation and has no code in it!\n\nI originally had separate handlers for each data type, and that was really icky, so I condensed it into the RichDocumentRequestHandler.  I could also merge in the CSVRequestHandler into it as well, by just taking out the logic for parsing CSV and putting it into a CSVParser.  However, the CSVRequestHandler has very complex and rich semantics that these unstructured documents don't really need.\n "
        },
        {
            "author": "Eric Pugh",
            "id": "comment-12509905",
            "date": "2007-07-03T15:18:36+0000",
            "content": "Updated patch file, properly handling missing stream.types, and cleaning up error messages a bit. "
        },
        {
            "author": "Eric Pugh",
            "id": "comment-12512497",
            "date": "2007-07-13T14:29:24+0000",
            "content": "Updated to SVN revision 555996 "
        },
        {
            "author": "Eric Pugh",
            "id": "comment-12524843",
            "date": "2007-09-04T19:48:03+0000",
            "content": "Update patches for revision 572774 "
        },
        {
            "author": "Eric Pugh",
            "id": "comment-12524851",
            "date": "2007-09-04T20:10:48+0000",
            "content": "Java Source code for RichDocumentRequestHandler and friends. "
        },
        {
            "author": "Eric Pugh",
            "id": "comment-12524854",
            "date": "2007-09-04T20:13:55+0000",
            "content": "add the test code for richdocumenthandler. "
        },
        {
            "author": "Eric Pugh",
            "id": "comment-12524856",
            "date": "2007-09-04T20:14:33+0000",
            "content": "test code, this time with granted license! "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12525750",
            "date": "2007-09-07T15:41:37+0000",
            "content": "In regards to Tika not having any code, you may also find http://aperture.sourceforge.net does many of the same things for handling different file formats, etc. "
        },
        {
            "author": "Juri Kuehn",
            "id": "comment-12541535",
            "date": "2007-11-10T11:22:37+0000",
            "content": "Hi Eric, thank you for this handler, works like a charm!\nI need to use non-numeric ids which are fine with solr but are rejected by RichDocumentRequestHandler. I'm not familiar with the solr-code, i patched RichDocumentRequestHandler.java to not to convert id to int, which didn't cause trouble so far:\n\nRichDocumentRequestHandler.java.patch\nIndex: RichDocumentRequestHandler.java\n===================================================================\n--- RichDocumentRequestHandler.java\t(revision 0)\n+++ RichDocumentRequestHandler.java\t(working copy)\n@@ -133,7 +133,7 @@\n \tString streamFieldname;\n \tString[] fieldnames;\n \tSchemaField[] fields;\n-\tint id;\n+\tString id;\n \t  \n \tfinal AddUpdateCommand templateAdd;\n \n@@ -153,7 +153,7 @@\n \t    String fn = params.get(FIELDNAMES);\n \t    fieldnames = fn != null ? commaSplit.split(fn,-1) : null;\n \t    \n-\t    id = params.getInt(ID);\n+\t    id = params.get(ID);\n \n \t\ttemplateAdd = new AddUpdateCommand();\n \t\ttemplateAdd.allowDups = false;\n@@ -202,7 +202,7 @@\n \t * @param desc\n \t *            TODO\n \t */\n-\tvoid doAdd(int id, String text, DocumentBuilder builder, AddUpdateCommand template)\n+\tvoid doAdd(String id, String text, DocumentBuilder builder, AddUpdateCommand template)\n \tthrows IOException {\n \n \t  // first, create the lucene document\n@@ -225,7 +225,7 @@\n \t  handler.addDoc(template);\n \t}\n \n-\tvoid addDoc(int id, String text) throws IOException {\n+\tvoid addDoc(String id, String text) throws IOException {\n \t\ttemplateAdd.indexedId = null;\n \t\tdoAdd(id, text, builder, templateAdd);\n \t}\n\n\n\nTests were ok, maybe you can apply it to your sources.\n\nBest regards,\nJuri "
        },
        {
            "author": "Eric Pugh",
            "id": "comment-12541879",
            "date": "2007-11-12T18:23:04+0000",
            "content": "Juri,\n\nThanks for the vote on the issue!  The next time I update this patch to work with the latest code, I'll apply your change.  Since this is still a pending patch, I am not actively maintaining it.  Thanks for voting for this patch, there is only one other patch with more votes, hopefully it will be added soon.  I'd love to hear what the use case you have for this patch is.\n\n\nhttps://issues.apache.org/jira/browse/SOLR?report=com.atlassian.jira.plugin.system.project:popularissues-panel\n\nEric "
        },
        {
            "author": "Jonathan Hipkiss",
            "id": "comment-12551506",
            "date": "2007-12-13T13:50:29+0000",
            "content": "This is crucial functionaility if Solr is to be accepted as a solution in any organisation.  A search engine that can't parse Microsoft or other closed formats is useless to most organisations.\nThis is a MUST! "
        },
        {
            "author": "Pompo Stenberg",
            "id": "comment-12569275",
            "date": "2008-02-15T14:43:05+0000",
            "content": "I wrote a simple patch for RichDocumentUpdateHandler to accept multivalued fields. Just POST the same field name multiple times, e.g. category=TVs&category=Radios\n\nRichDocumentRequestHandler.java.patch\nIndex: RichDocumentRequestHandler.java\n===================================================================\n--- RichDocumentRequestHandler.java\t(revision 0)\n+++ RichDocumentRequestHandler.java\t(working copy)\n@@ -211,7 +211,10 @@\n \t  for (int i =0; i < fields.length;i++){\n \t    String fieldName = fields[i].getName();\n    \n-  \t    builder.addField(fieldName,params.get(fieldName),1.0f);\n+           String[] values = params.getParams(fieldName);\n+           for(String value : values) {\n+             \t    builder.addField(fieldName,value,1.0f);\n+           }\n \t      \n \t  }\n\n\n\nSeems to work for me.\n\nBest Regards,\nPompo "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12582062",
            "date": "2008-03-25T21:32:09+0000",
            "content": "I'm thinking it would be handy if RichDocumentRequestHandler could support indexing text and HTML files, in addition to the fancier formats (pdf, doc, etc.). That way I could use RichDocumentRequestHandler for all my indexing needs (except commits and optimizes), rather than use it for for some doc types but still have to use XmlUpdateRequestHandler for text and HTML docs. Would anyone else find this useful?\n\nI skimmed the source, and adding support for text files looks trivial. (It's just a pass-through.) And if you had this, then I guess you'd have at least one version of HTML support for free; in particular, you could upload your HTML file to RichDocumentRequestHandler, telling the handler that the document is in plain text format, and then strip off the HTML tags later by using the HTMLStripStandardTokenizer in your schema.xml.\n\nAlternatively, RichDocumentRequestHandler could provide its own explicit HTML to text conversion. There would probably be some advantages to this, but I'm not sure exactly what they would be. One, I guess, would be that you could use tokenizers that didn't make use of HTMLStripReader. "
        },
        {
            "author": "Eric Pugh",
            "id": "comment-12583231",
            "date": "2008-03-28T23:13:31+0000",
            "content": "Chris,  I like what you are thinking...  Really this is sort of becoming the AllDocumentsUnderTheSunRequestHandler, but what that highlights is that the current solution really doesn't do what we need, which is making it dirt simple to add new handlers...   \n\nWhile there are some efforts under way to do that, to provide the \"uber\" solution, I think adding another hack/method to RichDocumentRequestHandler is cool with me.  Since it's just a patch file, feel free to take it, munge it, and post it back as the \"current\" patch.  If you do, make sure to add to the docs on the wiki at http://wiki.apache.org/solr/UpdateRichDocuments.\n\nHeck, you may want to rip in Pompo's fix as well! "
        },
        {
            "author": "Eric Pugh",
            "id": "comment-12583233",
            "date": "2008-03-28T23:14:59+0000",
            "content": "Oh, and don't forget to vote for it as well:\n\nhttps://issues.apache.org/jira/browse/SOLR?report=com.atlassian.jira.plugin.system.project:popularissues-panel\n\nIt's the current leading vote getter! "
        },
        {
            "author": "Kristoffer Dyrkorn",
            "id": "comment-12586735",
            "date": "2008-04-08T10:05:08+0000",
            "content": "Very handy!\n\nIt could be beneficial to have an option to save the extracted text as xml (so it can be stored) just before adding it to the Solr index. Thus, if the Solr schema needs to be changed (in a way that triggers a full reindex) the content can then be quickly re-fed from a \"near source\". "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12587352",
            "date": "2008-04-09T21:31:39+0000",
            "content": "Replacing rich.patch. The new one:\n\n1) Rolls together into one handy package all of these:\n\n\n\tthe old rich.patch\n\tthe contents of source.zip and test.zip\n\tPompo's multivalued fields patch.\n\n\n\nNote: It does not include the contents of libs.zip or test-files.zip. I'm not sure what the protocol is around those larger files.\n\nNote: The old rich.patch included a change to Config.java that  searched for an alternative config file in \"src/test/test-files/solr/conf/\". I've removed that change because I think it's debugging code that we don't want in an official patch. Let me know if I'm wrong, though.\n\n2) Makes things work against the latest revision in trunk, r646483. (It had stopped working with the latest version.)\n\nI haven't added any new test cases, but the old ones all pass.\n\nI grant my modifications to ASF according to the Apache License. Someone might want to check that the underlying contributions have been appropriately licensed as well. "
        },
        {
            "author": "Michel Benevento",
            "id": "comment-12594730",
            "date": "2008-05-06T22:59:53+0000",
            "content": "Hi, just new here, I am working on Rich Document support for solr-ruby and acts_as_solr. If you are interested, see prelim results at: http://wiki.apache.org/solr/solr-ruby/BrainStorming\n\nFor acts_as_solr I need the ID field to be a String, same as Juri Kuehn above who supplied the fix for this.\n\nIs there a specific reason it was not added to the latest rich.patch? I would appreciate it.\n\nThanks,\nMIchel "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12594987",
            "date": "2008-05-07T18:17:13+0000",
            "content": "Here's a new version of rich.patch. My previous attempt didn't actually include all the necessary files! (Curses upon you, TortoiseSVN.) This one also includes preliminary support for plaintext and HTML files. (HTML support is done by running the input through the HTMLStripReader.) "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12594989",
            "date": "2008-05-07T18:19:05+0000",
            "content": "New version of test-files.zip. Contains new file, simple.txt, that is used by a new unit test for plaintext files. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12594992",
            "date": "2008-05-07T18:31:23+0000",
            "content": "Why not just use Tika (or Aperture, but it's license isn't as friendly)?  Doesn't make sense to reinvent the wheel here. "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12595007",
            "date": "2008-05-07T19:15:00+0000",
            "content": "I'm not sure this patch entirely reinvents the wheel, as it does most of the heavy lifting with preexisting components, namely PDFBox, POI, and Solr's own HTMLStripReader. It also has the advantage of already existing, whereas tying Solr to Tika or Aperture would take additional effort.\n\nTika or Aperture do look really nice, though. The most obvious advantage these projects have over this patch is that they can already extract text from more file formats than this patch, and that the developers will probably continue to add more file formats over time. Are you thinking of additional advantages on top of this, Grant? Do you have any cool ideas about how Tika/Aperture's metadata extraction facilities might be integrated into Solr? Is there a potentially interesting interface between Aperture's crawling facilities and Solr? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12595015",
            "date": "2008-05-07T19:46:56+0000",
            "content": "\n\n\nI think Tika will actually take less effort, as you only need one  \ninterface, as I understand it.  You don't need separate handlers for  \neach type, we just need to write the interface between Solr and Tika.\n\nNutch is already using Tika.\n\n\n+1\n\n\nYes, someone else maintains the code.  We just maintain the interface  \nand upgrade when appropriate.\n\n\nwell, metadata makes for nice fields to sort, filter and facet on,  \nright?\n\n\nI think it is more likely that you will see Nutch integration w/ Solr  \n(in fact, there is already a patch for it), but yeah, I think it makes  \nsense to consider Solr as a sink for any crawler.\n\nSome of this also overlaps w/ the Data Import Request Handler on  \nSOLR-469.   I don't think we want to get Solr into the crawling game,  \nbut we also shouldn't prevent it from playing nicely with crawlers  \n(not saying it doesn't already)\n\n\n--------------------------\nGrant Ingersoll\n\nLucene Helpful Hints:\nhttp://wiki.apache.org/lucene-java/BasicsOfPerformance\nhttp://wiki.apache.org/lucene-java/LuceneFAQ\n\n\n\n\n\n "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12595033",
            "date": "2008-05-07T20:37:55+0000",
            "content": "Attaching another patch revision. I've been totally asleep at the wheel today, and my previous one contained not only the feature described in this JIRA issue but also the Data Import RequestHandler patch (SOLR-469). Hopefully I've finally made a patch that's actually correct. I can at least promise that the unit tests pass when applied to r654253. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12595360",
            "date": "2008-05-08T18:39:33+0000",
            "content": "+1 for Tika\nBut also +1 for committing this in the mean time \u2013 wow, lots of watchers and voters! "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12595365",
            "date": "2008-05-08T18:59:10+0000",
            "content": "I don't agree on committing it.  If Tika is the right solution, then  \nwe should work towards Tika.  Not saying this isn't good, just saying  \nit's going to create more maintenance than we want and then we just  \nend up deprecating it in the near future.\n\n\n\n "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12595372",
            "date": "2008-05-08T19:44:11+0000",
            "content": "I'm on the fence about whether this patch makes sense to include in Solr right now. One thing I'm wondering, though: Can we assess the odds at this point whether it could make sense for a Tika-based handler to offer the same public interface that the handler in this patch presents? That is, even if the underlying implementation were switched to Tika at some point, could we avoid changing the URL schema and such that Solr clients would use to interact with it?\n\nIf it's likely that the public interface could indeed remain the same for the first Tika-based handler release (or at least more or less the same), would this alleviate any of Grant's concerns?\n\nAlso, would putting this handler into a contrib directory rather than in the main code base, as has been mentioned on the mailing list, make committing it any less problematic? "
        },
        {
            "author": "Mike Klaas",
            "id": "comment-12602831",
            "date": "2008-06-05T22:57:26+0000",
            "content": "Removing from 1.3.  No committer has taken ownership.\n\n(It might make sense as a contrib, but I can see the argument for not duplicating tika) "
        },
        {
            "author": "Rog\u00e9rio Pereira Ara\u00fajo",
            "id": "comment-12614992",
            "date": "2008-07-19T13:50:14+0000",
            "content": "Who is working on tika based handler? The work on tika based handler can be started or it isn't mature enougth? "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12615057",
            "date": "2008-07-19T22:52:36+0000",
            "content": "I don't think anyone is working on it (publicly), so you are welcome to contribute it. "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12621989",
            "date": "2008-08-12T21:27:52+0000",
            "content": "Trivial update to merge cleanly against r685275. "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12627082",
            "date": "2008-08-29T19:34:41+0000",
            "content": "The patch, as currently stands, treats a field called \"id\" as a special case. First, it is a required field. Second, unlike any other field, you don't need to declare it in the fieldnames parameter. Finally, since the fieldSolrParams.getInt(), that field is required to be an int.\n\nThis special-case treatment seems a little too particular to me; not everyone wants to have a field called \"id\", and not everyone who does wants that field to be an int. So what I propose is to eliminate the special treatment of \"id\". See un-hardcode-id.diff for what this might mean in particular. (That file is not complete; to correctly make this change, I'd have to update the test cases.)\n\nThis is a breaking change, because if you are using an id field, you'll now have to specifically indicate that fact in the fieldnames parameter. Thus, instead of\n\nhttp://localhost:8983/solr/update/rich?stream.file=myfile.doc&stream.type=doc&id=100&stream.fieldname=text&fieldnames=subject,author&subject=mysubject&author=eric\n\nyou'll have to put\n\nhttp://localhost:8983/solr/update/rich?stream.file=myfile.doc&stream.type=doc&id=100&stream.fieldname=text&fieldnames=id,subject,author&subject=mysubject&author=eric\n\nI think asking users of this patch to make this slight change in their client code is not an unreasonable burden, but I'm curious what Eric and others have to say. "
        },
        {
            "author": "Eric Pugh",
            "id": "comment-12627309",
            "date": "2008-08-31T13:22:40+0000",
            "content": "So, in typical open source fashion, I wrote the original patch to scratch my own itch.  Which meant that it was okay to make id be hardcoded.  However, even when I first posted the patch to this JIRA issue, I felt a little \"icky\" about the id field.  It seemed like a code smell to have this magic id!   So, from that standpoint, I think the changes that Chris has posted look great.  \n\nI think it's a good example of a patch getting better and better everytime someone else uses it!\n\nNow, if only this almost 14 month old patch could be applied!  With 28 votes, and 16 active watches, clearly somebody out there finds this useful!   \n\nAnd at this point it is miles better then what I first posted!  Keep up the great work and great contributions back! "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12627333",
            "date": "2008-08-31T19:04:05+0000",
            "content": "While we're on the subject of breaking changes, I'm now seeing some merit in replacing the fieldnames parameter with a field-specifying prefix.\n\nCurrently when you want to set a non-body field, you introduce the field name in the fieldnames parameter and then specify its value in another parameter, like so:\n\n   /update/rich/...fieldnames=f1,f2,f3&f1=val1&f2=val2&f3=val3\n\nThe alternative would be to to signal the fields f1, f2, and f3 by a field prefix, like so:\n\n  /update/rich/...f.f1=val1&f.f2=val2&f.f3=val3\n\nBecause the f prefix says \"this is a field\", there's no need for the fieldnames parameter.\n\nThis isn't an Earth-shattering improvement, but there are three things I like about it:\n\n1. The URLs are shorter\n\n2. If you rename a field (e.g. rename f3 to g3), you can't accidentally half-update the URL in the client code, like this:\n\n  /update/rich/...fieldnames=f1,f2,g3&f1=val1&f2=val2&f3=val3\n\n3. Currently there are certain reserved words (e.g. \"fieldnames\", \"commit\") that you can't use, because they have special meaning to the handler. But with this change they become legitimate field names. For example, maybe I want each of my documents to have a \"commit\" field that describes who made the most recent relevant commit in a version control system.\n\n  /update/rich/...commit=true&f.commit=chris\n\nI can't think of any downsides right now, other than breaking people's code. (I do admit that is a downside.)\n\nAny comments? "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12627882",
            "date": "2008-09-03T01:07:37+0000",
            "content": "A couple of Tika things:\n\nI glanced at Tika yesterday, and it looks like switching this patch over to it wouldn't be too hard. (The only thing half-worthy of note is that org.apache.tika.parser.Parser.parse outputs XHTML [via a SAX interface], which we would probably then need to turn into plaintext.) I haven't yet looked into Eric's code to see if it does anything special that Tika doesn't do.\n\nI also noticed something else, though. Earlier comments say that Nutch uses Tika, but when I looked through Nutch trunk this seemed to only sort of be the case. In particular, Nutch definitely uses the stuff in the org.apache.tika.mime namepsace, to do things like auto-detect content types, but it doesn't seem to use the stuff in org.apache.tika.parser to do the actual document parsing; instead, it uses its own separate org.apache.nutch.parse.Parser class (and subclasses thereof). For example, org.apache.nutch.parse.html.HtmlParser does not delegate to org.apache.tika.parser.html.HtmlParser but rather does its own direct manipulation of the tagsoup and/or nekohtml libraries. (Things are similar with the Nutch PDF parser.) Nor does there seem to be an alternative class along the lines of org.apache.nutch.parse.TikaBasedParserThatCanParseLotsOfDifferentContentTypesIncludingHtml. And the string \"org.apache.tika.parser\" doesn't seem to occur in the Nutch source.\n\nI'm wondering if anyone knows why Nutch does not seem to make use of all of Tika's functionality. Are they planning to switch everything over to Tika eventually? "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12628214",
            "date": "2008-09-04T00:33:11+0000",
            "content": "This update is just to make a tiny refactoring, bringing all the handler's parsing classes under \n\nsrc\\java\\org\\apache\\solr\\handler\\rich\n\nand all the testing classes under \n\nsrc\\test\\org\\apache\\solr\\handler\\rich\n\nAll tests pass. "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12628706",
            "date": "2008-09-05T19:22:11+0000",
            "content": "THIS IS A BREAKING CHANGE TO RICH.PATCH! CLIENT URLs NEED TO BE UPDATED!\n\nAll unit tests pass.\n\nChanges:\n\n\n\tAs suggested earlier, the \"id\" parameter is no longer treated as a special case; it is not required, and it does not need to be an int. If you do use a field called \"id\", you must now declare it in the fieldnames parameter, as you would any other field\n\n\n\n\n\tDo updates with with UpdateRequestProcessor and SolrInputDocument, rather than UpdateHandler and DocumentBuilder. (The latter pair appear to be obsolete.)\n\n\n\n\n\tPreviously if you declared a field in the fieldnames parameter but did not then did not specify a value for that field, you would get a NullPointerException. Now you can specify any nonnegative number of values for a declared field, including zero. (I've added a unit test for this.)\n\n\n\n\n\tIn SolrPDFParser, properly close PDDocument when PDF parsing throws an exception\n\n\n\n\n\tLog the stream type in the solr log, rather than on the console\n\n\n\n\n\tSome not-very-thorough conversion of tabs to spaces\n\n\n\nAs an aside, I've noticed that I failed in my earlier efforts to incorporate Juri Kuehn's change to allow the id field to be non-integer. Sorry about that, Juri; that was not at all intentional. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12646347",
            "date": "2008-11-10T20:13:15+0000",
            "content": "FYI, I intend to integrate Tika now that it has graduated from incubation and is a full-fledged Lucene sub-project.  I will do my best to be back-compatible with this patch, but make no guarantees as of know, since I have not reviewed this patch in a long time. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12646947",
            "date": "2008-11-12T16:35:50+0000",
            "content": "Some initial thoughts on moving forward:\n\nI think we can add some generic functionality here via the request params:\n\n1. Tika can provide a lot of metadata about a document.  By metadata, I mean things like the actual author, pages, etc. as provided by the document, not the hardcoded metadata in the http://wiki.apache.org/solr/UpdateRichDocuments.  The hardcoded metadata is also useful and should be retained.  With these, we then need a way to map fields from Tika's metadata to Solr fields.  If no mapping is specified, it tries to use the Tika metadata name as the field name.  If that doesn't exist, then we can rely on dynamic fields or we can allow for a param that passes in the name of a default field to map to.\n\n2.  We can auto detect the mime type or allow for it to be passed in.  Thus, stream.type becomes optional, but is still useful.\n\n3. Tika provides a mechanism for implementing your own SAX ContentHandler and passing that in.  I will likely make this pluggable such that people can provide there own.  I think this would allow people to make even further refinements to the content (i.e. splitting on paragraphs or other things like that?????)\n\nI should have a start of a patch today or tomorrow. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12646987",
            "date": "2008-11-12T18:13:48+0000",
            "content": "\n3. Tika provides a mechanism for implementing your own SAX ContentHandler and passing that in. I will likely make this pluggable such that people can provide there own. I think this would allow people to make even further refinements to the content (i.e. splitting on paragraphs or other things like that?????)\n\nNow that I'm digging in more, this actually isn't needed.  The ProcessorChain can be used for this stuff "
        },
        {
            "author": "Eric Pugh",
            "id": "comment-12647003",
            "date": "2008-11-12T19:07:34+0000",
            "content": "Grant,  I am really excited that you are looking at this patch!  \n\nWhile I am proud of it, and very proud of the number of organizations that have used it, and the people who have improved it (Thanks Chris!); it was just written to scratch an itch, and feel free to rip it apart to come up with a better solution for Solr.  The ability for Solr to injest more formats I think is key aspect, not how this patch works.\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12647017",
            "date": "2008-11-12T19:29:05+0000",
            "content": "I'll separate out my two patches. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12647618",
            "date": "2008-11-14T14:42:17+0000",
            "content": "Question for the people watching this:\n\nWould you prefer a new wiki page and keep the old one for those using Chris/Eric's patch, or would you rather I overwrite/edit the current one?\n\nFWIW, some of the parameters will be the same, but I'm also adding in quite a bit more: boosting, XPath expression support (Tika returns everything as XHTML, so it then becomes possible to restrict down what parts you want to pay attention to), extraction only (i.e. no indexing), support for metadata extraction and indexing, support for sending in \"literals\" which are like the current fieldnames parameter and likely some other pieces.\n\nFYI: Out of the box, Tika has support for: http://incubator.apache.org/tika/formats.html and I know they are adding more things as well, like Flash, etc.\n\nIt should also be noted, that if you are just indexing metadata about a file, it makes more sense to do the work on the client side. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12647619",
            "date": "2008-11-14T14:46:52+0000",
            "content": "I'd rather see the old (err, current) wiki page replaced/renamed, and kept current with the latest patch/commit from this issue.  Nice work Grant! "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12647632",
            "date": "2008-11-14T15:26:37+0000",
            "content": "Grant,\n\nI don't really care if you take over the old wiki page's name or start a new one; maybe it depends on if the updated handler is still going to have a similar name or be called something else. I do think, though, that it might be handy nice to have some wiki page (and maybe some JIRA issue) to maintain the older patch on a temporary basis.\n\nThanks,\nChris "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12647670",
            "date": "2008-11-14T17:24:38+0000",
            "content": "OK, I've created http://wiki.apache.org/solr/ExtractingRequestHandler and linked it from the old page.  I will have a preliminary patch up today. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12647747",
            "date": "2008-11-14T22:59:40+0000",
            "content": "First crack at this.  You'll need to download http://people.apache.org/~gsingers/extraction-libs.tar as it is too big to fit in JIRA.\n\nThere's probably lots wrong with it, so be gentle!  See http://wiki.apache.org/solr/ExtractingRequestHandler to get started. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12647748",
            "date": "2008-11-14T23:01:43+0000",
            "content": "Things to do:\n\n1. Documentation\n2. Way more testing, esp. unit tests of the various parameters\n3. Update NOTICES and LICENSE.txt for the new dependencies. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12647840",
            "date": "2008-11-15T13:03:13+0000",
            "content": "Captured fields weren't being indexed properly. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12647846",
            "date": "2008-11-15T13:19:47+0000",
            "content": "Fix issue with literal mapping "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12647873",
            "date": "2008-11-15T16:45:12+0000",
            "content": "Separated out ID generation to make it easier to override.\n\nI think this is pretty close to being ready to commit, so please review.  I'm wrapped up next week, so I probably won't commit until the end of next week (after 11/21) so please review and provide feedback.  Also, Tika is about to release 0.2, so I may just wait to add that in.\n\nAdded in NOTICE and LICENSE information. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12647875",
            "date": "2008-11-15T16:47:35+0000",
            "content": "Still to do:\n\n1. More unit tests\n\n2. We need to do the crypto notice for Solr once this is committed.   See https://issues.apache.org/jira/browse/NUTCH-621 for examples.  I will link a new issue for this so as not to hold up this patch from being committed.  It just needs to be done before releasing 1.4 "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12647876",
            "date": "2008-11-15T16:52:23+0000",
            "content": "Let's name the patch right, eh? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12647895",
            "date": "2008-11-15T21:12:58+0000",
            "content": "Fix an issue w/ XPath and extract only.  See http://tika.markmail.org/message/kknu3hw7argwiqin "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12649542",
            "date": "2008-11-20T23:45:57+0000",
            "content": "Is the latest patch supposed to contain a file \"solr-word.pdf\"? I don't see one, and my \"ant test\" is failing along these lines:\n\n        org.apache.solr.common.SolrException: java.io.FileNotFoundException: solr-word.pdf (The system cannot find the file specified)\n\tat org.apache.solr.handler.ExtractingDocumentLoader.load(ExtractingDocumentLoader.java:160)\n\tat org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:54)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1313)\n\tat org.apache.solr.util.TestHarness.queryAndResponse(TestHarness.java:331)\n\tat org.apache.solr.handler.ExtractingRequestHandlerTest.loadLocal(ExtractingRequestHandlerTest.java:97)\n\tat org.apache.solr.handler.ExtractingRequestHandlerTest.testExtraction(ExtractingRequestHandlerTest.java:27) "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12649551",
            "date": "2008-11-21T00:11:53+0000",
            "content": "A few comment on the ExtractingDocumentLoader:\n\n\n\tI think I like where this is going.\n\n\n\n\n\tCurrently the default is ext.ignore.und.fl (IGNORE_UNDECLARED_FIELDS) == false, which means that if Tika returns a metadata field and you haven't made an explicit mapping from the Tika fieldname to your Solr fieldname, then Solr will throw an exception and your document add will fail. This doesn't seem sound very robust for a production environment, unless Tika will only ever use a finite list of metadata field names. (That doesn't sound plausible, though I admit I haven't looked into it.) Even in that case, I think I'd rather not have to set up a mapping for every possible field name in order to get started with this handler. Would true perhaps be a better default?\n\n\n\n\n\text.capture / CAPTURE_FIELDS: Do you have a use case in mind for this feature, Grant? The example in the patch is of routing text from <div> tags to one Solr field while routing text from other tags to a different Solr field. I'm kind of curious when this would be useful, especially keeping in mind that, in general, Tika source documents are not HTML, and so when <div> tags are generated they're as much artifacts of Tika as reflecting anything in the underlying document. (You could maybe ask a similar question about ext.inx.attr / INDEX_ATTRIBUTES.)\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12649985",
            "date": "2008-11-22T23:56:32+0000",
            "content": "Here's the solr-word PDF. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12649986",
            "date": "2008-11-23T00:10:20+0000",
            "content": "\nI think I like where this is going.\n\nGreat!  I think the nice thing is as Tika grows, we'll get many more formats all for free.  For instance, I saw someone working on a Flash extractor.\n\n\nCurrently the default is ext.ignore.und.fl (IGNORE_UNDECLARED_FIELDS) == false, which means that if Tika returns a metadata field and you haven't made an explicit mapping from the Tika fieldname to your Solr fieldname, then Solr will throw an exception and your document add will fail. This doesn't seem sound very robust for a production environment, unless Tika will only ever use a finite list of metadata field names. (That doesn't sound plausible, though I admit I haven't looked into it.) Even in that case, I think I'd rather not have to set up a mapping for every possible field name in order to get started with this handler. Would true perhaps be a better default?\n\nI guess I was thinking that most people will probably start out with this by sending their docs through the engine and see what happens.  I think an exception helps them see sooner what they are missing.  That being said, I don't feel particularly strong about it.   It's easy enough to set it to true in the request handler mappings.    From what I see of Tika, though, the possible values for metadata is fixed within a version.  Perhaps the bigger issue is what happens when someone updates Tika to a newer version with newer Metadata options.\n\n\next.capture / CAPTURE_FIELDS: Do you have a use case in mind for this feature, Grant? The example in the patch is of routing text from <div> tags to one Solr field while routing text from other tags to a different Solr field. I'm kind of curious when this would be useful, especially keeping in mind that, in general, Tika source documents are not HTML, and so when <div> tags are generated they're as much artifacts of Tika as reflecting anything in the underlying document. (You could maybe ask a similar question about ext.inx.attr / INDEX_ATTRIBUTES.)\n\nFor capture fields, it's similar to a copy field function.  Say, for example, you want a whole document in one field, but also to be able to search within paragraphs.  Then, you could use a capture field on a <p> tag to do that.  Thus, you get the best of both worlds.  The Tika output, is XHTML.\n\nAlso, since extraction is happening on the server side, I want to make sure we have lots of options for dealing with the content.  I don't know where else one would have options to muck with the content post-extraction, but pre-indexing.  Hooking into the processor chain is too late, since then the Tika structure is gone.  That's my reasoning, anyway.  \n\nSimilarly, for index attributes.  When extracting from an HTML file, and it comes across anchor tags (<a>) it will provide the attributes of the tags as XML attributes.  So, one may want to extract out the links separately from the main content and put them into a separate field. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12650353",
            "date": "2008-11-24T22:10:39+0000",
            "content": "if Tika returns a metadata field and you haven't made an explicit mapping from the Tika fieldname to your Solr fieldname, then Solr will throw an exception and your document add will fail. This doesn't seem sound very robust for a production environment, unless Tika will only ever use a finite list of metadata field names.\n\nI'm not familiar with the state of the patch, but i'm assuming that (by default) all of the metadata fields produced by tika have a common naming convention \u2013 either in terms of a common prefix or a common suffix.  in which case people can always make a dynamicField declaration to ignore all metadata fields not already explicitly declared. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12650359",
            "date": "2008-11-24T22:24:39+0000",
            "content": "\nI'm not familiar with the state of the patch, but i'm assuming that (by default) all of the metadata fields produced by tika have a common naming convention - either in terms of a common prefix or a common suffix. in which case people can always make a dynamicField declaration to ignore all metadata fields not already explicitly declared.\n\nNo, they don't, but that is a good idea for Tika. "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12650363",
            "date": "2008-11-24T22:31:38+0000",
            "content": "The 2008-11-15 01:12 PM version of SOLR-284.patch contains modifications to client/java/solrj/src/org/apache/solr/client/solrj/util/ClientUtils.java related to date handling. That's not intentional, is it? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12650365",
            "date": "2008-11-24T22:52:26+0000",
            "content": "\nThe 2008-11-15 01:12 PM version of SOLR-284.patch contains modifications to client/java/solrj/src/org/apache/solr/client/solrj/util/ClientUtils.java related to date handling. That's not intentional, is it?\n\nYes, it is intentional.  The user will need to be able to pass in/configure their own Date formats for their documents and the implementation has to be able to map those to Solr's canonical date format.  Thus, I moved the date handling stuff to a \"common\" DateUtils class (and deprecated it in ClientUtils) because it is needed on the server side too.  Unfortunately, it looks like I did some reformatting on the class as a whole, too.  Sorry 'bout that. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12650368",
            "date": "2008-11-24T23:00:54+0000",
            "content": "I like how Erik has given names to contribs, etc.: Flare, Celeritas, etc.  So, I thought I would give one too:\n\nI was typing the javadocs and wrote \"Solr Content Extraction Library\".   Which then lead me to \"Solr Cell\" as the project name?  http://en.wikipedia.org/wiki/Solar_cell  It's also nice, b/c a Solar Cell's job is to convert the raw energy of the Sun to electricity, and this contrib's module is responsible for \"raw\" content of a document to something usable by Solr.\n\nI know, I know, get a life...    Still, it beats \"ExtractingRequestHandler\" as a name! "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12650431",
            "date": "2008-11-25T01:08:42+0000",
            "content": "I'm not familiar with the state of the patch, but i'm assuming that (by default) all of the metadata fields produced by tika have a common naming convention - either in terms of a common prefix or a common suffix. in which case people can always make a dynamicField declaration to ignore all metadata fields not already explicitly declared.\n\nTika doesn't need to do this explicitly.... you know all fields coming out of your call to the Tika API will be Tika fields.  Solar Cell (I'm on board with that nickname, Grant - now you're catching on  - thus we could map all Tika output fields to tika_* where * is the Tika outputted field name.  And with field name mapping this default would be overridden, say tika_title mapped to \"title\".   Just some off the cuff thoughts. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12650634",
            "date": "2008-11-25T17:21:26+0000",
            "content": "Tika doesn't need to do this explicitly.... you know all fields coming out of your call to the Tika API will be Tika fields. Solar Cell could map all Tika output fields to tika_* where * is the Tika outputted field name. And with field name mapping this default would be overridden, say tika_title mapped to \"title\". \n\nI can add in an option to have it do this mapping. "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12651066",
            "date": "2008-11-26T17:18:19+0000",
            "content": "The 2008-11-15 01:12 PM SOLR-284.patch wasn't applying cleanly to trunk r720403 for me. (One of the hunks for client/java/solrj/src/org/apache/solr/client/solrj/util/ClientUtils.java wouldn't apply.) With this very small update, it does apply cleanly. "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12651073",
            "date": "2008-11-26T17:42:28+0000",
            "content": "On r720403B, I'm noticing that before I apply this patch tests pass, whereas after I apply this patch the following tests fail:\n\nsolr.client.solrj.embedded.JettyWebappTest\nsolr.client.solrj.embedded.LargeVolumeJettyTest\nsolr.client.solrj.embedded.SolrExampleJettyTest\nsolr.client.solrj.response.TestSpellCheckResponse\n\nIn each case Solr outputs this exception: \"On Solr startup: SEVERE: org.apache.solr.common.SolrException: Error loading class 'org.apache.solr.handler.ExtractingRequestHandler'\"\n\nI'm not sure the best way to get the ExtractingRequestHandler into the classpath here.\n\nSort of related, I've noticed that ExtractingRequestHandler doesn't currently get built into the .war file when you run \"ant example\", in contrast to DataImportHandler, which does get put into the .war by means of this target in its build.xml (among other targets):\n\n  <target name=\"dist\" depends=\"build\">\n  \t<copy todir=\"../../build/web\">\n  \t\t<fileset dir=\"src/main/webapp\" includes=\"**\" />\n  \t</copy>\n  \t<mkdir dir=\"../../build/web/WEB-INF/lib\"/>\n  \t<copy file=\"target/${fullnamever}.jar\" todir=\"${solr-path}/build/web/WEB-INF/lib\"></copy>\n  \t<copy file=\"target/${fullnamever}.jar\" todir=\"${solr-path}/dist\"></copy>\n  </target>\n\nShould ExtractingRequestHandler's build.xml perhaps have an analogous \"dist\" target, along these lines:\n\n  <target name=\"dist\" depends=\"build\">\n  \t<mkdir dir=\"../../build/web/WEB-INF/lib\"/>\n  \t<copy file=\"build/${fullnamever}.jar\" todir=\"${solr-path}/build/web/WEB-INF/lib\"></copy>\n  \t<copy file=\"build/${fullnamever}.jar\" todir=\"${solr-path}/dist\"></copy>\n  </target> "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12651146",
            "date": "2008-11-26T21:15:57+0000",
            "content": "Small change to the 2008-11-26 09:18 AM SOLR-284.patch (my previous one), this time adding an \"example\" ant target to contrib/javascript/build.xml. (Without this top-level \"ant example\" was failing.) "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12651212",
            "date": "2008-11-27T01:23:18+0000",
            "content": "This should be the last change for today.\n\nThis change adds a resource.name parameter that you can pass to the handler. (I'm guessing you'll probably typically pass a filename, though Tika does use the more general term \"resource name\".) If you provide it, Tika can take advantage of it when applying its heuristics to determine the MIME type.\n\nAffected files:\n\n\n\tExtractingParams.java\n\tExtractingDocumentLoader.java\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12651813",
            "date": "2008-11-30T13:17:40+0000",
            "content": "Sort of related, I've noticed that ExtractingRequestHandler doesn't currently get built into the .war file when you run \"ant example\", in contrast to DataImportHandler, which does get put into the .war by means of this target in its build.xml (among other targets):\n\nYes, it does NOT get put into the WAR on purpose.  Unfortunately, I think the DIH does this wrong (but it's probably too late now).  A contrib should be optional, as not everyone wants it/needs it.  Solr Cell works solely by putting it into the Solr Home lib directory and then hooking it into the config. "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12652630",
            "date": "2008-12-03T00:49:57+0000",
            "content": "Changes since my previous upload:\n\n\n\tsync CHANGES.txt with trunk\n\ttest cases for adding plain text data\n\tyou aren't forced to map a field if you use the resource.name parameter\n\n "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12652967",
            "date": "2008-12-03T20:46:28+0000",
            "content": "As I mentioned before, tests for these \n\n  solr.client.solrj.embedded.JettyWebappTest\n  solr.client.solrj.embedded.LargeVolumeJettyTest\n  solr.client.solrj.embedded.SolrExampleJettyTest\n  solr.client.solrj.embedded.TestSpellCheckResponse\n\nwere failing, with Solr giving a classnotfoundexception for one of the extracting document loader (ie Solr Cell) classes.\n\nThis revision fixes this by removing all references to this Tika handler from /trunk/example/conf/solrconfig.xml and /trunk/example/conf/schema.xml. Note that these references still exist (and are still used for testing) in /trunk/contrib/extraction/src/test/resources/solr/conf.\n\nThere are probably other ways to make these tests pass, perhaps involving changing the setUp() methods for the above mentioned tests' java files. (For example, maybe you could fiddle with the path parameter passed to the WebAppContext constructor in JettyWebappTest.java? I don't really know anything about this embedded stuff.) I like the current approach, though, because it avoids further changes to code that's logically independent of this handler. "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12652993",
            "date": "2008-12-03T22:01:27+0000",
            "content": "Currently this patch deploys the Tika libs to /trunk/example/solr/lib. I'm curious where the Tika handler's lib/ directory is supposed to go in a multicore deployment. I created my own multicore setup more or less like this:\n\n\n\tant example\n\tCopy /trunk/example to /trunk/solr-10000\n\tCopy /trunk/solr-10000/multicore/* to /trunk/solr-10000/solr.\n\n\n\n(Solr-10000 means \"copy of Solr I plan to run on port 10000.\")\n\nThis seems to be the easiest way to set things up so that I can cd to /trunk/solr-10000 and run start.jar to get multicore Solr running.\n\nOr rather, that would get multicore Solr running, except that Solr gets a can't-find-the-Tika-classes exception. So I guess /trunk/solr-10000/solr/lib is not where the lib directory goes for multicore deployment.\n\nSo I tried putting Tika libs instead in /trunk/solr-10000/solr/core0/lib, and that loaded fine. That doesn't seem like the right place for the directory, though; it seems like each core shouldn't have to have its own separate copy of the Tika libs.\n\nSo where do the Tika libs go? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12653137",
            "date": "2008-12-04T02:30:23+0000",
            "content": "I think in multicore you can specify a shared library in the solr.xml directory, so you could put the tika stuff in that dir.\n\n\nAs for the tests, I didn't know the tests had a dependency on the example directory.  That doesn't seem good.  I'm with a client all this week, but will try to get to it this weekend. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12654063",
            "date": "2008-12-06T13:06:49+0000",
            "content": "Committed revision 723977.\n\nCommitted Chris' patch, w/ the modification that I put the ext prefix on the resource.name and stream.type.\n\nI also added a ext.metadata.prefix option, which can be used to map the Tika metadata to a dynamic Field, as Erik described above.\n\nSee the Wiki page for details: http://wiki.apache.org/solr/ExtractingRequestHandler\n\nThanks for everyone's input and work! "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12654234",
            "date": "2008-12-07T19:34:32+0000",
            "content": "Looks like there are a bunch of duplicate .jar files in lib.  You could remove these and use the ones that are already in /lib\n\n\nIndex: contrib/extraction/lib/commons-io-1.4.jar\nIndex: contrib/extraction/lib/commons-codec-1.3.jar\nIndex: contrib/extraction/lib/commons-lang-2.1.jar\nIndex: contrib/extraction/lib/commons-logging-1.0.4.jar\nIndex: contrib/extraction/lib/junit-3.8.1.jar\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12654235",
            "date": "2008-12-07T19:59:54+0000",
            "content": "Thanks, Ryan, I will remove them. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12656018",
            "date": "2008-12-12T14:08:12+0000",
            "content": "Forgot a couple of things on this:\n\n1. To hook into the release/javadoc mechanism.\n2. In order to facilitate separation of the javadocs and other things, I'm going to move the code to o.a.s.handler.extraction package.\n3. Need to publish the Maven artifacts. "
        },
        {
            "author": "Rog\u00e9rio Pereira Ara\u00fajo",
            "id": "comment-12656023",
            "date": "2008-12-12T14:38:36+0000",
            "content": "Grant, lemme know how can I help. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12656032",
            "date": "2008-12-12T15:22:04+0000",
            "content": "OK, I just committed:\n\n1. Upgraded to Tika 0.2 official release\n2. Put in POM support\n3. Hooked in various other build things. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12662616",
            "date": "2009-01-10T04:02:45+0000",
            "content": "The ExtractingRequestHandler has its own UUID generation code. \n\nShould the schema designer just use the UUID field type or decide to have no unique key field?  This seems more modular and follows other aspects of the design.\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12662660",
            "date": "2009-01-10T14:11:08+0000",
            "content": "Should the schema designer just use the UUID field type or decide to have no unique key field? This seems more modular and follows other aspects of the design.\n\nI guess I usually prefer having a unique key field, as it always gives you that one last handle to grab on to to find a specific document.  However, I'm not sure I follow what you mean by having no uniq. field being more modular.\n\nI put in the code b/c I figured it was better to generate an ID than to outright reject the document, since unlike when adding XML, sending large files can be really expensive, so I wanted it to handle as many edge cases as possible and still accept a document.\n\nHere's the code:\n\nSchemaField uniqueField = schema.getUniqueKeyField();\n    if (uniqueField != null) {\n      String uniqueFieldName = uniqueField.getName();\n      SolrInputField uniqFld = document.getField(uniqueFieldName);\n      if (uniqFld == null) {\n        String uniqId = generateId(uniqueField);\n        if (uniqId != null) {\n          document.addField(uniqueFieldName, uniqId);\n        }\n      }\n    }\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12662696",
            "date": "2009-01-10T21:36:26+0000",
            "content": "I put in the code b/c I figured it was better to generate an ID than to outright reject the document,\n\nHmmm ... that means that if i have a schema with a uniqueKey field, and i forget to specify a uniqueKey value when indexing my document, the handler will \"silently succeed\" in adding a document with a key i have no control over instead of failing in a way that will make me aware of my mistake \u2013 and i have no way of configuring solr to prevent that kind of \"silent success\"\n\nIf i wanted that behavior, i could configure the schema with a UUIDFIeld as the uniqueKey and take advantage of the default.  but as it is now i have no way to prevent it.\n\nI would think consistency and flexibility is more important, and remove that \"generateId\" functionality along the lines of Lance's suggestion. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12662793",
            "date": "2009-01-11T15:44:05+0000",
            "content": "Hmmm ... that means that if i have a schema with a uniqueKey field, and i forget to specify a uniqueKey value when indexing my document, the handler will \"silently succeed\" in adding a document with a key i have no control over instead of failing in a way that will make me aware of my mistake - and i have no way of configuring solr to prevent that kind of \"silent success\"\n\nActually, there is a mechanism for avoiding it, and it is documented on in http://wiki.apache.org/solr/ExtractingRequestHandler#head-6cda7b8832bb2ccaf6b0b57a6ef524b553db489e\n\nI could, however, see adding a flag to specify whether one wants \"silent success\" or not.  I think the use case for content extraction is different than the normal XML message path.  Often times, these files are quite large and the cost of sending them to the system is significant.  \n\nAnother thing that might be interesting to do is to actually return in the the response the generated id. "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12663150",
            "date": "2009-01-13T00:01:11+0000",
            "content": "I could, however, see adding a flag to specify whether one wants \"silent success\" or not. I think the use case for content extraction is different than the normal XML message path. Often times, these files are quite large and the cost of sending them to the system is significant.\n\nIn my own use case of the handler, I imagine the fail-on-missing-key policy would be the more helpful policy. This is because I want to be in control of my own key, and if Solr fails as soon as I don't provide one, that's going to help me find the bug in my indexing code right away, whereas \"silent success\" will allow that bug to fester. I'm not sure there would be significant countervailing advantages to the other policy. It's true that transferring a large file when you're just going to get an error message wastes some time, but I feel like in debugging there's potential to waste a lot more time.\n\nMy first choice would be for fail-on-missing-key to be the default, followed by having an easy-to-set flag. In any case, though, it would be nice not to have to create a custom SolrContentHandler just to get this one sanity check. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12663186",
            "date": "2009-01-13T02:03:44+0000",
            "content": "I guess I'm fine with it.  So, should we remove key generation all together? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12671482",
            "date": "2009-02-07T16:01:44+0000",
            "content": "Remove Key Generation.  Will commit shortly "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12671483",
            "date": "2009-02-07T16:03:27+0000",
            "content": "I removed the auto key generation: Committed revision 741907.  I think this can officially close out this patch. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12724855",
            "date": "2009-06-27T14:09:58+0000",
            "content": "Not sure if I should open a new issue or keep improvements here.\nI think we need to improve the OOTB experience with this...\nhttp://search.lucidimagination.com/search/document/302440b8a2451908/solr_cell\n\nIdeas for improvement:\n\n\tauto-mapping names of the form Last-Modified to a more solrish field name like last_modified\n\tdrop \"ext.\" from parameter names, and revisit naming to try and unify with other update handlers like CSV\n  note: in the future, one could see generic functionality like boosting fields, setting field value defaults, etc, being handled by a generic component or update processor... all the better reason to drop the ext prefix.\n\tI imagine that metadata is normally useful, so we should\n  1. predefine commonly used metadata fields in the example schema... there's really no cost to this\n  2. use mappings to normalize any metadata names (if such normalization isn't already done in Tika)\n  3. ignore or drop fields that have little use\n  4. provide a way to handle new attributes w/o dropping them or throwing an error\n\tenable the handler by default - lazy to avoid a dependency on having all the tika libs available\n\n "
        },
        {
            "author": "Eric Pugh",
            "id": "comment-12724856",
            "date": "2009-06-27T14:11:50+0000",
            "content": "I am out of the office 6/29 - 6/30.  For urgent issues, please contact\nJason Hull at jhull@opensourceconnections.com or phone at (434)\n409-8451. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12724858",
            "date": "2009-06-27T14:33:04+0000",
            "content": "Oops, there is a \"ext.metadata.prefix\" that I missed on the first pass.  This should be defaulted to handle unknown attributes. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12724859",
            "date": "2009-06-27T14:36:01+0000",
            "content": "ext.capture seems problematic in that one needs a separate ext.map statement to move what you capture... but it doesn't seem to work well if you already have fieldnames that might match something you are trying to capture.\n\nperhaps something of the form\ncapture.targetfield=expression\nwould work better? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12724862",
            "date": "2009-06-27T14:58:44+0000",
            "content": "I just tried setting ext.idx.attr=false, and I didn't see any change after indexing a PDF.\nPerhaps we don't even need this option if we map attributes to an ignored_ field that is ignored?\nIn any case, the default seems like it should generate / index attributes. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12724863",
            "date": "2009-06-27T15:11:19+0000",
            "content": "Another comment on parameter naming: period is more like a scoping operator, and less like a word separator.  Hence ext.ignore.und.fl is more readable as ext.ignore_undefined or something. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12724871",
            "date": "2009-06-27T16:58:28+0000",
            "content": "Apologies for not reviewing this sooner after it was committed - but this is the last/best chance to improve the interface before 1.4 is released (and this is very important new functionality).\n\nSince the \"ext.\" seems unnecessary and removing is already a name change, we might as well revisit the names themselves anyway.  Here are my first thoughts on it:\n\n//////// generic type stuff that could be reused by other update handlers\nboost.myfield=2.3\nliteral.myfield=Hello\nmap.origfield=newfield\nuprefix=attr_ \n  // map any unknown fields using a standard prefix... good for\n  // dynamic field mapping.\n\n//////// more solr cell specific\ncapture.target_field=div\n  // does capture + field-map in single step... avoids name clashes\nxpath=xpath_expr\n  // future: could do xpath.targetfield=xpath_expr\nextract_only=true  // period's aren't word separators, but scoping operators\n // in the future, this could be replaced with a generic update operation\n // to return the document(s) instead of indexing them.\nresource.name=test.pdf\n\nNew idea:\n  nicenames=true // Last-Modified -> last_modified\n\n\nREMOVED:\next.ignore.und.fl \n  // throwing an exception when a field-type doesn't exist is generic\n  // and not needed.  we should never silently ignore.\next.idx.attr\n  // do we ever want this to be false?  we can ignore all attributes\n  // with field mappings if we want to\next.metadata.prefix\n  // seems like we only want to map unknown fields, not all fields\next.def.fl \n  // we can use a standard field name for indexing main content\n  // and use map to move it if desired. \"content\"? \n\n\n\nDo people view this as an improvement? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12724937",
            "date": "2009-06-28T10:41:00+0000",
            "content": "I just tried setting ext.idx.attr=false, and I didn't see any change after indexing a PDF.\n\nThis is often needed for HTML, where it is used to index the attributes of tags.  Same would go for XML. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12724938",
            "date": "2009-06-28T10:41:31+0000",
            "content": "I will review your comments more tomorrow.  Still waist deep in boxes from the move! "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12725237",
            "date": "2009-06-29T16:35:35+0000",
            "content": "Apologies for not reviewing this sooner after it was committed - but this is the last/best chance to improve the interface before 1.4 is released (and this is very important new functionality).\n\nMy only request is that, if you're changing how field mapping works and maybe removing ext.ignore.und.fl, you make sure it stays easy to say, \"Tika, I don't care about any of your parsed metadata. Please leave it out of my Solr index.\" In my current use case I already know all the metadata I want, and including the Tika-parsed fields would result in index bloat. (My temptation would be to make excluding Tika-parsed fields the default, though it sounds like other people have the opposite inclination.) "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12725355",
            "date": "2009-06-29T21:18:08+0000",
            "content": "ext.ignore.und.fl\n\nI think this should be kept and this is a case where we should silently ignore.  Parsing rich data is a different beast than normal Solr XML or other structured content.  There are a lot of times where you only want to get specific fields and there can be a large number of fields.  It is burdensome to have to add the ignores for all the metadata.  Not to mention different types may have different metadata.  So, -1 on removing.\n\next.idx.attr\n\nYes, we may want it to be false.  That's why I put it in!    It can be used to extract things like HREF into other fields or not.  Think faceting.\n\next.metadata.prefix\n\nThis is not a mapping thing so much as a way to separately handle metadata fields from the main text fields.  I'm not sure if it differs from the uprefix approach you are proposing except you can know exactly what is metadata and what isn't.\n\n\nOther questions that Yonik brought up:\n\n1. I don't think trying to auto map is a good idea.  New file formats will have new ways of doing them, it's better to have the user handle it.  \n2. Fine with dropping ext for common names\n3. Metadata is often not useful and I don't think we need to do work as suggested.  See Eric's comment above.\n4. Enabling by default is fine.\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12726113",
            "date": "2009-07-01T15:42:44+0000",
            "content": "My only request is that, if you're changing how field mapping works and maybe removing ext.ignore.und.fl, you make sure it stays easy to say, \"Tika, I don't care about any of your parsed metadata.\n\nMap unknown fields to an ignored fieldtype.\nuprefix=ignored_ "
        },
        {
            "author": "Eric Pugh",
            "id": "comment-12726115",
            "date": "2009-07-01T15:47:48+0000",
            "content": "I am out of the office 6/29 - 6/30.  For urgent issues, please contact\nJason Hull at jhull@opensourceconnections.com or phone at (434)\n409-8451. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12726116",
            "date": "2009-07-01T15:49:34+0000",
            "content": "It is burdensome to have to add the ignores for all the metadata.\n\nIt would be easy to change the default from index to ignore:\nuprefix=ignored_    // ignored_ will be defined in the schema as indexed=false, stored=false\nuprefix=attr_\n\nActually, that brings up another random question... when we get the metadata back from Tika, is it typed (can we tell that number of pages is an integer?) "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12726122",
            "date": "2009-07-01T16:03:06+0000",
            "content": ">> I just tried setting ext.idx.attr=false, and I didn't see any change after indexing a PDF.\n> This is often needed for HTML, where it is used to index the attributes of tags. Same would go for XML.\n\nThat's confusing given that the examples on the wiki show PDFs being indexed with ext.idx.attr=true\n\nIt also confused me since the docs say \"Index the Tika XHTML attributes into separate fields, named after the attribute.\" and the docs also say \"Tika does everything by producing an XHTML stream that it feeds to a SAX ContentHandler\".\nThat led me to believe that ext.idx.attr was for all tika generated metadata (or maybe it is, but tika doesn't generally use attributes?)\n\nIt's also rather confusing just what rules can be applied to what.  For example, does ext.metadata.prefix work on stuff produced by ext.idx.attr?\nedit: nope, I just tried, and that does not work. "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12726123",
            "date": "2009-07-01T16:04:21+0000",
            "content": "\nMy only request is that, if you're changing how field mapping works and maybe removing ext.ignore.und.fl, you make sure it stays easy to say, \"Tika, I don't care about any of your parsed metadata.\n\nMap unknown fields to an ignored fieldtype.\nuprefix=ignored_\n\nThat seems fine.\n\nTangentially, I wonder how fast Tika's metadata extraction is, compared to its main body text extraction. If the latter doesn't dwarf the former, there might be value in adding a \"Solr, don't even ask Tika to calculate for metadata at all; just have it extract the body text\" flag; this could potentially speed things up for people that don't need the metadata. Maybe it would make sense to benchmark things before adding such a flag, though. I also don't have a good sense of how many people will want to use the metadata feature vs how many don't. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12728144",
            "date": "2009-07-07T15:19:00+0000",
            "content": "The current ext.metadata.prefix parameter adds the prefix to all attributes, even those that have already been mapped (so last_modified appears instead as attr_last_modified).  Seems like one really wants a prefix appended only for those params that are not explicitly mapped (or don't appear in the schema)... this is what the proposed \"uprefix\" (unknown field prefix) would do. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12728303",
            "date": "2009-07-07T19:10:32+0000",
            "content": "The date.format thing is interesting.... but shouldn't that really be part of a Date fieldType that can accept all those formats?\nTransforming in the update handler only means that you could add a literal.mydate=date1 via the update handler, and then fail to query it (because the date parsing was specific to the update handler.)\n\nPerhaps we could add this to the new trie field for dates? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12728636",
            "date": "2009-07-08T11:40:43+0000",
            "content": "The date.format thing is interesting.... but shouldn't that really be part of a Date fieldType that can accept all those formats?\n\nAgreed, I was just wanting more Date Field Type capabilities the other day.  It would be nice to be able to specify two things on the Date fieldType:\n1. Input formats accepted like what the ExtractingRequestHandler offers\n2. Output granularity.  That is, may not want to store seconds, etc. so Solr should drop the precision.  Note, this is different from Trie in that it is only indexing one token.\n\nProbably should handle on a separate issue. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12730516",
            "date": "2009-07-13T21:21:05+0000",
            "content": "OK, here's my first crack at cleaning things up a little before release.  Changes:\n\n\tthere were no tests for XML attribute indexing.\n\tcapture had no unit tests\n\tboost has no unit tests\n\tignoring unknown fields had no unit test\n\tmetadata prefix had no unit test\n\tlogging ignored fields at the INFO level for each document loaded is too verbose\n\tremoved handling of undeclared fields and let downstream components\n  handle this.\n\tavoid the String catenation code for single valued fields when Tika only\n  produces a single value (for performance)\n\tremove multiple literal detection handling for single valued fields - let a downstream component handle it\n\tmap literal values just as one would with generated metadata, since the user may be just supplying the extra metadata.  also apply transforms (date formatting currently)\n\tfixed a bug where null field values were being added (and later dropped by Solr... hence it was never caught).\n\tavoid catching previously thrown SolrExceptions... let them fly through\n\tremoved some unused code (id generation, etc)\n\tadded lowernames option to map field names to lowercase/underscores\n\tswitched builderStack from synchronized Stack to LinkedList\n\tfixed a bug that caused content to be appended with no whitespace in between\n\tmade extracting request handler lazy loading in example config\n\tadded ignored_ and attr_ dynamic fields in example schema\n\n\n\nInterface:\n\nThe default field is always \"content\" - use map to change it to something else\nlowernames=true/false  // if true, map names like Content-Type to content_type\nmap.<fname>=<target_field>\nboost.<fname>=<boost>\nliteral.<fname>=<literal_value>\nxpath=<xpath_expr>  - only generate content for the matching xpath expr\nextractOnly=true/false - if true, just return the extracted content\ncapture=<xml_element_name>  // separate out these elements \ncaptureAttr=<xml_element_name>   // separate out the attributes for these elements\nuprefix=<prefix>  // unknown field prefix - any unknown fields will be prepended with this value\nstream.type\nresource.name\n\n\n\nTo try and make things more uniform, all fields, whether \"content\" or metadata or attributes or literals, all go through the same process.\n1) map to lowercase if lowernames=true\n2) apply map.field rules\n3) if the resulting field is unknown, prefix it with uprefix\n\nHopefully people will agree that this is an improvement in general.  I think in the future we'll need more advanced options, esp around dealing with links in HTML and more powerful xpath constructs, but that's for after 1.4 IMO. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12730929",
            "date": "2009-07-14T15:54:39+0000",
            "content": "OK, I've committed the above.  I'll work on updating the wiki, including clarifying things that didn't make sense the first time I looked at this. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12753865",
            "date": "2009-09-10T22:31:32+0000",
            "content": "Attaching a schema update to define some common useful metadata fields to improve the OOTB experience.\nAny concerns or suggestions for improvements?  I'd like to commit shortly to get it into 1.4\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12754626",
            "date": "2009-09-12T22:16:44+0000",
            "content": "I don't think we should drop ext.def.fl (the name can change, but the functionality is useful) and am going to reopen this.  Namely, it is often the case where one wants all values that aren't explicitly mapped to go into a default field and I don't think that is possible using uprefix.  Since all metadata fields aren't knowable up front, there is currently no way to express this in the ExtractingRequestHandler. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12754635",
            "date": "2009-09-12T23:01:56+0000",
            "content": "Adds in defaultField parameter and tests. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12754644",
            "date": "2009-09-13T00:25:48+0000",
            "content": "it is often the case where one wants all values that aren't explicitly mapped to go into a default field\n\nWhat's the real use-case, to be able to search all metadata?  One could use a dynamic copyField into a single indexed field.  That also helps if one sttill wants to keep all of the stored values for the metadata in separate fields. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12754646",
            "date": "2009-09-13T01:17:25+0000",
            "content": "What's the real use-case, to be able to search all metadata? One could use a dynamic copyField into a single indexed field. That also helps if one sttill wants to keep all of the stored values for the metadata in separate fields.\n\nYeah, that works too, but it is convoluted and I may not care about storing the attributes nor want to deal with copyFields and the extra performance costs.  It just seems easier to have a default field capability.  Then one can just have everything go to it. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12755008",
            "date": "2009-09-14T14:52:54+0000",
            "content": "Yonik, any objections to me committing the current patch given your concerns? "
        },
        {
            "author": "David Smiley",
            "id": "comment-12755053",
            "date": "2009-09-14T16:12:14+0000",
            "content": "Grant, your response confuses me.  How does a copyField necessitate storing the fields?  And how is the copyField slower than this feature mapping to a common attribute which ends up with an equivalent outcome? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12755058",
            "date": "2009-09-14T16:22:31+0000",
            "content": "How does a copyField necessitate storing the fields?\n\nYonik suggested his approach helps with stored values for the metadata.\n\nAnd how is the copyField slower than this feature mapping to a common attribute which ends up with an equivalent outcome? \n\nAs I understand Yonik's response, he was suggesting that I use the uprefix combined with copy fields.  That involves two field entries, when I only care about the one catch all.  copyFields do have a cost, especially when you don't need them.\n\nAt any rate, with the patch I put up, you have the option of doing it either way. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12755468",
            "date": "2009-09-15T12:42:24+0000",
            "content": "Committed revision 815293. "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12756154",
            "date": "2009-09-16T18:25:52+0000",
            "content": "This caught me by surprise, so I'm noting it here in case it helps anyone else:\n\nIn SVN r815830 (September 16, 2009), Grant renamed the field name mapping argument \"map\" to \"fmap\". The reason was to make naming more consistent with the CSV handler. For more info on this see the following thread:\n\nhttp://www.nabble.com/Fwd%3A-CSV-Update---Need-help-mapping-csv-field-to-schema%27s-ID-td25463942.html\n "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12756259",
            "date": "2009-09-16T22:27:45+0000",
            "content": "Grant and company: I just noticed that the example solrconfig.xml at the head of SVN trunk still uses map, not fmap. (In particular, there's \"map.content\", \"map.a\", and \"map.div\".) I assume this should be fixed for the 1.4 release. Interestingly, this doesn't seem to make any unit tests fail. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12756266",
            "date": "2009-09-16T22:43:36+0000",
            "content": "example solrconfig.xml at the head of SVN trunk still uses map, not fmap.\n\nThanks, I just fixed this. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12775499",
            "date": "2009-11-10T15:51:42+0000",
            "content": "Bulk close for Solr 1.4 "
        }
    ]
}