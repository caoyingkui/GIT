{
    "id": "SOLR-11598",
    "title": "ExportWriter should support sorting on more than 4 sort fields",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "streaming expressions"
        ],
        "type": "Improvement",
        "fix_versions": [
            "7.5",
            "master (8.0)"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "I am a user of Streaming and I am currently trying to use rollups on an 10 dimensional document.\nI am unable to get correct results on this query as I am bounded by the limitation of the export handler which supports only 4 sort fields.\nI do not see why this needs to be the case, as it could very well be 10 or 20.\nMy current needs would be satisfied with 10, but one would want to ask why can't it be any decent integer n, beyond which we know performance degrades, but even then it should be caveat emptor.\n\n\nVarun Thacker \n\nCode Link:\nhttps://github.com/apache/lucene-solr/blob/19db1df81a18e6eb2cce5be973bf2305d606a9f8/solr/core/src/java/org/apache/solr/handler/ExportWriter.java#L455\n\nError\nnull:java.io.IOException: A max of 4 sorts can be specified\n\tat org.apache.solr.handler.ExportWriter.getSortDoc(ExportWriter.java:452)\n\tat org.apache.solr.handler.ExportWriter.writeDocs(ExportWriter.java:228)\n\tat org.apache.solr.handler.ExportWriter.lambda$null$1(ExportWriter.java:219)\n\tat org.apache.solr.common.util.JavaBinCodec.writeIterator(JavaBinCodec.java:664)\n\tat org.apache.solr.common.util.JavaBinCodec.writeKnownType(JavaBinCodec.java:333)\n\tat org.apache.solr.common.util.JavaBinCodec.writeVal(JavaBinCodec.java:223)\n\tat org.apache.solr.common.util.JavaBinCodec$1.put(JavaBinCodec.java:394)\n\tat org.apache.solr.handler.ExportWriter.lambda$null$2(ExportWriter.java:219)\n\tat org.apache.solr.common.util.JavaBinCodec.writeMap(JavaBinCodec.java:437)\n\tat org.apache.solr.common.util.JavaBinCodec.writeKnownType(JavaBinCodec.java:354)\n\tat org.apache.solr.common.util.JavaBinCodec.writeVal(JavaBinCodec.java:223)\n\tat org.apache.solr.common.util.JavaBinCodec$1.put(JavaBinCodec.java:394)\n\tat org.apache.solr.handler.ExportWriter.lambda$write$3(ExportWriter.java:217)\n\tat org.apache.solr.common.util.JavaBinCodec.writeMap(JavaBinCodec.java:437)\n\tat org.apache.solr.handler.ExportWriter.write(ExportWriter.java:215)\n\tat org.apache.solr.core.SolrCore$3.write(SolrCore.java:2601)\n\tat org.apache.solr.response.QueryResponseWriterUtil.writeQueryResponse(QueryResponseWriterUtil.java:49)\n\tat org.apache.solr.servlet.HttpSolrCall.writeResponse(HttpSolrCall.java:809)\n\tat org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:538)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:361)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:305)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1691)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:582)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:119)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.rewrite.handler.RewriteHandler.handle(RewriteHandler.java:335)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:534)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:273)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:95)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\tat java.lang.Thread.run(Thread.java:745)",
    "attachments": {
        "SOLR-11598.patch": "https://issues.apache.org/jira/secure/attachment/12898348/SOLR-11598.patch",
        "streaming-export reports.xlsx": "https://issues.apache.org/jira/secure/attachment/12930068/streaming-export%20reports.xlsx",
        "SOLR-11598-6_6.patch": "https://issues.apache.org/jira/secure/attachment/12897060/SOLR-11598-6_6.patch",
        "SOLR-11598-6_6-streamtests": "https://issues.apache.org/jira/secure/attachment/12897070/SOLR-11598-6_6-streamtests",
        "SOLR-11598-master.patch": "https://issues.apache.org/jira/secure/attachment/12897059/SOLR-11598-master.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-11-03T03:08:35+0000",
            "content": "I don't think the export handler will perform very well with 10 sort fields. There is a real tradeoff between the number of sort fields and performance. This is also true for the number of fields in the field list. So while it is easy to increase the number of sort fields, it will very likely be too slow for any kind of production use. ",
            "author": "Joel Bernstein",
            "id": "comment-16237025"
        },
        {
            "date": "2017-11-10T10:34:34+0000",
            "content": "Aroop,\n\nI have attached patches against master and branch_6_6 branches supporting maximum 8 fields instead of current 4, so that we can analyse how the performance gets affected. I have also included very basic tests for ExportWriter; but effective. ",
            "author": "Amrit Sarkar",
            "id": "comment-16247330"
        },
        {
            "date": "2017-11-10T11:55:02+0000",
            "content": "Added another \"experimental\" patch: SOLR-11598-6_6-streamtests against branch_6_6 with nocommit supporting stream expressions (unique & rollup) can take more than 4 sort fields now.\n\nPlease mind, these patches are for pure experimental performance analysis purpose. ",
            "author": "Amrit Sarkar",
            "id": "comment-16247386"
        },
        {
            "date": "2017-11-18T18:41:27+0000",
            "content": "On seeing the SortDoc implementation for Single, Double, .. Quad in ExportWriter.java; it seems repeated code for me since most of the code is already implemented in SortDoc expect compareTo function which I did on the newly uploaded patch. All the tests are getting passed. \n\nAlso increasing the max sort fields to 10, as repeated tests on large dataset with increased sort fields showed very little difference in performance. Looking at the code closely, seems the performance difference is linear, than exponential / polynomial :: lessThan and compareTo methods. ",
            "author": "Amrit Sarkar",
            "id": "comment-16258146"
        },
        {
            "date": "2017-11-19T08:38:53+0000",
            "content": "Aroop,\n\nI will perform tests with the patch and share results if permitted.\nI think everyone would be pleased if you share. I tested with 1M records and didn't see almost any performance degradation but I think we need to verify this on larger dataset.\n\nAlso, if you have determined this to have O(N) performance characteristic, are you planning to make it a lot larger and not bounded under 10? \nThere maybe a very good chance I am missing some factor in terms of performance on sorting n-dimensional variables like Joel Bernstein mentioned. I think after analysing your test results, we can safely conclude whether we can increase the bound or even 10 is high.\n\nThanks. ",
            "author": "Amrit Sarkar",
            "id": "comment-16258392"
        },
        {
            "date": "2018-01-08T13:20:20+0000",
            "content": "Aroop,\n\nDo you have some metrics and numbers on Export writer with more than 4 sort fields. We are looking forward to them. ",
            "author": "Amrit Sarkar",
            "id": "comment-16316288"
        },
        {
            "date": "2018-02-18T05:33:27+0000",
            "content": "If and when this patch get's committed , we'd need to edit the \"group by\" docs under http://lucene.apache.org/solr/guide/parallel-sql-interface.html#select-distinct-queries\u00a0which also states that the limit is upto 4 fields. ",
            "author": "Varun Thacker",
            "id": "comment-16368444"
        },
        {
            "date": "2018-02-20T03:42:05+0000",
            "content": "Thank you Varun Thacker for pin-pointing that. I have improved the patch as per recommendation. ",
            "author": "Amrit Sarkar",
            "id": "comment-16369685"
        },
        {
            "date": "2018-03-05T07:33:38+0000",
            "content": "Improved the patch proving the sorting is taking place at Nth position. I can improve the tests more, by adding more documents with diverse field values. Let me know if we need to have that in place. ",
            "author": "Amrit Sarkar",
            "id": "comment-16385722"
        },
        {
            "date": "2018-03-06T15:41:43+0000",
            "content": "I tried the patch and I see promising results over multiple runs:\n\nQuery Times in ms for Search Streaming Expressions with different number of fields in the sort expressions:\n\n\n\nNumber of Sort Fields\n1st Call\n2nd Call\n3rd Call\n4th Call\n\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 10\n9635\n7945\n7042\n6804\n\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 8\n12065\n9017\n6792\n6314\n\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 6\n10733\n7433\n6146\n5508\n\n\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 4\n9654\n6746\n5224\n4497\n\n\n\n\n\n\u00a0\n\nSimilarly for a much more complicated streaming expression of the form\u00a0[select(rollup(search())) ]\n\n\n\nNumber of Sort Fields\nQtime (ms)\u00a0 (1st call)\nQtime (ms) (2nd call)\n\n\n10\n\u00a033,303\n2389\n\n\n8\n\u00a031,453\n1150\n\n\n6\n\u00a026,403\n990\n\n\n4\n\u00a016,346\n705\n\n\n\n\n\n\u00a0\n\nTest Setup details:\n\n\tThese tests were done with the same fields in each call.\u00a0\n\tThis was run against an\u00a0ailas which points to about 34 collections each of size ~ 20 GB. \u00a0(total size: 640 GB, total documents:\u00a05,808,140,268.00 [5.8 Billion] )\n\tThe queries used in this set would have actually fetched data from 2 collections (due to filters ), so 40 GB size and\u00a0456,903,090 documents\u00a0 [456 Million]\n\n\n\n\u00a0 ",
            "author": "Aroop",
            "id": "comment-16387975"
        },
        {
            "date": "2018-03-07T05:53:25+0000",
            "content": "Thank you Aroop for sharing the performance stats. This indeed looks promising and sub-linearity in total sort fields to QTime is expected. Requesting further feedback on this from the community. ",
            "author": "Amrit Sarkar",
            "id": "comment-16389063"
        },
        {
            "date": "2018-05-02T16:11:46+0000",
            "content": "Hi Team - is there feedback on this ? Can this be made a configurable setting in solrconfig or some other jetty properties file ? Per the data I shared above there does not seem any problem with increasing the number of fields. This should be something a Solr architect should be able to configure and tune.  ",
            "author": "Aroop",
            "id": "comment-16461242"
        },
        {
            "date": "2018-06-20T21:29:50+0000",
            "content": "Pinging this thread again.\u00a0\n\nIs there any progress on this ? This is a very important feature for the analytics use cases. ",
            "author": "Aroop",
            "id": "comment-16518620"
        },
        {
            "date": "2018-06-20T21:33:25+0000",
            "content": "Hi Joel Bernstein what do you think about this patch? Do the numbers help convince for the case of increasing this limit ?\u00a0\n\nI'll start looking at the patch early next week in greater detail but wanted your thoughts on it as well ",
            "author": "Varun Thacker",
            "id": "comment-16518622"
        },
        {
            "date": "2018-06-20T23:40:57+0000",
            "content": "If it's not technically difficult to allow\u00a0a large number of sorts as implied above, I think it should be allowed, but if there are strong performance implications that should also be clearly documented too. A low, artificial limit merely prevents the user from making a trade-off decision. For\u00a0a use case that's valuable enough,\u00a0it might be worth it to them to fund a really beefy box (or cluster of beefy boxes) to handle the load. Give the user the tool and the information, let them decide. ",
            "author": "Gus Heck",
            "id": "comment-16518725"
        },
        {
            "date": "2018-06-21T01:37:48+0000",
            "content": "Just reading through the numbers. A couple of questions:\n\nWhat were the number of records exported? Also were the exports done in parallel?\n\nI'm actually surprised that it's performing this well with so many sort fields. I think its fine to move this forward. I'll try to help out with the review and manual testing.\n\n\u00a0 ",
            "author": "Joel Bernstein",
            "id": "comment-16518789"
        },
        {
            "date": "2018-06-21T02:59:30+0000",
            "content": "Hi Joel Bernstein\n\n\u00a0By, number of records exported, do you mean the result-set size of the entire streaming expression? Then that was ~50,000 records after rollup. (this would increase with dimensionality of course)\n\nBy, parallel if you mean if the parallel stream decorator was used then yes we used it. The overall streaming expression was of the type parallel(select(rollup(search()))).\n\nPer, my experiments the growth in dimensions shows linear performance.\u00a0 ",
            "author": "Aroop",
            "id": "comment-16518857"
        },
        {
            "date": "2018-06-26T15:53:07+0000",
            "content": "Hi Joel,\n\nThis is my understanding of the export writer . Does this sound right to you?\u00a0\n\n1. Go through all the matched documents ( q=..&fq.. )\u00a0 and collect 30k sorted docs ( priority queue ) and stream them out\n 2. Repeat till all matched documents have been collected\n\nThe number of matches is key in performance and that we chose 30k docs as the heap size.\n\n\u00a0\n\nSo the number of sort fields will increase the number of seeks against doc-values every time we add an item to the priority queue. Hence the performance would greatly depend on how much memory does the OS have to mmap doc-values?\u00a0\n\n\u00a0 ",
            "author": "Varun Thacker",
            "id": "comment-16523898"
        },
        {
            "date": "2018-06-26T17:20:29+0000",
            "content": "Hi Varun,\n\nYou pretty much have it right. Having a large amount of memory available to the filesystem cache will help. But doc values have other overhead as well which can add up. If you read the source code to the different doc values implementations you'll see there is a fair amount of work involved in retrieving the values. The fewer doc values reads you make, the faster the export. ",
            "author": "Joel Bernstein",
            "id": "comment-16524017"
        },
        {
            "date": "2018-07-03T06:44:47+0000",
            "content": "Thanks, Varun and Joel. I did some performance tests on export handler for variant sort fields and have attached the reports. It validates; \"The fewer doc values read you make, the faster the export.\" Please see the attached report. Though I am greatly\u00a0inclined towards users to decide what's the benchmark for their application and not limit the functionality from Solr end. ",
            "author": "Amrit Sarkar",
            "id": "comment-16530908"
        },
        {
            "date": "2018-07-03T18:12:49+0000",
            "content": "Amrit - Thanks for your analysis and data. I think this is promising considering the growth in response time with cardinality is pretty stable and growth in response time with growth in number of sort fields is also somewhat linear. This all looks very sustainable for someone to validate and tune per their needs. (tune both this number n=number of export fields and file/disk cache)\n\nWould it be possible to do the same analysis with your patch, with 4,5,6,7,8,9 sort fields ? ",
            "author": "Aroop",
            "id": "comment-16531757"
        },
        {
            "date": "2018-07-03T18:47:22+0000",
            "content": "Is it really necessary to validate this more? The fear was that increasing the number of sort fields would increase the response time in a highly non-linear fashion. That appears not to be true. Would a simple \"increasing the number of sort fields will increase query processing times/costs and should be measured in your environment\" suffice? No matter what we try, YMMV enough to make it wise to test... ",
            "author": "Erick Erickson",
            "id": "comment-16531801"
        },
        {
            "date": "2018-07-04T01:55:26+0000",
            "content": "I think it's fine to move forward with this. As part of the testing I did with the current export handler I did quite a bit of manual testing on very large exports. I wrote the exports to disk and verified the data was exported properly. The trick is working with data sets that have lots of randomness in the different sort fields so sub-sorts get exercised in different patterns.\n\nI think it would make sense that whoever commits this takes the time to do this manual testing as well, because unit tests will have a hard time simulating this type of heavy export. ",
            "author": "Joel Bernstein",
            "id": "comment-16532144"
        },
        {
            "date": "2018-07-04T02:04:26+0000",
            "content": "I hate hard limits on things like this. They don\u2019t age well or take into account the data or use case or hardware. They should be user overridable at the least and otherwise handled with good documentation IMO. ",
            "author": "Mark Miller",
            "id": "comment-16532149"
        },
        {
            "date": "2018-07-04T03:39:34+0000",
            "content": "+1 to everything Mark said.\n\nI think it would make sense that whoever commits this takes the time to do this manual testing as well, because unit tests will have a hard time simulating this type of heavy export.\n\nIs your concern a failure bug or a performance bug?  If the latter, I don't think Varun or whoever needs to aim for perfection as a requirement for this. ",
            "author": "David Smiley",
            "id": "comment-16532202"
        },
        {
            "date": "2018-07-04T04:34:42+0000",
            "content": "Hi Erick -\u00a0I personally would be satisfied at this point (actually would have been a few months ago as well \u00a0) as my experiments posted here earlier had shown linearity already, but the fact the experiments were done again by Amrit made me wonder if we were\u00a0aiming for more thoroughness. But personally I see all my needs as expressed in this Jira to have been addressed. If these configurations could be exposed in solrconfig it would be very useful for users like me.\n\nAmrit - Great work.\n\nJoel - Thanks for reviewing this at length. ",
            "author": "Aroop",
            "id": "comment-16532231"
        },
        {
            "date": "2018-07-13T22:05:14+0000",
            "content": "Patch which builds on top of Amrit's patch\n\nChanges:\n\n\tI've split up the several sub-classes within ExportWriter to their own individual class. It's all contained within one package. Personally I found it easier to read the code after splitting out the classes. It does make the patch a lot bigger though\n\tAdds some extra tests. We weren't testing boolean fields and\u00a0a scenario where\u00a0both docs\u00a0have the same values. This caught a bug in the previous patch where if two docs are the same the higher docId would get selected rather than the lower docId one ( index order )\u00a0\n\tOne optimization around setting queue size i.e if total hits is less than 30k create the queueSize with totalHits\u00a0\n\nint queueSize = 30000;\nif (totalHits < 30000) {\n  queueSize = totalHits;\n}\n\n\u00a0\n\n\n\nToday when creating the priority queue , we do doc-value seeks for all the sort fields. When we stream out docs we again make doc-value seeks against the fl fields . In most common use-cases I'd imagine fl = sort fields , so if we can pre-collect the values while sorting it , we can halve the doc-value seeks potentially bringing us speed improvements. I believe Amrit is already working on this and can be tackled in another Jira\n\n\u00a0\n\nBased on Mark's and David's comments , should we still limit the sort fields to 10 or keep it say 50? I've added this line to the export writer ref guide page with the patch already\n\nThe export performance will get slower as you add more sort fields. If there is enough physical memory available outside of the JVM to load up the sort fields then the performance will be linearly slower with additional of sort fields. It can get worse otherwise.\n\n\u00a0\n\nI'll begin some manual testing on this patch ",
            "author": "Varun Thacker",
            "id": "comment-16543791"
        },
        {
            "date": "2018-07-17T16:15:21+0000",
            "content": "Thanks Varun for re-arranging the classes, the ExportWriter class\u00a0looks much better now.\n\nI am attaching a patch, built on top of latest one uploaded on the Jira with extensive tests for testing 'n' sort fields\u00a0with 1000s of documents for ExportWriter. Also, I made the maximum sort fields to\u00a025,\u00a0please feel free to change as necessary. ",
            "author": "Amrit Sarkar",
            "id": "comment-16546835"
        },
        {
            "date": "2018-07-17T16:30:57+0000",
            "content": "In general, we shouldn't have limits at all on stuff like this.  If the performance degradation and memory use is linear, there is no trap waiting to bite someone (except for the arbitrary limit itself). ",
            "author": "Yonik Seeley",
            "id": "comment-16546851"
        },
        {
            "date": "2018-07-17T20:31:12+0000",
            "content": "Some changes I made on top of the last patch:\n\nDoubleValue#setCurrentValue , the following if statement now throws an asssertion error. LongValue, IntValue and FloatValue were doing the same thing\n\nif (docId < lastDocID) {\nthrow new AssertionError(\"docs were sent out-of-order: lastDocID=\" + lastDocID + \" vs doc=\" + docId);\n}\n\nChanged all the compareTo methods in IntAsc/Desc , LongAsc/Desc , DoubleAsc/Desc , FloatAsc/Desc to use Float.compare/Int.compare etc. We should add tests where we have float values like -0.0 and 0.0\n\nRemoved the 25 sort limit and a test which was trying to assert an error if max sort fields limit has been reached.\n\nPrototyped NewIntValue which could replace IntValue . If this looks cleaner I'll implement it for the others . Thoughts ?\n\nLastly, I was trying to understand why are we using getSlowAtomicReader to get string doc-values. So as an experiment I am passing null to StringValue so that it will get per segment doc-values and not lookup against the getSlowAtomicReader\nThe tests passed with this change. So was that no needed or do our tests not catch this? I want to fix this as part of this patch ",
            "author": "Varun Thacker",
            "id": "comment-16547073"
        },
        {
            "date": "2018-07-17T22:23:47+0000",
            "content": "The tests passed with this change. So was that no needed or do our tests not catch this? I want to fix this as part of this patch\nI was chatting to Hoss offline and he pointed out that to sort we obviously need to global ords. I'm going to leave this for now, but the usage in\u00a0ExportWriter's StringValue#setCurrentValue vs\u00a0ExpandComponent#collect differs slightly.\n\nFYI no failing test is still a concern. I manually added 4 docs\u00a0\n\n[\n{\"id\":\"1\", \"cat_s\" : \"a\"},\n{\"id\":\"2\", \"cat_s\" : \"ABC\"}\n]\n\ncommit \n\n[\n{\"id\":\"3\", \"cat_s\" : \"xyz\"}, \n{\"id\":\"4\", \"cat_s\" : \"a\"} \n]\n\ncommit\n\n\u00a0\n\nThe sort order is wrong . The way I cross checked was against the select handler. Maybe we should do some validation tests against the select handler as well? ",
            "author": "Varun Thacker",
            "id": "comment-16547162"
        },
        {
            "date": "2018-07-18T03:40:43+0000",
            "content": "Took another stab adding some more randomness to tests. I think I'll spend maybe another few hours seeing if\u00a0\n\ntestMultipleSorts can test more edge cases.\n\n\u00a0\n\nOne change made to StringValue:\n\nIn setCurrentValue replaced this section from\n\nint ord = currentVals.ordValue();\nif(globalOrds != null) {\ncurrentOrd = (int)globalOrds.get(ord);\n} else {\ncurrentOrd = ord;\n}\n\nto\u00a0\n\nprotected LongValues toGlobal = LongValues.IDENTITY; // this segment to global ordinal. NN;\n^^ ensuring that for non MultiDocValues.MultiSortedDocValues instances it simmply returns the ord itself\n\n//and then we can just do\n\nif (docId == docValues.docID()) {\ncurrentOrd = (int) toGlobal.get(docValues.ordValue());\n}\n\nCopied this technique from FacetFieldProcessorByHashDV. ExpandComponent also does the same thing roughly ",
            "author": "Varun Thacker",
            "id": "comment-16547338"
        },
        {
            "date": "2018-07-18T19:51:11+0000",
            "content": "I wrote a test that tries validating the export handler vs the select handler ( rows=num_docs )\u00a0\n\nThe assert never passed. Although the results of the export are correct .\n\nThe difference is that for docs with the same value the ordering is different when it comes to export vs select.\u00a0\n\nJoel Bernstein does that sound right to you?\u00a0 ",
            "author": "Varun Thacker",
            "id": "comment-16548329"
        },
        {
            "date": "2018-07-18T21:20:46+0000",
            "content": "That's interesting that the ties are handled differently. I guess there is a different tie breaker or no tie breaker in the export handler? ",
            "author": "Joel Bernstein",
            "id": "comment-16548418"
        },
        {
            "date": "2018-07-18T21:24:04+0000",
            "content": "I believe ties are handled with this condition in SortDoc#lessThan\n\nreturn docId+docBase > sd.docId+sd.docBase;\n\nIn the patch if I've added TestExportWriter#testEqualValue , where this condition is triggered .\u00a0 ",
            "author": "Varun Thacker",
            "id": "comment-16548422"
        },
        {
            "date": "2018-07-18T22:40:35+0000",
            "content": "Hi Joel Bernstein ,\n\nI figured out the issue!\u00a0\n\nIn SortDoc we were never setting docBase . So it was always -1 . Setting this correctly fixes the problem\n\nIf we index 4 documents in this fashion\u00a0\n\nassertU(adoc(\"id\",\"1\", \"stringdv\",\"a\"));\nassertU(adoc(\"id\",\"2\", \"stringdv\",\"a\"));\n\nassertU(commit());\n\nassertU(adoc(\"id\",\"3\", \"stringdv\",\"a\"));\nassertU(adoc(\"id\",\"4\", \"stringdv\",\"a\"));\n\nassertU(commit());\n\nAnd then sort on either\u00a0\"stringdv asc\" or \"stringdv desc\"\u00a0 we will always get docs in\u00a0this order:\u00a01,2,3,4\u00a0\n\nIf you try this on 7.4 the order will be : 1,3,2,4\n\n\u00a0\n\nI'll clean up my local patch and upload it shortly. ",
            "author": "Varun Thacker",
            "id": "comment-16548510"
        },
        {
            "date": "2018-07-18T22:54:04+0000",
            "content": "Yep, I see it too. It looks like the old code is using just the segment level docId as a tie breaker rather then global docId. Good catch! ",
            "author": "Joel Bernstein",
            "id": "comment-16548537"
        },
        {
            "date": "2018-07-19T00:44:51+0000",
            "content": "To recap the changes in the final patch to make it easier for reviewing\u00a0\n\n\tThe inner class have been extracted into their own file. It's all in the export package\n\tThere is no longer a limit on the number of sort fields. Earlier it was 4\n\tOne optimization around setting queue size i.e if total hits is less than 30k create the queueSize with totalHits\u00a0\n\nint queueSize = 30000;\nif (totalHits < 30000) {\nqueueSize = totalHits;\n}\n\n\n\n\n\n\tFor docs that have the same sort value, we respect the index order. This required setting docBase in SortDoc\n\tDoubleValue#setCurrentValue , the following if statement now throws an assertion error. LongValue, IntValue and FloatValue were doing the same thing\n\n\n\n\nif (docId < lastDocID) {\nthrow new AssertionError(\"docs were sent out-of-order: lastDocID=\" + lastDocID + \" vs doc=\" + docId);\n}\n\n\n\n\tSome superficial changes to StringValue which borrows a technique from FacetFieldProcessorByHashDV\n\tMore tests\n\n\n\n\nToday when creating the priority queue , we do doc-value seeks for all the sort fields. When we stream out docs we again make doc-value seeks against the fl fields . In most common use-cases I'd imagine fl = sort fields , so if we can pre-collect the values while sorting it , we can\u00a0reduce the doc-value seeks potentially bringing speed improvements. I believe Amrit is already working on this and can be tackled in another Jira\nWe'll create a separate followup Jira for this\n\nI've run 100 iterations of TestExportWriter and StreamingTest.\n\nTODO:\n\n\tfix precommit. It's currently failing while building the ref guide.\n\n\n\n\u00a0\n\nFeedback welcome!\u00a0\n\n\u00a0 ",
            "author": "Varun Thacker",
            "id": "comment-16548627"
        },
        {
            "date": "2018-07-20T17:41:04+0000",
            "content": "Final patch! Pre-commit passes.\n\nPlan on committing this after running another round of tests ",
            "author": "Varun Thacker",
            "id": "comment-16551036"
        },
        {
            "date": "2018-07-20T19:23:09+0000",
            "content": "Commit 9d9c3a0cd87832980a4745ec96fb2cd1216dcb4e in lucene-solr's branch refs/heads/master from Varun Thacker\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=9d9c3a0 ]\n\nSOLR-11598: Support more than 4 sort fields in the export writer ",
            "author": "ASF subversion and git services",
            "id": "comment-16551161"
        },
        {
            "date": "2018-07-20T19:24:32+0000",
            "content": "Thanks Amrit for the patch and testing,\u00a0 catching a bug which was causing slowdowns from the earlier patches!\u00a0\n\nI'll commit it to branch_7x on Monday\u00a0 ",
            "author": "Varun Thacker",
            "id": "comment-16551163"
        },
        {
            "date": "2018-07-22T05:47:58+0000",
            "content": "Varun Thacker\nI think there is a typo in the class name\nhttps://github.com/apache/lucene-solr/blob/master/solr/core/src/java/org/apache/solr/handler/export/DoubeValue.java\n\nInstead of DoubleValue, the class name is DoubeValue ",
            "author": "Munendra S N",
            "id": "comment-16551907"
        },
        {
            "date": "2018-07-23T14:46:37+0000",
            "content": "Policeman Jenkins found a reproducing seed for a TestExportWriter.testMultipleSorts() failure https://jenkins.thetaphi.de/job/Lucene-Solr-master-Linux/22506/.  git bisect says the first bad commit is 9d9c3a0, committed on this issue:\n\n\nChecking out Revision 6e3f61f6f9b42cb11f45b0eb97587f07e5198ba4 (refs/remotes/origin/master)\n[...]\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestExportWriter -Dtests.method=testMultipleSorts -Dtests.seed=C00F1A5AE8E7D6FE -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=fr-CF -Dtests.timezone=Etc/GMT-3 -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] FAILURE 0.70s J1 | TestExportWriter.testMultipleSorts <<<\n   [junit4]    > Throwable #1: org.junit.ComparisonFailure: Position:0 has different id value expected:<[866]> but was:<[530]>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([C00F1A5AE8E7D6FE:E93DD2B3C1F2CBD8]:0)\n   [junit4]    > \tat org.apache.solr.handler.export.TestExportWriter.validateSort(TestExportWriter.java:644)\n   [junit4]    > \tat org.apache.solr.handler.export.TestExportWriter.testMultipleSorts(TestExportWriter.java:603)\n   [junit4]    > \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n   [junit4]    > \tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n   [junit4]    > \tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n   [junit4]    > \tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n   [junit4]    > \tat java.base/java.lang.Thread.run(Thread.java:844)\n[...]\n   [junit4]   2> NOTE: test params are: codec=Lucene70, sim=Asserting(org.apache.lucene.search.similarities.AssertingSimilarity@105e3812), locale=fr-CF, timezone=Etc/GMT-3\n   [junit4]   2> NOTE: Linux 4.15.0-24-generic amd64/Oracle Corporation 10.0.1 (64-bit)/cpus=8,threads=1,free=191750024,total=459276288\n\n ",
            "author": "Steve Rowe",
            "id": "comment-16552949"
        },
        {
            "date": "2018-07-23T19:22:31+0000",
            "content": "Commit 877bde7347794c93569201040538edfdae057709 in lucene-solr's branch refs/heads/master from Varun Thacker\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=877bde7 ]\n\nSOLR-11598: Fix test case + class name typo ",
            "author": "ASF subversion and git services",
            "id": "comment-16553290"
        },
        {
            "date": "2018-07-23T19:24:10+0000",
            "content": "Thanks Steve for pointing it out! Fixed the test failure and beasted the test 500 times to make sure more failures don't crop up.\u00a0 It was a stupid test bug on my part\n\nMunendra , thanks for spotting that. It's fixed in the latest commit\u00a0 ",
            "author": "Varun Thacker",
            "id": "comment-16553291"
        },
        {
            "date": "2018-07-24T06:02:28+0000",
            "content": "Commit 1c5b38f1808c3cd08e0cf2a920bb193ccfe05e90 in lucene-solr's branch refs/heads/branch_7x from Varun Thacker\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1c5b38f ]\n\nSOLR-11598: Support more than 4 sort fields in the export writer\n\n(cherry picked from commit 9d9c3a0) ",
            "author": "ASF subversion and git services",
            "id": "comment-16553846"
        },
        {
            "date": "2018-07-24T06:02:30+0000",
            "content": "Commit a08eb293ceff2ebea56038335dd375627cee7b45 in lucene-solr's branch refs/heads/branch_7x from Varun Thacker\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a08eb29 ]\n\nSOLR-11598: Fix test case + class name typo\n\n(cherry picked from commit 877bde7) ",
            "author": "ASF subversion and git services",
            "id": "comment-16553847"
        },
        {
            "date": "2018-07-24T18:31:01+0000",
            "content": "Commit 3edd07b20bf637a62d4303c4f7a646e166ac55b6 in lucene-solr's branch refs/heads/branch_7x from Chris Hostetter\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=3edd07b ]\n\nSOLR-11598: fix precommit ",
            "author": "ASF subversion and git services",
            "id": "comment-16554647"
        },
        {
            "date": "2018-07-24T18:37:07+0000",
            "content": "Ughh. Sorry for that Hoss!\u00a0\n\nI had found that when running precommit but looks like I never added that to the commit ",
            "author": "Varun Thacker",
            "id": "comment-16554654"
        },
        {
            "date": "2018-07-24T19:32:03+0000",
            "content": "I found another reproducible test failure while beasting TestExportWriter offline . Looking at the failure now\n\nant test\u00a0 -Dtestcase=TestExportWriter -Dtests.method=testIndexOrder -Dtests.seed=6A9A3B6D803E0772\n\n\u00a0 ",
            "author": "Varun Thacker",
            "id": "comment-16554705"
        },
        {
            "date": "2018-07-24T20:09:10+0000",
            "content": "Commit a254e7d7bcd11c2163dfbb7b3d4a3986aee7867b in lucene-solr's branch refs/heads/master from Varun Thacker\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a254e7d ]\n\nSOLR-11598: Fix test failure due to ordering of sub-tests ",
            "author": "ASF subversion and git services",
            "id": "comment-16554740"
        },
        {
            "date": "2018-07-24T20:09:16+0000",
            "content": "Commit 3bf98da39f36d862745be166a0a59303c4ed1714 in lucene-solr's branch refs/heads/branch_7x from Varun Thacker\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=3bf98da ]\n\nSOLR-11598: Fix test failure due to ordering of sub-tests\n\n(cherry picked from commit dfc8697) ",
            "author": "ASF subversion and git services",
            "id": "comment-16554741"
        },
        {
            "date": "2018-07-25T08:33:54+0000",
            "content": "Commit a254e7d7bcd11c2163dfbb7b3d4a3986aee7867b in lucene-solr's branch refs/heads/jira/http2 from Varun Thacker\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a254e7d ]\n\nSOLR-11598: Fix test failure due to ordering of sub-tests ",
            "author": "ASF subversion and git services",
            "id": "comment-16555368"
        },
        {
            "date": "2018-08-08T20:28:13+0000",
            "content": "Commit e9f3a3ce1d482bd90ba8aca6e8cb7fe6c86756eb in lucene-solr's branch refs/heads/master from Varun Thacker\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=e9f3a3c ]\n\nSOLR-12616: Optimize Export writer upto 4 sort fields to get better performance. This was removed in SOLR-11598 but brought back in the same version ",
            "author": "ASF subversion and git services",
            "id": "comment-16573821"
        },
        {
            "date": "2018-08-08T20:30:26+0000",
            "content": "Commit 13b9e28f9dbb0d117d8758c37d8df7d4c17a9edc in lucene-solr's branch refs/heads/branch_7x from Varun Thacker\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=13b9e28f ]\n\nSOLR-12616: Optimize Export writer upto 4 sort fields to get better performance. This was removed in SOLR-11598 but brought back in the same version\n\n(cherry picked from commit e9f3a3c) ",
            "author": "ASF subversion and git services",
            "id": "comment-16573825"
        },
        {
            "date": "2018-08-10T09:13:49+0000",
            "content": "Commit e9f3a3ce1d482bd90ba8aca6e8cb7fe6c86756eb in lucene-solr's branch refs/heads/jira/http2 from Varun Thacker\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=e9f3a3c ]\n\nSOLR-12616: Optimize Export writer upto 4 sort fields to get better performance. This was removed in SOLR-11598 but brought back in the same version ",
            "author": "ASF subversion and git services",
            "id": "comment-16575996"
        },
        {
            "date": "2018-08-24T00:17:07+0000",
            "content": "Reproducing failure from https://jenkins.thetaphi.de/job/Lucene-Solr-master-Linux/22732/:\n\n\nChecking out Revision 4368ad72d2ccbb40583fa7d2e55464c47e341f8b (refs/remotes/origin/master)\n[...]\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestExportWriter -Dtests.method=testMultipleSorts -Dtests.seed=FBDB2884CBB0889D -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=fi -Dtests.timezone=Antarctica/Troll -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] FAILURE 9.95s J2 | TestExportWriter.testMultipleSorts <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([FBDB2884CBB0889D:D2E9E06DE2A595BB]:0)\n   [junit4]    > \tat org.apache.solr.handler.export.TestExportWriter.validateSort(TestExportWriter.java:697)\n   [junit4]    > \tat org.apache.solr.handler.export.TestExportWriter.testMultipleSorts(TestExportWriter.java:637)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:748)\n[...]\n   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene80): {id=PostingsFormat(name=LuceneVarGapDocFreqInterval), field1_s_dv=FSTOrd50}, docValues:{number_i_p=DocValuesFormat(name=Asserting), field1_i_p=DocValuesFormat(name=Asserting), number_ls_ni_p=DocValuesFormat(name=Asserting), number_ls_t=DocValuesFormat(name=Asserting), number_ds_p=DocValuesFormat(name=Asserting), number_ds_t=DocValuesFormat(name=Asserting), booleandv=DocValuesFormat(name=Asserting), number_i_t=DocValuesFormat(name=Asserting), field2_i_p=DocValuesFormat(name=Lucene70), number_is_ni_p=DocValuesFormat(name=Lucene70), number_is_ni_t=DocValuesFormat(name=Lucene70), longdv=DocValuesFormat(name=Lucene70), number_ls_p=DocValuesFormat(name=Asserting), id=DocValuesFormat(name=Lucene70), field1_s_dv=DocValuesFormat(name=Lucene70), number_dts_p=DocValuesFormat(name=Asserting), stringdv=DocValuesFormat(name=Lucene70), longdv_m=DocValuesFormat(name=Asserting), number_ls_ni_t=DocValuesFormat(name=Asserting), number_is_t=DocValuesFormat(name=Direct), number_dts_t=DocValuesFormat(name=Asserting), doubledv_m=DocValuesFormat(name=Direct), number_is_p=DocValuesFormat(name=Direct), doubledv=DocValuesFormat(name=Lucene70), field4_i_p=DocValuesFormat(name=Direct), number_dts_ni_t=DocValuesFormat(name=Asserting), field3_i_p=DocValuesFormat(name=Lucene70), intdv=DocValuesFormat(name=Direct), floatdv=DocValuesFormat(name=Lucene70), number_fs_p=DocValuesFormat(name=Lucene70), number_f_t=DocValuesFormat(name=Direct), field6_i_p=DocValuesFormat(name=Lucene70), number_l_p=DocValuesFormat(name=Lucene70), number_dt_p=DocValuesFormat(name=Lucene70), number_f_p=DocValuesFormat(name=Direct), number_d_t=DocValuesFormat(name=Lucene70), number_dt_t=DocValuesFormat(name=Lucene70), int_is_t=DocValuesFormat(name=Lucene70), number_dts_ni_p=DocValuesFormat(name=Asserting), number_fs_t=DocValuesFormat(name=Lucene70), datedv_m=DocValuesFormat(name=Asserting), int_is_p=DocValuesFormat(name=Lucene70), number_l_t=DocValuesFormat(name=Lucene70), number_ds_ni_t=DocValuesFormat(name=Lucene70), stringdv_m=DocValuesFormat(name=Direct), intdv_m=DocValuesFormat(name=Lucene70), number_ds_ni_p=DocValuesFormat(name=Lucene70), number_d_ni_p=DocValuesFormat(name=Direct), field5_i_p=DocValuesFormat(name=Asserting), number_fs_ni_p=DocValuesFormat(name=Lucene70), number_d_ni_t=DocValuesFormat(name=Direct), number_fs_ni_t=DocValuesFormat(name=Lucene70), field8_i_p=DocValuesFormat(name=Direct), datedv=DocValuesFormat(name=Asserting), number_f_ni_t=DocValuesFormat(name=Lucene70), floatdv_m=DocValuesFormat(name=Asserting), number_l_ni_t=DocValuesFormat(name=Direct), number_i_ni_p=DocValuesFormat(name=Lucene70), number_dt_ni_p=DocValuesFormat(name=Direct), number_f_ni_p=DocValuesFormat(name=Lucene70), number_i_ni_t=DocValuesFormat(name=Lucene70), number_dt_ni_t=DocValuesFormat(name=Direct), number_d_p=DocValuesFormat(name=Lucene70), field3_l_p=DocValuesFormat(name=Lucene70), field7_i_p=DocValuesFormat(name=Lucene70), number_l_ni_p=DocValuesFormat(name=Direct)}, maxPointsInLeafNode=740, maxMBSortInHeap=7.377656711554688, sim=Asserting(org.apache.lucene.search.similarities.AssertingSimilarity@2e7b4e25), locale=fi, timezone=Antarctica/Troll\n   [junit4]   2> NOTE: Linux 4.15.0-32-generic amd64/Oracle Corporation 1.8.0_172 (64-bit)/cpus=8,threads=1,free=248750568,total=518979584\n\n ",
            "author": "Steve Rowe",
            "id": "comment-16590974"
        },
        {
            "date": "2018-08-24T03:46:11+0000",
            "content": "Thanks Steve!\u00a0Amrit and I are looking into it ",
            "author": "Varun Thacker",
            "id": "comment-16591118"
        },
        {
            "date": "2018-08-24T05:51:03+0000",
            "content": "While trying to debug this test case I ran into a NPE caused from this Jira and was able to isolate it to a new test\u00a0\n\n@Test\npublic void testSortingOnFieldWithNoValues() throws Exception {\n  assertU(delQ(\"*:*\"));\n  assertU(commit());\n\n  assertU(adoc(\"id\",\"1\"));\n  assertU(commit());\n\n  // 10 fields\n  List<String> fieldNames = new ArrayList<>(Arrays.asList(\"floatdv\", \"intdv\", \"stringdv\", \"longdv\", \"doubledv\",\n      \"datedv\", \"booleandv\", \"field1_s_dv\", \"field2_i_p\", \"field3_l_p\"));\n  for (String sortField : fieldNames) {\n    String resp = h.query(req(\"q\", \"*:*\", \"qt\", \"/export\", \"fl\", \"id,\" + sortField, \"sort\", sortField + \" desc\"));\n    assertJsonEquals(resp, \"{\\n\" +\n        \"  \\\"responseHeader\\\":{\\\"status\\\":0},\\n\" +\n        \"  \\\"response\\\":{\\n\" +\n        \"    \\\"numFound\\\":1,\\n\" +\n        \"    \\\"docs\\\":[{\\n\" +\n        \"        \\\"id\\\":\\\"1\\\"}]}}\");\n  }\n\n}\n\n\u00a0 ",
            "author": "Varun Thacker",
            "id": "comment-16591180"
        },
        {
            "date": "2018-08-24T06:55:21+0000",
            "content": "The NPE and the reproducing seed are related. The bug is in StringValue#setNextReader which wasn't setting/resetting docValues correctly. This was introduced in this Jira only.\n\nSo if there are no values the incorrect set would cause an NPE . If there are more than 30k docs to be exported then the docValues was not being reset correctly and hence the doc at which it was failing was 29999.\u00a0\n\nCommitting a fix for this shortly. I'll try expanding the tests more in a separate Jira to catch stuff like this. ",
            "author": "Varun Thacker",
            "id": "comment-16591227"
        },
        {
            "date": "2018-08-24T07:08:45+0000",
            "content": "Commit 9e78be40c338005b75609a3b123778aea822bcf0 in lucene-solr's branch refs/heads/master from Varun Thacker\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=9e78be4 ]\n\nSOLR-11598: Fix bug while setting and resetting string doc-values while exporting documents ",
            "author": "ASF subversion and git services",
            "id": "comment-16591236"
        },
        {
            "date": "2018-08-24T07:16:48+0000",
            "content": "Commit f9dc235243279866ccb1fab15b3d230661cecb18 in lucene-solr's branch refs/heads/branch_7x from Varun Thacker\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=f9dc235 ]\n\nSOLR-11598: Fix bug while setting and resetting string doc-values while exporting documents ",
            "author": "ASF subversion and git services",
            "id": "comment-16591242"
        },
        {
            "date": "2018-08-25T03:09:47+0000",
            "content": "Commit 9e78be40c338005b75609a3b123778aea822bcf0 in lucene-solr's branch refs/heads/jira/http2 from Varun Thacker\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=9e78be4 ]\n\nSOLR-11598: Fix bug while setting and resetting string doc-values while exporting documents ",
            "author": "ASF subversion and git services",
            "id": "comment-16592415"
        }
    ]
}