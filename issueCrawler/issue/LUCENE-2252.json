{
    "id": "LUCENE-2252",
    "title": "stored field retrieve slow",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/store"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "3.0",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "IndexReader.document() on a stored field is rather slow. Did a simple multi-threaded test and profiled it:\n\n40+% time is spent in getting the offset from the index file\n30+% time is spent in reading the count (e.g. number of fields to load)\n\nAlthough I ran it on my lap top where the disk isn't that great, but still seems to be much room in improvement, e.g. load field index file into memory (for a 5M doc index, the extra memory footprint is 20MB, peanuts comparing to other stuff being loaded)\n\nA related note, are there plans to have custom segments as part of flexible indexing feature?",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2010-02-06T20:35:22+0000",
            "content": "John, couldnt you simply write your own Directory if you want to put the fdx in RAM? I am not sure about 'peanuts', some people may not to pay 8 bytes/doc or whatever it is for this stored field offset, when the memory could be used better for other purposes. ",
            "author": "Robert Muir",
            "id": "comment-12830564"
        },
        {
            "date": "2010-02-06T22:16:16+0000",
            "content": "FileSwitchDirectory comes into my mind. Just delegate the *.fdx extension into RAMDirectory. On instantiation of the dir, create the copy during wrapping with FileSwitchDir. ",
            "author": "Uwe Schindler",
            "id": "comment-12830582"
        },
        {
            "date": "2010-02-07T00:33:10+0000",
            "content": "Thanks Uwe for the pointer. Will check that out!\n\nRobert, we can get away with 4 bytes per doc assuming we are not storing 2GB of data per doc. This memory would be less than the data structure needed to be held in memory for only 1 field cache entry for sort. I understand it is always better to use less memory, but sometimes we do have to make trade-off decisions.\nBut you are right, different applications have different needs/requirements, so having support for custom segments would be a good thing. e.g. LUCENE-1914 ",
            "author": "John Wang",
            "id": "comment-12830599"
        },
        {
            "date": "2010-02-07T01:04:10+0000",
            "content": "The thing about stored fields is that it's normally not inner-loop stuff.  The index may be 100M documents, but the average application pages through hits a handful at a time.  And when loading stored fields gets really slow, it tends to be the OS cache misses due to the index being large.  We should still optimize it if we can of course (some apps do access many fields at once), but I agree with Robert that a direct in-memory stored field index probably wouldn't be a good default.\n\nJohn, do you have a specific use case where this is the bottleneck, or are you just looking for places to optimize in general? ",
            "author": "Yonik Seeley",
            "id": "comment-12830603"
        },
        {
            "date": "2010-02-07T02:39:07+0000",
            "content": "Robert, we can get away with 4 bytes per doc assuming we are not storing 2GB of data per doc\n\nI do not understand, I think the fdx index is the raw offset into fdt for some doc, and must remain a long if you have more than 2GB total across all docs. ",
            "author": "Robert Muir",
            "id": "comment-12830615"
        },
        {
            "date": "2010-02-07T03:17:28+0000",
            "content": "I do not understand, I think the fdx index is the raw offset into fdt for some doc, and must remain a long if you have more than 2GB total across all docs.\n\nas stated earlier,  assuming we are not storing 2GB of data per doc, you don't need to keep a long per doc. There are many ways of representing this without paying much performance penalty. Off the top of my head, this would work:\n\nsince positions are always positive, you can indicate using the first bit to see if MAX_INT is reached, if so, add MAX_INT to the masked bits. You get away with int per doc.\n\nI am sure with there are other tons of neat stuff for this the Mikes or Yonik can come up with \n\nJohn, do you have a specific use case where this is the bottleneck, or are you just looking for places to optimize in general?\n\nHi Yonik, I understand this may not be a common use case. I am trying to use Lucene as a store solution. e.g. supporting just get()/put() operations as a content store. We wrote something simple in house and I compared it against lucene, and the difference was dramatic. So after profiling, just seems this is an area with lotsa room for improvement. (posted earlier)\n\nReasons:\n1) Our current setup is that the content is stored outside of the search cluster. It just seems being able to fetch the data for rendering/highlighting within our search cluster would be good.\n2) If the index contains the original data, changing indexing schema, e.g. reindexing can be done within each partition/node. Getting data from our authoratative datastore is expensive.\n\nPerhaps LUCENE-1912 is the right way to go rather than \"fixing\" stored fields. If you also agree, I can just dup it over.\n\nThanks\n\n-John ",
            "author": "John Wang",
            "id": "comment-12830627"
        },
        {
            "date": "2010-02-07T03:18:02+0000",
            "content": "Sorry, I meant LUCENE-1914 ",
            "author": "John Wang",
            "id": "comment-12830628"
        },
        {
            "date": "2010-02-07T03:26:37+0000",
            "content": "as stated earlier, assuming we are not storing 2GB of data per doc, you don't need to keep a long per doc.\n\nright, you stated this, but even if your 'store long into an int' works, I still think 4 bytes/doc is too much (its too much wasted ram for virtually no gain)\n\nI dont understand why you need something like a custom segment file to do this, why cant you just simply use Directory to load this particular file into memory for your use case? ",
            "author": "Robert Muir",
            "id": "comment-12830632"
        },
        {
            "date": "2010-02-07T03:58:06+0000",
            "content": "I still think 4 bytes/doc is too much (its too much wasted ram for virtually no gain)\n\nThat depends on the application. In modern machines (at least with the machines we are using, e.g. a macbook pro) we can afford it  I am not sure I agree with \"virtually no gain\" if you look at the numbers I posted. IMHO, the gain is significant.\n\nI hate to get into a subjective argument on this though.\n\nI dont understand why you need something like a custom segment file to do this, why cant you just simply use Directory to load this particular file into memory for your use case?\n\nHaving a custom segment allows me to not having to get into this subjective argument in what is too much memory or what is the gain, since it just depends on my application, right?\n\nFurthermore, with the question at hand, even if we do use Directory implementation Uwe suggested, it is not optimal. For my use case, the cost of the seek/read for the count on the data file is very wasteful. Also even for getting position, I can just a random access into an array compare to a in-memory seek,read/parse.\n\nThe very simple store mechanism we have written outside of lucene has a gain of >85x, yes, 8500%, over lucene stored fields. We would like to however, take advantage of the some of the good stuff already in lucene, e.g.  merge mechanism (which is very nicely done), delete handling etc. ",
            "author": "John Wang",
            "id": "comment-12830641"
        },
        {
            "date": "2010-02-07T04:13:39+0000",
            "content": "In modern machines (at least with the machines we are using, e.g. a macbook pro)\n\nits not really subjective, or based on modern machines. you are talking about 5M documents, some indexes have a lot more documents and 4bytes/doc in ram adds up to a lot!\nfor the case of using lucene as a search engine library, this memory could be better spent on other things.\nI dont think this is subjective, because its a search engine library, not a document store.\n\nFurthermore, with the question at hand, even if we do use Directory implementation Uwe suggested, it is not optimal.\n\nbut it is easy, and takes away your disk seek. the \"in-memory seek, read/parse\" is as you say, peanuts in comparison. ",
            "author": "Robert Muir",
            "id": "comment-12830642"
        },
        {
            "date": "2010-02-07T10:10:29+0000",
            "content": "The very simple store mechanism we have written outside of lucene has a gain of >85x, yes, 8500%, over lucene stored fields.\n\nJohn can you describe the approach here? ",
            "author": "Michael McCandless",
            "id": "comment-12830689"
        },
        {
            "date": "2010-03-24T00:07:34+0000",
            "content": "Hi Mike:\n\n     Sorry for the late reply. We have written something for this purpose: \n\nhttp://snaprojects.jira.com/wiki/display/KRTI/Krati+Performance+Evaluation\n\nThanks\n\n-John ",
            "author": "John Wang",
            "id": "comment-12848972"
        }
    ]
}