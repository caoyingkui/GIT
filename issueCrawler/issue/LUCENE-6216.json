{
    "id": "LUCENE-6216",
    "title": "Make it easier to modify Japanese token attributes downstream",
    "details": {
        "resolution": "Unresolved",
        "affect_versions": "None",
        "components": [
            "modules/analysis"
        ],
        "labels": "",
        "fix_versions": [],
        "priority": "Minor",
        "status": "Open",
        "type": "Improvement"
    },
    "description": "Japanese-specific token attributes such as PartOfSpeechAttribute, BaseFormAttribute, etc. get their values from a org.apache.lucene.analysis.ja.Token through a setToken() method.  This makes it cumbersome to change these token attributes later on in the analysis chain since the Token instances are difficult to instantiate (sort of read-only objects).\n\nI've ran into this issue in LUCENE-3922 (JapaneseNumberFilter) where it would be appropriate to update token attributes to also reflect Japanese number normalization.\n\nI think it might be more practical to allow setting a specific value for these token attributes directly rather than through a Token since it makes the APIs simpler, allows for easier changing attributes downstream, and also supporting additional dictionaries easier.\n\nThe drawback with the approach that I can think of is a performance hit as we will miss out on the inherent lazy retrieval of these token attributes from the Token object (and the underlying dictionary/buffer).\n\nI'd like to do some testing to better understand the performance impact of this change. Happy to hear your thoughts on this.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14303308",
            "author": "Robert Muir",
            "date": "2015-02-03T14:13:50+0000",
            "content": "I think we can do it without a performance hit. The last time I benchmarked, the current loading was fairly important to e.g. a \"simple\" analyzer, because some attributes like reading are a fair number of bytes per character to process. \n\nits not really lazy loading, but decodes from the dictionary on every single request. So maybe we should just make it lazy loaded?\n\nInstead of:\n\nString getPartOfSpeech() {\n  return token == null ? null : token.getPartOfSpeech();\n}\n\n\n\nadd a setPartOfSpeech() and have the code work something like this, so its just \"caches\" but can be changed:\n\nif (pos == null) {\n  if (token != null) {\n    pos = token.getPartOfSpeech();\n  }\n}\nreturn pos;\n\n\n\nThe disadvantage would be any semantics around 'null', but there are other ways to implement the same idea. "
        },
        {
            "id": "comment-14304342",
            "author": "Christian Moen",
            "date": "2015-02-03T23:57:25+0000",
            "content": "Thanks, Robert.\n\nI had the same idea and I tried this out last night.  The advantage of the approach is that we only read the buffer data for the token attributes we use, but it leaves the API a bit slightly awkward in my opinion since we would have both a setToken() and a setPartOfSpeech().  That said, this is still perhaps the best way to go for performance reasons and these APIs being very low-level and not commonly used.\n\nFor the sake of exploring an alternative idea; a different approach could be to have separate token filters set these attributes.  The tokenizer would set a CharTermAttribute, etc. and a JapaneseTokenAttribute (or something suitably named) that holds the Token.  A separate JapanesePartOfSpeechFilter would be responsible for setting the PartOfSpeechAttribute by getting the data from the JapaneseTokenAttribute using a getToken() method. We'd still need logic similar to the above to deal with setPartOfSpeech(), etc. so I don't think we gain anything by taking this approach, and it's a big change, too. "
        },
        {
            "id": "comment-14304479",
            "author": "Robert Muir",
            "date": "2015-02-04T01:49:30+0000",
            "content": "Yeah for the second idea, i guess my main concern is that it surfaces what should be an implementation detail up to the user. It has some practical challenges too, e.g. today if you have just a simple JapaneseTokenizer-only chain, you can see e.g. POS and so on when debugging. But with the alternate approach, you'd have to modify your analysis chain to see \"everything\".\n\nIt doesn't mean the current approach is the way it should be though: the whole chain could work differently rather than exposing all the attributes. But, if we stay with what we have, we should definitely try to clean this up more. For example, i hate that every Japanese*Attribute has a setToken() method at all. They should be more pojo-like with get/set. The current lazy-loaded/backed-by-token is an \"optimization\" that should somehow only be known by the Tokenizer and the *Impl. And as an optimization, setToken() should still work. "
        }
    ]
}