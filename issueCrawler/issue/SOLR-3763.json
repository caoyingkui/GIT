{
    "id": "SOLR-3763",
    "title": "Make solr use lucene filters directly",
    "details": {
        "affect_versions": "4.0,                                            4.1,                                            6.0",
        "status": "Open",
        "fix_versions": [
            "6.0"
        ],
        "components": [],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "Presently solr uses bitsets, queries and collectors to implement the concept of filters. This has proven to be very powerful, but does come at the cost of introducing a large body of code into solr making it harder to optimise and maintain.\n\nAnother issue here is that filters currently cache sub-optimally given the changes in lucene towards atomic readers.\n\nRather than patch these issues, this is an attempt to rework the filters in solr to leverage the Filter subsystem from lucene as much as possible.\n\nIn good time the aim is to get this to do the following:\n\n\u2218 Handle setting up filter implementations that are able to correctly cache with reference to the AtomicReader that they are caching for rather that for the entire index at large\n\n\u2218 Get the post filters working, I am thinking that this can be done via lucenes chained filter, with the \u201fexpensive\u201d filters being put towards the end of the chain - this has different semantics internally to the original implementation but IMHO should have the same result for end users\n\n\u2218 Learn how to create filters that are potentially more efficient, at present solr basically runs a simple query that gathers a DocSet that relates to the documents that we want filtered; it would be interesting to make use of filter implementations that are in theory faster than query filters (for instance there are filters that are able to query the FieldCache)\n\n\u2218 Learn how to decompose filters so that a complex filter query can be cached (potentially) as its constituent parts; for example the filter below currently needs love, care and feeding to ensure that the filter cache is not unduly stressed\n\n\n  'category:(100) OR category:(200) OR category:(300)'\n\n\n\nReally there is no reason not to express this in a cached form as \n\n\nBooleanFilter(\n    FilterClause(CachedFilter(TermFilter(Term(\"category\", 100))), SHOULD),\n    FilterClause(CachedFilter(TermFilter(Term(\"category\", 200))), SHOULD),\n    FilterClause(CachedFilter(TermFilter(Term(\"category\", 300))), SHOULD)\n  )\n\n\n\nThis would yield better cache usage I think as we can reuse docsets across multiple queries, as well as avoid issues when filters are presented in differing orders\n\n\u2218 Instead of end users providing costing we might (and this is a big might FWIW), be able to create a sort of execution plan of filters, leveraging a combination of what the index is able to tell us as well as sampling and \u201feducated guesswork\u201d; in essence this is what some DBMS software, for example postgresql does - it has a genetic algo that attempts to solve the travelling salesman - to great effect\n\n\u2218 I am sure I will probably come up with other ambitious ideas to plug in here ..... :S \n\nPatches obviously forthcoming but the bulk of the work can be followed here https://github.com/GregBowyer/lucene-solr/commits/solr-uses-lucene-filters",
    "attachments": {
        "SOLR-3763-Make-solr-use-lucene-filters-directly.patch": "https://issues.apache.org/jira/secure/attachment/12542729/SOLR-3763-Make-solr-use-lucene-filters-directly.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Greg Bowyer",
            "id": "comment-13442997",
            "date": "2012-08-28T06:48:07+0000",
            "content": "Initial version, this has some hacks in it and does not pass testing for caches since that needs to be reworked "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13443202",
            "date": "2012-08-28T15:10:03+0000",
            "content": "Interesting work Greg!  A few points:\n\nAnother issue here is that filters currently cache sub-optimally given the changes in lucene towards atomic readers.\n\nThis really depends on the problem - sometimes top-level cache is more optimal, and sometimes per-segment caches are more optimal.  IMO, we shouldn't force either, but add the ability to cache per-segment.\n\nThere are already issues open for caching disjunction clauses separately too - it's a rather orthogonal issue.\n\nIt might be a better idea to start off small: we could make a QParser that creates a CachingWrapperFilter wrapped in a FilteredQuery and hence will cache per-segment.  That should be simple and non-invasive enough to make it into 4.0 "
        },
        {
            "author": "Greg Bowyer",
            "id": "comment-13443409",
            "date": "2012-08-28T19:11:32+0000",
            "content": "I guess my next step is to get caching working, I am not sure quite how to take baby steps with this beyond getting to feature parity. "
        },
        {
            "author": "Greg Bowyer",
            "id": "comment-13647872",
            "date": "2013-05-02T20:39:39+0000",
            "content": "Updated to latest trunk, the cache unit test still fails as does the spatial lat/lon tests "
        },
        {
            "author": "Greg Bowyer",
            "id": "comment-13655676",
            "date": "2013-05-12T23:13:25+0000",
            "content": "Version that has a basic (but hopefully working) cache implementation.\n\nPostFilters are still a bit of an unknown, since these are needed for spacial I will look at how they can be supported "
        },
        {
            "author": "Greg Bowyer",
            "id": "comment-13655715",
            "date": "2013-05-13T02:09:43+0000",
            "content": ".... Trunk moves really quickly these days (or I move slowly)\n\nUpdated patch to cope with recent trunk changes "
        }
    ]
}