{
    "id": "SOLR-857",
    "title": "Memory Leak during the indexing of large xml files",
    "details": {
        "affect_versions": "1.3",
        "status": "Resolved",
        "fix_versions": [],
        "components": [],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Won't Fix"
    },
    "description": "While indexing a set of SOLR xml files that contain 5000 document adds within them and are about 30MB each, SOLR 1.3 seems to continually use more and more memory until the heap is exhausted, while the same files are indexed without issue with SOLR 1.2.\n\nSteps used to reproduce.\n\n1 - Download SOLR 1.3\n2 - Modify example schema.xml to match fields required\n3 - start example server with following command java -Xms512m -Xmx1024m -XX:MaxPermSize=128m -jar start.jar\n4 - Index files as follow java -Xmx128m -jar .../examples/exampledocs/post.jar *.xml\n\nDirectory with xml files contains about 100 xml files each of about 30MB each.  While indexing after about the 25th file SOLR 1.3 runs out of memory, while SOLR 1.2 is able to index the entire set of files without any problems.",
    "attachments": {
        "OQ_SOLR_00001.xml.zip": "https://issues.apache.org/jira/secure/attachment/12393947/OQ_SOLR_00001.xml.zip",
        "Dup_files7.zip": "https://issues.apache.org/jira/secure/attachment/12394571/Dup_files7.zip",
        "Dup_files10.zip": "https://issues.apache.org/jira/secure/attachment/12394575/Dup_files10.zip",
        "Dup_files3.zip": "https://issues.apache.org/jira/secure/attachment/12394566/Dup_files3.zip",
        "Dup_files2.zip": "https://issues.apache.org/jira/secure/attachment/12394565/Dup_files2.zip",
        "schema.xml": "https://issues.apache.org/jira/secure/attachment/12393946/schema.xml",
        "solr.zip": "https://issues.apache.org/jira/secure/attachment/12394398/solr.zip",
        "Dup_files5.zip": "https://issues.apache.org/jira/secure/attachment/12394569/Dup_files5.zip",
        "Dup_files8.zip": "https://issues.apache.org/jira/secure/attachment/12394572/Dup_files8.zip",
        "Dup_files9.zip": "https://issues.apache.org/jira/secure/attachment/12394574/Dup_files9.zip",
        "Dup_files1.zip": "https://issues.apache.org/jira/secure/attachment/12394564/Dup_files1.zip",
        "schema.xml.dup": "https://issues.apache.org/jira/secure/attachment/12394563/schema.xml.dup",
        "leaksuspects.gif": "https://issues.apache.org/jira/secure/attachment/12394979/leaksuspects.gif",
        "Dup_files6.zip": "https://issues.apache.org/jira/secure/attachment/12394570/Dup_files6.zip",
        "leaksuspects2.gif": "https://issues.apache.org/jira/secure/attachment/12394980/leaksuspects2.gif",
        "solr256MBHeap.jpg": "https://issues.apache.org/jira/secure/attachment/12394019/solr256MBHeap.jpg",
        "Dup_files4.zip": "https://issues.apache.org/jira/secure/attachment/12394568/Dup_files4.zip"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Ruben Jimenez",
            "id": "comment-12647656",
            "date": "2008-11-14T16:41:27+0000",
            "content": "This is the schema file that we are using. "
        },
        {
            "author": "Ruben Jimenez",
            "id": "comment-12647664",
            "date": "2008-11-14T17:10:34+0000",
            "content": "This is a sample of the files that we are indexing. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12647676",
            "date": "2008-11-14T17:36:28+0000",
            "content": "What is your ramBufferSizeMB set to?\nWhat happens if you commit periodically?\n\nI suggest you take this so solr-user@lucene first.  This may not be a bug, but simply operations/config issue. "
        },
        {
            "author": "Ruben Jimenez",
            "id": "comment-12647679",
            "date": "2008-11-14T17:54:45+0000",
            "content": "changing the ramBufferSizeMB in solrconfig didn't seem to help.  I tried varying that as well as the mergefactor, but neither seemed to help.\n\nI've tried testing then entire set as well as in batches of 10 files at a time.  In both cases the failure occurred at the same time unless I completely restarted at some point before the 25th file.\n\nI'll send something out to the mailing list as well. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12647683",
            "date": "2008-11-14T18:19:47+0000",
            "content": "Do any of the files fail or have errors? It almost looks like maybe the parser won't be closed on an exception.\n\nI'm looping in copies of your test file and no trouble so far - memory footprint looks decent. Surviving generations is on a constant upward path, so thats a bit worrying, but so far memory looks good. "
        },
        {
            "author": "Bill Au",
            "id": "comment-12647684",
            "date": "2008-11-14T18:24:28+0000",
            "content": "Have you tried starting your JVM with -XX:-HeapDumpOnOutOfMemoryError and then looking at the heap dump? "
        },
        {
            "author": "Ruben Jimenez",
            "id": "comment-12647685",
            "date": "2008-11-14T18:32:11+0000",
            "content": "I've double checked that there are no file errors and have tested with different batches of files with more or less the same results.  Essentially what I see is that memory usage continues to rise during the indexing process until the heap is exhausted.  Unfortunately only one file isn't enough to reproduce the error.  I can provide more but figured I'd hold off on that until asked due to the size of each one. "
        },
        {
            "author": "Ruben Jimenez",
            "id": "comment-12647695",
            "date": "2008-11-14T18:34:05+0000",
            "content": "Bill,\n\nI haven't tried the -XX:-HeapDumpOnOutOfMemoryError yet.  I'll give that a go and post the results. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12647757",
            "date": "2008-11-14T23:44:06+0000",
            "content": "I haven't tried the -XX:-HeapDumpOnOutOfMemoryError yet. I'll give that a go and post the results.\n\nPlease do. I looped trunk on that file 40 some times and got all of my memory back at the end. I'll try again on just 1.3 if taking a look at the heap doesn't help. Netbeans has a great heap inspector and I think IBM has a great one that can be used with much larger dumps. "
        },
        {
            "author": "Ruben Jimenez",
            "id": "comment-12648027",
            "date": "2008-11-16T21:03:22+0000",
            "content": "This is a screenshot of the heap dump summary when the max heap size is set to 256m.  I can't get a heap dump to load into either Netbeans or the IBM heap analyzer for heap values larger than this. "
        },
        {
            "author": "Ruben Jimenez",
            "id": "comment-12648031",
            "date": "2008-11-16T21:15:25+0000",
            "content": "Mark,\n\nI'm trying to reproduce the error with duplicates of the file I uploaded.  I'll provide the parameters used and how long it takes for me to reproduce the error when duplicating this file and allowing for dupes.\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12648037",
            "date": "2008-11-16T22:00:56+0000",
            "content": "Looks like you have some form of IndexReader (or i guess, possibly lower) leak. In my experience your char[] is on the low end compared to the other classes though. Let me know how the single file test goes. If you can repeat it with that, I'll give a shot with the 1.3 release. "
        },
        {
            "author": "Bill Au",
            "id": "comment-12648070",
            "date": "2008-11-17T03:03:51+0000",
            "content": "HashMap$Entry is on top of your list there.  I would look for a large HashMap in the heap dump. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12649086",
            "date": "2008-11-19T16:39:24+0000",
            "content": "Any Luck? "
        },
        {
            "author": "Ruben Jimenez",
            "id": "comment-12649374",
            "date": "2008-11-20T14:12:02+0000",
            "content": "Sorry I've been sick all week and am just getting back on my feet today.\n\nI haven't been able to reproduce the issue by duplicating a single file and modifying it to allowDups.  i've also tried by duplicating a random set of about 3 files.  \n\nBy looking through the heap dump I noticed that the \"$Norms\" memory usage was pretty high, so I modified the schema to have the text field omitNorms.  This helped but simply delayed the problem.\n\nAs of right now we believe that the issue might be related to the fact that we are creating a large number of dynamic fields, hence the large number of FieldInfo instances in memory in the heap dump.  I'll try to see what is the minimum number of files required to reproduce the issue, but I have a feeling it will be on the order of 23 or so.   "
        },
        {
            "author": "Ruben Jimenez",
            "id": "comment-12649587",
            "date": "2008-11-21T03:55:25+0000",
            "content": "I did some poking around and think I pinpointed the source of the memory problems.  I took Bill's advice to look for a large HashMap so I walked through each of them in the heapdump and found that there were 16 rather large HashMaps.  Each of these had a size of over 120,000.  Upon further inspection I also found that these 16 Maps seem to be three distinct Maps.  I came to this conclusion by looking at the first item found in each to group them initially and then confirmed this by choosing two additional random locations per group to verify the each map location contained the same object.  See FieldInfoSample.PNG to see a sample.\n\nThis may not be related but I did a Google Search for lucene fieldinfos leak 2008 and the following came up:  http://mail-archives.apache.org/mod_mbox/lucene-java-commits/200809.mbox/%3C20080914103317.D6AFB2388A0F@eris.apache.org%3E\n\nAssuming I'm unable to find a way to reproduce this error without a rather large number of these files should I just start zipping them and uploading one at a time? "
        },
        {
            "author": "Ruben Jimenez",
            "id": "comment-12650236",
            "date": "2008-11-24T16:03:47+0000",
            "content": "The minimum number of files that seem to be able to reproduce this issue by duplicating them is 20.  Here are the steps to reproduce.\n\n1 - Extract Dup_files1.zip - Dup_files10.zip to a directory and copy each file twice such that you have 60 files in the folder.\n2 - Delete solr data directly completely (C:\\dev\\apache-solr-1.3.0\\example\\solr\\data)\n3 - Overwrite schema.xml in C:\\dev\\apache-solr-1.3.0\\example\\solr\\conf with schema.xml.dup attached here.\n4 - Open up command prompt,  go to folder c:\\dev\\apache-solr-1.3.0\\example and start solr with the following command: \njava -XX:+HeapDumpOnOutOfMemoryError -Xmx256m -XX:MaxPermSize=128m -jar start.jar\n5 - open up command prompt, go to folder from 1 above and execute the following command:\njava -Xmx128m -jar c:\\dev\\apache-solr-1.3.0\\example\\exampledocs\\post.jar *.xml\n\nThe application should run out of memory after about the 26th file. "
        },
        {
            "author": "Ruben Jimenez",
            "id": "comment-12651022",
            "date": "2008-11-26T15:01:40+0000",
            "content": "I seem to have been able to resolve this issue by replacing the lucene core jar file that is packaged with the solr 1.3 release as well as the nightly build (as of 11/25), with the release version of lucene 2.4.  In order to do this I checked out the solr trunk, replaced the jar file in the lib directory and generated an example build using ant.  I was then able to start up this example instance and index the original set of 100 files. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12651034",
            "date": "2008-11-26T15:28:00+0000",
            "content": "Ruben, were there other changes that you made besides replacing the Lucene JAR file? "
        },
        {
            "author": "Ruben Jimenez",
            "id": "comment-12651164",
            "date": "2008-11-26T22:28:00+0000",
            "content": "Ok I do take that back.  When I ran the same exact test as before with the exception of the lucene core update, I still get a failure.  I ran the test against the entire set of files this time with a larger max heap size and with fewer fields being stored.  At this point I'm assuming that the schema changes are allowing the indexing process to proceed further than it did before.  I'll run some tests with larger sets of files to get a sense of what the exact limitation is when using a larger heap size.   "
        },
        {
            "author": "Ruben Jimenez",
            "id": "comment-12651883",
            "date": "2008-11-30T23:59:13+0000",
            "content": "It looks like org.apache.lucene.index.ReadOnlySegmentReader might be the root cause of the memory issues.  At least this is the case according to the MAT heap analysis plugin to eclipse (http://download.eclipse.org/technology/mat/0.7/update-site/).\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12651884",
            "date": "2008-12-01T00:08:09+0000",
            "content": "\n\nYou can only have so many unique fields when norms are on. A norm is loaded into memory for every doc, even if one doc has the field. "
        },
        {
            "author": "Ruben Jimenez",
            "id": "comment-12651890",
            "date": "2008-12-01T00:47:35+0000",
            "content": "Does this mean that I should be setting omitNorms=\"true\" for all of my fields or just the dynamic fields which would be introducing the large number of unique fields? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12651893",
            "date": "2008-12-01T00:59:08+0000",
            "content": "You have to balance it. Generally, you want norms on fields that will have more than a very short bit of text. However, you pay for every field that has norms - and you pay fully even if only one doc has the field. \n\nIf you have dynamic fields that introduce a large number of unique fields, I would start by trying to omit norms on most if not all of those. If they contain very short text, its a no brainer to turn them off. Even with norms, you can usually handle quite a few fields, but there is a practical limit. "
        },
        {
            "author": "Ruben Jimenez",
            "id": "comment-12653789",
            "date": "2008-12-05T14:15:01+0000",
            "content": "So after some testing it seems that the omitNorms makes a big difference.  With my dynamic fields set to omitNorms I'm able to index a batch of 100 files (500,000 documents) without any issues whereas before this failed at about half way through the test batch.  I'm still running into memory problems during indexing, but these problems seem to come about after the index has grown to about 1.5 million documents.  I'm currently looking into simply adding more memory to the server so that I can increase the heap size and considering taking a shard approach to distributing the data across multiple instances.  \n\nI guess at this point we can consider this issue closed and I'll just post questions to the user mailing list as I continue to try and find an optimal configuration for my data.  \n\nThanks for the help. "
        }
    ]
}