{
    "id": "SOLR-10130",
    "title": "Serious performance degradation in Solr 6.4.1 due to the new metrics collection",
    "details": {
        "labels": "",
        "priority": "Blocker",
        "components": [
            "metrics"
        ],
        "type": "Bug",
        "fix_versions": [
            "6.4.2",
            "7.0"
        ],
        "affect_versions": "6.4,                                            6.4.1",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "We've stumbled on serious performance issues after upgrading to Solr 6.4.1. Looks like the new metrics collection system in MetricsDirectoryFactory is causing a major slowdown. This happens with an index configuration that, as far as I can see, has no metrics specific configuration and uses luceneMatchVersion 5.5.0. In practice a moderate load will completely bog down the server with Solr threads constantly using up all CPU (600% on 6 core machine) capacity with a load that normally  where we normally see an average load of < 50%.\n\nI took stack traces (I'll attach them) and noticed that the threads are spending time in com.codahale.metrics.Meter.mark. I tested building Solr 6.4.1 with the metrics collection disabled in MetricsDirectoryFactory getByte and getBytes methods and was unable to reproduce the issue.\n\nAs far as I can see there are several issues:\n1. Collecting metrics on every single byte read is slow.\n2. Having it enabled by default is not a good idea.\n3. The comment \"enable coarse-grained metrics by default\" at https://github.com/apache/lucene-solr/blob/branch_6x/solr/core/src/java/org/apache/solr/update/SolrIndexConfig.java#L104 implies that only coarse-grained metrics should be enabled by default, and this contradicts with collecting metrics on every single byte read.",
    "attachments": {
        "solr-8983-console-f1.log": "https://issues.apache.org/jira/secure/attachment/12852336/solr-8983-console-f1.log",
        "SOLR-10130.patch": "https://issues.apache.org/jira/secure/attachment/12852444/SOLR-10130.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-02-13T16:35:04+0000",
            "content": "I\u2019m seeing similar problems here. With 6.4.0, we were handling 6000 requests/minute. With 6.4.1 it is 1000 rpm with median response times around 2.5 seconds. I also switched to the G1 collector. I\u2019m going to back that out and retest today to see if the performance comes back.\n\nDoes disabling metrics fix it or we we need to go back to 6.4.0?\n\nwunder ",
            "author": "Walter Underwood",
            "id": "comment-15863948"
        },
        {
            "date": "2017-02-13T22:25:04+0000",
            "content": "Does disabling metrics fix it or we we need to go back to 6.4.0?\nUnfortunately no, these metrics are always turned on both in 6.4.0 and in 6.4.1. I'll upload a patch that disables this by default and allows turning it on via a solrconfig.xml knob. ",
            "author": "Andrzej Bialecki",
            "id": "comment-15864585"
        },
        {
            "date": "2017-02-14T00:27:31+0000",
            "content": "This patch turns off Directory and Index metrics by default, and adds config knobs to selectively turn them on in solrconfig.xml (default are all false now, so this section is optional):\n\n<config>\n...\n  <indexConfig>\n    <mergeFactor>...\n    ...\n    <metrics>\n      <bool name=\"directory\">false</bool>\n      <bool name=\"directoryDetails\">false</bool>\n      <bool name=\"merge\">false</bool>\n      <bool name=\"mergeDetails\">false</bool>\n    </metrics>  \n\n ",
            "author": "Andrzej Bialecki",
            "id": "comment-15864751"
        },
        {
            "date": "2017-02-14T09:38:51+0000",
            "content": "I ran some benchmarks, with and without this patch.\n\nBenchmarking suite: https://github.com/chatman/solr-upgrade-tests/blob/master/BENCHMARKS.md\nEnvironment: packet.net, Type 0 server (https://www.packet.net/bare-metal/servers/type-0/)\n\n6.4.1 Without patch\n------------------------\njava -cp target/org.apache.solr.tests.upgradetests-0.0.1-SNAPSHOT-jar-with-dependencies.jar:. org.apache.solr.tests.upgradetests.SimpleBenchmarks -v 72f75b2503fa0aa4f0aff76d439874feb923bb0e -Nodes 1 -Shards 1 -Replicas 1 -numDocs 100000 -threads 6 -benchmarkType generalIndexing\n\nIndexing times: 188,190\n\n6.4.1 With patch\n--------------------\njava -cp target/org.apache.solr.tests.upgradetests-0.0.1-SNAPSHOT-jar-with-dependencies.jar:. org.apache.solr.tests.upgradetests.SimpleBenchmarks -v 72f75b2503fa0aa4f0aff76d439874feb923bb0e -patchUrl https://issues.apache.org/jira/secure/attachment/12852444/SOLR-10130.patch -Nodes 1 -Shards 1 -Replicas 1 -numDocs 100000 -threads 6 -benchmarkType generalIndexing\n\nIndexing times: 171,165\n\n ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15865470"
        },
        {
            "date": "2017-02-14T09:54:10+0000",
            "content": "I would've expected a much larger difference with this patch, if this indeed was the cause of the slowdown - the patch completely turns off metrics collection at directory and index level.\n\nWith 6.4.0, we were handling 6000 requests/minute. With 6.4.1 it is 1000 rpm\nWalter Underwood This is odd, too - the same metrics code is present in both 6.4.1 and 6.4.0, with the same defaults, so I would expect that both versions should show similar performance. Could you please collect some stacktraces (or sample / profile) to verify that you see the same hotspots as Ere Maijala ? ",
            "author": "Andrzej Bialecki",
            "id": "comment-15865492"
        },
        {
            "date": "2017-02-14T10:05:09+0000",
            "content": "6.4.0 shows very similar numbers as compared to 6.4.1\n\n\n6.4.0 Without patch\n--------------------\njava -cp target/org.apache.solr.tests.upgradetests-0.0.1-SNAPSHOT-jar-with-dependencies.jar:. org.apache.solr.tests.upgradetests.SimpleBenchmarks -v 680153de29c5b01d4a8afad88d4a7b84ab01e145 -Nodes 1 -Shards 1 -Replicas 1 -numDocs 100000 -threads 6 -benchmarkType generalIndexing\n\nIndexing times: 191,184\n\n ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15865510"
        },
        {
            "date": "2017-02-14T10:57:53+0000",
            "content": "6.3.0 was faster (same as 6.4.1 with patch).\n\n\njava -cp target/org.apache.solr.tests.upgradetests-0.0.1-SNAPSHOT-jar-with-dependencies.jar:. org.apache.solr.tests.upgradetests.SimpleBenchmarks -v 6fa26fe8553b7b65dee96da741f2c1adf4cb6216 -patchUrl http://147.75.108.131/LUCENE-7651.patch -Nodes 1 -Shards 1 -Replicas 1 -numDocs 100000 -threads 6 -benchmarkType generalIndexing\nIndexing times: 168,167\n\n ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15865584"
        },
        {
            "date": "2017-02-14T15:08:37+0000",
            "content": "I haven't been able to reproduce such drastic slowdown using simple benchmarks - example results from indexing using post tool, fairly representative from several runs on each branch:\n\n* branch_6_3\nreal    4m14.804s\nuser    0m0.883s\nsys     0m2.279s\n\n* branch_6_4\nreal    5m0.987s\nuser    0m0.910s\nsys     0m2.276s\n\n* jira/solr-10130\nreal    4m38.097s\nuser    0m0.881s\nsys     0m2.287s\n\n\nProfiler indeed shows that one of the hotspots on branch_6_4 is the Meter.mark code that is called in org.apache.solr.core.MetricsDirectoryFactory$MetricsInput.readByte. In my test the profiler showed that this consumes ~ 3% CPU, which is indeed something that we should avoid and turn off by default.\n\nHowever, this still doesn't explain the order of magnitude slowdown reported above.\n\nEre Maijala and Walter Underwood - please apply the above patch in your environment and see what is the impact. It makes sense to make this change anyway, so I'm going to apply this or similar version to all affected branches, but maybe there's more we can do here. ",
            "author": "Andrzej Bialecki",
            "id": "comment-15865912"
        },
        {
            "date": "2017-02-14T15:20:37+0000",
            "content": "We've also seen performance degradation with SolrCloud on 6.4.1, as I've posted on solr-user ( http://lucene.472066.n3.nabble.com/Performance-degradation-after-upgrading-from-6-2-1-to-6-4-1-td4320226.html ):\n\nHere are a couple of graphs.  As you can see, 6.4.1 was introduced 2/10 \n12:00: \n\nhttps://www.dropbox.com/s/qrc0wodain50azz/solr1.png?dl=0\nhttps://www.dropbox.com/s/sdk30imm8jlomz2/solr2.png?dl=0\nhttps://www.dropbox.com/s/rgd8bq86i3c5mga/solr2b.png?dl=0\n\nThese are two very different usage scenarios: \n\n\n\tSolr1 has constant updates and very volatile data (30 minutes TTL, 20\nshards with no replicas, across 8 servers).  Requests in the 99 percentile \nwent from ~400ms to 1000-1500ms. (Hystrix cutoff at 1.5s) \n\n\n\n\n\tSolr2 is a more traditional instance with long-lived data (updated once a\nday, 24 shards with 2 replicas, across 8 servers).  Requests in the 99 \npercentile went from ~400ms to at least 1s. (Hystrix cutoff at 1s) \n\n ",
            "author": "Henrik",
            "id": "comment-15865945"
        },
        {
            "date": "2017-02-14T15:42:35+0000",
            "content": "Sorry, the 6000 rpm was with 6.2.1, not 6.4.0.\n\nI've backrev'ed the cluster to 6.3.0 and I'll be running load benchmarks today. ",
            "author": "Walter Underwood",
            "id": "comment-15865980"
        },
        {
            "date": "2017-02-14T16:27:33+0000",
            "content": "Just a matter of how many little IOs are involved in your request.\nI was easily able to reproduce a 5x slowdown with a prefix query that matches many terms. ",
            "author": "Yonik Seeley",
            "id": "comment-15866067"
        },
        {
            "date": "2017-02-14T16:33:50+0000",
            "content": "Thanks Yonik. I'm working on query performance benchmarks for this. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15866077"
        },
        {
            "date": "2017-02-14T16:43:16+0000",
            "content": "I have a JMeter-based load script I can share. It replays access logs. I reload the collection to clear caches, run warming queries, then run queries at a controlled rate. After, it calculates percentiles.\n\nThis was a test of 6.4.1. Really slow. The errors are usually log lines with queries so long that they are truncated and end up with bad syntax. There is one column per request handler, so these results are for /auto, /mobile, /select, and /srp.\n\nMon Feb 13 12:01:29 PST 2017 ; INFO testing solr-cloud.test.cheggnet.com:8983\nMon Feb 13 12:01:29 PST 2017 ; INFO testing with 2000 requests/min\nMon Feb 13 12:01:29 PST 2017 ; INFO testing with 240000 requests\nMon Feb 13 12:01:29 PST 2017 : splitting log into cache warming (first 2000 lines) and benchmark for /home/wunder/2016-12-12-peak-questions-traffic-clean.log\nMon Feb 13 12:01:36 PST 2017 : starting cache warming to solr-cloud.test.cheggnet.com:8983\nMon Feb 13 12:24:29 PST 2017 : starting benchmarking to solr-cloud.test.cheggnet.com:8983\nMon Feb 13 12:24:29 PST 2017 : benchmark should run for 120 minutes\nMon Feb 13 12:24:29 PST 2017 : to get a count of requests sent so far, use \"wc -l out-32688.jtl\"\nMon Feb 13 14:55:01 PST 2017 : WARNING 207 error responses from solr-cloud.test.cheggnet.com\nMon Feb 13 14:55:01 PST 2017 : INFO Removing 207 error responses from JMeter output file before analysis\nMon Feb 13 14:55:01 PST 2017 : analyzing results\n/home/wunder/search-test/load-test\nMon Feb 13 14:55:04 PST 2017 : 25th percentiles are 3151.0,3389.0,9329.0,5647.0\nMon Feb 13 14:55:04 PST 2017 : medians are 6101.0,10579.0,18692.0,8780.0\nMon Feb 13 14:55:04 PST 2017 : 75th percentiles are 6871.0,12499.0,25000.0,12580.0\nMon Feb 13 14:55:04 PST 2017 : 90th percentiles are 7593.0,13481.0,27623.0,14068.0\nMon Feb 13 14:55:04 PST 2017 : 95th percentiles are 8079.0,14039.0,28566.0,16606.0\nMon Feb 13 14:55:04 PST 2017 : full results are in test.csv ",
            "author": "Walter Underwood",
            "id": "comment-15866092"
        },
        {
            "date": "2017-02-14T22:10:43+0000",
            "content": "I could reproduce a 1.6x slowdown for prefix queries.\n\nBenchmarking suite: https://github.com/chatman/solr-upgrade-tests/blob/master/BENCHMARKS.md\nEnvironment: packet.net, Type 0 server (https://www.packet.net/bare-metal/servers/type-0/)\n\nPrefix query times for 6.4.1\n----------------------------\njava -cp target/org.apache.solr.tests.upgradetests-0.0.1-SNAPSHOT-jar-with-dependencies.jar:. org.apache.solr.tests.upgradetests.SimpleBenchmarks -v 72f75b2503fa0aa4f0aff76d439874feb923bb0e -Nodes 1 -Shards 1 -Replicas 1 -numDocs 10000 -threads 4 -benchmarkType generalQuerying\n\nGot results for prefix queries: 10000\nMax time (prefix queries): 2156ms\nTotal time (prefix queries): 1324856ms\n\nPrefix query times for 6.3.0\n----------------------------\njava -cp target/org.apache.solr.tests.upgradetests-0.0.1-SNAPSHOT-jar-with-dependencies.jar:. org.apache.solr.tests.upgradetests.SimpleBenchmarks -v 6fa26fe8553b7b65dee96da741f2c1adf4cb6216 -patchUrl http://147.75.108.131/LUCENE-7651.patch -Nodes 1 -Shards 1 -Replicas 1 -numDocs 10000 -threads 4 -benchmarkType generalQuerying\n\nGot results for prefix queries: 10000\nMax time (prefix queries): 1358ms\nTotal time (prefix queries): 839534ms\n\n\n\nNotes:\n1. The -threads parameter here is for no. of indexing threads, and number of querying threads is 4 times that, i.e. 16 in this case.\n2. Total time is the sum of all times, as reported in the response header's \"QTime\". Max time is the QTime for the worst performing query. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15866806"
        },
        {
            "date": "2017-02-14T23:01:34+0000",
            "content": "\nPrefix query times for 6.4.1 with SOLR-10130 patch\n-----------------------------------------------------------------\njava -cp target/org.apache.solr.tests.upgradetests-0.0.1-SNAPSHOT-jar-with-dependencies.jar:. org.apache.solr.tests.upgradetests.SimpleBenchmarks -v 72f75b2503fa0aa4f0aff76d439874feb923bb0e -patchUrl https://issues.apache.org/jira/secure/attachment/12852444/SOLR-10130.patch -Nodes 1 -Shards 1 -Replicas 1 -numDocs 10000 -threads 4 -benchmarkType generalQuerying\n\nGot results for prefix queries: 10000\nMax time (prefix queries): 1716\nTotal time (prefix queries): 852266\n\n ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15866888"
        },
        {
            "date": "2017-02-15T07:48:40+0000",
            "content": "I don't have proper benchmarks at hand, but I can support others' findings about the serious query performance degradation. I suppose it's easily overlooked when testing with light concurrency, but with enough concurrent queries being served it gets CPU-heavy. We use queries with a lot of filters so that may play a role too. I'll see if I came come up with a reproducible-enough test results from our actual queries. ",
            "author": "Ere Maijala",
            "id": "comment-15867402"
        },
        {
            "date": "2017-02-15T11:07:02+0000",
            "content": "Same issue here. Worked perfectly fine on Solr 6.2.0 and CPU is trashing on Solr 6.4.1. I didn't see this bug report and logged a duplicate - https://issues.apache.org/jira/browse/SOLR-10140 showing slowdown in comparison.\n\nIn our case, Solr 6.4.1 works perfectly fine under production load for about 1 hour and then CPU starts trashing. From the New Relic reports you will see  that Solr 6.4.1 is flaring CPU substantially more than prior versions. ",
            "author": "bidorbuy",
            "id": "comment-15867647"
        },
        {
            "date": "2017-02-15T11:59:03+0000",
            "content": "Patch with some cleanup and CHANGES.txt entry. I'll commit this shortly. ",
            "author": "Andrzej Bialecki",
            "id": "comment-15867701"
        },
        {
            "date": "2017-02-15T12:33:43+0000",
            "content": "Commit a9eb001f44ca846b64d4ed6e46af316fe12ce3d0 in lucene-solr's branch refs/heads/branch_6_4 from Andrzej Bialecki \n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a9eb001 ]\n\nSOLR-10130 Serious performance degradation in Solr 6.4.1 due to the\nnew metrics collection. ",
            "author": "ASF subversion and git services",
            "id": "comment-15867744"
        },
        {
            "date": "2017-02-15T13:19:25+0000",
            "content": "Commit 835c96ba97a01c61978535c0e8fe34708755dc28 in lucene-solr's branch refs/heads/branch_6x from Andrzej Bialecki \n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=835c96b ]\n\nSOLR-10130 Serious performance degradation in Solr 6.4.1 due to the\nnew metrics collection. ",
            "author": "ASF subversion and git services",
            "id": "comment-15867817"
        },
        {
            "date": "2017-02-15T13:56:42+0000",
            "content": "Commit b6f49dc1fb4ad6ef890ae1d09f6d4c0584bb6f64 in lucene-solr's branch refs/heads/master from Andrzej Bialecki \n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b6f49dc ]\n\nSOLR-10130 Serious performance degradation in Solr 6.4.1 due to the\nnew metrics collection. ",
            "author": "ASF subversion and git services",
            "id": "comment-15867867"
        },
        {
            "date": "2017-02-15T13:58:39+0000",
            "content": "Fixed in branch_6_4, branch_6x and master. ",
            "author": "Andrzej Bialecki",
            "id": "comment-15867870"
        },
        {
            "date": "2017-02-15T14:15:46+0000",
            "content": "Andrzej Bialecki  would I have to do a Solr build myself to get the patch in or should I rather wait for 6.4.2 (if so, any indication of when it would be released)? ",
            "author": "bidorbuy",
            "id": "comment-15867900"
        },
        {
            "date": "2017-02-15T14:22:06+0000",
            "content": "You can of course build Solr yourself from branch_6_4 that contains the patch. I don't know of any specific timeline for 6.4.2, but this is a pretty serious issue so I think we should do it soon - let's discuss this on mailing lists. ",
            "author": "Andrzej Bialecki",
            "id": "comment-15867912"
        },
        {
            "date": "2017-02-15T17:38:54+0000",
            "content": "We just deployed the latest from branch_6_4 (a9eb001f44) and our systems are performing normally again.  Thanks for your work on this Andrzej Bialecki ! ",
            "author": "Henrik",
            "id": "comment-15868222"
        },
        {
            "date": "2017-02-16T01:55:56+0000",
            "content": "I don't have hard numbers, but core recovery after a restart with 6.4.0 was taking a really long time. Maybe 30 minutes. Back-reved to 6.3.0, it is maybe five minutes. ",
            "author": "Walter Underwood",
            "id": "comment-15868973"
        },
        {
            "date": "2017-02-16T09:41:15+0000",
            "content": "What was causing the Query time slowdown for prefix queries ?\nHas this been discovered ? ",
            "author": "Alessandro Benedetti",
            "id": "comment-15869625"
        },
        {
            "date": "2017-02-16T10:34:17+0000",
            "content": "Prefix query that matches many terms causes many seek & read ops, which meant that the instrumentation in org.apache.solr.core.MetricsDirectoryFactory$MetricsInput.readByte was called for every small read. This normally wouldn't matter for regular Directory implementations because they use caching extensively, precisely to avoid the overhead of reading single bytes, but MetricsDirectory being a wrapper on top of any Directory implementation couldn't benefit from this caching and still maintain the read/write counters. The overhead of individual Meter.mark call is in the order of microseconds, but invoking it a few million times resulted in significant slowdown.  ",
            "author": "Andrzej Bialecki",
            "id": "comment-15869702"
        },
        {
            "date": "2017-02-16T11:17:17+0000",
            "content": "Thanks, for the clear explanation!\nGood spot ! ",
            "author": "Alessandro Benedetti",
            "id": "comment-15869753"
        },
        {
            "date": "2017-02-16T15:54:36+0000",
            "content": "The slowdown is impressive under heavy query load. Here are two load benchmarks with a 16 node cluster, c4.8xlarge instances (36 CPUs, 60 GB RAM), 15.7 million docs, 4 shards, replication factor 4 using production query logs. These are very long text queries, up to 40 words. Benchmark runs for two or three hours, depending on my patience. Java 8u121, G1 collector.\n\n6.4.0 with 1000 requests/minute is running out of CPU. Median and 95th percentile response times for an ngram/prefix match are 7.5 and 9.8 seconds. For a word match, they are 11 and 25.4 seconds.\n\n6.3.0 with 6000 rpm, the times are 0.4 and 2.7 seconds, and 0.7 and 4.3 seconds, respectively. CPU usage is under 50%.\n\nShort version, 6.4 is 10X slower than 6.3 handling 1/6 the load.  ",
            "author": "Walter Underwood",
            "id": "comment-15870165"
        },
        {
            "date": "2017-02-16T16:14:08+0000",
            "content": "Also, recovery is much, much slower in 6.4. Each core is about 8 GB. After a server process restart, the core is recovering for a few minutes in 6.3, but for about a half hour in 6.4. ",
            "author": "Walter Underwood",
            "id": "comment-15870198"
        },
        {
            "date": "2017-02-16T17:58:04+0000",
            "content": "Walter Underwood: If it's easy, could you try a manual fetchindex? Which you can even do in cloud mode. See: https://cwiki.apache.org/confluence/display/solr/Index+Replication#IndexReplication-HTTPAPICommandsfortheReplicationHandler\n\nOr maybe just see if the logs show that this very long recovery happens when you have a full recovery, i.e. you're copying the full index down from the leader/master...\n\n\n ",
            "author": "Erick Erickson",
            "id": "comment-15870379"
        },
        {
            "date": "2017-02-16T18:12:27+0000",
            "content": "I'm looking at how long the core is marked \"recovering\" in the cloud view of the admin UI.\n\nThere shouldn't be any recovery. The server process is restarted hours after the most recent update. I think this is how long it takes to get the core loaded and ready for search. Startup time, really. ",
            "author": "Walter Underwood",
            "id": "comment-15870429"
        },
        {
            "date": "2017-02-16T18:20:26+0000",
            "content": "Ah, ok. I take it this is a replica with a bunch of data in it? Although this doesn't make sense, there shouldn't be that much work do fire up a core absent tlog replay and the like but it sounds like you're far beyond that so I'm missing something.\n\nIs there any way you could pull/build a new version of Solr 6.4 (or apply the patch on this JIRA locally) and try? I'd hate to have the 6.4.2 release get out (coming soon, due to this) and not have fixed a different issue. ",
            "author": "Erick Erickson",
            "id": "comment-15870444"
        },
        {
            "date": "2017-02-16T21:38:28+0000",
            "content": "Is there any way you could pull/build a new version of Solr 6.4 (or apply the patch on this JIRA locally) and try? I'd hate to have the 6.4.2 release get out (coming soon, due to this) and not have fixed a different issue.\nI concur. Walter Underwood - we are not sure if your situation is caused by the issue fixed here or by some other bug, it would be very helpful if you could try a build that contains this patch to see if it solves the problem in your environment. ",
            "author": "Andrzej Bialecki",
            "id": "comment-15870734"
        },
        {
            "date": "2017-02-16T21:49:37+0000",
            "content": "This might be part of it:\n\n[wunder@new-solr-c01.test3]# ls -lh /solr/data/questions_shard2_replica1/data/tlog/\ntotal 4.7G\nrw-rr- 1 bin bin 4.7G Feb 13 11:04 tlog.0000000000000000000\n[wunder@new-solr-c01.test3]# du -sh /solr/data/questions_shard2_replica1/data/*\n8.4G\t/solr/data/questions_shard2_replica1/data/index\n4.0K\t/solr/data/questions_shard2_replica1/data/snapshot_metadata\n4.7G\t/solr/data/questions_shard2_replica1/data/tlog\n\n\nLast Modified: 3 days ago\nNum Docs: 3683075\nMax Doc: 3683075\nHeap Memory Usage: -1\nDeleted Docs: 0\nVersion: 2737\nSegment Count: 26\nOptimized: yes\nCurrent: yes\n ",
            "author": "Walter Underwood",
            "id": "comment-15870751"
        },
        {
            "date": "2017-02-16T21:58:10+0000",
            "content": "How on earth did you get a 4.7G tlog? It looks like you somehow didn't commit, shut the node down are replaying a ton of docs (well, how much does 4.7G weigh anyway?) from the tlog.\n\nSo, simple test:\n1> wait for the node to come up.\n2> insure you've issued a hard commit\n3> try restarting.\n\nMy claim is that the restart will be reasonable and the slowness you're seeing is a result of somehow shutting down without doing a commit. Of course depending on your autocommit interval you may not need to do the hard commit before restarting...\n\nBest,\nErick ",
            "author": "Erick Erickson",
            "id": "comment-15870760"
        },
        {
            "date": "2017-02-17T12:23:01+0000",
            "content": "I still don't have proper benchmarks, but I've tested enough to say with fair confidence that this is fixed for us. ",
            "author": "Ere Maijala",
            "id": "comment-15871772"
        },
        {
            "date": "2017-02-23T18:47:46+0000",
            "content": "Adding a link to https://issues.apache.org/jira/browse/SOLR-10182 for backing out the changes that caused these perf degradations. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15881006"
        },
        {
            "date": "2017-03-15T13:31:50+0000",
            "content": "The reassignment was accidental, fixed.  I hate the fact that Jira responds with real actions to just typing on the keyboard.  I sometimes forget which window has the focus, assume that an SSH session I can clearly see is active, and find that I'm giving unknown commands to something that accepts keypresses as commands, like Thunderbird or Jira. ",
            "author": "Shawn Heisey",
            "id": "comment-15926179"
        },
        {
            "date": "2017-04-27T15:53:08+0000",
            "content": "Have a question related to this issue.  Somebody on the IRC channel running 6.4.2 is seeing continued performance degradation compared to 4.x.  They were running an earlier 6.4.x release, until they were advised about this issue.\n\nLooking at the utilization for threads, the top threads on 6.4.2 are all named starting with qtp, which I believe means they are Jetty threads.\n\nhttps://gist.github.com/msporleder-work/7313ebedbdab2e178ca0aa2e889d006b\n\nIf I'm not mistaken, we enabled container-level metrics with the changes that went into 6.4.0.  If that's true, do we perhaps have those metrics dialed up to 11? ",
            "author": "Shawn Heisey",
            "id": "comment-15986859"
        },
        {
            "date": "2017-04-27T16:12:09+0000",
            "content": "I don't think this should make much of a difference - InstrumentedQueuedThreadPool only exposes gauges, which basically don't add CPU overhead unless accessed, and InstrumentedHandler collects only a few specific metrics, so the overhead should also be minimal, in the order of microseconds / request.\n\nA drill-down into these threads to find their hot-spots would be useful. ",
            "author": "Andrzej Bialecki",
            "id": "comment-15986913"
        },
        {
            "date": "2017-04-27T17:22:46+0000",
            "content": "Not sure I have the tooling right now for a full drill down, but here are some examples of a thread dump:\n\n\"qtp968514068-37953\" - Thread t@37953\n   java.lang.Thread.State: RUNNABLE\n    at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)\n    at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)\n    at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)\n    at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)\n    - locked <2e101d2c> (a sun.nio.ch.Util$2)\n    - locked <59c1f901> (a java.util.Collections$UnmodifiableSet)\n    - locked <54c6a926> (a sun.nio.ch.EPollSelectorImpl)\n    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)\n    at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)\n    at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:243)\n    at org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:191)\n    at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:249)\n    at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n    at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n    at java.lang.Thread.run(Thread.java:745)\n\n   Locked ownable synchronizers:\n    - None\n\n\"qtp968514068-37952\" - Thread t@37952\n   java.lang.Thread.State: TIMED_WAITING\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for <2d78e562> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)\n    at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)\n    at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:392)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:563)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.access$800(QueuedThreadPool.java:48)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:626)\n    at java.lang.Thread.run(Thread.java:745)\n\n\n\nMost are in that TIMED_WAITING and most CPU time is spend on org.eclipse.jetty.util.BlockingArrayQueue.poll according to visualvm ",
            "author": "Matthew Sporleder",
            "id": "comment-15987079"
        },
        {
            "date": "2017-04-27T17:57:39+0000",
            "content": "Did you change JDK version between these two installs? I found an old issue (but still open!) that may indicate it's a JDK bug: https://github.com/netty/netty/issues/327 . There are other similar reports for Jetty, but for older versions ... can't say whether that's relevant here.\n\nHowever, what these stacktraces do NOT show is anything related to metrics. ",
            "author": "Andrzej Bialecki",
            "id": "comment-15987160"
        },
        {
            "date": "2017-04-27T18:35:25+0000",
            "content": "Both are running java version \"1.8.0_45\" sun jdk ",
            "author": "Matthew Sporleder",
            "id": "comment-15987240"
        },
        {
            "date": "2017-04-27T19:57:34+0000",
            "content": "Metrics was just a theory, sounds like that's not it.  Thanks Andrzej Bialecki  for the assist. ",
            "author": "Shawn Heisey",
            "id": "comment-15987488"
        }
    ]
}