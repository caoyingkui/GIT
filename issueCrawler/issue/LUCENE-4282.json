{
    "id": "LUCENE-4282",
    "title": "Automaton Fuzzy Query doesn't deliver all results",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/search"
        ],
        "type": "Bug",
        "fix_versions": [
            "4.0-BETA",
            "6.0"
        ],
        "affect_versions": "4.0-ALPHA",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Having a small index with n documents where each document has one of the following terms:\nWEBER, WEBE, WEB, WBR, WE, (and some more)\nThe new FuzzyQuery (Automaton) with maxEdits=2 only delivers the expected terms WEBER and WEBE in the rewritten query. The expected terms WEB and WBR which have an edit distance of 2 as well are missing.",
    "attachments": {
        "LUCENE-4282-tests.patch": "https://issues.apache.org/jira/secure/attachment/12538901/LUCENE-4282-tests.patch",
        "LUCENE-4282.patch": "https://issues.apache.org/jira/secure/attachment/12538908/LUCENE-4282.patch",
        "ModifiedFuzzyTermsEnum.java": "https://issues.apache.org/jira/secure/attachment/12538897/ModifiedFuzzyTermsEnum.java"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-08-02T08:50:13+0000",
            "content": "This is caused by the rewrite method not FuzzyQuery itsself. The rewrite mode uses an internal priority queue, where it collects all terms from the index, that match the levensthein distance. If there are more terms available, some are dropped. This depends on their distance and other factors. If you want to use a larger PQ, create a separate instance of the TopTermsScoringBooleanQueryRewrite, giving a queue size. ",
            "author": "Uwe Schindler",
            "id": "comment-13427184"
        },
        {
            "date": "2012-08-02T08:56:27+0000",
            "content": "Thanks for the quick response Uwe.\nI don't think that is the cause. My test index is very small (less than 100 terms), so I don't think the terms get dropped. I thinkt they are missed by the automaton.\nMy rewritten query has only 2 terms: NAME:WEBE^0.5833334 NAME:WEBER ",
            "author": "Johannes Christen",
            "id": "comment-13427185"
        },
        {
            "date": "2012-08-02T08:58:45+0000",
            "content": "What was your query? ",
            "author": "Uwe Schindler",
            "id": "comment-13427187"
        },
        {
            "date": "2012-08-02T09:01:37+0000",
            "content": "Query query = new FuzzyQuery(new Term(\"NAME\", \"WEBER\"),2,1);\n\nHere are all the terms for the field NAME in my index:\nLANGE\nLUETH\nPIRSING\nRIEGEL\nTRZECZIAK\nWALKER\nWBR\nWE\nWEB\nWEBE\nWEBER\nWITTKOPF\nWOJNAROWSKI\nWRICKE ",
            "author": "Johannes Christen",
            "id": "comment-13427188"
        },
        {
            "date": "2012-08-02T09:36:38+0000",
            "content": "There is indeed something strange, I have to wait for Robert to get awake. The following test failes (when added to TestFuzzyQuery.java):\n\n\n  public void test2() throws Exception {\n    Directory directory = newDirectory();\n    RandomIndexWriter writer = new RandomIndexWriter(random(), directory, new MockAnalyzer(random(), MockTokenizer.KEYWORD, false));\n    addDoc(\"LANGE\", writer);\n    addDoc(\"LUETH\", writer);\n    addDoc(\"PIRSING\", writer);\n    addDoc(\"RIEGEL\", writer);\n    addDoc(\"TRZECZIAK\", writer);\n    addDoc(\"WALKER\", writer);\n    addDoc(\"WBR\", writer);\n    addDoc(\"WE\", writer);\n    addDoc(\"WEB\", writer);\n    addDoc(\"WEBE\", writer);\n    addDoc(\"WEBER\", writer);\n    addDoc(\"WITTKOPF\", writer);\n    addDoc(\"WOJNAROWSKI\", writer);\n    addDoc(\"WRICKE\", writer);\n\n    IndexReader reader = writer.getReader();\n    IndexSearcher searcher = newSearcher(reader);\n    writer.close();\n\n    FuzzyQuery query = new FuzzyQuery(new Term(\"field\", \"WEBER\"), 2, 1);\n    ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;\n    assertEquals(4, hits.length);\n\n    reader.close();\n    directory.close();\n}\n\n\n\nThe two missing terms have 2 deletions, so they are in edit distance. ",
            "author": "Uwe Schindler",
            "id": "comment-13427204"
        },
        {
            "date": "2012-08-02T09:55:33+0000",
            "content": "The same happens, if I disable traspositions, so the transposition supporting automatons are not the problem. ",
            "author": "Uwe Schindler",
            "id": "comment-13427219"
        },
        {
            "date": "2012-08-02T10:00:07+0000",
            "content": "Yes I tried this as well. Also the prefix is not the problem.\nI expect the error deep in the automaton. ",
            "author": "Johannes Christen",
            "id": "comment-13427220"
        },
        {
            "date": "2012-08-02T10:06:01+0000",
            "content": "I also added more terms that are in fact longer than WEBER (WEBERE and WEBERES), both are returned, only the shorter ones not. WBRE also works. I dont think the automaton is broken, it may be the FuzzyTermsEnum that does some stuff on top of AutomatonTermsEnum. We have to wait for Robert, he might understand whats going on. ",
            "author": "Uwe Schindler",
            "id": "comment-13427223"
        },
        {
            "date": "2012-08-02T10:14:29+0000",
            "content": "Ok. I keep on digging in the code and come back when I found something. ",
            "author": "Johannes Christen",
            "id": "comment-13427229"
        },
        {
            "date": "2012-08-02T11:57:48+0000",
            "content": "Modification of FuzzyTermEnum class fixing issue LUCENE-4282 ",
            "author": "Johannes Christen",
            "id": "comment-13427253"
        },
        {
            "date": "2012-08-02T11:59:01+0000",
            "content": "Well. I think I found the solution.\nYou were right Uwe. It happens in the FuzzyTermsEnum:AutomatonFuzzyTermsEnum class.\nCalculating the similarity in the accept() method is based on the offset of the smallest length of request term and index term.\n\nI attached my ModifiedFuzzyTermEnum class, where you can find the modification which makes it work.\nBTW. There are some more modifications, fixing bugs in calculating the similarity out of the edit distance and vise versa.\nThe modification of the boost factor was only necessary for my boolean address search approach and possibly doesn't apply here.\nThe modified bits are marked with USERCODE_BEGIN and USERCODE_END tags.\n\n ",
            "author": "Johannes Christen",
            "id": "comment-13427254"
        },
        {
            "date": "2012-08-02T12:12:20+0000",
            "content": "Thanks for help. We are starting to investigate what's wrong!\n\nI did another test in parallel:\n\nquery.setRewriteMethod(FuzzyQuery.SCORING_BOOLEAN_QUERY_REWRITE);\n\n\n\nWith that one it is also failing, so the boost attribute itsself is not the problem. Because this rewrite method does not use it at all (no PriorityQueue).\n\nAlso the Automaton is correct, if you pass the terms to the automaton, they all pass:\n\n\nLevenshteinAutomata builder = new LevenshteinAutomata(\"EBER\", true);\nAutomaton a = builder.toAutomaton(2);\na = BasicOperations.concatenate(BasicAutomata.makeChar('W'), a);\nSystem.out.println(BasicOperations.run(a, \"WBR\"));\nSystem.out.println(BasicOperations.run(a, \"WEB\"));\nSystem.out.println(BasicOperations.run(a, \"WEBE\"));\nSystem.out.println(BasicOperations.run(a, \"WEBER\"));\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-13427262"
        },
        {
            "date": "2012-08-02T12:19:01+0000",
            "content": "thanks for reporting and looking into this!\n\nI think the bug is just the use of floats at all in this enum.\n\n-        if (similarity > minSimilarity) {\n+        if (ed <= maxEdits) {\n           boostAtt.setBoost((similarity - minSimilarity) * scale_factor);\n           //System.out.println(\"  yes\");\n           return AcceptStatus.YES;\n         } else {\n+          System.out.println(\"reject: \" + term.utf8ToString());\n           return AcceptStatus.NO;\n         }\n\n\n\nThis seems to fix it for me. We should remove all float crap from this enum,\nwe dont need it, only a slower deprecated class in the sandbox needs it.\n ",
            "author": "Robert Muir",
            "id": "comment-13427264"
        },
        {
            "date": "2012-08-02T12:26:29+0000",
            "content": "Robert Muir: I added my tests as patch. TestFuzzyQuery is currently not the best test we have: All terms there have equal length, this helps here. I added some more terms (longer ones, too), still the 2 shorter ones fail without a fix.\n\nI am now away, I hope that helps. ",
            "author": "Uwe Schindler",
            "id": "comment-13427267"
        },
        {
            "date": "2012-08-02T12:27:56+0000",
            "content": "Hi Robert. Yes this might be right, but I am still using the similarity float based stuff, since 2 edits on a three letter word is much more difference to me than 2 edits on a 10 letter word.\nIf you apply the stuff I sent, it will work for both cases. ",
            "author": "Johannes Christen",
            "id": "comment-13427269"
        },
        {
            "date": "2012-08-02T12:44:08+0000",
            "content": "Johannes: we will have the same scoring when i say 'removing floats' only less code actually (we can remove this entire if i think).\n\nthe only floats will be what is put into the boost attribute: but no comparisons against floats. the latter is what causes the bug. ",
            "author": "Robert Muir",
            "id": "comment-13427276"
        },
        {
            "date": "2012-08-02T13:55:22+0000",
            "content": "here's a patch, with Uwe's test.\n\nThe float comparison is wasted cpu for FuzzyQuery, as you already know its accepted by the automaton.\n\nBut the deprecated SlowFuzzyQuery in sandbox needs this, because it has crazier logic. So it overrides the logic and does the float comparison. We should really remove that one from trunk since its deprecated since 4.x, it will make it easier to clean this up to be much simpler. ",
            "author": "Robert Muir",
            "id": "comment-13427327"
        },
        {
            "date": "2012-08-02T14:47:47+0000",
            "content": "I will think about this one more: the patch is correct for 'edits' but the scoring\nbecomes crazy. this is because of the historical behavior of this query.\n\nJust try porting Uwe's test to 3.6 and you will see what I mean \n\nI think its too tricky for the query in core (and used by spellchecker) to also\nbe the base for the SlowFuzzyQuery which is supposed to mimic the old crazy behavior.\n ",
            "author": "Robert Muir",
            "id": "comment-13427360"
        },
        {
            "date": "2012-08-02T19:01:03+0000",
            "content": "A simpler patch, i also benchmarked.\n\nThe problem is this comment in the legacy scoring (in all previous lucene versions):\n\n      // this will return less than 0.0 when the edit distance is\n      // greater than the number of characters in the shorter word.\n      // but this was the formula that was previously used in FuzzyTermEnum,\n      // so it has not been changed (even though minimumSimilarity must be\n      // greater than 0.0)\n\n\n\nBecause of that its really impossible to fix until we remove that deprecated one completely \n\nSo i think this one is good to commit, and separately I will look at removing the deprecated one from trunk and cleaning all this up when i have time (I would port the math-proof tests from automata-package to run as queries so we are sure). ",
            "author": "Robert Muir",
            "id": "comment-13427529"
        },
        {
            "date": "2012-08-02T19:54:23+0000",
            "content": "+1 ",
            "author": "Michael McCandless",
            "id": "comment-13427559"
        },
        {
            "date": "2012-08-03T07:07:16+0000",
            "content": "Would that cleanup mean the SlowFuzzyQuery is gone and we can only search fuzzy with maximum 2 edits? I hope not, because I also need the old slow version for greater edit distances. ",
            "author": "Johannes Christen",
            "id": "comment-13427870"
        },
        {
            "date": "2012-08-03T08:09:34+0000",
            "content": "+1 to fix with easy patch for now ",
            "author": "Uwe Schindler",
            "id": "comment-13427900"
        },
        {
            "date": "2012-08-03T13:14:26+0000",
            "content": "Thanks for reporting this! ",
            "author": "Robert Muir",
            "id": "comment-13428064"
        }
    ]
}