{
    "id": "LUCENE-253",
    "title": "Merging indexes leads to out-of-memory condition",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "1.4",
        "resolution": "Cannot Reproduce",
        "status": "Closed"
    },
    "description": "Version: 1.4 Final\nPlatform: Red Hat 9\nMethod: IndexWriter.addIndexes\n\nMy experience with merging indexes of any size results in one outcome: \njava.lang.OutOfMemoryError. This is true even on a 1GB machine where almost all \nfree memory is allocated to the VM.\n\nIs there a way to reliably merge indexes?\n\nI've marked this issue as 'major' for the following reason: Without an \nefficient merge capability, the developer must deploy hundreds of 'little' \nindexes that will need to be searched in parallel. Unfortunately, this \nconstraint plus java makes an application too slow to be usable in a production \nenvironment.\n\nThank you for looking into this issue.\n\n\n>>>>>>>>> sample code\n\nDirectory dest = FSDirectory.getDirectory( destination, true);\nIndexWriter writer = new IndexWriter( dest, new TermAnalyzer( \nStopWords.SEARCH_MAP), true);\n\nwriter.addIndexes( sources);\nwriter.optimize();\n\nwriter.close();",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2004-07-31T14:25:29+0000",
            "content": "You said 'any size'.  Are you saying that if you have 2 indices with 10\ndocuments each, and you try to merge them, you will run out of memory?\n\nHave you tried giving your JVM more memory with -Xmx (e.g. java -Xmx777m)?\nCan you provide a working, self-enclosed code that shows this? ",
            "author": "Otis Gospodnetic",
            "id": "comment-12321788"
        },
        {
            "date": "2004-07-31T23:23:42+0000",
            "content": ">>> You: You said 'any size'.  Are you saying that if you have 2 indices with 10\ndocuments each, and you try to merge them, you will run out of memory?\n\n>>> Me: should read an indexes \"of size.\" I didn't use the words small, medium, \nor large because this is relative. The merge operation fails on what I consider \nto be small indexes. Yes, 2 x 10 works fine. But 20 x 5GB (read small) fails \nevery time. I just had one fail again this morning.\n\n>>> You: Have you tried giving your JVM more memory with -Xmx (e.g. java \n-Xmx777m)?\n\n>>> Me: Yes my message states that I have worked with larger VMs than this. \nSuffice it to say, any developer trying to allocate ever increasing blocks of \nmemory to perform a merge is playing a losing game. Without investigating the \ninternals, one can speculate that memory-based merge operations need to be \nshifted to disk.\n\n>>> You: Can you provide a working, self-enclosed code that shows this? I \nincluded a code sample with my post. \n ",
            "author": "dan",
            "id": "comment-12321789"
        },
        {
            "date": "2004-08-06T19:31:23+0000",
            "content": "We are also merging big indexes and we do not have these memory problems.\n\nI fixed a bug that left a SegmentReader open in addIndexes(IndexReader[] readers)\nand it also left obsolete index files undeleted.\n\nBut this could have hardly caused your memory problems. ",
            "author": "Christoph Goller",
            "id": "comment-12321790"
        },
        {
            "date": "2004-08-06T19:59:03+0000",
            "content": "The call to writer.optimize() isn't necessary, BTW. Also, please try with \nStandardAnaylzer to make sure the problem isn't in your Termanalyzer. If that (and \nChristophs patch) doesn't help, please submit a self-contained testcase (without the \nindex, of course). \n ",
            "author": "Daniel Naber",
            "id": "comment-12321791"
        },
        {
            "date": "2004-08-16T16:32:02+0000",
            "content": "As there's no reply, I close this bug. If the problem still exists, please \nprovide a test case.  ",
            "author": "Daniel Naber",
            "id": "comment-12321792"
        },
        {
            "date": "2005-12-28T22:11:04+0000",
            "content": "To give you an idea how much (or little) memory is needed for indexing in practice: I was able to index 9,999,999 documents without -Xmx.  My documents are about 3-4 KB ascii text. Then, on the 10Mth document, the merge of the ten indexes (a little over 1GB on average) into one, failed with OutOfMemoryError.\n\nGiving it lots of memory doesn't make it any faster on a 1.5.0_05 Sun JVM (one might have expected a very little gain from a less constrained GC), it just gets past large merges. The -Xmx doesn't hurt either; the memory isn't used when it's not needed. ",
            "author": "Kurt De Grave",
            "id": "comment-12361323"
        }
    ]
}