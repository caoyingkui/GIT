{
    "id": "SOLR-3360",
    "title": "Problem with DataImportHandler multi-threaded",
    "details": {
        "affect_versions": "3.6",
        "status": "Closed",
        "fix_versions": [
            "3.6.1"
        ],
        "components": [],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Hi,\n\nIf I use dataimport with 1 thread, I got:\n\n<lst name=\"statusMessages\">\n   <str name=\"Total Requests made to DataSource\">5001</str>\n   <str name=\"Total Rows Fetched\">1000</str>\n   <str name=\"Total Documents Skipped\">0</str>\n   <str name=\"Full Dump Started\">2012-04-16 11:21:57</str>\n   <str name=\"\">Indexing completed. Added/Updated: 1000 documents. Deleted 0 documents.</str>\n   <str name=\"Committed\">2012-04-16 11:23:19</str>\n   <str name=\"Total Documents Processed\">1000</str>\n   <str name=\"Time taken\">0:1:22.390</str>\n</lst>\n\nIf I use datamport with 10 threads, I got:\n\n<lst name=\"statusMessages\">\n   <str name=\"Total Requests made to DataSource\">0</str>\n   <str name=\"Total Rows Fetched\">10000</str>\n   <str name=\"Total Documents Skipped\">0</str>\n   <str name=\"Full Dump Started\">2012-04-16 11:31:43</str>\n   <str name=\"\">Indexing completed. Added/Updated: 10000 documents. Deleted 0 documents.</str>\n   <str name=\"Committed\">2012-04-16 11:41:50</str>\n   <str name=\"Total Documents Processed\">10000</str>\n   <str name=\"Time taken\">0:10:7.586</str>\n</lst>\n\nThe configuration of 10 threads consumed 10 times longer than the configuration with 1 thread.\nI have 1000 records in the database.\nMy db-data-config.xml is shown below:\n\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<dataConfig>\n   <dataSource driver=\"com.microsoft.sqlserver.jdbc.SQLServerDriver\" url=\"jdbc:sqlserver://200.XXX.XXX.XXX:1433;databaseName=test\" user=\"user\" password=\"pass\"/>\n      <document>\n         <entity name=\"indice\" rootEntity=\"true\" threads=\"10\" transformer=\"RegexTransformer,TemplateTransformer\" query=\"select top 1000 i.id_indice, i.a, i.b from indice i where i.status = 'I'\" deltaImportQuery=\"i.id_indice, i.a, i.b from indice i where id_indice in ('${dataimporter.delta.id_indice}')\" deltaQuery=\"select id_indice from indice where status='I' and data_hora_modificacao >= convert(datetime, '${dataimporter.last_index_time}', 120)\" deletedPkQuery=\"select id_indice from indice where status='D' and data_hora_modificacao >= convert(datetime, '${dataimporter.last_index_time}', 120)\">\t\n            <field column=\"id_indice\" name=\"id_indice\" />\n            <field column=\"a\" name=\"a\" />\n            <field column=\"b\" name=\"b\" />\n            <entity name=\"filtro\" transformer=\"RegexTransformer,TemplateTransformer\" query=\"select categoria, sub_categoria from filtro where indice_id_indice = '${indice.id_indice}'\">\n               <field name=\"filtro_categoria\" column=\"categoria\" />\n               <field name=\"filtro_sub_categoria\" column=\"sub_categoria\" />\n               <field name=\"nv_sub_categoria\" column=\"nv_sub_categoria\" template=\"${filtro.categoria}|${filtro.sub_categoria}\" />\n            </entity>\n            <entity name=\"pagina_relacionada\" query=\"select url from pagina_relacionada where indice_id_indice = '${indice.id_indice}'\">\n               <field name=\"pagina_relacionada_url\" column=\"url\" />\n            </entity>\n            <entity name=\"veja_mais\" query=\"select chamada, url from veja_mais where indice_id_indice = '${indice.id_indice}'\">\n               <field name=\"veja_mais_chamada\" column=\"chamada\" />\n               <field name=\"veja_mais_url\" column=\"url\" />\n            </entity>\n            <entity name=\"video\" query=\"select url from video where indice_id_indice = '${indice.id_indice}'\">\n               <field name=\"video_url\" column=\"url\" />\n            </entity>\n            <entity name=\"galeria\" query=\"select url from galeria where indice_id_indice = '${indice.id_indice}'\">\n               <field name=\"galeria_url\" column=\"url\" />\n            </entity>\n         </entity>\n      </document>\n</dataConfig>\n\nThanks.",
    "attachments": {
        "SOLR-3360-test.patch": "https://issues.apache.org/jira/secure/attachment/12523647/SOLR-3360-test.patch",
        "SOLR-3360.patch": "https://issues.apache.org/jira/secure/attachment/12523720/SOLR-3360.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "James Dyer",
            "id": "comment-13254765",
            "date": "2012-04-16T15:37:00+0000",
            "content": "Claudio,\n\nThanks for reporting this.  Was this working prior with 3.5?  (We did some work with the \"threads\" feature in 3.6, so it'd be helpful to know if this is a new bug).  \n\nAlso, can you try it (1) without any transformers and (2) with just the parent entity (take out the sub-entities).  Do you get 10,000 or 1,000 ?  This might help in diagnosing any maybe solving this problem.\n\nFinally, you may want to be aware that 3.6 is the last release that will support the DIH \"threads\" feature.  It simply had too many bugs and was too difficult to maintain to keep it in.  But we did try and fix as many bugs for 3.6 as we could.  Possibly in \"fixing\" what we could, we introduced this as a new problem? "
        },
        {
            "author": "Claudio R",
            "id": "comment-13254855",
            "date": "2012-04-16T18:05:49+0000",
            "content": "Hi James,\n\nAbout the version 3.5.0, I got unstable behavior with 10 threads. In first full-import, I got successful import:\n\n<lst name=\"statusMessages\">\n   <str name=\"Total Requests made to DataSource\">0</str>\n   <str name=\"Total Rows Fetched\">1000</str>\n   <str name=\"Total Documents Skipped\">0</str>\n   <str name=\"Full Dump Started\">2012-04-16 14:12:08</str>\n   <str name=\"\">Indexing completed. Added/Updated: 1000 documents. Deleted 0 documents.</str>\n   <str name=\"Committed\">2012-04-16 14:13:21</str>\n   <str name=\"Optimized\">2012-04-16 14:13:21</str>\n   <str name=\"Total Documents Processed\">1000</str>\n   <str name=\"Time taken \">0:1:12.875</str>\n</lst>\n\nBut, in second, third full-import I got Indexing failed. Rolled back all changes.\n\n<lst name=\"statusMessages\">\n   <str name=\"Time Elapsed\">0:0:6.906</str>\n   <str name=\"Total Requests made to DataSource\">0</str>\n   <str name=\"Total Rows Fetched\">12</str>\n   <str name=\"Total Documents Processed\">11</str>\n   <str name=\"Total Documents Skipped\">0</str>\n   <str name=\"Full Dump Started\">2012-04-16 14:15:38</str>\n   <str name=\"\">Indexing failed. Rolled back all changes.</str>\n   <str name=\"Rolledback\">2012-04-16 14:15:43</str>\n</lst>\n\nAt catalina.out, I got:\n\nSEVERE: Full Import failed:java.lang.RuntimeException: Error in multi-threaded import\n\tat org.apache.solr.handler.dataimport.DocBuilder.doFullDump(DocBuilder.java:265)\n\tat org.apache.solr.handler.dataimport.DocBuilder.execute(DocBuilder.java:187)\n\tat org.apache.solr.handler.dataimport.DataImporter.doFullImport(DataImporter.java:359)\n\tat org.apache.solr.handler.dataimport.DataImporter.runCmd(DataImporter.java:427)\n\tat org.apache.solr.handler.dataimport.DataImporter$1.run(DataImporter.java:408)\nCaused by: org.apache.solr.handler.dataimport.DataImportHandlerException: Unable to execute query: select categoria, sub_categoria from filtro where indice_id_indice = '257346'\n\tat org.apache.solr.handler.dataimport.DataImportHandlerException.wrapAndThrow(DataImportHandlerException.java:72)\n\tat org.apache.solr.handler.dataimport.JdbcDataSource$ResultSetIterator.<init>(JdbcDataSource.java:253)\n\tat org.apache.solr.handler.dataimport.JdbcDataSource.getData(JdbcDataSource.java:210)\n\tat org.apache.solr.handler.dataimport.JdbcDataSource.getData(JdbcDataSource.java:39)\n\tat org.apache.solr.handler.dataimport.SqlEntityProcessor.initQuery(SqlEntityProcessor.java:59)\n\tat org.apache.solr.handler.dataimport.SqlEntityProcessor.nextRow(SqlEntityProcessor.java:73)\n\tat org.apache.solr.handler.dataimport.ThreadedEntityProcessorWrapper.nextRow(ThreadedEntityProcessorWrapper.java:84)\n\tat org.apache.solr.handler.dataimport.DocBuilder$EntityRunner.runAThread(DocBuilder.java:446)\n\tat org.apache.solr.handler.dataimport.DocBuilder$EntityRunner.run(DocBuilder.java:399)\n\tat org.apache.solr.handler.dataimport.DocBuilder$EntityRunner.runAThread(DocBuilder.java:466)\n\tat org.apache.solr.handler.dataimport.DocBuilder$EntityRunner.access$000(DocBuilder.java:353)\n\tat org.apache.solr.handler.dataimport.DocBuilder$EntityRunner$1.run(DocBuilder.java:406)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:619)\nCaused by: com.microsoft.sqlserver.jdbc.SQLServerException: socket closed\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.terminate(SQLServerConnection.java:1368)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.terminate(SQLServerConnection.java:1355)\n\tat com.microsoft.sqlserver.jdbc.TDSChannel.read(IOBuffer.java:1532)\n\tat com.microsoft.sqlserver.jdbc.TDSReader.readPacket(IOBuffer.java:3274)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.startResponse(IOBuffer.java:4433)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.doExecuteStatement(SQLServerStatement.java:784)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement$StmtExecCmd.doExecute(SQLServerStatement.java:685)\n\tat com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:4026)\n\tat com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:1416)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:185)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:160)\n\tat com.microsoft.sqlserver.jdbc.SQLServerStatement.execute(SQLServerStatement.java:658)\n\tat org.apache.solr.handler.dataimport.JdbcDataSource$ResultSetIterator.<init>(JdbcDataSource.java:246)\n\t... 13 more\n\nIn version 3.6.0 I did not get unstable behavior as obtained in version 3.5.0 with 10 threads.\nIn version 3.6.0 I tried without transformers:\n\n<lst name=\"statusMessages\">\n   <str name=\"Total Requests made to DataSource\">0</str>\n   <str name=\"Total Rows Fetched\">10000</str>\n   <str name=\"Total Documents Skipped\">0</str>\n   <str name=\"Full Dump Started\">2012-04-16 14:30:39</str>\n   <str name=\"\">Indexing completed. Added/Updated: 10000 documents. Deleted 0 documents.</str>\n   <str name=\"Committed\">2012-04-16 14:39:45</str>\n   <str name=\"Total Documents Processed\">10000</str>\n   <str name=\"Time taken\">0:9:5.719</str>\n</lst>\n\nIn version 3.6.0, with just parent entity:\n\n<lst name=\"statusMessages\">\n   <str name=\"Total Requests made to DataSource\">0</str>\n   <str name=\"Total Rows Fetched\">10000</str>\n   <str name=\"Total Documents Skipped\">0</str>\n   <str name=\"Full Dump Started\">2012-04-16 14:42:49</str>\n   <str name=\"\">Indexing completed. Added/Updated: 10000 documents. Deleted 0 documents.</str>\n   <str name=\"Committed\">2012-04-16 14:49:05</str>\n   <str name=\"Total Documents Processed\">10000</str>\n   <str name=\"Time taken\">0:6:16.0</str>\n</lst>\n\nIt's weird obtain 10000 documents processed. I only have 1000 records in the database.\nThanks. "
        },
        {
            "author": "Claudio R",
            "id": "comment-13254875",
            "date": "2012-04-16T18:25:53+0000",
            "content": "I ran in version 3.6.0 with 20 threads and got 20000 documents processed:\n\n<lst name=\"statusMessages\">\n   <str name=\"Total Requests made to DataSource\">0</str>\n   <str name=\"Total Rows Fetched\">20000</str>\n   <str name=\"Total Documents Skipped\">0</str>\n   <str name=\"Full Dump Started\">2012-04-16 15:10:22</str>\n   <str name=\"\">Indexing completed. Added/Updated: 20000 documents. Deleted 0 documents.</str>\n   <str name=\"Committed\">2012-04-16 15:24:04</str>\n   <str name=\"Total Documents Processed\">20000</str>\n   <str name=\"Time taken\">0:13:42.110</str>\n</lst> "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13254919",
            "date": "2012-04-16T18:58:01+0000",
            "content": "Claudio,\n\nThank you for providing feedback. I have several considerations for your issue:\n\n\n\tto be honest I didn't pay much attention to these counter when fixing threads, I didn't assert it. So, it might be a bug with counters. But the main subject is your index is it correct? Does it has expected number of docs? Are all master entities were properly connected to the details ones? Pls let us know your observations.\n\n\n\n\n\teven DIH code would be correct, you add too many threads. The reason of adding threads is get high CPU utilization, if you exceeds your IO limits you waste CPU time for contentions. Could you start from 2?\n\n\n\n\n\tI suppose significant time were spend for obtaining JDBC connections, btw how many of them are avalable in parallel? If you are not happy how DIH scales you can check what does it spent time for. Logs with debug level for DIH enabled are appreciated. You also can take sampling by jconsole, or even manually run jstack <JVMPID>\n\n\n\nThanks     "
        },
        {
            "author": "James Dyer",
            "id": "comment-13254940",
            "date": "2012-04-16T19:18:00+0000",
            "content": "I think we need to verify whether or not it is adding the same 1000 documents 10x, or if its just counting each document 10x.  The fact that the successful 10-thread 3.5 run took 1:12 but that same 10-thread run on 3.6 took 14:15 makes me wonder if each thread is actually duplicating the work and not just doing extra counting?\n\nBut then again the successful ONE-thread 3.6 run took 1:12 also... hmm...\n\nProbably we need a unit test that does a simple SQL import with 2 threads and counts how many times SolrWriter#upload got called, then compares it both with the # of docs sent and the # docs reported to the user.  Then we'd know what is actually broken.  It'd be interesting to see what that same test against 3.5 does (if it can be made to run to completion).  Possibly this is broken in 3.5 too (except the counters) but nobody noticed because they always got synchronization problems and gave up?? "
        },
        {
            "author": "Claudio R",
            "id": "comment-13255044",
            "date": "2012-04-16T20:57:47+0000",
            "content": "Hi James Dyer and Mikhail Khludnev,\n\nI added in logging.properties of tomcat the line below:\n\norg.apache.solr.handler.dataimport.JdbcDataSource.level=FINE\n\nAnd ran again in 3.6.0 with 10 threads.\nThe select below was performed 10 times\n\nselect url from video where indice_id_indice = '257933'\n\nThis select of sub-entity should have been executed only one time.\n "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13258851",
            "date": "2012-04-21T13:19:03+0000",
            "content": "James, \nI checked what was committed in SOLR-3011. The problem is that all N+1 cases ( <entity name=\\\"y\\\" query=\\\"select * from y where y.A=${x.id}\\\">\\n\") were dropped off from my patch for TestThreaded.java from 16th Mar. After they were not covered anymore, I suppose that \"halting\" problem (child entities selected again and again) were introduced by fix SOLR-3307 (shame on me I was out of the loop). My plan is bring N+1 cases back in TestThreaded.java, and provide correct fix for SOLR-3307. It's just first feeling. The worst case is SOLR-3307 can conflict with halting problem.    "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13258940",
            "date": "2012-04-21T20:21:38+0000",
            "content": "Ok.\n\nI picked up old test https://raw.github.com/m-khl/solr-patches/solr3011/solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestThreaded.java\n\nfor testNPlusOneTenThreads_FullImport I have \n21-04-2012 14:55:39 org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: \n{deleteByQuery=*:*,add=[3, 2, 4, 2, 1, 1, 2, 2, ... (40 adds)],commit=}\n 0 2086\nwhich is what issue is about. \n\nsingle thread is fine\ntestNPlusOneSingleThread_FullImport()\n21.04.2012 13:08:21 org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: \n{deleteByQuery=*:*,add=[2, 3, 4, 1],commit=}\n 0 1289\n\nso, this test can reproduce the problem but not actually test it, I need to make it more restrictive. \n\n\n "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13258941",
            "date": "2012-04-21T20:30:08+0000",
            "content": "SOLR-3360-test.patch I returned testNPlusOne* methods, and add restriction to retrieve the data only once from the MockDatasource. Then I rolled back changes for EntityProcessorBase in SOLR-3307.\nNow I have testNPlusOne* green (it means that the subj issue is resolved) but all testCached* fail due to \"only once\" datasource restriction (that's sad). \nStay tuned.  "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13259235",
            "date": "2012-04-22T20:01:09+0000",
            "content": "Ok. Fix is attached. But SOLR-3307 is broken by this patch. Work in progress "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13259267",
            "date": "2012-04-22T21:20:37+0000",
            "content": "my final solution is attached SOLR-3360.patch. SOLR-3307 is mostly rolled back, and fixed by slightly different way see change in XPathEntityProcessor.java \nAll tests are green "
        },
        {
            "author": "Claudio R",
            "id": "comment-13259333",
            "date": "2012-04-23T01:27:36+0000",
            "content": "Hi Mikhail and James,\n\nWhen I ran the test with only the root entity (without sub-entities) the problem also occurs. This problem does not appear to be related only to the sub-entities (N+1 case). "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13259388",
            "date": "2012-04-23T05:26:36+0000",
            "content": "Claudio,\n\nIt's not clear what you did. Have you applied SOLR-3360.patch attached?  "
        },
        {
            "author": "Claudio R",
            "id": "comment-13259618",
            "date": "2012-04-23T14:13:05+0000",
            "content": "Hi Mikhail,\n\nI didn\u00b4t apply the SOLR-3360.path. The my test was over version 3.6.0 final\nWhich svn revision should I apply the patch? "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13259843",
            "date": "2012-04-23T19:07:50+0000",
            "content": "Claudio,\npatched sources are https://github.com/m-khl/solr-patches/tree/solr3360\npatched jar is https://github.com/downloads/m-khl/solr-patches/apache-solr-dataimporthandler-3.6.1-SNAPSHOT.jar\nI work with http://svn.apache.org/viewvc/lucene/dev/branches/lucene_solr_3_6/ "
        },
        {
            "author": "Claudio R",
            "id": "comment-13259868",
            "date": "2012-04-23T19:34:37+0000",
            "content": "Hi Mikhail,\n\nI did checkout from: http://svn.apache.org/repos/asf/lucene/dev/tags/lucene_solr_3_6_0, applied the SOLR-3360.patch and compiled the dataimporthandler. The code worked correctly. In the log there is no repetition of queries as before.\n\nI got:\n\n<lst name=\"statusMessages\">\n   <str name=\"Total Requests made to DataSource\">0</str>\n   <str name=\"Total Rows Fetched\">1000</str>\n   <str name=\"Total Documents Skipped\">0</str>\n   <str name=\"Full Dump Started\">2012-04-23 15:56:48</str>\n   <str name=\"\">Indexing completed. Added/Updated: 1000 documents. Deleted 0 documents.</str>\n   <str name=\"Committed\">2012-04-23 15:57:29</str>\n   <str name=\"Total Documents Processed\">1000</str>\n   <str name=\"Time taken\">0:0:41.390</str>\n</lst>\n\nThe time spent decreased. \nBefore, with 1 thread I had obtained 0:1:22.390\nNow, with 10 threads I obtain 0:0:41.390\n\nGreat job Mikhail.\nThank you very much.\nCould you tell me if this will be fix in version 3.6.1?\n "
        },
        {
            "author": "James Dyer",
            "id": "comment-13259954",
            "date": "2012-04-23T21:07:13+0000",
            "content": "Committed to 3.6 branch:  r1329444\n\nThanks Mikhail for staying on top of this. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13260351",
            "date": "2012-04-24T08:25:52+0000",
            "content": "James,\n\nI checked the commit http://svn.apache.org/viewvc?view=revision&revision=1329444 code changes looks ok. But I insist on comitting TestThreaded too. It asserts quite important N+1 cases and Once-Datasource semantic, pls have a look essential but not last test coverage improvements https://github.com/m-khl/solr-patches/commit/0a98a827a2df6373ed7a227a240c822e2c150486#solr/contrib/dataimporthandler/src/test/org/apache/solr/handler/dataimport/TestThreaded.java . Patch already has these changes for TestThreaded.java. Do you like me to raise separate issue for improving test coverage or you want me to polish them somehow? \n "
        },
        {
            "author": "James Dyer",
            "id": "comment-13268400",
            "date": "2012-05-04T14:31:56+0000",
            "content": "Mikhail,\n\nI'm looking over your \"n+1\" test and I don't see how it tests anything that isn't being covered in what I committed.  These three tests...\n\ntestCachedThreadless_FullImport()\ntestCachedSingleThread_FullImport()\ntestCachedThread_FullImport()\n\n...they all randomly select (1) is the parent entity cached?, (2) is the child entity cached (or is it going to do an n+1 select)?, and (3) how many threads (2-10)? (for the third test).\n\nWhat is your version testing that this doesn't cover? "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13268557",
            "date": "2012-05-04T17:33:48+0000",
            "content": "TestThreaded in a patch has the following significant additions:\n\ntestNPlusOneThreadless_FullImport()\ntestNPlusOneSingleThread_FullImport() \ntestNPlusOneTenThreads_FullImport()\n\nthese guys use separate dataconfig called dataConfigNPulsOne which has \" where y.A=${x.id}\" as wells as the topic starter's config. Current tests covers only  where=\\\"xid=x.id\\\" in-mem join scenario.\n\nAlso all data provided for MockDataSource are wrapped by  new Once(parentRow) that enforces the verification of the subject issue - halting or query repeating problem. \n\nRegards "
        },
        {
            "author": "James Dyer",
            "id": "comment-13268593",
            "date": "2012-05-04T18:10:44+0000",
            "content": "Mikhail,\n\nSee \"SOLR-3360-test.patch\".  If you agree this covers everything your version does then I'll commit it shortly. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13268704",
            "date": "2012-05-04T21:01:49+0000",
            "content": "OK. I check it and like your laconic approach. \n\n\tI can suggest to rename Once class to something more easily gettable (I'm sorry about that name).\n\tAnd also I've found that for N+1 case (where y.A=${x.id}\\\">\\n\") CachedSqlEntityProcessor makes no sense i.e. it should be\nprocessor=\\\"\"((childCached && !useWhereParam) ? \"Cached\":\"\")\"SqlEntityProcessor\\\">\\n\". It didn't work for me at least.\nAnd it can be worth to warn user somehow if he configured CachedSqlEntityProcessor for N+1. Separate jira or YAGNI?\n\n "
        },
        {
            "author": "James Dyer",
            "id": "comment-13269675",
            "date": "2012-05-07T14:58:23+0000",
            "content": "Mikhail,\n\nHere is a patch incorporating your last comments.  One problem though the test fails for me with -Dtests.seed=4fe0e50054781418:4b35f2f982289e2f:-668154ae4e16ecdb\n\nThis creates a configuration for the 1-thread test:\n\n<dataConfig>\n<dataSource  type=\"MockDataSource\"/>\n       <document>\n               <entity name=\"x\" threads=\"1\"                 query=\"select * from x\"                  processor=\"SqlEntityProcessor\">\n                       <field column=\"id\" />\n                       <entity name=\"y\" query=\"select * from y\" where=\"xid=x.id\"                          processor=\"SqlEntityProcessor\">\n                               <field column=\"desc\" />\n                       </entity>\n               </entity>\n       </document>\n</dataConfig>\n\nAny ideas? "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13269758",
            "date": "2012-05-07T16:41:00+0000",
            "content": "James,\n\nI'm looking into but how /where=\"xid=x.id\"/ is possible with  processor=\"SqlEntityProcessor\" ? It seems to me that your version of ternary operator is not so restrictive as mine.  "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13269775",
            "date": "2012-05-07T16:59:54+0000",
            "content": "Hi,\n\nApplied a tiny modification:\n\n      + (useWhereParam\n      ? \"                       <entity name=\\\"y\\\" query=\\\"select * from y\\\" where=\\\"xid=x.id\\\" \" \n      : \"                       <entity name=\\\"y\\\" query=\\\"select * from y where y.A=${x.id}\\\" \" \n      )+\"                         processor=\\\"\"+(childCached || useWhereParam ? \"Cached\":\"\")+\"SqlEntityProcessor\\\">\\n\"\n\n\n\nto prevent where=\\\"xid=x.id\\\" with plain SqlEntityProcessor "
        },
        {
            "author": "James Dyer",
            "id": "comment-13269796",
            "date": "2012-05-07T17:28:36+0000",
            "content": "See the latest \"SOLR-3360-test.patch\".  I changed the ternary operator as you suggested.\n\nI still get a failure with this:  -Dtests.seed=-55eeb72d0a16dfec:4e1a59f5738a6b25:4bf3cbf2bd3b659a "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13269925",
            "date": "2012-05-07T19:28:29+0000",
            "content": "James,\n\nI can't reproduce the failure. \nmkhl$ ant test-contrib -Dtests.seed=-55eeb72d0a16dfec:4e1a59f5738a6b25:4bf3cbf2bd3b659a -Dtestcase=TestThreaded\n\njunit report \n\n\n<property name=\"tests.seed\" value=\"-55eeb72d0a16dfec:4e1a59f5738a6b25:4bf3cbf2bd3b659a\" />\n\n  <testcase classname=\"org.apache.solr.handler.dataimport.TestThreaded\" name=\"testCachedThread_FullImport\" time=\"0.965\" />\n  <testcase classname=\"org.apache.solr.handler.dataimport.TestThreaded\" name=\"testCachedThreadless_FullImport\" time=\"0.055\" />\n  <testcase classname=\"org.apache.solr.handler.dataimport.TestThreaded\" name=\"testCachedSingleThread_FullImport\" time=\"0.053\" />\n\n\n\n\n\nFinally I added _Total test which enumerates all test params \nhttps://github.com/m-khl/solr-patches/commit/0532e653a3319247519f90bd8987c84171ac6a56.diff\n\nat core i5 MacBook Pro\n\n:solr mkhl$ ant test-contrib -Dtestcase=TestThreaded\njunit-sequential:\n    [junit] Testsuite: org.apache.solr.handler.dataimport.TestThreaded\n    [junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 3.899 sec\n    [junit] \n\njunit-parallel:\n\n\n </properties>\n  <testcase classname=\"org.apache.solr.handler.dataimport.TestThreaded\" name=\"testCachedThread_FullImport\" time=\"1.042\" />\n  <testcase classname=\"org.apache.solr.handler.dataimport.TestThreaded\" name=\"testCachedThreadless_FullImport\" time=\"0.047\" />\n  <testcase classname=\"org.apache.solr.handler.dataimport.TestThreaded\" name=\"testCachedSingleThread_FullImport\" time=\"0.052\" />\n  <testcase classname=\"org.apache.solr.handler.dataimport.TestThreaded\" name=\"testCachedThread_Total\" time=\"0.898\" />\n  <system-out><![CDATA[]]></system-out>\n\n\n\n\nPlease give me a clue how to reproduce the failure. What do you use IDE or script? Have you clean before test? Pls show me exact command, junit report, log/output, etc "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13420222",
            "date": "2012-07-22T16:05:41+0000",
            "content": "Bulk close for 3.6.1 "
        }
    ]
}