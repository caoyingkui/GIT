{
    "id": "LUCENE-7745",
    "title": "Explore GPU acceleration",
    "details": {
        "labels": "",
        "priority": "Major",
        "resolution": "Unresolved",
        "affect_versions": "None",
        "status": "Open",
        "type": "Improvement",
        "components": [],
        "fix_versions": []
    },
    "description": "There are parts of Lucene that can potentially be speeded up if computations were to be offloaded from CPU to the GPU(s). With commodity GPUs having as high as 12GB of high bandwidth RAM, we might be able to leverage GPUs to speed parts of Lucene (indexing, search).\n\nFirst that comes to mind is spatial filtering, which is traditionally known to be a good candidate for GPU based speedup (esp. when complex polygons are involved). In the past, Mike McCandless has mentioned that \"both initial indexing and merging are CPU/IO intensive, but they are very amenable to soaking up the hardware's concurrency.\"\n\nI'm opening this issue as an exploratory task, suitable for a GSoC project. I volunteer to mentor any GSoC student willing to work on this this summer.",
    "attachments": {
        "TermDisjunctionQuery.java": "https://issues.apache.org/jira/secure/attachment/12949838/TermDisjunctionQuery.java",
        "gpu-benchmarks.png": "https://issues.apache.org/jira/secure/attachment/12929377/gpu-benchmarks.png"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15928714",
            "date": "2017-03-16T19:42:20+0000",
            "content": "Hi I am willing to work on this. ",
            "author": "vikash"
        },
        {
            "id": "comment-15931975",
            "date": "2017-03-19T21:38:57+0000",
            "content": "Here are some ideas on things to start out with:\n\n\tCopy over and index lots of points and corresponding docids to the GPU as an offline, one time operation. Then, given a query point, return top-n nearest indexed points.\n\tCopy over and index lots of points and corresponding docids to the GPU as an offline, one time operation. Then, given a polygon (complex shape), return all points that lie inside the polygon.\n\n\n\nIn both the cases, compare performance against existing Lucene spatial search. One would need to choose the most suitable algorithm for doing these as efficiently as possible. Any GPGPU API can be used for now (OpenCL, CUDA) for initial exploration.\n\nDavid Smiley, Karl Wright, Nicholas Knize, Michael McCandless, given your depth and expertise in this area, do you have any suggestions? Any other area of Lucene that comes to mind which should be easiest to start with, in terms of exploring GPU based parallelization? ",
            "author": "Ishan Chattopadhyaya"
        },
        {
            "id": "comment-15932556",
            "date": "2017-03-20T12:29:09+0000",
            "content": "Another experiment that, I think, is worth trying out:\n\n\tBenchmarking an aggregation over a DocValues field (e.g. using sqrt(), haversine distance etc.), and comparing the corresponding performance when executed on the GPU. This could potentially speed up scoring of results.\n\n\n\nFor reference, Postgresql seems to have experienced speedup in some areas (esp. aggregations over column oriented fields): https://www.slideshare.net/kaigai/gpgpu-accelerates-postgresql ",
            "author": "Ishan Chattopadhyaya"
        },
        {
            "id": "comment-15932568",
            "date": "2017-03-20T12:38:30+0000",
            "content": "I have a question to us all.  (a) could whatever comes of this actually be contributed to Lucene itself given the likelihood of requiring native O.S. bindings (lets presume in spatial-extras as it seems this is the only module that can have an external dependency), and (b) does that matter for GSOC or to the expectations of the contributor?  If (a) is a \"no\", we need to be honest up front with the contributor.  I know in the past Solr has been denied off-heap filters that would have required a un-pure Java approach.  A native binding would be another degree of un-purity  ",
            "author": "David Smiley"
        },
        {
            "id": "comment-15932589",
            "date": "2017-03-20T12:54:06+0000",
            "content": "Maybe even the basic hit scoring that e.g. BooleanScorer does with disjunction of high frequency terms, would be amenable to GPU acceleration?  Today BooleanScorer processes a whole window of hits at once, doing fairly simple math (the Similarity methods) on each. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-15932604",
            "date": "2017-03-20T13:07:46+0000",
            "content": "David Smiley, that is a very important question. Afaik, there is no Apache compatible GPGPU framework. Both OpenCL and CUDA are likely incompatible with Apache (I am not fully sure). I see that jCUDA is MIT license, which is a wrapper around the native libraries.\n\nIf there are benefits to using GPGPU processing, my thought is that we can ensure all necessary plumbing in our codebase in order to offload processing to some plugin, whereby the user can plugin the exact GPU kernels from outside the Lucene distribution (if those kernels also violate any licensing restrictions we have). If there are clear benefits in speeding things up using a GPU, it would not be, for the end-user, the end of the world if the code comes outside Apache distribution.\n\nIf (a) is a \"no\", we need to be honest up front with the contributor.\nThat is a good point, and we can document this clearly. ",
            "author": "Ishan Chattopadhyaya"
        },
        {
            "id": "comment-15932714",
            "date": "2017-03-20T14:23:20+0000",
            "content": "Hi,\nin General, including CUDA into Lucene may be a good idea, but I see no real possibility to do this inside Lucene Core or any other module. My idea would be to add some abstraction to the relevant parts of Lucene and make it easier to \"plug in\" different implementations. Then this code could also be hosted outside Lucene (if Licenses is a problem) anywhere on Github.\n\nWe still should have the following in our head: Mike's example looks \"simple\" as a quick test if we see gains, but making the whole thing ready for commit or bundling in any project in/outside Lucene is a whole different story. Currently BooleanScorer calls a lot of classes, e.g. the BM25 similarity or TF-IDF to do the calculation that could possibly be parallelized. But for moving all this to CUDA, you have to add \"plugin points\" all there and change APIs completely. It is also hard to test, because none of our Jenkins servers has a GPU! Also for 0/8/15 users of Lucene, this could be a huge problem, if we add native stuff into Lucene that they may never use. Because of that it MUST BE SEPARATED from Lucene core. Completely...\n\nIMHO, I'd create a full new search engine like CLucene in C code if I would solely focus on GPU parallelization. The current iterator based approaches are not easy to transform or plug into CUDA...\n\nFor the GSoc project, we should make sure to the GSoc student that this is just a project to \"explore\" GPU acceleration: if it brings any performance - I doubt that, because the call overhead between Java and CUDA is way too high - in contrast to Postgres where all in plain C/C++. The results would then be used to plan and investigate ways how to include that into Lucene as \"plugin points\" (e.g., as SPI modules). ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-15944884",
            "date": "2017-03-28T09:47:11+0000",
            "content": "Hi all, I have been reading about GPU acceleration and in particular to be precise about GPU accelerated computing I find this project very interesting and so can anyone give me further lead what is to be done now? I mean the ideas that Ishaan suggested are pretty good but I am still not able to understand that what Mr David means by  (a) could whatever comes of this actually be contributed to Lucene itself, whydo you think that you doubt that the outcome of this project not be contributed to Lucene. ",
            "author": "vikash"
        },
        {
            "id": "comment-15945065",
            "date": "2017-03-28T12:39:13+0000",
            "content": "vikash: not all working code contributed to any open-source project is necessarily welcome.  Usually it is but sometimes project members or ASF rules insist on certain things for the perceived greater good.  In this case, I believe Uwe doesn't want Lucene to include anything that would only work with certain hardware or JVM vendors \u2013 even if it was optional opt-in.  If hypothetically nobody had such concerns here, be aware that any 3rd party (non-ASF) libraries need to meet certain qualifications.  For example, if whatever Java CUDA library you find happens to be licensed as GPL, then it's incompatible with ASF run projects like this one.  That's a hypothetical; I have no idea what Java CUDA libraries exist and what their licenses are.  Regardless... if you come up with something useful, it's probably not necessary that Lucene itself change, and as seen here we have some willingness to change Lucene (details TBD) if it enables people to use Lucene with CUDA.  Lucene has many extension points already.  Though I could imagine you might unfortunately need to copy/fork some long source files \u2013 Uwe mentioned some.\n\nGood luck. ",
            "author": "David Smiley"
        },
        {
            "id": "comment-15945632",
            "date": "2017-03-28T17:54:43+0000",
            "content": "Hi Vikash,\n\nRegarding licensing issue:\nThe work done in this project would be exploratory. That code won't necessarily go into Lucene. When we are at a point where we see clear benefits from the work done here, we would then have to explore all aspects of productionizing it (including licensing).\n\nRegarding next steps:\n\nBooleanScorer calls a lot of classes, e.g. the BM25 similarity or TF-IDF to do the calculation that could possibly be parallelized.\n\n\tFirst, understand how BooleanScorer calls these similarity classes and does the scoring. There are unit tests in Lucene that can help you get there. This might help: https://wiki.apache.org/lucene-java/HowToContribute\n\tWrite a standalone CUDA/OpenCL project that does the same processing on the GPU.\n\tBenchmark the speed of doing so on GPU vs. speed observed when doing the same through the BooleanScorer. Preferably, on a large resultset. Include time for copying results and scores in and out of the device memory from/to the main memory.\n\tOptimize step 2, if possible.\n\n\n\nOnce this is achieved (which in itself could be a sufficient GSoC project), one can have stretch goals to try out other parts of Lucene to optimize (e.g. spatial search).\n\nAnother stretch goal, if the results for optimizations are positive, could be to integrate the solution into Lucene. Most suitable way to do so would be to create hooks into Lucene so that plugins can be built to delegate parts of the processing to external code. And then, write a plugin (that uses jCuda, for example) and do an integration test. ",
            "author": "Ishan Chattopadhyaya"
        },
        {
            "id": "comment-15945637",
            "date": "2017-03-28T18:01:21+0000",
            "content": "Java CUDA libraries exist and what their licenses\njCuda happens to be MIT, which is, afaik, compatible with Apache license.\nhttp://www.jcuda.org/License.txt ",
            "author": "Ishan Chattopadhyaya"
        },
        {
            "id": "comment-15952375",
            "date": "2017-04-01T20:03:42+0000",
            "content": "Hello all, \nI have been reading a lot about GPU working and GPU parallelization in particularly about General Purpose computing on Graphics Processing Units and have also looked into in detail the source code of the BooleanScorer.java file , its a nice thing and I am having no difficulty understanding its working since Java is my speciality so the job was quite fun . There are a few things that seem unclear to me but I am reading and experimenting so I will resolve them soon.  It is a nice idea to use gpu to perform the search and indexing operations on a document using the GPU and that would be faster using the GPU. And regarding the licencing issue, since we are generating code and as it was said above the code that we generate may not go to Lucene necessarily so assuming this happens then will licencing still be an issue if we use the libraries in our code? And as Uwe Schindler  said we may host the code on github and certainly it would not be a good idea to develop code for special hardware, but still we can give it a try and try to make it compatible with most of the hardwares. I dont mind if this code does not go to Lucene, but we can try to change lucene and make it better and I am preparing myself for it and things would stay on track with your kind mentorship .\nSo should I submit my proposal now or do I need to complete all the four steps that Ishaan told to do in the last comment and then submit my proposal? And which one of the ideas would you prefer to mentor me on that is which one do you think would be a better one to continue with? \n\n>Copy over and index lots of points and corresponding docids to the GPU as an offline, one time operation. Then, given a query point, return top-n nearest indexed points.\n>Copy over and index lots of points and corresponding docids to the GPU as an offline, one time operation. Then, given a polygon (complex shape), return all points that lie inside the polygon.\n>Benchmarking an aggregation over a DocValues field  and comparing the corresponding performance when executed on the GPU. \n>Benchmarking the speed of calculations on GPU vs. speed observed when doing the same through the BooleanScorer. Preferably, on a large result set with the time for copying results and scores in and out of the device memory from/to the main memory included?\n-Vikash ",
            "author": "vikash"
        },
        {
            "id": "comment-15952388",
            "date": "2017-04-01T20:29:41+0000",
            "content": "Hi Vikash,\nI suggest you read the student manuals for GSoC.\nSubmit a proposal how you want to approach this project, including technical details (as much as possible) and detailed timelines.\n\nRegarding the following:\n\n\n1    First, understand how BooleanScorer calls these similarity classes and does the scoring. There are unit tests in Lucene that can help you get there. This might help: https://wiki.apache.org/lucene-java/HowToContribute\n2    Write a standalone CUDA/OpenCL project that does the same processing on the GPU.\n3    Benchmark the speed of doing so on GPU vs. speed observed when doing the same through the BooleanScorer. Preferably, on a large resultset. Include time for copying results and scores in and out of the device memory from/to the main memory.\n 4   Optimize step 2, if possible.\n\n\n\nIf you've already understood step 1, feel free to make a proposal on how you will use your GSoC coding time to achieve steps 2-4. Also, you can look at other stretch goals to be included in the coding time. I would consider that steps 2-4, if done properly and successfully, is itself a good GSoC contribution. And if these steps are done properly, then either Lucene integration can be proposed for the latter part of the coding phase (last 2-3 weeks, I'd think), or exploratory work on other part of Lucene (apart from the BooleanScorer, e.g. spatial search filtering etc.) could be taken up. \n\nTime is running out, so kindly submit a proposal as soon as possible. You can submit a draft first, have one of us review it and then submit it as final after the review. If the deadline is too close, there might not be enough time for this round of review, and in such a case just submit the draft as final.\n\nAlso, remember a lot of the GPGPU coding is done on C, so familiarity/experience with that is a plus.\n\n(Just a suggestion that makes sense to me, and feel free to ignore: bullet points work better than long paragraphs, even though the length of sentences can remain the same) ",
            "author": "Ishan Chattopadhyaya"
        },
        {
            "id": "comment-15952838",
            "date": "2017-04-02T19:54:03+0000",
            "content": "Yeah I had already read the student manual and the deadline is 3rd April and its too close, in the preparation I had almost missed the deadline for application. OK so for the proposal my draft is here (you may comment on it and I will do the needful)\n\nhttps://docs.google.com/document/d/1HGxU1ZudNdAboj0s9WKTWJk3anbZm86JY-abaflXoEI/edit?usp=sharing . ",
            "author": "vikash"
        },
        {
            "id": "comment-15952849",
            "date": "2017-04-02T20:35:47+0000",
            "content": "I have left initial comments on your draft. Let me know if you want another round of review, perhaps after you've addressed the current comments. ",
            "author": "Ishan Chattopadhyaya"
        },
        {
            "id": "comment-15953288",
            "date": "2017-04-03T11:11:25+0000",
            "content": "Hi Ishaan ,\nI have changed the proposal according to your instructions, can you review it again? ",
            "author": "vikash"
        },
        {
            "id": "comment-15953521",
            "date": "2017-04-03T14:00:33+0000",
            "content": "Hi Vikash,\nI have reviewed the proposal. It is still extremely disorganized and it is not clear what your goals are and how you have split them up into tasks. It contains lots of copy paste of comments/statements from this JIRA or comments from the proposal itself. The level of details still seems inadequate to me.\n\nI had proposed a possible way to structure your proposal (by splitting the three months into three different areas of focus, all of them I specified in the comments), but I don't see that you've done so. I asked you to find out, at least, what the default Similarity in Lucene is called (and to attempt to simulate the scoring for that on the GPU). It seems you have not done so.\n\nAt this point, I don't think much can be done (just 2 hours to go for submission). Wish you the best. ",
            "author": "Ishan Chattopadhyaya"
        },
        {
            "id": "comment-15953579",
            "date": "2017-04-03T14:35:53+0000",
            "content": "It is my First GSOC and so it was a bit difficult for me to draft the proposal properly. ",
            "author": "vikash"
        },
        {
            "id": "comment-15953673",
            "date": "2017-04-03T15:33:42+0000",
            "content": "If you still haven't submitted your proposal, I have an idea for you to improve your chances.\nInclude a link to a github repository in the application for your initial experiments.\nAfter that, you can try to build a prototype in the next few days (until assessment starts) that demonstrates that you are on the right track. This is not strictly necessary, but just throwing out an idea that might benefit you.\n\nAll the best and regards! ",
            "author": "Ishan Chattopadhyaya"
        },
        {
            "id": "comment-15953766",
            "date": "2017-04-03T16:41:54+0000",
            "content": "oops i could not do that, i submitted my proposal, and if you check it now the latest edited format is the submitted version I made some changes to it again before submitting it and sadly i could not change the github link, it only points to my home directory in github,  but can I start working still now and I shall give you the link that has my working and if it would be possible for you , you could show to the Apache Software Foundation my works, will that be ok? \nAnd since as I have said in my proposal that I will work from april itself so I will do some working and so will the repository I build for lucene and work I store there be checked by ASF by visiting my profile and navigating to the lucene repository i create there? can that help me increase my chances?\nAnd by whom will my proposal be checked? ",
            "author": "vikash"
        },
        {
            "id": "comment-16524890",
            "date": "2018-06-27T10:38:05+0000",
            "content": "Here [0] are some very initial experiments that I ran, along with Kishore Angani, a colleague at Unbxd.\n\n1. Generic problem: Given a result set (of document hits) and a scoring function, return a sorted list of documents along with the computed scores (which may leverage one or more indexed fields).\n2. Specific problem: Given (up to 11M) points and associated docids, compute the distance from a given query point. Return the sorted list of documents based on these distances.\n3. GPU implementation based on Thrust library (C++ based Apache 2.0 licensed library), called from JNI wrapper. Timings include copying data (scores and sorted docids) back from GPU to host system and access from Java (via DirectByteBuffer).\n4. CPU implementation was based on SpatialExample [1], which is perhaps not the fastest (points fields are better, I think).\n5. Hardware: CPU is i7 5820k 4.3GHz (OC), 32GB RAM @ 2133MHz. GPU is Nvidia GTX 1080, 11GB GDDR5 memory.\n\nResults seem promising. The GPU is able to score 11M documents in ~50ms!. Here, blue is GPU and red is CPU (Lucene). \n\n\n\n\n[0] - https://github.com/chatman/gpu-benchmarks\n[1] - https://github.com/apache/lucene-solr/blob/master/lucene/spatial-extras/src/test/org/apache/lucene/spatial/SpatialExample.java ",
            "author": "Ishan Chattopadhyaya"
        },
        {
            "id": "comment-16526467",
            "date": "2018-06-28T16:06:22+0000",
            "content": "David, I'm not sure this was meant to be specific to lucene/spatial, Mark only mentioned it as a way to conduct an initial benchmark? The main thing that we identified as being a potential candidate for integration with Cuda is actually BooleanScorer (BS1, the one that does scoring in bulk) based on previous comments? ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16526486",
            "date": "2018-06-28T16:25:12+0000",
            "content": "Mark who?  You must mean Ishan?  I think that if GPUs are used to accelerate different things, then they would get separate issues and not be lumped under one issue.  Does that sound reasonable?  Granted the problem posted started off as a bit of an umbrella ticket and perhaps the particular proposal Ishan is presenting in his most recent comment ought to go in a new issue specific to spatial.    Accelerating Haversine calculations sounds way different to me than BooleanScorer stuff; no? ",
            "author": "David Smiley"
        },
        {
            "id": "comment-16526506",
            "date": "2018-06-28T16:40:52+0000",
            "content": "Not sure why I confused names, I meant Ishan indeed. Sorry for that. I'll let Ishan decide how he wants to manage this issue, I'm personally fine either way, I'm mostly following.  It just caught me by surprise since I was under the impression that we were still exploring which areas might benefit from GPU acceleration. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16526510",
            "date": "2018-06-28T16:43:57+0000",
            "content": "np.  Oh this caught me by surprise too!  I though this was about BooleanScorer or postings or something and then low and behold it's spatial \u2013 and then I thought this is so non-obvious by the issue title.  So I thought it'd do a little JIRA gardening. ",
            "author": "David Smiley"
        },
        {
            "id": "comment-16527334",
            "date": "2018-06-29T08:50:37+0000",
            "content": "Ah, I think I wasn't clear on my intentions behind those numbers.\n\u00a0\nif it brings any performance - I doubt that, because the call overhead between Java and CUDA is way too high - in contrast to Postgres where all in plain C/C++\nI wanted to start with those experiments just to prove to myself that there are no significant overheads or bottlenecks (as we've feared in the past) and that there can be clear benefits to be realized.\n\nI wanted to try bulk scoring, and chose the distance calculation and sorting as an example because (1) it leverages two fields, (2) it was fairly isolated & easy to try out.\n\nIn practical usecases of spatial search, the spatial filtering doesn't require score calculation & sorting on  the entire dataset (just those documents that are in the vicinity of the user point, filtered down by the geohash or bkd tree node); so in some sense I was trying out an absolute worst case of Lucene spatial search. \n\nNow, that I'm convinced that this overall approach works and overheads are low, I can now move on to looking at Lucene internals, maybe starting with scoring in general (BooleanScorer, for example). Other parts of Lucene/Solr that might see benefit could be streaming expressions (since they seem computation heavy), LTR re-ranking etc.\n\n\nActually incorporating all these benefits into Lucene would require considerable effort, and we can open subsequent JIRAs once we've had a chance to explore them separately. Till then, I'm inclined to keep this issue as a kitchen sink for all-things-GPU, if that makes sense? ",
            "author": "Ishan Chattopadhyaya"
        },
        {
            "id": "comment-16701855",
            "date": "2018-11-28T13:10:08+0000",
            "content": "Hi everyone,\n\nI wanted to check if this issue was still open.\u00a0 I have been experimenting with CUDA for a bit and would love to take a stab at this.\n\nA few thoughts:\n\n\tThis is something I'll do over weekends and so I'm going to be horribly slow (its going to be just me on this unless you have someone working on it and I can collaborate with them) - would that be OK?\n\tI think the right thing to do would be to build a CUDA library (C/C++), put JNI and then integrate it into Lucene.\u00a0 If done right then I think this library will be useful to (and be possible to integrate with) other Analytic tools.\n\tIf I get it right, then I'd love to create an OS library that other OS tools can integrate and use (Yes, I'm thinking of an OpenCL port in the future but given the tools available in CUDA and my familiarity with it...)\n\tLicensing is not an issue as I prefer the Apache License.\n\tTesting (especially scalability testing) will be an issue - like you said, your setups won't have GPUs but would it be possible to rent a few GPU instances on the cloud (AWS, Google)?\u00a0 I can do my dev testing locally as I have a GPU (its a\u00a0 pretty old and obsolete one but good enough for my needs) on my dev machine.\n\tIt is important to get a few users who will experiment with this.\u00a0 Can you guys help in having someone deploy, experiment and give feedback?\n\tI would rather take something that is used by everyone and I'm thinking that indexing, filtering and searching is something that I would rather take up: http://lucene.apache.org/core/7_5_0/demo/overview-summary.html#overview.description\n\t\n\t\tThese can certainly be accelerated.\u00a0 I think I should be able to get some acceleration out of a GPU enabled search.\n\t\tThe good part of this is one would able to scale volumes almost linearly on a multi-GPU machine.\n\t\tRelated to the previous point (though this is in the future). I don't have a multi-GPU setup and will not be able to develop multi-GPU versions. I'll need help in getting the infrastructure to do that. We can talk about that once a single GPU version is done.\n\t\tYes I agree that it will be better to have a separate library / classes doing this rather than directly integrating it into Lucene's class library.\u00a0 This suits me too as I can develop this as a separate library that other OS components can integrate and I can package this as part of nVidia's OS libraries.\n\t\n\t\n\tI'm open to other alternatives - I scanned the ideas above but didn't consider them as they would not bring massive value to the users and I don't really want to experiment as I know what I'm doing.\n\tRelated to the previous point, I don't know Lucene (Help!! - do I really need to?) and will need support/hand-holding in terms of reviewing the identification/interfacing/design/code etc., etc.,\n\tFinally, this IS GOING TO take time because thinking (and programming) massively parallel is completely different from writing a simple sequential search and sort.\u00a0 How much time, think 7-10x at least given all my constraints.\n\n\n\nIf you guys like, I can write a brief (one or two paras) description of what is possible for indexing, searching, filtering (with zero knowledge of Lucene of course) to start off...\n\nYour thoughts please... ",
            "author": "Rinka Singh"
        },
        {
            "id": "comment-16701879",
            "date": "2018-11-28T13:33:21+0000",
            "content": "(Unrelated to your comment Rinka, but seeing activity on this issue reminded me that I wanted to share something) There are limited use-cases for GPU accelelation in Lucene due to the fact that query processing is full of branches, especially since we added support for impacts and WAND. That said Mike initially mentioned that BooleanScorer might be one scorer that could benefit from GPU acceleration as it scores large blocks of documents at once. I just attached a specialization of a disjunction over term queries that should make it easy to experiment with Cuda, see the TODO in the end on top of the computeScores method. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16701941",
            "date": "2018-11-28T14:16:33+0000",
            "content": "Hi Rinka, Kishore and I made some progress on this and presented our current state of this initiative here: https://www.youtube.com/watch?v=cY_4ApOAVJQ\nThe code is not worth a patch right now, but will soon have something. I shall update on the latest state here as soon as I find myself some time (winding down from a hectic Black Friday/Cyber Monday support schedule). ",
            "author": "Ishan Chattopadhyaya"
        },
        {
            "id": "comment-16701951",
            "date": "2018-11-28T14:24:35+0000",
            "content": "Your thoughts please...\nThanks for your interest in this. Seems like your proposed ideas are very much inline with our approach that we're trying out as well. There are some initial experiments and results that we are performing as we speak, and I can see that there are benefits in the niche usecases that Adrien mentioned. ",
            "author": "Ishan Chattopadhyaya"
        },
        {
            "id": "comment-16701976",
            "date": "2018-11-28T14:58:04+0000",
            "content": "> The code is not worth a patch right now, but will soon have something. I shall update on the latest state\n> here as soon as I find myself some time (winding down from a hectic Black Friday/Cyber Monday support schedule).\n\nDo you think I could take a look at the code, I could do a quick review and perhaps add a bit of value.  I'm fine if the code is in dev state.\n\nWould you have written up something to describe what you are doing? ",
            "author": "Rinka Singh"
        },
        {
            "id": "comment-16701990",
            "date": "2018-11-28T15:06:19+0000",
            "content": "Adrien Grand\n(Unrelated to your comment Rinka, but seeing activity on this issue reminded me that I wanted to share something) There are limited use-cases for GPU accelelation in Lucene due to the fact that query processing is full of branches, especially since we added support for impacts and WAND.\n\nWhile Yes branches do impact the performance, well designed (GPU) code will consist of a combo of both CPU (the decision making part) and GPU code.  For example, I wrote a histogram as a test case that saw SIGNIFICANT acceleration and I also identified further code areas that can be improved.  I'm fairly sure (gut feel), I can squeeze out a 40-50x kind of improvement at the very least on a mid-sized GPU (given the time etc.,). I think things will be much, much better on a high end GPU and with further scale-up on a multi-gpu system...\n\nMy point is - thinking (GPU) parallel is a completely different ball-game and requires a mind-shift.  Once that happens, the value add will be massive and gut tells me Lucene is a huge opportunity.\n\nIncidentally, this is why I want to develop a library that I can put out there for integration.\n\nThat said Mike initially mentioned that BooleanScorer might be one scorer that could benefit from GPU acceleration as it scores large blocks of documents at once. I just attached a specialization of a disjunction over term queries that should make it easy to experiment with Cuda, see the TODO in the end on top of the computeScores method.\n\nLucene is really new to me (and so is working with Apache - sorry, I am a newbie to Apache) . Please will you put links here...   ",
            "author": "Rinka Singh"
        },
        {
            "id": "comment-16702135",
            "date": "2018-11-28T16:50:43+0000",
            "content": "Edited.  Sorry...\n\nA few questions.\n\n\tHow critical is the inverted index to the user experience?\n\tWhat happens if the inverted index is speeded up?\n\tHow many AWS instances would usually be used for searching through ~140GB sized inverted index and are there any performance numbers around this? (I'd like to compare to a server with 8 GPUs costing about $135-140K) - not sure what the equivalent GPU instances on Google Cloud/AWS would cost...\n\n\n\nAssumptions (please validate):\n\n\tDocuments are being added to the inverted index however the Index itself doesn't grow rapidly\n\tthe Maximum Index size will be less than 140GB - I assume 8 GPUs\n\n ",
            "author": "Rinka Singh"
        },
        {
            "id": "comment-16703835",
            "date": "2018-11-29T21:26:31+0000",
            "content": "How critical is the inverted index to the user experience?\n\nCompletely: almost all queries run on the inverted index. Unlike other datastores that run queries via linear scans and allow to speed things up by building indices, Lucene only enables querying vian an index.\n\nWhat happens if the inverted index is speeded up?\n\nThen most queries get a speed up too.\n\nHow many AWS instances would usually be used for searching through ~140GB sized inverted index\n\nHard to tell, it depends on your search load, how expensive queries are, etc. ",
            "author": "Adrien Grand"
        }
    ]
}