{
    "id": "LUCENE-7355",
    "title": "Leverage MultiTermAwareComponent in query parsers",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [
            "6.2",
            "7.0"
        ],
        "priority": "Minor",
        "status": "Closed",
        "type": "Improvement"
    },
    "description": "MultiTermAwareComponent is designed to make it possible to do the right thing in query parsers when in comes to analysis of multi-term queries. However, since query parsers just take an analyzer and since analyzers do not propagate the information about what to do for multi-term analysis, query parsers cannot do the right thing out of the box.",
    "attachments": {
        "LUCENE-7355.patch": "https://issues.apache.org/jira/secure/attachment/12813742/LUCENE-7355.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15351156",
            "author": "Adrien Grand",
            "date": "2016-06-27T14:39:36+0000",
            "content": "I propose the following plan:\n\n\tadd TokenStream tokenStreamMultiTerm(String fieldName, String text) to Analyzer.\n\tchange Analyzer.createComponents to take an additional boolean multiTerm parameter to know which parts of the analysis chain it should use when analyzing multi-term queries. For instance, the standard analyzer would apply a keyword tokenizer rather than a standard tokenizer, and only apply the standard and lowercase filters (no stop words). CustomAnalyzer would only apply the factories that implement MultiTermAwareComponent and pass them through MultiTermAwareComponent.getMultiTermComponent().\n\tchange query parsers to call tokenStreamMultiTerm rather than tokenStream when analyzing text for wildcard, regexp or fuzzy queries.\n\n "
        },
        {
            "id": "comment-15351222",
            "author": "Adrien Grand",
            "date": "2016-06-27T15:21:41+0000",
            "content": "Here is what the above plan would look like on Analyzer/StandardAnalyzer/CustomAnalyzer. Please comment if you do not like the idea or if you have suggestions as it would take time to update all analyzers. "
        },
        {
            "id": "comment-15351238",
            "author": "Robert Muir",
            "date": "2016-06-27T15:29:48+0000",
            "content": "Instead of passing a boolean to createComponents, can we just have a separate method? This would avoid lots of if-then-else logic (which is ripe for bugs).  "
        },
        {
            "id": "comment-15351310",
            "author": "Adrien Grand",
            "date": "2016-06-27T16:01:51+0000",
            "content": "Thanks for having a look. Does it look better this way? I also made Analyzer hold 2 storedValue s to make ReusableStrategy less complicated. "
        },
        {
            "id": "comment-15351327",
            "author": "Robert Muir",
            "date": "2016-06-27T16:08:13+0000",
            "content": "OK, my other suggestion would be to default the implementation to keywordtokenizer. This is already what is happening today, and I feel this is corner case functionality, we shouldn't make it any harder to make a new analyzer? "
        },
        {
            "id": "comment-15351343",
            "author": "Adrien Grand",
            "date": "2016-06-27T16:19:21+0000",
            "content": "This sounds good to me. "
        },
        {
            "id": "comment-15351782",
            "author": "Uwe Schindler",
            "date": "2016-06-27T20:46:32+0000",
            "content": "Hi,\nI have to think about this! Do we really need to change Analyzer's API? To me it sounds a bit strange to replace the Tokenizer by KeywordTokenizer by default... "
        },
        {
            "id": "comment-15352511",
            "author": "Adrien Grand",
            "date": "2016-06-28T07:23:56+0000",
            "content": "We want a way to tell the analyzer to normalize a piece of text, so it should not tokenize (this is why it replaces the tokenizer) and apply all normalization filters (lowercasing, ascii folding, etc.) but not transformations (stop word removal, stemming, etc.). I don't think we can do it without adding a new API to the Analyzer class (or at least a parameter to an existing method)? The main use-case is the parsing of multi-term queries in query parsers. Once we have such an API, query parsers would not need the lowercaseExpandedTerms parameter as they could directly use this new method that would do the right thing out of the box, including not only lowercasing but also eg. ascii folding, which is something that there is no way to do currently. Now that I am thinking about it more, I don't think we need the low-level TokenStream API as a return value for this new method, so maybe we could make it just String normalize(String field, String text). That would probably make it easier to use? "
        },
        {
            "id": "comment-15352539",
            "author": "Uwe Schindler",
            "date": "2016-06-28T07:47:49+0000",
            "content": "I don't think we need the low-level TokenStream API as a return value for this new method, so maybe we could make it just String normalize(String field, String text). That would probably make it easier to use?\n\nI was thinking about the same. Then we won't even need a KeywordTokenizer! We could just populate the termAttribute with the full term and call the filters. This would allow to remove the dependency to analysis-common from Analyzer (core). Just use the one from the document/field API to generate a single-value tokenstream (we use it for non-tokenized fields). Of course this can only work if the tokenfilters don't split terms, which a multi-term aware filter should never do.\n\nThese are just thoughts! We can implement the normalize method (like tokenStream) final taking a string and returning a string. "
        },
        {
            "id": "comment-15353173",
            "author": "Adrien Grand",
            "date": "2016-06-28T15:14:38+0000",
            "content": "This sounded appealing so I gave it a try but I hit a couple problems:\n\n\tsome analyzers need to apply char filters too, so we cannot expect to have a String in all cases we need some sort of KeywordTokenizer\n\tsome consumers need to get the binary representation of terms, which depends on the AttributeFactory (LUCENE-4176). So maybe we should return a TokenStream rather than a String an let consumers decide whether they want to add a CharTermAttribute or a TermToBytesRefAttribute. Is there a better option?\n\n "
        },
        {
            "id": "comment-15356787",
            "author": "Adrien Grand",
            "date": "2016-06-30T09:10:23+0000",
            "content": "I think I have something better now:\n\n\tthe method is BytesRef normalize(String field, String text), it can be configured with a subset of the char filters / token filters of the default analysis chain, and uses the same AttributeFactory as the default analysis chain\n\tsetLowerCaseExpandedTerms has been removed from query parsers, which now use Analyzer.normalize to process range/prefix/fuzzy/wildcard/regexp queries\n\tAnalyzingQueryParser and the classic QueryParser have been merged together\n\tboth SimpleQueryParser and the classic QueryParser now work with a non-default AttributeFactory that eg. uses a different encoding for terms (it was only the case before for wildcard queries and the classic QueryParser when analyzeRangeTerms was true). Other query parsers could be fixed too but it will require more work as they are using String representations for terms rather than binary.\n\n "
        },
        {
            "id": "comment-15357613",
            "author": "Robert Muir",
            "date": "2016-06-30T18:23:34+0000",
            "content": "I think normalize should call `end` for consistency? Its defined on TokenStream, and its going to always be called in the ordinary case, so its strange if \"for wildcards\" its not called, i can see bugs from that. "
        },
        {
            "id": "comment-15359027",
            "author": "Adrien Grand",
            "date": "2016-07-01T14:19:51+0000",
            "content": "Updated patch that calls TokenStream.end() in Analyzer.normalize(). "
        },
        {
            "id": "comment-15360979",
            "author": "Adrien Grand",
            "date": "2016-07-04T07:50:32+0000",
            "content": "Any objections to the latest patch? "
        },
        {
            "id": "comment-15363890",
            "author": "David Smiley",
            "date": "2016-07-06T07:09:09+0000",
            "content": "I suspect there may be a problem with your regexp: \n\nPattern.compile(\"(\\\\.)|([?*]+)\");\n\nShouldn't those back slashes be doubled up yet again \u2013 escape Java String and escape Regexp:\n\nPattern.compile(\"(\\\\\\\\.)|([?*]+)\");\n\n\nWhen I applied the patch and set a breakpoint at the \"continue\" in the condition that looks for group(1) it never hit for TestQueryParser.  When I updated the regexp as above it did.  It's getting late for me so maybe I'm missing something obvious.\n\nThe only other thing is that I'm surprised that the javadocs for normalize() don't mention anything about Wildcard/MultiTermQueries.  Shouldn't it to clarify it's intended use? "
        },
        {
            "id": "comment-15364075",
            "author": "Adrien Grand",
            "date": "2016-07-06T10:00:20+0000",
            "content": "Thanks David for having a look. This regular expression comes from AnalyzingQueryParser. I'll check it but I suspect you're right and it's been broken for a long time. I'll add the javadocs too. "
        },
        {
            "id": "comment-15364601",
            "author": "Adrien Grand",
            "date": "2016-07-06T16:46:03+0000",
            "content": "I fixed the regular expression and added a test. Regarding javadocs, they were already mentioning wildcard queries, maybe you were looking at the wrong #normalize method (there is a public one for external consumption and a protected one that analyzers need to extend in order to set the list of token filters to apply). While I was looking at it I also added a mention about fuzzy queries to be clearer that it is not only about wildcards. "
        },
        {
            "id": "comment-15364734",
            "author": "David Smiley",
            "date": "2016-07-06T17:50:54+0000",
            "content": "Those changes look good Adrien.\n\nThe patch grew by a lot; it appears you accidentally included other WIP in various places (benchmark module, some ivy files, ...)\n\nLooking at Analyzer.normalize()...\n\n\tWhy create a StringTokenStream; isn't KeywordTokenizer fine?  Oh I see that's in another module... kinda seems like a generic utility that should be in core to me IMO.\n\tAn easy optimization is to check if initReaderForNormalization returns the input StringReader.  If so, simply set filteredText to text.\n\tIt's a shame to call createComponents just to get the AttributeFactory.  Perhaps some future TODO issue could be to add a createAttributeFactory method used here and by createComponents' impls?  But then if some AnalyzerWrapper is in play then it's perhaps very cheap.\n\n\n\nI suppose a separate issue might be for Solr to do this when someone configures a custom Analyzer.\n\nNo blockers really; just feedback/questions. "
        },
        {
            "id": "comment-15365704",
            "author": "Adrien Grand",
            "date": "2016-07-07T06:53:13+0000",
            "content": "it appears you accidentally included other WIP\n\nSorry I probably generated the patch against the wrong base commit, hence these unrelated changes.\n\nWhy create a StringTokenStream; isn't KeywordTokenizer fine? Oh I see that's in another module... kinda seems like a generic utility that should be in core to me IMO.\n\nI'd be fine to have KeywordTokenizer in core too, let's discuss it in another issue and then potentially cut over to it if it makes it to core?\n\nAn easy optimization is to check if initReaderForNormalization returns the input StringReader. If so, simply set filteredText to text.\n\nThe way #normalize works is indeed not very efficient at the moment. In addition to this, it does not cache its analysis chain like we do for #tokenStream. But it's probably ok since this method should not be called as intensively as #tokenStream? (we can still improve in the future if this proves to be a bottleneck)\n\nIt's a shame to call createComponents just to get the AttributeFactory\n\nAgreed, this one annoys me too. I initially wanted to add a method but this is a pity since this information is already available. That said, maybe the method approach is better since borrowing the attribute factory from the regular analysis chain makes us close the token stream before it has been consumed, which some analysis chains might not like. I updated the patch.\n\nI suppose a separate issue might be for Solr to do this when someone configures a custom Analyzer.\n\nSolr already solves this problem in a different way by having a different analyzer for multi-term queries which is computed using MultiTermAwareComponent. I agree it would be nice for it to switch to Analyzer#normalize but this would have the drawback that it would either require to drop support for configuring a custom multi-term analyzer or the integration would be a bit weird, ie. it would have to use Analyzer.tokenStream on the multiterm analyzer if it is configured or fall back to Analyzer.normalize on the default analyzer if no multi-term analyzer is configured - which might be controversial. "
        },
        {
            "id": "comment-15366159",
            "author": "David Smiley",
            "date": "2016-07-07T14:07:13+0000",
            "content": "I like the new Analyzer.attributeFactory() method but I don't like that it documents that it's for #normalize \u2013 as if it should only be used for normalize.  Wouldn't it be useful for createComponents() too?  That would be a bigger change, however, since there are lots of times when a Tokenizer is created within the context of an Analyzer that would ideally be updated to call this method.  That seems like it deserves its own issue?  Or maybe for the time being we will accept that it's currently only used by normalize.  It would be nice to see CustomAnalyzer have a customizable AttributeFactory for TokenStream and to be returned by this proposed method.\n\nThat said, maybe the method approach is better since borrowing the attribute factory from the regular analysis chain makes us close the token stream before it has been consumed, which some analysis chains might not like.\n\nI think token streams should be tolerant of this or something in the TS chain is broken IMO.\n\nRE Solr, I only mean if there is an <analyzer class=\"...\" type=\"query\"> and thus the actual chain is opaque to Solr so it can't use it's normal means of determining the default multiTerm analysis chain.  This is a bit of a fringe issue any way since in my experience setting class= is rare.\n\nBTW nice work on this issue; it's nice to see AnalyzingQueryParser go away and the lowercase options get removed. "
        },
        {
            "id": "comment-15366384",
            "author": "Adrien Grand",
            "date": "2016-07-07T16:47:54+0000",
            "content": "I'll fix the docs to not bo specific to #normalize. I agree using attributeFactory() in tokenStream() has a large scope and probably deserves its own issue...\n\nBTW nice work on this issue; it's nice to see AnalyzingQueryParser go away and the lowercase options get removed.\n\nThanks! "
        },
        {
            "id": "comment-15367296",
            "author": "Adrien Grand",
            "date": "2016-07-08T06:36:37+0000",
            "content": "Patch that updates javadocs of #attributeFactory to not be specific to normalization (even though it is only used for normalization in practice for now). "
        },
        {
            "id": "comment-15367816",
            "author": "Adrien Grand",
            "date": "2016-07-08T15:36:08+0000",
            "content": "Fixing a typo. "
        },
        {
            "id": "comment-15368111",
            "author": "David Smiley",
            "date": "2016-07-08T18:17:37+0000",
            "content": "+1 "
        },
        {
            "id": "comment-15372967",
            "author": "ASF subversion and git services",
            "date": "2016-07-12T14:36:46+0000",
            "content": "Commit e92a38af90d12e51390b4307ccbe0c24ac7b6b4e in lucene-solr's branch refs/heads/master from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=e92a38a ]\n\nLUCENE-7355: Add Analyzer#normalize() and use it in query parsers. "
        },
        {
            "id": "comment-15373136",
            "author": "ASF subversion and git services",
            "date": "2016-07-12T16:03:18+0000",
            "content": "Commit 7c2e7a0fb80a5bf733cf710aee6cbf01d02629eb in lucene-solr's branch refs/heads/branch_6x from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7c2e7a0 ]\n\nLUCENE-7355: Add Analyzer#normalize() and use it in query parsers. "
        },
        {
            "id": "comment-15373149",
            "author": "Adrien Grand",
            "date": "2016-07-12T16:08:06+0000",
            "content": "Thanks David for helping me iterate on this issue.\n\nOn the 6.x branch, AnalyzingQueryParser uses the new Analyzer#normalize functionality while the classic QueryParser still relies on the lowercaseExpandedTerms option. "
        },
        {
            "id": "comment-15379415",
            "author": "Uwe Schindler",
            "date": "2016-07-15T14:06:46+0000",
            "content": "This broke the usage of Default attribute factory, see LUCENE-7382. I will fix this in a later commit. The default should be the same as the default as given by Tokenizers. The AttributeFactory as defined as default here is just \"slow\" and brings problems (e.g., LUCENE-7382), because it is not the one as used by Lucene as default otherwise. Sorry for not seeing the problem earlier! "
        },
        {
            "id": "comment-15379481",
            "author": "Uwe Schindler",
            "date": "2016-07-15T14:37:18+0000",
            "content": "I posted a patch to fix on LUCENE-7382. "
        },
        {
            "id": "comment-15380610",
            "author": "ASF subversion and git services",
            "date": "2016-07-16T08:10:05+0000",
            "content": "Commit 2585c9f3ff750b8e551f261412625aef0e7d4a4b in lucene-solr's branch refs/heads/master from Uwe Schindler\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=2585c9f ]\n\nLUCENE-7382: Fix bug introduced by LUCENE-7355 that used the wrong default AttributeFactory for new Tokenizers "
        },
        {
            "id": "comment-15380612",
            "author": "ASF subversion and git services",
            "date": "2016-07-16T08:11:19+0000",
            "content": "Commit d71a358601ad7438d9052861b816d151d11d471b in lucene-solr's branch refs/heads/branch_6x from Uwe Schindler\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d71a358 ]\n\nLUCENE-7382: Fix bug introduced by LUCENE-7355 that used the wrong default AttributeFactory for new Tokenizers "
        },
        {
            "id": "comment-15380615",
            "author": "Uwe Schindler",
            "date": "2016-07-16T08:12:41+0000",
            "content": "I fixed the bug with the AttributeSource in LUCENE-7382. "
        },
        {
            "id": "comment-15438984",
            "author": "Michael McCandless",
            "date": "2016-08-26T13:59:18+0000",
            "content": "Bulk close resolved issues after 6.2.0 release. "
        }
    ]
}