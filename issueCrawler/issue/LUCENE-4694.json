{
    "id": "LUCENE-4694",
    "title": "Add back IndexReader.fields() -> Multi*, or discourage term vectors in some better way",
    "details": {
        "components": [],
        "fix_versions": [],
        "affect_versions": "None",
        "priority": "Major",
        "labels": "",
        "type": "Bug",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "Users can easily get term vectors from any indexreader, but not postings lists. this encourages them to do really slow things: like pulling term vectors for every single document.\n\nthis is really really so much worse than going through multifields or whatever.",
    "attachments": {
        "LUCENE-4694.patch": "https://issues.apache.org/jira/secure/attachment/12571985/LUCENE-4694.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2013-01-17T15:03:32+0000",
            "content": "I think there is a similar problem with doc values vs. stored fields? The API makes fetching all values of a specific field easy with stored fields and hard with doc values. ",
            "author": "Adrien Grand",
            "id": "comment-13556250"
        },
        {
            "date": "2013-01-17T15:11:18+0000",
            "content": "+1, I think it makes sense. ",
            "author": "Tommaso Teofili",
            "id": "comment-13556255"
        },
        {
            "date": "2013-03-04T23:33:19+0000",
            "content": "Patch, moving the postings APIs up from AR to IR, and calling MultiFields.getFields by default. ",
            "author": "Michael McCandless",
            "id": "comment-13592797"
        },
        {
            "date": "2013-03-04T23:50:16+0000",
            "content": "Oh my god  ",
            "author": "Uwe Schindler",
            "id": "comment-13592812"
        },
        {
            "date": "2013-03-04T23:58:48+0000",
            "content": "Mike: If you really, really want to move it up:\n\n\tMake fields() abstract in IndexReader, only add the final methods that delegate to the abstract one\n\tleave fields() abstract in AtomicReader and CompositeReader, too\n\tadd the MultiFields implementation into BaseCompositeReader -> and only there (maybe final)\n\tthe javadocs with warning are only in CompositeReader\n\n\n\nThe current patch is as worse as IndexReader before the split into composite/atomic! IndexReader is an abstract class without concrete implementations. The final methods only delegate to abstract methods, so they are no \"implementations\".\n\nBut I don't like the whole thing in general. Just because few users are doing those thing, we should never ever expose postings on composite readers. In my opinion, the term vectors should simply go away?\n\nStored fields with the binary search are only there to make \"result display\" possible. Maybe we should remove stored fields from IndexReader base class, too. Instead stored field are only accessible through IndexSearcher (which delegates using its reader contexts)? ",
            "author": "Uwe Schindler",
            "id": "comment-13592819"
        },
        {
            "date": "2013-03-05T17:23:38+0000",
            "content": "I think ... we have to do something here?\n\nEither remove term vectors from IR, or, move postings back to IR.  It's very trappy now because people look at IR and the only postings-like API they see is term vectors. ",
            "author": "Michael McCandless",
            "id": "comment-13593616"
        },
        {
            "date": "2013-03-08T12:24:25+0000",
            "content": "I opened this issue because of stuff I see on the mailing lists and on the internet. \n\nI dont care which way we fix it, but the complete fix cannot simply be removal of methods.\nIf we want to force users to use these things per-segment, then the API and doc must be fixed\nso that its easy and natural to do so.\n\nFor example IR has nothing in its javadocs about how to work the leaves per segment. its also\nconfusing that leaves() returns AtomicReaderContext. I'm confused why we have to return such a class\ninstead of AtomicReaders directly from leaves. Someone going per-segment doesnt need this to know docbases right?\n\n\nint docBase - 0;\nfor (AtomicReader ar : ir.leaves()) {\n  // something\n  docBase += ar.maxDoc();\n}\n\n\n ",
            "author": "Robert Muir",
            "id": "comment-13597069"
        },
        {
            "date": "2013-03-10T08:17:35+0000",
            "content": "we should never ever expose postings on composite readers\n\nI don't know if I agree with this. Iterating on a posting list is something very basic IMO. And it has no inefficiencies whatsoever when a CompositeReader needs to implement it. The returned CompositeDocsEnum can do the iteration on the sub-DocsEnum itself, using liveDocs of each (not MultiLiveDocs) and you get both a friendly API as well as that's what users will need to do anyway ...\n\nFor example IR has nothing in its javadocs about how to work the leaves per segment\n\nI don't think it's just a matter of javadocs. If your application has an IndexReader r, it's entirely not clear today how to get to the postings/DocValues API. leaves() doesn't give the hint, and the fact that document() and getTV() are there is even more confusing.\n\nI personally think it's ok if IndexReader lets you get docsValues(doc), document(doc), getTV(doc) and termDocsEnum(term). There's nothing inefficient about supporting them, as far as I can see.\n\nAbout API that needs to \"merge\" things, like fields(), terms(field), getLiveDocs() ... well, what's the harm of exposing that with documentation that these are implemented inefficiently, and you should use the respective API on the AtomicReader returned from leaves()? We don't need to make everyone an expert Lucene developer, especially when it doesn't matter (e.g. for simple stupid tests that need to wrap an IR with SlowComposite)... however, since I appreciate all the work that was done to separate the API, I'm fine if uses need to do wrap w/ SlowComposite. That makes the 'slowness' more evident. But let's force users to do that only for the API that really cannot be implemented efficiently? ",
            "author": "Shai Erera",
            "id": "comment-13598186"
        },
        {
            "date": "2013-03-10T08:29:26+0000",
            "content": "\nI personally think it's ok if IndexReader lets you get docsValues(doc), document(doc), getTV(doc) and termDocsEnum(term). There's nothing inefficient about supporting them, as far as I can see.\n\nthis is not correct at all. \n\nfor the sorted types we need to iterate through all of the values and create a datastructure mapping per-segment ordinals to global ones, and also cache this somewhere. \n\nadditionally, all docvalues types and norms on a composite reader would pay the cost of binary-search for each docid access: and due to the way they are used, typically many docids are accessed.\n\nstored fields are used for summary results, so on a 100 million doc index who cares if you do 10 or 20 binary searches: who cares.\n\nterm vectors are used for highlighting summary results, MoreLikeThis, etc: both of which are small top-N just like the stored fields case. so its also fine.\n\nbut docvalues is used in scoring and sorting, so this would be 100 million binary searches. its a big damn difference.\n\nthe postings is pretty much just an additional check per document, so its a little more up in the air what to do. but as mentioned in the description, users look at IndexReader.java and the only postings api they see is term vectors. ",
            "author": "Robert Muir",
            "id": "comment-13598189"
        },
        {
            "date": "2013-03-10T08:56:43+0000",
            "content": "but docvalues is used in scoring and sorting, so this would be 100 million binary searches. its a big damn difference.\n\nI did not propose to ditch the per-segment efficient implementation, but rather offer on the top-level those APIs as well. Whoever implements a Collector/Scorer will still work with the AtomicReader instance which implements things efficiently. Only those who need to work on top-level will need to do SlowComposite.wrap() and work with inefficient implementations. ",
            "author": "Shai Erera",
            "id": "comment-13598194"
        }
    ]
}