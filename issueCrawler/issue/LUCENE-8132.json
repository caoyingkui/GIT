{
    "id": "LUCENE-8132",
    "title": "HyphenationDecompoundTokenFilter does not set position/offset attributes correctly",
    "details": {
        "labels": "",
        "priority": "Major",
        "resolution": "Unresolved",
        "affect_versions": "6.6.1,                                            7.2.1",
        "status": "Open",
        "type": "Bug",
        "components": [
            "modules/analysis"
        ],
        "fix_versions": []
    },
    "description": "HyphenationDecompoundTokenFilter and DictionaryDecompoundTokenFilter set positionIncrement to 0 for all subwords, reuse start/endoffset of the original token and ignore positionLength completly.\n\nIn consequence, the QueryBuilder generates a SynonymQuery comprising all subwords, which should rather treated as individual terms.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16333962",
            "date": "2018-01-22T07:36:20+0000",
            "content": "I agree this sounds wrong. Unfortunately, inserting positions in a token filter is hard to do right if the analysis chain has a preceding token filter that sets synonyms, as you need to fix positions on all paths. This issue touches this problem a bit: LUCENE-5012. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16334016",
            "date": "2018-01-22T08:27:50+0000",
            "content": "Ok, seems hard to get right for all cases. I wonder, if the current implementation could work at query time for anyone. \nHowever, I\u2018m working on a fix for HyphenationDecompounderTokenFilter that handles offset, posInc and posLength, though not in case a synonym filter is applied before. ",
            "author": "Holger Bruch"
        },
        {
            "id": "comment-16334027",
            "date": "2018-01-22T08:37:00+0000",
            "content": "Maybe the right solution is just to fix it correctly and simply enforce input instanceof Tokenizer? Because its really like an extension of tokenization. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16334065",
            "date": "2018-01-22T09:14:03+0000",
            "content": "I'm not sure how practical this would be: some tokenizers today sometimes set the pos inc to 0 I think (JapanesTokenizer?) and it would only allow one of such filters in the analysis chain. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16334074",
            "date": "2018-01-22T09:30:07+0000",
            "content": "why do you need to decompound more than once? The japanesetokenizer example is the same issue (as it already decompounds) ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16334080",
            "date": "2018-01-22T09:46:37+0000",
            "content": "I haven't though about concrete use-cases, but for instance I suspect some users perform decompounding using both an algorithm and a dictionary? ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16334273",
            "date": "2018-01-22T13:44:13+0000",
            "content": "Thats what HyphenationDecompoundTokenFilter already does. I think maybe the name is confusing, at least look at the class javadocs \n\nIn this case I'm sorry but I think you are stretching, (and you aren't correct). We should fix these filters and enforce tokenizer as input, seriously. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16335427",
            "date": "2018-01-23T07:02:05+0000",
            "content": "I\u2019m not as deeply in Lucene as you are. What would be the pros and cons of ensuring the input is an instance of tokenizer?\nWould it still be possible to apply a token filters like WDF or lowercase filter before the HyphenationDecompunder? ",
            "author": "Holger Bruch"
        },
        {
            "id": "comment-16335455",
            "date": "2018-01-23T07:34:35+0000",
            "content": "No, the hyphenation decompounder would have to be the first token filter in the analysis chain. ",
            "author": "Adrien Grand"
        }
    ]
}