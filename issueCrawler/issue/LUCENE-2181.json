{
    "id": "LUCENE-2181",
    "title": "benchmark for collation",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/benchmark"
        ],
        "type": "New Feature",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Steven Rowe attached a contrib/benchmark-based benchmark for collation (both jdk and icu) under LUCENE-2084, along with some instructions to run it... \n\nI think it would be a nice if we could turn this into a committable patch and add it to benchmark.",
    "attachments": {
        "top.100k.words.de.en.fr.uk.wikipedia.2009-11.tar.bz2": "https://issues.apache.org/jira/secure/attachment/12429835/top.100k.words.de.en.fr.uk.wikipedia.2009-11.tar.bz2",
        "LUCENE-2181.patch": "https://issues.apache.org/jira/secure/attachment/12429834/LUCENE-2181.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-01-02T23:37:24+0000",
            "content": "Attached .zip'd patch (over 10MB because of the 4 languages' LineDocs) integrated into the Ant build for the ICU contrib, rather than integrated into the Benchmark build.\n\nInvoke using ant benchmark from the contrib/icu/ directory. ",
            "author": "Steve Rowe",
            "id": "comment-12795929"
        },
        {
            "date": "2010-01-03T09:13:42+0000",
            "content": "Hi Steven, one idea we can do here is to put the large files online (zipped) and have ant download them when we run the benchmark... \n\nI can put them on people.apache.org for this purpose, we already do the same with the huge wikipedia file... what do you think? ",
            "author": "Robert Muir",
            "id": "comment-12795959"
        },
        {
            "date": "2010-01-03T19:13:35+0000",
            "content": "Works for me.\n\nI do have one concern, though: the LineDocSource parser doesn't know how to handle comments, so these four files don't have Apache2 license declarations in them.  We should put a README (or something like it) with these files to indicate the license.\n\nDifferent subject: I'm not sure where it would go, but the code I used to produce these top-TF wikipedia files may be useful to other people - where do you think it could live?  An example, maybe? ",
            "author": "Steve Rowe",
            "id": "comment-12796017"
        },
        {
            "date": "2010-01-04T02:43:10+0000",
            "content": "I do have one concern, though: the LineDocSource parser doesn't know how to handle comments, so these four files don't have Apache2 license declarations in them. We should put a README (or something like it) with these files to indicate the license.\n\nAre they really apache license? or derived from wikipedia content? if these files are only being downloaded when you run 'ant benchmark' for collation, then it is just like the enwiki task in benchmark, it downloads some huge wikipedia data and runs it. So someone please correct me if I am wrong, but I don't think we should be putting apache license headers in these files anyway, its just like the benchmark enwiki task, we are not shipping it with our source distribution.\n\nDifferent subject: I'm not sure where it would go, but the code I used to produce these top-TF wikipedia files may be useful to other people - where do you think it could live? An example, maybe?\n\nhmm I will have to think about this... anyone got ideas?  I think this would be useful too (I admit to not having yet looked at the implementation), here are two examples:\n\n\tKarl could use this to evaluate his swedish stemming improvements, taking frequency into account.\n\tthe obvious use of when you need to build a stopword list, these top terms are where you want to start.\n\n ",
            "author": "Robert Muir",
            "id": "comment-12796066"
        },
        {
            "date": "2010-01-04T14:09:17+0000",
            "content": "\n... these four files don't have Apache2 license declarations in them. We should put a README (or something like it) with these files to indicate the license.\n\nAre they really apache license? or derived from wikipedia content?... I don't think we should be putting apache license headers in these files\n\nHmm, I just assumed that since these files were not (anything even close to) verbatim copies that they were independently licensable new works, but it's definitely more complicated than that...\n\nThis looks like the place to start where licensing is concerned:\n\nhttp://en.wikipedia.org/wiki/Wikipedia_Copyright\n\nMy (way non-expert) reading of this is that Wikipedia-derived works (and I'm pretty sure these frequency lists qualify as such) must be licensed under the Creative Commons Attribution-Share Alike 3.0 Unported license, which does not appear to me to be entirely compatible with the Apache2 license.\n\nSo I agree with you  - with the caveat that some form of attribution and a pointer to licensing info should be included with these files. ",
            "author": "Steve Rowe",
            "id": "comment-12796181"
        },
        {
            "date": "2010-01-09T22:14:51+0000",
            "content": "Hi Robert, \n\nIn the new version of the patch, ant benchmark from the contrib/icu/ directory attempts to download the attached tar.bz2 file from http://people.apache.org/~rmuir/wikipedia (please change this to the location where you end up putting the file), then unpacks the archive to the contrib/icu/src/benchmark/work/ directory, then compiles and runs the benchmark.\n\nIn addition to the top 100K word lists, the tar.bz2 file contains LICENSE.txt, which contains links to the Wikipedia dumps from which the lists were extracted, along with a link to the license that Wikipedia uses. ",
            "author": "Steve Rowe",
            "id": "comment-12798413"
        },
        {
            "date": "2010-01-10T04:39:56+0000",
            "content": "Hi Steve, thanks a lot for your work here, this is nice.\n\nI put the files in my apache directory, but modified your patch somewhat\n\n\tI moved it to the benchmark package proper\n\tI created two tasks: NewLocale and NewCollationAnalyzer\n\n\n\nThe NewLocale task sets a Locale in the PerfRunData, in my opinion this is reusable beyond collation since it could be used for testing Localized sorting and range searching as well, it supports all Locale params (lang,country,variant)\n\nThe NewCollationAnalyzer creates a [ICU]CollationKeyAnalyzer with the collator defaults depending upon this Locale, right now i only added options to specify impl (impl:jdk or impl:icu), but in the future we can add strength, decomposition, etc.\n\nTake a look and let me know what you think, surely I made some mistakes but it appears to work fine. ",
            "author": "Robert Muir",
            "id": "comment-12798444"
        },
        {
            "date": "2010-01-10T17:11:30+0000",
            "content": "Looks good.  I like the way you've integrated it into the benchmark suite, and as you say the NewLocaleTask should prove useful elsewhere.\n\nI put the files in my apache directory, but modified your patch somewhat\n\nOne major thing you changed but didn't mention above is that rather than applying the collation key transform only to the LineDoc body field, it's now applied also to the title and date fields.  Given the nature of the top 100k words files \u2013 the title is an integer representing term frequency, and the date is essentially meaningless (the date on which I created the file) \u2013 I don't think this makes sense (and that's why I made analyzers that only applied collation to the body field). ",
            "author": "Steve Rowe",
            "id": "comment-12798508"
        },
        {
            "date": "2010-01-10T17:24:06+0000",
            "content": "Steve ahh i was wondering about the per-field analyzer wrapper (sorry i neglected to mention this, i just forgot about it)... there are likely other problems too, and the new stuff needs tests.\n\nWhat about this per-field thing, what if in the data files, title and date were simply blank?\n\nOr should we worry, I agree its stupid, does it skew the results though?\nOne way to look at it is that its also fairly realistic (even though its meaningless, you see numbers and dates everywhere).\n\nThe downside to doing per-analyzer wrapper is that it introduces some complexity, in all honesty this is not really specific to this collation task, right? (i.e. the existing analysis/tokenization benchmarks have this same problem) ",
            "author": "Robert Muir",
            "id": "comment-12798510"
        },
        {
            "date": "2010-01-10T17:34:00+0000",
            "content": "Steven, another idea: what if we simply added the options to DocMaker so we could turn off the tokenization of title and date fields?\n\nright now you can only control this stuff for the body fields. imo this would be the best solution.\n\nedit: ok this is already there, just a little strange. we can use doc.tokenized to turn it off for these other fields, and doc.body.tokenized turned on so that we are only analyzing the field we want.\n\ni'll update the alg file and produce a new patch ",
            "author": "Robert Muir",
            "id": "comment-12798514"
        },
        {
            "date": "2010-01-10T17:54:59+0000",
            "content": "What about this per-field thing, what if in the data files, title and date were simply blank?\n\nHmm, although the date field value is meaningless, I like the TF-in-title-field thing.\n\n\nOr should we worry, I agree its stupid, does it skew the results though?\nOne way to look at it is that its also fairly realistic (even though its meaningless, you see numbers and dates everywhere).\n\nI was thinking that it would, and that it's not really a meaningful test of collation - who's going to bother running collation over integers and dates? - but since the comparison here is between two implementations of collation, I think you're right that there is no skew in doing this comparison:\n\nicu(kiwi) + icu(apple) + icu(orange) : jdk(kiwi) + jdk(apple) + jdk(orange)\n\ninstead of this one:\n\nkeyword(kiwi) + keyword(apple) + icu(orange) : keyword(kiwi) + keyword(apple) + jdk(orange)\n\n(where the icu(X) transform = keyword(X) + icu-collation(X), and similarly for the jdk(X) transform)\n\nThe downside to doing per-analyzer wrapper is that it introduces some complexity, in all honesty this is not really specific to this collation task, right? (i.e. the existing analysis/tokenization benchmarks have this same problem)\n\nYup, you're right.  A general facility to do this will end up looking (modulo syntax) like Solr's per-field analysis specification. ",
            "author": "Steve Rowe",
            "id": "comment-12798519"
        },
        {
            "date": "2010-01-10T17:58:17+0000",
            "content": "Steven, another idea: what if we simply added the options to DocMaker so we could turn off the tokenization of title and date fields?\n\nGood idea!\n\ni'll update the alg file and produce a new patch\n\nExcellent, thanks! ",
            "author": "Steve Rowe",
            "id": "comment-12798520"
        },
        {
            "date": "2010-01-10T18:38:34+0000",
            "content": "Steven I also havent forgotten about your other contribution, the thing that creates the benchmark corpus in the first place from wikipedia.\n\nOne idea I had would be that such a thing wouldn't be too out of place in the open relevance project... (munging corpora etc) ",
            "author": "Robert Muir",
            "id": "comment-12798527"
        },
        {
            "date": "2010-01-10T23:13:47+0000",
            "content": "ok i think we might be close to something committable now:\n\n\twrote tests for NewLocaleTask and NewCollationAnalyzerTask\n\tset doc.stored=false, doc.tokenized=false, doc.body.tokenized=true in the collation.alg file\n\ti moved the two scripts into a 'scripts' directory, i thought this made more sense?\n\tI also renamed the bm2jira.pl script to collation.bm2jira.pl\n\n\n\nhere is the output from 'ant collation' from the benchmark package:\n\n\n\n\nLanguage\njava.text\nICU4J\nKeywordAnalyzer\nICU4J Improvement\n\n\nEnglish\n10.78s\n7.32s\n1.58s\n60%\n\n\nFrench\n11.48s\n7.52s\n1.59s\n67%\n\n\nGerman\n11.19s\n7.52s\n1.61s\n62%\n\n\nUkrainian\n13.03s\n8.68s\n1.66s\n62%\n\n\n\n\n\ni think its more accurate relative to KeywordAnalyzer now that we aren't storing the body text in a stored field and things like that, but of course you can change the .alg file to see if the differences matter in the context of overall indexing by turning these back on. ",
            "author": "Robert Muir",
            "id": "comment-12798553"
        },
        {
            "date": "2010-01-10T23:22:15+0000",
            "content": "fix a bug in testCollator/assertEqualCollation, so its actually testing correct behavior rather than being a no-op  ",
            "author": "Robert Muir",
            "id": "comment-12798555"
        },
        {
            "date": "2010-01-11T00:20:21+0000",
            "content": "ok, somehow it completely bypassed my brain you are using ReadTokens task \n\nso this is a problem, because ReadTokens does not respect the DocMaker configuration. In my opinion it should not tokenize fields unless they are configured to be tokenized. \n\nSo I added the following in this patch to fix this:\n\n     for(final Fieldable field : fields) {\n+      if (!field.isTokenized()) continue;\n+\n\n\n\nnow we get the results we expect:\n\n\n\nLanguage\njava.text\nICU4J\nKeywordAnalyzer\nICU4J Improvement\n\n\nEnglish\n3.43s\n2.21s\n1.15s\n115%\n\n\nFrench\n3.78s\n2.37s\n1.17s\n117%\n\n\nGerman\n3.84s\n2.42s\n1.18s\n115%\n\n\nUkrainian\n5.81s\n3.67s\n1.24s\n88%\n\n\n\n\n\nif you comment out the doc.tokenized=false, then you get the other results i just posted instead, as it will analyze the other fields too. ",
            "author": "Robert Muir",
            "id": "comment-12798560"
        },
        {
            "date": "2010-01-11T06:11:03+0000",
            "content": "Works for me:\n\nJAVA:\njava version \"1.5.0_15\"\nJava(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_15-b04)\nJava HotSpot(TM) 64-Bit Server VM (build 1.5.0_15-b04, mixed mode)\n\nOS:\ncygwin\nWinVistaService Pack 2\nService Pack 26060022202561\n\n\n\n\nLanguage\njava.text\nICU4J\nKeywordAnalyzer\nICU4J Improvement\n\n\nEnglish\n5.53s\n2.03s\n1.20s\n422%\n\n\nFrench\n6.41s\n2.13s\n1.19s\n455%\n\n\nGerman\n6.36s\n2.19s\n1.22s\n430%\n\n\nUkrainian\n8.92s\n3.62s\n1.21s\n220%\n\n\n\n ",
            "author": "Steve Rowe",
            "id": "comment-12798584"
        },
        {
            "date": "2010-01-11T06:39:10+0000",
            "content": "I just ran the contrib/benchmark tests, and I got one test failure:\n\n\n    [junit] Testcase: testReadTokens(org.apache.lucene.benchmark.byTask.TestPerfTasksLogic):    FAILED\n    [junit] expected:<3108> but was:<3128>\n    [junit] junit.framework.AssertionFailedError: expected:<3108> but was:<3128>\n    [junit]     at org.apache.lucene.benchmark.byTask.TestPerfTasksLogic.testReadTokens(TestPerfTasksLogic.java:480)\n    [junit]     at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:212)\n    [junit] \n    [junit] \n    [junit] Test org.apache.lucene.benchmark.byTask.TestPerfTasksLogic FAILED\n\n ",
            "author": "Steve Rowe",
            "id": "comment-12798588"
        },
        {
            "date": "2010-01-11T06:44:37+0000",
            "content": "I think NewCollationAnalyzerTask should be a little more careful about parsing its parameters - here's a slightly modified version of your setParams() that understands \"impl:jdk\" and complains about unrecognized params:\n\n\n@Override\n  public void setParams(String params) {\n    super.setParams(params);\n    \n    StringTokenizer st = new StringTokenizer(params, \",\");\n    while (st.hasMoreTokens()) {\n      String param = st.nextToken();\n      StringTokenizer expr = new StringTokenizer(param, \":\");\n      String key = expr.nextToken();\n      String value = expr.nextToken();\n      // for now we only support the \"impl\" parameter.\n      // TODO: add strength, decomposition, etc\n      if (key.equals(\"impl\")) {\n        if (value.equalsIgnoreCase(\"icu\"))\n          impl = Implementation.ICU;\n        else if (value.equalsIgnoreCase(\"jdk\"))\n          impl = Implementation.JDK;\n        else\n          throw new RuntimeException(\"Unknown parameter \" + param);\n      } else {\n        throw new RuntimeException(\"Unknown parameter \" + param);\n      }\n    }\n  }\n\n ",
            "author": "Steve Rowe",
            "id": "comment-12798589"
        },
        {
            "date": "2010-01-11T06:46:08+0000",
            "content": "\nSteven I also havent forgotten about your other contribution, the thing that creates the benchmark corpus in the first place from wikipedia.\n\nOne idea I had would be that such a thing wouldn't be too out of place in the open relevance project... (munging corpora etc)\n\nInteresting idea, thanks - I'll take a look at what's there now and see how my stuff would fit in. ",
            "author": "Steve Rowe",
            "id": "comment-12798590"
        },
        {
            "date": "2010-01-11T12:55:07+0000",
            "content": "I just ran the contrib/benchmark tests, and I got one test failure: \n\nOK, I think this is from adjusting the ReadTokensTask...\n\nI looked at the test and i think it .... should be improved....  ",
            "author": "Robert Muir",
            "id": "comment-12798655"
        },
        {
            "date": "2010-01-11T13:14:11+0000",
            "content": "corrected this testReadTokens(), it tests by adding up token freq across the index and comparing it to the number of tokens read, but there is a non-tokenized, but indexed field (DocMaker.ID_FIELD), the keywords from this should not add to the expected count.\n\nadded your addt'l parameter checking for NewCollationAnalyzerTask ",
            "author": "Robert Muir",
            "id": "comment-12798662"
        },
        {
            "date": "2010-01-11T14:49:17+0000",
            "content": "+1, tests all pass, and \"ant collation\" produced expected output.\n\nOne minor detail, though - shouldn't the output files be renamed to identify their purpose, similarly to how you renamed bm2jira.pl?  Here's the relevant section in contrib/benchmark/build.txt:\n\n\n<property name=\"collation.output.file\" value=\"${working.dir}/benchmark.output.txt\"/>\n<property name=\"collation.jira.output.file\"  value=\"${working.dir}/bm2jira.output.txt\"/>\n\n ",
            "author": "Steve Rowe",
            "id": "comment-12798679"
        },
        {
            "date": "2010-01-11T18:43:39+0000",
            "content": "Steven thanks, in addition to your comments I also changed the config to download the top100k files to the temp directory, and expand to work/top100k-out, consistent with the other benchmark datasets.\n\nI think this one is ready. If there is no objection I will commit in a day or two. ",
            "author": "Robert Muir",
            "id": "comment-12798774"
        },
        {
            "date": "2010-01-12T03:38:02+0000",
            "content": "+1, once again, tests all pass, and \"ant collation\" produced expected output.  ",
            "author": "Steve Rowe",
            "id": "comment-12799056"
        },
        {
            "date": "2010-01-12T20:08:04+0000",
            "content": "Committed revision 898491.\n\nThanks Steven! ",
            "author": "Robert Muir",
            "id": "comment-12799361"
        }
    ]
}