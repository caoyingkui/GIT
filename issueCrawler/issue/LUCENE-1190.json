{
    "id": "LUCENE-1190",
    "title": "a lexicon object for merging spellchecker and synonyms from stemming",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/search",
            "modules/other"
        ],
        "type": "New Feature",
        "fix_versions": [],
        "affect_versions": "2.3",
        "resolution": "Won't Fix",
        "status": "Resolved"
    },
    "description": "Some Lucene features need a list of referring word. Spellchecking is the basic example, but synonyms is an other use. Other tools can be used smoothlier with a list of words, without disturbing the main index : stemming and other simplification of word (anagram, phonetic ...).\nFor that, I suggest a Lexicon object, wich contains words (Term + frequency), wich can be built from Lucene Directory, or plain text files.\nClassical TokenFilter can be used with Lexicon (LowerCaseFilter and ISOLatin1AccentFilter should be the most useful).\nLexicon uses a Lucene Directory, each Word is a Document, each meta is a Field (word, ngram, phonetic, fields, anagram, size ...).\nAbove a minimum size, number of differents words used in an index can be considered as stable. So, a standard Lexicon (built from wikipedia by example) can be used.\nA similarTokenFilter is provided.\nA spellchecker will come soon.\nA fuzzySearch implementation, a neutral synonym TokenFilter can be done.\nUnused words can be remove on demand (lazy delete?)\n\nAny criticism or suggestions?",
    "attachments": {
        "aphone+lexicon.patch": "https://issues.apache.org/jira/secure/attachment/12376437/aphone%2Blexicon.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-02-29T19:11:02+0000",
            "content": "News features:\nhelper to extends query with similarity of each term :\n+type:dog +name:rintint*\nwill become:\n+type+dog (dogs doggy)^0.7) +name:rintint*\n\n\"Do you mean pattern\" packaged over IndexSearcher. If search result is under a thresold, sorted suggestion list for each term is provided, and a rewritten query sentence:\ntruc:brawn\nwill become:\ntruc:brown \n\n ",
            "author": "Mathieu Lecarme",
            "id": "comment-12573907"
        },
        {
            "date": "2008-03-02T04:00:40+0000",
            "content": "This sounds like something that might be interesting, but honestly I don't follow the initial description and the 300KB+ patch is a big one.\n\nFor example, I don't know what you mean by \"Some Lucene features need a list of referring word\".  Do you mean \"a list of associated words\"?\n\n\nLexicon uses a Lucene Directory, each Word is a Document, each meta is a Field (word, ngram, phonetic, fields, anagram, size ...).\n\nEach meta is a Field.... what do you mean by that?  Could you please give an example?\n\n\nAbove a minimum size, number of differents words used in an index can be considered as stable. So, a standard Lexicon (built from wikipedia by example) can be used.\n\nHm, not sure I know what you mean.  Are you saying that once you create a sufficiently large lexicon/dictionary/index, the number of new terms starts decreasing? (Heap's Law? http://en.wikipedia.org/wiki/Heaps'_law ) ",
            "author": "Otis Gospodnetic",
            "id": "comment-12574182"
        },
        {
            "date": "2008-03-02T12:30:51+0000",
            "content": "\nWith a FuzzyQuery, for example, you iterate over Term in index, and  \nlooking for the nearest one. PrefixQuery or regular expression work in  \na similar way.\nIf you say, fuzzy querying will never gives a word with different size  \nof 1 (size+1 or size -1), you can restrict the list of candidates, and  \nngram index can help you more.\n\nSome token filter destroy the word. Stemmer for example. If you wont  \nto search wide, stemmer can help you, but can't use PrefixQuery with  \nstemmed word. So, you can stemme word in a lexicon and use it as a  \nsynonym. You index \"dog\" and look for \"doggy\",  \"dogs\" and \"dog\".  \nLexicon can use static list of word, from hunspell index or wikipedia  \nparsing, or words extracted from your index.\n\nfor the word \"Lucene\" :\n\nword:lucene\npop:42\nanagram.anagram:celnu\naphone.start:LS\naphone.gram:LS\naphone.gram:SN\naphone.end:SN\naphone.size:3\naphone.phonem:LSN\nngram.start:lu\nngram.gram:lu\nngram.gram:uc\nngram.gram:ce\nngram.gram:en\nngram.gram:ne\nngram.end:ne\nngram.size:6\nstemmer.stem:lucen\n\n\nYes.\n\nM. ",
            "author": "Mathieu Lecarme",
            "id": "comment-12574214"
        },
        {
            "date": "2008-03-07T22:27:45+0000",
            "content": "A simpler preview of Lexicon features :\nhttp://blog.garambrogne.net/index.php?post/2008/03/07/A-lexicon-approach-for-Lucene-index ",
            "author": "Mathieu Lecarme",
            "id": "comment-12576415"
        },
        {
            "date": "2010-04-27T21:15:44+0000",
            "content": "Just came across this old issue, and still can't easily follow it.  But I wonder if this issue has become irrelevant with all the new work on analyzers that Robert Muir & Co. are doing? ",
            "author": "Otis Gospodnetic",
            "id": "comment-12861546"
        },
        {
            "date": "2010-04-27T22:03:33+0000",
            "content": "Hi Otis, I took a look, and followed the blog link and explored \nthe linked svn there (it was easier than reading the patch).\n\nI guess the interesting approach I see here is what looks to be \nsome generation of phonetic filters (similar to the ones in Solr)\nfrom aspell resources. \n\nHonestly though, I am not knowledgeable on aspell to know\nto what degree this would work for some of these languages,\nand how it would compare to things like metaphone.\n\nSo, we could potentially use this idea if people wanted some\nmore phonetic 'hash' functions available for specific languages,\nbut I have a few concerns:\n\n\tI do not know the license of the aspell resources these were gen'ed from\n\tAs mentioned above, I don't know the quality.\n\tI think it would be preferable for the filter to work from the aspell files rather\nthan gen'ing code if possible\n\n\n\nAs far as what hunspell offers in comparison, I am not sure that\nit has this, instead it offers things like typical replacements that\ncan be attempted for spellchecking and such.. Chris Male might\nknow more as he has really been the one digging in. ",
            "author": "Robert Muir",
            "id": "comment-12861574"
        },
        {
            "date": "2010-05-07T14:46:49+0000",
            "content": "I'll close this shortly, unless people object and want to use something from here. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12865179"
        },
        {
            "date": "2013-03-10T13:28:35+0000",
            "content": "SPRING_CLEANING_2013 We can reopen if necessary. ",
            "author": "Erick Erickson",
            "id": "comment-13598243"
        }
    ]
}