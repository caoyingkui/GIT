{
    "id": "SOLR-7256",
    "title": "Multiple data dirs",
    "details": {
        "components": [],
        "type": "New Feature",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "4.10.3",
        "status": "Closed",
        "resolution": "Won't Fix",
        "priority": "Major"
    },
    "description": "Request to support multiple dataDirs as indexing a large collection fills up only one of many disks in modern servers (think colocating on Hadoop servers with many disks).\n\nWhile HDFS is another alternative, it results in poor performance and index corruption under high online indexing loads (SOLR-7255).\n\nWhile it should be possible to do multiple cores with different dataDirs, that could be very difficult to manage and not humanly scale well, so I think Solr should support use of multiple dataDirs natively.\n\nRegards,\n\nHari Sekhon\nhttp://www.linkedin.com/in/harisekhon",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2015-03-17T12:02:15+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "It is already possible. You can use the addreplica Collection API with a custom dataDir.\n\nAh I see you have already seen the collection API, my bad. What are you expecting of Solr? Do you want to e.g. configure multiple data dir paths for a Solr node and have Solr allocate shards taking that into account? ",
            "id": "comment-14365027"
        },
        {
            "date": "2015-03-17T12:21:48+0000",
            "author": "Hari Sekhon",
            "content": "In solrconfig.xml I would like to be able to provide multiple comma separated dataDir paths as you would in say Hadoop and have it use the space on all of those disks equally (assuming that every directory specified is a separate disk - this is how Hadoop does it).\n\nThis way we would only deploy / manage 1 replica instance per node using the normal tooling and it would simply follow the pre-configured solrconfig.xml to utilize all the different disks and space.\n\nThe one problem I can see with this is that in Hadoop the configs are stored on local directories eg /etc/hadoop/conf but in SolrCloud they are stored in ZooKeeper, effectively forcing the same configuration down on all nodes, which may or may not have the same disks available (and quite likely one disk may fail requiring the config to exclude it).\n\nThe workaround to that would be to use a variable\n\n${solr.data.dir:}\n\nand have some kind of local /etc/solr/solr-env.sh that contains the variable uniquely configurable per node if needed. ",
            "id": "comment-14365047"
        },
        {
            "date": "2015-03-17T12:52:16+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "\nIn solrconfig.xml I would like to be able to provide multiple comma separated dataDir paths as you would in say Hadoop and have it use the space on all of those disks equally (assuming that every directory specified is a separate disk - this is how Hadoop does it).\n\nOkay, well maybe not solrconfig.xml but we can figure out where this configuration lives.\n\n\nThis way we would only deploy / manage 1 replica instance per node using the normal tooling and it would simply follow the pre-configured solrconfig.xml to utilize all the different disks and space.\n\nIsn't that kind of thing solved by RAID configurations? I can see a case for shard allocation across disks but what you are describing is either 1) spreading index files across multiple directories 2) having micro-shards managed as-if it was just one core. Both are quite impractical, imo. The former has problems such as atomically locking the index such that only one writer is active and also that different index files have different sizes so spreading them around is a problem. The latter is just a lot of surgery of Solr internals which doesn't have any benefits over creating multiple shards. This sort of thing is easier for hadoop because the block size is fixed. ",
            "id": "comment-14365075"
        },
        {
            "date": "2015-03-17T13:33:58+0000",
            "author": "Mark Miller",
            "content": "Yeah, I don't think we want to add any complexity here. There are much better ways to stripe across disks outside of Solr. ",
            "id": "comment-14365106"
        },
        {
            "date": "2015-03-17T13:52:56+0000",
            "author": "Hari Sekhon",
            "content": "RAID is fine if you're doing nothing but a purpose built SolrCloud... but one of the best use cases right now is SolrCloud co-located with Hadoop where there is a JBOD of multiple disks that you can't utilize the storage from and manage well without this feature.\n\nPerhaps a workaround would be to add better tooling for multiple shard replicas per node, one per disk? However this goes back to the different sizes problem as shards can end up being not that well balanced.\n\nWith regards to locking across disks, the two options are 1) Solr locks a file (can be any location/disk) and then controls the disk writes across all the disks, or 2) Solr acquires a lock per dataDir as Hadoop does. ",
            "id": "comment-14365136"
        },
        {
            "date": "2015-03-20T11:45:08+0000",
            "author": "Hari Sekhon",
            "content": "Btw Elasticsearch has multiple data dirs so I replaced my SolrCloud deployment with Elasticsearch yesterday as it solved this data distribution and other issues around scaling. ",
            "id": "comment-14371178"
        },
        {
            "date": "2016-06-06T03:29:37+0000",
            "author": "rex0zhao",
            "content": "user how to decide which shard put in which disk? why solr could't offer this auto decide feature to creeate in one of multi disks. ",
            "id": "comment-15316161"
        },
        {
            "date": "2016-06-07T14:22:25+0000",
            "author": "Jan H\u00f8ydahl",
            "content": "Resolving as won't fix. Viable solution are RAID or HDFS ",
            "id": "comment-15318549"
        },
        {
            "date": "2016-06-07T14:32:35+0000",
            "author": "Hari Sekhon",
            "content": "FYI this was co-located on a Hadoop cluster where Raid would have meant destroying the existing hdfs data and making it unsuitable for Hadoop cluster node usage and conversely storing the indices on HDFS resulted in severe performance degradation, eg. SOLR-7393 - which is why the Elastic.co folks never wanted to put their indices on HDFS as they had reported similar performances issues. ",
            "id": "comment-15318574"
        },
        {
            "date": "2016-06-07T19:16:54+0000",
            "author": "Jan H\u00f8ydahl",
            "content": "Hari, I closed this issue due to inactivity for a year, and it appeared you were happy with switching to ES. But feel free to re-open if you want to revive this issue and contribute towards finding elegant ways to utilize multiple disks natively.\n\nIf Solr was to spread the data, a first step could be to automatically spread new cores across multiple disks, if multiple disks are configured. Another more complicated way could perhaps be a MultiFSDirectoryFactory which always writes new index segments to the disk with the most free space, and does the book-keeping of where it put the files. If that is at all practically doable, it would be a non-intrusive solution that people could plug in if needed and otherwise leave out? ",
            "id": "comment-15319166"
        }
    ]
}