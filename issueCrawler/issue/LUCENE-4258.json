{
    "id": "LUCENE-4258",
    "title": "Incremental Field Updates through Stacked Segments",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Reopened"
    },
    "description": "Shai and I would like to start working on the proposal to Incremental Field Updates outlined here (http://markmail.org/message/zhrdxxpfk6qvdaex).",
    "attachments": {
        "LUCENE-4258.branch.6.patch": "https://issues.apache.org/jira/secure/attachment/12584865/LUCENE-4258.branch.6.patch",
        "LUCENE-4258.branch.4.patch": "https://issues.apache.org/jira/secure/attachment/12582868/LUCENE-4258.branch.4.patch",
        "LUCENE-4258.r1412262.patch": "https://issues.apache.org/jira/secure/attachment/12554675/LUCENE-4258.r1412262.patch",
        "LUCENE-4258.r1416438.patch": "https://issues.apache.org/jira/secure/attachment/12555736/LUCENE-4258.r1416438.patch",
        "LUCENE-4258.r1422495.patch": "https://issues.apache.org/jira/secure/attachment/12561181/LUCENE-4258.r1422495.patch",
        "LUCENE-4258-API-changes.patch": "https://issues.apache.org/jira/secure/attachment/12544627/LUCENE-4258-API-changes.patch",
        "LUCENE-4258.branch3.patch": "https://issues.apache.org/jira/secure/attachment/12581342/LUCENE-4258.branch3.patch",
        "LUCENE-4258.r1410593.patch": "https://issues.apache.org/jira/secure/attachment/12553960/LUCENE-4258.r1410593.patch",
        "IncrementalFieldUpdates.odp": "https://issues.apache.org/jira/secure/attachment/12544626/IncrementalFieldUpdates.odp",
        "LUCENE-4258.r1416617.patch": "https://issues.apache.org/jira/secure/attachment/12555823/LUCENE-4258.r1416617.patch",
        "LUCENE-4258.r1423010.patch": "https://issues.apache.org/jira/secure/attachment/12561307/LUCENE-4258.r1423010.patch",
        "LUCENE-4258.branch.2.patch": "https://issues.apache.org/jira/secure/attachment/12574133/LUCENE-4258.branch.2.patch",
        "LUCENE-4258.branch.5.patch": "https://issues.apache.org/jira/secure/attachment/12582916/LUCENE-4258.branch.5.patch",
        "LUCENE-4258.branch.1.patch": "https://issues.apache.org/jira/secure/attachment/12566293/LUCENE-4258.branch.1.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-07-26T13:33:15+0000",
            "content": "A few things I dont understand:\n\n\twhen updating a document, how do you know which terms to apply negative postings to?\n\thow can the idea of updating \"individual terms\" work as far as length normalization information? How will that be reconciled?\n\n\n\nIn truth I think term is too fine-grained of a level for updates in lucene because of norms, \nonly updating whole fields of a document will really work (as then the norm can simply be recomputed and replaced). \n ",
            "author": "Robert Muir",
            "id": "comment-13423055"
        },
        {
            "date": "2012-07-26T15:16:40+0000",
            "content": "There is more to it than just the referenced email. I've had a couple of discussions in the past about this with various people (and it is my fault that I didn't wrote them down and shared them with the rest of you) \u2013 I'll try to summarize below a more detailed proposal:\n\nAPI\nAdd an updateFields method which takes a Constraint and OP (eventually, it might replace today's updateDocument):\n\n\tConstraint defines 'which documents' should be updated, and follows today's deleteDocument API (takes Term, Query and arrays of each)\n\tOP defines the actual update to do on those documents:\n\t\n\t\tIt has a TYPE, with 3 values (At least for now):\n\t\t\n\t\t\tREPLACE_DOC \u2013 replaces an entire document (essentially what updateDocument is today)\n\t\t\tUPDATE_FIELD \u2013 incrementally update a field\n\t\t\tREPLACE_FIELD \u2013 replaces a field entirely\n\t\t\n\t\t\n\t\tIn addition, it takes a Field[] (or Iterable) to remove/add.\n\t\tIn light of the recent changes to IndexableField and friends, perhaps what it should take is a concrete UpdateField with a boolean specifying whether to add/remove its content. Suggestions are welcome !\n\t\n\t\n\n\n\nImplementation\nThe idea is to create StackedSegments, which, well, stack on top of current segments. The inspiration came from deletes, which can be viewed as a segment stacked on an existing segment, that marks which documents are deleted.\n\nFollowing that semantics, a segment could be comprised of these files:\n\n\tLayer 1: _0.prx, _0.fnm, _0.fdt ...\n\tLayer 2: _0_1.prx, _0_1.fdt (no updates to .fnm) \u2013 override/merge info from layer 1\n\tLayer 3: _0_2.prx  \u2013 override/merge info from layer 2\n\tLayer 4: _0_1.del \u2013 deletes are always the last layer, irregardless of their 'layer id' \u2013 _0_1.del overrides everything, even _0_100.prx.\n\t\n\t\tAnd they can be stacked on themselves as today, e.g. _0_2.del etc.\nI believe that we'll need an UpdateCodec or something ... this is the part of the internal API that we still need to understand better. Help from folks like you Robert will be greatly appreciated !\n\t\n\t\n\n\n\nTwo options to encode the posting lists:\n\n\tfield:value --> +1, -5, +8, +12, -17 ... (simple, but cannot be encoded efficiently\n\t\n\t\t+field:value --> 1, 8, 12\n\t\t-field:value --> 5, 17\n\t\n\t\n\n\n\nIdeally, the way incremental updates will be applied will follow how deletes are applied today:\n\n\tAn update always applies to all documents that are flushed\n\tAnd to all documents currently in the RAM buffer\n\tBut never to documents that are indexed later\n\n\n\nAgain, this is an internal detail that I'd appreciate if someone can give us a pointer to where that happens in the code today (now with concurrent flushing). I remember PackedDeletes existed at some point, has that changed?\n\nIf it's a new Codec, then SegmentReader may not even need to change ...\n\nThe REPLACE_FIELD OP is tricky ... perhaps it's like how deletes are materialized on disk \u2013 as a sparse bit vector that marks the documents that are no longer associated with it ...\n\nI also think that we should introduce this feature in steps:\n\n\tSupport only fields that omit TFAP (i.e. DOCS_ONLY). This is very valuable for fields like ACL, TAGS, CATEGORIES etc.\n\t\n\t\tIdeally, the app would just need to say \"add/remove ACL:SHAI to/from document X\", rather than passing the entire list of ACLs every on every update operation.\n\t\tThis I believe is also the most common use case for incremental field updates\n\t\n\t\n\tSupport stored fields, whether as part of (1) or a follow-on, but adding TAG:LUCENE to the postings, but not the stored fields, is limiting ...\n\tSupport terms with positions, but no norms. What I'm thinking about are terms that store stuff in the payload, but don't care about the positions themselves. An example are the category dimensions of the facet module, which stores category ordinals in the payload\n\t\n\t\tPositions are tricky, and we'll need to do this carefully, I know. But I don't rule it out at this point.\n\t\n\t\n\tThen, support fields with norms. I get your concern Robert, and I agree it's a challenge, hence why I leave it to last. The scenario I have in mind is: a search engine that lets you comment on a result or tag it, and the comment/tag should be added to the document's 'catchall' field for later searches. I think it's a valuable scenario, and this is something I'd like to support. If we cannot find a way to deal with it and the norms, then I see two options:\n\t\n\t\tDocument a limitation to updating a field with norms, at your own risk.\n\t\tEnforce REPLACE_FIELD OP on fields with norms.\n\t\n\t\n\n\n\n\n\tSince norms are under DocValues now, maybe that's solvable, I don't know. At the moment I think that we have a lot to do before we worry about norms ...\n\n\n\n\n\tI also think that we should start with the simpler ADD_FIELD operation, and not support REMOVE_FIELD ... really to keep things simple at start.\n\n\n\nI suggest we do this work in a dedicated branch of course. Ideally, we can port everything to 4.x at some point, as I think most of the changes are internal details ... ",
            "author": "Shai Erera",
            "id": "comment-13423139"
        },
        {
            "date": "2012-07-26T15:40:50+0000",
            "content": "I don't think I'm sold on introducing the feature in steps.\n\nI think its critical for something of this magnitude that we figure out the design totally, up-front,\nso it will work for the major use-cases. I think its fine to implement in steps if we need though.\n\nHonestly I think we should throw it all out on the table and get to the real problems I think\nthat most people face today:\n\n\tFor many document sizes, use-cases (especially rapidly changing stuff): The real problem is not the\n  speed of lucene reindexing the document, its that the user must rebuild the entire document. Solr solved\n  this by providing an option where you just say \"update field X\" and internally it reindexes the\n  document from stored fields (for that feature to work, the whole thing must be stored). We shouldn't\n  discard the possibility of implementing cleaner support for a solution like this, which wouldnt \n  complicate indexwriter at all.\n\tA second problem (not solved by the above) is that many people are using scoring factors with a variety\n  of signals and these are changing often. I think unfortunately, people are often putting these in\n  a normal indexed field and uninverting these on the fieldcache, requiring the whole document to\n  be reindexed just because of how they implemented the scoring factor. People could instead solve this\n  by putting their apps primary key into a docvalues field, allowing them to keep these scoring factors\n  completely external to lucene (e.g. their own array or whatever), indexed by their own primary key. But\n  the problem is I think people want lucene to manage this, they don't want to implement themselves whats\n  necessary to make it consistent with commits etc.\n\n\n\nSo we can look at several approaches to solving this stuff. I feel like both these problems could be\nsolved via a contrib module without modifying indexwriter at all for many use cases: maybe better if\nwe go for more tight integration. And with those simple approaches I describe above, searching doesn't\nget any slower.\n\nBut if we really feel like we need a \"full incremental update API\" (i know there are a few use cases\nwhere it can help, I'm not discarding that), then I feel like there are a few things I want:\n\n\tI want scoring to be correct: this is a must. If we provide a incremental update API on IW and it doesnt\n  achieve the same thing as updateDocument today, then its broken. But I think its ok for things to \n  be temporarily off (as long as this is in a consistent way) until merging takes place, just like \n  deletes today.\n\tI want to know for any incremental update API, the cost to search performance.\n  I want to know, at what document size is any incremental update API actually faster than us just \n  reindexing the document internally, and how much faster is it? I also want us to consider that\n  compared to the slowdown in search performance. We should know what the tradeoffs are before committing \n  such APIs.\n\n\n\nI strongly feel like if we just add these incremental APIs to indexwriter without being careful about these\nthings, the end result could be that people use them without thinking and end out with slower search\nand worse relevance, thats why I am asking so many questions. ",
            "author": "Robert Muir",
            "id": "comment-13423159"
        },
        {
            "date": "2012-07-26T17:05:03+0000",
            "content": "I think it's ok if we introduced IFU for DOCS_ONLY at first, throwing exceptions otherwise. E.g., UpdateField override setOmitNorms and such and throws UOE... at first.\n\nEverything else will still work as it is today...\n\nCodecs didn't handle all segment files first... stored fields and such were added later. I do agree though that we should keep in mind the full range of scenarios.\n\nSorry for the short response, JIRA isn't great with smart phones: -). ",
            "author": "Shai Erera",
            "id": "comment-13423240"
        },
        {
            "date": "2012-07-26T17:26:15+0000",
            "content": "\nCodecs didn't handle all segment files first... stored fields and such were added later. I do agree though that we should keep in mind the full range of scenarios.\n\nI don't think thats really comparable at all, for two reasons:\n1. Codecs can be considered a \"rote\" refactoring of the XXXWriter in 3.x. I'm not trying to diminish the value but its just an introduced abstraction layer. Something like this is different in that its algorithmic.\n2. The fact that Codecs only handled postings at first wasn't easy to fix after they were introduced as postings-only. Once they handled postings initially, this was a significant refactoring.\n\nI'm not trying to pick on your proposal, I'm just saying there are things I don't like about the design.\n\n\tI think that updating individual terms is a fringe use-case, and not the major use case for incremental updates, which is to update the contents of one field, without reindexing the entire document. This was also noted by someone else on the discussion thread. This issue seems to be solely about supporting the 'tagging' use case, which is just one of many.\n\tI think requiring no positions, no frequencies, and no norms makes it even more fringe. This means its not really useful for any search purposes. And we are a search engine library.\n\tI think that negatives won't compress well, as in general compression algorithms for IR in the last years focus on positive integers.\n\tI think merging the postings will be slow: I don't like the tradeoff of slowing down searching so much for what I'm not even sure will be a significant speedup to indexing.\n\n ",
            "author": "Robert Muir",
            "id": "comment-13423258"
        },
        {
            "date": "2012-07-26T19:52:00+0000",
            "content": "...which is to update the contents of one field, without reindexing the entire document\n\nI agree, but I distinguish between two operations:\n\n\treplacing the content of a field entirely with a new content (or remove the field)\n\tupdate the field's content by adding/removing individual terms\n\n\n\nI think requiring no positions, no frequencies, and no norms makes it even more fringe. This means its not really useful for any search purposes. And we are a search engine library.\n\nI disagree. Where I come from, the most common use case where such operation will be useful is when a single change affects hundreds and sometimes thousands of documents. An example is a document library like application which manages folders with ACLs. You can add an ACL to a top-level folder and it affects the entire documents and folder beneath it. That results in reindexing, sometimes, a huge amount of documents.\n\nI don't diminish the use case of updating a field for scoring purposes, not at all. Just saying that starting by supporting one use case is more than supporting no use case.\n\nNow, and this probably stems from my lack of understanding of the Lucene internals \u2013 I see \"supporting terms that omit TFAP\" as a starting point because that is the easiest case, and even that requires a lot of understanding of the internals. After we do that, I'll feel more comfortable discussing other types of updates for other field types ... at least, I'll feel that I have more intelligent things to say .\n\nRegarding your other concerns, I share them with you, and we of course need to benchmark everything. I don't know how this affect search or not. But those updates will get merged away when segments are merged, so while I'm sure search will be affected, it's not for eternity - only until that segment is merged. And, I think we need to add capability to MergePolicy to findSegmentsForMergeUpdates, just like we expungeDeletes.\n\nIf the first step means that in order to update a field used for scoring (i.e. w/ norms) means that you need to replace the content of the field entirely by a new content, I'm ok with it. As one esteem member of this community always says \"progress, not perfection\" - I'm totally soled for that ! ",
            "author": "Shai Erera",
            "id": "comment-13423397"
        },
        {
            "date": "2012-07-27T11:10:37+0000",
            "content": "I don't think its progress if we add a design that can only work with omitTFAP and no norms,\nand can only update individual terms, but not fields.\n\nit means to support these things we have to also totally clear out whats there, and then introduce a new design\n\nIn fact this issue shouldnt be called incremental field updates: its not. its \"term updates\" or something else entirely different. ",
            "author": "Robert Muir",
            "id": "comment-13423798"
        },
        {
            "date": "2012-07-27T11:38:13+0000",
            "content": "can only update individual terms, but not fields\n\nWho said that? One of the update operations I listed is REPLACE_FIELD, which means replace the field's content entirely with the new content.\n\nI don't think its progress if we add a design that can only work with omitTFAP and no norms\n\nI never said that will be the design. What I said is that in order to update a field at the term level, we'll start with such fields only. The rest of the fields (i.e. w/ norms, payloads and what not) will be updated through REPLACE_FIELD. The way I see it, we still address all the issues, only for some fields we require a whole field replace, and not an optimized term-based update. That can be improved further along, or not.\n\nIn fact this issue shouldnt be called incremental field updates: its not. its \"term updates\" or something else entirely different.\n\nThat is my idea of incremental field updates and I'm not sure that it's not your idea as well . You seem to only want to support REPLACE_FIELD, while I say that for some field types we can support UPDATE_FIELD (i.e. at the term level), that's it ! ",
            "author": "Shai Erera",
            "id": "comment-13423813"
        },
        {
            "date": "2012-07-30T04:12:04+0000",
            "content": "I had a chat about this with Robert a couple of days ago, figured it'll be easier to discuss the differences in approaches/opinions, rather than back and forth JIRA comments. Our idea of incremental field updates is not much different. Robert stressed that in his opinion we should tackle first the REPLACE_FIELD operation, which replaces the content of a field entirely by a new content, because he believes that's the most common scenario (i.e., update the title field). I believe that term-based updates are very important too, at least in the scenarios that I face (i.e. adding/removing one ACL, one social tag, one category etc.).\n\nWe concluded that the design should take REPLACE_FIELD into consideration from the get go. Whether we'll also implement UPDATE_FIELD (or UPDATE_TERMS as a better name?) depends on the complexity of it. Because initially UPDATE_TERMS can be implemented through REPLACE_FIELD, so we don't lose functionality. UPDATE_TERMS can come later as an optimization.\n\nRobert, if I misrepresented our conclusions, please correct me. ",
            "author": "Shai Erera",
            "id": "comment-13424683"
        },
        {
            "date": "2012-07-30T07:54:43+0000",
            "content": "Seems like in any case we need to have a separation between fields given with UPDATE_FIELD and REPLACE_FIELD. There are two ways I could think of for implementing this separation. \n\nThe first is at the segment level, where we can have separate \"update\" and \"replace\" segments, where the semantic is that a field in an \"update\" segment is merged with fields in previous segments, while a field in a \"replace\" segment ignores previous segments.\n\nThe second option is to separate at the field level, choosing one type as the default behavior (maybe this can be configurable) and marking the fields of the non-default type by altering the field name or some other solution.\n\nI lean towards the segment level separation, since it requires less conventions and will probably require less work for Codec implementations to handle. ",
            "author": "Sivan Yogev",
            "id": "comment-13424736"
        },
        {
            "date": "2012-07-30T11:09:48+0000",
            "content": "BTW, since the new method is to handle multiple fields (as the name suggests), the operation descriptions should also be in plural: UPDATE_FIELDS and REPLACE_FIELDS. ",
            "author": "Sivan Yogev",
            "id": "comment-13424793"
        },
        {
            "date": "2012-07-30T11:53:34+0000",
            "content": "BTW, since the new method is to handle multiple fields (as the name suggests), the operation descriptions should also be in plural: UPDATE_FIELDS and REPLACE_FIELDS.\n\n+1\n\nI think this design sounds good!  REPLACE_FIELDS should easily be able\nto update norms correctly, right?  Because the full per-field stats\nare recomputed from scratch.  So then scores should be identical:\nshould be a nice simple testcase to create \n\nI don't see how UPDATE_FIELDS can do so unless we somehow save the raw\nstats (FieldInvertState) in the index.  It seems like UPDATE_FIELDS\nshould forever be limited to DOCS_ONLY, no norms updating?  Positions\nalso seems hard to update, and if the only reason to do so is for\npayloads... seems like the app should be using doc values instead, and\nwe should (eventually) make doc values updatable?.\n\nI do think this is a common use case (ACLs, filters, social\ntags)... though I'm not sure how bad it'd really be in practice for\nthe app to simply REPLACE_FIELDS with the full set of tags.  I guess\nif we build REPLACE_FIELDS first we can test that.\n\nThe implementation should be able to piggy-back on all the\nbuffering/tracking we currently do for buffered deletes.\n\nI think this change should live entirely above Codec?  Ie Codec just\nthinks it's writing a segment, not knowing if that segment is the base\nsegment, or one of the stacked ones.  If the +postings and -postings\nare simply 2 terms then the Codec need not know...\n\nSeems like only SegmentInfos needs to track how segments stack up, and\nthen I guess we'd need a new StackedSegmentReader that is atomic,\nholds N SegmentReaders, and presents the merged codec APIs by merging\ndown the stack on the fly?  I suspect this (having to use a PQ to\nmerge the docIDs in the postings) will be a huge search performance\nhit....\n\nI think UnionDocs/AndPositionsEnum (in MultiPhraseQuery.java) is\nalready doing what we want?  (Except it doesn't handle negative\npostings).\n\nWhat about merging?  Seems like the merge policy should know about\nstacking and should sometimes (aggressively?) merge a stack down? ",
            "author": "Michael McCandless",
            "id": "comment-13424803"
        },
        {
            "date": "2012-07-30T12:03:05+0000",
            "content": "How does this relate (if at all, I confess I just looked at the title) to Andrzej's proposal here? https://issues.apache.org/jira/browse/LUCENE-3837 ",
            "author": "Erick Erickson",
            "id": "comment-13424806"
        },
        {
            "date": "2012-07-30T14:02:09+0000",
            "content": "\nI don't see how UPDATE_FIELDS can do so unless we somehow save the raw\nstats (FieldInvertState) in the index. It seems like UPDATE_FIELDS\nshould forever be limited to DOCS_ONLY, no norms updating?\n\nActually its DOCS_ONLY plus OMIT_NORMS.\n\nAnyway why not start with updating the entire contents of a field as I suggested?\nIt seems to be the most general solution, and there is some discussion about how scoring can\nwork correctly on LUCENE-3837 (the stats, not just norms).\n\n\nI do think this is a common use case (ACLs, filters, social\ntags)... though I'm not sure how bad it'd really be in practice for\nthe app to simply REPLACE_FIELDS with the full set of tags. I guess\nif we build REPLACE_FIELDS first we can test that.\n\nThis is why we should do 'replace contents of a field' first. Its the most well-defined and general.\n\nIts also still controversial, myself I'm not convinced it will actually help most people that think\nthey want it, I think it will just slow down searches. ",
            "author": "Robert Muir",
            "id": "comment-13424870"
        },
        {
            "date": "2012-07-30T18:15:31+0000",
            "content": "BTW, since the new method is to handle multiple fields (as the name suggests), the operation descriptions should also be in plural: UPDATE_FIELDS and REPLACE_FIELDS.\n\nOk. I think to not confuse though, we should call it UPDATE_TERMS (not FIELDS). Then someone can updateFields() twice, once for all the fields which he wants to REPLACE and second for the fields he just wants to update their terms.\n\nWhat about merging?\n\nI wrote about it above \u2013 MergePolicy will need to take care of these stacked segments, and we'll add something like ,merge/expungeFieldUpdates so the app can call it deliberately.\n\nseems like the app should be using doc values instead, and we should (eventually) make doc values updatable?\n\nI agree we should not UPDATE_TERMS fields that record norms. I'm not sure that every use case of storing info in the payload today can be translated to using DocValues, so I don't want to limit things. So, let's start with UPDATE_TERMS taking care of fields that omit norms. Then, if we handle payload or not for few use cases, can become as an optimization later on. In the meanwhile, apps will just need to replace the entire field.\n\nProgress, not perfection !  ",
            "author": "Shai Erera",
            "id": "comment-13425079"
        },
        {
            "date": "2012-08-01T18:57:16+0000",
            "content": "How does this relate (if at all, I confess I just looked at the title) to Andrzej's proposal here?\n\nThe basic idea is the same. One major difference is that in Andrzej's proposal the stacked updates are added to a new index with different doc IDs, and then the SegmentReader needs to map to the original doc IDs. The plan in this proposal (Shai correct me if I'm wrong) is for the stacked updates not to be stand alone segments. Although they will have the structure of regular segments they will be tightly coupled with the original segment, with doc IDs matching those of the original segment. ",
            "author": "Sivan Yogev",
            "id": "comment-13426821"
        },
        {
            "date": "2012-08-07T12:49:20+0000",
            "content": "Working on the details, it seems that we need to add a new layer of information for stacked segments. For each field that was added with REPLACE_FIELDS, we need to hold the documents in which a replace took place, with the number of the latest generation that had the replacement. Name this list the \"generation vector\". That way, TermDocs provided by StackedSegmentReader for a certain term is a special merge of that term's TermDocs for all stacked segments. The \"special\" part about it is that we ignore occurrences from documents in which the term's field was replaced in a later generation.\n\nAn example. Assume we have doc 1 with title \"I love bananas\" and doc 2 with title \"I love oranges\", and the segment is flushed. We will have the following base segment (ignoring positions):\n\nbananas: doc 1\nI: doc1, doc 2\nlove: doc 1, doc 2\noranges: doc2\n\nNow we add to doc 1 additional title field \"I hate apples\", and replace the title of doc 2 with \"I love lemons\", and flush. We will have the following segment for generation 1:\n\napples: doc 1\nhate: doc 1\nI: doc 1, doc 2\nlemons: doc 2\nlove: doc 2\ngeneration vector for field \"title\": (doc 2, generation 1)\n\nTermDocs for a few terms: \n\n\ttitle:bananas : \n{1}\n, uses the TermDocs of the base segment and not affected by the field title generation vector.\n\ttitle:oranges : {}, uses the TermDocs of the base segment, doc 2 title affected for generations < 1, and the generation is 0.\n\ttitle:lemons : \n{2}\n, uses the TermDocs of generation 1. Doc 2 title affected for generations < 1, but the term appears in generation 1.\n\ttitle:love : \n{1,2}\n, uses the TermDocs of both segments. Doc 2 title affected for generations < 1, but the term appears in generation 1.\n\n\n\nI propose to initially use PackedInts for the generation vector, since we know how many generations the curent segment has upon flushing. Later we might consider special treatment for sparse vectors. ",
            "author": "Sivan Yogev",
            "id": "comment-13430322"
        },
        {
            "date": "2012-09-11T11:08:23+0000",
            "content": "Adding a design proposal presentation, and two patches following the proposal concepts. The first patch includes proposed API changes (does not compile) for, and the other one inner changes for those interested in the implementation details. The second patch contains a new test named TestFieldsUpdates which currently fails. ",
            "author": "Sivan Yogev",
            "id": "comment-13452918"
        },
        {
            "date": "2012-09-11T11:21:36+0000",
            "content": "Forgot to mention that the implementation patch still missing many components... ",
            "author": "Sivan Yogev",
            "id": "comment-13452928"
        },
        {
            "date": "2012-09-11T14:23:11+0000",
            "content": "On slide 4 one of the enumerated operations is field deletion but I am not sure how to do it with the proposed API on slide 5?\n\nIt is just a tought, but your work plan only mentions Lucene fields. Wouldn't it be easier to start working with DocValues? I guess it would help us get started with document updates and would already solve most use-cases (I'm especially thinking of scoring factors). ",
            "author": "Adrien Grand",
            "id": "comment-13453050"
        },
        {
            "date": "2012-09-12T05:50:10+0000",
            "content": "We will take care of DocValues too, eventually. I think this can be handled in a separate issue though.\n\nand would already solve most use-cases \n\nI have a problem with that statement. Robert thinks that the most common use cases are to replace a field's content entirely. In our world (Sivan and mine's), updating a field's terms (removing / adding single terms) is the most common use case. And perhaps in your world updating DocValues for scoring purposes is the most common use case.\n\nTherefore I don't think that there is one common use case, and therefore IMO we shouldn't aim at solving one first. Personally, DocValues are relatively new (compared to posting lists and payloads) and therefore I believe that being able to update them should come second (just because I estimate they are not as widely used as the others). But that's just my opinion \u2013 obviously one that relies solely on DocValues would state otherwise .\n\nThe design currently doesn't cover DocValues at all. I think, in order to keep this issue focused, we should handle that in a separate issue after we land updateable fields.\n\nOn slide 4 one of the enumerated operations is field deletion but I am not sure how to do it with the proposed API on slide 5?\n\nGood point. Well, you could say that replaceField(\"f\", \"value\") called as replaceField(\"f\", null) would mean \"delete 'f'\". That should work, but perhaps we can come up with something more explicit. ",
            "author": "Shai Erera",
            "id": "comment-13453739"
        },
        {
            "date": "2012-10-26T21:52:47+0000",
            "content": "New patch, naive test of adding updates to a single-document segment before or after update working. Working on more complex tests with multiple segments, documents and updates. ",
            "author": "Sivan Yogev",
            "id": "comment-13485229"
        },
        {
            "date": "2012-11-18T10:39:57+0000",
            "content": "New patch implementing some previously missing parts, with preliminary code to enable field replacements. ",
            "author": "Sivan Yogev",
            "id": "comment-13499681"
        },
        {
            "date": "2012-11-22T13:02:57+0000",
            "content": "New patch with additional testing and bug fixes. \n\nCurrently the term statistics does not take into account field replacements, and therefore term counts are wrong and CheckIndex fails. \n\nI can think of two possible solutions for this. The first is for CheckIndex to identify updated segments and ignore term statistics - is there similar mechanism for deletions?\n\nThe other solution is to pre-compute term statistics for updated segments. However, this will be costly - requires going through the entire posting list for every term, and count non-replaced occurrences.\n\nAny suggestions? ",
            "author": "Sivan Yogev",
            "id": "comment-13502756"
        },
        {
            "date": "2012-12-03T10:53:29+0000",
            "content": "New patch. This patch contains some failing tests, some probably due to problems in my implementation of SegmentReader.getTermVectors(int), and others in handling of stored fields. \n\nCan anyone with knowledge of these two areas check what is it that I do wrong? Thanks. ",
            "author": "Sivan Yogev",
            "id": "comment-13508643"
        },
        {
            "date": "2012-12-03T13:10:09+0000",
            "content": "Solved the problem with stored fields, I understood that in order to have the right number of documents for the stored fields reader the last document must have a stored field. Term Vectors still failing... ",
            "author": "Sivan Yogev",
            "id": "comment-13508709"
        },
        {
            "date": "2012-12-03T16:24:14+0000",
            "content": "Guys:\n\nIt's great that you're tackling this. I wanted to encourage you to be patient about responses, there are just a few people in the universe who understand the details well enough to comment on the code (I'm sure not one of them!) , so it might feel like you're shouting down a well.... ",
            "author": "Erick Erickson",
            "id": "comment-13508831"
        },
        {
            "date": "2012-12-03T20:34:03+0000",
            "content": "Patch with stored fields bug fixed. ",
            "author": "Sivan Yogev",
            "id": "comment-13509020"
        },
        {
            "date": "2012-12-03T23:15:32+0000",
            "content": "Trying to catch up here ... I just have a bunch of random questions\n(don't fully understand the patch yet):\n\nNot sure why some files show as all deleted / all added lines, eg at\nleast FrozenBufferedDeletes.java.\n\nPatch also has tabs, which should be spaces... (eg IndexWriter.java).\n\nWhy do we have FieldsUpdate.Operation.ADD_DOCUMENT?  It seems weird to\npass that to IW.updateFields?  Shouldn't apps just use\nIW.addDocument?\n\nWhy do we need SegmentInfoReader.readFilesList?  It seems like it's\nonly privately used inside the codec?  I'm confused why the \"normal\"\nfile tracking we have on write is insufficient... oh I see, a single\nSegmentInfo references all stacked segments too?  But since they are\nwritten \"later\" their files won't be automatically tracked ... ok.  I\nwonder if each stacked segment should get its own SegmentInfo, linked\nto the base segment...\n\nIt looks like merge policies don't yet know about / target stacked\nsegments ...\n\nIt seems like we don't invert the document updates until the updates\nare applied?  Ie, we just buffer the IndexableField provided by the\nuser and when it's time to apply updates, we then analyzing/invert?\nHow do we track RAM in this case?  (Eg the field could be something\narbitrary, eg pre-tokenized).  Another option is to do the invert and\nbuffer the resulting postings, and then later \"replay\" them (remapping\ndocIDs) when it's time to apply.\n\nWhy does StoredFieldsReader.visitDocument need a Set for ignored\nfields? ",
            "author": "Michael McCandless",
            "id": "comment-13509314"
        },
        {
            "date": "2012-12-05T11:57:31+0000",
            "content": "Erick - thanks for the support!\n\nMike - thanks for the comments, here's an attempt to supply answers:\n\nRegarding formatting and all deletes - I will check and try to fix those.\n\n\nWhy do we have FieldsUpdate.Operation.ADD_DOCUMENT? It seems weird to\npass that to IW.updateFields? Shouldn't apps just use\nIW.addDocument?\nWe have ADD_ and REPLACE_ for FIELDS, and also REPLACE_DOCUMENTS, so having ADD_DOCUMENT would allow applications to work only with updateFields. There certainly are actions that can be performed in more than one way in this API, do you find this too confusing?\n\n\nWhy do we need SegmentInfoReader.readFilesList? ...\nI considered the alternative you propose of having a segmentInfo for each stacked segment, and it seemed too complex to manage than what is done with .del files, so I chose the .del files approach. You are right about it's privacy, I removed it from SegmentInfoReader and the actual readers have it privately.\n\n\nIt looks like merge policies don't yet know about / target stacked\nsegments ...\nI was planning to have it in another issue. should I create it already?\n\n\nIt seems like we don't invert the document updates until the updates\nare applied? ...\nI went for the simple solution trying to introduce as less new concepts as possible (and still the patch size is >7000 lines). Your proposal should certainly be considered and maybe tested. I need to make sure I do the RAM calculations right, the added documents must be reflected in the RAM consumption of the deletions queue.\n\n\nWhy does StoredFieldsReader.visitDocument need a Set for ignored\nfields?\nWhen fetching stored fields from a segment with replacements, it is possible that all contents of a certain field for the base and first n stacked segments should be ignored. Therefore, the implementation starts the visiting from the most recent updates. If we encounter at some stage a field replacement, that field name is added to the Set of ignored fields, and later the content of that field in the stacked segments we encounter (which are older updates) is ignored. ",
            "author": "Sivan Yogev",
            "id": "comment-13510441"
        },
        {
            "date": "2012-12-07T18:29:35+0000",
            "content": "\nWhy do we have FieldsUpdate.Operation.ADD_DOCUMENT? It seems weird to pass that to IW.updateFields? Shouldn't apps just use IW.addDocument?\n\nWe have ADD_ and REPLACE_ for FIELDS, and also REPLACE_DOCUMENTS, so having ADD_DOCUMENT would allow applications to work only with updateFields. There certainly are actions that can be performed in more than one way in this API, do you find this too confusing?\n\nWell I just generally prefer that there is one [obvious] way to do\nsomething ... it can cause confusion otherwise, ie users will wonder\nwhat's the difference between addDocument and\nupdateFields(Operation.ADD_DOCUMENT, ...)\n\n\nWhy do we need SegmentInfoReader.readFilesList? ...\n\nI considered the alternative you propose of having a segmentInfo for each stacked segment, and it seemed too complex to manage than what is done with .del files, so I chose the .del files approach. You are right about it's privacy, I removed it from SegmentInfoReader and the actual readers have it privately.\n\nOK.\n\n\nIt looks like merge policies don't yet know about / target stacked segments ...\n\nI was planning to have it in another issue. should I create it already?\n\nAnother issue is a good idea!  No need to create it yet ... but it\nseems like it will be important for real usage.\n\nDo we have any sense of how performance degrades as the stack gets\nbigger?  It's more on-the-fly merging at search-time...\n\nI'm worried about that search-time merge cost ... I think it's usually\nbetter to pay a higher indexing cost in exchange for faster search\ntime, which makes LUCENE-4272 a compelling alternate approach...\n\n\nIt seems like we don't invert the document updates until the updates are applied? ...\n\nI went for the simple solution trying to introduce as less new concepts as possible (and still the patch size is >7000 lines). Your proposal should certainly be considered and maybe tested. I need to make sure I do the RAM calculations right, the added documents must be reflected in the RAM consumption of the deletions queue.\n\nOK that makes sense; we should definitely do whatever's\neasiest/fastest to get to a dirt path.\n\nWe should think through the tradeoffs.  I think it may confuse apps\nthat the Field is not \"consumed\" after IW.updateFields returns, but\nrather cached and processed later.  This means you cannot reuse\nfields, you have to be careful with pre-tokenized fields (can't reuse\nthe TokenStream), etc.\n\nIt also means NRT reopen is unexpectedly costly, because only on flush\nwill we invert & index the documents, and it's a single-threaded\noperation during reopen (vs per-thread if we invert up front).\n\nStill it makes sense to do this for starters ... it's simpler.\n\n\nWhy does StoredFieldsReader.visitDocument need a Set for ignored fields?\n\nWhen fetching stored fields from a segment with replacements, it is possible that all contents of a certain field for the base and first n stacked segments should be ignored. Therefore, the implementation starts the visiting from the most recent updates. If we encounter at some stage a field replacement, that field name is added to the Set of ignored fields, and later the content of that field in the stacked segments we encounter (which are older updates) is ignored.\n\nAhhh right.\n\nAre stored fields now sparse?  Meaning if I have a segment w/ many\ndocs, and I update stored fields on one doc, in that tiny stacked\nsegments will the stored fields files also be tiny? ",
            "author": "Michael McCandless",
            "id": "comment-13526609"
        },
        {
            "date": "2012-12-16T11:16:28+0000",
            "content": "Let me start with the last question.\n\nAre stored fields now sparse? Meaning if I have a segment w/ many docs, and I update stored fields on one doc, in that tiny stacked segments will the stored fields files also be tiny?\n\nIn such case you will get the equivalent of a segment with multiple docs with only one of them containing stored fields. I assume the impls of stored fields handle these cases well and you will indeed get tiny stored fields.\n\nRegarding the API - I made some cleanup, and removed also Operation.ADD_DOCUMENT. Now there is only one way to perform each operation, and updateFields only allows you to add or replace fields given a term.\n\nThis means you cannot reuse fields, you have to be careful with pre-tokenized fields (can't reuse the TokenStream), etc.\n\nThis is referred in the Javadoc of updateFields, let me know if there's a better way to address it.\n\nAs for the heavier questions. NRT support should be considered separately, but the guideline I followed was to keep things as closely as possible to the way deletions are handled. Therefore, we need to add to SegmentReader a field named liveUpdates - an equivalent to liveDocs. I already put a TODO for this (SegmentReader line 131), implementing it won't be simple...\n\nThe performance tradeoff you are rightfully concerned about should be handled through merging. Once you merge an updated segment all updates are \"cleaned\", and the new segment has no performance issues. Apps that perform updates should make sure (through MergePolicy) to avoid reader-side updates as much as possible.\n ",
            "author": "Sivan Yogev",
            "id": "comment-13533359"
        },
        {
            "date": "2012-12-16T11:37:15+0000",
            "content": "Patch with some additional bug fixes and more elaborate tests, all working. Ready to commit? ",
            "author": "Sivan Yogev",
            "id": "comment-13533361"
        },
        {
            "date": "2012-12-16T15:38:46+0000",
            "content": "Some existing tests fail with the latest patch, working on fixes. ",
            "author": "Sivan Yogev",
            "id": "comment-13533410"
        },
        {
            "date": "2012-12-17T16:38:18+0000",
            "content": "New patch, concurrency bugs fixed. All tests pass. ",
            "author": "Sivan Yogev",
            "id": "comment-13534054"
        },
        {
            "date": "2012-12-17T22:04:20+0000",
            "content": "Its exciting to see progress here!  I too live in the \"world\" that Shai speaks of \u2013 DOCS_ONLY (w/ no norms).  I don't need to update a title field, I need to update ACLs and various categorical \"tag\" fields that will subsequently influence faceting or filtering.\n\nHey Rob, early on you made this excellent point:\nA second problem (not solved by the above) is that many people are using scoring factors with a variety\nof signals and these are changing often. I think unfortunately, people are often putting these in\na normal indexed field and uninverting these on the fieldcache, requiring the whole document to\nbe reindexed just because of how they implemented the scoring factor. People could instead solve this\nby putting their apps primary key into a docvalues field, allowing them to keep these scoring factors\ncompletely external to lucene (e.g. their own array or whatever), indexed by their own primary key. But\nthe problem is I think people want lucene to manage this, they don't want to implement themselves whats\nnecessary to make it consistent with commits etc.\n\nSo true.  What if Lucene had more hooks to make it easier to manage commit-consistency with side-car data?  I have no clue what's needed exactly, only that I don't dare do this without such hooks because I fear the complexity.  With hooks and documentation, it can become clear how to maintain data alongside Lucene's index, and this opens doors.  Like making it easier to store data in something custom (e.g. a DB) instead of stored-fields (won't have to pay needless merge cost), or putting metrics that influence scoring somewhere as you hinted at above.  ",
            "author": "David Smiley",
            "id": "comment-13534352"
        },
        {
            "date": "2012-12-18T12:43:24+0000",
            "content": "\nAre stored fields now sparse? Meaning if I have a segment w/ many docs, and I update stored fields on one doc, in that tiny stacked segments will the stored fields files also be tiny?\n\nIn such case you will get the equivalent of a segment with multiple docs with only one of them containing stored fields. I assume the impls of stored fields handle these cases well and you will indeed get tiny stored fields.\n\nYou're right, this is up to the codec ... hmm but the API isn't sparse (you have\nto .addDocument 1M times to \"skip over\" 1M docs right?), and I'm not sure how well our\ncurrent default (Lucene41StoredFieldsFormat) handles it.  Have you tested it?\n\nRegarding the API - I made some cleanup, and removed also Operation.ADD_DOCUMENT. Now there is only one way to perform each operation, and updateFields only allows you to add or replace fields given a term.\n\nOK thanks!\n\n\nThis means you cannot reuse fields, you have to be careful with pre-tokenized fields (can't reuse the TokenStream), etc.\n\nThis is referred in the Javadoc of updateFields, let me know if there's a better way to address it.\n\nMaybe also state that one cannot reuse Field instances, since the\nField may not be actually \"consumed\" until some later time (we should\nbe vague since this really is an implementation detail).\n\nAs for the heavier questions. NRT support should be considered separately, but the guideline I followed was to keep things as closely as possible to the way deletions are handled. Therefore, we need to add to SegmentReader a field named liveUpdates - an equivalent to liveDocs. I already put a TODO for this (SegmentReader line 131), implementing it won't be simple...\n\nOK ... yeah it's not simple!\n\nThe performance tradeoff you are rightfully concerned about should be handled through merging. Once you merge an updated segment all updates are \"cleaned\", and the new segment has no performance issues. Apps that perform updates should make sure (through MergePolicy) to avoid reader-side updates as much as possible.\n\nMerging is very important.  Hmm, are we able to just merge all updates\ndown to a single update?  Ie, without merging the base segment?  We\ncan't express that today from MergePolicy right?  In an NRT setting\nthis seems very important (ie it'd be best bang (= improved search\nperformance) for the buck (= merge cost)).\n\nI suspect we need to do something with merging before committing\nhere.\n\nHmm I see that\nStackedTerms.size()/getSumTotalTermFreq()/getSumDocFreq() pulls a\nTermsEnum and goes and counts/aggregates all terms ... which in\ngeneral is horribly costly?  EG these methods are called per-query to\nsetup the Sim for scoring ... I think we need another solution here\n(not sure what).  Also getDocCount() just returns -1 now ... maybe we\nshould only allow updates against DOCS_ONLY/omitsNorms fields for now?\n\nHave you done any performance tests on biggish indices?\n\nI think we need a test that indexes a known (randomly generated) set\nof documents, randomly sometimes using add and sometimes using\nupdate/replace field, mixing in deletes (just like TestField.addDocuments()),\nfor the first index, and for the second index only using addDocument\non the \"surviving\" documents, and then we assertIndexEquals(...) in the\nend?  Maybe we can factor out code from TestDuelingCodecs or\nTestStressIndexing2.\n\nWhere do we account for the RAM used by these buffered updates?  I see\nBufferedUpdates.addTerm has some accounting the first time it sees a\ngiven term, but where do we actually add in the RAM used by the\nFieldsUpdate itself? ",
            "author": "Michael McCandless",
            "id": "comment-13534859"
        },
        {
            "date": "2012-12-20T12:25:32+0000",
            "content": "After rethinking the point-of-inversion issue, seems like the right time to do it is ASAP - not to hold the added fields and invert them later, but rather invert them immediately and save their inverted version. 3 reasons for that:\n1. Take out the constraint I inserted to the API, so update fields can be reused and contain Reader/TokenStrem,\n2. NRT support: we cannot search until we invert, and if we invert earlier NRT support will be less complicated, probably some variation on multi-reader to view uncommitted updates,\n3. You are correct that we currently do not account for the RAM usage of the FieldsUpdate, since I thought using RAMUsageEstimator will be too costly. It will probably be more efficient to calculate RAM usage of the inverted fields, maybe even during inversion?\n\nSo my question in that regard is how can I invert a document and hold its inverted form to be used by NRT and later inserted into stacked segment? Should I create a temporary Directory and invert into it? Is there another way to do this?\n\nMerging is very important. Hmm, are we able to just merge all updates down to a single update? Ie, without merging the base segment? We can't express that today from MergePolicy right? In an NRT setting this seems very important (ie it'd be best bang (= improved search performance) for the buck (= merge cost)).\n\nShai is helping in creation of a benchmark to test performance in various scenarios. I will start adding updates aspects to the merge policy. I am not sure if merging just updates of a segment is feasible. In what cases would it be better than collapsing all updates into the base segment?\n\nI think we need a test that indexes a known (randomly generated) set of documents, randomly sometimes using add and sometimes using update/replace field, mixing in deletes (just like TestField.addDocuments()), for the first index, and for the second index only using addDocument on the \"surviving\" documents, and then we assertIndexEquals(...) in the end? Maybe we can factor out code from TestDuelingCodecs or TestStressIndexing2.\n\nTestFieldReplacements already had a test which randomly adds documents, replaces documents, adds fields and replaces fields. I refactored it to enable using a seed, and created a \"clean\" version with only addDocument(...) calls. However, the FieldInfos of the \"clean\" version do not include things that the \"full\" version includes because in the full version fields possessing certain field traits where added and then deleted. I will look at the other suggestions. ",
            "author": "Sivan Yogev",
            "id": "comment-13536992"
        },
        {
            "date": "2012-12-20T12:44:07+0000",
            "content": "I am not sure if merging just updates of a segment is feasible. In what cases would it be better than collapsing all updates into the base segment?\n\nJust like expungeDeletes, I think that we should have collapseFieldUpdates() which can be called explicitly by the app, but also IW should call MP.findSegmentsForFieldUpdates() (or some such name). And it should collapse all updates into the segment, implies rewriting that segment. If we collapse all updates but keep the base segment + a single stacked segment, I don't think that we're doing much. The purpose is to get rid of updates entirely.\n\nAlso, regarding statistics. I think that as a first step, we should not go out of our way to return the correct statistics. Just like the stats today do not account for deleted documents, so should the updates. I realize that it's not the same as deleted documents, but it certainly simplifies matters. Stats will be correct following collapseFieldUpdates or regular segment merges.\n\nAs a second step, we can try to return statistics including stacked segments more efficiently. I.e., if a term appears in both the base and stacked segment, we return the stats from base. But if it exists only in the stacked segment, we can return the stats from there? I'm not too worried about the stats though, because that's a temporary thing, which gets fixed once updates are collapsed.\n\nAnd if the MergePolicy will have separate settings for collapsing field updates (I think it should!), then the collapsing could occur more frequently than regular merges (and expunging deleted documents). Also, it will give apps a way to control how often do they want to get accurate statistics.\n\nCan we leave statistics outside the scope of this issue? And for now change CheckIndex to detect that it's a segment with field updates, and therefore check stats from the base segment only? I think it does something like that with deleted documents already, no? ",
            "author": "Shai Erera",
            "id": "comment-13536999"
        },
        {
            "date": "2012-12-20T12:45:16+0000",
            "content": "\n\nAfter rethinking the point-of-inversion issue, seems like the right time to do it is ASAP - not to hold the added fields and invert them later, but rather invert them immediately and save their inverted version. 3 reasons for that:\n1. Take out the constraint I inserted to the API, so update fields can be reused and contain Reader/TokenStrem,\n2. NRT support: we cannot search until we invert, and if we invert earlier NRT support will be less complicated, probably some variation on multi-reader to view uncommitted updates,\n3. You are correct that we currently do not account for the RAM usage of the FieldsUpdate, since I thought using RAMUsageEstimator will be too costly. It will probably be more efficient to calculate RAM usage of the inverted fields, maybe even during inversion?\n\n+1\n\nI would also add \"4. Inversion of updates is single-threaded\", ie once\nwe move inversion into .updateFields it will be multi-threaded again.\n\nSo my question in that regard is how can I invert a document and hold its inverted form to be used by NRT and later inserted into stacked segment? Should I create a temporary Directory and invert into it? Is there another way to do this?\n\nI think we should somehow re-use the existing code that inverts (eg\nFreqProxTermsWriter)?  Ie, invert into an in-RAM segment, with\n\"temporary\" docIDs, and then when it's time to apply the updates, you\nneed to rewrite the postings to disk with the re-mapped docIDs.\n\nI wouldn't do anything special for NRT for starters, meaning, from\nNRT's standpoint, it opens these stacked segments from disk as it\nwould if a new non-NRT reader was being opened.  So I would leave that\nTODO in SegmentReader as a TODO for now   Later, we can optimize\nthis and have updates carry in RAM like we do for deletes, but I\nwouldn't start with that ...\n\n\nMerging is very important. Hmm, are we able to just merge all updates down to a single update? Ie, without merging the base segment? We can't express that today from MergePolicy right? In an NRT setting this seems very important (ie it'd be best bang (= improved search performance) for the buck (= merge cost)).\n\nShai is helping in creation of a benchmark to test performance in various scenarios. I will start adding updates aspects to the merge policy. I am not sure if merging just updates of a segment is feasible. In what cases would it be better than collapsing all updates into the base segment?\n\nImagine a huge segment that's accumulating updates ... say it has 20\nstacked segments.  First off, those stacked segments are each tying up\nN file descriptors on open, right?  (Well, only one if it's CFS).  But\nsecond off, I would expect search perf with 1 base + 20 stacked is\nworse than 1 base + 1 stacked?  We need to test if that's true\n... it's likely that the most perf loss is going from no stacked\nsegments to 1 stacked segment ... and then going from 1 to 20 stacked\nsegments doesn't hurt \"that much\".  We have to test and see.\n\nSimply merging that big base segment with its 20 stacked segments is\ngoing to be too costly to do very often.\n\n\nI think we need a test that indexes a known (randomly generated) set of documents, randomly sometimes using add and sometimes using update/replace field, mixing in deletes (just like TestField.addDocuments()), for the first index, and for the second index only using addDocument on the \"surviving\" documents, and then we assertIndexEquals(...) in the end? Maybe we can factor out code from TestDuelingCodecs or TestStressIndexing2.\n\nTestFieldReplacements already had a test which randomly adds documents, replaces documents, adds fields and replaces fields. I refactored it to enable using a seed, and created a \"clean\" version with only addDocument(...) calls. However, the FieldInfos of the \"clean\" version do not include things that the \"full\" version includes because in the full version fields possessing certain field traits where added and then deleted. I will look at the other suggestions.\n\nIt should be fine if the FieldInfos don't match?  Ie, when comparing\nthe two indices we should not compare field numbers?  We should be\ncomparing by only external things like fieldName, which id we had\nindexed, etc. ",
            "author": "Michael McCandless",
            "id": "comment-13537000"
        },
        {
            "date": "2012-12-23T08:43:47+0000",
            "content": "Woops, resolved wrong issue  ",
            "author": "Shai Erera",
            "id": "comment-13538995"
        },
        {
            "date": "2012-12-23T08:53:38+0000",
            "content": "I branched https://svn.apache.org/repos/asf/lucene/dev/branches/lucene4258. The patch is really immense and I think it'll be easier to work on this feature in a separate branch. Committed rev 1425441. ",
            "author": "Shai Erera",
            "id": "comment-13538999"
        },
        {
            "date": "2012-12-30T08:39:08+0000",
            "content": "Started switching to the invert-first approach following Mike's advices. My thought was to have a single directory for each fields update, and when flushing do something similar to IndexWriter.addIndexes(IndexReader...) and build the stacked segment. However, I encountered two problems with this approach:\n1. If a certain document is updated more than once in a certain generation, two inverted documents should be merged into one,\n2. extension to 1, where a field added in the first update is to be replaced in the second one.\nSo, what I will try to do in such cases is to move the later updates to a new update generation. This will increase the number of generations, but I think it's a fair price to pay in light of the benefits offered by the invert-first approach. ",
            "author": "Sivan Yogev",
            "id": "comment-13541053"
        },
        {
            "date": "2013-01-24T11:27:53+0000",
            "content": "New patch over the issue branch. Inversion of updated fields done directly when added into a RAMDirectory, and updated segments are created by merging such directories. If more than one update to be applied on same document, the later update is moved to another updated segment.\n\nStill missing:\n1. Implement RAM usage computation for updates,\n2. fix TestFieldReplacements.testIndexEquality(). ",
            "author": "Sivan Yogev",
            "id": "comment-13561571"
        },
        {
            "date": "2013-03-10T15:14:57+0000",
            "content": "I upgraded branch to trunk (w/o latest patch). TestFieldUpdate fails on norms assertion. I'm not sure exactly what happened but SegmentReader had many strange conflicts, so I hope I didn't screw something up when resolving them. Will try to get to the bottom of it later. ",
            "author": "Shai Erera",
            "id": "comment-13598283"
        },
        {
            "date": "2013-03-10T18:35:26+0000",
            "content": "Sivan, I tried to apply the patch to the branch (after the upgrade to trunk) but it does not apply. Some files are missing, some have issues. Can you perhaps bring this patch up to the current branch's version and post another one? ",
            "author": "Shai Erera",
            "id": "comment-13598347"
        },
        {
            "date": "2013-03-10T20:53:46+0000",
            "content": "Thanks Shai for applying the patch, it will take me some time to do adjustments to the changes in trunk, including some fixes to the branch. ",
            "author": "Sivan Yogev",
            "id": "comment-13598380"
        },
        {
            "date": "2013-03-18T11:21:16+0000",
            "content": "Updated patch over branch after merging with trunk. New tests, still with bugs in handling replacements. ",
            "author": "Sivan Yogev",
            "id": "comment-13605025"
        },
        {
            "date": "2013-03-20T16:17:21+0000",
            "content": "Committed the latest patch to the branch and upgraded branch to latest trunk. ",
            "author": "Shai Erera",
            "id": "comment-13607794"
        },
        {
            "date": "2013-05-01T06:58:17+0000",
            "content": "Added tests and fixed bugs. The most thorough test is testIndexEquality() which runs a complicated scenario of adding and replacing documents and fields and compares the resulting index to an index created only using addDocument. The IndexWriterConfig used can be either created using new IndexWriterConfig() or newIndexWriterConfig() which introduces randomness. When there is no randomness the test passes, with randomness and -Dtests.seed=FFC28997A6951FFB it fails. ",
            "author": "Sivan Yogev",
            "id": "comment-13646429"
        },
        {
            "date": "2013-05-01T19:52:51+0000",
            "content": "Shai, can you please commit the patch to the branch? Thanks. ",
            "author": "Sivan Yogev",
            "id": "comment-13646875"
        },
        {
            "date": "2013-05-07T06:47:18+0000",
            "content": "Committed the patch to the branch. I'll also upgrade branch to trunk. ",
            "author": "Shai Erera",
            "id": "comment-13650567"
        },
        {
            "date": "2013-05-07T11:31:07+0000",
            "content": "I upgraded branch to trunk. Sivan, I get many NPEs from tests like TestFieldReplacements. The NPEs come from FrozenBufferedDeletes which I had some difficulty merging. The NPEs are caused because 'deletes' are null. I checked trunk code, and FrozenBufferedDeletes assumes they are not null (as in the branch), so either I broke something while merging, or something has changed in trunk that the updates code doesn't respect. Can you take a look? ",
            "author": "Shai Erera",
            "id": "comment-13650734"
        },
        {
            "date": "2013-05-07T11:40:26+0000",
            "content": "Thanks Shai, will look into it, modifying FrozenBufferedDeletes is the main challenge so far. ",
            "author": "Sivan Yogev",
            "id": "comment-13650737"
        },
        {
            "date": "2013-05-12T21:22:17+0000",
            "content": "1. Removed some assertions which were based on assumptions that stacked segments break\n2. Added a mechanism to apply updates on already applied ones, now all tests pass\n3. Did some house cleaning\nWhat's left? Improve calculation of bytes used, make merge policy updates-aware, add the option to collapse stacked segments for segments that cannot be merged, check performance... ",
            "author": "Sivan Yogev",
            "id": "comment-13655640"
        },
        {
            "date": "2013-05-13T11:27:24+0000",
            "content": "New patch handling another scenario of an update relating to previous update in the same segment. ",
            "author": "Sivan Yogev",
            "id": "comment-13655904"
        },
        {
            "date": "2013-05-13T13:25:18+0000",
            "content": "Committed this to the branch + upgraded to trunk. Sivan, there are tests still failing \u2013 is this expected? ",
            "author": "Shai Erera",
            "id": "comment-13655957"
        },
        {
            "date": "2013-05-26T10:48:40+0000",
            "content": "New patch, all tests pass. After adding a test which mixed updates and deletes I realized that mixing should be constrained, since it opens the door to complex scenarios and will cause changes to the way deletes are saved and that's not a good idea. So I put in IndexWriter a flag which marks whether there are deletes pending, so when a fields update is added and the flag is set there is an automatic commit. So in FrozenBufferedDeletes we can be sure that deletes are to be applied after updates.\nI guess my implementation is not safe, Shai & Mike can you please take a look? ",
            "author": "Sivan Yogev",
            "id": "comment-13667276"
        },
        {
            "date": "2013-06-23T07:22:27+0000",
            "content": "New version to previous patch, incorporating changes from trunk up to revision 1487827.\nShai, can you please commit this to the branch? ",
            "author": "Sivan Yogev",
            "id": "comment-13691370"
        },
        {
            "date": "2013-06-23T12:09:53+0000",
            "content": "It would be nice to have the feature description incorporated right here in the Jira itself. At least a concise summary of what it will do, how it will do it, and an example or two of how it would be used. Solr \"atomic update\" has an \"increment\" feature for numeric fields as well as an \"add\" for multivalued fields - does that proposal include (or at least be compatible with) those extensions as well? I mean, it would be nice if Solr could switch over to this approach, if possible. Of course, that begs the question of how this proposal would \"stack up\" against the technique that Solr uses today for incremental field update.\n\nAlso, it sounds like this is more of a 5.0 feature, even if maybe it does eventually make it into some future 4.x release as well. I mean, 4.4 is expected within the next few weeks, right? While this feature sounds like it will require a lot of testing and baking to be ready for prime time.\n\nIt sounds like it has an assignee, as well. Or, does that have to be a committer.\n ",
            "author": "Jack Krupansky",
            "id": "comment-13691445"
        },
        {
            "date": "2013-07-01T17:22:07+0000",
            "content": "I'd like to echo Jack's comment.  I have been looking for exactly this feature and I'm thrilled to see people working on it!  I'd like the same high level info:  What version will have this feature, when will it be released (roughly), is there a high-level functional spec, and what impact do performance tests show?  Or if this information isn't available yet, when might it be (roughly)? ",
            "author": "Scott Schneider",
            "id": "comment-13696978"
        },
        {
            "date": "2013-07-01T20:39:48+0000",
            "content": "Well, we definitely hope to release it in 5.0, but that of course is still subject to performance results and overall review of the code. Since 5.0 is nowhere near sight as far as I know, I have high hopes it will make it there. Whether it's then back-ported to 4.x is still an open question that we will address after it baked for a while in trunk.\n\nAs for times, we've started to work on a benchmark for this. However we don't work on it at full capacity. If anyone is interested to help, feel free to chime in. I hope that we'll have some numbers within a few weeks time. ",
            "author": "Shai Erera",
            "id": "comment-13697151"
        },
        {
            "date": "2013-07-02T07:12:26+0000",
            "content": "Sivan, I committed the patch to the branch ",
            "author": "Shai Erera",
            "id": "comment-13697572"
        },
        {
            "date": "2013-07-02T07:48:55+0000",
            "content": "Thanks Shai, I will use this version in the performance tests. ",
            "author": "Sivan Yogev",
            "id": "comment-13697586"
        },
        {
            "date": "2013-07-03T01:45:18+0000",
            "content": "Thanks, Shai!  Do you have a rough idea of when this feature (the current phase \u2013 not necessarily all IFU functionality) might be finished?  And how long after that might it get backported?  If you can give me a ~2 month range, that would be helpful.  This feature is important to our next release and I'd like to just have this info for planning. ",
            "author": "Scott Schneider",
            "id": "comment-13698489"
        },
        {
            "date": "2013-07-03T07:50:44+0000",
            "content": "I think we're looking at not less than 4 months, and that assumes performance shows no big concerns. Otherwise, it's game open again. But in a release .. that could take a while.\n\nDepending how adventurous you are, you can compile the branch and run with it for a while . That will surely help us pinpoint issues sooner. ",
            "author": "Shai Erera",
            "id": "comment-13698719"
        },
        {
            "date": "2013-07-05T23:41:34+0000",
            "content": "Gotcha.  Could my help speed this up significantly, or not much?  I hesitate to even ask, because on the one hand, I'm not familiar with Lucene internals, and on the other hand (or the same hand), I doubt I could get time to work on this... but I am an experienced Java developer and if my help would make a difference, I could make a case for it! ",
            "author": "Scott Schneider",
            "id": "comment-13701189"
        },
        {
            "date": "2013-07-23T18:44:31+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13716976"
        },
        {
            "date": "2013-07-24T07:25:14+0000",
            "content": "Started benchmark work using enwiki data. So far so good - running the same indexing and search on both trunk and the branch results in similar timings. \n\nNext step is to have an additional indexing phase, where for some of the documents titles are replaced through the field updates mechanism. The title will be replaced with the same value so the queries will be the same, trying to understand the effect of having updates to segments on the reader/searcher. ",
            "author": "Sivan Yogev",
            "id": "comment-13718070"
        },
        {
            "date": "2013-12-26T07:34:13+0000",
            "content": "I have integrated this awesome patch and seems to work nicely till now. There is one suggestion I would like to get an opinion on...\n\nThere must be many users pursuing an update-codec external to lucene. One such PoC is described at http://www.flax.co.uk/blog/2012/06/22/updating-individual-fields-in-lucene-with-a-redis-backed-codec/\n\nSince all the hard-work of buffering updates to del-queue etc... is already done in this patch, how about exposing the actual write something on the lines of Codec, like an UpdateCodec?. This will free impls from worrying about segment-merges, NRTs etc... \n\nEx: \ninterface UpdateCodec \n{ //May be exposed via IWConfig??}\n      UpdatedSegment writeUpdatedSegment(SegmentReader reader, UpdatedSegmentData data, SegmentInfoPerCommit sipc)\n}\n\npublic final class UpdatedSegment {\npublic UpdatedSegment(SegmentInfo updtInfo, Set<String> genReplaceFiles)\n}\n\nImplementations not wishing to take a generational-approach [knowing fully the risks] will probably return \"new UpdatedSegments(null, null)\" and DocWriter will handle accordingly.\n\nThe StackedReaders should also handle a non-generational update regime. Ex: Instead of a StackedTermsEnum, it should actually get the TermsEnum from UpdateCodec etc...\n\nWhat do you think?\n\n\u2013\nRavi\n ",
            "author": "Ravikumar",
            "id": "comment-13856791"
        },
        {
            "date": "2014-03-12T17:33:06+0000",
            "content": "Hi Guys,\n\nI've been looking at this patch and wanted to know if there's any update on the release date for this patch.\n\nI was able to try out this patch and observed some issues regarding the term offsets for the stacked up segment data. It seems like when a new update is made on top of the stack (Operation.ADD_FIELDS), their offsets begins back from 0. For example (and a use case) : Let a document be \n{ term1 term2 term3 term4 term5}\n. Now we send the whole document in multiple chunks. \nUpdate 1: term1 term2 term3\nUpdate 2: term4 term5\n\nNow the stack looks like (along with their positions):\nterm4:::0 term5:::1\nterm1:::0 term2:::1 term3:::2\n\nSo what we end up getting is two terms appearing at position 0, two on position1 etc.\nCONS: Phrase queries, etc, won't work in this case, for instance, as search for \"term3 term4\". \n\nJust wanted to have a take from you guys to see if that issue could be resolved easily ? \n\nPS: Not sure if it's trivial to resolve that as we'll need to know the max length of the actual document chunk in the previous stack, and not the max position of the last term added to the stack, as last term in the actual doc could be a stopword, hence won't appear in the index, based on the configuration.  \n ",
            "author": "Sunny Khatri",
            "id": "comment-13932048"
        },
        {
            "date": "2014-03-12T19:38:28+0000",
            "content": "Sunny, there are many issues w/ the current patch. It doesn't handle many corner cases and it's very complicated. I'm not saying the approach won't work, just that the patch attempts to solve many update types in one go. For that reason I opened LUCENE-5189 which laid out the ground for updating DocValues fields. Even though it currently supports only NumericDocValues, we already have a new issue LUCENE-5513 to add support for BinaryDV and I assume after that we'll move to cover the rest of the DV types.\n\nAs for updating postings, I haven't yet given up on doing that, but it's more complicated and we may do it in baby steps too (e.g. allow to only add tokens for StringFields). But it will take time. ",
            "author": "Shai Erera",
            "id": "comment-13932255"
        },
        {
            "date": "2014-03-13T10:14:08+0000",
            "content": "Sunny - you are correct that the new terms are added stating at position 0, but this is handled by the reader, which if I remember correctly adds a padding of 10,000 for each new stacked segment. Therefore the actual positions returned by the reader will be:\nterm4:::10000 term5:::10001\nterm1:::10000 term2:::10001 term3:::10002 ",
            "author": "Sivan Yogev",
            "id": "comment-13933066"
        },
        {
            "date": "2014-03-13T15:17:15+0000",
            "content": "Do we know where this magic number of 10000 is defined. Now the position increments (0,1, 2 etc.) is the value getPositionIncrementGap() for the particular analyzer, but I did not notice 10000 being used as the start offset (I'm on Lucene4258 trunk, may be things different there ?).\n\nAlso, seems like there are more pressing issues with the patch, apart from this.  Do we have all the issues and limitations for the patch listed anywhere ? ",
            "author": "Sunny Khatri",
            "id": "comment-13933386"
        },
        {
            "date": "2014-04-16T12:54:36+0000",
            "content": "Move issue to Lucene 4.9. ",
            "author": "Uwe Schindler",
            "id": "comment-13970812"
        }
    ]
}