{
    "id": "SOLR-4793",
    "title": "Solr Cloud can't upload large config files ( > 1MB)  to Zookeeper",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "7.4",
            "master (8.0)"
        ],
        "components": [],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Workaround"
    },
    "description": "Zookeeper set znode size limit to 1MB by default. So we can't start Solr Cloud with some large config files, like synonyms.txt.\n\nJan H\u00f8ydahl has a good idea:\n\"SolrCloud is designed with an assumption that you should be able to upload your whole disk-based conf folder into ZK, and that you should be able to add an empty Solr node to a cluster and it would download all config from ZK. So immediately a splitting strategy automatically handled by ZkSolresourceLoader for large files could be one way forward, i.e. store synonyms.txt as e.g. __001_synonyms.txt __002_synonyms.txt....\"",
    "attachments": {
        "SOLR-4793.patch": "https://issues.apache.org/jira/secure/attachment/12919955/SOLR-4793.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Yago Riveiro",
            "id": "comment-13651066",
            "date": "2013-05-07T17:06:47+0000",
            "content": "A workaround for this is set the param -Djute.maxbuffer with a value greater than 1M.\n\nFrom zookeeper doc\n\n\njute.maxbuffer:\n(Java system property: jute.maxbuffer)\n\nThis option can only be set as a Java system property. There is no zookeeper prefix on it. It specifies the maximum size of the data that can be stored in a znode. The default is 0xfffff, or just under 1M. If this option is changed, the system property must be set on all servers and clients otherwise problems will arise. This is really a sanity check. ZooKeeper is designed to store data on the order of kilobytes in size.\n\nNotice that to use this configuration param, is necessary set the param in solrcloud and zookeeper init script. "
        },
        {
            "author": "Elaine Cario",
            "id": "comment-14038797",
            "date": "2014-06-20T13:41:35+0000",
            "content": "I'm finding that setting jute.maxbuffer both in Solr (-D option at tomcat startup) and zookeeper (zoo.cfg) doesn't seem to work (at least with solr 4.8).  This is really becoming a blocker for us, as we are using index time synonym replacement as a \"poor man's lemmatization\" and these files get quite large.  Would be nice to have some option to have these files managed outside of zookeeper. "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-14038810",
            "date": "2014-06-20T14:01:16+0000",
            "content": "I think that version 4.8 updates zookeeper version to 3.4.6\n\nIf the workaround doesn't work then is serious issue if you have a large number of collections and replicas because all metadata about the cluster is into clusterstate.json file.\n\nElaine Cario, How you notice it that the workaround doesn't work? Have you any logs or something? and last question, do you upgrade Solr from 4.7 to 4.8 or is a fresh install?\n\n "
        },
        {
            "author": "Nicole Lacoste",
            "id": "comment-14038834",
            "date": "2014-06-20T14:21:22+0000",
            "content": "Elaine,\nWe got stuck at the same thing and if I remember right we put the synonyms files in a matching folder on each of the machines and put the full path in the schema. It means if there is an update you have to take care of it yourself. I am not sure if the REST api for the synonyms work in this case you'd have to test that.   "
        },
        {
            "author": "Elaine Cario",
            "id": "comment-14038835",
            "date": "2014-06-20T14:21:46+0000",
            "content": "I got this exception in the logs, they always happen on the lemmatization files, which run anywhere from 2MB to 20MB in size:\n\nERROR - 2014-06-20 09:29:59.082; org.apache.solr.core.ZkContainer;\norg.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /configs/dictindex_uk/en_lemmatization.txt\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n        at org.apache.solr.common.cloud.SolrZkClient$10.execute(SolrZkClient.java:432)\n        at org.apache.solr.common.cloud.ZkCmdExecutor.retryOperation(ZkCmdExecutor.java:73)\n        at org.apache.solr.common.cloud.SolrZkClient.makePath(SolrZkClient.java:429)\n        at org.apache.solr.common.cloud.SolrZkClient.makePath(SolrZkClient.java:339)\n        at org.apache.solr.cloud.ZkController.uploadToZK(ZkController.java:1318)\n        at org.apache.solr.cloud.ZkController.uploadConfigDir(ZkController.java:1355)\n        at org.apache.solr.cloud.ZkController.bootstrapConf(ZkController.java:1565)\n        at org.apache.solr.core.ZkContainer.initZooKeeper(ZkContainer.java:188)\n        at org.apache.solr.core.ZkContainer.initZooKeeper(ZkContainer.java:67)\n        at org.apache.solr.core.CoreContainer.load(CoreContainer.java:216)\n        at org.apache.solr.servlet.SolrDispatchFilter.createCoreContainer(SolrDispatchFilter.java:189)\n        at org.apache.solr.servlet.SolrDispatchFilter.init(SolrDispatchFilter.java:136)\n        at org.apache.catalina.core.ApplicationFilterConfig.initFilter(ApplicationFilterConfig.java:279)\n        at org.apache.catalina.core.ApplicationFilterConfig.getFilter(ApplicationFilterConfig.java:260)\n        at org.apache.catalina.core.ApplicationFilterConfig.<init>(ApplicationFilterConfig.java:105)\n        at org.apache.catalina.core.StandardContext.filterStart(StandardContext.java:4809)\n        at org.apache.catalina.core.StandardContext.startInternal(StandardContext.java:5485)\n        at org.apache.catalina.util.LifecycleBase.start(LifecycleBase.java:150)\n        at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:901)\n        at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:877)\n        at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:632)\n        at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:1073)\n        at org.apache.catalina.startup.HostConfig$DeployWar.run(HostConfig.java:1857)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\nERROR - 2014-06-20 09:29:59.087; org.apache.solr.servlet.SolrDispatchFilter; Could not start Solr. Check solr/home property and the logs\n\nWe were working around it by splitting the files up in < 1MB chunks, but that sometimes worked and sometimes didn't (we'd still get errors), and for the larger files it's, well, a pain to list > 20 files in the schema.  We tried moving the files, but then it complains that relative paths aren't allowed in the specification. \n\nThis is a fresh install of 4.8 (technically it was an upgrade, but we removed the index and re-fed all our content).  But we didn't upgrade zookeeper, so I will try that and report back. "
        },
        {
            "author": "Nicole Lacoste",
            "id": "comment-14038842",
            "date": "2014-06-20T14:29:32+0000",
            "content": "I also had trouble with relative paths, try full paths.   "
        },
        {
            "author": "Elaine Cario",
            "id": "comment-14038845",
            "date": "2014-06-20T14:31:34+0000",
            "content": "Scratch that, we're already at zookeeper 3.4.6.\n\nThanks Nicole - that was going to be my very next step.  Updates aren't an issue, the files are under version control and our deployment scripts can take care of moving them wherever we need to make it work .\n\nWill report back... "
        },
        {
            "author": "Elaine Cario",
            "id": "comment-14038868",
            "date": "2014-06-20T15:00:55+0000",
            "content": "Absolute paths didn't work either - it seems to turn it into a relaitve path, and still uses the ZKSolrResourceLoader to find it.  I may have to either split the files, or dig into the FSTSynonymFilterFactory to figure out some other workaround.\n\nERROR - 2014-06-20 10:45:13.522; org.apache.solr.core.CoreContainer; Unable to create core: dictindex_uk\norg.apache.solr.common.SolrException: Could not load core configuration for core dictindex_uk\n        at org.apache.solr.core.ConfigSetService.getConfig(ConfigSetService.java:66)\n        at org.apache.solr.core.CoreContainer.create(CoreContainer.java:554)\n        at org.apache.solr.core.CoreContainer$1.call(CoreContainer.java:261)\n        at org.apache.solr.core.CoreContainer$1.call(CoreContainer.java:253)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:744)\nCaused by: java.lang.RuntimeException: java.io.IOException: Error opening /configs/dictindex_uk//apps/solr/collections/en_lemmatization.txt\n        at org.apache.solr.schema.IndexSchema.<init>(IndexSchema.java:169)\n        at org.apache.solr.schema.IndexSchemaFactory.create(IndexSchemaFactory.java:55)\n        at org.apache.solr.schema.IndexSchemaFactory.buildIndexSchema(IndexSchemaFactory.java:69)\n        at org.apache.solr.core.ConfigSetService.createIndexSchema(ConfigSetService.java:89)\n        at org.apache.solr.core.ConfigSetService.getConfig(ConfigSetService.java:62)\n        ... 9 more\nCaused by: java.io.IOException: Error opening /configs/dictindex_uk//apps/solr/collections/en_lemmatization.txt\n        at org.apache.solr.cloud.ZkSolrResourceLoader.openResource(ZkSolrResourceLoader.java:83)\n        at org.apache.lucene.analysis.synonym.FSTSynonymFilterFactory.loadSynonyms(FSTSynonymFilterFactory.java:137)\n        at org.apache.lucene.analysis.synonym.FSTSynonymFilterFactory.inform(FSTSynonymFilterFactory.java:112)\n        at org.apache.lucene.analysis.synonym.SynonymFilterFactory.inform(SynonymFilterFactory.java:90)\n        at org.apache.solr.core.SolrResourceLoader.inform(SolrResourceLoader.java:675)\n        at org.apache.solr.schema.IndexSchema.<init>(IndexSchema.java:167)\n        ... 13 more "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-14038874",
            "date": "2014-06-20T15:04:28+0000",
            "content": "Elaine can you paste the configuration for tomcat and zookeeper that you have for the jute.maxbuffer? "
        },
        {
            "author": "Elaine Cario",
            "id": "comment-14038889",
            "date": "2014-06-20T15:11:56+0000",
            "content": "Here is the SOLR_OPTS for Tomcat:\n\nSOLR_OPTS=\"-Dhost.port=8080 -Dhost.context=solr -Dsolr.install.dir=${SOLR_DIST} -Dsolr.solr.home=${SOLR_HOME} -Dsolr.base.data.dir=$SOLR_DATA -Dbootstrap_conf=true -Dsolr.log=${SOLR_LOGDIR} -Dcollection.configName=wkcontent -DnumShards=2 -DzkHost=10.208.152.230:2181 -Djute.maxbuffer=50000000 -Dlog4j.configuration=file:///apps/solr/collections/log4j.properties\"\n\nHere is zoo.cfg\n\ntickTime=2000\ninitLimit=10\nsyncLimit=5\ndataDir=/apps/zookeeper/zk_datadir\ndataLogDir=/apps/zookeeper/zk_logs\nclientPort=2181\nserver.1=zookeep01:2881:3881\njute.maxbuffer=50000000 "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-14038898",
            "date": "2014-06-20T15:16:41+0000",
            "content": "About tocamt's configuration I have the same configuration.\n\nIn the case of Zookeeper I have all custom configurations into a file named zookeeper-env.sh located into bin/conf folder with this content:\n\n\n#!/usr/bin/env bash\n\nZOO_ENV=\"-Djute.maxbuffer= 50000000\"\n\n "
        },
        {
            "author": "Elaine Cario",
            "id": "comment-14038913",
            "date": "2014-06-20T15:25:09+0000",
            "content": "Yago,  I was going to ask how that gets loaded, but I just found an old post of yours with more info: let me try that (reposting it here):\n\n In my case I have:\n>\n> - Zookeeper: a file into conf folder named zookeeper-env.sh with:\n>\n> #!/usr/bin/env bash\n>\n> ZOO_ENV=\"-Djute.maxbuffer=10485761\"\n>\n> The zookeeper-env.sh is loaded automatically by the zkEnv.sh if the file\n> exists.\n> "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-14038918",
            "date": "2014-06-20T15:32:29+0000",
            "content": "Indeed, after dive into the zkEnv file, I realised that if the zookeeper-env.sh exists, the zookeeper append the configurations to the init command. "
        },
        {
            "author": "Elaine Cario",
            "id": "comment-14038948",
            "date": "2014-06-20T16:00:21+0000",
            "content": "Yago - we have partial success!  I added the file as you did, but for some reason the parameter was still not being added to the zookeeper startup (checked PID using ps -ww -fp PID, and didn't see it).\n\nSo, for the moment I modified the zkServer.sh to just add the param directly to the java startup, and that seemed to work - everything started up with the big files.  Now I just need to figure out what's going wrong with the scripting, but that's just some investigation in my part.  I'm sure I did something wrong there.\n\nThanks so much for your help!!  This has been bugging me for weeks.\n "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-14038970",
            "date": "2014-06-20T16:16:57+0000",
            "content": "Elaine now is easier to do the debug, you know where the \"problem\" is .\n\nNote: I'm using the 3.4.5 version of zookeeper, I don't know if the zkServer.sh was changed .... "
        },
        {
            "author": "Elaine Cario",
            "id": "comment-14039111",
            "date": "2014-06-20T18:04:21+0000",
            "content": "Not sure either, but I couldn't find where ZOO_ENV was actually referenced in any of the zookeeper scripts, so I changed the zookeeper-env.sh to this:\n\nJVMFLAGS=\"$JVMFLAGS -Djute.maxbuffer=50000000\"\n\nAnd then it all came together. "
        },
        {
            "author": "Yago Riveiro",
            "id": "comment-14039198",
            "date": "2014-06-20T19:05:55+0000",
            "content": "it's probably that I was tweaked the zkServer file a bit ...  "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-16445295",
            "date": "2018-04-20T04:47:05+0000",
            "content": "I've attached a patch that adds jute.maxbuffer documentation to the ref guide page on setting up an external ZooKeeper cluster that Cassandra Targett improved over on SOLR-12163.\n\nI've included links to the new section:\n\n\tfrom the large model section on the LTR page, as an alternative to SOLR-11250's DefaultWrapperModel; inspired by the patch on SOLR-11049 - cc Christine Poerschke;\n\tfrom the OpenNLP NER mention on the URP page (LUCENE-2899); and\n\tfrom the OpenNLP NER URP javadocs.\n\n\n\nFeedback is welcome. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-16445311",
            "date": "2018-04-20T05:06:48+0000",
            "content": "FYI I considered adding info on configuring ZooKeeper clients other than those invoked by bin/solr, e.g. ZK's zkCli.sh and Solr's cloud script zkcli.sh, but neither of those are covered elsewhere in the ref guide, and I think bin/solr zk commands cover most users needs, so I didn't end up including info on configuring those clients.  (ZK's zkCli.sh reads zookeeper-env.sh, so a user employing that approach for configuring ZK nodes will get that client configured for free.) "
        },
        {
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16445350",
            "date": "2018-04-20T05:45:59+0000",
            "content": "I think the long term solution could be to implement something like a BlobStoreResourceLoader, and a configset (as a whole or in parts) could be loaded from ZK or blob store. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-16445422",
            "date": "2018-04-20T07:23:34+0000",
            "content": "Can we keep all but large files in zk. When zkClient is asked to upload a large file it will upload it to blob instead and create a shadow file with same name in Zk, but with a body telling it is a blob file with a pointer to blob store ID. Then when zk resource loader gets a file it will detect such files and serve them from blob transparently. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-16445424",
            "date": "2018-04-20T07:24:28+0000",
            "content": "This probably means that backup/restore feature also needs to backup the blob store? "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-16445754",
            "date": "2018-04-20T13:57:50+0000",
            "content": "I think the long term solution could be to implement something like a BlobStoreResourceLoader\n\nAgreed - see SOLR-8751 and SOLR-9175.\n\nand a configset (as a whole or in parts) could be loaded from ZK or blob store.\n\nI'm not sure how useful it would be to store whole configsets in the blob store.  In any case, that won't be the first step here.\n\nCan we keep all but large files in zk. When zkClient is asked to upload a large file it will upload it to blob instead and create a shadow file with same name in Zk, but with a body telling it is a blob file with a pointer to blob store ID. Then when zk resource loader gets a file it will detect such files and serve them from blob transparently.\n\nHmm, I don't think we should be starting with this kind of magic - I'm much more comfortable with separate blob store upload (already implemented) and schema reference steps (SOLR-8751/SOLR-9175)\n\nThis probably means that backup/restore feature also needs to backup the blob store?\n\nYes, but this is already true right now: solrconfig.xml can load handler and component classes from blobs in the blob store.\n "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-16446036",
            "date": "2018-04-20T17:05:23+0000",
            "content": "Hmm, I don't think we should be starting with this kind of magic\n\nIf there are still things to work out with the blob store (this is the .system collection, right?) then I agree, we should let those things bake for a while before we implement automagic redirection from ZK.  I do like Jan's idea, though. "
        },
        {
            "author": "Christine Poerschke",
            "id": "comment-16446257",
            "date": "2018-04-20T19:34:21+0000",
            "content": "I think the long term solution could be to implement something like a BlobStoreResourceLoader, and a configset (as a whole or in parts) could be loaded from ZK or blob store.\n\nI like the idea of (where appropriate) getting parts of a configset from 'elsewhere' and that could be from a blob store or from somewhere else.\n\nticket cross-reference:\n\n\tSOLR-9887 is about stopwords and synonyms from a JDBC source - cc Tobias K\u00e4ssmann Torsten B\u00f8gh K\u00f6ster + David Smiley re: the pulling from a streaming expression mention\n\n\n "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-16446293",
            "date": "2018-04-20T20:06:53+0000",
            "content": "Commit 9592221193971732b8d2c4b2c2994417bd7a3072 in lucene-solr's branch refs/heads/branch_7x from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=9592221 ]\n\nSOLR-4793: Document usage of ZooKeeper's jute.maxbuffer sysprop for increasing the file size limit above 1MB "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-16446294",
            "date": "2018-04-20T20:06:55+0000",
            "content": "Commit 22c4b9c36f5dfdf0578bacea2e83740714512765 in lucene-solr's branch refs/heads/master from Steve Rowe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=22c4b9c ]\n\nSOLR-4793: Document usage of ZooKeeper's jute.maxbuffer sysprop for increasing the file size limit above 1MB "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-16446296",
            "date": "2018-04-20T20:10:06+0000",
            "content": "Resolving this issue for now with status \"Workaround\" as a result of the added documentation.\n\nI'll open an issue to remove this documentation once the blob store is a viable alternative to ZK storage of large configuration files everywhere. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-16446404",
            "date": "2018-04-20T21:51:40+0000",
            "content": "Yes, better start simple and choose one location for\u00a0config sets. Blob, HDFS, FileSystem...\n\nLooks like we have a cleanup job with the SolrResourceLoader hierarchy, there is a ton of zkSolrResourceLoader hardcoding, and we should probably generalise some of the zkRL features up into SolrResourceLoader, or a new SolrResourceLoaderBase. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-16506491",
            "date": "2018-06-08T19:40:21+0000",
            "content": "Commit 4c7b7c0063b2fd194d24444037fd769c2b0e5fcf in lucene-solr's branch refs/heads/master from Cassandra Targett\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=4c7b7c0 ]\n\nSOLR-4793: Ref Guide: shorten the section heading & fix refs "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-16506493",
            "date": "2018-06-08T19:41:06+0000",
            "content": "Commit c35edc8fc394f4b88185fe24b83f748b7793dde9 in lucene-solr's branch refs/heads/branch_7x from Cassandra Targett\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c35edc8 ]\n\nSOLR-4793: Ref Guide: shorten the section heading & fix refs "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-16506495",
            "date": "2018-06-08T19:41:47+0000",
            "content": "Commit 4031935f3ba008a6beb8216b94e67548bfbf9ec2 in lucene-solr's branch refs/heads/branch_7_4 from Cassandra Targett\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=4031935 ]\n\nSOLR-4793: Ref Guide: shorten the section heading & fix refs "
        }
    ]
}