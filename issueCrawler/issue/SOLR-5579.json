{
    "id": "SOLR-5579",
    "title": "Leader stops processing collection-work-queue after failed collection reload",
    "details": {
        "affect_versions": "4.5.1",
        "status": "Open",
        "fix_versions": [],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "I've been experiencing the same problem a few times now. My leader in /overseer_elect/leader stops processing the collection queue at /overseer/collection-queue-work. The queue will build up and it will trigger an alert in my monitoring tool.\n\nI haven't been able to pinpoint the reason that the leader stops, but usually I kill the leader node to trigger a leader election. The new node will pick up the queue. And this is where the problems start.\n\nWhen the new leader is processing the queue and picks up a reload for a shard without an active leader, the queue stops. It keeps repeating the message that there is no active leader for the shard. But a new leader is never elected:\n\n\nERROR - 2013-12-24 14:43:40.390; org.apache.solr.common.SolrException; Error while trying to recover. core=magento_349_shard1_replica1:org.apache.solr.common.SolrException: No registered leader was found, collection:magento_349 slice:shard1\n        at org.apache.solr.common.cloud.ZkStateReader.getLeaderRetry(ZkStateReader.java:482)\n        at org.apache.solr.common.cloud.ZkStateReader.getLeaderRetry(ZkStateReader.java:465)\n        at org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:317)\n        at org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:219)\n\nERROR - 2013-12-24 14:43:40.391; org.apache.solr.cloud.RecoveryStrategy; Recovery failed - trying again... (7) core=magento_349_shard1_replica1\nINFO  - 2013-12-24 14:43:40.391; org.apache.solr.cloud.RecoveryStrategy; Wait 256.0 seconds before trying to recover again (8)\n\nIs the leader election in some way connected to the collection queue? If so, can this be a deadlock, because it won't elect until the reload is complete?",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-13856929",
            "date": "2013-12-26T16:38:12+0000",
            "content": "Have to look for the issue, but may have been fixed in 4.6.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13857195",
            "date": "2013-12-27T00:31:29+0000",
            "content": "Nope, scratch the above comment. Will need to look into this. "
        },
        {
            "author": "Eric Bus",
            "id": "comment-13866696",
            "date": "2014-01-09T15:13:03+0000",
            "content": "Just a quick update: the leader again stopped working. I had to restart the cluster to get everything working again. The script that is running to check the status did not work, so unfortunately I don't have additional information from the logs. When I do, I'll report back here. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13908116",
            "date": "2014-02-21T09:33:33+0000",
            "content": "Not sure im struck by this too but a cluster started to fail after a successful collection reload.\n\nedit: the reload was successful because specific config files got reloaded. But this pops up on some logs.\n\n\n2014-02-20 11:48:28,985 WARN [solr.cloud.OverseerCollectionProcessor] - [Overseer-91283517052878861-xx.xx.xx.xx:8080_solr-n_0000000936] - : OverseerCollectionProcessor.processMessage : reloadcollection , {\n  \"operation\":\"reloadcollection\",\n  \"name\":\"collection\"}\n\n "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13908161",
            "date": "2014-02-21T10:52:49+0000",
            "content": "Also, i am now sure SOLR-4260 is a problem again, after the the collection got unusable, i now have a shards out of sync. The strange thing was that while the cluster was unusable (could not query) we did continue sending updates, without errors. "
        },
        {
            "author": "Ryan Cooke",
            "id": "comment-14189269",
            "date": "2014-10-29T23:23:34+0000",
            "content": "Pretty sure we are also encountering this issue, the collection reload http requests issued through the core admin are timing out and a corresponding message is sitting in the collection-work-queue. Reloading cores using the reload button in the admin gui will successfully reload the local collection however. Issuing the reload http request with the parameter async=true seems to behave in the same way (request time out) "
        }
    ]
}