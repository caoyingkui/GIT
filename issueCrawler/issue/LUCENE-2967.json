{
    "id": "LUCENE-2967",
    "title": "Use linear probing with an additional good bit avalanching function in FST's NodeHash.",
    "details": {
        "labels": "",
        "priority": "Trivial",
        "components": [],
        "type": "Improvement",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Won't Fix",
        "status": "Closed"
    },
    "description": "I recently had an interesting discussion with Sebastiano Vigna (fastutil), who suggested that linear probing, given a hash mixing function with good avalanche properties, is a way better method of constructing lookups in associative arrays compared to quadratic probing. Indeed, with linear probing you can implement removals from a hash map without removed slot markers and linear probing has nice properties with respect to modern CPUs (caches). I've reimplemented HPPC's hash maps to use linear probing and we observed a nice speedup (the same applies for fastutils of course).\n\nThis patch changes NodeHash's implementation to use linear probing. The code is a bit simpler (I think . I also moved the load factor to a constant \u2013 0.5 seems like a generous load factor, especially if we allow large FSTs to be built. I don't see any significant speedup in constructing large automata, but there is no slowdown either (I checked on one machine only for now, but will verify on other machines too).",
    "attachments": {
        "LUCENE-2967.patch": "https://issues.apache.org/jira/secure/attachment/12473674/LUCENE-2967.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2011-03-15T13:35:30+0000",
            "content": "Linear probing in NodeHash. ",
            "author": "Dawid Weiss",
            "id": "comment-13006917"
        },
        {
            "date": "2011-03-15T17:24:04+0000",
            "content": "Hmm, unfortunately, I'm seeing the patch make FST building slower, at\nleast in my env/test set.  I built FST for the 38M wikipedia terms.\n\nI ran 6 times each, alternating trunk & patch.\n\nI also turned off saving the FST, and ran -noverify, so I'm only\nmeasuring time to build it.  I run java -Xmx2g -Xms2g -Xbatch, and\nmeasure wall clock time.\n\nTimes on trunk (seconds):\n\n\n  43.795\n  43.493\n  44.343\n  44.045\n  43.645\n  43.846\n\n\n\nTimes w/ patch:\n\n\n  46.595\n  47.751\n  47.901\n  47.901\n  47.901\n  47.700\n\n\n\nWe could also try less generous load factors... ",
            "author": "Michael McCandless",
            "id": "comment-13007031"
        },
        {
            "date": "2011-03-15T20:41:10+0000",
            "content": "Yes, now I see this difference on the 38M too:\n\ntrunk:\n\n56.462\n55.725\n55.544\n55.522\n\n\nw/patch:\n\n59.9\n59.6\n\n\n\nI'll see if I can find out the problem here; I assume the collision ratio should be nearly identical... but who knows. This is of no priority, but interesting stuff. I'll close if I can't get it better than the trunk version. ",
            "author": "Dawid Weiss",
            "id": "comment-13007164"
        },
        {
            "date": "2011-03-23T13:09:27+0000",
            "content": "I spent some time on this. It's quite fascinating: the number of collisions for the default probing is smaller than:\n\na) linear probing with murmurhash mix of the original hash\nb) linear probing without murmurhash mix (start from raw hash only).\n\nCuriously, the number of collisions for (b) is smaller than for (a) \u2013 this could be explained if we assume bits are spread evently throughout the entire 32-bit range after murmurhash, so after masking to table size there should be more collisions on lower bits compared to a raw hash (this would have more collisions on upper bits and fewer on lower bits because it is multiplicative... or at least I think so).\n\nAnyway, I tried many different versions and I don't see any significant difference in favor of linear probing here. Measured the GC overhead during my tests too, but it is not the primary factor contributing to the total cost of constructing the FST (about 3-5% of the total time, running in parallel, typically). ",
            "author": "Dawid Weiss",
            "id": "comment-13010105"
        }
    ]
}