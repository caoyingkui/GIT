{
    "id": "SOLR-7569",
    "title": "Create an API to force a leader election between nodes",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "New Feature",
        "labels": "",
        "fix_versions": [
            "5.4",
            "6.0"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "There are many reasons why Solr will not elect a leader for a shard e.g. all replicas' last published state was recovery or due to bugs which cause a leader to be marked as 'down'. While the best solution is that they never get into this state, we need a manual way to fix this when it does get into this  state. Right now we can do a series of dance involving bouncing the node (since recovery paths between bouncing and REQUESTRECOVERY are different), but that is difficult when running a large cluster. Although it is possible that such a manual API may lead to some data loss but in some cases, it is the only possible option to restore availability.\n\nThis issue proposes to build a new collection API which can be used to force replicas into recovering a leader while avoiding data loss on a best effort basis.",
    "attachments": {
        "SOLR-7569.patch": "https://issues.apache.org/jira/secure/attachment/12751039/SOLR-7569.patch",
        "SOLR-7569-testfix.patch": "https://issues.apache.org/jira/secure/attachment/12771330/SOLR-7569-testfix.patch",
        "SOLR-7569_lir_down_state_test.patch": "https://issues.apache.org/jira/secure/attachment/12752012/SOLR-7569_lir_down_state_test.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-08-18T14:03:00+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "Trying to tackle this situation where all replicas (including the leader) are somehow marked \"down\" (maybe due to bugs?) and there is no leader in the shard, and hence the entire shard is down. Adding a new collection API \"RECOVERSHARD\".\n\nIn this strawman patch, I am evaluating the following approach:\n\n\n\tRemove all leader initiated recovery flags for this shard. [TODO]\n\tPick the next leader: If the leader election queue is not empty and the first replica in the queue is on a live node, choose the replica as the next leader. Otherwise, pick a random replica, which is on a live node, to become the next leader (TODO: we can have the user specify which replica he/she wants as the next leader).\n\tIf the chosen leader is not at the head of the leader election queue, have it join the election at the head (similar to what REBALANCELEADERS tries to do). [TODO]\n\tMark the chosen next leader as \"active\". Mark rest of the replicas, which are on live nodes, as \"recovering\".\n\tIssue core admin REQUESTRECOVERY command to all the replicas marked \"recovering\".\n\tWait till recovery completes. [TODO]\n\n\n\nDoes the above approach sound reasonable? Does the general path taken in the patch seem reasonable? ",
            "id": "comment-14701293"
        },
        {
            "date": "2015-08-18T14:24:37+0000",
            "author": "Mark Miller",
            "content": "maybe due to bugs?\n\nThe current design allows for this.\n\nSee SOLR-7034 and SOLR-7065 as possible improvement steps.\n\nThis is kind of a hack solution to a current production problem or to force a leader election even when we know it probably means data loss, those issues are closer to what is supposed to come next in terms of improving the current design.\n\nI may have also just seen a bug where LIR info in ZK prevents anyone from becoming the leader even on full restart. SOLR-7065 should address those kinds of bugs if done right. ",
            "id": "comment-14701327"
        },
        {
            "date": "2015-08-18T14:53:09+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "Thanks Mark Miller for the pointer to the issues. \nI agree this is a sledge hammer to undo the effects of bugs, which shouldn't be needed if we go by an improved design. We have observed the effects of these bugs in production clusters of our clients, and this is to help them in such a scenario. Do you think we should continue down this sledge hammer path, parallel to fixing the bugs? ",
            "id": "comment-14701387"
        },
        {
            "date": "2015-08-18T14:58:04+0000",
            "author": "Mark Miller",
            "content": "Yes, I think having this option is useful in the short term and in the longer term. The system will generally refuse to continue on if it thinks it may have data loss and stopping could allow a user to possibly recover that data. This could act as a way for a user to override that. ",
            "id": "comment-14701392"
        },
        {
            "date": "2015-08-18T16:11:53+0000",
            "author": "Erick Erickson",
            "content": "bq: If the chosen leader is not at the head of the leader election queue, have it join the election at the head (similar to what REBALANCELEADERS tries to do).\n\nBe really careful if you are trying to manipulate the leader election stuff, it's very easy to get wrong. Or at least it was last time I looked, perhaps it's changed a lot since then. I'd be glad to chat about what I remember if you'd like. ",
            "id": "comment-14701503"
        },
        {
            "date": "2015-08-20T13:49:06+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "Updated patch with some cleanup. Still some TODOs, nocommits to go. ",
            "id": "comment-14704919"
        },
        {
            "date": "2015-08-21T06:52:27+0000",
            "author": "Varun Thacker",
            "content": "Pick the next leader: If the leader election queue is not empty and the first replica in the queue is on a live node, choose the replica as the next leader. Otherwise, pick a random replica, which is on a live node, to become the next leader (TODO: we can have the user specify which replica he/she wants as the next leader).\n\nMaybe pick the leader amongst the replicas which has the latest commit timestamp? ",
            "id": "comment-14706311"
        },
        {
            "date": "2015-08-21T13:21:55+0000",
            "author": "Mark Miller",
            "content": "I don't really like the idea of choosing a leader. It seems to me this feature should force a new election and address the state that prevents someone from becoming leader somehow. You still want the sync stage and the system to pick the best leader though. This should just get you out of the state that is preventing a leader from being elected. ",
            "id": "comment-14706723"
        },
        {
            "date": "2015-08-24T13:58:03+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "I just tried to simulate the scenario where all the replicas are in down state due to LIR, and there is no leader. In this state, the leader election queue is empty.\n\nSo, I am thinking of some way to have the replicas (that are on live nodes) to join the leader election. Is there any clean way of doing that, short of a core reload? ",
            "id": "comment-14709330"
        },
        {
            "date": "2015-08-24T13:58:43+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "A patch containing the test that simulates the state described above. ",
            "id": "comment-14709331"
        },
        {
            "date": "2015-08-24T14:50:23+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "> In this state, the leader election queue is empty.\nIgnore that, I was catching that state before the replicas had a chance to rejoin the election. The last assert in the patch is inappropriate. ",
            "id": "comment-14709403"
        },
        {
            "date": "2015-08-26T04:25:07+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "This should just get you out of the state that is preventing a leader from being elected.\nUpdating the patch that attempts to do just that. \n\n\tClear LIR znodes\n\tUnset any existing leader flags.\n\tMark all nodes as active\n\tWait for leader election so that normal state is restored\n\n\n\n(I also tried to mark just one of the replicas as active instead of all the replicas, hoping it will become leader and others would recover from it. However, this resulted in one of the other down replicas becoming leader but still staying down. Looking into why that could be happening; bug?) ",
            "id": "comment-14712483"
        },
        {
            "date": "2015-08-26T14:34:17+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "Adding a wait for recoveries to finish after the recovery operation in the test. ",
            "id": "comment-14713536"
        },
        {
            "date": "2015-08-27T11:58:06+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "Adding more logging for tests, tests for asserting indexing fails during down state and works again after the recover operation. ",
            "id": "comment-14716533"
        },
        {
            "date": "2015-08-27T16:03:53+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks Ishan! A few comments:\n\n\tnit - RecoverShardTest has an unused notLeader1 variable\n\tShouldn't the \"Wait for a long time for a steady state\" piece of code be before the proxies for the two replicas are reopened? The LIR state will surely be set at indexing time and only if the proxy is closed. Also if you move that wait before the proxy is reopened then you are sure to have the LIR state as 'down'.\n\tThe check for 'numActiveReplicas' and 'numReplicasOnLiveNodes' should be done after force refreshing the cluster state of the cloudClient otherwise spurious failures can happen\n\tnit - Why is sendDoc overridden in RecoverShardTest? The minRf is same, just the max retries has been increased and wait between retries has been decreased\n\tThe OCMH.recoverShard() isn't unsetting the leader properly. It should be as simple as:\n\nZkNodeProps m = new ZkNodeProps(Overseer.QUEUE_OPERATION, OverseerAction.LEADER.toLower(),\n          ZkStateReader.SHARD_ID_PROP, shardId, ZkStateReader.COLLECTION_PROP, collection);\n      Overseer.getInQueue(zkClient).offer(Utils.toJSON(m));\n\n\n\tCan you please write a test to ensure that this API works with 'async' parameter?\n\n\n\nI think some simple scenarios are not being taken care of. This command only helps if there a LIR node exists but we can do a bit more:\n\n\tLeader is live but 'down' -> mark it 'active'\n\tLeader itself is in LIR -> delete the LIR node\n\tLeader is not live:\n\t\n\t\tReplicas are live but 'down' or 'recovering' -> mark them 'active'\n\t\tReplicas are live but in LIR -> delete the LIR nodes\n\t\n\t\n\n\n\nCan you please add some tests exercising each of the above scenarios?\n\nI also tried to mark just one of the replicas as active instead of all the replicas, hoping it will become leader and others would recover from it. However, this resulted in one of the other down replicas becoming leader but still staying down. Looking into why that could be happening; bug?\n\nDid you find out why/how that happened? If this is reproducible, can you please create an issue and post the test there? ",
            "id": "comment-14716934"
        },
        {
            "date": "2015-09-02T16:03:06+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "\n1.  nit - RecoverShardTest has an unused notLeader1 variable\nThanks. Made some refactoring to the test and this has gone away now.\n\n2.    Shouldn't the \"Wait for a long time for a steady state\" piece of code be before the proxies for the two replicas are reopened? The LIR state will surely be set at indexing time and only if the proxy is closed. Also if you move that wait before the proxy is reopened then you are sure to have the LIR state as 'down'.\nThis makes sense, I've made the change.\n\n3.    The check for 'numActiveReplicas' and 'numReplicasOnLiveNodes' should be done after force refreshing the cluster state of the cloudClient otherwise spurious failures can happen\n\nI didn't know about this force update of the cluster state; I've now added it.\n\n4.    nit - Why is sendDoc overridden in RecoverShardTest? The minRf is same, just the max retries has been increased and wait between retries has been decreased\nThe tests were (and still are) taking too long, and reducing the wait from 30sec to 1sec was helpful.\n\n5.    The OCMH.recoverShard() isn't unsetting the leader properly. It should be as simple as:\nThanks, I've cleaned this up.\n\n6.    Can you please write a test to ensure that this API works with 'async' parameter?\nTODO.\n\nLeader is live but 'down' -> mark it 'active'\nThis works now. Added testLeaderDown() method.\n\nLeader itself is in LIR -> delete the LIR node\nThis should work, since the API method first clears the LIR state. Couldn't add a test for this, since I couldn't simulate this state in a test.\n\nLeader is not live:       Replicas are live but 'down' or 'recovering' -> mark them 'active'\nThis works now. Added testAllReplicasDownNoLeader() method.\n\nLeader is not live:       Replicas are live but in LIR -> delete the LIR nodes\nThis works as last patch. The corresponding test is now at testReplicasInLIRNoLeader().\n\nDid you find out why/how that happened? If this is reproducible, can you please create an issue and post the test there?\nAdded SOLR-7989 for this, will look deeper soon. ",
            "id": "comment-14727549"
        },
        {
            "date": "2015-09-02T17:48:49+0000",
            "author": "Mark Miller",
            "content": "This seems to have some overlap with SOLR-6236 based on the comments. Tim did some work here as well:\n\nAt a high-level, the issue boils down to giving SolrCloud operators a way to either a) manually force a leader to be elected, or b) set an optional configuration property that triggers the force leader behavior after seeing so many failed recoveries due to no leader. So this can be considered an optional availablity-over-consistency mode with respect to leader-failover. ",
            "id": "comment-14727720"
        },
        {
            "date": "2015-09-02T18:00:29+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "At a high-level, the issue boils down to giving SolrCloud operators a way to either a) manually force a leader to be elected, or b) set an optional configuration property that triggers the force leader behavior after seeing so many failed recoveries due to no leader.\nI think the difference is that in this issue, we're just trying to (manually) clean up the LIR state and mark affected down replicas as active and hope that normal leader election is initiated and normalcy is restored. Based on initial glance at SOLR-6236, it seems that the intention there is to actually force one of the replicas to become a leader (either manually or automatically).\n\nI am not sure which path we should take, but it seems the approach taken here is less intrusive/safer, if/when it works. ",
            "id": "comment-14727740"
        },
        {
            "date": "2015-09-02T18:14:36+0000",
            "author": "Mark Miller",
            "content": "Timothy Potter, what is your impression? ",
            "id": "comment-14727757"
        },
        {
            "date": "2015-09-02T18:16:25+0000",
            "author": "Mark Miller",
            "content": "we're just trying to (manually) clean up the LIR state and mark affected down replicas as active and hope that normal leader election is initiated and normalcy is restored.\n\nI like that approach too, but I want to make sure we consider SOLR-6236. ",
            "id": "comment-14727759"
        },
        {
            "date": "2015-09-02T18:16:45+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "This seems to have some overlap with SOLR-6236 based on the comments.\nI think I should've posted the patches there, instead of here. Seems like I'm trying to solve the same problem here (albeit, in a different way). ",
            "id": "comment-14727760"
        },
        {
            "date": "2015-09-03T15:46:16+0000",
            "author": "Timothy Potter",
            "content": "Hi, will dig into this in detail later today, sorry for the delay (been on another project ) ... ",
            "id": "comment-14729291"
        },
        {
            "date": "2015-09-08T10:42:15+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "\n\tPassing the async parameter through,\n\tTests now randomly make async requests for the recover shard API call.\n\n ",
            "id": "comment-14734598"
        },
        {
            "date": "2015-09-11T12:17:26+0000",
            "author": "Timothy Potter",
            "content": "Looks good Ishan. Sorry for the delay getting a review done. In putNonLeadersIntoLIR, you probably want to wait a little bit before killing the leader after sending doc #2 to give the leader time to put the replicas into LIR; this works quickly on our local workstations but can take a little more time on Jenkins.\n\nI'm also wondering if you should bring the original downed leader back into the mix (the one that got killed in the putNonLeadersIntoLIR method) in the testReplicasInLIRNoLeader test after the new leader is selected and see what state it comes back to. Also, try sending another doc #5 once the Jetty hosting the original leader is back online.\n\nLastly, what happened to the idea of allowing the user to pick the leader as part of the recover shard request? I read the comments above and agree that just triggering a re-election is preferred, but sometimes us humans actually know which replica is best. It seems reasonable to me to accept an optional parameter that specifies the replica that should be selected. However, if others don't like that idea, then I'm fine with this for now. ",
            "id": "comment-14740684"
        },
        {
            "date": "2015-09-11T14:07:15+0000",
            "author": "Mark Miller",
            "content": "what happened to the idea of allowing the user to pick the leader as part of the recover shard request?\n\nAs long as it's optional and documented so that users understand the risks, it's probably okay. But, I think in most cases the system will beat most users in most cases in understanding who should really be the leader. ",
            "id": "comment-14740864"
        },
        {
            "date": "2015-09-19T06:56:09+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "Timothy Potter Thanks for your review. I've added an extra wait before killing the leader. I tried to bring back a killed leader, but seems not to be working. Tried many things, but was stuck with \"address already in use\" exception, perhaps due to the socket proxy holding on the port. ",
            "id": "comment-14876933"
        },
        {
            "date": "2015-09-19T07:01:37+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "what happened to the idea of allowing the user to pick the leader as part of the recover shard request?\nI had a look at your patch for SOLR-6236, I think we can tackle this using that approach. At this point, I'm inclined to keep this patch at this and tackle it separately. Most likely, the system will pick a reasonable leader, and it will sync with other replicas and the shard will be restored. ",
            "id": "comment-14876938"
        },
        {
            "date": "2015-09-19T12:42:56+0000",
            "author": "Mark Miller",
            "content": "I wonder if Recover is the right terminology. It seems so broad and \"fix anything\" like. Perhaps it should be something close to 'forceleader' - something that is specific about what is happening and gives an idea that you are overriding the system as you are. ",
            "id": "comment-14877089"
        },
        {
            "date": "2015-09-19T13:00:54+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "I had the same dilemma while naming this. Recover does seem like it will fix things if anything is broken, which can be misleading since at this time we aren't doing anything other than helping fix the LIR state to bring the shard back up.\nOn the other hand, I am not sure about force leader, because we aren't really forcing a leader, but just paving things for an election to happen. I'm really not totally sure either way.\n\nHow about keeping this as recover shard, documenting this as an advanced API which can potentially cause data loss, and then later add whatever else we need to recover the system from to this API itself? ",
            "id": "comment-14877101"
        },
        {
            "date": "2015-09-19T13:26:59+0000",
            "author": "Mark Miller",
            "content": "That sounds reasonable as long as we have good doc warning about it. ",
            "id": "comment-14877114"
        },
        {
            "date": "2015-09-19T13:30:06+0000",
            "author": "Mark Miller",
            "content": "because we aren't really forcing a leader, but just paving things for an election to happen. \n\nI guess it comes down to how you want to think about. When you use this, it will be because the system is blocking a leader from taking over. By running this API command, you remove the blocks, thus 'forcing' a leader the system would not normally pick - or at least attempting to force a leader the system would not really pick. It depends on if you want to get bogged down in implementation or design.\n\nI think your proposal is fine though. ",
            "id": "comment-14877117"
        },
        {
            "date": "2015-09-19T14:47:51+0000",
            "author": "Timothy Potter",
            "content": "+1 on FORCE_LEADER for the name of the action.\n\nwas stuck with \"address already in use\" exception\nYou should have access to the SocketProxy if you need to close it down before trying to restart the original leader's Jetty. If not, we should fix that.\n\nI'm inclined to keep this patch at this and tackle it separately\n\nsounds good ... may not ever be needed in the wild with the solution you've created here  ",
            "id": "comment-14877156"
        },
        {
            "date": "2015-09-21T13:30:06+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "When you use this, it will be because the system is blocking a leader from taking over. By running this API command, you remove the blocks, thus 'forcing' a leader the system would not normally pick - or at least attempting to force a leader the system would not really pick.\n\nThat makes sense!  I've renamed this to FORCELEADER now. \nAlso managed to reincarnate the killed off leaders into the tests. ",
            "id": "comment-14900690"
        },
        {
            "date": "2015-10-20T11:18:37+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks Ishan. \n\n\n\tForceLeaderTest.testReplicasInLIRNoLeader has a 5 second sleep, why? Isn't waitForRecoveriesToFinish() enough?\n\tSimilarly, ForceLeaderTest.testLeaderDown has a 15 second sleep for steady state to be reached? What is this steady state, is there a better way than waiting for an arbitrary amount of time? In general, Thread.sleep should be avoided as much as possible as a way to reach steady state.\n\tCan you please add some javadocs on the various test methods describing the scenario that they are test?\n\tminor nit - can you use assertEquals when testing equality of state etc instead of assertTrue. The advantage with assertEquals is that it logs the mismatched values in the exception messages.\n\tIn OverseerCollectionMessageHandler, lirPath can never be null. The lir path should probably be logged in debug rather than INFO.\n\n// Clear out any LIR state\n      String lirPath = overseer.getZkController().getLeaderInitiatedRecoveryZnodePath(collection, sliceId);\n      if (lirPath != null && zkStateReader.getZkClient().exists(lirPath, true)) {\n        StringBuilder sb = new StringBuilder();\n        zkStateReader.getZkClient().printLayout(lirPath, 4, sb);\n        log.info(\"Cleaning out LIR data, which was: \" + sb);\n        zkStateReader.getZkClient().clean(lirPath);\n      }\n\n\n\tThere's no need to send an empty string as the role while publishing the state of the replica.\n\tminor nit - you can compare enums directly using == instead of .equals\n\tReferring to the following, what is the thinking behind it? when can this happen? is there a test which specifically exercises this scenario? seems like this can interfere with the leader election if the leader election was taking some time?\n\n// If we still don't have an active leader by now, it maybe possible that the replica at the head of the election queue\n      // was the leader at some point and never left the queue, but got marked as down. So, if the election queue is not empty,\n      // and the replica at the head of the queue is live, then mark it as a leader.\n\n\n\n ",
            "id": "comment-14964979"
        },
        {
            "date": "2015-10-22T12:25:50+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "Thanks Shalin for looking into the patch and your review.\n\nForceLeaderTest.testReplicasInLIRNoLeader has a 5 second sleep, why? Isn't waitForRecoveriesToFinish() enough?\nFixed. This was a left over from some previous patch. I think I wanted to put the waitForRecoveriesToFinish(), but forgot to remove the 5 second sleep.\n\nSimilarly, ForceLeaderTest.testLeaderDown has a 15 second sleep for steady state to be reached? What is this steady state, is there a better way than waiting for an arbitrary amount of time? In general, Thread.sleep should be avoided as much as possible as a way to reach steady state.\nIn this case, waiting those 15 seconds results in one of the down replicas to become a leader (but stay down). This is the situation I'm using FORCELEADER to recover from. Instead of waiting 15 seconds, I've added some polling with wait to wake up earlier if needed, while increasing the timeout from 15s to 25s.\n\n\nCan you please add some javadocs on the various test methods describing the scenario that they are test?\nSure, added.\n\nminor nit - can you use assertEquals when testing equality of state etc instead of assertTrue. The advantage with assertEquals is that it logs the mismatched values in the exception messages.\nUsed assertEquals() now.\n\nIn OverseerCollectionMessageHandler, lirPath can never be null. The lir path should probably be logged in debug rather than INFO.\nThanks for the pointer, I've removed the null check. I feel this should be INFO instead of DEBUG, so that if a user says I issued FORCELEADER but still nothing worked for him, his logs would help us understand if we ever had any LIR state which was cleared out. But, please feel free to remove it if this doesn't make sense.\n\nminor nit - you can compare enums directly using == instead of .equals\nFixed.\n\nReferring to the following, what is the thinking behind it? when can this happen? is there a test which specifically exercises this scenario? seems like this can interfere with the leader election if the leader election was taking some time? \n\nI modified the comment text to make it more clear. This is for the situation when all replicas are (somehow, due to bug maybe?) down/recovering (but not in LIR), and there is no leader, even though many replicas are on live; I don't know if this ever happens (the LIR case happens, I know). The testAllReplicasDownNoLeader test exercises this scenario. This is more or less the scenario that you described (with one difference that there is no leader as well): Leader is not live: Replicas are live but 'down' or 'recovering' -> mark them 'active'.\n\nAs you point out, I think it can indeed interfere with any on-going leader election; my thought was that this FORCELEADER call is issued only because the leader election isn't achieving a stable leader, so force marking the queue head replica as leader is okay. But I defer to your judgement if this is fine or not, and I can remove (or you feel free to remove) that code path from the patch if you feel it is not right. ",
            "id": "comment-14969085"
        },
        {
            "date": "2015-10-23T15:39:40+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "Based on an offline conversation with Shalin (and the discussion above), I've removed that extra handling of the situation where:\n\n\tthere is no LIR involved\n\tall replicas are down\n\tthere is no leader.\n\n\n\nThis involved force marking the replica at the election queue head as a leader, which might have other unintended consequences. Hopefully, this situation never occurs in the real world. If it does, then we can tackle this in a separate issue.\n\nThe following situation is still taken care of:\n\n\tthere is no LIR involved\n\tall replicas are down\n\n\n\nShalin Shekhar Mangar please review the changes. Thanks. ",
            "id": "comment-14971184"
        },
        {
            "date": "2015-10-23T15:52:03+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks Ishan but I think you missed the test in your latest patch? Its size has decreased from 36kb to 8kb. ",
            "id": "comment-14971221"
        },
        {
            "date": "2015-10-23T15:53:50+0000",
            "author": "Mark Miller",
            "content": "// Marking all live nodes as active.\n\nWe do we do this manually like this? Shouldn't we allow this to happen naturally? ",
            "id": "comment-14971223"
        },
        {
            "date": "2015-10-23T15:55:01+0000",
            "author": "Mark Miller",
            "content": "It seems like what we really want is to make sure the last published state for each replica does not prevent it from becoming the leader? ",
            "id": "comment-14971226"
        },
        {
            "date": "2015-10-23T16:30:41+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "Ah, missed out the test in my last patch. Here it is. ",
            "id": "comment-14971311"
        },
        {
            "date": "2015-10-23T17:25:12+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "It seems like what we really want is to make sure the last published state for each replica does not prevent it from becoming the leader?\n\nDo you mean that removing blockers like LIR is enough? ",
            "id": "comment-14971406"
        },
        {
            "date": "2015-10-29T13:41:01+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "It seems like what we really want is to make sure the last published state for each replica does not prevent it from becoming the leader?\nIt seems to me that there's no easy way to set the last published state of a replica without the replicas doing it themselves. Do you think we should be doing that instead of marking them as active? Or do you think that just clearing the LIR is enough? ",
            "id": "comment-14980440"
        },
        {
            "date": "2015-10-29T15:03:43+0000",
            "author": "Mark Miller",
            "content": "There are two main things I think that prevent replicas from becoming a leader - if there last published state on the clouddescriptor is not ACTIVE or LIR. I thought we would want to clear LIR and perhaps add an ADMIN command that will set the last published state on the clouddescriptor to ACTIVE for each replica. ",
            "id": "comment-14980581"
        },
        {
            "date": "2015-11-04T09:02:00+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "Updated patch. It now depends on SOLR-8233 and SOLR-7989.\n\nAfter removing the part that was doing the force marking of down live replicas as active, the recovery from a situation where all replicas were down (due to LIR) was not working. Reason was that a down replica, when elected as leader, never gets marked as active. Fixed that in SOLR-7989.\n\nMark Miller, Shalin Shekhar Mangar Please review this (as well as SOLR-8233 and SOLR-7989). Also, can you please edit this issue to depend on those two issues (I can't edit, since Shalin created the issue). ",
            "id": "comment-14989157"
        },
        {
            "date": "2015-11-04T10:28:18+0000",
            "author": "Noble Paul",
            "content": "Let's not keep the core admin command as OVERRIDELASTPUBLISHED. This means it can be a generic enough API which may be abused by others for other things. Let's not tell others what we are doing internally and keep the command name opaque\n\nThis particular collection admin operation does not really have to  go to overseer, it can be performed by the receiving node itself because the clearing of LIR node does not have to be done at overseer anyway ",
            "id": "comment-14989269"
        },
        {
            "date": "2015-11-04T11:44:23+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "Thanks for your review, Noble Paul.\n\nLet's not keep the core admin command as OVERRIDELASTPUBLISHED. This means it can be a generic enough API which may be abused by others for other things. Let's not tell others what we are doing internally and keep the command name opaque\n\nThis patch uses FORCEPREPAREFORLEADERSHIP from SOLR-8233. Does this sound fine?\n\nThis particular collection admin operation does not really have to  go to overseer, it can be performed by the receiving node itself because the clearing of LIR node does not have to be done at overseer anyway\n\nThe reason why I wanted to keep it at Overseer was that most cluster management code is there. I can move this to CollectionsHandler instead of OCMH. ",
            "id": "comment-14989410"
        },
        {
            "date": "2015-11-04T12:08:15+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "One down side of not having something like OVERRIDELASTPUBLISHED is that in the test, I couldn't set the last published to DOWN and check if it was set back to ACTIVE by the FORCELEADER. In this updated patch with FORCEPREPAREFORLEADERSHIP, the test has no easy way of setting the last published to down before the API command is called. Not a deal breaker, but just putting it out there. I'm personally fine either ways (or if there's another name that is more suitable). ",
            "id": "comment-14989439"
        },
        {
            "date": "2015-11-05T11:07:42+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "This particular collection admin operation does not really have to go to overseer, it can be performed by the receiving node itself because the clearing of LIR node does not have to be done at overseer anyway\n\nHere is a patch that adds the API command (FORCELEADER) to the CollectionsHandler instead of the OCMH. I couldn't find a way to do this ASYNC, which I could do it at OCMH, did I miss something? Does this look fine? (Noble Paul ?) \n\nI somehow feel doing it in CollectionsHandler is a bit misplaced, and would rather do it at OCMH. But I am fine either ways so long as we do it; both patches are there.\n\nNote: As with the previous patch (that puts the meat into the OCMH), this patch depends on prior application of patches in SOLR-8233 and SOLR-7989. ",
            "id": "comment-14991537"
        },
        {
            "date": "2015-11-05T16:24:21+0000",
            "author": "Mark Miller",
            "content": "I'm kind of split on where it should go. For something simple and brute force like this, CollectionsHandler is probably fine. Either way seems ok.\n\nI wouldn't really worry about it being async if it stays in CollectionsHandler. ",
            "id": "comment-14991913"
        },
        {
            "date": "2015-11-05T19:25:16+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1712851 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1712851 ]\n\nSOLR-7569: A collection API called FORCELEADER when all replicas in a shard are down ",
            "id": "comment-14992304"
        },
        {
            "date": "2015-11-05T19:28:39+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1712852 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1712852 ]\n\nSOLR-7569: changed message ",
            "id": "comment-14992310"
        },
        {
            "date": "2015-11-05T19:37:16+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1712854 from Noble Paul in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1712854 ]\n\nSOLR-7569: A collection API called FORCELEADER when all replicas in a shard are down ",
            "id": "comment-14992322"
        },
        {
            "date": "2015-11-09T11:30:58+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "There was a failure on Windows in one of the tests,\nhttp://jenkins.thetaphi.de/job/Lucene-Solr-trunk-Windows/5385/testReport/org.apache.solr.cloud/ForceLeaderTest/testReplicasInLIRNoLeader/\n\nI'd like to get this change in, please  (SOLR-7569-testfix.patch). Sorry for the trouble. ",
            "id": "comment-14996395"
        },
        {
            "date": "2015-11-11T17:26:29+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1713898 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1713898 ]\n\nSOLR-7989, SOLR-7569: Ignore this test. ",
            "id": "comment-15000722"
        },
        {
            "date": "2015-11-11T17:27:53+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1713899 from Mark Miller in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1713899 ]\n\nSOLR-7989, SOLR-7569: Ignore this test. ",
            "id": "comment-15000726"
        },
        {
            "date": "2015-11-11T17:28:50+0000",
            "author": "Mark Miller",
            "content": "This feature and it's test counts on behavior that was added that was incorrect. See SOLR-7989. I've disabled the test for now. ",
            "id": "comment-15000730"
        },
        {
            "date": "2015-11-11T17:37:32+0000",
            "author": "Mark Miller",
            "content": "A better approach is probably for this API to deal with a DOWN but valid leader itself. It should only ever happen due to manually screwing up LIR and if this API is messing with LIR, it should also fix the ramifications.\n\nPerhaps the last thing the API should do is run through each shard and see if the registered leader is DOWN, and if it is make it ACTIVE (preferably by asking it to publish itself as ACTIVE - we don't want to publish for someone else). If the call waits around to make sure all the leaders come up, this should be simple. ",
            "id": "comment-15000753"
        },
        {
            "date": "2015-11-11T18:01:57+0000",
            "author": "Mark Miller",
            "content": "It should only ever happen due to manually screwing up LIR and if this API is messing with LIR\n\nDown the road though, we will want to solve this for SOLR-7034 and SOLR-7065.\n\nI've taken a crack at making SOLR-7989 work. ",
            "id": "comment-15000797"
        },
        {
            "date": "2015-11-12T15:37:34+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "I've taken a crack at making SOLR-7989 work.\nThanks!\n\nPerhaps the last thing the API should do is run through each shard and see if the registered leader is DOWN, and if it is make it ACTIVE (preferably by asking it to publish itself as ACTIVE - we don't want to publish for someone else). If the call waits around to make sure all the leaders come up, this should be simple.\nThis makes sense. I think this is something that Shalin alluded to (please excuse me if I'm mistaken) when he said, 1. Leader is live but 'down' -> mark it 'active'. The suggestion for the replicas to mark themselves ACTIVE instead of someone else marking them ACTIVE seems like a good thing to do. ",
            "id": "comment-15002251"
        },
        {
            "date": "2015-11-16T16:05:01+0000",
            "author": "Ishan Chattopadhyaya",
            "content": "Can we close this now, and create new JIRAs for future enhancements? Mark Miller, Shalin Shekhar Mangar, Noble Paul? ",
            "id": "comment-15006835"
        },
        {
            "date": "2015-11-17T18:15:08+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1714842 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1714842 ]\n\nSOLR-7569 test failure fix ",
            "id": "comment-15009164"
        },
        {
            "date": "2015-11-17T18:22:40+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1714844 from Noble Paul in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1714844 ]\n\nSOLR-7569 test failure fix ",
            "id": "comment-15009179"
        },
        {
            "date": "2015-11-17T18:25:20+0000",
            "author": "Mike Drob",
            "content": "Can we close this now, and create new JIRAs for future enhancements? Mark Miller, Shalin Shekhar Mangar, Noble Paul?\nI agree with this. ",
            "id": "comment-15009186"
        },
        {
            "date": "2015-11-17T18:38:36+0000",
            "author": "Mark Miller",
            "content": "This was only reopened because the test was ignored due to reverting SOLR-7989. With that resolved, this should be fine. ",
            "id": "comment-15009213"
        }
    ]
}