{
    "id": "LUCENE-5696",
    "title": "Remove Weakhashmap from CachingWrapperFilter",
    "details": {
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved",
        "components": [],
        "affect_versions": "None",
        "status": "Open",
        "fix_versions": []
    },
    "description": "Filters can take up a good amount of space, I think its terrible this thing relies entirely upon weak references.\n\nActually I don't think it should use weak references at all, in my opinion, it should instead use a ConcurrentHashmap and coreClosedListeners to purge things.\n\nWe already ensure and test since LUCENE-5553 that these listeners are always fired on close, even under exceptional cases, so I don't understand why we need weak references anywhere.\n\nSo maybe the fix is to just move up coreClosedListener to AtomicReader? And maybe nuke getCoreCacheKey and getCombinedCoreAndDeletesKey.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14006253",
            "author": "Michael McCandless",
            "content": "+1 to make it explicit. ",
            "date": "2014-05-22T18:25:12+0000"
        },
        {
            "id": "comment-14006254",
            "author": "Adrien Grand",
            "content": "+1 to a concurrent hash table and removing the weak hash map\n\nSo maybe the fix is to just move up coreClosedListener to AtomicReader?\n\nThat would be nice. For example Elasticsearch currently uses a few hacks in order to get the segment reader behind an Atomic reader in order to be able to use the listener mechanism, this change would help get rid of them.\n\nIf the point of the weak hash table was to manage memory, maybe Lucene should also provide a way to have a set of CachingWrapperFilter instances with bounded memory usage? (probably with the help of LUCENE-5695 that would make computing RAM consumption cheaper) ",
            "date": "2014-05-22T18:25:36+0000"
        },
        {
            "id": "comment-14006287",
            "author": "Robert Muir",
            "content": "\nIf the point of the weak hash table was to manage memory, maybe Lucene should also provide a way to have a set of CachingWrapperFilter instances with bounded memory usage? (probably with the help of LUCENE-5695 that would make computing RAM consumption cheaper)\n\nI think it sounds like a good idea, it is pretty tricky because if you are managing a cache of these things, its a map of maps. If we want to allow it to be bounded by memory usage, its more than just a simple policy because of that. At least it seems difficult enough to me to do it \"correctly\" that we should look into making it easier. ",
            "date": "2014-05-22T18:40:54+0000"
        },
        {
            "id": "comment-14006394",
            "author": "Uwe Schindler",
            "content": "If the point of the weak hash table was to manage memory, maybe Lucene should also provide a way to have a set of CachingWrapperFilter instances with bounded memory usage? (probably with the help of LUCENE-5695 that would make computing RAM consumption cheaper)\n\nWeak references can never be meant to manage memory. This is a common misunderstanding. The use of wek references here is to remove the hard reference between the two objects. Indeed its a problem here.\n\nBut if you use a IndexReaderClosedListener it may make the problem worse: The listener has hard reference to the CachingWrapperFilter. Nowadays, if you throw away the CachingWrapper filter, the CachingWrapperFilter is free (as nothing has a reference anymore to the Filter). But once this one \"registers\" itsself to the IndexReader, it has a suddenly a hard reference from IndexReader to the filter -> oh no! Now you have a leak. You need a weak reference in one direction to allow the cleanup. ",
            "date": "2014-05-22T20:06:39+0000"
        },
        {
            "id": "comment-14006409",
            "author": "Robert Muir",
            "content": "\nBut if you use a IndexReaderClosedListener it may make the problem worse\n\nHow will it makes things worse (are you worried about listeners piling up in the static index case?) \n\nIt would be good to not force this to be cleared on GC. For example we could register listeners, but also add an explicit evict() method to CachingWrapperFilter (e.g. to be called in removeEldestEntry or wherever) that would remove the listeners from any still-alive cores explicitly.\n ",
            "date": "2014-05-22T20:26:16+0000"
        },
        {
            "id": "comment-14006451",
            "author": "Uwe Schindler",
            "content": "How will it makes things worse (are you worried about listeners piling up in the static index case?)\n\nThe way how CWF is intended to be used: Code can use it, wrap a filter and throw it away. This would no longer be possible. I know BlockJoin and similar stuff use this that way.\n\nIn my opinion, we should make a FilterManager included with SearcherManager, that has LRU stuff or better cache eviction.\n\nThe weak refs in CWF have nothing to do with memory alocation. If you revert the whole thing (make the listener weak), it would still be the same.\n\nThe only thing that weak refs do bad is: concurrent mark phase gets heavier (which you see as more work done by GC threads). But they don't ever prevent stuff from beeing freed by garbage collector! ",
            "date": "2014-05-22T20:46:39+0000"
        }
    ]
}