{
    "id": "LUCENE-1383",
    "title": "Work around ThreadLocal's \"leak\"",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Bug",
        "fix_versions": [
            "2.4"
        ],
        "affect_versions": "1.9,                                            2.0.0,                                            2.1,                                            2.2,                                            2.3,                                            2.3.1,                                            2.3.2",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Java's ThreadLocal is dangerous to use because it is able to take a\nsurprisingly very long time to release references to the values you\nstore in it.  Even when a ThreadLocal instance itself is GC'd, hard\nreferences to the values you had stored in it are easily kept for\nquite some time later.\n\nWhile this is not technically a \"memory leak\", because eventually\n(when the underlying Map that stores the values cleans up its \"stale\"\nreferences) the hard reference will be cleared, and GC can proceed,\nits end behavior is not different from a memory leak in that under the\nright situation you can easily tie up far more memory than you'd\nexpect, and then hit unexpected OOM error despite allocating an\nextremely large heap to your JVM.\n\nLucene users have hit this many times.  Here's the most recent thread:\n\n  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200809.mbox/%3C6e3ae6310809091157j7a9fe46bxcc31f6e63305fcdc%40mail.gmail.com%3E\n\nAnd here's another:\n\n  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3CF5FC94B2-E5C7-40C0-8B73-E12245B91CEE%40mikemccandless.com%3E\n\nAnd then there's LUCENE-436 and LUCENE-529 at least.\n\nA google search for \"ThreadLocal leak\" yields many compelling hits.\n\nSun does this for performance reasons, but I think it's a terrible\ntrap and we should work around it with Lucene.",
    "attachments": {
        "ScreenHunter_07 Sep. 13 19.13.jpg": "https://issues.apache.org/jira/secure/attachment/12390072/ScreenHunter_07%20Sep.%2013%2019.13.jpg",
        "ScreenHunter_03 Sep. 13 08.43.jpg": "https://issues.apache.org/jira/secure/attachment/12390057/ScreenHunter_03%20Sep.%2013%2008.43.jpg",
        "ScreenHunter_01 Sep. 13 08.40.jpg": "https://issues.apache.org/jira/secure/attachment/12390055/ScreenHunter_01%20Sep.%2013%2008.40.jpg",
        "ScreenHunter_02 Sep. 13 08.42.jpg": "https://issues.apache.org/jira/secure/attachment/12390056/ScreenHunter_02%20Sep.%2013%2008.42.jpg",
        "LUCENE-1383.patch": "https://issues.apache.org/jira/secure/attachment/12389971/LUCENE-1383.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-09-11T21:04:18+0000",
            "content": "Attached patch.  All tests pass.\n\nThe patch adds o.a.l.util.CloseableThreadLocal.  It's a wrapper around ThreadLocal that wraps the values inside a WeakReference, but then also holds a strong reference to the value (to ensure GC doesn't reclaim it) until you call the close method.  On calling close, GC is then free to reclaim all values you had stored, regardless of how long it takes ThreadLocal's implementation to actually release its references.\n\nThere are a couple places in Lucene where I left the current usage of ThreadLocal.\n\nFirst, Analyzer.java uses ThreadLocal to hold reusable token streams.  There is no \"close\" called for Analyzer, so unless we are willing to add a finalizer to call CloseableThreadLocal.close() I think we can leave it.\n\nSecond, some of the contrib/benchmark tasks use ThreadLocal to store per-thread DateFormat which should use tiny memory. ",
            "author": "Michael McCandless",
            "id": "comment-12630367"
        },
        {
            "date": "2008-09-13T15:46:51+0000",
            "content": "I can confirm this patch fixed the memory problem, in general. Thanks!\n\nHowever, if not nitpicking, I did noticed in this patch, during the transition time between the previous index searcher/reader close and the new index searcher/reader open, there is a bump in the memory graph.\n  ___/___\nThere are 2 RAMDirectory instances in the memory for a short period of time.\n\nIn previous Lucene version without Lucene-1195, the memory graph looks more clean, with a V dip.\n____  ____\n         \\/\n\nAnd in one index refresh during the same index run, there were 2 RAMDirectory in the memory for a fairly long time. But not repeatable.\n\nSo I suspect the closing is kind of delayed. And sometimes it could be missed.\n\nAttaching the screenshot for the memory bump, and 2 the memory graphs of the begin and after of 2RAMDirecory in the memory(needed 2 because the time is fairly long) ",
            "author": "Chris Lu",
            "id": "comment-12630784"
        },
        {
            "date": "2008-09-13T15:48:09+0000",
            "content": "A little bump when closing and opening the index. The test did the closing first, then opening. ",
            "author": "Chris Lu",
            "id": "comment-12630785"
        },
        {
            "date": "2008-09-13T15:50:46+0000",
            "content": "Beginning of a memory increase of 2 RAMDirectory in the memory. This is also during a index closing, then opening. ",
            "author": "Chris Lu",
            "id": "comment-12630786"
        },
        {
            "date": "2008-09-13T15:51:36+0000",
            "content": "Endding of a memory increase of 2 RAMDirectory in the memory. This is also during a index closing, then opening. No particular reason, but the RAMDirectory dropped back to 1. ",
            "author": "Chris Lu",
            "id": "comment-12630787"
        },
        {
            "date": "2008-09-13T16:05:04+0000",
            "content": "The test is repeatedly close and open the index. \n\nAfter the repeat close/open test run finished, I closed the RAMDirectory based searcher/reader, and switched to FSDirectory, the RAMDirectory(which is fairly large) was left in the memory. I did several rounds of GC, but the RAMDirectory was not cleaned. At this time, I tried to do a memory dump, but an OOM occured.\n ",
            "author": "Chris Lu",
            "id": "comment-12630789"
        },
        {
            "date": "2008-09-13T18:23:23+0000",
            "content": "Hmm, the sometimes long-lasting bump in Used memory is odd.\n\nWhen you close the old IndexSearcher & RAMDirectory do you forcefully dereference them (set all variables that point to them to null) before then reopening the new ones?  (Seems like you must, since you saw the V shape before LUCENE-1195, but I just want to verify).  If you simply re-assign the variable from old to new then two objects will exist at once until the opening of the new one finishes.\n\nI think the bump may just be GC taking its time collecting the objects, which should be harmless.  Maybe using a WeakReference to a long-lived object causes GC to take longer to collect?\n\nIs it possible to use the profiler to look for a reference to the old RAMDirectory while the bump is \"up\"?  That would be a smoking gun that we do still have a reference, unless it's only via the WeakReference.\n\nOr, could you try lowering the heap size in your JRE to something slightly less than 2X one RAMDirectory (but not so low that the \"other stuff\" you have in the JRE can't fit).  This would force GC to work harder / more immediately in reclaiming the unreachable objects. ",
            "author": "Michael McCandless",
            "id": "comment-12630811"
        },
        {
            "date": "2008-09-14T01:26:50+0000",
            "content": "On second thought, I think this is normal behavior. Like you said, the GC may take a while before really cleaning up stuff. So if I did not wait a while between the close and open, the little bump should be normal.\n\nSo I would say this bug is fixed. Thanks! ",
            "author": "Chris Lu",
            "id": "comment-12630845"
        },
        {
            "date": "2008-09-14T02:14:54+0000",
            "content": "this is the V shape memory graph using the patch. This also confirms the leak is fixed. ",
            "author": "Chris Lu",
            "id": "comment-12630846"
        },
        {
            "date": "2008-09-14T10:14:43+0000",
            "content": "OK super!  Thanks for testing Chris.  I'll commit shortly. ",
            "author": "Michael McCandless",
            "id": "comment-12630872"
        },
        {
            "date": "2008-09-14T10:33:51+0000",
            "content": "Thanks Chris! ",
            "author": "Michael McCandless",
            "id": "comment-12630875"
        },
        {
            "date": "2008-09-14T10:34:02+0000",
            "content": "Committed revision 695184. ",
            "author": "Michael McCandless",
            "id": "comment-12630876"
        },
        {
            "date": "2008-10-01T15:32:23+0000",
            "content": "Even if the issue is closed, for those who want to know why ThreadLocal had to be fixed : http://www.javaspecialists.eu/archive/Issue164.html\n\n\"Best Practices\n\nIf you must use ThreadLocal, make sure that you remove the value as soon as you are done with it and preferably before you return your thread to a thread pool. Best practice is to use remove() rather than set(null), since that would cause the WeakReference to be removed immediately, together with the value.\n\nKind regards\n\nHeinz\" ",
            "author": "Adrian Tarau",
            "id": "comment-12636063"
        },
        {
            "date": "2008-10-01T16:03:56+0000",
            "content": "It doesn't need to be \"fixed\".\n\nIt works fine, you need to understand how to use it properly which may require calling remove() if your threads are long-lived.\n\nAlso, remove() was only added in 1.5, and that is why that technique is not valid for Lucene. It is also not valid, since you need to clear all values across all threads, remove() only clears the entry for the calling thread.\n\nThe current patch solves the problem suitably for Lucene, at the expense of some performance degradation.\n ",
            "author": "robert engels",
            "id": "comment-12636075"
        },
        {
            "date": "2008-10-01T16:21:09+0000",
            "content": "The current patch solves the problem suitably for Lucene, at the expense of some performance degradation.\n\nThe performance degradation should be negligible in the case of long-lived threads.  No synchronized code was added in the get() path.\n\nIt is also not valid, since you need to clear all values across all threads, remove() only clears the entry for the calling thread.\n\nThis best practice does work correctly: you're supposed to call remove() from the very thread that had inserted something into the ThreadLocal.\n\nBesides the 1.5 issue, this is also difficult for Lucene because we don't have a clean point to \"know\" when the thread has finished interacting with Lucene.  A thread comes out of the pool, runs a search, gathers docIDs from in a collector, returns from the search, one by one looks up stored docs / term vectors for these docIDs, maybe does secondary search up front to build up filters, etc., finishes rendering the result and returns to the pool.  Unless we create a new method \"detachThread(...)\" somewhere in Lucene, there is no natural point now to do the remove().  And I don't like creating such a method because nobody will ever know they need to call it in their App server until they start hitting cryptic OOMs. ",
            "author": "Michael McCandless",
            "id": "comment-12636082"
        },
        {
            "date": "2008-10-01T16:30:18+0000",
            "content": "Ok, maybe \"fixed\" was too much and it sounded like ThreadLocal has a problem. ThreadLocal works fine except that it is tricky with long-lived threads.\n\nAnyway, it is a good article in case somebody wants to understand why this change in Lucene. ",
            "author": "Adrian Tarau",
            "id": "comment-12636085"
        },
        {
            "date": "2008-10-01T16:41:15+0000",
            "content": "Michael,\n\nMaybe some sort of cleanup method should be created even if regular users will not call it. \n\nI presume it can be highlighted somewhere in the documentation, like in \"Best practices\" : http://wiki.apache.org/lucene-java/BestPractices \n\nI would really like to be able to control this, to be able to clean thread(s) storage programmatically. ",
            "author": "Adrian Tarau",
            "id": "comment-12636089"
        },
        {
            "date": "2008-10-01T16:56:05+0000",
            "content": "You cannot control this 'externally\", since there is no way to force the cleaning of the stale entries (no API method).\n\nThe current patch sort of allows this - the entries are not cleared but the weak references in them are - via the close(), but if this method is called at the wrong time, everything will break (since the stream needs to be cached), thus it is not exposed to the outside world.\n\nIf you want to cleanup programatically, close the index readers and writers.\n ",
            "author": "robert engels",
            "id": "comment-12636095"
        },
        {
            "date": "2008-10-01T17:05:32+0000",
            "content": "If closing all index readers and writers releases all Lucene  thread locals it's great.\n\nThanks. ",
            "author": "Adrian Tarau",
            "id": "comment-12636097"
        }
    ]
}