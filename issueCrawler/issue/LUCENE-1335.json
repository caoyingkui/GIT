{
    "id": "LUCENE-1335",
    "title": "Correctly handle concurrent calls to addIndexes, optimize, commit",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/index"
        ],
        "type": "Bug",
        "fix_versions": [
            "2.4"
        ],
        "affect_versions": "2.3,                                            2.3.1",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Spinoff from here:\n\n    http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3Cc7b302c50807111018j58b6d08djd56b5889f6b3780d@mail.gmail.com%3E",
    "attachments": {
        "LUCENE-1335.patch": "https://issues.apache.org/jira/secure/attachment/12387553/LUCENE-1335.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-08-05T10:08:40+0000",
            "content": "\nAttached patch.  Here're the changes:\n\n\n\tOnly one addIndexes can run at once, so call to 2nd or more\n    addIndexes just blocks until the one is done.\n\n\n\n\n\tclose() and rollback() wait for any running addIndexes to finish\n    and then blocks new addIndexes calls\n\n\n\n\n\tcommit() waits for any running addIndexes, or any already running\n    commit, to finish, then quickly takes a snapshot of the segments\n    and syncs the files referenced by that snapshot.  While syncing is\n    happening addIndexes are then allowed to run again.\n\n\n\n\n\toptimize() is allowed to run concurrently with addIndexes; the two\n    simply wait for their respective merges to finish.\n\n\n\nI think we should not make any API promises about what it means when\nthese methods (commit, close, rollback, optimize, addIndexes) are\ncalled concurrently from different threads, except that the methods\nall work correctly, IndexWriter won't throw an errant exception, and\nyour index won't become corrupt.\n\nI made one additional change which is technically a break in backwards\ncompatibility, but I think  a minor acceptable one: I no longer\nallow the same Directory to be passed into addIndexes* more than once.\nThis was necessary because we identify a SegmentInfo by its\nDirectory/name pair, and passing in the same Directory allowed dup\nSegmentInfo instances to enter SegmentInfos, which is dangerous.\n\nI'll wait a few days before committing. ",
            "author": "Michael McCandless",
            "id": "comment-12619834"
        },
        {
            "date": "2008-08-05T14:14:28+0000",
            "content": "I think we should not make any API promises about what it means when\nthese methods (commit, close, rollback, optimize, addIndexes) are\ncalled concurrently from different threads, except that the methods\nall work correctly, IndexWriter won't throw an errant exception, and\nyour index won't become corrupt.\n\nAgree... higher level synchronization by the user is the right way to ensure/enforce an ordering. ",
            "author": "Yonik Seeley",
            "id": "comment-12619890"
        },
        {
            "date": "2008-08-07T18:41:43+0000",
            "content": "I've just started reviewing this patch.\nSince doWait() can return after 1 sec, the pattern is to use a while loop with the condition that caused it to be called.\nBut in some cases, it's hard to tell if the code is correct.... for example copyExternalSegments() is hard because of the other non-trival code code in the loop where  it's not clear if it's safe/correct to call that code again.  Perhaps registerMerge() detects the conflict with another merge with the same segments and that's what keeps things correct?  Comments to the effect of why it's OK to run certain code more than once would be very welcome. ",
            "author": "Yonik Seeley",
            "id": "comment-12620713"
        },
        {
            "date": "2008-08-07T19:28:34+0000",
            "content": "Thanks, Yonik.  I'll add a comment there, and any other places where I call doWait that look similarly confusing... ",
            "author": "Michael McCandless",
            "id": "comment-12620723"
        },
        {
            "date": "2008-08-07T19:44:26+0000",
            "content": "Improved comments in expungeDeletes & copyExternalSegments. ",
            "author": "Michael McCandless",
            "id": "comment-12620727"
        },
        {
            "date": "2008-08-12T11:14:23+0000",
            "content": "Yonik, any more feedback on this patch? ",
            "author": "Michael McCandless",
            "id": "comment-12621774"
        },
        {
            "date": "2008-08-19T20:02:02+0000",
            "content": "I plan to commit this in a day or two. ",
            "author": "Michael McCandless",
            "id": "comment-12623775"
        },
        {
            "date": "2008-08-22T14:40:36+0000",
            "content": "Hi Mike, could you update the patch? I cannot apply it. Thanks! ",
            "author": "Ning Li",
            "id": "comment-12624851"
        },
        {
            "date": "2008-08-22T15:23:49+0000",
            "content": "OK, attached refreshed patch to trunk. ",
            "author": "Michael McCandless",
            "id": "comment-12624867"
        },
        {
            "date": "2008-08-22T23:15:25+0000",
            "content": "I agree that we should not make any API promises about what\nit means when the methods (commit, close, rollback, optimize,\naddIndexes) are called concurrently from different threads.\nThe discussion below is on their current behavior.\n\n> Only one addIndexes can run at once, so call to 2nd or more\n> addIndexes just blocks until the one is done.\n\nThis is achieved by the read-write lock.\n\n> close() and rollback() wait for any running addIndexes to finish\n> and then blocks new addIndexes calls\n\nJust to clarify: close(waitForMerges=false) and rollback() make\nan ongoing addIndexes[NoOptimize](dirs) abort, but wait for\naddIndexes(readers) to finish. It'd be nice if they make any\naddIndexes* abort for a quick shutdown, but that's for later.\n\n> commit() waits for any running addIndexes, or any already running\n> commit, to finish, then quickly takes a snapshot of the segments\n> and syncs the files referenced by that snapshot. While syncing is\n> happening addIndexes are then allowed to run again.\n\ncommit() and commit(long) use the read-write lock to wait for\na running addIndexes. \"committing\" is used to serialize commit()\ncalls. Why isn't it also used to serialize commit(long) calls?\n\n> optimize() is allowed to run concurrently with addIndexes; the two\n> simply wait for their respective merges to finish.\n\nThis is nice.\n\nMore detailed comments:\n\n\tIn finishMerges, acquireRead and releaseRead are both called.\n  Isn't addIndexes allowed again?\n\n\n\n\n\tIn copyExternalSegments, merges involving external segments\n  are carried out in foreground. So why the changes? To relax\n  that assumption? But other part still makes the assumption.\n\n\n\n\n\taddIndexes(readers) should optimize before startTransaction, no?\n\n\n\n\n\tThe newly added method segString(dir) in SegmentInfos is\n  not used anywhere.\n\n ",
            "author": "Ning Li",
            "id": "comment-12624998"
        },
        {
            "date": "2008-08-23T12:47:11+0000",
            "content": "\n\nJust to clarify: close(waitForMerges=false) and rollback() make\nan ongoing addIndexes[NoOptimize](dirs) abort, but wait for\naddIndexes(readers) to finish. It'd be nice if they make any\naddIndexes* abort for a quick shutdown, but that's for later.\n\nTrue, agreed.\n\n\ncommit() and commit(long) use the read-write lock to wait for\na running addIndexes. \"committing\" is used to serialize commit()\ncalls. Why isn't it also used to serialize commit(long) calls?\n\nIt's because commit() calls prepareCommit(), which throws a\n\"prepareCommit was already called\" exception if the commit was already\nprepared.  Whereas commit(long) doesn't call prepareCommit (eg, it\ndoesn't need to flush).  Without this, I was hitting exceptions in one\nof the tests that calls commit() from multiple threads at the same\ntime.\n\n\n\n\tIn finishMerges, acquireRead and releaseRead are both called.\n      Isn't addIndexes allowed again?\n\n\nThis is to make sure any just-started addIndexes cleanly finish or\nabort before we enter the wait loop.  I was seeing cases where the\nwait loop would think no more merges were pending, but in fact an\naddIndexes was just getting underway and was about to start merging.\nIt's OK if a new addIndexes call starts up, because it'll be forced to\ncheck the stop conditions (closing=true or stopMerges=true) and then\nabort the merges.  I'll add comments to this effect.\n\n\n\n\tIn copyExternalSegments, merges involving external segments\n      are carried out in foreground. So why the changes? To relax\n      that assumption? But other part still makes the assumption.\n\n\nThis method has always carried out merges in the FG, but it's in fact\npossible that a BG merge thread on finishing a previous merge may pull\na merge involving external segments.  So I changed this method to wait\nfor all such BG merges to complete, because it's not allowed to return\nuntil there are no more external segments in the index.\n\nIt is tempting to fully schedule these external merges (ie allow them\nto run in BG), but there is a problem: if there is some error on doing\nthe merge, we need that error to be thrown in the FG thread calling\ncopyExternalSegments (so the transcaction above unwinds).  (Ie we\ncan't just stuff these external merges into the merge queue then wait\nfor their completely).  So I think we need to leave is as is?\n\n\n\n\taddIndexes(readers) should optimize before startTransaction, no?\n\n\n\nI had to move the optimize() inside the transaction because it could\nhappen that after the optimize() is finished, some other thread sneaks\nin a call to addIndexes* and gets additional segments added to the\nindex such that by the time we start the transaction we now have more\nthan one segment.\n\nBut this change will tie up more disk space than addIndexes used to\n(since it will also rollback the optimize on hitting an exception).\nReally I just need to pre-acquire the write lock, then I can leave\noptimize() out of the transaction.  I'll do that.\n\n\n\n\tThe newly added method segString(dir) in SegmentInfos is\n      not used anywhere.\n\n\n\nYeah I was using this for internal debugging, and I think it's\ngenerally useful for future debugging, so I left it in. ",
            "author": "Michael McCandless",
            "id": "comment-12625048"
        },
        {
            "date": "2008-08-23T13:00:25+0000",
            "content": "New patch incorporating Ning's comments (thanks Ning!). ",
            "author": "Michael McCandless",
            "id": "comment-12625052"
        },
        {
            "date": "2008-08-23T15:49:58+0000",
            "content": "> It's because commit() calls prepareCommit(), which throws a\n> \"prepareCommit was already called\" exception if the commit was already\n> prepared.  Whereas commit(long) doesn't call prepareCommit (eg, it\n> doesn't need to flush).  Without this, I was hitting exceptions in one\n> of the tests that calls commit() from multiple threads at the same\n> time.\n\nIs it better to simplify things by serializing all commit()/commit(long) calls?\n\n> This is to make sure any just-started addIndexes cleanly finish or\n> abort before we enter the wait loop.  I was seeing cases where the\n> wait loop would think no more merges were pending, but in fact an\n> addIndexes was just getting underway and was about to start merging.\n> It's OK if a new addIndexes call starts up, because it'll be forced to\n> check the stop conditions (closing=true or stopMerges=true) and then\n> abort the merges.  I'll add comments to this effect.\n\nI wonder if we can simplify the logic... Currently in setMergeScheduler,\nmerges can start between finishMerges and set the merge scheduler.\nThis one can be fixed by making setMergeScheduler synchronized.\n\n> This method has always carried out merges in the FG, but it's in fact\n> possible that a BG merge thread on finishing a previous merge may pull\n> a merge involving external segments.  So I changed this method to wait\n> for all such BG merges to complete, because it's not allowed to return\n> until there are no more external segments in the index.\n\nHmm... so merges involving external segments may be in FG or BG?\nSo copyExternalSegments not only copies external segments, but also\nwaits for BG merges involving external segments to finish. We need\na better name?\n\n> It is tempting to fully schedule these external merges (ie allow them\n> to run in BG), but there is a problem: if there is some error on doing\n> the merge, we need that error to be thrown in the FG thread calling\n> copyExternalSegments (so the transcaction above unwinds).  (Ie we\n> can't just stuff these external merges into the merge queue then wait\n> for their completely).\n\nThen what about those BG merges involving external segments? ",
            "author": "Ning Li",
            "id": "comment-12625078"
        },
        {
            "date": "2008-08-23T16:11:39+0000",
            "content": "\n\n\n> It's because commit() calls prepareCommit(), which throws a\n> \"prepareCommit was already called\" exception if the commit was already\n> prepared. Whereas commit(long) doesn't call prepareCommit (eg, it\n> doesn't need to flush). Without this, I was hitting exceptions in one\n> of the tests that calls commit() from multiple threads at the same\n> time.\n\nIs it better to simplify things by serializing all commit()/commit(long) calls?\n\nI don't think so: with autoCommit=true, the merges calls commit(long)\nafter finishing, and I think we want those commit calls to run\nconcurrently?\n\n\n> This is to make sure any just-started addIndexes cleanly finish or\n> abort before we enter the wait loop. I was seeing cases where the\n> wait loop would think no more merges were pending, but in fact an\n> addIndexes was just getting underway and was about to start merging.\n> It's OK if a new addIndexes call starts up, because it'll be forced to\n> check the stop conditions (closing=true or stopMerges=true) and then\n> abort the merges. I'll add comments to this effect.\n\nI wonder if we can simplify the logic... Currently in setMergeScheduler,\nmerges can start between finishMerges and set the merge scheduler.\nThis one can be fixed by making setMergeScheduler synchronized.\n\nGood catch \u2013 I'll make setMergeScheduler synchronized.\n\n\n> This method has always carried out merges in the FG, but it's in fact\n> possible that a BG merge thread on finishing a previous merge may pull\n> a merge involving external segments. So I changed this method to wait\n> for all such BG merges to complete, because it's not allowed to return\n> until there are no more external segments in the index.\n\nHmm... so merges involving external segments may be in FG or BG?\nSo copyExternalSegments not only copies external segments, but also\nwaits for BG merges involving external segments to finish. We need\na better name?\n\nSure we can change the name \u2013 do you have one in mind?  Maybe\n\"resolveExternalSegments\" or \"waitForExternalSegments\"?\n\n\n> It is tempting to fully schedule these external merges (ie allow them\n> to run in BG), but there is a problem: if there is some error on doing\n> the merge, we need that error to be thrown in the FG thread calling\n> copyExternalSegments (so the transcaction above unwinds). (Ie we\n> can't just stuff these external merges into the merge queue then wait\n> for their completely).\n\nThen what about those BG merges involving external segments?\n\nWhat'll happen is the BG merge will hit an exception, roll itself\nback, and then the FG thread will pick up the merge and try again.\nLikely it'll hit the same exception, which is then thrown back to the\ncaller.  It may not hit an exception, eg say it was disk full: the BG\nmerge was probably trying to merge 10 segments, whereas the FG merge\nis just copying over the 1 segment.  So it may complete successfully\ntoo. ",
            "author": "Michael McCandless",
            "id": "comment-12625079"
        },
        {
            "date": "2008-08-25T18:36:21+0000",
            "content": "> I don't think so: with autoCommit=true, the merges calls commit(long)\n> after finishing, and I think we want those commit calls to run\n> concurrently?\n\nAfter we disable autoCommit, all commit calls will be serialized, right?\n\n\n> What'll happen is the BG merge will hit an exception, roll itself\n> back, and then the FG thread will pick up the merge and try again.\n> Likely it'll hit the same exception, which is then thrown back to the\n> caller.  It may not hit an exception, eg say it was disk full: the BG\n> merge was probably trying to merge 10 segments, whereas the FG merge\n> is just copying over the 1 segment.  So it may complete successfully\n> too.\n\nBack to the issue of running an external merge in BG or FG.\nIn ConcurrentMergeScheduler.merge, an external merge is run in FG,\nnot in BG. But in ConcurrentMergeScheduler.MergeThread.run,\nwhether a merge is external is no longer checked. Why this difference? ",
            "author": "Ning Li",
            "id": "comment-12625455"
        },
        {
            "date": "2008-08-25T21:30:33+0000",
            "content": "\n> I don't think so: with autoCommit=true, the merges calls commit(long)\n> after finishing, and I think we want those commit calls to run\n> concurrently?\n\nAfter we disable autoCommit, all commit calls will be serialized, right?\n\nRight.\n\n\nBack to the issue of running an external merge in BG or FG.\nIn ConcurrentMergeScheduler.merge, an external merge is run in FG,\nnot in BG. But in ConcurrentMergeScheduler.MergeThread.run,\nwhether a merge is external is no longer checked. Why this difference?\nGood point!  We no longer need to check for isExternal in CMS's merge() method \u2013 we can run all merges in the BG.  In fact I think it's no longer necessary to even compute & record isExternal (this was its only use).  Hmmm, except when I take this out I'm seeing testAddIndexOnDiskFull hang.  I'll dig. ",
            "author": "Michael McCandless",
            "id": "comment-12625521"
        },
        {
            "date": "2008-08-26T16:06:37+0000",
            "content": "OK new patch attached with changes discussed above.\n\nI did fix CMS to happily perform merges involving external segments\nwith its BG threads.  The hang I was seeing before was because as each\nBG thread hit the disk-full exception (in the test), it would abort\nthat thread, and eventually no threads were doing merges even though\nmerges were still pending.  So copyExternalSegments would then\nwait forever.\n\nThe fix was simple: I changed resolveExternalSegments (renamed from\ncopyExternalSegments) to pick up any pending merges that involve\nexternal segments and run the merge itself, only falling back to the\n\"wait\" call when all such merges were already in progress in CMS.\nThis way the disk full error is hit in the FG and the transaction (in\naddIndexesNoOptimize) unwinds. ",
            "author": "Michael McCandless",
            "id": "comment-12625777"
        },
        {
            "date": "2008-08-27T15:23:24+0000",
            "content": "Maybe this should be a separate JIRA issue. In doWait(), the comment says \"as a defense against thread timing hazards where notifyAll() falls to be called, we wait for at most 1 second...\" In some cases, it seems that notifyAll() simply isn't called, such as some of the cases related to runningMerges. Maybe we should take a closer look at and possibly simplify the concurrency control in IndexWriter, especially when autoCommit is disabled? ",
            "author": "Ning Li",
            "id": "comment-12626158"
        },
        {
            "date": "2008-08-27T22:21:21+0000",
            "content": "Maybe we should take a closer look at and possibly simplify the concurrency control in IndexWriter, especially when autoCommit is disabled?\n\nI agree \u2013 I'm looking forward to taking autoCommit=true case out in 3.0.  I'll try to simplify the concurrency control at that point, and test for any deadlocks if doWait is replaced with the \"real\" wait(), to catch any missing notifyAll()'s. ",
            "author": "Michael McCandless",
            "id": "comment-12626344"
        },
        {
            "date": "2008-08-29T00:19:21+0000",
            "content": "Ning (or anyone), any more feedback on this one?  Else I plan to commit soon... ",
            "author": "Michael McCandless",
            "id": "comment-12626805"
        },
        {
            "date": "2008-08-30T17:17:04+0000",
            "content": "Committed revision 690537 ",
            "author": "Michael McCandless",
            "id": "comment-12627222"
        }
    ]
}