{
    "id": "LUCENE-7396",
    "title": "Speed up flush of points",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [
            "6.2",
            "7.0"
        ],
        "priority": "Minor",
        "status": "Closed",
        "type": "Improvement"
    },
    "description": "1D points already have an optimized merge implementation which works when points come in order. So maybe we could make IndexWriter's PointValuesWriter sort before feeding the PointsFormat and somehow propagate the information to the PointsFormat?\n\nThe benefit is that flushing could directly stream points to disk with little memory usage.",
    "attachments": {
        "LUCENE-7396.patch": "https://issues.apache.org/jira/secure/attachment/12820001/LUCENE-7396.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15392480",
            "author": "Adrien Grand",
            "date": "2016-07-25T18:54:49+0000",
            "content": "Here is a patch that demonstrates the idea. PointValuesWriter sorts before feeding the PointsFormat's writer and Lucene60PointsWriter.addField consumes the PointsReader twice in the one dimension case: once to detect whether it is sorted, and once to write values similarly to the way merging works. In spite of the fact that the reader is consumed twice, I saw better indexing performance of IndexAndSearchOpenStreetMaps1D: 3 runs on master gave me 74, 78 and 75 seconds while 3 runs with this patch gave me 65, 67 and 67 seconds.\n\nIt would be nice if somehow we could propagate the information that the reader is sorted rather than having to iterate it twice? "
        },
        {
            "id": "comment-15392481",
            "author": "Michael McCandless",
            "date": "2016-07-25T18:54:50+0000",
            "content": "+1, cool! "
        },
        {
            "id": "comment-15394200",
            "author": "Adrien Grand",
            "date": "2016-07-26T17:54:18+0000",
            "content": "Here is a patch that uses a different approach. Flush passes a special implementation of a PointsReader that allows points to be reordered, so that codecs can sort points in the order that they are interested in. The benefit compared to the previous patch is that it is not specific to a codec anymore and also that it can be used in the multi-dimensional case. I got the following flush times (as reported by the IndexWriter log) with a 1GB buffer:\n\n\n\n\n Flush time (ms)\nmaster\npatch\n\n\nIndexAndSearchOpenStreetMaps1D (1 dim)\n31089\n18954 (-39.0%)\n\n\nIndexAndSearchOpenStreetMaps (2 dims)\n123461\n85235 (-30.1%)\n\n\n\n\n\nThis looks encouraging, especially given that it also uses less memory than the current approach. However the patch is a bit disappointing in that it has a completely different implementation of the writing of the tree depending on whether the input can be reordered or not. I'll look into whether I can clean this up a bit. "
        },
        {
            "id": "comment-15394781",
            "author": "Michael McCandless",
            "date": "2016-07-26T23:45:04+0000",
            "content": "These results are awesome!  I tested IndexAndSearchOpenStreetMaps1D and saw good indexing gains.\n\nHowever, I also tested with the NYC taxi data (http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml).  Each of these docs has ~20 points, and unfortunately somehow writing points (from the IW log) is a bit (10-20%) slower.  I wonder if something about the data distribution somehow affects the performance change here?\n\nAlso, I don't think we should pre-budget into IW's buffer for the int[] ords we allocate?  That is really a transient thing, only allocated (and then freed, or at least reclaimable by GC) for the one field currently writing its points, so I think it's fair to not count that against IW's buffer?  It's allowed that IW allocates heap beyond its RAM buffer for transient things like this... "
        },
        {
            "id": "comment-15394797",
            "author": "Michael McCandless",
            "date": "2016-07-26T23:53:56+0000",
            "content": "I added a sop for how long it takes to flush each field's points.\n\nTrunk:\n\n127.0.0.1: IW 0 [2016-07-26T23:44:01.962Z; LuceneIndexing-1-thread-1]: 2047 msec to write docValues\n127.0.0.1: FIELD surcharge: 2218.62956 msec\n127.0.0.1: FIELD pick_up_lon: 1968.539463 msec\n127.0.0.1: FIELD mta_tax: 1967.196068 msec\n127.0.0.1: FIELD fare_amount: 2179.47771 msec\n127.0.0.1: FIELD trip_distance: 2042.19908 msec\n127.0.0.1: FIELD pick_up_lat: 2002.735026 msec\n127.0.0.1: FIELD drop_off_date_time: 1315.330631 msec\n127.0.0.1: FIELD tolls_amount: 1942.725832 msec\n127.0.0.1: FIELD passenger_count: 797.681094 msec\n127.0.0.1: FIELD drop_off_lon: 1937.206549 msec\n127.0.0.1: FIELD total_amount: 1987.058896 msec\n127.0.0.1: FIELD pick_up_date_time: 1296.005252 msec\n127.0.0.1: FIELD tip_amount: 2063.410329 msec\n127.0.0.1: FIELD drop_off_lat: 2045.020601 msec\n127.0.0.1: IW 0 [2016-07-26T23:44:27.726Z; LuceneIndexing-1-thread-1]: 25764 msec to write points\n\n\n\nand patch:\n\n\n127.0.0.1: IW 0 [2016-07-26T23:49:54.494Z; LuceneIndexing-1-thread-1]: 2033 msec to write docValues\n127.0.0.1: FIELD surcharge: 2137.926903 msec\n127.0.0.1: FIELD pick_up_lon: 2511.391725 msec\n127.0.0.1: FIELD mta_tax: 2144.822578 msec\n127.0.0.1: FIELD fare_amount: 3232.977894 msec\n127.0.0.1: FIELD trip_distance: 2545.771801 msec\n127.0.0.1: FIELD pick_up_lat: 2939.796276 msec\n127.0.0.1: FIELD drop_off_date_time: 1272.857191 msec\n127.0.0.1: FIELD tolls_amount: 2042.863782 msec\n127.0.0.1: FIELD passenger_count: 565.551751 msec\n127.0.0.1: FIELD drop_off_lon: 2493.79608 msec\n127.0.0.1: FIELD total_amount: 2596.043882 msec\n127.0.0.1: FIELD pick_up_date_time: 1308.397927 msec\n127.0.0.1: FIELD tip_amount: 2316.962831 msec\n127.0.0.1: FIELD drop_off_lat: 2748.935673 msec\n127.0.0.1: IW 0 [2016-07-26T23:50:25.415Z; LuceneIndexing-1-thread-1]: 30920 msec to write points\n\n\n\nThis is using 1 thread w/ 1 GB IW buffer.\n\nSome fields seem to take similar time, but others are sizably different (e.g. the lat/lon points) ... odd. "
        },
        {
            "id": "comment-15394801",
            "author": "Michael McCandless",
            "date": "2016-07-26T23:55:27+0000",
            "content": "Also, for these tests, I fixed IW's buffer to be the same (i.e. didn't count the int[] ords against IW's buffer), so that in both cases we are flushing after the same number of docs. "
        },
        {
            "id": "comment-15395943",
            "author": "Adrien Grand",
            "date": "2016-07-27T16:32:37+0000",
            "content": "I looked into the slow down with Mike. The radix sort I was using in the 1D case has a fallback to quicksort when the range gets more narrow, which was pretty slow since it would call ByteBlockPool.readBytes for every single byte, it should be better now. I suspect I did not hit the problem with IndexAndSearchOpenStreetMaps1D because most of the time was spent on the first levels of recursion.\n\nThe 2D case also got improved by using a radix select when recursively building the bkd tree instead of quickselect. The tie breaking on doc ids got improved by only looking at relevant bytes since we can know the number of required bits up-front thanks to maxDoc. And IW does not pre-budget ords anymore.\n\nI got the following IW logs when running IndexTaxis and IndexAndSearchOpenStreetMaps:\n\n\nmaster IndexAndSearchOpenStreetMaps, rambuffer=128MB\nIW 0 [2016-07-27T15:38:21.308Z; Thread-0]: 17525 msec to write points\nIW 0 [2016-07-27T15:38:44.657Z; Thread-0]: 16746 msec to write points\nIW 0 [2016-07-27T15:39:08.278Z; Thread-0]: 16982 msec to write points\nIW 0 [2016-07-27T15:39:32.613Z; Thread-0]: 17568 msec to write points\nIW 0 [2016-07-27T15:39:56.056Z; Thread-0]: 16684 msec to write points\nIW 0 [2016-07-27T15:40:06.646Z; main]: 7324 msec to write points\n\nmaster IndexTaxis, first 4 flushes\nIW 0 [2016-07-27T15:42:10.401Z; Thread-0]: 34422 msec to write points\nIW 0 [2016-07-27T15:43:15.561Z; Thread-0]: 32306 msec to write points\nIW 0 [2016-07-27T15:44:20.702Z; Thread-0]: 31753 msec to write points\nIW 0 [2016-07-27T15:45:24.920Z; Thread-0]: 32340 msec to write points\n\npatch IndexAndSearchOpenStreetMaps, ramBuffer=128MB\nIW 0 [2016-07-27T15:55:49.959Z; Thread-0]: 10581 msec to write points\nIW 0 [2016-07-27T15:56:08.098Z; Thread-0]: 10306 msec to write points\nIW 0 [2016-07-27T15:56:25.445Z; Thread-0]: 10226 msec to write points\nIW 0 [2016-07-27T15:56:42.513Z; Thread-0]: 10308 msec to write points\nIW 0 [2016-07-27T15:56:59.898Z; Thread-0]: 10162 msec to write points\nIW 0 [2016-07-27T15:57:08.497Z; main]: 4593 msec to write points\n\npatch IndexTaxis, first 4 flushes\nIW 0 [2016-07-27T15:47:10.906Z; Thread-0]: 25673 msec to write points\nIW 0 [2016-07-27T15:48:06.356Z; Thread-0]: 23615 msec to write points\nIW 0 [2016-07-27T15:49:03.327Z; Thread-0]: 23915 msec to write points\nIW 0 [2016-07-27T15:49:59.424Z; Thread-0]: 23482 msec to write points\n\n "
        },
        {
            "id": "comment-15397340",
            "author": "Michael McCandless",
            "date": "2016-07-28T09:56:48+0000",
            "content": "Thanks Adrien Grand, I also saw similar initial speedups when running\nIndexTaxis.java from luceneutil.  I'll re-index all 1.2 B points\nw/ N threads and compare performance shortly.\n\nThis is a very impressive change; here's my attempt at the high-level\nsummary:\n\n\n\tImproves utility apis for the select algorithm: new abstract\n    Selector class, with Quick/Radix/IntroSelector impls\n\n\n\n\n\tAt flush, takes advantage of the fact that all values are already\n    in heap (IW's RAM buffer), more cleanly than LUCENE-7390 (should\n    we revert that?)\n\n\n\n\n\tFix 1D BKD flushing to also use the optimized writing that we do at\n    merge\n\n\n\n\n\tFix N dim flushing to avoid up front sort on each dimension and\n    instead use select algo at each recursion to find the split\n    point, and only do a sort at each leaf block.\n\n\n\n\n\tThe general case (merging, N dims) still requires up front sort,\n    likely using offline sorter\n\n\n\nUsing select to split on each dim is faster for at least two reasons:\n\n\n\tSelect algo in the typical case is linear time, and N * log(N)\n        sorting 1000 small chunks that are 1000 times smaller each is\n        less work: N * log(N) vs N * log(N/1000).\n\n\n\n\n\tThe sorting in the leaf block is only in the 1 dimension\n        needed for the run-length compression, vs N dimensions\n        currently.\n\n\n\nMaybe MutablePointsReader should expose public byte getByteAt(int i, int bytePos);?\nThis would save copying a whole value just because you\nwant to see a specific byte.\n\nThat's very clever how you logically \"concatenate\" the packed docID\nonto the end of the byte[] values that we are selecting in BKDWriter:\n\n\n    final int shift = bitsPerDocId - ((k - cmpBytes + 1) << 3);\n    return (reader.getDocID(i) >>> Math.max(0, shift)) & 0xff;\n\n\n\nI hope our points test cases are stressful enough in testing this\ntie break case!\n\nOn line 522 of BKDWriter.java can't byte[] pivot = scratch1 be\nfinal?  And also line 1426?\n\nThe javadocs for RadixSelector.getFallbackSelector say sorter\nbut should be selector?  Can you improve those javadacs,\ne.g. fallback is used when too much recursion has happened or when the\nrange is tiny?\n\nCan we remove RadixSelector.quickSelect and inline it into the one\nplace it's called in select?\n\nHmm, why are we changing ord from long to int here?:\n\n\n   // only called from assert\n-  private boolean valueInOrder(long ord, byte[] lastPackedValue, byte[] packedValue, int packedValueOffset) {\n-    if (ord > 0 && StringHelper.compare(bytesPerDim, lastPackedValue, 0, packedValue, packedValueOffset) > 0) {\n-      throw new AssertionError(\"values out of order: last value=\" + new BytesRef(lastPackedValue) + \" current value=\" + new BytesRef(packedValue, packedValueOffset, packedBytesLength) + \" ord=\" + ord);\n+  private boolean valueInOrder(int ord, int sortedDim, byte[] lastPackedValue, byte[] packedValue, int packedValueOffset,\n+      int[] docs, int docsOffset) {\n+    int dimOffset = sortedDim * bytesPerDim;\n+    if (ord > 0) {\n+      int cmp = StringHelper.compare(bytesPerDim, lastPackedValue, dimOffset, packedValue, packedValueOffset + dimOffset);\n+      if (cmp > 0) {\n+        throw new AssertionError(\"values out of order: last value=\" + new BytesRef(lastPackedValue) + \" current value=\" + new BytesRef(packedValue, packedValueOffset, packedBytesLength) + \" ord=\" + ord);\n+      }\n+      if (cmp == 0 && docs[docsOffset + ord - 1] > docs[docsOffset + ord]) {\n+        throw new AssertionError(\"docs out of order: last doc=\" + docs[docsOffset + ord - 1] + \" current doc=\" + docs[docsOffset + ord] + \" ord=\" + ord);\n+      }\n     }\n-    System.arraycopy(packedValue, packedValueOffset, lastPackedValue, 0, bytesPerDim);\n+    System.arraycopy(packedValue, packedValueOffset, lastPackedValue, 0, packedBytesLength);\n     return true;\n   }\n\n\n\nIt looks like we no longer call this from assert in the merge 1D\ncase, except within one leaf block?  Was that intentional? "
        },
        {
            "id": "comment-15397631",
            "author": "Michael McCandless",
            "date": "2016-07-28T14:44:52+0000",
            "content": "I indexed all 1.2 B NYC taxi rides using luceneserver (https://github.com/mikemccand/luceneserver) and this patch is 13% faster overall indexing time (200.7 K docs/sec -> 226.7 K docs/sec): nice! "
        },
        {
            "id": "comment-15397879",
            "author": "Adrien Grand",
            "date": "2016-07-28T17:41:26+0000",
            "content": "This is a great summary of the change. \n\nmore cleanly than LUCENE-7390 (should we revert that?)\n\nI agree we should only have this or LUCENE-7390, not both.\n\nMaybe MutablePointsReader should expose public byte getByteAt(int i, int bytePos);? This would save copying a whole value just because you want to see a specific byte.\n\nI initally wanted to limit the number of methods to a minimum but since the use of this API is very contained it is probably ok. It also seems to help with flush performance, I am now seeing flush taking ~20 secs instead of 23 previously with IndexTaxis.\n\nI hope our points test cases are stressful enough in testing this tie break case!\n\nIndeed it was not, changing this branch to return a constant does not break the tests. I extracted the sorting/partitioning logic to a helper class with dedicated tests to test this better (it would require too many docs to be indexed otherwise).\n\nIt looks like we no longer call this from assert in the merge 1D case, except within one leaf block? Was that intentional?\n\nDefinitely not. I added it back. "
        },
        {
            "id": "comment-15398172",
            "author": "Michael McCandless",
            "date": "2016-07-28T20:50:33+0000",
            "content": "Thanks Adrien Grand, I'm re-indexing the 1.2B taxi rides!\n\nCan't we use the MutableReader.getByteAt when computing the cardinality of each dim's suffix prefix byte in the leaf block writing too?\n\n{@cod mid} should be {@code mid} in MutablePointsReaderUtils.partition, but hopefully ant precommit would catch that.\n\nThe patch looks great: +1 to commit.  We can polish in follow on issues ... I think it's important to get the builds chewing on this great improvement.\n\nI'll revert LUCENE-7390 once you've pushed. "
        },
        {
            "id": "comment-15398278",
            "author": "Michael McCandless",
            "date": "2016-07-28T21:51:25+0000",
            "content": "I made a separate speedup in luceneserver to bring indexing rate to 232.7 K docs/sec, and then with this patch it gets even faster to 261.4 K docs/sec, net/net ~27% speedup! "
        },
        {
            "id": "comment-15398950",
            "author": "ASF subversion and git services",
            "date": "2016-07-29T08:26:53+0000",
            "content": "Commit 6b8b34ed358287a1cadf848e14089601a3ce1671 in lucene-solr's branch refs/heads/branch_6x from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6b8b34e ]\n\nLUCENE-7396: Speed flush of points. "
        },
        {
            "id": "comment-15398951",
            "author": "ASF subversion and git services",
            "date": "2016-07-29T08:26:55+0000",
            "content": "Commit 60975d2dfa0bf855220db6d9755b7e24c14a59bb in lucene-solr's branch refs/heads/master from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=60975d2 ]\n\nLUCENE-7396: Speed flush of points. "
        },
        {
            "id": "comment-15398954",
            "author": "Adrien Grand",
            "date": "2016-07-29T08:29:45+0000",
            "content": "Michael McCandless I'm curious what this separate speedup is about? "
        },
        {
            "id": "comment-15399689",
            "author": "Michael McCandless",
            "date": "2016-07-29T17:05:04+0000",
            "content": "Michael McCandless I'm curious what this separate speedup is about?\n\nIt was this change: https://github.com/mikemccand/luceneserver/commit/e69c560d1e22f7b7e03b9ff88fd1d6f6d0079fdc\n\nPreviously luceneserver was always building facets, but I fixed it to skip that if there are no facet fields since it's a somewhat expensive no-op in that case. "
        },
        {
            "id": "comment-15399772",
            "author": "ASF subversion and git services",
            "date": "2016-07-29T18:02:21+0000",
            "content": "Commit 1aecdd28d130c757770de67bfde52f3c989bd134 in lucene-solr's branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1aecdd2 ]\n\nLUCENE-7390: revert this change, since it's obsoleted by the much better LUCENE-7396 "
        },
        {
            "id": "comment-15399877",
            "author": "ASF subversion and git services",
            "date": "2016-07-29T19:13:55+0000",
            "content": "Commit c5c5335c9b36b30b5f49f8d354011d6fa874f383 in lucene-solr's branch refs/heads/branch_6x from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c5c5335 ]\n\nLUCENE-7390: revert this change, since it's obsoleted by the much better LUCENE-7396 "
        },
        {
            "id": "comment-15403155",
            "author": "ASF subversion and git services",
            "date": "2016-08-02T01:07:34+0000",
            "content": "Commit 60975d2dfa0bf855220db6d9755b7e24c14a59bb in lucene-solr's branch refs/heads/apiv2 from Adrien Grand\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=60975d2 ]\n\nLUCENE-7396: Speed flush of points. "
        },
        {
            "id": "comment-15403157",
            "author": "ASF subversion and git services",
            "date": "2016-08-02T01:07:39+0000",
            "content": "Commit 1aecdd28d130c757770de67bfde52f3c989bd134 in lucene-solr's branch refs/heads/apiv2 from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1aecdd2 ]\n\nLUCENE-7390: revert this change, since it's obsoleted by the much better LUCENE-7396 "
        },
        {
            "id": "comment-15439023",
            "author": "Michael McCandless",
            "date": "2016-08-26T14:00:09+0000",
            "content": "Bulk close resolved issues after 6.2.0 release. "
        }
    ]
}