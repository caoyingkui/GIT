{
    "id": "SOLR-6810",
    "title": "Faster searching limited but high rows across many shards all with many hits",
    "details": {
        "components": [
            "search"
        ],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "Searching \"limited but high rows across many shards all with many hits\" is slow\nE.g.\n\n\tQuery from outside client: q=something&rows=1000\n\tResulting in sub-requests to each shard something a-la this\n\t\n\t\t1) q=something&rows=1000&fl=id,score\n\t\t2) Request the full documents with ids in the global-top-1000 found among the top-1000 from each shard\n\t\n\t\n\n\n\nWhat does the subject mean\n\n\t\"limited but high rows\" means 1000 in the example above\n\t\"many shards\" means 200-1000 in our case\n\t\"all with many hits\" means that each of the shards have a significant number of hits on the query\nThe problem grows on all three factors above\n\n\n\nDoing such a query on our system takes between 5 min to 1 hour - depending on a lot of things. It ought to be much faster, so lets make it.\n\nProfiling show that the problem is that it takes lots of time to access the store to get id\u2019s for (up to) 1000 docs (value of rows parameter) per shard. Having 1000 shards its up to 1 mio ids that has to be fetched. There is really no good reason to ever read information from store for more than the overall top-1000 documents, that has to be returned to the client.\n\nFor further detail see mail-thread \"Slow searching limited but high rows across many shards all with high hits\" started 13/11-2014 on dev@lucene.apache.org",
    "attachments": {
        "branch_5x_rev1642874.patch": "https://issues.apache.org/jira/secure/attachment/12684844/branch_5x_rev1642874.patch",
        "branch_5x_rev1645549.patch": "https://issues.apache.org/jira/secure/attachment/12687505/branch_5x_rev1645549.patch",
        "SOLR-6810-hack-eoe.patch": "https://issues.apache.org/jira/secure/attachment/12818958/SOLR-6810-hack-eoe.patch",
        "SOLR-6810-trunk.patch": "https://issues.apache.org/jira/secure/attachment/12697531/SOLR-6810-trunk.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2014-12-03T08:35:57+0000",
            "author": "Per Steffensen",
            "content": "We have solved the problem (reducing response-time by a factor of 60 on our particular system/data/distribution) the following way\n\nIntroduced the concept of \"distributed query algorithm\" (DQA) controlled by request parameter dqa. Naming the existing (default) distributed query algorithm find-id-relevance_fetch-by-ids (short-alias firfbi) and introducing a new alternative distributed query algorithm called find-relevance_find-ids-limited-rows_fetch-by-ids (short-alias frfilrfbi) \n\n\tfind-id-relevance_fetch-by-ids does as always - see JavaDoc of ShardParams.FIND_ID_RELEVANCE_FETCH_BY_IDS\n\tfind-relevance_find-ids-limited-rows_fetch-by-ids does it in a different way  - see JavaDoc of ShardParams.FIND_RELEVANCE_FIND_IDS_LIMITED_ROWS_FETCH_BY_IDS\n\n\n\nBelieve \u201cdistributed query algorithm\u201d is a pendant to elasticsearch's \u201csearch type\u201d, but just with much better naming that say something about what it is actually controlling \n\nBoth DQAs support the disturb.singlePass flag. I have renamed it to dqa.forceSkipGetIds because it is only find-id-relevance_fetch-by-ids that becomes single-pass (going from 2 to 1 pass) with this flag. find-relevance_find-ids-limited-rows_fetch-by-ids goes from 3 to 2 passes. dqa.forceSkipGetIds=true is default for find-relevance_find-ids-limited-rows_fetch-by-ids.\n\nAttaching patch corresponding to our solution - going into production as we speak to reduce our response-times by a factor of 60. You do not necessarily need to just adopt it. But lets at least consider it a starting-point for a discussion. Details about the patch\n\n\tShardParams.DQA: Enum of the DQA\u2019s, including different helper methods that IMHO belongs here\n\tQueryComponent/ResponseBuilder: Changed to implement both DQA\u2019s now\n\tSolrIndexSearcher.doc: Does not go to store, if only asking for score. This is important for the optimization\n\tTestIndexSearcher: Added a test to test this particular new aspect of SolrIndexSearcher\n\tTestDistributedQueryAlgorithm: A new test-class dedicated tests of DQA\u2019s. testDocReads-test really shows exactly what this new DQA does for you. Test asserts that you only go to store X times across the cluster and not (up to) #shards * X times (X = rows in outer query)\n\tLeafReaderTestWrappers: Test-wrappers for LeafReader s. Can help collecting information about how LeafReader s are used in different test-scenarios. Used by TestIndexSearcher. Can be extended with other kinds of wrappers that collect different kinds of information.\n\tSolrIndexSearcherTestWrapper and SolrCoreTestWrapper. Generic classes that can help wrapping all LeafReader s under a SolrIndexSearcher or a SolrCore respectively. Used by TestDistributedQueryAlgorithm\n\tDistributedQueryComponentOptimizationTest: Updated with new tests around DQA\u2019s. And made more systematic in the way the tests are performed. Do not want to add hundreds of almost similar code-lines\n\tShardRoutingTest: Same comments as for DistributedQueryComponentOptimizationTest above\n\tSolrTestCaseJ4: Randomly selecting a DQA for each individual query fired running the test-suite - when you do not specify which DQA you want explicitly in the request. With helper-methods for fixing the DQA for tests that focus on DQA testing\n\tFix for SOLR-6812 is included in the patch because it is need to keep the test-suite green. But should probably be committed as part of SOLR-6812, and left out of this SOLR-6810. New DQA (find-relevance_find-ids-limited-rows_fetch-by-ids) has dqa.forceSkipGetIds (old disturb.singlePass) set to true by default. And since we run tests randomly selecting the DQA for every query, we are also indirectly randoming dqa.forceSkipGetIds. Therefore the test-suite will likely fail if skip-get-ids does not work for all kinds of requests. This is actually also a good way to have dqa.forceSkipGetIds (old distrib.singlePass) tested, so that we will not have a partially-working feature (as before SOLR-6795/SOLR-6796/SOLR-6812/SOLR-6813). The tests added to DistributedQueryComponentOptimizationTest in SOLR-6795 and SOLR-6796 have been removed again, because the problems (along with any other problems with dqa.forceSkipGetIds) will now (potentially) be revealed anyway because of indirect randomized testing of dqa.forceSkipGetIds\n\tI do not have a solution to SOLR-6813, so temporarily making sure that it will not make the test-suite fail, by forcing the particular query in DistributedExpandComponentTest to use find-id-relevance_fetch-by-ids (making it use dqa.forceSkipGetIds=false) - the lines switchToOriginalDQADefaultProvider() and switchToTestDQADefaultProvider(). Those lines should be removed when SOLR-6813 has been resolved. It will also work with find-relevance_find-ids-limited-rows_fetch-by-ids and dqa.forceSkipGetIds=false, so it is not find-relevance_find-ids-limited-rows_fetch-by-ids that does not work. It is dqa.forceSkipGetIds=true that does not work for this particular query.\n\n ",
            "id": "comment-14232772"
        },
        {
            "date": "2014-12-03T08:43:12+0000",
            "author": "Per Steffensen",
            "content": "SOLR-6795, SOLR-6796, SOLR-6812 and SOLR-6813 where all just preparation for this patch. It makes dqa.forceSkipGetIds (old disturb.singlePass) work for all queries (ever fired during the test-suite). So we have a complete feature now (when those issues are corrected). ",
            "id": "comment-14232781"
        },
        {
            "date": "2014-12-04T10:52:28+0000",
            "author": "Per Steffensen",
            "content": "Got in doubt how exactly the store-read-improvements would be when start-param is > 0. Now TestDistributedQueryAlgorithm.testDocReads clearly show that.\nBesides that improved test-coverage for sort + start > 0 queries in TestDistributedQueryAlgorithm. And better assert messages. ",
            "id": "comment-14234118"
        },
        {
            "date": "2014-12-13T10:05:10+0000",
            "author": "Per Steffensen",
            "content": "Now that SOLR-6812 has been solved (in a different way than I suggested), my solution to SOLR-6812 should be removed from the patch. ",
            "id": "comment-14245264"
        },
        {
            "date": "2014-12-13T10:06:59+0000",
            "author": "Per Steffensen",
            "content": "Thanks for taking care of SOLR-6795, SOLR-6796 and SOLR-6812 (and SOLR-6813), Shalin Shekhar Mangar!!! ",
            "id": "comment-14245266"
        },
        {
            "date": "2014-12-16T15:33:08+0000",
            "author": "Per Steffensen",
            "content": "New patch where the my solution to SOLR-6812 has been removed. Patch now matches revision 1645549 where the final solution to SOLR-6812 is included ",
            "id": "comment-14248372"
        },
        {
            "date": "2014-12-24T16:47:06+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks Per. This is great. I'm still going through the patch in detail but I have a few questions and comments.\n\n\n     * Algorithm\n     * - Shard-queries 1) Ask, by forwarding the outer query, each shard for relevance of the (up to) #rows most relevant matching documents\n     * - Find among those relevances the #rows highest global relevances\n     * Note for each shard (S) how many entries (docs_among_most_relevant(S)) it has among the #rows globally highest relevances\n     * - Shard-queries 2) Ask, by forwarding the outer query, each shard S for id and relevance of the (up to) #docs_among_most_relevant(S) most relevant matching documents\n     * - Find among those id/relevances the #rows id's with the highest global relevances (lets call this set of id's X)\n     * - Shard-queries 3) Ask, by sending id's, each shard to return the documents from set X that it holds\n     * - Return the fetched documents to the client \n\n\n\nSince dqa.forceSkipGetIds is always true for this new algorithm then computing the set X is not necessary and we can just directly fetch all return fields from individual shards and return the response to the user. Is that correct?\n\nI think the DefaultProvider and DefaultDefaultProvider aren't necessary? We can just keep a single static ShardParams.getDQA(SolrParams params) method and modify it if we ever need to change the default. If a user wants to change the default, the dqa can be set in the \"defaults\" section of the search handler.\n\nWhy do we need the switchToTestDQADefaultProvider() and switchToOriginalDQADefaultProvider() methods? You are already applying the DQA for each request so why is the switch necessary?\n\nThere's still the ShardParams.purpose field which you added in SOLR-6812 but I removed it. I still think it is unnecessary for purpose to be sent to shard. Is it necessary for this patch or is it just an artifact from SOLR-6812?\n\nDid you benchmark it against the current algorithm for other kinds of use-cases as well (3-5 shards, small number of rows)? Not asking for id can speed up responses there too I think.\n\n\n\"all with many hits\" means that each of the shards have a significant number of hits on the query\n\nUnless I missed something, the algorithm has no effect with respect to how many docs are hit by query on each shard? ",
            "id": "comment-14258369"
        },
        {
            "date": "2014-12-24T19:08:36+0000",
            "author": "Yonik Seeley",
            "content": "Thanks for pulling out the algorithm Shalin.  I'm still not quite groking it though... do you understand it well enough to give a high level description for those who know Solr but who haven't looked at the patch?  As in... what's the high level description of what this patch implements?\n\nThe main idea seems to be: you don't need IDs to merge the top docs from each shard.  Correct?\nAre there other ideas/optimizations in this patch?\n\nedit: Also, does this patch also improve things if docValues are used for the ID field? ",
            "id": "comment-14258456"
        },
        {
            "date": "2014-12-24T21:48:46+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "The main idea seems to be: you don't need IDs to merge the top docs from each shard. Correct?\n\nYes, exactly.\n\nI'm still not quite groking it though... do you understand it well enough to give a high level description for those who know Solr but who haven't looked at the patch?\n\nThe idea is to:\n\n\tGet score for top N docs from each shard in the first pass, (say rows=3 and shard1 returns scores 0.8, 0.5, 0.3 and shard2 returns 0.9, 0.6, 0.1)\n\tMerge them together to find the top N scores (0.9, 0.8, 0.6) and track number of results from each shard in top N scores (shard1 has 1 docs in top 3 and shard2 has 2 doc in top 3)\n\tGet corresponding docs (id and all return fields) from each shard in the second pass. (retrieve top 1 docs from shard1 and top 2 doc from shard2)\n\n\n\nAs in... what's the high level description of what this patch implements?\n\nThe patch implements this algorithm of course. It makes the algorithm configurable using a new 'dqa' parameter. There are some refactorings in ShardParams, ResponseBuilder to make this work. There are good randomized tests such that all Solr tests switch between the new and old algorithms. The patch also adds wrapper classes for SolrCore, SolrIndexSearcher and LeafReader which are used only during tests to assert things like number of shard requests, number of stored field accesses etc.\n\nAlso, does this patch also improve things if docValues are used for the ID field?\n\nNo. ",
            "id": "comment-14258554"
        },
        {
            "date": "2014-12-24T22:23:59+0000",
            "author": "Yonik Seeley",
            "content": "The patch implements this algorithm of course.\n\nHeh.  That was my issue... there was an excellent description of the problem,  but no high level description of the proposed fix.  IMO, one shouldn't have to look at the patch to figure out what it's trying to do.\n\nSomething to keep in mind for future optimizations: If we can use searcher leases, we know exactly which documents we need to retrieve from step 2 and can pass their ordinals in step 3.  That would appear to represent another very large speedup... if you need doc 42 and 77 from a shard, you can get just those two docs instead of docs 1 through 77.  \n\nedit: either ordinals (positions in the ranked doc list) or internal lucene docids would work if we're using searcher leases.\n\n\n>Also, does this patch also improve things if docValues are used for the ID field?\nNo.\n\nWhich begs the question: what are the downsides of using docValues for the ID field by default, and are those downsides enough to implement this alternate merge implementation?  I'm not saying otherwise... just throwing it out there.\n ",
            "id": "comment-14258565"
        },
        {
            "date": "2014-12-25T10:27:39+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "\nSomething to keep in mind for future optimizations: If we can use searcher leases, we know exactly which documents we need to retrieve from step 2 and can pass their ordinals in step 3. That would appear to represent another very large speedup... if you need doc 42 and 77 from a shard, you can get just those two docs instead of docs 1 through 77.\n\nedit: either ordinals (positions in the ranked doc list) or internal lucene docids would work if we're using searcher leases.\n\nMaybe I missed something but if we make sure that step 2 is executed on the same replicas as step 1 (which we would have to do for searcher leases anyway) then the query results should already be in the cache and the ordinals in the ranked doc list are just the top N?\n\nWhich begs the question: what are the downsides of using docValues for the ID field by default, and are those downsides enough to implement this alternate merge implementation? I'm not saying otherwise... just throwing it out there.\n\nI don't know. I'll create a benchmark to experiment with these ideas. In any case, existing indexes where ID are not doc values will also get a speed up with this new algorithm. ",
            "id": "comment-14258710"
        },
        {
            "date": "2014-12-26T16:15:57+0000",
            "author": "Yonik Seeley",
            "content": "Maybe I missed something but if we make sure that step 2 is executed on the same replicas as step 1 (which we would have to do for searcher leases anyway) then the query results should already be in the cache and the ordinals in the ranked doc list are just the top N?\n\nWhen a different searcher is used (because of a commit) the ordinals could refer to different docs.\nBut this seems to lead to acceptable behavior (unlike using internal docids which leads to catastrophic types of fails).\nYou may get a different doc than expected in the second phase, but it will still be highly ranked.\nThe failure modes (if you can call them that) when the index changes seem to be relatively equivalent to using external IDs.\n\nFor straight sorting, the ordinals being requested would always be contiguous (as you say, top N when offset=0).\n\nSo if it's actually true that the behavior is comparable to the current strategy when the index changes between phases, we should consider changing implementations (as opposed to keeping the old implementation and making it configurable).\n\nBenefits of new strategy:\n\n\tNo need to retrieve external IDs on first phase (this is slow for stored fields, fast for docvalues)\n\tNo need to return external IDs to the top-level searcher (reduced network traffic)\n\tSaves external ID -> internal docid lookup at the shard level on the last phase\n\n\n\nDisadvantages of new strategy:\n\n\tDoesn't work well with non-standard sorts? (like a diversifying sort?)\n\tMore sensitive to query-cache size (should be minor)\n\tQuery re-execution when index changes (minor impact except for very high frequency commits?)\n\n\n\nEverything I'm thinking of so far leads me to believe the new strategy should be the default. ",
            "id": "comment-14259130"
        },
        {
            "date": "2014-12-26T19:00:03+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "\nWhen a different searcher is used (because of a commit) the ordinals could refer to different docs.\nBut this seems to lead to acceptable behavior (unlike using internal docids which leads to catastrophic types of fails).\nYou may get a different doc than expected in the second phase, but it will still be highly ranked.\nThe failure modes (if you can call them that) when the index changes seem to be relatively equivalent to using external IDs.\n\nI think the only failure mode that gets worse is the one with duplicate ids across shards. When we merged using ids in the first phase, a dup would be discarded but we would (usually) have enough ids to return the requested number of rows. In this new algorithm, we will either:\n\n\treturn less rows than requested (which can be very confusing considering that numFound will likely be larger than rows), or\n\tfetch more docs than necessary in the second phase (just in case there are dups)\n\tintroduce a third request to fetch more docs as and when necessary\n\n\n\nBut the advantages of this algorithm outweigh this annoyance.\n\nSo if it's actually true that the behavior is comparable to the current strategy when the index changes between phases, we should consider changing implementations (as opposed to keeping the old implementation and making it configurable).\n\nPerhaps this is too big a change to do without some field testing?\n\nDoesn't work well with non-standard sorts? (like a diversifying sort?)\n\nYes that'd need some special handling. Maybe something in tandem with LUCENE-6066 can work.\n\nQuery re-execution when index changes (minor impact except for very high frequency commits?)\n\nWorth benchmarking though.\n\nEverything I'm thinking of so far leads me to believe the new strategy should be the default.\n\n+1 ",
            "id": "comment-14259180"
        },
        {
            "date": "2014-12-27T18:03:46+0000",
            "author": "Per Steffensen",
            "content": "Since dqa.forceSkipGetIds is always true for this new algorithm then computing the set X is not necessary and we can just directly fetch all return fields from individual shards and return the response to the user. Is that correct?\n\nThis is what happens by default with the new algorithm. But dqa.forceSkipGetIds is not always true. It is true by default, but you can explicitly set it to false by sending dqa.forceSkipGetIds=false in your request. So basically there are four options\n\n\told alg without dqa.forceSkipGetIds or with dqa.forceSkipGetIds=false (default before SOLR-6810, and currently also after SOLR-6810)\n\told alg with dqa.forceSkipGetIds=true (same as with distrib.singlePass=true before SOLR-6810)\n\tnew alg without dqa.forceSkipGetIds or with dqa.forceSkipGetIds=true (does as you describe above)\n\tnew alg with dqa.forceSkipGetIds=false (does as described in the JavaDoc you quoted)\n\n\n\nThe JavaDoc descriptions describe how the alg works WITHOUT dqa.forceSkipGetIds switched on. But dqa.forceSkipGetIds is switched on for the new alg by default. The JavaDoc for ShardParams.DQA.FORCE_SKIP_GET_IDS_PARAM describes how the two algs are altered when running with dqa.forceSkipGetIds=true. The thing is that you need to know this part as well to understand how the new alg works by default.\n\nI think the DefaultProvider and DefaultDefaultProvider aren't necessary? We can just keep a single static ShardParams.getDQA(SolrParams params) method and modify it if we ever need to change the default.\n\nWell I would prefer to keep ShardParams.DQA.get(params) instead of having a ShardParams.getDQA(params) - it think it is better \"context'ing\". But I will survive if you want to change it.\nDefaultProvider in supposed to isolate the default decisions. DefaultDefaultProvider is an implementation that calculates the out-of-the-box defaults. It could be done directly in ShardParams.DQA.get, but I like to structure things. But I have to admit that the main reason I added the DefaultProvider thing, was that it makes it easier to change the default-decisions made when running the test-suite. I would like to randomly select the DQA to be used for every single query fired across the entire test-suite. This way we will have a very thorough test-coverage of both algs. Having the option of changing the DefaultProvider made it very easy to achieve this in SolrTestCaseJ4\n\nprivate static DQA.DefaultProvider testDQADefaultProvider =\n    new DQA.DefaultProvider() {\n      @Override\n      public DQA getDefault(SolrParams params) {\n        // Select randomly the DQA to use\n        int algNo = Math.abs(random().nextInt()%(ShardParams.DQA.values().length));\n        return DQA.values()[algNo];\n      }\n    };\n\n\n\nDQA.setDefaultProvider(testDQADefaultProvider);\n\n\n\nIf a user wants to change the default, the dqa can be set in the \"defaults\" section of the search handler.\n\nI know it is a matter of opinion but in my mind the best place to deal with default for DQA is in the code that deals with DQA - not somewhere else. This makes a much better isolation and it makes code easier to understand. You can essentially navigate to ShardParams.DQA and read the code and JavaDoc and understand everything about DQA's. You do not have to know that there is a decision about default in the SeachHandler. But if you want to change that, it is ok for me.\n\nWhy do we need the switchToTestDQADefaultProvider() and switchToOriginalDQADefaultProvider() methods? You are already applying the DQA for each request so why is the switch necessary?\n\nNo I am not applying the DQA for each request. I trust you understand why I want to run with randomized DQA across the entire test-suite - this is why I invented the testDQADefaultProvider. In tests that explicitly deal with testing DQA stuff, in some cases I want to switch on the real DefaultProvider because some of those tests are actually testing out-of-the-box default-behaviour. E.g. verifyForceSkipGetIds-tests in DistributedQueryComponentOptimizationTest. Also need it in DistributedExpandComponentTest until SOLR-6813 has been solved.\n\nThere's still the ShardParams.purpose field which you added in SOLR-6812 but I removed it. I still think it is unnecessary for purpose to be sent to shard. Is it necessary for this patch or is it just an artifact from SOLR-6812?\n\nYou are right. It is a mistake that I did not remove ShardParams.purpose\n\nDid you benchmark it against the current algorithm for other kinds of use-cases as well (3-5 shards, small number of rows)? Not asking for id can speed up responses there too I think.\n\nI did not do any concrete benchmarking for other requests. We have changed our DQA in production for a particular request where it reduces response-time by a factor of 60 from minutes/hours to secs/minutes. We want to take it in two steps, starting out just switching to the new DQA in the case where we have shown that it makes a huge difference. We will soon look into whether or not it will help us for all or some of the other queries we do.\n\nThe new DQA might help in case of \"few\" shards or small \"rows\"-values. What I wanted to say is that the speedup from using new DQA will increase in the factors mentioned\n\n\tThe \"rows\" you ask for (and there are actually many hits on each shard)\n\tThe number of shards searched\n\n\n\nI did not mean to say that there is no benefit from the new DQA even with small rows-values or few shards. But I do believe that there is a lower limit as to when you should apply this DQA. I do not believe it is always better to use the new DQA than using \"the old DQA + dqa.forceSkipGetIds\". Main reason it that \"the old DQA + dqa.forceSkipGetIds\" only does one round-trip to all the shards. The new DQA does two round-trips. So for very fast/simple queries the extra round-trip might actually just increase total response-time.\n\n\nAlso, does this patch also improve things if docValues are used for the ID field?\n\nNo.\n\nWhich begs the question: what are the downsides of using docValues for the ID field by default, and are those downsides enough to implement this alternate merge implementation?\n\nI am not sure exactly when doc-values kick in in the search-flow, so I am not sure \"No\" is the correct answer. But even though id field is doc-value it is still cheaper not to fetch it at all than fetching it from doc-value. The other issue (maybe only relevant for us) is that it takes a significant amount of time making the id field doc-value if it isnt already - we started using SolrCloud before doc-value was even possible, so in some of our older systems id field in not doc-value. To turn the id field into doc-value I believe you currently need to re-index it all - that is not feasible with a thousand billion documents. I know that Toke Eskildsen has recently done some work making it possible to add doc-value to a field without re-indexing it completely from scratch (basically just calculating the doc-value data and leaving index/store as it already is), but even with that approach it is not something you just do in a 24x7 system.\n\nI do believe doc-value and new DQA are orthogonal optimizations.\n\n\nWhen a different searcher is used (because of a commit) the ordinals could refer to different docs.\nBut this seems to lead to acceptable behavior (unlike using internal docids which leads to catastrophic types of fails)\n\nFor a multi-phase algorithm, if there are commits (changing the result of the search) between phases, you will get a response somewhere between \"the correct response before the commit\" and \"the correct response after the commit\". This goes both for the old and the new DQA. Therefore I assumed that it is also acceptable for the new DQA. Making multi-phase DQAs always response correctly according to the state when the first phase was carried out, will require ACID-like transactions with a very strong (serializable'ish) isolation-level - do NOT go there for a high performance no-sql database! Remember that there can also be deletes in between-phase commits.\n\nreturn less rows than requested\n\nThis can also happen with the old DQA - if deletes happen in the between-phases commit. But the new DQA have a problem with duplicates that the old one does not. On the other hand the new DQA is more likely to return the correct number of documents in case of deletes between phases.\n\n\nEverything I'm thinking of so far leads me to believe the new strategy should be the default.\n\n+1\n\nJust remember\n\n\tThe new DQA has an extra rount-trip to shards, compared to \"old DQA plus dqa.forceSkipGetIds=true\". It has the same number of round-trips (2) to shards as the current default algorithm though (\"old DQA with dqa.forceSkipGetIds=false\")\n\tBefore SOLR-6813 is fixed, the new DQA does not work for some (very limited) expand-request (those expand-requests does not work for \"old DQA plus dqa.forceSkipGetIds=true\" either). We probably want a default DQA to work for any request\n\n ",
            "id": "comment-14259438"
        },
        {
            "date": "2014-12-27T18:30:21+0000",
            "author": "Per Steffensen",
            "content": "IMO, one shouldn't have to look at the patch to figure out what it's trying to do.\n\nSeems reasonable. The way things change is IMHO fairly well documented in JavaDocs of ShardParams.DQA so I will just steal from there\n\n\tOld DQA (FIND_ID_RELEVANCE_FETCH_BY_IDS)\n\n   /**\n    * Algorithm\n    * - Shard-queries 1) Ask, by forwarding the outer query, each shard for id and relevance of the (up to) #rows most relevant matching documents\n    * - Find among those id/relevances the #rows id's with the highest global relevances (lets call this set of id's X)\n    * - Shard-queries 2) Ask, by sending id's, each shard to return the documents from set X that it holds\n    * - Return the fetched documents to the client\n    */\n...\n       // Default do not force skip get-ids phase\n\n\n\tNew DQA (FIND_RELEVANCE_FIND_IDS_LIMITED_ROWS_FETCH_BY_IDS)\n\n   /**\n    * Algorithm\n    * - Shard-queries 1) Ask, by forwarding the outer query, each shard for relevance of the (up to) #rows most relevant matching documents\n    * - Find among those relevances the #rows highest global relevances\n    * Note for each shard (S) how many entries (docs_among_most_relevant(S)) it has among the #rows globally highest relevances\n    * - Shard-queries 2) Ask, by forwarding the outer query, each shard S for id and relevance of the (up to) #docs_among_most_relevant(S) most relevant matching documents\n    * - Find among those id/relevances the #rows id's with the highest global relevances (lets call this set of id's X)\n    * - Shard-queries 3) Ask, by sending id's, each shard to return the documents from set X that it holds\n    * - Return the fetched documents to the client \n    * \n    * Advantages\n    * Asking for data from store (id in shard-queries 1) of FIND_ID_RELEVANCE_FETCH_BY_IDS) can be expensive, therefore sometimes you want to ask for data\n    * from as few documents as possible.\n    * The main purpose of this algorithm it to limit the rows asked for in shard-queries 2) compared to shard-queries 1) of FIND_ID_RELEVANCE_FETCH_BY_IDS.\n    * Lets call the number of rows asked for by the outer request for \"outer-rows\"\n    * shard-queries 2) will never ask for data from more than \"outer-rows\" documents total across all involved shards. shard-queries 1) of FIND_ID_RELEVANCE_FETCH_BY_IDS\n    * will ask each shard for data from \"outer-rows\" documents, and in worst case if each shard contains \"outer-rows\" matching documents you will\n    * fetch data for \"number of shards involved\" * \"outer-rows\".\n    * Using FIND_RELEVANCE_FIND_IDS_LIMITED_ROWS_FETCH_BY_IDS will become more beneficial the more\n    * - shards are involved\n    * - and/or the more matching documents each shard holds\n    */\n...\n    // Default force skip get-ids phase. In this algorithm there are really never any reason not to skip it\n\n\n\tdqa.forceSkipGetIds\n\n   /** Request parameter to force skip get-ids phase of the distributed query? Value: true or false \n    * Even if you do not force it, the system might choose to do it anyway\n    * Skipping the get-ids phase\n    * - FIND_ID_RELEVANCE_FETCH_BY_IDS: Fetch entire documents in Shard-queries 1) and skip Shard-queries 2)\n    * - FIND_RELEVANCE_FIND_IDS_LIMITED_ROWS_FETCH_BY_IDS: Fetch entire documents in Shard-queries 2) and skip Shard-queries 3)\n    */\n\n\n\n ",
            "id": "comment-14259445"
        },
        {
            "date": "2014-12-27T18:34:37+0000",
            "author": "Per Steffensen",
            "content": "TestDistributedQueryAlgorithm.testDocReads shows very well exactly how the number of store accesses is reduced\n\n// Test the number of documents read from store using FIND_RELEVANCE_FIND_IDS_LIMITED_ROWS_FETCH_BY_IDS\n// vs FIND_ID_RELEVANCE_FETCH_BY_IDS. This demonstrates the advantage of FIND_RELEVANCE_FIND_IDS_LIMITED_ROWS_FETCH_BY_IDS\n// over FIND_ID_RELEVANCE_FETCH_BY_IDS (and vice versa)\nprivate void testDocReads() throws Exception {\n  for (int startValue = 0; startValue <= MAX_START; startValue++) {\n    // FIND_RELEVANCE_FIND_IDS_LIMITED_ROWS_FETCH_BY_IDS (assuming skipGetIds used - default)\n    // Only reads data (required fields) from store for \"rows + (#shards * start)\" documents across all shards\n    // This can be optimized to become only \"rows\" \n    // Only reads the data once\n    testDQADocReads(ShardParams.DQA.FIND_RELEVANCE_FIND_IDS_LIMITED_ROWS_FETCH_BY_IDS, startValue, ROWS, ROWS + (startValue * jettys.size()), ROWS + (startValue * jettys.size()));\n\n    // DQA.FIND_ID_RELEVANCE_FETCH_BY_IDS (assuming skipGetIds not used - default)\n    // Reads data (ids only) from store for \"(rows + startValue) * #shards\" documents for each shard\n    // Besides that reads data (required fields) for \"rows\" documents across all shards\n    testDQADocReads(ShardParams.DQA.FIND_ID_RELEVANCE_FETCH_BY_IDS, startValue, ROWS, (ROWS + startValue) * jettys.size(), ROWS + ((ROWS + startValue) * jettys.size()));\n  }\n}\n\n\n\ntestDQADocReads(ShardParams.DQA dqa, int start, int rows, int expectedUniqueIdCount, int expectedTotalCount) {\n...\n}\n\n ",
            "id": "comment-14259447"
        },
        {
            "date": "2014-12-29T16:51:05+0000",
            "author": "Yonik Seeley",
            "content": "The new DQA has an extra rount-trip to shards\n\nI think the strategy that Shalin & I were talking about as a potential default was one that never collected IDs separately, hence no extra round-trip.\nstep 1: retrieve sort field values (then merge and calculate the range of ordinals needed for each shard)\nstep 2: retrieve stored fields by specifying the ordinals from each shard ",
            "id": "comment-14260217"
        },
        {
            "date": "2014-12-30T07:27:20+0000",
            "author": "Per Steffensen",
            "content": "\nI think the strategy that Shalin & I were talking about as a potential default was one that never collected IDs separately, hence no extra round-trip.\nstep 1: retrieve sort field values (then merge and calculate the range of ordinals needed for each shard)\nstep 2: retrieve stored fields by specifying the ordinals from each shard\n\nIt sounds like third DQA - does not seem to be exactly what my new algorithm does. But you suggestion still has 2 round-trips. The old/current default-DQA with dqa.forceSkipGetIds/distrib.singlePass=false (default) has 2 round-trips. My new DQA with dqa.forceSkipGetIds=true (default) has 2 round-trips, so choosing that as the new default-DQA will not introduce an extra round-trip compared to todays default-DQA. But we could choose to select old/current default-DQA with dqa.forceSkipGetIds/distrib.singlePass=true as the new default-DQA. It has only 1 round-trip, so compared to that both your DQA (above) and my new DQA has an extra round-trip. My new DQA will never become 1-round-trip only, because the essence is to make a round-trip first (inexpensive because you do not access store - not even for ids), collecting information needed to limit \"rows\" for the second round-trip where you actually retrieve the stored fields.\n\none that never collected IDs separately\n\nMy new DQA does not collect IDs separately (as long as you do not explicitly set dqa.forceSkipGetIds=false) ",
            "id": "comment-14260880"
        },
        {
            "date": "2015-02-09T18:26:55+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Patch for trunk. There are a few test failures related to distributed IDF which needs fixing but all other tests pass. I'll try to get through them this week. ",
            "id": "comment-14312569"
        },
        {
            "date": "2015-04-08T08:49:38+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Patch which reconciles the new query algorithm with the distributed IDF changes. All tests except for DistributedDebugComponentTest pass.\n\nThere are many todos/nocommits yet:\n\n\tTests using the TrackingShardHandlerFactory are required inside DistributedQueryComponentOptimizationTest for the new query algorithm\n\tDebugComponent needs some modifications because in the new algorithm the query is executed twice but we shouldn't try to collect the same information again. Also the 2nd execution may be from the cache but the timing stats shouldn't replace the old values.\n\tNeed to find better names for the new/old algorithm\n\tWe need to deprecate the distrib.singlePass parameter in favor of distrib.forceSkipGetIds but we may need to support both for 5.x for back-compat\n\tNeed to write a benchmark for new vs old query algorithm on various cluster sizes\n\n ",
            "id": "comment-14484954"
        },
        {
            "date": "2015-04-29T18:56:57+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Latest patch.\n\nThe DistributedDebugComponentTest collects the filter_queries for every GET_FIELD requests. The existing DQA makes GET_FIELD requests only to those shards which contribute some documents to the results. However, the new DQA implementation makes a request to each shard regardless of whether it contributes a document or not. IMO, this is a bug (the extra request to every shard) but it is a bit tricky to solve for the following reasons:\n\n\tA user can make a request with rows=0 in order to get just the facet responses. In such cases, we must make the STAGE_EXECUTE_QUERY/PURPOSE_GET_FIELDS requests to each shard. In fact many tests such as TestJsonFacets and some pivot facet tests do this.\n\tSecondly, we must record things like numFound and maxScore from the results of the STAGE_LIMIT_ROWS itself if we are to skip requests to shards in the 2nd phase.\n\n\n\nAt this stage, all tests pass and I am working on removing the nocommits (the biggest of which I detailed above). ",
            "id": "comment-14519946"
        },
        {
            "date": "2015-04-29T20:13:20+0000",
            "author": "Per Steffensen",
            "content": "Happy to see the activity here. Currently I am very busy, so I do not have the time to follow the work closely. Hope I will soon be able to. ",
            "id": "comment-14520125"
        },
        {
            "date": "2016-03-22T03:26:11+0000",
            "author": "Erick Erickson",
            "content": "Shalin Shekhar Mangar Now that SOLR-8220 is committed, what's the status of this JIRA? ",
            "id": "comment-15205706"
        },
        {
            "date": "2016-07-19T23:21:35+0000",
            "author": "Erick Erickson",
            "content": "SOLR-8220 does NOT resolve this, but I think it lays the groundwork for a much smaller implementation.\n\nI've attached a patch that is a PoC, note there are //nocommits where I write to system.out from CopmressingStoredFieldsReader just for easy verification that we're decompressing or not....\n\nAlso see the nocommit in DocsStreamer. To make this work you need to define your id field as stored=false, dv=true. I don't think I understand useDocValuesAsStored, because setting stored=true useDocValuesAsStored=true still gets the stored field, I'll have to figure that out.\n\nI'm sure this isn't an optimal implementation, but maybe it'll prompt some more carefully thought-out approaches.\n\nMostly putting this up for comment, I'm probably not going to pursue this in the near future though. ",
            "id": "comment-15385054"
        },
        {
            "date": "2016-08-01T14:37:13+0000",
            "author": "Per Steffensen",
            "content": "I have not looked much into SOLR-8220, but from the little reading I have done, I guess you would have to doc-value the id-field for SOLR-8220 to help on the SOLR-6810 issue.\n\nI guess, most systems do not have id-field doc-valued. I also guess, for most systems it is feasible to do reindex it all, so that id's become doc-value. But not for all systems - e.g. not for our systems. We have 1000 billion documents in one of our systems, and you do not just reindex all of that. It takes weeks and is a very complex operation (we have tried it a few times). I do not know if such an argument counts here, but anyway... ",
            "id": "comment-15402138"
        },
        {
            "date": "2018-08-09T01:01:40+0000",
            "author": "Gavin",
            "content": "Move issue from deprecated 'In  Progress' back to 'Open' ",
            "id": "comment-16574150"
        }
    ]
}