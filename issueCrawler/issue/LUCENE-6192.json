{
    "id": "LUCENE-6192",
    "title": "Long overflow in LuceneXXSkipWriter can corrupt skip data",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [
            "4.10.5",
            "5.0",
            "6.0"
        ],
        "priority": "Major",
        "status": "Closed",
        "type": "Bug"
    },
    "description": "I've been iterating with Tom on this corruption that CheckIndex detects in his rather large index (720 GB in a single segment):\n\n\n java -Xmx16G -Xms16G -cp $JAR -ea:org.apache.lucene... org.apache.lucene.index.CheckIndex /XXXX/shards/4/core-1/data/test_index -verbose 2>&1 |tee -a shard4_reoptimizedNewJava\n\n\nOpening index @ /htsolr/lss-reindex/shards/4/core-1/data/test_index\n\nSegments file=segments_e numSegments=1 version=4.10.2 format= userData={commitTimeMSec=1421479358825}\n  1 of 1: name=_8m8 docCount=1130856\n    version=4.10.2\n    codec=Lucene410\n    compound=false\n    numFiles=10\n    size (MB)=719,967.32\n    diagnostics = {timestamp=1421437320935, os=Linux, os.version=2.6.18-400.1.1.el5, mergeFactor=2, source=merge, lucene.version=4.10.2, os.arch=amd64, mergeMaxNumSegments=1, java.version=1.7.0_71, java.vendor=Oracle Corporation}\n    no deletions\n    test: open reader.........OK\n    test: check integrity.....OK\n    test: check live docs.....OK\n    test: fields..............OK [80 fields]\n    test: field norms.........OK [23 fields]\n    test: terms, freq, prox...ERROR: java.lang.AssertionError: -96\njava.lang.AssertionError: -96\n        at org.apache.lucene.codecs.lucene41.ForUtil.skipBlock(ForUtil.java:228)\n        at org.apache.lucene.codecs.lucene41.Lucene41PostingsReader$BlockDocsAndPositionsEnum.skipPositions(Lucene41PostingsReader.java:925)\n        at org.apache.lucene.codecs.lucene41.Lucene41PostingsReader$BlockDocsAndPositionsEnum.nextPosition(Lucene41PostingsReader.java:955)\n        at org.apache.lucene.index.CheckIndex.checkFields(CheckIndex.java:1100)\n        at org.apache.lucene.index.CheckIndex.testPostings(CheckIndex.java:1357)\n        at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:655)\n        at org.apache.lucene.index.CheckIndex.main(CheckIndex.java:2096)\n    test: stored fields.......OK [67472796 total field count; avg 59.665 fields per doc]\n    test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]\n    test: docvalues...........OK [0 docvalues fields; 0 BINARY; 0 NUMERIC; 0 SORTED; 0 SORTED_NUMERIC; 0 SORTED_SET]\nFAILED\n    WARNING: fixIndex() would remove reference to this segment; full exception:\njava.lang.RuntimeException: Term Index test failed\n        at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:670)\n        at org.apache.lucene.index.CheckIndex.main(CheckIndex.java:2096)\n\nWARNING: 1 broken segments (containing 1130856 documents) detected\nWARNING: would write new segments file, and 1130856 documents would be lost, if -fix were specified\n\n\n\nAnd Rob spotted long -> int casts in our skip list writers that look like they could cause such corruption if a single high-freq term with many positions required > 2.1 GB to write its positions into .pos.",
    "attachments": {
        "LUCENE-6192.patch": "https://issues.apache.org/jira/secure/attachment/12693399/LUCENE-6192.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14284456",
            "author": "Robert Muir",
            "date": "2015-01-20T21:35:12+0000",
            "content": "The bug is actually old (even in 3.x) but in 4.x we added \"skipMultiplier\" which means we write less skipdata and it uncovers it. it just takes a > 2.1GB delta at a higher level to screw things up.\n\nTom's blog post here mentions > 2GB .pos data for \"the\".\n\nedit: add url http://www.hathitrust.org/blogs/large-scale-search/slow-queries-and-common-words-part-1 "
        },
        {
            "id": "comment-14284472",
            "author": "Michael McCandless",
            "date": "2015-01-20T21:49:37+0000",
            "content": "Here's a patch, only for 4.10.x for starters (Lucene41SkipWriter) so Tom can test it and see if it fixes his exception.  The good thing about skip data is it's ignored during merging, so to test this you just need to apply the patch, compile & deploy Lucene core JAR, then optimize so the skip data is regenerated...\n\nOn 5.x/trunk we must also fix Lucene50SkipWriter... "
        },
        {
            "id": "comment-14284595",
            "author": "Ryan Ernst",
            "date": "2015-01-20T23:00:46+0000",
            "content": "+1 to the patch.\n\nAnd just to confirm, we shouldn't need a format change, since this is a bug in writing, and the reader isn't really changing (vlong is a superset of vint)? "
        },
        {
            "id": "comment-14284700",
            "author": "Robert Muir",
            "date": "2015-01-21T00:15:41+0000",
            "content": "+1. Ryan: yes, thats the case.  "
        },
        {
            "id": "comment-14285818",
            "author": "ASF subversion and git services",
            "date": "2015-01-21T16:18:41+0000",
            "content": "Commit 1653577 from Michael McCandless in branch 'dev/branches/lucene_solr_4_10'\n[ https://svn.apache.org/r1653577 ]\n\nLUCENE-6192: don't overflow int when writing skip data for high freq terms in extremely large indices "
        },
        {
            "id": "comment-14285828",
            "author": "ASF subversion and git services",
            "date": "2015-01-21T16:24:10+0000",
            "content": "Commit 1653580 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1653580 ]\n\nLUCENE-6192: don't overflow int when writing skip data for high freq terms in extremely large indices "
        },
        {
            "id": "comment-14285835",
            "author": "ASF subversion and git services",
            "date": "2015-01-21T16:29:17+0000",
            "content": "Commit 1653585 from Michael McCandless in branch 'dev/branches/lucene_solr_5_0'\n[ https://svn.apache.org/r1653585 ]\n\nLUCENE-6192: don't overflow int when writing skip data for high freq terms in extremely large indices "
        },
        {
            "id": "comment-14285842",
            "author": "ASF subversion and git services",
            "date": "2015-01-21T16:33:10+0000",
            "content": "Commit 1653588 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1653588 ]\n\nLUCENE-6192: don't overflow int when writing skip data for high freq terms in extremely large indices "
        },
        {
            "id": "comment-14285857",
            "author": "ASF subversion and git services",
            "date": "2015-01-21T16:45:46+0000",
            "content": "Commit 1653593 from Michael McCandless in branch 'dev/branches/lucene_solr_4_10'\n[ https://svn.apache.org/r1653593 ]\n\nLUCENE-6192: don't overflow int when writing skip data for high freq terms in extremely large indices "
        },
        {
            "id": "comment-14285862",
            "author": "ASF subversion and git services",
            "date": "2015-01-21T16:49:33+0000",
            "content": "Commit 1653594 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1653594 ]\n\nLUCENE-6192: don't overflow int when writing skip data for high freq terms in extremely large indices "
        },
        {
            "id": "comment-14285868",
            "author": "Michael McCandless",
            "date": "2015-01-21T16:53:52+0000",
            "content": "Resolving ... Tom can you post back here the results of testing with this fix?   Thanks.  Hopefully this is the bug you were hitting! "
        },
        {
            "id": "comment-14285904",
            "author": "ASF subversion and git services",
            "date": "2015-01-21T17:13:40+0000",
            "content": "Commit 1653606 from Michael McCandless in branch 'dev/branches/lucene_solr_5_0'\n[ https://svn.apache.org/r1653606 ]\n\nLUCENE-6192: don't overflow int when writing skip data for high freq terms in extremely large indices "
        },
        {
            "id": "comment-14285955",
            "author": "Tom Burton-West",
            "date": "2015-01-21T17:43:25+0000",
            "content": "I'll report as soon as I have some results.   Still have about 10% (about 1.3 million books or slightly less than a terabyte of OCR) to index.  Once that is done we will deploy a Solr war with the patch and optimize.  That will take overnight. When the optimize is done we will then run CheckIndex.   So hopefully by Friday I will have something to report.  "
        },
        {
            "id": "comment-14292536",
            "author": "Tom Burton-West",
            "date": "2015-01-26T22:36:41+0000",
            "content": "Patch works!  Thanks Mike!\n\nDeployed Solr war with the patch and ran optimize on 12 shards.  All  CheckIndexes passed. \nBelow are some of the stats on one of the shards. \n\nTom\n\n About 1 million docs and 700GB index with about 4 billion unique terms, 270 billion tokens\n\ndocCount=1086381\n size (MB)=693,308.47\n\n   test: terms, freq, prox...OK [4113882974 terms; 61631126560 terms/docs pairs; 270670957886 tokens]\n\n field \"ocr\":\n        index FST:\n          27157406 nodes\n          77300582 arcs\n          1262090664 bytes\n        terms:\n          4087713620 terms\n          50227574334 bytes (12.3 bytes/term)\n        blocks:\n          132202225 blocks\n          96419097 terms-only blocks\n          40757 sub-block-only blocks\n          35742371 mixed blocks\n          27202047 floor blocks\n          44718055 non-floor blocks\n          87484170 floor sub-blocks\n          23560113026 term suffix bytes (178.2 suffix-bytes/block)\n          8227225977 term stats bytes (62.2 stats-bytes/block)\n          19664735257 other bytes (148.7 other-bytes/block)\n          by prefix length: "
        },
        {
            "id": "comment-14292564",
            "author": "Michael McCandless",
            "date": "2015-01-26T22:52:13+0000",
            "content": "OK that's great news; thanks for bringing closure Tom! "
        },
        {
            "id": "comment-14296913",
            "author": "ASF subversion and git services",
            "date": "2015-01-29T14:26:16+0000",
            "content": "Commit 1655678 from Michael McCandless in branch 'dev/branches/lucene_solr_5_0'\n[ https://svn.apache.org/r1655678 ]\n\nLUCENE-6192: add CHANGES entry "
        },
        {
            "id": "comment-14296921",
            "author": "ASF subversion and git services",
            "date": "2015-01-29T14:31:07+0000",
            "content": "Commit 1655681 from Michael McCandless in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1655681 ]\n\nLUCENE-6192: add CHANGES entry "
        },
        {
            "id": "comment-14296924",
            "author": "ASF subversion and git services",
            "date": "2015-01-29T14:31:58+0000",
            "content": "Commit 1655682 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1655682 ]\n\nLUCENE-6192: add CHANGES entry "
        },
        {
            "id": "comment-14332710",
            "author": "Anshum Gupta",
            "date": "2015-02-23T05:01:28+0000",
            "content": "Bulk close after 5.0 release. "
        }
    ]
}