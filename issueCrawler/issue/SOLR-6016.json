{
    "id": "SOLR-6016",
    "title": "Failure indexing exampledocs with example-schemaless mode",
    "details": {
        "affect_versions": "4.7.2,                                            4.8",
        "status": "Resolved",
        "fix_versions": [],
        "components": [
            "documentation",
            "Schema and Analysis"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Steps to reproduce:\n\n\tcd example; java -Dsolr.solr.home=example-schemaless/solr -jar start.jar\n\tcd exampledocs; java -jar post.jar *.xml\n\n\n\nOutput from post.jar\n\nPosting files to base url http://localhost:8983/solr/update using content-type application/xml..\nPOSTing file gb18030-example.xml\nPOSTing file hd.xml\nPOSTing file ipod_other.xml\nSimplePostTool: WARNING: Solr returned an error #400 Bad Request\nSimplePostTool: WARNING: IOException while reading response: java.io.IOException: Server returned HTTP response code: 400 for URL: http://localhost:8983/solr/update\nPOSTing file ipod_video.xml\nSimplePostTool: WARNING: Solr returned an error #400 Bad Request\nSimplePostTool: WARNING: IOException while reading response: java.io.IOException: Server returned HTTP response code: 400 for URL: http://localhost:8983/solr/update\nPOSTing file manufacturers.xml\nPOSTing file mem.xml\nSimplePostTool: WARNING: Solr returned an error #400 Bad Request\nSimplePostTool: WARNING: IOException while reading response: java.io.IOException: Server returned HTTP response code: 400 for URL: http://localhost:8983/solr/update\nPOSTing file money.xml\nPOSTing file monitor2.xml\nSimplePostTool: WARNING: Solr returned an error #400 Bad Request\nSimplePostTool: WARNING: IOException while reading response: java.io.IOException: Server returned HTTP response code: 400 for URL: http://localhost:8983/solr/update\nPOSTing file monitor.xml\nSimplePostTool: WARNING: Solr returned an error #400 Bad Request\nSimplePostTool: WARNING: IOException while reading response: java.io.IOException: Server returned HTTP response code: 400 for URL: http://localhost:8983/solr/update\nPOSTing file mp500.xml\nSimplePostTool: WARNING: Solr returned an error #400 Bad Request\nSimplePostTool: WARNING: IOException while reading response: java.io.IOException: Server returned HTTP response code: 400 for URL: http://localhost:8983/solr/update\nPOSTing file sd500.xml\nSimplePostTool: WARNING: Solr returned an error #400 Bad Request\nSimplePostTool: WARNING: IOException while reading response: java.io.IOException: Server returned HTTP response code: 400 for URL: http://localhost:8983/solr/update\nPOSTing file solr.xml\nPOSTing file utf8-example.xml\nPOSTing file vidcard.xml\nSimplePostTool: WARNING: Solr returned an error #400 Bad Request\nSimplePostTool: WARNING: IOException while reading response: java.io.IOException: Server returned HTTP response code: 400 for URL: http://localhost:8983/solr/update\n14 files indexed.\nCOMMITting Solr index changes to http://localhost:8983/solr/update..\nTime spent: 0:00:00.401\n\n\n\nExceptions in Solr (I am pasting just one of them):\n\n5105 [qtp697879466-14] ERROR org.apache.solr.core.SolrCore  \u2013 org.apache.solr.common.SolrException: ERROR: [doc=EN7800GTX/2DHTV/256M] Error adding field 'price'='479.95' msg=For input string: \"479.95\"\n\tat org.apache.solr.update.DocumentBuilder.toDocument(DocumentBuilder.java:167)\n\tat org.apache.solr.update.AddUpdateCommand.getLuceneDocument(AddUpdateCommand.java:77)\n\tat org.apache.solr.update.DirectUpdateHandler2.addDoc0(DirectUpdateHandler2.java:234)\n\tat org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)\n\tat org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:69)\n\tat org.apache.solr.update.processor.UpdateRequestProcessor.processAdd(UpdateRequestProcessor.java:51)\n......\nCaused by: java.lang.NumberFormatException: For input string: \"479.95\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Long.parseLong(Long.java:441)\n\tat java.lang.Long.parseLong(Long.java:483)\n\tat org.apache.solr.schema.TrieField.createField(TrieField.java:609)\n\tat org.apache.solr.schema.TrieField.createFields(TrieField.java:660)\n\n\n\nThe full solr.log is attached.\n\nI understand why these errors occur but since we ship example data with Solr to demonstrate our core features, I expect that indexing exampledocs should work without errors.",
    "attachments": {
        "solr.log": "https://issues.apache.org/jira/secure/attachment/12641926/solr.log",
        "SOLR-6016.patch": "https://issues.apache.org/jira/secure/attachment/12655453/SOLR-6016.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Vitaliy Zhovtyuk",
            "id": "comment-14060098",
            "date": "2014-07-13T12:44:04+0000",
            "content": "The reason is should be know issue with TypeMapping in schemaless mode:\n\nWhen Integer type has priority over Double and Float and integer price \"10\" come first it's mapped like org.apache.solr.schema.TrieLongField, next price values like \"10.6\" that supposed to be org.apache.solr.schema.TrieDoubleField fail to be indexed.\n\nTherefore if Double has priority over Integer this will solve the issue, but then values that supposed to be like int and long can be mapped as double. I think make sense to distinguish numeric types with type ending \"10i\" as integer and \"10d\" as double and default type as integer. This allow to solve the issue with ambiguous types. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-14060157",
            "date": "2014-07-13T16:28:14+0000",
            "content": "bq: \"10i\" as integer and \"10d\" as double and default...\n\nI totally disagree. The instant you start doing that, you won't be able to index, say, a part number that happens to end in 'i' or 'd' as a string/text type (and someone will want to, guaranteed). Any rules like this will break something else down the road leading to endless thrashing. Not to mention endless questions on the user's list ...\n\nIf you have enough control over the input to do something like append the 'd' or 'i' to the input, you also have enough control to format all the fields \"properly\", i.e. if you require only integers, format all numbers without decimal points. If you require double, format everything with decimal points, and you can do this right now without imposing rules that'll cause unexpected behavior down the road.\n\nThat rant has no bearing on making the example documents work with schemaless mode though. Seems reasonable to me but that's just changing some of the example documents....\n\nErick\n "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-14060164",
            "date": "2014-07-13T16:46:23+0000",
            "content": "This is just one reason why I completely disagree with schemaless mode. It was really a stupid idea to have this in Elasticsearch and now - just because ES has it - Solr must somehow copy this horrible default behaviour. Sorry, this is all a very negative experience to everybody trying to use Solr.\n\nIn my opinion, it is much better to enforce people to define their schema before using Solr, although its more work. But fixing your schema/mapping once it was created is a bigger headache. Because you have to look at your data in any case. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-14060175",
            "date": "2014-07-13T17:19:10+0000",
            "content": "The schemaless mode is useful for one reason: To automatically generate a schema/mapping that you can later modify! But for that it would be better to have a \"schema learning\" mode in Solr, where Solr does not index documents: You just pass a bunch of documents to solr and after you have done this (yes: AFTER) it creates the \"best\" schema for you and returns it. By that the user has best flexibility: The system analyzes many documents and finds the schema, which can be used as the basis for your own schema.\n\nThe problem with the current schemaless mode is the fact, that it defines fields by the first occurence. The problem is visible with double fields without a colon. If the first document looks like an integer it creates an integer. In my proposal, Solr would look at like 10 or 100 documents and then decide for a schema. Much more user-friendly.\n\nThe schema-learning mode would have been a better feature than schemaless mode in Elasticsearch. By just copying the functionality we gained nothing (actually, it was a step back in my opinion) and now have the same problems all first-time users of Elasticsearch have. "
        },
        {
            "author": "Varun Thacker",
            "id": "comment-14060236",
            "date": "2014-07-13T23:37:34+0000",
            "content": "The schemaless mode is useful for one reason: To automatically generate a schema/mapping that you can later modify! But for that it would be better to have a \"schema learning\" mode in Solr, where Solr does not index documents: You just pass a bunch of documents to solr and after you have done this (yes: AFTER) it creates the \"best\" schema for you and returns it. By that the user has best flexibility: The system analyzes many documents and finds the schema, which can be used as the basis for your own schema.\n\n+1 . We could add a SchemaGeneratorHandler which would generate the \"best\" schema. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14060934",
            "date": "2014-07-14T17:54:18+0000",
            "content": "We could add a SchemaGeneratorHandler which would generate the \"best\" schema.\n\nYou wouldn't need/want a handler for this \u2013 you'd just need an UpdateProcessorFactory to use in place of RunUpdateProcessorFactory that would look at the datatpes of the fields in each document w/o doing any indexing and pick the least common denominator.\n\nSo then you'd have a chain with all of your normal update processors including the TypeMapping processors configured with the preccedence orders and locales and format strings you want \u2013 and at the end you'd have your BestFitScheamGeneratorUpdateProcessorFactory that would look at all those docs, study their values, and throw them away \u2013 until a commit comes along, at which point it does all the under the hood schema field addition calls.\n\nSo do learn, you'd send docs using whatever handler/format you wnat (json, xml, extraction, etc...) with an update.chain=my.datatype.learning.processor.chain request param ... and once you've sent a bunch and giving it a lot of variety to see, then you send a commit so it creates the schema and then you re-index your docs for real w/o that special chain.\n\nVarun: want to open a new issue for this idea? ... it's realted but independent to the current issue which might have other tweaks/improvements on it's own. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14060939",
            "date": "2014-07-14T17:56:06+0000",
            "content": "I think make sense to distinguish numeric types with type ending \"10i\" as integer and \"10d\" as double and default type as integer. This allow to solve the issue with ambiguous types.\n\n-1 .. but that doesn't mean there aren't improvements that can be made to the sample data itself: starting by switching from things like 10 to 10.0 in fields where the intention is to be a float/double. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-14060949",
            "date": "2014-07-14T18:01:00+0000",
            "content": "\nI think make sense to distinguish numeric types with type ending \"10i\" as integer and \"10d\" as double and default type as integer. This allow to solve the issue with ambiguous types.\n\n-1 .. but that doesn't mean there aren't improvements that can be made to the sample data itself: starting by switching from things like 10 to 10.0 in fields where the intention is to be a float/double.\n\nI agree with Hoss - we can't expect schemaless mode users to add suffixes to their data values to get correct behavior from Solr.\n\nThe minimal fix here, as Hoss says, is to change the example data so that they're all consistent.\n\nVitaliy Zhovtyuk, it would be useful to add a test that runs all example data through a schemaless setup, and randomizes the order in which they're indexed, so that we don't run into this problem again. "
        },
        {
            "author": "Vitaliy Zhovtyuk",
            "id": "comment-14083996",
            "date": "2014-08-03T14:46:33+0000",
            "content": "Added test with random order of values on schemaless example config, asserted thrown exception "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14084004",
            "date": "2014-08-03T15:01:55+0000",
            "content": "The schemaless mode is useful for one reason: To automatically generate a schema/mapping that you can later modify!\n\nI disagree. As you say, a tool that looks at a bunch of sample data is better for that. \n\nThe tool, while cool, would not provide for the only reason schmealess is actually useful - the new user and the prototyper. You should be able to pass a flag that puts Solr in 'easy' mode like this because that is the best getting started experience. That is the easiest way to play around right away. There is a lot of benefit to take obstacles out of the way as users learn, and then having them go back once they have firmer ideas in their head.\n\nIMO, schemaless is very valuable for getting started and random prototyping. It's not very useful for production setups. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-14087014",
            "date": "2014-08-06T00:18:24+0000",
            "content": "Added test with random order of values on schemaless example config, asserted thrown exception\n\nVitaliy Zhovtyuk, as I said earlier: \"The minimal fix here, as Hoss says, is to change the example data so that they're all consistent.\"  The kind of random test I was thinking of would ensure that the example data could be ingested by schemaless mode no matter what order they're presented in; your test is randomly ordered types of data, which may also be useful.\n\nAlso, the changes in solrconfig.xml in your latest patch include lots of whitespace-only changes (try to avoid those by telling your IDE not to change whitespace), as well as a vestigial change:\n\n\n@@ -1587,13 +1587,13 @@\n         <str name=\"fieldType\">tdates</str>\n       </lst>\n       <lst name=\"typeMapping\">\n+        <str name=\"valueClass\">java.lang.Number</str>\n+        <str name=\"fieldType\">tdoubles</str>\n+      </lst>\n+      <lst name=\"typeMapping\">\n         <str name=\"valueClass\">java.lang.Long</str>\n         <str name=\"valueClass\">java.lang.Integer</str>\n         <str name=\"fieldType\">tlongs</str>\n-      </lst>\n-      <lst name=\"typeMapping\">\n-        <str name=\"valueClass\">java.lang.Number</str>\n-        <str name=\"fieldType\">tdoubles</str>\n       </lst>\n     </processor>\n\n "
        },
        {
            "author": "Varun Thacker",
            "id": "comment-14087685",
            "date": "2014-08-06T13:54:52+0000",
            "content": "I updated SOLR-6127. The example now can be ingested by schemaless mode.\n\nTried it by running the following -\n\n java -jar post.jar ~/movie-dataset/film.xml \n\n\n\nRegarding testing I am not sure how useful will it be. Even if we read from the csv/xml/json file and index them to make sure they work with every release I don't think SolrExampleTestsBase will run the tests against all the example folders we ship with right? "
        },
        {
            "author": "Vitaliy Zhovtyuk",
            "id": "comment-14100056",
            "date": "2014-08-17T18:56:45+0000",
            "content": "File solrconfig.xml does not contain whitespace changes, the order of typeMapping elements changed intentionally to favor Double over Integer, therefore any tests with random values will pass by this configuration (since all will be as double). "
        },
        {
            "author": "Vitaliy Zhovtyuk",
            "id": "comment-14237259",
            "date": "2014-12-07T20:11:59+0000",
            "content": "Updated to latest trunk. Removed whitespace changes. "
        },
        {
            "author": "Timothy Potter",
            "id": "comment-14239828",
            "date": "2014-12-09T18:50:05+0000",
            "content": "Vitaliy Zhovtyuk Thanks for the update, but seems like the last patch is missing updates to server/solr/configsets/data_driven_schema_configs/conf/solrconfig.xml. "
        },
        {
            "author": "Vitaliy Zhovtyuk",
            "id": "comment-14240782",
            "date": "2014-12-10T07:55:28+0000",
            "content": "Yes, you're right. Missing file server/solr/configsets/data_driven_schema_configs/conf/solrconfig.xml added. "
        },
        {
            "author": "Alexandre Rafalovitch",
            "id": "comment-14260414",
            "date": "2014-12-29T20:25:25+0000",
            "content": "Chris Hostetter (Unused): You wouldn't need/want a handler for this \u2013 you'd just need an UpdateProcessorFactory to use in place of RunUpdateProcessorFactory that would look at the datatpes of the fields in each document w/o doing any indexing and pick the least common denominator.\n\nThis sounds like an awesome and doable idea. Has JIRA been created for this? If not, we should. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-14260416",
            "date": "2014-12-29T20:28:18+0000",
            "content": "I vote for trunk (and even 5x) that we switch over to the films example data (which addresses this issue) and get rid of the old techproducts stuff.   But maybe the best thing to do is first fix the existing data and then deal with switching it over. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14260998",
            "date": "2014-12-30T11:00:24+0000",
            "content": "Commit 1648504 from Erik Hatcher in branch 'dev/trunk'\n[ https://svn.apache.org/r1648504 ]\n\nSOLR-6016: Fix example techproduct data to work with schemaless example mode "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14260999",
            "date": "2014-12-30T11:01:28+0000",
            "content": "Commit 1648505 from Erik Hatcher in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1648505 ]\n\nSOLR-6016: Fix example techproduct data to work with schemaless example mode (merged from trunk r1648504) "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-14261002",
            "date": "2014-12-30T11:03:47+0000",
            "content": "calling this fixed - example data has been adjusted to have floating point numbers for price and weight fields. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-14270652",
            "date": "2015-01-09T06:50:27+0000",
            "content": "This sounds like an awesome and doable idea. Has JIRA been created for this? If not, we should.\n\nSOLR-6939 "
        }
    ]
}