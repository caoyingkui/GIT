{
    "id": "LUCENE-4583",
    "title": "StraightBytesDocValuesField fails if bytes > 32k",
    "details": {
        "components": [
            "core/index"
        ],
        "fix_versions": [
            "4.5",
            "6.0"
        ],
        "affect_versions": "4.0,                                            4.1,                                            6.0",
        "priority": "Critical",
        "labels": "",
        "type": "Bug",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "I didn't observe any limitations on the size of a bytes based DocValues field value in the docs.  It appears that the limit is 32k, although I didn't get any friendly error telling me that was the limit.  32k is kind of small IMO; I suspect this limit is unintended and as such is a bug.    The following test fails:\n\n\n  public void testBigDocValue() throws IOException {\n    Directory dir = newDirectory();\n    IndexWriter writer = new IndexWriter(dir, writerConfig(false));\n\n    Document doc = new Document();\n    BytesRef bytes = new BytesRef((4+4)*4097);//4096 works\n    bytes.length = bytes.bytes.length;//byte data doesn't matter\n    doc.add(new StraightBytesDocValuesField(\"dvField\", bytes));\n    writer.addDocument(doc);\n    writer.commit();\n    writer.close();\n\n    DirectoryReader reader = DirectoryReader.open(dir);\n    DocValues docValues = MultiDocValues.getDocValues(reader, \"dvField\");\n    //FAILS IF BYTES IS BIG!\n    docValues.getSource().getBytes(0, bytes);\n\n    reader.close();\n    dir.close();\n  }",
    "attachments": {
        "LUCENE-4583.patch": "https://issues.apache.org/jira/secure/attachment/12555952/LUCENE-4583.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-12-01T16:24:46+0000",
            "content": "I'm guessing that limit is due to the implementation with PagedBytes?\nThese limits were sensible when applied to indexed values, but obviously not so much to stored values (unless we decide that DocValues are only meant for smallish values and document the limit).  ",
            "author": "Yonik Seeley",
            "id": "comment-13507998"
        },
        {
            "date": "2012-12-02T15:01:00+0000",
            "content": "The most important thing: if this implementation (or if we decide dv itself) should be limited,\nthen it should check this at index-time and throw a useful exception. ",
            "author": "Robert Muir",
            "id": "comment-13508284"
        },
        {
            "date": "2012-12-03T14:27:34+0000",
            "content": "FWIW the app the triggered this has a document requiring ~68k but there's a long tail down such that most documents only need 8 bytes.  As a hack, I could use multiple fields to break out of the 32k limit and concatenate each together (yuck).  It'd be great if this 32k limit wasn't there. ",
            "author": "David Smiley",
            "id": "comment-13508759"
        },
        {
            "date": "2012-12-03T14:29:59+0000",
            "content": "I'm not concerned if the limit is 10 bytes.\n\nif it is, then it is what it is.\n\nIts just important that IW throw exception at index-time when any such limit is exceeded. ",
            "author": "Robert Muir",
            "id": "comment-13508761"
        },
        {
            "date": "2012-12-03T14:33:54+0000",
            "content": "The only correct bugfix here is a missing check.\n\nany discussion about extending limitations in the supported lengths belongs on another issue. ",
            "author": "Robert Muir",
            "id": "comment-13508763"
        },
        {
            "date": "2012-12-03T18:08:46+0000",
            "content": "Looking at the issue name and the problem that David ran into, this issue is certainly about more than a missing check during indexing.\nSmall hidden limits can still cause stuff to blow up in production - the user may not have thought to test anything above 32K.  Small limits need to be documented.\n\nLike David, I also suspect that the limit was unintended and represents a bug.\nThe question is on a practical level, how easy is it to raise the limit, and are there any negative consequences of doing so?  If it's not easy (or there are negative consequences), I think it's OK to leave it at 32K and document it as a current limitation.  Off the top of my head, I can't really think of use cases that would require more, but perhaps others might?\n\nOf course we should also fail early if someone tries to add a value above that limit. ",
            "author": "Yonik Seeley",
            "id": "comment-13508910"
        },
        {
            "date": "2012-12-03T18:28:18+0000",
            "content": "Again, I really think adding the check and documenting current limits is what should happen here.\n\nJust like length of indexed terms are limited to 32k, its a bigger issue to try to deal with this (especially in the current DV situation).\n\nI think also if you are putting very large values in DV, you know its perfectly acceptable to require a custom codec for this kind of situation. the one we provide can be general purpose.\n\nI dont think we should try to cram in a huge change to the limits masqueraded as a bug... the bug is not recognizing the limit at index time (and of course documenting it in the codec, or indexwriter, depending on where it is (currently here i think its the codec). ",
            "author": "Robert Muir",
            "id": "comment-13508924"
        },
        {
            "date": "2012-12-03T19:22:18+0000",
            "content": "I dont think we should try to cram in a huge change to the limits masqueraded as a bug...\n\nIf the limit was not intentional then it was certainly a bug (not just a missing check). Now we have to figure out what to do about it.\n\nThis issue is about deciding what the limit should be (and 32K may be fine, depending on the trade-offs, as I said), and then documenting and enforcing that limit.\nFor example your previous \"I'm not concerned if the limit is 10 bytes.\" would get a -1 from me as \"clearly not big enough... lets fix it\". ",
            "author": "Yonik Seeley",
            "id": "comment-13508955"
        },
        {
            "date": "2012-12-03T19:49:30+0000",
            "content": "Just like changing the length here gets a -1 from me. Pretty simple. ",
            "author": "Robert Muir",
            "id": "comment-13508979"
        },
        {
            "date": "2012-12-03T19:54:56+0000",
            "content": "Just like changing the length here gets a -1 from me. Pretty simple.\n\nRegardless of how easy or hard it might be, regardless of what use cases are brought up (I was waiting to hear from David at least, since he hit it), regardless of what the trade-offs might be involved with changing an unintended/accidental limit?  That's just silly.\n\nIt also doesn't make sense to -1 something \"here\" vs somewhere else.  +1s/-1s are for code changes, regardless of where they are discussed. ",
            "author": "Yonik Seeley",
            "id": "comment-13508988"
        },
        {
            "date": "2012-12-03T21:03:33+0000",
            "content": "The limitation comes from PagedBytes. When PagedBytes is created it is given a number of bits to use per block. The blockSize is set to (1 << blockBits). From what I've seen, classes that use PagedBytes usually pass in 15 as the blockBits. This leads to the 32768 byte limit.\n\nThe fillSlice function of the PagedBytes.Reader will return a block of bytes that is either inside one block or overlapping two blocks. If you try to give it a length that is over the block size it will hit the out of bounds exception. For the project I am working on, we need more than 32k bytes for our DocValues. We need that much rarely, but we still need that much to keep the search functioning. I fixed this for our project by changing fillSlices to this:\n\nhttp://pastebin.com/raw.php?i=TCY8zjAi\n\nTest unit:\nhttp://pastebin.com/raw.php?i=Uy29BGGJ\n\nAfter placing this in our Solr instance, the search no longer crashes and returns the correct values when the document has a DocValues field more than 32k bytes. As far as I know there is no limit now. I haven't noticed a performance hit. It shouldn't really affect performance unless you have many of these large DocValues fields. Thank you to David for his help with this.\n\nEdit: This only works when start == 0. Seeing if I can fix it. ",
            "author": "Barakat Barakat",
            "id": "comment-13509044"
        },
        {
            "date": "2012-12-03T22:08:34+0000",
            "content": "I am working on, we need more than 32k bytes for our DocValues. We need that much rarely, but we still need that much to keep the search functioning.\n\nThanks!  I suspected one generic use case would be \"normally small, but hard to put an upper bound on\".  That's great if the only issue really is PagedBytes.fillSlices()!  That definitely shouldn't have any performance impact since the first \"if\" will always be true in the common case.\n\nEdit: This only works when start == 0. Seeing if I can fix it.\n\nA simple loop might be easiest to understand, rather than calculating with DIV and MOD? ",
            "author": "Yonik Seeley",
            "id": "comment-13509117"
        },
        {
            "date": "2012-12-04T16:59:17+0000",
            "content": "I updated the code to work when start isn't zero. The code can still crash if you ask for a length that goes beyond the total size of the paged bytes, but I'm not sure how you guys like to prevent things like that. The code seems to be working fine with our Solr core so far. I am new to posting patches and writing test units in Java so please let me know if there is anything wrong with the code. ",
            "author": "Barakat Barakat",
            "id": "comment-13509856"
        },
        {
            "date": "2013-05-08T14:20:21+0000",
            "content": "I recently switched from 4.1 to 4.3, and my patch needed to be updated because of the changes to DocValues. The problem was almost fixed for BinaryDocValues, but it just needed one little change. I've attached a patch that removes the BinaryDocValues exception when the length is over BYTE_BLOCK_SIZE (32k), fixes ByteBlockPool#readBytes:348, and changes the TestDocValuesIndexing#testTooLargeBytes test to check for accuracy. ",
            "author": "Barakat Barakat",
            "id": "comment-13651926"
        },
        {
            "date": "2013-05-08T14:46:09+0000",
            "content": "I dont think we should just bump the limit like this.\n\nthe patch is not safe: Some codecs rely upon this limit (e.g. they use 64k-size pagedbytes and other things).\n\nBut in general anyway I'm not sure what the real use cases are for storing > 32kb inside a single document value. ",
            "author": "Robert Muir",
            "id": "comment-13651945"
        },
        {
            "date": "2013-05-08T15:29:22+0000",
            "content": "A user hit this limitation on the user list a couple weeks ago while try to index a very large number of facets: http://markmail.org/message/dfn4mk3qe7advzcd. So this is one usecase, and also it appears there is an exception thrown at indexing time? ",
            "author": "Shai Erera",
            "id": "comment-13651990"
        },
        {
            "date": "2013-05-08T15:37:52+0000",
            "content": "Over 6,000 facets per doc?\n\nYes there is an exception thrown at index time. I added this intentionally and added a test that ensures you get it. ",
            "author": "Robert Muir",
            "id": "comment-13651994"
        },
        {
            "date": "2013-05-08T16:02:41+0000",
            "content": "Over 6,000 facets per doc?\n\nRight. Not sure how realistic is his case (something about proteins, he explains it here: http://markmail.org/message/2wxavktzyxjtijqe), and whether he should enable facet partitions or not, but he agreed these are extreme cases and expects only few docs like that. At any rate, you asked for a usecase . Not saying we should support it, but it is one. If it can be supported without too much pain, then perhaps we should. Otherwise, I think we can live w/ the limitation and leave it for the app to worry about a workaround / write its own Codec. ",
            "author": "Shai Erera",
            "id": "comment-13652012"
        },
        {
            "date": "2013-05-08T16:04:46+0000",
            "content": "For the faceting use case, we have SortedSetDocValuesField, which can hold 2B facets per-field and you can have.... maybe 2B fields per doc.\n\nSo the limitation here is not docvalues.\n\nBINARY datatype isnt designed for faceting. ",
            "author": "Robert Muir",
            "id": "comment-13652014"
        },
        {
            "date": "2013-05-08T16:43:35+0000",
            "content": "I think we should fix the limit in core, and then existing codecs should enforce their own limits, at indexing time?\n\nThis way users that sometimes need to store > 32 KB binary doc value can do so, with the right DVFormat. ",
            "author": "Michael McCandless",
            "id": "comment-13652045"
        },
        {
            "date": "2013-05-08T17:33:41+0000",
            "content": "For the faceting use case, we have SortedSetDocValuesField\n\nNo, this is for one type of faceting, no hierarchies etc. It's also slower than the BINARY DV method ...\n\nBINARY datatype isnt designed for faceting.\n\nMaybe, but that's the best we have for now. ",
            "author": "Shai Erera",
            "id": "comment-13652097"
        },
        {
            "date": "2013-05-08T17:40:10+0000",
            "content": "\nNo, this is for one type of faceting, no hierarchies etc. It's also slower than the BINARY DV method ...\n\nIts not inherently slower. I just didnt spend a month inlining vint codes or writing custom codecs like you did for the other faceting method. \nInstead its the simplest thing that can possibly work right now.\n\nI will call you guys out on this every single time you bring it up.\n\nI'm -1 to bumping the limit ",
            "author": "Robert Muir",
            "id": "comment-13652105"
        },
        {
            "date": "2013-05-08T17:41:47+0000",
            "content": "I looked over the patch carefully and stepped through the relevant code in a debugging session and I think it's good.\n\nI don't see why this arbitrary limit was here. The use-case for why Barakat hit this is related to storing values per document so that a custom ValueSource I wrote can examine it.  It's for spatial multi-value fields and some businesses (docs) have a ton of locations out there (e.g. McDonalds).  FWIW very few docs have such large values, and if it were to become slow then I have ways to more cleverly examine a subset of the returned bytes.  I think its silly to force the app to write a Codec (even if its trivial) to get out of this arbitrary limit.\n ",
            "author": "David Smiley",
            "id": "comment-13652108"
        },
        {
            "date": "2013-05-09T14:14:30+0000",
            "content": "I iterated from Barakat's patch: improved the test, and added\nenforcing of the limit in the appropriate DocValuesFormats impls.\n\nDisk, SimpleText, CheapBastard and (I think ... still need a test\nhere) Facet42DVFormat don't have a limit, but Lucene40/42 still do.\n\nI'm -1 to bumping the limit\n\nAre you also against just fixing the limit in the core code\n(IndexWriter/BinaryDocValuesWriter) and leaving the limit enforced in\nthe existing DVFormats (my patch)?\n\nI thought that was a good compromise ...\n\nThis way at least users can still build their own / use DVFormats that\ndon't have the limit. ",
            "author": "Michael McCandless",
            "id": "comment-13652983"
        },
        {
            "date": "2013-05-09T20:54:31+0000",
            "content": "the patch is not safe: Some codecs rely upon this limit (e.g. they use 64k-size pagedbytes and other things).\n\nRob, can you please elaborate? ",
            "author": "David Smiley",
            "id": "comment-13653179"
        },
        {
            "date": "2013-05-12T12:09:20+0000",
            "content": "\nAre you also against just fixing the limit in the core code\n(IndexWriter/BinaryDocValuesWriter) and leaving the limit enforced in\nthe existing DVFormats (my patch)?\n\nI thought that was a good compromise ...\n\nThis way at least users can still build their own / use DVFormats that\ndon't have the limit.\n\nI'm worried about a few things:\n\n\tI think the limit is ok, because in my eyes its the limit of a single term. I feel that anyone arguing for increasing the limit only has abuse cases (not use cases) in mind. I'm worried about making dv more complicated for no good reason.\n\tI'm worried about opening up the possibility of bugs and index corruption (e.g. clearly MULTIPLE people on this issue dont understand why you cannot just remove IndexWriter's limit without causing corruption).\n\tI'm really worried about the precedent: once these abuse-case-fans have their way and increase this limit, they will next argue that we should do the same for SORTED, maybe SORTED_SET, maybe even inverted terms. They will make arguments that its the same as binary, just with sorting, and why should sorting bring in additional limits. I can easily see this all spinning out of control.\n\tI think that most people hitting the limit are abusing docvalues as stored fields, so the limit is providing a really useful thing today actually, and telling them they are doing something wrong.\n\n\n\nThe only argument i have for removing the limit is that by expanding BINARY's possible abuse cases (in my opinion, thats pretty much all its useful for), we might prevent additional complexity from being added elsewhere to DV in the long-term. ",
            "author": "Robert Muir",
            "id": "comment-13655522"
        },
        {
            "date": "2013-05-12T12:35:31+0000",
            "content": "abusing docvalues as stored fields\n\nGreat point. I have to admit that I still don't have a 100% handle on the use case(s) for docvalues vs. stored fields, even though I've asked on the list. I mean, sometimes the chatter seems to suggest that dv is the successor to stored values. Hmmm... in that case, I should be able to store the full text of a 24 MB PDF file in a dv. Now, I know that isn't true.\n\nMaybe we just need to start with some common use cases, based on size: tiny (16 bytes or less), small (256 or 1024 bytes or less), medium (up to 32K), and large (upwards of 1MB, and larger.) It sounds like large implies stored field.\n\nA related \"concern\" is dv or stored fields that need a bias towards being in memory and in the heap, vs. a bias towards being \"off heap\". Maybe the size category is the hint: tiny and small bias towards on-heap, medium and certainly large bias towards off-heap. If people are only going towards DV because they think they get off-heap, then maybe we need to reconsider the model of what DV vs. stored is really all about. But then that leads back to DV somehow morphing out of column-stride fields. ",
            "author": "Jack Krupansky",
            "id": "comment-13655525"
        },
        {
            "date": "2013-05-12T12:45:40+0000",
            "content": "\nI'm worried about a few things:\nI think the limit is ok, because in my eyes its the limit of a single term. I feel that anyone arguing for increasing the limit only has abuse cases (not use cases) in mind. I'm worried about making dv more complicated for no good reason.\n\nI guess I see DV binary as more like a stored field, just stored\ncolumn stride for faster access.  Faceting (and I guess spatial)\nencode many things inside one DV binary field.\n\nI'm worried about opening up the possibility of bugs and index corruption (e.g. clearly MULTIPLE people on this issue dont understand why you cannot just remove IndexWriter's limit without causing corruption).\n\nI agree this is a concern and we need to take it slow, add good\ntest coverage.\n\n\nI'm really worried about the precedent: once these abuse-case-fans have their way and increase this limit, they will next argue that we should do the same for SORTED, maybe SORTED_SET, maybe even inverted terms. They will make arguments that its the same as binary, just with sorting, and why should sorting bring in additional limits. I can easily see this all spinning out of control.\nI think that most people hitting the limit are abusing docvalues as stored fields, so the limit is providing a really useful thing today actually, and telling them they are doing something wrong.\n\nI don't think we should change the limit for sorted/set nor terms: I\nthink we should raise the limit ONLY for BINARY, and declare that DV\nBINARY is for these \"abuse\" cases.  So if you really really want\nsorted set with a higher limit then you will have to encode yourself\ninto DV BINARY.\n\n\nThe only argument i have for removing the limit is that by expanding BINARY's possible abuse cases (in my opinion, thats pretty much all its useful for), we might prevent additional complexity from being added elsewhere to DV in the long-term.\n\n+1 ",
            "author": "Michael McCandless",
            "id": "comment-13655526"
        },
        {
            "date": "2013-05-12T12:55:18+0000",
            "content": "I have to admit that I still don't have a 100% handle on the use case(s) for docvalues vs. stored fields, even though I've asked on the list. I mean, sometimes the chatter seems to suggest that dv is the successor to stored values. Hmmm... in that case, I should be able to store the full text of a 24 MB PDF file in a dv. Now, I know that isn't true.\n\nThe big difference is that DV fields are stored column stride, so you\ncan decide on a field by field basis whether it will be in RAM on disk\netc., and you get faster access if you know you just need to work with\njust one or two fields.\n\nVs stored fields where all fields for one document are stored\n\"together\".\n\nEach has different tradeoffs so it's really up to the app to decide\nwhich is best... if you know you need 12 fields loaded for each\ndocument you are presenting on the current page, stored fields is\nprobably best.\n\nBut if you need one field to use as a scoring factor (eg maybe you are\nboosting by recency) then column-stride is better. ",
            "author": "Michael McCandless",
            "id": "comment-13655529"
        },
        {
            "date": "2013-05-13T14:28:16+0000",
            "content": "Quoting Mike:\nI don't think we should change the limit for sorted/set nor terms: I\nthink we should raise the limit ONLY for BINARY, and declare that DV\nBINARY is for these \"abuse\" cases. So if you really really want\nsorted set with a higher limit then you will have to encode yourself\ninto DV BINARY.\n\n+1.  DV Binary is generic for applications to use as it might see fit.  There is no use case to abuse.  If this issue passes, I'm not going to then ask for terms > 32k or something silly like that. ",
            "author": "David Smiley",
            "id": "comment-13655998"
        },
        {
            "date": "2013-05-14T05:31:31+0000",
            "content": "I can compromise with this. \n\nHowever, I don't like the current patch.\n\nI don't think we should modify ByteBlockPool. Instead, I think BinaryDocValuesWriter should do the following:\n\n\tuse PagedBytes to append the bytes (which has append-only writing, via getDataOutput)\n\timplement the iterator with PagedBytes.getDataInput (its just an iterator so this is simple)\n\tstore the lengths instead as absolute offsets with MonotonicAppendingLongBuffer (this should be more efficient)\n\n\n\nThe only thing ByteBlockPool gives is some \"automagic\" ram accounting, but this is not as good as it looks anyway.\n\n\ttoday, it seems to me ram accounting is broken for this dv type already (the lengths are not considered or am i missing something!?)\n\tsince we need to fix that bug anyway (by just adding updateBytesUsed like the other consumers), the magic accounting buys us nothing really anyway.\n\n\n\nAnyway I can help with this, tomorrow. ",
            "author": "Robert Muir",
            "id": "comment-13656831"
        },
        {
            "date": "2013-05-14T10:47:03+0000",
            "content": "+1 to cutover to PagedBytes! ",
            "author": "Michael McCandless",
            "id": "comment-13656945"
        },
        {
            "date": "2013-05-14T12:03:21+0000",
            "content": "New patch, cutting over to PagedBytes.  I also revived .getDataInput/Output (from 4.x).\n\nI also added javadocs about these limits in DocValuesType.\n\nThere are still nocommits to resolve ... the biggest one is whether we should fix default DV impl to accept values > 32 KB ... right now it throws IAE. ",
            "author": "Michael McCandless",
            "id": "comment-13656989"
        },
        {
            "date": "2013-05-14T12:10:46+0000",
            "content": "\nThere are still nocommits to resolve ... the biggest one is whether we should fix default DV impl to accept values > 32 KB ... right now it throws IAE.\n\nI do not think this should change. This disagrees from your earlier comments: its already spinning out of control just as I mentioned it might. ",
            "author": "Robert Muir",
            "id": "comment-13656994"
        },
        {
            "date": "2013-05-14T16:00:28+0000",
            "content": "store the lengths instead as absolute offsets with MonotonicAppendingLongBuffer (this should be more efficient)\n\nI just had a quick discussion about this with Robert, and since AppendingLongBuffer stores deltas from the minimum value of the block (and not 0), AppendingLongBuffer is better (ie. faster and more compact) than MonotonicAppendingLongBuffer to store lengths. This means that if all lengths are 7, 8 or 9 in a block, it will only require 2 bits per value instead of 4. ",
            "author": "Adrien Grand",
            "id": "comment-13657156"
        },
        {
            "date": "2013-05-14T18:01:53+0000",
            "content": "I do not think this should change. \n\nOK I just removed that nocommit.\n\nI just had a quick discussion about this with Robert, and since AppendingLongBuffer stores deltas from the minimum value of the block (and not 0), AppendingLongBuffer is better (ie. faster and more compact) than MonotonicAppendingLongBuffer to store lengths. This means that if all lengths are 7, 8 or 9 in a block, it will only require 2 bits per value instead of 4.\n\nAhh, OK: I switched back to AppendingLongBuffer.\n\nNew patch.  I factored the testHugeBinaryValues up into the\nBaseDocValuesFormatTestCase base class, and added protected method so\nthe codecs that don't accept huge binary values can say so.  I also\nadded a test case for Facet42DVFormat, and cut back to\nAppendingLongBuffer.\n\nI downgraded the nocommit about the spooky unused PagedBytes.blockEnds\nto a TODO ... this class is somewhat dangerous because e.g. you can\nuse copyUsingLengthPrefix method and then get a .getDataOutput and get\ncorrumpted bytes out. ",
            "author": "Michael McCandless",
            "id": "comment-13657296"
        },
        {
            "date": "2013-05-14T18:16:27+0000",
            "content": "this one is looking pretty good, but needs a few fixes. at least, we should fix the tests:\n\nNOTE: reproduce with: ant test  -Dtestcase=TestDocValuesFormat -Dtests.method=testHugeBinaryValues -Dtests.seed=5F68849875ABAC05 -Dtests.slow=true -Dtests.locale=es_PY -Dtests.timezone=Canada/Mountain -Dtests.file.encoding=ISO-8859-1\n ",
            "author": "Robert Muir",
            "id": "comment-13657321"
        },
        {
            "date": "2013-05-14T18:39:12+0000",
            "content": "I like the new test, Mike \u2013 in particular it doesn't mandate a failure if the codec accepts > 32k.\n\nI want to make sure it's clear what the logic is behind the decisions being made by Mike & Rob on this thread regarding the limits for binary doc values (not other things).  Firstly there is no intrinsic technical limitation that the Lucene42Consumer has on these values to perhaps 2GB (not sure but \"big\").  Yet it is being decided to artificially neuter it to 32k.  I don't see anything in this thread establishing a particular use of binary DocValues that established it's intended use; I see it as general purpose as stored values, with different performance characteristics (clearly it's column-stride, for example).  The particular use I established earlier would totally suck if it had to use stored values.  And the reason for this limit... I'm struggling to find the arguments in this thread but appears to be that hypothetically in the future, there might evolve newer clever encodings that simply can't handle more than 32k.  If that's it then wouldn't such a new implementation simply have this different limit, and leave both as reasonable choices by the application?  If that isn't it then what is the reason? ",
            "author": "David Smiley",
            "id": "comment-13657345"
        },
        {
            "date": "2013-05-14T18:44:14+0000",
            "content": "\nFirstly there is no intrinsic technical limitation that the Lucene42Consumer has on these values to perhaps 2GB (not sure but \"big\"). Yet it is being decided to artificially neuter it to 32k.\n\nThis is absolutely not correct. The fact that this limitation exists (based on pagedbytes blocksize) and that nobody sees it makes me really want to rethink what we are doing here: I dont want to allow index corruption, sorry. ",
            "author": "Robert Muir",
            "id": "comment-13657357"
        },
        {
            "date": "2013-05-14T19:01:09+0000",
            "content": "I don't follow.  From the javadocs, I get the impression that PagedBytes can handle basically any data size.  blockBits appears to tune the block size, but I don't see how that limits the total capacity in any significant way. The only method I see that appears to have a limit is copy(BytesRef bytes, BytesRef out) which is only used in uninverting doc terms which doesn't apply to binary DocValues. ",
            "author": "David Smiley",
            "id": "comment-13657384"
        },
        {
            "date": "2013-05-14T19:04:10+0000",
            "content": "just look at the java docs for the only PagedBytes method that Lucene42's binary dv producer actually uses:\n\n\n     * ...\n     * Slices spanning more than one block are not supported.\n     * ...\n     **/\n    public void fillSlice(BytesRef b, long start, int length) {\n\n ",
            "author": "Robert Muir",
            "id": "comment-13657388"
        },
        {
            "date": "2013-05-14T20:41:05+0000",
            "content": "Aha; thanks for the clarification.  I see it now.  And I see that after I commented the limit check, the assertion was hit.  I didn't hit this assertion with Barakat's patch when I last ran it; weird but whatever.\n\nBTW ByteBlockPool doesn't really have this limit, notwithstanding the bug that Barakat fixed in his patch. It's not a hard limit as BBP.append() and readBytes() will conveniently loop for you whereas if code uses PagedBytes then you could loop on fillSlice() yourself to support big values.  That is a bona-fide bug on ByteBlockPool that it didn't implement that loop correctly and it should be fixed if not in this issue then another.\n\nSo a DocValues codec that supports large binary values could be nearly identical to the current codec but call fillSlice() in a loop, and only for variable-sized binary values (just like BBP's algorithm), and that would basically be the only change. Do you support such a change? If not then why not (a technical reason please)?  If you can't support such a change, then would you also object to the addition of a new codec that simply lifted this limit as I proposed?  Note that would include potentially a bunch of duplicated code just to call fillSlice() in a loop; I propose it would be simpler and more maintainable to not limit binary docvalues to 32k. ",
            "author": "David Smiley",
            "id": "comment-13657491"
        },
        {
            "date": "2013-05-14T20:51:23+0000",
            "content": "Just to clarify, before 4.3 I was fixing the \"bug\" by changing PagedBytes#fillSlice to the first patch I posted in this issue. ",
            "author": "Barakat Barakat",
            "id": "comment-13657498"
        },
        {
            "date": "2013-05-14T21:04:29+0000",
            "content": "No, I don't support changing this codec. Its an all-in-memory one (which is an unfortunate default, but must be until various algorithms in grouping/join/etc package are improved such that we can safely use something more like DiskDV as a default). Other all-memory implementations like DirectPostingsFormat/MemoryPostings have similar limitations, even the specialized faceting one (e.g. entire segment cannot have more than 2GB total bytes).\n\nI dont want to add a bunch of stuff in a loop here or any of that, because it only causes additional complexity for the normal case, and I think its unreasonable to use a RAM docvalues impl if you have more than 32KB per-document cost anyway. Sorry, thats just crazy: and I don't think we should add any additional trappy codec to support that.\n\nSo if you want ridiculously huge per-document values, just use DiskDV which supports that. These abuse cases are extreme: if you really really want that all in RAM, then use it with FileSwitchDirectory.\n\nI mentioned before I was worried about this issue spinning out of control, and it appears this has taken place. Given these developments, i'd rather we not change the current limit at all. ",
            "author": "Robert Muir",
            "id": "comment-13657509"
        },
        {
            "date": "2013-05-14T21:19:50+0000",
            "content": "David can you open a separate issue about changing the limit for existing codecs?  The default DVFormat today is all-in-RAM, and I think it's OK for it to have limits, and e.g. if/when we change the default to DiskDV, it has no limit.\n\nI think this issue should focus solely on fixing core/indexer to not enforce the limit (i.e., moving the limit enforcing to those DVFormats that have it). ",
            "author": "Michael McCandless",
            "id": "comment-13657518"
        },
        {
            "date": "2013-05-15T05:57:15+0000",
            "content": "I can understand that an all in-RAM codec has size sensitivities.  In that light, I can also understand that 32KB per document is a lot.  The average per-document variable byte length size for Barakat's index is a measly 10 bytes.  The maximum is around 69k.  Likewise for the user Shai referenced on the list who was using it for faceting, it's only the worst-case document(s) that exceeded 32KB.\n\nMight the \"new PagedBytes(16)\" in Lucene42DocValuesProducer.loadBinary() be made configurable? i.e. Make 16 configurable?  And/or perhaps make loadBinary() protected so another codec extending this one can keep the change somewhat minimal.\n\nMike, in your latest patch, one improvement that could be made is instead of Lucene42DocValuesConsumer assuming the limit is \"ByteBlockPool.BYTE_BLOCK_SIZE - 2\" (which it technically is but only by coincidence), you could instead reference a calculated constant shared with the actual code that has this limit which is Lucene42DocValuesProducer.loadBinary().  For example, set the constant to 2^16-2 but then add an assert in loadBinary that the constant is consistent with the PagedBytes instance's config.  Or something like that.\n\nDavid can you open a separate issue about changing the limit for existing codecs?\n\nUh... all the discussion has been here so seems too late to me. And I'm probably done making my arguments.  I can't be more convincing than pointing out the 10-byte average figure for my use case. ",
            "author": "David Smiley",
            "id": "comment-13658088"
        },
        {
            "date": "2013-05-15T12:17:06+0000",
            "content": "You convinced me dont worry, you convinced me we shouldnt do anything on this whole issue at all. Because the stuff you outlined here is absolutely the wrong path for us to be going down. ",
            "author": "Robert Muir",
            "id": "comment-13658287"
        },
        {
            "date": "2013-05-16T11:35:54+0000",
            "content": "\nMike, in your latest patch, one improvement that could be made is instead of Lucene42DocValuesConsumer assuming the limit is \"ByteBlockPool.BYTE_BLOCK_SIZE - 2\" (which it technically is but only by coincidence), you could instead reference a calculated constant shared with the actual code that has this limit which is Lucene42DocValuesProducer.loadBinary(). For example, set the constant to 2^16-2 but then add an assert in loadBinary that the constant is consistent with the PagedBytes instance's config. Or something like that.\n\n+1\n\nBut, again, let's keep this issue focused on not enforcing a limit in the core indexing code.\n\nPer-codec limits are separate issues.\n ",
            "author": "Michael McCandless",
            "id": "comment-13659447"
        },
        {
            "date": "2013-05-21T14:02:28+0000",
            "content": "Ok.  I have nothing further to say in this issue; I welcome your improvements, Mike. ",
            "author": "David Smiley",
            "id": "comment-13662991"
        },
        {
            "date": "2013-06-19T07:37:08+0000",
            "content": "A few comments up someone asked for a use case, shouldn't something like http://www.elasticsearch.org/guide/reference/mapping/source-field/ be a perfect thing to use BinaryDocValues for? \n\nI was trying to store something similar using DiskDocValuesFormat and hit the 32k limit ",
            "author": "selckin",
            "id": "comment-13687730"
        },
        {
            "date": "2013-06-19T11:13:31+0000",
            "content": "good god no.\n\nDocValues are not stored fields... \n\nThis reinforces the value of the limit! ",
            "author": "Robert Muir",
            "id": "comment-13687883"
        },
        {
            "date": "2013-06-19T11:38:50+0000",
            "content": "Ok, from the talks i watched on them & other info gathered it seemed like it would be a good fit, guess i really missed the point somewhere, can't find much info in the javadocs either, but guess this is for the user list and i shouldn't pollute this issue ",
            "author": "selckin",
            "id": "comment-13687894"
        },
        {
            "date": "2013-06-19T16:17:38+0000",
            "content": "Should the \"closed\" status and resolution change to \"not a problem\" mean that Michael McCandless improvement's in his patch here (that don't change the limit) won't get applied?  They looked good to me.  And you? ",
            "author": "David Smiley",
            "id": "comment-13688113"
        },
        {
            "date": "2013-06-19T17:05:25+0000",
            "content": "I still think we should fix the limitation in core; this way apps that want to store large binary fields per-doc are able to use a custom DVFormat. ",
            "author": "Michael McCandless",
            "id": "comment-13688162"
        },
        {
            "date": "2013-06-19T17:07:23+0000",
            "content": "I still think we should fix the limitation in core; this way apps that want to store large binary fields per-doc are able to use a custom DVFormat.\n\n+1\narbitrary limits are not a feature. ",
            "author": "Yonik Seeley",
            "id": "comment-13688163"
        },
        {
            "date": "2013-06-21T20:20:41+0000",
            "content": "Another iteration on the patch:\n\n\n\tI added constants MAX_BINARY_FIELD_LENGTH to\n    Lucene4\n{0,2}\nDocValuesFormat, and then reference that in the\n    Writer/Consumer to catch too-big values.\n\n\n\n\n\tI added another test to BaseDocValuesFormatTestCase, to test the\n    exact maximum length value.\n\n\n\n\n\tI fixed that test failure, by passing the String field to the\n    codecAcceptsHugeBinaryValues method, and adding a _TestUtil helper\n    method to check this.\n\n\n\nAn alternative to the protected method would be to have two separate\ntests in the base class, one test verifying a clean IllegalArgumentExc\nis thrown when the value is too big, and another verifying huge binary\nvalues can be indexed successfully.  And then I'd fix each DVFormat's\ntest to subclass and @Ignore whichever base test is not appropriate.\n\nBut I don't think this would simplify things much?  Ie,\nTestDocValuesFormat would still need logic to check depending on the\ndefault codec. ",
            "author": "Michael McCandless",
            "id": "comment-13690685"
        },
        {
            "date": "2013-07-23T18:44:29+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13716964"
        },
        {
            "date": "2013-07-23T18:50:03+0000",
            "content": "Patch looks good. I prefer the current way of the test (the 'protected' method).\n\nAlso, you have a printout in Lucene40DocValuesWriter after the \"if (b.length > MAX_BINARY)\" - remove/comment?\n\n+1 to commit. ",
            "author": "Shai Erera",
            "id": "comment-13717474"
        },
        {
            "date": "2013-07-29T18:00:30+0000",
            "content": "Cool; I didn't know of the Facet42 codec with its support for large doc values.  Looks like I can use it without faceting.  I'll have to try that.\n\n+1 to commit. ",
            "author": "David Smiley",
            "id": "comment-13722740"
        },
        {
            "date": "2013-08-10T20:13:35+0000",
            "content": "New patch.\n\nThanks for the review Shai; I removed that leftover print and sync'd\npatch to trunk.  I think it's ready ... I'll wait a few days. ",
            "author": "Michael McCandless",
            "id": "comment-13736018"
        },
        {
            "date": "2013-08-12T19:24:47+0000",
            "content": "I'm confused by the following comment:\n\n\n+  /** Maximum length for each binary doc values field,\n+   *  because we use PagedBytes with page size of 16 bits. */\n+  public static final int MAX_BINARY_FIELD_LENGTH = (1 << 16) + 1;\n\n\n\nBut this patch removes the PagedBytes limitation, right?\n\nAfter this patch, are there any remaining code limitations that prevent raising the limit, or is it really just self imposed via\n\n+      if (v.length > Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH) {\n+        throw new IllegalArgumentException(\"DocValuesField \\\"\" + field.name + \"\\\" is too large, must be <= \" + Lucene42DocValuesFormat.MAX_BINARY_FIELD_LENGTH);\n+      }\n\n ",
            "author": "Yonik Seeley",
            "id": "comment-13737260"
        },
        {
            "date": "2013-08-13T00:45:04+0000",
            "content": "Another user has hit this arbitrary limit: http://markmail.org/message/sotbq6xpib4xwozz\nIf it is arbitrary at this point, we should simply remove it. ",
            "author": "Yonik Seeley",
            "id": "comment-13737648"
        },
        {
            "date": "2013-08-13T11:31:20+0000",
            "content": "I'm confused by the following comment:\n\nI fixed the comment; it's because those DVFormats use PagedBytes.fillSlice, which cannot handle more than 2 pages.\n\nNew patch w/ that fix ... ",
            "author": "Michael McCandless",
            "id": "comment-13738101"
        },
        {
            "date": "2013-08-14T15:05:19+0000",
            "content": "Actually, I'm going to roll the limit for 40/42 DVFormats back to what IndexWriter currently enforces ... this way we don't get into a situation where different 40/42 indices out there were built with different limits enforced. ",
            "author": "Michael McCandless",
            "id": "comment-13739750"
        },
        {
            "date": "2013-08-16T12:04:59+0000",
            "content": "Commit 1514669 from Michael McCandless in branch 'dev/trunk'\n[ https://svn.apache.org/r1514669 ]\n\nLUCENE-4583: IndexWriter no longer places a limit on length of DV binary fields (individual codecs still have their limits, including the default codec) ",
            "author": "ASF subversion and git services",
            "id": "comment-13742145"
        },
        {
            "date": "2013-08-16T18:56:24+0000",
            "content": "Commit 1514848 from Michael McCandless in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1514848 ]\n\nLUCENE-4583: IndexWriter no longer places a limit on length of DV binary fields (individual codecs still have their limits, including the default codec) ",
            "author": "ASF subversion and git services",
            "id": "comment-13742521"
        },
        {
            "date": "2013-10-05T10:18:41+0000",
            "content": "4.5 release -> bulk close ",
            "author": "Adrien Grand",
            "id": "comment-13787005"
        }
    ]
}