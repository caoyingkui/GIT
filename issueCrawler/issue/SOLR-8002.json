{
    "id": "SOLR-8002",
    "title": "Add column alias support to the Parallel SQL Interface",
    "details": {
        "components": [
            "search"
        ],
        "type": "New Feature",
        "labels": "",
        "fix_versions": [
            "6.0"
        ],
        "affect_versions": "6.0",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "Currently field aliases are not supported for SQL queries against SQL Handler. E.g. below SQL query\n\n select id,name as product_name from techproducts limit 20\n\ncurrently fails as data returned contains still \"name\" as the field/column key than product_name",
    "attachments": {
        "SOLR-8002.patch": "https://issues.apache.org/jira/secure/attachment/12781898/SOLR-8002.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-09-02T01:14:27+0000",
            "author": "Joel Bernstein",
            "content": "This is actually a new feature request. The SQL interface doesn't yet support field aliases.\n\nAlso, Susheel Kumar patches are welcome!\n\nI'll be happy to review any patches in this area. ",
            "id": "comment-14726539"
        },
        {
            "date": "2015-09-02T13:41:31+0000",
            "author": "Dennis Gove",
            "content": "SOLR-7669 adds a SelectStream to the streaming and expression apis. It includes support for both aliases and operations and might be a good basis for adding these features to the SQL api. ",
            "id": "comment-14727352"
        },
        {
            "date": "2015-09-02T14:35:26+0000",
            "author": "Joel Bernstein",
            "content": "This makes sense. I haven't had a chance to review SOLR-7669 yet. \n\nSusheel Kumar, if you're working on a patch you might want to see if you can work with the SelectStream. You'll need to apply the patch from SOLR-7669 as it's not yet in trunk. ",
            "id": "comment-14727429"
        },
        {
            "date": "2015-09-03T03:18:15+0000",
            "author": "Susheel Kumar",
            "content": "Sure, Joel.  Let me start looking into the patch SOLR-7669.  ",
            "id": "comment-14728447"
        },
        {
            "date": "2015-09-09T05:02:17+0000",
            "author": "Susheel Kumar",
            "content": "Hi Joel, Davis,\n\nJust to clarify that utilizing the SelectStream in SQL api (SQLHandler.java) would require transforming the SQL expression into SOLR streaming expressions for SelectStream to work. So for e.g. SQL expression\n\nselect id, field_i, str_s from collection1 where text='XXXX' order by field_i desc  \n\nwould be transformed to Solr Streaming expression \n\nsearch(collection1, q=\"text:XXXX\", fl=\"id,field_i,str_s\", sort=\"field_i desc\")\n\nPlease let me know your thoughts & if that is the correct understanding.\n\nThanks,\nSusheel\n\n\n\n ",
            "id": "comment-14736174"
        },
        {
            "date": "2015-09-09T14:49:27+0000",
            "author": "Dennis Gove",
            "content": "Before I give my thoughts on this I do want to just put into writing an important note that I think will help frame the conversation.  \n\nNone of the Streaming classes require the use of streaming expressions. Streaming Expressions is just a way to turn a human readable expression into valid streaming constructs. You can, if you want, just create instances of streaming classes without ever thinking about expressions. The SQL api (in SQLHandler.java) currently works this way.\n\nThat said, I do feel that expressions are the easiest and clearest way to interact with Streams. They provide a very concrete yet expressive way to turn a streaming query into objects that do all the actual work. And they are bi-directional in that you can turn a set of stream objects into an expression as easily as you can turn an expression into a set of stream objects. From a user's perspective I find expressions easier to understand than even standard Solr queries, particularly when performing things like aggregations, top, and joins. I can look at an expression and know exactly what I should expect to receive as a result.\n\nI'm not adverse to the your suggestion that we create a pipeline of SQL Statement -> Expression -> Stream object. That said, I don't think it would be good idea (from a performance perspective) to create a streaming expression in string format. Ie, \"SELECT fieldA FROM collection1 WHERE fieldB = 1\" -> search(collection1, fl=\"fieldA\", q=\"fieldB:1\"). I would instead suggest that we turn a SQL statement into a StreamExpression object. This would remove the need to reparse a string just to end up with a StreamExpression object. For example, the above SQL statement could be turned into a StreamExpression with the code\n\n  StreamExpression expression = new StreamExpression(\"search\")\n      .withParameter(new StreamExpressionValue(\"collection1\"))\n      .withParameter(new StreamExpressionNamedParameter(\"fl\",\"fieldA\"))\n      .withParameter(new StreamExpressionNamedParameter(\"q\",\"fieldB:1\"));\n\n\nwhich can then be turned into a Stream with\n\n  TupleStream stream = streamFactory.constructStream(expression);\n\n\n\nThinking about this wrt the string representation of an expression, we really end up with a rather clear pipeline of [SQL Statement | String Expression] -> StreamExpression -> TupleStream. This pipeline makes it very clear that whatever your input format is you only need to convert it into a StreamExpression object and that's it - all the other work of creating a stream from a StreamExpression object is already done. \n\nI think this is the correct approach. ",
            "id": "comment-14736974"
        },
        {
            "date": "2015-09-09T15:06:07+0000",
            "author": "Joel Bernstein",
            "content": "Actually the SQLHandler serializes to a StreamingExpression when the Streaming API objects are sent to worker nodes. As a general rule I think that all built-in Solr features that use the Streaming API should serialize to StreamingExpressions for parallel operations, because it's extremely efficient.\n\n ",
            "id": "comment-14737002"
        },
        {
            "date": "2015-09-09T15:15:32+0000",
            "author": "Joel Bernstein",
            "content": "Susheel Kumar, this ticket is actually pretty tricky to get started with, especially because SOLR-7903 is very close to being committed. Once SOLR-7903 is committed it will also need field alias support.\n\nMy gut feeling is that it would be more effective to work on SOLR-7986. The other nice thing about working on the JDBC driver is that it will provide another set of overlapping tests of the SQLHandler going into the Solr 6 release. ",
            "id": "comment-14737016"
        },
        {
            "date": "2015-09-09T15:15:56+0000",
            "author": "Dennis Gove",
            "content": "I was thinking more in the initial parsing of the SQL statement. At that point perhaps create a StreamExpression object. If I'm reading the code correctly it appears to go directly from a SQL statement to a Stream object. ",
            "id": "comment-14737018"
        },
        {
            "date": "2015-09-09T15:22:35+0000",
            "author": "Joel Bernstein",
            "content": "Ah, just read your post more closely. Building up a Streaming Expression first rather then going directly to the Streaming API object is an interesting approach that I hadn't thought of. I'll take a look at how the SQLHandler is doing things and see what that would look like building up a Streaming Expression directly. ",
            "id": "comment-14737025"
        },
        {
            "date": "2015-09-10T02:11:26+0000",
            "author": "Susheel Kumar",
            "content": "Thanks, Dennis for the explanation & I agree with the approach that either SQL statement or String Expression would be converted to Stream Expression object. ",
            "id": "comment-14738005"
        },
        {
            "date": "2015-09-10T02:35:49+0000",
            "author": "Susheel Kumar",
            "content": "Agree, Joel that this is tricky to start.  I'll anyway continue with more tests and will look into SOLR-7986 as well.   ",
            "id": "comment-14738038"
        },
        {
            "date": "2016-01-12T03:22:19+0000",
            "author": "Joel Bernstein",
            "content": "I've been mulling over the best way to implement alias support. Here is one possibility:\n\n1) During the parsing collect the field > alias map and alias>field map. We'll need both directions for the implementation. Attach these maps to the SQLVisitor.\n2) Wrap the outer most TupleStream in a SelectStream that includes field -> alias translation. This will take care of outputing the column aliases in the Tuple.\n3) Add reverse alias translation to the ORDER BY, GROUP BY, and HAVING clauses.  This will support using column alias's in these clauses.  ",
            "id": "comment-15093218"
        },
        {
            "date": "2016-01-12T19:37:33+0000",
            "author": "Joel Bernstein",
            "content": "Patch with full alias support and tests. Aliases are supported for columns and functions and can be used in GROUP BY, ORDER BY and HAVING clauses.\n\nAliases are not supported in the WHERE clause which is consistent with the SQL spec. ",
            "id": "comment-15094614"
        },
        {
            "date": "2016-01-12T19:55:53+0000",
            "author": "Joel Bernstein",
            "content": "Added a test for aliases when running aggregates without grouping. ",
            "id": "comment-15094650"
        },
        {
            "date": "2016-01-12T20:47:34+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1724319 from Joel Bernstein in branch 'dev/trunk'\n[ https://svn.apache.org/r1724319 ]\n\nSOLR-8002: Add column alias support to the Parallel SQL Interface ",
            "id": "comment-15094914"
        },
        {
            "date": "2016-01-12T21:12:05+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1724323 from Joel Bernstein in branch 'dev/trunk'\n[ https://svn.apache.org/r1724323 ]\n\nSOLR-8002: Added more test cases ",
            "id": "comment-15094951"
        },
        {
            "date": "2016-01-13T02:59:16+0000",
            "author": "Kevin Risden",
            "content": "Joel Bernstein Any reason to not always return a SelectStream instead of checking if there are columnAliases?\n\nI'm thinking about the JDBC implementation of get*() by index. The SelectStream can guarantee the tuples have the fields in the same order as the SQL statement. For example: select a, b, c from table; will result in tuple.get(0) being a, tuple.get(1) = b, etc. Without the SelectStream wrapping, the fields are in an arbitrary order.\n\nIn addition to wrapping the result in SelectStream, this also requires that the columnAliases, reverseColumnAliases, and tuple.fields are LinkedHashMap instead of just plain HashMap to keep the keys in insertion order. I have a rough patch that has these changes just wasn't sure if it made sense to add to this or to a separate JIRA. ",
            "id": "comment-15095511"
        },
        {
            "date": "2016-01-13T03:08:00+0000",
            "author": "Kevin Risden",
            "content": "added my thoughts and rough patch to SOLR-8512 ",
            "id": "comment-15095519"
        },
        {
            "date": "2016-01-13T03:21:00+0000",
            "author": "Dennis Gove",
            "content": "I think this is one of those situations where what one almost always sees in a JDBC implementation can actually be reversed. Normally in a JDBC implementation when someone asks for a value via columnName that name will be translated to a columnIdx and that index will be used to lookup the actual value. But because values are keyed by columnName in the tuple I think it could just be reversed where when asking for a value by columnIndex the index could be translated to the columnName and that name used to lookup the actual value. \n\nThis removes the need to depend on order of the fields in the tuple. ",
            "id": "comment-15095528"
        },
        {
            "date": "2016-01-13T03:21:20+0000",
            "author": "Joel Bernstein",
            "content": "Yep, this makes perfect sense. ",
            "id": "comment-15095529"
        },
        {
            "date": "2016-01-13T03:25:56+0000",
            "author": "Joel Bernstein",
            "content": "Just mulling over the linkedHashMap, not sure how well that will serialize in JSON. ",
            "id": "comment-15095531"
        },
        {
            "date": "2016-01-13T03:32:27+0000",
            "author": "Kevin Risden",
            "content": "\nBut because values are keyed by columnName in the tuple I think it could just be reversed where when asking for a value by columnIndex the index could be translated to the columnName and that name used to lookup the actual value.\n\nI agree that looking up by index could be translated into column name, but that requires that the mapping of name to index or vice versa to be available at the driver. Currently, there is no metadata that keeps track of positional index to column name that gets passed back from the SQLHandler. I had a working solution that added some metadata to the stream (and had to change the JSONTupleStream parsing) and that seemed awkward/wrong. Maybe there is a better way to pass the metadata back from the SQL handler?\n\nThe least intrusive to me seems to me to be that the Tuple object be able to keep its fields in order and not only by name. I don't know how to enforce this though other than maybe using the LinkedHashMap and some tests to ensure that a new stream doesn't cause issues. ",
            "id": "comment-15095533"
        },
        {
            "date": "2016-01-13T03:34:35+0000",
            "author": "Kevin Risden",
            "content": "\nJust mulling over the linkedHashMap, not sure how well that will serialize in JSON.\n\nIt seems to serialize just fine from the tests I was running with the patch on SOLR-8512. It serializes just like a regular Map and when it gets deserialized by the JSONTupleStream it comes out as a LinkedHashMap and gets cast as a regular Map. This would be the expected behavior from what I can tell. ",
            "id": "comment-15095536"
        }
    ]
}