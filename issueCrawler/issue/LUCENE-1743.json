{
    "id": "LUCENE-1743",
    "title": "MMapDirectory should only mmap large files, small files should be opened using SimpleFS/NIOFS",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/store"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "affect_versions": "2.9",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "This is a followup to LUCENE-1741:\nJavadocs state (in FileChannel#map): \"For most operating systems, mapping a file into memory is more expensive than reading or writing a few tens of kilobytes of data via the usual read and write methods. From the standpoint of performance it is generally only worth mapping relatively large files into memory.\"\nMMapDirectory should get a user-configureable size parameter that is a lower limit for mmapping files. All files with a size<limit should be opened using a conventional IndexInput from SimpleFS or NIO (another configuration option for the fallback?).",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2009-07-14T19:55:10+0000",
            "content": "indeed! obvious idea, \n\nthe only thing I do not like with it is making these hidden, deceptive decisions \"I said I want MMapDirectory and someone else decided something else for me\"... it does not matter if we have conses here now, it may change tomorrow \n\nprobably better way would be to turbo charge FileSwitchDirectory with sexy parametrization options, \nMMapDirectory <- F(fileExtension, minSize, maxSize) // If <fileExtension> and file size less than <maxSize> and greater than <minSize> than open file with MMapDirectory... than go on on next rule... (can be designed upside down as well... changes nothing in idea)\n\nthe same for RAMDir, NIO, FS... \n\nWith this, we can make UwesBestOfMMapDirectoryFor32BitOSs (your proposal here) or \nHighlyConcurentForWindows64WithTermDictionaryInRamAndStoredFieldsOnDiskDirectory just for me  \n\nSo the most of the end users take some smart defaults we provide in core, and freaks (Expert users in official lingo  have their job easy, just to configure TurboChargedFileSwitchDirectory\n\nShould be easy to come up with clean design for these \"Concrete Directory selection rules\" by keeping concrete Directories \"pure\"\n\nCheers, Eks \n\n ",
            "author": "Eks Dev",
            "id": "comment-12731085"
        },
        {
            "date": "2009-07-14T20:04:01+0000",
            "content": "At a minimum we should make FileSwitchDirectory friendly for subclassing, eg so you can override the currently private getDirectory method to implement your own custom logic.  Hmm... we should somehow pass the \"context\" (at least 'read' vs 'write') to getDirectory() as well... ",
            "author": "Michael McCandless",
            "id": "comment-12731091"
        },
        {
            "date": "2009-07-14T20:21:06+0000",
            "content": "HighlyConcurentForWindows64WithTermDictionaryInRamAndStoredFieldsOnDiskDirectory just for me  \n\nBy the way, for MMapDirectory the MappedByteBuffer.load() method should be somehow accesible/configureable to create this TermDictionaryInRam part (IndexInput would call load() and tells the OS to swap as much as possible of the mmaped file into RAM). Just an idea.\n\nThe Hyper FileSwitchDirectory was my idea yesterday, too. As Mike said, at least the getDirectory() should be configureable.\n\nAnd for some good defaults, a factory could be provided like getUwesBestOfMMapDirectoryFor32BitOSs(File, LockFactory). What I do not like with the current FileSwitchDir is the fact, that you must create instances with the same Dir and LockFactory for each sub-directory (e.g. what happens if you use 2 different LockFactories on the same physical dir inside a FileSwitchDir?). Maybe the FileSwitchDirectory could just get the File and LockFactory once and creates the instances? Many ideas... ",
            "author": "Uwe Schindler",
            "id": "comment-12731101"
        },
        {
            "date": "2009-07-14T20:25:14+0000",
            "content": "right, it is not everything about reading index, you have to write it as well...\n\nwhy not making  it an abstract class with \nabstract Directory getDirectory(String file, int minSize, int maxSize, String [read/write/append], String context);\nString getName(); // for logging\n\nWhat do you understand under \"context\"? Something along the lines /Give me directory for \"segment merges\", \"read only\" for search./ \n...Maybe one day we will have possibility not to kill OS cache by merging,\n ",
            "author": "Eks Dev",
            "id": "comment-12731104"
        },
        {
            "date": "2009-07-14T20:37:14+0000",
            "content": "Another idea to think about:\nMaybe we make the base FSDirectory configureable, that you can define rules for the IndexInput or IndexOutput instances used. Why create three directories, when you could only use on that just decides, what is the right IndexInput/IndexOutput? In this case you need no MMapDirectory, no NIOFSDir, no SimpleFSDir. The basic list files, rename and so on are always identical (for FSDirs). The only difference is the IndexInput and IndexOutput.\n\nIn this case you would only have one LockFactory, which would be good in my opinion (see above).\n\nSo we only provide the Input/Output classes as separate top-level classes: SimpleFSIndexInput, MMapIndexInput (internal two classes for chunking or not), NIOIndexInput and for output at the beginning only the current SimpleFSIndexOutput. You can then configure your FSDirectory to use different impls depending on an method or rule defined like above. For backwards compatibility, the current MMapDirectory and so on are simple subclasses of this universal FSDir with a fixed configuration.\n\nFileSwitchDirectory should only used, when you really want to separate two directories in complete (stored fields on disk, index on SSD in two physical different dirs). ",
            "author": "Uwe Schindler",
            "id": "comment-12731118"
        },
        {
            "date": "2009-07-15T19:07:41+0000",
            "content": "The initial motive for the issue seems wrong to me.\n\n\"For most operating systems, mapping a file into memory is more expensive than reading or writing a few tens of kilobytes of data via the usual read and write methods. From the standpoint of performance it is generally only worth mapping relatively large files into memory.\"\nIt is probably right if you're doing a single read through the file. If you're opening/mapping it and do thousands of repeated reads, mmap would be superior, because after initial mapping it's just a memory access VS system call for file.read().\n\nAdd: In case you're not doing repeated reads, and just read these small files once from time to time, you can totally neglect speed difference between mmap and fopen. At least it doesn't warrant increased complexity. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12731632"
        },
        {
            "date": "2009-07-15T19:17:25+0000",
            "content": "A typical example, where MMap would be the wrong thing are e.g. norms. They are read one time and then the file is not accessed anymore. It would only be cool, if MMapDir could directly return the mapped array (but MappedByteBuffer.array() does not work) and use it as norms array. That would be cool.\n\nMy problem was more with all these small files like segments_XXXX and segments.gen or *.del files. They are small and only used one time. Mapping them is just waste of resources and completely useless (even slower that opening them directly). This is why I said, some limit or file extension based mapping would be good. ",
            "author": "Uwe Schindler",
            "id": "comment-12731638"
        },
        {
            "date": "2009-07-15T19:26:22+0000",
            "content": "My problem was more with all these small files like segments_XXXX and segments.gen or *.del files. They are small and only used one time.\nI can only reiterate my point. These files aren't opened like 10k files per second, so your win is going to be in the order of microseconds per reopen - at the cost of increased complexity. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12731639"
        },
        {
            "date": "2013-07-23T18:44:37+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13717003"
        },
        {
            "date": "2014-04-16T12:54:48+0000",
            "content": "Move issue to Lucene 4.9. ",
            "author": "Uwe Schindler",
            "id": "comment-13970885"
        }
    ]
}