{
    "id": "SOLR-7836",
    "title": "Possible deadlock when closing refcounted index writers.",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.4",
            "6.0"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "Preliminary patch for what looks like a possible race condition between writerFree and pauseWriter in DefaultSorlCoreState.\n\nLooking for comments and/or why I'm completely missing the boat.",
    "attachments": {
        "SOLR-7836-reorg.patch": "https://issues.apache.org/jira/secure/attachment/12751314/SOLR-7836-reorg.patch",
        "SOLR-7836-synch.patch": "https://issues.apache.org/jira/secure/attachment/12749489/SOLR-7836-synch.patch",
        "SOLR-7836.patch": "https://issues.apache.org/jira/secure/attachment/12747427/SOLR-7836.patch",
        "deadlock_3.res.zip": "https://issues.apache.org/jira/secure/attachment/12749915/deadlock_3.res.zip",
        "deadlock_test": "https://issues.apache.org/jira/secure/attachment/12749916/deadlock_test",
        "deadlock_5_pass_iw.res.zip": "https://issues.apache.org/jira/secure/attachment/12749942/deadlock_5_pass_iw.res.zip"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-07-27T21:28:55+0000",
            "author": "Erick Erickson",
            "content": "Looking for comments:\n\nThis looks like I changed more than I did. Expanded the scope of the try/finally block in newIndexWriter, moved the common code for initializing outside an else clause and put a try/finally block in closeIndexWriter. The rest of the diff is just noise due to indentation.\n\nI have a field report and stack traces of a deadlock, here's the stack trace:\n\nI think that the first two in DefaultSolrCoreState are where I'm guessing the root of the problem lies.\n\n********************First thread in DefaultSolrCoreState \n\n\tThe thread that owns the updateLock is stuck waiting for pauseWriter to go to false so it can get the IndexWriter:\njava.lang.Object@c0d4b9 \njava.lang.Object.wait\u200b(Native Method) \norg.apache.solr.update.DefaultSolrCoreState.getIndexWriter\u200b(DefaultSolrCoreState.java:94) \norg.apache.solr.update.DirectUpdateHandler2.addAndDelete\u200b(DirectUpdateHandler2.java:436) \norg.apache.solr.update.DirectUpdateHandler2.addDoc0\u200b(DirectUpdateHandler2.java:216) \norg.apache.solr.update.DirectUpdateHandler2.addDoc\u200b(DirectUpdateHandler2.java:160) \norg.apache.solr.update.processor.RunUpdateProcessor.processAdd\u200b(RunUpdateProcessorFactory.java:69) \norg.apache.solr.update.processor.UpdateRequestProcessor.processAdd\u200b(UpdateRequestProcessor.java:51) \norg.apache.solr.update.processor.DistributedUpdateProcessor.doLocalAdd\u200b(DistributedUpdateProcessor.java:928) \norg.apache.solr.update.processor.DistributedUpdateProcessor.versionAdd\u200b(DistributedUpdateProcessor.java:1082) \norg.apache.solr.update.processor.DistributedUpdateProcessor.processAdd\u200b(DistributedUpdateProcessor.java:695) \norg.apache.solr.handler.loader.JavabinLoader$1.update\u200b(JavabinLoader.java:96) \norg.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readOuterMostDocIterator\u200b(JavaBinUpdateRequestCodec.java:166) \norg.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readIterator\u200b(JavaBinUpdateRequestCodec.java:136) \norg.apache.solr.common.util.JavaBinCodec.readVal\u200b(JavaBinCodec.java:225) \norg.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readNamedList\u200b(JavaBinUpdateRequestCodec.java:121) \norg.apache.solr.common.util.JavaBinCodec.readVal\u200b(JavaBinCodec.java:190) \norg.apache.solr.common.util.JavaBinCodec.unmarshal\u200b(JavaBinCodec.java:116) \norg.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec.unmarshal\u200b(JavaBinUpdateRequestCodec.java:173) \norg.apache.solr.handler.loader.JavabinLoader.parseAndLoadDocs\u200b(JavabinLoader.java:106) \norg.apache.solr.handler.loader.JavabinLoader.load\u200b(JavabinLoader.java:58) \norg.apache.solr.handler.UpdateRequestHandler$1.load\u200b(UpdateRequestHandler.java:92) \norg.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody\u200b(ContentStreamHandlerBase.java:74) \norg.apache.solr.handler.RequestHandlerBase.handleRequest\u200b(RequestHandlerBase.java:135) \norg.apache.solr.core.SolrCore.execute\u200b(SolrCore.java:1956) \norg.apache.solr.servlet.SolrDispatchFilter.execute\u200b(SolrDispatchFilter.java:799) \norg.apache.solr.servlet.SolrDispatchFilter.doFilter\u200b(SolrDispatchFilter.java:422) \norg.apache.solr.servlet.SolrDispatchFilter.doFilter\u200b(SolrDispatchFilter.java:208) \norg.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter\u200b(ServletHandler.java:1419) \ncom.apple.cie.search.plugin.auth.TrustFilter.doFilter\u200b(TrustFilter.java:43) \norg.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter\u200b(ServletHandler.java:1419) \norg.eclipse.jetty.servlet.ServletHandler.doHandle\u200b(ServletHandler.java:455) \norg.eclipse.jetty.server.handler.ScopedHandler.handle\u200b(ScopedHandler.java:137) \norg.eclipse.jetty.security.SecurityHandler.handle\u200b(SecurityHandler.java:557) \norg.eclipse.jetty.server.session.SessionHandler.doHandle\u200b(SessionHandler.java:231) \norg.eclipse.jetty.server.handler.ContextHandler.doHandle\u200b(ContextHandler.java:1075) \norg.eclipse.jetty.servlet.ServletHandler.doScope\u200b(ServletHandler.java:384) \norg.eclipse.jetty.server.session.SessionHandler.doScope\u200b(SessionHandler.java:193) \norg.eclipse.jetty.server.handler.ContextHandler.doScope\u200b(ContextHandler.java:1009) \norg.eclipse.jetty.server.handler.ScopedHandler.handle\u200b(ScopedHandler.java:135) \norg.eclipse.jetty.server.handler.ContextHandlerCollection.handle\u200b(ContextHandlerCollection.java:255) \norg.eclipse.jetty.server.handler.HandlerCollection.handle\u200b(HandlerCollection.java:154) \norg.eclipse.jetty.server.handler.HandlerWrapper.handle\u200b(HandlerWrapper.java:116) \norg.eclipse.jetty.server.Server.handle\u200b(Server.java:368) \norg.eclipse.jetty.server.AbstractHttpConnection.handleRequest\u200b(AbstractHttpConnection.java:489) \norg.eclipse.jetty.server.BlockingHttpConnection.handleRequest\u200b(BlockingHttpConnection.java:53) \norg.eclipse.jetty.server.AbstractHttpConnection.content\u200b(AbstractHttpConnection.java:953) \norg.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content\u200b(AbstractHttpConnection.java:1014) \norg.eclipse.jetty.http.HttpParser.parseNext\u200b(HttpParser.java:953) \norg.eclipse.jetty.http.HttpParser.parseAvailable\u200b(HttpParser.java:240) \norg.eclipse.jetty.server.BlockingHttpConnection.handle\u200b(BlockingHttpConnection.java:72) \norg.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run\u200b(SocketConnector.java:264) \norg.eclipse.jetty.util.thread.QueuedThreadPool.runJob\u200b(QueuedThreadPool.java:608) \norg.eclipse.jetty.util.thread.QueuedThreadPool$3.run\u200b(QueuedThreadPool.java:543) \njava.lang.Thread.run\u200b(Thread.java:745)\n\n\n\n\n********************Second thread in DefaultSolrCoreState \n\n\tThe thread that sets pauseWriter to true that is supposed to later set it to false is stuck on waiting for writerFree to go to true:\njava.lang.Object@c0d4b9 \njava.lang.Object.wait\u200b(Native Method) \norg.apache.solr.update.DefaultSolrCoreState.newIndexWriter\u200b(DefaultSolrCoreState.java:156) \norg.apache.solr.core.SolrCore.reload\u200b(SolrCore.java:431) \norg.apache.solr.core.CoreContainer.reload\u200b(CoreContainer.java:586) \norg.apache.solr.handler.admin.CoreAdminHandler.handleReloadAction\u200b(CoreAdminHandler.java:701) \norg.apache.solr.handler.admin.CoreAdminHandler.handleRequestInternal\u200b(CoreAdminHandler.java:225) \norg.apache.solr.handler.admin.CoreAdminHandler.handleRequestBody\u200b(CoreAdminHandler.java:188) \norg.apache.solr.handler.RequestHandlerBase.handleRequest\u200b(RequestHandlerBase.java:135) \norg.apache.solr.servlet.SolrDispatchFilter.handleAdminRequest\u200b(SolrDispatchFilter.java:751) \norg.apache.solr.servlet.SolrDispatchFilter.doFilter\u200b(SolrDispatchFilter.java:259) \norg.apache.solr.servlet.SolrDispatchFilter.doFilter\u200b(SolrDispatchFilter.java:208) \norg.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter\u200b(ServletHandler.java:1419) \ncom.apple.cie.search.plugin.auth.TrustFilter.doFilter\u200b(TrustFilter.java:43) \norg.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter\u200b(ServletHandler.java:1419) \norg.eclipse.jetty.servlet.ServletHandler.doHandle\u200b(ServletHandler.java:455) \norg.eclipse.jetty.server.handler.ScopedHandler.handle\u200b(ScopedHandler.java:137) \norg.eclipse.jetty.security.SecurityHandler.handle\u200b(SecurityHandler.java:557) \norg.eclipse.jetty.server.session.SessionHandler.doHandle\u200b(SessionHandler.java:231) \norg.eclipse.jetty.server.handler.ContextHandler.doHandle\u200b(ContextHandler.java:1075) \norg.eclipse.jetty.servlet.ServletHandler.doScope\u200b(ServletHandler.java:384) \norg.eclipse.jetty.server.session.SessionHandler.doScope\u200b(SessionHandler.java:193) \norg.eclipse.jetty.server.handler.ContextHandler.doScope\u200b(ContextHandler.java:1009) \norg.eclipse.jetty.server.handler.ScopedHandler.handle\u200b(ScopedHandler.java:135) \norg.eclipse.jetty.server.handler.ContextHandlerCollection.handle\u200b(ContextHandlerCollection.java:255) \norg.eclipse.jetty.server.handler.HandlerCollection.handle\u200b(HandlerCollection.java:154) \norg.eclipse.jetty.server.handler.HandlerWrapper.handle\u200b(HandlerWrapper.java:116) \norg.eclipse.jetty.server.Server.handle\u200b(Server.java:368) \norg.eclipse.jetty.server.AbstractHttpConnection.handleRequest\u200b(AbstractHttpConnection.java:489) \norg.eclipse.jetty.server.BlockingHttpConnection.handleRequest\u200b(BlockingHttpConnection.java:53) \norg.eclipse.jetty.server.AbstractHttpConnection.content\u200b(AbstractHttpConnection.java:953) \norg.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content\u200b(AbstractHttpConnection.java:1014) \norg.eclipse.jetty.http.HttpParser.parseNext\u200b(HttpParser.java:861) \norg.eclipse.jetty.http.HttpParser.parseAvailable\u200b(HttpParser.java:240) \norg.eclipse.jetty.server.BlockingHttpConnection.handle\u200b(BlockingHttpConnection.java:72) \norg.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run\u200b(SocketConnector.java:264) \norg.eclipse.jetty.util.thread.QueuedThreadPool.runJob\u200b(QueuedThreadPool.java:608) \norg.eclipse.jetty.util.thread.QueuedThreadPool$3.run\u200b(QueuedThreadPool.java:543) \njava.lang.Thread.run\u200b(Thread.java:745)\n\n\n\n****************Lots of threads stuck in DirectUpdateHandler2, but I suspect these aren't where the problem really is:\njava.lang.Object@73cdbe11 \norg.apache.solr.update.DirectUpdateHandler2.addAndDelete\u200b(DirectUpdateHandler2.java:435) \norg.apache.solr.update.DirectUpdateHandler2.addDoc0\u200b(DirectUpdateHandler2.java:216) \norg.apache.solr.update.DirectUpdateHandler2.addDoc\u200b(DirectUpdateHandler2.java:160) \norg.apache.solr.update.processor.RunUpdateProcessor.processAdd\u200b(RunUpdateProcessorFactory.java:69) \norg.apache.solr.update.processor.UpdateRequestProcessor.processAdd\u200b(UpdateRequestProcessor.java:51) \norg.apache.solr.update.processor.DistributedUpdateProcessor.doLocalAdd\u200b(DistributedUpdateProcessor.java:928) \norg.apache.solr.update.processor.DistributedUpdateProcessor.versionAdd\u200b(DistributedUpdateProcessor.java:1082) \norg.apache.solr.update.processor.DistributedUpdateProcessor.processAdd\u200b(DistributedUpdateProcessor.java:695)  ",
            "id": "comment-14643433"
        },
        {
            "date": "2015-07-28T12:39:50+0000",
            "author": "Erick Erickson",
            "content": "OK, the patch doesn't address the real issue. I still think it's bad to leave those dangling pauseWriters around, but the real issue appears to be over in DirectUpdateHandler2. There's a ref counted index writer obtained in addDoc0. But then addDoc0 calls addAndDelete which tries to get a ref counted index writer again. If another thread sets pauseWriter in between, then it's deadlocked.\n\nI think the solution is to just pass the IndexWriter down to addAndDelete, but won't have time to really look until this evening. ",
            "id": "comment-14644312"
        },
        {
            "date": "2015-07-28T14:21:13+0000",
            "author": "Mark Miller",
            "content": "That sounds like it's right - been awhile since I've looked at the code, but the idea is, you get a writer, use it briefly, then release it in a finally. There should not be code that gets a writer, then gets a writer, then tries to release both of them after. ",
            "id": "comment-14644420"
        },
        {
            "date": "2015-08-07T05:45:15+0000",
            "author": "Erick Erickson",
            "content": "Well, I've kicked the can down the road, but now there's a different deadlock. I wrote a test to beat the heck out of the update/delete-by-query/reload stuff and after the changes above....\n\nIn the morning when I'm fresher I'll look at why there are so many different things that need to have locks on them. But we have updateLock, and loops like below that seem to be locking each other out.\n\nYonik Seeley Mark Miller (and anyone else) Loops like this seem fraught, or is it just me and it's late?:\n        while (!writerFree) {\n          try \n{\n            writerPauseLock.wait(100);\n          }\n catch (InterruptedException e) {}\n          if (closed) \n{\n            throw new SolrException(ErrorCode.SERVICE_UNAVAILABLE, \"SolrCoreState already closed\");\n          }\n        }\n\nand wait until some other thread sets writerFree to true.  ",
            "id": "comment-14661370"
        },
        {
            "date": "2015-08-07T12:17:05+0000",
            "author": "Mark Miller",
            "content": "What's wrong with the loop?\n\nThe code in place is pretty tricky because it had to retrofitted onto a model that did not fit the right model to make it fit the right model \n\nI'm not exactly sure what you are point out in the loop though.\n\nDo you have stack traces for the latest deadlock? Can you share the new test? ",
            "id": "comment-14661739"
        },
        {
            "date": "2015-08-07T14:26:02+0000",
            "author": "Erick Erickson",
            "content": "Well, something that sits in a while loop waiting for another thread to change a variable before doing anything seems like it's harder to get right than some kind of notification. As I'm finding. But I'm terrified of changing the total approach. \n\nI need to dig some more before posting stack traces, plus it's the trace with this patch applied (and maybe changed)....\n\nMostly bringing it up to see if someone looks at the loops and says \"Yuck, how did we do THAT?\" before thinking about replacing the mechanism. You've shot that hope in the head though . And I'm assuming that part of the complexity here is that we want the index writer to be held by a thread as briefly as possible so as to not introduce bottlenecks.\n\nAnyway, I'll dig some more after I take care of morning tasks.\n\nThanks!\n ",
            "id": "comment-14661898"
        },
        {
            "date": "2015-08-07T15:00:49+0000",
            "author": "Mark Miller",
            "content": "Previously it was done via notification - the time out and check of isClosed was added at some point because this could hang on shutdown (or at least it really slowed down shutdown - can't remember which). ",
            "id": "comment-14661956"
        },
        {
            "date": "2015-08-08T05:30:11+0000",
            "author": "Erick Erickson",
            "content": "This might do it. I'm running 1,000 iterations (or until morning, whichever comes first) but it's gone through 150 or so already which was usually more than enough to trigger the new deadlock I found so I'm hopeful. Actually, I think this really the old deadlock, the first fix wasn't very good.\n\nThere are two things I'd appreciate any opinions on:\n1> this patch moves the UpdateLog.add() command out of the ref counted IndexWriter in DirectUpdateHandler2.addDoc0() in multiple places (refactored to methods). But UpdateLog.add gets a new IndexWriter itself which is where the deadlock appears to be as it's fighting with CoreContainer.reload() which also calls DefaultSolrCoreState.newIndexWriter.\n\n2> There are two nocommits, but they are to make it easy for someone to see the other bit that other eyes would be most welcome on, I moved \nif (ulog != null) ulog.add(cmd, true); \nout of a synchronized block. This fits the pattern in other places in that code too, so I'm not too worried, but wanted to draw attention to it if anyone wants to look. Actually, there are a lot of operations on ulog.something that are synchronized on solrCoreState.getUpdateLock(), and a bunch that aren't. What's up there? The change marked by //nocommit matches the (non-synchronized) usages in addDoc0() though.\n\nAnyway, the problem was in DirectUpdateHandler2.addDoc0(). The ulog.add command line being inside a ref-counted IndexWriter when adding documents (addAndDelete case). ulog.add eventually tries to get a new Searcher which can be deadlocked with another thread called from SolrCore.reload since the reload wants a new IndexWriter.\n\nI haven't run precommit or the full test suite yet, I want to make sure the iterations I'm doing tonight work.\n\nThis looks more intrusive than it is. I refactored some methods out of DirectUpdateHandler2.addDoc0() in order to make this pattern more visible. The original code concealed the fact that the ref counted index writer surrounded all the code, including the ulog.add.\n\nthis is the pattern for all the refactored methods:\n\nRefCounted<IndexWriter> iw = solrCoreState.getIndexWriter(core);\u2028try {\u2028 \n   IndexWriter writer = iw.get();\u2028 \n    do the right thing\n} finally {\n    iw.decref();\n} \n\nif (ulog != null) ulog.add(cmd, true);\n\n\nI think this is a better approach than the original fix which passed the index writer down to the addAndDelete method. we'd have had to pass the IndexWriter on to the ulog.add command or something which would have been a pain (even if it was possible).\n\nNew test case is in this patch. Note that it's a race condition to get this to fail, so it needs to be run a lot. And using tests.iters apparently triggers the timeouts so.... ",
            "id": "comment-14662814"
        },
        {
            "date": "2015-08-08T15:26:45+0000",
            "author": "Mark Miller",
            "content": "Can we get the stress test in as a nightly? ",
            "id": "comment-14663030"
        },
        {
            "date": "2015-08-08T15:41:51+0000",
            "author": "Erick Erickson",
            "content": "Looking a bit more, all the ulog.(some operation) commands have this at the top: \n\nsynchronized (this) {\n\n \nso they're serialized. I don't like the fact that lots of these are called from within\n\nsynchronized (solrCoreState.getUpdateLock()) {\n\n\nas that seems like a good place for a deadlock if it ever happens that the ulog operations want to get that same lock, but I'm not about to change that now by, say, moving all the ulog operations out of the synchronized blocks since I don't know why they were put in the first place, we can make that a separate JIRA if necessary/desirable. I'm comfortable enough with the one I moved out to go ahead with this commit.\n\nSo I'll clean up the patch and commit unless there are objections today/tomorrow. ",
            "id": "comment-14663038"
        },
        {
            "date": "2015-08-08T16:39:33+0000",
            "author": "Erick Erickson",
            "content": "Final patch I think. It:\n1> tightens up a bit acquiring the index writer in the extracted methods. This is really a minor nit.\n\n2> makes the test nightly.\n\nI'll commit this tonight/tomorrow, need to run it through precommit and the rest of the tests. ",
            "id": "comment-14663062"
        },
        {
            "date": "2015-08-09T03:38:09+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1694854 from Erick Erickson in branch 'dev/trunk'\n[ https://svn.apache.org/r1694854 ]\n\nSOLR-7836: Possible deadlock when closing refcounted index writers ",
            "id": "comment-14663242"
        },
        {
            "date": "2015-08-09T05:46:44+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1694855 from Erick Erickson in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1694855 ]\n\nSOLR-7836: Possible deadlock when closing refcounted index writers ",
            "id": "comment-14663271"
        },
        {
            "date": "2015-08-09T05:47:17+0000",
            "author": "Erick Erickson",
            "content": "Thanks Jessica! ",
            "id": "comment-14663272"
        },
        {
            "date": "2015-08-09T21:34:51+0000",
            "author": "Erick Erickson",
            "content": "I modified the three refactored methods in DirectUpdateHandler2 to be:\n\n\n   //old code\n    if (ulog != null) ulog.add(cmd);\n \n\n   //new code\n   synchronized (solrCoreState.getUpdateLock()) {\n      if (ulog != null) ulog.add(cmd);\n    }\n\n\n\nand 200 iterations later no failures from TestStressReorder (I'll try the new test code momentarily, but it'll take longer).\n\nYonik Seeley Mark Miller\nHere's what I don't like about this. None of these three operations (the three new methods: addAndDelete, doNormalUpdate and allowDuplicateUpdate) couples actually changing the index with the write to the ulog. Now, only the code in addAndDelete did before, except that we may have gotten away with this because the indexWriter was grabbed at the very top of addDoc0() and effectively locked other operations out. Maybe.\n\nBut if the ulog.add goes within the synch on the updatelock in addAndDelete, there's certainly a deadlock so putting it back the way it was isn't really an option.\n\nI'm starting to wonder if this isn't a bit backwards. Rather than going at this piecemeal, what about synchronizing on updateLock at the top of addDoc0 and in CoreContainer.reload() which drives one of the failure cases for deadlock? Not sure I really like the idea, but I'll give it a (local) test....\n\nMeanwhile, I'll check the current fix in since it's certainly better pending more experimentation. ",
            "id": "comment-14679360"
        },
        {
            "date": "2015-08-09T21:46:43+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1694913 from Erick Erickson in branch 'dev/trunk'\n[ https://svn.apache.org/r1694913 ]\n\nSOLR-7836: Possible deadlock when closing refcounted index writers. Surrounded ulog updates with updatelock ",
            "id": "comment-14679364"
        },
        {
            "date": "2015-08-09T21:47:29+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1694914 from Erick Erickson in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1694914 ]\n\nSOLR-7836: Possible deadlock when closing refcounted index writers. Surrounded ulog updates with updatelock ",
            "id": "comment-14679365"
        },
        {
            "date": "2015-08-09T23:20:00+0000",
            "author": "Erick Erickson",
            "content": "bq: I'm starting to wonder if this isn't a bit backwards. Rather than going at this piecemeal, what about synchronizing on updateLock at the top of addDoc0 and in CoreContainer.reload() which drives one of the failure cases for deadlock? Not sure I really like the idea, but I'll give it a (local) test....\n\nThat's a really bad idea, fails first time every time. ",
            "id": "comment-14679398"
        },
        {
            "date": "2015-08-10T00:19:35+0000",
            "author": "Mark Miller",
            "content": "What about the nightly test?\n\n\n   //new code\n   synchronized (solrCoreState.getUpdateLock()) {\n      if (ulog != null) ulog.add(cmd);\n    }\n\n\n\nWhile that says getUpdateLock, it appears to have been additionally proposed as a deleteLock judging by it's callers and variable name. It looks like the old index update lock was tied into also being a delete lock.\n\nWhat is the logic that says you now need that also locking adds other than this test? Why would the index update lock that did not block adds and also was used as a delete lock also lock adds now?\n\nYonik Seeley should really look at this as this looks like his code area.\n\nI'm skeptical that this is the right fix. ",
            "id": "comment-14679424"
        },
        {
            "date": "2015-08-10T01:32:27+0000",
            "author": "Erick Erickson",
            "content": "bq: What about the nightly test?\n\nI'm beating that up as well, 50+ runs and iterating without a problem so far.\n\nbq: While that says getUpdateLock, it appears to have been additionally proposed as a deleteLock judging by it's callers and variable name. It looks like the old index update lock was tied into also being a delete lock\n\nHmmm. which may explain why changing in in addAndDelete was a problem and apparently not in the other cases.\n\nbq: What is the logic that says you now need that also locking adds other than this test?\n\nMay well be flawed logic, but I find it really suspicious that the the ulog.add() call is sometimes protected and sometimes not. Although your point that it appears to really be about deletes (which the others don't do) may be why. Moving getting the indexwriter away from being done at the beginning of addDoc0() did remove a layer of protection around the ulog.add calls in all the code paths in addDoc0(). The ref counted index writer (unintentionally?) locked out some other updates possibly. ulog.add can get a new ref counted index writer which lead to the deadlock in the addAndDelete case. ",
            "id": "comment-14679455"
        },
        {
            "date": "2015-08-10T11:14:49+0000",
            "author": "Mark Miller",
            "content": "I find it really suspicious that the the ulog.add() call is sometimes protected and sometimes not. \n\nThat's pretty unsound logic for making a change in such a complicated area of code... ",
            "id": "comment-14679959"
        },
        {
            "date": "2015-08-10T11:17:13+0000",
            "author": "Mark Miller",
            "content": "I'm pretty sure that deadlocks around accessing an index writer should not involved synchronization work with the tlog. It may have inadvertently helped, but the two things are pretty unrelated. ",
            "id": "comment-14679961"
        },
        {
            "date": "2015-08-10T11:42:22+0000",
            "author": "Mark Miller",
            "content": "Once you get the stress test up, I'm happy to do some parallel investigation into the deadlock. ",
            "id": "comment-14679976"
        },
        {
            "date": "2015-08-10T12:21:43+0000",
            "author": "Mark Miller",
            "content": "Once you get the stress test up\n\nNevermind, I see it's in an earlier test. I'll take a look. ",
            "id": "comment-14680024"
        },
        {
            "date": "2015-08-10T14:39:33+0000",
            "author": "Erick Erickson",
            "content": "bq: I'm pretty sure that deadlocks around accessing an index writer should not involved synchronization work with the tlog. It may have inadvertently helped, but the two things are pretty unrelated.\n\nI don't disagree, but the update log and index writer are intertwined, that's the problem. I'm perfectly willing to agree that they should be separated out completely, but haven't had any confirmation that they can be, or were ever intended to be separated.\n\nulog.add() calls openNewSearcher which gets an indexWriter which is where things to south. Of course it calls getIndexWriter with null which has the note \"core == null is a signal to just return the current writer, or null\"; It doesn't really increment the reference count but does go through the interlock with pauseWriter and the like. Of course then openNewSearcher does a decref on the writer, which was never incremented in the first place and only works because the decref for index writer doesn't decrement if the count is 0.\n\nI've no objection to taking the two additional synchronized blocks out of DirectUpdateHandler2. The one in addAndDelete was already there although it was enclosed by getting an index writer (which is where all the problems happened). I'm not adverse to taking that one out too\n\nBTW, you can't use tests.iters for the new test. I didn't want to wait for the default suite timeout so I set it locally to 10 minutes and that timer apparently runs across all iters. I wrote a shell script to re-invoke the test for a long time (500 times last night). ",
            "id": "comment-14680200"
        },
        {
            "date": "2015-08-11T14:46:04+0000",
            "author": "Mark Miller",
            "content": "Still trying to duplicate a hang with the above patch removed, but one strange thing I had to work around because the test kept failing relatively quickly for me (JVM bug?):\n\nIndex: solr/core/src/java/org/apache/solr/core/SolrCore.java\n===================================================================\n--- solr/core/src/java/org/apache/solr/core/SolrCore.java\t(revision 1695180)\n+++ solr/core/src/java/org/apache/solr/core/SolrCore.java\t(working copy)\n@@ -1639,7 +1639,9 @@\n           tmp = new SolrIndexSearcher(this, newIndexDir, getLatestSchema(),\n               (realtime ? \"realtime\":\"main\"), newReader, true, !realtime, true, directoryFactory);\n         } else  {\n-          RefCounted<IndexWriter> writer = getUpdateHandler().getSolrCoreState().getIndexWriter(this);\n+          // when this was getUpdateHandler#getSolrCoreState it could hit an NPE somehow,\n+          // even though variables are all final\n+          RefCounted<IndexWriter> writer = solrCoreState.getIndexWriter(this);\n           DirectoryReader newReader = null;\n           try {\n             newReader = indexReaderFactory.newReader(writer.get(), this);\n\n ",
            "id": "comment-14681889"
        },
        {
            "date": "2015-08-11T15:01:21+0000",
            "author": "Yonik Seeley",
            "content": "there are a lot of operations on ulog.something that are synchronized on solrCoreState.getUpdateLock(), and a bunch that aren't. What's up there? \n\nSome background here: I wrote the original tlog code (and DUH2 code that called it).  There was no solrCoreState.getUpdateLock() (and no sharing writers across reloads even).  Mark implemented that part and changed synchronized(this) to synchronized(solrCoreState.getUpdateLock()) I believe (to account for the fact that we could have 2 DUH2 instances).\n\nHopefully there are comments about when something is synchronized (and why it needed to be).  The intent was to have the common case unsynchronized for best throughput.  For example, I don't believe writer.updateDocument for the common case is synchronized.  That would be bad for indexing performance.\n\ndeleteByQuery (or an add where we detect a reordered DBQ that we need to apply again) contains the following\ncomment next to the synchronize statement:\n\n      //\n      // synchronized to prevent deleteByQuery from running during the \"open new searcher\"\n      // part of a commit.  DBQ needs to signal that a fresh reader will be needed for\n      // a realtime view of the index.  When a new searcher is opened after a DBQ, that\n      // flag can be cleared.  If those thing happen concurrently, it's not thread safe.\n      //\n\n\n\nI'm re-reviewing all this code now to get it back in my head... ",
            "id": "comment-14681912"
        },
        {
            "date": "2015-08-11T17:12:19+0000",
            "author": "Erick Erickson",
            "content": "Thanks guys, getting it all in my head is...interesting.\n\nYonik Seeley Yeah, I saw that comment. In that case, removing the two synchronizations in the refactored methods other than addAndDelete is probably indicated. The one in addAndDelete was there originally, just within the IndexWriter try/finally which is where the issues was since it'd go out and get a new searcher eventually.\n\nMark Miller I had to write a shell script to re-submit that individual test repeatedly, it'd pretty much always fail for me by 50 runs. I'll back those changes out and run it on my machine where it fails reliably and post the results when I get a deadlock. ",
            "id": "comment-14682096"
        },
        {
            "date": "2015-08-11T18:08:22+0000",
            "author": "Mark Miller",
            "content": "Yeah, I have a beasting script that does the same, though can also launch runs in parallel (https://gist.github.com/markrmiller/dbdb792216dc98b018ad). Still no deadlock on my machine yet though. I'll keep trying for a while though. ",
            "id": "comment-14682191"
        },
        {
            "date": "2015-08-11T19:30:35+0000",
            "author": "Erick Erickson",
            "content": "Here's the stack trace. A few things:\n\n> I got this source by \"svn checkout -r1694809 https://svn.apache.org/repos/asf/lucene/dev/trunk\" and added the test to that code base.\n\n> It appears to need two things to fail; a reload operation and a delete by query.\n\n> thread WRITER5 and TEST-TestReloadDeadlock.testReloadDeadlock-seed#[4CFFCB253DB33784]\" are the ones I think are fighting here.\n\n> The original fix was to pass the index writer from addDoc0() to addAndDelete, but this doesn't work either. I'll see if I can attach a run with that change for comparison.\n\n> I've also attached the script I use to run this, although I don't run it in parallel.\n ",
            "id": "comment-14682339"
        },
        {
            "date": "2015-08-11T20:54:55+0000",
            "author": "Erick Erickson",
            "content": "Here's a fail with passing the index writer from addDoc0() to addAndDelete, my first attempt at a \"fix\".\n\nI commented out the necessary lines rather than add or delete them, so the line numbers should correspond to the checkout I mentioned above. ",
            "id": "comment-14682486"
        },
        {
            "date": "2015-08-13T13:24:52+0000",
            "author": "Mark Miller",
            "content": "Strange, I have not been able to get the deadlock with that test on my machine.\n\nDo you have the full set of stack traces for a deadlock state available? ",
            "id": "comment-14695206"
        },
        {
            "date": "2015-08-13T15:17:06+0000",
            "author": "Erick Erickson",
            "content": "Mark Miller I thought everything was in the two zip files I attached a couple of days ago, what did I miss? See my comments two days ago for the provenance.\n\nBTW, I'll be intermittently available today and not at all tomorrow (business hours CA time) if you'd like to chat. ",
            "id": "comment-14695341"
        },
        {
            "date": "2015-08-13T15:55:44+0000",
            "author": "Mark Miller",
            "content": "Ah, sorry, I was looking for the full strack traces pre two days ago - didn't realize they were in the zip. I've been having trouble extracting that into something viable on my mac. I'll try on my Ubuntu machine. ",
            "id": "comment-14695417"
        },
        {
            "date": "2015-08-13T16:07:52+0000",
            "author": "Erick Erickson",
            "content": "Let me know if you need them in another format. I did just download them and extract, so they probably aren't corrupt at least. ",
            "id": "comment-14695441"
        },
        {
            "date": "2015-08-13T17:24:04+0000",
            "author": "Yonik Seeley",
            "content": "Yeah, I saw that comment. In that case, removing the two synchronizations in the refactored methods other than addAndDelete is probably indicated. The one in addAndDelete was there originally, just within the IndexWriter try/finally which is where the issues was since it'd go out and get a new searcher eventually.\n\nMaybe... but it's also worth noting that those comments (and my thinking around it) were before (i think) index writers could be shared across reloads.\n\nIt looks fine to me that you moved the ulog.add() outside of the writer get/release. ",
            "id": "comment-14695610"
        },
        {
            "date": "2015-08-13T17:35:09+0000",
            "author": "Yonik Seeley",
            "content": "Let me see how repeatable a deadlock is on my system, then we can look into removing some of the extra sync... ",
            "id": "comment-14695628"
        },
        {
            "date": "2015-08-13T18:16:47+0000",
            "author": "Erick Erickson",
            "content": "bq: It looks fine to me that you moved the ulog.add() outside of the writer get/release.\n\nThat's exactly what makes me nervous. But having it within the get/release is where the problem was.\n\nLet me know if there's anything you'd like to chat about.\n\nAnd it wasn't predictably repeatable (of course). Sometimes the script I attached would run 10-15 iterations OK then barf. Sometimes faster. Never more than 50. With the changes I ran with 400-500 iterations without a problem.\n\nSince I can get it to fail reasonably predictably though, let me know if you want me to run any changes locally. ",
            "id": "comment-14695714"
        },
        {
            "date": "2015-08-13T18:26:22+0000",
            "author": "Mark Miller",
            "content": "It looks fine to me that you moved the ulog.add() outside of the writer get/release.\n\nDoh, that is my problem - I only reverted the first commit to DirectUpdateHandler2. ",
            "id": "comment-14695729"
        },
        {
            "date": "2015-08-13T18:28:42+0000",
            "author": "Mark Miller",
            "content": "That's exactly what makes me nervous. \n\nThat should make you nervous - the random test fails around the transcation log tests after should be more in the terrified range though  ",
            "id": "comment-14695732"
        },
        {
            "date": "2015-08-13T19:03:38+0000",
            "author": "Yonik Seeley",
            "content": "Erick, I just now ran across this email from you 4 days ago:\n\nBacked out SOLR-7836 changes and TestStressReorder doesn't fail in 100\niterations, failed 16 times in 100 iterations with changes.\nDigging....\n\nIs this still the case (that TestStressReorder now sometimes fails?) ",
            "id": "comment-14695772"
        },
        {
            "date": "2015-08-13T19:45:32+0000",
            "author": "Erick Erickson",
            "content": "bq: Is this still the case (that TestStressReorder now sometimes fails?)\n\nYonik Seeley It better not be still failing or I'll have to fall on my sword. I'm pretty sure I fixed that up, I can't imagine that I would just check the changes in since the TestStressReorder was what I ripped off to make my new test to avoid that deadlock and the link between the two is pretty obvious, even to me \n\nI can't do it right now, but I'll run TestStressReorder a lot this evening to be sure.\n ",
            "id": "comment-14695827"
        },
        {
            "date": "2015-08-14T19:46:31+0000",
            "author": "Yonik Seeley",
            "content": "OK, so it wasn't too hard for me to replicate the deadlocks w/ the latest commits backed out, and the traces are similar to what was posted before... it's getIndexWriter fighting with newIndexWriter.\n\n\n  2> \"WRITER10\" ID=28 TIMED_WAITING on java.lang.Object@d8cdd45\n  2>    at java.lang.Object.wait(Native Method)\n  2>    - timed waiting on java.lang.Object@d8cdd45\n  2>    at org.apache.solr.update.DefaultSolrCoreState.getIndexWriter(DefaultSolrCoreState.java:96)\n  2>    at org.apache.solr.core.SolrCore.openNewSearcher(SolrCore.java:1588)\n  2>    at org.apache.solr.update.UpdateLog.add(UpdateLog.java:455)\n  2>    - locked org.apache.solr.update.UpdateLog@1df2955d\n  2>    at org.apache.solr.update.DirectUpdateHandler2.addAndDelete(DirectUpdateHandler2.java:452)\n[...]\n  2> \"TEST-TestReloadDeadlock.testReloadDeadlock-seed#[D13A45EBBFA304C4]\" ID=12 TIMED_WAITING on java.lang.Object@d8cdd45\n  2>    at java.lang.Object.wait(Native Method)\n  2>    - timed waiting on java.lang.Object@d8cdd45\n  2>    at org.apache.solr.update.DefaultSolrCoreState.newIndexWriter(DefaultSolrCoreState.java:158)\n  2>    - locked org.apache.solr.update.DefaultSolrCoreState@7d338874\n  2>    at org.apache.solr.core.SolrCore.reload(SolrCore.java:479)\n  2>    at org.apache.solr.core.CoreContainer.reload(CoreContainer.java:830)\n  2>    at org.apache.solr.search.TestReloadDeadlock.testReloadDeadlock(TestReloadDeadlock.java:182)\n\n\n\nOn straight trunk, I still get failures (just not deadlocks):\n\n  2> 3406 ERROR (WRITER6) [    ] o.a.s.c.SolrCore org.apache.solr.common.SolrException: Error opening new searcher\n  2>    at org.apache.solr.core.SolrCore.openNewSearcher(SolrCore.java:1673)\n  2>    at org.apache.solr.core.SolrCore.getRealtimeSearcher(SolrCore.java:1530)\n  2>    at org.apache.solr.update.VersionInfo.getVersionFromIndex(VersionInfo.java:202)\n  2>    at org.apache.solr.update.UpdateLog.lookupVersion(UpdateLog.java:783)\n  2>    at org.apache.solr.update.VersionInfo.lookupVersion(VersionInfo.java:195)\n  2>    at org.apache.solr.update.processor.DistributedUpdateProcessor.versionAdd(DistributedUpdateProcessor.java:1088)\n  2>    at org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:705)\n  2>    at org.apache.solr.update.processor.LogUpdateProcessor.processAdd(LogUpdateProcessorFactory.java:104)\n  2>    at org.apache.solr.handler.loader.JsonLoader$SingleThreadedJsonLoader.handleAdds(JsonLoader.java:470)\n  2>    at org.apache.solr.handler.loader.JsonLoader$SingleThreadedJsonLoader.processUpdate(JsonLoader.java:134)\n  2>    at org.apache.solr.handler.loader.JsonLoader$SingleThreadedJsonLoader.load(JsonLoader.java:113)\n  2>    at org.apache.solr.handler.loader.JsonLoader.load(JsonLoader.java:76)\n  2>    at org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:98)\n  2>    at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:74)\n  2>    at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:151)\n  2>    at org.apache.solr.core.SolrCore.execute(SolrCore.java:2079)\n  2>    at org.apache.solr.servlet.DirectSolrConnection.request(DirectSolrConnection.java:131)\n  2>    at org.apache.solr.SolrTestCaseJ4.updateJ(SolrTestCaseJ4.java:1104)\n  2>    at org.apache.solr.SolrTestCaseJ4.addAndGetVersion(SolrTestCaseJ4.java:1250)\n  2>    at org.apache.solr.search.TestReloadDeadlock.addDoc(TestReloadDeadlock.java:200)\n  2>    at org.apache.solr.search.TestReloadDeadlock.access$100(TestReloadDeadlock.java:46)\n  2>    at org.apache.solr.search.TestReloadDeadlock$1.run(TestReloadDeadlock.java:156)\n  2> Caused by: java.lang.NullPointerException\n  2>    at org.apache.solr.core.SolrCore.openNewSearcher(SolrCore.java:1642)\n  2>    ... 21 more\n\n ",
            "id": "comment-14697613"
        },
        {
            "date": "2015-08-14T20:02:45+0000",
            "author": "Mark Miller",
            "content": "and the traces are similar to what was posted before\n\nThere is not a third thread that has the Writer out? Both of those traces line up with another thread having the writer. I don't spend why they would deadlock each other without another thread involved.\n\nI'll take a closer look now that I can duplicate it. I still suspect the proper fix for this is in SolrCoreState. ",
            "id": "comment-14697655"
        },
        {
            "date": "2015-08-14T21:01:38+0000",
            "author": "Erick Erickson",
            "content": "I ran the StressTestReorder yesterday with the changes and didn't get it to fail for 50 runs on my laptop. Then last night I kicked off a 1,000 repetition run on my home machine and... didn't remember to look this morning before I left home.\n\nSo I'm pretty sure that StressTestReorder is OK with the latest patch, I'll look tonight to be sure.\n\nYonik Seeley I saw what I think is the same failure on the dev list from a Jenkins build last night and AFAIK the changes are still in there. ",
            "id": "comment-14697747"
        },
        {
            "date": "2015-08-14T21:21:30+0000",
            "author": "Yonik Seeley",
            "content": "There is not a third thread that has the Writer out?\n\nI just checked again... didn't see another.\n\nBoth of those traces line up with another thread having the writer. I don't spend why they would deadlock each other without another thread involved.\n\nThe first thread has the writer?  I have never really reviewed SolrCoreState in depth though... so I'm still not really sure. ",
            "id": "comment-14697771"
        },
        {
            "date": "2015-08-14T22:20:17+0000",
            "author": "Yonik Seeley",
            "content": "Here's the scenario w/ the old code.\n\nthread1 does an add:\n  1) in DUH2, calls coreState.getIndexWriter()\n\n\tthis increments the ref count on the writer and sets coreState.writerFree=false\n  2) calls UpdateLog.add\n\n\n\nthread2 calls core reload():\n  3) calls coreState.newIndexWriter()\n\n\tcoreState.pauseWriter is set to true, so no new references will be handed out\n\tgoes into a loop waiting for writerFree=true (for all other references that were handed out to be returned)\n\n\n\nthread1:\n  4) UpdateLog.add continues and indirectly causes coreState.getIndexWriter() to be called\n\n\tsees coreState.pauseWriter set to true and thus does into wait loop\n\n\n\nSo: one can't call getIndexWriter() and then do anything else that will eventually call getIndexWriter() or newIndexWriter()\nIf we keep that restriction, then moving the ulog.add outside of the getIndexWriter/release block was correct.\n\nDon't know about the other changes... the extra sync added in DUH2 does still seem unnecessary.  And I haven't looked at what changes were made to SolrCoreState yet. ",
            "id": "comment-14697858"
        },
        {
            "date": "2015-08-15T01:52:09+0000",
            "author": "Erick Erickson",
            "content": "Yonik Seeley Glad to see you're seeing the same thing I saw, senility is not here yet!\n\nI looked at the StressTestReorder when I got back home tonight and 1,000 iterations went through fine. Whew!\n\nI'll remove the extra synch in DUH2 and run the tests tonight to see what happens, along with some additional debugging in case we hit the null pointer exception again. Not a clue what's happening there. ",
            "id": "comment-14698050"
        },
        {
            "date": "2015-08-15T03:22:01+0000",
            "author": "Yonik Seeley",
            "content": "The NPE failures are likely unrelated to any changes made thus far... I think it's probably related to the fact that \"uhandler\" can abruptly change in the UpdateLog on a reload, and can be used perhaps before it's ready?  I haven't gone down that rabbit hole... ",
            "id": "comment-14698083"
        },
        {
            "date": "2015-08-15T13:09:12+0000",
            "author": "Yonik Seeley",
            "content": "I ran a while with trunk, with removing the extra sync on the normal \"add\" case only, not the \"addDelete\" case.\nI hit another deadlock.\nThis looks like the case Mark was looking for before... with a different thread holding the writer.\n\n\n  2> \"WRITER5\" ID=23 BLOCKED on java.lang.Object@5764facb owned by \"WRITER4\" ID=22\n  2>    at org.apache.solr.update.DirectUpdateHandler2.commit(DirectUpdateHandler2.java:593)\n  2>    - blocked on java.lang.Object@5764facb\n  2>    at org.apache.solr.update.processor.RunUpdateProcessor.processCommit(RunUpdateProcessorFactory.java:95)\n  2>    at org.apache.solr.update.processor.UpdateRequestProcessor.processCommit(UpdateRequestProcessor.java:64)\n  2>    at org.apache.solr.update.processor.DistributedUpdateProcessor.doLocalCommit(DistributedUpdateProcessor.java:1641)\n  2>    at org.apache.solr.update.processor.DistributedUpdateProcessor.processCommit(DistributedUpdateProcessor.java:1618)\n  2>    at org.apache.solr.update.processor.LogUpdateProcessor.processCommit(LogUpdateProcessorFactory.java:161)\n  2>    at org.apache.solr.handler.loader.XMLLoader.processUpdate(XMLLoader.java:270)\n[...]\n  2> \"WRITER4\" ID=22 TIMED_WAITING on java.lang.Object@5a94059a\n  2>    at java.lang.Object.wait(Native Method)\n  2>    - timed waiting on java.lang.Object@5a94059a\n  2>    at org.apache.solr.update.DefaultSolrCoreState.getIndexWriter(DefaultSolrCoreState.java:96)\n  2>    at org.apache.solr.core.SolrCore.openNewSearcher(SolrCore.java:1588)\n  2>    at org.apache.solr.update.UpdateLog.add(UpdateLog.java:455)\n  2>    - locked org.apache.solr.update.UpdateLog@5911991e\n  2>    at org.apache.solr.update.DirectUpdateHandler2.addAndDelete(DirectUpdateHandler2.java:331)\n  2>    - locked java.lang.Object@5764facb\n  2>    at org.apache.solr.update.DirectUpdateHandler2.addDoc0(DirectUpdateHandler2.java:200)\n  2>    at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:164)\n  2>    at org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:69)\n[...]\n  2> \"TEST-TestReloadDeadlock.testReloadDeadlock-seed#[D4E455E167793E1]\" ID=12 TIMED_WAITING on java.lang.Object@5a94059a\n  2>    at java.lang.Object.wait(Native Method)\n  2>    - timed waiting on java.lang.Object@5a94059a\n  2>    at org.apache.solr.update.DefaultSolrCoreState.newIndexWriter(DefaultSolrCoreState.java:156)\n  2>    - locked org.apache.solr.update.DefaultSolrCoreState@1b88614d\n  2>    at org.apache.solr.core.SolrCore.reload(SolrCore.java:479)\n\n\n\nWriter5 wants to do a commit\n\n\tcalls solrCoreState.getIndexWriter()\n\tblocks waiting for updateLock\n\n\n\nWriter4 wants to do an addAndDelete\n\n\taquires updateLock\n\taquires UpdateLog.this\n\tcalls DefaultSolrCoreState.getIndexWriter and waits forever\n\n\n\nMain-test-thread wants to do a reload:\n\n\tcalls DefaultSolrCoreState.newIndexWriter and waits forever\n\n\n\nIt feels like this type of deadlock can still be hit on trunk now unmodified?\nPerhaps the right solution was to just pass the IndexWriter down once you aquire it.  ",
            "id": "comment-14698257"
        },
        {
            "date": "2015-08-15T13:40:50+0000",
            "author": "Yonik Seeley",
            "content": "Passing the indexWriter down would also fix the NPEs I think since then openReader wouldn't try to obtain it from the yet-to-be-finished new update handler? ",
            "id": "comment-14698267"
        },
        {
            "date": "2015-08-16T13:49:27+0000",
            "author": "Yonik Seeley",
            "content": "OK, I let the current unmodified trunk run overnight.\nI got 14 fails, 4 of which were deadlocks.\n\nHere's the only changes I made to the test:\n\nIndex: solr/core/src/test/org/apache/solr/search/TestReloadDeadlock.java\n===================================================================\n--- solr/core/src/test/org/apache/solr/search/TestReloadDeadlock.java\t(revision 1695727)\n+++ solr/core/src/test/org/apache/solr/search/TestReloadDeadlock.java\t(working copy)\n@@ -74,7 +74,7 @@\n     int nWriteThreads = 5 + random().nextInt(10);\n \n     // query variables\n-    final AtomicLong reloads = new AtomicLong(50);  // number of reloads. Increase this number to force failure.\n+    final AtomicLong reloads = new AtomicLong(200);  // number of reloads. Increase this number to force failure.\n \n     ifVerbose(\"commitPercent\", commitPercent, \"deleteByQueryPercent\", deleteByQueryPercent\n         , \"ndocs\", ndocs, \"nWriteThreads\", nWriteThreads, \"reloads\", reloads);\n@@ -177,7 +177,7 @@\n \n     // The reload operation really doesn't need to happen from multiple threads, we just want it firing pretty often.\n     while (reloads.get() > 0) {\n-      Thread.sleep(10 + random().nextInt(250));\n+      Thread.sleep(1 + random().nextInt(50));\n       reloads.decrementAndGet();\n       h.getCoreContainer().reload(\"collection1\");\n     }\n\n ",
            "id": "comment-14698671"
        },
        {
            "date": "2015-08-16T14:16:08+0000",
            "author": "Mark Miller",
            "content": "\nSo: one can't call getIndexWriter() and then do anything else that will eventually call getIndexWriter() or newIndexWriter()\nIf we keep that restriction, then moving the ulog.add outside of the getIndexWriter/release block was correct.\n\nRight - the expected use is to be careful and quickly get and release the writer so that you don't accidentally try and getwriter or newwriter in between. This is why I thought Erik's first fix attempt sounded right. But then I guess he started hitting the deadlock you are still getting and figured it was not correct. Seems like it is correct, but we also have to address this other issue. ",
            "id": "comment-14698680"
        },
        {
            "date": "2015-08-16T14:22:37+0000",
            "author": "Mark Miller",
            "content": "Perhaps the right solution was to just pass the IndexWriter down once you aquire it.\nPassing the indexWriter down would also fix the NPEs I think since then openReader wouldn't try to obtain it from the yet-to-be-finished new update handler?\n\nWasn't that Erik's first fix attempt?\n\nI think the solution is to just pass the IndexWriter down to addAndDelete, but won't have time to really look until this evening. ",
            "id": "comment-14698687"
        },
        {
            "date": "2015-08-16T14:39:38+0000",
            "author": "Yonik Seeley",
            "content": "Wasn't that Erik's first fix attempt?\n\nYeah, I was essentially saying that seems to be the correct approach.\n ",
            "id": "comment-14698697"
        },
        {
            "date": "2015-08-16T14:47:12+0000",
            "author": "Mark Miller",
            "content": "Hmm, I wonder what was still not working about it yet. I should be able to duplicate this too now, I'll take a look at the stack traces with just that change. ",
            "id": "comment-14698699"
        },
        {
            "date": "2015-08-16T15:29:12+0000",
            "author": "Yonik Seeley",
            "content": "At some point in time, it seems like everyone develops some sort of script to run a test over and over again...\nI really should have put this up years ago:\nhttps://github.com/yonik/misc/blob/master/scripts/test.sh ",
            "id": "comment-14698724"
        },
        {
            "date": "2015-08-16T16:48:02+0000",
            "author": "Erick Erickson",
            "content": "Mark Miller If you check out trunk revision 1694809, then the stack trace above in the deadlock_5_pass_iw.res.zip is the problem I hit by passing the indexwriter down to DUH2.addanddelete. It really doesn't help because updatelog.add has this bit of code:\n\n        try {\n          RefCounted e1 = this.uhandler.core.openNewSearcher(true, true);\n          e1.decref();\n        } catch (Exception var8) {\n          SolrException.log(log, \"Error opening realtime searcher for deleteByQuery\", var8);\n        }\n\n\nso the IW passed to addAndDelete in DUH2 doesn't make a difference, the fact that the IW is held open is blocking the call to openNewSearcher above; it doesn't matter whether addAndDelete has an already opened IW passed in or opens its own, as long as the IW is open across the call to ulog.add(), a deadlock occurs. Sometimes.\n\nBased on the exception message, would a better approach be to open up the new searcher in delete by query instead? Would that cover all the cases? I'll look a little here. Then I could move all the ulog.add()s back inside the associated IW and again tightly couple the document adds with updating the ulog.\n\nOr maybe the answer is making realtime searchers sensitive to whether anything's been indexed since the last time it was opened rather than pre-opening a new one in addDoc0(). Don't even know if it's possible, haven't looked yet. Essentially this would make opening a realtime searcher \"lazy\". Which I can argue is a better solution than adding this overhead to all updates, although it also seems it would make realtime gets vary a lot depending on whether they had to open a searcher or not.\n\n ",
            "id": "comment-14698752"
        },
        {
            "date": "2015-08-16T17:45:06+0000",
            "author": "Erick Erickson",
            "content": "Poking a little more, opening a new searcher in add happens only when clearCaches==true, which only happens explicitly in DUH2.addAndDelete which is where all this started. There's also a call in the CDCR code that passes a variable in, but I don't think that's really relevant.\n\nIt's simple enough to move opening a new searcher up to these two places, I'll give it a try to evaluate. I don't like that solution much since it's trappy; a new call to add(cmd, true) that fails to open a new searcher could re-introduce the problem that opening that searcher where it's done now is designed to prevent. I suppose a big fat warning is in order?\n\nLet me try it just to see whether it cures things or not. I'm pretty sure it'll cure the deadlock problem, I'll first try to just comment out the openSearcher and see if I can blow up the real time get tests, then move the open out and see if either realtime get tests or the new deadlock test fail with the reorganized code. When I collect that data we can discuss some more. Probably have something later today.\n\nYonik Seeley Those numbers in the new test were chosen completely arbitrarily, I'm guessing that the point of your changes is to drive the failure more often without lengthening the time the test takes, so I'll incorporate them. ",
            "id": "comment-14698792"
        },
        {
            "date": "2015-08-19T18:11:40+0000",
            "author": "Erick Erickson",
            "content": "This patch should be applied after SOLR-7836.patch if anyone is back-porting\n\nHere's a new patch for comment that \n\n\tputs the ulog writes back inside the IW blocks\n\tpulls out the problematic open searcher in ulog.add to a separate method.\n\tcalls the extracted method from the two places that could call UpdateLog.add(cmd, true), which was the condition for opening a new searcher. The calls to the new method must be outside the IW block.\n\tremoves the extra synchronized blocks on solrCoreState.getUpdatelock()\n\tchanges the test to hit this condition harder as per Yonik.\n\n\n\nIt's possible that the CDCR code calls ulog.add with clearCaches==true, in which case the extracted method in ulog is called. Frankly I doubt that's a necessary thing, but seems harmless.\n\nI don't particularly like decoupling the open searcher from the updatelog.add, but I like lockups even less. Not to mention possible tlog craziness. So I'll live with my dislike.\n\nI think this addresses concerns about the tlog synchronization.\n\nI ran this last night for 360 iterations, then made some trivial changes (yeah, right). I'll try some beasting on this today plus StressTestReorder, then do the usual precommit and full test. Assuming all that goes well I'll probably check this in tomorrow and call this done unless there are objections.\n\nThis, coupled with Yoniks changes for the NPE should put this to bed.\n\nMark Miller Yonik Seeley all comments welcome of course. ",
            "id": "comment-14703493"
        },
        {
            "date": "2015-08-20T14:53:40+0000",
            "author": "Yonik Seeley",
            "content": "pulls out the problematic open searcher in ulog.add to a separate method.\n\nThere are a few areas with complex synchronization that should not be changed unless one is confident about understanding why all the synchronization was there in the first place.  Having the tests pass isn't a high enough bar for these areas because of the difficulty in actually getting a test to expose subtle race conditions or thread safety issues.  This comes back to my original \"get it back in my head\" - I don't fee comfortable messing with this stuff either until I've really internalized the bigger picture again... and it doesn't last \n\nFor the specific case above, one can't just take what was one synchronized block and break it up into two.  It certainly creates race conditions and breaks the invariants we try to keep.  The specific invariant here is that if it's not in the tlog maps, then it is guaranteed to be in the realtime reader.  Hopefully some of our tests would fail with this latest patch... but it's hard stuff to test.\n\nI worked up a patch that passed down the IndexWriter (it needs to be passed all the way down to SolrCore.openSearcher to actually avoid deadlocks).  That ended up changing more code than I'd like... so now I'm working up a patch to make IW locking re-entrant.  That approach should be less fragile going forward (i.e. less likely to easily introduce a deadlock through seemingly unrelated changes). ",
            "id": "comment-14705052"
        },
        {
            "date": "2015-08-20T15:50:51+0000",
            "author": "Yonik Seeley",
            "content": "I tried applying the last patch and running TestStressReorder and luckily it does fail often for me. ",
            "id": "comment-14705157"
        },
        {
            "date": "2015-08-20T18:19:23+0000",
            "author": "Erick Erickson",
            "content": "bq: I tried applying the last patch and running TestStressReorder and luckily it does fail often for me.\n\nYep, I saw that last night. I looked a bit at whether it was a test artifact and apparently it's not so I was going to dive into that today.\n\nAnyway, since you're working up alternatives, I'll leave it to you. The current checkin (not the latest patch which I won't commit) at least avoids the deadlock that started me down this path in the first place. Whether it creates other issues is, of course, the $64K question. Let me know if there's anything I can do besides cheer you on. ",
            "id": "comment-14705478"
        },
        {
            "date": "2015-08-20T19:08:20+0000",
            "author": "Yonik Seeley",
            "content": "OK, here's a patch that uses a reentrant read-write lock in DefaultSolrCoreState.\n\nNotes:\n\n\tno more pauseWriter / freeWriter variables\n\treadlock is for grabbing the current writer, writeLock is for ensuring there are no readers\n\tthe RefCounted<IndexWriter> doesn't do much any more... decRef releases the readLock, that's the only important thing.  I just kept it to keep the API the same for now.\n\tall of newIndexWriter, closeIndexWriter, and openIndexWriter all delegate to changeIndexWriter to remove duplicated code.  This does change the semantics of some of these methods, but I hope for the better?\n\t\n\t\tThe old openIndexWriter method only synchronized on writerPauseLock, so if there were any readers and writers in the lock loop, or even readers open, this method could go in and still change the writer anyway.  Sharing the code in changeWriter now means that it would wait for any readers to finish (which looking at the comments implies there should not be anyway?  and if there were, new IW creation would fail), close any existing old writer, and then open the new one.\n\t\n\t\n\tI reordered some things so that if we got an exception while opening a new writer, the writer reference would already be cleared so we won't simply default to using the old (presumably now closed) writer, but will try to open again.  Should be more resilient in the face of a spurious exception.\n\tit's really not clear if polling is still needed in getIndexWriter, but I kept it for now just to be safe.\n\n\n\nlooping some tests now... ",
            "id": "comment-14705580"
        },
        {
            "date": "2015-08-20T19:54:57+0000",
            "author": "Erick Erickson",
            "content": "I'll beast the stressdeadlock and stresstestreorder for a while as well.\n\nThanks! ",
            "id": "comment-14705653"
        },
        {
            "date": "2015-08-20T23:22:29+0000",
            "author": "Erick Erickson",
            "content": "FWIW, 96 runs each (look, 96 divides by 6 processors evenly, OK?) and both StressTestReorder and TestReloadDeadlock seem happy, along with precommit. Running full test suite now. ",
            "id": "comment-14705966"
        },
        {
            "date": "2015-08-21T23:26:47+0000",
            "author": "Erick Erickson",
            "content": "Yonik Seeley Since you've done the heavy lifting here and I've beasted both the the tests, and a full test also worked, do you want me to just go ahead and check it in? Or do you want to polish it yet? ",
            "id": "comment-14707634"
        },
        {
            "date": "2015-08-22T01:24:47+0000",
            "author": "Yonik Seeley",
            "content": "I'm running some more tests and will do a little more polishing on comments... ",
            "id": "comment-14707760"
        },
        {
            "date": "2015-08-26T00:10:03+0000",
            "author": "Yonik Seeley",
            "content": "I've been running ChaosMonkeySafeLeaderTest for about 3 days with my test script that also searches for corrupt indexes or assertion failures even when the test still passes.\nCurrent trunk (as of last week): 9 corrupt indexes\nPatched trunk: 14 corrupt indexes and 2 test failures (inconsistent shards)\n\nThe corrupt indexes may not be a problem, I don't really know.  We kill off servers, perhaps during replication?  Seems like that could produce corrupt indexes, but I don't know if that's the scenario or not.  Increasing the incidence of those doesn't necessarily point to a problem either.  But inconsistent shards vs not... does seem like a problem if it holds.\n\nI've reviewed the locking code again, and it looks solid, so I'm not sure what's going on.\n\nHere's a typical corrupt index trace:\n\n  2> 21946 WARN  (RecoveryThread-collection1) [n:127.0.0.1:51815_ c:collection1 s:shard1 r:core_node2 x:collection1] o.a.s.h.IndexFetcher Could not retrie\nve checksum from file.\n  2> org.apache.lucene.index.CorruptIndexException: codec footer mismatch (file truncated?): actual footer=1698720114 vs expected footer=-1071082520 (resource=MMapIndexInput(path=\"/opt/code/lusolr_clean2/solr/build/solr-core/test/J0/temp/solr.cloud.ChaosMonkeySafeLeaderTest_B7DC9C42462BF20D-001/shard-2-001/cores/collection1/data/index/_0.fdt\"))\n  2>    at org.apache.lucene.codecs.CodecUtil.validateFooter(CodecUtil.java:416)\n  2>    at org.apache.lucene.codecs.CodecUtil.retrieveChecksum(CodecUtil.java:401)\n  2>    at org.apache.solr.handler.IndexFetcher.compareFile(IndexFetcher.java:876)\n  2>    at org.apache.solr.handler.IndexFetcher.downloadIndexFiles(IndexFetcher.java:839)\n  2>    at org.apache.solr.handler.IndexFetcher.fetchLatestIndex(IndexFetcher.java:437)\n  2>    at org.apache.solr.handler.IndexFetcher.fetchLatestIndex(IndexFetcher.java:265)\n  2>    at org.apache.solr.handler.ReplicationHandler.doFetch(ReplicationHandler.java:382)\n  2>    at org.apache.solr.cloud.RecoveryStrategy.replicate(RecoveryStrategy.java:162)\n  2>    at org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:437)\n  2>    at org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:227)\n\n ",
            "id": "comment-14712222"
        },
        {
            "date": "2015-08-27T13:56:33+0000",
            "author": "Yonik Seeley",
            "content": "I'm trying some different variants of my patch, trying to pin down exactly what change in semantics are causing the fails.  It takes some time though... I normally need to let it loop for a day to get failures. ",
            "id": "comment-14716703"
        },
        {
            "date": "2015-09-01T16:51:49+0000",
            "author": "Yonik Seeley",
            "content": "OK, I found the issue.\nWhen I re-wrote the SolrCoreState locking to use reentrant read-write locks, I was using the current trunk as a reference.  Unfortunately, a previous commit in this issue had introduced a bug by changing the semantics.  I didn't realize it until I went back to look at trunk before any commits on this issue.\n\nThe bug was the addition of this to closeIndexWriter:\n\n     } finally {\n        pauseWriter = false;\n        writerPauseLock.notifyAll();\n      }\n\n\n\ncloseIndexWriter / newIndexWriter are meant to be used as a pair, and block any readers inbetween them being called.\n\nI've been looping ChaosMonkeySafeLeaderTest since yesterday afternoon and no fails yet. ",
            "id": "comment-14725676"
        },
        {
            "date": "2015-09-01T21:17:43+0000",
            "author": "Mark Miller",
            "content": "The bug was the addition of this to closeIndexWriter:\n\nOuch, lucky to notice that. Shows how careful we have to be when touching update side synchronization. Quick commits can quickly get lost and introduce hard to track down bugs. ",
            "id": "comment-14726204"
        },
        {
            "date": "2015-09-02T04:46:58+0000",
            "author": "Erick Erickson",
            "content": "Hmmm, looks like I screwed the pooch on that one. Siiiggggghhhhh. ",
            "id": "comment-14726738"
        },
        {
            "date": "2015-09-02T14:54:04+0000",
            "author": "Erick Erickson",
            "content": "Worth merging into 5.3 in case a 5.3.1 gets cut? ",
            "id": "comment-14727456"
        },
        {
            "date": "2015-09-03T15:07:25+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1701043 from Yonik Seeley in branch 'dev/trunk'\n[ https://svn.apache.org/r1701043 ]\n\nSOLR-7836: change SolrCoreState to use a reentrant locking implementation to fix deadlocks ",
            "id": "comment-14729210"
        },
        {
            "date": "2015-09-03T15:08:48+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1701044 from Yonik Seeley in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1701044 ]\n\nSOLR-7836: change SolrCoreState to use a reentrant locking implementation to fix deadlocks ",
            "id": "comment-14729212"
        },
        {
            "date": "2015-09-03T15:16:37+0000",
            "author": "Yonik Seeley",
            "content": "Not sure if there will even be a 5.3.1 release.\nIf there is, perhaps everything that was committed as part of this issue should go back?  Would be nice to let it bake a while longer in 5x/trunk just to make sure that nothing else is broken. ",
            "id": "comment-14729227"
        },
        {
            "date": "2015-09-04T15:34:43+0000",
            "author": "Erick Erickson",
            "content": "Yonik Seeley] re: back-porting to 5.3.1. Never mind...\n\nI just checked to be sure. Noble cut the 5.3 branch 3 days before my first checkin for this ticket. If the problems you found with that check in were in the release a back-port was in order. But  since they're not, I see no need. ",
            "id": "comment-14730965"
        }
    ]
}