{
    "id": "SOLR-7338",
    "title": "A reloaded core will never register itself as active after a ZK session expiration",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.1",
            "6.0"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "If a collection gets reloaded, then a core's isReloaded flag is always true. If a core experiences a ZK session expiration after a reload, then it won't ever be able to set itself to active because of the check in ZkController#register:\n\n\n        UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n        if (!core.isReloaded() && ulog != null) {\n          // disable recovery in case shard is in construction state (for shard splits)\n          Slice slice = getClusterState().getSlice(collection, shardId);\n          if (slice.getState() != Slice.State.CONSTRUCTION || !isLeader) {\n            Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler().getUpdateLog().recoverFromLog();\n            if (recoveryFuture != null) {\n              log.info(\"Replaying tlog for \" + ourUrl + \" during startup... NOTE: This can take a while.\");\n              recoveryFuture.get(); // NOTE: this could potentially block for\n              // minutes or more!\n              // TODO: public as recovering in the mean time?\n              // TODO: in the future we could do peersync in parallel with recoverFromLog\n            } else {\n              log.info(\"No LogReplay needed for core=\" + core.getName() + \" baseURL=\" + baseUrl);\n            }\n          }\n          boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n              collection, coreZkNodeName, shardId, leaderProps, core, cc);\n          if (!didRecovery) {\n            publish(desc, ZkStateReader.ACTIVE);\n          }\n        }\n\n\n\nI can easily simulate this on trunk by doing:\n\n\nbin/solr -c -z localhost:2181\nbin/solr create -c foo\nbin/post -c foo example/exampledocs/*.xml\ncurl \"http://localhost:8983/solr/admin/collections?action=RELOAD&name=foo\"\nkill -STOP <PID> && sleep <PAUSE_SECONDS> && kill -CONT <PID>\n\n\n\nWhere <PID> is the process ID of the Solr node. Here are the logs after the CONT command. As you can see below, the core never gets to setting itself as active again. I think the bug is that the isReloaded flag needs to get set back to false once the reload is successful, but I don't understand what this flag is needed for anyway???\n\n\nINFO  - 2015-04-01 17:28:50.962; org.apache.solr.common.cloud.ConnectionManager; Watcher org.apache.solr.common.cloud.ConnectionManager@5519dba0 name:ZooKeeperConnection Watcher:localhost:2181 got event WatchedEvent state:Disconnected type:None path:null path:null type:None\nINFO  - 2015-04-01 17:28:50.963; org.apache.solr.common.cloud.ConnectionManager; zkClient has disconnected\nINFO  - 2015-04-01 17:28:51.107; org.apache.solr.common.cloud.ConnectionManager; Watcher org.apache.solr.common.cloud.ConnectionManager@5519dba0 name:ZooKeeperConnection Watcher:localhost:2181 got event WatchedEvent state:Expired type:None path:null path:null type:None\nINFO  - 2015-04-01 17:28:51.107; org.apache.solr.common.cloud.ConnectionManager; Our previous ZooKeeper session was expired. Attempting to reconnect to recover relationship with ZooKeeper...\nINFO  - 2015-04-01 17:28:51.108; org.apache.solr.cloud.Overseer; Overseer (id=93579450724974592-192.168.1.2:8983_solr-n_0000000000) closing\nINFO  - 2015-04-01 17:28:51.108; org.apache.solr.cloud.ZkController$WatcherImpl; A node got unwatched for /configs/foo\nINFO  - 2015-04-01 17:28:51.108; org.apache.solr.cloud.Overseer$ClusterStateUpdater; Overseer Loop exiting : 192.168.1.2:8983_solr\nINFO  - 2015-04-01 17:28:51.109; org.apache.solr.cloud.OverseerCollectionProcessor; According to ZK I (id=93579450724974592-192.168.1.2:8983_solr-n_0000000000) am no longer a leader.\nINFO  - 2015-04-01 17:28:51.108; org.apache.solr.cloud.ZkController$4; Running listeners for /configs/foo\nINFO  - 2015-04-01 17:28:51.109; org.apache.solr.common.cloud.DefaultConnectionStrategy; Connection expired - starting a new one...\nINFO  - 2015-04-01 17:28:51.109; org.apache.solr.core.SolrCore$11; config update listener called for core foo_shard1_replica1\nINFO  - 2015-04-01 17:28:51.110; org.apache.solr.common.cloud.ConnectionManager; Waiting for client to connect to ZooKeeper\nERROR - 2015-04-01 17:28:51.110; org.apache.solr.common.SolrException; OverseerAutoReplicaFailoverThread had an error in its thread work loop.:org.apache.solr.common.SolrException: Error reading cluster properties\n\tat org.apache.solr.common.cloud.ZkStateReader.getClusterProps(ZkStateReader.java:772)\n\tat org.apache.solr.cloud.OverseerAutoReplicaFailoverThread.doWork(OverseerAutoReplicaFailoverThread.java:150)\n\tat org.apache.solr.cloud.OverseerAutoReplicaFailoverThread.run(OverseerAutoReplicaFailoverThread.java:129)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.InterruptedException: sleep interrupted\n\tat java.lang.Thread.sleep(Native Method)\n\tat org.apache.solr.common.cloud.ZkCmdExecutor.retryDelay(ZkCmdExecutor.java:108)\n\tat org.apache.solr.common.cloud.ZkCmdExecutor.retryOperation(ZkCmdExecutor.java:76)\n\tat org.apache.solr.common.cloud.SolrZkClient.exists(SolrZkClient.java:308)\n\tat org.apache.solr.common.cloud.ZkStateReader.getClusterProps(ZkStateReader.java:765)\n\t... 3 more\n\nWARN  - 2015-04-01 17:28:51.110; org.apache.solr.cloud.ZkController$4; listener throws error\norg.apache.solr.common.SolrException: org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /configs/foo/params.json\n\tat org.apache.solr.core.RequestParams.getFreshRequestParams(RequestParams.java:160)\n\tat org.apache.solr.core.SolrConfig.refreshRequestParams(SolrConfig.java:907)\n\tat org.apache.solr.core.SolrCore$11.run(SolrCore.java:2503)\n\tat org.apache.solr.cloud.ZkController$4.run(ZkController.java:2351)\nCaused by: org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /configs/foo/params.json\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:127)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1045)\n\tat org.apache.solr.common.cloud.SolrZkClient$4.execute(SolrZkClient.java:294)\n\tat org.apache.solr.common.cloud.SolrZkClient$4.execute(SolrZkClient.java:291)\n\tat org.apache.solr.common.cloud.ZkCmdExecutor.retryOperation(ZkCmdExecutor.java:61)\n\tat org.apache.solr.common.cloud.SolrZkClient.exists(SolrZkClient.java:291)\n\tat org.apache.solr.core.RequestParams.getFreshRequestParams(RequestParams.java:150)\n\t... 3 more\nERROR - 2015-04-01 17:28:51.110; org.apache.solr.cloud.Overseer$ClusterStateUpdater; could not read the data\norg.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /overseer_elect/leader\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:127)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)\n\tat org.apache.solr.common.cloud.SolrZkClient$7.execute(SolrZkClient.java:345)\n\tat org.apache.solr.common.cloud.SolrZkClient$7.execute(SolrZkClient.java:342)\n\tat org.apache.solr.common.cloud.ZkCmdExecutor.retryOperation(ZkCmdExecutor.java:61)\n\tat org.apache.solr.common.cloud.SolrZkClient.getData(SolrZkClient.java:342)\n\tat org.apache.solr.cloud.Overseer$ClusterStateUpdater.checkIfIamStillLeader(Overseer.java:298)\n\tat org.apache.solr.cloud.Overseer$ClusterStateUpdater.access$300(Overseer.java:83)\n\tat org.apache.solr.cloud.Overseer$ClusterStateUpdater$2.run(Overseer.java:259)\nINFO  - 2015-04-01 17:28:51.114; org.apache.solr.common.cloud.ConnectionManager; Watcher org.apache.solr.common.cloud.ConnectionManager@5519dba0 name:ZooKeeperConnection Watcher:localhost:2181 got event WatchedEvent state:SyncConnected type:None path:null path:null type:None\nINFO  - 2015-04-01 17:28:51.115; org.apache.solr.common.cloud.ConnectionManager; Client is connected to ZooKeeper\nINFO  - 2015-04-01 17:28:51.115; org.apache.solr.common.cloud.ConnectionManager$1; Connection with ZooKeeper reestablished.\nINFO  - 2015-04-01 17:28:51.115; org.apache.solr.cloud.ZkController$1; ZooKeeper session re-connected ... refreshing core states after session expiration.\nINFO  - 2015-04-01 17:28:51.115; org.apache.solr.cloud.ZkController; publishing core=foo_shard1_replica1 state=down collection=foo\nINFO  - 2015-04-01 17:28:51.118; org.apache.solr.cloud.ElectionContext; canceling election /overseer_elect/election/93579450724974592-192.168.1.2:8983_solr-n_0000000000\nWARN  - 2015-04-01 17:28:51.119; org.apache.solr.cloud.ElectionContext; cancelElection did not find election node to remove /overseer_elect/election/93579450724974592-192.168.1.2:8983_solr-n_0000000000\nINFO  - 2015-04-01 17:28:51.121; org.apache.solr.cloud.OverseerElectionContext; I am going to be the leader 192.168.1.2:8983_solr\nINFO  - 2015-04-01 17:28:51.121; org.apache.solr.common.cloud.SolrZkClient; makePath: /overseer_elect/leader\nINFO  - 2015-04-01 17:28:51.122; org.apache.solr.cloud.Overseer; Overseer (id=93579450724974594-192.168.1.2:8983_solr-n_0000000001) starting\nINFO  - 2015-04-01 17:28:51.128; org.apache.solr.cloud.OverseerAutoReplicaFailoverThread; Starting OverseerAutoReplicaFailoverThread autoReplicaFailoverWorkLoopDelay=10000 autoReplicaFailoverWaitAfterExpiration=30000 autoReplicaFailoverBadNodeExpiration=60000\nINFO  - 2015-04-01 17:28:51.128; org.apache.solr.cloud.OverseerCollectionProcessor; Process current queue of collection creations\nINFO  - 2015-04-01 17:28:51.128; org.apache.solr.common.cloud.ZkStateReader; Updating cluster state from ZooKeeper... \nINFO  - 2015-04-01 17:28:51.129; org.apache.solr.cloud.Overseer$ClusterStateUpdater; Starting to work on the main queue\nINFO  - 2015-04-01 17:28:51.130; org.apache.solr.common.cloud.ZkStateReader; addZkWatch foo\nINFO  - 2015-04-01 17:28:51.131; org.apache.solr.common.cloud.ZkStateReader; Updating collection state at /collections/foo/state.json from ZooKeeper... \nINFO  - 2015-04-01 17:28:51.131; org.apache.solr.common.cloud.ZkStateReader; Updating data for foo to ver 4 \nINFO  - 2015-04-01 17:28:51.131; org.apache.solr.cloud.ZkController; Register node as live in ZooKeeper:/live_nodes/192.168.1.2:8983_solr\nINFO  - 2015-04-01 17:28:51.132; org.apache.solr.common.cloud.SolrZkClient; makePath: /live_nodes/192.168.1.2:8983_solr\nINFO  - 2015-04-01 17:28:51.133; org.apache.solr.cloud.ZkController; Register replica - core:foo_shard1_replica1 address:http://192.168.1.2:8983/solr collection:foo shard:shard1\nINFO  - 2015-04-01 17:28:51.133; org.apache.solr.common.cloud.ZkStateReader; Updating data for foo to ver 4 \nINFO  - 2015-04-01 17:28:51.133; org.apache.solr.cloud.ElectionContext; canceling election /collections/foo/leader_elect/shard1/election/93579450724974592-core_node1-n_0000000000\nWARN  - 2015-04-01 17:28:51.134; org.apache.solr.cloud.ElectionContext; cancelElection did not find election node to remove /collections/foo/leader_elect/shard1/election/93579450724974592-core_node1-n_0000000000\nINFO  - 2015-04-01 17:28:51.134; org.apache.solr.cloud.Overseer$ClusterStateUpdater; processMessage: queueSize: 1, message = {\n  \"core\":\"foo_shard1_replica1\",\n  \"core_node_name\":\"core_node1\",\n  \"roles\":null,\n  \"base_url\":\"http://192.168.1.2:8983/solr\",\n  \"node_name\":\"192.168.1.2:8983_solr\",\n  \"numShards\":\"1\",\n  \"state\":\"down\",\n  \"shard\":\"shard1\",\n  \"collection\":\"foo\",\n  \"operation\":\"state\"} current state version: 1\nINFO  - 2015-04-01 17:28:51.135; org.apache.solr.cloud.overseer.ReplicaMutator; Update state numShards=1 message={\n  \"core\":\"foo_shard1_replica1\",\n  \"core_node_name\":\"core_node1\",\n  \"roles\":null,\n  \"base_url\":\"http://192.168.1.2:8983/solr\",\n  \"node_name\":\"192.168.1.2:8983_solr\",\n  \"numShards\":\"1\",\n  \"state\":\"down\",\n  \"shard\":\"shard1\",\n  \"collection\":\"foo\",\n  \"operation\":\"state\"}\nINFO  - 2015-04-01 17:28:51.136; org.apache.solr.cloud.ShardLeaderElectionContext; Running the leader process for shard shard1\nINFO  - 2015-04-01 17:28:51.136; org.apache.solr.cloud.overseer.ZkStateWriter; going to update_collection /collections/foo/state.json version: 4\nINFO  - 2015-04-01 17:28:51.136; org.apache.solr.cloud.ActionThrottle; The last leader attempt started 198060ms ago.\nINFO  - 2015-04-01 17:28:51.137; org.apache.solr.common.cloud.ZkStateReader$7; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/collections/foo/state.json for collection foo has occurred - updating... (live nodes size: 1)\nINFO  - 2015-04-01 17:28:51.137; org.apache.solr.common.cloud.ZkStateReader; Updating data for foo to ver 5 \nINFO  - 2015-04-01 17:28:51.138; org.apache.solr.cloud.DistributedQueue$LatchWatcher; NodeChildrenChanged fired on path /overseer/queue state SyncConnected\nINFO  - 2015-04-01 17:28:51.138; org.apache.solr.cloud.ShardLeaderElectionContext; Enough replicas found to continue.\nINFO  - 2015-04-01 17:28:51.138; org.apache.solr.cloud.ShardLeaderElectionContext; I may be the new leader - try and sync\nINFO  - 2015-04-01 17:28:51.138; org.apache.solr.cloud.SyncStrategy; Sync replicas to http://192.168.1.2:8983/solr/foo_shard1_replica1/\nINFO  - 2015-04-01 17:28:51.138; org.apache.solr.cloud.SyncStrategy; Sync Success - now sync replicas to me\nINFO  - 2015-04-01 17:28:51.138; org.apache.solr.cloud.SyncStrategy; http://192.168.1.2:8983/solr/foo_shard1_replica1/ has no replicas\nINFO  - 2015-04-01 17:28:51.139; org.apache.solr.cloud.ShardLeaderElectionContext; I am the new leader: http://192.168.1.2:8983/solr/foo_shard1_replica1/ shard1\nINFO  - 2015-04-01 17:28:51.139; org.apache.solr.common.cloud.SolrZkClient; makePath: /collections/foo/leaders/shard1\nINFO  - 2015-04-01 17:28:51.139; org.apache.solr.cloud.Overseer$ClusterStateUpdater; processMessage: queueSize: 1, message = {\n  \"operation\":\"leader\",\n  \"shard\":\"shard1\",\n  \"collection\":\"foo\"} current state version: 1\nINFO  - 2015-04-01 17:28:51.140; org.apache.solr.cloud.overseer.ZkStateWriter; going to update_collection /collections/foo/state.json version: 5\nINFO  - 2015-04-01 17:28:51.141; org.apache.solr.common.cloud.ZkStateReader$7; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/collections/foo/state.json for collection foo has occurred - updating... (live nodes size: 1)\nINFO  - 2015-04-01 17:28:51.142; org.apache.solr.cloud.DistributedQueue$LatchWatcher; NodeChildrenChanged fired on path /overseer/queue state SyncConnected\nINFO  - 2015-04-01 17:28:51.142; org.apache.solr.common.cloud.ZkStateReader; Updating data for foo to ver 6 \nINFO  - 2015-04-01 17:28:51.144; org.apache.solr.cloud.Overseer$ClusterStateUpdater; processMessage: queueSize: 1, message = {\n  \"operation\":\"leader\",\n  \"shard\":\"shard1\",\n  \"collection\":\"foo\",\n  \"base_url\":\"http://192.168.1.2:8983/solr\",\n  \"core\":\"foo_shard1_replica1\",\n  \"state\":\"active\"} current state version: 1\nINFO  - 2015-04-01 17:28:51.145; org.apache.solr.cloud.overseer.ZkStateWriter; going to update_collection /collections/foo/state.json version: 6\nINFO  - 2015-04-01 17:28:51.145; org.apache.solr.common.cloud.ZkStateReader$7; A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/collections/foo/state.json for collection foo has occurred - updating... (live nodes size: 1)\nINFO  - 2015-04-01 17:28:51.146; org.apache.solr.common.cloud.ZkStateReader; Updating data for foo to ver 7 \nINFO  - 2015-04-01 17:28:51.147; org.apache.solr.cloud.DistributedQueue$LatchWatcher; NodeChildrenChanged fired on path /overseer/queue state SyncConnected\nINFO  - 2015-04-01 17:28:51.196; org.apache.solr.cloud.ZkController; We are http://192.168.1.2:8983/solr/foo_shard1_replica1/ and leader is http://192.168.1.2:8983/solr/foo_shard1_replica1/\nINFO  - 2015-04-01 17:28:51.196; org.apache.solr.cloud.ZkController; \n\n In register, core.isReloaded? true \n\n\nINFO  - 2015-04-01 17:28:51.199; org.apache.solr.common.cloud.ZkStateReader; Updating data for foo to ver 7 \nINFO  - 2015-04-01 17:28:51.199; org.apache.solr.cloud.ZkController; watch zkdir /configs/foo\nINFO  - 2015-04-01 17:28:51.199; org.apache.solr.schema.ZkIndexSchemaReader; Creating ZooKeeper watch for the managed schema at /configs/foo/managed-schema\nINFO  - 2015-04-01 17:28:51.199; org.apache.solr.cloud.ZkController$4; Running listeners for /configs/foo\nINFO  - 2015-04-01 17:28:51.200; org.apache.solr.core.SolrCore$11; config update listener called for core foo_shard1_replica1\nINFO  - 2015-04-01 17:28:51.200; org.apache.solr.schema.ZkIndexSchemaReader; Current schema version 5 is already the latest\nINFO  - 2015-04-01 17:28:51.200; org.apache.solr.schema.ZkIndexSchemaReader; Creating ZooKeeper watch for the managed schema at /configs/foo/managed-schema\nINFO  - 2015-04-01 17:28:51.200; org.apache.solr.core.SolrConfig; current version of requestparams : 0\nINFO  - 2015-04-01 17:28:51.201; org.apache.solr.schema.ZkIndexSchemaReader; Current schema version 5 is already the latest\nINFO  - 2015-04-01 17:28:51.201; org.apache.solr.common.cloud.DefaultConnectionStrategy; Reconnected to ZooKeeper\nINFO  - 2015-04-01 17:28:51.202; org.apache.solr.common.cloud.ConnectionManager; Connected:true\nINFO  - 2015-04-01 17:28:51.211; org.apache.solr.cloud.ZkController$WatcherImpl; A node got unwatched for /configs/foo\nINFO  - 2015-04-01 17:28:51.211; org.apache.solr.cloud.ZkController$4; Running listeners for /configs/foo\nINFO  - 2015-04-01 17:28:51.211; org.apache.solr.core.SolrCore$11; config update listener called for core foo_shard1_replica1\nINFO  - 2015-04-01 17:28:51.212; org.apache.solr.core.SolrConfig; current version of requestparams : 0\nINFO  - 2015-04-01 17:29:07.080; org.apache.solr.servlet.SolrDispatchFilter; [admin] webapp=null path=/admin/cores params={indexInfo=false&wt=json&_=1427909347078} status=0 QTime=0 \nINFO  - 2015-04-01 17:29:07.112; org.apache.solr.servlet.SolrDispatchFilter; [admin] webapp=null path=/admin/info/system params={wt=json&_=1427909347099} status=0 QTime=11",
    "attachments": {
        "SOLR-7338_test.patch": "https://issues.apache.org/jira/secure/attachment/12708985/SOLR-7338_test.patch",
        "SOLR-7338.patch": "https://issues.apache.org/jira/secure/attachment/12708813/SOLR-7338.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-04-01T19:09:58+0000",
            "author": "Timothy Potter",
            "content": "but I don't understand what this flag is needed for anyway???\n\nSorry, I wasn't clear ... I get the idea of having a flag to tell that a core is in the process of reloading, so that we can make various decisions about what to do with the tlogs, etc, however, I think that flag should be set back to false after a core is fully reloaded and active again, no? I can't see a reason why after a successful reload is complete, that flag should stay == true. ",
            "id": "comment-14391258"
        },
        {
            "date": "2015-04-01T19:26:20+0000",
            "author": "Mark Miller",
            "content": "looks like some broken refactoring or something.\n\nLook at what it used to be:\n\n\n      UpdateLog ulog = core.getUpdateHandler().getUpdateLog();\n      if (!core.isReloaded() && ulog != null) {\n        Future<UpdateLog.RecoveryInfo> recoveryFuture = core.getUpdateHandler()\n            .getUpdateLog().recoverFromLog();\n        if (recoveryFuture != null) {\n          recoveryFuture.get(); // NOTE: this could potentially block for\n          // minutes or more!\n          // TODO: public as recovering in the mean time?\n          // TODO: in the future we could do peersync in parallel with recoverFromLog\n        } else {\n          log.info(\"No LogReplay needed for core=\"+core.getName() + \" baseURL=\" + baseUrl);\n        }\n      }      \n      boolean didRecovery = checkRecovery(coreName, desc, recoverReloadedCores, isLeader, cloudDesc,\n          collection, coreZkNodeName, shardId, leaderProps, core, cc);\n      if (!didRecovery) {\n        publish(desc, ZkStateReader.ACTIVE);\n      }\n\n ",
            "id": "comment-14391279"
        },
        {
            "date": "2015-04-01T19:29:06+0000",
            "author": "Mark Miller",
            "content": "I can't see a reason why after a successful reload is complete, that flag should stay == true.\n\nWhy not? A reloaded core should never do various things like replay it's transaction log in register. I don't see how it makes any sense if it doesn't stay true. The core has either been reloaded before or it has not. ",
            "id": "comment-14391284"
        },
        {
            "date": "2015-04-01T19:33:56+0000",
            "author": "Mark Miller",
            "content": "looks like some broken refactoring or something.\n\nOr an incomplete attempt at a bug fix. We also do not want to recover on a reload, so this is probably an incomplete attempt at fixing that. It just didn't take into account that we still need that ACTIVE publish.\n ",
            "id": "comment-14391292"
        },
        {
            "date": "2015-04-01T19:40:36+0000",
            "author": "Mark Miller",
            "content": "Okay, so, total story looks like:\n\nThe isReloaded call was originally just used to prevent tlog replay other than on first startup.\n\nRecovery was always done, even on a reload.\n\nLater, it was noticed we should not be recovering on reload and the recovery check was also brought under isReloaded. \n\nThis was not okay - the recovery check, unlike the tlog replay check, needs to know if this register attempt is the result of a reload - not if the core has ever been reloaded.\n\nWe need a test that can catch this. ",
            "id": "comment-14391301"
        },
        {
            "date": "2015-04-01T19:57:53+0000",
            "author": "Timothy Potter",
            "content": "Thanks for digging that up. I still don't understand why we have to keep track of whether a core has ever been reloaded as part of the long term state of a core. Using a flag to decide that we don't need to replay the tlog after a reload, while the reloaded core is initializing is one thing, but what's the point of remembering core A has been reloaded at some point in the past after core A has fully initialized and become active? I guess my point is it seems like a temporary state and not part of the long term state of a core. But that's orthogonal to this issue anyway, so I'll fix the register code and add a test for this. ",
            "id": "comment-14391329"
        },
        {
            "date": "2015-04-01T21:10:20+0000",
            "author": "Mark Miller",
            "content": "The flag is \"has this core been reloaded before\" - how is that a temporary state?\n\nThat whole change I show above was indeed a bug. You can see that in checkRecovery, that is where we deal with skipping the recovery on a reloaded core. The calling code just got mangled. ",
            "id": "comment-14391470"
        },
        {
            "date": "2015-04-01T21:13:18+0000",
            "author": "Mark Miller",
            "content": "A related bug is SOLR-6583.\n\nWe should be skipping tlog replay on 'afterExpiration==true' as well. ",
            "id": "comment-14391475"
        },
        {
            "date": "2015-04-01T21:33:17+0000",
            "author": "Timothy Potter",
            "content": "The flag is \"has this core been reloaded before\" - how is that a temporary state?\n\nI'm not saying it's temporary, I'm saying I can't see what possible value knowing that provides long term? I can see how that would be useful for reporting if it were a timestamp of some sort, such as last reloaded on, but a simple boolean makes no sense to me, nor do I see it being used anywhere in the code other than determining if things like tlog replay should be skipped while the core is initializing. Once it is fully active, the flag seems useless from either a reporting perspective or a state management perspective. But I think we've wasted enough time on this one ... ",
            "id": "comment-14391503"
        },
        {
            "date": "2015-04-01T21:46:38+0000",
            "author": "Mark Miller",
            "content": "I think this is a lot closer to how it's supposed to work, though I'm sure it could all be cleaned up to be a little more clear. ",
            "id": "comment-14391537"
        },
        {
            "date": "2015-04-01T21:49:22+0000",
            "author": "Mark Miller",
            "content": "I'm saying I can't see what possible value knowing that provides long term?\n\nFor example, the value it provides in the specific code we are talking about? If the core has been reloaded and its not afterExpiration, then it is a core reload register call. I don't understand how it doesn't provide value...\n\nI'm not saying it's temporary,\n\nBut you said that a couple times... ",
            "id": "comment-14391543"
        },
        {
            "date": "2015-04-01T21:53:24+0000",
            "author": "Timothy Potter",
            "content": "whatever ... I also said several times I'm talking about after register runs, not during ... of course I see value up until register happens, but you win  ",
            "id": "comment-14391552"
        },
        {
            "date": "2015-04-01T21:57:37+0000",
            "author": "Mark Miller",
            "content": "Also keep in mind, a reloaded SolrCore has differences from a non reloaded core as well - for example, a reloaded core did not create it's index writer or SolrCoreState like a new core does - it inherits them.  ",
            "id": "comment-14391557"
        },
        {
            "date": "2015-04-01T21:59:33+0000",
            "author": "Mark Miller",
            "content": "I also said several times I'm talking about after register runs, not during\n\nI guess I just don't understand that all. Being able to tell if a core is a reloaded core has absolutely nothing to do with register. Register just happens to use that call because it allows register to tell if the register is coming from a new core or a reloaded one.\n\nI am unable to spot the issue. ",
            "id": "comment-14391559"
        },
        {
            "date": "2015-04-01T22:05:24+0000",
            "author": "Mark Miller",
            "content": "If you do a call hierarchy on the SolrCore#isReloaded call, you can find a couple other uses for it as well. It's just kind of info we have to know. I guess we could use a different method of figuring out what we want in register, but this call works, is available, and is generally necessary. ",
            "id": "comment-14391567"
        },
        {
            "date": "2015-04-02T11:42:03+0000",
            "author": "Yonik Seeley",
            "content": "Without looking at the code/patches  I understand what Tim is saying, and agree.  \"isReloaded\" feels more like \"isReloading\" (i.e. it's state that is only temporarily used to make other decisions during initialization.)  I don't know how hard it would be to factor out though... prob not worth it. ",
            "id": "comment-14392603"
        },
        {
            "date": "2015-04-02T13:56:59+0000",
            "author": "Timothy Potter",
            "content": "Not sure if it's useful to you, but here's the unit test I started working on (basically implements the scenario I described in the issue description) ... it currently fails as expected until the register code is fixed up ... it passes with your patch applied though. ",
            "id": "comment-14392724"
        },
        {
            "date": "2015-04-02T14:32:52+0000",
            "author": "Mark Miller",
            "content": "\"isReloaded\" feels more like \"isReloading\"\n\nWhat's the gain, what's the point, what's the alternative?\n\nI don't get that at all. It tells you if the core has been reloaded. This is often useful in things that happen on creating a new SolrCore.\n\nWho cares about isReloading? I'm lost.\n\nIs it just too difficult to understand what isReloaded means?\n\nI'd be more confused with this temporary isReloading call - seems so easy for that to be tricky. isReloaded is so permanent and easy to understand. The core has been reloaded or it hasn't. How the heck does trying to track exactly when the core is actually in the process of reload or not more useful?\n\nAnyone else? ",
            "id": "comment-14392763"
        },
        {
            "date": "2015-04-02T14:38:54+0000",
            "author": "Mark Miller",
            "content": "Anyway, we can spin that off into a new issue if someone wants to try and refactor it. I just don't yet get the impetus for it. Onlookers feel free to chime in here until/unless a new issue is made.\n\nbut here's the unit test I started working on\n\nCool.\n ",
            "id": "comment-14392770"
        },
        {
            "date": "2015-04-03T16:16:06+0000",
            "author": "Timothy Potter",
            "content": "Hi Mark Miller, do you think anything else needs to be done on this one? I'd actually like to get this into the 5.1 release - patch looks good to me. If you're comfortable with the unit test I posted, I can combine them and commit. Thanks. ",
            "id": "comment-14394640"
        },
        {
            "date": "2015-04-03T18:23:37+0000",
            "author": "David Smiley",
            "content": "Here's a question for you Mark Miller: If every core were to be reloaded, would that change anything?  What if I go and do that too all my cores.  Can we just assume that all cores may have been reloaded at some point in the past?  If we do assume that, we do we lose anything?  \u2013 other than complexity  ",
            "id": "comment-14394840"
        },
        {
            "date": "2015-04-03T22:45:03+0000",
            "author": "Mark Miller",
            "content": "I can combine them and commit.\n\nGo ahead. I think that's the right current fix and it also addresses SOLR-6583. ",
            "id": "comment-14395200"
        },
        {
            "date": "2015-04-06T15:42:14+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1671554 from Timothy Potter in branch 'dev/trunk'\n[ https://svn.apache.org/r1671554 ]\n\nSOLR-7338: A reloaded core will never register itself as active after a ZK session expiration, also fixes SOLR-6583 ",
            "id": "comment-14481319"
        },
        {
            "date": "2015-04-06T15:56:58+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1671562 from Timothy Potter in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1671562 ]\n\nSOLR-7338: A reloaded core will never register itself as active after a ZK session expiration, also fixes SOLR-6583 ",
            "id": "comment-14481345"
        },
        {
            "date": "2015-04-06T16:14:27+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1671570 from Timothy Potter in branch 'dev/branches/lucene_solr_5_1'\n[ https://svn.apache.org/r1671570 ]\n\nSOLR-7338: A reloaded core will never register itself as active after a ZK session expiration, also fixes SOLR-6583 ",
            "id": "comment-14481364"
        },
        {
            "date": "2015-04-06T20:51:51+0000",
            "author": "Mark Miller",
            "content": "Given RecoveryZkTest and ChaosMonkey test fails I've seen since this went in, I think it either broke something or exposed something. ",
            "id": "comment-14481910"
        },
        {
            "date": "2015-04-06T21:00:10+0000",
            "author": "Timothy Potter",
            "content": "good catch ... going to try to get the failure to reproduce locally now ",
            "id": "comment-14481921"
        },
        {
            "date": "2015-04-06T21:53:18+0000",
            "author": "Timothy Potter",
            "content": "running beast with 20 iters ... haven't been able to reproduce with 10 ... the logs from the failed test make it seem like the recovery process succeeded OK:\n\n\n[junit4]   2> added docs:1022 with 0 fails deletes:510\n   [junit4]   2> 956835 T6623 C:collection1 S:shard1 c:collection1 oasc.AbstractFullDistribZkTestBase.waitForThingsToLevelOut Wait for recoveries to finish - wait 120 for each attempt\n   [junit4]   2> 956836 T6623 C:collection1 S:shard1 c:collection1 oasc.AbstractDistribZkTestBase.waitForRecoveriesToFinish Wait for recoveries to finish - collection: collection1 failOnTimeout:true timeout (sec):120\n   [junit4]   2> 959842 T6734 C:collection1 S:shard1 c:collection1 C1144 P10059 oasc.RecoveryStrategy.doRecovery Attempting to PeerSync from http://127.0.0.1:10050/p_/z/collection1/ core=collection1 - recoveringAfterStartup=true\n   [junit4]   2> 959843 T6734 C:collection1 S:shard1 c:collection1 C1144 P10059 oasu.PeerSync.sync PeerSync: core=collection1 url=http://127.0.0.1:10059/p_/z START replicas=[http://127.0.0.1:10050/p_/z/collection1/] nUpdates=100\n   [junit4]   2> 959845 T6734 C:collection1 S:shard1 c:collection1 C1144 P10059 oasu.PeerSync.sync WARN no frame of reference to tell if we've missed updates\n   [junit4]   2> 959846 T6734 C:collection1 S:shard1 c:collection1 C1144 P10059 oasc.RecoveryStrategy.doRecovery PeerSync Recovery was not successful - trying replication. core=collection1\n   [junit4]   2> 959846 T6734 C:collection1 S:shard1 c:collection1 C1144 P10059 oasc.RecoveryStrategy.doRecovery Recovery was cancelled\n   [junit4]   2> 959846 T6734 C:collection1 S:shard1 c:collection1 C1144 P10059 oasc.RecoveryStrategy.doRecovery Finished recovery process. core=collection1\n   [junit4]   2> 959846 T6737 oasc.ActionThrottle.minimumWaitBetweenActions The last recovery attempt started 7015ms ago.\n   [junit4]   2> 959846 T6737 oasc.ActionThrottle.minimumWaitBetweenActions Throttling recovery attempts - waiting for 2984ms\n   [junit4]   2> 959847 T6672 C:collection1 S:shard1 R:core_node1 c:collection1 C1145 P10050 oasc.SolrCore.execute [collection1] webapp=/p_/z path=/get params={version=2&getVersions=100&distrib=false&qt=/get&wt=javabin} status=0 QTime=1 \n   [junit4]   2> 962833 T6739 C1144 P10059 oasc.RecoveryStrategy.run Starting recovery process.  core=collection1 recoveringAfterStartup=false\n   [junit4]   2> 962834 T6739 C1144 P10059 oasc.RecoveryStrategy.doRecovery Publishing state of core collection1 as recovering, leader is http://127.0.0.1:10050/p_/z/collection1/ and I am http://127.0.0.1:10059/p_/z/collection1/\n   [junit4]   2> 962834 T6739 C:collection1 c:collection1 C1144 P10059 oasc.ZkController.publish publishing core=collection1 state=recovering collection=collection1\n   [junit4]   2> 962834 T6739 C:collection1 c:collection1 C1144 P10059 oasc.ZkController.publish numShards not found on descriptor - reading it from system property\n   [junit4]   2> 962836 T6649 oasc.DistributedQueue$LatchWatcher.process NodeChildrenChanged fired on path /overseer/queue state SyncConnected\n   [junit4]   2> 962837 T6650 oasc.Overseer$ClusterStateUpdater.run processMessage: queueSize: 1, message = {\n   [junit4]   2> \t  \"collection\":\"collection1\",\n   [junit4]   2> \t  \"core_node_name\":\"core_node2\",\n   [junit4]   2> \t  \"state\":\"recovering\",\n   [junit4]   2> \t  \"shard\":\"shard1\",\n   [junit4]   2> \t  \"base_url\":\"http://127.0.0.1:10059/p_/z\",\n   [junit4]   2> \t  \"roles\":null,\n   [junit4]   2> \t  \"node_name\":\"127.0.0.1:10059_p_%2Fz\",\n   [junit4]   2> \t  \"core\":\"collection1\",\n   [junit4]   2> \t  \"operation\":\"state\",\n   [junit4]   2> \t  \"numShards\":\"1\"} current state version: 5\n   [junit4]   2> 962837 T6650 oasco.ReplicaMutator.updateState Update state numShards=1 message={\n   [junit4]   2> \t  \"collection\":\"collection1\",\n   [junit4]   2> \t  \"core_node_name\":\"core_node2\",\n   [junit4]   2> \t  \"state\":\"recovering\",\n   [junit4]   2> \t  \"shard\":\"shard1\",\n   [junit4]   2> \t  \"base_url\":\"http://127.0.0.1:10059/p_/z\",\n   [junit4]   2> \t  \"roles\":null,\n   [junit4]   2> \t  \"node_name\":\"127.0.0.1:10059_p_%2Fz\",\n   [junit4]   2> \t  \"core\":\"collection1\",\n   [junit4]   2> \t  \"operation\":\"state\",\n   [junit4]   2> \t  \"numShards\":\"1\"}\n   [junit4]   2> 962837 T6739 C:collection1 S:shard1 c:collection1 C1144 P10059 oasc.RecoveryStrategy.sendPrepRecoveryCmd Sending prep recovery command to http://127.0.0.1:10050/p_/z; WaitForState: action=PREPRECOVERY&core=collection1&nodeName=127.0.0.1%3A10059_p_%252Fz&coreNodeName=core_node2&state=recovering&checkLive=true&onlyIfLeader=true&onlyIfLeaderActive=true\n   [junit4]   2> 962838 T6650 oasco.ZkStateWriter.writePendingUpdates going to update_collection /collections/collection1/state.json version: 10\n   [junit4]   2> 962839 T6671 oasha.CoreAdminHandler.handleWaitForStateAction Going to wait for coreNodeName: core_node2, state: recovering, checkLive: true, onlyIfLeader: true, onlyIfLeaderActive: true\n   [junit4]   2> 962839 T6678 oascc.ZkStateReader$7.process A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/collections/collection1/state.json for collection collection1 has occurred - updating... (live nodes size: 3)\n   [junit4]   2> 962839 T6728 C:collection1 S:shard1 c:collection1 oascc.ZkStateReader$7.process A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/collections/collection1/state.json for collection collection1 has occurred - updating... (live nodes size: 3)\n   [junit4]   2> 962840 T6728 C:collection1 S:shard1 c:collection1 oascc.ZkStateReader.updateWatchedCollection Updating data for collection1 to ver 11 \n   [junit4]   2> 962841 T6671 oascc.ZkStateReader.updateWatchedCollection Updating data for collection1 to ver 11 \n   [junit4]   2> 962841 T6671 oasha.CoreAdminHandler.handleWaitForStateAction Will wait a max of 183 seconds to see collection1 (shard1 of collection1) have state: recovering\n   [junit4]   2> 962842 T6678 oascc.ZkStateReader.updateWatchedCollection Updating data for collection1 to ver 11 \n   [junit4]   2> 962842 T6671 oasha.CoreAdminHandler.handleWaitForStateAction In WaitForState(recovering): collection=collection1, shard=shard1, thisCore=collection1, leaderDoesNotNeedRecovery=false, isLeader? true, live=true, checkLive=true, currentState=recovering, localState=active, nodeName=127.0.0.1:10059_p_%2Fz, coreNodeName=core_node2, onlyIfActiveCheckResult=false, nodeProps: core_node2:{\"state\":\"recovering\",\"base_url\":\"http://127.0.0.1:10059/p_/z\",\"node_name\":\"127.0.0.1:10059_p_%2Fz\",\"core\":\"collection1\"}\n   [junit4]   2> 962842 T6671 oasha.CoreAdminHandler.handleWaitForStateAction Waited coreNodeName: core_node2, state: recovering, checkLive: true, onlyIfLeader: true for: 0 seconds.\n   [junit4]   2> 962843 T6671 oass.SolrDispatchFilter.handleAdminRequest [admin] webapp=null path=/admin/cores params={version=2&onlyIfLeader=true&checkLive=true&coreNodeName=core_node2&state=recovering&nodeName=127.0.0.1:10059_p_%252Fz&onlyIfLeaderActive=true&core=collection1&action=PREPRECOVERY&wt=javabin} status=0 QTime=5 \n   [junit4]   2> 969843 T6739 C:collection1 S:shard1 c:collection1 C1144 P10059 oasc.RecoveryStrategy.doRecovery Attempting to PeerSync from http://127.0.0.1:10050/p_/z/collection1/ core=collection1 - recoveringAfterStartup=false\n   [junit4]   2> 969844 T6739 C:collection1 S:shard1 c:collection1 C1144 P10059 oasu.PeerSync.sync PeerSync: core=collection1 url=http://127.0.0.1:10059/p_/z START replicas=[http://127.0.0.1:10050/p_/z/collection1/] nUpdates=100\n   [junit4]   2> 969847 T6672 C:collection1 S:shard1 R:core_node1 c:collection1 C1145 P10050 oasc.SolrCore.execute [collection1] webapp=/p_/z path=/get params={version=2&getVersions=100&distrib=false&qt=/get&wt=javabin} status=0 QTime=0 \n   [junit4]   2> 969848 T6739 C:collection1 S:shard1 c:collection1 C1144 P10059 oasu.PeerSync.handleVersions PeerSync: core=collection1 url=http://127.0.0.1:10059/p_/z  Received 100 versions from http://127.0.0.1:10050/p_/z/collection1/\n   [junit4]   2> 969849 T6739 C:collection1 S:shard1 c:collection1 C1144 P10059 oasu.PeerSync.handleVersions PeerSync: core=collection1 url=http://127.0.0.1:10059/p_/z  Our versions are newer. ourLowThreshold=1497723899508424704 otherHigh=1497723899760082944\n   [junit4]   2> 969849 T6739 C:collection1 S:shard1 c:collection1 C1144 P10059 oasu.PeerSync.sync PeerSync: core=collection1 url=http://127.0.0.1:10059/p_/z DONE. sync succeeded\n   [junit4]   2> 969849 T6739 C:collection1 S:shard1 c:collection1 C1144 P10059 oasu.DirectUpdateHandler2.commit start commit{,optimize=false,openSearcher=true,waitSearcher=true,expungeDeletes=false,softCommit=false,prepareCommit=false}\n   [junit4]   2> 969865 T6739 C:collection1 S:shard1 c:collection1 C1144 P10059 oasc.SolrDeletionPolicy.onCommit SolrDeletionPolicy.onCommit: commits: num=2\n   [junit4]   2> \t\tcommit{dir=MockDirectoryWrapper(RAMDirectory@e80270f lockFactory=org.apache.lucene.store.SingleInstanceLockFactory@7f845138),segFN=segments_1,generation=1}\n   [junit4]   2> \t\tcommit{dir=MockDirectoryWrapper(RAMDirectory@e80270f lockFactory=org.apache.lucene.store.SingleInstanceLockFactory@7f845138),segFN=segments_2,generation=2}\n   [junit4]   2> 969866 T6739 C:collection1 S:shard1 c:collection1 C1144 P10059 oasc.SolrDeletionPolicy.updateCommits newest commit generation = 2\n   [junit4]   2> 969869 T6739 C:collection1 S:shard1 c:collection1 C1144 P10059 oass.SolrIndexSearcher.<init> Opening Searcher@46806cf3[collection1] main\n   [junit4]   2> 969869 T6739 C:collection1 S:shard1 c:collection1 C1144 P10059 oasu.DirectUpdateHandler2.commit end_commit_flush\n   [junit4]   2> 969870 T6730 C:collection1 S:shard1 c:collection1 oasc.SolrCore.registerSearcher [collection1] Registered new searcher Searcher@46806cf3[collection1] main{ExitableDirectoryReader(UninvertingDirectoryReader(Uninverting(_0(5.2.0):C115/43:delGen=1) Uninverting(_1(5.2.0):C360/137:delGen=1)))}\n   [junit4]   2> 969870 T6739 C:collection1 S:shard1 c:collection1 C1144 P10059 oasc.RecoveryStrategy.doRecovery PeerSync Recovery was successful - registering as Active. core=collection1\n   [junit4]   2> 969871 T6739 C:collection1 S:shard1 c:collection1 C1144 P10059 oasc.ZkController.publish publishing core=collection1 state=active collection=collection1\n   [junit4]   2> 969871 T6739 C:collection1 S:shard1 c:collection1 C1144 P10059 oasc.ZkController.publish numShards not found on descriptor - reading it from system property\n   [junit4]   2> 969872 T6649 oasc.DistributedQueue$LatchWatcher.process NodeChildrenChanged fired on path /overseer/queue state SyncConnected\n   [junit4]   2> 969874 T6650 oasc.Overseer$ClusterStateUpdater.run processMessage: queueSize: 1, message = {\n   [junit4]   2> \t  \"collection\":\"collection1\",\n   [junit4]   2> \t  \"core_node_name\":\"core_node2\",\n   [junit4]   2> \t  \"state\":\"active\",\n   [junit4]   2> \t  \"shard\":\"shard1\",\n   [junit4]   2> \t  \"base_url\":\"http://127.0.0.1:10059/p_/z\",\n   [junit4]   2> \t  \"roles\":null,\n   [junit4]   2> \t  \"node_name\":\"127.0.0.1:10059_p_%2Fz\",\n   [junit4]   2> \t  \"core\":\"collection1\",\n   [junit4]   2> \t  \"operation\":\"state\",\n   [junit4]   2> \t  \"numShards\":\"1\"} current state version: 5\n   [junit4]   2> 969875 T6650 oasco.ReplicaMutator.updateState Update state numShards=1 message={\n   [junit4]   2> \t  \"collection\":\"collection1\",\n   [junit4]   2> \t  \"core_node_name\":\"core_node2\",\n   [junit4]   2> \t  \"state\":\"active\",\n   [junit4]   2> \t  \"shard\":\"shard1\",\n   [junit4]   2> \t  \"base_url\":\"http://127.0.0.1:10059/p_/z\",\n   [junit4]   2> \t  \"roles\":null,\n   [junit4]   2> \t  \"node_name\":\"127.0.0.1:10059_p_%2Fz\",\n   [junit4]   2> \t  \"core\":\"collection1\",\n   [junit4]   2> \t  \"operation\":\"state\",\n   [junit4]   2> \t  \"numShards\":\"1\"}\n   [junit4]   2> 969876 T6650 oasco.ZkStateWriter.writePendingUpdates going to update_collection /collections/collection1/state.json version: 11\n   [junit4]   2> 969876 T6728 C:collection1 S:shard1 c:collection1 oascc.ZkStateReader$7.process A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/collections/collection1/state.json for collection collection1 has occurred - updating... (live nodes size: 3)\n   [junit4]   2> 969876 T6678 oascc.ZkStateReader$7.process A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/collections/collection1/state.json for collection collection1 has occurred - updating... (live nodes size: 3)\n   [junit4]   2> 969877 T6728 C:collection1 S:shard1 c:collection1 oascc.ZkStateReader.updateWatchedCollection Updating data for collection1 to ver 12 \n   [junit4]   2> 969877 T6678 oascc.ZkStateReader.updateWatchedCollection Updating data for collection1 to ver 12 \n   [junit4]   2> 970870 T6623 C:collection1 S:shard1 c:collection1 oasc.AbstractDistribZkTestBase.waitForRecoveriesToFinish Recoveries finished - collection: collection1\n   [junit4]   2> 970873 T6638 C:control_collection S:shard1 R:core_node1 c:collection1 C1146 P10043 oasu.DirectUpdateHandler2.commit start commit{,optimize=false,openSearcher=true,waitSearcher=true,expungeDeletes=false,softCommit=false,prepareCommit=false}\n   [junit4]   2> 970920 T6638 C:control_collection S:shard1 R:core_node1 c:collection1 C1146 P10043 oasc.SolrDeletionPolicy.onCommit SolrDeletionPolicy.onCommit: commits: num=2\n   [junit4]   2> \t\tcommit{dir=MockDirectoryWrapper(RAMDirectory@25082424 lockFactory=org.apache.lucene.store.SingleInstanceLockFactory@397cca7a),segFN=segments_1,generation=1}\n   [junit4]   2> \t\tcommit{dir=MockDirectoryWrapper(RAMDirectory@25082424 lockFactory=org.apache.lucene.store.SingleInstanceLockFactory@397cca7a),segFN=segments_2,generation=2}\n   [junit4]   2> 970921 T6638 C:control_collection S:shard1 R:core_node1 c:collection1 C1146 P10043 oasc.SolrDeletionPolicy.updateCommits newest commit generation = 2\n   [junit4]   2> 970924 T6638 C:control_collection S:shard1 R:core_node1 c:collection1 C1146 P10043 oass.SolrIndexSearcher.<init> Opening Searcher@1508b423[collection1] main\n   [junit4]   2> 970925 T6638 C:control_collection S:shard1 R:core_node1 c:collection1 C1146 P10043 oasu.DirectUpdateHandler2.commit end_commit_flush\n   [junit4]   2> 970926 T6654 C:control_collection c:collection1 oasc.SolrCore.registerSearcher [collection1] Registered new searcher Searcher@1508b423[collection1] main{ExitableDirectoryReader(UninvertingDirectoryReader(Uninverting(_0(5.2.0):C1303/622:delGen=1) Uninverting(_1(5.2.0):C740/398:delGen=1)))}\n   [junit4]   2> 970926 T6638 C:control_collection S:shard1 R:core_node1 c:collection1 C1146 P10043 oasup.LogUpdateProcessor.finish [collection1] webapp=/p_/z path=/update params={version=2&waitSearcher=true&softCommit=false&wt=javabin&commit=true} {commit=} 0 53\n   [junit4]   2> 970931 T6668 C:collection1 S:shard1 R:core_node1 c:collection1 C1145 P10050 oasu.DirectUpdateHandler2.commit start commit{,optimize=false,openSearcher=true,waitSearcher=true,expungeDeletes=false,softCommit=false,prepareCommit=false}\n   [junit4]   2>  C1144_STATE=coll:collection1 core:collection1 props:{state=active, base_url=http://127.0.0.1:10059/p_/z, node_name=127.0.0.1:10059_p_%2Fz, core=collection1}\n   [junit4]   2> 970932 T6717 C:collection1 S:shard1 R:core_node2 c:collection1 C1144 P10059 oasu.DirectUpdateHandler2.commit start commit{,optimize=false,openSearcher=true,waitSearcher=true,expungeDeletes=false,softCommit=false,prepareCommit=false}\n   [junit4]   2> 970933 T6717 C:collection1 S:shard1 R:core_node2 c:collection1 C1144 P10059 oasu.DirectUpdateHandler2.commit No uncommitted changes. Skipping IW.commit.\n   [junit4]   2> 970933 T6717 C:collection1 S:shard1 R:core_node2 c:collection1 C1144 P10059 oasc.SolrCore.openNewSearcher SolrIndexSearcher has not changed - not re-opening: org.apache.solr.search.SolrIndexSearcher\n   [junit4]   2> 970934 T6717 C:collection1 S:shard1 R:core_node2 c:collection1 C1144 P10059 oasu.DirectUpdateHandler2.commit end_commit_flush\n   [junit4]   2> 970934 T6717 C:collection1 S:shard1 R:core_node2 c:collection1 C1144 P10059 oasup.LogUpdateProcessor.finish [collection1] webapp=/p_/z path=/update params={version=2&commit_end_point=true&waitSearcher=true&update.distrib=FROMLEADER&openSearcher=true&distrib.from=http://127.0.0.1:10050/p_/z/collection1/&expungeDeletes=false&softCommit=false&wt=javabin&commit=true} {commit=} 0 2\n   [junit4]   2> 970978 T6668 C:collection1 S:shard1 R:core_node1 c:collection1 C1145 P10050 oasc.SolrDeletionPolicy.onCommit SolrDeletionPolicy.onCommit: commits: num=2\n   [junit4]   2> \t\tcommit{dir=MockDirectoryWrapper(RAMDirectory@5b80c596 lockFactory=org.apache.lucene.store.SingleInstanceLockFactory@632c29c8),segFN=segments_1,generation=1}\n   [junit4]   2> \t\tcommit{dir=MockDirectoryWrapper(RAMDirectory@5b80c596 lockFactory=org.apache.lucene.store.SingleInstanceLockFactory@632c29c8),segFN=segments_2,generation=2}\n   [junit4]   2> 970979 T6668 C:collection1 S:shard1 R:core_node1 c:collection1 C1145 P10050 oasc.SolrDeletionPolicy.updateCommits newest commit generation = 2\n   [junit4]   2> 970982 T6668 C:collection1 S:shard1 R:core_node1 c:collection1 C1145 P10050 oass.SolrIndexSearcher.<init> Opening Searcher@4273e191[collection1] main\n   [junit4]   2> 970983 T6668 C:collection1 S:shard1 R:core_node1 c:collection1 C1145 P10050 oasu.DirectUpdateHandler2.commit end_commit_flush\n   [junit4]   2> 970983 T6680 C:collection1 c:collection1 oasc.SolrCore.registerSearcher [collection1] Registered new searcher Searcher@4273e191[collection1] main{ExitableDirectoryReader(UninvertingDirectoryReader(Uninverting(_0(5.2.0):C861/415:delGen=1) Uninverting(_1(5.2.0):C1182/605:delGen=1)))}\n   [junit4]   2> 970984 T6668 C:collection1 S:shard1 R:core_node1 c:collection1 C1145 P10050 oasup.LogUpdateProcessor.finish [collection1] webapp=/p_/z path=/update params={version=2&commit_end_point=true&waitSearcher=true&update.distrib=FROMLEADER&openSearcher=true&distrib.from=http://127.0.0.1:10050/p_/z/collection1/&expungeDeletes=false&softCommit=false&wt=javabin&commit=true} {commit=} 0 53\n   [junit4]   2> 970986 T6670 C:collection1 S:shard1 R:core_node1 c:collection1 C1145 P10050 oasup.LogUpdateProcessor.finish [collection1] webapp=/p_/z path=/update params={version=2&waitSearcher=true&_stateVer_=collection1:6&softCommit=false&wt=javabin&commit=true} {commit=} 0 57\n   [junit4]   2> 970991 T6666 C:collection1 S:shard1 R:core_node1 c:collection1 C1145 P10050 oasc.SolrCore.execute [collection1] webapp=/p_/z path=/select params={version=2&tests=checkShardConsistency&distrib=false&rows=0&q=*:*&wt=javabin} hits=1023 status=0 QTime=1 \n   [junit4]   2> 970993 T6721 C:collection1 S:shard1 R:core_node2 c:collection1 C1144 P10059 oasc.SolrCore.execute [collection1] webapp=/p_/z path=/select params={version=2&tests=checkShardConsistency&distrib=false&rows=0&q=*:*&wt=javabin} hits=295 status=0 QTime=0 \n   [junit4]   2> 970994 T6623 C:collection1 S:shard1 c:collection1 oasc.AbstractFullDistribZkTestBase.waitForThingsToLevelOut shard inconsistency - waiting ...\n\n ",
            "id": "comment-14482033"
        },
        {
            "date": "2015-04-06T22:15:01+0000",
            "author": "Timothy Potter",
            "content": "hmmm ... no failures with -Dbeast.iters=20 either ... not much jumping out at me in the logs either. The replica with missing data definitely received the update that is being reported at the end of the test as missing, for instance:\n\n\n###### Only in http://127.0.0.1:10050/p_/z/collection1: [{_version_=1497723893329166337, id=2-472}\n\n\n\nbut earlier in the logs, we see:\n\n\n[junit4]   2> 949616 T6637 C:control_collection S:shard1 R:core_node1 c:collection1 C1136 P10043 oasup.LogUpdateProcessor.finish [collection1] webapp=/p_/z path=/update params={version=2&CONTROL=TRUE&wt=javabin} {add=[2-472 (1497723893326020609)]} 0 0\n   [junit4]   2> 949621 T6686 C:collection1 S:shard1 R:core_node2 c:collection1 C1134 P10059 oasup.LogUpdateProcessor.finish [collection1] webapp=/p_/z path=/update params={version=2&update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:10050/p_/z/collection1/&wt=javabin} {add=[2-472 (1497723893329166337)]} 0 0\n   [junit4]   2> 949622 T6670 C:collection1 S:shard1 R:core_node1 c:collection1 C1135 P10050 oasup.LogUpdateProcessor.finish [collection1] webapp=/p_/z path=/update params={version=2&wt=javabin} {add=[2-472 (1497723893329166337)]} 0 3\n\n\n\nI used the same seed as the failed build as well - FBFBECE5D5AABA29\n\nYou see anything on your side Mark Miller? ",
            "id": "comment-14482068"
        },
        {
            "date": "2015-04-06T22:51:04+0000",
            "author": "Mark Miller",
            "content": "A lot of my ChaosMonkey test runs on my local jenkins machine starting failing after that commit. I have not had a chance to dig into the logs yet though. ",
            "id": "comment-14482127"
        },
        {
            "date": "2015-04-07T01:51:54+0000",
            "author": "Mark Miller",
            "content": "May have been coincidental fails (still, bad new(ish) replicas out of sync fails) - I'll spend some time tomorrow looking closer and post what I find or close this issue again.  ",
            "id": "comment-14482399"
        },
        {
            "date": "2015-04-07T02:59:46+0000",
            "author": "Timothy Potter",
            "content": "Ok cool - I have RC built for 5.1 with this fix in, but will do it if needed ... can't get it to reproduce with 50 beasts!\n\n\nant beast -Dbeast.iters=50  -Dtestcase=RecoveryZkTest -Dtests.method=test -Dtests.seed=FBFBECE5D5AABA29 -Dtests.multiplier=2 -Dtests.slow=true -Dtests.locale=ar_SD -Dtests.timezone=America/Marigot -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n\n ",
            "id": "comment-14482493"
        },
        {
            "date": "2015-04-07T20:40:05+0000",
            "author": "Timothy Potter",
            "content": "Mark Miller I don't think the RecoveryZkTest failure is due to this ticket as it failed in a similar fashion prior to that commit:\nhttps://builds.apache.org/job/Lucene-Solr-Tests-5.x-Java7/2888/\n\nI've actually beast'd that test with 5.1 100 times w/o failure so I'd like to move forward with the RC candidate I've already built and staged unless you advise otherwise  ",
            "id": "comment-14484005"
        },
        {
            "date": "2015-04-07T20:45:38+0000",
            "author": "Mark Miller",
            "content": "Same status as yesterday - I'll look into this today. ",
            "id": "comment-14484013"
        },
        {
            "date": "2015-04-07T21:16:37+0000",
            "author": "Mark Miller",
            "content": "Okay - I've looked through my Jenkins logs. There are some concerning new fails, but just looks like coincidence that they and the regular jenkins 'replica out of sync' popped up together after this commit. Nothing to indicate an ongoing problem started by this commit looking deeper. ",
            "id": "comment-14484064"
        },
        {
            "date": "2015-04-15T00:30:53+0000",
            "author": "Timothy Potter",
            "content": "Bulk close after 5.1 release ",
            "id": "comment-14495382"
        }
    ]
}