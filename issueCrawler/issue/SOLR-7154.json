{
    "id": "SOLR-7154",
    "title": "Wildcard query matches special characters",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Minor"
    },
    "description": "I have a string field raw_name defined like this:\n\n\n<fieldType name=\"string\" class=\"solr.StrField\" sortMissingLast=\"true\" omitNorms=\"true\"/>\n...\n<field name=\"raw_name\" type=\"string\" indexed=\"true\" stored=\"true\" />\n\n\n\nI have a document like this:\n\n{raw_name: beyonce\u0301}\n\n\nNotice that the last character is a special character (accented e).\n\nWhen I issue this wildcard query:\n\nq=raw_name:beyonce*\n\n\ni.e. with the last character simply being the ASCII 'e', Solr returns me the above document.\n\nExact query:\n\n/select?q=raw_name:beyonce*&wt=json&fl=raw_name\n\n\n\nResponse:\n\n\n{\n  \"responseHeader\": {\n    \"status\": 0,\n    \"QTime\": 0,\n    \"params\": {\n      \"fl\": \"raw_name\",\n      \"q\": \"raw_name:beyonce*\",\n      \"wt\": \"json\"\n    }\n  },\n  \"response\": {\n    \"numFound\": 2,\n    \"start\": 0,\n    \"docs\": [\n      {\n        \"raw_name\": \"beyonce\u0301\"\n      },\n      {\n        \"raw_name\": \"beyonce\u0301\"\n      }\n    ]\n  }\n}\n\n\n\nI used the analysis tool in Solr admin (with Jetty). The raw bytes look like this:\n\nRaw bytes for beyonce: [62 65 79 6f 6e 63 65]\nRaw bytes for beyonce\u0301: [62 65 79 6f 6e 63 65 cc 81]\n\nSo when you look at the bytes, it seems to explain why beyonce* might match beyonce\u0301.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2015-02-24T20:22:02+0000",
            "author": "Alexandre Rafalovitch",
            "content": "Which version of Solr is this against. This needs to be tested against version 5 or at least 4.10.3 to be actionable.\n\nAlso, the definition of the bug seems simple enough to build a simple complete use case. Would be useful to have that. ",
            "id": "comment-14335379"
        },
        {
            "date": "2015-02-24T20:22:13+0000",
            "author": "Toke Eskildsen",
            "content": "To quote Steve Jobs: \"You are holding it wrong\": Your Beyonce\u0301 should be Beyonc\u00e9.\n\nThe difference between the two e's is that the first one has the ping added with the unicode \"Combining acute accent\" (http://www.fileformat.info/info/unicode/char/0301/index.htm), while the second one is a \"Latin small letter with acute\" (http://www.fileformat.info/info/unicode/char/e9/index.htm).\n\nA proper normalizer would convert e\u0301 and \u00e9 into the same character, but you are using the raw string, so you do not have that luxury. If you use a text field, you can avoid this by normalising into letters with build-in diacritics (as opposed to letters followed with combining diacritics). Unfortunately that does not work well if the user query contains a truncation with combining diacritics, as truncated queries are not normalized (which I think they should, but that is a matter for another JIRA). ",
            "id": "comment-14335381"
        },
        {
            "date": "2015-02-24T20:56:50+0000",
            "author": "Arun Rangarajan",
            "content": "I had initially done this on Solr 4.2.1. After seeing your comment, I tried the same on Solr 5.0.0 and it gives the same results. ",
            "id": "comment-14335436"
        },
        {
            "date": "2015-02-24T21:05:16+0000",
            "author": "Arun Rangarajan",
            "content": "Right, that seems to be the issue. I added another document with \"Latin small letter with acute\" and that document does not match the wild-card query. So I think this needs to be fixed in my data source itself. ",
            "id": "comment-14335452"
        },
        {
            "date": "2015-02-24T21:08:50+0000",
            "author": "Alexandre Rafalovitch",
            "content": "Either in data source itself or by switching to a text analyzer with KeywordTokenizer and ICU normalization in the pipeline. Still need to be careful with multi-word expansion, you may need to define multiterm chain explicitly as well (needs to be tested).\n\nRegards,\n  Alex. ",
            "id": "comment-14335460"
        }
    ]
}