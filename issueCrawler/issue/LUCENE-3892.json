{
    "id": "LUCENE-3892",
    "title": "Add a useful intblock postings format (eg, FOR, PFOR, PFORDelta, Simple9/16/64, etc.)",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [
            "4.0",
            "6.0"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "On the flex branch we explored a number of possible intblock\nencodings, but for whatever reason never brought them to completion.\nThere are still a number of issues opened with patches in different\nstates.\n\nInitial results (based on prototype) were excellent (see\nhttp://blog.mikemccandless.com/2010/08/lucene-performance-with-pfordelta-codec.html\n).\n\nI think this would make a good GSoC project.",
    "attachments": {
        "LUCENE-3892-blockFor&hardcode(base).patch": "https://issues.apache.org/jira/secure/attachment/12538373/LUCENE-3892-blockFor%26hardcode%28base%29.patch",
        "LUCENE-3892-for&pfor-with-javadoc.patch": "https://issues.apache.org/jira/secure/attachment/12536431/LUCENE-3892-for%26pfor-with-javadoc.patch",
        "LUCENE-3892_for_unfold_method.patch": "https://issues.apache.org/jira/secure/attachment/12532691/LUCENE-3892_for_unfold_method.patch",
        "LUCENE-3892-blockFor-with-packedints-decoder.patch": "https://issues.apache.org/jira/secure/attachment/12537371/LUCENE-3892-blockFor-with-packedints-decoder.patch",
        "LUCENE-3892-blockpfor.patch": "https://issues.apache.org/jira/secure/attachment/12540045/LUCENE-3892-blockpfor.patch",
        "LUCENE-3892-pfor-compress-iterate-numbits.patch": "https://issues.apache.org/jira/secure/attachment/12536548/LUCENE-3892-pfor-compress-iterate-numbits.patch",
        "LUCENE-3892_for_byte[].patch": "https://issues.apache.org/jira/secure/attachment/12532905/LUCENE-3892_for_byte%5B%5D.patch",
        "LUCENE-3892-blockFor-with-packedints.patch": "https://issues.apache.org/jira/secure/attachment/12537346/LUCENE-3892-blockFor-with-packedints.patch",
        "LUCENE-3892-pfor-compress-slow-estimate.patch": "https://issues.apache.org/jira/secure/attachment/12536549/LUCENE-3892-pfor-compress-slow-estimate.patch",
        "LUCENE-3892-BlockTermScorer.patch": "https://issues.apache.org/jira/secure/attachment/12533086/LUCENE-3892-BlockTermScorer.patch",
        "LUCENE-3892-trunk.patch": "https://issues.apache.org/jira/secure/attachment/12541482/LUCENE-3892-trunk.patch",
        "LUCENE-3892-handle_open_files.patch": "https://issues.apache.org/jira/secure/attachment/12535698/LUCENE-3892-handle_open_files.patch",
        "LUCENE-3892_for_int[].patch": "https://issues.apache.org/jira/secure/attachment/12532888/LUCENE-3892_for_int%5B%5D.patch",
        "LUCENE-3892-liveregs.patch": "https://issues.apache.org/jira/secure/attachment/12542291/LUCENE-3892-liveregs.patch",
        "LUCENE-3892_settings.patch": "https://issues.apache.org/jira/secure/attachment/12524461/LUCENE-3892_settings.patch",
        "LUCENE-3892-non-specialized.patch": "https://issues.apache.org/jira/secure/attachment/12539952/LUCENE-3892-non-specialized.patch",
        "LUCENE-3892-bulkVInt.patch": "https://issues.apache.org/jira/secure/attachment/12539881/LUCENE-3892-bulkVInt.patch",
        "LUCENE-3892-javadocs.patch": "https://issues.apache.org/jira/secure/attachment/12541479/LUCENE-3892-javadocs.patch",
        "LUCENE-3892_pfor_unfold_method.patch": "https://issues.apache.org/jira/secure/attachment/12532690/LUCENE-3892_pfor_unfold_method.patch",
        "LUCENE-3892-direct-IntBuffer.patch": "https://issues.apache.org/jira/secure/attachment/12532679/LUCENE-3892-direct-IntBuffer.patch",
        "LUCENE-3892-blockFor&packedecoder(comp).patch": "https://issues.apache.org/jira/secure/attachment/12538374/LUCENE-3892-blockFor%26packedecoder%28comp%29.patch",
        "LUCENE-3892_pulsing_support.patch": "https://issues.apache.org/jira/secure/attachment/12535700/LUCENE-3892_pulsing_support.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-03-28T13:38:44+0000",
            "content": "Hi, \nI have submitted my proposal. Comments are welcome!\nAlso, I made it public: http://www.google-melange.com/gsoc/proposal/review/google/gsoc2012/billybob/1 ",
            "author": "Han Jiang",
            "id": "comment-13240403"
        },
        {
            "date": "2012-03-28T16:46:41+0000",
            "content": "That's great Han, I'll have a look.\n\nI can be a mentor for this... ",
            "author": "Michael McCandless",
            "id": "comment-13240527"
        },
        {
            "date": "2012-04-03T14:55:36+0000",
            "content": "The proposal at\nhttp://www.google-melange.com/gsoc/proposal/review/google/gsoc2012/billybob/1\nlooks great!  Some initial feedback:\n\n\n\tThere are actually more than 2 codecs (eg we also have Lucene3x,\n    SimpleText, sep/intblock (abstract), random codecs/postings\n    formats for testing...), but our default codec now is Lucene40.\n\n\n\n\n\tI think you can use the existing abstract sep/intblock classes\n    (ie, they implement layers like FieldsProducer/Consumer...), and\n    then you can \"just\" implement the required methods (eg to\n    encode/decode one int[] block).\n\n\n\n\n\tWe may need to tune the skipper settings, based on profiling\n    results from skip-intensive (Phrase, And) queries... since it's\n    currently geared towards single-doc-at-once encoding.  I don't think\n    we should try to make a new skipper impl here... (there is a separate\n    issue for that).\n\n\n\n\n\tMaybe explore the combination of pulsing and PForDelta codecs;\n    seems like the combination of those two could be important, since\n    for low docFreq terms, retrieving the docs is now more\n    expensive...\n\n ",
            "author": "Michael McCandless",
            "id": "comment-13245374"
        },
        {
            "date": "2012-04-05T12:02:39+0000",
            "content": "Hi Mike,\nI have changed my proposal a bit, but here are some questions here:\n\n\n\n\tThere are actually more than 2 codecs (eg we also have Lucene3x,\nSimpleText, sep/intblock (abstract), random codecs/postings\nformats for testing...), but our default codec now is Lucene40.\n\n\n\nYes, but it seems that our baseline will be Lucene40 and Pulsing? Lucene3x is read-only, and other approaches are not productive.\nAnd, what is random codec? Does it mean to randomly pick up a codec for user?\n\n\n\n\tI think you can use the existing abstract sep/intblock classes\n(ie, they implement layers like FieldsProducer/Consumer...), and\nthen you can \"just\" implement the required methods (eg to\nencode/decode one int[] block).\n\n\n\nAnd this was my initial thought about the PForDelta interface:\n\nThe class hierarchy will be as below (quite similar to pulsing):\n\n\tPForDeltaPostingsFormat(extends PostingsFormat):\n   \tIt will define global behaviors such as file suffix, and provide customized FieldsWriter/Reader\n\tPForDeltaFieldsWriter(extends FieldsConsumer):\n    \tIt will define how terms,docids,freq,offset are written into posting files.\n    \tinner classes include: \n\t\n\t\tPForDeltaTermsConsumer(extends TermsConsumer)\n\t\tPForDeltaPostingsConsumer(extends PostingsConsumer)\n\t\n\t\n\tPForDeltaFieldsReader(extends FieldsProducer):\n    \tIt will define how postings are read from index, and provide *Enum class to iterate docids, freqs etc.\n    \tinner classes include:\n\t\n\t\tPForDeltaFieldsEnum(extends FieldsEnum)\n\t\tPForDeltaTermsEnum(extends TermsEnum)\n\t\tPForDeltaDocsEnum(extends DocsEnum)\n\t\tPForDeltaDocsAndPositonsEnum(extends DocsAndPostionsEnum)\n\t\tPForDeltaTerms(extends Terms)\n\t\n\t\n\n\n\nIt seems that \"BlockTermsReader/Writer\" have already implement those subclasses, and we can just pass our Postings(Writer/Reader)Base as an argument, like PatchedFrameOfRefCodec::fieldsConsumer() does.\nThen, to introduce PForDeltaCodec into trunk, we should also introduce the \"fixed codec\"? Also, why isn't lucene40codec implemented with this line? \n\n\n\n\tWe may need to tune the skipper settings, based on profiling\nresults from skip-intensive (Phrase, And) queries... since it's\ncurrently geared towards single-doc-at-once encoding. I don't think\nwe should try to make a new skipper impl here... (there is a separate\nissue for that).\n\n\n\nIt seems that skip settings are not so related to backend codec? Do you mean the nocommit line in FixedPostingsWriterImpl.java:117 ?\n\n\n\n\tMaybe explore the combination of pulsing and PForDelta codecs;\nseems like the combination of those two could be important, since\nfor low docFreq terms, retrieving the docs is now more\nexpensive...\n\n\n\nYes, it seems that if PForDelta outperforms current approaches, a Pulsing version will work better. This feature will also come as \"phase 2\". ",
            "author": "Han Jiang",
            "id": "comment-13247175"
        },
        {
            "date": "2012-04-24T02:05:28+0000",
            "content": "Thank all of you for providing me this opportunity! Let us begin! ",
            "author": "Han Jiang",
            "id": "comment-13260149"
        },
        {
            "date": "2012-04-24T10:41:12+0000",
            "content": "Hi Billy, I'm very excited your proposal is accepted!  Congrats   Now the fun work begins... ",
            "author": "Michael McCandless",
            "id": "comment-13260422"
        },
        {
            "date": "2012-04-26T16:04:26+0000",
            "content": "It's quite strange that sometimes I cannot access repo1.maven.org, therefore \"ant ivy-boostrap\" & \"ant resolve\" will fail to work.(Since I'm in China, the network connection might be limited).\n\nOnce Mike and I hoped to make things work by configuring \"lucene/common-build.xml\" & \"dev-tools/scripts/poll-mirrors.pl\" to another maven mirror, listed in http://docs.codehaus.org/display/MAVENUSER/Mirrors+Repositories. Unfortunately, the main site \"repo1.maven.org\" was configured into ivy-2.2.0.jar, and even we pass \"ant ivy-bootstrap\", \"ant resolve\" still fails.\n\nWell, here is how I get things work(too ugly, hope a better suggestion!):\n\nchange /etc/hosts,\nand redirect current maven site to a mirror with same directory structure, for example: \n\n194.8.197.22    repo1.maven.org # to http://mirror.netcologne.de/ ",
            "author": "Han Jiang",
            "id": "comment-13262694"
        },
        {
            "date": "2012-04-26T16:13:09+0000",
            "content": "Phew, I'm glad to hear you got it working!  So \"ant resolve\" finished successfully? ",
            "author": "Michael McCandless",
            "id": "comment-13262701"
        },
        {
            "date": "2012-04-26T16:17:53+0000",
            "content": "Yes, and \"ant test\" is running now. Maybe we can configure something to avoid the ugly hack? ",
            "author": "Han Jiang",
            "id": "comment-13262707"
        },
        {
            "date": "2012-04-26T16:23:12+0000",
            "content": "Maybe a good solution is if we have an ant property (that we somehow pass to ivy), and\nwe conditionally set it in ant by default to a server we know that works in china,\nif the \"${user.language}\"=\"zh\" ? ",
            "author": "Robert Muir",
            "id": "comment-13262711"
        },
        {
            "date": "2012-04-26T16:27:03+0000",
            "content": "Thank you, Robert! But currently, the maven mirror in China(http://mirrors.redv.com/maven2) is not available. And can we pass a property to ivy to replace the \"repo1*\" stuff? ",
            "author": "Han Jiang",
            "id": "comment-13262715"
        },
        {
            "date": "2012-04-26T16:59:30+0000",
            "content": "can you remove your hack and try this patch? ",
            "author": "Robert Muir",
            "id": "comment-13262741"
        },
        {
            "date": "2012-04-26T17:29:44+0000",
            "content": "Thank you Robert! The patch works well.  ",
            "author": "Han Jiang",
            "id": "comment-13262772"
        },
        {
            "date": "2012-04-26T17:32:00+0000",
            "content": "Patch does not yet fix ivy-bootstrap. Ivy-bootstrap still only tries repo1.maven.org. We need a different strategy for that: either we depend on try-catch from ant contrib (undesired), use custom ant task (grrrr), or use a chain of targets with fail-on-error=false unless the file already exists and checksum at the end... Lemme see if i can fix ivy-bootstrap, too! ",
            "author": "Robert Muir",
            "id": "comment-13262777"
        },
        {
            "date": "2012-04-26T18:16:45+0000",
            "content": "updated patch with also logic for ivy-bootstrap. if repo1.maven.org fails, we try the same china-friendly mirror (currently http://mirror.netcologne.de/maven2). We disable fail-on-error, instead sha1-checksum the result at the end to determine real success or not (and if it fails that, prints a message suggesting you manually download it) ",
            "author": "Robert Muir",
            "id": "comment-13262815"
        },
        {
            "date": "2012-04-26T18:32:39+0000",
            "content": "I will commit this patch: please let us know if you have more problems from china!  ",
            "author": "Robert Muir",
            "id": "comment-13262835"
        },
        {
            "date": "2012-04-27T02:00:58+0000",
            "content": "OK, and thanks for the new commit! ",
            "author": "Han Jiang",
            "id": "comment-13263302"
        },
        {
            "date": "2012-05-01T18:12:44+0000",
            "content": "A postings format named VSEncoding also seems promising! \n\nIt is available here: http://integerencoding.isti.cnr.it/\n\nAnd license compatible: https://github.com/maropu/integer_encoding_library/blob/master/LICENSE ",
            "author": "Han Jiang",
            "id": "comment-13265950"
        },
        {
            "date": "2012-06-02T09:30:39+0000",
            "content": "Here is a initial implementation of PForPostingsFormat. It is registered in oal.codecs.mockrandom.MockRandomPostingsFormat, and all tests have passed (Maybe I should modify some other mock files as well?).\n\nThis version is orginally inspired by the pfor and pfor2 impls in bulk_branch, mostly by the idea of pfor. Currently, the compressed data consists of three parts: header, normal area, and excpetion area. The normal area encodes each small  value as b bits, as well as exception values. The exception area stores each large value directly, possibly as 8,16,or 32 bits. NumFrameBits range from 1-32 are all supported.\n\nI haven't test the performance, but there are some known bottlenecks: For example, data = \n{0, 0xffffffff, 0, 1, 0, 1, 0}\n, numFrameBits=1, then the following '1's will be forced as exceptions, which will dramatically increase compressed size. ",
            "author": "Han Jiang",
            "id": "comment-13287876"
        },
        {
            "date": "2012-06-02T13:30:19+0000",
            "content": "Awesome progress!  Nice to have a dirt path online that we can then\niterate from ...\n\nHmm, I'm seeing some test failures when I run:\n\nant test -Dtests.postingsformat=PFor\n\n\nEg, TestNRTThreads, TestShardSearching, TestTimeLimitingCollector.\n\nRemember to add the standard copyright headers to each new source\nfile...\n\nWe don't have to do this now, but I wonder if we can share code w/ the\npacked ints impl we have, instead generating another one with the .py\nsource.\n\nTestDemo makes a nice TestMin... I usually start with TestDemo when\ntesting scary new code, and then it's a huge milestone once TestDemo\npasses \n\nWe should definitely cutover to BlockTree terms dict (I would upgrade\nthat TODO to a nocommit!).\n\nI suspect that wrapping the blocks byte[] as ByteBuffer and then\nIntBuffer is going to be too costly per decode so we should init them\nonce and re-use (upgrade that TODO to a nocommit). ",
            "author": "Michael McCandless",
            "id": "comment-13287936"
        },
        {
            "date": "2012-06-02T14:20:50+0000",
            "content": "Ah, yes, I forgot to use -Dtests.postingsformat...I can see the errors\nnow.\n\n\nTestDemo makes a nice TestMin... I usually start with TestDemo when\ntesting scary new code, and then it's a huge milestone once TestDemo\npasses \nHmm, that means I should remove TestMin.java? This testcase works fine\nfor the patch.\n\n\nWe should definitely cutover to BlockTree terms dict (I would upgrade\nthat TODO to a nocommit!).\nI'm not quite familiar with these sign stuff, shall I change all the \n\"TODO\" sign into \"nocommit\"? Are the signs related to documentation, \nor just marked to remember not to commit current codes? ",
            "author": "Han Jiang",
            "id": "comment-13287951"
        },
        {
            "date": "2012-06-02T14:41:13+0000",
            "content": "Hmm, that means I should remove TestMin.java? This testcase works fine for the patch.\n\nOh it's fine to keep TestMin now that you wrote it ... I was just saying that TestDemo is the test I run when I want the most trivial test for a new big change.\n\n\nI'm not quite familiar with these sign stuff, shall I change all the \n \"TODO\" sign into \"nocommit\"? Are the signs related to documentation, \n or just marked to remember not to commit current codes?\n\nSorry - this is just a convention I use: I put a // nocommit comment whenever there's a \"blocker\" to committing; this way I can grep for nocommit to see what still needs fixing... and towards the end, nocommits will often be downgraded to TODOs since on closer inspection they really don't have to block committing... ",
            "author": "Michael McCandless",
            "id": "comment-13287952"
        },
        {
            "date": "2012-06-03T18:40:29+0000",
            "content": "Ah, just cannot wait for a performance optimization!\n\nThis version should now pass all tests below: \n\nant test-core -Dtests.postingsformat=PFor\n\nIt fixes: 1) trailing forced exceptions will be ignored and encoded as normal value; 2) IntBuffer is maintained at IndexInput/Output level; 3) Former nocommit issues such as BlockTreeTerms* and code licence.\n\nThe patch also contains a minimal change with the help of Robert's patch: https://issues.apache.org/jira/secure/attachment/12530685/LUCENE-4102.patch. Hope Dawid will commit the complete version into trunk soon!\n\nI'll try to optimize these codes later. ",
            "author": "Han Jiang",
            "id": "comment-13288232"
        },
        {
            "date": "2012-06-04T17:00:42+0000",
            "content": "Excellent!  All tests also pass for me w/ PFor postings format as\nwell... this is a great starting point  One Solr test failed\n(ContentStreamTest)... but I think it was false failure...\n\nI did notice the tests seem to run slower, especially certain ones eg\nTestJoinUtil.\n\nStill missing a couple license headers (TestMin, TestCompress)...\n\nI ran a quick perf test using\nhttp://code.google.com/a/apache-extras.org/p/luceneutil on a 10M doc\nWikipedia index.\n\nIndexing time is ~18% slower than Lucene40PostingsFormat (1071 sec vs\n1261 sec).\n\nBut more important is the slower search times:\n\n\n                Task    QPS base StdDev base    QPS pfor StdDev pfor      Pct diff\n              Phrase        8.52        0.50        4.43        0.40  -55% -  -39%\n        SloppyPhrase       12.52        0.39        7.87        0.51  -43% -  -30%\n          AndHighMed       67.69        2.82       44.22        1.47  -39% -  -29%\n            SpanNear        5.19        0.12        3.90        0.28  -31% -  -17%\n            PKLookup      112.16        1.71       95.61        1.30  -17% -  -12%\n         AndHighHigh       13.22        0.34       11.86        0.72  -17% -   -2%\n            Wildcard       46.04        0.37       41.68        4.45  -19% -    1%\n              Fuzzy1       50.11        2.03       48.06        1.91  -11% -    3%\n           OrHighMed        9.26        0.48        8.90        0.37  -12% -    5%\n          OrHighHigh       12.28        0.56       11.83        0.49  -11% -    5%\n      TermBGroup1M1P       40.47        1.94       39.88        2.51  -11% -   10%\n              Fuzzy2       53.71        2.66       53.01        2.08   -9% -    7%\n         TermGroup1M       36.46        1.21       35.99        1.58   -8% -    6%\n        TermBGroup1M       55.53        1.99       55.26        2.68   -8% -    8%\n             Respell       69.71        4.49       69.73        2.07   -8% -   10%\n                Term       94.38        7.62       94.96       12.19  -18% -   23%\n             Prefix3       41.63        0.34       42.21        5.82  -13% -   16%\n              IntNRQ        7.08        0.15        7.28        1.29  -17% -   23%\n\n\n\nThe queries that do skipping are quite a bit slower; this makes sense,\nsince on skip we do a full block decode.  A smaller block size (we use\n128 now right?) should help I think.\n\nIt's strange that the non-skipping queries (Term, OrHighMed,\nOrHighHigh) don't show any performance gain ... maybe we need to\noptimize the decode... or it could be the removal of the bulk api\nis hurting us here.\n\nI'm also curious if we tried a pure FOR (no patching, so we must set\nnumBits according to the max value = larger index but hopefully faster\ndecode) if the results would improve...\n ",
            "author": "Michael McCandless",
            "id": "comment-13288675"
        },
        {
            "date": "2012-06-05T03:00:08+0000",
            "content": "Thanks Mike, we have so much details to help optimize!\n\nStill missing a couple license headers (TestMin, TestCompress)...\nOk, I'll add them later.\n\nI ran a quick perf test using http://code.google.com/a/apache-extras.org/p/luceneutil on a 10M doc Wikipedia index.\nThe script is wonderful! But the wiki data is missing? Can I get it from a wiki dump instead?\n\nIndexing time is ~18% slower than Lucene40PostingsFormat (1071 sec vs 1261 sec).\nYes, it is expected, actually it scans every block 33 times to estimate metadata such as numFrameBits and numExceptions. ",
            "author": "Han Jiang",
            "id": "comment-13289104"
        },
        {
            "date": "2012-06-05T10:05:10+0000",
            "content": "Hi Billy,\n\nCan I get it from a wiki dump instead?\n\nYou can download it at http://people.apache.org/~mikemccand/enwiki-20120502-lines-1k.txt.lzma\n\nThat's ~6.3 GB (compressed) and 28.7 GB (decompressed); it's the 2012/05/02 Wikipedia en export, filtered to plain text and then broken into 33.3 M ~1 KB sized docs.  I can help you get the luceneutil env set up...\n\n\nIndexing time is ~18% slower than Lucene40PostingsFormat (1071 sec vs 1261 sec).\n\nYes, it is expected, actually it scans every block 33 times to estimate metadata such as numFrameBits and numExceptions.\n\nOK, in that case I'm surprised it's only ~18% slower! ",
            "author": "Michael McCandless",
            "id": "comment-13289296"
        },
        {
            "date": "2012-06-08T16:32:41+0000",
            "content": "OK, here is a result I tried to reproduce with Mike's test script:\nIndexing time:\n    trunk: 2396 sec\n    patch: 2793 sec\n\nSearching time:\n\n          TaskQPS Lucene40StdDev Lucene40    QPS PFor StdDev PFor      Pct diff\n          AndHighMed       22.76        0.54       14.68        1.00  -41% -  -29%\n        SloppyPhrase        3.58        0.17        2.46        0.27  -41% -  -19%\n            SpanNear        5.90        0.09        4.08        0.37  -38% -  -23%\n         AndHighHigh       10.00        0.17        8.08        0.57  -26% -  -11%\n              Phrase        1.68        0.07        1.45        0.17  -27% -    0%\n             Respell       37.65        0.74       33.41        1.04  -15% -   -6%\n              Fuzzy1       38.00        1.60       34.37        1.06  -15% -   -2%\n              IntNRQ        4.27        0.33        3.87        0.19  -19% -    3%\n              Fuzzy2       16.35        0.60       15.02        0.31  -13% -   -2%\n            Wildcard       30.24        0.57       28.24        1.85  -14% -    1%\n            PKLookup       85.82        5.04       83.25        2.81  -11% -    6%\n             Prefix3       19.20        0.40       19.19        1.46   -9% -    9%\n           OrHighMed        9.25        0.59        9.41        0.70  -11% -   16%\n         TermGroup1M       11.46        0.62       11.74        0.81   -9% -   15%\n          OrHighHigh        3.15        0.17        3.28        0.23   -8% -   17%\n      TermBGroup1M1P       19.28        0.38       20.32        1.14   -2% -   13%\n        TermBGroup1M        6.23        0.21        6.71        0.46   -3% -   19%\n                Term       30.86        1.52       34.34        3.26   -4% -   28%\n\n\n\nIt is done on a 64bit AMD server with Java 1.7.0. ",
            "author": "Han Jiang",
            "id": "comment-13291850"
        },
        {
            "date": "2012-06-18T13:41:48+0000",
            "content": "The new \"3892_pfor\" patch fixed some \"SuppressingCodec\" stuff since last two weeks. And the \"3892_for\" lazily implements \"For\" postingsformat based on current codes. These two patches are temporary separated, in order to prevent performance reduction for the sake of method overriding.\n\nCurrently, blocksize ranges from 32 to 128 are tested on both two patches. However, for those skipping-intensive queries, there is no significant performance gain when smaller blocksize was applied. \n\nHere is a previous result for PFor, with blockSize=64, comparing with 128(in brackets):\n\n                Task    QPS Base StdDev Base    QPS PFor StdDev PFor      Pct diff\n              Phrase        4.93        0.36        3.10        0.33  -47% -  -25%  (-47% -  -25%)\n          AndHighMed       27.92        2.26       19.16        1.72  -42% -  -18%  (-37% -  -15%)\n            SpanNear        2.73        0.16        1.96        0.24  -40% -  -14%  (-36% -  -13%)\n        SloppyPhrase        4.19        0.21        3.20        0.30  -34% -  -12%  (-30% -   -6%)\n            Wildcard       19.44        0.87       17.11        0.94  -20% -   -2%  (-17% -    3%)\n         AndHighHigh        7.50        0.38        6.61        0.59  -23% -    1%  (-19% -    6%)\n              IntNRQ        4.06        0.52        3.88        0.35  -22% -   19%  (-16% -   24%)\n             Prefix3       31.00        1.69       30.45        2.29  -13% -   11%  ( -6% -   20%)\n          OrHighHigh        4.16        0.47        4.11        0.34  -18% -   20%  (-14% -   27%)\n           OrHighMed        4.98        0.59        4.94        0.41  -18% -   22%  (-14% -   27%)\n             Respell       40.29        2.11       40.11        2.13  -10% -   10%  (-15% -    2%)\n        TermBGroup1M       20.50        0.32       20.52        0.80   -5% -    5%  (  1% -   10%)\n         TermGroup1M       13.51        0.43       13.61        0.40   -5% -    7%  (  1% -    9%)\n              Fuzzy1       43.20        1.83       44.02        1.95   -6% -   11%  (-11% -    1%)\n            PKLookup       87.16        1.78       89.52        0.94    0% -    5%  ( -2% -    7%)\n              Fuzzy2       16.09        0.80       16.54        0.77   -6% -   13%  (-11% -    6%)\n                Term       43.56        1.53       45.26        3.84   -8% -   16%  (  2% -   26%)\n      TermBGroup1M1P       21.33        0.64       22.24        1.23   -4% -   13%  (  0% -   14%) \n\n\n\nAlso, the For postingsformat shows few performance change. So I suppose the bottleneck isn't in this method: PForUtil.patchException.\nHere is an example with blockSize=64:\n\n                Task    QPS Base StdDev Base     QPS For  StdDev For      Pct diff\n              Phrase        5.03        0.45        3.30        0.43  -47% -  -18%\n          AndHighMed       28.05        2.33       18.83        1.77  -43% -  -19%\n            SpanNear        2.69        0.18        1.94        0.25  -40% -  -12%\n        SloppyPhrase        4.19        0.20        3.22        0.35  -34% -  -10%\n         AndHighHigh        7.61        0.46        6.41        0.54  -27% -   -2%\n             Respell       41.36        1.65       37.94        2.42  -17% -    1%\n            Wildcard       19.20        0.77       17.89        0.99  -15% -    2%\n          OrHighHigh        4.22        0.37        3.94        0.32  -21% -   10%\n           OrHighMed        5.06        0.46        4.73        0.39  -21% -   11%\n              Fuzzy1       44.15        1.31       42.38        1.74  -10% -    2%\n              Fuzzy2       16.48        0.59       15.84        0.76  -11% -    4%\n         TermGroup1M       13.32        0.35       13.44        0.53   -5% -    7%\n            PKLookup       87.70        1.81       88.62        1.22   -2% -    4%\n        TermBGroup1M       20.14        0.47       20.40        0.59   -3% -    6%\n             Prefix3       30.31        1.49       31.08        2.26   -9% -   15%\n      TermBGroup1M1P       21.13        0.46       21.79        1.42   -5% -   12%\n              IntNRQ        3.96        0.45        4.14        0.46  -16% -   31%\n                Term       43.07        1.51       46.06        4.50   -6% -   21%\n\n ",
            "author": "Han Jiang",
            "id": "comment-13395884"
        },
        {
            "date": "2012-06-18T13:54:15+0000",
            "content": "There's a potential bottleneck during method calling...Here is an example for PFor, with blocksize=128, exception rate = 97%, normal value <= 2 bits, exception value <= 32 bits:\n\n\nDecoding normal values:                              4703 ns\nPatching exceptions:                                 5797 ns\nSingle call of PForUtil.decompress totally takes:   58318 ns\n\n\n\nIn addition, it costs about 4000ns to record the time span. ",
            "author": "Han Jiang",
            "id": "comment-13395894"
        },
        {
            "date": "2012-06-18T22:43:09+0000",
            "content": "On the For patch ... we shouldn't encode/decode numInts right?  It's\nalways 128?\n\nUp above, in ForFactory, when we readInt() to get numBytes ... it\nseems like we could stuff the header numBits into that same int and\nsave checking that in FORUtil.decompress....\n\nI think there are a few possible ideas to explore to get faster\nPFor/For performance:\n\n\n\tGet more direct access to the file as an int[]; eg MMapDir could\n    expose an IntBuffer from its ByteBuffer (saving the initial copy\n    into byte[] that we now do).  Or maybe we add\n    IndexInput.readInts(int[]) and dir impl can optimize how that's\n    done (MMapDir could use Unsafe.copyBytes... except for little\n    endian architectures ... we'd probably have to have separate\n    specialized decoder rather than letting Int/ByteBuffer do the byte\n    swapping).  This would require the whole file stays aligned w/ int\n    (eg the header must be 0 mod 4).\n\n\n\n\n\tCopy/share how oal.packed works, i.e. being able to waste a bit to\n    have faster decode (eg storing the 7 bit case as byte[], wasting 1\n    bit for each value).\n\n\n\n\n\tSkipping: can we partially decode a block?  EG if we are skipping\n    and we know we only want values after the 80th one, then we\n    shouldn't decode those first 80...\n\n\n\n\n\tSince doc/freq are \"aligned\", when we store pointers to a given\n    spot, eg in the terms dict or in skip data, we should only store\n    the offset once (today we store it twice).\n\n\n\n\n\tAlternatively, maybe we should only save skip data on doc/freq\n    block boundaries (prox would still need skip-within-block).\n\n\n\n\n\tMaybe we should store doc & frq blocks interleaved in a single\n    file (since they are \"aligned\") and then skip would skip to the\n    start of a doc/frq block pair.\n\n\n\nOther ideas...? ",
            "author": "Michael McCandless",
            "id": "comment-13396325"
        },
        {
            "date": "2012-06-19T19:03:01+0000",
            "content": "Oh, thank you Mike! I haven't thought too much about those skipping policies.\n\nUp above, in ForFactory, when we readInt() to get numBytes ... it seems like we could stuff the header numBits into that same int and save checking that in FORUtil.decompress....\nAh, yes, I just forgot to remove the redundant codes. Here is a initial try to remove header and call ForDecompressImpl directly in readBlock():with For, blockSize=128. Data in bracket show prior benchmark.\n\n                Task    QPS Base StdDev Base     QPS For  StdDev For      Pct diff\n              Phrase        4.99        0.37        3.57        0.26  -38% -  -17% (-44% -  -18%)\n          AndHighMed       28.91        2.17       22.66        0.82  -29% -  -12% (-38% -   -9%)\n            SpanNear        2.72        0.14        2.22        0.13  -26% -   -8% (-36% -   -8%)\n        SloppyPhrase        4.24        0.26        3.70        0.16  -21% -   -3% (-33% -   -6%)\n             Respell       40.71        2.59       37.66        1.36  -16% -    2% (-18% -    0%)\n              Fuzzy1       43.22        2.01       40.66        0.32  -10% -    0% (-12% -    0%)\n              Fuzzy2       16.25        0.90       15.64        0.26  -10% -    3% (-12% -    3%)\n            Wildcard       19.07        0.86       19.07        0.73   -8% -    8% (-21% -    3%)\n         AndHighHigh        7.76        0.47        7.77        0.15   -7% -    8% (-21% -   10%)\n            PKLookup       87.50        4.56       88.51        1.24   -5% -    8% ( -2% -    5%)\n        TermBGroup1M       20.42        0.87       21.32        0.74   -3% -   12% (  2% -   10%)\n           OrHighMed        5.33        0.68        5.61        0.14   -9% -   23% (-16% -   25%)\n          OrHighHigh        4.43        0.53        4.69        0.12   -8% -   23% (-15% -   24%)\n         TermGroup1M       13.30        0.34       14.31        0.40    2% -   13% (  0% -   13%)\n      TermBGroup1M1P       20.92        0.59       23.71        0.86    6% -   20% ( -1% -   22%)\n             Prefix3       30.30        1.41       35.14        1.76    5% -   27% (-14% -   21%)\n              IntNRQ        3.90        0.54        4.58        0.47   -7% -   50% (-25% -   33%)\n                Term       42.17        1.55       52.33        2.57   13% -   35% (  1% -   33%)\n\n\nThe improvement is quite general. However, I still suppose this just benefits from less method calling. I'm trying to change the PFor codes, and remove those nested call. (this is not actually true, since I was using percentage diff instead of QPS during comparison)\n\nGet more direct access to the file as an int[]; ...\nOk, this will be considered when the pfor+pulsing is completed. I'm just curious why we don't have readInts in ora.util yet...\n\nSkipping: can we partially decode a block? ...\nThe pfor-opt approach(encode lower bits of exception in normal area, and other bits in exception area)  natually fits \"partially decode a block\", that'll be possible when we optimize skipping queries. ",
            "author": "Han Jiang",
            "id": "comment-13396987"
        },
        {
            "date": "2012-06-20T03:16:52+0000",
            "content": "And result for PFor(blocksize=128):\n\n                Task    QPS Base StdDev Base    QPS PFor StdDev PFor      Pct diff\n              Phrase        4.87        0.36        3.39        0.18  -38% -  -20% (-47% -  -25%)\n          AndHighMed       27.78        2.35       21.13        0.52  -31% -  -14% (-37% -  -15%)\n            SpanNear        2.70        0.14        2.20        0.11  -26% -   -9% (-36% -  -13%)\n        SloppyPhrase        4.17        0.15        3.77        0.21  -17% -    0% (-30% -   -6%)\n             Respell       39.97        1.56       37.65        1.95  -14% -    3% (-15% -    2%)\n            Wildcard       19.08        0.77       18.33        0.92  -12% -    5% (-17% -    3%)\n              Fuzzy1       42.29        1.13       40.78        1.44   -9% -    2% (-11% -    1%)\n         AndHighHigh        7.61        0.55        7.45        0.08   -9% -    6% (-19% -    6%)\n              Fuzzy2       15.79        0.55       15.64        0.70   -8% -    7% (-11% -    6%)\n            PKLookup       86.71        2.13       88.92        2.24   -2% -    7% ( -2% -    7%)\n         TermGroup1M       13.04        0.23       14.03        0.40    2% -   12% (  1% -    9%)\n              IntNRQ        3.97        0.48        4.35        0.61  -15% -   41% (-16% -   24%)\n      TermBGroup1M1P       21.04        0.35       23.20        0.60    5% -   14% (  0% -   14%)\n        TermBGroup1M       19.27        0.47       21.28        0.84    3% -   17% (  1% -   10%)\n          OrHighHigh        4.13        0.47        4.63        0.27   -5% -   34% (-14% -   27%)\n           OrHighMed        4.95        0.59        5.58        0.34   -5% -   35% (-14% -   27%)\n             Prefix3       30.33        1.36       34.26        2.14    1% -   25% ( -6% -   20%)\n                Term       41.99        1.19       50.75        1.72   13% -   28% (  2% -   26%)\n\n\nIt works, and it is quite interesting that StdDev for Term query is reduced significantly. (same as last comment, when comparing two versions directly(method call vs. unfolded, the improvement is somewhat noisy)) ",
            "author": "Han Jiang",
            "id": "comment-13397228"
        },
        {
            "date": "2012-06-20T11:39:16+0000",
            "content": "The For index is 5.2 GB vs 4.9 GB for vInt: not bad to have only 5%\nincrease in index size when using For PF (10M wikipedia index).\n\n\nGet more direct access to the file as an int[]; eg MMapDir could\nexpose an IntBuffer from its ByteBuffer (saving the initial copy\ninto byte[] that we now do). \n\nI tested this, by making hacked up changes to Billy's For patch\nrequiring MMapDirectory and pulling an IntBuffer directly from its\nByteBuffer, saving one copy of bytes into the byte[] first.  But,\ncuriously, it didn't seem to improve things much:\n\n\n                Task    QPS base StdDev base     QPS for  StdDev for      Pct diff\n          AndHighMed       24.32        0.60       14.24        0.41  -44% -  -38%\n            PKLookup      131.98        3.09      108.35        1.47  -20% -  -14%\n         AndHighHigh        5.36        0.18        4.66        0.02  -16% -   -9%\n              Phrase        1.48        0.02        1.33        0.10  -18% -   -2%\n        SloppyPhrase        1.40        0.04        1.26        0.03  -13% -   -5%\n            SpanNear        1.14        0.01        1.04        0.02  -10% -   -6%\n              IntNRQ       12.13        0.70       11.27        0.46  -15% -    2%\n             Prefix3       34.51        1.17       34.11        1.28   -8% -    6%\n              Fuzzy1       90.63        1.74       89.68        1.46   -4% -    2%\n             Respell       77.22        2.62       76.99        1.62   -5% -    5%\n            Wildcard       11.84        0.40       12.20        0.37   -3% -    9%\n              Fuzzy2       34.34        0.82       36.16        1.08    0% -   11%\n      TermBGroup1M1P        4.71        0.11        5.02        0.18    0% -   12%\n           OrHighMed        7.87        0.28        8.50        0.55   -2% -   19%\n        TermBGroup1M        3.47        0.03        3.78        0.03    7% -   11%\n         TermGroup1M        2.96        0.01        3.25        0.03    8% -   11%\n          OrHighHigh        3.55        0.12        3.91        0.21    0% -   20%\n                Term        9.72        0.28       10.87        0.44    4% -   19%\n\n\n\nMaybe, instead, reading into an int[] and decoding from an int array\n(hopefully avoiding bounds checks) will be faster than calling\nIntBuffer.get for each encoded int... ",
            "author": "Michael McCandless",
            "id": "comment-13397430"
        },
        {
            "date": "2012-06-20T13:41:03+0000",
            "content": "The *unfold_method.patch just remove the nested call of PForDecompressImpl.decode, and also clip out numBytes information for ForPF.  ",
            "author": "Han Jiang",
            "id": "comment-13397500"
        },
        {
            "date": "2012-06-20T16:14:27+0000",
            "content": "OK I created a branch and committed last For patch: https://svn.apache.org/repos/asf/lucene/dev/branches/pforcodec_3892 ",
            "author": "Michael McCandless",
            "id": "comment-13397605"
        },
        {
            "date": "2012-06-20T17:56:47+0000",
            "content": "OK, just reproduce your test. But Mike, are we using a same task file? Our relative speeds for different queries are not the same. \n\n                Task    QPS Base StdDev Base     QPS For  StdDev For      Pct diff\n              Phrase        5.07        0.45        3.76        0.19  -35% -  -14% (-44% -  -18%)\n          AndHighMed       28.32        2.34       22.67        0.67  -28% -  -10% (-38% -   -9%)\n            SpanNear        2.72        0.13        2.36        0.14  -22% -   -3% (-36% -   -8%)\n        SloppyPhrase        4.18        0.20        3.83        0.15  -16% -    0% (-33% -   -6%)\n             Respell       42.02        1.83       38.86        2.30  -16% -    2% (-18% -    0%)\n              Fuzzy1       44.96        1.58       42.85        1.69  -11% -    2% (-12% -    0%)\n              Fuzzy2       16.78        0.69       16.34        0.68  -10% -    5% (-12% -    3%)\n            PKLookup       89.11        2.15       87.33        2.19   -6% -    2% ( -2% -    5%)\n         AndHighHigh        7.61        0.44        7.69        0.21   -7% -   10% (-21% -   10%)\n            Wildcard       19.50        0.91       20.02        0.72   -5% -   11% (-21% -    3%)\n        TermBGroup1M       20.82        0.37       21.73        0.69    0% -    9% (  2% -   10%)\n         TermGroup1M       13.79        0.13       14.61        0.32    2% -    9% (  1% -    9%)\n              IntNRQ        4.11        0.56        4.56        0.56  -14% -   43% (-25% -   33%)\n      TermBGroup1M1P       21.45        0.75       24.00        0.51    5% -   18% ( -1% -   22%)\n           OrHighMed        5.08        0.49        5.73        0.15    0% -   28% (-16% -   25%)\n          OrHighHigh        4.22        0.39        4.78        0.13    1% -   28% (-15% -   24%)\n             Prefix3       30.91        1.63       35.65        2.02    3% -   28% (-14% -   21%)\n                Term       44.36        1.87       54.01        1.96   12% -   31% ( -1% -   33%)\n\n ",
            "author": "Han Jiang",
            "id": "comment-13397694"
        },
        {
            "date": "2012-06-20T22:40:56+0000",
            "content": "But Mike, are we using a same task file? Our relative speeds for different queries are not the same.\n\nSorry, I'm using a hand edited \"hard\" tasks file; I'll commit & push to luceneutil.  But, separately: each run picks a different subset of the tasks from each category to run, so results from one run to another in general aren't comparable unless we fix the random seed it uses. ",
            "author": "Michael McCandless",
            "id": "comment-13397958"
        },
        {
            "date": "2012-06-21T15:48:34+0000",
            "content": "For decompressing phase, replace the use of IntBuffer with a direct int[] to int[] decoder. Method convert() is supposed to be performant enough...coz it is not different from the inner implementation of IntBuffer.get(), i.e.http://massapi.com/source/jdk1.6.0_17/src/java/nio/Bits.java.html, line 193. However, result isn't interesting.\n\nHmm, there is an extra block of memory write here, which Mike wanted to avoid in previous patch. That should be the cause. ",
            "author": "Han Jiang",
            "id": "comment-13398507"
        },
        {
            "date": "2012-06-21T16:55:24+0000",
            "content": "Now remove the memory write codes, and replace IntBuffer.get() with getInt(byte,byte,byte,byte), since this patch contains method unfolding, there is no actually difference...Seems that we're paying attenting on a wrong point.\n\n                Task    QPS Base StdDev Base     QPS For  StdDev For      Pct diff\n              Phrase        5.02        0.46        3.66        0.30  -38% -  -13% (-38% -  -17%)\n          AndHighMed       28.08        2.29       23.04        1.01  -27% -   -6% (-29% -  -12%)\n            SpanNear        2.69        0.16        2.30        0.19  -25% -    0% (-26% -   -8%)\n        SloppyPhrase        4.18        0.22        3.83        0.18  -16% -    1% (-21% -   -3%)\n             Respell       41.92        2.15       39.54        2.45  -15% -    5% (-16% -    2%)\n              Fuzzy1       44.47        1.99       43.34        3.07  -13% -    9% (-10% -    0%)\n            Wildcard       19.70        1.06       19.60        1.16  -11% -   11% ( -8% -    8%)\n              Fuzzy2       16.54        0.86       16.52        1.16  -11% -   12% (-10% -    3%)\n            PKLookup       87.32        2.47       88.62        1.33   -2% -    6% ( -5% -    8%)\n         AndHighHigh        7.55        0.43        7.84        0.15   -3% -   12% ( -7% -    8%)\n        TermBGroup1M       19.86        0.14       21.41        0.70    3% -   12% ( -3% -   12%)\n         TermGroup1M       13.35        0.17       14.40        0.38    3% -   12% (  2% -   13%)\n              IntNRQ        4.10        0.57        4.45        0.73  -20% -   46% ( -7% -   50%)\n      TermBGroup1M1P       21.29        0.63       23.45        0.82    3% -   17% (  6% -   20%)\n             Prefix3       31.13        1.71       35.53        2.90    0% -   30% (  5% -   27%)\n           OrHighMed        4.96        0.61        5.83        0.35   -1% -   42% ( -9% -   23%)\n          OrHighHigh        4.13        0.49        4.87        0.29    0% -   41% ( -8% -   23%)\n                Term       42.93        1.17       52.11        2.21   13% -   30% ( 13% -   35%)\n\n\nIt is compared with result in https://issues.apache.org/jira/browse/LUCENE-3892?focusedCommentId=13396987&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13396987 ",
            "author": "Han Jiang",
            "id": "comment-13398582"
        },
        {
            "date": "2012-06-21T19:50:27+0000",
            "content": "And same codes with the wikimediumhard.tasks file.(This is really a hard testcase, since QPS are so small that we can hardly depend on Pct Diff  )\n\n                Task    QPS Base StdDev Base     QPS For  StdDev For      Pct diff\n          AndHighMed       10.76        0.21        6.47        0.32  -43% -  -35%\n         AndHighHigh        2.89        0.08        2.57        0.19  -20% -   -1%\n            SpanNear        0.60        0.01        0.55        0.01  -11% -   -6%\n        SloppyPhrase        0.61        0.01        0.57        0.01   -9% -   -3%\n            PKLookup       87.72        2.61       86.28        1.48   -6% -    3%\n              Fuzzy1       36.22        1.14       35.90        0.97   -6% -    5%\n              Phrase        1.22        0.03        1.22        0.08   -9% -    8%\n             Respell       32.84        0.92       33.55        0.87   -3% -    7%\n              IntNRQ        3.66        0.35        3.74        0.08   -8% -   15%\n              Fuzzy2       21.62        0.66       22.10        0.51   -3% -    7%\n             Prefix3       13.30        0.49       14.09        0.76   -3% -   15%\n           OrHighMed        3.43        0.16        3.65        0.45  -10% -   25%\n          OrHighHigh        1.66        0.09        1.79        0.22  -10% -   28%\n            Wildcard        3.39        0.14        3.74        0.20    0% -   21%\n      TermBGroup1M1P        1.84        0.03        2.10        0.16    3% -   25%\n         TermGroup1M        1.14        0.03        1.34        0.10    5% -   29%\n        TermBGroup1M        1.49        0.05        1.78        0.13    7% -   32%\n                Term        3.49        0.13        4.38        0.65    2% -   49%\n\n ",
            "author": "Han Jiang",
            "id": "comment-13398800"
        },
        {
            "date": "2012-06-22T18:37:51+0000",
            "content": "I was curious how much the \"layers\" (SepPostingsReader,\nFixedIntBlock.IntIndexInput, ForFactor) between the FOR block decode\nand the query scoring were hurting performance, so I wrote a\nspecialized scorer (BlockTermScorer) for just TermQuery.\n\nThe scorer is only used if the postings format is ForPF, and if no\nskipping will be done (I didn't implement advance...).\n\nThe scorer reaches down and holds on to the decoded int[] buffer, and\nthen does its own adding up of the doc deltas, reading the next block,\netc.\n\nThe baseline is the current branch (not trunk!):\n\n\n                Task    QPS base StdDev base   QPS patch StdDev patch     Pct diff\n            Wildcard       10.31        0.40       10.10        0.17   -7% -    3%\n         AndHighHigh        4.90        0.10        4.82        0.15   -6% -    3%\n             Prefix3       28.50        1.06       28.11        0.50   -6% -    4%\n              IntNRQ        9.72        0.46        9.60        0.57  -11% -    9%\n        SloppyPhrase        0.92        0.03        0.92        0.02   -6% -    5%\n            PKLookup      106.21        2.54      105.66        2.07   -4% -    3%\n              Phrase        1.56        0.00        1.56        0.01   -1% -    0%\n              Fuzzy1       90.33        3.48       90.19        2.25   -6% -    6%\n              Fuzzy2       29.66        0.61       29.64        0.85   -4% -    4%\n          AndHighMed       14.87        0.29       15.02        0.81   -6% -    8%\n             Respell       78.83        2.46       79.62        1.54   -3% -    6%\n            SpanNear        1.18        0.02        1.19        0.04   -4% -    6%\n         TermGroup1M        2.78        0.06        3.28        0.14   10% -   25%\n          OrHighHigh        4.19        0.24        5.04        0.20    9% -   32%\n           OrHighMed        8.21        0.45        9.87        0.23   11% -   30%\n      TermBGroup1M1P        5.11        0.20        6.21        0.26   12% -   31%\n        TermBGroup1M        4.49        0.11        5.49        0.27   13% -   31%\n                Term        8.89        0.58       11.90        1.52    9% -   61%\n\n\n\nSeems like we get a good boost removing the abstractions. ",
            "author": "Michael McCandless",
            "id": "comment-13399501"
        },
        {
            "date": "2012-06-23T07:23:28+0000",
            "content": "It's really interesting the effect of peeling back those abstractions. ",
            "author": "Chris Male",
            "id": "comment-13399869"
        },
        {
            "date": "2012-06-23T08:36:37+0000",
            "content": "Yes, really interesting. And that should make sense. As far as I know, a method with exception handling may be quite slow than a simple if statement check.(Hmm, now I think this is not true, the improvement should mainly come the framework change) Here is part of the result in my test, with Mike's patch:\n\n           OrHighMed        2.53        0.31        2.57        0.13  -13% -   21%\n            Wildcard        3.86        0.12        3.94        0.38  -10% -   15%\n          OrHighHigh        1.57        0.18        1.61        0.08  -12% -   21%\n      TermBGroup1M1P        1.93        0.03        2.48        0.10   21% -   35%\n         TermGroup1M        1.37        0.02        1.81        0.05   26% -   37%\n        TermBGroup1M        1.17        0.02        1.64        0.07   32% -   47%\n                Term        2.92        0.13        4.46        0.23   38% -   68%\n\n ",
            "author": "Han Jiang",
            "id": "comment-13399883"
        },
        {
            "date": "2012-07-02T17:55:29+0000",
            "content": "This version will make PFor work side by side with For. Hmm, the performance results might changed a little, I'll post them later. ",
            "author": "Han Jiang",
            "id": "comment-13405169"
        },
        {
            "date": "2012-07-03T00:44:26+0000",
            "content": "Thanks Billy, I committed this to the branch. ",
            "author": "Michael McCandless",
            "id": "comment-13405477"
        },
        {
            "date": "2012-07-09T17:25:02+0000",
            "content": "Current branch cannot pass tests like this: \n\nant test -Dtestcase=TestConcurrentMergeScheduler -Dtests.method=testFlushExceptions -Dtests.seed=C2ED637AF330E96A -Dtests.postingsformat=For \n\n\nMaybe we should handle the IndexOutput more gracefully? ",
            "author": "Han Jiang",
            "id": "comment-13409661"
        },
        {
            "date": "2012-07-09T17:26:18+0000",
            "content": "And this patch ensures pulsing support for PFor and For postings format. ",
            "author": "Han Jiang",
            "id": "comment-13409662"
        },
        {
            "date": "2012-07-09T17:30:58+0000",
            "content": "Current branch cannot pass tests like this:\n\nThanks, I committed the patch. ",
            "author": "Michael McCandless",
            "id": "comment-13409664"
        },
        {
            "date": "2012-07-11T10:13:43+0000",
            "content": "Add javadocs for previous codes. I'm still not sure about the IOUtils.closeWhileHandlingException(), I think the exceptions should not be suppressed when out.close() is called? This patch also makes some minor change on randomness in testcase. \nThe Pulsing parts in last patch is not included here, because they doesn't improve performance significantly.  ",
            "author": "Han Jiang",
            "id": "comment-13411373"
        },
        {
            "date": "2012-07-11T13:21:33+0000",
            "content": "I'm still not sure about the IOUtils.closeWhileHandlingException(), I think the exceptions should not be suppressed when out.close() is called?\n\nActually I think you want them to be suppressed, so that the original exception is seen? ",
            "author": "Michael McCandless",
            "id": "comment-13411472"
        },
        {
            "date": "2012-07-11T13:25:48+0000",
            "content": "Docs/cleanup patch looks good, I'll commit to the branch!  Thanks. ",
            "author": "Michael McCandless",
            "id": "comment-13411475"
        },
        {
            "date": "2012-07-11T13:41:26+0000",
            "content": "Actually I think you want them to be suppressed, so that the original exception is seen?\n\nNot my idea actually, I think the exception should be thrown for out.close()? closeWhileHandlingException() will suppress those exceptions. So in this patch I use out.close() instead of IOUtils.closeWhileHandlingException() ",
            "author": "Han Jiang",
            "id": "comment-13411495"
        },
        {
            "date": "2012-07-11T13:57:27+0000",
            "content": "Not my idea actually, I think the exception should be thrown for out.close()? closeWhileHandlingException() will suppress those exceptions\n\nBut the problem is some other exception has already been thrown (because success is false).  If out.close then hits a second exception we have to pick which one should be thrown, and I think the original one is better?  (Since it's likely the root cause of whatever went wrong). ",
            "author": "Michael McCandless",
            "id": "comment-13411515"
        },
        {
            "date": "2012-07-11T13:59:29+0000",
            "content": "The Pulsing parts in last patch is not included here, because they doesn't improve performance significantly. \n\nHere are some tests between For vs PulsingFor, PFor vs PulsingPFor. Run on the 1M docs with wikimediumhard.tasks\n\nIt is strange that PKLookup still doesn't benefit for FixedBlockInt:\n\n\n                Task     QPS For  StdDev ForQPS PulsingForStdDev PulsingFor      Pct diff\n         AndHighHigh       23.01        0.33       22.94        0.66   -4% -    4%  \n          AndHighMed       56.41        0.76       57.41        1.74   -2% -    6%  \n              Fuzzy1       86.74        0.85       82.22        2.39   -8% -   -1% \n              Fuzzy2       28.23        0.38       26.15        0.97  -11% -   -2% \n              IntNRQ       41.78        1.65       40.78        3.53  -14% -   10% \n          OrHighHigh       14.44        0.34       14.50        0.92   -8% -    9%  \n           OrHighMed       30.59        0.77       31.12        1.93   -6% -   10% \n            PKLookup      110.31        2.03      109.22        2.43   -4% -    3%  \n              Phrase        8.18        0.44        7.97        0.40  -12% -    8%  \n             Prefix3       99.64        2.38       97.09        3.46   -8% -    3%  \n             Respell       99.66        0.45       92.76        2.81  -10% -   -3% \n        SloppyPhrase        4.28        0.16        4.08        0.13  -11% -    2%  \n            SpanNear        4.08        0.13        3.93        0.06   -7% -    0%  \n                Term       33.63        1.25       34.06        1.71   -7% -   10% \n        TermBGroup1M       15.54        0.46       15.78        0.56   -4% -    8%  \n      TermBGroup1M1P       20.34        0.73       20.62        0.62   -5% -    8%  \n         TermGroup1M       19.18        0.52       19.72        0.49   -2% -    8%  \n            Wildcard       34.86        0.88       34.27        1.77   -9% -    6% \n\n\n\n\n         AndHighHigh       19.98        0.31       19.92        0.26   -3% -    2%  \n          AndHighMed       58.21        1.51       57.86        1.18   -5% -    4%  \n              Fuzzy1       91.86        1.17       85.86        1.18   -8% -   -4% \n              Fuzzy2       32.66        0.58       30.08        0.57  -11% -   -4% \n              IntNRQ       33.89        0.82       32.66        1.10   -9% -    2%  \n          OrHighHigh       15.79        1.29       14.96        0.67  -16% -    7%\n           OrHighMed       30.31        2.09       28.91        1.67  -15% -    8%\n            PKLookup      112.80        0.81      111.82        2.90   -4% -    2%\n              Phrase        6.14        0.11        6.23        0.10   -1% -    5%\n             Prefix3      147.80        2.88      138.35        2.11   -9% -   -3%\n             Respell      118.57        1.18      108.30        1.86  -11% -   -6%\n        SloppyPhrase        5.78        0.15        5.66        0.29   -9% -    5%\n            SpanNear        6.32        0.14        6.40        0.16   -3% -    6%\n                Term       41.60        2.44       38.12        0.33  -14% -   -1%\n        TermBGroup1M       14.40        0.48       13.73        0.19   -8% -    0%\n      TermBGroup1M1P       23.68        0.44       22.82        0.44   -7% -    0%\n         TermGroup1M       15.25        0.48       14.51        0.20   -9% -    0%\n            Wildcard       32.76        0.53       31.76        0.62   -6% -    0%\n\n ",
            "author": "Han Jiang",
            "id": "comment-13411517"
        },
        {
            "date": "2012-07-11T14:05:57+0000",
            "content": "But the problem is some other exception has already been thrown (because success is false). If out.close then hits a second exception we have to pick which one should be thrown, and I think the original one is better? (Since it's likely the root cause of whatever went wrong).\n\nOK, I see, then let's change ForPostingsFormat.fieldsConsumer/Producer as well. ",
            "author": "Han Jiang",
            "id": "comment-13411533"
        },
        {
            "date": "2012-07-11T14:17:40+0000",
            "content": "OK I committed that!  Let me know if I missed any... ",
            "author": "Michael McCandless",
            "id": "comment-13411546"
        },
        {
            "date": "2012-07-11T14:25:16+0000",
            "content": "OK, thanks! ",
            "author": "Han Jiang",
            "id": "comment-13411556"
        },
        {
            "date": "2012-07-12T16:12:09+0000",
            "content": "Fix a minor bug for maxChain, which will not be detected when blockSize<256. \n\nSupport numBits=0, however, in wiki data, there are only 0.09% of the blocks encoded as numBits=0, this doesn't actually affect the performance.\n\nAlso add some testcases. ",
            "author": "Han Jiang",
            "id": "comment-13412900"
        },
        {
            "date": "2012-07-12T22:57:24+0000",
            "content": "Thanks Billy, I'll commit!\n\nOne thing I noticed: I think we shouldn't separately read numBytes and the int header?  Can't we do a single readVInt(), and that encodes numBytes as well as format (bit width and format, once we tie into oal.util.packed APIs)?  Also, we shouldn't encode numInts at all, ie, this should be fixed for the whole segment, and not written per block. ",
            "author": "Michael McCandless",
            "id": "comment-13413312"
        },
        {
            "date": "2012-07-12T23:03:19+0000",
            "content": "I didn't commit lucene/core/src/java/org/apache/lucene/codecs/pfor/ForPostingsFormat.java \u2013 your IDE had changed it to a wildcard import (I prefer we stick with individual imports).\n\nWas the numBits==0 case for all 0s not all 1s?  We may want to have it mean all 1s instead? ",
            "author": "Michael McCandless",
            "id": "comment-13413314"
        },
        {
            "date": "2012-07-13T12:12:17+0000",
            "content": "Was the numBits==0 case for all 0s not all 1s? We may want to have it mean all 1s instead?\nOK, I just tested this, and for most cases(93%) when the whole block shares one value v, v==1. This change improves index speed and reduce file size a bit(280s vs 320s and 589M vs 591M). But why? Does lucene store freq() when it is 0 as well, so a whole block with v==1 will be more possible? ",
            "author": "Han Jiang",
            "id": "comment-13413682"
        },
        {
            "date": "2012-07-13T13:00:04+0000",
            "content": "But why? Does lucene store freq() when it is 0 as well, so a whole block with v==1 will be more possible?\n\nA whole block of 1s can easily happen: if all freqs are one (the term always occurred only once in each document), or if the term occurs in every document than the delta between docIDs is always 1.\n\nI don't think we should ever hit an all 0s block today (hmm: except for positions, if the given term always occurred at the first position in each doc).\n\nWe could in theory subtract 1 from all these deltas (except the first one!  so maybe we add one to the docID to begin with...) so that these turn into all 0s blocks, but then at decode time we'd have to add 1 back and I'm not sure that'd net/net be a win. ",
            "author": "Michael McCandless",
            "id": "comment-13413698"
        },
        {
            "date": "2012-07-13T13:17:40+0000",
            "content": "We could in theory subtract 1 from all these deltas (except the first one! so maybe we add one to the docID to begin with...) so that these turn into all 0s blocks, but then at decode time we'd have to add 1 back and I'm not sure that'd net/net be a win.\n\nHmm , so current strategy is: 1.for docIDs, store v[i+1]-v[i]-1; 2. for freq and positions, store v[i] directly? Yes there are blocks with all 0s, although very rare to see.  ",
            "author": "Han Jiang",
            "id": "comment-13413709"
        },
        {
            "date": "2012-07-13T13:26:22+0000",
            "content": "No, for docIDs we store docID - lastDocID.  So that delta can be 0 for the first doc in a posting list, and then >= 1 thereafter.\n\nBut an all 0s block is possible if a bunch of terms in a row occurred only in doc 0. ",
            "author": "Michael McCandless",
            "id": "comment-13413719"
        },
        {
            "date": "2012-07-13T14:37:38+0000",
            "content": "This patch cut the extra header and merge numBytes into header. Also, it store only one int when a whole block share the same value. So no matter which strategy we use (d-gap, or d-gap minus 1), it will work well.\n\nAnd here are the changes between different methods, postingsformat=PFor:\n\nmethod          base     all_0s  all_1s  all_Vs  all_Vs+header_cut\nindex time(s):  324     315     279     279     291\nindex size(MB): 591     591     589     589     577\n\n\n\npostingsFormat=For: \n\nmethod          base     all_Vs+header_cut\nindex time(s):  250     251\nindex size(MB): 611     598\n\n\n\nwhere raw refers to the version when numBits==0 isn't supported, all_0s refers to last patch, all_Vs+header_cut refers to this patch. As for PFor, now the index size is almost equal to Lucene40(590.7M vs 590.0M). ",
            "author": "Han Jiang",
            "id": "comment-13413781"
        },
        {
            "date": "2012-07-13T18:07:54+0000",
            "content": "previous patch is a little messy, do some cleanups. ",
            "author": "Han Jiang",
            "id": "comment-13413919"
        },
        {
            "date": "2012-07-13T18:22:04+0000",
            "content": "Those are interesting results!  Curious how much faster indexing is for PFor if you use all_Vs; cutting the header is also a nice reduction on index size.\n\nInstead of having P/ForUtil reach up into P/ForPostingsFormat for the default block size, I think we can assume the int[] array length (of the decoded buffer) is the size of the block? ",
            "author": "Michael McCandless",
            "id": "comment-13413935"
        },
        {
            "date": "2012-07-13T18:30:35+0000",
            "content": "Instead of having P/ForUtil reach up into P/ForPostingsFormat for the default block size, I think we can assume the int[] array length (of the decoded buffer) is the size of the block?\n+1, I'll update the patch ",
            "author": "Han Jiang",
            "id": "comment-13413942"
        },
        {
            "date": "2012-07-15T13:15:38+0000",
            "content": "Maybe we should cleanup those patches first? The latest LUCENE-3892-for&pfor-with-javadoc.patch should be a baseline for current methods. \n\nThe patch marked as \"iterate numbits\" uses previous method to estimate compressed size, ignoring all forced exception, while the other one marked as \"slow estimate\" will fakely compress the whole block several times, and get the lower bound of our compressed size. Here is a comparison: \n\nmethod         header_cut iter_numBits slow_estimate\nindex size(M)  577        573          554\nindex time(s)  275        258          296 \n\n  ",
            "author": "Han Jiang",
            "id": "comment-13414626"
        },
        {
            "date": "2012-07-16T12:55:27+0000",
            "content": "Thanks Billy, I committed last baseline patch! ",
            "author": "Michael McCandless",
            "id": "comment-13415072"
        },
        {
            "date": "2012-07-16T13:10:22+0000",
            "content": "I opened LUCENE-4225 with a new base PostingsFormat that gives better perf for For than Sep... ",
            "author": "Michael McCandless",
            "id": "comment-13415085"
        },
        {
            "date": "2012-07-16T15:34:40+0000",
            "content": "I think a good thing to explore next is to stop using our own packed ints impl and instead cutover to oal.util.packed?  (Since so much effort has gone into making those impls fast).\n\nLUCENE-4161 has already taken a big step towards making them usable ... we should prototype an initial cutover and then iterate? ",
            "author": "Michael McCandless",
            "id": "comment-13415226"
        },
        {
            "date": "2012-07-16T15:43:21+0000",
            "content": "+1 Don't hesitate to tell me if you're missing methods for this issue (I'm thinking at least of bulk int[] read/write, we currently only make it possible with longs). ",
            "author": "Adrien Grand",
            "id": "comment-13415230"
        },
        {
            "date": "2012-07-16T15:49:54+0000",
            "content": "I opened LUCENE-4225 with a new base PostingsFormat that gives better perf for For than Sep...\nWow, the result looks great! Quite curious why some queries improve so much, like AndHighHigh.\n\nLUCENE-4161 has already taken a big step towards making them usable ... we should prototype an initial cutover and then iterate?\nYes, but we should make the PostingsFormat pass test first? Currently it also fails some tests for ForPF. ",
            "author": "Han Jiang",
            "id": "comment-13415236"
        },
        {
            "date": "2012-07-16T16:01:49+0000",
            "content": "Yes, but we should make the PostingsFormat pass test first? Currently it also fails some tests for ForPF.\n\nUh oh I didn't know tests are failing on the branch: do you have a seed? ",
            "author": "Michael McCandless",
            "id": "comment-13415335"
        },
        {
            "date": "2012-07-20T13:51:53+0000",
            "content": "An initial try with PackedInts in current trunk version. I replaced all the int[] buffer with long[] buffer so we can use the API directly. I don't quite understand the Writer part, so we have to save each long value one by one.\n\nHowever, it is the Reader part we are concerned:\n\n                Task    QPS base StdDev base QPS packedStdDev packed      Pct diff\n         AndHighHigh       29.60        1.56       23.78        0.51  -25% -  -13%\n          AndHighMed       74.68        3.92       53.15        2.31  -35% -  -21%\n              Fuzzy1       88.23        1.21       87.13        1.41   -4% -    1%\n              Fuzzy2       30.09        0.45       29.47        0.47   -5% -    1%\n              IntNRQ       41.96        3.88       38.16        2.48  -22% -    6%\n          OrHighHigh       17.56        0.34       15.45        0.15  -14% -   -9%\n           OrHighMed       34.71        0.76       30.77        0.53  -14% -   -7%\n            PKLookup      111.00        1.90      110.52        1.59   -3% -    2%\n              Phrase        9.03        0.23        7.62        0.41  -22% -   -8%\n             Prefix3      123.56        8.42      110.94        5.43  -20% -    1%\n             Respell      102.37        1.11      101.79        1.38   -2% -    1%\n        SloppyPhrase        3.97        0.19        3.52        0.07  -17% -   -4%\n            SpanNear        8.24        0.18        7.22        0.25  -17% -   -7%\n                Term       45.16        3.15       37.47        2.32  -27% -   -5%\n        TermBGroup1M       17.19        1.09       15.86        0.77  -17% -    3%\n      TermBGroup1M1P       23.47        1.66       20.43        1.16  -23% -   -1%\n         TermGroup1M       19.20        1.14       17.73        0.84  -16% -    2%\n            Wildcard       42.75        3.27       36.75        1.96  -24% -   -1%\n\n\n\nMaybe we should try PACKED_SINGLE_BLOCK for some special value of numBits, instead of using PACKED all the time?\n\nThanks to Adrien, we have a more direct API in LUCENE-4239, I'm trying that now. ",
            "author": "Han Jiang",
            "id": "comment-13419144"
        },
        {
            "date": "2012-07-20T17:15:04+0000",
            "content": "Patch with the decoder interface, mentioned in LUCENE-4239. I'm afraid that the for loop of readLong() hurts the performance. Here is the comparison against last patch:\n\n                Task    QPS base StdDev base    QPS comp StdDev comp      Pct diff\n         AndHighHigh       21.89        0.64       22.14        0.43   -3% -    6%\n          AndHighMed       52.23        2.34       52.94        1.74   -6% -    9%\n              Fuzzy1       86.61        1.63       87.29        3.14   -4% -    6%\n              Fuzzy2       30.54        0.54       30.95        1.18   -4% -    7%\n              IntNRQ       38.00        1.23       38.14        1.04   -5% -    6%\n          OrHighHigh       16.37        0.21       16.68        0.79   -4% -    8%\n           OrHighMed       39.59        0.69       40.34        2.16   -5% -    9%\n            PKLookup      111.51        1.34      112.78        1.37   -1% -    3%\n              Phrase        4.54        0.12        4.52        0.13   -5% -    5%\n             Prefix3      107.85        2.51      109.13        2.10   -3% -    5%\n             Respell      123.21        2.18      125.15        5.01   -4% -    7%\n        SloppyPhrase        6.51        0.11        6.44        0.29   -7% -    5%\n            SpanNear        5.36        0.16        5.31        0.14   -6% -    4%\n                Term       42.49        1.66       44.10        1.86   -4% -   12%\n        TermBGroup1M       17.86        0.80       17.82        0.51   -7% -    7%\n      TermBGroup1M1P       21.08        0.55       21.10        0.62   -5% -    5%\n         TermGroup1M       19.57        0.82       19.57        0.64   -7% -    7%\n            Wildcard       43.99        1.21       44.80        1.10   -3% -    7%\n\n ",
            "author": "Han Jiang",
            "id": "comment-13419359"
        },
        {
            "date": "2012-07-20T17:46:17+0000",
            "content": "\nI'm afraid that the for loop of readLong() hurts the performance. Here is the comparison against last patch:\n\nI think so too. I think in each enum, up front you want a pre-allocated byte[] (maximum size possible for the block),\nand you do ByteBuffer.wrap.asLongBuffer.\n\nafter you read the header, call readBytes() and then just rewind()?\n\nSo this is just like what you do now in the branch, except with LongBuffer instead of IntBuffer ",
            "author": "Robert Muir",
            "id": "comment-13419381"
        },
        {
            "date": "2012-07-20T18:57:20+0000",
            "content": "So I changed the patch to readBytes():\n\nbase: PackedInts.getReaderNoHeader().get(long[]), file io is handled by PackedInts.\ncomp: PackedInts.getDecoder().decode(LongBuffer,LongBuffer), use byte[] to hold the compressed block, and ByteBuffer.wrap().asLongBuffer as a wrapper.\n\nWell, not as expected.\n\n                Task    QPS base StdDev base    QPS comp StdDev comp      Pct diff\n         AndHighHigh       23.78        1.06       23.38        0.42   -7% -    4%\n          AndHighMed       52.06        3.28       50.82        1.21  -10% -    6%\n              Fuzzy1       88.56        0.59       88.98        2.38   -2% -    3%\n              Fuzzy2       28.80        0.36       28.97        0.83   -3% -    4%\n              IntNRQ       41.92        1.67       41.34        0.50   -6% -    3%\n          OrHighHigh       15.85        0.45       15.89        0.39   -4% -    5%\n           OrHighMed       20.38        0.61       20.50        0.62   -5% -    6%\n            PKLookup      110.72        2.19      111.74        2.53   -3% -    5%\n              Phrase        7.51        0.12        7.05        0.18   -9% -   -2%\n             Prefix3      106.27        2.65      105.37        1.13   -4% -    2%\n             Respell      112.03        0.81      112.79        2.71   -2% -    3%\n        SloppyPhrase       15.43        0.48       14.92        0.27   -7% -    1%\n            SpanNear        3.52        0.10        3.41        0.06   -7% -    1%\n                Term       39.19        1.34       39.04        0.81   -5% -    5%\n        TermBGroup1M       18.45        0.68       18.33        0.56   -7% -    6%\n      TermBGroup1M1P       22.78        0.90       22.26        0.56   -8% -    4%\n         TermGroup1M       19.50        0.73       19.42        0.63   -7% -    6%\n            Wildcard       29.56        1.13       29.18        0.28   -5% -    3%\n\n ",
            "author": "Han Jiang",
            "id": "comment-13419444"
        },
        {
            "date": "2012-07-23T16:02:57+0000",
            "content": "FYI: I committed the TestPostingsFormat here to trunk/4.x to get it going in jenkins.\n\nI will merge back to the branch... it can then be modified/improved as usual! ",
            "author": "Robert Muir",
            "id": "comment-13420732"
        },
        {
            "date": "2012-07-30T17:07:40+0000",
            "content": "Previous experiments showed a net loss with packed ints API, however there're slight difference e.g. all-value-the-same case is not handled equally. I suppose these two patches should make the comparison fair enough. \n\nBase: BlockForPF + hardcoded decoder\nComp: BlockForPF + PackedInts.Decoder\n\n                Task    QPS base StdDev base    QPS comp StdDev comp      Pct diff\n         AndHighHigh       25.66        0.31       22.61        1.21  -17% -   -6%\n          AndHighMed       74.17        1.45       59.48        3.62  -26% -  -13%\n              Fuzzy1       95.60        1.51       96.06        2.22   -3% -    4%\n              Fuzzy2       28.67        0.50       28.51        0.75   -4% -    3%\n              IntNRQ       33.31        0.60       30.73        1.51  -13% -   -1%\n          OrHighHigh       17.58        0.59       16.22        1.18  -17% -    2%\n           OrHighMed       34.42        0.93       32.14        2.33  -15% -    2%\n            PKLookup      217.08        4.25      213.76        1.37   -4% -    1%\n              Phrase        6.10        0.12        5.34        0.07  -15% -   -9%\n             Prefix3       77.27        1.26       70.42        2.87  -13% -   -3%\n             Respell       92.91        1.34       92.61        1.83   -3% -    3%\n        SloppyPhrase        5.35        0.16        5.00        0.29  -14% -    1%\n            SpanNear        6.05        0.15        5.47        0.07  -12% -   -6%\n                Term       37.62        0.32       33.08        1.70  -17% -   -6%\n        TermBGroup1M       17.45        0.64       16.40        0.73  -13% -    1%\n      TermBGroup1M1P       25.20        0.69       23.47        1.24  -14% -    0%\n         TermGroup1M       18.53        0.65       17.40        0.76  -13% -    1%\n            Wildcard       44.39        0.49       40.51        1.69  -13% -   -3%\n\n\n\nHmm, quite strange that we are already getting perf loss with baseline patch:\n\nBase: BlockForPF in current branch\nComp: BlockForPF + hardcoded decoder(patch file)\n\n                Task    QPS base StdDev base    QPS comp StdDev comp      Pct diff\n         AndHighHigh       26.71        0.98       24.15        0.82  -15% -   -2%\n          AndHighMed       73.37        5.01       61.30        1.97  -24% -   -7%\n              Fuzzy1       85.73        4.95       84.30        1.79   -9% -    6%\n              Fuzzy2       30.15        2.05       29.52        0.66  -10% -    7%\n              IntNRQ       38.56        1.69       36.91        1.27  -11% -    3%\n          OrHighHigh       16.98        1.48       16.82        0.94  -13% -   14%\n           OrHighMed       34.60        2.79       34.70        2.22  -13% -   16%\n            PKLookup      214.93        3.99      213.86        1.23   -2% -    1%\n              Phrase       11.53        0.23       10.75        0.42  -12% -   -1%\n             Prefix3      107.15        3.83      102.12        2.69  -10% -    1%\n             Respell       87.41        5.41       86.08        1.76   -9% -    7%\n        SloppyPhrase        5.90        0.15        5.66        0.21   -9% -    2%\n            SpanNear        4.99        0.12        4.79        0.01   -6% -   -1%\n                Term       49.37        2.38       45.53        0.49  -12% -   -2%\n        TermBGroup1M       17.23        0.40       16.44        0.53   -9% -    0%\n      TermBGroup1M1P       22.02        0.50       22.42        0.60   -3% -    7%\n         TermGroup1M       13.65        0.29       13.05        0.28   -8% -    0%\n            Wildcard       48.73        2.01       46.35        1.31  -11% -    2%\n\n ",
            "author": "Han Jiang",
            "id": "comment-13425010"
        },
        {
            "date": "2012-07-30T19:39:37+0000",
            "content": "I'm confused by these two patches: are they against trunk?  How come eg they have mods to build.xml? ",
            "author": "Michael McCandless",
            "id": "comment-13425153"
        },
        {
            "date": "2012-07-30T21:15:14+0000",
            "content": "OK I think I understand the two patches now.\n\nFirst, the build.xml changes are noise I think.  Second, the patches\nboth mix in the removal of the current For/PFor postings formats based\non sep (I will separately commit this removal: BlockPF is faster).\n\nThen, one patch (LUCENE-3892-blockFor&hardcode(base).patch) keeps\nusing the separate packed-ints impl we have, but cuts over to\nLongBuffer instead of int[] for the decoded values (still uses\nIntBuffer for the encoded values), while the other patch\n(LUCENE-3892-blockFor&packedecoder(comp).patch) uses oal.util.packed\nand LongBuffer for both encoded and decoded values.\n\nSo it's nice to see that \"merely\" switching to LongBuffer to pass\nencoded/decoded values around doesn't seem to hurt much, except for\nAnd queries (odd?), but then switching to oal.util.packed does hurt\n(also odd because our packed ints impl has been heavily optimized\nlately). ",
            "author": "Michael McCandless",
            "id": "comment-13425246"
        },
        {
            "date": "2012-07-30T22:00:27+0000",
            "content": "My benchmark results are a little different but oal.util.packed is still behind... (it compares the current branch vs. patched with PackedInts):\n\n\n                TaskQPS pforcodecStdDev pforcodecQPS pforcodec-packedintsStdDev pforcodec-packedints      Pct diff\n              Phrase       38.21        3.01       35.73        2.41  -19% -    8%\n            SpanNear       27.99        1.30       26.30        1.23  -14% -    3%\n        SloppyPhrase       43.32        2.98       41.02        2.53  -16% -    7%\n          AndHighMed      230.23        8.48      219.88        9.35  -11% -    3%\n         AndHighHigh       52.53        2.02       50.80        2.62  -11% -    5%\n              IntNRQ       43.24        3.42       41.84        2.79  -16% -   12%\n            Wildcard      113.26        3.17      109.91        3.50   -8% -    3%\n             Prefix3      194.56        9.56      189.39        9.64  -11% -    7%\n                Term      301.86       14.49      295.28       17.51  -12% -    8%\n           OrHighMed      100.60        8.30       99.06        8.00  -16% -   15%\n          OrHighHigh       32.35        2.92       31.90        2.88  -17% -   18%\n              Fuzzy2       36.27        0.67       35.87        0.93   -5% -    3%\n              Fuzzy1       81.14        1.24       80.24        1.68   -4% -    2%\n       TermGroup100K      193.40        3.36      191.27        4.13   -4% -    2%\n    TermBGroup100K1P      152.78        5.06      151.23        3.98   -6% -    5%\n      TermBGroup100K      242.78        7.06      240.71        8.01   -6% -    5%\n             Respell       85.75        1.36       85.17        2.04   -4% -    3%\n            PKLookup      206.02        5.05      205.57        4.63   -4% -    4%\n\n\n\nI am not sure why oal.util.packed is slower. The only differences I see is that they use inheritance instead of a switch block to know how to decode data and that they encode values in the high-order long bits first while the branch currently starts with the low-order int bits. I'll try to dig deeper to understand what happens... ",
            "author": "Adrien Grand",
            "id": "comment-13425296"
        },
        {
            "date": "2012-07-30T22:17:31+0000",
            "content": "I just committed a new BlockPacked postings format, which is a copy of\nBlock postings format but using oal.util.packed for encode/decode.\n\nI left Block unchanged, except I moved the util classes it had been\nusing out of oal.codecs.pfor, and removed oal.codecs.pfor.\n\nSo now we can iterate to speed up packed ints cutover, and do perf\ntests off the branch. ",
            "author": "Michael McCandless",
            "id": "comment-13425306"
        },
        {
            "date": "2012-07-30T22:19:26+0000",
            "content": "Sorry I meant to say: the BlockPacked PF is from Billy's LUCENE-3892-blockFor&packedecoder(comp).patch. ",
            "author": "Michael McCandless",
            "id": "comment-13425313"
        },
        {
            "date": "2012-07-30T23:37:58+0000",
            "content": "I tested Block vs BlockPacked as checked in.\n\nOn a Westmere Xeon machine (Java 1.7.0_04):\n\n\n                Task    QPS base StdDev base     QPS for  StdDev for      Pct diff\n          AndHighMed       15.14        0.14       13.78        0.13  -10% -   -7%\n        SloppyPhrase        2.55        0.11        2.33        0.09  -15% -   -1%\n          OrHighHigh        3.75        0.16        3.44        0.09  -14% -   -1%\n            Wildcard        8.44        0.01        7.78        0.28  -11% -   -4%\n            SpanNear        1.11        0.04        1.03        0.04  -13% -    0%\n             Prefix3       17.91        0.08       16.63        0.50  -10% -   -3%\n           OrHighMed       11.35        0.65       10.63        0.44  -15% -    3%\n              IntNRQ        6.73        0.03        6.32        0.27  -10% -   -1%\n        TermBGroup1M        3.87        0.03        3.68        0.04   -6% -   -3%\n         AndHighHigh        4.86        0.09        4.63        0.03   -7% -   -2%\n              Phrase        1.10        0.06        1.05        0.06  -14% -    6%\n                Term        7.86        0.03        7.52        0.04   -5% -   -3%\n      TermBGroup1M1P        4.65        0.12        4.49        0.06   -6% -    0%\n         TermGroup1M        2.97        0.04        2.88        0.02   -4% -   -1%\n              Fuzzy1       71.22        1.93       71.02        1.44   -4% -    4%\n              Fuzzy2       49.76        1.33       49.90        1.23   -4% -    5%\n             Respell       76.23        2.67       76.93        2.67   -5% -    8%\n            PKLookup      161.89        3.28      168.28        7.87   -2% -   11%\n\n\n\nAnd on an desktop Ivy Bridge (Java 1.7.0_04):\n\n                Task    QPS base StdDev base     QPS for  StdDev for      Pct diff\n          AndHighMed       17.32        0.12       15.41        0.03  -11% -  -10%\n        SloppyPhrase        2.74        0.21        2.56        0.11  -16% -    5%\n              Phrase        1.32        0.07        1.23        0.06  -15% -    3%\n            Wildcard        9.65        0.11        9.08        0.12   -8% -   -3%\n            SpanNear        1.20        0.01        1.13        0.01   -7% -   -3%\n         AndHighHigh        5.32        0.03        5.04        0.02   -6% -   -4%\n             Prefix3       18.93        0.20       18.04        0.24   -6% -   -2%\n              IntNRQ        7.79        0.13        7.48        0.13   -7% -    0%\n                Term        9.48        0.10        9.15        0.43   -8% -    2%\n        TermBGroup1M        4.74        0.05        4.59        0.12   -6% -    0%\n           OrHighMed       13.01        0.24       12.60        0.55   -9% -    2%\n          OrHighHigh        4.08        0.05        3.97        0.17   -8% -    2%\n         TermGroup1M        3.30        0.03        3.22        0.07   -5% -    0%\n      TermBGroup1M1P        5.52        0.11        5.42        0.22   -7% -    4%\n            PKLookup      194.62        4.43      193.44        5.07   -5% -    4%\n              Fuzzy1       79.23        1.31       79.21        0.96   -2% -    2%\n             Respell       78.97        1.04       79.87        1.15   -1% -    3%\n              Fuzzy2       56.17        0.93       56.82        0.64   -1% -    4%\n\n\n\nSo packed is still behind ... ",
            "author": "Michael McCandless",
            "id": "comment-13425388"
        },
        {
            "date": "2012-08-05T23:33:49+0000",
            "content": "I just committed an optimization to BlockPF DocsEnum.advance, inlining\nthe scanning step (still have to do D&PEnum and EverythingEnum):\n\n\n                Task    QPS base StdDev base     QPS for  StdDev for      Pct diff\n              IntNRQ       12.46        1.45       11.60        0.04  -16% -    5%\n            Wildcard       54.36        2.75       52.72        0.38   -8% -    2%\n             Prefix3       85.43        4.97       83.08        0.47   -8% -    3%\n              Fuzzy2       63.86        2.13       62.44        1.79   -8% -    4%\n             Respell       62.75        1.52       61.42        2.02   -7% -    3%\n              Fuzzy1       75.68        1.65       74.69        1.44   -5% -    2%\n         LowSpanNear        9.24        0.20        9.13        0.19   -5% -    3%\n            PKLookup      192.89        2.91      190.66        2.43   -3% -    1%\n        HighSpanNear        1.71        0.05        1.69        0.05   -6% -    4%\n         MedSpanNear        4.80        0.11        4.76        0.12   -5% -    4%\n           MedPhrase       12.57        0.27       12.56        0.21   -3% -    3%\n     MedSloppyPhrase        6.57        0.11        6.56        0.11   -3% -    3%\n           LowPhrase       21.55        0.35       21.55        0.28   -2% -    2%\n     LowSloppyPhrase        7.25        0.16        7.28        0.12   -3% -    4%\n          HighPhrase        1.81        0.11        1.82        0.10  -10% -   13%\n    HighSloppyPhrase        1.94        0.10        1.96        0.05   -6% -    9%\n             LowTerm      512.53        5.66      518.31        2.30    0% -    2%\n             MedTerm      196.09        4.68      198.76        0.30   -1% -    3%\n            HighTerm       35.53        0.95       36.11        0.03   -1% -    4%\n           OrHighMed       23.34        0.83       23.85        0.70   -4% -    9%\n           OrHighLow       26.91        0.98       27.53        0.82   -4% -    9%\n          OrHighHigh       11.27        0.41       11.53        0.34   -4% -    9%\n         AndHighHigh       21.24        0.05       23.79        0.13   11% -   12%\n          AndHighLow      553.19        8.47      621.35        4.01    9% -   14%\n          AndHighMed       57.45        0.13       67.78        0.70   16% -   19%\n\n ",
            "author": "Michael McCandless",
            "id": "comment-13428936"
        },
        {
            "date": "2012-08-07T14:47:17+0000",
            "content": "I backported Mike's changes to the BlockPacked codec and tried to understand why it was slower than Block...\n\nThe use of java.nio.*Buffer seemed to be the bottleneck (ByteBuffer.asLongBuffer and ByteBuffer.getLong especially are very slow) of the decoding step so I switched back to decoding from long[] (instead of LongBuffer) and added direct decoding from byte[] to avoid having to convert the bytes to longs before decoding.\n\nTests passed with -Dtests.postingsformat=BlockPacked. Here are the results of the benchmark (unfortunately, it started before Mike committed r1370179):\n\n\n                Task    QPS 3892 StdDev 3892QPS 3892-packedStdDev 3892-packed      Pct diff\n            PKLookup      259.41        9.06      255.77        8.89   -8% -    5%\n          AndHighLow     1656.30       50.44     1653.85       55.05   -6% -    6%\n         AndHighHigh       82.90        1.82       83.47        2.52   -4% -    6%\n          AndHighMed      274.76       11.11      278.51       13.42   -7% -   10%\n             Prefix3      285.41        4.82      289.60        6.31   -2% -    5%\n            HighTerm      230.78       14.33      235.16       20.61  -12% -   18%\n              IntNRQ       55.91        1.03       57.13        2.73   -4% -    9%\n             LowTerm     1720.10       47.06     1759.16       55.47   -3% -    8%\n            Wildcard      290.54        3.82      297.39        5.42    0% -    5%\n             MedTerm      733.01       35.38      750.46       50.37   -8% -   14%\n        HighSpanNear        6.93        0.23        7.12        0.39   -6% -   11%\n          HighPhrase        6.46        0.22        6.65        0.46   -7% -   14%\n             Respell       96.11        2.84       99.00        3.98   -3% -   10%\n          OrHighHigh       38.07        2.53       39.23        3.06  -10% -   19%\n              Fuzzy2       50.29        1.70       51.87        2.25   -4% -   11%\n           MedPhrase       26.20        0.94       27.03        1.07   -4% -   11%\n           OrHighMed      138.83        7.76      143.54        9.79   -8% -   16%\n              Fuzzy1      100.58        2.15      104.21        3.99   -2% -    9%\n    HighSloppyPhrase        5.26        0.11        5.45        0.24   -3% -   10%\n           OrHighLow       78.43        5.55       81.80        6.89  -10% -   21%\n         MedSpanNear       32.75        1.13       34.28        1.73   -3% -   13%\n           LowPhrase       90.27        3.20       95.06        3.58   -2% -   13%\n         LowSpanNear       46.40        1.95       48.89        2.40   -3% -   15%\n     MedSloppyPhrase       36.29        1.00       38.59        1.46    0% -   13%\n     LowSloppyPhrase       37.41        1.11       40.48        1.39    1% -   15%\n\n\n\nMike, Billy, could you check that BLockPacked is at least as fast as Block on your computer too? ",
            "author": "Adrien Grand",
            "id": "comment-13430373"
        },
        {
            "date": "2012-08-07T16:50:01+0000",
            "content": "Thanks Adrien! Your codes are really clean!\n\nAt first glance, I think we should still support all-value-the-same case? For some applications(like index with payloads), that might be helpful.\n\nAnd, I'm a little confused about your performance test. Did you use BlockPF before r1370179 as a baseline, and compare it with your latest commit? Here, I tested these two PF under latest versions(r1370345).\n\n\n                Task    QPS base StdDev base    QPS comp StdDev comp      Pct diff\n         AndHighHigh      124.53        9.36      100.46        3.31  -27% -   -9%\n          AndHighLow     2141.08       63.93     1922.73       36.32  -14% -   -5%\n          AndHighMed      281.48       36.49      218.68       13.10  -35% -   -5%\n              Fuzzy1       84.33        2.56       83.94        1.67   -5% -    4%\n              Fuzzy2       30.49        1.13       30.48        0.71   -5% -    6%\n          HighPhrase        9.08        0.28        7.56        0.20  -21% -  -11%\n    HighSloppyPhrase        5.46        0.21        4.88        0.23  -17% -   -2%\n        HighSpanNear       10.12        0.21        9.21        0.30  -13% -   -3%\n            HighTerm      176.52        6.13      146.13        5.43  -22% -  -11%\n              IntNRQ       59.56        1.98       51.05        1.33  -19% -   -9%\n           LowPhrase       40.02        1.03       32.75        0.37  -21% -  -15%\n     LowSloppyPhrase       59.59        2.85       51.49        1.33  -19% -   -6%\n         LowSpanNear       73.86        3.17       61.98        1.45  -21% -  -10%\n             LowTerm     1755.38       15.56     1622.61       26.87   -9% -   -5%\n           MedPhrase       25.99        0.47       21.01        0.17  -21% -  -16%\n     MedSloppyPhrase       30.52        0.89       24.77        0.55  -22% -  -14%\n         MedSpanNear       22.26        0.43       18.73        0.47  -19% -  -12%\n             MedTerm      651.90       18.97      573.34       19.25  -17% -   -6%\n          OrHighHigh       26.75        0.33       23.53        0.50  -14% -   -9%\n           OrHighLow      151.69        2.13      134.17        3.19  -14% -   -8%\n           OrHighMed      102.48        1.48       90.73        2.01  -14% -   -8%\n            PKLookup      216.59        5.70      215.99        2.99   -4% -    3%\n             Prefix3      166.00        0.78      145.25        1.29  -13% -  -11%\n             Respell       82.01        3.01       82.80        1.66   -4% -    6%\n            Wildcard      151.66        2.22      141.14        1.57   -9% -   -4%\n\n\n\nStrange that it isn't working well on my computer. And results are similar when I change MMapDirectory to NIOFSDirectory. ",
            "author": "Han Jiang",
            "id": "comment-13430423"
        },
        {
            "date": "2012-08-07T17:05:11+0000",
            "content": "Hmm also not great results on my env (base=Block, packed=BlockPacked), based on current branch head:\n\n\n                Task    QPS base StdDev base  QPS packedStdDev packed      Pct diff\n          AndHighMed       59.23        3.07       34.24        0.69  -46% -  -37%\n          AndHighLow      576.35       21.09      349.57        7.44  -42% -  -35%\n         AndHighHigh       23.83        0.72       15.53        0.29  -37% -  -31%\n           MedPhrase       12.56        0.20        8.87        0.31  -32% -  -25%\n           LowPhrase       20.52        0.21       14.89        0.43  -30% -  -24%\n     MedSloppyPhrase        7.46        0.20        5.41        0.13  -31% -  -23%\n     LowSloppyPhrase        6.73        0.18        4.92        0.12  -30% -  -22%\n         LowSpanNear        7.63        0.32        5.65        0.19  -31% -  -20%\n    HighSloppyPhrase        1.90        0.08        1.52        0.05  -25% -  -14%\n          HighPhrase        1.57        0.04        1.26        0.08  -26% -  -12%\n         MedSpanNear        3.84        0.18        3.14        0.14  -25% -  -10%\n             LowTerm      433.22       34.89      364.03       15.63  -25% -   -4%\n        HighSpanNear        1.40        0.07        1.19        0.06  -23% -   -6%\n              IntNRQ        9.50        0.43        8.09        0.92  -27% -    0%\n            HighTerm       29.47        4.89       25.46        2.35  -32% -   13%\n             MedTerm      148.76       21.53      129.17        9.59  -29% -    9%\n             Prefix3       72.81        2.20       63.65        3.88  -20% -   -4%\n            Wildcard       44.79        0.92       39.91        2.20  -17% -   -4%\n           OrHighMed       16.81        0.48       15.28        0.21  -12% -   -5%\n           OrHighLow       21.85        0.67       20.03        0.32  -12% -   -3%\n          OrHighHigh        8.49        0.28        7.80        0.14  -12% -   -3%\n              Fuzzy1       61.33        1.95       58.91        1.11   -8% -    1%\n            PKLookup      156.87        1.14      154.08        2.13   -3% -    0%\n             Respell       58.72        1.57       59.60        1.28   -3% -    6%\n              Fuzzy2       60.98        2.34       62.03        1.89   -5% -    9%\n\n\n\nI think optimizing the all-values-same case is actually quite important for payloads (but luceneutil doesn't test this today).\n\nBut, curiously, my BlockPacked index is a bit smaller than my Block index (4643 MB vs 4650 MB).\n\nI do wonder about using long[] to hold the uncompressed results (they only need int[]); that's one big difference still.  Also: I'd love to see how acceptableOverheadRatio > 0 does ... (and, using PACKED_SINGLE_BLOCK ... we'd have to put a bit in the header to record the format). ",
            "author": "Michael McCandless",
            "id": "comment-13430439"
        },
        {
            "date": "2012-08-07T18:31:58+0000",
            "content": "I tried smaller block sizes than 128.  Here's 128 (base) vs 64:\n\n                Task    QPS base StdDev base QPS block64StdDev block64      Pct diff\n         AndHighHigh       23.91        0.57       22.28        0.27  -10% -   -3%\n          AndHighMed       60.63        1.02       56.96        1.13   -9% -   -2%\n     MedSloppyPhrase        7.69        0.01        7.30        0.13   -6% -   -3%\n    HighSloppyPhrase        1.93        0.02        1.83        0.04   -8% -   -1%\n     LowSloppyPhrase        6.84        0.03        6.57        0.11   -6% -   -1%\n              Fuzzy1       65.49        0.85       63.50        1.68   -6% -    0%\n          HighPhrase        1.57        0.04        1.53        0.04   -7% -    3%\n           OrHighLow       22.89        0.98       22.38        0.61   -8% -    4%\n           OrHighMed       17.65        0.70       17.27        0.43   -8% -    4%\n              IntNRQ        9.50        0.48        9.33        0.36  -10% -    7%\n          OrHighHigh        8.98        0.36        8.84        0.19   -7% -    4%\n            HighTerm       29.60        2.64       29.16        1.44  -13% -   13%\n              Fuzzy2       65.54        0.86       64.63        2.13   -5% -    3%\n            Wildcard       45.27        1.27       44.78        0.48   -4% -    2%\n             MedTerm      150.40       12.65      148.99        6.63  -12% -   12%\n             Prefix3       72.55        2.55       72.31        1.02   -5% -    4%\n             LowTerm      421.62       38.27      422.40        9.47  -10% -   12%\n         LowSpanNear        7.55        0.34        7.62        0.22   -6% -    8%\n        HighSpanNear        1.34        0.09        1.35        0.06   -9% -   12%\n           MedPhrase       12.45        0.24       12.66        0.13   -1% -    4%\n             Respell       59.54        1.80       60.95        1.86   -3% -    8%\n         MedSpanNear        3.70        0.24        3.80        0.15   -7% -   14%\n            PKLookup      154.56        2.45      158.96        1.89    0% -    5%\n           LowPhrase       20.21        0.33       20.95        0.15    1% -    6%\n          AndHighLow      577.81       12.46      637.96       29.80    3% -   18%\n\n\n\nAnd 128 (base) vs 32:\n\n                Task    QPS base StdDev base QPS block64StdDev block64      Pct diff\n         AndHighHigh       23.86        0.52       20.68        0.59  -17% -   -8%\n              IntNRQ        9.48        0.38        8.84        0.46  -15% -    2%\n    HighSloppyPhrase        1.87        0.04        1.76        0.06  -11% -    0%\n             Prefix3       72.65        2.18       68.24        2.96  -12% -    1%\n            HighTerm       29.91        1.40       28.28        2.94  -19% -    9%\n            Wildcard       44.74        0.83       42.43        1.49  -10% -    0%\n        HighSpanNear        1.37        0.08        1.30        0.07  -15% -    6%\n             MedTerm      152.73        5.28      145.45       14.69  -17% -    8%\n     MedSloppyPhrase        7.46        0.12        7.12        0.25   -9% -    0%\n          HighPhrase        1.57        0.03        1.50        0.01   -7% -   -1%\n           OrHighLow       22.94        0.70       22.00        1.10  -11% -    3%\n          AndHighMed       58.72        1.79       56.60        1.95   -9% -    2%\n     LowSloppyPhrase        6.67        0.10        6.44        0.20   -7% -    1%\n           OrHighMed       17.52        0.56       17.00        0.82  -10% -    5%\n         LowSpanNear        7.53        0.35        7.34        0.39  -11% -    7%\n          OrHighHigh        8.84        0.31        8.62        0.43  -10% -    6%\n         MedSpanNear        3.79        0.20        3.71        0.21  -12% -    9%\n            PKLookup      153.34        3.22      150.19        4.91   -7% -    3%\n              Fuzzy1       62.93        1.77       62.28        2.23   -7% -    5%\n             LowTerm      410.23       21.57      410.83       35.19  -13% -   14%\n           MedPhrase       12.55        0.14       12.65        0.08    0% -    2%\n           LowPhrase       20.42        0.17       20.77        0.21    0% -    3%\n              Fuzzy2       61.44        3.12       64.13        1.97   -3% -   13%\n             Respell       56.65        3.29       60.21        1.39   -1% -   15%\n          AndHighLow      588.05       12.37      720.63       19.33   16% -   28%\n\n\n\nIt looks like there's some speedup to AndHighLow and LowPhrase ... but\nslowdowns in other (harder) queries... so I think net/net we should\nleave block size at 128. ",
            "author": "Michael McCandless",
            "id": "comment-13430507"
        },
        {
            "date": "2012-08-08T01:38:48+0000",
            "content": "Thanks Mike. And detailed comparison result on my computer is here: http://pastebin.com/HLaAuCNp\nI tried block size range from 1024~32, also used 128 as the base. ",
            "author": "Han Jiang",
            "id": "comment-13430798"
        },
        {
            "date": "2012-08-08T13:53:33+0000",
            "content": "And result on skipMulitiplier, use current 8 as the baseline: http://pastebin.com/TG4C6u6S\nSomewhat noisy, but or-queries benifit a little when skipMultiplier=32.\nAnd results when we set blockSize fixed to 64: http://pastebin.com/FQBiKGim ",
            "author": "Han Jiang",
            "id": "comment-13431117"
        },
        {
            "date": "2012-08-08T15:51:38+0000",
            "content": "I tested BulkVInt again, ie to decouple the cutover from Sep to BlockPF vs the\nvInt/FOR change.\n\nBase=Lucene40, comp=BlockPF(BulkVInt):\n\n\n                Task    QPS base StdDev baseQPS bulkVIntStdDev bulkVInt   Pct diff\n          AndHighLow      857.35       20.10      614.20       10.73  -31% -  -25%\n             Respell       62.99        2.35       60.53        1.34   -9% -    2%\n          AndHighMed       65.64        2.24       63.61        0.93   -7% -    1%\n              Fuzzy2       62.83        1.75       61.72        1.31   -6% -    3%\n            PKLookup      195.97        1.87      194.73        5.00   -4% -    2%\n              IntNRQ       12.50        0.10       12.43        1.49  -13% -   12%\n              Fuzzy1       72.68        1.12       73.84        0.88   -1% -    4%\n          HighPhrase        1.75        0.05        1.78        0.08   -5% -    8%\n         LowSpanNear        9.01        0.12        9.27        0.13    0% -    5%\n           LowPhrase       19.73        0.43       20.64        0.15    1% -    7%\n         MedSpanNear        4.52        0.06        4.74        0.01    3% -    6%\n           MedPhrase       11.74        0.31       12.40        0.09    2% -    9%\n             LowTerm      435.96       13.41      467.22        9.10    1% -   12%\n             Prefix3       75.47        0.51       81.52        4.38    1% -   14%\n            Wildcard       48.66        0.44       52.79        2.79    1% -   15%\n          OrHighHigh       10.11        0.63       11.06        0.32    0% -   20%\n           OrHighMed       20.85        1.31       22.99        0.63    0% -   20%\n        HighSpanNear        1.50        0.02        1.67        0.01    8% -   13%\n           OrHighLow       23.55        1.46       26.51        0.76    2% -   23%\n     LowSloppyPhrase        6.45        0.14        7.37        0.18    9% -   19%\n             MedTerm      163.46       10.30      188.55        5.22    5% -   26%\n     MedSloppyPhrase        5.74        0.12        6.65        0.15   10% -   20%\n    HighSloppyPhrase        1.69        0.04        1.98        0.11    8% -   26%\n         AndHighHigh       19.00        0.53       22.91        0.24   16% -   25%\n            HighTerm       28.28        1.95       34.48        0.99   10% -   34%\n\n\n\n\nBase=BlockPF(BulkVInt), comp=BlockPF(FOR):\n\n\n                Task    QPS base StdDev base     QPS for  StdDev for      Pct diff\n              IntNRQ       12.10        1.70       11.61        0.02  -16% -   11%\n    HighSloppyPhrase        2.00        0.11        1.95        0.03   -8% -    4%\n          HighPhrase        1.85        0.05        1.81        0.07   -8% -    4%\n            Wildcard       52.32        3.09       52.49        0.24   -5% -    7%\n     LowSloppyPhrase        7.41        0.24        7.43        0.19   -5% -    6%\n     MedSloppyPhrase        6.69        0.18        6.72        0.21   -5% -    6%\n           OrHighMed       22.99        0.55       23.23        0.85   -4% -    7%\n             Respell       61.99        2.01       62.70        1.57   -4% -    7%\n           OrHighLow       26.52        0.69       26.83        1.00   -5% -    7%\n              Fuzzy1       74.72        1.34       75.59        1.43   -2% -    4%\n            PKLookup      189.68        7.14      192.09        3.82   -4% -    7%\n          OrHighHigh       11.05        0.27       11.21        0.42   -4% -    7%\n              Fuzzy2       62.78        1.86       63.70        1.87   -4% -    7%\n        HighSpanNear        1.65        0.03        1.69        0.02    0% -    5%\n             Prefix3       80.25        5.44       82.57        1.03   -4% -   11%\n         AndHighHigh       22.79        0.11       23.53        0.13    2% -    4%\n         LowSpanNear        9.16        0.26        9.48        0.21   -1% -    8%\n         MedSpanNear        4.67        0.09        4.84        0.07    0% -    7%\n           MedPhrase       12.59        0.26       13.07        0.24    0% -    7%\n           LowPhrase       20.86        0.33       22.06        0.30    2% -    8%\n          AndHighLow      618.27       13.15      655.52        3.30    3% -    8%\n            HighTerm       33.95        1.11       36.02        0.08    2% -    9%\n             MedTerm      186.09        5.51      198.46        0.09    3% -    9%\n          AndHighMed       63.71        1.15       69.15        0.45    5% -   11%\n             LowTerm      469.17        7.25      514.55        2.83    7% -   12%\n\n\n\nSo ... most of the gains come from BlockPF cutover.  This is sort of\n... surprising/disappointing, ie, our bottlenecks are the abstraction\nlayers, not the actual decode cost.  Still it's good to make progress\non removing the abstractions.\n\nAlso, it looks like the only query that is slower than Lucene40 is\nAndHighLow ... however, it's also an extremely fast query to begin\nwith so I think it's a fine tradeoff that it gets slower while the\nhard/slower queries get faster. ",
            "author": "Michael McCandless",
            "id": "comment-13431183"
        },
        {
            "date": "2012-08-08T16:08:24+0000",
            "content": "\nSo ... most of the gains come from BlockPF cutover. This is sort of\n... surprising/disappointing, ie, our bottlenecks are the abstraction\nlayers, not the actual decode cost. Still it's good to make progress\non removing the abstractions.\n\nI don't think its that disappointing. This isnt a very interesting\nbenchmark for a compression algorithm like FOR: instead imagine the\nvery common case of apps today indexing small fields like product names,\nrestaurant names, or something like that. Freqs are nearly always 1,\nand positions are tiny, but often people still want the ability to\nuse things like phrase queries. And imagine cases where people\nare indexing data from a database and there are only a few unique\nvalues (e.g. product type = tshirt, pants, shoes) in a field. \n\nI think the wikipedia benchmark doesn't do a very good job of illustrating \nperformance on use-cases like this, which I think are common and also\nwhere I'm fairly positive FOR will be a win. \n\nIts nice that its not slower or too much bigger in the \"worst case\"\nof large docs where the numbers aren't so tiny?\n\n\nAlso, it looks like the only query that is slower than Lucene40 is\nAndHighLow ... however, it's also an extremely fast query to begin\nwith so I think it's a fine tradeoff that it gets slower while the\nhard/slower queries get faster.\n\n+1, lets not even think twice about that one.\n ",
            "author": "Robert Muir",
            "id": "comment-13431193"
        },
        {
            "date": "2012-08-08T19:37:13+0000",
            "content": "I did some changes to the BlockPacked codec:\n\n\tencoding and decoding using int[] instead of long[]\n\tselection of the format based on a configurable overhead ratio.\n\n\n\nThe results are encouraging (using acceptableOverheadRatio = PackedInts.DEFAULT = 20%):\n\n                Task    QPS 3892 StdDev 3892QPS 3892-packedStdDev 3892-packed      Pct diff\n            PKLookup      256.93        8.89      256.85        7.47   -6% -    6%\n           OrHighLow      145.14        9.86      145.14        9.35  -12% -   14%\n             Respell      110.26        1.84      110.27        2.01   -3% -    3%\n         AndHighHigh      112.97        0.81      113.19        2.17   -2% -    2%\n              Fuzzy1      102.15        1.47      102.86        3.13   -3% -    5%\n          OrHighHigh       94.56        6.56       95.43        6.35  -11% -   15%\n              Fuzzy2       42.49        0.77       42.89        1.43   -4% -    6%\n           OrHighMed      175.30       11.34      177.42       10.83  -10% -   14%\n          AndHighLow     1925.02       23.92     1952.57       48.68   -2% -    5%\n          HighPhrase        8.96        0.41        9.11        0.46   -7% -   11%\n            Wildcard      189.79        2.13      193.12        1.57    0% -    3%\n        HighSpanNear        6.47        0.15        6.59        0.25   -4% -    8%\n             Prefix3      256.67        2.58      262.40        2.84    0% -    4%\n             LowTerm     1746.52       52.80     1789.54       54.30   -3% -    8%\n            HighTerm      238.70       13.46      245.63       16.60   -9% -   16%\n             MedTerm      923.64       38.19      951.18       46.85   -5% -   12%\n          AndHighMed      364.46        3.65      377.09       10.03    0% -    7%\n              IntNRQ       56.58        1.02       58.84        0.80    0% -    7%\n    HighSloppyPhrase       11.73        0.30       12.40        0.62   -2% -   13%\n         LowSpanNear       29.64        0.96       32.44        0.98    2% -   16%\n         MedSpanNear       22.96        0.72       25.16        0.85    2% -   16%\n           MedPhrase       40.99        1.25       45.09        1.24    3% -   16%\n     LowSloppyPhrase       37.88        0.99       41.98        1.49    4% -   17%\n           LowPhrase       64.40        2.04       71.84        1.41    5% -   17%\n     MedSloppyPhrase       42.29        1.16       47.32        1.54    5% -   18%\n\n\n\nI hope this will be confirmed on your computers this time . ",
            "author": "Adrien Grand",
            "id": "comment-13431324"
        },
        {
            "date": "2012-08-08T22:59:19+0000",
            "content": "I also see (smaller) gains with BlockPacked vs Block (this is 10M doc index):\n\n                Task    QPS base StdDev base  QPS packedStdDev packed      Pct diff\n          AndHighMed       69.19        0.53       66.43        0.63   -5% -   -2%\n              Fuzzy2       63.71        1.24       62.25        1.58   -6% -    2%\n             Respell       62.69        1.41       61.53        1.47   -6% -    2%\n              IntNRQ       11.86        0.43       11.73        0.03   -4% -    2%\n              Fuzzy1       75.48        1.21       75.05        1.52   -4% -    3%\n            Wildcard       53.23        0.63       52.96        0.25   -2% -    1%\n         MedSpanNear        4.88        0.16        4.88        0.11   -5% -    5%\n            PKLookup      191.48        2.84      191.62        3.98   -3% -    3%\n            HighTerm       35.71        0.63       35.91        0.06   -1% -    2%\n             Prefix3       83.14        1.34       83.83        0.49   -1% -    3%\n             LowTerm      513.35        0.77      517.92        1.50    0% -    1%\n        HighSpanNear        1.70        0.06        1.71        0.03   -4% -    6%\n         AndHighHigh       23.45        0.09       23.69        0.10    0% -    1%\n           OrHighLow       27.27        1.06       27.59        0.15   -3% -    5%\n           OrHighMed       23.61        0.92       23.89        0.17   -3% -    6%\n          OrHighHigh       11.42        0.44       11.59        0.12   -3% -    6%\n     MedSloppyPhrase        6.84        0.17        6.95        0.23   -4% -    7%\n           LowPhrase       22.02        0.39       22.43        0.15    0% -    4%\n             MedTerm      196.76        3.01      200.62        0.33    0% -    3%\n         LowSpanNear        9.60        0.24        9.82        0.31   -3% -    8%\n           MedPhrase       13.08        0.30       13.41        0.12    0% -    5%\n     LowSloppyPhrase        7.55        0.21        7.77        0.27   -3% -    9%\n          AndHighLow      649.84       18.26      669.08        6.63    0% -    6%\n    HighSloppyPhrase        1.98        0.08        2.04        0.09   -4% -   12%\n          HighPhrase        1.76        0.11        1.96        0.10    0% -   24%\n\n\n\nThe index is 4669 MB with Block and 4790 with BlockPacked = ~2.6%\nlarger ... seems worth it!  Apps can always tune the 20% too. ",
            "author": "Michael McCandless",
            "id": "comment-13431485"
        },
        {
            "date": "2012-08-08T23:05:02+0000",
            "content": "I created a non-specialized (ie single method to handle all numBits\ncases) packed int decoder that decodes directly from byte[].  Baseline\nis current BlockPF (FOR w/ specialized decoder), comp is w/ the patch\n(using non-specialized decoder):\n\n\n                Task    QPS base StdDev base     QPS for  StdDev for      Pct diff\n          AndHighMed       69.04        0.77       36.41        1.91  -50% -  -43%\n          AndHighLow      649.70       17.03      346.71       18.22  -50% -  -42%\n         LowSpanNear        9.88        0.25        5.53        0.06  -45% -  -42%\n           MedPhrase       13.25        0.26        7.74        0.07  -43% -  -39%\n     LowSloppyPhrase        7.59        0.15        4.54        0.13  -43% -  -37%\n           LowPhrase       22.29        0.31       13.77        0.08  -39% -  -36%\n         AndHighHigh       23.55        0.12       15.22        0.63  -38% -  -32%\n     MedSloppyPhrase        6.88        0.12        4.60        0.16  -36% -  -29%\n    HighSloppyPhrase        1.98        0.07        1.38        0.05  -35% -  -25%\n            HighTerm       36.11        0.01       25.31        0.87  -32% -  -27%\n         MedSpanNear        5.02        0.16        3.56        0.03  -31% -  -26%\n             MedTerm      198.76        0.34      142.92        4.34  -30% -  -25%\n          HighPhrase        1.83        0.08        1.32        0.02  -31% -  -23%\n           OrHighLow       27.32        1.10       20.55        0.54  -29% -  -19%\n           OrHighMed       23.65        0.93       17.83        0.44  -29% -  -19%\n          OrHighHigh       11.42        0.46        8.72        0.20  -28% -  -18%\n        HighSpanNear        1.74        0.06        1.38        0.01  -24% -  -17%\n              IntNRQ       11.61        0.01        9.26        0.02  -20% -  -20%\n             LowTerm      513.60        2.26      411.60        7.65  -21% -  -18%\n             Prefix3       82.36        1.05       67.48        1.29  -20% -  -15%\n            Wildcard       52.63        0.44       43.45        0.81  -19% -  -15%\n              Fuzzy1       74.74        1.02       70.03        0.80   -8% -   -3%\n            PKLookup      192.60        3.94      191.87        2.07   -3% -    2%\n              Fuzzy2       62.50        1.29       62.74        1.10   -3% -    4%\n             Respell       61.69        1.04       62.79        0.84   -1% -    4%\n\n\n\nSo... is it's clear all our the specializing does help! ",
            "author": "Michael McCandless",
            "id": "comment-13431491"
        },
        {
            "date": "2012-08-08T23:13:56+0000",
            "content": "Thanks Mike for your tests. Do you think BlockPacked is now fast enough to replace Block with BlockPacked? I am asking because it is a little painful to always have to backport changes from one format to the other. ",
            "author": "Adrien Grand",
            "id": "comment-13431498"
        },
        {
            "date": "2012-08-08T23:29:18+0000",
            "content": "Yes I think we should do a hard cutover now?  Ie, merge any final changes (sorry for all the commits!  we are nearly ready to land I think...) over to BlockPacked, then remove Block and rename BlockPacked to Block? ",
            "author": "Michael McCandless",
            "id": "comment-13431504"
        },
        {
            "date": "2012-08-08T23:34:32+0000",
            "content": "Sounds good. I think the only commits that have not been merged yet are 1371010 and 1371011. ",
            "author": "Adrien Grand",
            "id": "comment-13431506"
        },
        {
            "date": "2012-08-08T23:49:02+0000",
            "content": "OK I'll merge & replace Block w/ BlockPacked... likely sometime tomorrow.  Thanks Adrien! ",
            "author": "Michael McCandless",
            "id": "comment-13431511"
        },
        {
            "date": "2012-08-09T09:44:33+0000",
            "content": "The comment you added in 1371011 on the value of BLOCK_SIZE caught my attention: I think that BLOCK_SIZE should be at least 64 with PackedInts encoding/decoding since these conversions are long-aligned (I backported your two commits and added a comment about this). For example, the PACKED 7-bits encoder cannot encode less than 64 values in one iteration.\n\nIn case someone would really want to use smaller block sizes (eg. 32), I think it should still perform pretty well if acceptableOverheadRatio >= ~25% (in that case, all bits-per-value in the [1-24] range either use a PACKED_SINGLE_BLOCK encoder or an 8-bits, 16-bits or 24-bits PACKED encoder).\n\nDo we plan to make the block size configurable? ",
            "author": "Adrien Grand",
            "id": "comment-13431709"
        },
        {
            "date": "2012-08-09T10:53:22+0000",
            "content": "Thanks Adrien.  So now we just have to replace Block with BlockPacked right?\n\nOK let's just fix the comment to be multiple of 64.\n\nI don't think we need to make BLOCK_SIZE configurable. ",
            "author": "Michael McCandless",
            "id": "comment-13431733"
        },
        {
            "date": "2012-08-09T11:04:49+0000",
            "content": "So now we just have to replace Block with BlockPacked right?\n\nYes, I think so.\n\nI don't think we need to make BLOCK_SIZE configurable.\n\nIn that case, should we also hard-code the value of acceptableOverheadRatio?\n ",
            "author": "Adrien Grand",
            "id": "comment-13431735"
        },
        {
            "date": "2012-08-09T11:22:07+0000",
            "content": "Actually let's hold off a bit on replacing Block w/ BlockPacked: Billy was going to do some more tests with PFOR...\n\nIn that case, should we also hard-code the value of acceptableOverheadRatio?\n\nHmm that one seems more compelling to let apps change? ",
            "author": "Michael McCandless",
            "id": "comment-13431742"
        },
        {
            "date": "2012-08-09T11:45:24+0000",
            "content": "Shouldn't MIN_ENCODED_SIZE be MAX_ENCODED_SIZE?  Ie the max number of\nbytes encoding will ever require.  And I think the same for\nMIN -> MAX_DATA_SIZE?  Or maybe MIN_REQUIRED_XXX?\n\nI think readVIntBlock shouldn't be in ForUtil?  Ie it's very\npostings-format-specific and it's not using packed ints at all.  Also\nthe \"equivalent\" readVIntBlock code for the positions case (in the\nreadPositions methods) is still in the BlockPackedPostingsReader.  I\nthink it's great to have writeBlock/readBlock/skipBlock in ForUtil.\n\nDo we really need to write/write the 32 format.getId(), numBits into\nthe postings file header?  I guess it's either that or ... store the float\nacceptableOverheadRatio (eg using Float.floatToIntBits I guess) and\nhave some back-compat enforced in the logic in\nPackedInts.fastestFormatAndBits... hmm.\n\nHmm ... MIN_DATA_SIZE is 147 (PACKED_SINGLE_BLOCK, bpv=3), but\nBLOCK_SIZE is 128 ... so I guess this means if we ever pick that\nformat (because acceptableOverheadRatio allowed us to), we're\nencoding/decoding those extra 19 unused ints right?  (I was just\ntrying to understand why we alloc all the int[] to MIN_DATA_SIZE not\nBLOCK_SIZE...).\n\nForUtil.getMinRequiredBufferSize seems like dead code? ",
            "author": "Michael McCandless",
            "id": "comment-13431753"
        },
        {
            "date": "2012-08-09T12:13:21+0000",
            "content": "Thank you Adrien! The BlockPacked PF also worked well on my computer \n\n                Task    QPS base StdDev base  QPS packedStdDev packed      Pct diff\n         AndHighHigh      122.57        3.01      123.90        2.49   -3% -    5%\n          AndHighLow     2260.53       21.18     2273.77       55.09   -2% -    3%\n          AndHighMed      328.01        8.18      329.31       11.36   -5% -    6%\n              Fuzzy1       86.37        0.94       86.24        2.12   -3% -    3%\n              Fuzzy2       31.40        0.46       31.22        0.64   -4% -    2%\n          HighPhrase        9.09        0.51        9.15        0.40   -8% -   11%\n    HighSloppyPhrase        5.30        0.25        5.34        0.08   -5% -    7%\n        HighSpanNear       10.11        0.44       10.42        0.34   -4% -   11%\n            HighTerm      179.43        7.26      178.96        5.70   -7% -    7%\n              IntNRQ       61.87        3.79       60.59        4.31  -14% -   11%\n           LowPhrase       41.23        1.54       42.97        1.32   -2% -   11%\n     LowSloppyPhrase       62.83        2.11       68.23        0.99    3% -   14%\n         LowSpanNear       81.28        2.74       85.74        2.67   -1% -   12%\n             LowTerm     1763.70       29.21     1778.41       23.07   -2% -    3%\n           MedPhrase       27.06        1.16       27.54        0.88   -5% -    9%\n     MedSloppyPhrase       31.82        1.16       33.70        0.14    1% -   10%\n         MedSpanNear       23.09        0.93       23.84        0.79   -4% -   11%\n             MedTerm      659.09       22.65      671.54       19.79   -4% -    8%\n          OrHighHigh       27.36        0.52       27.41        1.25   -6% -    6%\n           OrHighLow      154.99        2.07      156.20        7.08   -5% -    6%\n           OrHighMed      105.13        1.52      105.30        4.65   -5% -    6%\n            PKLookup      210.64        6.95      217.57        2.08    0% -    7%\n             Prefix3      170.22        6.22      166.80        4.18   -7% -    4%\n             Respell       83.96        1.47       83.75        1.25   -3% -    3%\n            Wildcard      155.08        4.31      155.31        3.12   -4% -    5%\n\n ",
            "author": "Han Jiang",
            "id": "comment-13431762"
        },
        {
            "date": "2012-08-09T12:16:11+0000",
            "content": "I think, for a fair test, we should also test w/ acceptableOverheadRatio=0 ... I'll run that. ",
            "author": "Michael McCandless",
            "id": "comment-13431764"
        },
        {
            "date": "2012-08-09T12:22:07+0000",
            "content": "Shouldn't MIN_ENCODED_SIZE be MAX_ENCODED_SIZE?\n\nI prefixed with \"MIN\" because it is the minimum size the encoded buffer size must have to be able to handle all cases. But I think you are right, \"MAX\" or \"REQUIRED\" would be clearer.\n\nI think readVIntBlock shouldn't be in ForUtil?\n\nI'll move it back to BlockPackedPostingsReader.\n\n\n Do we really need to write/write the 32 format.getId(), numBits into\nthe postings file header? I guess it's either that or ... store the float\nacceptableOverheadRatio (eg using Float.floatToIntBits I guess) and\nhave some back-compat enforced in the logic in\nPackedInts.fastestFormatAndBits... hmm.\n\nI hesitated between these two approaches but I think writing all cases to the header is less error-prone? Moreover it would allow us to change the logic of fastestFormatAndBits without having to bump the version number.\n\n Hmm ... MIN_DATA_SIZE is 147 (PACKED_SINGLE_BLOCK, bpv=3), but\nBLOCK_SIZE is 128 ... so I guess this means if we ever pick that\nformat (because acceptableOverheadRatio allowed us to), we're\nencoding/decoding those extra 19 unused ints right? (I was just\ntrying to understand why we alloc all the int[] to MIN_DATA_SIZE not\nBLOCK_SIZE...).\n\nExactly. The other problem is that we are also storing these unnecessary 19 values (but it is not easy to fix since PACKED_SINGLE_BLOCK writes values in the low-order long bits first (little endian)). Maybe we should make PACKED_SINGLE_BLOCK write values in the high-order bits first and split byte encoders and decoders from the long ones (so that they have a lower valueCount()). \n\nForUtil.getMinRequiredBufferSize seems like dead code?\n\nI'll remove it. ",
            "author": "Adrien Grand",
            "id": "comment-13431767"
        },
        {
            "date": "2012-08-09T15:01:46+0000",
            "content": "I revived the PFor codes, and test it agains BlockFor and BlockPacked:\n\nBlockFor as base:\n\n                Task    QPS base StdDev base    QPS pfor StdDev pfor      Pct diff\n         AndHighHigh      121.54        1.37      116.69        2.03   -6% -   -1%\n          AndHighLow     2286.36       14.19     2212.92       11.48   -4% -   -2%\n          AndHighMed      322.97        7.37      294.19        4.76  -12% -   -5%\n              Fuzzy1       85.56        1.46       87.97        3.27   -2% -    8%\n              Fuzzy2       30.94        0.56       32.16        1.34   -2% -   10%\n          HighPhrase        9.39        0.38        9.02        0.45  -12% -    5%\n    HighSloppyPhrase        5.38        0.08        5.24        0.12   -6% -    1%\n        HighSpanNear       10.38        0.39        9.92        0.08   -8% -    0%\n            HighTerm      180.30        6.87      172.83        6.26  -11% -    3%\n              IntNRQ       62.01        3.73       60.89        3.54  -12% -   10%\n           LowPhrase       42.44        0.67       38.73        0.89  -12% -   -5%\n     LowSloppyPhrase       62.82        0.79       56.79        0.43  -11% -   -7%\n         LowSpanNear       81.79        2.00       74.10        1.13  -12% -   -5%\n             LowTerm     1763.95       39.62     1721.30       34.22   -6% -    1%\n           MedPhrase       27.87        0.59       25.82        0.74  -11% -   -2%\n     MedSloppyPhrase       32.15        0.41       29.91        0.31   -9% -   -4%\n         MedSpanNear       23.48        0.71       22.00        0.05   -9% -   -3%\n             MedTerm      662.11       24.22      638.81       19.31   -9% -    3%\n          OrHighHigh       26.82        0.47       27.14        1.93   -7% -   10%\n           OrHighLow      152.40        3.54      156.58       11.11   -6% -   12%\n           OrHighMed      103.20        2.26      105.84        7.55   -6% -   12%\n            PKLookup      216.38        4.32      219.32        2.59   -1% -    4%\n             Prefix3      169.89        4.97      163.82        3.34   -8% -    1%\n             Respell       83.23        1.44       86.20        3.00   -1% -    9%\n            Wildcard      155.81        2.79      152.30        2.54   -5% -    1%\n\n\n\nBlockPacked as base:\n\n                Task    QPS base StdDev base    QPS pfor StdDev pfor      Pct diff\n         AndHighHigh      122.94        3.43      116.24        1.90   -9% -   -1%\n          AndHighLow     2294.32       58.32     2199.14       31.97   -7% -    0%\n          AndHighMed      325.55       12.44      290.20        3.80  -15% -   -6%\n              Fuzzy1       88.33        1.84       87.86        2.54   -5% -    4%\n              Fuzzy2       31.92        0.80       32.00        0.92   -5% -    5%\n          HighPhrase        9.73        0.47        9.04        0.29  -14% -    0%\n    HighSloppyPhrase        5.49        0.19        5.16        0.03   -9% -   -1%\n        HighSpanNear       10.93        0.23        9.90        0.09  -12% -   -6%\n            HighTerm      178.31        6.37      171.06        6.14  -10% -    3%\n              IntNRQ       60.87        4.71       62.38        5.49  -13% -   20%\n           LowPhrase       44.97        1.18       38.36        1.01  -19% -  -10%\n     LowSloppyPhrase       69.61        1.19       55.90        1.39  -23% -  -16%\n         LowSpanNear       88.50        0.66       72.80        2.23  -20% -  -14%\n             LowTerm     1769.84       32.66     1717.02       39.75   -6% -    1%\n           MedPhrase       28.88        0.84       25.57        0.68  -16% -   -6%\n     MedSloppyPhrase       34.47        0.50       29.29        0.54  -17% -  -12%\n         MedSpanNear       24.88        0.32       21.69        0.38  -15% -  -10%\n             MedTerm      667.95       21.61      633.73       22.17  -11% -    1%\n          OrHighHigh       27.96        1.29       26.82        0.81  -11% -    3%\n           OrHighLow      158.62        5.82      155.08        5.05   -8% -    4%\n           OrHighMed      107.16        4.19      104.81        3.17   -8% -    4%\n            PKLookup      217.22        1.86      216.83        1.87   -1% -    1%\n             Prefix3      167.32        6.72      166.12        6.53   -8% -    7%\n             Respell       85.25        2.27       85.85        2.16   -4% -    6%\n            Wildcard      156.24        5.69      154.63        3.02   -6% -    4%\n\n\n\nCurrent PFor impl only saves 1.8% against For, but get quite some perf loss. Let's use the Packed version! ",
            "author": "Han Jiang",
            "id": "comment-13431882"
        },
        {
            "date": "2012-08-09T15:42:34+0000",
            "content": "I compared Block w/ BlockPacked, but set acceptableOverheadRatio to 0 for a fairer test:\n\n\n                Task    QPS base StdDev base    QPS pack StdDev pack      Pct diff\n    HighSloppyPhrase        1.94        0.01        1.91        0.05   -4% -    2%\n           LowPhrase       21.05        0.07       20.84        0.37   -3% -    1%\n           MedPhrase       13.05        0.04       12.93        0.23   -3% -    1%\n            Wildcard       43.87        2.76       43.49        2.10  -11% -   10%\n              IntNRQ        8.88        1.39        8.83        0.78  -21% -   28%\n              Fuzzy1       63.07        1.96       62.78        1.46   -5% -    5%\n     LowSloppyPhrase        6.92        0.01        6.91        0.13   -2% -    1%\n             Prefix3       71.38        5.20       71.35        3.17  -10% -   12%\n            PKLookup      157.00        1.78      158.01        2.01   -1% -    3%\n          AndHighLow      668.76        4.82      674.80        7.48    0% -    2%\n          HighPhrase        1.56        0.03        1.58        0.03   -3% -    5%\n     MedSloppyPhrase        7.71        0.03        7.80        0.11    0% -    2%\n          AndHighMed       74.05        0.49       75.35        0.36    0% -    2%\n         AndHighHigh       25.92        0.30       26.78        0.19    1% -    5%\n             Respell       57.07        2.70       59.20        1.80   -3% -   12%\n              Fuzzy2       60.81        2.92       63.32        1.68   -3% -   12%\n          OrHighHigh        8.99        0.17        9.39        0.11    1% -    7%\n           OrHighMed       17.65        0.37       18.52        0.13    2% -    7%\n         MedSpanNear        3.90        0.17        4.11        0.09   -1% -   12%\n           OrHighLow       22.99        0.51       24.22        0.15    2% -    8%\n        HighSpanNear        1.40        0.06        1.48        0.03    0% -   12%\n         LowSpanNear        7.84        0.31        8.32        0.17    0% -   12%\n             LowTerm      406.02       28.53      444.21       37.75   -6% -   27%\n             MedTerm      149.83        8.11      167.60       15.06   -3% -   28%\n            HighTerm       29.57        1.67       33.42        3.20   -3% -   31%\n\n\n\nCuriously it seems even faster than w/ acceptableOverheadRatio=0.2!  But it makes it clear we should do a hard cutover. ",
            "author": "Michael McCandless",
            "id": "comment-13431914"
        },
        {
            "date": "2012-08-09T15:43:05+0000",
            "content": "I revived the PFor codes, and test it agains BlockFor and BlockPacked\n\nThanks Billy, I'll run a test too ... ",
            "author": "Michael McCandless",
            "id": "comment-13431916"
        },
        {
            "date": "2012-08-09T16:22:23+0000",
            "content": "Curiously it seems even faster than w/ acceptableOverheadRatio=0.2! But it makes it clear we should do a hard cutover.\n\nI had been doing some tests with the bulk version of PackedInts.get (which uses the same methods that we use for BlockPacked) while working on LUCENE-4098 and it seemed that the bottleneck was more memory bandwidth than CPU (for large arrays at least). If you look at the last graph of http://people.apache.org/~jpountz/packed_ints3.html, the throughput seems to depend more on the memory efficiency of the picked impl than on the way it stores data. Maybe we are experiencing a similar phenomenon here...\n\nUnless I am missing something, the only difference between BlockPacked and Block is that BlockPacked decodes directly from byte[] whereas Block uses ByteBuffer.asLongBuffer to translate from bytes to ints and then decodes from the ints... Interesting to know it has so much overhead... ",
            "author": "Adrien Grand",
            "id": "comment-13431951"
        },
        {
            "date": "2012-08-09T17:07:24+0000",
            "content": "OK indeed PFOR is slower for me too:\n\n\n                Task    QPS base StdDev base    QPS pfor StdDev pfor      Pct diff\n          HighPhrase        1.56        0.03        1.25        0.12  -28% -  -10%\n           MedPhrase       13.05        0.10       10.50        0.58  -24% -  -14%\n           LowPhrase       21.08        0.08       17.35        0.85  -22% -  -13%\n          AndHighMed       73.78        0.66       62.50        1.68  -18% -  -12%\n          AndHighLow      674.60        2.54      573.00       12.06  -17% -  -12%\n         LowSpanNear        8.04        0.17        6.97        0.23  -17% -   -8%\n         MedSpanNear        3.97        0.10        3.58        0.15  -15% -   -3%\n     MedSloppyPhrase        7.58        0.11        6.93        0.14  -11% -   -5%\n         AndHighHigh       25.71        0.47       23.58        0.61  -12% -   -4%\n        HighSpanNear        1.42        0.04        1.31        0.05  -12% -   -1%\n             MedTerm      155.44       18.75      144.46       12.33  -24% -   14%\n            HighTerm       30.27        4.31       28.25        2.88  -26% -   19%\n     LowSloppyPhrase        6.73        0.13        6.28        0.12  -10% -   -3%\n          OrHighHigh        9.06        0.24        8.53        0.33  -11% -    0%\n           OrHighLow       23.09        0.67       21.88        0.91  -11% -    1%\n           OrHighMed       17.71        0.51       16.79        0.67  -11% -    1%\n    HighSloppyPhrase        1.88        0.05        1.80        0.04   -9% -    0%\n              IntNRQ        9.42        0.50        9.05        0.89  -17% -   11%\n             Prefix3       72.67        2.42       70.42        3.61  -11% -    5%\n              Fuzzy1       63.71        1.07       62.34        1.55   -6% -    1%\n            Wildcard       45.25        0.99       44.28        1.55   -7% -    3%\n            PKLookup      159.04        2.13      157.17        1.90   -3% -    1%\n              Fuzzy2       62.51        2.28       63.40        1.65   -4% -    8%\n             LowTerm      400.06       57.60      407.73       52.40  -22% -   34%\n             Respell       56.72        3.19       59.83        2.10   -3% -   15%\n\n\n\nI think we should replace Block with BlockPacked now? ",
            "author": "Michael McCandless",
            "id": "comment-13431983"
        },
        {
            "date": "2012-08-09T17:09:50+0000",
            "content": "I had been doing some tests with the bulk version of PackedInts.get (which uses the same methods that we use for BlockPacked) while working on LUCENE-4098 and it seemed that the bottleneck was more memory bandwidth than CPU (for large arrays at least). \n\nAhh, interesting...\n\nSo I think we should test different acceptableOverheadRatios to find the best ... it could be it's 0! ",
            "author": "Michael McCandless",
            "id": "comment-13431985"
        },
        {
            "date": "2012-08-09T17:17:08+0000",
            "content": "\nDo we really need to write/write the 32 format.getId(), numBits into the postings file header? I guess it's either that or ... store the float acceptableOverheadRatio (eg using Float.floatToIntBits I guess) and have some back-compat enforced in the logic in PackedInts.fastestFormatAndBits... hmm.\n\nI hesitated between these two approaches but I think writing all cases to the header is less error-prone? Moreover it would allow us to change the logic of fastestFormatAndBits without having to bump the version number.\n\nMaybe for starters we should just hardwire acceptableOverheadRatio at\n0 ... then we simplify this back-compat until/unless we really need to\nmake this configurable. ",
            "author": "Michael McCandless",
            "id": "comment-13431991"
        },
        {
            "date": "2012-08-09T17:21:32+0000",
            "content": "The other problem is that we are also storing these unnecessary 19 values (but it is not easy to fix since PACKED_SINGLE_BLOCK writes values in the low-order long bits first (little endian)). Maybe we should make PACKED_SINGLE_BLOCK write values in the high-order bits first and split byte encoders and decoders from the long ones (so that they have a lower valueCount()).\n\nOK, we can explore that later (another reason to simply always use Format.PACKED for now...). ",
            "author": "Michael McCandless",
            "id": "comment-13432001"
        },
        {
            "date": "2012-08-09T17:32:15+0000",
            "content": "\nOK indeed PFOR is slower for me too:\n\nI think for starters since you guys have gotten FOR pretty nice we should just focus on that one?\n\nWe could later see if PFOR could get additional wins as a second step: getting FOR working nice and fast\nis awesome on its own! ",
            "author": "Robert Muir",
            "id": "comment-13432009"
        },
        {
            "date": "2012-08-09T22:30:45+0000",
            "content": "I think for starters since you guys have gotten FOR pretty nice we should just focus on that one?\n\nYeah I think we should do that.  I think the branch is nearly ready to land!\n\nI just replaced Block with BlockPacked ... ",
            "author": "Michael McCandless",
            "id": "comment-13432235"
        },
        {
            "date": "2012-08-10T12:13:49+0000",
            "content": "I ran the comparison between acceptableOverheadRatio=PackedInts.COMPACT (0%) and PackedInts.DEFAULT (20%) and it seems to be much faster with PackedInts.COMPACT:\n\n\nbase=COMPACT, challenger=DEFAULT\n                Task    QPS base StdDev base     QPS def  StdDev def      Pct diff\n              IntNRQ       81.83        5.43       74.14        2.94  -18% -    0%\n            HighTerm      146.55       10.34      133.57        9.02  -20% -    4%\n           LowPhrase       93.91        1.63       86.90        1.67  -10% -   -4%\n             MedTerm      824.58       43.48      766.35       38.78  -16% -    3%\n     LowSloppyPhrase       83.29        1.99       77.65        1.18  -10% -   -3%\n           OrHighMed       94.15        5.28       88.34        4.54  -15% -    4%\n          OrHighHigh      100.63        5.42       94.57        4.20  -14% -    3%\n           OrHighLow      128.62        7.21      120.92        6.07  -15% -    4%\n          HighPhrase       13.05        0.45       12.29        0.39  -11% -    0%\n             Prefix3      217.06        6.82      205.05        4.62  -10% -    0%\n           MedPhrase       27.50        0.97       26.33        0.79  -10% -    2%\n            Wildcard      183.20        4.87      175.58        3.89   -8% -    0%\n             LowTerm     1763.31       43.24     1693.31       39.29   -8% -    0%\n    HighSloppyPhrase       10.05        0.48        9.67        0.40  -11% -    5%\n         AndHighHigh      111.59        1.15      107.45        1.66   -6% -   -1%\n         LowSpanNear       56.16        1.32       54.25        1.01   -7% -    0%\n          AndHighMed      423.44        7.40      409.32        5.10   -6% -    0%\n         MedSpanNear       33.14        0.91       32.32        0.74   -7% -    2%\n          AndHighLow     2177.50       30.79     2134.05       28.64   -4% -    0%\n              Fuzzy1       95.34        2.41       93.66        2.32   -6% -    3%\n        HighSpanNear        5.28        0.17        5.21        0.11   -6% -    3%\n     MedSloppyPhrase       18.41        0.72       18.19        0.70   -8% -    6%\n              Fuzzy2       37.73        1.31       37.31        1.14   -7% -    5%\n             Respell      109.71        3.09      108.64        2.76   -6% -    4%\n            PKLookup      257.32        6.64      260.00        7.15   -4% -    6%\n\n ",
            "author": "Adrien Grand",
            "id": "comment-13432716"
        },
        {
            "date": "2012-08-13T13:55:54+0000",
            "content": "(From mailing-list) So I think if its this ambiguous for wikipedia we should shoot for the most COMPACT form as a safe default.\n\n+1 too. I just committed the change. ",
            "author": "Adrien Grand",
            "id": "comment-13433143"
        },
        {
            "date": "2012-08-15T12:51:08+0000",
            "content": "Uwe just started builds for this branch (thanks!): http://jenkins.sd-datasolutions.de/job/pforcodec-3892-branch ",
            "author": "Michael McCandless",
            "id": "comment-13435033"
        },
        {
            "date": "2012-08-18T12:29:15+0000",
            "content": "Patch from Billy improving javadocs ... ",
            "author": "Michael McCandless",
            "id": "comment-13437296"
        },
        {
            "date": "2012-08-18T15:36:17+0000",
            "content": "Patch, applyable to trunk (use patch -p1 < ...).\n\nThe branch builds look stable ... I think this is ready to land on trunk!\n\nI think we should leave Lucene40 as default PF for now, until we BlockPF bakes on trunk for a while, but as some point (maybe for 4.1?) I think we should cutover to BlockPF as the default. ",
            "author": "Michael McCandless",
            "id": "comment-13437347"
        },
        {
            "date": "2012-08-20T10:05:09+0000",
            "content": "From r1373332:\n\n\n\tprivate static final int PACKED_INTS_VERSION = 0; // nocommit: encode in the stream?\n+  private static final int PACKED_INTS_VERSION_START = 0;\n+  private static final int PACKED_INTS_VERSION_CURRENT = PACKED_INTS_VERSION_START;\n\n\n\nMike, is there any reason why you didn't use PackedInts.VERSION_START and PackedInts.VERSION_CURRENT instead? ",
            "author": "Adrien Grand",
            "id": "comment-13437770"
        },
        {
            "date": "2012-08-20T11:19:11+0000",
            "content": "Mike, is there any reason why you didn't use PackedInts.VERSION_START and PackedInts.VERSION_CURRENT instead?\n\nWoops, no, I forgot we had version info in PackedInts!  I'll switch it over. ",
            "author": "Michael McCandless",
            "id": "comment-13437789"
        },
        {
            "date": "2012-08-20T12:13:15+0000",
            "content": "OK I committed that, and also added version checking in getEncoder/Decoder, and I now loop over all versions when computing MAX_DATA_SIZE \u2013 can you double check Adrien?  Thanks! ",
            "author": "Michael McCandless",
            "id": "comment-13437806"
        },
        {
            "date": "2012-08-20T12:31:28+0000",
            "content": "You toasted me, I was just doing exactly the same change!  The diff looks good to me. ",
            "author": "Adrien Grand",
            "id": "comment-13437818"
        },
        {
            "date": "2012-08-20T12:39:43+0000",
            "content": "Woops sorry! ",
            "author": "Michael McCandless",
            "id": "comment-13437825"
        },
        {
            "date": "2012-08-20T12:57:36+0000",
            "content": "\nThe branch builds look stable ... I think this is ready to land on trunk!\n\nI think we should leave Lucene40 as default PF for now, until we BlockPF bakes on trunk for a while, but as some point (maybe for 4.1?) I think we should cutover to BlockPF as the default.\n\n+1 ",
            "author": "Robert Muir",
            "id": "comment-13437834"
        },
        {
            "date": "2012-08-20T14:18:15+0000",
            "content": "Woops sorry!\n\nNP, good to know we had planned the same changes.\n\nThe branch builds look stable ... I think this is ready to land on trunk!\n\n+1 too ",
            "author": "Adrien Grand",
            "id": "comment-13437889"
        },
        {
            "date": "2012-08-21T12:16:38+0000",
            "content": "I just merged to trunk & 4.x.  Thanks Billy, this is a great addition to Lucene! ",
            "author": "Michael McCandless",
            "id": "comment-13438643"
        },
        {
            "date": "2012-08-21T12:34:51+0000",
            "content": "Thank you Mike! And thanks to all of you! I learnt really much this summer! ",
            "author": "Han Jiang",
            "id": "comment-13438653"
        },
        {
            "date": "2012-08-21T13:42:04+0000",
            "content": "Thanks Billy for all the hard work and endless benchmarking, so nice to have a block codec that is simple and clean and reuses our packed ints optimizations. ",
            "author": "Robert Muir",
            "id": "comment-13438728"
        },
        {
            "date": "2012-08-21T13:45:37+0000",
            "content": "Thanks Billy for all the hard work and endless benchmarking, so nice to have a block codec that is simple and clean and reuses our packed ints optimizations.\n\n+1 ",
            "author": "Adrien Grand",
            "id": "comment-13438731"
        },
        {
            "date": "2012-08-24T16:39:45+0000",
            "content": "I tried changing the generated bulk decode methods to re-use variables so we have much fewer local variables required (patch attached), but on quick testing it didn't seem to make much difference in performance. ",
            "author": "Michael McCandless",
            "id": "comment-13441287"
        },
        {
            "date": "2012-08-24T16:52:09+0000",
            "content": "We should keep the size of methods small, as bigger methods work against the code cache of hotspot and if Lucene is not used alone, may get de-optimized. ",
            "author": "Uwe Schindler",
            "id": "comment-13441295"
        },
        {
            "date": "2013-05-10T10:32:41+0000",
            "content": "Closed after release. ",
            "author": "Uwe Schindler",
            "id": "comment-13653695"
        }
    ]
}