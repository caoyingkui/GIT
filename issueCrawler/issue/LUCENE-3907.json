{
    "id": "LUCENE-3907",
    "title": "Improve the Edge/NGramTokenizer/Filters",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [
            "4.4"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Our ngram tokenizers/filters could use some love.  EG, they output ngrams in multiple passes, instead of \"stacked\", which messes up offsets/positions and requires too much buffering (can hit OOME for long tokens).  They clip at 1024 chars (tokenizers) but don't (token filters).  The split up surrogate pairs incorrectly.",
    "attachments": {
        "LUCENE-3907.patch": "https://issues.apache.org/jira/secure/attachment/12581988/LUCENE-3907.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-03-29T15:23:27+0000",
            "content": "Hi,\n\nI'm interested in this project. I have done a Natural Language Processing project in language classification in which I did tokenization using Stanford's NLP tool. I'm also currently doing an Information Retrieval project in documents indexing and searching using Lucene and Weka. I might not be too familiar with Lucene's ngram tokenizer, but I have been working with NGram and Lucene before, so I believe that I would be able to learn quickly. Thanks \n\nBest regards,\nReinardus Surya Pradhitya ",
            "author": "Reinardus Surya Pradhitya",
            "id": "comment-13241303"
        },
        {
            "date": "2012-03-29T16:19:27+0000",
            "content": "Awesome!  We just need a possible mentor here... volunteers...? ",
            "author": "Michael McCandless",
            "id": "comment-13241357"
        },
        {
            "date": "2012-04-03T16:56:32+0000",
            "content": "I would like to be the mentor for this. I wanted to fix those long time ago and I am happy somebody helps.\n\nP.S.: Maybe we also get a new ShingleMatrix LOL ",
            "author": "Uwe Schindler",
            "id": "comment-13245492"
        },
        {
            "date": "2013-03-07T16:06:51+0000",
            "content": "I think we should remove the Side (BACK/FRONT) enum: an app can always use ReverseStringFilter if it really wants BACK grams (what are BACK grams used for?). ",
            "author": "Michael McCandless",
            "id": "comment-13596002"
        },
        {
            "date": "2013-03-07T20:48:08+0000",
            "content": "Back grams would work for leading wildcards. They might be useful for things where the head is at the end (tail-first?), like domain names.\n\nNot super-useful, but it is a small part of the code in the tokenizer. ",
            "author": "Walter Underwood",
            "id": "comment-13596346"
        },
        {
            "date": "2013-03-07T21:39:00+0000",
            "content": "Back grams would work for leading wildcards. They might be useful for things where the head is at the end (tail-first?), like domain names.\n\nIf you need reverse n-grams, you could always add a filter to do that afterwards. There is no need to have this as separate logic in this filter. We should split logic and keep filters as simple as possible. ",
            "author": "Uwe Schindler",
            "id": "comment-13596413"
        },
        {
            "date": "2013-03-07T21:42:26+0000",
            "content": "Edge is the wrong name for something that only works on one edge.  Maybe rename to LeadingNgram?  ",
            "author": "Steve Rowe",
            "id": "comment-13596419"
        },
        {
            "date": "2013-05-06T23:04:26+0000",
            "content": "Here is a patch that fixes EdgeNGramTokenizer and EdgeNGramTokenFilter so that they pass TestRandomChains.\n\nIt also deprecates Side.BACK so when committing, I propose to follow Steve's advice to rename them to LeadingNGramTokenizer and LeadingNGramTokenFilter in trunk. ",
            "author": "Adrien Grand",
            "id": "comment-13650209"
        },
        {
            "date": "2013-05-07T05:37:01+0000",
            "content": "Hi Adrien, thanks for the fixes. You can take the issue and assign it to you!\n\nUwe ",
            "author": "Uwe Schindler",
            "id": "comment-13650530"
        },
        {
            "date": "2013-05-07T14:16:07+0000",
            "content": "As Steve suggested, I think these tokenizers/filters need to be renamed (trunk only) since they don't support backward graming anymore. Please don't hesitate to let me know if you have a good idea for a name, otherwise I plan to rename them to \"Leading...\" in the next few days. ",
            "author": "Adrien Grand",
            "id": "comment-13650856"
        },
        {
            "date": "2013-05-09T06:20:13+0000",
            "content": "Why not make the big changes for trunk/5.0, but leave the existing filters/tokenziers in 4.x as deprecated. Add the \"leading\" replacements as well in 4x, but be sure to preserve the existing stuff with support for \"back\" in 4x - as deprecated.\n ",
            "author": "Jack Krupansky",
            "id": "comment-13652770"
        },
        {
            "date": "2013-05-09T11:24:41+0000",
            "content": "The previous behaviour could trigger highlighting bugs so I think it is important that we fix it in 4.x. In case the broken behaviour is still needed, it can be emulated by providing Version.LUCENE_43 as the Lucene match version. ",
            "author": "Adrien Grand",
            "id": "comment-13652878"
        },
        {
            "date": "2013-05-09T11:50:33+0000",
            "content": "Look, the \"fix\" of position bugs here is to keep the position the same for all tokens, right? And that logic can simply be applied to \"back\" as well, for the same reasons and with the same effect. So, how could \"back\" - which should apply that same position logic be a separate cause of \"highlighting bugs\"?\n\n\"previous behavior\" (incremented position) is simply NOT linked to front vs. back. I'm not sure why you are claiming that it is!\n\nThe Jira record simply shows that some people \"want\" to eliminate a feature... not that the feature (if fixed in the same manner as the rest of the fix) \"could trigger highlighting bugs\" - unless I'm missing something, and if I'm missing something it is because you are not stating it clearly! So, please do so. ",
            "author": "Jack Krupansky",
            "id": "comment-13652885"
        },
        {
            "date": "2013-05-09T15:25:21+0000",
            "content": "\"previous behavior\" (incremented position) is simply NOT linked to front vs. back. I'm not sure why you are claiming that it is!\n\nIndeed these issues are unrelated, and backward n-graming doesn't cause highlighting issues. Sorry if I seemed to mean the opposite, it was not intentional.\n\nMy main motivation was to fix the positions/offsets bugs. I also deprecated support for backward n-graming since there seemed to be lazy consensus: as Uwe noted, backward n-graming can be obtained by applying ReverseStringFilter, then EdgeNGramTokenFilter and then ReverseStringFilter again. This helps make filters simpler, hence easier to understand and to test.\n\nSo now, here is how you would use filters depending on whether you want front or back n-graming and with or without the new positions/offsets.\n\n\n\n\n\u00a0\n previous positions/offsets (broken) \n new positions/offsets \n\n\n front n-graming \n EdgeNGramTokenFilter(version=LUCENE_43,side=FRONT) \n EdgeNGramTokenFilter(version=LUCENE_44,side=FRONT) \n\n\n back n-graming \n EdgeNGramTokenFilter(version=LUCENE_43,side=BACK) \n ReverseStringFilter, EdgeNGramTokenFilter(version=LUCENE_44,side=FRONT), ReverseStringFilter \n\n\n\n\n\nIt is true that the patch prevents users from constructing EdgeNGramTokenFilter with version>=LUCENE_44 and side=BACK to encourage users to upgrade their analysis chain. But if you think we should allow for it, I'm open for discussion. ",
            "author": "Adrien Grand",
            "id": "comment-13653011"
        },
        {
            "date": "2013-07-23T18:37:12+0000",
            "content": "Bulk close resolved 4.4 issues ",
            "author": "Steve Rowe",
            "id": "comment-13716780"
        }
    ]
}