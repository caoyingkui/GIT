{
    "id": "LUCENE-2361",
    "title": "OutOfMemoryException while Indexing",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "2.9.1",
        "resolution": "Cannot Reproduce",
        "status": "Closed"
    },
    "description": "Hi,\nWe use lucene 2.9.1 version.\nWe see the following OutOfMemory error in our environment, I think This is happening at a significant high load. Have you observed this anytime? Please let me know your thoughts on this.\n\norg.apache.lucene.index.MergePolicy$MergeException: java.lang.OutOfMemoryError: PermGen space\n\tat org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:351)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:315)\nCaused by: java.lang.OutOfMemoryError: PermGen space\n\tat java.lang.String.$$YJP$$intern(Native Method)\n\tat java.lang.String.intern(Unknown Source)\n\tat org.apache.lucene.util.SimpleStringInterner.intern(SimpleStringInterner.java:74)\n\tat org.apache.lucene.util.StringHelper.intern(StringHelper.java:36)\n\tat org.apache.lucene.index.FieldInfos.read(FieldInfos.java:356)\n\tat org.apache.lucene.index.FieldInfos.<init>(FieldInfos.java:71)\n\tat org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:116)\n\tat org.apache.lucene.index.SegmentReader.get(SegmentReader.java:638)\n\tat org.apache.lucene.index.SegmentReader.get(SegmentReader.java:608)\n\tat org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:686)\n\tat org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4979)\n\tat org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:4614)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:235)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:291)",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2010-04-02T23:22:43+0000",
            "content": "Which JVM is that? ",
            "author": "Earwin Burrfoot",
            "id": "comment-12852994"
        },
        {
            "date": "2010-04-02T23:23:28+0000",
            "content": "Also, are you running under profiler? Does that happen without it? ",
            "author": "Earwin Burrfoot",
            "id": "comment-12852996"
        },
        {
            "date": "2010-04-02T23:29:22+0000",
            "content": "java 6 update 18. Yes it happens without the profiler also.\n\nThanks ",
            "author": "Shivender Devarakonda",
            "id": "comment-12853001"
        },
        {
            "date": "2010-04-03T00:20:53+0000",
            "content": "Basically, my bet is you're using a truckload of unique field names. Lucene interns them for speed, as it does quite a lot of field name comparisons, and the whole thing explodes.\nUnless you fill up your PermGen in some other manner, and Lucene just happens to hit the consequences.\n\nThe strange thing is - last time I tested String.intern(), it failed to cause OOMs being bombarded by random strings, like it did in java 1.4.something. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12853021"
        },
        {
            "date": "2010-04-03T00:27:01+0000",
            "content": "I am sure we have less than 20 fields in the document. Do you think it still causes this issue? ",
            "author": "Shivender Devarakonda",
            "id": "comment-12853025"
        },
        {
            "date": "2010-04-03T00:47:08+0000",
            "content": "20 is absolutely ok. Does exception always have this stacktrace? Maybe someone else litters in PermGen? ",
            "author": "Earwin Burrfoot",
            "id": "comment-12853033"
        },
        {
            "date": "2010-04-03T05:02:50+0000",
            "content": "The stack trace is the same in other cases too.  Basically, We have load that continuously pushes specific objects and we  index each incoming object. .  If we do not put that load then it is working fine. ",
            "author": "Shivender Devarakonda",
            "id": "comment-12853074"
        },
        {
            "date": "2010-04-03T09:30:30+0000",
            "content": "Can you share some details on how you're using Lucene?  What's IndexWriters ramBufferSizeMB?  How much heap are you giving the JVM?  Are you calling commit (or closing/opening a new IndexWriter) frequently or rarely?\n\nCan you enable IndexWriter's infoStream and post the output? ",
            "author": "Michael McCandless",
            "id": "comment-12853110"
        },
        {
            "date": "2010-04-03T16:25:16+0000",
            "content": "Thanks for helping us on this.\n\nWe create a Daemon Thread that does the index updating functionality.\n\n1-  Create a IndexDir like this:\n fDirectoryName = new File(tracesDir, relativeIndexDir);\n        fIndexDir = FSDirectory.open(fDirectoryName, NoLockFactory.getNoLockFactory());\n\n        fIndexDir.setReadChunkSize(8192);\n\n\n\n2- The Index inserter thread does the following functionality, The new writer is invoked for a batch size of 100.\n\n  \t\tIndexWriter writer = null;\n\n                try\n                {\n                    writer = new IndexWriter(fIndexDir, this.getAnalyzer(), !this.indexExists(), IndexWriter.MaxFieldLength.LIMITED);\n                    writer.setMaxBufferedDocs(batchSize);\n                    writer.addDocument(doc);\n\n                    // while we're open and there's more in the queue, add more\n\n\n\n\n                    numInserted += addMore(writer, luceneQueue, batchSize);\n                }\n                finally\n                {\n                    Utils.close(writer);\n                    insertionTime = MasterClock.currentTimeMillis() - startTime;\n                }\n\nWe do not set any RAMBufferSize in this case.\n\nPlease let me know if you need anything on this. ",
            "author": "Shivender Devarakonda",
            "id": "comment-12853154"
        },
        {
            "date": "2010-04-03T19:18:07+0000",
            "content": "OK, thanks.  Is batchSize always 100?\n\nWhy are you using NoLockFactory? That's generally very dangerous.\n\nAlso, why do you call FSDir's setReadChunkSizeMB?\n\nCan you post the infoStream output? ",
            "author": "Michael McCandless",
            "id": "comment-12853186"
        },
        {
            "date": "2010-04-03T23:16:35+0000",
            "content": "1 - Yes it is 100.\n\n2 - I am not sure on this statement, I need to reachout to the original author of this code, will update you once I get a reply from the author. Do you think it will caues any issue?\n\n3- We had some OOM issues realted to SUNJVM issue that is where we have used this call.\n\n4- This issue is not always reproducible, I will post the infostream output once I get it reproduced again.  ",
            "author": "Shivender Devarakonda",
            "id": "comment-12853209"
        },
        {
            "date": "2010-04-05T22:20:33+0000",
            "content": "I saw the following infostream ouptut when I started our product but after that there was no infostream output and I saw the same OOM error again. Did I miss anything?\n\nIFD [main]: setInfoStream deletionPolicy=org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy@1cbf655\nIW 0 [main]: setInfoStream: dir=org.apache.lucene.store.SimpleFSDirectory@C:\\Program Files (x86)\\CA Wily\\Introscope9.0d595\\traces\\index autoCommit=false mergePolicy=org.apache.lucene.index.LogByteSizeMergePolicy@1996d53 mergeScheduler=org.apache.lucene.index.ConcurrentMergeScheduler@1d882b6 ramBufferSizeMB=16.0 maxBufferedDocs=-1 maxBuffereDeleteTerms=-1 maxFieldLength=10000 index=_2r5:c16187 _570:c40197 _7mt:c41371 _9yq:c42466 _cml:c45324 _fgk:c46186 _i63:c45931 _kz8:c46568 _ntr:c47071 _qiq:c46080 _qin:c18 _qip:c1 _qir:c4 _qit:c19 _qiv:c1 _qix:c51 _qiz:c2\nIW 0 [main]: now flush at close\nIW 0 [main]:   flush: segment=null docStoreSegment=null docStoreOffset=0 flushDocs=false flushDeletes=true flushDocStores=false numDocs=0 numBufDelTerms=0\nIW 0 [main]:   index before flush _2r5:c16187 _570:c40197 _7mt:c41371 _9yq:c42466 _cml:c45324 _fgk:c46186 _i63:c45931 _kz8:c46568 _ntr:c47071 _qiq:c46080 _qin:c18 _qip:c1 _qir:c4 _qit:c19 _qiv:c1 _qix:c51 _qiz:c2\nIW 0 [main]: CMS: now merge\nIW 0 [main]: CMS:   index: _2r5:c16187 _570:c40197 _7mt:c41371 _9yq:c42466 _cml:c45324 _fgk:c46186 _i63:c45931 _kz8:c46568 _ntr:c47071 _qiq:c46080 _qin:c18 _qip:c1 _qir:c4 _qit:c19 _qiv:c1 _qix:c51 _qiz:c2\nIW 0 [main]: CMS:   no more merges pending; now return\nIW 0 [main]: now call final commit()\nIW 0 [main]: startCommit(): start sizeInBytes=0\nIW 0 [main]:   skip startCommit(): no changes pending\nIW 0 [main]: commit: pendingCommit == null; skip\nIW 0 [main]: commit: done\nIW 0 [main]: at close: _2r5:c16187 _570:c40197 _7mt:c41371 _9yq:c42466 _cml:c45324 _fgk:c46186 _i63:c45931 _kz8:c46568 _ntr:c47071 _qiq:c46080 _qin:c18 _qip:c1 _qir:c4 _qit:c19 _qiv:c1 _qix:c51 _qiz:c2\n\n\n ",
            "author": "Shivender Devarakonda",
            "id": "comment-12853562"
        },
        {
            "date": "2010-04-06T09:05:51+0000",
            "content": "Hmm are you sure you're setting infoStream on all writers you create?  The above output just shows an IW that was opened, not used for anything (no deletions, additions), and then closed.\n\nAnd eg the OOME in the opening here during a merge should result in some details being posted to the infoStream. ",
            "author": "Michael McCandless",
            "id": "comment-12853811"
        },
        {
            "date": "2010-04-06T18:02:44+0000",
            "content": "Yes I set infostream on all writers.  ",
            "author": "Shivender Devarakonda",
            "id": "comment-12854091"
        },
        {
            "date": "2010-04-06T18:48:11+0000",
            "content": "Hmm but the above infoStream output shows no exception... I'm confused. ",
            "author": "Michael McCandless",
            "id": "comment-12854120"
        },
        {
            "date": "2010-04-06T20:05:36+0000",
            "content": "I did not see any infostream output when it had gone OOM. I see the same exception trace that I posted above. Do you think I missed anything? ",
            "author": "Shivender Devarakonda",
            "id": "comment-12854149"
        },
        {
            "date": "2014-10-29T15:55:21+0000",
            "content": "Hi Thomas,\nI am facing similar problem as mentioned above. I could see your comments \"made changes\". In what version this problem got resolved. Appreciate your help. ",
            "author": "Surendhar",
            "id": "comment-14188486"
        },
        {
            "date": "2014-10-29T16:01:20+0000",
            "content": "I could see still its is open, please let me know what version problem got resolved. ",
            "author": "Surendhar",
            "id": "comment-14188493"
        },
        {
            "date": "2014-10-29T19:20:03+0000",
            "content": "IMO this shouldn't be a JIRA issue... it should have been an email thread on Lucene java-user list.  Once a reproducible problem is found, then create an issue.  OOM's are quite possible simply by allocating too little heap to Java. ",
            "author": "David Smiley",
            "id": "comment-14188824"
        }
    ]
}