{
    "id": "SOLR-5551",
    "title": "Error while updating replicas",
    "details": {
        "affect_versions": "4.6",
        "status": "Reopened",
        "fix_versions": [],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "There is an error with peersynch in SolrCloud mode with decimal values. I have described the issue in detail here: http://lucene.472066.n3.nabble.com/Solr-Cloud-error-with-shard-update-td4106260.html",
    "attachments": {
        "26.log": "https://issues.apache.org/jira/secure/attachment/12639065/26.log",
        "24.log": "https://issues.apache.org/jira/secure/attachment/12639063/24.log",
        "25.log": "https://issues.apache.org/jira/secure/attachment/12639064/25.log"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Maxim Novikov",
            "id": "comment-13962142",
            "date": "2014-04-07T18:56:08+0000",
            "content": "I have got the same issue. Not sure that it is related to the data type, but at least I can see the described behavior.\n\nPS Solr v. 4.7.1. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13962163",
            "date": "2014-04-07T19:24:11+0000",
            "content": "Maxim Novikov: You'll need to provide options to your JDBC driver that force it to use standard Java data types, or you'll need to change the SQL that you are sending so that it produces standard types.  You'll need to contact your database vendor for help constructing the JDBC URL or the SQL query.\n\nI have no idea why the issue reporter was able to get it working on an older release of Solr.  I've seen similar issues with older releases of Solr.\n\nClosing the issue as invalid.  If you want to support nonstandard data types in DIH, you'll need to write custom code.\n\nRepeating my followup on the mailing list thread:\n\n\nThe real problem here is not DIH, but the JDBC driver.  The convertType parameter that you have set is for your JDBC driver. For most people, these details don't matter, because they are writing Java code themselves and can adjust according to the peculiarities of a specific JDBC driver.  DIH is a generic solution that can only deal with standard types.\n\nBigDecimal is not a standard java data type. Although it is included in the standard JVM, it is part of the math package, it is not built into Java.\n\nhttp://docs.oracle.com/javase/7/docs/api/java/math/BigDecimal.html\n\nYou might wonder why DIH doesn't convert the data.  The answer is that without the programmer explicitly providing code to detect each nonstandard type, it won't know HOW to convert it. Solr and Lucene can't be expected to support every data type, especially if the data type is not even available until you import a class.  "
        },
        {
            "author": "Maxim Novikov",
            "id": "comment-13962170",
            "date": "2014-04-07T19:37:41+0000",
            "content": "I don't use decimal type in MS SQL, only \"int\" and \"varchar\" exist in that table I get the data for indexing from. How can they be \"non-standard\"? I don't think MS JDBC driver maps \"int\" type to BigDecimal either. Again, as I wrote I hardly think it's a data type related issue. "
        },
        {
            "author": "Maxim Novikov",
            "id": "comment-13962179",
            "date": "2014-04-07T19:48:05+0000",
            "content": "PS I meant that I don't have the error about \"decimal\" data type at all, but I can see a behavior with similar errors on Solr machines:\n\nOne node:\nWARN DistributedUpdateProcessor Error sending update\n\nAnother node:\nWARN RecoveryStrategy Stopping recovery for zkNodeName=core_node3core=...\n\nPlease either reopen the issue as I believe its title is directly related, or if this particular one concerns \"decimal\" type thing only, rename it and create another one to track the problem(s) with such type of errors. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13962193",
            "date": "2014-04-07T19:58:11+0000",
            "content": "Can you attach a log showing the exact errors you get?  It needs to include the entire java stacktrace for any exceptions. "
        },
        {
            "author": "Maxim Novikov",
            "id": "comment-13962213",
            "date": "2014-04-07T20:29:14+0000",
            "content": "I don't think I can as the issue is closed and there is no such option in \"More\" menu now. Can you please ping me at csbubbles-at-gmail-com, I will send you what I got.\n\nPS And please rename this issue as it is kinda misleading. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13962269",
            "date": "2014-04-07T21:18:01+0000",
            "content": "Re-opening issue so user can attach logs. "
        },
        {
            "author": "Maxim Novikov",
            "id": "comment-13962282",
            "date": "2014-04-07T21:28:41+0000",
            "content": "25.log - the master node at the time (10.64.20.26:8080)\n24.log and 26.log - the other two instances in the cloud (10.64.20.24:8080 and 10.64.20.26:8080)\n\nI update the index using DIH on the node #24, and all the data seems to be replicated successfully (just to make clear here that I can't see the actual data inconsistency caused by any errors in Solr logs).\n\nSometimes the master node tries to communicate with all the nodes (including itself as well) but fails with \"DistributedUpdateProcessor  ? Error sending update\" error. It looks like it gets a timeout, however, all the nodes are alive and there are no issues with connection to each of them at all.\n\nIt might be an issue with configuration, but in that case please let me know where to look at, as I have no clue. Perhaps, logs should provide more relevant information if that is the case.\n\nPS If you need more detailed info, I can probably set up the logging more correctly to get timestamps as well. But the issue is easily reproducible in this infrastructure. Also, it looks like I am getting that error pretty stable once per hour. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13962303",
            "date": "2014-04-07T21:48:56+0000",
            "content": "I would guess underlying performance issues causing Solr to timeout while talking to zookeeper and to the other Solr instances.  It's certainly possible that you are running into a bug, but my early guess is that there's not enough system resources or possibly a config problem.  We need to move this to either the solr-user mailing list or to the IRC channel for further troubleshooting.\n\nFurther reading: http://wiki.apache.org/solr/SolrPerformanceProblems\n\nIf it turns out that there is a bug, you'll either be pointed at an existing issue number or asked to open a new one. "
        },
        {
            "author": "SimonDZhu",
            "id": "comment-14150518",
            "date": "2014-09-27T10:12:05+0000",
            "content": "Hi All,\n\nI run into a similar issue on solrcloud which always gives the following error message.\n\nCould anyone help please,\n\nThanks in advance,\n\nSimon\n\n14/09/27 17:27:05 INFO mapreduce.Job:  map 100% reduce 68%\n14/09/27 17:27:05 INFO mapreduce.Job: Task Id : attempt_1411790808211_0012_r_000000_2, Status : FAILED\nError: org.apache.solr.common.SolrException: Bad Request\n\nBad Request\n\nrequest: http://master:8899/solr/update?wt=javabin&version=2\n\tat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n\tat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n\tat org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n\tat org.apache.nutch.indexwriter.solr.SolrIndexWriter.write(SolrIndexWriter.java:135)\n\tat org.apache.nutch.indexer.IndexWriters.write(IndexWriters.java:88)\n\tat org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:50)\n\tat org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:41)\n\tat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.write(ReduceTask.java:493)\n\tat org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:422)\n\tat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:323)\n\tat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:53)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\n\n14/09/27 17:27:06 INFO mapreduce.Job:  map 100% reduce 0%\n14/09/27 17:27:28 INFO mapreduce.Job:  map 100% reduce 15%\n14/09/27 17:27:51 INFO mapreduce.Job:  map 100% reduce 32%\n14/09/27 17:27:54 INFO mapreduce.Job:  map 100% reduce 33%\n14/09/27 17:29:26 INFO mapreduce.Job:  map 100% reduce 67%\n14/09/27 17:29:59 INFO mapreduce.Job:  map 100% reduce 68%\n14/09/27 17:30:09 INFO mapreduce.Job:  map 100% reduce 100%\n14/09/27 17:30:15 INFO mapreduce.Job: Job job_1411790808211_0012 failed with state FAILED due to: Task failed task_1411790808211_0012_r_000000\nJob failed as tasks failed. failedMaps:0 failedReduces:1\n\n14/09/27 17:30:17 INFO mapreduce.Job: Counters: 40\n\tFile System Counters\n\t\tFILE: Number of bytes read=177143751\n\t\tFILE: Number of bytes written=1245091385\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=593913729\n\t\tHDFS: Number of bytes written=0\n\t\tHDFS: Number of read operations=136\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=0\n\tJob Counters \n\t\tFailed map tasks=5\n\t\tFailed reduce tasks=4\n\t\tLaunched map tasks=39\n\t\tLaunched reduce tasks=4\n\t\tOther local map tasks=5\n\t\tData-local map tasks=33\n\t\tRack-local map tasks=1\n\t\tTotal time spent by all maps in occupied slots (ms)=1871174\n\t\tTotal time spent by all reduces in occupied slots (ms)=486005\n\t\tTotal time spent by all map tasks (ms)=1871174\n\t\tTotal time spent by all reduce tasks (ms)=486005\n\t\tTotal vcore-seconds taken by all map tasks=1871174\n\t\tTotal vcore-seconds taken by all reduce tasks=486005\n\t\tTotal megabyte-seconds taken by all map tasks=1916082176\n\t\tTotal megabyte-seconds taken by all reduce tasks=497669120\n\tMap-Reduce Framework\n\t\tMap input records=3751130\n\t\tMap output records=3751130\n\t\tMap output bytes=1055861232\n\t\tMap output materialized bytes=1064042642\n\t\tInput split bytes=4799\n\t\tCombine input records=0\n\t\tSpilled Records=3795415\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=0\n\t\tGC time elapsed (ms)=3251\n\t\tCPU time spent (ms)=127480\n\t\tPhysical memory (bytes) snapshot=14463537152\n\t\tVirtual memory (bytes) snapshot=173441761280\n\t\tTotal committed heap usage (bytes)=14498136064\n\tFile Input Format Counters \n\t\tBytes Read=593908930\n14/09/27 17:30:17 ERROR indexer.IndexingJob: Indexer: java.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:836)\n\tat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:114)\n\tat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:176)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\tat org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:186)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:212) "
        }
    ]
}