{
    "id": "LUCENE-8323",
    "title": "New ConcatenateFilter, a TokenFilter to concat/join tokens",
    "details": {
        "components": [
            "modules/analysis"
        ],
        "status": "Resolved",
        "resolution": "Duplicate",
        "fix_versions": [],
        "affect_versions": "None",
        "labels": "",
        "priority": "Major",
        "type": "New Feature"
    },
    "description": "Here I introduce the ConcatenateFilter (with Factory) to concatenate/join tokens with a provided separator to produce one final token.  It's similar to FingerprintFilter but doesn't deduplicate or sort.  It's useful for doing exact-ish search on short text (think names or titles) with simple analysis.  At this task, its faster than a PhraseQuery equivalent, and solves the issue of matching completely and not a portion of the tokens.  It's also useful for using Lucene to hold a dictionary of short names/phrases for entity-extraction (aka text tagging).  The OpenSextant SolrTextTagger uses it for this purpose, which is where I'm taking it from.",
    "attachments": {
        "LUCENE-8323.patch": "https://issues.apache.org/jira/secure/attachment/12924199/LUCENE-8323.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16481219",
            "author": "David Smiley",
            "content": "All the new files in this patch except for TestConcatenateFilterFactory came from https://github.com/OpenSextant/SolrTextTagger and have the headers there (ASL v2, MITRE copyright, with some other legal mumbo jumbo.  I've added this to NOTICE.txt.\n\nIt's not quite a direct copy.  I improved the tests, improved the end() handling, and I copied aspects of FingerprintFilter & Factory into these to intentionally make them more consistent (thus fixing that end() bug).  I simplified FinterprintFilter.incrementToken's end state condition to be a boolean it was already using, which I think is much clearer.  I also added an empty-analysis test to both.\n\nWhat I'd really like to see is this thing enhanced to use GraphTokenStreamFiniteStrings.  That did not exist at the time it was developed.  It would add proper support for different position increments & lengths so that the analysis chain can usefully add synonyms, etc. ",
            "date": "2018-05-18T21:26:39+0000"
        },
        {
            "id": "comment-16489174",
            "author": "Adrien Grand",
            "content": "+1 I like that you documented explicitly that the behavior is currently undefined for stacked tokens and gaps.\n\nSome minor comments:\n\n\tDid you use those license headers on purpose for new files? They don't look like the usual ones that we use.\n\tMaybe use the start offset of the first token rather than 0 as a start offset.\n\tMake \"buf\" final.\n\tNit: can you add some text to the ctor javadocs? It looks a bit weird when there are only parameter descriptions.\n\n ",
            "date": "2018-05-24T15:04:07+0000"
        },
        {
            "id": "comment-16489305",
            "author": "Jim Ferenczi",
            "content": "\nWhat I'd really like to see is this thing enhanced to use GraphTokenStreamFiniteStrings. That did not exist at the time it was developed. It would add proper support for different position increments & lengths so that the analysis chain can usefully add synonyms, etc.\n\nYou can also check the CompletionTokenStream in the suggest package. It does exactly what you want and it's already a TokenStream so maybe it can be renamed and moved to the analysis module ? ",
            "date": "2018-05-24T16:16:55+0000"
        },
        {
            "id": "comment-16489618",
            "author": "David Smiley",
            "content": "Thanks for the review Adrien.\n\nDid you use those license headers on purpose for new files? They don't look like the usual ones that we use.\n\nIt's deliberate; this comes from another project, remember.  (see my 1st comment).\n\nI agree with all your other points but it may be moot now based my discovery of CompletionTokenStream...\n\nYou can also check the CompletionTokenStream in the suggest package. It does exactly what you want and it's already a TokenStream so maybe it can be renamed and moved to the analysis module ?\n\nWow thanks Jim; this is exactly what I'm looking for!\n\n+1 to move make move/rename CompletionTokenStream for broader use.\n\nI think it should be made a TokenFilter so that it can be used easily with, say, CustomAnalyzer.  I did this as a quick hack and it's mostly okay.  I had to debug some various tokenstream lifecycle stuff though that wasn't so much because it's a Filter and more due to with it getting tested in a more hardened way thanks to BaseTokenStreamTestCase.\n\nWhat name?  CompletionGraphTokenFilter maybe but the word \"Completion\" is tied too much to it's original use-case.  Maybe ConcatenateGraphTokenFilter or shorter ConcatGraphTokenFilter?  FiniteStringsGraphTokenFilter is another idea though it's name seems very non-obvious to all but internal Lucene devs.\n\nI think we should add \"@see\" references between GraphTokenStreamFiniteStrings and CompletionTokenStream as these things do very similar things.  It appears TokenStreamToAutomaton (used by CompletionTokenStream) and GraphTokenStreamFiniteStrings.build is a duplicated algorithm... they could be reused maybe.  But I didn't look closely to see.\n\nI just did a quick hack experiment of using CompletionTokenStream in place of ConcatenateFilter with the SolrTextTagger tests and it basically works.  I mentioned some lifecycle stuff above I debugged.  I needed to make the separator customizable (e.g. to be a space).  One weird thing is that the first position increment of CompletionTokenStream is 0 which IndexWriter is unhappy with so I set it to 1.  Interestingly, BaseTokenStreamTestCase didn't complain about this yet real world use complained right away.  Maybe BaseTokenStreamTestCase needs to explicitly test this?\n\nI'll throw up a patch once I get confirmation on a name. ",
            "date": "2018-05-24T19:13:04+0000"
        },
        {
            "id": "comment-16489956",
            "author": "Jim Ferenczi",
            "content": "\nI think it should be made a TokenFilter so that it can be used easily with, say, CustomAnalyzer.\n\nIt's a TokenStream because it consumes the entire input in the first call to incrementToken (which invokes input.reset(), input.end(), input.close()). It's fine though because TokenFilterFactory returns a TokenStream so you can already use it in a CustomAnalyzer or in an AnalyzerWrapper like the CompletionAnalyzer does. Also note that there is a BaseTokenStreamTestCase already: CompletionTokenStreamTest. \n\n\nWhat name? CompletionGraphTokenFilter maybe but the word \"Completion\" is tied too much to it's original use-case. Maybe ConcatenateGraphTokenFilter or shorter ConcatGraphTokenFilter? FiniteStringsGraphTokenFilter is another idea though it's name seems very non-obvious to all but internal Lucene devs.\n\n+1 to ConcatenateGraphTokenStream\n\n\nIt appears TokenStreamToAutomaton (used by CompletionTokenStream) and GraphTokenStreamFiniteStrings.build is a duplicated algorithm... they could be reused maybe. But I didn't look closely to see.\n\nI guess they could share some logic but that's a different issue   ",
            "date": "2018-05-24T22:59:40+0000"
        },
        {
            "id": "comment-16490171",
            "author": "David Smiley",
            "content": "I decided to close this issue, replacing it with LUCENE-8332 which I just filed. The approach is quite different than the original patch, though I might adopt some of the tests. ",
            "date": "2018-05-25T03:17:56+0000"
        }
    ]
}