{
    "id": "SOLR-5758",
    "title": "need ref guide doc on building indexes with mapreduce (morphlines-cell contrib)",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [],
        "components": [
            "contrib - MapReduce",
            "documentation"
        ],
        "type": "Task",
        "priority": "Major",
        "labels": "",
        "resolution": "Won't Fix"
    },
    "description": "This is marked experimental for 4.7, but we should have a section on it in the ref guide in 4.8",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-13909623",
            "date": "2014-02-23T01:32:36+0000",
            "content": "As a start, here is the help text for the MapReduceIndexerTool:\n\n\nusage: hadoop [GenericOptions]... jar solr-map-reduce-*.jar \n       [--help] --output-dir HDFS_URI [--input-list URI] --morphline-file FILE [--morphline-id STRING] [--update-conflict-resolver FQCN] [--mappers INTEGER] [--reducers INTEGER] [--max-segments INTEGER] [--fair-scheduler-pool STRING] [--dry-run] [--log4j FILE] [--verbose] [--show-non-solr-cloud]\n       [--zk-host STRING] [--go-live] [--collection STRING] [--go-live-threads INTEGER] [HDFS_URI [HDFS_URI ...]]\n\nMapReduce batch job driver that takes a morphline and creates a set of Solr index shards from a set of input files  and  writes  the  indexes  into  HDFS,  in  a  flexible,  scalable and fault-tolerant manner. It also supports merging the output shards into a set of live customer facing Solr servers, typically a\nSolrCloud. The program proceeds in several consecutive MapReduce based phases, as follows:\n\n1) Randomization phase: This (parallel) phase randomizes the list of input files in order to spread indexing load more evenly among the mappers of the subsequent phase.\n\n2) Mapper phase: This (parallel) phase takes the input files, extracts the relevant content, transforms it and hands SolrInputDocuments to  a  set  of  reducers.  The  ETL  functionality is flexible and customizable using chains of arbitrary morphline commands that pipe records from one transformation command to\nanother. Commands to parse and transform a set of standard data formats such as Avro, CSV, Text, HTML, XML, PDF, Word, Excel, etc. are provided out of  the  box, and additional custom commands and parsers for additional file or data formats can be added as morphline plugins. This is done by implementing a simple\nJava interface that consumes a record (e.g. a file in the form of an InputStream plus some headers plus contextual metadata) and generates as output zero or more  records.  Any  kind of data format can be indexed and any Solr documents for any kind of Solr schema can be generated, and any custom ETL logic can be\nregistered and executed.\nRecord fields, including MIME types, can also explicitly be passed by force from the CLI to the morphline, for example: hadoop ... -D morphlineField._attachment_mimetype=text/csv\n\n3) Reducer phase: This (parallel) phase loads the mapper's SolrInputDocuments into one EmbeddedSolrServer per reducer. Each such reducer and Solr server can be seen as a (micro) shard. The Solr servers store their data in HDFS.\n\n4) Mapper-only merge phase: This (parallel) phase merges the set of reducer shards into the number of solr shards expected by the user, using a mapper-only job. This phase is omitted if the number of shards is already equal to the number of shards expected by the user. \n\n5) Go-live phase: This optional (parallel) phase merges the output shards of the previous phase into a set of live customer facing Solr servers, typically a SolrCloud. If this phase is omitted you can explicitly point each Solr server to one of the HDFS output shard directories.\n\nFault Tolerance: Mapper and reducer task attempts are retried on failure per the standard MapReduce semantics. On program startup all data in the --output-dir is deleted if that output directory already exists. If the whole job fails you can retry simply by rerunning the program again using the same arguments.\n\npositional arguments:\n  HDFS_URI               HDFS URI of file or directory tree to index. (default: [])\n\noptional arguments:\n  --help, -help, -h      Show this help message and exit\n  --input-list URI       Local URI or HDFS URI of a UTF-8 encoded file containing a list of HDFS URIs to index, one URI per line in the file. If '-' is specified, URIs are read from the standard input. Multiple --input-list arguments can be specified.\n  --morphline-id STRING  The identifier of the morphline that shall be executed within the morphline config file specified by --morphline-file. If the --morphline-id option is ommitted the first (i.e. top-most) morphline within the config file is used. Example: morphline1\n  --update-conflict-resolver FQCN\n                         Fully qualified class name of a Java class that implements the UpdateConflictResolver interface. This enables deduplication and ordering of a series of  document  updates for the same unique document key. For example, a MapReduce batch job might index multiple files in the same job where\n                         some of the files contain old and new versions of the very same document, using the same unique document key.\n                         Typically, implementations of this interface forbid collisions by throwing an exception, or ignore all but the most recent document version, or,  in  the  general case, order colliding updates ascending from least recent to most recent (partial) update. The caller of this interface (i.e.\n                         the Hadoop Reducer) will then apply the updates to Solr in the order returned by the orderUpdates() method.\n                         The  default  RetainMostRecentUpdateConflictResolver  implementation  ignores  all  but  the   most   recent   document   version,   based   on   a   configurable   numeric   Solr   field,   which  defaults  to  the  file_last_modified  timestamp  (default:  org.apache.solr.hadoop.dedup.\n                         RetainMostRecentUpdateConflictResolver)\n  --mappers INTEGER      Tuning knob that indicates the maximum number of MR mapper tasks to use. -1 indicates use all map slots available on the cluster. (default: -1)\n  --reducers INTEGER     Tuning knob that indicates the number of reducers to index into. -1 indicates use all reduce slots available on the cluster. 0 indicates use  one reducer per output shard, which disables the mtree merge MR algorithm. The mtree merge MR algorithm improves scalability by spreading load (in\n                         particular CPU load) among a number of parallel reducers that can be much larger than the number of solr shards expected by the  user.  It  can  be seen as an extension of concurrent lucene merges and tiered lucene merges to the clustered case. The subsequent mapper-only phase merges the\n                         output of said large number of reducers to the number of shards expected by the user, again by utilizing more available parallelism on the cluster. (default: -1)\n  --max-segments INTEGER\n                         Tuning knob that indicates the maximum number of segments to be contained on output in the index of each reducer shard. After  a  reducer  has  built  its  output  index it applies a merge policy to merge segments until there are <= maxSegments lucene segments left in this index. Merging\n                         segments involves reading and rewriting all data in all these segment files, potentially multiple times, which is very I/O intensive and  time  consuming.  However,  an  index  with fewer segments can later be merged faster, and it can later be queried faster once deployed to a live Solr\n                         serving shard. Set maxSegments to 1 to optimize the index for low query latency. In a nutshell, a small maxSegments value trades indexing latency for subsequently improved query latency. This can be a reasonable trade-off for batch indexing systems. (default: 1)\n  --fair-scheduler-pool STRING\n                         Optional tuning knob that indicates the name of the fair scheduler pool to submit jobs to. The Fair Scheduler is a pluggable MapReduce scheduler that  provides a way to share large clusters. Fair scheduling is a method of assigning resources to jobs such that all jobs get, on average, an\n                         equal share of resources over time. When there is a single job running, that job uses the entire cluster. When other jobs are submitted, tasks slots  that free up are assigned to the new jobs, so that each job gets roughly the same amount of CPU time. Unlike the default Hadoop scheduler,\n                         which forms a queue of jobs, this lets short jobs finish in reasonable time while not starving long jobs. It is also an easy way to share  a cluster between multiple of users. Fair sharing can also work with job priorities - the priorities are used as weights to determine t\nGeneric options supported are\n  --conf <configuration file>\n                         specify an application configuration file\n  -D <property=value>    use value for given property\n  --fs <local|namenode:port>\n                         specify a namenode\n  --jt <local|jobtracker:port>\n                         specify a job tracker\n  --files <comma separated list of files>\n                         specify comma separated files to be copied to the map reduce cluster\n  --libjars <comma separated list of jars>\n                         specify comma separated jar files to include in the classpath.\n  --archives <comma separated list of archives>\n                         specify comma separated archives to be unarchived on the compute machines.\n\nThe general command line syntax is\nbin/hadoop command [genericOptions] [commandOptions]\n\nExamples: \n\n# (Re)index an Avro based Twitter tweet file:\nsudo -u hdfs hadoop \\\n  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\n  jar target/solr-map-reduce-*.jar \\\n  -D 'mapred.child.java.opts=-Xmx500m' \\\n  --log4j src/test/resources/log4j.properties \\\n  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\n  --solr-home-dir src/test/resources/solr/minimr \\\n  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\n  --shards 1 \\\n  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\n\n# (Re)index all files that match all of the following conditions:\n# 1) File is contained in dir tree hdfs:///user/$USER/solrloadtest/twitter/tweets\n# 2) file name matches the glob pattern 'sample-statuses*.gz'\n# 3) file was last modified less than 100000 minutes ago\n# 4) file size is between 1 MB and 1 GB\n# Also include extra library jar file containing JSON tweet Java parser:\nhadoop jar target/solr-map-reduce-*.jar com.cloudera.cdk.morphline.hadoop.find.HdfsFindTool \\\n  -find hdfs:///user/$USER/solrloadtest/twitter/tweets \\\n  -type f \\\n  -name 'sample-statuses*.gz' \\\n  -mmin -1000000 \\\n  -size -100000000c \\\n  -size +1000000c \\\n| sudo -u hdfs hadoop \\\n  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\n  jar target/solr-map-reduce-*.jar \\\n  -D 'mapred.child.java.opts=-Xmx500m' \\\n  --log4j src/test/resources/log4j.properties \\\n  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadJsonTestTweets.conf \\\n  --solr-home-dir src/test/resources/solr/minimr \\\n  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\n  --shards 100 \\\n  --input-list -\n\n# Go live by merging resulting index shards into a live Solr cluster\n# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\nsudo -u hdfs hadoop \\\n  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\n  jar target/solr-map-reduce-*.jar \\\n  -D 'mapred.child.java.opts=-Xmx500m' \\\n  --log4j src/test/resources/log4j.properties \\\n  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\n  --solr-home-dir src/test/resources/solr/minimr \\\n  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\n  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\n  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\n  --go-live \\\n  hdfs:///user/foo/indir\n\n# Go live by merging resulting index shards into a live SolrCloud cluster\n# (discover shards and Solr URLs through ZooKeeper):\nsudo -u hdfs hadoop \\\n  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\n  jar target/solr-map-reduce-*.jar \\\n  -D 'mapred.child.java.opts=-Xmx500m' \\\n  --log4j src/test/resources/log4j.properties \\\n  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\n  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\n  --zk-host zk01.mycompany.com:2181/solr \\\n  --collection collection1 \\\n  --go-live \\\n  hdfs:///user/foo/indir\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13909624",
            "date": "2014-02-23T01:33:25+0000",
            "content": "Kite Morphlines documentation: http://kitesdk.org/docs/current/kite-morphlines/ "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13914760",
            "date": "2014-02-27T17:25:58+0000",
            "content": "That output is affect by SOLR-5782 - I'll make another dump shortly. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13915182",
            "date": "2014-02-27T23:18:42+0000",
            "content": "An updated dump:\n\n\nusage: hadoop [GenericOptions]... jar solr-map-reduce-*.jar \n       [--help] --output-dir HDFS_URI [--input-list URI]\n       --morphline-file FILE [--morphline-id STRING]\n       [--update-conflict-resolver FQCN] [--mappers INTEGER]\n       [--reducers INTEGER] [--max-segments INTEGER]\n       [--fair-scheduler-pool STRING] [--dry-run] [--log4j FILE]\n       [--verbose] [--show-non-solr-cloud] [--zk-host STRING] [--go-live]\n       [--collection STRING] [--go-live-threads INTEGER]\n       [HDFS_URI [HDFS_URI ...]]\n\nMapReduce batch job driver that  takes  a  morphline  and  creates a set of\nSolr index shards from a set  of  input  files  and writes the indexes into\nHDFS, in a flexible, scalable  and  fault-tolerant manner. It also supports\nmerging the output shards into a set  of live customer facing Solr servers,\ntypically  a  SolrCloud.  The  program   proceeds  in  several  consecutive\nMapReduce based phases, as follows:\n\n1) Randomization phase: This (parallel) phase  randomizes the list of input\nfiles in order to spread  indexing  load  more  evenly among the mappers of\nthe subsequent phase.\n\n2) Mapper phase: This (parallel) phase  takes the input files, extracts the\nrelevant content, transforms it and  hands  SolrInputDocuments  to a set of\nreducers. The ETL functionality is  flexible  and customizable using chains\nof arbitrary morphline commands that  pipe  records from one transformation\ncommand to another. Commands to parse and  transform a set of standard data\nformats such as Avro, CSV,  Text,  HTML,  XML,  PDF,  Word, Excel, etc. are\nprovided out of the box,  and  additional  custom  commands and parsers for\nadditional file or data formats can be  added as morphline plugins. This is\ndone by implementing a simple Java  interface  that consumes a record (e.g.\na file in the form  of  an  InputStream  plus  some headers plus contextual\nmetadata) and generates as output zero  or  more  records. Any kind of data\nformat can be indexed and any  Solr  documents  for any kind of Solr schema\ncan be generated, and any custom ETL logic can be registered and executed.\nRecord fields, including  MIME  types,  can  also  explicitly  be passed by\nforce  from  the  CLI  to  the   morphline,  for  example:  hadoop  ...  -D\nmorphlineField._attachment_mimetype=text/csv\n\n3)   Reducer   phase:   This   (parallel)    phase   loads   the   mapper's\nSolrInputDocuments into  one  EmbeddedSolrServer  per  reducer.  Each  such\nreducer and Solr server can be  seen  as  a (micro) shard. The Solr servers\nstore their data in HDFS.\n\n4) Mapper-only  merge  phase:  This  (parallel)  phase  merges  the  set of\nreducer shards into the number of  solr  shards expected by the user, using\na mapper-only job.  This  phase  is  omitted  if  the  number  of shards is\nalready equal to the number of shards expected by the user. \n\n5) Go-live phase: This optional  (parallel)  phase merges the output shards\nof the previous phase into  a  set  of  live  customer facing Solr servers,\ntypically a SolrCloud. If this  phase  is  omitted you can explicitly point\neach Solr server to one of the HDFS output shard directories.\n\nFault Tolerance: Mapper and reducer  task  attempts  are retried on failure\nper the standard MapReduce semantics. On program startup all data in the --\noutput-dir is deleted  if  that  output  directory  already  exists. If the\nwhole job fails you can retry  simply  by rerunning the program again using\nthe same arguments.\n\npositional arguments:\n  HDFS_URI               HDFS URI  of  file  or  directory  tree  to index.\n                         (default: [])\n\noptional arguments:\n  --help, -help, -h      Show this help message and exit\n  --input-list URI       Local URI or  HDFS  URI  of  a  UTF-8 encoded file\n                         containing a list of HDFS  URIs  to index, one URI\n                         per line in the  file.  If  '-' is specified, URIs\n                         are read  from  the  standard  input.  Multiple --\n                         input-list arguments can be specified.\n  --morphline-id STRING  The identifier  of  the  morphline  that  shall be\n                         executed  within   the   morphline   config   file\n                         specified by --morphline-file. If the --morphline-\n                         id option is  ommitted  the  first (i.e. top-most)\n                         morphline  within  the   config   file   is  used.\n                         Example: morphline1\n  --update-conflict-resolver FQCN\n                         Fully qualified class name  of  a  Java class that\n                         implements the  UpdateConflictResolver  interface.\n                         This  enables  deduplication  and  ordering  of  a\n                         series of document  updates  for  the  same unique\n                         document key. For example,  a  MapReduce batch job\n                         might index multiple files  in  the same job where\n                         some of the files contain  old and new versions of\n                         the very  same  document,  using  the  same unique\n                         document key.\n                         Typically,  implementations   of   this  interface\n                         forbid collisions  by  throwing  an  exception, or\n                         ignore all but the  most  recent document version,\n                         or, in the general  case,  order colliding updates\n                         ascending  from  least   recent   to  most  recent\n                         (partial) update. The caller of this interface (i.\n                         e.  the  Hadoop  Reducer)   will  then  apply  the\n                         updates to  Solr  in  the  order  returned  by the\n                         orderUpdates() method.\n                         The                                        default\n                         RetainMostRecentUpdateConflictResolver\n                         implementation ignores  all  but  the  most recent\n                         document version, based on  a configurable numeric\n                         Solr    field,    which     defaults     to    the\n                         file_last_modified timestamp (default: org.apache.\n                         solr.hadoop.dedup.\n                         RetainMostRecentUpdateConflictResolver)\n  --mappers INTEGER      Tuning knob that indicates  the  maximum number of\n                         MR mapper tasks to use.  -1  indicates use all map\n                         slots available on the cluster. (default: -1)\n  --reducers INTEGER     Tuning knob that indicates  the number of reducers\n                         to index into. -1  indicates  use all reduce slots\n                         available on  the  cluster.  0  indicates  use one\n                         reducer  per  output  shard,  which  disables  the\n                         mtree merge  MR  algorithm.  The  mtree  merge  MR\n                         algorithm improves scalability  by  spreading load\n                         (in  particular  CPU  load)   among  a  number  of\n                         parallel reducers that  can  be  much  larger than\n                         the number of solr  shards  expected  by the user.\n                         It can  be  seen  as  an  extension  of concurrent\n                         lucene merges  and  tiered  lucene  merges  to the\n                         clustered case. The  subsequent  mapper-only phase\n                         merges  the  output  of   said   large  number  of\n                         reducers to the number  of  shards expected by the\n                         user,   again   by    utilizing   more   available\n                         parallelism on the cluster. (default: -1)\n  --max-segments INTEGER\n                         Tuning knob that indicates  the  maximum number of\n                         segments to be contained  on  output  in the index\n                         of each reducer shard.  After  a reducer has built\n                         its output index  it  applies  a  merge  policy to\n                         merge segments  until  there  are  <=  maxSegments\n                         lucene  segments  left  in   this  index.  Merging\n                         segments involves reading  and  rewriting all data\n                         in all these  segment  files, potentially multiple\n                         times,  which  is  very  I/O  intensive  and  time\n                         consuming. However, an  index  with fewer segments\n                         can later be merged  faster,  and  it can later be\n                         queried  faster  once  deployed  to  a  live  Solr\n                         serving shard. Set  maxSegments  to  1 to optimize\n                         the index for low query  latency. In a nutshell, a\n                         small maxSegments  value  trades  indexing latency\n                         for subsequently improved query  latency. This can\n                         be  a  reasonable  trade-off  for  batch  indexing\n                         systems. (default: 1)\n  --fair-scheduler-pool STRING\n                         Optional tuning knob  that  indicates  the name of\n                         the fair scheduler  pool  to  submit  jobs to. The\n                         Fair Scheduler is a  pluggable MapReduce scheduler\n                         that provides a way to  share large clusters. Fair\n                         scheduling is a method  of  assigning resources to\n                         jobs such that all jobs  get, on average, an equal\n                         share of resources  over  time.  When  there  is a\n                         single job  running,  that  job  uses  the  entire\n                         cluster. When  other  jobs  are  submitted,  tasks\n                         slots that free up are  assigned  to the new jobs,\n                         so that each job gets  roughly  the same amount of\n                         CPU time.  Unlike  the  default  Hadoop scheduler,\n                         which forms a queue of  jobs, this lets short jobs\n                         finish in reasonable time  while not starving long\n                         jobs. It is also an  easy  way  to share a cluster\n                         between multiple of users.  Fair  sharing can also\n                         work with  job  priorities  -  the  priorities are\n                         used as  weights  to  determine  the  fraction  of\n                         total compute time that each job gets.\n  --dry-run              Run in local mode  and  print  documents to stdout\n                         instead of loading them  into  Solr. This executes\n                         the  morphline  in  the  client  process  (without\n                         submitting a job  to  MR)  for  quicker turnaround\n                         during early  trial  &  debug  sessions. (default:\n                         false)\n  --log4j FILE           Relative or absolute  path  to  a log4j.properties\n                         config file on the  local  file  system. This file\n                         will  be  uploaded  to   each  MR  task.  Example:\n                         /path/to/log4j.properties\n  --verbose, -v          Turn on verbose output. (default: false)\n  --show-non-solr-cloud  Also show options for  Non-SolrCloud  mode as part\n                         of --help. (default: false)\n\nRequired arguments:\n  --output-dir HDFS_URI  HDFS directory to  write  Solr  indexes to. Inside\n                         there one  output  directory  per  shard  will  be\n                         generated.    Example:     hdfs://c2202.mycompany.\n                         com/user/$USER/test\n  --morphline-file FILE  Relative or absolute path  to  a local config file\n                         that contains one  or  more  morphlines.  The file\n                         must     be      UTF-8      encoded.      Example:\n                         /path/to/morphline.conf\n\nCluster arguments:\n  Arguments that provide information about your Solr cluster. \n\n  --zk-host STRING       The address of a ZooKeeper  ensemble being used by\n                         a SolrCloud cluster. This  ZooKeeper ensemble will\n                         be examined  to  determine  the  number  of output\n                         shards to create  as  well  as  the  Solr  URLs to\n                         merge the output shards into  when using the --go-\n                         live option. Requires that  you  also  pass the --\n                         collection to merge the shards into.\n                         \n                         The   --zk-host   option   implements   the   same\n                         partitioning semantics as  the  standard SolrCloud\n                         Near-Real-Time (NRT)  API.  This  enables  to  mix\n                         batch  updates  from   MapReduce   ingestion  with\n                         updates from standard  Solr  NRT  ingestion on the\n                         same SolrCloud  cluster,  using  identical  unique\n                         document keys.\n                         \n                         Format is: a  list  of  comma  separated host:port\n                         pairs,  each  corresponding   to   a   zk  server.\n                         Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:\n                         2183' If the optional  chroot  suffix  is used the\n                         example  would  look  like:  '127.0.0.1:2181/solr,\n                         127.0.0.1:2182/solr,127.0.0.1:2183/solr'     where\n                         the client would  be  rooted  at  '/solr'  and all\n                         paths would  be  relative  to  this  root  -  i.e.\n                         getting/setting/etc... '/foo/bar' would  result in\n                         operations being run on  '/solr/foo/bar' (from the\n                         server perspective).\n                         \n\nGo live arguments:\n  Arguments for  merging  the  shards  that  are  built  into  a  live Solr\n  cluster. Also see the Cluster arguments.\n\n  --go-live              Allows you to  optionally  merge  the  final index\n                         shards into a  live  Solr  cluster  after they are\n                         built. You can pass the  ZooKeeper address with --\n                         zk-host and the relevant  cluster information will\n                         be auto detected.  (default: false)\n  --collection STRING    The SolrCloud  collection  to  merge  shards  into\n                         when  using  --go-live   and  --zk-host.  Example:\n                         collection1\n  --go-live-threads INTEGER\n                         Tuning knob that indicates  the  maximum number of\n                         live merges  to  run  in  parallel  at  one  time.\n                         (default: 1000)\n\nGeneric options supported are\n  --conf <configuration file>\n                         specify an application configuration file\n  -D <property=value>    use value for given property\n  --fs <local|namenode:port>\n                         specify a namenode\n  --jt <local|jobtracker:port>\n                         specify a job tracker\n  --files <comma separated list of files>\n                         specify comma separated files to  be copied to the\n                         map reduce cluster\n  --libjars <comma separated list of jars>\n                         specify comma separated  jar  files  to include in\n                         the classpath.\n  --archives <comma separated list of archives>\n                         specify comma separated archives  to be unarchived\n                         on the compute machines.\n\nThe general command line syntax is\nbin/hadoop command [genericOptions] [commandOptions]\n\nExamples: \n\n# (Re)index an Avro based Twitter tweet file:\nsudo -u hdfs hadoop \\\n  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\n  jar target/solr-map-reduce-*.jar \\\n  -D 'mapred.child.java.opts=-Xmx500m' \\\n  --log4j src/test/resources/log4j.properties \\\n  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\n  --solr-home-dir src/test/resources/solr/minimr \\\n  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\n  --shards 1 \\\n  hdfs:///user/$USER/test-documents/sample-statuses-20120906-141433.avro\n\n\n# Go live by merging resulting index shards into a live Solr cluster\n# (explicitly specify Solr URLs - for a SolrCloud cluster see next example):\nsudo -u hdfs hadoop \\\n  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\n  jar target/solr-map-reduce-*.jar \\\n  -D 'mapred.child.java.opts=-Xmx500m' \\\n  --log4j src/test/resources/log4j.properties \\\n  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\n  --solr-home-dir src/test/resources/solr/minimr \\\n  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\n  --shard-url http://solr001.mycompany.com:8983/solr/collection1 \\\n  --shard-url http://solr002.mycompany.com:8983/solr/collection1 \\\n  --go-live \\\n  hdfs:///user/foo/indir\n\n# Go live by merging resulting index shards into a live SolrCloud cluster\n# (discover shards and Solr URLs through ZooKeeper):\nsudo -u hdfs hadoop \\\n  --config /etc/hadoop/conf.cloudera.mapreduce1 \\\n  jar target/solr-map-reduce-*.jar \\\n  -D 'mapred.child.java.opts=-Xmx500m' \\\n  --log4j src/test/resources/log4j.properties \\\n  --morphline-file ../search-core/src/test/resources/test-morphlines/tutorialReadAvroContainer.conf \\\n  --output-dir hdfs://c2202.mycompany.com/user/$USER/test \\\n  --zk-host zk01.mycompany.com:2181/solr \\\n  --collection collection1 \\\n  --go-live \\\n  hdfs:///user/foo/indir\n\n\n "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13971001",
            "date": "2014-04-16T12:56:51+0000",
            "content": "Move issue to Solr 4.9. "
        }
    ]
}