{
    "id": "LUCENE-1410",
    "title": "PFOR implementation",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/index"
        ],
        "type": "New Feature",
        "fix_versions": [
            "Bulk Postings branch"
        ],
        "affect_versions": "None",
        "resolution": "Implemented",
        "status": "Closed"
    },
    "description": "Implementation of Patched Frame of Reference.",
    "attachments": {
        "autogen.tgz": "https://issues.apache.org/jira/secure/attachment/12391554/autogen.tgz",
        "TestPFor2.java": "https://issues.apache.org/jira/secure/attachment/12391413/TestPFor2.java",
        "LUCENE-1410b.patch": "https://issues.apache.org/jira/secure/attachment/12391344/LUCENE-1410b.patch",
        "LUCENE-1410-codecs.tar.bz2": "https://issues.apache.org/jira/secure/attachment/12421437/LUCENE-1410-codecs.tar.bz2",
        "for-summary.txt": "https://issues.apache.org/jira/secure/attachment/12436708/for-summary.txt",
        "LUCENE-1410.patch": "https://issues.apache.org/jira/secure/attachment/12450845/LUCENE-1410.patch",
        "TermQueryTests.tgz": "https://issues.apache.org/jira/secure/attachment/12391969/TermQueryTests.tgz",
        "LUCENE-1410c.patch": "https://issues.apache.org/jira/secure/attachment/12391815/LUCENE-1410c.patch",
        "LUCENE-1410d.patch": "https://issues.apache.org/jira/secure/attachment/12392034/LUCENE-1410d.patch",
        "LUCENE-1410e.patch": "https://issues.apache.org/jira/secure/attachment/12396203/LUCENE-1410e.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-10-01T19:52:10+0000",
            "content": "20081001: Initial implementation of PFOR.\n\nThe target package is o.a.l.util.pfor, mostly for testing convenience,\neven though the code has no dependencies on Lucene.\n\nTo try it out please use jvmarg -server during the test, see also TestPFor.java on how to do this. The command\nant -Dtestcase=TestPFor test-core\nshould start the test after applying the patch.\nThe test will take about 25 seconds to run. \n\nThere is some optimization for decompression included. On my machine\nwith a 1.6.0_03-b05 Sun jvm, the decompression performance for 1-7 bits\nframe size varies between about 60M ints/sec unoptimized and 200M ints/sec\noptimized, as reported by the test. This appears adequate for practical use,\nbut it should be noted that this performance is from CPU cache to CPU cache.\n\nThe implementation still needs quite a bit of work. I'm posting it now because\nI'd like feedback on the interface for compression and decompression.\nTypical intended usage is present in TestPFor.java. ",
            "author": "Paul Elschot",
            "id": "comment-12636144"
        },
        {
            "date": "2008-10-01T22:24:39+0000",
            "content": "Awesome, I'll have a look!\n\nThe TestPFor.java didn't make it into the patch (it just references a link). ",
            "author": "Michael McCandless",
            "id": "comment-12636211"
        },
        {
            "date": "2008-10-02T06:32:12+0000",
            "content": "Sorry about the mistake in the patch, a correction will follow shortly. ",
            "author": "Paul Elschot",
            "id": "comment-12636292"
        },
        {
            "date": "2008-10-02T06:38:23+0000",
            "content": "A corrected b.patch including TestPFor.java ",
            "author": "Paul Elschot",
            "id": "comment-12636293"
        },
        {
            "date": "2008-10-02T07:00:07+0000",
            "content": "Another detail about the decompression performance as reported above: these are all cases without exceptions, so an expected performance degradation for patching exceptions is not included in the performance results.\nPatching exceptions is provided for in the code, but that code is not yet optimized. ",
            "author": "Paul Elschot",
            "id": "comment-12636295"
        },
        {
            "date": "2008-10-03T11:25:43+0000",
            "content": "\nPaul, I'm eager to test pfor on real Lucene vInts.\n\nSo I created a simple test program (attached TestPFor2.java).  Run it\nlike this:\n\n\n  Usage: java org.apache.lucene.util.pfor.TestPFor2 <indexDirName> <vIntFileNameIn> <pForFileNameOut>\n  \n  Eg: java org.apache.lucene.util.pfor.TestPFor2 /lucene/index _l.prx _l.prx.pfor\n\n\n\nwhere indexDirName is the directory of a Lucene index, vIntFileNameIn\nshould be a file that just has a bunch of vInts (Lucene's *.frq and\n*.prx fit this) and pForFileNameOut is a file it writes with blocks\nencoded in PFor.\n\nIt first encodes the vInts from vIntFileNameIn into pfor blocks\nwritten to pForFileNameOut.  Then it measures decode time of reading\nall vInts from vIntFileNameIn vs decode time of reading all pfor\nblocks.  It runs each round 5 times.\n\nThe test has certain serious flaws:\n\n\n\tCan you add a method that figures out the right frame size to use\n    for a given block of ints (so that ~90% of the ints are < N bits)?\n\n\n\n\n\tI'm using fixed 6-bit frame size.  Can you add bigger bit sizes to\n    your pfor decompress?\n\n\n\nWith these fixes the test would be more fair to pfor.\n\nIn the PFor file that I write, I simply write an int (# bytes)\nfollowed by the bytes, for each block.  Then when reading these blocks\nI read the #bytes, then read into the byte array backing the intArray\npassed to the PFor for decompress.  In a real integration I think\nwriting the int #bytes should be unecessary (is pfor self\npuncuating?).\n\nThis is inefficient because in doing this for real we should avoid the\ndouble-copy of the byte[].  In fact, we might push it even lower\n(under IndexInput, eg, create a IntBlockIndexInput) to possibly avoid\nthe copy into byte[] in BufferedIndexInput by maybe using direct\nByteBuffers from the OS.  So even once we fix the top two issues\nabove, the results of a \"real\" integration should be still faster.\n\nI ran this on a 622 MB prx file from a Wikipedia index, and even with\nthe above 2 limitations it's still a good amount faster:\n\n\njava org.apache.lucene.util.pfor.TestPFor2 /lucene/big _l.prx _l.prx.pfor\n\nencode _l.prx to _l.prx.pfor...\n442979072 vints; 888027375 bytes compressed vs orig size 651670377\n\ndecompress using pfor:\n4198 msec to decode 442979072 vInts (105521 vInts/msec)\n4332 msec to decode 442979072 vInts (102257 vInts/msec)\n4165 msec to decode 442979072 vInts (106357 vInts/msec)\n4122 msec to decode 442979072 vInts (107467 vInts/msec)\n4061 msec to decode 442979072 vInts (109081 vInts/msec)\n\ndecompress using readVInt:\n7315 msec to read 442979104 vInts (60557 vInts/msec)\n7390 msec to read 442979104 vInts (59943 vInts/msec)\n5816 msec to read 442979104 vInts (76165 vInts/msec)\n5937 msec to read 442979104 vInts (74613 vInts/msec)\n5970 msec to read 442979104 vInts (74200 vInts/msec)\n\n\n\nIt's really weird how the time gets suddenly faster during readVInt.\nIt's very repeatable. on another machine I see it get suddenly slower\nstarting at the same (3rd) round.  Adding -server and -Xbatch doesn't\nchange this behavior.  This is with (build 1.6.0_10-rc-b28) on Linux\nand (build 1.6.0_05-b13-120) on Mac OS X 10.5.5.\n ",
            "author": "Michael McCandless",
            "id": "comment-12636607"
        },
        {
            "date": "2008-10-03T15:13:19+0000",
            "content": "Q: Can you add a method that figures out the right frame size to use\nfor a given block of ints (so that ~90% of the ints are < N bits)?\nA: PFor.getNumFrameBits() does this for a given int[] at offset and size.\nChoosing the right size is a dilemma: too small will need too much header decoding\nand too large may result in using too large number of frame bits, i.e. too large compressed size.\n\nQ: I'm using fixed 6-bit frame size. Can you add bigger bit sizes to your pfor decompress?\nA: It should work for 1<=numFrameBits<=30.\n\nQ: is pfor self punctuating?\nA: PFor.bufferByteSize() returns the number of bytes used in the compressed buffer, see also the javadocs.\nFor practical use, the start of each compressed block must be known, either from somewhere else,\nor from the size of the previously encoded block.\nThe number of compressed integers is encoded in the header, but I'm not sure whether I\nmade that info available before decompression to allow allocation of an int[] that is large\nenough to hold the decompressed data.\n\n>: It's really weird how the time gets suddenly faster during readVInt.\nA: it's probably the JIT jumping in. That's why I preferred to test in 3 1-second loops and reporting\nperformance each second. The 2nd second always has better performance.\n\nIt's nice to see that PFor is faster than VInt, I had not tested that yet.\nWhich block size did you use for PFor? Never mind, I'll take a look at the code of TestPFor2.\n\nBtw. after decompressing, the things are ints not vInts. ",
            "author": "Paul Elschot",
            "id": "comment-12636658"
        },
        {
            "date": "2008-10-03T15:23:35+0000",
            "content": "Q: I'm using fixed 6-bit frame size. Can you add bigger bit sizes to your pfor decompress?\n A: It should work for 1<=numFrameBits<=30.\n\nThat answer was too short. There is some fallback code (decodeAnyFrame) that will decompress\nfor any frame or reference. The patch contains unrolled versions of that for up to 7 bits iirc.\nI'll add higher numbers of bits later, the code is straightforward and it could actually be generated.\nBtw. on my machine the unrolled 9 bit version is actually a bit slower than the loop, I don't know why,\nmaybe there are too many buffer references (9) in the loop for the jit to cope with.\n ",
            "author": "Paul Elschot",
            "id": "comment-12636660"
        },
        {
            "date": "2008-10-03T17:25:31+0000",
            "content": "A: PFor.getNumFrameBits() does this for a given int[] at offset and size.\n\nSuper, I missed that \u2013 I'll use it.\n\nBtw. after decompressing, the things are ints not vInts.\n\nOh yeah, I'll fix the prints in my test.\n\nThere is some fallback code (decodeAnyFrame) that will decompress for any frame or reference\n\nRight I was wanting the unrolled version to see the fastest result we can get for pfor, but your next observation is spooky so maybe this isn't really going to help our test:\n\n\nBtw. on my machine the unrolled 9 bit version is actually a bit slower than the loop, I don't know why,\nmaybe there are too many buffer references (9) in the loop for the jit to cope with.\nWe should look at the asm that's produced?\n\nit's probably the JIT jumping in.\n\nBut strangely with -Xbatch I still see the effect.  And even stranger, on another machine (Mac OS X) it gets slower when the JIT jumps in.  I'm spooked.\n\nWhich block size did you use for PFor?\n\nI used 128 but I'll try different sizes.\n\nFor practical use, the start of each compressed block must be known, either from somewhere else, or from the size of the previously encoded block.\n\nBut can I also get the \"bytes consumed\" when I ask decompress() to\nrun?\n\nWhen we really integrate, things like term infos and skipping data\nwill know how to jump to the start of blocks.  But for raw sequential\nscanning, if the header is self-punctuating we would not need to store\nthe block size between each block. ",
            "author": "Michael McCandless",
            "id": "comment-12636685"
        },
        {
            "date": "2008-10-03T17:26:19+0000",
            "content": "For people not intimately familiar with PFOR (like me), I found the following to be helpful:\nhttp://cis.poly.edu/cs912/indexcomp.pdf ",
            "author": "Otis Gospodnetic",
            "id": "comment-12636686"
        },
        {
            "date": "2008-10-03T17:38:41+0000",
            "content": "There's also this recent comparison of index compression approaches: http://www2008.org/papers/pdf/p387-zhangA.pdf and http://homepages.cwi.nl/~heman/downloads/msthesis.pdf. ",
            "author": "Michael McCandless",
            "id": "comment-12636689"
        },
        {
            "date": "2008-10-03T18:31:03+0000",
            "content": "Q: We should look at the asm that's produced?\nA: Maybe. But would that be java bytecodes or x86 or powerpc? I'd prefer to wait with that.\nThere are some nice machine instrns to this on the various architectures, but that would require to use native code.\n\nQ: But can I also get the \"bytes consumed\" when I ask decompress() to run?\nA; Yes, but only after decompression or after compression.\nThe number of exceptions is not explicitly coded in the header, so the size to encode the exceptions is not known beforehand. That could be changed, but than the header will get bigger.\n(For compression, it's possible to do a run without buffer first.)\n\nBlock size 128 should be fine, one could also try 64 and 32.\n\nThanks for posting the links here, they are the ones I used to code this up. (Does that count as homework?  )\nI did not put the links in the code because of possible link rot. The article titles and authors are in the javadocs.\nCurrently the easy way to find the links is via google scholar. ",
            "author": "Paul Elschot",
            "id": "comment-12636705"
        },
        {
            "date": "2008-10-03T19:09:00+0000",
            "content": "New TestPFor2 attached.  I changed vInt -> int in the prints and now\ncompute \"best\" bit size per-block using getNumFrameBits() and use that\nper block.\n\nI took a big frq file and computed %tg for each #bits:\n\n\n\n\nbits\ncount\npct\n\n\n1\n0\n0.0\n\n\n2\n24328\n0.9\n\n\n3\n94689\n3.7\n\n\n4\n213887\n8.4\n\n\n5\n277510\n10.8\n\n\n6\n284905\n11.1\n\n\n7\n193081\n7.5\n\n\n8\n262857\n10.3\n\n\n9\n260460\n10.2\n\n\n10\n212046\n8.3\n\n\n11\n162872\n6.4\n\n\n12\n109801\n4.3\n\n\n13\n77366\n3.0\n\n\n14\n56620\n2.2\n\n\n15\n34294\n1.3\n\n\n16\n31000\n1.2\n\n\n17\n28803\n1.1\n\n\n18\n21617\n0.8\n\n\n19\n22317\n0.9\n\n\n20\n30326\n1.2\n\n\n21\n162663\n6.4\n\n\n\n\n\nAnd for prx:\n\n\n\n\nbits\ncount\npct\n\n\n1\n23034\n0.7\n\n\n2\n12637\n0.4\n\n\n3\n49286\n1.4\n\n\n4\n56344\n1.6\n\n\n5\n69385\n2.0\n\n\n6\n81964\n2.4\n\n\n7\n108847\n3.1\n\n\n8\n179296\n5.2\n\n\n9\n428787\n12.4\n\n\n10\n873828\n25.2\n\n\n11\n957913\n27.7\n\n\n12\n534426\n15.4\n\n\n13\n81856\n2.4\n\n\n14\n2436\n0.1\n\n\n15\n474\n0.0\n\n\n16\n200\n0.0\n\n\n17\n43\n0.0\n\n\n18\n17\n0.0\n\n\n19\n1\n0.0\n\n\n\n\n\nIt's interesting that prx is so much more tightly clustered than frq.\n\nNew results for decoding vInt vs pfor, for frq file:\n\n\n327864576 ints; 431137397 bytes compressed vs orig size 447077047\n\ndecompress using pfor:\n2514 msec to decode 327864576 ints (130415 ints/msec)\n2171 msec to decode 327864576 ints (151020 ints/msec)\n2137 msec to decode 327864576 ints (153422 ints/msec)\n2148 msec to decode 327864576 ints (152637 ints/msec)\n2067 msec to decode 327864576 ints (158618 ints/msec)\n\ndecompress using readVInt:\n4549 msec to read 327864691 ints (72074 ints/msec)\n4421 msec to read 327864691 ints (74160 ints/msec)\n3240 msec to read 327864691 ints (101192 ints/msec)\n3199 msec to read 327864691 ints (102489 ints/msec)\n3323 msec to read 327864691 ints (98665 ints/msec)\n\nPFor is 54.766% faster\n\n\n\nand prx file:\n\n\nencode _l.prx to _l.prx.pfor...\n442979072 ints; 628580878 bytes compressed vs orig size 651670377\ndecompress using pfor:\n6385 msec to decode 442979072 ints (69378 ints/msec)\n5904 msec to decode 442979072 ints (75030 ints/msec)\n5796 msec to decode 442979072 ints (76428 ints/msec)\n5807 msec to decode 442979072 ints (76283 ints/msec)\n5803 msec to decode 442979072 ints (76336 ints/msec)\n\ndecompress using readVInt:\n6893 msec to read 442979104 ints (64265 ints/msec)\n6759 msec to read 442979104 ints (65539 ints/msec)\n5304 msec to read 442979104 ints (83517 ints/msec)\n5275 msec to read 442979104 ints (83977 ints/msec)\n5792 msec to read 442979104 ints (76481 ints/msec)\n\nPFor is 8.989% slower\n\n\n\nSome comments:\n\n\n\tIn both cases, the pfor file a bit smaller than the original\n    Lucene file.  And, it's unfair because I inject the 4 extra bytes\n    per block (which I think won't be needed \"for real\").\n\n\n\n\n\tFor frq file, pfor is quite a bit faster (54.8%) but for prx file\n    it's slower (9.0%).  Maybe it's because prx file has much higher\n    occurrence of bigger bit sizes (where we don't have unrolled\n    version)?  It's odd; maybe I'm doing something wrong.  With a\n    fixed 6 bits it is quite a bit faster (I retested to verify!)\n    which is also weird because there would be many exceptions.\n    Exception processing ought to be slower than non-exception\n    processing!\n\n\n\nStepping back a bit: I wonder what %tg of the time in a \"typical\"\nLucene search is spent decoding vInts?  That would bound how much\nimprovement we could ever expect from this optimization during\nsearching. ",
            "author": "Michael McCandless",
            "id": "comment-12636712"
        },
        {
            "date": "2008-10-03T19:18:53+0000",
            "content": "\nQ: We should look at the asm that's produced?\nA: Maybe. But would that be java bytecodes or x86 or powerpc? I'd prefer to wait with that.\nThere are some nice machine instrns to this on the various architectures, but that would require to use native code.\nI was thinking local CPU's native asm, so that we could at least see if the benefits of pfor (no hard-for-cpu-to-predict if statement inside inner loop) \"survive\" through the Java stack.  I'll try to look.\n\n\nQ: But can I also get the \"bytes consumed\" when I ask decompress() to run?\nA; Yes, but only after decompression or after compression.\nThe number of exceptions is not explicitly coded in the header, so the size to encode the exceptions is not known beforehand. That could be changed, but than the header will get bigger.\n\nOK so it is self-punctuating, so, we wouldn't need to encode block size in bytes into the file. ",
            "author": "Michael McCandless",
            "id": "comment-12636715"
        },
        {
            "date": "2008-10-03T20:23:30+0000",
            "content": "The number of bits is not really informative, it would be better to have a distribution of the result of getNumFrameBits(). Then we can see for which nrs of bits loop unrolling might be tried.\n\nAs for decompression speed, please remember that the patching code (that decodes the exceptions into the output) has not yet been optimized at all.\n\nThe lucene .frq file contains the docids as deltas and the frequencies but with a special encoding in case the frequency is one. I'd rather try compressing the real delta docids and the real frequencies than the encoded versions.\n\nThe .prx file should be useable as it is, and from the results reported in the articles I would expect a real performance advantage for PFor for that.\nMichael, could you post a distribution of the number of frame bits for the .prx file? I'd like to know a reasonable maximum for unrolling the corresponding loops.\n\n>: maybe I'm doing something wrong\nI don't think so, the code is still young. Try running TestPFor and have a look at the output near the end for the case of 6 frame bits. That should show the unrolled decoding speed for the case without exceptions. Then compare that to the cases with lower and higher nrs of frame bits.\n\n>: Stepping back a bit: I wonder what %tg of the time in a \"typical\"\nLucene search is spent decoding vInts? That would bound how much\nimprovement we could ever expect from this optimization during\nsearching.\n\nA: there is also the influence of the reduction of data to be fetched (via various caches) from the index. As reported in the articles, PFor strikes a nice optimum between decompression speed and fetching speed from (fast) disks.\n\n>: I was thinking local CPU's native asm.\nA: I'd try a C version first. Iirc gcc has a decent optimizer for bit ops, but it's been a while for me that I used C.\n\nFor the record, to show the variation in decompression speeds for different numbers of frame bits without exceptions, here is my current output from TestPFor:\n\ntest901PerfFor1Decompress starting\nCompressed 128 bytes into 8, ratio 0.0625\ntest901PerfFor1Decompress 0 numFrameBits 1 decompressed 104857600 in 1021 msecs, 102700 ints/msec, (25 iters).\ntest901PerfFor1Decompress 1 numFrameBits 1 decompressed 171966464 in 1020 msecs, 168594 ints/msec, (41 iters).\ntest901PerfFor1Decompress 2 numFrameBits 1 decompressed 171966464 in 1012 msecs, 169927 ints/msec, (41 iters).\n\ntest902PerfFor2Decompress starting\nCompressed 128 bytes into 12, ratio 0.09375\ntest902PerfFor2Decompress 0 numFrameBits 2 decompressed 130023424 in 1017 msecs, 127849 ints/msec, (31 iters).\ntest902PerfFor2Decompress 1 numFrameBits 2 decompressed 155189248 in 1000 msecs, 155189 ints/msec, (37 iters).\ntest902PerfFor2Decompress 2 numFrameBits 2 decompressed 159383552 in 1022 msecs, 155952 ints/msec, (38 iters).\n\ntest903PerfFor3Decompress starting\nCompressed 128 bytes into 16, ratio 0.125\ntest903PerfFor3Decompress 0 numFrameBits 3 decompressed 188743680 in 1016 msecs, 185771 ints/msec, (45 iters).\ntest903PerfFor3Decompress 1 numFrameBits 3 decompressed 205520896 in 1018 msecs, 201886 ints/msec, (49 iters).\ntest903PerfFor3Decompress 2 numFrameBits 3 decompressed 201326592 in 1026 msecs, 196224 ints/msec, (48 iters).\n\ntest904PerfFor4Decompress starting\nCompressed 128 bytes into 20, ratio 0.15625\ntest904PerfFor4Decompress 0 numFrameBits 4 decompressed 146800640 in 1014 msecs, 144773 ints/msec, (35 iters).\ntest904PerfFor4Decompress 1 numFrameBits 4 decompressed 159383552 in 1007 msecs, 158275 ints/msec, (38 iters).\ntest904PerfFor4Decompress 2 numFrameBits 4 decompressed 159383552 in 1011 msecs, 157649 ints/msec, (38 iters).\n\ntest905PerfFor5Decompress starting\nCompressed 128 bytes into 24, ratio 0.1875\ntest905PerfFor5Decompress 0 numFrameBits 5 decompressed 159383552 in 1010 msecs, 157805 ints/msec, (38 iters).\ntest905PerfFor5Decompress 1 numFrameBits 5 decompressed 176160768 in 1009 msecs, 174589 ints/msec, (42 iters).\ntest905PerfFor5Decompress 2 numFrameBits 5 decompressed 176160768 in 1004 msecs, 175458 ints/msec, (42 iters).\n\ntest906PerfFor6Decompress starting\nCompressed 128 bytes into 28, ratio 0.21875\ntest906PerfFor6Decompress 0 numFrameBits 6 decompressed 117440512 in 1001 msecs, 117323 ints/msec, (28 iters).\ntest906PerfFor6Decompress 1 numFrameBits 6 decompressed 125829120 in 1002 msecs, 125577 ints/msec, (30 iters).\ntest906PerfFor6Decompress 2 numFrameBits 6 decompressed 125829120 in 1004 msecs, 125327 ints/msec, (30 iters).\n\ntest907PerfFor7Decompress starting\nCompressed 128 bytes into 32, ratio 0.25\ntest907PerfFor7Decompress 0 numFrameBits 7 decompressed 125829120 in 1016 msecs, 123847 ints/msec, (30 iters).\ntest907PerfFor7Decompress 1 numFrameBits 7 decompressed 150994944 in 1015 msecs, 148763 ints/msec, (36 iters).\ntest907PerfFor7Decompress 2 numFrameBits 7 decompressed 150994944 in 1012 msecs, 149204 ints/msec, (36 iters).\n\ntest908PerfFor8Decompress starting\nCompressed 124 bytes into 35, ratio 0.28225806\ntest908PerfFor8Decompress 0 numFrameBits 8 decompressed 85327872 in 1015 msecs, 84066 ints/msec, (21 iters).\ntest908PerfFor8Decompress 1 numFrameBits 8 decompressed 121896960 in 1021 msecs, 119389 ints/msec, (30 iters).\ntest908PerfFor8Decompress 2 numFrameBits 8 decompressed 121896960 in 1022 msecs, 119272 ints/msec, (30 iters).\n\ntest909PerfFor9Decompress starting\nCompressed 124 bytes into 39, ratio 0.31451613\ntest909PerfFor9Decompress 0 numFrameBits 9 decompressed 52822016 in 1016 msecs, 51990 ints/msec, (13 iters).\ntest909PerfFor9Decompress 1 numFrameBits 9 decompressed 56885248 in 1028 msecs, 55335 ints/msec, (14 iters).\ntest909PerfFor9Decompress 2 numFrameBits 9 decompressed 56885248 in 1029 msecs, 55282 ints/msec, (14 iters).\n\n\nThe loop for 9 bits is not unrolled, unrolling makes it a tiny little bit (55->54 Mint/sec) slower. ",
            "author": "Paul Elschot",
            "id": "comment-12636730"
        },
        {
            "date": "2008-10-03T20:59:48+0000",
            "content": "I wrote: The number of bits is not really informative, it would be better to have a distribution of the result of getNumFrameBits(). Then we can see for which nrs of bits loop unrolling might be tried.\n\nBut I misread what was meant by the number of bits, actually the number of frame bits that I requested is already listed there, so I'll have a go at unrolling for larger numbers of frame bits, especially for the .prx data. ",
            "author": "Paul Elschot",
            "id": "comment-12636736"
        },
        {
            "date": "2008-10-04T09:32:25+0000",
            "content": "As for decompression speed, please remember that the patching code (that decodes the exceptions into the output) has not yet been optimized at all.\n\nBut this is what I find so weird: for prx, if I fix the encoding at 6 bits, which generates a zillion exceptions, we are ~31% faster than decoding vInts, and the pfor file size is much bigger (847 MB vs 621 MB) than vInt.  But if instead I use the bit size as returned by getNumFrameBits(), which has far fewer exceptions, we are 9.0% slower and file size is a bit smaller than vInt.  Exception processing seems to be very fast, or, it's the non-unrolled (ForDecompress.decodeAnyFrame) that's slow which could very well be.\n\nThe lucene .frq file contains the docids as deltas and the frequencies but with a special encoding in case the frequency is one. I'd rather try compressing the real delta docids and the real frequencies than the encoded versions.\n\nI'll try that.  I bet if we had two separate streams (one for the docIDs and one for the corresponding freqs) we'd get even better pFor compression.  If we did that \"for real\" in Lucene that'd also make it fast to use a normally-indexed field for pure boolean searching (you wouldn't have to index 2 different fields as you do today to gain the performance at search time).  In fact, for AND queries on a normal Lucene index this should also result in faster searching since when searching for the common docIDs you at first don't need the freq data.\n\nMarvin has been doing some performance testing recently and seems to have concluded that keeping prx and frq as separate files (like Lucene does today but KinoSearch does not) gives better performance.  Pushing that that same trend further, I think it may very well make sense to separate docID and frq as well.\n\nA: there is also the influence of the reduction of data to be fetched (via various caches) from the index. As reported in the articles, PFor strikes a nice optimum between decompression speed and fetching speed from (fast) disks.\n\nTrue, but we are seeing just a bit of compression vs Lucene's current encoding.  I think splitting out frq from docID may show better compression.\n\n\n>: I was thinking local CPU's native asm.\nA: I'd try a C version first. Iirc gcc has a decent optimizer for bit ops, but it's been a while for me that I used C.\n\nWell... that would just make me depressed (if from Javaland the CPU\nlevel benefits of pFor don't really \"survive\", but from C-land they\ndo)   But yes I agree.\n\nFor the record, to show the variation in decompression speeds for different numbers of frame bits without exceptions, here is my current output from TestPFor:\n\nI have similar results (up to 7 bits \u2013 can you post your new TestPFor.java?).\n\nThe even-byte sizes (16, 24) have very sizable leads over the others. The 9-bit size is fantastically slow; it's insane that unrolling it isn't helping.  Seems like we will need to peek at asm to understand what's going on at the \"bare metal\" level.... ",
            "author": "Michael McCandless",
            "id": "comment-12636799"
        },
        {
            "date": "2008-10-04T10:14:43+0000",
            "content": "I'm working on the 10 right now, but I'm hitting another bug, it will take a while.\n\nThe performance of TestPFor2 should be better after running TestPFor for all bit sizes, this could be a good reason to combine them. \n\nThe correctness tests in TestPFor are not really exhaustive, so I'd like to have a simple test for correctness in TestPFor2, for example a running (long) sum of all decompressed ints that should equal before and after.\n\nTo avoid having to peek at the asm level, there is also the possibility to use a slightly higher number of frame bits when we know one number of bits will be slow to decompress. ",
            "author": "Paul Elschot",
            "id": "comment-12636803"
        },
        {
            "date": "2008-10-05T14:56:30+0000",
            "content": "\nAttached new TestPfor2.java:\n\n\n\tIf you run it on _x.frq, it will split that into _x.frq.frq and\n    _x.frq.doc so we can separately test frq vs docs\n\n\n\n\n\tAdded checksum.\n\n\n\n\n\tAdded zeroing of reused IntBuffer before calling PFor.compress\n    (seems to be necessary?  Else I trip an assert inside PFor).\n\n\n\nPaul, indeed I get a different checksum for vint vs pfor decoding.\n\n I think the bug is somewhere in pfor, I'm guessing in the exception\nlogic, because the difference I see is suddenly pfor returns 0 when it\nshould have returned a large int relative to the other ints nearby.\n\nMaybe this is why exception processing looked so much faster \n\nI'll hold off posting more perf results until we can resolve that.\n\nTo see the checksum run it with asserts, eg like this:\n\n  java -ea oal.util.pfor.TestPFor2 /path/to/index _x.prx\n\nIt then prints out SUM lines after each iteration.\n\nIf you set DEBUG = true, it'll print the first 1000 values and then\nsearch for \"v=0\". ",
            "author": "Michael McCandless",
            "id": "comment-12636903"
        },
        {
            "date": "2008-10-06T15:54:26+0000",
            "content": "\n(Attached autogen.tgz).\n\nI started peeking at the ASM generated by different tweaks to the\nbit decoder methods.\n\nIt's rather depressing: small things, like using relative not absolute\ngetInt from ByteBuffer, declaring things final, reading into array\nfirst instead of getInt per int, make sizable differences in the\nresulting asm and decode speed.\n\nTo try out these various changes to the Java sources, I created a\nPython script to generate the 31 different decode methods.  I then\nmade a new standalone perf test that reads in a prx file as a series\nof packed N-bit ints, and reports the best of 5 runs.\n\nThese results are NOT testing the full pfor \u2013 just different possible\nmethods for doing the N-bit int unpacking part of pfor.  Paul, I\nhaven't integrated these into the pfor code, but I think we probably\neventually should.\n\nFinally, I switched the autogen to C++/gcc to see how much faster\nsimple C code is.\n\nIn both the java and C tests, by far the fastest way was to mmap the\nfile and read ints from it, sequentially (relative gets) using\nByteBuffer, so all 3 take that approach.\n\n(To run these, extract autogen.tgz, then open *.py and see the comment\nat the top; you'll have to edit the sources to fix the hardwired path\nto the prx file).\n\nJava1 code calls getInt() one at a time, like this:\n\n\n  static void decode4(final ByteBuffer in, final int[] out) {\n    final int i1 = in.getInt();\n    out[0] = i1 & 15;\n    out[1] = (i1 >>> 4) & 15;\n    out[2] = (i1 >>> 8) & 15;\n    out[3] = (i1 >>> 12) & 15;\n    out[4] = (i1 >>> 16) & 15;\n    out[5] = (i1 >>> 20) & 15;\n    out[6] = (i1 >>> 24) & 15;\n    out[7] = (i1 >>> 28) & 15;\n    final int i2 = in.getInt();\n    out[8] = i2 & 15;\n    out[9] = (i2 >>> 4) & 15;\n    out[10] = (i2 >>> 8) & 15;\n    ...\n  }\n\n\n\nJava 2 code gets all N ints up front, like this:\n\n\n  static void decode4(IntBuffer in, int[] inbuf, int[] out) {\n    in.get(inbuf, 0, 16);\n    out[0] = inbuf[0] & 15;\n    out[1] = (inbuf[0] >>> 4) & 15;\n    out[2] = (inbuf[0] >>> 8) & 15;\n    out[3] = (inbuf[0] >>> 12) & 15;\n    out[4] = (inbuf[0] >>> 16) & 15;\n    out[5] = (inbuf[0] >>> 20) & 15;\n    out[6] = (inbuf[0] >>> 24) & 15;\n    out[7] = (inbuf[0] >>> 28) & 15;\n    out[8] = inbuf[1] & 15;\n    out[9] = (inbuf[1] >>> 4) & 15;\n    out[10] = (inbuf[1] >>> 8) & 15;\n    ...\n  }\n\n\n\nC++ code is analogous to Java 1 (data is mmap'd):\n\n\nstatic bool decode4(int* out) {\n  int i1 = *(data++);\n  *(out++) = i1 & 15;\n  *(out++) = (i1 >> 4) & 15;\n  *(out++) = (i1 >> 8) & 15;\n  *(out++) = (i1 >> 12) & 15;\n  *(out++) = (i1 >> 16) & 15;\n  *(out++) = (i1 >> 20) & 15;\n  *(out++) = (i1 >> 24) & 15;\n  *(out++) = (i1 >> 28) & 15;\n  int i2 = *(data++);\n  *(out++) = i2 & 15;\n  ...\n}\n\n\n\nHere's the performance for each bit size:\n\n\n\n\nbits\nJava 1 (kints/msec)\nJava 2 (kints/msec)\nC++ (kints/msec)\nC advantage\n\n\n1\n916.6\n648.5\n1445.3\n1.6x\n\n\n2\n793.8\n593.4\n1118.3\n1.4x\n\n\n3\n616.7\n541.9\n1068.8\n1.7x\n\n\n4\n656.6\n512.1\n1161.6\n1.8x\n\n\n5\n499.8\n469.0\n897.3\n1.8x\n\n\n6\n410.6\n444.9\n899.5\n2.0x\n\n\n7\n367.4\n409.0\n801.7\n2.0x\n\n\n8\n414.0\n386.7\n816.6\n2.0x\n\n\n9\n306.3\n366.9\n710.8\n1.9x\n\n\n10\n278.8\n341.9\n665.8\n1.9x\n\n\n11\n258.1\n307.8\n623.6\n2.0x\n\n\n12\n245.9\n311.7\n592.7\n1.9x\n\n\n13\n223.9\n285.0\n574.5\n2.0x\n\n\n14\n204.2\n271.8\n538.1\n2.0x\n\n\n15\n190.3\n260.0\n522.6\n2.0x\n\n\n16\n229.9\n250.4\n519.7\n2.1x\n\n\n17\n190.8\n223.7\n488.3\n2.2x\n\n\n18\n171.6\n198.1\n461.2\n2.3x\n\n\n19\n158.3\n180.5\n436.2\n2.4x\n\n\n20\n163.1\n207.4\n416.4\n2.0x\n\n\n21\n156.3\n166.3\n403.0\n2.4x\n\n\n22\n147.0\n163.5\n387.8\n2.4x\n\n\n23\n141.6\n174.1\n357.5\n2.1x\n\n\n24\n141.9\n162.0\n362.6\n2.2x\n\n\n25\n133.2\n177.6\n335.3\n1.9x\n\n\n26\n125.8\n153.5\n334.7\n2.2x\n\n\n27\n121.6\n139.6\n314.0\n2.2x\n\n\n28\n122.7\n130.1\n316.7\n2.4x\n\n\n29\n107.3\n123.9\n296.7\n2.4x\n\n\n30\n111.0\n127.6\n300.7\n2.4x\n\n\n31\n108.1\n94.0\n290.5\n2.7x\n\n\n\n\n\nC code is between 1.4-2.7 X faster than the best Java run.  Reading\none int at a time is faster when the #bits is small <=5), but then\nreading all N ints up front is general faster for larger bit sizes. ",
            "author": "Michael McCandless",
            "id": "comment-12637119"
        },
        {
            "date": "2008-10-06T17:42:13+0000",
            "content": "I can't see the .tgz attachment at the moment.\n\nVery nice table, this java/c++ comparison. Meanwhile I also have a java program generator for decompression, and I got very similar results.\n\nI found a bug for the 17 bits case in decodeAnyFrame(), this might explain the sum test failure that you saw.\n\nOne detail on the generated code: the final mask can be omitted, for example the last bit extraction for the 5 bit case:\n\n (i4 >>> 27)\n\n\nOn the java side leaving this last mask out did not make a difference, but it might be noticeable in c++. ",
            "author": "Paul Elschot",
            "id": "comment-12637149"
        },
        {
            "date": "2008-10-06T17:46:52+0000",
            "content": "Woops I forgot the attachment... ",
            "author": "Michael McCandless",
            "id": "comment-12637151"
        },
        {
            "date": "2008-10-06T17:57:05+0000",
            "content": "One detail on the generated code: the final mask can be omitted, for example the last bit extraction for the 5 bit case:\n\nAhh good catch.  I'll fix in my autogen.\n\nMeanwhile I also have a java program generator for decompression, and I got very similar results.\n\nExcellent!  Did you also switch to relative getInts?  This way, I could change my TestPFor2 to rely on self-punctuation when reading.  And, I can pass down a ByteBuffer derived directly from the file, instead of copying bytes into byte[] first.  This would make the test more fair to pfor.\n\nI found a bug for the 17 bits case in decodeAnyFrame(), this might explain the sum test failure that you saw.\n\nOK I can re-test when you post this, to see if the checksums match. ",
            "author": "Michael McCandless",
            "id": "comment-12637154"
        },
        {
            "date": "2008-10-06T18:15:35+0000",
            "content": "Did you also switch to relative getInts?\n\nNo. I thought about that, but I wanted to make it work correctly first.\n\nThis way, I could change my TestPFor2 to rely on self-punctuation when reading. And, I can pass down a ByteBuffer derived directly from the file, instead of copying bytes into byte[] first.\n\nThe only concern I have there is that when using a byte[] directly from the file getting the int values may result in non 4 byte aligned fetches. Can current hardware do this well?\n\nIt's tempting to move to a full C implementation directly now. Should we do that?\nA similar move was made in the past by letting gcc deal with vInts, but meanwhile the jvms caught up there. ",
            "author": "Paul Elschot",
            "id": "comment-12637163"
        },
        {
            "date": "2008-10-06T19:30:36+0000",
            "content": "\nThe only concern I have there is that when using a byte[] directly from the file getting the int values may result in non 4 byte aligned fetches. Can current hardware do this well?\n\nGood question, though if that's the case we could presumably work\naround it by ensuring the header of the file is 0 mod 4?\n\nOr... is this because a full block (header + bits + exception data)\nmay not be 0 mod 4?  (Though we too could pad if necessary)\n\nI think the API we want to reach (eventually) is an IntBlockInput in\nDirectory where you call read(int[]) and it returns the next 128 (say)\nints in an array and moves itself to the end of that block (start of\nthe next block).\n\n\nIt's tempting to move to a full C implementation directly now. Should we do that?\nA similar move was made in the past by letting gcc deal with vInts, but meanwhile the jvms caught up there.\n\nMaybe we should explore this eventually, but I think for now we should\nfirst try to get the Java version online?\n\nI'd really love to get a prototype integration working so that we can\nthen do an end-to-end performance test (ie, actual searches).  I'm still\nwondering how much speedups at this layer will actually affect overall\nsearch time. ",
            "author": "Michael McCandless",
            "id": "comment-12637191"
        },
        {
            "date": "2008-10-06T22:10:24+0000",
            "content": "Or... is this because a full block (header + bits + exception data)  may not be 0 mod 4? (Though we too could pad if necessary)\n\nThe exceptions can be 1, 2 or 4 bytes and there is one more coding possible, we might reserve that for long.\nI'd like to use some padding space to get the exceptions aligned to their natural border, 2 bytes exceptions at even byte position, and 4 bytes exceptions at  (... mod 4 == 0) natural int border. That way the rest of the padding space (if any) could be used to align to natural int border.\n\nI'd really love to get a prototype integration working so that we can then do an end-to-end performance test (ie, actual searches). I'm still wondering how much speedups at this layer will actually affect overall search time.\n\nThe decoding speeds reported here so far are about the same as  the ones reported by Zhang in 2008. That means there is room for disk speeds to catch up with CPU speed. In other words, adding this for proximities (and maybe docids and freqs) should make lucene a natural fit for ssd's, see also ZhangfFig. 15. ",
            "author": "Paul Elschot",
            "id": "comment-12637272"
        },
        {
            "date": "2008-10-07T09:27:38+0000",
            "content": "That means there is quite a bit of room for disk speeds to catch up with CPU speed.\n\nExactly!\n\nSearch optimization is tricky because for an index that can't fit\nentirely in the OS's IO cache, reducing the CPU cost of searching\n(which we are diong, here) is basically usless: the end user won't see\nmuch benefit.\n\nFor an index entirely in the IO cache, I think these optimizations\nmight make a big difference.\n\nIn some sense, we have been allowed to hide behind the slow\nperformance of magnetic hard drives and not worry much about reducing\nthe CPU cost of searching.\n\nHowever: relatively soon most computers will use SSDs, and then\nsuddenly it's as if every index is in the IO cache (albeit a somewhat\nslower one, but still far faster than magnetic media).  So now is\nthe time for us to reduce the cpu cost of searching for Lucene.\n\nAnd this means for this issue and other sources of optimizing search\nperformance, we should largely focus only on indices entirely cached\nin the IO cache. ",
            "author": "Michael McCandless",
            "id": "comment-12637427"
        },
        {
            "date": "2008-10-09T14:52:07+0000",
            "content": "The 1410c patch is much like the 1410b. It adds some bug fixes and code cleanups and it includes a generator for java decompression classes. After applying the patch, run the generator once in src/java/org/apache/lucene/util/pfor :\n\npython gendecompress.py\n\nAfter that, in the trunk directory:\n\nant -Dtestcase=TestPFor test-core\n\nshould finish successfully in just over a minute, as it also includes decompression performance tests.\n\nTo be done:\n\n\tpossibly split the performance tests from the functional tests, this might require adding an ant target for performance tests,\n\tpossibly add byte padding to move exceptions to natural position,\n\tadd performance tests for decompression with exception patching,\n\tswitch decompression to relative buffer addressing (get() instead of get(index)),\n\toptimize the exception decoding (i.e. patching) code,\n\toptimize the compression code,\n\tgenerate/correct the javadocs.\n\n ",
            "author": "Paul Elschot",
            "id": "comment-12638295"
        },
        {
            "date": "2008-10-10T12:33:29+0000",
            "content": "Paul, in decompress I added \"inputSize = -1\" at the top, so that the header is re-read.  I need this so I can re-use a single PFor instance during decompress. ",
            "author": "Michael McCandless",
            "id": "comment-12638543"
        },
        {
            "date": "2008-10-10T14:18:43+0000",
            "content": "Did you also move to relative addressing in the buffer?\n\nAnother question: I suppose the place to add this initially would be in IndexOutput and IndexInput?\nIn that case it would make sense to reserve (some bits of) the first byte in the compressed buffer\nfor the compression method, and use these bits there to call PFor or another (de)compression method. ",
            "author": "Paul Elschot",
            "id": "comment-12638564"
        },
        {
            "date": "2008-10-10T14:56:02+0000",
            "content": "\nAnother thing that bit me was the bufferByteSize(): if this returns\nsomething that's not 0 mod 4, you must increase it to the next\nmultiple of 4 otherwise you will lose data since ByteBuffer is big\nendian by default.  We should test little endian to see if performance\nchanges (on different CPUs).\n\nDid you also move to relative addressing in the buffer? \n\nNo I haven't done that, but I think we should.  I believe it's faster.  I'm trying now to get a rudimentary test working for TermQuery using pfor.\n\n\nAnother question: I suppose the place to add this initially would be in IndexOutput and IndexInput?\nIn that case it would make sense to reserve (some bits of) the first byte in the compressed buffer\nfor the compression method, and use these bits there to call PFor or another (de)compression method.\n\nThis gets into flexible indexing...\n\nIdeally we do this in a pluggable way, so that PFor is just one such\nplugin, simple vInts is another, etc.\n\nI could see a compression layer living \"above\" IndexInput/Output,\nsince logically how you encode an int block into bytes is independent\nfrom the means of storage.\n\nBut: such an abstraction may hurt performance too much since during\nread it would entail an extra buffer copy.  So maybe we should just\nadd methods to IndexInput/Output, or, make a new\nIntBlockInput/Output.\n\nAlso, some things you now store in the header of each block should\npresumably move to the start of the file instead (eg the compression\nmethod), or if we move to a separate \"schema\" file that can record\nwhich compressor was used per file, we'd put this there.\n\nSo I'm not yet exactly sure how we should tie this in \"for real\"... ",
            "author": "Michael McCandless",
            "id": "comment-12638573"
        },
        {
            "date": "2008-10-11T12:09:40+0000",
            "content": "I'm trying now to get a rudimentary test working for TermQuery using pfor.\n\nIt should really make a difference for stop words and disjunction queries depending on DocIdSetIterator.next().\n\nConjunctions that depend on skipTo(docNum) will probably make it necessary to impose an upperbound the size of the compressed arrays. This upperbound could be the same as the skip distance in the index.\n\nI'm wondering whether it would make sense to add skip info to the term positions of very large documents. Any ideas on that? ",
            "author": "Paul Elschot",
            "id": "comment-12638744"
        },
        {
            "date": "2008-10-11T19:16:25+0000",
            "content": "The strange behaviour at 9 bits I reported earlier was due to a mistake in the test data. It contained only 31 integers (124 bytes, see posted listing), so the unrolled loop would never be called.\n\nAnd another point to be done:\n\n\tchange the decompression code generator to use an array get() from the buffer for 6 or more frame bits (see the Java 1/2 performance number numbers listed above.)\n\n ",
            "author": "Paul Elschot",
            "id": "comment-12638788"
        },
        {
            "date": "2008-10-12T19:48:47+0000",
            "content": "It should really make a difference for stop words and disjunction queries depending on DocIdSetIterator.next().\n\nYes.\n\nConjunctions that depend on skipTo(docNum) will probably make it necessary to impose an upperbound the size of the compressed arrays.\n\nYes.  Though, I think when optimizing search performance we should\nfocus entirely on the high-latency queries.  TermQuery on very\nfrequent terms, disjunctions queries involving common terms,\nphrase/span queries that have many matches, etc.\n\nEG if PFOR speeds up high-latency queries say by 20% (say 10 sec -> 8\nsec), but causes queries that are already fast (say 30 msec) to get a\nbit slower (say 40 msec) I think that's fine.  It's the high-latency\nqueries that kill us because those ones limit how large a collection\nyou can put on one box before you're forced to shard your index.\n\nAt some point we should make use of concurrency when iterating over\nlarge result sets.  EG if estimated # total hits is > X docs, use\nmultiple threads where each threads skips to it's own \"chunk\" and\niterates over it, and then merge the results.  Then we should be able\nto cut down on the max latency query and handle more documents on a\nsingle machine.  Computers are very quickly become very concurrent.\n\nI'm wondering whether it would make sense to add skip info to the term positions of very large documents. Any ideas on that?\n\nProbably we should \u2013 yet another issue  ",
            "author": "Michael McCandless",
            "id": "comment-12638888"
        },
        {
            "date": "2008-10-12T20:01:01+0000",
            "content": "In order to understand how time is spent overall during searching, I\ntook TermQuery and reimplemented it in several different ways.\n\nWhile each implementatation is correct (checksum of final top N docs\nmatches) they are very much prototypes and nowhere near committable.\n\nI then took the 100 most frequent terms (total 23.3 million hits) from\nmy Wikipedia index and ran them each in sequence.  Each result is best\nof 25 runs (all results are from the OS's IO cache):\n\n\n\n\nTest\nTime (msec)\nHits/msec\nSpeedup\n\n\nBaseline\n674\n34496\n1.00X\n\n\n+ Code Speedups\n591\n39340\n1.14X\n\n\n+ Code Speedups + PFOR\n295\n78814\n2.28X\n\n\n+ Code Speedups + BITS\n247\n94130\n2.73X\n\n\n+ Code Speedups + BITS (native)\n230\n101088\n2.93X\n\n\n\n\n\nHere's what the test names mean:\n\n\n\tBaseline is the normal TermQuery, searching with TopDocsCollector\n    for top 10 docs.\n\n\n\n\n\tCode Speedups means some basic optimizations, eg made my own\n    specialized priority queue, unrolled loops, etc.\n\n\n\n\n\tPFOR means switching to PFOR for storing docs & freqs as separate\n    streams.  Each term's posting starts a new PFOR block.\n\n\n\n\n\tBITS just means using packed n-bit ints for each block (ie, it has\n    no exceptions, so it sets N so that all ints will fit).  The\n    resulting frq file was 18% bigger and doc file was 10% bigger \u2013\n    but this is just for the 100 most frequent terms.\n\n\n\n\n\tBITS (native) is BITS but running as JNI (C++) code.\n\n\n\nNext, I tried running the same things above, but I turned off\ncollection of hits.  So this really just tests decode time:\n\n\n\n\nTest\nTime (msec)\nHits/msec\nSpeedup\n\n\n+ Code Speedups\n384\n60547\n1.76X\n\n\n+ Code Speedups + PFOR\n91\n255497\n7.41X\n\n\n+ Code Speedups + BITS\n49\n474496\n13.76X\n\n\n+ Code Speedups + BITS (native)\n32\n726572\n21.06X\n\n\n\n\n\nSome observations:\n\n\n\tPFOR really does speed up TermQuery overall, so, I think we should\n    pursue it and get it committed.\n\n\n\n\n\tBITS is a good speedup beyond PFOR, but we haven't optimized PFOR\n    yet.  Also, BITS would be very seekable.  We could also use PFOR\n    but increase the bit size of blocks, to get the same thing.\n\n\n\n\n\tOnce we swap in PFOR and/or BITS or other interesting int block\n    compression, accumulating hits becomes the slowest part of\n    TermQuery.\n\n\n\n\n\tNative BITS decoding code is quite a bit faster for decoding, but,\n    probably not worth pursuing for now since with PFOR decode time\n    becomes a small part of the overall time.\n\n\n\n\n\tTermQuery is the simplest query; other queries will spend more\n    CPU time coordinating, or time handling positions, so we can't\n    yet conclude how much of an impact PFOR will have on them.\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12638895"
        },
        {
            "date": "2008-10-12T20:58:17+0000",
            "content": "So there's roughly a factor 2 between PFOR and BITS (which is actually just FOR i.e. PFOR without patches/exceptions). Meanwhile I've started working on speeding up the patching,  combined with always using a multiple of 4 bytes. This also involves changing the layout for the exceptions slightly, but it should allow faster patching and avoid the byte ordering problems mentioned before. It will take another while to finish. ",
            "author": "Paul Elschot",
            "id": "comment-12638900"
        },
        {
            "date": "2008-10-13T21:16:42+0000",
            "content": "1410d patch: as 1410c, with the following further changes:\n\n\tmoved exceptions to their natural borders,\n\tunrolled exception patching code,\n\tsome more testcases for exceptions,\n\ta specialized version for \"decompressing\" with 32 frame bits,\n\tall other frame decompression is left to generated code (not contained in the patch),\n\tcompressed and decompressed sizes are now reported in numbers of integers,\n\talso contains an adapted version of TestPFor2.\n\n\n\nThis should diminish the difference in performance between PFOR and BITS. ",
            "author": "Paul Elschot",
            "id": "comment-12639195"
        },
        {
            "date": "2008-11-08T10:37:38+0000",
            "content": "I've been at this quite irregularly. I'm trying to give the PFor class a more OO interface and to get exception patching working at more decent speeds. In case someone else wants to move this forward faster than it is moving now, please holler.\n\nAfter rereading this, and also after reading up a bit on MonetDb performance improvement techniques, I have few more rants:\n\nTaking another look at the decompression performance figures, and especially the differences between native C++ and java, it could become worthwhile to also implement TermQuery in native code.\n\nWith the high decompression speeds of FOR/BITS at lower numbers of frame bits it might also become worthwhile to compress character data, for example numbers with a low number of different characters.\nAdding a dictionary as in PDICT might help compression even further.\nThis was probably one of the reasons for the column storage discussed earlier, I'm now sorry I ignored that discussion.\nIn the index itself, column storage is also useful. One example is the splitting of document numbers and frequency into separate streams, another example is various offsets for seeking in the index.\n\nI think it would be worthwhile to add a compressed integer array to the basic types used in IndexInput and IndexOutput. I'm still strugling with the addition of skip info into a tree of such compressed integer arrays (skip offsets\ndon't seem to fit naturally into a column, and I don't know whether the skip size should be the same as the decompressed array size).\nPlacement of such compressed arrays in the index should also be aware of CPU cache lines and of VM page (disk block) boundaries.\nIn higher levels of a tree of such compressed arrays, frame exceptions would be best avoided to allow direct addressing, but the leafs could use frame exceptions for better compression.\n\nFor terms that will occur at most once in one document more compression is possible, so it might be worthwhile to add these as a key. At the moment I have no idea how to enforce the restriction of at most once though.\n ",
            "author": "Paul Elschot",
            "id": "comment-12645973"
        },
        {
            "date": "2008-12-16T13:41:47+0000",
            "content": "I'd like to split off the performance test cases from the functional test cases for this, but there does not seem to be a nice way to do that using the current test methods via ant.\n\nI'm thinking of using a special class name prefix like PerfTest... (or maybe something shorter like Perf...) and adding a test target for that in the build scripts.\n\nAre there other places where splitting performance tests and functional tests could be useful?\nWhen so, what would be a good way to implement it? ",
            "author": "Paul Elschot",
            "id": "comment-12656988"
        },
        {
            "date": "2008-12-16T15:49:31+0000",
            "content": "The 1410e patch splits off a FrameOfRef class as the superclass from the PFor class. Test cases are split similarly, all added test cases pass.\nThis should make it easier to chose between the patching version PFor and the non patching version FrameOfRef.\n\nThere lots of small changes compared to the 1410d patch, mostly in the o.a.l.util.pfor package, and a few in the util package.\nSome javadocs are added, but javadoc generation is not yet tested.\n\nAfter applying the patch, run gendecompress.py as for the 1410b patch to generate the java sources for decompression. Then:\nant -Dtestcase=TestFrameOfRef test-core\nand\nant -Dtestcase=TestPFor test-core\nshould pass.\n\nThe following (at least) is still to be done:\n\n\tpeformance tests for patching decompression,\n\trelative compressed data buffer addressing,\n\tgenerate code for compression in the same way as for decompression,\n\ta few fixme's and checkme's in the code,\n\tjavadoc generation.\n\n ",
            "author": "Paul Elschot",
            "id": "comment-12657029"
        },
        {
            "date": "2009-03-23T13:18:01+0000",
            "content": "It looks like Google went there as well (Block encoding), \n\nsee: Blog http://blogs.sun.com/searchguy/entry/google_s_postings_format\nhttp://research.google.com/people/jeff/WSDM09-keynote.pdf (Slides 47-63)\n ",
            "author": "Eks Dev",
            "id": "comment-12688284"
        },
        {
            "date": "2009-03-23T20:54:56+0000",
            "content": "The encoding in the google research slides is another one.\nThey use 2 bits prefixing the first byte and indicating the number of bytes used for the encoded number (1-4), and then they group 4 of those prefixes together to get a single byte of 4 prefixes followed by the non prefixed bytes of the 4 encoded numbers.\nThis requires a 256 way switch (indexed jump) for every 4 encoded numbers, and I would expect that jump to limit performance somewhat when compared to pfor that has a 32 way switch for 32/64/128 encoded numbers.\nBut since the prefixes only indicate the numbers of bytes used for the encoded numbers, no shifts and masks are needed, only byte moves.\nSo it could well be wortwhile to give this encoding a try, too, especially for lists of numbers shorter than 16 or 32. ",
            "author": "Paul Elschot",
            "id": "comment-12688409"
        },
        {
            "date": "2009-05-12T19:58:02+0000",
            "content": "A very recent paper with some improvements to PFOR:\nYan, Ding, Suel,\nInverted Index Compression and Query Processing with Optimized Document Ordering,\nWWW 2009, April 20-24 2009, Madrid, Spain\n\nRoughly quoting par. 4.2, Optimizing PForDelta compression:\nFor an exception, we store its lower b bits instead of the offset to the next exception in its corresponding slot, while we store the higher overflow bits and the offset in two separate arrays. These two arrays are compressed using the Simple16 method.\nAlso b is chosen to optimize decompression speed. This makes the dependence of b on the data quite simple, (in the PFOR above here this dependence is more complex) and this improves compression speed.\n\nBtw. the document ordering there is by URL. For many terms this gives more shorter delta's between doc ids allowing a higher decompression speed of the doc ids. ",
            "author": "Paul Elschot",
            "id": "comment-12708590"
        },
        {
            "date": "2009-10-06T16:22:10+0000",
            "content": "Attaching sep, intblock and pfordelta codecs, spun out of the last patch on LUCENE-1458.\n\nOnce LUCENE-1458 is in, we should finish the pfordelta codec to make it a real choice.\n\nI actually think some combination of pulsing, standard, pfordelta and simple bit packing (in order by increasing term's docFreq), within a single codec, may be best.\n\nIe, rare terms (only in a doc or two) could be inlined into the the terms dict.  Slightly more common terms can use the more CPU intensive standard codec.  Common terms can use cpu-friendly-yet-still-decent-compression pfordelta.  Obsenely common terms can use bit packing for the fastest decode. ",
            "author": "Michael McCandless",
            "id": "comment-12762674"
        },
        {
            "date": "2009-10-06T19:12:57+0000",
            "content": "Mike, \nThat is definitely the way to go, distribution dependent encoding, where every Term gets individual treatment.\n\nTake for an example simple, but not all that rare case where Index gets sorted on some of the indexed fields (we use it really extensively, e.g. presorted doc collection on user_rights/zip/city, all indexed). There you get perfectly \"compressible\"  postings by simply managing intervals of set bits. Updates distort this picture, but we rebuild index periodically and all gets good again.  At the moment we load them into RAM as Filters in IntervalSets. if that would be possible in lucene, we wouldn't bother with Filters (VInt decoding on such super dense fields was killing us, even in RAMDirectory) ...  \n\nThinking about your comments, isn't pulsing somewhat orthogonal to packing method? For example, if you load index into RAMDirecectory, one could avoid one indirection level and inline all postings.    \n\nFlex Indexing rocks, that is going to be the most important addition to lucene since it started (imo)... I would even bet on double search speed  in first attempt for average queries \n\nCheers, \neks  ",
            "author": "Eks Dev",
            "id": "comment-12762742"
        },
        {
            "date": "2010-01-04T07:46:27+0000",
            "content": "To encode the exceptions as suggested by Yan,Ding&Suel, Simple9 or Simple16 can be used. A Simple9 implementation is at LUCENE-2189. ",
            "author": "Paul Elschot",
            "id": "comment-12796107"
        },
        {
            "date": "2010-01-18T15:22:08+0000",
            "content": "Hi,\n\nI have performed some benchmarks using the PFOR index I/O interface in order to check if the index reader and block reader were not adding too much overhead (I was afraid that the block reading interface was adding too much overhead, and, as a consequence, loosing the decompression speed benefits of block based compression.\n\nIn this benchmark, I have jsut tested FOR (and not PFOR) using various block size. The benchmark is setup as follow:\n\n\tI generate an integer array of size 33554432, containing uniformly distributed integer (0 <= x <  65535)\n\tI compress the data using PFORDeltaIndexOutput (in fact, aa similar class that I modified for this benchmark in order to support various blocksize)\n\tI measure the time to decompress the full list of integers using PFORDeltaIndexInput#next()\n\n\n\nI have performed a similar test using a basic IndexInput/Output with VInt encoding. The performance was 39Kints/msec.\n\nFor FrameOfRef, the best block size seems to be 4096 (256 - 270 kints / msec), larger block size do not provide significant performance improvement,\n\nWe can observe that FOR provides ~7 times read performance increase. To conclude, it looks like the reader and block reader interface do not add to much overhead ;o).\n\nP.S.: It is possible that these results are not totally correct. I'll try to double check the code, and upload it here. \n\nResults:\n\nBlockSize = 32\nFrameOfRef 0 decompressed 33554432 in 368 msecs, 91 kints/msec, (1 iters).\nFrameOfRef 1 decompressed 33554432 in 314 msecs, 106 kints/msec, (1 iters).\nFrameOfRef 2 decompressed 33554432 in 294 msecs, 114 kints/msec, (1 iters).\nBlockSize = 64\nFrameOfRef 0 decompressed 33554432 in 242 msecs, 138 kints/msec, (1 iters).\nFrameOfRef 1 decompressed 33554432 in 239 msecs, 140 kints/msec, (1 iters).\nFrameOfRef 2 decompressed 33554432 in 237 msecs, 141 kints/msec, (1 iters).\nBlockSize = 128\nFrameOfRef 0 decompressed 33554432 in 223 msecs, 150 kints/msec, (1 iters).\nFrameOfRef 1 decompressed 33554432 in 228 msecs, 147 kints/msec, (1 iters).\nFrameOfRef 2 decompressed 33554432 in 224 msecs, 149 kints/msec, (1 iters).\nBlockSize = 256\nFrameOfRef 0 decompressed 33554432 in 219 msecs, 153 kints/msec, (1 iters).\nFrameOfRef 1 decompressed 33554432 in 218 msecs, 153 kints/msec, (1 iters).\nFrameOfRef 2 decompressed 33554432 in 219 msecs, 153 kints/msec, (1 iters).\nBlockSize = 512\nFrameOfRef 0 decompressed 33554432 in 170 msecs, 197 kints/msec, (1 iters).\nFrameOfRef 1 decompressed 33554432 in 176 msecs, 190 kints/msec, (1 iters).\nFrameOfRef 2 decompressed 33554432 in 173 msecs, 193 kints/msec, (1 iters).\nBlockSize = 1024\nFrameOfRef 0 decompressed 33554432 in 136 msecs, 246 kints/msec, (1 iters).\nFrameOfRef 1 decompressed 33554432 in 139 msecs, 241 kints/msec, (1 iters).\nFrameOfRef 2 decompressed 33554432 in 147 msecs, 228 kints/msec, (1 iters).\nBlockSize = 2048\nFrameOfRef 0 decompressed 33554432 in 133 msecs, 252 kints/msec, (1 iters).\nFrameOfRef 1 decompressed 33554432 in 135 msecs, 248 kints/msec, (1 iters).\nFrameOfRef 2 decompressed 33554432 in 139 msecs, 241 kints/msec, (1 iters).\nBlockSize = 4096\nFrameOfRef 0 decompressed 33554432 in 124 msecs, 270 kints/msec, (1 iters).\nFrameOfRef 1 decompressed 33554432 in 131 msecs, 256 kints/msec, (1 iters).\nFrameOfRef 2 decompressed 33554432 in 131 msecs, 256 kints/msec, (1 iters).\nBlockSize = 8192\nFrameOfRef 0 decompressed 33554432 in 126 msecs, 266 kints/msec, (1 iters).\nFrameOfRef 1 decompressed 33554432 in 128 msecs, 262 kints/msec, (1 iters).\nFrameOfRef 2 decompressed 33554432 in 127 msecs, 264 kints/msec, (1 iters).\nBlockSize = 16384\nFrameOfRef 0 decompressed 33554432 in 127 msecs, 264 kints/msec, (1 iters).\nFrameOfRef 1 decompressed 33554432 in 125 msecs, 268 kints/msec, (1 iters).\nFrameOfRef 2 decompressed 33554432 in 129 msecs, 260 kints/msec, (1 iters).\nBlockSize = 32768\nFrameOfRef 0 decompressed 33554432 in 123 msecs, 272 kints/msec, (1 iters).\nFrameOfRef 1 decompressed 33554432 in 132 msecs, 254 kints/msec, (1 iters).\nFrameOfRef 2 decompressed 33554432 in 135 msecs, 248 kints/msec, (1 iters).\n\n\n\n\nEDIT: Here is new results comparing FOR and a block-based VInt using various block size. The decompression loop is repeated to reach > 300ms (for JIT effect, see post below). The loop recreates a new block reader each time (which causes some overhead, as you can see with the FOR performance results, compared to the one below).\n\n\n\n\n\u00a0\n32\n64\n128\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\n\n\nVInt (kints/msec)\n28\n30\n30\n31\n48\n65\n84\n94\n100\n104\n104\n\n\nFOR (kints/msec)\n104\n126\n131\n132\n164\n195\n202\n214\n217\n220\n223\n\n\n\n ",
            "author": "Renaud Delbru",
            "id": "comment-12801813"
        },
        {
            "date": "2010-01-18T19:14:26+0000",
            "content": "Did you see any effect of the JIT? This is possible by making a loop that runs for about 1 second, and use that loop 3 times, see the output posting of 3 Oct 2008. When the 2nd and 3rd time are just about equal one can assume that the JIT is done.\nPerhaps a shorter time than 1 second can be used nowadays.\n\nFor the blocks that you've been measuring one might also see an effect of L1/L2 data cache size.\nThis effect is probably not present in my previous postings here because I have not yet used this on larger blocks. ",
            "author": "Paul Elschot",
            "id": "comment-12801905"
        },
        {
            "date": "2010-01-18T19:56:34+0000",
            "content": "\nDid you see any effect of the JIT? This is possible by making a loop that runs for about 1 second, and use that loop 3 times, see the output posting of 3 Oct 2008. When the 2nd and 3rd time are just about equal one can assume that the JIT is done.\n\nI haven't seen the effect of the JIT on FOR.\nI have re-performed the benchmark, repeating the decompression until the test time is superior to 300 ms or 1s (as it is done in TestFrameOfRef#doDecompPerfTestUsing() method), but I haven't observed a difference with the previous benchmark. \nIn fact, it seems that it happens only for block size of 32, in the other cases, it seems imperceptible (in the previous benchmark, the number looks unstable, in the new benchmark however it looks more stable, but no significant increase between the first loop and the other ones).\n\nIn the first benchmark, I was not repeating the loop until 1 second is reached since there is no easy way to \"reset\" the reader. In the new benchmark, I am recreating the reader in the loop (which causes some overhead). Do you think it can have a consequence on the JIT ? \n\n\nFor the blocks that you've been measuring one might also see an effect of L1/L2 data cache size.\n\nYes, it should be an effect of the cache size. It was to check the increase of performance and find the optimal block size. This optimal block size may be dependent on my hardware (Intel(R) Core(TM)2 Duo CPU T7300  @ 2.00GHz, cache size : 4096 KB). ",
            "author": "Renaud Delbru",
            "id": "comment-12801918"
        },
        {
            "date": "2010-01-18T20:31:30+0000",
            "content": "\nIn the first benchmark, I was not repeating the loop until 1 second is reached since there is no easy way to \"reset\" the reader. In the new benchmark, I am recreating the reader in the loop (which causes some overhead). Do you think it can have a consequence on the JIT ?\nI don't know how wheather JIT deals with the code for invidual objects (other than classes and their code). There could be an indirect effect via the garbage collector. \n\nThe uniform random range of about 65000 generates almost always 14-16 bits per number, so these results are roughly comparable to the 14-16 bit cases posted on 6 October 2008 by Michael. As it happens they are quite close. ",
            "author": "Paul Elschot",
            "id": "comment-12801934"
        },
        {
            "date": "2010-01-19T13:05:15+0000",
            "content": "On another aspect, why the PFOR/FOR is encoding the number of compressed integers into the block header since this information is already stored in the stream header (block size information written in FixedIntBlockIndexOutput#init()). Is there a particular use case for that ? Is it for the special case when a block is complete (when the block encodes the reamaining integer of the list) ? ",
            "author": "Renaud Delbru",
            "id": "comment-12802235"
        },
        {
            "date": "2010-01-19T17:01:00+0000",
            "content": "The only reason why the number of compressed integers is encoded in the block header here is that when I coded it I did not know that this was not necessary in lucene indexes.\n\nThat also means that the header can be used for different compression methods, for example in the following way:\ncases encoded in 1st byte:\n32 FrameOfRef cases (#frameBits) followed by 3 bytes for #exceptions (0 for BITS, > 0 for PFOR)\n16-64 cases for a SimpleNN variant\n1-8 cases for run length encoding (for example followed by 3 bytes for length and value)\nTotal #cases is 49-104 or 6-7 bits.\n\nRun length encoding is good for terms that occur in every document and for the frequencies of primary keys.\n\nThe only concern I have is that the instruction cache might get filled up with the code for all these decoding cases.\nAt the moment I don't know how to deal with that other than by adding such cases slowly while doing performance tests all the time. ",
            "author": "Paul Elschot",
            "id": "comment-12802335"
        },
        {
            "date": "2010-01-21T12:20:06+0000",
            "content": "\nThe only concern I have is that the instruction cache might get filled up with the code for all these decoding cases.\nAt the moment I don't know how to deal with that other than by adding such cases slowly while doing performance tests all the time.\n\nI am not very familiar with such technologies, and it is new to me to start thinking of this kind of problems.\nHowever, from what I understand in the WSDM slides about Google Group Varint encoding, they are using a 256-entry table, which is much higher than the 49-107 cases you're proposing. From what I understand, the instruction cache will be filled with such table, so it seems you have still some margin compared to Group Varint encoding. Or is there other information that risks to be added to the instruction cache ?  ",
            "author": "Renaud Delbru",
            "id": "comment-12803272"
        },
        {
            "date": "2010-01-21T17:28:03+0000",
            "content": "There is a margin indeed, but it depends on the cache size. For the decoding of all 32 FrameOfRef cases, a rough estimation would be 32 cases * (av. 16 indexed reads, 32 shifts, 32 masks, 32 indexed writes) = 32 * 112 = 3500 instructions. With 3 bytes per instruction on avarage that would be about 10k byte, in a normal instruction cache of 16k or 32k byte.\nFor Simple9 there is a lot less code, (fewer cases, less code per case) but when a lot of variations on this are added, perhaps 3k would be needed in total.\nSo it's not only the number of cases, but also the amount of code per case.\nFortunately not all cases are used frequently, so they won't appear in the instruction cache.\n\nThis can be compared that to the code needed for VByte, which is just about nothing.\n\nDuring query execution there are other things in the instruction cache, too, for example the code of the Scorers used by the query and the code to get the data from the index.\nInitially the byte code interpreter and the JIT compiler of the JVM are also in the instruction cache, but they normally disappear when the JIT is finished. But with more code, the JIT will take longer to finish. ",
            "author": "Paul Elschot",
            "id": "comment-12803376"
        },
        {
            "date": "2010-01-23T19:14:39+0000",
            "content": "Zhang, 2008, see above, reports this:\n\nThe poor speed of variable- byte on position data is primarily due\nto the fact that position values are larger and more often require\n2 bytes under variable-byte; this case tends to be much slower due\nto a branch mispredict.\n\nTaking another look at the position data above (3 October 2008) 11.6% of prx values take 7 bits or less,\nand the rest fits in 15 bits. So why not encode the position data as VShort (1 bit as in VByte and 15 bits data) ?\nThat would enlarge a typical prx file by about 6% and increase position decoding speed a lot,\nprobably about 3 times (see Table 1 in the same paper). ",
            "author": "Paul Elschot",
            "id": "comment-12804127"
        },
        {
            "date": "2010-01-24T09:33:17+0000",
            "content": "So why not encode the position data as VShort (1 bit as in VByte and 15 bits data) ?\n\nThat sounds interesting... we should give it a shot and see what gains Lucene actually sees on real data! ",
            "author": "Michael McCandless",
            "id": "comment-12804209"
        },
        {
            "date": "2010-01-24T11:40:37+0000",
            "content": "I opened LUCENE-2232 to use VShort for positions. ",
            "author": "Paul Elschot",
            "id": "comment-12804227"
        },
        {
            "date": "2010-02-15T16:22:39+0000",
            "content": "When performing some tests on PFOR, I have noticed that your algorithm was favoring very small numFrameBits for a very large number of exceptions. For example, on block of size 512, I have noticed that many of the blocks was with bestFrameBits = 1, and the number of exceptions reaching > 450.\n\nI found that this was due to the seeting of the allowedNumExceptions variable (in the last part of the PFOR#frameBitsForCompression() method) which was set to the number of current exceptions + the maximum allowed (which at the end is generally extremely large).Is it a bug, or is it something I don't understand in the current PFOR algorithm ?\n\nP.S.: btw, the previous benchmark results I have posted are wrong due to a bug which was due to the hardcoded byte buffer size (1024) in PForIndexInput/Output. I'll post soon updated results, with a comparison with GroupVarInt (from WSDM09 - Jeff Dean talk). ",
            "author": "Renaud Delbru",
            "id": "comment-12833872"
        },
        {
            "date": "2010-02-15T17:29:16+0000",
            "content": "That might indeed be a bug.\nIn general the frameBitsForCompression() method should try and find the number that has the quickest decompression.\nThe trade off is between exceptions taking extra time, and a smaller number of frame bits taking less time.\nI have not timed the exception decoding yet, so I would not expect the current implementation of frameBitsForCompression() to be right on the spot.\n\nPlease note that I'd still like to change the exception encoding, see the comment of 12 May 2009:\n\"For an exception, we store its lower b bits instead of the offset to the next exception in its corresponding slot, while we store the higher overflow bits and the offset in two separate arrays. These two arrays are compressed using the Simple16 method.\" ",
            "author": "Paul Elschot",
            "id": "comment-12833895"
        },
        {
            "date": "2010-02-15T18:54:29+0000",
            "content": "\nIn general the frameBitsForCompression() method should try and find the number that has the quickest decompression.\n\nIn fact,  the method is trying to find the configuration that has the smaller size in term of bytes (with the test totalBytes <= bestBytes). But it seems that it is not always the best case for quick decompression. I haven't test the decompression speed of PFOR yet, but I imagine that with 89% of exceptions (while in the original algorithm exception should occurs only a few times), it should not be the best performance.\n\nWhy not just computing the index (in the sorted copy array) that will represent the percentage of values that should be packed, and the remaining of the array encoded as exceptions ? ",
            "author": "Renaud Delbru",
            "id": "comment-12833922"
        },
        {
            "date": "2010-02-15T20:07:13+0000",
            "content": "Why not just computing the index (in the sorted copy array) that will represent the percentage of values that should be packed, and the remaining of the array encoded as exceptions ?\n\nBecause there are cases in which no exceptions are needed, for example 90% of the values using the same max nr of bits.\n\nAnyway, the articles on PFor do not completely specify how this should be done, and they are based on C, not on Java.\nThat means there will be room for improvement. ",
            "author": "Paul Elschot",
            "id": "comment-12833952"
        },
        {
            "date": "2010-02-23T13:08:08+0000",
            "content": "I am reporting here some experiments I performed over the past weeks while I was trying to improve the FOR implementation. \nI re-implement the FOR algorithms by getting rid of the IntBuffer and working directly with the byte array. I have implemented multiple variants, such as one that directly performs transformation over bytes to create the uncompressed data (what I call byte-level in the next), and one that first convert bytes into integers, and then performs transformation on integers to create the uncompress data (what I call integer-level in the next). The last one is very similar to your original FOR implementation, but without the IntBuffer.\n\nI think these results can be of interest for you, especially to optimise certain cases (byte level manipulation for certain cases such as bit frame of 2, 4 or 8 seems more suitable). I have attached a file containing a summary of the results for space consideration. I can provide you the raw results, and the code if you would like to test it on your side.\nHowever, I get very different results if I perform the benchmarks on a 64 bits OS or 32 Bits OS (on a same computer, IBM T61, the only difference is that on one computer Ubuntu 9.10 32 bits is installed, on the other one it is Ubuntu 9.10 64 bits).\n\nI am a little bit puzzled by these results. I thought that removing the IntBuffer and working directly with the byte array will be faster (as I noticed in other compression algorithms, such as GroupVarInt). The IntBuffer you are currently using is a view on a byte buffer. It therefore does the conversion between byte to int, plus it does several checks (if the index is in the range of the buffer) and function calls.\nBut it seems that with FOR this does not make too much a difference on large integers (> 8 bits). Moreover, I observe a decrease of performance on 64 bits OS.\nMaybe, you have an idea about the difference in behavior. ",
            "author": "Renaud Delbru",
            "id": "comment-12837231"
        },
        {
            "date": "2010-02-23T13:11:02+0000",
            "content": "File containing a summary of the results for each bit frame, based on variants of FOR. ",
            "author": "Renaud Delbru",
            "id": "comment-12837232"
        },
        {
            "date": "2010-02-28T22:22:08+0000",
            "content": "I thought that removing the IntBuffer and working directly with the byte array will be faster ...\n\nWhen the int values are in processor byte order, a call to  IntBuffer.get() may be reduced by the JIT to a single hardware instruction. This is why the initial implementation uses IntBuffer.\nAlso, the index bound checks need only be done once for the first and last index used.\n\nI have no idea why a 64 bit OS would be slower than a 32 bit OS. ",
            "author": "Paul Elschot",
            "id": "comment-12839509"
        },
        {
            "date": "2010-04-02T16:16:00+0000",
            "content": "Here some results on the performance of compression-decompression of various algorithms over the wikipedia dataset (english articles).\n\nFirst, the performance on compressing/decompressing over the full .doc posting list using block size of 1024. \nThe Compression and Decompression are in KInt/ms and the size in bytes.\n\n\n\n\nCodec\nCompression\nDecompression\nSize\n\n\nFOR (Orig)\n20\n106\n448856\n\n\nPFOR (Orig)\n8\n107\n 383596\n\n\nVINT\n37\n74\n421764\n\n\nFOR (Opt)\n39\n102\n447369\n\n\nPFOR (Opt)\n10\n108\n382379\n\n\nRICE\n8\n31\n350687\n\n\nS9\n16\n65\n408218\n\n\n\n\n\n\n\tVInt is a block based version of variable integer (unlike the simple int block, I am creating blocks using vint, then read the full block in memory and decompress it using vint one integer at a time).\n\tFor (Orig) and PFOR (Orig) are your implementation of FOR and PFOR.\n\tFOR (Opt) and PFOR (Opt) are my implementation of FOR and PFOR (using array of functors for each compression decompression case).\n\tRice is one implementation of the rice algorithm, but block-based.\n\tS9 is your implementation of simple 9 with some bug fixes, but block-based.\n\n\n\nI have created a *IndexInput and *IndexOutput for each algorithm, and use them to compress and decompress the posting file in this experiment.\n\nWe can see that using an array of functors for compression in FOR provides a good speed increase in compression. However, on PFOR, the compression speed increase is limited. From my test, it is due to the sort step which is necessary for compressing each block. \nPFOR provides a good balance between decompression speed and size, but it is very slow in compression (it is as slow as Rice). I don't think it is possible to improve the compression speed due to the sort step, which seems to impose a hard upper limit in term of performance (I have tried various heuristics to avoid sorting, but not very successful so far).\n\nFollowing this first benchmark, I have created various Lucene codec that uses the *IndexInput and *IndexOutput of the different compression algorithms. I have indexed the full wikipedia dataset. Each block-based codec is configured to use block of 1024. \nI have recorded the average commit time and the optimise time. Then, I have executing random keyword queries and I have recorded the average query time and the number of bytes read for answering the query.\n\nCompression\n\nTime is in ms, Size in bytes.\n\n\n\n\nCodec\nAvg Commit Time\nTotal Commit Time\nOptimise Time\nIndex Size\n\n\nSTD\n1276\n359978\n113386\n1423251\n\n\nSEP\n1437\n405286\n117163\n1666870\n\n\nVINT\n1572\n426551\n122557\n1742083\n\n\nRICE\n1791\n505312\n230636\n1339703\n\n\nS9\n1575\n444157\n146216\n1530666\n\n\nFOR\n1520\n428847\n134502\n1754578\n\n\nPFOR\n1852\n522401\n189262\n1511154\n\n\n\n\n\n\n\tSTD is the Standard lucene codec.\n\tSEP the Sep codec.\n\tAll the other algorithms are based on the Sep codec, but block-based.\n\tFOR and PFOR is my implementation.\n\n\n\nWe can see that the sep codec and block-based codecs have a certain overhead (in indexing time and index size) compared to the standard lucene codec. But, I imagine that this overhead can be reduced with some code optimisation. For the index size, the overhead in block-based codeca is due to the skip list, which is much bigger (127MB for SEP against 189MB for block-based) since we need to encode the block offset, and the inner document offset in the block.\n\nIn term of compression speed and index size, we can see that this results follows the previous results. We can observe also that PFOR is the slowest.\n\nDecompression\n\nFor each codec, I have performed five runs, each run having 200 random keyword queries, and average the time over the 5 runs and 200 queries (i.e., 200*5 = 1000 query execution).\nFor each run, I am opening a new IndexReader and IndexSearcher and I am reusing them across the 200 random queries.\n\nTo generate the queries, I first grouped the terms into three group: HIGH, MEDIUM and LOW, based on their frequency, in order to be able to generate random keyword queries based on this three frequency groups. Then, I have performed benchmarks with random queries of one, two or more keywords, using all possible combinations (e.g., HIGH & HIGH, HIGH & MEDIUM, etc). I have executed the queries using a single thread, and also using multiple threads.\n\nI am reporting below a few of query results, but for most of the other queries, the results were following the same scheme. I don't report the results of the Standard and Sep codec, but they always outperform block-based codec, whatever compression used.\n\nHowever, the results for the query time are completely contradictory with the results of the first benchmark. The slowest algorithm (Rice, Vint) generally provides the faster query execution time, and the faster algorithm (FOR, PFOR) provides the slowest query execution time. Simple9 seems to be as fast as VInt. \nI currently have no explanation for that. Maybe the dataset (wikipedia articles) is too small to see the benefits. But I would say instead there is a problem somewhere. How come VInt and even Rice could provide better query execution times than FOR and PFOR ? While, in the first benchmark, it is clear that FOR and PFOR provides nearly 40% of decompression performance increase.\nDo you have some advices on how to improve the benchmark, or some ideas on the causes of this frop of performance ? I'll be able to re-perform the benchmarks if you propose something.\n\nHIGH:MUST - 1 thread - 200 random queries\n\n\n\n\nCodec\nAvg Query Time\n\n\nRice\n1.5 ms\n\n\nVInt\n1.2 ms\n\n\nPFOR\n2.3 ms\n\n\nFOR\n2.5 ms\n\n\nS9\n1 ms\n\n\n\n\n\nHIGH:MUST - 2 thread - 200 random queries for each thread\n\n\n\n\nCodec\nAvg Query Time\n\n\nRice\n2 ms\n\n\nVInt\n1.9 ms\n\n\nPFOR\n3 ms\n\n\nFOR\n3.5 ms\n\n\nS9\n1.6 ms\n\n\n\n\n\nHIGH:MUST HIGH:SHOULD HIGH:SHOULD HIGH:SHOULD - 1 thread - 200 random queries\n\n\n\n\nCodec\nAvg Query Time\n\n\nRice\n1.5 ms\n\n\nVInt\n1.2 ms\n\n\nPFOR\n2.7 ms\n\n\nFOR\n2.5 ms\n\n\nS9\n1.3 ms\n\n\n\n\n\nHIGH:MUST HIGH:SHOULD HIGH:SHOULD HIGH:SHOULD - 2 threads - 200 random queries for each thread\n\n\n\n\nRice\n2.5 ms\n\n\nVInt\n2 ms\n\n\nPFOR\n3 ms\n\n\nFOR\n3.4 ms\n\n\nS9\n2 ms\n\n\n\n\n ",
            "author": "Renaud Delbru",
            "id": "comment-12852855"
        },
        {
            "date": "2010-04-02T17:11:18+0000",
            "content": "Thanks for sharing these thorough results Renaud!\n\nCurious that PFOR/FOR don't do well during searching...  have you tried profiling?  Maybe something silly is going one.\n\nOne issue is MUST mixed with other clauses \u2013 the scoring for such a query will do alot of seeking, which for block based codecs will be costly.  But it's still strange you don't see speedups for single term query.  Have you tried only SHOULD clauses? ",
            "author": "Michael McCandless",
            "id": "comment-12852868"
        },
        {
            "date": "2010-04-02T17:38:44+0000",
            "content": "\nCurious that PFOR/FOR don't do well during searching... have you tried profiling? Maybe something silly is going one.\n\nI will try profiling. But I am surprised, because I am using the same *IndexInput and *IndexOutut than for the first benchmark. So, if there is a problem, it should be \"outside\" the indexinput.\nBut, I'll double check.\n\n\nOne issue is MUST mixed with other clauses - the scoring for such a query will do alot of seeking, which for block based codecs will be costly. But it's still strange you don't see speedups for single term query. Have you tried only SHOULD clauses?\n\nHere is the results with only SHOULD clause.\n\nHIGH:SHOULD HIGH:SHOULD HIGH:SHOULD HIGH:SHOULD - 1 thread - 200 random queries\n\n\n\n\nCodec\nAvg Query Time\n\n\nRice\n6 ms\n\n\nVInt\n5 ms\n\n\nPFOR\n6.5 ms\n\n\nFOR\n6.8 ms\n\n\nS9\n4.7 ms\n\n\n\n ",
            "author": "Renaud Delbru",
            "id": "comment-12852885"
        },
        {
            "date": "2010-04-02T19:08:07+0000",
            "content": "I think the mixed performance results for decompression and query times may be caused by the use of only a single method. For very short sequences (1 to 2 or 3 integers), I would expect VInt (actually VByte) to perform best. For long sequences (from about 25 integers) , (P)FOR should do best. In between the two, (a variant of) S9.\nThe problem will be to find the optimal bordering sequence sizes to change the compression method.\n\nThe fact that S9 is already doing better than VInt is encouraging. Since (P)FOR can do even better than S9, when using (P)FOR only for longer sequences, I'd expect a real performance boost for queries using frequently occurring terms in the index.\n\nAlso, I'd recommend to verify query results for each method. S9 as I implemented it is only tested by its own test cases.\nWhen the query results are incorrect, measuring performance is not really useful, and this has happened already for the PFOR implementation here, see above in early October 2008. ",
            "author": "Paul Elschot",
            "id": "comment-12852924"
        },
        {
            "date": "2010-04-02T19:32:06+0000",
            "content": "\nThe fact that S9 is already doing better than VInt is encouraging. Since (P)FOR can do even better than S9, when using (P)FOR only for longer sequences, I'd expect a real performance boost for queries using frequently occurring terms in the index.\n\nSo, you're suggesting that maybe in my benchmark, the decoded sequence are not long enough to see the performance improvements of FOR/PFOR ? \n\nAlso, in my benchmark, VINT means block-based VInt. So, there is the same overhead, i.e., decompress the full block even if a partial sequence is needed, than for FOR, PFOR, S9 and the others. But, even with these settings, as you see, the average query time is smaller when using VInt than FOR/PFOR. \n\n\nAlso, I'd recommend to verify query results for each method. S9 as I implemented it is only tested by its own test cases.\n\nWe implemented more unit tests for each codec. Also, in the benchmark, I output the number of hits for each query per codec, and the number of hits for each codec is the same (but I haven't checked if they returns all the same document ids). ",
            "author": "Renaud Delbru",
            "id": "comment-12852930"
        },
        {
            "date": "2010-04-02T19:44:34+0000",
            "content": "Just an update,\n\nI just executed the same benchmark, but instead of executing only 200 queries per run, I have executed 1000 queries, and it seems that the results of FOR and PFOR are better. Maybe, this is again an effect of the JIT. 200 queries per run were not enough to see the effect of the JIT, and therefore FOR and PFOR were penalized.\n\nI'll perform again the full benchmarks with a larger number of random queries, and report to you the results (hopefully good results). ",
            "author": "Renaud Delbru",
            "id": "comment-12852933"
        },
        {
            "date": "2010-07-29T17:13:41+0000",
            "content": "Attached patch, just modernizing and merging all prior patches into a single one.  All tests pass with the PForDelta codec. ",
            "author": "Michael McCandless",
            "id": "comment-12893706"
        },
        {
            "date": "2010-07-29T21:30:43+0000",
            "content": "I'm sorry that there is no code yet for a better patching implementation, see my remark of 12 May 2009. This would need some version of Simple9 and I'm still pondering a generalization of that, but I have no time plan for finishing it.\nA rough implementation might just use vByte for such patches. ",
            "author": "Paul Elschot",
            "id": "comment-12893831"
        },
        {
            "date": "2010-08-03T00:51:52+0000",
            "content": "Very rough patch with some non-committable optimizations.\n\nThe biggest issue was that we were not using the buik-read API when scoring, because the doc IDs are deltas and because we had to take deletions into account.  I hacked around this (and other issues)...\n\nI ran some perf tests, described at http://chbits.blogspot.com/2010/08/lucene-performance-with-pfordelta-codec.html\n\nThe results are encouraging!  FOR/PFOR gets sizable gains for certain queries. ",
            "author": "Michael McCandless",
            "id": "comment-12894769"
        },
        {
            "date": "2010-08-03T05:11:28+0000",
            "content": "Nice blog post, Mike! ",
            "author": "Michael Busch",
            "id": "comment-12894827"
        },
        {
            "date": "2010-12-15T10:44:11+0000",
            "content": "OK I committed the prototype impl onto the bulk postings branch. ",
            "author": "Michael McCandless",
            "id": "comment-12971616"
        },
        {
            "date": "2010-12-16T21:46:00+0000",
            "content": "This patch is to add codec support for PForDelta compression algorithms.\n\n\nChanges by Hao Yan (hyan2008@gmail.com)\n\nIn summary, I added five files to support and test the codec.\n\nIn Src,\n1.      org.apache.lucene.index.codecs.pfordelta.PForDelta.java\n2.      org.apache.lucene.index.codecs.pfordelta.Simple16.java\n3.      org.apache.lucene.index.codecs.PForDeltaFixedBlockCodec.java\n4.      org.apache.lucene.index.codecs.intblock.FixedIntBlockIndexOutputWithGetElementNum.java\n\nIn Test,\n5.      org.apache.lucene.index.codecs.intblock.TestPForDeltaFixedIntBLockCodec.java\n\n1)      In particular, the firs class PForDelta is the core implementation\nof PForDelta algorithm, which compresses exceptions using Simple16\nthat is implemented in the second class Simple16.\n2)      The third classs PForDeltaFixedBlockCodec is similar to\norg.apache.lucene.index.codesc.ockintblock.MockFixedIntBlockCodec in\nTest, except that it uses PForDelta to encode the data in the buffer.\n3)      The fourth class is almost the same as\norg.apache.lucene.index.codecs.intblock.FixedIntBlockINdexOuput,\nexcept that it provides an additional public function to retrieve the\nvalue of the upto field, which is private filed in\nFixedIntBlockINdexOuput. The reason I added this public function is\nthat the number of elements in the block that have meaningful values is not always equal to the blockSize or the buffer\nsize since the last block/buffer of a stream of data usually only\ncontain less number of data. In the case, I will fill all elements after the meaningful elements with 0s. Thus, we alwasy compress one entire block.\n\n4)      The last class is the unit test to test PForDeltaFixedIntBlockCodec\nwhich is very similar to\norg.apache.lucene.index.codecs.mintblock.TestIntBlockCodec.\n\nI also changed the LuceneTestCase class to add the new\nPForDeltaFixeIntBlockCOde.\n\nThe unit tests and all lucence tests have passed. ",
            "author": "hao yan",
            "id": "comment-12972222"
        },
        {
            "date": "2010-12-19T13:41:27+0000",
            "content": "Thanks for the new PForDelta impl!  Now we are swimming in int\nencoders... \n\nI attached a new patch, to get this patch working on the bulkpostings\nbranch\n(https://svn.apache.org/repos/asf/lucene/dev/branches/bulkpostings).\nI also swapped in assert / throw execption instead of\nSystem.out.println in some places.  All lucene core tests pass w/ this\ncodec.\n\nSome notes/questions:\n\n\n\tThe compress looks somewhat costly?  You're allocating several new\n    int[], and also the search for the best bit size looks costly\n    (though, so does the search for the other prototype pfor we have,\n    committed on the branch).\n\n\n\n\n\tDecompress also allocates several new int[].  I think we have to\n    reuse this int[] buffers and minimize copying to get good perf...\n\n\n\n\n\tThe compressor was assuming that the compressed ints would always\n    fit inside the input buffer, but in my case anyway this was not\n    true.  I'm using a block size of 128 and in one case it wanted 136\n    ints in the output, which seems odd since it should just leave the\n    ints uncompressed in that case and use only 129 ints (+1 int for\n    the header).  I changed the code to just return the compressed\n    buffer instead of trying to copy over the input buffer.\n\n\n\n\n\tIt's nice that this new pfor impl handles all 0s, but, Lucene will\n    never hit this (at least today; if we subtracted 1 from our freqs\n    then we may hit it).  So we can same some CPU by not checking if\n    bits==0.  We can also avoid null checks, checking for input length\n    0, etc. \u2013 we can trade this safety for perf.  Such checks can be\n    done as asserts instead.\n\n\n\n\n\tHow come only certain bit sizes are valid?  EG 17,18,19 all seem\n    to round up to 20?\n\n\n\n\n\tHow come bits cannot go up to 31?  Or maybe you just use a full\n    int if it's over 28?  Seems like a good idea...\n\n\n\n\n\tIt's neat that you separately encode exc positions & values (high\n    bits), leaving low bits in the slot. Does this give better perf\n    than linking them together?  (I think pfor1 links).\n\n\n\nAlthough all tests pass if I run w/ -Dtests.codec=PatchedFrameOfRef2,\nif I try to build a big wikipedia index I hit this:\n\n\njava.lang.ArrayIndexOutOfBoundsException: 386\n\tat org.apache.lucene.util.pfor2.Simple16.s16Compress(Simple16.java:68)\n\tat org.apache.lucene.util.pfor2.PForDelta.compressBlockByS16(PForDelta.java:270)\n\tat org.apache.lucene.util.pfor2.PForDelta.compressOneBlockCore(PForDelta.java:221)\n\tat org.apache.lucene.util.pfor2.PForDelta.compressOneBlock(PForDelta.java:82)\n\tat org.apache.lucene.index.codecs.pfordelta2.PForDeltaFixedIntBlockCodec.encodeOneBlockWithPForDelta(PForDeltaFixedIntBlockCodec.java:87)\n\tat org.apache.lucene.index.codecs.pfordelta2.PForDeltaFixedIntBlockCodec$PForDeltaIntFactory$2.flushBlock(PForDeltaFixedIntBlockCodec.java:151)\n\tat org.apache.lucene.index.codecs.intblock.FixedIntBlockIndexOutput.write(FixedIntBlockIndexOutput.java:129)\n\tat org.apache.lucene.index.codecs.sep.SepPostingsWriterImpl.startDoc(SepPostingsWriterImpl.java:204)\n\tat org.apache.lucene.index.codecs.PostingsConsumer.merge(PostingsConsumer.java:79)\n\tat org.apache.lucene.index.codecs.TermsConsumer.merge(TermsConsumer.java:97)\n\tat org.apache.lucene.index.codecs.FieldsConsumer.merge(FieldsConsumer.java:49)\n\tat org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:634)\n\tat org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:147)\n\tat org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3281)\n\tat org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2804)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:339)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:407)\n\n\n\nBut, then I remembered, I think Simple16 cannot represent values >=\n2^28?  Which is a problem here since in general Lucene indexes can\nhave such values (though they should be rarish).  Maybe I'm hitting\none here and this is what happens?\n\nAnyway, I think somehow we need to merge pfor1 and pfor2 to get the\nbest of both words.  I like that pfor2 uses Simple16 for encoding\nexceptions, and how it encodes exceptions (if indeed this is better\nperf), but I like that pfor1 (currently committed on the branch) is\nfast decode (specializes the N bit cases) and allows pure FOR (ie no\nexceptions, you encode to max bit size, which wastes bits but is\nfast).\n\nI'd like to commit this as \"pfor2\" (on the bulkpostings branch); over\ntime we can iterate to merge the two impls, before landing on trunk. ",
            "author": "Michael McCandless",
            "id": "comment-12972987"
        },
        {
            "date": "2010-12-19T13:51:48+0000",
            "content": "Also VSEncoding\n(http://puma.isti.cnr.it/publichtml/section_cnr_isti/cnr_isti_2010-TR-016.html)\nlooks very interesting \u2013 faster that PForDelta! ",
            "author": "Michael McCandless",
            "id": "comment-12972988"
        },
        {
            "date": "2010-12-19T13:56:53+0000",
            "content": "How come bits cannot go up to 31? Or maybe you just use a full int if it's over 28? Seems like a good idea...\n\nSeems like we need to solve this with simple9/simple16 too?\n\nAlthough all tests pass if I run w/ -Dtests.codec=PatchedFrameOfRef2,\nif I try to build a big wikipedia index I hit this:\n\nMike, I've encountered this problem myself while messing with for/pfor.\nI know for these things we need low-level unit tests, but can we cheat in some way?\n\nLike a random test that encodes/decodes a ton of integers (including things that would be rare deltas)\nvia the codec API? ",
            "author": "Robert Muir",
            "id": "comment-12972990"
        },
        {
            "date": "2010-12-19T14:03:57+0000",
            "content": "Seems like we need to solve this with simple9/simple16 too?\n\nYes!\n\n\nLike a random test that encodes/decodes a ton of integers (including things that would be rare deltas)\nvia the codec API?\n\nI completely agree: we need heavy low-level tests for the int encoders... I'll stick a nocommit in when I commit! ",
            "author": "Michael McCandless",
            "id": "comment-12972991"
        },
        {
            "date": "2010-12-19T17:12:59+0000",
            "content": "... separately encode exc positions & values (high bits), leaving low bits in the slot. Does this give better perf than linking them together? (I think pfor1 links).\n\nThis saves forced exceptions for low numbers of frame bits, so the treatment of exceptions is cleaner.\n\nSimple16 cannot represent values >= 2^28?\n\nA simple solution is to treat the high bit of the 28 bit value just the same as in vByte, and allow a vByte to follow in the 28 bit case. The high bit can also be added to the selector easily to avoid testing for it. ",
            "author": "Paul Elschot",
            "id": "comment-12973021"
        },
        {
            "date": "2010-12-20T10:27:58+0000",
            "content": "A simple solution is to treat the high bit of the 28 bit value just the same as in vByte, and allow a vByte to follow in the 28 bit case. The high bit can also be added to the selector easily to avoid testing for it.\n\nThat sounds great!  Any chance you could fix this up on one of the Simple16 impls?  I'd really like to have a Simple9/16 codec to better test our variable int block codec infrastructure... ",
            "author": "Michael McCandless",
            "id": "comment-12973159"
        },
        {
            "date": "2010-12-20T10:30:35+0000",
            "content": "OK I committed \"pfor2\" onto the branch.\n\nI also add a new low-level \"encode random ints\" tests. pfor1 passes the test but pfor2 fails it (I'm guessing this is the 2^28 limitation of Simple16, but I'm really not sure), so I left the pfor2 random ints test @Ignore for now... ",
            "author": "Michael McCandless",
            "id": "comment-12973161"
        },
        {
            "date": "2010-12-20T18:19:13+0000",
            "content": "In the 1410e patch here are test cases that have not made into the bulkpostings branch. I'll try and revive these first. ",
            "author": "Paul Elschot",
            "id": "comment-12973286"
        },
        {
            "date": "2010-12-21T11:48:32+0000",
            "content": "In the 1410e patch here are test cases that have not made into the bulkpostings branch. I'll try and revive these first.\n\nUgh, sorry   Thanks! ",
            "author": "Michael McCandless",
            "id": "comment-12973593"
        },
        {
            "date": "2010-12-21T16:16:01+0000",
            "content": "No need to be sorry. Thanks for taking this on. ",
            "author": "Paul Elschot",
            "id": "comment-12973733"
        },
        {
            "date": "2010-12-21T17:49:26+0000",
            "content": "On LUCENE-2723, I uploaded a \"bulk vint\" codec that shares most of the same codepath as FOR/PFOR,\nexcept it writes blocks of 128 vint-encoded integers.\n\nThere are performance numbers there compared to our Standard vint-based codec, as you can see\nit differs dramatically due to other reasons.\n\nSo I thought it would be useful to then compare FOR to this, since its a good measure of just the compression\nalgorithm, but everything else is the same (comparing two 128-block size FixedIntBlock codecs with the same \nindex layout, etc etc). This way we compare apples to apples.\n\n\n\n\nQuery\nQPS BulkVInt\nQPS FOR\nPct diff\n\n\nunited~1.0\n9.43\n9.39\n-0.5%\n\n\nunited~2.0\n2.02\n2.02\n-0.3%\n\n\nunit~1.0\n6.37\n6.36\n-0.1%\n\n\nunit~2.0\n6.13\n6.21\n1.2%\n\n\n\"unit state\"~3\n3.45\n3.51\n2.0%\n\n\nspanNear([unit, state], 10, true)\n2.89\n2.99\n3.3%\n\n\nunit*\n30.04\n31.42\n4.6%\n\n\nunit state\n8.00\n8.40\n5.0%\n\n\n\"unit state\"\n5.97\n6.37\n6.7%\n\n\nspanFirst(unit, 5)\n11.29\n12.10\n7.2%\n\n\nuni*\n17.36\n18.69\n7.6%\n\n\n+unit +state\n10.99\n12.18\n10.8%\n\n\n+nebraska +state\n65.74\n73.06\n11.1%\n\n\nstate\n28.90\n32.37\n12.0%\n\n\nu*d\n10.54\n12.45\n18.1%\n\n\nun*d\n40.06\n47.61\n18.9%\n\n\n\n ",
            "author": "Robert Muir",
            "id": "comment-12973790"
        },
        {
            "date": "2010-12-21T19:21:45+0000",
            "content": "I had a quick look at the codecs for this, but I couldn't find the answer to this question easily:\nAre the positions here also encoded by the bulk int encoders (VInt and FOR)? ",
            "author": "Paul Elschot",
            "id": "comment-12973827"
        },
        {
            "date": "2010-12-21T19:25:10+0000",
            "content": "Yes positions are bulk coded too, but we haven't cutover any positional queries yet to use the bulk enum API... we should cutover at least one (I think exact PhraseQuery is probably easiest!). ",
            "author": "Michael McCandless",
            "id": "comment-12973830"
        },
        {
            "date": "2010-12-21T21:26:13+0000",
            "content": "I'm running into a nocommit for the nio byte buffer allocation in ForDecompress.java.\nShall I try and move the buffer handling from there into FORIndexInput and PForDeltaIndexInput at the codecs?\nI could leave it as it is, but then the test cases from the 1410e patch would have to be adapted again when the nocommit is fixed.\n\nAlso the package/directory naming o.a.l.util.pfor and o.a.l.index.codecs.pfordelta may be confusing.\nProbably pfordelta could could be renamed to pfor, since delta refers to differences (in docids and positions) that are treated elsewhere.\nBut I'd rather not change that now.  ",
            "author": "Paul Elschot",
            "id": "comment-12973897"
        },
        {
            "date": "2010-12-21T21:44:24+0000",
            "content": "\nI'm running into a nocommit for the nio byte buffer allocation in ForDecompress.java.\nShall I try and move the buffer handling from there into FORIndexInput and PForDeltaIndexInput at the codecs?\n\nI am to blame for this I think! Actually I think the buffer handling could stay and we could just remove the nocommit?\nI've tested everything I can think of and it seems this nio ByteBuffer/IntBuffer approach is always the fastest:\nits only slower to do it other ways, and it doesnt help to do trickier things like IntBuffer views of MMap even.\n\nOne thing that would be good, is it possible to encode the length in decompressed bytes (or the length in bytes of exceptions)\ninto PFOR's int header? this would allow us to remove the wasted per-block int that we currently encode now.\n\nThen we could \"put FOR and PFOR back together\" again... sorry i split apart the decompressors to remove the wasted int\nin the FOR case since we can get it from its header already. ",
            "author": "Robert Muir",
            "id": "comment-12973914"
        },
        {
            "date": "2010-12-21T21:51:40+0000",
            "content": "Sorry, correction (i meant the length in bytes or ints compressed, to tell us how many bytes to read)\n\nIn the FOR case we now do:\n\n\n    int header = in.readInt();\n    final int numFrameBits = ((header >>> 8) & 31) + 1;\n    in.readBytes(input, 0, numFrameBits << 4);\n\n\n\nBut in PFOR we still have \"two headers\"\n\n\n    int numBytes = in.readInt(); // nocommit: is it possible to encode # of exception bytes in header?\n    in.readBytes(input, 0, numBytes);\n    compressedBuffer.rewind();\n    int header = compressedBuffer.get();\n\n ",
            "author": "Robert Muir",
            "id": "comment-12973927"
        },
        {
            "date": "2010-12-22T17:57:37+0000",
            "content": "... is it possible to encode # of exception bytes in header?\n\nIn the first implementation the start index of the exception chain is in the header (5 or 6 bits iirc). In the second implementation (by Hoa Yan) there is no exception chain, so the number of exceptions must somehow be encoded in the header.\nThat means encoding the # exception bytes in the header would be easier in the second implementation, but it is also possible in the first one.\n\nI would expect that a few bits for the number of encoded integers would also be added in the header (think 32, 64, 128...).\nThe number of frame bits takes 5 bits.\nThat means that there are about 2 bytes unused in the header now, and I'd expect 1 byte to be enough to encode the number of bytes for the exceptions. For example a bad case in the first implementation of 10 exceptions of 4 bytes means 40 bytes data, that fits in 6 bits, the same\nbad case in the second implementation would also need to store the indexes of the exceptions in 10*5 bits,  for a total of  about 48 bytes that can be still be encoded in 6 bits. However, I don't know what the worst case # exceptions is. (This gets into vsencoding...)\n\nFor the moment I'll just leave this unchanged and get the tests working on the current first implementation. ",
            "author": "Paul Elschot",
            "id": "comment-12974303"
        },
        {
            "date": "2010-12-22T18:06:35+0000",
            "content": "I've tested everything I can think of and it seems this nio ByteBuffer/IntBuffer approach is always the fastest ...\n\nDid you also test without a copy (without the readbytes() call) into the underlying byte array for the IntBuffer? That might be even faster,\nand it could be possible when using for example a BufferedIndexInput or an MMapDirectory.\nFor decent buffer.get() speed the starting byte would need to be aligned at an int border.\n ",
            "author": "Paul Elschot",
            "id": "comment-12974307"
        },
        {
            "date": "2010-12-22T18:22:54+0000",
            "content": "\nDid you also test without a copy (without the readbytes() call) into the underlying byte array for the IntBuffer? That might be even faster,\nand it could be possible when using for example a BufferedIndexInput or an MMapDirectory.\nFor decent buffer.get() speed the starting byte would need to be aligned at an int border.\n\nYes, for the mmap case I tried the original dangerous hack, exposing in Intbuffer view of its internal mapped byte buffer.\nI also tried mmapindexinput keeping track of its own intbuffer view.\n\nwe might be able to have some gains by allowing a directory to return an IntBufferIndexInput of some sort (separate from DataInput/IndexInput)\nthat basically just positions an IntBuffer view (the default implementation would fill from an indexinput into a bytebuffer like we do now),\nbut I haven't tested this across all the directories yet... it might help NIOFS though as it would bypass the double-buffering of BufferedIndexInput.\nFor SimpleFS it would be the same, and for MMap i'm not very hopeful it would be better, but maybe not worse.\n\nif that worked maybe we could do the same with Long, for things like simple-8b (http://onlinelibrary.wiley.com/doi/10.1002/spe.948/abstract) ",
            "author": "Robert Muir",
            "id": "comment-12974315"
        },
        {
            "date": "2010-12-23T22:06:03+0000",
            "content": "I tried to revive the tests from the 1410e patch, but it does not make much sense because they test rather short sequences of input to be compressed, and decompressor is now hardcoded to always decompress 128 values. ",
            "author": "Paul Elschot",
            "id": "comment-12974765"
        },
        {
            "date": "2010-12-24T12:21:37+0000",
            "content": ".. we might be able to have some gains by allowing a directory to return an IntBufferIndexInput of some sort (separate from DataInput/IndexInput) that basically just positions an IntBuffer view (the default implementation would fill from an indexinput into a bytebuffer like we do now),\n\nSince things are moving on the nio buffer front (see also LUCENE-2292), how about trying to be independent from the buffer implementation?\nThat might be done by allowing an IntBuffer wrapping a byte array or as view alongside a ByteBuffer, or temporary IntBuffer as above.\n\nTo be independent from the buffer implementation we could add some methods to IndexInput:\n\nvoid startAlignToInt() // basically a seek to the next multiple of 4 byte when not already there. Could also start using an IntBuffer somehow.\nint readAlignedInt() // get the next int, default to readInt(), use an IntBuffer when available.\nvoid endAlignToInt() // switch back to byte reading, set the byte buffer to the the position corresponding to the int buffer.\n\n(Adding this to DataInput seems to be a more natural place, but DataInput cannot seek.)\n\nWould that work, and could it work fast? ",
            "author": "Paul Elschot",
            "id": "comment-12974901"
        },
        {
            "date": "2012-03-24T17:07:38+0000",
            "content": "Out of curiousity, is the PFOR effort dead?  I was thinking about running some newer benchmarks using Java 7, and see if that makes a difference.  \n\nDo you guys think that's worthwhile? ",
            "author": "The Alchemist",
            "id": "comment-13237604"
        },
        {
            "date": "2012-03-25T15:29:05+0000",
            "content": "Out of curiousity, is the PFOR effort dead? \n\nNothing in open source is ever dead!  (Well, rarely...).  It's just that nobody has picked this up again and pushed it to a committable state.\n\nI think now that we have no more bulk API in trunk, it may not be that much work to finish... though there could easily be surprises.\n\nI opened LUCENE-3892 to do exactly this, as a Google Summer of Code project. ",
            "author": "Michael McCandless",
            "id": "comment-13237893"
        },
        {
            "date": "2012-08-19T09:31:49+0000",
            "content": "There is no point in continuing this here with LUCENE-3892 well on its way. ",
            "author": "Paul Elschot",
            "id": "comment-13437487"
        },
        {
            "date": "2012-08-19T09:32:59+0000",
            "content": "See LUCENE-3892 ",
            "author": "Paul Elschot",
            "id": "comment-13437488"
        }
    ]
}