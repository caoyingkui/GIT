{
    "id": "SOLR-6640",
    "title": "Replication can cause index corruption.",
    "details": {
        "components": [
            "replication (java)"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.0",
            "6.0"
        ],
        "affect_versions": "5.0",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Blocker"
    },
    "description": "Test failure found on jenkins:\nhttp://jenkins.thetaphi.de/job/Lucene-Solr-5.x-Linux/11333/\n\n\n1 tests failed.\nREGRESSION:  org.apache.solr.cloud.ChaosMonkeySafeLeaderTest.testDistribSearch\n\nError Message:\nshard2 is not consistent.  Got 62 from http://127.0.0.1:57436/collection1lastClient and got 24 from http://127.0.0.1:53065/collection1\n\nStack Trace:\njava.lang.AssertionError: shard2 is not consistent.  Got 62 from http://127.0.0.1:57436/collection1lastClient and got 24 from http://127.0.0.1:53065/collection1\n        at __randomizedtesting.SeedInfo.seed([F4B371D421E391CD:7555FFCC56BCF1F1]:0)\n        at org.junit.Assert.fail(Assert.java:93)\n        at org.apache.solr.cloud.AbstractFullDistribZkTestBase.checkShardConsistency(AbstractFullDistribZkTestBase.java:1255)\n        at org.apache.solr.cloud.AbstractFullDistribZkTestBase.checkShardConsistency(AbstractFullDistribZkTestBase.java:1234)\n        at org.apache.solr.cloud.ChaosMonkeySafeLeaderTest.doTest(ChaosMonkeySafeLeaderTest.java:162)\n        at org.apache.solr.BaseDistributedSearchTestCase.testDistribSearch(BaseDistributedSearchTestCase.java:869)\n\n\n\nCause of inconsistency is:\n\nCaused by: org.apache.lucene.index.CorruptIndexException: file mismatch, expected segment id=yhq3vokoe1den2av9jbd3yp8, got=yhq3vokoe1den2av9jbd3yp7 (resource=BufferedChecksumIndexInput(MMapIndexInput(path=\"/mnt/ssd/jenkins/workspace/Lucene-Solr-5.x-Linux/solr/build/solr-core/test/J0/temp/solr.cloud.ChaosMonkeySafeLeaderTest-F4B371D421E391CD-001/tempDir-001/jetty3/index/_1_2.liv\")))\n   [junit4]   2> \t\tat org.apache.lucene.codecs.CodecUtil.checkSegmentHeader(CodecUtil.java:259)\n   [junit4]   2> \t\tat org.apache.lucene.codecs.lucene50.Lucene50LiveDocsFormat.readLiveDocs(Lucene50LiveDocsFormat.java:88)\n   [junit4]   2> \t\tat org.apache.lucene.codecs.asserting.AssertingLiveDocsFormat.readLiveDocs(AssertingLiveDocsFormat.java:64)\n   [junit4]   2> \t\tat org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:102)",
    "attachments": {
        "corruptindex.log": "https://issues.apache.org/jira/secure/attachment/12694442/corruptindex.log",
        "SOLR-6640.patch": "https://issues.apache.org/jira/secure/attachment/12685801/SOLR-6640.patch",
        "Lucene-Solr-5.x-Linux-64bit-jdk1.8.0_20-Build-11333.txt": "https://issues.apache.org/jira/secure/attachment/12676178/Lucene-Solr-5.x-Linux-64bit-jdk1.8.0_20-Build-11333.txt",
        "SOLR-6920.patch": "https://issues.apache.org/jira/secure/attachment/12697107/SOLR-6920.patch",
        "SOLR-6640_new_index_dir.patch": "https://issues.apache.org/jira/secure/attachment/12689536/SOLR-6640_new_index_dir.patch",
        "SOLR-6640-test.patch": "https://issues.apache.org/jira/secure/attachment/12693283/SOLR-6640-test.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2014-10-21T21:07:20+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "The test logs from jenkins are attached. ",
            "id": "comment-14179101"
        },
        {
            "date": "2014-10-21T23:38:28+0000",
            "author": "Robert Muir",
            "content": "This exception translates to: \"this file _1_2.liv is mismatched and does not belong to this segment _1, instead it belongs to another segment (likely _0)\"\n\nYou can see via the suppressed checksum exception from the full stacktrace that the contents are themselves consistent and not corrupt (checksum passes), its just mismatched.\n ",
            "id": "comment-14179322"
        },
        {
            "date": "2014-10-28T10:25:02+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks Robert. In order to make sure that we are actually downloading files belonging to the correct segment, we need access to the segment version but it doesn't seem like there's an easy way to read them? Can the segment ids be exposed in the IndexCommit object? Also, can we provide a custom segment id instead of having Lucene generate them? As it stands today, we have no way of controlling the segment ids generated by replicas and therefore I'm afraid that replica recovery might end up downloading the complete index. ",
            "id": "comment-14186676"
        },
        {
            "date": "2014-10-28T12:15:01+0000",
            "author": "Robert Muir",
            "content": "\n In order to make sure that we are actually downloading files belonging to the correct segment, we need access to the segment version but it doesn't seem like there's an easy way to read them? Can the segment ids be exposed in the IndexCommit object?\n\nSegmentInfos infos = SegmentInfos.readCommit(directory, commit.getSegmentsFileName());\n\n\nAlso, can we provide a custom segment id instead of having Lucene generate them?\n\nNo: this is an internal safety mechanism from lucene. Its detecting that solr replicates the wrong file (again: in this case, the wrong deletes generation). Apparently, we cannot entrust apps to not mismatch up our index files.\n\n\nAs it stands today, we have no way of controlling the segment ids generated by replicas and therefore I'm afraid that replica recovery might end up downloading the complete index.\n\nMaybe instead of worrying about optimizing this, its best to start with \"not corrupting the index\". ",
            "id": "comment-14186746"
        },
        {
            "date": "2014-12-03T06:08:50+0000",
            "author": "Varun Thacker",
            "content": "This is interesting - When I was running the test a few weeks back it was reproducible all the time on my local machine . I ran it a few times against branch_5x right now and it passed every time. \n\n\nant test  -Dtestcase=ChaosMonkeySafeLeaderTest -Dtests.method=testDistribSearch -Dtests.seed=F4B371D421E391CD -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=pt_BR -Dtests.timezone=Africa/Douala -Dtests.file.encoding=US-ASCII\n\n\n\nI am digging more into what changed. Anyways I will upload a patch I have on my machine which uses the lucene segment-ids to verify what files need to be downloaded. ",
            "id": "comment-14232641"
        },
        {
            "date": "2014-12-08T18:20:41+0000",
            "author": "Varun Thacker",
            "content": "Rough patch.\n\ncompareFile() checks if the file is present and also if the length and checksum for each file matches . Used instead of slowFileExists.\n\nSmall refactoring -\nRemoved createTempindexDir\nmoveAFile() never utilized  List<String> copiedfiles. So I removed it.\nModified the DirectoryFileFetcher ctor to take only the necessary params\nIn DirectoryFileFetcher saveAs was always the fileName. So I removed it.\n\nWe need to see if we need to download all files in a segment when they are out of sync as opposed to just downloading the missing files as we do currently. ",
            "id": "comment-14238193"
        },
        {
            "date": "2014-12-09T18:19:04+0000",
            "author": "Mark Miller",
            "content": "Cool - this is a very important issue to solve. ",
            "id": "comment-14239783"
        },
        {
            "date": "2014-12-16T16:32:48+0000",
            "author": "Varun Thacker",
            "content": "This test passes with the patch - ant test  -Dtestcase=ChaosMonkeySafeLeaderTest -Dtests.method=testDistribSearch -Dtests.seed=EDA082CD42EB33E3 -Dtests.slow=true -Dtests.locale=hi_IN -Dtests.timezone=Asia/KuchingDtests.asserts=true -Dtests.file.encoding=ISO-8859-1 \n\nThe patch does something very simple - Before we begin to download segment files, check against the current commit point which files are extra and remove them.\n\nFor example -\n\n-001/jetty3/index.20141216162501726\n   [junit4]   2> 22194 T113 C7 P59775 oash.SnapPuller.fetchLatestIndex SOLR-6640:: indexDir.listAll() pre remove _0.cfe _0.cfs _0.si _0_1.liv _1.fdt _1.fdx segments_1 \n   [junit4]   2> 22195 T79 C6 P59766 oasup.LogUpdateProcessor.finish [collection1] webapp=/_ path=/update params={wt=javabin&version=2} {add=[0-19 (1487664160941539328)]} 0 1\n   [junit4]   2> 22196 T113 C7 P59775 oash.SnapPuller.fetchLatestIndex SOLR-6640:: indexDir.listAll() post remove segments_1 \n\n\n\nSo it's these files which are not getting removed when we do IW.rollback that were causing the problem - \n_0.cfe _0.cfs _0.si _0_1.liv _1.fdt _1.fdx\n\nI am yet to figure out whether these files should have been removed by IW.rollback() or not?  ",
            "id": "comment-14248456"
        },
        {
            "date": "2014-12-17T10:56:04+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "I am looking at this failure too and I see another bug. I was wondering why the replica had these writes in the first place considering that recovery on startup had not completed.\n\n\n\tRecoveryStrategy publishes the state of the replica as 'recovering' before it sets the update log to buffering mode which is why the leader sends updates to this replica that affect the index.\n\tThe test itself doesn't wait for a steady state e.g. by calling waitForRecovery or waitForThingsToLevelOut before starting the indexing threads. This is probably a good thing because that's what has helped us find this problem.\n\tShouldn't the peersync also be done while update log is set to buffering mode?\n\n\n\n\nSo it's these files which are not getting removed when we do IW.rollback that were causing the problem - \n_0.cfe _0.cfs _0.si _0_1.liv _1.fdt _1.fdx\nI am yet to figure out whether these files should have been removed by IW.rollback() or not?\n\nThese files hang around because an IndexReader is open using the IndexWriter due to soft commit(s). ",
            "id": "comment-14249708"
        },
        {
            "date": "2014-12-17T12:07:13+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "So what is the right way to implement partial replication in this case? Force deleting the file (Varun's patch) probably won't work on windows and/or not play well with the open searchers. In SolrCloud we could just close the searcher before rollback because a replica in recovery won't get any search requests but that's not practical in standalone Solr because it'd cause downtime. ",
            "id": "comment-14249771"
        },
        {
            "date": "2014-12-29T14:41:17+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "I had a discussion with Varun about this issue. We have two problems here\n\n\tSolr corrupts the index during replication recovery\n\tSuch a corrupt index puts Solr into an infinite recovery loop\n\n\n\nFor #1 the problem is clear \u2013 we have open searchers on uncommitted/flushed files which are mixed with files from the leader causing corruption.\n\nPossible solutions for #1 are either a) switch to a different index dir and move/copy files from committed segments and use the index.properties approach to open a searcher on the new index dir or b) close the searcher then rollback the writer and then download the necessary files.\n\nClosing the searcher.... is not as simple as it sounds because the searcher is ref counted and close() doesn't really close immediately. Also, at any time, a request might open a new searcher so it is a very involved change.\n\nFor #2, every where we open a reader/searcher or writer, we should be ready to handle the corrupt index exceptions.\n\nI think we should first try to first solve the problem of corrupting the index. So let's try the deletion approach that Varun outlined. If that fails then we should switch to a new index dir, move/copy over files from commit points, fetch the missing segments from the leader and use the index.properties approach to completely move to a new index directory.\n\nThe second problem that we need to solve is that a corrupted index trashes the server. We should be able to recover from such a scenario instead of going into an infinite recovery loop.\n\nLet's fix these two problems (in that order) and then figure out ways to optimize recovery.\n\nLonger term we need to change our code such that we can close the searchers, rollback the writer and delete uncommitted files and then attempt replication recovery.\n\nAlso my earlier comment on non-cloud Solr was wrong:\nIn SolrCloud we could just close the searcher before rollback because a replica in recovery won't get any search requests but that's not practical in standalone Solr because it'd cause downtime.\n\nIn stand alone Solr this is not a problem because indexing and soft-commits do not happen on slaves. But anyway changing to close the searcher etc is a big change. ",
            "id": "comment-14260134"
        },
        {
            "date": "2014-12-30T17:44:16+0000",
            "author": "Varun Thacker",
            "content": "This patch uses the following approach - Download all missing files from leader into the tempIndexDir and then copy over the remaining files from the current index directory into the tempIndexDir. Now use the index.properties method to switch to the new index. \n\nMain changes are in moveIndexFiles() method.\n\nStill very rough and needs refactoring, but wanted to share this approach and get some thoughts on it. ",
            "id": "comment-14261293"
        },
        {
            "date": "2015-01-05T19:35:28+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Here's a better patch which:\n\n\tRe-opens IW after rolling back the previous one\n\tRe-opens the searcher using the new IW (we can't re-open the old one because it was created using the previous IW which will be closed on rollback)\n\tCalls IW.deleteUnusedFiles repeatedly with a 1 second sleep until all unused files have been cleaned up. This may take some time because the old searcher or a virus scanner may be using these files.\n\tIf we could not clean up files within 30 seconds then we fall back to a full replication\n\n ",
            "id": "comment-14264971"
        },
        {
            "date": "2015-01-06T20:38:01+0000",
            "author": "Mark Miller",
            "content": "a searcher might be using some flushed but committed segments\n\nshouldn't that be 'not committed'? ",
            "id": "comment-14266716"
        },
        {
            "date": "2015-01-06T21:32:38+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "shouldn't that be 'not committed'?\n\nUmm yeah but who are you quoting?  ",
            "id": "comment-14266814"
        },
        {
            "date": "2015-01-06T22:16:01+0000",
            "author": "Mark Miller",
            "content": "I'm quoting from the patch that was attached yesterday. ",
            "id": "comment-14266870"
        },
        {
            "date": "2015-01-06T22:48:45+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Ah, okay, yes you're right. I'll fix that comment. ",
            "id": "comment-14266906"
        },
        {
            "date": "2015-01-07T08:29:21+0000",
            "author": "Varun Thacker",
            "content": "Shouldn't the logic be a try-finally block instead of try-with-resource ?\n\n\ntry {\n            IndexWriter indexWriter = writer.get();\n            int c = 0;\n            indexWriter.deleteUnusedFiles();\n            while (hasUnusedFiles(indexDir, commit)) {\n              indexWriter.deleteUnusedFiles();\n              LOG.info(\"Sleeping for 1000ms to wait for unused lucene index files to be delete-able\");\n              Thread.sleep(1000);\n              c++;\n              if (c >= 30)  {\n                LOG.warn(\"SnapPuller unable to cleanup unused lucene index files so we must do a full copy instead\");\n                isFullCopyNeeded = true;\n                break;\n              }\n            }\n            if (c > 0)  {\n              LOG.info(\"SnapPuller slept for \" + (c * 1000) + \"ms for unused lucene index files to be delete-able\");\n            }\n          } finally {\n            if (writer != null) {\n              writer.decref();\n            }\n          }\n\n ",
            "id": "comment-14267403"
        },
        {
            "date": "2015-01-10T17:35:09+0000",
            "author": "Mark Miller",
            "content": "Something similar to this corrupted index issue also causes this test to timeout sometimes because a sever will never recover. It keeps trying, but keeps ending up with a corrupted index problem and eventually the test framework hard kills it. ",
            "id": "comment-14272617"
        },
        {
            "date": "2015-01-12T21:54:16+0000",
            "author": "Mark Miller",
            "content": "Can we get this in? I'd love to see it's affect on some common test fails over some time before 5.0 hits and see that nothing else pops out. ",
            "id": "comment-14274233"
        },
        {
            "date": "2015-01-12T23:15:13+0000",
            "author": "Anshum Gupta",
            "content": "Changing this to a Blocker for 5.0 as I think it needs to go in for 5.0. ",
            "id": "comment-14274380"
        },
        {
            "date": "2015-01-20T11:07:20+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Here's a new test called RecoveryAfterSoftCommitTest which reproduces these failures.\n\nMy last fix isn't quite right; it leaks recovery threads and increases the recovery time by a lot. I am trying to figure out a better way. ",
            "id": "comment-14283706"
        },
        {
            "date": "2015-01-20T15:36:43+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks Varun for catching my mistake. The try-with-resource was wrong and I should have used try/finally to decref the writer. This patch does that.\n\nThis patch closes the searcher itself in cloud mode and did away with re-opening the searcher. All tests pass. I'll commit this shortly and bake it on jenkins for a while. ",
            "id": "comment-14283938"
        },
        {
            "date": "2015-01-20T16:19:40+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1653281 from shalin@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1653281 ]\n\nSOLR-6640: Close searchers before rollback and recovery to avoid index corruption ",
            "id": "comment-14283967"
        },
        {
            "date": "2015-01-20T16:22:51+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1653283 from shalin@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1653283 ]\n\nSOLR-6640: No need for SSL in this test ",
            "id": "comment-14283972"
        },
        {
            "date": "2015-01-22T12:59:34+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1653837 from shalin@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1653837 ]\n\nSOLR-6640: Add changelog entry ",
            "id": "comment-14287376"
        },
        {
            "date": "2015-01-22T13:00:19+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1653838 from shalin@apache.org in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1653838 ]\n\nSOLR-6640: Close searchers before rollback and recovery to avoid index corruption ",
            "id": "comment-14287377"
        },
        {
            "date": "2015-01-22T13:02:03+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1653839 from shalin@apache.org in branch 'dev/branches/lucene_solr_5_0'\n[ https://svn.apache.org/r1653839 ]\n\nSOLR-6640: Close searchers before rollback and recovery to avoid index corruption ",
            "id": "comment-14287380"
        },
        {
            "date": "2015-01-22T13:12:47+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1653844 from shalin@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1653844 ]\n\nSOLR-6640: Exclude lock files from unused file check ",
            "id": "comment-14287393"
        },
        {
            "date": "2015-01-22T13:13:37+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1653845 from shalin@apache.org in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1653845 ]\n\nSOLR-6640: Exclude lock files from unused file check ",
            "id": "comment-14287395"
        },
        {
            "date": "2015-01-22T13:14:18+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1653846 from shalin@apache.org in branch 'dev/branches/lucene_solr_5_0'\n[ https://svn.apache.org/r1653846 ]\n\nSOLR-6640: Exclude lock files from unused file check ",
            "id": "comment-14287397"
        },
        {
            "date": "2015-01-25T20:04:06+0000",
            "author": "Mark Miller",
            "content": "The Chaos Monkey tests are already in much better shape - I run a variety of them in normal and nightly mode on my local jenkins machine, and the 3 day performance is great.\n\nThere are a couple things to look into with ChaosMonkeyNothingIsSafeTest, but that appears unrelated to this change. ",
            "id": "comment-14291244"
        },
        {
            "date": "2015-01-25T20:11:48+0000",
            "author": "Mark Miller",
            "content": "Spoke too soon - one of the jobs did hit a corrupt index yesterday. I'll post the logs. ",
            "id": "comment-14291248"
        },
        {
            "date": "2015-01-26T16:04:30+0000",
            "author": "Varun Thacker",
            "content": "Hi Mark Miller,\n\nI was unable to reproduce it just by running this on branch_5x - ant test  -Dtestcase=ChaosMonkeySafeLeaderTest -Dtests.seed=22E5E46764398970. \n\nAlso I don't see this line which generally comes with the the fail - \"NOTE: reproduce with: ant test....\"\n\nAre you able to reproduce it? If yes then can you tell me how? I'd also like to test it out and see why it got corrupted. ",
            "id": "comment-14291993"
        },
        {
            "date": "2015-01-27T12:20:49+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks Mark. I am trying to figure out the failure and create a smaller reproducible test case. ",
            "id": "comment-14293425"
        },
        {
            "date": "2015-01-28T09:41:50+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1655249 from Anshum Gupta in branch 'dev/trunk'\n[ https://svn.apache.org/r1655249 ]\n\nSOLR-6640: Fixing the comment ",
            "id": "comment-14294934"
        },
        {
            "date": "2015-01-28T09:43:12+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1655250 from Anshum Gupta in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1655250 ]\n\nSOLR-6640: Fixing the comment (merge from trunk) ",
            "id": "comment-14294936"
        },
        {
            "date": "2015-01-28T09:45:09+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1655251 from Anshum Gupta in branch 'dev/branches/lucene_solr_5_0'\n[ https://svn.apache.org/r1655251 ]\n\nSOLR-6640: Fixing the comment (merge from branch_5x) ",
            "id": "comment-14294937"
        },
        {
            "date": "2015-01-30T04:44:47+0000",
            "author": "Mark Miller",
            "content": "Hey Shalin Shekhar Mangar, shouldn't\n\nCollection<String> files = info.info.files(); // All files that belong to this segment\n\nbe\n\nCollection<String> files = info.files(); // All files that belong to this segment\n\nThe former skips some logic in the later - one note in particular in info.files() mentions adding live files, and in tests, I'm seeing this exhibit bad behavior and timeout waiting for .liv files to be deletable. ",
            "id": "comment-14298195"
        },
        {
            "date": "2015-02-02T16:49:34+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "I had been ill last week so I am resuming work on this today. \n\nI suspect you're right, Mark. I should use the info.files(). Better yet, we can use SegmentInfos.files(Directory dir, boolean includeSegmentsFile) method and avoid iterating through SegmentCommitInfo objects. ",
            "id": "comment-14301444"
        },
        {
            "date": "2015-02-03T06:55:03+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1656630 from shalin@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1656630 ]\n\nSOLR-6640: Use SegmentInfos.files in unused file check ",
            "id": "comment-14302857"
        },
        {
            "date": "2015-02-03T06:56:11+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1656631 from shalin@apache.org in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1656631 ]\n\nSOLR-6640: Use SegmentInfos.files in unused file check ",
            "id": "comment-14302858"
        },
        {
            "date": "2015-02-04T20:44:54+0000",
            "author": "Anshum Gupta",
            "content": "I spoke to Shalin and he suggested me to wait until Friday to see if all stays good after the last commit. If nothing fails, I'll continue with the next RC for Solr 5.0.\nBy the time we get done with the vote, we'd have given the fix even more time. ",
            "id": "comment-14305909"
        },
        {
            "date": "2015-02-05T06:57:09+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1657490 from shalin@apache.org in branch 'dev/branches/lucene_solr_5_0'\n[ https://svn.apache.org/r1657490 ]\n\nSOLR-6640: Use SegmentInfos.files in unused file check ",
            "id": "comment-14306762"
        },
        {
            "date": "2015-02-06T12:07:08+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "I have found another failure. This time on HdfsBasicDistributedZk2Test and it is easily reproducible with:\n\nant test  -Dtestcase=HdfsBasicDistributedZk2Test -Dtests.method=test -Dtests.seed=A7C02AE49EA55A58 -Dtests.nigly=true -Dtests.slow=true -Dtests.locale=iw_IL -Dtests.timezone=America/St_Vincent -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n\n\n\nI'd appreciate some help in tracking down the root cause. ",
            "id": "comment-14309044"
        },
        {
            "date": "2015-02-06T15:32:35+0000",
            "author": "Mark Miller",
            "content": "I'm taking a look. ",
            "id": "comment-14309328"
        },
        {
            "date": "2015-02-06T15:54:19+0000",
            "author": "Mark Miller",
            "content": "Those troublesome .si files...\n\nWe don't download because we think we already have the right one I believe.\n\nVarun's nice issue fixes this: SOLR-6920: During replication use checksums to verify if files are the same ",
            "id": "comment-14309347"
        },
        {
            "date": "2015-02-06T15:55:59+0000",
            "author": "Mark Miller",
            "content": "Of course that only works if your segments are new enough to have checksum info in that patch. ",
            "id": "comment-14309350"
        },
        {
            "date": "2015-02-06T18:17:28+0000",
            "author": "Mark Miller",
            "content": "I've been focusing on SOLR-6920 with Varun. I think that is the right path to remove the rest of these issues. ",
            "id": "comment-14309557"
        },
        {
            "date": "2015-02-06T19:58:36+0000",
            "author": "Varun Thacker",
            "content": "1. Changed the log message in SnapPuller.compareFile() since checksum won't be present in case of exception.\nFrom - LOG.warn(\"File {} did not match. expected checksum is {} and actual is checksum {}. \" + \"expected length is {} and actual length is {}\", filename, backupIndexFileChecksum indexFileChecksum, backupIndexFileLen, indexFileLen);\nTo - LOG.warn(\"File {} did not match. expected length is {} and actual length is {}\", filename,backupIndexFileLen, indexFileLen);\n\n2. In SnapPuller.downloadIndexFiles made (String) file.get(NAME)) into a variable for better readibility.\n\n3. The if condition still needs tweaking? We could still have a non checksummed/checksum threw error .si/.liv/segments_n file of equal length and we wouldn't re-download?\n\nMaybe the condition could be the check you initially proposed -\n1. If file is a .si/.liv/segments_n file download regardless\n2. else if  if (!compareResult.equal || downloadCompleteIndex) then re-download? ",
            "id": "comment-14309769"
        },
        {
            "date": "2015-02-06T19:59:32+0000",
            "author": "Varun Thacker",
            "content": "Sorry this was supposed to go in SOLR-6920. I'll repost it there. ",
            "id": "comment-14309772"
        },
        {
            "date": "2015-02-06T21:44:04+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1657969 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1657969 ]\n\nSOLR-6920, SOLR-6640: A replicated index can end up corrupted when small files end up with the same file name and size. ",
            "id": "comment-14309982"
        },
        {
            "date": "2015-02-07T16:39:05+0000",
            "author": "Mark Miller",
            "content": "Since committing that to trunk, my trunk runs are looking pretty good and I still see some nasty fails on 5x. So far, so good. ",
            "id": "comment-14310808"
        },
        {
            "date": "2015-02-07T17:05:16+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1658078 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1658078 ]\n\nSOLR-6920, SOLR-6640: When we so not have a checksum for a file, always download files under 100kb and other small improvements. ",
            "id": "comment-14310819"
        },
        {
            "date": "2015-02-07T17:38:02+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1658083 from Mark Miller in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1658083 ]\n\nSOLR-6920, SOLR-6640: A replicated index can end up corrupted when small files end up with the same file name and size. ",
            "id": "comment-14310837"
        },
        {
            "date": "2015-02-07T17:46:08+0000",
            "author": "Mark Miller",
            "content": "Anyone that can help with review of these commits, please do. ",
            "id": "comment-14310840"
        },
        {
            "date": "2015-02-07T17:58:14+0000",
            "author": "Mark Miller",
            "content": "We should not log an error when the compare method does not see the file locally - that is an expected path. ",
            "id": "comment-14310846"
        },
        {
            "date": "2015-02-07T18:17:20+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1658089 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1658089 ]\n\nSOLR-6920, SOLR-6640: Do not log an error when a file does not exist for comparison. ",
            "id": "comment-14310855"
        },
        {
            "date": "2015-02-07T18:22:25+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1658090 from Mark Miller in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1658090 ]\n\nSOLR-6920, SOLR-6640: Do not log an error when a file does not exist for comparison. ",
            "id": "comment-14310858"
        },
        {
            "date": "2015-02-07T20:30:27+0000",
            "author": "Gregory Chanan",
            "content": "Did a quick pass, looks good.\n\nOne suggestion I would make is renaming the function filesToAlwaysDownloadIfChecksumFails.  Checksum fails sounds like the checksum didn't match up in which case I'd hope we'd download every file, not that the checksum wasn't present.  Maybe filesToAlwaysDownloadIfNoChecksums? filesToAlwaysDownloadIfNoChecksumsPresent? ",
            "id": "comment-14310917"
        },
        {
            "date": "2015-02-08T00:15:03+0000",
            "author": "Mark Miller",
            "content": "Thanks Greg, I'll incorporate - wasn't very happy with the name myself.\n\nI don't know that it's critical for this blocker issue, but another change we should probably make is: if the replication into an existing index fails, don't keep trying that - move to replicating to a new index folder. ",
            "id": "comment-14311034"
        },
        {
            "date": "2015-02-08T03:46:57+0000",
            "author": "Mark Miller",
            "content": "I've had my local jenkins machine push it's 10 chaos monkey tests on this all day. So far so good in terms of index corruption. I'll add in the rename mentioned above and let jenkins crank overnight - let's look at moving to 5.0 in the morning when we have some more confidence. ",
            "id": "comment-14311091"
        },
        {
            "date": "2015-02-08T03:58:47+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1658129 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1658129 ]\n\nSOLR-6920, SOLR-6640: Rename method to filesToAlwaysDownloadIfNoChecksums. ",
            "id": "comment-14311099"
        },
        {
            "date": "2015-02-08T04:03:53+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1658130 from Mark Miller in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1658130 ]\n\nSOLR-6920, SOLR-6640: Rename method to filesToAlwaysDownloadIfNoChecksums. ",
            "id": "comment-14311102"
        },
        {
            "date": "2015-02-08T06:13:41+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks Mark. This has helped a lot. I wrote a test which would hit corruption at least once in 10 iterations before SOLR-6920 but I can't get it to fail anymore. ",
            "id": "comment-14311134"
        },
        {
            "date": "2015-02-08T15:10:39+0000",
            "author": "Mark Miller",
            "content": "Thanks shalin - my chaosmonkey jobs went well overnight too.\n\nI'll look at the merge into 5.0 shortly.\n\nMore review wouldn't hurt if anyone has time. ",
            "id": "comment-14311358"
        },
        {
            "date": "2015-02-08T18:54:32+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1658240 from Mark Miller in branch 'dev/branches/lucene_solr_5_0'\n[ https://svn.apache.org/r1658240 ]\n\nSOLR-6920, SOLR-6640: A replicated index can end up corrupted when small files end up with the same file name and size. ",
            "id": "comment-14311492"
        },
        {
            "date": "2015-02-08T20:33:25+0000",
            "author": "Anshum Gupta",
            "content": "Thanks for wrapping this one up Mark. Looks good! \n\nI think it'd be good to declare a final int MIN_FILE_SIZE or something to use in filesToAlwaysDownloadIfNoChecksums. Also, the comment says files < 100kb would be copied but the check seems to be for 100,000 bytes or about 97kb? It's ok but I think but we should either change the comment or the number (nothing pressing).\n\nAlso, I think the concat of 2 strings while logging was an IDE issue\nSnapPuller.java\nLOG.warn(\n                \"File {} did not match. \"  + \"expected length is {} and actual length is {}\",\n                filename, backupIndexFileChecksum, indexFileChecksum,\n            \n\n ",
            "id": "comment-14311548"
        },
        {
            "date": "2015-02-08T20:55:36+0000",
            "author": "Mark Miller",
            "content": "< 100kb would be copied but the check seems to be for 100,000 bytes or about 97kb? \n\nI went with Google's conversion using digital storage: 100000 bytes to kb = 100 kilobytes ",
            "id": "comment-14311555"
        },
        {
            "date": "2015-02-08T21:20:58+0000",
            "author": "Anshum Gupta",
            "content": "I think we use the standard 1024 conversion factor everywhere but I think it's ok and probably not worth debating about. ",
            "id": "comment-14311557"
        },
        {
            "date": "2015-02-08T22:40:19+0000",
            "author": "Mark Miller",
            "content": "Yeah, I'm not really arguing one way or another - I think that debate has been had  Eithet way - I simply went with Google is all.  ",
            "id": "comment-14311600"
        },
        {
            "date": "2015-02-08T23:04:43+0000",
            "author": "Anshum Gupta",
            "content": "With you on that anyways. I think this issue is now handled and I can cut an RC later in the day today/tomorrow morning. What do you think? ",
            "id": "comment-14311611"
        },
        {
            "date": "2015-02-08T23:49:05+0000",
            "author": "Mark Miller",
            "content": "Yeah, I think we are prob good. I'll make the improvement suggested on trunk and 5x.  ",
            "id": "comment-14311626"
        },
        {
            "date": "2015-02-09T18:44:43+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1658519 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1658519 ]\n\nSOLR-6920, SOLR-6640: Make constant and fix logging. ",
            "id": "comment-14312607"
        },
        {
            "date": "2015-02-09T18:49:35+0000",
            "author": "Mark Miller",
            "content": "Thanks to all involved in this. Long time, bad time issue. ",
            "id": "comment-14312614"
        },
        {
            "date": "2015-02-09T18:50:58+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1658524 from Mark Miller in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1658524 ]\n\nSOLR-6920, SOLR-6640: Make constant and fix logging. ",
            "id": "comment-14312617"
        },
        {
            "date": "2015-02-09T18:58:39+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1658526 from Mark Miller in branch 'dev/branches/lucene_solr_5_0'\n[ https://svn.apache.org/r1658526 ]\n\nSOLR-6920, SOLR-6640: Make constant and fix logging. ",
            "id": "comment-14312635"
        },
        {
            "date": "2015-02-23T05:01:59+0000",
            "author": "Anshum Gupta",
            "content": "Bulk close after 5.0 release. ",
            "id": "comment-14332804"
        }
    ]
}