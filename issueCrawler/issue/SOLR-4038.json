{
    "id": "SOLR-4038",
    "title": "On OOM, SolrCloud indexing blocks if shard is marked as DOWN",
    "details": {
        "affect_versions": "4.0",
        "status": "Closed",
        "fix_versions": [
            "4.2",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Won't Fix"
    },
    "description": "See: http://lucene.472066.n3.nabble.com/SolrCloud-indexing-blocks-if-node-is-recovering-td4017827.html\n\nWhile indexing (without CloudSolrServer at that time) one node dies with an OOME perhaps  because of the linked issue SOLR-4032. The OOME stack traces are varied but here are some ZK-related logs between the OOME stack traces:\n\n\n2012-11-02 14:14:37,126 INFO [solr.update.UpdateLog] - [RecoveryThread] - : Dropping buffered updates FSUpdateLog{state=BUFFERING, tlog=null}\n2012-11-02 14:14:37,127 ERROR [solr.cloud.RecoveryStrategy] - [RecoveryThread] - : Recovery failed - trying again... (2) core=shard_e\n2012-11-02 14:14:37,127 INFO [solr.cloud.RecoveryStrategy] - [RecoveryThread] - : Wait 8.0 seconds before trying to recover again (3)\n2012-11-02 14:14:45,328 INFO [solr.cloud.ZkController] - [RecoveryThread] - : numShards not found on descriptor - reading it from system property\n2012-11-02 14:14:45,363 INFO [solr.cloud.RecoveryStrategy] - [RecoveryThread] - : Starting Replication Recovery. core=shard_e\n2012-11-02 14:14:45,363 INFO [solrj.impl.HttpClientUtil] - [RecoveryThread] - : Creating new http client, config:maxConnections=128&maxConnectionsPerHost=32&followRedirects=false\n2012-11-02 14:14:45,775 INFO [common.cloud.ZkStateReader] - [main-EventThread] - : A cluster state change has occurred - updating... (10)\n2012-11-02 14:14:50,987 INFO [solr.cloud.RecoveryStrategy] - [RecoveryThread] - : Begin buffering updates. core=shard_e\n2012-11-02 14:14:50,987 INFO [solr.update.UpdateLog] - [RecoveryThread] - : Starting to buffer updates. FSUpdateLog{state=ACTIVE, tlog=null}\n2012-11-02 14:14:50,987 INFO [solr.cloud.RecoveryStrategy] - [RecoveryThread] - : Attempting to replicate from http://rot05.solrserver:8080/solr/shard_e/. core=shard_e\n2012-11-02 14:14:50,987 INFO [solrj.impl.HttpClientUtil] - [RecoveryThread] - : Creating new http client, config:maxConnections=128&maxConnectionsPerHost=32&followRedirects=false\n2012-11-02 14:15:03,303 INFO [solr.core.CachingDirectoryFactory] - [RecoveryThread] - : Releasing directory:/opt/solr/cores/shard_f/data/index\n2012-11-02 14:15:03,303 INFO [solr.handler.SnapPuller] - [RecoveryThread] - : removing temporary index download directory files NRTCachingDirectory(org.apache.lucene.store.MMapDirectory@/opt/solr/cores/shard_f/data/index.20121102141424591 lockFactory=org.apache.lucene.store.SimpleFSLockFactory@1520a48c; maxCacheMB=48.0 maxMergeSizeMB=4.0)\n2012-11-02 14:15:09,421 INFO [apache.zookeeper.ClientCnxn] - [main-SendThread(rot1.zkserver:2181)] - : Client session timed out, have not heard from server in 11873ms for sessionid 0x13abc504486000f, closing socket connection and attempting reconnect\n2012-11-02 14:15:09,422 ERROR [solr.core.SolrCore] - [http-8080-exec-1] - : org.apache.solr.common.SolrException: Ping query caused exception: Java heap space\n.....\n.....\n2012-11-02 14:15:09,867 INFO [common.cloud.ConnectionManager] - [main-EventThread] - : Watcher org.apache.solr.common.cloud.ConnectionManager@305e7020 name:ZooKeeperConnection Watcher:rot1.zkserver:2181,rot2.zkserver:2181 got event WatchedEvent state:Disconnected type:None path:null path:null type:None\n2012-11-02 14:15:09,867 INFO [common.cloud.ConnectionManager] - [main-EventThread] - : zkClient has disconnected\n2012-11-02 14:15:09,869 ERROR [solr.cloud.RecoveryStrategy] - [RecoveryThread] - : Error while trying to recover:java.lang.OutOfMemoryError: Java heap space\n.....\n.....\n2012-11-02 14:15:10,159 INFO [solr.update.UpdateLog] - [RecoveryThread] - : Dropping buffered updates FSUpdateLog{state=BUFFERING, tlog=null}\n2012-11-02 14:15:10,159 ERROR [solr.cloud.RecoveryStrategy] - [RecoveryThread] - : Recovery failed - trying again... (3) core=shard_e\n2012-11-02 14:15:10,159 INFO [solr.cloud.RecoveryStrategy] - [RecoveryThread] - : Wait 16.0 seconds before trying to recover again (4)\n2012-11-02 14:15:09,878 INFO [solr.core.CachingDirectoryFactory] - [RecoveryThread] - : Releasing directory:/opt/solr/cores/shard_f/data/index.20121102141424591\n2012-11-02 14:15:10,192 INFO [solr.core.CachingDirectoryFactory] - [RecoveryThread] - : Releasing directory:/opt/solr/cores/shard_f_f/data/index\n2012-11-02 14:15:10,192 ERROR [solr.handler.ReplicationHandler] - [RecoveryThread] - : SnapPull failed :org.apache.solr.common.SolrException: Unable to download _773.tvf completely. Downloaded 246\n415360!=562327645\n.....\n.....\n\n \n\nAt this point indexing has already been blocked. Some nodes do not write anything to the logs and the two surrounding nodes are still busy doing some replication. Most nodes show a increased number of state changes:\n\n\n2012-11-02 14:16:47,768 INFO [common.cloud.ZkStateReader] - [main-EventThread] - : A cluster state change has occurred - updating... (10)\n\n\n\nIt's about 10 minutes of doing little by trying replication by the surrounding nodes and mostly state changes for the other nodes before are documents indexed again. At that point i stopped the node.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Markus Jelsma",
            "id": "comment-13543964",
            "date": "2013-01-04T15:46:46+0000",
            "content": "I checked if i could reproduce this behaviour after two months and many commits later and it is still here. However, it is not blocked when the shards on the node are recovering but when they are marked as down.\n\nWe ungracefully killed one machine holding two shards while indexing from Hadoop and using CloudSolrServer, the shards are now marked as GONE. After a while we started the node again and it became marked as DOWN. During this state the cluster did not accept incoming updates. Only after a minute orso the node finally started recovery on both shards and indexing resumed. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13546532",
            "date": "2013-01-08T02:18:53+0000",
            "content": "On first glance, I almost think the best solution is to setup the java startup command where you can specify what to run when you get an OOM to do a kill on the process. That's usually the best thing to do for OOM's IMO. It's in an undefined state when you get an OOM, and a node likely could look like it was up to other nodes but act a bit like really sticky honey... "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13546819",
            "date": "2013-01-08T12:04:51+0000",
            "content": "I agree the servlet container should be killed instead of keeping it running but that would not actually solve the problem right? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13546868",
            "date": "2013-01-08T13:14:24+0000",
            "content": "why wouldnt it? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13546871",
            "date": "2013-01-08T13:19:05+0000",
            "content": "Because i think that regardless of the state of a single node the cluster should continue to operate. What if a node freezes up because of a JVM bug or other very bad situation? I'm fine if we close this issue if it's just a problem of OOM. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13546874",
            "date": "2013-01-08T13:24:49+0000",
            "content": "If a node freezes up or OOM's I think you are in trouble. It needs to be killed. Luckily, you can take care of this pretty easily in the OOM case - in the general freeze case, there is less you can do - hopefully that just means the zk connection will be closed though - hard to imagine it not being closed unless it's a really weird bug.\n\nIn the OOM case, things are just not good. The node will still be somewhat operating, might hold connections open, but not respond well. Once you OOM the jvm is in an undefined state - you don't know what it will do. There is not a lot we can do about that easily - I think the best thing you can do is set up your jvms to be 'kill'd when an OOM happens. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13546876",
            "date": "2013-01-08T13:27:43+0000",
            "content": "I'd have -XX:OnOutOfMemoryError= kick off commands to log the problem, kill the process and start it again. Or if you have the process supervised, you just kill it and let it start up again. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13546878",
            "date": "2013-01-08T13:29:35+0000",
            "content": "Alright, i think we can close this as a no problem then. I'll revisit this if it really turns out the cluster is blocking if one node is marked as DOWN regardless of being OOM or frozen up  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13546883",
            "date": "2013-01-08T13:37:18+0000",
            "content": "If there is a problem that doesn't involve an OOM, I'll certainly try to fix it. And if someone has a good idea about how to improve the situation with the OOM, I'm all ears. I just think it's problematic. We can leave this open for a bit and see if someone else chimes in. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13554397",
            "date": "2013-01-15T21:44:37+0000",
            "content": "Mark, can this issue be resolved now? "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13654314",
            "date": "2013-05-10T10:34:45+0000",
            "content": "Closed after release. "
        }
    ]
}