{
    "id": "LUCENE-3425",
    "title": "NRT Caching Dir to allow for exact memory usage, better buffer allocation and \"global\" cross indices control",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "affect_versions": "3.4,                                            4.0-ALPHA",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "A discussion on IRC raised several improvements that can be made to NRT caching dir. Some of the problems it currently has are:\n\n1. Not explicitly controlling the memory usage, which can result in overusing memory (for example, large new segments being committed because refreshing is too far behind).\n2. Heap fragmentation because of constant allocation of (probably promoted to old gen) byte buffers.\n3. Not being able to control the memory usage across indices for multi index usage within a single JVM.\n\nA suggested solution (which still needs to be ironed out) is to have a BufferAllocator that controls allocation of byte[], and allow to return unused byte[] to it. It will have a cap on the size of memory it allows to be allocated.\n\nThe NRT caching dir will use the allocator, which can either be provided (for usage across several indices) or created internally. The caching dir will also create a wrapped IndexOutput, that will flush to the main dir if the allocator can no longer provide byte[] (exhausted).\n\nWhen a file is \"flushed\" from the cache to the main directory, it will return all the currently allocated byte[] to the BufferAllocator to be reused by other \"files\".",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2011-09-09T21:12:39+0000",
            "content": "Also, a quick win on trunk is to use IOCtx's FlushInfo.estimatedSegmentSize to decide up front whether to try caching or not.\n\nIe if the to-be-flushed segment is too large we should not cache it. ",
            "author": "Michael McCandless",
            "id": "comment-13101567"
        },
        {
            "date": "2011-09-13T16:15:33+0000",
            "content": "Actually, NRTCachingDir does explicitly control the RAM usage in that\nif its cache is using too much RAM then the next createOutput will go\nstraight to disk.\n\nThe one thing it does not do is evict the created files after they\nclose.  So, if you flush a big segment in IW, then NRTCachingDir will\nkeep those files in RAM even though its now over-budget.  (But the\nnext segment to flush will go straight to disk).\n\nI think this isn't that big a problem in practice; ie, as long as you\nset your IW RAM buffer to something not too large, or you ensure you\nare opening a new NRT reader often enough that the accumulated docs\nwon't create a very large segment, then the excess RAM used by\nNRTCachingDir will be bounded.\n\nStill it would be nice to fix it so it evicts the files that set it\nover, such that it's always below the budget once the outputs is\nclosed.  And I agree we should make it possible to have a single pool\nfor accounting purposes, so you can share this pool across multiple\nNRTCachingDirs (and other things that use RAM). ",
            "author": "Michael McCandless",
            "id": "comment-13103723"
        },
        {
            "date": "2012-03-21T18:14:22+0000",
            "content": "Bulk of fixVersion=3.6 -> fixVersion=4.0 for issues that have no assignee and have not been updated recently.\n\nemail notification suppressed to prevent mass-spam\npsuedo-unique token identifying these issues: hoss20120321nofix36 ",
            "author": "Hoss Man",
            "id": "comment-13234770"
        },
        {
            "date": "2013-07-23T18:44:40+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13717018"
        },
        {
            "date": "2013-09-12T06:56:03+0000",
            "content": "Actually, in my case, using NRTCachingDir always causes OutOfMemory error!\nafter it happened, we analysed a gc.hprof file by Eclipse MAT, we believe that:\ncore segment reader will hold a big size of RAMFile instance that created by NRTCachingDir.createOutput,\nif core segment reader not merged or closed, those RAMFile instances will use more and more buffer, \nbut never have chance to uncache from NRTCachingDir instance,\nso, when all segment reader allocated buffer size is bigger than we have, it will causes OutOfMemory error! ",
            "author": "caviler",
            "id": "comment-13765217"
        },
        {
            "date": "2013-09-12T11:07:21+0000",
            "content": "It's true that open SegmentReaders will hold onto those RAMFiles.\n\nHowever, those segments should be the smallish ones and they should be merged away.\n\nIt's odd that in your case you see them lasting so long.\n\nCan you give more details about how you're using it?  Have you changed the default MergePolicy/settings?  Are you closing old readers after reopening new ones? ",
            "author": "Michael McCandless",
            "id": "comment-13765336"
        },
        {
            "date": "2013-09-13T02:01:51+0000",
            "content": "Yes, i changed the default MergePolicy to AverageMergePolicy, i closed old reader after reopenIfChanged,\n\nbefore upgrade to Lucene 4.4.0, we use this AverageMergePolicy is work well, recently we upgrade to Lucene 4.4.0 and modified this AverageMergePolicy to adapt version 4.4.0, after this, in testing(in very short time we add a lot of documents), we noticed the small segments is so many(more than 40,000), and those small segments seems not have chance to be merged?\n\nWhat difference between version 3.6.2 and 4.4.0 behavior in merge process???\n\n\nour index files about 72G, split to 148 segment by AverageMergePolicy, every segment about 500M size.\n\n\n\npublic class AverageMergePolicy extends MergePolicy\n{\n    /**\n     * Default noCFSRatio. If a merge's size is >= 10% of the index, then we\n     * disable compound file for it.\n     * \n     * @see #setNoCFSRatio\n     */\n    public static final double DEFAULT_NO_CFS_RATIO = 0.1;\n\n    private long               maxSegmentSizeMB     = 100L;                // \n\n    protected double           noCFSRatio           = DEFAULT_NO_CFS_RATIO;\n\n    private boolean            partialExpunge       = false;               // \n\n    protected boolean          useCompoundFile      = true;\n\n    public AverageMergePolicy()\n    {\n    }\n\n    @Override\n    public void close()\n    {\n    }\n\n    @Override\n    public MergeSpecification findForcedDeletesMerges(final SegmentInfos infos) throws CorruptIndexException,\n            IOException\n    {\n        try\n        {\n\n            // \n            SegmentInfoPerCommit best = null;\n\n            final int numSegs = infos.size();\n            for (int i = 0; i < numSegs; i++)\n            {\n                final SegmentInfoPerCommit info = infos.info(i);\n                if (info.hasDeletions())\n                {\n                    if (null == best || info.getDelCount() > best.getDelCount())\n                    {\n                        best = info;\n                    }\n                }\n            }\n\n            final Collection<SegmentInfoPerCommit> mergingSegments = writer.get().getMergingSegments();\n\n            if (mergingSegments.contains(best))\n            {\n                return null; // skip merging segment\n            }\n\n            final MergeSpecification spec = new MergeSpecification();\n            if (null != best)\n            {\n                spec.add(new OneMerge(Collections.singletonList(best)));\n            }\n\n            return spec;\n        }\n        catch (final Throwable e)\n        {\n            e.printStackTrace();\n            return null;\n        }\n    }\n\n    @Override\n    public MergeSpecification findForcedMerges(final SegmentInfos infos,\n                                               final int maxNumSegments,\n                                               final Map<SegmentInfoPerCommit, Boolean> segmentsToMerge)\n            throws IOException\n    {\n        return findMerges(MergeTrigger.EXPLICIT, infos);\n    }\n\n    @Override\n    public MergeSpecification findMerges(final MergeTrigger mergeTrigger, final SegmentInfos infos) throws IOException\n    {\n        // partialExpunge = false; //\n        // partialExpunge = true;\n\n        final long maxSegSize = maxSegmentSizeMB * 1024L * 1024L; // \n        long bestSegSize = maxSegSize; // \n\n        try\n        {\n            final int numSegs = infos.size();\n\n            int numBestSegs = numSegs;\n            {\n                // \n                SegmentInfoPerCommit info;\n                long totalSegSize = 0;\n\n                // compute the total size of segments\n                for (int i = 0; i < numSegs; i++)\n                {\n                    info = infos.info(i);\n                    final long size = size(info);\n                    totalSegSize += size;\n                }\n\n                numBestSegs = (int) ((totalSegSize + bestSegSize - 1) / bestSegSize); // \n\n                bestSegSize = (numBestSegs == 0) ? totalSegSize : (totalSegSize + maxSegSize - 1) / numBestSegs; // \n                if (bestSegSize > maxSegSize)\n                {\n                    bestSegSize = maxSegSize; // \n                    numBestSegs = (int) ((totalSegSize + maxSegSize - 1) / bestSegSize);\n                }\n            }\n\n            MergeSpecification spec = findOneMerge(infos, bestSegSize);\n\n            //int branch = 0;\n            if (null == spec && partialExpunge)\n            {\n                //branch = 1;\n                // \n                final OneMerge expunge = findOneSegmentToExpunge(infos, 0);\n                if (expunge != null)\n                {\n                    spec = new MergeSpecification();\n                    spec.add(expunge);\n                }\n            }\n\n            // MergeLogger.collect(branch, spec);\n\n            Application.sleep(100); // \n\n            return spec;\n        }\n        catch (final Throwable e)\n        {\n            e.printStackTrace();\n            return null;\n        }\n    }\n\n    /**\n     * \n     * \n     * \n     * \n     * @param sizes\n     * @param bestSegSize\n     *            \n     * @return\n     */\n    private int[] findOneMerge(final long[] sizes, final long bestSegSize)\n    {\n        final int[] merge = findSmallestOneMerge(sizes, bestSegSize);\n\n        if (null != merge)\n        {\n            return merge;\n        }\n\n        final int n = sizes.length;\n        for (int i = 0; i < n; i++)\n        {\n            final long size1 = sizes[i];\n\n            if (size1 < bestSegSize) // \n            {\n                // \n                for (int j = 0; j < n; j++)\n                {\n                    if (i != j)\n                    {\n                        final long size2 = sizes[j];\n\n                        if (size1 + size2 <= bestSegSize) // \n                        {\n                            return new int[]\n                            { i, j };\n                        }\n                    }\n                }\n            }\n        }\n\n        return null;\n    }\n\n    /**\n     * \n     */\n    private MergeSpecification findOneMerge(final SegmentInfos infos, final long bestSegSize) throws IOException\n    {\n        final int n = infos.size();\n        final long[] sizes = new long[n];\n\n        // \n        {\n            SegmentInfoPerCommit info;\n            for (int i = 0; i < n; i++)\n            {\n                info = infos.info(i);\n                sizes[i] = size(info);\n            }\n        }\n\n        final int[] pair = findOneMerge(sizes, bestSegSize);\n\n        if (null == pair)\n        {\n            return null; // \n        }\n\n        final int target1 = pair[0];\n        final int target2 = pair[1];\n\n        final SegmentInfoPerCommit info1 = infos.info(target1);\n        final SegmentInfoPerCommit info2 = infos.info(target2);\n\n        final Collection<SegmentInfoPerCommit> mergingSegments = writer.get().getMergingSegments();\n\n        if (mergingSegments.contains(info1))\n        {\n            return null; // skip merging segment\n        }\n\n        if (mergingSegments.contains(info2))\n        {\n            return null; // skip merging segment\n        }\n\n        //    MergeLogger.debug(\"findOneMerge info1 = \" + info1.name + \" \" + SizeUtil.normalizeSizeString(sizes[target1]));\n        //    MergeLogger.debug(\"findOneMerge info2 = \" + info2.name + \" \" + SizeUtil.normalizeSizeString(sizes[target2]));\n\n        final List<SegmentInfoPerCommit> mergeInfos = new ArrayList<SegmentInfoPerCommit>(2);\n        mergeInfos.add(info1);\n        mergeInfos.add(info2);\n\n        final MergeSpecification spec = new MergeSpecification();\n\n        spec.add(new OneMerge(mergeInfos));\n\n        return spec;\n    }\n\n    /**\n     * \n     * \n     * @param infos\n     * @param maxNumSegments\n     * @return\n     * @throws IOException\n     */\n    private OneMerge findOneSegmentToExpunge(final SegmentInfos infos, final int maxNumSegments) throws IOException\n    {\n        int expungeCandidate = -1;\n        int maxDelCount = 0;\n\n        for (int i = maxNumSegments - 1; i >= 0; i--)\n        {\n            final SegmentInfoPerCommit info = infos.info(i);\n            final int delCount = info.getDelCount();\n            if (delCount > maxDelCount)\n            {\n                expungeCandidate = i;\n                maxDelCount = delCount;\n            }\n        }\n\n        if (maxDelCount > 0)\n        {\n            return new OneMerge(Collections.singletonList(infos.info(expungeCandidate)));\n        }\n\n        return null;\n    }\n\n    /**\n     * \n     * \n     * @param sizes\n     * @param bestSegSize\n     * @return\n     */\n    private int[] findSmallestOneMerge(final long[] sizes, final long bestSegSize)\n    {\n        long targetSize = -1;\n        int targetIndex = -1;\n        final int n = sizes.length;\n\n        final int skip = 0; // \n\n        // \n        for (int i = n - skip - 1; i >= 0; i--) // \n        {\n            final long size1 = sizes[i];\n            if (size1 < bestSegSize) // \n            {\n                if (-1 == targetSize || size1 < targetSize)\n                {\n                    targetSize = size1;\n                    targetIndex = i;\n                }\n            }\n        }\n\n        if (-1 != targetIndex)\n        {\n            for (int j = n - skip - 1; j >= 0; j--)//\n            {\n                if (targetIndex != j)\n                {\n                    final long size2 = sizes[j];\n\n                    if (targetSize + size2 <= bestSegSize) // \n                    {\n                        return new int[]\n                        { targetIndex, j };\n                    }\n                }\n            }\n        }\n\n        return null;\n    }\n\n    public long getMaxSegmentSizeMB()\n    {\n        return maxSegmentSizeMB;\n    }\n\n    //    /** @see #setNoCFSRatio */\n    //    @Override\n    //    public double getNoCFSRatio()\n    //    {\n    //        return noCFSRatio;\n    //    }\n\n    public boolean getPartialExpunge()\n    {\n        return partialExpunge;\n    }\n\n    /**\n     * Returns true if newly flushed and newly merge segments are written in\n     * compound file format. @see #setUseCompoundFile\n     */\n    public boolean getUseCompoundFile()\n    {\n        return useCompoundFile;\n    }\n\n    public void setMaxSegmentSizeMB(final long maxSegmentSizeMB)\n    {\n        this.maxSegmentSizeMB = maxSegmentSizeMB;\n    }\n\n    //    /**\n    //     * If a merged segment will be more than this percentage of the total size\n    //     * of the index, leave the segment as non-compound file even if compound\n    //     * file is enabled. Set to 1.0 to always use CFS regardless of merge size.\n    //     */\n    //    @Override\n    //    public void setNoCFSRatio(final double noCFSRatio)\n    //    {\n    //        if (noCFSRatio < 0.0 || noCFSRatio > 1.0)\n    //        {\n    //            throw new IllegalArgumentException(\"noCFSRatio must be 0.0 to 1.0 inclusive; got \" + noCFSRatio);\n    //        }\n    //        this.noCFSRatio = noCFSRatio;\n    //    }\n\n    public void setPartialExpunge(final boolean partialExpunge)\n    {\n        this.partialExpunge = partialExpunge;\n    }\n\n    /**\n     * Sets whether compound file format should be used for newly flushed and\n     * newly merged segments.\n     */\n    public void setUseCompoundFile(final boolean useCompoundFile)\n    {\n        this.useCompoundFile = useCompoundFile;\n    }\n\n    // Javadoc inherited\n    @Override\n    public boolean useCompoundFile(final SegmentInfos infos, final SegmentInfoPerCommit mergedInfo) throws IOException\n    {\n        final boolean doCFS;\n\n        if (!useCompoundFile)\n        {\n            doCFS = false;\n        }\n        else if (noCFSRatio == 1.0)\n        {\n            doCFS = true;\n        }\n        else\n        {\n            long totalSize = 0;\n            for (final SegmentInfoPerCommit info : infos)\n            {\n                totalSize += size(info);\n            }\n\n            doCFS = size(mergedInfo) <= noCFSRatio * totalSize;\n        }\n        return doCFS;\n    }\n}\n\n ",
            "author": "caviler",
            "id": "comment-13766169"
        },
        {
            "date": "2013-09-13T11:10:04+0000",
            "content": "Hmm, I don't understand what AverageMergePolicy is doing?  Can you describe its purpose at a high level?  Somehow it's failing to merge those 40,000 small segments?\n\nAnd offhand I don't know what changed between 4.3 and 4.4 that would cause AverageMergePolicy to stop merging small segments.\n\nMaybe turn on IndexWriter's infoStream and watch which merges are being selected. ",
            "author": "Michael McCandless",
            "id": "comment-13766406"
        },
        {
            "date": "2013-09-13T12:19:33+0000",
            "content": "Because inside IndexSearcher.search method, it use thread pool to execute Segment.search in every segments,\n\nso, if some segment is too big,  it will causes this big segment's searcher too slow, eventually entire search method will too slow.\n\nexample, we have 2G index files.\n\n=========================================\nuse AverageMergePolicy, IndexSearcher.search spent time = 1s\n\nsegment size        segment.search spent time\n  1         500M                          1s\n  2         500M                          1s\n  3         500M                          1s\n  4         500M                          1s\n\n=========================================\nuse other MergePolicy, IndexSearcher.search spent time = 5s\nsegment size   segment.search spent time\n  1         2000M                        5s\n\n=========================================\n\nWhy not use LogByteSizeMergePolicy but AverageMergePolicy?\n\nBecause:\n1. I want every semgent as small as possible!\n2. I want semgent as more as possible!\n\nwe don't known how big of one segment size when entire index is growing, so can't use LogByteSizeMergePolicy.\n\nif use LogByteSizeMergePolicy and setMaxMergeMB(200M):\n\nindex =   200M, LogByteSizeMergePolicy =   1 segment(per 200M),  AverageMergePolicy = 4 segments(per 50M) \nindex = 2000M, LogByteSizeMergePolicy = 10 segment(per 200M),  AverageMergePolicy = 4 segments(per 500M) \n\n\n\n\n\n ",
            "author": "caviler",
            "id": "comment-13766430"
        },
        {
            "date": "2013-09-15T11:02:56+0000",
            "content": "OK, thanks for the explanation; now I understand AverageMergePolicy's purpose, and it makes sense.  It's ironic that a fully \"optimized\" index is the worst thing you could do when searching segments concurrently ...\n\nBut, I still don't understand why AverageMergePolicy is not merging the little segments from NRTCachingDir.  Do you tell it to target a maximum number of segments in the index?  If so, once the index is large enough, it seems like that'd force the small segments to be merged.  Maybe, you could also tell it a minimum size of the segments, so that it would merge away any segments still held in NRTCachingDir? ",
            "author": "Michael McCandless",
            "id": "comment-13767770"
        },
        {
            "date": "2014-04-16T12:54:23+0000",
            "content": "Move issue to Lucene 4.9. ",
            "author": "Uwe Schindler",
            "id": "comment-13970739"
        }
    ]
}