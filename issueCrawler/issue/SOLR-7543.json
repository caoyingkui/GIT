{
    "id": "SOLR-7543",
    "title": "Create GraphQuery that allows graph traversal as a query operator.",
    "details": {
        "components": [
            "search"
        ],
        "type": "New Feature",
        "labels": "",
        "fix_versions": [
            "6.0"
        ],
        "affect_versions": "None",
        "status": "Resolved",
        "resolution": "Fixed",
        "priority": "Minor"
    },
    "description": "I have a GraphQuery that I implemented a long time back that allows a user to specify a \"startQuery\" to identify which documents to start graph traversal from.  It then gathers up the edge ids for those documents , optionally applies an additional filter.  The query is then re-executed continually until no new edge ids are identified.  I am currently hosting this code up at https://github.com/kwatters/solrgraph and I would like to work with the community to get some feedback and ultimately get it committed back in as a lucene query.\n\nHere's a bit more of a description of the parameters for the query / graph traversal:\n\nq - the initial start query that identifies the universe of documents to start traversal from.\nfromField - the field name that contains the node id\ntoField - the name of the field that contains the edge id(s).\ntraversalFilter - this is an additional query that can be supplied to limit the scope of graph traversal to just the edges that satisfy the traversalFilter query.\nmaxDepth - integer specifying how deep the breadth first search should go.\nreturnStartNodes - boolean to determine if the documents that matched the original \"q\" should be returned as part of the graph.\nonlyLeafNodes - boolean that filters the graph query to only return documents/nodes that have no edges.\n\nWe identify a set of documents with \"q\" as any arbitrary lucene query.  It will collect the values in the fromField, create an OR query with those values , optionally apply an additional constraint from the \"traversalFilter\" and walk the result set until no new edges are detected.  Traversal can also be stopped at N hops away as defined with the maxDepth.  This is a BFS (Breadth First Search) algorithm.  Cycle detection is done by not revisiting the same document for edge extraction.  \n\nThis query operator does not keep track of how you arrived at the document, but only that the traversal did arrive at the document.",
    "attachments": {
        "SOLR-7543.patch": "https://issues.apache.org/jira/secure/attachment/12765007/SOLR-7543.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-05-14T16:22:08+0000",
            "author": "Yonik Seeley",
            "content": "Cool stuff!\nLets just focus on the interface/semantics of the qparser - the rest is implementation detail and can be further optimized later.\n\nSince this sort of looks like a multi-step join, perhaps use the same convention of using the primary value (or v) as the start query?\n\n{!graph fromField=f1 toField=f2 ...}\nmy_start_query\nWhich is equivalent to \n{!graph fromField=f1 toField=f2 ... v=my_start_query}\n\nActually, looking at join again, it uses \"from\" and \"to\" - should we use those or stick with \"fromField\" and \"toField\"? ",
            "id": "comment-14543949"
        },
        {
            "date": "2015-05-14T16:43:56+0000",
            "author": "Kevin Watters",
            "content": "Hi Yonik,  thanks for chiming in!  Yup, you can think of this as a multi-step join.  In fact,  I use the graph operator with a maxDepth of 1 to implement an inner join.\nI like things to be consistent (it's easier for others to grok that way), we can rename the fromField and the toField to be \"from\" and \"to\". When it comes to the GraphQueryParser(Plugin),  I'm open to whatever the community likes and whatever is consistent with the other parsers out there. \n (I've always been a bit thrown by the !parser_name syntax, which is why I also have a client side object model so that I programmatically build up an expression, I serialize that expression over to my custom parser that deserializes and converts into the appropriate lucene query objects.  ).  I suppose I just want to make sure that the  \"v=my_start_query\"  can be any arbitrary lucene query.   \nI also still need to work up some richer examples and test cases as part of this ticket. ",
            "id": "comment-14543982"
        },
        {
            "date": "2015-05-14T16:52:47+0000",
            "author": "Yonik Seeley",
            "content": "I've always been a bit thrown by the !parser_name syntax\n\nYeah, but I think doing it like all the other qparsers is best for now... and then we can add support for JSON along with the rest of them later (there's a ticket open... just haven't gotten around to it yet).\n\nI suppose I just want to make sure that the \"v=my_start_query\" can be any arbitrary lucene query. \n\nYep, just use the qparser framework and then the query (or queries) can be of any type parseable by another qparser (including another graph qparser for instance).\n\n\n        baseParser = subQuery(localParams.get(QueryParsing.V), null);\n        Query q = baseParser.getQuery();\n\n ",
            "id": "comment-14544003"
        },
        {
            "date": "2015-05-15T01:14:04+0000",
            "author": "Dennis Gove",
            "content": "For interface/semantics, I think this might be able to benefit from the Expression stuff recently added for streams (SOLR-7377). With that, you could do something like\n\n\ngraph(root=search(collection1, q=\"<some query>\", fl=\"<used fields>\"), traverse=search(collection1, q=\"<some dynamic query>\",fl=\"<used fields>\"), on=\"parent.field=child.field\", maxDepth=5, returnRoot=true, returnOnlyLeaf=false)\n\n\n\nThis would also allow you to do other things like make use of stream merging, uniquing, etc....\n\nWould even allow for tree traversal across multiple collections. ",
            "id": "comment-14544713"
        },
        {
            "date": "2015-05-15T14:07:42+0000",
            "author": "Kevin Watters",
            "content": "Interesting Dennis,  I wasn't aware of SOLR-7377,  I'll have to take a bit more time to understand what that means in the context of the graph query.  I'm not sure how cross collection graph traversal will play with my implementation.  Issue is, my lucene graph query currently is local to a single shard/core.  I have been chatting with Joel Bernstein about the distributed graph traversal use case, and I think there is a play for streaming aggregation there.  There is one line that needs to be coordinated/synchronized across the cluster to do the distributed graph traversal, I think that's where the streaming stuff comes in.  \nI like the idea of renaming   \"returnStartNodes\" to \"returnRoot\" ... less words and hopefully more descriptive of what is happening, (same for returnOnlyLeaf  .. )  maybe the word \"nodes\" is redundant, and it obscures that it's really just a \"document\" at the end of the day. ",
            "id": "comment-14545546"
        },
        {
            "date": "2015-05-15T14:47:30+0000",
            "author": "Dennis Gove",
            "content": "This might be crazy, but would allow a little more flexibility on what to return\n\nreturn=\"Root | Leaf\"  => would return documents that are either in the root, or a leaf.\nreturn=\"Root & Leaf\" => would return documents that are in root and are leafs themselves (no children)\nreturn=\"Leaf | Children>4\" => would return documents that are leaf or have more than 4 children. ",
            "id": "comment-14545604"
        },
        {
            "date": "2015-05-15T17:43:06+0000",
            "author": "Per Steffensen",
            "content": "Sounds interesting. I can't help thinking that this will help users do only one particular graph'ish search. But there are millions of other graph'ish searches one might want to do. The solution here might be too specific. A while back I wrote a Solr indexing-backend for the Titan graph database. We can do a storage-backend as well. Putting a full blown graph database on top of Solr (by supporting indexing- and potentially storage-backends for e.g. Titan) might be the way to go instead, so that we will not end up with lots and lots of very specific graph-search query-parsers/resolvers. And this way you will get all the other cool stuff from a full blown graph database - e.g. I liked playing with Titans REPL. Just a thought ",
            "id": "comment-14545858"
        },
        {
            "date": "2015-05-15T19:17:21+0000",
            "author": "Kevin Watters",
            "content": "Per Steffensen  My mantra here is relates to something I heard once.  \"A graph is a filter on top of your data.\"-someone  So, I'm offering this implementation up to solve that use case.  Analytics on top of that graph would be acheived via faceting or streaming aggregation.  Maybe there's something that Titan could leverage from this implementation?  There are some starting plans on doing a distributed version of this query operator.  \n\nDennis Gove Interesting syntax.  The usecase of children > 4 isn't currently supported in my impl.  My impl doesn't have any history of the paths through the graph.  It only has the bitset that represents the matched documents.  I wanted to keep it as lean as possible.  We could start keeping around additional data structures during the traversal to count, but that can get very expensive very quickly.  My goal/desire here is to keep the memory usage to one bitset. ",
            "id": "comment-14546031"
        },
        {
            "date": "2015-05-15T19:37:59+0000",
            "author": "Dennis Gove",
            "content": "I'm with you on wanting to keep the memory usage as low as possible - I thought maybe you had that info hanging around already. In either case, I think this syntax might lower the bar to entry for usage, especially if people are already using streaming aggregation for other things.  ",
            "id": "comment-14546062"
        },
        {
            "date": "2015-05-18T07:35:04+0000",
            "author": "Per Steffensen",
            "content": "\"A graph is a filter on top of your data.\"\n\nYes I completely agree!\n\nIt is just that, as far a I understand the discussion here, it is not just the very simple basics for doing graphs on top of Solr. Seems like you design for a specific \"query\" that you can do with a graph database: start at some docs --> recursively follow links/edges (up to) maxDepth times --> then return to me all nodes/vertices you came across (or just the leaves). This is one particular graph traversal you can do, but if you just implemented the graph-basics as a Titan search-backend you could perform all kinds of graph \"queries\" with the Titan search-tool Gremlin (https://github.com/tinkerpop/gremlin/wiki) - that is more like a full graph-solution on top of Solr.\n\nI just realized from the Titan main-page (http://thinkaurelius.github.io/titan/) that there is now some kind of integration with Solr. There wasnt when I did my implementation, but never mind, I mainly did it to understand how Titan works.\n\nIt is not that I do not like this issue SOLR-7543. I definitely would love to see graphs on top of Solr. I am just trying to take a step back or and look at a more generic approach to all this graph stuff ",
            "id": "comment-14547645"
        },
        {
            "date": "2015-05-19T01:27:36+0000",
            "author": "Kevin Watters",
            "content": "Hi Per Steffensen I agree we want to do all sorts of great types of graph queries.  Problem is, as soon as you take the step to maintain metadata about the graph traversal, the memory requirements for such an operation can be huge.\n\nThe way I see it is there are likely 3 things to do to close the gap:\n\n\tMake the traversalFilter a more complex datastructure (like an array), to allow different filters at different graph traversal levels.\n\taccumulate a \"weight\" field on the traversed edges as part of the relevancy score (currently no ranking is done)\n\tmaintain the history of edges that traverse into a node.\n\n\n\nAll of these could be considered for future functionality, but it would really take some re-thinking of how it all works.  For now, having the functionality to apply the graph as a filter to the result set is the goal.\n\nIn many cases, if you nest these graph queries, and the documents are structured properly, you should still be able to achieve the result that you desire, but we'd have to take that on a case by case basis. ",
            "id": "comment-14549615"
        },
        {
            "date": "2015-05-19T01:54:05+0000",
            "author": "Kevin Watters",
            "content": "Ok, my initial graph query parser is handling the following syntax\n\n{!graph from=\"node_field\" to=\"edge_field\" returnRoot=\"true\" returnOnlyLeaf=\"false\" maxDepth=-1 traversalFilter=foo:bar}\nid:doc_8\n\nThe above would start traversal at doc_8 and only walk nodes that have a field foo containing the value bar.  This seems to be (more) consistent with the rest of the query parsers. ",
            "id": "comment-14549652"
        },
        {
            "date": "2015-05-19T08:27:54+0000",
            "author": "Joel Bernstein",
            "content": "Thanks for the nice contribution Kevin Watters!\n\nA couple of thoughts on the discussion particularly on general VS specific use case. I think this ticket covers a useful specific usecase, particularly for access control. And I suspect people will find other interesting uses for this type of graph query. It works for non-distributed graph traversals which is where we can do all kinds of low level things to improve performance. \n\nBut it's also an opportunity to open the discussion about the generic use case which will be distributed graph queries. Per Steffensen mentions a Titan integration which would be very useful. But also as [~dgove1] mentions Streaming Expressions provides us with an elegant framework for all kinds of parallel computing tasks. I think it's worth exploring how to model parallel graph joins using the Streaming Expression language and the Streaming API. ",
            "id": "comment-14550031"
        },
        {
            "date": "2015-05-19T13:34:05+0000",
            "author": "Yonik Seeley",
            "content": "Syntax looks good Kevin, having an initial simple syntax certainly doesn't preclude adding other syntaxes (or integrations) later.  Can you put up a patch? ",
            "id": "comment-14550438"
        },
        {
            "date": "2015-05-19T14:40:58+0000",
            "author": "Kevin Watters",
            "content": "Yonik Seeley , right now, it builds against 4.x  If I submit a patch, should be done for trunk, or is a 4.x branch ok? I'm just finishing up the unit tests, either way, I hope to have a patch submitted by the end of the week. ",
            "id": "comment-14550544"
        },
        {
            "date": "2015-05-19T14:50:43+0000",
            "author": "Yonik Seeley",
            "content": "You can submit a patch against 4x (or whatever you want).  Before it's committed, it will need to be against trunk though.\nThat doesn't necessarily mean that you have to do the work to get it on trunk... if the port is too hard or you don't have time, you can put up what you have and others may be able to help. ",
            "id": "comment-14550560"
        },
        {
            "date": "2015-10-05T13:25:11+0000",
            "author": "Kevin Watters",
            "content": "Patch with GraphQuery / parsers / unit tests. ",
            "id": "comment-14943355"
        },
        {
            "date": "2015-10-06T21:00:10+0000",
            "author": "Yonik Seeley",
            "content": "Thanks Kevin,\nI just uploaded a patch with some minor improvements, including:\n\nSlightly simplified some of the parameter parsing... example:\nFrom:\nboolean onlyLeafNodes = Boolean.valueOf(localParams.get(\"returnOnlyLeaf\", \"false\"));\nTo:\nboolean onlyLeafNodes = localParams.getBool(\"returnOnlyLeaf\", false);\n\nSimplified some of the query handling, for instance:\n\n    SolrParams params = getParams();\n    SolrParams solrParams = SolrParams.wrapDefaults(localParams, params);\n    QParser baseParser = subQuery(solrParams.get(QueryParsing.V), null);\n    // grab the graph query options / defaults\n    Query rootNodeQuery = baseParser.getQuery();  \n\n\nWas replaced with\n\n    Query rootNodeQuery = subQuery(localParams.get(QueryParsing.V), null).getQuery();\n\n\n\nI rewrote buildFrontierQuery to use TermsQuery instead of BooleanQuery (more efficient for this use case, and no 1024 limit).\n\nI also marked FrontierQuery as internal and made it package protected... it's an implementation detail and it feels like\nwe could get rid of it in the future.\n\nUnless anyone has ideas of how to improve the current interface, I think this is ready to commit! (at least to trunk)  We can continue to make more optimizations to the implementation at any point. ",
            "id": "comment-14945785"
        },
        {
            "date": "2015-10-06T21:16:22+0000",
            "author": "Kevin Watters",
            "content": "Nice improvements!  The new TermsQuery, that definitely is a nice fit for this type of query.  (though that code path is only active if useAutn=false so it doesn't do the automaton compilation. )\nLooks good to me, lets roll with it! ",
            "id": "comment-14945822"
        },
        {
            "date": "2015-10-09T21:27:05+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1707818 from Yonik Seeley in branch 'dev/trunk'\n[ https://svn.apache.org/r1707818 ]\n\nSOLR-7543: basic graph traversal query ",
            "id": "comment-14951223"
        },
        {
            "date": "2015-10-09T21:28:22+0000",
            "author": "Yonik Seeley",
            "content": "Committed.  Thanks Kevin! ",
            "id": "comment-14951227"
        }
    ]
}