{
    "id": "LUCENE-2369",
    "title": "Locale-based sort by field with low memory overhead",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/search"
        ],
        "type": "New Feature",
        "fix_versions": [],
        "affect_versions": "4.0",
        "resolution": "Won't Fix",
        "status": "Resolved"
    },
    "description": "The current implementation of locale-based sort in Lucene uses the FieldCache which keeps all sort terms in memory. Beside the huge memory overhead, searching requires comparison of terms with collator.compare every time, making searches with millions of hits fairly expensive.\n\nThis proposed alternative implementation is to create a packed list of pre-sorted ordinals for the sort terms and a map from document-IDs to entries in the sorted ordinals list. This results in very low memory overhead and faster sorted searches, at the cost of increased startup-time. As the ordinals can be resolved to terms after the sorting has been performed, this approach supports fillFields=true.\n\nThis issue is related to https://issues.apache.org/jira/browse/LUCENE-2335 which contain previous discussions on the subject.",
    "attachments": {
        "LUCENE-2369.patch": "https://issues.apache.org/jira/secure/attachment/12453679/LUCENE-2369.patch",
        "lucene-2369-20101011.patch": "https://issues.apache.org/jira/secure/attachment/12456816/lucene-2369-20101011.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-04-06T10:26:03+0000",
            "content": "Toke, I still think it would be better to use ICU collation keys here.\n\nfor your danish text, the memory usage will still be smaller than trunk (as ICU collation keys as byte[] are smaller than java's internal utf-16 encoding).\nthen you can do this sort at index-time...\n\none step towards this is to switch collation in flex to use byte[] rather than encoding in char[] like it does today. ",
            "author": "Robert Muir",
            "id": "comment-12853838"
        },
        {
            "date": "2010-04-06T10:58:46+0000",
            "content": "I would like to note that new types of sort caches do not require invasive surgery on SegmentReader, of the type seen in previous issue.\nOr did I miss some peculiar requirement current APIs do not satisfy? ",
            "author": "Earwin Burrfoot",
            "id": "comment-12853883"
        },
        {
            "date": "2010-04-06T11:02:09+0000",
            "content": "The current implementation accepts Comparator<Object> (which must accept Strings) as well as a Locale (which is converted to Collator.getInstance(locale) under the hoo)d as arguments. Plugging in the ICU collator directly should be trivial. If/when it gets possible to use byte[] for sorters in general, I'll add support for that.\n\nIndexing ICU collator keys and using them in combination with LUCENE-2369 is an interesting idea, as it would speed up the building process quite a lot, while keeping the memory usage down. As long as fillFields=false, the two methods are independent as should work well with each other. Fairly easy to try.\n\nFor fillFields=true, it gets a bit trickier and requires a special FieldComparatorSource that keeps two maps from docID: One to the ICU collator key, one to the original term. Still, it should not be that hard to implement and I'll be happy to do it if the fillFields=false-case turns out to work well. ",
            "author": "Toke Eskildsen",
            "id": "comment-12853884"
        },
        {
            "date": "2010-04-06T11:04:06+0000",
            "content": "Moved from LUCENE-2335 as it really belongs here.\n\nLotsa devils in the details when you're poking around in the belly of Lucene, but modulo some business with deleted documents, it looks fine for simple (no parallel and multi readers) usage. fillFields=true works just as it should by delaying the actual term resolving until the documents ID's are determined. The current code makes it possible to create an exposed Sort quite easily:\n\n      ExposedFieldComparatorSource exposedFCS =\n          new ExposedFieldComparatorSource(reader, new Locale(\"da\"));\n      Sort sort = new Sort(new SortField(\"mySortField\", exposedFCS));\n\n\n\nFor the curious, a modified Lucene-JAR can be downloaded at http://github.com/tokee/lucene/downloads and tested with\n\njava -cp lucene-core-3.1-dev-LUCENE-2335-20100405.jar org.apache.lucene.index.ExposedPOC expose <index> <sortField> <locale> <defaultField>\n\n\nthis will present the user with a simple shell where searches can be performed. Heap-usage and execution times are displayed along with the search result.\n\nI did a little bit of real world experimenting: A 2.5GB index with 400K documents with 320K unique sort terms took 14 seconds to open. After that, a locale-based sorted search that hit 250K documents and returned the first 50 took 30ms (fully warmed by re-searching 5 times). Max heap was specified to 40MB of which 20MB was used after the building of the sort structure was finished.\n\nThe same search using the standard locale-oriented sorter took about 1 second to start up, After that, the 250K search took 130ms, fully warmed. Max heap was specified to 100MB.\n\nThe default sorter was able to get by with 80MB, but execution-time increased drastically to 2000ms. Probably because of the GC-overhead that the Collator introduces by temporarily allocating two new objects for each comparison.\n\n\nThe bad news is that this is quite a bit of code (400+ extra lines for the SegmentReader alone) with several levels of indirection in the data structures. As an example, getting the actual term for a given docID in the ExposedFieldComparatorSource is done with\n\n        final long resolvedDocOrder = docOrder.get(order[slot]);\n        return resolvedDocOrder == undefinedTerm ? null : reader.getTermText(\n            (int)termOrder.get((int)resolvedDocOrder));\n\n\nwhich is not easily digested without a very thorough explanation, preferably with a diagram.\n\nThe API-changes to the IndexReaders is the addition of two methods:\n\nString getTermText(int ordinal) throws IOException;\n\n\nis self-explanatory, but \n\nExposedIterator getExposedTuples(\n      String persistenceKey, Comparator<Object> comparator, String field,\n      boolean collectDocIDs) throws IOException;\n\n\nis real hard to do when writing a new IndexReader. My current approach is to use an interface ExposedReader with the methods and let the updated IndexReaders implement that, thereby making it optional for IndexReaders to be Exposed. \n\nLUCENE-2335 seems to fulfill the promises so far, with the previously discussed trade-offs:\n\n\tLong startup (~1 min/1M documents, less on re-open)\n\tFast locale-based sorted search (supports fillFields=true and has near-zero GC overhead)\n\tVery low memory overhead (both permanent and for initializing)\n\n\n\nRegarding making a proper patch, I would like to know what I should patch against. I use LUCENE-1990 so I need to do it against a fairly new version. I can see that Flex is about to be merged, so I guess it would make sense to wait for that one. ",
            "author": "Toke Eskildsen",
            "id": "comment-12853885"
        },
        {
            "date": "2010-04-06T11:10:19+0000",
            "content": "Earwin, it would be great if this can be done without modifying IndexReaders at all. I know that the getExposedTuples can be refactored out - it'll be somewhat clunky with checks for the class for a given IndexReader and different handling of different readers though. But, however I tweak it, I still need to be able to access a given term by its ordinal from outside the reader. Last time I looked, this was not possible. Has this changed since then? ",
            "author": "Toke Eskildsen",
            "id": "comment-12853886"
        },
        {
            "date": "2010-04-06T17:18:12+0000",
            "content": "A few experiments with the current implementation:\n\n40GB index, 5 segments, 7.5M documents, 5.5M unique sort terms, 87M terms total, sort locale da, top-20 displayed with fillFields=true. Just opening the index without any Sort requires 140MB.\nStandard sorter: -Xmx1800m, 26 seconds for first search\nExposed sorter: -Xmx350m, 7 minutes for first search (~4\u00bd minutes for segment sorting, ~2\u00bd minutes for merging).\n\nFully warmed searches, approximate mean:\n6.5M hits: standard 2500 ms, exposed 240 ms\n4.1M hits: standard 1600 ms, exposed 190 ms\n2.1M hits: standard 900 ms, exposed 90 ms\n1.2M hits: standard 500 ms, exposed 45 ms\n0.5M hits: standard 220 ms, exposed 40 ms\n0.1M hits: standard 80 ms, exposed 6 ms\n1.7K hits: standard 3 ms, exposed <1 ms\n\n\n2.5GB index, 4 segments, 420K documents, 240K unique sort terms, 11M terms total, sort locale da, top-20 displayed with fillFields=true. Just opening the index without any Sort requires 18MB.\nStandard sorter: -Xmx120m, 2 seconds for first search\nExposed sorter: -Xmx50m, 14 seconds for first search (9 seconds for segment sorting, 5 seconds for merging).\n\nFully warmed searches, approximate mean:\n420K hits: standard 170 ms, exposed 15 ms\n200K hits: standard 85 ms, exposed 9 ms\n100K hits: standard 50 ms, exposed 8 ms\n 10K hits: standard 6 ms, exposed 0-1 ms\n\nAs can be seen, the timings are fairly consistent for this small ad-hoc test. The difference between standard and exposed sorting is the time it takes for the collator to perform compares. I'll have to test if that can be improved by using a plain int-array to hold the order of the documents, just as the non-locale-using String sorter does. ",
            "author": "Toke Eskildsen",
            "id": "comment-12854069"
        },
        {
            "date": "2010-05-02T23:34:36+0000",
            "content": "Earwin: I've spend some time looking through trunk and happily the access-by-ordinal is part of it. This seems to make it possible to do the low-mem sort trick without the invasive surgery to the IndexReaders. However, I find it hard to do totally clean: The FieldCache.DEFAULT is used throughout Lucene but is hardwired for specific types of Terms. As it would be best to have purging of closed IndexReaders done automatically, some sort of extension of the DEFAULT is needed. Making a wrapper FieldCache that encapsulates the existing DEFAULT and replaces it should work and since the DEFAULT is public static, this might even be the intended way of doing it? This would change LUCENE-2369 from a large patch for Lucene core to a standard contrib.\n\nThanks for the heads-up, Earwin. ",
            "author": "Toke Eskildsen",
            "id": "comment-12863219"
        },
        {
            "date": "2010-05-07T13:01:03+0000",
            "content": "FieldCache should move to become a plugin of IndexReader in some future. So there's no longer any statics and no need to call purge.\n\n> I know that the getExposedTuples can be refactored out - it'll be somewhat clunky with checks for the class for a given IndexReader and different handling of different readers though\nAnd Mike seems to push the split between primitive SegmentReaders and all kinds of MultiReaders, so they no longer extend same base class. So that task should be easier. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12865133"
        },
        {
            "date": "2010-08-31T14:49:17+0000",
            "content": "A status update might be in order. Switching to the current Lucene trunk with flex did require a lot of changes. Luckily it seems that they are all external so this could be a contrib.\n\nThe current implementation (not patch yet) seems to scale fairly well: Quick tests were made with a test-index with 5 fields of which one contained random Strings at average length 10 characters. No index optimize. The goal was to perform a Collator-based sorted search with fillFields=true (the terms used for sorting are returned along with the result) to get top-20 out of a lot of hits. Search-time was kept low by a field that was defined with the same term for every other document. The hardware was a Dell M6500 with i7@1.7GHz, PC1333 RAM, Intel X-25G2 SSD. The tests were performed in the background while coding and ZIPping 6M files.\n\n2M document index, search hits 1M documents:\n\n\tInitial exposed search: 1:16 minutes\n\tSubsequent exposed searches: 45 ms\n\tTotal heap usage for Lucene + exposed structure: 21 MB\n\tInitial default Lucene search: 3.3 s\n\tSubsequent default Lucene searches: 2.1 s\n\tTotal heap usage for Lucene + field cache: 54 MB\n\n\n\n20M document index, search hits 10M documents:\n\n\tInitial exposed search: 15:27 minutes\n\tSubsequent exposed searches: 370 ms\n\tTotal heap usage for Lucene + exposed structure: 209 MB\n\tInitial default Lucene search: 28 s\n\tSubsequent default Lucene searches: 20 s\n\tTotal heap usage for Lucene + field cache: 530 MB\n\n\n\n200M document index, search hits 100M documents:\n\n\tInitial exposed search: 186:31 minutes\n\tSubsequent exposed searches: 3.5 s\n\tTotal heap usage for Lucene + exposed structure: 2300 MB\n\tNo data for default Lucene search as there was OOM with 6 GB of heap.\n\n\n\nObservations:\n\n\tThe memory-requirement for the exposed structures is larger than the strict minimum. This is necessary in order to provide support for fast re-opening of indexes (the order of the terms in unchanged segments is reused). It seems like an obvious option to disable this cache.\n\tThe time for startup scales about n * log n with the number of terms. Comparing the 200M to 2M: 186 minutes / (200M * log(200M)) * (2M * log(2M)) ~= 1:25 min(observed was 1:16 min).\n\tNo tests with 100M documents yet, but 1\u00bd hour for build and 1.5GB of RAM would be the expected requirement. Having a startup-time of 1 hour+ is of course excessive, but if one made the calculated structures persistent (and thereby reduced restart time to near zero), this would work well with a classic \"update once every night\"-scenario. This would provide Collator-sorted search for a 100M document index on a machine with 2GB of RAM.\n\n ",
            "author": "Toke Eskildsen",
            "id": "comment-12904642"
        },
        {
            "date": "2010-08-31T15:08:42+0000",
            "content": "No tests with 100M documents yet, but 1\u00bd hour for build and 1.5GB of RAM would be the expected requirement.\n\nToke, have you tried doing this 'build' at index time instead? I would recommend applying LUCENE-2551 and indexing with ICU Collation, strength=primary\n\nNow that we can mostly do everything as bytes, I think this slow functionality to do collation/range query at 'runtime' might soon be on its way out of lucene (see patches on LUCENE-2514).\n\nInstead, I think its better to encourage users to index their content accordingly for the use cases they need. ",
            "author": "Robert Muir",
            "id": "comment-12904651"
        },
        {
            "date": "2010-08-31T20:53:31+0000",
            "content": "\nToke, have you tried doing this 'build' at index time instead? I would recommend applying LUCENE-2551 and indexing with ICU Collation, strength=primary\n\nRobert, I'll sum up my understanding of the issue:\n\n\tICU collator keys makes sorting very fast at the cost of some extra disk space, as one will probably want to store the original Term together with the key. It requires a non-trivial memory overhead, in the ideal case as many bytes as there are characters in the terms. Works extremely well with reopening.\n\tMy experiment makes sorting relatively low-memory and extremely fast at the cost of very high pre-calculation time. Works halfway well with reopening as some structures are reused.\n\n\n\nThe two approaches are not in conflict and combining them would indeed seem to give many benefits. Moving the building of the structures to index-time seems fairly easy: If nothing else, it could just be a post-processing of the index.\n\nICU is clearly what's on people's mind when it comes to collator based sorting. I can see that I have to do some Lucene standard vs. ICU vs. pre-calculated vs. ICU+pre-calculated tests to explore what the benefits of the different approaches are.\n\n\nNow that we can mostly do everything as bytes, I think this slow functionality to do collation/range query at 'runtime' might soon be on its way out of lucene (see patches on LUCENE-2514).\n\nNo argument from me. I'll keep my work at the runtime level for now though, but that's just to avoid working on two fronts at the same time.\n\n\nInstead, I think its better to encourage users to index their content accordingly for the use cases they need.\n\nI agree that the sort-fields as well as sort-locale is well known at index time in most cases. ",
            "author": "Toke Eskildsen",
            "id": "comment-12904765"
        },
        {
            "date": "2010-08-31T21:35:58+0000",
            "content": "ICU collator keys makes sorting very fast at the cost of some extra disk space, as one will probably want to store the original Term together with the key. It requires a non-trivial memory overhead, in the ideal case as many bytes as there are characters in the terms. Works extremely well with reopening.\n\nThis doesnt make sense, why do you need the original term also?\n\nWhat 'memory overhead'? indexing collation keys, even at tertiary strength (the largest size) is in general less than 2 bytes per character. this is actually less than the cost of a term in ram in lucene 3.1, so i don't understand this?\n\nThe two approaches are not in conflict and combining them would indeed seem to give many benefits\n\nif you are using collation keys, then binary order gives you collated results. So thats what I am hinting at here, is there a more general improvement here you can apply to sorting bytes? If this issue has some ideas that can improve the more general case, I think we should look at factoring those improvements out, and leave the locale stuff as an indexing-time thing.\n\nI agree that the sort-fields as well as sort-locale is well known at index time in most cases.\n\nIn all cases really. I don't see this issue really helping if you dont know the locale at index time, by invoking the collator over all the terms at startup you are essentially reindexing in RAM.\n\nif one doesnt know the necessary locales at index-time, i suggest using a generic UCA collator: ULocale.ROOT as a 'catch-all' field for all other locales. ",
            "author": "Robert Muir",
            "id": "comment-12904780"
        },
        {
            "date": "2010-09-01T07:26:26+0000",
            "content": "\nThis doesnt make sense, why do you need the original term also?\n\nI was thinking aggregation, but you are right. For aggregation one would of course just use the keys and have no need for the original Strings. Then we're left with federated search.\n\n\nWhat 'memory overhead'? indexing collation keys, even at tertiary strength (the largest size) is in general less than 2 bytes per character. this is actually less than the cost of a term in ram in lucene 3.1, so i don't understand this?\n\nThat is the memory overhead. If you have 20M terms of average length 10 chars, that is 400MB in raw bytes and quite a bit more when you're taking pointers into account. When I'm talking memory overhead, my baseline is a newly opened Lucene index without field caching of terms. Having a beefy machine and a small index is trivial. Low-end hardware, virtualized servers and huge indexes all calls for conserving memory. PackedInts helps a lot, avoiding storing the terms (or ICU keys) i RAM helps more.\n\n\nif you are using collation keys, then binary order gives you collated results. So thats what I am hinting at here, is there a more general improvement here you can apply to sorting bytes? If this issue has some ideas that can improve the more general case, I think we should look at factoring those improvements out, and leave the locale stuff as an indexing-time thing.\n\nMy approach is in theory very simple: Provide an order mapping for each segment, merge the order maps for index level. Having segments with the ICU keys already in sorted order would make the segment maps 1:1 (i.e. they do not require a sort) leaving only the index level merging of order maps. This would halve the build time and together with the 10 times speedup (guessing from the ICU website) that ICU gives, doing the calculation for 20M terms in my example above should take less than a minute. As the caches for the segments are then superfluous, the memory requirements are halved and thus providing faster sort at a memory cost of 100MB as compared to ICU sort speed and 400MB of memory.\n\n\nI don't see this issue really helping if you dont know the locale at index time, by invoking the collator over all the terms at startup you are essentially reindexing in RAM.\n\nI fail to see why that is a bad thing if we're looking at the rare scenario of having to postpone the sorting decision to search time. What is the alternative? Right now, search-time collator-based sorting with field cache has low startup time, high memory usage and horrible execution time for large results. ",
            "author": "Toke Eskildsen",
            "id": "comment-12904957"
        },
        {
            "date": "2010-09-01T11:51:54+0000",
            "content": "I was thinking aggregation, but you are right. For aggregation one would of course just use the keys and have no need for the original Strings. Then we're left with federated search.\n\nI don't see why federated search needs anything but sort keys?\n\nThat is the memory overhead. If you have 20M terms of average length 10 chars, that is 400MB in raw bytes and quite a bit more when you're taking pointers into account.\n\nThe \"memory\" overhead is no different than the \"overhead\" of regular terms, there is nothing special about the collation key case, this is my point (see below). and in practice for most people, its encoded as way less than 2 bytes/char.\n\n\nI fail to see why that is a bad thing if we're looking at the rare scenario of having to postpone the sorting decision to search time. What is the alternative? Right now, search-time collator-based sorting with field cache has low startup time, high memory usage and horrible execution time for large results.\n\nBecause \"search-time\" collator-sorting is the wrong approach, and should not exist at all.\n\nIndexing with collation keys once we fix LUCENE-2551 has:\n\n\tsame startup time as regular terms\n\tapproximately the same memory usage as regular terms [e.g. PRIMARY key for \"Robert Muir\" is 12 bytes versus 11 bytes]\n\tsame execution time (binary compare) as regular terms\n\n ",
            "author": "Robert Muir",
            "id": "comment-12905002"
        },
        {
            "date": "2010-09-01T12:45:26+0000",
            "content": "\nI don't see why federated search needs anything but sort keys?\n\nBecause the sort terms themselves are needed when a search-result from your service is merged with a result from a source that you do not control by an aggregator that is not you.\n\n\nThe \"memory\" overhead is no different than the \"overhead\" of regular terms, there is nothing special about the collation key case, this is my point (see below). and in practice for most people, its encoded as way less than 2 bytes/char.\n\nWe're clearly not talking about the same thing here. Maybe I've misunderstood something. Let me try and rephrase myself and break it down so that we can pinpoint where the problem is.\n\n1) Opening a Lucene index and performing a search with relevance ranking requires X bytes of heap.\n2) Performing a locale-based sorted search with Lucene 3.0.2 takes X + A bytes, where A is relatively large as all Terms from the sort field is kept in memory as Strings.\n3) Performing the same search with Lucene trunk takes X + B bytes, where B is quite a lot smaller than A as BytesRefs are used for the Terms kept in memory.\n4) Performing the same search on an ICU key infused index with Lucene trunk + ICU magic takes X + C bytes, where C is about the same size as B.\n5) Performing the same search doing pre-sorting (what I'm doing) takes X + D bytes, where D is smaller than C. None of the Terms are kept in memory.\n\nIt seems to me that you are assuming that the sort-terms are kept in memory in case 5? Or not kept in memory in any of the cases 3, 4 and 5?\n\n\nBecause \"search-time\" collator-sorting is the wrong approach, and should not exist at all.\n\nOpinion noted. Circular argumentation skipped.\n\n\nIndexing with collation keys once we fix LUCENE-2551 has:\n\n\n\tsame startup time as regular terms\n\tapproximately the same memory usage as regular terms [e.g. PRIMARY key for \"Robert Muir\" is 12 bytes versus 11 bytes]\n\tsame execution time (binary compare) as regular terms\n\n\n\nIndexing with pre-sorting (or whatever we call what I'm trying to do) has\n\n\n\tHuge startup time (or index commit time penalty if we move it to indexing)\n\tLower memory usage that sorting with regular terms or ICU keys\n\tFaster execution time (single integer compare) than LUCENE-2551 or regular terms\n\n\n\nHave I misunderstood something here? When a sorted search with ICU keys is performed, the keys themselves are still compared to each other for each search, right?\n\n\nCombining the two approaches and doing the pre-sorting at index time, we have\n\n\n\tTime penalty at index commit (1 min / 10M terms? More? This requires real testing)\n\tFaster startup time than regular term or ICU key sorting (load two PackedInts structures)\n\tLower memory usage that sorting with regular terms or ICU keys\n\tFaster execution time (single integer compare) than LUCENE-2551\n\n\n\nBad for real-time, good for fast sorting and low memory usage.\n\nIf you agree on this breakdown, the next logical step for me is to create a performance (speed & heap) test for the different cases to see whether the memory savings and the alleged faster sorting with integer comparison is enough to warrant the hassle. ",
            "author": "Toke Eskildsen",
            "id": "comment-12905011"
        },
        {
            "date": "2010-09-01T13:07:07+0000",
            "content": "Faster startup time than regular term or ICU key sorting (load two PackedInts structures)\n\nBut this is what I keep trying to get at (the whole point of my comments):\n\n\tICU keys are just byte[] just like regular terms. they are \"regular terms\"\n\tCan we forget about the stupid runtime Locale sort, if you have a way to improve memory usage for byte[] terms, lets look just at that? Then this could be more general and more useful.\n\n ",
            "author": "Robert Muir",
            "id": "comment-12905015"
        },
        {
            "date": "2010-09-01T13:32:48+0000",
            "content": "\nICU keys are just byte[] just like regular terms. they are \"regular terms\"\n\nDo they or do they not need to be loaded into heap in order to be used for sorted search?\n\n\nCan we forget about the stupid runtime Locale sort, if you have a way to improve memory usage for byte[] terms, lets look just at that? Then this could be more general and more useful.\n\nEasy now. The whole runtime-vs-index-time issue is something that I don't care much about at this point. Pre-sorting can be done both at index and search time. Let's just say that we do it at index-time and go from there.\n\nNot holding the sort-terms in memory (whether they be Strings, BytesRefs, regular terms or ICU keys) and doing all possible sorting up front (in the case of a hybrid ICU-approach: A merge-sort of the already sorted segments), is what I'm looking at. Could you please re-read my comment with that in mind and see if my breakdown and trade-off lists makes sense? It seems to me that you're quite certain that there is something I've missed, but I haven't yet understood what it is. I do know that ICU keys are just regular terms in the technical sense. When I use the designation ICU keys, I do it to make it clear that we're getting locale-specific ordering.\n\nDeep breaths, ok? I'm going to fetch the kids from school, so you don't need to rush your answer. ",
            "author": "Toke Eskildsen",
            "id": "comment-12905020"
        },
        {
            "date": "2010-09-01T13:48:08+0000",
            "content": "Do they or do they not need to be loaded into heap in order to be used for sorted search?\n\nThey are just regular terms! you can do a TermQuery on them, sort them as byte[], etc. \nits just the bytes use 'collation encoding' instead of 'utf-8 encoding'.\nThis is why i want to factor out the whole 'locale' thing from the issue, since sorting is agnostic to whats in the byte[], its unrelated and it would simplify the issue to just discuss that.\n\nEasy now. The whole runtime-vs-index-time issue is something that I don't care much about at this point. Pre-sorting can be done both at index and search time. Let's just say that we do it at index-time and go from there.\n\nWell, the thing is, its something i care a lot about. The problems are:\n\n\tUsers who develop localized applications tend to use methods with Locale/Collator parameters if they are available: its best practice.\n\tIn the case of lucene, it is not best practice, but a silly trap (as you get horrible performance).\n\tHowever, users are used to the concept of collation keys wrt indexing (e.g. when building a database index)\n\tThe apis here are wrong anyway: it shouldnt take Locale but Collator.\nThere is no way to set strength or any other options, and theres no way to supply a Collator i made myself (e.g. from RuleBasedCollator)\n\n ",
            "author": "Robert Muir",
            "id": "comment-12905026"
        },
        {
            "date": "2010-09-02T13:38:40+0000",
            "content": "Preliminary patch with the primary purpose of testing the viability of this issue.\n\nCreate a Lucene flex index (Version.40+) and call\njava -cp build/lucene-core-4.0-dev.jar org.apache.lucene.search.exposed.poc.ExposedPOC\nto test. ",
            "author": "Toke Eskildsen",
            "id": "comment-12905503"
        },
        {
            "date": "2010-09-02T13:49:37+0000",
            "content": "\nThey are just regular terms! you can do a TermQuery on them, sort them as byte[], etc.\nits just the bytes use 'collation encoding' instead of 'utf-8 encoding'.\n\nYes, you have stated that repeatedly.\n\nAs you are unwilling to answer my questions, I've tweaked my tests to see for myself. It worked very well, as I learned something new. To answer one of the questions, then yes, the terms are loaded into memory when a sort is performed on a field. This is the case for the locale-based sort (yes, I have understood that you do not consider that a viable form of sorting and I only write it for completeness), for STRING-sorting (natural order with ordinal based speedup) and for STRING_VAL-sorting (natural order without the ordinal-speedup). All this against Lucene truck, no patches.\n\n\nThis is why i want to factor out the whole 'locale' thing from the issue, since sorting is agnostic to whats in the byte[], its unrelated and it would simplify the issue to just discuss that.\n\nFor my new tests I've switched to natural order sorting (direct byte[] comparison aka Lucene STRING sorted search without specifying a Locale). The tests should be fairly telling for the different scenarios as the ICU keys should be about the same size as the original terms.\n\n\n\n\tThe apis here are wrong anyway: it shouldnt take Locale but Collator.\n      There is no way to set strength or any other options, and theres no way to supply a Collator i made myself (e.g. from RuleBasedCollator)\n\n\n\nI fully agree. The code I've made so far takes a Comparator<BytesRef>, with optimization for wrapped Collators. The title of LUCENE-2369 is not technically correct but was chosen as \"Locale\" is a fairly known concept while \"Collator\" is more complex. That might have been a mistake.\n\nOnwards to testing with natural order (new Sort(new SortField(myField, SortField.STRING))) for Lucene and the hybrid approach (natural order + pre-sorting) for exposed. No ZIPping in the background this time, so measurements differ from the previous test. Heap sizes were measured after a call to System.gc().\n\n2M document index, search hits 1M documents, top 10 hits extracted:\n\n\n\tInitial exposed search: 0:16 minutes\n\tSubsequent exposed searches: 40 ms\n\tTotal heap usage for Lucene + exposed structure: 20 MB\n\tInitial default Lucene search: 0.8 s\n\tSubsequent default Lucene searches: 25 ms\n\tTotal heap usage for Lucene + field cache: 60 MB\n\n\n\n20M document index, search hits 10M documents, top 10 hits extracted:\n\n\n\tInitial exposed search: 2:53 minutes\n\tSubsequent exposed searches: 330 ms\n\tTotal heap usage for Lucene + exposed structure: 154 MB\n\tInitial default Lucene search: 6 s\n\tSubsequent default Lucene searches: 220 ms\n\tTotal heap usage for Lucene + field cache: 600 MB\n\n\n\n200M document index, search hits 100M documents, top 10 hits extracted:\n\n\n\tInitial exposed search: 31:33 minutes\n\tSubsequent exposed searches: 3200 ms\n\tTotal heap usage for Lucene + exposed structure: 1660 MB\n\tNo data for default Lucene search as there was OOM with 7 GB of heap.\n\n\n\nWhat did we learn from this?\n\n\n\tNatural order search in Lucene with STRING is very fast (as most of the work is ordinal comparison).\n\tExposed sorting is actually slower than natural order (that's news for me). The culprit is a modified PackedInts-structure. I'll look into that.\n\tThe exposed structure build penalty for the hybrid approach (i.e. relying on natural order instead of doing an explicit sort) was indeed markedly lower than exposed with explicit sorting. A factor 5. I would have expected it to be more though.\n\tThe hybrid approach uses less than a third of the amount of RAM required by Lucene natural order sorting.\n\n\n\nSo, Robert, does this answer your challenge \"if you have a way to improve memory usage for byte[] terms, lets look just at that?\"? ",
            "author": "Toke Eskildsen",
            "id": "comment-12905507"
        },
        {
            "date": "2010-09-23T13:47:24+0000",
            "content": "This patch is to keep in sync with Lucene trunk (20100923) and to explore some ideas. Besides the updated code with some bug fixing and some optimization, there's sample code for faceting and index lookup (check out the unit-test TestExposedFacets.testScale). I know that this does not belong in Lucene core, so see it as a demonstration of the potential in providing the doc/term mappings.\n\nNow, revisiting the previous test with the updated code and this time actually remembering not to do an explicit sort in the exposed-part (simulating that ICU collator keys are indexed), the numbers are\n\n2M document index, search hits 1M documents, top 10 hits extracted:\n\n\tOpening the index and doing a plain relevance-sorted search: 3 MB\n\tInitial exposed search: 3.5 seconds\n\tSubsequent exposed searches: 40-60 ms\n\tTotal heap usage for Lucene + exposed structure: 23 MB\n\tInitial default Lucene sorted search: 1.0 seconds\n\tSubsequent default Lucene searches: 30-35 ms\n\tTotal heap usage for Lucene + field cache: 61 MB\n\n\n\n20M document index, search hits 10M documents, top 10 hits extracted:\n\n\tOpening the index and doing a plain relevance-sorted search: 27 MB\n\tInitial exposed search: 44 seconds\n\tSubsequent exposed searches: 350-380 ms\n\tTotal heap usage for Lucene + exposed structure: 183 MB\n\tInitial default Lucene sorted search: 6.7 seconds\n\tSubsequent default Lucene searches: 220-240 ms\n\tTotal heap usage for Lucene + field cache: 614 MB\n\n\n\n200M document index, search hits 100M documents, top 10 hits extracted:\n\n\tOpening the index and doing a plain relevance-sorted search: 210 MB\n\tInitial exposed search: 7:35 minutes\n\tSubsequent exposed searches: 3320-3550 ms\n\tTotal heap usage for Lucene + exposed structure: 1744 MB\n\tNo data for default Lucene search as there was OOM with 7 GB of heap.\n\n\n\nWhile the time for first search is still substantial, it is a lot shorter than the previous measurements. Lucene natural order sorting is still nearly double as fast (I haven't tried switching to int[] instead of PackedInts yet, so that part is not closed). I'll try and find the time to do some more detailed tests with a more realistic number of hits, but I estimate that the speed will be the same, relative to Lucene natural order sort. ",
            "author": "Toke Eskildsen",
            "id": "comment-12914040"
        },
        {
            "date": "2010-10-10T22:51:01+0000",
            "content": "Some bug fixes, some sample code demonstrating how to build a hierarchical faceting system using ordered term ordinals.\n\nThis is moving away from the original issue at high speed. I'll try and sum up my observations and ideas on the mailing list Real Soon Now. ",
            "author": "Toke Eskildsen",
            "id": "comment-12919682"
        },
        {
            "date": "2010-11-19T22:49:03+0000",
            "content": "Bugfixes and maintenance. This patches against Lucene trunk revision 1036986 (latest one at the time of writing). Apply the patch in the lucene sub-folder. ",
            "author": "Toke Eskildsen",
            "id": "comment-12934042"
        },
        {
            "date": "2011-02-03T11:22:49+0000",
            "content": "Maintenance patch bringing the code up to date with Lucene trunk@1066767 (2011-02-03). Patch and test by e.g.\n\n\nsvn co http://svn.apache.org/repos/asf/lucene/dev/trunk@1066767 lucene-2369\ncd lucene-2369\npatch -p0 < LUCENE-2369.patch\ncd lucene\nant compile-test\ncd contrib/exposed/\nant compile-test\ncd ../..\njava -cp lib/junit-4.7.jar:build/classes/test/:build/classes/java:build/contrib/exposed/classes/java/:build/contrib/exposed/classes/test/ org.junit.runner.JUnitCore org.apache.lucene.search.exposed.facet.TestHierarchicalFacets\n\n ",
            "author": "Toke Eskildsen",
            "id": "comment-12990054"
        },
        {
            "date": "2011-07-14T10:49:44+0000",
            "content": "Maintenance patch bringing the code up to date with Lucene trunk@1145556 (2011-07-13). Patch and test by e.g.\n\n\nsvn co http://svn.apache.org/repos/asf/lucene/dev/trunk@1145556 lucene-2369\ncd lucene-2369\npatch -p0 < LUCENE-2369.patch\ncd modules/queryparser/\nant\ncd ../../lucene/contrib/exposed/\nant compile-test\ncd ../..\njava -cp lib/junit-4.7.jar:build/classes/test/:build/classes/java:build/contrib/exposed/classes/java/:build/contrib/exposed/classes/test/ org.junit.runner.JUnitCore org.apache.lucene.search.exposed.facet.TestHierarchicalFacets\n\n ",
            "author": "Toke Eskildsen",
            "id": "comment-13065174"
        },
        {
            "date": "2012-12-03T15:39:47+0000",
            "content": "A patch with code both for Lucene 4 and Solr 4 is now maintained at https://issues.apache.org/jira/browse/SOLR-2412\n\nThe code still works with Lucene 4 standalone and provides hierarchical faceting with custom sorting. Further development will be announced under that JIRA issue. ",
            "author": "Toke Eskildsen",
            "id": "comment-13508807"
        },
        {
            "date": "2018-11-23T20:31:11+0000",
            "content": "I am closing this as I have not touched it for years and since it was probably a bad idea to start with. ",
            "author": "Toke Eskildsen",
            "id": "comment-16697499"
        }
    ]
}