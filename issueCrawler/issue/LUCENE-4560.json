{
    "id": "LUCENE-4560",
    "title": "Support Filtering Segments During Merge",
    "details": {
        "components": [],
        "fix_versions": [],
        "affect_versions": "None",
        "priority": "Major",
        "labels": "",
        "type": "Improvement",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "Spun off from LUCENE-4557\n\nIt is desirable to be able to filter segments during merge.\nMost often, full reindex of content is not possible.\nMerging segments can sometimes have negative consequences when fields are have different options (most restrictive option is forced during merge)\nBeing able to filter segments during merges will allow gradually migrating indexed data to new index settings, support pruning/enhancing existing data gradually\n\n\nUse Cases:\n\n\tMigrate IndexOptions for fields (See LUCENE-4557)\n\tGradually Remove index fields no longer used\n\tMigrate indexed sort fields to DocValues\n\tSupport converting data types for indexed data\n\tand so on\n\n\n\npatch will be forthcoming",
    "attachments": {
        "LUCENE-4560-simple.patch": "https://issues.apache.org/jira/secure/attachment/12554178/LUCENE-4560-simple.patch",
        "LUCENE-4560.patch": "https://issues.apache.org/jira/secure/attachment/12553660/LUCENE-4560.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-11-15T16:02:40+0000",
            "content": "Attaching patch\n\npatch adds MergedSegmentFilter base class and adds config setter akin to IndexReaderWarmer on IndexWriterConfig (by all means, suggest better names)\n\nSegmentMerger will use this (if specified) to filter any segments being merged\n\nTest case included that uses filter to remove an indexed field during merge.\n\n\n ",
            "author": "Tim Smith",
            "id": "comment-13498087"
        },
        {
            "date": "2012-11-18T18:41:58+0000",
            "content": "Hi Tim. While in general I'm not against the idea (and I think that in general have some more control during the merge stage is needed), may I ask why can't you e.g. do this code (borrowing from your patch):\n\n\nIndexWriter writer = new IndexWriter(newDirectory);\nwriter.addIndexes(new RemoveFieldReader(oldReader));\n\n\n\nThat will accomplish, I believe, exactly what you want, no?\n\nThe benefits to your approach is that the filtering is done in-place, i.e. no need to add to a new directory, then switch old/new dirs. But it also may inadvertently add bugs, e.g. if someone mistakenly decided to remove a field, or worse, removes the wrong field ... w/ the addIndexes approach, you can do the process offline, investigate the result index and once you're contend with it, make the switch.\n\nI can see the benefits in both approaches, but I think that the addIndexes approach is safer, as it's not 'online' and does not change the source directory. I'm not sure how 'online' this process needs to be though. How often do you remove fields, or change index options? That's a fairly serious decision IMO, and should be done w/ care and lots of testing. Doing that in-place may be dangerous.\n\nAbout the patch, it's very simple and clean, which is a good thing ! I would make RemoveFieldReader extend FilterAtomicReader, to save you some lines of code, even though it's just a test class.\n\nIf you do (and others agree) want to continue w/ the online filtering approach, perhaps, instead of introducing a MergedSegmentFilter, we could make SegmentMerger pluggable, with few extension points that allow you to allocate your own AtomicReader ... just a thought, I know it's not directly related to this issue, but if we're going to open segment merging up for some serious hacking, let's do it w/ all intentions . ",
            "author": "Shai Erera",
            "id": "comment-13499871"
        },
        {
            "date": "2012-11-18T19:02:33+0000",
            "content": "Thinking about this some more, I really don't thing it's a 'gradual' thing that you do to the index:\n\n\n\tDepending on the state of the index, this migration may not happen at all to some segments, typically very large segments and are not picked for merge anymore. So what will happen is that you'll have code in your app that will never be invoked after some time ... not a good sign to me.\n\n\n\n\n\tI won't want to have code in my app that lives there forever. Rather, I'd like to make a decision to remove field 'foo', run the process which removes it once, and be done with it, moving the code to some \"tools\" area that is never run again.\n\t\n\t\tWith your approach, RemoveFieldReader will not go away, unless you can guarantee it ran on all segments, which is like forcing forceMerge(1) to run (note, it may not do what you want, per MP settings !), which is really like addIndexes\n\t\tWorse, today it's RemoveFieldReader, and tomorrow it will turn into RemoveFieldAndMigrateIndexOptionsReader, because as I wrote above, you cannot stop running that code if you cannot ensure that all segments have been migrated.\n\t\n\t\n\n\n\nSo I'm beginning to think that this process should not be an incremental/gradual/online thing, but rather an addIndexes type of process, that you run once, and know that you're done with it, until the next time where you need to rewrite the index, w/o actually re-indexing the content.\n\nBTW, did you take a look at LUCENE-2632? It is about adding a FilteringCodec which filters the data that it writes/reads. Could it help you here? If so, I think that it has better chances to get committed, than the approach in this issue (Codecs are already an extension point...). ",
            "author": "Shai Erera",
            "id": "comment-13499876"
        },
        {
            "date": "2012-11-19T02:54:12+0000",
            "content": "My base requirement here is that this be an online process.\nAs such, the add indexes approach is really not useful as i see it, especially as it requires 2x disk space, as well as completely new index directories, it does not play well with upgrading a user's existing index.\n\nwhat i see as needed is the ability to gradually migrate indexes such that any individual segment is itself consistent.\ncurrently, merging of indexes can result in loss of indexed data or otherwise break consistency, as in LUCENE-4557\n\nit is 100% ok if all segments have not been processed as i can identify each segment's settings at index open/search time, and optionally filter/search/read segments differently.\n\nIt is true that once you start using this SegmentMergeFilter, you pretty much have to keep using it forever.\nI don't see this as an issue as when dealing with supporting old indexes, you constantly have to support migration of data that was indexed using old code. For instance, as time goes on, my \"MergeSegmentFilter\" will do more, supporting migrating more and more old index formats/config settings to the latest indexing format/settings.\n\nAt quick glance, FilteringCodec looks like it applies to writing new content, not reading existing indexes?\nDoesn't seem quite like that would do the trick here. I would need some way to have the index writer wrap the codec for existing segments in order to inject my custom filtering that would apply during merging. That would be logically identical to the patch provided, however would potentially result in a much more complex patch.\n\n\n\n\n ",
            "author": "Tim Smith",
            "id": "comment-13499996"
        },
        {
            "date": "2012-11-19T06:40:48+0000",
            "content": "I don't see why you need to gradually migrate segments, rather than a one-off thing that you do when you decide to change the format of the index.\n\nIs it really important to do this gradually? Or if there was a tool which allowed you to do an in-place upgrade of certain segments, that would be something to consider?\nFor instance, you can do something similar to:\n\n\nDirectory dir; // directory with indexed documents\nIndexWriter writer; // your instance of IndexWriter\nIndexReader r = YourIndexReader.open(dir);\nwriter.deleteAll();\nwriter.addIndexes(r);\nwriter.commit();\n\n\n\nThis is all transactional, so until you commit, searches don't see any of this work.\n\nNote however that while it's seemingly done in-place, you need to copy all the segments, even if they don't need to be upgraded.\n\nI guess that I just can't think of a good reason to do a gradual upgrade of segments. Whenever I had to upgrade old indexes, it was done as a one-off process and that's it. E.g. IndexUpgrader is such a tool \u2013 upgrades the index in place.\n\nHaving said that, if others think that it might be useful to let one extend e.g. IndexWriter by providing different instances than SegmentReader (hard-coded in IW), then I prefer that route to the MergedSegmentFilter. Today it's SegmentMerger, tomorrow it will be something else. If we want to handle it, let's handle it from the root. SegmentMerger itself really has nothing to do with filtering readers. That way, you could write something like IndexUpgrader (or UpgradeMergePolicy) and upgrade the index as a one-off process, in place, touching only needed segments. ",
            "author": "Shai Erera",
            "id": "comment-13500050"
        },
        {
            "date": "2012-11-19T07:36:59+0000",
            "content": "We had something similar in the past (called PayloadProcessor), which was removed completely in 4.0 (without \"replacement\"). The reason was, that the stuff can be implemented inside a FilterAtomicReader and used with IW#addIndexes(IndexReader...). I agree with Shai, that this should be enough for most cases, especially as gradually merging segments can corrumpt your index if you have an error.\n\nIf you really want to merge in-place:\nYour patch has nice ideas from my perspective, only the \"wrapping\" should be done in the MP and not on IndexWriter level (the number of settings in IWConfig is already too big). So the main thing that needs to be done here is:\n\n\tMove the AtomicReader instances into MergePolicy.OneMerge\n\tAs a result, you need to implement a custom wrapper-MergePolicy like UpgradeIndexMergePolicy, that wraps the AtomicReaders when creating the MergePolicy.OneMerge instances.\n\n\n\nAnother possible approach without modification in Lucene core is:\n\n\topen IndexWriter\n\tget NRT Reader and wrap with one or more FilterAtomicReader, or leave as-is, if no upgrade is needed.\n\tdelete the old segments manually (e.g. by deleting all documents)\n\taddIndexes the filtered segments (optionally one-by-one, so it will not merge all atomic readers into one new segment)\n\tcommit\n\n\n\nUwe ",
            "author": "Uwe Schindler",
            "id": "comment-13500073"
        },
        {
            "date": "2012-11-19T13:39:36+0000",
            "content": "The \"gradual\" approach is very much required.\nIts possible that a config change by a user will result in the need to do a filtered reader on a merge.\n\nFor instance, if you index a field without offsets, then you shutdown, start up with indexing of offsets.\nCurrently, this situation will result in newly indexed offsets being obliterated on merge (LUCENE-4557) with no possible way to save them.\n\nEspecially in this case, the addIndexes() approach is way too costly just for a small configuration change.\nSmall config changes shouldn't require the equivalent of a full optimize to take effect.\n\n\nAlso, i argue that any addIndexes() approach is even more dangerous and just as prone to corruption.\nThis can result in the same filtering of readers as the attached patch provides, however it modifies the entire index, thereby causing any corruption to be much more widespread. (of course either way, it is up to the person implementing their custom filter to guarantee that no corruption occurs and that their code produces consistent indexes)\n\n\nI will look into the MergePolicy approach.\nOff hand, it looks like this may still require a patch as the SegmentMerger is currently only aware of SegmentReaders from merging,\nhowever i may be able to add my own SegmentInfo's to the OneMerge replacing the codec with a wrapped codec that will apply my filtering.\nit'll be about a week before i can get back to testing this, i'll report back then.\n\n\n ",
            "author": "Tim Smith",
            "id": "comment-13500226"
        },
        {
            "date": "2012-11-19T14:37:33+0000",
            "content": "Small config changes shouldn't require the equivalent of a full optimize to take effect.\n\nI wouldn't call these \"small config changes\" . And if you give your users the possibility to randomly decide to turn off tracking offsets ... well then either your users are all Lucene highly-involved committers/users, or you're putting too much power in their hands.\n\nI also can't tell, in your world - it's definitely not common in mines, how often will such a configuration be changed. Removing fields no longer needed - that's one thing. Changing IndexOptions, that's a totally different thing.\n\nAlso, i argue that any addIndexes() approach is even more dangerous and just as prone to corruption\n\nRight, but because of how addIndexes works, you get the chance to review the result index before you choose to publish it. If anything is not entirely as expected, you discard the operation, fix the code, start over. The source index remains intact. ",
            "author": "Shai Erera",
            "id": "comment-13500267"
        },
        {
            "date": "2012-11-19T14:45:24+0000",
            "content": "Off hand, it looks like this may still require a patch as the SegmentMerger is currently only aware of SegmentReaders from merging,\n\nThis is not true, you can merge any atomic reader! It may have some optimizations for SegmentReaders, but generally any type of atomic reader can merge into an index (e.g. with addIndexes(IndexReader...) -> which is the proposal by Shai and myself)\n\nAlso, i argue that any addIndexes() approach is even more dangerous and just as prone to corruption.\nThis can result in the same filtering of readers as the attached patch provides, however it modifies the entire index, thereby causing any corruption to be much more widespread. (of course either way, it is up to the person implementing their custom filter to guarantee that no corruption occurs and that their code produces consistent indexes)\n\nRead my comment carefully: You can just trigger a merge of segments that you really want to change. The code would look like:\n\n\topen IndexWriter\n\tget NRT reader and get its atomic leaves: DirectoryReader.open(IndexWriter).leaves()\n\tfilter all leaves, that you are interested in (e.g. my investigating the metadata and version numbers from the leaves' SegmentInfo; assuming they are SegmentReaders -> instanceof check)\n\twrap all leaves that you want to change with your custom filter\n\tdelete all documents by using IndexWriter.deleteAll()\n\tuse addIndexes and pass you list of partially wrapped atomic leaves.\n\tcommit\n\n\n\nThis will trigger something like a forceMerge(1), your resulting index will have one segment (it is optimized). This approach is as heavy as your merge approach, because in your do-it-on-merge you have to at least forcefully merge all segments to upgrade your index (e.g. call forceMerge(1)). ",
            "author": "Uwe Schindler",
            "id": "comment-13500273"
        },
        {
            "date": "2012-11-19T14:49:33+0000",
            "content": "One addition about your testcase extending FilterAtomicReader:\nThe filter reader there does not filter doc values or stored fields. We currently have a full-featured FieldFilterAtomicReader (http://lucene.apache.org/core/4_0_0/test-framework/org/apache/lucene/index/FieldFilterAtomicReader.html) in the Lucene test-framework, that can take a list of fields to exclude or include. It is used in some merge tests. My proposal was already to make this really useful class public. ",
            "author": "Uwe Schindler",
            "id": "comment-13500274"
        },
        {
            "date": "2012-11-19T14:49:56+0000",
            "content": "offsets can be used for highlighting\nusers want to configure highlighting per field\nusers don't always know what fields they want to highlight and may change this setting frequently\nsetting \"highlighting=true\" on a field should be fully possible without full reindex required (old documents of course will not be highlighted, or may default to a slower highlighting method that does not require offsets) (slowly refeeding old documents will allow users to get full functionality for old docs as well, however refeeding may take weeks and should not impact indexing of new content)\n\ni can't proactively always enable offsets on the off chance they will enable highlighting in the future as this implies additional disk requirements\n\nthis is the primary use case that spawned this ticket\nright now, due to the merging behavior, i cannot use indexed offsets for highlighting as a setting change will result in merges destroying offsets.\n\nthis filtering merge reader approach also fulfills other requirements i have for migrating old indexed content to use new features so it would be a win-win for me to use this filtered merge reader approach to ensure consistency and conformance with my schema.\n ",
            "author": "Tim Smith",
            "id": "comment-13500275"
        },
        {
            "date": "2012-11-19T14:53:27+0000",
            "content": "Thats not really true Tim.\n\nYou must be VERY CAREFUL when deciding upon index options.\nIts been this way since even 2.x, if you omitTF, then later decide you want TF and positions, you need to re-index.\nThats why these are expert options. ",
            "author": "Robert Muir",
            "id": "comment-13500277"
        },
        {
            "date": "2012-11-19T15:10:33+0000",
            "content": "Its been this way since even 2.x, if you omitTF, then later decide you want TF and positions, you need to re-index.\n\nre-index is the key word here\nre-indexing is not something that can always be done, or implies a massive cost.\nchanging a schema setting for one field should not require a full re-index.\ni'm afraid i'm in a world where re-index is a 4 letter word and should only be done in the most extreme of circumstances.\nmy whole point here is that migration should be possible via a pluggable policy\n\nThats why these are expert options.\n\ni know these are expert options, but there should also be a means to support migration to new settings (albeit an expert means to do so that may have some consequences for how old documents were indexed)\n\n\n\n\n\n\n\n ",
            "author": "Tim Smith",
            "id": "comment-13500291"
        },
        {
            "date": "2012-11-19T15:17:10+0000",
            "content": "Then it sounds like in your world, where you don't know how text should be indexed, you shouldn't use such advanced options.\n\nInstead maybe you should index in a generic way, e.g. using the classic highlighting techniques.\nThese index options are ways to customize what information the index should retain in situations where you know this.\n\nThere is no migration strategy at all if you choose the wrong options: you threw that information away, so you need to re-index. ",
            "author": "Robert Muir",
            "id": "comment-13500296"
        },
        {
            "date": "2012-11-19T15:26:17+0000",
            "content": "this filtering merge reader approach also fulfills other requirements i have for migrating old indexed content to use new features so it would be a win-win for me to use this filtered merge reader approach to ensure consistency and conformance with my schema.\n\nI just have to repeat myself: We dont need to modify Lucene to make this \"migration\" work! See my last comment, it lists the whole workflow to do this \"merge\" on the directory index without the need for temporary index instances, completely on-the-fly. Your testcase can be rewritten to exactly do what you want without any custom merge policy / SegmentMerger stuff. ",
            "author": "Uwe Schindler",
            "id": "comment-13500311"
        },
        {
            "date": "2012-11-19T15:33:36+0000",
            "content": "A migration strategy does exist and is very simple. It is up to the implementer to determine how data will be migrated and properly communicate that to the user base so expectations are set properly. All migration will have pros and cons, and my require gradual reindexing of content to ensure consistency for old documents. but this is up to the implementer, and shouldn't be imposed by the lucene apis.\n\nLets analyze the highlighting case based on indexed offsets.\n\nAssume documents were indexed with no offsets.\nHighlighting was being done for these documents using tokenstream based highlighting based on stored field text.\n\nNow, the user switches to using a more efficient offsets based highlighting.\nnew documents will be indexed with offsets.\n\nRight now, assuming no merging was done, it is very easy to see if a document has indexed offsets and on a per-document basis documents can be highlighted according to what was indexed.\n\nThen a merge happens. (currently, this will force tokenstream based highlighting for all documents, undoing the configuration setting)\n\nIf applying a migration policy, old documents can have 0,0 offsets applied. (this is the decision of the migration policy and is up to the implementer of the migration policy)\nNow, when highlighting is applied, if all positions have a 0,0 offset for a document, it can fall back to tokenstream based highlighting.\nif positions have offsets, it will use them to perform optimal, full-featured highlighting.\n\nThis will result in slightly slower highlighting for old documents.\nuser experience can then be improved by doing a gradual reindex of old documents, without requiring user to blast away their existing index.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n ",
            "author": "Tim Smith",
            "id": "comment-13500318"
        },
        {
            "date": "2012-11-19T15:38:47+0000",
            "content": "Here is your testcase rewritten to do the same without modifying any line of code in Lucene!\n\nPlease note that RemoveFieldReader is incomplete (does not remove stored fields or doc values), the test should use FieldFilterAtomicReader instead (which is completely removing fields). ",
            "author": "Uwe Schindler",
            "id": "comment-13500321"
        },
        {
            "date": "2012-11-19T15:39:33+0000",
            "content": "Uwe,\n\ni plan to investigate your suggestions, and it may result in not requiring any additional patching to lucene.\nit'll be about a week before i can get to that and i will post my results then.\ni still don't see the addIndexes() approach as viable, even how you suggest as that will require up front migration steps instead of gradual migration. The merge policy approach you suggested will likely be more useful to me, however this will be a nasty merge policy. ",
            "author": "Tim Smith",
            "id": "comment-13500322"
        },
        {
            "date": "2012-12-20T16:28:06+0000",
            "content": "i found a 100% pure codec approach for providing all the functionality i require here and more, requiring no patches\n\n\nif any committer has interest in pushing this ticket forward, i can clean up patch/add suggestions, etc, otherwise this ticket can be closed ",
            "author": "Tim Smith",
            "id": "comment-13537132"
        },
        {
            "date": "2012-12-20T17:15:05+0000",
            "content": "Tim is the codec approach you used generalizable such that we can benefit from it? Do you wanna open another issue for that? ",
            "author": "Simon Willnauer",
            "id": "comment-13537167"
        },
        {
            "date": "2012-12-20T18:14:51+0000",
            "content": "codec approach i'm taking is pretty specific, incorporating my schema/configuration to allow migrating/enhancing options/features/indexing formats/etc (still exploring all the possibilities)\n\nthere may be a few things that would reduce the overhead/enhance the ease the implementation.\ni will create new tickets with patches as i identify them.\n\nNOTE: the codec api is very nice. congrats to all involved in making that happen. ",
            "author": "Tim Smith",
            "id": "comment-13537223"
        }
    ]
}