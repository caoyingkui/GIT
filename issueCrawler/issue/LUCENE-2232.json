{
    "id": "LUCENE-2232",
    "title": "Use VShort to encode positions",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Not A Problem",
        "status": "Resolved"
    },
    "description": "Improve decoding speed for typical case of two bytes for a delta position at the cost of increasing the size of the proximity file.",
    "attachments": {
        "LUCENE-2232-nonbackwards.patch": "https://issues.apache.org/jira/secure/attachment/12431243/LUCENE-2232-nonbackwards.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-01-24T11:53:59+0000",
            "content": "From at LUCENE-1410, yesterday 23 Jan 2010:\n\n(Zhang, 2008) The poor speed of variable- byte on position data is primarily due\n to the fact that position values are larger and more often require\n 2 bytes under variable-byte; this case tends to be much slower due\n to a branch mispredict.\n\nSo using VShort (1 bit as in VByte and 15 bits data) should speed up the proximity decoding when compared to the current VByte encoding (1 bit flag, 7 bits data).\n\nThe disadvantage is a larger size of the proximity file. When only small document fields are present, the proximity file would double in size. For larger documents fields about 5-6% size increase is expected, see for example the prx statistics posted at LUCENE-1410. ",
            "author": "Paul Elschot",
            "id": "comment-12804228"
        },
        {
            "date": "2010-01-24T12:23:53+0000",
            "content": "The patch of 24 Jan 2010 is not entirely clean, there are some leftovers of other issues, but these won't hurt for performance testing.\n\nThis still fails TestBackwardsCompatibility, as expected, but to my surprise all other tests pass.\nThe refactoring for flexible indexing has really worked to keep this patch small.\n\nThe index must have been built with the patch applied for this to work.\n\nNo performance tests to be reported yet... ",
            "author": "Paul Elschot",
            "id": "comment-12804231"
        },
        {
            "date": "2010-01-24T21:00:03+0000",
            "content": "I tried running with the patch applied on contrib/benchmark using the reuters data and sloppy phrase queries there. That did not really show any performance difference. Some further word counting revealed that the av. number of word in a reuters article is just below 129, which puts it in the category of small fields for which hardly any performance difference is expected for queries.\nBuilding the index was somewhat slower, as expected.\n\nThen I tried doing using sloppy phrase queries on the wiki data. But I ran into an \"org.xml.sax.SAXParseException: Content is not allowed in prolog\" while trying to build an index from enwiki.txt. I think I'm doing something completely wrong here, but I have no idea how to improve.\nCould anyone give me some tips on how to continue with this?\nAre there enough wiki articles of at least say 256 words in the 20070527 wiki dump?\nIs there a way to generate interesting sloppy phrase queries for the wiki data? ",
            "author": "Paul Elschot",
            "id": "comment-12804299"
        },
        {
            "date": "2010-01-28T13:24:31+0000",
            "content": "Clean patch changing position delta encoding from vbyte to vshort. ",
            "author": "Paul Elschot",
            "id": "comment-12805913"
        },
        {
            "date": "2010-01-28T13:31:43+0000",
            "content": "Meanwhile I've been looking around some more through contrib/benchmark.\nI think I can make an index from the dump of the wiki articles, and the articles seem to be large enough.\nThe only question left is how to get/generate interesting (sloppy) phrase queries, any ideas there?\n\nAlso, if anyone has a test on real data with fields of at least 128 words and test queries including phrase\nqueries (anything that uses the position data) I'd like to hear about how this goes (index size, query performance).\nI'm expecting a decent speedup for those cases, but I have no idea how big the speedup would be. ",
            "author": "Paul Elschot",
            "id": "comment-12805916"
        },
        {
            "date": "2010-01-29T08:15:41+0000",
            "content": "Aside: the patch has an unrolled version of IndexInput.getVShortInt() into 3 explicit getShort()'s.\nHas it been tried to unroll getVInt()? Unrolling getVInt into 5 explicit getByte()'s has the advantage\nthat the later conditional jumps are easier to predict. ",
            "author": "Paul Elschot",
            "id": "comment-12806275"
        },
        {
            "date": "2010-02-01T18:31:20+0000",
            "content": "I have a little bit of sampling profiling data from YourKit that may be relevant. (Paul encouraged me to post anyway.) Note that the queries submitted were not limited to those requiring PRX data, although some of them (30%? 40%?) did. This data is without applying this LUCENE-2232 patch. YourKit was set to time java.io.RandomAccessFile.readBytes and .read with wall clock time.\n\n1. I replayed about 1000 queries taken from our user query logs on a test system that uses rotating drives, without first submitting any battery of warmup queries.\n\n\n SegmentTermPositions.readDeltaPosition()\n     IndexInput.readVInt() <----------\n\n\n\nI looked at the time spent in the marked call to IndexInput.readVInt(). 93% of the time in this readVint() was spent in I/O, leaving a maximum of 7% that could theoretically be wasted on the CPU decoding VInts.\n\n2. I profiled one of our live Solr servers that uses SSD drives, after the system had warmed up a bit. Here is the resulting profiling data, with times relative to SegmentTermPositions.readDeltaPosition():\n\n\nSegmentTermPositions.readDeltaPosition() - 100%\n  IndexInput.readVInt - 100%\n    BufferedIndexInput.readByte - 69%\n      BufferedIndexInput.refill - 69%\n        SimpleFSDirectory$SimpleFSIndexInput.readInternal - 69%\n          java.io.RandomAccessFile.read - 55%\n          java.io.RandomAccessFile.seek - 14%\n\n\n\nHere we have a healthier 31% of the time that could potentially be sped up by this patch. It partly depends on how much the patch would increase I/O, though. (I guess the hope is that it wouldn't increase I/O by too crazy amount if your documents are above a certain size.)\n\nUPDATE: For context on run #2, \n\n\n\t1,864,186ms total spent under solr.search.SolrIndexSearcher.search\n\t1,633,612ms total spent under lucene.search.IndexSearcher.search\n\t571,254ms total spent under lucene.index.SegmentTermPositions.readDeltaPositions\n\t\n\t\tOf this, about 18,500ms were from SegmentMerger.appendPostings, rather than from searches/highlighting\n\t\n\t\n\t1,330,565ms total spent under IndexInput.readVInt().\n\n\n\n(These are all \"time with children\", rather than \"own time\".) ",
            "author": "Chris Harris",
            "id": "comment-12828210"
        },
        {
            "date": "2010-02-01T18:50:10+0000",
            "content": "Has it been tried to unroll getVInt()? Unrolling getVInt into 5 explicit getByte()'s has the advantage that the later conditional jumps are easier to predict.\n\nIIRC, there was a JIRA issue in the past that tried this.  It showed a speedup in micro-benchmarks but a slowdown when it was actually used in lucene for something like termdocs. ",
            "author": "Yonik Seeley",
            "id": "comment-12828218"
        },
        {
            "date": "2010-02-01T20:07:09+0000",
            "content": "Yonik:\n\nI searched for that past issue bit, it was probably LUCENE-639 on what was then readVInt.\nThe issue was closed as won't fix, inconclusive.\nOne could conclude that it is probably not worthwhile to unroll the loop here, I'll provide another\nversion of the patch for that.\n\nChris:\n\nIndeed the hope is that for larger docs the amount of I/O will hardly increase.\n\nIt is good to know that readDeltaPosition spends 93% of time in readVInt when using a disk.\nIt might well be that the C++ compiler used by Zhang is not as good in preventing jumps that are difficult to predict\nas the current Hotspot JIT.\n\nThat 93% in readVInt falling to 69% on SSD is a nice confirmation that moving from disk to SSD makes\ndecoding speed more important.\n\nCould you also measure how much time is spent in total in query search, for example in IndexSearcher.search() ?\nThat would give an indication of an upperbound on the practical gain from this.\n\nAside: using PFOR for positions should also reduce reading time, and maybe disk seeking time, too. ",
            "author": "Paul Elschot",
            "id": "comment-12828251"
        },
        {
            "date": "2010-02-01T20:11:39+0000",
            "content": "While looking around for unrolling I also found HARMONY-3076 on unrolling in 2007.\nAny ideas on whether Harmony would be worth trying here? ",
            "author": "Paul Elschot",
            "id": "comment-12828256"
        },
        {
            "date": "2010-02-02T08:36:00+0000",
            "content": "After the update on the profile, the upperbound on practical gain for from the profile data would be about:\n1/3 of total IndexSearcher.search time spent in readDeltaPosition(),\nof this, 7% is used for decoding the VInt when using a disk, and 31% when using SSD.\nSo the upperbound on the gain is about 2,3% on a disk, and 10% when using SSD.\n\nI'll try and give a rought estimation based on CPU cycles of actual savings.\n(Add the usual disclaimers here.)\n\nAssuming delta positions typically take two bytes:\nreadVInt does 2 getByte, 1 shift, 2 masks + cond. jumps.\nreadVShortInt does 2 getByte, 1 shift, 1 mask + cond. jump (this patch does not avoid the shift).\nSo the difference is 1 mask + cond. jump on 2 getByte, 1 shift, 2 mask + cond. jump.\nWhen getByte() is inlined, it reduces to an incremented index array read and a test.\nExcluding the cond. jumps, each mask/shift/index/increment should take about one clock cycle\nwhen no I/O is necessary.\nAnother assumption: a mispredicted branch takes 6 cycles, a correctly predicted branch takes 0.\n\nAssuming all branches are predicted correctly, the total #cycles for readVInt would be 7.\nand 1 mask would be saved by the patch, or about 1/7 of time spent in readVInt (14%).\nWith one mispredicted branch, readVInt would take 13 cycles, of which 6 would be saved (just under 50%)\nWith two mispredicted branches, 19 cycles, of which 12 would be saved. (about 63%).\n\nOverall (search time) on a disk, from 1/3 * 7% * 14% = 0.3 % to 1/3 * 7% * 64% = 1.4 % would be saved.\nOn SSD, this would be about 4,5  (=31/7) times as much, 1.3% to 5.8%.\n\nSo on a disk, there is probably no loss or gain because of the extra i/o for the initial positions,\nOn ssd this patch could help a few percent, depending on how badly the branches in readVInt are predicted now. ",
            "author": "Paul Elschot",
            "id": "comment-12828535"
        },
        {
            "date": "2010-02-02T14:35:25+0000",
            "content": "A branch mispredict can be more costly, depending on the context.\nCache misses are a major factor, and allowing speculative execution further ahead in the code (by correctly predicting branches) one can overlap those cache misses.\nhttp://www.infoq.com/presentations/click-crash-course-modern-hardware\n\nThe complexity of modern hardware increases the need to do performance tests in context (unless it's clearly a win of course). ",
            "author": "Yonik Seeley",
            "id": "comment-12828633"
        },
        {
            "date": "2010-02-02T16:53:29+0000",
            "content": "Interesting presentation. It seems the time has come to add skip info/function to the proximity data and scoring. Use memory cache lines sparingly... ",
            "author": "Paul Elschot",
            "id": "comment-12828665"
        },
        {
            "date": "2010-03-19T16:06:50+0000",
            "content": "The main advantage of changing the index formats nowadays comes from better compression, and this patch gives a higher decoding speed at the cost of less compression, so it is probably not worthwhile to pursue this any further. ",
            "author": "Paul Elschot",
            "id": "comment-12847410"
        }
    ]
}