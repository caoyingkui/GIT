{
    "id": "SOLR-9671",
    "title": "TestMiniSolrCloudCluster blowup jvm with remote /get requests",
    "details": {
        "components": [],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Cannot Reproduce",
        "priority": "Major"
    },
    "description": "this is epic https://jenkins.thetaphi.de/job/Lucene-Solr-6.x-Linux/1994/\nThere is no many cores, I checked. It seems like cluster blow up when tries to launch after collection remove. Haven't tried to reproduce it locally",
    "attachments": {
        "TestMiniSolrCloudCluster-testCollectionCreateSearchDelete-fail.zip": "https://issues.apache.org/jira/secure/attachment/12834642/TestMiniSolrCloudCluster-testCollectionCreateSearchDelete-fail.zip",
        "TestMiniSolrCloudCluster-testCollectionCreateSearchDelete-fail-brief.txt": "https://issues.apache.org/jira/secure/attachment/12834647/TestMiniSolrCloudCluster-testCollectionCreateSearchDelete-fail-brief.txt",
        "SOLR-9671-no-luck-6x.patch": "https://issues.apache.org/jira/secure/attachment/12834892/SOLR-9671-no-luck-6x.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-10-20T15:29:08+0000",
            "author": "Mikhail Khludnev",
            "content": "when TestMiniSolrCloudCluster.testCollectionCreateSearchDelete() tries to create collection last time. we have wait loop \n(qtp1915946497-3004) [    ] o.a.s.h.a.CoreAdminOperation Checking request status for : a4310491-abb0-4d8d-a290-2f8d2a909be7241809864829983\nand\n(parallelCoreAdminExecutor-1329-thread-1) [    ] o.a.s.u.PeerSync PeerSync: core=testcollection_shard2_replica2 url=http://127.0.0.1:42320/solr DONE.  We have no versions.  sync failed.\n and then it just \njava.lang.OutOfMemoryError: Java heap space\n\nDo you have any clues to troubleshoot?  ",
            "id": "comment-15592134"
        },
        {
            "date": "2016-10-21T08:43:13+0000",
            "author": "Mikhail Khludnev",
            "content": "thread dump is full of hanging remote /get requests\n\n 696703 ERROR (qtp1915946497-10408) [    ] o.a.s.s.HttpSolrCall null:org.apache.solr.common.SolrException: Error trying to proxy request for url: http://127.0.0.1:42320/solr/testcollection_shard2_replica1/get\n   [junit4]   2>        at org.apache.solr.servlet.HttpSolrCall.remoteQuery(HttpSolrCall.java:590)\n   [junit4]   2>        at org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:444)\nThere are two container which are stuck on that :42320 and :36441. There is no stack of thread originates these requests..   ",
            "id": "comment-15594495"
        },
        {
            "date": "2016-10-21T12:46:01+0000",
            "author": "Mikhail Khludnev",
            "content": "TestMiniSolrCloudCluster-testCollectionCreateSearchDelete-fail-brief.txt clarifies the case\n\nparallelCoreAdminExecutor-1321-thread-1 creates testcollection_shard2_replica1\nparallelCoreAdminExecutor-1329-thread-1 creates  testcollection_shard2_replica2 \n\nbut the parallelCoreAdminExecutor-1321-thread-1 (replica1) will never appear in logs until death from OOME heap space \nAnyway parallelCoreAdminExecutor-1329-thread-1 seems try to sync shard2_replica2 with stalled shard2_replica1, and then give up \no.a.s.c.ShardLeaderElectionContext We failed sync, but we have no versions - we can't sync in that case - we were active before, so become leader anyway\n\n but the problem is that it saturate heap with \n\n749534 ERROR (qtp1915946497-6736) [    ] o.a.s.s.HttpSolrCall null:org.apache.solr.common.SolrException: Error trying to proxy request for url: http://127.0.0.1:42320/solr/testcollection_shard2_replica1/get\n   [junit4]   2>        at org.apache.solr.servlet.HttpSolrCall.remoteQuery(HttpSolrCall.java:590)\n   [junit4]   2>        at org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:444)\n\nfor me it's strange that it issues \"remoteQueries\" ie talks to a replica through other peers, and it's the only explanation why we have so many of them hanging on read - it seems like two nodes calls each other until heap saturation. WDYT? ",
            "id": "comment-15594974"
        },
        {
            "date": "2016-10-23T20:39:03+0000",
            "author": "Mikhail Khludnev",
            "content": "I still can't catch it. I added a delay into CoreContainer.create(CoreDescriptor, boolean) right after {{ zkSys.getZkController().preRegister(dcore);}} for one core (testcollection_shard2_replica2). I've got \nPeerSync PeerSync: core=testcollection_shard2_replica1 url=http://127.0.0.1:52001/solr START replicas=http://127.0.0.1:52182/solr/testcollection_shard2_replica2/ nUpdates=100 \no.a.s.c.ShardLeaderElectionContext I am the new leader: http://127.0.0.1:52001/solr/testcollection_shard2_replica1/ shard2\nhowever between these lines we see a protection from infinite remote calls:\nERROR (qtp2143243594-237) [n:127.0.0.1:52182_solr    ] o.a.s.s.HttpSolrCall got /testcollection_shard2_replica2/get=> null\nbut this didn't happen at the failed job. hmm....\n ",
            "id": "comment-15600279"
        },
        {
            "date": "2016-10-23T21:08:47+0000",
            "author": "Mikhail Khludnev",
            "content": "delaying preRegister for one of cores is pointless, because it's recognized too\no.a.s.c.SyncStrategy http://127.0.0.1:57253/solr/testcollection_shard1_replica1/ has no replicas ",
            "id": "comment-15600315"
        },
        {
            "date": "2016-10-24T07:45:39+0000",
            "author": "Mikhail Khludnev",
            "content": "attaching SOLR-9671-no-luck-6x.patch with a test injection, if someone wants to continue. So, far I can't reproduce.   ",
            "id": "comment-15601258"
        }
    ]
}