{
    "id": "SOLR-10873",
    "title": "Explore a utility for periodically checking the document counts for replicas of a shard",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "We've had several situations \"in the field\" and on the user's list where the number of documents on different replicas of the same shard differ. I've also seen situations where the numbers are wildly different (two orders of magnitude). I can force this situation by, say, taking down nodes, adding replicas that become the leader then starting the nodes back up. But it doesn't matter whether the discrepancy is a result of \"pilot error\" or a problem with the code, in either case it would be useful to flag it.\n\nStraw-man proposal:\nWe create a processor (modeled on DocExpirationUpdateProcessorFactory perhaps?) that periodically wakes up and checks that each replica in the given shard has the same document count (and perhaps other checks TBD?). Send some kind of notification if a problem was detected.\n\nIssues:\n1> this will require some way to deal with the differing commit times. \n1a> If we require a timestamp on each document we could check the config file to see the autocommit interval and, say, check NOW-(2 x opensearcher interval). In that case the config would just require the field to use be specified.\n1b> we could require that part of the configuration is a query to use to check document counts. I kind of like this one.\n\n2> How to let the admins know a discrepancy was found? e-mail? ERROR level log message? Other?\n\n3> How does this fit into the autoscaling initiative? This is a \"monitor the system and do something\" item. If we go forward with this we should do it with an eye toward fitting it in that framework.\n3a> Is there anything we can do to auto-correct this situation? Auto-correction could be tricky. Heuristics like \"make the replica with the most documents the leader and force full index replication on all the replicas that don't agree\" seem dangerous. \n\n4> How to keep the impact minimal? The simple approach would be for each replica to check all other replicas in the shard. So say there are 10 replicas on a single shard, that would be 90 queries. It would suffice for just one of those to check the other 9, not have all 10 check the other nine. Maybe restrict the checker to be the leader? Or otherwise just make it one replica/shard that does the checking?\n\n5> It's probably useful to add a collections API call to fire this off manually. Or maybe as part of CHECKSTATUS?\n\nWhat do people think?",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2017-06-12T23:31:20+0000",
            "content": " What if count is same but actual data is different. \nCan we use Index fingerprint instead to verify if replicas are in sync?  ",
            "author": "Pushkar Raste",
            "id": "comment-16047218"
        },
        {
            "date": "2017-06-13T00:21:57+0000",
            "content": "Good idea! Plus it seems lightweight as well.... ",
            "author": "Erick Erickson",
            "id": "comment-16047261"
        },
        {
            "date": "2017-06-13T00:46:11+0000",
            "content": "There are other advantages too\n\n\tYou can compute Index fingerprint upto any arbitrary version. Depending on tolerance, you can check if fingerprint matches the last version in the second from last tlog. No need to differ commits in this case\n\n\n\n\n\tIndex fingerprint is cached in SolrCore class and hence even if frequency of sync check is high you may not have recompute fingerprint every single time\n\n\n\n`RealTimeGetcomponent` already supports a call `processGetFingerprint` while working on SOLR-9446\n\n ",
            "author": "Pushkar Raste",
            "id": "comment-16047280"
        }
    ]
}