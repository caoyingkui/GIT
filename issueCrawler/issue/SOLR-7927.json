{
    "id": "SOLR-7927",
    "title": "Indexing large documents requires larger heap than may be necessary",
    "details": {
        "components": [
            "update"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.5",
            "6.0"
        ],
        "affect_versions": "5.2.1",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "Solr is started with 1280M heap.\n\n./bin/solr start -m 1280m\n\nIndexing a 100MB JSON file (using curl) containing large JSON documents from project Gutenberg fails with OOM but indexing a 549M JSON file containing small documents is indexed just fine.\n\nThe same 100MB JSON file with the same heap size can be indexed just fine if I disable the transaction log.",
    "attachments": {
        "SOLR-7927.patch": "https://issues.apache.org/jira/secure/attachment/12750980/SOLR-7927.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-08-14T15:45:27+0000",
            "author": "Yonik Seeley",
            "content": "Hmmm, the transaction log uses an 8K buffer (IIRC) so the memory use really should be limited to serializing it with JavaBin...\nIs there a really large string in the document?  Those are serialized to a temporary byte buffer of max size:\n\n    int maxSize = end * 4;\n    if (bytes == null || bytes.length < maxSize) bytes = new byte[maxSize];\n    int sz = ByteUtils.UTF16toUTF8(s, 0, end, bytes, 0);\n\n\n\nWe should have never moved away from the old string format (CESU-8 + number of java characters), it's just so much more efficient for Java since no temporary byte buffer is needed and you can stream-encode a string directly since you already know the length.\n\nThis same cost would be paid by a leader forwarding to replicas too. ",
            "id": "comment-14697254"
        },
        {
            "date": "2015-08-14T18:54:12+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Is there a really large string in the document?\n\nYes, the document has a single large content field (10MB). ",
            "id": "comment-14697550"
        },
        {
            "date": "2015-08-14T19:54:49+0000",
            "author": "Yonik Seeley",
            "content": "Yes, the document has a single large content field (10MB).\n\nOK, so the temporary buffer allocated would grow to 40MB to encode as UTF8.  Are you close enough to the limit that that would push it over?  A 100MB JSON file could also grow to 200MB in memory (single byte vs double byte strings).\n\nDisabling the transaction log will lower some other memory usage as well (like the bucket list in version info, and field cache entries for looking up version, etc?) ",
            "id": "comment-14697649"
        },
        {
            "date": "2015-08-17T21:14:52+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "I apologize Yonik. Actually the test JSON file had a single document of 100MB with a huge content field and not 10 docs of 10MB as I believed earlier. Now the OOM makes more sense but I think we can shave off some extra memory usage.\n\nFor example, JavaBinCodec.writeStr creates a byte array of size 4 * string.length but the same can be done in 3 * string.length as Lucene's CompressingStoredFieldsWriter.writeField() does? ",
            "id": "comment-14700245"
        },
        {
            "date": "2015-08-17T22:58:11+0000",
            "author": "Yonik Seeley",
            "content": "For example, JavaBinCodec.writeStr creates a byte array of size 4 * string.length but the same can be done in 3 * string.length\n\nHmmm, that code (the change from CESU8 & number-of-java-chars to UTF8) was done Mr Unicode Policeman, so it's interesting if it's wrong.\nI don't know myself if there are any 16 bit patterns (i.e. it may not be valid UTF16) that blows up to 4 bytes when encoded as UTF8... the unicode replacement character is only 3 bytes too, so I can't find anything that would result in 4x. ",
            "id": "comment-14700390"
        },
        {
            "date": "2015-08-17T23:21:05+0000",
            "author": "Steve Rowe",
            "content": "The maximum Unicode code point (as of Unicode 8 anyway) is U+10FFFF (http://www.unicode.org/glossary/#code_point).  This is encoded in UTF-16 as surrogate pair \\uDBFF\\uDFFF, which takes up two Java chars, and is represented in UTF-8 as the 4-byte sequence F4 8F BF BF.  This is likely where the mistaken 4-bytes-per-Java-char formulation came from: the maximum number of UTF-8 bytes required to represent a Unicode code point is 4.\n\nThe maximum Java char is \\uFFFF, which is represented in UTF-8 as the 3-byte sequence EF BF BF.\n\nSo I think it's safe to switch to using 3 bytes per Java char (the unit of measurement returned by String.length()), like CompressingStoredFieldsWriter.writeField() does. ",
            "id": "comment-14700420"
        },
        {
            "date": "2015-08-18T07:11:26+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Trivial patch attached to use array of 3 * string.length ",
            "id": "comment-14700833"
        },
        {
            "date": "2015-08-25T16:38:20+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks Steve for clarifying. I opened a sub-task SOLR-7971 to commit this optimization. I think we'll likely need more changes to fully optimize this case (which may be spread across more than one release) so I have decided to keep this issue open as a parent and renamed it. ",
            "id": "comment-14711593"
        },
        {
            "date": "2015-09-03T20:51:40+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "I opened LUCENE-6779 to optimize utf8 encoding in CompressingStoredFieldsWriter in the same way as done for JavaBinCodec in SOLR-7971 ",
            "id": "comment-14729757"
        }
    ]
}