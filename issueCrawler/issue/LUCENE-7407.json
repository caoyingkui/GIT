{
    "id": "LUCENE-7407",
    "title": "Explore switching doc values to an iterator API",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [
            "7.0"
        ],
        "priority": "Major",
        "status": "Resolved",
        "type": "Improvement"
    },
    "description": "I think it could be compelling if we restricted doc values to use an\niterator API at read time, instead of the more general random access\nAPI we have today:\n\n\n\tIt would make doc values disk usage more of a \"you pay for what\n    what you actually use\", like postings, which is a compelling\n    reduction for sparse usage.\n\n\n\n\n\tI think codecs could compress better and maybe speed up decoding\n    of doc values, even in the non-sparse case, since the read-time\n    API is more restrictive \"forward only\" instead of random access.\n\n\n\n\n\tWe could remove getDocsWithField entirely, since that's\n    implicit in the iteration, and the awkward \"return 0 if the\n    document didn't have this field\" would go away.\n\n\n\n\n\tWe can remove the annoying thread locals we must make today in\n    CodecReader, and close the trappy \"I accidentally shared a\n    single XXXDocValues instance across threads\", since an iterator is\n    inherently \"use once\".\n\n\n\n\n\tWe could maybe leverage the numerous optimizations we've done for\n    postings over time, since the two problems (\"iterate over doc ids\n    and store something interesting for each\") are very similar.\n\n\n\nThis idea has come up many in the past, e.g. LUCENE-7253 is a recent\nexample, and very early iterations of doc values started with exactly\nthis \n\nHowever, it's a truly enormous change, likely 7.0 only.  Or maybe we\ncould have the new iterator APIs also ported to 6.x side by side with\nthe deprecate existing random-access APIs.",
    "attachments": {
        "LUCENE-7407.patch": "https://issues.apache.org/jira/secure/attachment/12829030/LUCENE-7407.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15409760",
            "author": "Michael McCandless",
            "date": "2016-08-05T17:29:25+0000",
            "content": "I've been prototyping this, and I just pushed my current state to:\n\n  https://github.com/mikemccand/lucene-solr/tree/dv_iterators\n\nI just added a NumericDocValuesIterator which extends DISI,\nadding a public long longValue() method.  So I can make baby\nsteps, I added getNumericIterator side by side with getNumeric\nin DocValuesProducer with a stupid default implementation.  This\nlets me gradually cutover doc-values consumers to see what interesting\ncases emerge.  Index time sorting was one interesting case, and block\njoin sorting is another.  I'm sure there are others.\n\nI also pushed up throws IOException; I think it's silly we mask\nthis today in doc values.\n\nI'll keep cutting numerics over to assess feasibility... "
        },
        {
            "id": "comment-15409850",
            "author": "Adrien Grand",
            "date": "2016-08-05T18:20:43+0000",
            "content": "Wow, thanks for getting this started! I had a quick look at the diff, it looks great. One minor suggestion I have would be to make the API return a DocIdSetIterator (like Scorer) rather than extend it. In LUCENE-6919 this helped avoid having to wrap all the time, which also made performance better. In the case of doc values, codecs could have one way to get an iterator that would be reused by all doc value types (NUMERIC, SORTED, ...) for instance. "
        },
        {
            "id": "comment-15410479",
            "author": "Otis Gospodnetic",
            "date": "2016-08-06T05:01:25+0000",
            "content": "Can I label this with #AWESOME!!! ? Could Adrien's LUCENE-6928 piggyback on this? "
        },
        {
            "id": "comment-15410605",
            "author": "Michael McCandless",
            "date": "2016-08-06T13:00:59+0000",
            "content": "One minor suggestion I have would be to make the API return a DocIdSetIterator (like Scorer) rather than extend it.\n\nHmm, this seems somewhat awkward?\n\nIt would mean to iterate doc values, you would need to hold onto two\nclasses: the iterator you pulled, and the parent class you pulled it\nfrom.  I think it's also odd that the iterator is altering state in another\nclass.\n\nI realize we did this for performance reasons for Scorer, but it's\nnot great to compromise the API so much for performance unless the\nperformance change is really drastic.  Could that be the case here? "
        },
        {
            "id": "comment-15448637",
            "author": "Michael McCandless",
            "date": "2016-08-30T10:06:53+0000",
            "content": "Just a quick progress update here: I've managed to cut over 4 (Numeric, Binary, Sorted, norms) of the 6 DV classes ... working on SortedSet now, and then finally SortedNumeric.\n\nThen I need to remove the crazy bridge classes, then remove the old (non-iterator) classes, then rename the iterator classes back to the original names.\n\nNet/net I think can work, but I still have plenty of nocommits to sort out. "
        },
        {
            "id": "comment-15449180",
            "author": "David Smiley",
            "date": "2016-08-30T14:36:17+0000",
            "content": "Hi; is this API change only at the Codec level or is it higher level as well? "
        },
        {
            "id": "comment-15449308",
            "author": "Michael McCandless",
            "date": "2016-08-30T15:27:00+0000",
            "content": "Higher level as well: rather than a random access NumericDocValues, you have a NumericDocValuesIterator extending DocIdSetIterator and just adding a longValue method, for example. "
        },
        {
            "id": "comment-15454278",
            "author": "Otis Gospodnetic",
            "date": "2016-09-01T04:30:52+0000",
            "content": "Once these changes are made do you think one will be able to just replace the Lucene jar in e.g. ES 5.x? "
        },
        {
            "id": "comment-15454643",
            "author": "Michael McCandless",
            "date": "2016-09-01T07:45:13+0000",
            "content": "Sorry, I don't think so Otis Gospodnetic: this is a major change, I think it will only be for Lucene 7.0? "
        },
        {
            "id": "comment-15498793",
            "author": "Michael McCandless",
            "date": "2016-09-17T11:01:47+0000",
            "content": "I think this nearly ready!  I've fixed all nocommits, but ant precommit is a bit angry still... I'll fix before pushing.\n\nI'm attaching an applyable patch vs. current master.\n\nAll doc values usage has been switched to iterators instead of random access, and getDocsWithField is gone.\n\nI've done very little to improve the default codec to take advantage of this.  I think there is a lot of fun improvements we can make here, in follow-on issues, so that e.g. LUCENE-7253 (merging of sparse doc values fields) is fixed.\n\nTo write doc values we now pass a DocValuesProducer (instead of N Iterables), and I created legacy deprecated bridge classes (LegacyDocValuesIterables) to turns these back into Iterables for existing codecs.\n\nI also created legacy bridge classes to turn random access DVs into the new iterators. "
        },
        {
            "id": "comment-15499455",
            "author": "David Smiley",
            "date": "2016-09-17T18:21:05+0000",
            "content": "Awesome progress Mike! It's nice to see we don't have to worry anymore about the '0' value being possibly non-existent.\n\nIt appears advance(docId) is inherited from DISI and thus it must be called with sequential docIDs.  Did you run into any spot in the codebase that didn't advance sequentially and so you had to do something different? "
        },
        {
            "id": "comment-15501202",
            "author": "Michael McCandless",
            "date": "2016-09-18T15:56:30+0000",
            "content": "It's nice to see we don't have to worry anymore about the '0' value being possibly non-existent.\n\nRight!\n\nIt appears advance(docId) is inherited from DISI and thus it must be called with sequential docIDs. Did you run into any spot in the codebase that didn't advance sequentially and so you had to do something different?\n\nI was surprised how few places didn't go in order ...\n\nE.g., index sorting was tricky, since it very much relied on random access.\n\nA number of tests also accessed docIDs that came back in hits which of course may not be \"in order\". "
        },
        {
            "id": "comment-15507892",
            "author": "Otis Gospodnetic",
            "date": "2016-09-20T21:47:01+0000",
            "content": "there is a lot of fun improvements we can make here, in follow-on issues, so that e.g. LUCENE-7253 (merging of sparse doc values fields) is fixed.\n\nSo LUCENE-7253 is where the new Codec work for trunk will go?\nDid you maybe create the other issues you mentioned?  Asking because I'm curious what you have in mind and so I can link+watch. "
        },
        {
            "id": "comment-15509949",
            "author": "Michael McCandless",
            "date": "2016-09-21T13:42:45+0000",
            "content": "Did you maybe create the other issues you mentioned? \n\nI haven't created a follow-on issue yet ... I will soon.  I think it should be separate from LUCENE-7253. "
        },
        {
            "id": "comment-15510176",
            "author": "David Smiley",
            "date": "2016-09-21T14:48:14+0000",
            "content": "BTW one way that this commit could have been less massive it to split out the \"throws IOException\" additions as one change, and then subsequently get to the meat of the work here.  Any way, that's water under the bridge. "
        },
        {
            "id": "comment-15510634",
            "author": "Michael McCandless",
            "date": "2016-09-21T17:44:13+0000",
            "content": "BTW one way that this commit could have been less massive it to split out the \"throws IOException\" additions as one change\n\nThat's a good point  "
        },
        {
            "id": "comment-15510660",
            "author": "Michael McCandless",
            "date": "2016-09-21T17:52:49+0000",
            "content": "I opened LUCENE-7457 for the (important!) follow-on. "
        },
        {
            "id": "comment-15513376",
            "author": "ASF subversion and git services",
            "date": "2016-09-22T14:02:50+0000",
            "content": "Commit 7377d0ef9ea8fa9e2aa9a3ccb1249703d8d1d813 in lucene-solr's branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7377d0e ]\n\nLUCENE-7407: fix stale javadocs "
        },
        {
            "id": "comment-15516007",
            "author": "ASF subversion and git services",
            "date": "2016-09-23T09:57:04+0000",
            "content": "Commit 53dd74bd870437ecee0483096e3ef5669d844e57 in lucene-solr's branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=53dd74b ]\n\nLUCENE-7407: fix stale javadocs "
        },
        {
            "id": "comment-15533559",
            "author": "Steve Rowe",
            "date": "2016-09-29T18:10:50+0000",
            "content": "This change is implicated in a Solr test failure: SOLR-9582 "
        },
        {
            "id": "comment-15549552",
            "author": "ASF subversion and git services",
            "date": "2016-10-05T18:19:11+0000",
            "content": "Commit 001a3ca55b30656e0e42f612d927a7923f5370e9 in lucene-solr's branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=001a3ca ]\n\nLUCENE-7407: speed up iterating norms a bit by having default codec implement the iterator directly "
        },
        {
            "id": "comment-15551218",
            "author": "Adrien Grand",
            "date": "2016-10-06T07:44:03+0000",
            "content": "It looks like this last change helped significantly: http://people.apache.org/~mikemccand/lucenebench/Term.html "
        },
        {
            "id": "comment-15551602",
            "author": "Michael McCandless",
            "date": "2016-10-06T10:47:44+0000",
            "content": "It looks like this last change helped significantl\n\nNice  "
        },
        {
            "id": "comment-15584286",
            "author": "Yonik Seeley",
            "date": "2016-10-18T03:17:00+0000",
            "content": "It seems like this has had a rather large impact on what was supposed to be the common case: dense doc values.  See SOLR-9599 for some performance numbers, currently standing at 40% slower for faceting as of today, but also impacting all other docvalue uses also tested (sorting and function queries).\nPerhaps we should have both a random access API as well as an iterator API?\n\nWhat do people thing about the right path forward? "
        },
        {
            "id": "comment-15584729",
            "author": "Adrien Grand",
            "date": "2016-10-18T07:26:38+0000",
            "content": "I agree there are things to be improved there (see LUCENE-7462 too).\n\nThe comparison might not be entirely fair to the new API since the Lucene54 format was really designed with the old random-access API in mind. I'm wondering how much we can get back by more naturally implementing the new API. But I suspect we will have to do more to get back to performance that is close to what we had before, at least in the dense case. To me there are two ways that we can do it:\n\n\tadjust the iterator API of doc values to require less search-time work. Because of the advance() semantics, we currently need to guard all value accesses under something that looks like this:\n\n      int curDocID = docTerms.docID();\n      if (doc > curDocID) {\n        curDocID = docTerms.advance(doc);\n      }\n      if (doc == curDocID) {\n        // handle value\n      } else {\n        // handle missing value\n      }\n\n\n(copied from FieldComparator). The advance() semantics both return the next document that has a value, which we never need at search time so this is an unnecessary effort from the codec, and also require that the target is strictly beyond the current document, which prevents from calling advance(doc) blindly: we need to check whether the iterator is on the current document or beyond already. Maybe we could have instead something like an advanceExact(target) method that would only advance to the target document and return whether it has a value.\n\n\n\thave a 2nd DV API that looks like the old API (with the additional constraint that doc ids need to be consumed in order) and helpers in the DocValues class to convert from dv producers with an iterator API to this random-access API (the LUCENE-7462 proposal). When the codec specializes the dense case (which would be always the case with the default codec), the conversion would only unwrap the iterator to return a random-access API. And otherwise it would wrap in order to check the current doc ID and advance if necessary (like almost every consumer of the doc values APIs needs to do now in master).\n\n "
        },
        {
            "id": "comment-15585008",
            "author": "Michael McCandless",
            "date": "2016-10-18T09:33:02+0000",
            "content": "Hmm, in Lucene's nightly perf tests, the TermDateFacets got only a bit slower (~11%), not 40% slower.  Yonik, can you give more details on your benchmark so others can run it?\n\nWe should also look at the Solr faceting code to see if it can be improved on how it's using the DV iterators; I just did a quick cutover to the iterator API for this issue, and maybe there's something inefficient there.\n\nI don't think Lucene should have two APIs (LUCENE-7462).  This will lead to too much bifurcation of the code that consumes doc values.  That's the wrong tradeoff, and we shouldn't let performance mess up our APIs that heavily.\n\nThat said, I think an advanceExact would be a good middle ground, if we show it can in fact help performance.  We could make a simple default impl in either DISI or maybe a new base class for all doc values impls. "
        },
        {
            "id": "comment-15585202",
            "author": "Yonik Seeley",
            "date": "2016-10-18T11:19:12+0000",
            "content": "Hmm, in Lucene's nightly perf tests, the TermDateFacets got only a bit slower (~11%), not 40% slower. Yonik, can you give more details on your benchmark so others can run it?\n\nAmdahl's law? My tests are probably just isolating the docValues performance more.  These are full-stack tests (on both sides?)... so it may be that TermDateFacets spends less of it's execution time actually retrieving docValues, and has more bottlenecks elsewhere.  I'm also effectively cutting out the query portion (finding the root domain) by reusing the same base query each time (thus it will be cached).\n\nActually, if I test a field with a cardinality of 1M, the performance drop is on the order of 12% for me too.  The biggest contributor is most likely a higher cost to find the top N entries (the count array will have 1M entries) that is unrelated to the docvalues implementation.\n\nAs far as replicating some of these results... I think most of the relevant details (including what exact queries look like) in SOLR-9599.\nProbably one of the simplest to replicate at the lucene level is a sorting test:\n\nhttp://localhost:8983/solr/collection1/query?q=*:*%20mydate_dt:NOW&fl=id&sort=s10_s%20desc,%20s100_s%20desc,%20s1000_s%20desc\n\n\nSo basically, do a really inexpensive query that covers pretty much all of the index, and sorts by 3 fields (a field with a cardinality of 10, followed by a tiebreak with cardinality 100, followed by a tiebreak with cardinality 1000).  That helps isolate sorting-by-docvalue performance.  I quickly tested this by hand, and it was 50% slower (I just ran it multiple times and noted the lowest stable times).\n\nThat's the wrong tradeoff, and we shouldn't let performance mess up our APIs that heavily.\nSubjectively, I would chose the other trade-off as it's our job to be fast.  The previous API wasn't bad... it just needed help with sparse values. "
        },
        {
            "id": "comment-15585205",
            "author": "Yonik Seeley",
            "date": "2016-10-18T11:21:16+0000",
            "content": "\nI had missed  LUCENE-7462, thanks! "
        },
        {
            "id": "comment-15585511",
            "author": "Yonik Seeley",
            "date": "2016-10-18T13:55:26+0000",
            "content": "Sorting by numeric docvalues seems to have taken a greater hit (~45% for a low cardinality int field followed by another int field) than sorting by string for some reason. "
        },
        {
            "id": "comment-15588240",
            "author": "Michael McCandless",
            "date": "2016-10-19T09:55:17+0000",
            "content": "Well, your tests are also using synthetically generated data right?  If you run performance tests with synthetic data, you draw synthetic conclusions.\n\nCan you test a real corpus instead?  Maybe try faceting on e.g. city, state, country from geonames?\n\nI'll see if I can add a lower cardinality field from Wikipedia to Lucene's nightly benchmark.\n\nAnd while I appreciate your efforts to isolate doc values performance alone (\"finding the root domain\"), this is also a rather overly synthetic use case.  Most queries involve non-trivial cost, and the overall impact to real world use cases is what matters here.\n\nInstead of \"quickly testing things by hand\" please do a more thorough test, discarding warmup iterations, running N JVMs (to account for hotstpot noise) with M iterations each (to account for other JVM noise) with diverse concurrent query types (to prevent hotspot from falsely over optimizing), etc.\n\nSeparately, in scrutinizing the TermDateFacets charts in the nightly benchmark while digging here, I found a horrible bug in the benchmark code   Caused by me on 5/25 and fixed in last night's run, but still the net before/after looks ~9% lower performance.\n\nFinally, please review the changes I had to do to Solr; maybe there's something silly there: you are the Solr expert here, try to be part of the solution  "
        },
        {
            "id": "comment-15589400",
            "author": "Yonik Seeley",
            "date": "2016-10-19T17:57:53+0000",
            "content": "Well, your tests are also using synthetically generated data right? If you run performance tests with synthetic data, you draw synthetic conclusions.\n\nTesting with real requests is what users should do with their specific requests.  We have no single set of such typical requests...  we have too many users with too many use cases.\nGeneralized synthetic conclusions can be superior to a single \"real world\" use case that fails to cover enough scenarios that real users will encounter.  At first blush, it doesn't look like the lucenebench tests cover sorting and faceting that well.\n\nAnd while I appreciate your efforts to isolate doc values performance alone (\"finding the root domain\"), this is also a rather overly synthetic use case.  Most queries involve non-trivial cost, and the overall impact to real world use cases is what matters here.\n\nI disagree.  If one is measuring performance of a faceting change, then isolate it.  Then you can say \"this change improved faceting performance on large cardinality fields by up to 50%\".\nIf a request is doing other expensive stuff, of course the overall implact will be smaller.  One can make the impact arbitrarily small by adding other more expensive stuff to the request.\n\nAlso, some users out there will experience an impact of that magnitude.  We really don't have a single \"typical\" real-world use case... we have too many users trying to do too many crazy things.  Everything that might be considered a corner case is often represented by real users who depend on that performance.  For example, I've seen plenty of users who try to facet on dozens of fields per request.\n\nInstead of \"quickly testing things by hand\"\n\nA quick test by hand is still more informative than having no information at all.  The accuracy may be lower, but when I see changes of the size I saw, I know that it needs further investigation!\nFor example, I tested function queries (ValueSource) and sorting by multiple docvalue fields.  Are either of these things tested at all in https://home.apache.org/~mikemccand/lucenebench/ ?  The test names suggest that they are not, but it's hard to tell.\nAnd wrt to the by-hand sorting test, I did follow it up with a more thorough test:\nhttps://issues.apache.org/jira/browse/SOLR-9599?focusedCommentId=15584223&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15584223\n\nplease do a more thorough test, discarding warmup iterations, running N JVMs (to account for hotstpot noise) with M iterations each (to account for other JVM noise) with diverse concurrent query types (to prevent hotspot from falsely over optimizing), etc.\n\nYes, I did all that.\nRunning diverse fields in the same JVM run is esp important to prevent hotspot from over-optimizing for a single field cardinality (since different cardinalities have different docvalues encodings).\n\nHow many different numeric fields are concurrently sorted on for https://home.apache.org/~mikemccand/lucenebench/ ?\nThe names suggest just one: \" TermQuery (date/time sort)\"\nIf that is actually the case, then you're in danger of hotspot over-specializing for that single field/cardinality.\n\ntry to be part of the solution \nThat's an unnecessary personal dig.\n\n\tI've  already put in a lot of effort into benchmarking this, only to have it dismissed with hand waves, for cases that may not even be covered (or may be under stated) by your own benchmarks.\n\tI fully intend to dig into the solr side, but I was waiting until the API stabilizes (LUCENE-7462)\n\tI pointed at specific examples that reside entirely in lucene code (the sorting examples)\n\n\n\n "
        },
        {
            "id": "comment-15594967",
            "author": "Michael McCandless",
            "date": "2016-10-21T12:41:56+0000",
            "content": "At first blush, it doesn't look like the lucenebench tests cover sorting and faceting that well.\n\nFor example, I tested function queries (ValueSource) and sorting by multiple docvalue fields. Are either of these things tested at all in https://home.apache.org/~mikemccand/lucenebench/ ?\n\nRunning diverse fields in the same JVM run is esp important to prevent hotspot from over-optimizing for a single field cardinality (since different cardinalities have different docvalues encodings).\n\n\nHow many different numeric fields are concurrently sorted on for https://home.apache.org/~mikemccand/lucenebench/ ?\nThe names suggest just one: \" TermQuery (date/time sort)\"\nIf that is actually the case, then you're in danger of hotspot over-specializing for that single field/cardinality.\n\nThese are all good points, all things that I would like to improve\nabout Lucene's nightly benchmarks\n(https://home.apache.org/~mikemccand/lucenebench/).  Patches welcome \n\nI'll try to add some low cardinality faceting/sorting coverage, maybe\nusing month name and day-of-the-year from the last modified date.\n\nThe nightly Wikipedia benchmark facets on Date field as a hierarchy\n(year/month/day), and sorts on \"last modified\" (seconds resolution I\nthink) and title.\n\nI've also long wanted to add highlighters...\n\nA quick test by hand is still more informative than having no information at all.\n\nI disagree: it's reckless to run an overly synthetic benchmark and\nthen present the results as if they mean we should make poor API\ntradeoffs.\n\nIf one is measuring performance of a faceting change, then isolate it.\n\nIn the ideal world, yes, but this is notoriously problematic to do\nwith java: hotspot, GC, etc. will all behave very differently if you\nare testing a very narrow part of the code.\n\n\nThat's an unnecessary personal dig.\nI've already put in a lot of effort into benchmarking this, only to have it dismissed with hand waves, for cases that may not even be covered (or may be under stated) by your own benchmarks.\nI fully intend to dig into the solr side, but I was waiting until the API stabilizes (LUCENE-7462)\nI pointed at specific examples that reside entirely in lucene code (the sorting examples)\n\nMy point is that running synthetic benchmarks and mis-representing\nthem as \"meaningful\" is borderline reckless, and certainly nowhere\nnear as helpful as, say, improving our default codec, profiling and\nremoving slow spots, removing extra legacy wrappers, etc.  Those are\nmore positive ways to move our project forward.\n\nPerhaps you feel you have put in a lot of effort here, but from where\nI stand I see lots of complaining about how things got slower and little\neffort to actually improve the sources.  This issue alone was a\ntremendous amount of slogging for me, and I had to switch Solr over\nwithout fully understanding its sources: you or other Solr experts\ncould have stepped in to help me then.\n\nBut why not do that now?  I.e. review my Solr changes or function\nqueries, etc.?  I could easily have done something silly: it was just\na \"rote\" cutover to the iterator API.\n\nI think we could nicely optimize the browse only case, by just using\nnextDoc to step through all doc values for a given field.  Does Solr\ndo that today?\n\nWhy not test the patch on LUCENE-7462 to see if that API change helps?\n\nI am not disagreeing that DV access got slower: the Lucene nightly\nbenchmarks also show that.\n\nYet look at sort-by-title: at first it got slower, on initial cutover\nto iterators, but then thanks to Adrien Grand (thank you Adrien!), it's\nnow faster than it was before:\nhttps://home.apache.org/~mikemccand/lucenebench/TermTitleSort.html\n\nWith more iterations I expect we can do the same thing for the other\ndense cases.  An iteration-only API means we can do all sorts of nice\ncompression improvements not possible with the random access API, we\ndon't need per-lookup bounds checks, etc.  We should adopt from the\nmany things we do to compress postings, which have been iterators only\nforever.  And it means the sparse case, as a happy side effect,\nget to improve too.\n\nThis could lead to a point in the future where the dense cases perform\nbetter than they did with random access API, like sort-by-title does\nalready.  We've only just begun down this path, and in just a few\nweeks Adrien Grand has already made big gains. "
        },
        {
            "id": "comment-15595043",
            "author": "David Smiley",
            "date": "2016-10-21T13:19:14+0000",
            "content": "I wouldn't dare suggest to another committer how they should spend their time; it's entirely their prerogative.  That's crossing a line; please stop!  I think we should value all technical input, even if it's bad news (e.g. something got slower).  Building/running a benchmark is being helpful.  I understand if you don't like the benchmark in particular (I'm not going to argue it's a particularly good or bad one) but it's being helpful and it takes time to do these things.  I'd be depressed right now if I were in Yonik's shoes; but hey that's me and we need emotions of steel around here to survive. "
        },
        {
            "id": "comment-15595064",
            "author": "Joel Bernstein",
            "date": "2016-10-21T13:26:08+0000",
            "content": "I think this is progressing in a good way. The initial work was done in master and not backported 6x. The initial work had a performance impact and it's been noted. Now it's time to work on improving performance. I'll be happy to help out with the performance issues. As long we don't have a need to rush out 7.0 then we have some time to improve the performance. "
        },
        {
            "id": "comment-15595113",
            "author": "Yonik Seeley",
            "date": "2016-10-21T13:42:47+0000",
            "content": "> A quick test by hand is still more informative than having no information at all.\nI disagree: it's reckless to run an overly synthetic benchmark and then present the results as if they mean we should make poor API tradeoffs.\n\nWhen I did a quick test by hand, I always disclosed that.  It's a starting point, not an ending point.\nAnd even homogenous tests (that are prone to hotspot overspecialization) are a useful datapoint, if you know what they are.\nSome users will have exactly those types of requests - very homogeneous.\n\nMy point is that running synthetic benchmarks and mis-representing them as \"meaningful\" is borderline reckless\n\nThe implication being that you judge they are not meaningful? Wow.\n\nYou seemed to admit that the lucene benchmarks don't even cover some of these cases (or don't cover them adequately).\n\n\tThere is no single authoritative benchmark, and it's misleading to suggest there is (that somehow represents the true performance for users)\n\tThe lucene benchmarks are also synthetic to a degree (although based off of real data).  For example, the query cache is disabled.  Why?  I assume to better isolate what is being tested.\n\tMore realistic tests are always nice to verify that nothing was messed up... but a system will always have a bottleneck.  The question is which bottleneck are you effectively testing?\n\tMore tests are better.  If others have the time/ability, they should run their own!\n\n\n\n[...] nowhere near as helpful as, say, improving our default codec, profiling and removing slow spots, removing extra legacy wrappers, etc. Those are more positive ways to move our project forward.\n\nThe first step I'd take would be to try and realistically isolate and quantify the performance of what I was trying to change anyway.  I did that starting off with Solr faceting tests (lucene benchmarks don't test that).\n\nI will get around to trying and improve things.. in the meantime putting out the information I did have is better than hiding it.  Take it for what it is.\nIf you choose to just dismiss it as meaningless... well, I guess we'll have to agree to disagree. "
        },
        {
            "id": "comment-15599501",
            "author": "Michael McCandless",
            "date": "2016-10-23T10:49:19+0000",
            "content": "\nI think this is progressing in a good way. The initial work was done in master and not backported 6x. The initial work had a performance impact and it's been noted. Now it's time to work on improving performance. I'll be happy to help out with the performance issues. As long we don't have a need to rush out 7.0 then we have some time to improve the performance.\n\nThank you Joel Bernstein! "
        },
        {
            "id": "comment-15669305",
            "author": "Otis Gospodnetic",
            "date": "2016-11-16T04:00:21+0000",
            "content": "I had a quick look at Yonik Seeley's SOLR-9599 and then at Adrien Grand's patch in LUCENE-7462 that makes the search-time work less expensive.  Last comment from Yonik reporting faceting regression in Solr was from October 18.  Adrien't patch was committed on October 24.  Maybe things are working better for Solr now?\n\nIf not, in interest of moving forward, what do people think about Yonik's suggestion:\nPerhaps we should have both a random access API as well as an iterator API?\n? "
        },
        {
            "id": "comment-15673391",
            "author": "Michael McCandless",
            "date": "2016-11-17T10:51:56+0000",
            "content": "I don't think having to support two wildly different apis makes sense: that would be the worst of both worlds, because then the codec couldn't optimize to either, and we'd have bifurcated code all over Lucene, sometimes using one API, sometimes using another. "
        },
        {
            "id": "comment-16042582",
            "author": "Mikhail Khludnev",
            "date": "2017-06-08T11:40:00+0000",
            "content": "TestBlockJoinSelector.testSortedSelector()\n  final BitSet parents = new FixedBitSet(20);\n...\n    parents.set(15);\n    parents.set(19);\n\n    final BitSet children = new FixedBitSet(20);\n..\n    children.set(12);\n    children.set(17);\n\n    final int[] ords = new int[20];\n    Arrays.fill(ords, -1);\n...\n    ords[12] = 10;\n    ords[18] = 10;\n\n    final SortedDocValues mins = BlockJoinSelector.wrap(DocValues.singleton(new CannedSortedDocValues(ords)), BlockJoinSelector.Type.MIN, parents, children);\n...\n    assertEquals(15, mins.nextDoc());\n    assertEquals(10, mins.ordValue());\n    assertEquals(19, mins.nextDoc()); // <---- why??? \n    assertEquals(10, mins.ordValue());\n...\n\n\n\n19th parent has only 17th kid and value is assigned only on 18th.\nMichael McCandless, shouldn't it assert NO_MORE_DOCS here like it's done with numerics?   \n "
        },
        {
            "id": "comment-16042640",
            "author": "Adrien Grand",
            "date": "2017-06-08T12:48:08+0000",
            "content": "Agreed the sorted selector is buggy. It seems to ignore cases when none of the matching children in a block have a value. It is also a pity that we require a BitSet for the children while we only need forward access, we should make it a DocIdSetIterator, which would have the nice side-effect that we could easily combine the doc values iterator and the child filter using a ConjunctionDISI to efficiently only iterate over child doc ids that both have a value and match the child filter. "
        },
        {
            "id": "comment-16042654",
            "author": "Mikhail Khludnev",
            "date": "2017-06-08T13:00:03+0000",
            "content": "spawned  LUCENE-7871  "
        }
    ]
}