{
    "id": "LUCENE-2769",
    "title": "FilterIndexReader in trunk does not implement getSequentialSubReaders() correctly",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "4.0-ALPHA",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "Since LUCENE-2459, getSequentialSubReaders() in FilterIndexReader returns null, so it returns an atomic reader. But If you call then any of the enum methods, it throws Exception because the underlying reader is not atomic.\n\nWe should move the null-returning method to SlowMultiReaderWrapper and fix FilterIndexReader's default to return in.getSequentialSubReaders(). Ideally an implementation must of course also wrap the sub-readers.\n\nIf we change this we have to look into other Impls like the MultiPassIndexSplitter if we need to add atomicity.",
    "attachments": {
        "LUCENE-2769.patch": "https://issues.apache.org/jira/secure/attachment/12459982/LUCENE-2769.patch",
        "LUCENE-2769_norms.patch": "https://issues.apache.org/jira/secure/attachment/12459990/LUCENE-2769_norms.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-11-19T08:33:01+0000",
            "content": "In my opinion, LUCENE-2459 should be reverted and all places that can not correctly filter per segment  should use SlowMultiReaderWrapper as superclass (like e.g. MultiPassIndexSplitter) or better simply wrap with SlowMultiReaderWrapper in the ctor:\n\nctor(IndexReader in) {\n  super(SlowMultiReaderWrapper.wrap(in));\n}\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-12933722"
        },
        {
            "date": "2010-11-19T08:51:28+0000",
            "content": "ok try 2: we shouldn't encourage subclassing of SlowMultiReaderWrapper  ",
            "author": "Robert Muir",
            "id": "comment-12933726"
        },
        {
            "date": "2010-11-19T08:58:49+0000",
            "content": "Additionally to this:\nThe MultiPassIndexSplitter in 3.x and 3.0 should also return null in getSeqSubReaders. It works currently because we know how SegmentMerger works, but its still incorrect. To Filter the terms correctly it should imitate a atomic reader. ",
            "author": "Uwe Schindler",
            "id": "comment-12933729"
        },
        {
            "date": "2010-11-19T09:08:21+0000",
            "content": "We can use the SlowMultiWrapper also to remove norms merging in DirectoryReader.norms(field) and MultiReader.norms(field) should also throw UOE like fields(). If you need top-level norms use the SlowMultiReaderWrapper. ",
            "author": "Uwe Schindler",
            "id": "comment-12933736"
        },
        {
            "date": "2010-11-19T09:24:04+0000",
            "content": "This patch was a new FilterIndexReader subclass and it included some fixes to  FilterIndexReader and also refactoring of Junit tests to better support reader subclassing.\nA \"wrap\" method was added to FilterIndexReader to allow subclasses to specialize subreaders. ",
            "author": "Mark Harwood",
            "id": "comment-12933745"
        },
        {
            "date": "2010-11-19T09:28:16+0000",
            "content": "here is a hack patch for Uwe's idea about the norms.\nwe need to change SegmentMerger to not call norms on the top-level IR but populate its normBuffer from the subs.\n\nin my opinion it seems crazy we are currently creating these big arrays this way (yeah there is the hairy code for re-open that re-uses the big merged cache for the NRT case, but still).\n\nMaybe i am missing something. ",
            "author": "Robert Muir",
            "id": "comment-12933747"
        },
        {
            "date": "2010-11-19T09:58:04+0000",
            "content": "here's another hacky update: but still a few tests explicitly check these norms and need to be fixed.\n\nmaybe we could add an uncached \"MultiNorms\" or something at least in src/test for convenience,\njust to fill the byte arrays so these tests can assertEquals\n\notherwise we are going to have to put a lot of SlowMultiReaderWrappers in these tests. ",
            "author": "Robert Muir",
            "id": "comment-12933756"
        },
        {
            "date": "2010-11-19T12:17:16+0000",
            "content": "Here a better patch for the segment merger. We should even apply this if we not remove top-level norms!\n\nIt saves lots of memory during merging by using ReaderUtil to go down to segment level! Don't wonder about BytesRef, but we need a reference here because of the anonymous inner class. ",
            "author": "Uwe Schindler",
            "id": "comment-12933789"
        },
        {
            "date": "2010-11-19T12:45:30+0000",
            "content": "here is an updated patch, with core/contrib/solr tests passing.\n\nFor ParallelReader i forced it to require non-composite readers only (e.g. SlowMultiReaderWrap them if thats not the case).\n\nTODO: \n\n\tParallelReader shouldnt need multifields etc anymore\n\tthere are 5 @Ignore'd ParallelReader-related tests, because of things like reopen/isOptimized/isCurrent\n\tmerge in Uwe's improved SegmentsMerger\n\tclean up code.\n\n ",
            "author": "Robert Muir",
            "id": "comment-12933795"
        },
        {
            "date": "2010-11-19T14:49:34+0000",
            "content": "Here a new SegmentMerger now working only on atomic readers. All \"normal\" tests pass, but it seems that addIndexesWithThreads fails during merging term vectors. This is not clear, all other tests pass and the created indexes are fine.\n\nMike: Do you understand that? The problem seems to be some thread safety issue in addIndexes(IndexReader...). Not sure who changes who's internal structures? Maybe suddenly subreaders change? ",
            "author": "Uwe Schindler",
            "id": "comment-12933824"
        },
        {
            "date": "2010-11-19T14:54:47+0000",
            "content": "Last patch containes unused variable (a relic from very earlier times). \"int starts[]\" in merge terms is not needed. ",
            "author": "Uwe Schindler",
            "id": "comment-12933826"
        },
        {
            "date": "2010-11-19T16:16:04+0000",
            "content": "I removed the SegmentMerger patches from here and moved over to LUCENE-2770. The problem noted before is now solved. The SegmentMerger missed to clone TVReaders. This was a bug not recognized before. ",
            "author": "Uwe Schindler",
            "id": "comment-12933848"
        },
        {
            "date": "2010-11-19T18:34:27+0000",
            "content": "I thought about this more, i think for this issue, we should not make SlowMultiReaderWrapper complicated and force ParallelReader to take atomic readers.\n\ninstead, we should remove norms from Multi/DirReader like my patch suggests and make ParallelReader manage its own cached norms. \n\nFor ParallelReader, this is no worse than today, but no better either. We should seriously either figure out how to \nfix this ParallelReader or move it to contrib. ",
            "author": "Robert Muir",
            "id": "comment-12933907"
        },
        {
            "date": "2010-11-19T18:44:31+0000",
            "content": "I created new issue for the norms thing: LUCENE-2771\n\nI will commit the basic patch without \"norms\" soon and update the norms patch to latest trunk. ",
            "author": "Uwe Schindler",
            "id": "comment-12933910"
        },
        {
            "date": "2010-11-19T18:51:23+0000",
            "content": "Committed basic patch revision: 1036977 ",
            "author": "Uwe Schindler",
            "id": "comment-12933912"
        }
    ]
}