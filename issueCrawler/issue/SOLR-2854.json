{
    "id": "SOLR-2854",
    "title": "Load URL content stream on-demand, rather than automatically",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "3.6",
            "4.0-ALPHA"
        ],
        "components": [],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "I think the remote streaming feature should be limited to update request processors. I'm not sure if there is even any use of using it on a /select, but even if there is, it's an unintended security risk.  Observe this URL that is roughly the equivalent of an SQL injection attack:\n\nhttp://localhost:8983/solr/select?q=*:*&indent=on&wt=ruby&rows=2&stream.url=http%3A%2F%2Flocalhost%3A8983%2Fsolr%2Fupdate%3Fcommit%3Dtruetream.body%3D%3Cdelete%3E%3Cquery%3E*%3A*%3C%2Fquery%3E%3C%2Fdelete%3E\n\nYep; that's right \u2013 this search deletes all the data in your Solr instance! If you blocked off access to /update* based on IP then that isn't good enough.",
    "attachments": {
        "SOLR-2854_test_remote_streaming_not_done_on_select.patch": "https://issues.apache.org/jira/secure/attachment/12500966/SOLR-2854_test_remote_streaming_not_done_on_select.patch",
        "SOLR-2854-delay-stream-opening.patch": "https://issues.apache.org/jira/secure/attachment/12500878/SOLR-2854-delay-stream-opening.patch",
        "SOLR-2854_branch_3x_remote_streaming_fix.patch": "https://issues.apache.org/jira/secure/attachment/12505808/SOLR-2854_branch_3x_remote_streaming_fix.patch",
        "SOLR-2854-extract_fix.patch": "https://issues.apache.org/jira/secure/attachment/12501102/SOLR-2854-extract_fix.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Erik Hatcher",
            "id": "comment-13135929",
            "date": "2011-10-26T13:03:41+0000",
            "content": "I don't think we should restrict non-update request handlers from handling streams.  Consider DocumentAnalysisRequestHandler - it's handy to be able to stream in text for analysis.  There are other request handlers that leverage this as well.\n\nPerhaps, though, a solution is to not allow streams to be resolved unless they are truly needed by the request handler?  In your example, there is no need for the standard search request handler to access any streams and thus that URL shouldn't be hit. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-13135934",
            "date": "2011-10-26T13:19:08+0000",
            "content": "Here is a quick totally untested patch that should behave as Erik describes.  Rather then create the URLConnection in the constructor, it waits for someone to call getStream()\n\nthis will make effectively limit streaming to requests that hit something that uses it "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-13136024",
            "date": "2011-10-26T14:51:22+0000",
            "content": "+1 on Ryan's approach.  David, does that work for you?  Can someone drum up some test cases to add along with this?\n\nAlso, this isn't quite like a SQL injection attack where malicious strings can be put right into the query.  It would require some application level flaws to send an arbitrary stream.url parameter over with user/data input, especially to a /select search request.  But certainly the point is taken that bad things can happen with stream.url abuse. "
        },
        {
            "author": "David Smiley",
            "id": "comment-13136026",
            "date": "2011-10-26T14:54:34+0000",
            "content": "I will add a patch later to ensure that a select request doesn't access the stream. "
        },
        {
            "author": "David Smiley",
            "id": "comment-13136455",
            "date": "2011-10-26T21:54:37+0000",
            "content": "This patch simply adds a test class with a few tests.  One of those tests tries to use remote streaming with a select URL and it fails \u2013 by design.  Once a fix for this issue works, this test should succeed.\n\nThis is Test Driven Development, by the way.\n\nI don't have time left at the moment to see if Ryan's patch works. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-13136758",
            "date": "2011-10-27T04:02:29+0000",
            "content": "David - Thanks for the strong TDD example!  Thanks a lot for that, srsly.\n\nRyan - Thanks to you for the quick fix.\n\nI tried out the test patch first, got the failure, applied Ryan's patch, test passes.  TDD by the book.\n\nI've committed this to trunk, with the change history log of: \"Now load URL content stream data (via stream.url) when called for during request handling, rather than loading URL content streams automatically regardless of use.\"\n\nI think the security aspect of this is a separate issue.  What we've done here is only load URL content (file, etc content streams I double-checked, they late load already as it should) when a component calls out for it.  So someone could still send in that same evil stream.url to /analysis/document.  Let's spin off another issue for something like \"Enable fine grained control over allowed content streams\", such that one could disable URL content streams, but leave local file content streams possible, say.  Not sure that entirely satisfies this issue though, as it certainly is the case that one would have situations where stream.url to load content is really handy, but you certainly don't want any loopback (or fan-out) from malicious data to kill a system either.  What do others think about how to address this appropriately on the Solr side (even if that means simply making it clearer what stream.url really does underneath)? "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-13136760",
            "date": "2011-10-27T04:10:41+0000",
            "content": "What's left on this issue?  I suppose backporting it to 3.x is desirable for the masses?  Do these patches work out of the box for 3.x?  (if not, can someone whip that up?)  Is this particular issue done now?   Should we rename it to \"Load URL content streams when needed, rather than automatically regardless\"?   "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13136970",
            "date": "2011-10-27T11:27:29+0000",
            "content": "I assume the current test failures are due to this change?\n\n\n1 tests failed.\nREGRESSION:  org.apache.solr.common.util.ContentStreamTest.testURLStream\n\nError Message:\nnull\n\nStack Trace:\njava.lang.NullPointerException\n       at org.apache.solr.common.util.ContentStreamTest.testURLStream(ContentStreamTest.java:91)\n       at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:613)\n       at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:149)\n       at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:51)\n\n "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-13136987",
            "date": "2011-10-27T12:08:32+0000",
            "content": "Yonik - oops, you're right.  I was running the newly added test with -Dtestcase and just forgot to run the full suite before committing.  Thank goodness for continuous integration build.\n\nI committed a fix for the test, since ContentStreamBase.URLStream does not set the size until getStream/getReader is called.  But looking at where we use stream.getSize() (and other getters) other problems are caused as once a ContentStream is instantiated it is assumed to have a size and a type, but this isn't the case after Ryan's patch.  Should we adjust all the places where we use content streams to getStream/getReader before anything else?  I think so.  And note on getStream/getReader that it must be called prior to getting any details of the stream like size and type. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-13137151",
            "date": "2011-10-27T13:53:22+0000",
            "content": "I've been looking at the usage of content streams and the only place I found of concern is ExtractingDocumentLoader, which calls stream.getName()/getSize()/getContentType() before calling getStream(), but this seems to work fine on trunk despite the changes to lazy load the stream and those parameters.  This doesn't seem like it should work properly.  No? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13137164",
            "date": "2011-10-27T14:10:27+0000",
            "content": "This doesn't seem like it should work properly. No?\n\nDefinitely should be investigated...  exception being swallowed somewhere?  getStream() being called more than once?   "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-13137200",
            "date": "2011-10-27T15:14:17+0000",
            "content": "Here's a patch that fixes the extracting request handler to pull the stream properties after getStream() is called.  Stepping through with a debugger showed that before this change the metadata values were being set to null. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-13137204",
            "date": "2011-10-27T15:18:03+0000",
            "content": "Definitely should be investigated... exception being swallowed somewhere? getStream() being called more than once? \n\nNo exception or multiple getStream() calls.  Turns out the metadata attributes (which are ignored in the example /update/extract config) were being set to null (as evidenced by stepping through with a debugger attached).  The latest patch fixes this.  All seems to be well otherwise in investigating the use of other content stream usage where getStream() is called first.   I'm going to commit this patch and add a comment to the getStream() method to note that it should be called before the other properties. "
        },
        {
            "author": "David Smiley",
            "id": "comment-13137226",
            "date": "2011-10-27T16:01:45+0000",
            "content": "Regarding future steps to take to make Solr more secure with regards to remote streaming:  Personally, I think that, by default, the only handlers that should be able to use this are /update/ registered handlers. That makes Solr easier to secure and is also the biggest use case for this feature.  I'd like it to be clearer in solrconfig.xml exactly which handlers can use remote streaming. Presently, you have to have internal knowledge to know that /analysis/document will use it \u2013 and that's not cool from a security perspective.  You suggested  limiting specific URLs or files or files vs URLs but I don't think that is important. "
        },
        {
            "author": "David Smiley",
            "id": "comment-13157617",
            "date": "2011-11-26T22:08:41+0000",
            "content": "Whoops; we didn't back-port this to 3x!  That was a major over site and we just missed a release.  Do we need to create a new issue? "
        },
        {
            "author": "David Smiley",
            "id": "comment-13161156",
            "date": "2011-12-01T21:02:05+0000",
            "content": "re-opening for branch_3x fix "
        },
        {
            "author": "David Smiley",
            "id": "comment-13161158",
            "date": "2011-12-01T21:04:00+0000",
            "content": "Attached is a patch for 3x.  I saw the 4 commits Erik made to trunk and I applied it to my 3x checkout.  I also added a comment in solrconfig.xml to say:\n  SearchRequestHandler won't fetch it, but some others do.\n\nThe patch was made with git; I hope that won't create a problem. "
        },
        {
            "author": "David Smiley",
            "id": "comment-13215465",
            "date": "2012-02-24T07:32:47+0000",
            "content": "I committed the 3x backport in r1293115. "
        }
    ]
}