{
    "id": "SOLR-2613",
    "title": "DIH Cache backed w/bdb-je",
    "details": {
        "affect_versions": "4.0-ALPHA",
        "status": "Open",
        "fix_versions": [],
        "components": [
            "contrib - DataImportHandler"
        ],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "This is spun out of SOLR-2382, which provides a framework for multiple cacheing implementations with DIH.  This cache implementation is fast & flexible, supporting persistence and delta updates.  However, it depends on Berkley Database Java Edition so in order to evaluate this and use it you must download bdb-je from Oracle and accept the license requirements.",
    "attachments": {
        "SOLR-2613.patch": "https://issues.apache.org/jira/secure/attachment/12483356/SOLR-2613.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "James Dyer",
            "id": "comment-13054633",
            "date": "2011-06-24T19:14:08+0000",
            "content": "This version keeps BerkleyBackedCache in sync with the last version from SOLR-2382.  This version takes parameters from a Context object rather from a Map. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13065377",
            "date": "2011-07-14T16:43:14+0000",
            "content": "This is an update to stay in-sync with recent changes in Trunk. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13079763",
            "date": "2011-08-05T02:49:25+0000",
            "content": "Suggestions:\n\n\tTry a non-Oracle implementation. JDBM http://sourceforge.net/projects/jdbm/develop is under the Berkeley license.\n\tAn ant target that downloads the DB implementation.\n\t\n\t\tThe Oracle download requires interaction.\n\t\tThus, a different, completely free implementation would be nice.\n\t\n\t\n\tA way to pre-load the backing database. This would make caching scale by orders of magnitude.\n\n "
        },
        {
            "author": "James Dyer",
            "id": "comment-13080032",
            "date": "2011-08-05T15:47:29+0000",
            "content": "Sync'ed with current trunk and state of SOLR-2382. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13080041",
            "date": "2011-08-05T15:58:34+0000",
            "content": "\nTry a non-Oracle implementation.\nI understand the difficulties with bdb-je.  When I selected this, I though it would be ok to use because the Lucene db-contrib used it.  But since then \"db\" has gone away, partly because of this dependency, partly because there were no unit tests.  \n\n\nAn ant target\nAt least 1 commiter here was outraged that the Lucene build would automatically download a non-compatible-license file when doing a building the old db-contrib module, so I shy away from that option.  I thought about creating a mock-bdb jar that would just throw a special exception when you try to use it (then the unit tests can just print out a warning and users who want to really test/use the functionality could get bdb from Oracle).  I thought about apache-extras, but it would be easy for this to become out-of-sync with the rest of the project.  I also have a lucene-backed-cache that I wrote to help set up testing scenario.  I could contribute that and it would probably perform for many uses.  Others with more Lucene experience might improve it and maybe it could perform as good as the bdb-je one?  There's a lot of options but its all moot for the moment because the prerequsites in SOLR-2382 are not yet committed. \n\n\nA way to pre-load the backing database\nYou can pre-load the caches with DIHCacheWriter.  You then use DIHCacheProcessor to read the data back in later.  See TestDIHCacheWriterAndProcessor in SOLR-2382.  Is this what you mean? "
        },
        {
            "author": "James Dyer",
            "id": "comment-13132948",
            "date": "2011-10-21T19:07:01+0000",
            "content": "Sync'ed with the latest from SOLR-2382. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13161834",
            "date": "2011-12-02T20:23:29+0000",
            "content": "Synced with Trunk.  This version is compatible with the initial patch from SOLR-2943. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13162946",
            "date": "2011-12-05T18:52:36+0000",
            "content": "updated to fix a parameter-naming bug. "
        },
        {
            "author": "Adrian Rogers",
            "id": "comment-14015264",
            "date": "2014-06-02T08:47:22+0000",
            "content": "Is there going to be an update made to this case so that it's compatible with the latest version of SOLR-2943? It looks like a constant was removed or renamed which prevents this from compiling now. "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-15197981",
            "date": "2016-03-16T19:32:24+0000",
            "content": "This came up in a discussion on IRC today, talking about nested entity situations where the inner entities have a very large number of rows, so memory-based caches would require far more memory than the machine can hold.\n\nThe Oracle Berkeley DB implementation was specifically mentioned, which is why I'm here instead of opening a new issue.  This is licensed under the AGPL, so we can't distribute it, but I wonder if maybe we could implement enough of an API layer that a user could provide the jar themselves, tell Solr what class will be needed, and be in business.  Is this what the patch on this issue does?  I haven't looked deeply.\n\nOther ideas, which might need a separate issue for disk-based caching implementations:\n\nI had the idea of using SQLite for caching in a single-file database.  SQLite is public domain, and there are ways to access it from Java.\n\nEven just a simple implementation that writes little files to the disk would work.  To avoid tons of files in a single directory, perhaps this idea could get a 32-bit hash of the key and write to a four-level directory structure where each directory is two hex characters.  df/8c/12/b5\n\nA disk-based solution would not be as fast as the memory-based solution already available, but as long as it was on a local physical disk, it would probably be faster than making N+1 queries to a remote database. "
        },
        {
            "author": "James Dyer",
            "id": "comment-15198128",
            "date": "2016-03-16T20:49:00+0000",
            "content": "I wonder if maybe we could implement enough of an API layer that a user could provide the jar themselves, tell Solr what class will be needed, and be in business. Is this what the patch on this issue does?\n\nThis patch is an implementation of the DIHCache interface.  If this is on the classpath, you can specify 'cacheImpl=\"BerkleyBackedCache\" ' on any DIH Entity in data-config.xml, and it will cache to disk instead of memory.\n\nI developed this years ago and still use it in production, with an old dih jar, 4.6-vintage, built with this and SOLR-2943 applied.  I believe there may have been changes to DIH since 4.6-ish that prevent this from working, but it would not take a lot of work to update this either.  My experience is bdb-je is much faster than any sql database, or anything ootb I've tried.  If you have a lot of records to join, this is much faster than the n+1 select approach, slower than in-memory, but then again, this scales.  \n\nI also put a mapdb impl out there \u2013 SOLR-6144 \u2013 the license is friendlier, but the performance was not there.  Also, its just a quick impl, possibly full of bugs.\n\nOf course, you could write a DIHCache implementation that does it any way you want.  Just this was fairly quick to develop and it works good for us.   And yes, you'd have to have your client download the bdb-je jar for themselves.  I'm using this with je-6.2.31.jar . "
        }
    ]
}