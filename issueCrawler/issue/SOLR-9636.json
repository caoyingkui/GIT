{
    "id": "SOLR-9636",
    "title": "Add support for javabin for /stream, /sql internode communication",
    "details": {
        "components": [],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [
            "6.4",
            "7.0"
        ],
        "affect_versions": "None",
        "status": "Resolved",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "currently it uses json, which is verbose and slow",
    "attachments": {
        "SOLR-9636.patch": "https://issues.apache.org/jira/secure/attachment/12836587/SOLR-9636.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-11-02T14:50:25+0000",
            "author": "Noble Paul",
            "content": "This does a bunch of things\n\n\n\tIntroduce a \"json equivalent\" mode for javabin. Just use a subset of types which json supports\n\tA parser for reading this javabin in a streaming mode\n\tRefactored the code to avoid TextResponsewriter to be aware of TupleStream , Tuple etc\n\n ",
            "id": "comment-15629188"
        },
        {
            "date": "2016-11-02T15:48:36+0000",
            "author": "Yonik Seeley",
            "content": "Introduce a \"json equivalent\" mode for javabin. Just use a subset of types which json supports\n\nWhy is this necessary?  In general, it seems preferable to normalize on the client/read side rather than throwing information away.\nThere is a lot of refactoring in the patch that make it tough to tell exactly what the functional changes are. ",
            "id": "comment-15629365"
        },
        {
            "date": "2016-11-02T16:04:19+0000",
            "author": "Noble Paul",
            "content": "The streaming code is all written with the assumption that the response is json. It's extremely difficult to make that code work with these rich types ",
            "id": "comment-15629411"
        },
        {
            "date": "2016-11-02T19:26:04+0000",
            "author": "Joel Bernstein",
            "content": "I'm not sure that I agree this needs to be done. \n\nHow much performance improvements will we see in real world examples? ",
            "id": "comment-15630147"
        },
        {
            "date": "2016-11-02T19:31:01+0000",
            "author": "Joel Bernstein",
            "content": "And if we decide as a group that this is worth while doing, let's do it in a careful, fully thought through way. \n\nI'd like to see more details in the ticket as to approaches being taken. ",
            "id": "comment-15630161"
        },
        {
            "date": "2016-11-02T19:35:52+0000",
            "author": "Joel Bernstein",
            "content": "Also, I'm not convinced that the performance bottlenecks are caused by json.\n\nIt could be caused by low level docValues hotspots and it could be caused by network bottlenecks.\n\nSo, before doing anything let's understand where the bottlenecks actually are.\n ",
            "id": "comment-15630177"
        },
        {
            "date": "2016-11-02T20:28:03+0000",
            "author": "Noble Paul",
            "content": "Nobody ever said there was a bottleneck. We do all our internode communications in binary format for a reason. It offers real performance improvements and memory efficiency.  That's not to say that we should stop at this. The code has so many other inefficiencies too. All of them add up.  You should look at SOLR-486 to see the history of binary format.\n ",
            "id": "comment-15630360"
        },
        {
            "date": "2016-11-02T21:21:19+0000",
            "author": "Joel Bernstein",
            "content": "Typically the way you improve performance is to get an understanding of the bottlenecks. Then you work on improving the throughput of the bottlenecks. Then something else becomes the bottleneck and you fix that. And you keep doing this until you get diminishing returns. \n\nIf we're going to undertake a large refactoring of the streaming infrastructure for performance reasons, then I'm going to insist that we take that approach. ",
            "id": "comment-15630574"
        },
        {
            "date": "2016-11-02T23:27:37+0000",
            "author": "Erick Erickson",
            "content": "I've spent too much of my life fixing the wrong thing for efficiency, I've got to agree with Joel here. Let's positively identify the bottleneck before proposing a solution. SOLR-486 does show roughly a doubling of binary output compared to JSON, my uncertainty is how much of that improvement is actually forming the response. An interesting measurement would be to just avoid assembling anything for the output stream, just read the date from the docValues and throw it away. \n\nBut even there it's iffy. My testing purposely threw the bits on the floor, so network transmission was completely out of the picture. Whether the end user would see measurable improvement is an open question.\n\nContrariwise, see https://issues.apache.org/jira/browse/SOLR-9296 where I was doing some experimenting with re-using things like char[] rather than allocating new strings all the time and saw up to an 11% improvement in throughput. Don't know whether the notion of going to a binary format would make that irrelevant that or not. I'll link the two issues anyway just so don't lose the association. ",
            "id": "comment-15630883"
        },
        {
            "date": "2016-11-02T23:28:15+0000",
            "author": "Erick Erickson",
            "content": "How much of SOLR-9296 is made irrelevant by this JIRA (and vice-versa)? ",
            "id": "comment-15630885"
        },
        {
            "date": "2016-11-03T02:55:39+0000",
            "author": "Noble Paul",
            "content": "If we're going to undertake a large refactoring of the streaming infrastructure for performance reasons, then I'm going to insist that we take that approach.\n\nI'm not touching any of the streaming code for this. The rafactoring was done to cleanup the unnecessary dependencies it introduced un ResponseWriters etc. I am going to split the ticket further. the refactoring is unavoidable.  ",
            "id": "comment-15631337"
        },
        {
            "date": "2016-11-03T03:17:23+0000",
            "author": "Noble Paul",
            "content": "I've spent too much of my life fixing the wrong thing for efficiency, I've got to agree with Joel here. \n\nSolr has a modular design where we pick and choose components (wt param here). These are done to ensure that everything works with each other. In case of Streaming it is not there. That introduces too many dependencies on each component.\n\nI strongly recommend everyone to take a look at the patch because it may not be doing what you think it is doing ",
            "id": "comment-15631382"
        },
        {
            "date": "2016-11-03T03:37:51+0000",
            "author": "Joel Bernstein",
            "content": "Ok, let's do this work and it's sub-tasks in a branch. And let's get consensus before it hit's master. ",
            "id": "comment-15631417"
        },
        {
            "date": "2016-11-03T03:40:38+0000",
            "author": "Noble Paul",
            "content": "I've created sub tasks . The refactorings (SOLR-9720)  are long overdue. Let's move the discussion to their own sub issues ",
            "id": "comment-15631421"
        },
        {
            "date": "2016-12-27T23:15:52+0000",
            "author": "Joel Bernstein",
            "content": "I finally had the time to test out the javabin writer with the /export handler and streaming stack. My initial findings are really good. Here is a summary:\n\n1) Currently testing must be done on branch_6x. There is a bug in master which breaks the /export handler. I haven't gotten to the bottom of it yet but I'm pretty sure it was introduced with the new docValues iterator API which is only in master. I will open a ticket for this bug shortly and see if I can fix the problem.\n\nBut testing in branch_6x is better anyway as it won't be testing the new docValues iterator API performance at the same time as the javabin /export performance.\n\n2) For my test I worked on a single Solr instance (6g heap) with a single data shard (collection1) loaded with 10,000,000 small documents. I also created a worker collection with 8 shards (collection2). Then I ran the following expression with and without the javabin writer.\n\nparallel(collection2, workers=8, sort=\"test_s desc\", \n         rollup(over=\"test_s\", sum(price_f),\n                search(collection1, \n                       q=*:*,\n                       fl=\"test_s, price_f\", \n                       sort=\"test_s desc\", \n                       qt=\"/export\", \n                       wt=\"javabin\", \n                       partitionKeys=test_s)))\n\n\n\nNotice that there are 8 parallel workers (collection2) partitioning the stream from a single data shard (collection1). This is how you achieve maximum throughput from a single node.\n\n3) Throughput numbers were fairly impressive with this test expression:\n\n\n\tWith json writer: 1,200,000 Tuples per second.\n\tWith javabin writer: 1,400,000 Tuples per second.\n\n\n\nSo Javabin gives a significant throughput boost. \n\n4) Javabin also produced a much smaller output, roughly 50% smaller then json.\n\n5) I also reviewed the code and it looks really nice. Big improvement as far cleaning up the integration with Solr. \n\n6) The core export/sort algorithm looked to be untouched, which was nice because there was a lot of hardening on that in the past. My biggest concern going into this ticket was that refactoring would cause a change in the export/sort algorithm and we'd have go through the hardening all over again. But that was not the case.\n\nVery nice work Noble Paul! Big improvements and so far I haven't found any functional problems. I will continue testing.\n\n ",
            "id": "comment-15781496"
        },
        {
            "date": "2016-12-27T23:20:59+0000",
            "author": "Noble Paul",
            "content": "Waiting to see the numbers with bated breath ",
            "id": "comment-15781505"
        },
        {
            "date": "2016-12-28T14:02:37+0000",
            "author": "Joel Bernstein",
            "content": "After increasing the heap size for the test Solr instance to 6g I saw a large boost in throughput. I'll update the numbers above. ",
            "id": "comment-15782945"
        },
        {
            "date": "2016-12-29T15:10:11+0000",
            "author": "Joel Bernstein",
            "content": "I added a new NullStream to test the performance of exporting and sorting on a high cardinality field. High cardinality exporting/sorting is an important real world use case for supporting distributed joins on primary keys. The query looks like this:\n\n\nparallel(collection2, workers=7, sort=\"count desc\", \n      null(search(collection1, \n                   q=*:*, \n                   fl=\"id\", \n                   sort=\"id desc\", \n                   qt=\"/export\", \n                   wt=\"javabin\", \n                   partitionKeys=id)))\n\n\n\nNotice the new null function which eats the tuples and returns a count to verify the number of tuples processed.\n\nThe test query is sorting on the id field which has a unique value in each record. Again performance was impressive:\n\n\n\tWith json: 1,210,000 Tuples per second.\n\tWith javabin: 1,350,000 Tuples per second.\n\n\n\nSo the ExportWriter doesn't slow down sorting on a high cardinality field.\n\nGoing forward the NullStream will be useful for testing the raw performance of the ExportWriter in isolation. This will help developers diagnose where the bottlekneck is if distributed joins aren't performing as expected.\n\nFor example if a join is slow, but the same export using the NullStream is fast, then you can be sure that the bottleneck is not in the ExportWriter, and is likely in Join stream.\n\n ",
            "id": "comment-15785486"
        },
        {
            "date": "2016-12-30T09:29:41+0000",
            "author": "Noble Paul",
            "content": "Another useful metric would be to measure the memory used in both json and javabin formats ",
            "id": "comment-15787304"
        },
        {
            "date": "2017-01-03T15:00:26+0000",
            "author": "Joel Bernstein",
            "content": "I will look at gathering this as well. And also look a GC's, which in theory should be less frequent. ",
            "id": "comment-15795244"
        },
        {
            "date": "2017-01-06T20:28:58+0000",
            "author": "Joel Bernstein",
            "content": "I'll also test javabin with gatherNodes() graph traversal. gatherNodes simply passes through the parameters to CloudSolrStream so it's easy just take off and on the writer type and test performance. ",
            "id": "comment-15805657"
        }
    ]
}