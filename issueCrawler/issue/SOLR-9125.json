{
    "id": "SOLR-9125",
    "title": "CollapseQParserPlugin allocations are index based, not query based",
    "details": {
        "components": [
            "query parsers"
        ],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Minor"
    },
    "description": "Among other things, CollapsingQParserPlugin\u2019s OrdScoreCollector allocates space per-query for: \n1 int (doc id) per ordinal\n1 float (score) per ordinal\n1 bit (FixedBitSet) per document in the index\n\nSo the higher the cardinality of the thing you\u2019re grouping on, and the more documents in the index, the more memory gets consumed per query. Since high cardinality and large indexes are the use-cases CollapseQParserPlugin was designed for, I thought I'd point this out.\n\nMy real issue is that this does not vary based on the number of results in the query, either before or after collapsing, so a query that results in one doc consumes the same amount of memory as one that returns all of them. All of the Collectors suffer from this to some degree, but I think OrdScore is the worst offender.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2016-05-17T16:22:39+0000",
            "author": "Jeff Wartes",
            "content": "I messed around a little bit, but I don't have a solution for this. I thought I'd file the issue anyway just to shine some light.\n\nI had attempted to use CollapseQParserPlugin on a very large index using a collapse on a field whose cardinality was about 1/7th the doc count... it didn't go well. Worse, the issue didn't come up until pretty late in the game, because at low query rate and/or on smaller indexes, the problem isn't evident. I abandoned the attempt.\n\nSome stuff I tried:\n\n\n\tI thought about replacing the FBS with a DocIdSetBuilder, but DelegatingCollector.finish() gets called twice, and you can't DocIdSetBuilder.build() twice on the same builder. We'd need to save the first build() result and use it to initialize a new builder for the second, but I wasn't convinced I understood the distinction between the two passes.\n\tI did one quick test where I replaced the \"ords\" and \"scores\" arrays with an IntIntScatterMap IntFloatScatterMap, thinking those would work better for small result sets. That ended up being worse (from a total allocations standpoint) for the queries I was trying, probably due to the map resizing necessary. It might be possible to set initial size values from statistics and help this case that way. It would also be possible to encode the docId/score into a long and just use one IntLongScatterMap, but I didn't try that.\n\n ",
            "id": "comment-15286940"
        },
        {
            "date": "2016-05-17T18:29:53+0000",
            "author": "Joel Bernstein",
            "content": "Yeah, the CollapsingQParsePlugin can use a lot of memory. The original design goal was to increase performance for collapsing on high cardinality fields and large result sets, as opposed to large indexes. It was really designed to support fast collapse queries on large e-commerce catalogs which are still typically small compared to other data sets.\n\nIf we can find a way to maintain the performance and shrink the memory usage this would be a great thing. \n ",
            "id": "comment-15287208"
        },
        {
            "date": "2016-05-17T18:48:38+0000",
            "author": "Joel Bernstein",
            "content": "One approach that might work for switching to primitive maps, would be first to estimate the cardinality of the collapse values in the result set using hyperloglog, and then sizing the primitive map accordingly. But my guess is this approach is going to really hurt performance. \n ",
            "id": "comment-15287265"
        },
        {
            "date": "2016-05-17T19:19:22+0000",
            "author": "Jeff Wartes",
            "content": "Isn't there a chicken-and-egg situation there? You need the set of matching docs to figure out the HLL.cardinality to specify the initial size of the map you're going to save the set of matching docs in? \n\nOr maybe collect() would just throw every doc in the FBS, and finish() would do all the finding group heads and collapsing? ",
            "id": "comment-15287339"
        },
        {
            "date": "2016-05-17T19:47:06+0000",
            "author": "Joel Bernstein",
            "content": "What I was thinking was to first run the query and get the cardinality. But this is really not fun as the CollapsingQParserPlugin would have to know the main query and all the filter queries. Doesn't sound like it would be fun to write or maintain. ",
            "id": "comment-15287401"
        }
    ]
}