{
    "id": "LUCENE-1473",
    "title": "Implement standard Serialization across Lucene versions",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/search"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "2.4",
        "resolution": "Won't Fix",
        "status": "Closed"
    },
    "description": "To maintain serialization compatibility between Lucene versions, serialVersionUID needs to be added to classes that implement java.io.Serializable.  java.io.Externalizable may be implemented in classes for faster performance.",
    "attachments": {
        "custom-externalizable-reader.patch": "https://issues.apache.org/jira/secure/attachment/12395481/custom-externalizable-reader.patch",
        "lucene-contrib-remote.patch": "https://issues.apache.org/jira/secure/attachment/12395843/lucene-contrib-remote.patch",
        "LUCENE-1473.patch": "https://issues.apache.org/jira/secure/attachment/12395100/LUCENE-1473.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-12-02T16:02:14+0000",
            "content": "LUCENE-1473.patch\n\nTerm implements Externalizable.  Added serialVersionUID handling in the read/writeExternal methods.  The long encoding needs to be variable long encoded to reduce the size of the resulting serialized bytes.\n\nIf it looks ok, I will implement Externalizable in other classes.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12652412"
        },
        {
            "date": "2008-12-02T22:21:45+0000",
            "content": "Do we really need to write the serialVersionUID?  That's adding 8 bytes to the storage of each term.\n\nThe term storage is not particularly efficient when storing many terms in the same field, because eg the String field is not written as an intern'd string.\n\nAlso I see many tests failing with this, eg TestBoolean2 \u2013 I think we'll have to add:\n\n   public Term() {}\n\nso deserialization can work?  Which is then sort of annoying because it means it's possible to create a Term with null field & text (though, you can do that anyway by passing in \"null\" for each). ",
            "author": "Michael McCandless",
            "id": "comment-12652546"
        },
        {
            "date": "2008-12-02T22:31:49+0000",
            "content": "The serialVersionUID needs to be written if the class is going to evolve.  It's written now, and currently in default serialization the field names are also written.  We'll need empty constructors. ",
            "author": "Jason Rutherglen",
            "id": "comment-12652555"
        },
        {
            "date": "2008-12-02T22:48:05+0000",
            "content": "I share Michaels concerns. Whats the motivation for core Lucene classes supporting serialization and is it strong enough to warrant these changes? It comes with a cost even without the mentioned annoyances right? (which are only for the current class, there may be more?) ",
            "author": "Mark Miller",
            "id": "comment-12652569"
        },
        {
            "date": "2008-12-02T22:52:04+0000",
            "content": "Lucene supports serialization explicitly by implementing Serializable.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12652572"
        },
        {
            "date": "2008-12-02T22:56:25+0000",
            "content": "The serialVersionUID needs to be written if the class is going to evolve.\n\nCan we use a byte, not long, for serialVersionUID?  And maybe change its name to SERIAL_VERSION?  (I think serialVersionUID is completely unused once you implement Externalizable?).\n\nThis brings up another question: what's our back compat policy here?  For how many releases after you've serialized a Term can you still read it back?  This is getting complicated... I'm wondering if we shouldn't even go here (ie, make any promise that something serialized in release X will be deserializable on release Y).\n\nI also think serialization is better done \"at the top\" where you can do a better job encoding things.  EG this is the purpose of the TermsDict (to serialize many Terms, in sorted order).\n\nJason, what's the big picture use case here; what are you serializing? ",
            "author": "Michael McCandless",
            "id": "comment-12652576"
        },
        {
            "date": "2008-12-02T23:02:26+0000",
            "content": "Lucene supports serialization explicitly by implementing Serializable. \n\nRight, but we don't really support it (like many/most I would guess). There is a pain in the butt cost of support. Since this patch seems to push that pain around, I'm just wondering if the motivation for it is worth the cost (not knowing the motivation). ",
            "author": "Mark Miller",
            "id": "comment-12652580"
        },
        {
            "date": "2008-12-02T23:19:31+0000",
            "content": "Currently serialization between 2.3 and 2.4 is broken, backwards compatibility is broken.  \n\nSaying \"we don't really support it\" means Serializable will not be implemented in any classes.  \n\nThis really simple Java stuff that I'm surprised is raising concern here. ",
            "author": "Jason Rutherglen",
            "id": "comment-12652585"
        },
        {
            "date": "2008-12-02T23:27:51+0000",
            "content": "The attached patch optimizes java serialization.  Also, if we want java serialization to work cross-version, it gives us a leg to stand on.  It doesn't change anything for most Lucene users, since Lucene doesn't use java serialization except for RMI.  So, today, if you're using RemoteSearcher, and the local and remote versions have different versions of Term.java, things will probably fail, since, by default, the serialVersionUID is a hash of the method signatures and fields.  If we want RemoteSearcher to work cross-version, then we need to explicitly manage the serialVersionUID and readExternal/writeExternal implementations.  But is that really a priority?\n\nAs with all optimizations, the performance improvement should be measured and demonstrated to be significant.  Also, an invasive change like this is hard to justify when so little of Lucene depends on java serialization.\n ",
            "author": "Doug Cutting",
            "id": "comment-12652591"
        },
        {
            "date": "2008-12-02T23:29:51+0000",
            "content": "the fact an object implements Serializable implies this object can be serialized. It is a known good java programming practice to include a suid to the class (as a static variable) when the object declares itself to be Serializable. If it is not meant to be serialized, why did it implement Serializable. Furthermore, what is the reason to avoid it being serialized? I find the reason being the cost of support kinda ridiculous, seems this reason can be applied to any bug fix, because this at the end of the day, it is a bug.\n\nI don't understand the issue of \"extra bytes\" to the term dictionary if the Term instance is not actually serialized to the index (at least I really hope that is not done)\n\nThe serialVersionUID (suid) is a long because it is a java thing. Here is a link to some information on the subject:\nhttp://java.sun.com/developer/technicalArticles/Programming/serialization/\n\nUse case: deploying lucene in a distributed environment, we have a broker/server architecture. (standard stuff), we want roll out search servers with lucene 2.4 instance by instance. The problem is that the broker is sending a Query object to the searcher via java serialization at the server level, and the broker is running 2.3. And because of specifically this problem, 2.3 brokers cannot to talk to 2.4 search servers even when the Query object was not changed. \n\nTo me, this is a very valid use-case. The problem was two different people did the release with different compilers.\n\nAt the risk of pissing off the Lucene powerhouse, I feel I have to express some candor. I am growing more and more frustrated with the lack of the open source nature of this project and its unwillingness to work with the developer community. This is a rather trivial issue, and it is taking 7 back-and-forth's to reiterate some standard Java behavior that has been around for years.\n\nLucene is a great project and has enjoyed great success, and I think it is to everyone's interest to make sure Lucene grows in a healthy environment.\n ",
            "author": "John Wang",
            "id": "comment-12652594"
        },
        {
            "date": "2008-12-02T23:45:01+0000",
            "content": "The Spring framework http://www.springframework.org/ is a good example of a widely used open source Java project that implements and uses Serialization in most of it's classes.  If Serialization will not be fixed in Lucene then perhaps it's best to implement a serializable wrapper in the Spring project for Lucene. ",
            "author": "Jason Rutherglen",
            "id": "comment-12652599"
        },
        {
            "date": "2008-12-03T11:00:07+0000",
            "content": "\n\n> At the risk of pissing off the Lucene powerhouse, I feel I have to express some candor. I am growing more and more frustrated with the lack of the open source nature of this project and its unwillingness to work with the developer community. This is a rather trivial issue, and it is taking 7 back-and-forth's to reiterate some standard Java behavior that has been around for years.\nWhoa!  I'm sorry if my questions are giving this impression.  I don't\nintend to.\n\nBut I do have real questions, still, because I don't think\nSerialization is actually so simple.  I too was surprised on looking\nat what started as a simple patch yet on digging into it uncovered\nsome real challenges.\n\n\n>Use case: deploying lucene in a distributed environment, we have a broker/server architecture. (standard stuff), we want roll out search servers with lucene 2.4 instance by instance. The problem is that the broker is sending a Query object to the searcher via java serialization at the server level, and the broker is running 2.3. And because of specifically this problem, 2.3 brokers cannot to talk to 2.4 search servers even when the Query object was not changed.\nOK that is a great use case \u2013 thanks.  That helps focus the many\nquestions here.\n\n\n> It is a known good java programming practice to include a suid to the class (as a static variable) when the object declares itself to be Serializable.\n\nBut that alone gives a too-fragile back-compat solution because it's\ntoo coarse.  If we add field X to a class implementing Serializable,\nand must bump the SUID, that's a hard break on back compat.  So really\nwe need to override read/writeObject() or read/writeExternal() to do\nour own versioning.\n\nConsider this actual example: RangeQuery, in 2.9, now separately\nstores \"boolean includeLower\" and \"boolean includeUpper\".  In versions\n<= 2.4, it only stores \"boolean inclusive\".  This means we can't rely\non the JVM's default versioning for serialization.\n\n\n> The serialVersionUID (suid) is a long because it is a java thing.\n\nBut, that's only if you rely on the JVM's default serialization.  If\nwe implement our own (overriding read/writeObject or\nread/writeExtenral) we don't have to use \"long SUID\".\n\n\n> The problem was two different people did the release with different compilers.\n\nI think it's more likely the addition of a new ctor to Term (that\ntakes only String field), that changed the SUID.\n\n\n> If it is not meant to be serialized, why did it implement Serializable.\n\nBecause there are two different things it can \"mean\" when a class\nimplements Serializable, and I think that's the core\ndisconnect/challenge to this issue.\n\nThe first meaning (let's call it \"live serialization\") is: \"within the\nsame version of Lucene you can serialize/deserialize this object\".\n\nThe second meaning (let's call it \"long-term persistence\") is: \"you\ncan serialize this object in version X of Lucene and later deserialize\nit using a newer version Y of Lucene\".\n\nLucene, today, only guarantees \"live serialization\", and that's the\nintention when \"implements Serializable\" is added to a class.\n\nBut, what's now being asked for (expected) with this issue is\n\"long-term persistence\", which is really a very different beast and a\nmuch taller order.  With it comes a number of challenges, that warrant\nscrutiny:\n\n\n\tWhat's our back-compat policy for \"long-term persistence\"?\n\n\n\n\n\tThe storage protocol must have a version header, so future changes\n    can switch on that and decode older formats.\n\n\n\n\n\tWe need strong test cases that deserialize older versions of these\n    serialized classes so we don't accidentally break it.\n\n\n\n\n\tWe should look carefully at the protocol and not waste bytes if we\n    can (1 byte vs 8 byte version header).\n\n\n\nThese issues are the same issues we face with the index file format,\nbecause that is also long-term persistence. ",
            "author": "Michael McCandless",
            "id": "comment-12652740"
        },
        {
            "date": "2008-12-03T16:30:57+0000",
            "content": "I don't see why you can't just break compatibility between versions when dealing with Serialization. Just have it continue to mean live (or close to live) persistence.\n\nEven the JDK does this (e.g. Swing serialization makes no guarantees). Just do the same - bound to change between releases...\n\nAlso, different compilers will generate different SUID... usually due to synthetic methods. It's kind of a problem...\n\n ",
            "author": "robert engels",
            "id": "comment-12652843"
        },
        {
            "date": "2008-12-03T17:57:24+0000",
            "content": "For the record: i have limited understanding of java serialization issues...\n\nAt the risk of pissing off the Lucene powerhouse, I feel I have to express some candor. I am growing more and more frustrated with the lack of the open source nature of this project and its unwillingness to work with the developer community.\n\nThe developer community consists of hundreds (possibly thousands) of people, who participate at various levels.  At the time the above quoted comment was posted, 4 members of the community had expressed an opinion on this issue: 1 clearly in favor, and 3 questioning the advantages and disadvantages as they affect the whole community, both in terms of the performance impacts for existing use cases, and the long term support issues that might come from a change like this.\n\nHow anyone could consider these comments and questions \"unwillingness to work with the developer community\" blows my mind ... i do not see an overwhelming push by the community at large for a feature, i do not see a \"Lucene powerhouse\" arguing against the will of the masses ... I see two people arguing in favor of a change, and three questioning whether this change is a good idea (i'm not sure if i understand robert's post fully, i believe he's suggesting we maintain the status quo such that serialization is supported but no claims are made about back-compatible serialization).  \n\nI would define that as healthy discussion.\n\n\nThis, to me, seems to be the crux of the issue...\n\n\nLucene, today, only guarantees \"live serialization\", and that's the\nintention when \"implements Serializable\" is added to a class.\n\nBut, what's now being asked for (expected) with this issue is\n\"long-term persistence\", which is really a very different beast and a\nmuch taller order. With it comes a number of challenges, that warrant\nscrutiny:\n\n...this jives with even my limited experience with java serialization, and i share Michael's concerns.  The current behavior does not appear to be a bug, it appears to be the correct effects of a serializable class changing between two versions w/o any explicit policy on serialization compatibility.  The changes being discussed  seem to be a request for a new \"feature\": a back-compat commitment on the serialization of one (or more) classes.  However small the patch may be, it's still a significant change that should not be made lightly ... I certainly think all 4 of Michael's bullet points should be clearly addressed before committing anything, and I agree with Doug's earlier observation regarding performance: we need to test that a new serialization strategy won't adversely affect existing users who rely on (what Michael refered to as) \"live serialization\".\n\nThis is my opinion. I voice it not as a member of any sort of mythical \"Lucene powerhouse\" but as member of the Lucene community who is is concerned about making sure that decisions are made towards \"everyone's interest to make sure Lucene grows in a healthy environment.\" \u2013 not just the interests of two of vocal people. ",
            "author": "Hoss Man",
            "id": "comment-12652872"
        },
        {
            "date": "2008-12-03T18:10:06+0000",
            "content": "> the performance improvement should be measured and demonstrated to be significant\n\nThe initial concern was the incompatibility of serialized objects between Lucene versions.  The performance improvements created by using Externalizable are secondary and so providing tests would be a waste of time if the \"community\" believes it is too much effort to add 1 line of code to a handful of classes.  Implementing Externalizable is a way to reduce the size of the serialized objects, manage the serialized object versions, and provide performance improvements.  Externalizable provides the most benefits and is very similar to the system Hadoop uses with Writeable.  Externalizable works seamlessly with native object serialization and Serializable implemented classes, meaning it works with a number of existing Java classes in addition to Externalizable classes.  \n\nUsing distributed serialized objects for search in Lucene is a natural Java based way to run a Lucene system.  In many cases it is ideal because Java provides something C++ does not, dynamic in-process class loading.  In a large grid based search system that requires 100% uptime this feature can be particularly useful.  \n\nAdding a serialVersionUID to the classes is one option, adding Externalizable is another option.  \n\nIf the decision is to not support Serialization in Lucene then I recommend removing Serializable from all classes in Lucene 3.0 so that users do not mistakenly expect the search library to behave the way other Java libraries such as ICU4J, JDK class libraries, Spring, etc do. ",
            "author": "Jason Rutherglen",
            "id": "comment-12652878"
        },
        {
            "date": "2008-12-03T18:19:22+0000",
            "content": "> But, what's now being asked for (expected) with this issue is \"long-term persistence\", which is really a very different beast and a much taller order.\n\nThat's the crux, alright.  Does Lucene want to start adding cross-version guarantees about the durability of its objects when serialized by Java serialization.  This is a hard problem.  Systems like Thrift and ProtocolBuffers offer support for this, but Java Serialiation itself doesn't really provide much assistance.  One can roll one's own serialization compatibility story manually, as proposed by this patch, but that adds a burden to the project.  We'd need, for example, test cases that keep serialized instances from past versions, so that we can be sure that patches do not break this.\n\nThe use case provided may not use RMI, but it is similar: it involves transmitting Lucene objects over the wire between different versions of Lucene.  Since Java APIs, like Lucene, do not generally provide cross-version compatibility, it would be safer to architect such a system so that it controls the serialization of transmitted instances itself and can thus guarantee their compatibility as the system is updated.  Thus it would develop its own representations for queries independent of Lucene's Query, and map this to Lucene's Query.  Is that not workable in this case? ",
            "author": "Doug Cutting",
            "id": "comment-12652882"
        },
        {
            "date": "2008-12-03T18:40:30+0000",
            "content": "If it is not meant to be serialized, why did it implement Serializable. Furthermore, what is the reason to avoid it being serialized? I find the reason being the cost of support kinda ridiculous, seems this reason can be applied to any bug fix, because this at the end of the day, it is a bug.\n\nThe \"implements Serializeable\" was added to support RemoteSearchable.  If we believe this creates a bug, then perhaps we should remove this and implement RemoteSearchable in another way.  As it stands, Lucene does not support Java Serialization across Lucene versions.  That seems to me more like a limitation than a bug, no?\n\nEvery line of code added to Lucene is a support burden, so we must carefully weigh the costs and benefits of each line.  This issue proposes to add many lines, and to add a substantial new back-compatibility requirement.  Back-compatibility is something that Lucene takes seriously. We make promises about both API back-compatibility and file-format back-compatibility.  These already significantly constrain development.  Adding a new back-compatibility requirement should not be done lightly, but only after broad consensus is reached through patient discussion. ",
            "author": "Doug Cutting",
            "id": "comment-12652888"
        },
        {
            "date": "2008-12-03T18:49:05+0000",
            "content": "In regards to Doug's comment about an alternate form... doesn't SOLR already have a XML based query format?\n\nIf so, just persist the queries using this. You will be immune to serialization changes (provided the SOLR parser remains backwards compatible). ",
            "author": "robert engels",
            "id": "comment-12652892"
        },
        {
            "date": "2008-12-03T18:57:58+0000",
            "content": "The \"implements Serializeable\" was added to support RemoteSearchable. If we believe this creates a bug, then perhaps we should remove this and implement RemoteSearchable in another way. As it stands, Lucene does not support Java Serialization across Lucene versions. That seems to me more like a limitation than a bug, no?\n\nThere will be complaints no matter what. GWT tried getting around people having to implement Serializable by providing an interface with fewer promises: isSerizable. Many complained right away, as they had other classes that perhaps they where making Serializable simply for Apache XMLRpc or something. So now you can use either Serializable or isSerialiazble.\n\nPersonally, I think its fine to do as we are. I'm not against supporting more though. \n\nIf we choose not to go further (and from what I can tell that decision has not yet been made yet, against or for) add to the javadocs about what we support, as I don't think its a bug myself. The Serializable interface indicates that the class and its subclasses will be Serializable, my reading of the javadoc does not indicate what cross version compatibility must be supported. I believe that is up to the implementor.\n\n\n\tMark\n\n ",
            "author": "Mark Miller",
            "id": "comment-12652899"
        },
        {
            "date": "2008-12-03T19:26:15+0000",
            "content": "\"This is a hard problem.\"\n\nI disagree.  It's completely manageable.  Doesn't Hadoop handle versioning inside of Writeable classes?\n\nScoreDocComparator javadoc \"sortValue(ScoreDoc i) Returns the value used to sort the given document. The object returned must implement the java.io.Serializable interface. This is used by multisearchers to determine how to collate results from their searchers.\"\n\nThis kind of statement in the code leads one to believe that Lucene supports Serialization.  Maybe it should be removed from the Javadocs.  \n\n\"Thrift and ProtocolBuffers\" don't support dynamic class loading.  If one were to create their own Query class with custom code, serializing is the only way to represent the Query object and have Java load the additional implementation code.  One easy to see use case is if Analyzer were made Serializable then indexing over the network and trying different analyzing techniques could be accomplished with ease in a grid computing environment.  \n\n\"representations for queries independent of Lucene's Query, and map this to Lucene's Query. Is that not workable in this case?\"  \n\nMike wrote \"if we add field X to a class implementing Serializable,\nand must bump the SUID, that's a hard break on back compat. \"\n\nThere needs to be \"if statements\" in readExternal to handle backwards compatibility.  Given the number of classes, and the number of fields this isn't very much work.  Neither are the test cases.  I worked on RMI and Jini at Sun and elsewhere.  I am happy to put forth the effort to maintain and develop this functionality.  It is advantageous to place this functionality directly into the classes because in my experience many of the Lucene classes do not make all of the field data public, and things like dedicated serialization such as the XML query code are voluminous.  Also the half support of serialization right now seems to indicate there really isn't support for it.  \n\nHoss wrote: \"sort of mythical \"Lucene powerhouse\" \nLucene seems to run itself quite differently than other open source Java projects.  Perhaps it would be good to spell out the reasons for the reluctance to move ahead with features that developers work on, that work, but do not go in.  The developer contributions seem to be quite low right now, especially compared to neighbor projects such as Hadoop.  Is this because fewer people are using Lucene?  Or is it due to the reluctance to work with the developer community?  Unfortunately the perception in the eyes of some people who work on search related projects it is the latter.  \n\nMany developers seem to be working outside of Lucene and choosing not to open source in order to avoid going through the current hassles of getting code committed to the project.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12652914"
        },
        {
            "date": "2008-12-03T19:32:46+0000",
            "content": "\"In regards to Doug's comment about an alternate form... doesn't SOLR already have a XML based query format?  If so, just persist the queries using this. You will be immune to serialization changes (provided the SOLR parser remains backwards compatible).\"\n\nSOLR does not have an XML based query format.  XML is not ideal for distributed search because it is slow and verbose.  There are many ways to serialize things, the issue is not in choosing one, but in supporting what most Java libraries do today which is native to the Java platform.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12652920"
        },
        {
            "date": "2008-12-03T19:35:10+0000",
            "content": "Mark: \"There will be complaints no matter what. GWT tried getting around people having to implement Serializable by providing an interface with fewer promises: isSerizable. Many complained right away, as they had other classes that perhaps they where making Serializable simply for Apache XMLRpc or something. So now you can use either Serializable or isSerialiazble.\n\nPersonally, I think its fine to do as we are. I'm not against supporting more though. \"\n\nExternalizable and Serializable work interchangeably, a nice feature of Java.  For classes that no one submits an Externalizable patch for, the serialVersionUID needs to be added.  For ones that implement Externalizable, there is slightly more work, but not something someone with a year of Java experience can't maintain. ",
            "author": "Jason Rutherglen",
            "id": "comment-12652921"
        },
        {
            "date": "2008-12-03T19:55:56+0000",
            "content": "> Doesn't Hadoop handle versioning inside of Writeable classes?\n\nCurrently, yes, but this probably insufficient for a Hadoop 1.0 release.  Hadoop is a distributed system, and would like to provide RPC back-compatibility across minor versions after we go 1.0.  This is an explicit decision that's in the process of discussion within the Hadoop project.  RPC's within Hadoop daemons will probably require identical versions \u2013 all daemons in a cluster must be upgraded in lockstep.  We'll thus probably limit back-compatibility to client RPC's, so that a given client can talk to multiple clusters that are not running identical versions of Hadoop.  Lucene has made no such explicit policy decision.  Lucene is not an inherently distributed system.\n\nHadoop has not yet decided what mechanism to use to support back-compatible RPC, but Writable versioning is not sufficient, since it does not handle RPC protocols, and it's lower-level than we'd prefer.  We'll probably go with something more like Thrift or ProtocolBuffers.  Hadoop does not use Java serialization and makes no promises about that.\n\n> The developer contributions seem to be quite low right now, especially compared to neighbor projects such as Hadoop.\n\nAs one who monitors both projects, I don't see a marked difference.  In both there are sometimes patches that unfortunately languish, because, while they're important to the contributor, they fail to sufficiently engage a committer.  For example, HADOOP-3422 took over 6 months to get committed, probably because not many committers use Ganglia.\n\nThere is a difference in quantity: hadoop-dev has over 4x the traffic of lucene-dev.  But, normalized for that, the number of patches from non-committers feels comparable.  If anything I'd guess Lucene commits more patches from non-committers than does Hadoop. ",
            "author": "Doug Cutting",
            "id": "comment-12652932"
        },
        {
            "date": "2008-12-03T20:03:12+0000",
            "content": "The documentation should probably be fixed to state that Lucene's use of Serializeable currently assumes that all parties are using the exact same version of Lucene.  That's the default for Serializeable, but it probably bears stating explicitly.  Then we should decide, going forward, whether this should change, and, if so, for which classes and how.  Such a policy should be agreed on before code is written, no? ",
            "author": "Doug Cutting",
            "id": "comment-12652939"
        },
        {
            "date": "2008-12-03T20:03:18+0000",
            "content": "Jason, you are only partially correct.\n\nSOLR has XML definitions for updates/commits/deletes. It also supports string based queries. It also supports using XML for queries if you provide a handler, but the string syntax is simpler.\n\nAs for the serialization performance, you are mistaken.\n\nFor things like searches, the parsing time is extremely minor compared with the search time, so this additional overhead would be a fraction of the cost.\n\nWhen returning results sets, using XML can make a huge difference, as the overhead to paid on every item.  Even still with modern XML processors, the search time is still going to be the overriding performance factor by a huge margin.  Typically \"paged\" results are also used, so again, the XML parsing compared to the network overhead, is going to be minor.\n\nStill, if it is only for temporary serialization, binary works best - as long as it is Java to Java.\n\nWe have a search server that uses Java serialization for the message passing, including the results. It can be done without any changes to Lucene - again the overriding performance factor is the search itself (unless the queries returns 100k + documents and all are being returned - then the Java serialization time can be more than the search itself...\n ",
            "author": "robert engels",
            "id": "comment-12652940"
        },
        {
            "date": "2008-12-03T20:11:48+0000",
            "content": "Robert:\n> using XML for queries if you provide a handler, \n\nThat doesn't sound like query serialization.\n\nSOLR has a binary protocol due to criticisms about XML being slow.  \n\nI'm not sure why you and Doug and focusing on performance when that is not really the main issue I brought up.\n\nAlso I'm confused as to why dynamic classloading is being ignored by you folks as a Java feature that a Java search library could take advantage of to differentiate itself in the search (closed and open source) marketplace.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12652945"
        },
        {
            "date": "2008-12-03T20:27:24+0000",
            "content": "The reason the XML is not needed, is because the string format is robust enough, and is simpler...\n\nI am not focused on performance. It is just that Java serialization works well for temporary persistence. Other formats are better for long-term persistence. If you are only doing temporary persistence, you don't need backwards \ncompatibility.\n\nAlso if the API is exposed to the world (i.e. non-Java), a human readable (or even binary) format that is not based on Java serialization is going to work much better.\n\nif you use a non-binary protocol it is far easier to extend it in the future, and retain the ability to easily read older versions. This is why Swing uses XML for serialization, and not binary.\n\nYou could certainly use specialized class loaders to load old versions of classes in order to maintain the ability to read old now incompatible classes... it is just a lot of work (maintenance too, need to keep the old code around... etc.) for not a lot of benefit.\n\nAs for SOLR's binary protocol, fine, but it is probably for a fringe use case, or the submitter didn't do real world tests...  The XML parsing is just not that much greater than binary (at least in Java, since it is the object creation they both use that affects it). The search time is going to be far greater.\n\nFor large updates a binary loader can be more efficient that XML, but if you test it using real-world examples, I doubt you will see a huge difference - at least for the types of application TYPICALLY written using Lucene.\n ",
            "author": "robert engels",
            "id": "comment-12652962"
        },
        {
            "date": "2008-12-03T20:39:13+0000",
            "content": "> I'm not sure why you and Doug and focusing on performance when that is not really the main issue I brought up.\n\nIn the description of this issue you claim, \"This will make Serialization faster\".  I only added that, if this is a motivation, then it should be benchmarked and quantified.  If it's not a motivation, and cross-version compatibility is the only motivation, then that should be clarified.\n\n> dynamic classloading is being ignored by you folks\n\nPerhaps most Lucene developers are not using dynamic class loading, just as most Lucene developers seem not to be relying on java serialization.  Perhaps you can convince them to start using it, but if not, you may need to find a way to get Lucene to support it that minimally impacts other users of Lucene.  Adding methods to Term and Query that must be changed whenever these classes change adds a cost.  If folks don't see much benefit, then that cost outweighs.  Perhaps you can better enlighten us to the benefits rather than assert willful ignorance?  Jini promised great things with dynamic class loading, but the list of folks that use Jini is not long (http://www.jini.org/wiki/Who_uses_Jini%3F). ",
            "author": "Doug Cutting",
            "id": "comment-12652965"
        },
        {
            "date": "2008-12-03T22:12:34+0000",
            "content": "The discussion has evolved out of scope.  Cross-version compatibility is the main goal.  We have multiple versions of Lucene using the Spring RPC protocol.  The standard way to solve this is add a serialVersionUID like HashMap, String, and the other Java classes do.  \n\nPerformance is a topic of concern for Lucene users and existing RMI/Serialization users would transparently benefit by Externalizable being used.  \n\nI would like to implement this as a separate project, however performing reflection on each of the objects is not an efficient approach.  Writing wrapper code for each and every variation of Query is a waste of time when it implements Serializable and the communication is between Java systems. \n\nIt seems best to remove Serialization from Lucene so that users are not confused and create a better solution.\n\n> Perhaps most Lucene developers are not using dynamic class loading,\n\nDynamic classloading is popular and accepted in J2EE servers.  If done properly it is a very convenient way of deploying Java based systems.  Jini did not make this convenient enough.  Jini did not have a specific problem it was trying to solve and so was too complex and open ended.  Spring and J2EE make use of Java serialization for distributed objects.  People may not be using Lucene for this today but this is due largely to lack of support for things like standard Serialization.  With Lucene it is possible to make dynamic search classloading convenient in a search grid environment.  \n\nWhen J2EE was designed, no one was using Java on the server side.  A framework was composed and a handful of companies implemented the specification and then found it's way into projects.  If you are looking for users to ask for something that does not exist like this, it will not happen.  \n\nThe interface Lucene exposes is relatively static and known.  All of the top level classes, Query, Analyzer, Document, Similarity do not change very often.  In a search based grid computing environment, the ability to execute arbitrary code against the cloud saves time and effort in deploying new code to servers.  Restarting processes in a production environment is always expensive.  If one is implementing for example mutating genetic algorithms using Java over Lucene then it would be advantageous to dynamically load the classes that implement this.  They would be modifications of several classes such as Query, Similarity.  \n\nIt is peculiar all the effort that goes into backwards compatibility of the index, but for Serialization it is ignored.  This is and will be very confusing to users, especially ones who use J2EE. ",
            "author": "Jason Rutherglen",
            "id": "comment-12653001"
        },
        {
            "date": "2008-12-03T23:43:41+0000",
            "content": "The contrib section of Lucene contains an XML-based query parser which aims to provide full-coverage of Lucene queries/filters and provide extensibility to support 3rd party classes.\nI use this regularly in distributed deployments and this allows both non-Java clients and long-term persistence of queries with good stability across Lucene versions.\nAlthough I have not conducted formal benchmarks I have not been drawn to XML parsing as a bottleneck - search execution and/or document retrieves are normally the main bottlenecks.\n\nMaintaining XML parsing code is an overhead but ultimately helps decouple requests from the logic that executes requests. In serializing Lucene Query/Filter objects we are dealing with the classes which combine both the representation of the request criteria (what needs to be done) and the implementation (how things are done). We are forever finessing the \"how\" bit of this equation e.g. moving from RangeQuery to RangeFilters to TrieRangeFilter. The criteria however remains relatively static (\" I just want to search on a range\") and so it is dangerous to build clients that refer tdirectly to query implementation classes.\nThe XML parser provides a language-independent abstraction for clients to define what they want to be done without being too tied to how this is implemented.\n\nCheers\nMark\n ",
            "author": "Mark Harwood",
            "id": "comment-12653057"
        },
        {
            "date": "2008-12-03T23:46:17+0000",
            "content": "Even better. Thanks Mark. ",
            "author": "robert engels",
            "id": "comment-12653058"
        },
        {
            "date": "2008-12-04T01:29:41+0000",
            "content": "The contrib section of Lucene contains an XML-based query parser which aims to provide full-coverage of Lucene queries\n\nThanks for the reminder... Solr has pluggable query parsers now, and I've been meaning to check this out as a way to provide a more programmatic query specification. ",
            "author": "Yonik Seeley",
            "id": "comment-12653109"
        },
        {
            "date": "2008-12-04T13:44:31+0000",
            "content": "It seems best to remove Serialization from Lucene so that users are not confused and create a better solution.\n\nI don't think that's the case.  If we choose to only support \"live serialization\" then we should add \"implements Serializable\" but spell out clearly in the javadocs that there is no guarantee of cross-version compatibility (\"long term persistence\") and in fact that often there are incompatibilities.\n\nI think \"live serialization\" is still a useful feature. ",
            "author": "Michael McCandless",
            "id": "comment-12653297"
        },
        {
            "date": "2008-12-04T15:01:09+0000",
            "content": "For classes that no one submits an Externalizable patch for, the serialVersionUID needs to be added.\n\nThe serialVersionUID approach would be too simplistic, because we can't simply bump it up whenever we make a change since that then breaks back compatibility.  We would have to override write/readObject or write/readExternal, and serialVersionUID would not be used. ",
            "author": "Michael McCandless",
            "id": "comment-12653321"
        },
        {
            "date": "2008-12-04T17:42:24+0000",
            "content": "Mike:\n\n       If you have class A implements Serializable, with a defined suid, say 1.\n\n       Let A2 be a newer version of class A, and suid is not changed, say 1.\n\n        Let's say A2 has a new field.\n\n       Imaging A is running in VM1 and A2 is running in VM2. Serialization between VM1 and VM2 of class A is ok, just that A will not get the new fields. Which is fine since VM1 does not make use of it. \n\n       You can argue that A2 will not get the needed field from serialized A, but isn't that better than crashing?\n\n        Either the case, I think the behavior is better than it is currently. (maybe that's why Eclipse and Findbug both report the lacking of suid definition in lucene code a warning)\n\n       I agree adding Externalizable implementation is more work, but it would make the serialization story correct.\n\n-John ",
            "author": "John Wang",
            "id": "comment-12653378"
        },
        {
            "date": "2008-12-04T19:06:41+0000",
            "content": "> Serialization between VM1 and VM2 of class A is ok, just that A will not get the new fields. Which is fine since VM1 does not make use of it.\n\nBut VM1 might require an older field that the new field replaced, and VM1 may then crash in an unpredictable way.  Not defining explicit suid's is more conservative: you get a well-defined exception when things might not work.  Defining suid's but doing nothing else about compatibility is playing fast-and-loose: it might work in many cases, but it also might cause strange, hard-to-diagnose problems in others.  If we want Lucene to work reliably across versions, then we need to commit to that goal as a project, define the limits of the compatibility, implement Externalizeable, add tests, etc.  Just adding suid's doesn't achieve that, so far as I can see. ",
            "author": "Doug Cutting",
            "id": "comment-12653413"
        },
        {
            "date": "2008-12-04T19:18:42+0000",
            "content": "Even if you changed SUIDs based on version changes, there is the very real possibility that the new code CAN'T be instantiated in any meaningful way from the old data. Then what would you do?\n\nEven if you had all of the old classes, and their dependencies available from dynamic classloading, it still won't work UNLESS every new feature is designed with backwards compatibility with previous versions  - a burden that is just too great when required of all Lucene code.\n\nGiven that, as has been discussed, there are other formats that can be used where isolated backwards persistence is desired (like XML based query descriptions).  Even these won't work if the XML description references explicit classes - which is why designing such a format for a near limitless query structure (given user defined query classes) is probably impossible.\n\nSo strive for a decent solution that covers most cases, and fails gracefully when it can't work.\n\nusing standard serialization (with proper transient fields) seems to fit this bill, since in a stable API, most core classes should remain fairly constant, and those that are bound to change may take explicit steps in their serialization (if deemed needed) ",
            "author": "robert engels",
            "id": "comment-12653421"
        },
        {
            "date": "2008-12-04T23:38:01+0000",
            "content": "The discussion here is whether it is better to have 100% of the time failing vs. 10% of the time failing. (these are just meaningless numbers to express a point)\nI do buy Doug's comment about getting into a weird state due to data serialization, but this is something Externalizable would solve.\nThis discussion has digressed to general Java serialization design, where it originally scoped only to several lucene classes.\n\nIf it is documented that lucene only supports serialization of classes from the same jar, is that really enough, doesn't it also depend on the compiler, if someone were to build their own jar?\n\nFurthermore, in a distributed environment with lotsa machines, it is always idea to upgrade bit by bit, is taking this functionality away by imposing this restriction a good trade-off to just implementing Externalizable for a few classes, if Serializable is deemed to be dangerous, which I am not so sure given the lucene classes we are talking about. ",
            "author": "John Wang",
            "id": "comment-12653545"
        },
        {
            "date": "2008-12-04T23:51:18+0000",
            "content": "> This discussion has digressed to general Java serialization design, where it originally scoped only to several lucene classes. \n\nWhich classes?  The existing patch applies to one class.  Jason said, \"If it looks ok, I will implement Externalizable in other classes.\" but never said which.  It would be good to know how wide the impact of the proposed change would be. ",
            "author": "Doug Cutting",
            "id": "comment-12653553"
        },
        {
            "date": "2008-12-05T00:27:04+0000",
            "content": "For our problem, it is Query all all its derived and encapsulated classes. I guess the title of the bug is too generic.\n\nAs far as my comment about other lucene classes, one can just go to the lucene javadoc and click on \"Tree\" and look for Serializable. If you want me to, I can go an fetch the complete list, but here are some examples:\n\n1) Document (Field etc.)\n2) OpenBitSet, Filter ...\n3) Sort, SortField\n4) Term\n5) TopDocs, Hits etc.\n\nFor the top level API.\n ",
            "author": "John Wang",
            "id": "comment-12653563"
        },
        {
            "date": "2008-12-05T01:38:53+0000",
            "content": "LUCENE-1473.patch\n\nserialVersionUID added to the relevant classes manually.  Defaulted to 10 because it does not matter, as long it is different between versions.  Thought of writing some code to go through the Lucene JAR, do an instanceof on the classes for Serializable and then verify that the serialVersionUID is 10.  \n\nTerm implements Externalizable.  \n\nSerializationUtils was adapted from WriteableUtils of Hadoop for writing VLong.  \n\nTestSerialization use case does term serialization and serializes an arbitrary query to a file and compares them.  \n\nTODO: \n\n\tImplement Externalizable\n\tMore unit tests?  How to write a unit test for multiple versions?\n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12653586"
        },
        {
            "date": "2008-12-05T10:30:58+0000",
            "content": "SerializeUtils is missing from the patch. ",
            "author": "Michael McCandless",
            "id": "comment-12653735"
        },
        {
            "date": "2008-12-05T17:29:56+0000",
            "content": "> How to write a unit test for multiple versions?\n\nWe can save, in files, serialized instances of each query type from the oldest release we intend to support.  Then read each of thes queries and check that it s equal to a current query that's meant to be equivalent (ssuming all queries implement equals well).  Something similar would need to be done for each class that is meant to be transmitted cross-version.\n\nThis tests that older queries may be processed by newer code.  It does not test that newer queries can be processed by older code.  Documentation is a big part of this effort, that should be completed first.  What guarantees to we intend to provide?  Once we've documented these, then we can begin writing tests.  For example, we may only guarantee that older queries work with newer code, and that newer hits work with older code.  To test that we'd need to have an old jar around that we could test against.  This will be a trickier test to configure. ",
            "author": "Doug Cutting",
            "id": "comment-12653869"
        },
        {
            "date": "2008-12-05T17:47:51+0000",
            "content": "LUCENE-1473.patch\n\nAdded Externalizable to Document, Field, AbstractField (as compared to the previous patch).  SerializationUtils is included.\n\nTODO:\n\n\tMore Externalizable classes with test cases for each one\n\n\n\n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12653871"
        },
        {
            "date": "2008-12-05T22:45:06+0000",
            "content": "Doug wrote: We can save, in files, serialized instances of each query type from the oldest release we intend to support. Then read each of thes queries and check that it s equal to a current query that's meant to be equivalent (ssuming all queries implement equals well). Something similar would need to be done for each class that is meant to be transmitted cross-version.\n\nThis tests that older queries may be processed by newer code. It does not test that newer queries can be processed by older code. Documentation is a big part of this effort, that should be completed first. What guarantees to we intend to provide? Once we've documented these, then we can begin writing tests. For example, we may only guarantee that older queries work with newer code, and that newer hits work with older code. To test that we'd need to have an old jar around that we could test against. This will be a trickier test to configure.\n\n--------------\n\nMakes sense.  I guarantee 2.9 and above classes will be backward compatible with the previous classes.  I think that for 3.0 we'll start to create new replacement classes that will not conflict with the old classes.  I'd really like to redesign the query, similarity, and scoring code to work with flexible indexing and allow new algorithms.  This new code will not create changes in the existing query, similarity, and scoring code which will remain serialization compatible with 2.9.  The 2.9 query, similarity, and scoring should leverage the new query, similarity and scoring code to be backwards compatible.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12653955"
        },
        {
            "date": "2008-12-05T23:16:09+0000",
            "content": "> I guarantee 2.9 and above classes will be backward compatible with the previous classes.\n\nIt sounds like you are personally guaranteeing that all serializeable classes will be forever compatible.  That's not what we'd need.  We'd need a proposed policy for the project to consider in terms of major and minor releases, specifying forward and/or backward compatibility guarantees.  For example, we might say, \"within a major release cycle, serialized queries from older releases will work with newer releases, however serialized queries from newer releases will not generally work with older releases, since we might add new kinds of queries in the course of a major release cycle\".  Similarly detailed statements would need to be made for each Externalizeable, no? ",
            "author": "Doug Cutting",
            "id": "comment-12653972"
        },
        {
            "date": "2008-12-05T23:32:00+0000",
            "content": "LUCENE-1473.patch\n\nAdded some more Externalizables.  \n\no.a.l.util.Parameter is peculiar in that it implements readResolve to override the serialization and return a local object to emulate enums.  I haven't figured out the places this is used and what the best approach is to externalize them.\n\nTODO:\n\n\tSame as before\n\n\n\nDoug wrote: \"\"within a major release cycle, serialized queries from older releases will work with newer releases, however serialized queries from newer releases will not generally work with older releases, since we might add new kinds of queries in the course of a major release cycle\". Similarly detailed statements would need to be made for each Externalizeable, no?\"\n\nSerialized objects in minor releases will work.  Serialized objects of older versions starting with 2.9 will be compatible with newer versions.  New versions will be compatible with older versions on a classes by class basis defined in the release notes.  It could look something like this:\n\nSerialization notes:\nBooleanQuery added a scoreMap variable that does not have a default value in 3.0 and is now not backwards compatible with 2.9.  \nPhraseQuery added a ultraWeight variable that defaults to true in 3.0 and is backwards compatible with 2.9. ",
            "author": "Jason Rutherglen",
            "id": "comment-12653981"
        },
        {
            "date": "2008-12-06T09:24:54+0000",
            "content": "I really wouldn't want to add another backwards-compatibility\nrequirement to Lucene, as the others already stated. Often in the past\nwas ensuring backwards-compatibility the part of writing patches that\ntook the longest and involved the most discussions.\n\nBut maybe we can come up with a fair compromise here. What if we\nchange the classes that currently implement Serializable, so that they\nimplement Externalizable and add a suid as Jason suggests. But we make\nclear in the javadocs that we don't support backwards-compatiblity\nhere, so e.g. a Term externalized with Lucene 2.9 can't be read with\n3.0, only with 2.9.\nThen we add a new class CustomExternalizableReader to util:\n\npublic abstract class CustomExternalizableReader {\n  public abstract void readExternal(Object obj, ObjectInput in)\n      throws IOException, ClassNotFoundException;\n} \n\n\n\nadd a package-private, static variable of this type to a class that\nimplements Externalizable and implement the deserialization code in a\ndefault instance of such a reader. This could look like this:\n\npublic class SomeClass implements Externalizable {\n  private int one;\n  private int two;\n\n...\n\n  static CustomExternalizableReader extReader = new CustomExternalizableReader() {\n    public void readExternal(Object obj, ObjectInput in) throws IOException,\n        ClassNotFoundException {\n      SomeClass s = (SomeClass) obj;\n      long uid = in.readLong();\n      if (uid != serialVersionUID) {\n        throw new IOException(\"Wrong serialVerionUID: \" + uid);\n      }\n      int one = in.readInt();\n      int two = in.readInt();\n      s.init(one, two);\n    }\n  };\n\n  // initialization method for readExternal\n  void init(int one, int two) {\n    this.one = one;\n    this.two = two;\n  }\n\n\n\nNote that I also specified an init() method. Since both init() and\nextReader are both package-private, they are not protected by our\nbackwards-compatibility policy and we can change them in any release.\n\nNow if in the next version of this class we add a new variable 'three'\nwe have to change init() and the reader:\n\n\npublic class SomeClassNewVersion implements Externalizable {\n  private int one;\n  private int two;\n  private int three;\n\n  static final long serialVersionUID = 2L;\n\n  public void readExternal(ObjectInput in) throws IOException,\n      ClassNotFoundException {\n    extReader.readExternal(this, in);\n  }\n\n  public void writeExternal(ObjectOutput out) throws IOException {\n    out.writeLong(serialVersionUID);\n    out.writeInt(one);\n    out.writeInt(two);\n    out.writeInt(three);\n  }\n\n  /**\n   * This reader can only read the externalized format created with the same\n   * version of this class. If backwards-compatibility is desired, a custom\n   * reader has to be implemented.\n   */\n  static CustomExternalizableReader extReader = new CustomExternalizableReader() {\n    public void readExternal(Object obj, ObjectInput in) throws IOException,\n        ClassNotFoundException {\n      SomeClassNewVersion s = (SomeClassNewVersion) obj;\n      long uid = in.readLong();\n      if (uid != serialVersionUID) {\n        throw new IOException(\"Wrong serialVerionUID: \" + uid);\n      }\n      int one = in.readInt();\n      int two = in.readInt();\n      int three = in.readInt();\n      s.init(one, two, three);\n    }\n  };\n\n  void init(int one, int two, int three) {\n    this.one = one;\n    this.two = two;\n    this.three = three;\n  }\n\n\n\nNow if someone tries to deserialize an object that was written with\nan old Lucene version, an exception will be thrown.\n\nBut the user can simply implement an own, backwards-compatible reader:\n\n\n    // Now the user implements their own backwards compatible reader\n    SomeClassNewVersion.extReader = new CustomExternalizableReader() {\n      public void readExternal(Object obj, ObjectInput in) throws IOException,\n          ClassNotFoundException {\n        SomeClassNewVersion c_new = (SomeClassNewVersion) obj;\n        long uid = in.readLong();\n        int one = in.readInt();\n        int two = in.readInt();\n        int three;\n        if (uid == 1) {\n          // old version - initialze with default value\n          three = -3;\n        } else {\n          // new version\n          three = in.readInt();\n        }\n        c_new.init(one, two, three);\n      }\n    };\n\n\n\nWith this approach we have to clearly document in the javadocs the\nexternalization format. Also if externalizable classes contain private\ninner classes that need to be serialized, then those inner classes\nhave to be made package-private.\n\nThe nice thing here is that we allow backwards-compatibility, but push\nthe burden of maintaining it to the user.\n\nI coded this all up as an example that I'm attaching here. Let me know\nwhat you think, please. The patch file contains a Demo.java with a main\nmethod that demonstrates what I'm proposing here. ",
            "author": "Michael Busch",
            "id": "comment-12654043"
        },
        {
            "date": "2008-12-06T14:26:07+0000",
            "content": "\n> Often in the past was ensuring backwards-compatibility the part of\n> writing patches that took the longest and involved the most\n> discussions.\n\nIt very much still is, as I'm learning with LUCENE-1458!\n\nYour first example is missing the read/writeExternal methods.\n\nI think the proposed approach is rather heavy-weight \u2013 we will have\nimplemented readExternal, writeExternal, this new\nCustomExtenralizableReader, package private init methods, make private\ninner classes package private, the need to javadoc specifically the\ncurrent externalization format written for each of our classes, the\nfuture need to help users to understand how they an achieve back\ncompatibility by subclassing CustomExternalizableReader, etc.\n\nI guess my feeling is all of that is a good amount more work than just\ndeciding to directly implement back compatibility, ourselves.\n\nEG, to do your example in a future world where we do support back\ncompat of serialized classes (NOTE \u2013 none of the code below is\ncompiled/tested):\n\nFirst a util class for managing versions:\n\npublic class Versions {\n  private int current;\n\n  int add(String desc) {\n    // TODO: do something more interesting with desc\n    return current++;\n  }\n\n  void write(ObjectOutput out) throws IOException {\n    // TODO: writeVInt\n    out.writeByte((byte) current);\n  }\n\n  void read(ObjectInput in) throws IOException {\n    // TODO: readVInt\n    final byte version = in.readByte();\n    if (version > current)\n      throw new IOException(\"this object was serialized by a newer version of Lucene (got \" + version + \" but expected <= \" + current + \")\");\n  }\n}\n\n\n\nThen, someone creates SomeClass:\n\n\npublic class SomeClass implements Externalizable {\n  private int one;\n  private int two;\n\n  private static final Versions versions = new Versions();\n  private static final int VERSION0 = versions.add(\"start\");\n\n  public SomeClass() {};\n\n  public void writeExternal(ObjectOutput out) throws IOException {\n    versions.write(out);\n    out.writeInt(one);\n    out.writeInt(two);\n  }\n\n  public void readExternal(ObjectInput in) throws IOException {\n    versions.read(in);\n    one = in.readInt();\n    two = in.readInt();\n  }\n\n  ...\n}\n\n\n\nThen on adding field three:\n\n\npublic class SomeClass implements Externalizable {\n  private int one;\n  private int two;\n  private int three;\n\n  private static final Versions versions = new Versions();\n  private static final int VERSION0 = versions.add(\"start\");\n  private static final int VERSION1 = versions.add(\"the new field three\");\n\n  public SomeClass() {};\n\n  public void writeExternal(ObjectOutput out) throws IOException {\n    versions.write(out);\n    out.writeInt(one);\n    out.writeInt(two);\n  }\n\n  public void readExternal(ObjectInput in) throws IOException {\n    int version = versions.read(in);\n    one = in.readInt();\n    two = in.readInt();\n    if (version >= VERSION1)\n      three = in.readInt();\n    else\n      // default\n      three = -3;\n  }\n\n  ...\n}\n\n\n\nIn fact I think we should switch to Versions utils class for writing/reading our index files...\n ",
            "author": "Michael McCandless",
            "id": "comment-12654108"
        },
        {
            "date": "2008-12-06T20:41:28+0000",
            "content": "\nYour first example is missing the read/writeExternal methods.\n\nOups, I forgot to copy&paste it. It's in the attached patch file though.\n\n\nI think the proposed approach is rather heavy-weight\n\nReally? In case we go the Externalizable way anyway, then I think this\napproach doesn't add too much overhead. You only need to add init()\nand move the deserialization code from readExternal() to the reader's\nreadExternal. It's really not too much more code.\n\nAnd, the code changes are straightforward when the class changes. No\nneed to worry about how to initialize newly added variables if an old \nversion is read for example.\n\nWhat I think will be the most work is documenting and explaining\nthis. But this would be an expert API, so probably people who really\nneed to use it are most likely looking into the sources anyway.\n\nBut for the record: I'm totally fine with using Serializable and just\nadding the serialVersionUID. Just if we use Externalizable, we might\nwant to consider something like this to avoid new backwards-\ncompatibility requirements. ",
            "author": "Michael Busch",
            "id": "comment-12654147"
        },
        {
            "date": "2008-12-08T18:50:48+0000",
            "content": "Would it take any more lines of code to remove Serializeable from the core classes and re-implement RemoteSearchable in a separate layer on top of the core APIs?  That layer could be a contrib module and could get all the externalizeable love it needs.  It could support a specific popular subset of query and filter classes, rather than arbitrary Query implementations.  It would be extensible, so that if folks wanted to support new kinds of queries, they easily could.  This other approach seems like a slippery slope, complicating already complex code with new concerns.  It would be better to encapsulate these concerns in a layer atop APIs whose back-compatibility we already make promises about, no? ",
            "author": "Doug Cutting",
            "id": "comment-12654513"
        },
        {
            "date": "2008-12-11T15:32:32+0000",
            "content": "This seems to be the right way to go. The patch attached removes all dependencies to Serializable and Remote from the core and moves it to contrib/remote. I introduced a new interface RemoteSearcher\n(not RemoteSearchable because I didn't want to pass Weights around), implemented by DefaultRemoteSearcher. An adapter realizing Searchable and delegating to RemoteSearcher is also included (RemoteSearcherAdapter. Encoding/Decoding of Lucene objects is delegated to the org.apache.lucene.remote.Serializer. For a sample serialization, I employed XStream which offers XML serialization (nearly) out-of-the-box. \nEverything is rather undocumented and would need a lot of cleanup, but as proof-of-concept it should be ok. Core and remote tests pass, with one exception: it is not possible anymore to serialize a RAMDirectory.\nWhat I don't like with the current patch is that a lot of different objects are passed around to keep the Searchable interface alive. Would it be possible to refactor such that Searchable represents a higher-level interface (or introduce a new alternative abstraction)? ",
            "author": "Wolf Siberski",
            "id": "comment-12655676"
        },
        {
            "date": "2008-12-11T15:35:14+0000",
            "content": "This patch removes all dependencies to Serializable and Remote from the core and adds contrib/remote as replacement ",
            "author": "Wolf Siberski",
            "id": "comment-12655677"
        },
        {
            "date": "2008-12-11T16:33:07+0000",
            "content": "Thanks Wolf, +1 on the change. This issue proposes to do the same thing: LUCENE-1407 ",
            "author": "Mark Miller",
            "id": "comment-12655689"
        },
        {
            "date": "2008-12-11T23:24:50+0000",
            "content": "Thanks, Wolf, this looks like a promising approach.\n\nJason, John: would this sort of thing meet your needs?\n\nI'm not sure we can remove everything from trunk immediately.  Rather we should deprecate things and remove them in 3.0.  The removal of Serializeable will break compatibility, so must be well-advertised.\n\nHitCollector-based search should simply not be supported in distributed search.  The Searchable API was designed for remote use and does not include HitCollector-based access.\n\nWeighting, and hence ranking, does not appear to be implemented correctly by this patch.  An approach that might work would be to:\n\n\textend MultiSearcher\n\tpass its CachedDfSource to remote searchers along with queries\n\tconstruct a Weight on the search node using the CachedDfSource\nDoes that make sense?\n\n ",
            "author": "Doug Cutting",
            "id": "comment-12655806"
        },
        {
            "date": "2008-12-12T01:37:52+0000",
            "content": "To Wolf: Your patch looked like it was quite a bit of work, nice job!  Restricting people to XML will probably not be suitable though.  Some may want JSON or something that more directly encodes the objects.  \n\nGeneral:\nIt seems the alternative solutions to serialization simply shift the problem around but do not really solve the underlying issues (speed, versioning, writing custom serialization code, and perhaps dynamic classloading).  The externalizable code will not be too lengthy and should be more convenient than alternatives to implement (with the code necessary being roughly equivalent to an equals method).  For example protocol buffers requires maintaining files that remind me of IDL files from CORBA to describe the objects.  \n\nDeprecating serialization entirely needs to be taken to the java-user mailing list as there are quite a number of installations relying on it.  If this is something that overlaps with SOLR then it would be good for the SOLR folks to separate it out as a serialization library that could be used outside of the SOLR server.  This would be a good idea for most of the SOLR functionality otherwise there would seem to be redundant development occurring.  \n\nI'll finish up the Externalizable patch once LUCENE-1314 is completed (IndexReader.clone) as it is something that needs feedback and testing to ensure it's workable for 2.9, whereas Externalizable is somewhat easier.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12655845"
        },
        {
            "date": "2008-12-12T05:24:17+0000",
            "content": "> shift the problem around but do not really solve the underlying issues \n\nThat's the idea, actually, to shift it out of the core into contrib.  We could use Externalizeable there, with no XML.\n\n> Deprecating serialization entirely needs to be taken to the java-user mailing list as there are quite a number of installations relying on it.\n\nNo, we make decisions on the java-dev mailing list.  Also, it won't go away, folks might just have to update their code to use different APIs if and when when they upgrade to 3.0. ",
            "author": "Doug Cutting",
            "id": "comment-12655894"
        },
        {
            "date": "2008-12-12T09:56:37+0000",
            "content": "Thanks to Doug and Jason for your constructive feedback. Let me first clarify the purpose and scope of the patch. IMHO, the discussion about Serialization in Lucene is not clear-cut at all. My opinion is that moving all distribution-related code out of the core leads to a cleaner separation of concerns and thus is better design. On the other hand with removing Serializable we limit the Lucene application space at least a bit (e.g., no support for dynamic class loading), and abandon the advantages default Java serialization offers. Therefore the patch is to be taken as contribution to explore the design space (as Michaels patch on custom readers explored the Serializable option), and not as a full-fledged solution proposal.\n\n> [Doug] The removal of Serializeable will break compatibility, so must be well-advertised.\nSure. I removed Serializable to catch all related errors; this was not meant as proposal for a final patch.\n\n>  [Doug] The Searchable API was designed for remote use and does not include HitCollector-based access.\nCurrently Searchable does include a HitCollector-based search method, although the comment says that 'HitCollector-based access to remote indexes is discouraged'. The only reason to provide an implementation is that I wanted to keep the Searchable contract. Is remote access the only purpose of Searchable/MultiSearcher? Is it ok to break compatibility with respect to these classes? IMHO a significant fraction of the current clumsiness in the remote package stems from my attempt to fully preserve the Searchable API.\n\n>  [Doug] Weighting, and hence ranking, does not appear to be implemented correctly by this patch. \nTrue, I was a bit too fast here. We could either solve it along the line you propose, or revert to pass the Weight again instead of the Query. The issue IMHO is orthogonal to the Serializable discussion and more related to the question how a good remote search interface and protocol should look like.\n\n> [Jason] Restricting people to XML will probably not be suitable though.\nThe patch does not limit serialization to XML. It just requires that encoding to and decoding from String is implemented, no matter how. I used XML/XStream as proof-of-concept implementation, but don't propose to make XML mandatory. The main reason for introduction of the Serializer interface was to emphasize that XML/XStream is just one implemantation option. Actually, the current approach feels like at least one indirection more than required; for a final solution I would try to come up with a better design.\n\n> [Jason] It seems the alternative solutions to serialization simply shift the problem around but do not really solve \n> the underlying issues (speed, versioning, writing custom serialization code, and perhaps dynamic classloading).\nIn a sense, the problem is indeed 'only' shifted around and not yet solved. The good thing about this shift is that Lucene core becomes decoupled from these issues. The only real limitation I see is that dynamic classloading can't be realized anymore. \n\nWith respect to speed, I don't think that encoding/decoding is a significant performance factor in distributed search, but this would need to be benchmarked. With respect to versioning, my patch still keeps all options open. What is more important, Lucene users can now decide if they need compatibility between different versions, and roll their own encoding/decoding if they need it. Of course, if they are willing to contribute and maintain custom serializers which preserve back compatibility, they can do it in contrib as well as they could have done it in the core. Custom serialization is still possible although the standard Java serialization framework can't be used anymore for that purpose, and I admit that this is a disadvantage. ",
            "author": "Wolf Siberski",
            "id": "comment-12655944"
        },
        {
            "date": "2008-12-12T17:19:22+0000",
            "content": "> Therefore the patch is to be taken as contribution to explore the design space [ ... ]\n\nYes, and it is much appreciated for that.  Thanks again!\n\n> Currently Searchable does include a HitCollector-based search method [ ... ]\n\nYou're right.  I misremembered.  This dates back to the origin of Searchable.\n\nhttp://svn.apache.org/viewvc?view=rev&revision=149813\n\nPersonally, I think it would be reasonable for a distributed implementation to throw an exception if one tries to use a HitCollector.\n\n> We could either solve it along the line you propose, or revert to pass the Weight again instead of the Query.\n\nWithout using an introspection-based serialization like Java serialization it would be difficult to pass a Weight over the wire using public APIs, since most implementations are not public.  But, since Weight's are constructed via a standard protocol, the method I outlined could work. ",
            "author": "Doug Cutting",
            "id": "comment-12656071"
        },
        {
            "date": "2011-01-24T21:16:46+0000",
            "content": "Won't be working on these and they're old ",
            "author": "Jason Rutherglen",
            "id": "comment-12986025"
        }
    ]
}