{
    "id": "LUCENE-1224",
    "title": "NGramTokenFilter creates bad TokenStream",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Bug",
        "fix_versions": [
            "4.3"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "With current trunk NGramTokenFilter(min=2,max=4) , I index \"abcdef\" string into an index, but I can't query it with \"abc\". If I query with \"ab\", I can get a hit result.\n\nThe reason is that the NGramTokenFilter generates badly ordered TokenStream. Query is based on the Token order in the TokenStream, that how stemming or phrase should be anlayzed is based on the order (Token.positionIncrement).\n\nWith current filter, query string \"abc\" is tokenized to : ab bc abc \nmeaning \"query a string that has ab bc abc in this order\".\nExpected filter will generate : ab abc(positionIncrement=0) bc\nmeaning \"query a string that has (ab|abc) bc in this order\"\n\nI'd like to submit a patch for this issue.",
    "attachments": {
        "NGramTokenFilter.patch": "https://issues.apache.org/jira/secure/attachment/12377679/NGramTokenFilter.patch",
        "LUCENE-1224.patch": "https://issues.apache.org/jira/secure/attachment/12377997/LUCENE-1224.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-03-13T10:54:51+0000",
            "content": "Modified to set a right start/end offset value in Token properties. ",
            "author": "Hiroaki Kawai",
            "id": "comment-12578239"
        },
        {
            "date": "2008-03-15T18:08:17+0000",
            "content": "Please add unit tests to the patch demonstrating the issue. ",
            "author": "Grant Ingersoll",
            "id": "comment-12579069"
        },
        {
            "date": "2008-03-16T14:46:30+0000",
            "content": "Patch updated with unit test.\n\nLUCENE-1225 is easier to understand this problem. This patch also includes token filter issues that is more complicated. ",
            "author": "Hiroaki Kawai",
            "id": "comment-12579196"
        },
        {
            "date": "2008-05-14T11:24:56+0000",
            "content": "Hi Hiroaki,\n\nI have been reviewing the tests for this and have a couple of comments.  First, I don't see why you need to bring indexing into the equation.  Second, the changes to testNGrams still don't test the issue, namely they don't examine that the output ngrams are actually in the correct position.  I think you deduce this later in testIndexAndQuery, but it is never explicitly stated.  I'd drop testIndexAndQuery and just fix testNGrams such that it checks the positions appropriately.  \n\nOn a more philosophical level, it is a bit curious to me that if we have the strings \"abcde fghi\" that we are fine with \"b\" being at position 1, and not at position 0, but \"ab\" needs to be at position 0.  I wonder if there is any thoughts on what the relative positions of ngrams should be.  Should they all occur at the same position?  It seems to me, that it doesn't make sense that the \"f\" ngrams don't start until some position other than 1.  This would currently prevent doing phrase queries such as \"ab fg\" with no slop.\n\nI'm assuming this applies to LUCENE-1225 as well.\n\nI will link 1225 to this issue, and you can attach a single patch. ",
            "author": "Grant Ingersoll",
            "id": "comment-12596729"
        },
        {
            "date": "2008-05-15T09:29:29+0000",
            "content": "Q: Why it is necessary to index\nA: Because it was necessary to show how the query is performed. \nThat is the point I wanted to address. \n\nQ: testNGrams don't test the issue\nA: Exactlly. it don't test the issue.\nI modified the test because it failed with my patch, that\nToken.toString() prints additional incrementPosition information.\nI read the existing test program, and found that current test\nprogram depends on Token.toString() method.\nI thought we'd better test it without Token.toString().\nCurrent test program tests that the Token have NO positionIncrement.\n\ntestIndexAndQuery is the very test that address the issue.\nPlease don't drop it. Think the case, we want to search a word \nthat contain \"abcd\" with 2-gram index.\nThe test does searching \"abcd\" with 2,3-gram.\n\nWe have the 2gram of abcde; 'ab', 'bc', 'cd', 'de'.\nReffering the current lucene implementation, the position gap \nof 'ab' and 'bc' must be 1. ",
            "author": "Hiroaki Kawai",
            "id": "comment-12597069"
        },
        {
            "date": "2008-05-15T10:25:55+0000",
            "content": "OK, let me change the comment.  You can test this problem without indexing and querying.  All of the information is available on the token.  I would suggest you revert the test to it's original and then modify testNGrams()  by adding asserts that check that the positionIncrement value is set properly.   By going the indexing/querying route, you are not only testing the token filters, but pretty much all of Lucene and are thus subject to any problems there.  In other words, it ain't a unit test.  If you set the posiitionIncrement properly and test for it, it will work in Lucene for the queries, etc.  If it doesn't, we have much bigger problems than ngrams.  That being said, if you want to fix testNgrams, and leave the query case in, that is fine by me.\n ",
            "author": "Grant Ingersoll",
            "id": "comment-12597079"
        },
        {
            "date": "2008-05-15T10:27:25+0000",
            "content": "FWIW, I also think we should address the more philosophical question of what the intermediate positions should be of the tokens.  The more I think about it, the more I think all \"grams\" of a given word should be at the same position, but I would like to hear from others on this before deciding. ",
            "author": "Grant Ingersoll",
            "id": "comment-12597080"
        },
        {
            "date": "2008-05-15T11:36:58+0000",
            "content": "Umm..., if you don't like indexing and querying in the unit test, where should I place the join test that use NGramTokenizer? It might be nice if we could place that join test in a proper place.\n\nI placed the testIndexAndQuery in the code because the other code like KeywordAnalyzer (in the core) test code has index&query test code in its unit tests.\n\nI'm fine with separating the codes into different files. ",
            "author": "Hiroaki Kawai",
            "id": "comment-12597094"
        },
        {
            "date": "2008-05-15T11:45:14+0000",
            "content": "In my understanding,\n----------\nThe sequence we have \"This is an example\"\n\nIf we want to tokenize with white space tokenizer, the tokens are\n\"This\", \"is\", \"an\", \"example\"\npositions are 0,1,2,3\n\nIf we want to tokenize with 2-gram, the tokens are\n\"Th\" \"hi\" \"is\" \"s \" \" i\" \"is\" \"s \" \" a\" \"an\" \"n \" \" e\" \"ex\" \"xa\" \"am\" \"mp\" \"pl\" \"le\"\npositions are 0,1,2,3,4,... ",
            "author": "Hiroaki Kawai",
            "id": "comment-12597098"
        },
        {
            "date": "2008-05-15T12:03:20+0000",
            "content": "Umm..., if you don't like indexing and querying in the unit test, where should I place the join test that use NGramTokenizer? It might be nice if we could place that join test in a proper place.\n\nMy point is, I don't think the test needs to do any indexing/querying at all to satisfy the change.  It adds absolutely nothing to the test and only complicates the matter.\n\nI placed the testIndexAndQuery in the code because the other code like KeywordAnalyzer (in the core) test code has index&query test code in its unit tests.\n\nJust because another does it doesn't make it right.\n\n\nIf we want to tokenize with white space tokenizer, the tokens are\n\"This\", \"is\", \"an\", \"example\"\npositions are 0,1,2,3\n\nIf we want to tokenize with 2-gram, the tokens are\n\"Th\" \"hi\" \"is\" \"s \" \" i\" \"is\" \"s \" \" a\" \"an\" \"n \" \" e\" \"ex\" \"xa\" \"am\" \"mp\" \"pl\" \"le\"\npositions are 0,1,2,3,4,...\n\nYes, I understand how it currently works.  My question is more along the lines of is this the right way of doing it?  I don't know that it is, but it is a bigger question than you and me.  I mean, if we are willing to accept that this issue is a bug, then it presents plenty of other problems in terms of position related queries.  For example, I think it makes sense to search for \"th ex\" as a phrase query, but that is not possible do to the positions (at least not w/o a lot of slop)\n\n ",
            "author": "Grant Ingersoll",
            "id": "comment-12597107"
        },
        {
            "date": "2008-05-15T12:32:58+0000",
            "content": "My take as a user:\n\nMaybe, I don't understand the application of an n-gram filter, but my expectation is that words from the input that are indexed are positioned. Isn't that required to be able to do \"near\" searches?\n\nIt would not matter to me if the n-grams have sub-positions to distinguish them (e.g. 1.a, 1.b, 1.c, 1,d, 2.a, ... for the example above. note: not implying any representation in this notation) ",
            "author": "DM Smith",
            "id": "comment-12597112"
        },
        {
            "date": "2008-05-15T15:15:30+0000",
            "content": "About test code: I'm not going to say that \"I'm right\". I just wanted to address the issue and share what we should solve. If you don't like the code, please just tell me how I should do (the better way). I initially put the code there because I thought it was reasonable and proper, but I'm fine with changing it.\n\n\nFor example, I think it makes sense to search for \"th ex\" as a phrase query\n\nFor example, I think it makes sense to search for \"example\" as a phrase query instead.\n\nI want to address that NGramTokenizer is very useful for non-white-space-separated languages, for example Japanese. In that case, we won't search \"th ex\", because it assumes sentences are separated by whte space. I want to search by a fragment of a text sequence.\n\nI agree that this might be a big problem. IMHO, the issues comes from concept mismatch of TokenFilter and TermPosition. The discussion should moved to mailing-list? ",
            "author": "Hiroaki Kawai",
            "id": "comment-12597156"
        },
        {
            "date": "2008-05-15T15:32:56+0000",
            "content": "\n\n\n\nI think the right way is simply to change the existing test to check  \nthat the term positions are correct per the changes.  Right now, it  \ndoesn't check the position increment and it should.  This can be done  \nby looking at the positionIncrement on the Token that is produced by  \nthe TokenStream and doesn't require indexing. \n ",
            "author": "Grant Ingersoll",
            "id": "comment-12597161"
        },
        {
            "date": "2008-05-15T16:05:08+0000",
            "content": "Hiroaki:\nI agree with Grant about unit tests.  I looked at the unit tests and thought the same thing as Grant - why is Hiroaki adding indexing/searching into the mix?  Your change is about modifying the positions of n-grams, and you don't need to index or search for that.  The test will be a lot simpler if you just test for positions, like Grant suggested.\n\nAlso, once you change the unit test this way, it will be a lot easier to play with positions and figure out what the \"right\" way to handle positions is.\n\nFinally, it might turn out that people have different needs or different expectations for n-gram positions.  Thus, when making changes, perhaps you can think of a mechanism that allows the caller to instruct the n-gram tokenizer which token positioning approach to take (e.g. the \"incremental\" one, or the one based on the position of the originating token, or...) ",
            "author": "Otis Gospodnetic",
            "id": "comment-12597174"
        },
        {
            "date": "2008-05-16T14:32:04+0000",
            "content": "After all, where should I place the testIndexAndQuery? Does anybody have a suggestion? ",
            "author": "Hiroaki Kawai",
            "id": "comment-12597489"
        },
        {
            "date": "2008-10-09T23:04:15+0000",
            "content": "This bug caused me major headaches trying to figure out why substring matching with an NGramTokenFilter wasn't working for anything other then when setting min and max to the same values. \n\nThe patch seems to fix the issue when applied locally, however it also has a bug in it. It will stop parsing a token stream if a token comes through that is less then the minGramSize, even if there are tokens yet in the stream that are greater then minGramSize. ",
            "author": "Todd Feak",
            "id": "comment-12638423"
        },
        {
            "date": "2009-08-10T23:56:14+0000",
            "content": "Sounds like this should really be addressed ... along with LUCENE-1225 ",
            "author": "Mark Miller",
            "id": "comment-12741628"
        },
        {
            "date": "2010-02-16T21:11:42+0000",
            "content": "I too think its really important we fix this. I have to agree with Hiroaki's analysis of the situation, and the problems can be seen by looking at the code in both the filter/tokenizers and the tests themselves.\n\nCurrently the tokenizers are limited to 1024 characters (LUCENE-1227), this is very related to this issue.\nLook at the test for 1,3 ngrams of \"abcde\":\n\npublic void testNgrams() throws Exception {\n        NGramTokenizer tokenizer = new NGramTokenizer(input, 1, 3);\n        assertTokenStreamContents(tokenizer,\n          new String[]{\"a\",\"b\",\"c\",\"d\",\"e\", \"ab\",\"bc\",\"cd\",\"de\", \"abc\",\"bcd\",\"cde\"}, \n\n\n\nin my opinion the output should instead be: a, ab, ...\nOtherwise the tokenizer will either always be limited to 1024 chars or must read the entire document into RAM.\nThis same problem exists for the EdgeNgram variants.\n\nI agree with Grant's comment about the philosophical discussion about positions of the tokens, perhaps we need an option for this (where they are all posInc=1, or the posInc=0 is generated based on whitespace). I guess I think we could accomodate both needs by having tokenizer/filter variants too, but I'm not sure.\n\nThe general problem i have with trying to determine a fix is that it will break backwards compatibility, and I also know that EdgeNGram is being used for some purposes such as \"auto-suggest\". So I don't really have any idea beyond making new filters/tokenizers, as I think there is another use case where the old behavior fits? ",
            "author": "Robert Muir",
            "id": "comment-12834471"
        },
        {
            "date": "2013-04-26T22:20:07+0000",
            "content": "All n-grams now have the same position and offsets as the original token (LUCENE-4955). ",
            "author": "Adrien Grand",
            "id": "comment-13643307"
        },
        {
            "date": "2013-05-10T10:34:38+0000",
            "content": "Closed after release. ",
            "author": "Uwe Schindler",
            "id": "comment-13654280"
        }
    ]
}