{
    "id": "LUCENE-5042",
    "title": "Improve NGramTokenizer",
    "details": {
        "components": [],
        "fix_versions": [
            "4.4",
            "6.0"
        ],
        "affect_versions": "None",
        "priority": "Major",
        "labels": "",
        "type": "Improvement",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Now that we fixed NGramTokenizer and NGramTokenFilter to not produce corrupt token streams, the only way to have \"true\" offsets for n-grams is to use the tokenizer (the filter emits the offsets of the original token).\n\nYet, our NGramTokenizer has a few flaws, in particular:\n\n\tit doesn't have the ability to pre-tokenize the input stream, for example on whitespaces,\n\tit doesn't play nice with surrogate pairs.\n\n\n\nSince we already broke backward compatibility for it in 4.4, I'd like to also fix these issues before we release.",
    "attachments": {
        "LUCENE-5042.patch": "https://issues.apache.org/jira/secure/attachment/12586688/LUCENE-5042.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2013-06-07T09:35:02+0000",
            "content": "Patch:\n\n\n\tComputes n-grams based on unicode code points instead of java chars\n\n\n\n\n\tAdds the ability to split the input stream on some chars like CharTokenizer\n\n ",
            "author": "Adrien Grand",
            "id": "comment-13677923"
        },
        {
            "date": "2013-06-07T09:56:49+0000",
            "content": "hey adrien, this looks very cool! I have a couple of minor comments:\n\n\n\tcan we factor out the toCodepoints calculation into a method in for instance CharacterUtils I think we use this elsewhere as well in a similar way and you might want to reuse it in the future as well.\n\tcan we have a comment on NGramTokenizer that every method should be final except of isTokenChar\n\tif you can think of a hard limit for the while(true) loop in NGramTokenizer can we add an assert that makes sure we always make progress ie. never walk backwards or don't consume anything? not sure if it is posssible.\n\tcan you use more parentesis for readability like in:\n\n\n\n\nif (gramSize > maxGram || bufferStart + gramSize > bufferEnd) \n// vs.\n\nif (gramSize > maxGram || (bufferStart + gramSize) > bufferEnd) \n\n ",
            "author": "Simon Willnauer",
            "id": "comment-13677930"
        },
        {
            "date": "2013-06-08T18:38:48+0000",
            "content": "Thanks for the review Simon, here is a new patch that should satisfy your concerns. Additionally,\n\n\n\tit also fixes the other (edge) n-gram tokenizers and filters\n\n\n\n\n\tI factored out some methods into CharacterUtils\n\n\n\n\n\tI went into a bug because Character.codePointAt(char[], int) doesn't know the end of the char[], this  can be a problem when working with buffers which are not filled up so I made this API forbidden and fixed other places that relied on it: codePointAt(char[], int, int) looks safer to me.\n\n\n\n\n\tI changed the CharacterUtil.fill API so that it reads fully (which it didn't do although the documentation stated it does).\n\n ",
            "author": "Adrien Grand",
            "id": "comment-13678814"
        },
        {
            "date": "2013-06-12T12:39:16+0000",
            "content": "Wow! this looks very cool! I wonder if we should rename the CharacterUtils classes into UTF32CharUtils and UTF16CharUtils rather than Java5 and Java4 I think this makes more sense in terms of how we use it today?\n\nOtherwise I am +1 to commit! good stuff Adrien! ",
            "author": "Simon Willnauer",
            "id": "comment-13681171"
        },
        {
            "date": "2013-06-12T12:57:31+0000",
            "content": "I opened LUCENE-5054 for this ",
            "author": "Simon Willnauer",
            "id": "comment-13681183"
        },
        {
            "date": "2013-06-12T13:17:49+0000",
            "content": "[trunk commit] jpountz\nhttp://svn.apache.org/viewvc?view=revision&revision=1492185\n\nLUCENE-5042: Fix the n-gram tokenizers and filters.\n\nThis commit fixes n-gram tokenizers and filters so that they handle\nsupplementary characters correctly and adds the ability to pre-tokenize the\nstream in tokenizers. ",
            "author": "Commit Tag Bot",
            "id": "comment-13681203"
        },
        {
            "date": "2013-06-12T14:48:47+0000",
            "content": "[branch_4x commit] jpountz\nhttp://svn.apache.org/viewvc?view=revision&revision=1492231\n\nLUCENE-5042: Fix the n-gram tokenizers and filters. ",
            "author": "Commit Tag Bot",
            "id": "comment-13681303"
        },
        {
            "date": "2013-06-12T15:59:00+0000",
            "content": "[trunk commit] jpountz\nhttp://svn.apache.org/viewvc?view=revision&revision=1492257\n\nLUCENE-5042: Reset the CharBuffer in TokenStream.reset(). ",
            "author": "Commit Tag Bot",
            "id": "comment-13681352"
        },
        {
            "date": "2013-06-12T16:03:05+0000",
            "content": "[branch_4x commit] jpountz\nhttp://svn.apache.org/viewvc?view=revision&revision=1492259\n\nLUCENE-5042: Reset the CharBuffer in TokenStream.reset(). ",
            "author": "Commit Tag Bot",
            "id": "comment-13681355"
        },
        {
            "date": "2013-06-12T20:52:52+0000",
            "content": "[trunk commit] sarowe\nhttp://svn.apache.org/viewvc?view=revision&revision=1492420\n\nLUCENE-5042: Maven configuration: add chars.txt to forbiddenapis config ",
            "author": "Commit Tag Bot",
            "id": "comment-13681626"
        },
        {
            "date": "2013-06-12T20:54:58+0000",
            "content": "[branch_4x commit] sarowe\nhttp://svn.apache.org/viewvc?view=revision&revision=1492422\n\nLUCENE-5042: Maven configuration: add chars.txt to forbiddenapis config (merged trunk r1492420) ",
            "author": "Commit Tag Bot",
            "id": "comment-13681627"
        },
        {
            "date": "2013-06-13T07:06:14+0000",
            "content": "[trunk commit] jpountz\nhttp://svn.apache.org/viewvc?view=revision&revision=1492543\n\nLUCENE-5042: Refuse to convert if the length is negative. ",
            "author": "Commit Tag Bot",
            "id": "comment-13681976"
        },
        {
            "date": "2013-06-13T07:21:37+0000",
            "content": "[branch_4x commit] jpountz\nhttp://svn.apache.org/viewvc?view=revision&revision=1492549\n\nLUCENE-5042: Refuse to convert if the length is negative. ",
            "author": "Commit Tag Bot",
            "id": "comment-13681985"
        },
        {
            "date": "2013-07-23T18:37:02+0000",
            "content": "Bulk close resolved 4.4 issues ",
            "author": "Steve Rowe",
            "id": "comment-13716729"
        }
    ]
}