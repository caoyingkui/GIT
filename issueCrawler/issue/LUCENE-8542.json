{
    "id": "LUCENE-8542",
    "title": "Provide the LeafSlice to CollectorManager.newCollector to save memory on small index slices",
    "details": {
        "components": [
            "core/search"
        ],
        "status": "Patch Available",
        "resolution": "Unresolved",
        "fix_versions": [],
        "affect_versions": "None",
        "labels": "",
        "priority": "Minor",
        "type": "Improvement"
    },
    "description": "I have an index consisting of 44 million documents spread across 60 segments. When I run a query against this index with a huge number of results requested (e.g. 5 million), this query uses more than 5 GB of heap if the IndexSearch was configured to use an ExecutorService.\n\n(I know this kind of query is fairly unusual and it would be better to use paging and searchAfter, but our architecture does not allow this at the moment.)\n\nThe reason for the huge memory requirement is that the search will create a TopScoreDocCollector for each segment, each one with numHits = 5 million. This is fine for the large segments, but many of those segments are fairly small and only contain several thousand documents. This wastes a huge amount of memory for queries with large values of numHits on indices with many segments.\n\nTherefore, I propose to change the CollectorManager - interface in the following way:\n\n\tchange the method newCollector to accept a parameter LeafSlice that can be used to determine the total count of documents in the LeafSlice\n\tMaybe, in order to remain backwards compatible, it would be possible to introduce this as a new method with a default implementation that calls the old method - otherwise, it probably has to wait for Lucene 8?\n\tThis can then be used to cap numHits for each TopScoreDocCollector to the leafslice-size.\n\n\n\nIf this is something that would make sense for you, I can try to provide a patch.",
    "attachments": {
        "LUCENE-8542.patch": "https://issues.apache.org/jira/secure/attachment/12950019/LUCENE-8542.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16703283",
            "author": "Christoph Kaser",
            "content": "I attached a patch - hopefully this shows more clearly what I meant.\n\nSince CollectorManager is marked as experimental, I think it might be possible to port this patch against Lucene 7 as well without providing a default implementation of the new method and keeping the old method. ",
            "date": "2018-11-29T14:41:08+0000"
        },
        {
            "id": "comment-16703818",
            "author": "Adrien Grand",
            "content": "This doesn't look like a great solution to your problem? If I were you, I'd probably rather fork TopScoreDocCollector to dynamically grow the heap. ",
            "date": "2018-11-29T21:07:39+0000"
        },
        {
            "id": "comment-16704304",
            "author": "Lucene/Solr QA",
            "content": "\n\n\n  +1 overall \n\n\n\n\n\n\n\n\n\n Vote \n Subsystem \n Runtime \n Comment \n\n\n\u00a0\n\u00a0\n\u00a0\n  Prechecks  \n\n\n +1 \n  test4tests  \n   0m  0s \n  The patch appears to include 1 new or modified test files.  \n\n\n\u00a0\n\u00a0\n\u00a0\n  master Compile Tests  \n\n\n +1 \n  compile  \n   1m 38s \n  master passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Patch Compile Tests  \n\n\n +1 \n  compile  \n   0m 51s \n  the patch passed  \n\n\n +1 \n  javac  \n   0m 51s \n  the patch passed  \n\n\n +1 \n  Release audit (RAT)  \n   0m 30s \n  the patch passed  \n\n\n +1 \n  Check forbidden APIs  \n   0m 20s \n  the patch passed  \n\n\n +1 \n  Validate source patterns  \n   0m 20s \n  the patch passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Other Tests  \n\n\n +1 \n  unit  \n  30m 12s \n  core in the patch passed.  \n\n\n +1 \n  unit  \n   3m 10s \n  facet in the patch passed.  \n\n\n  \n   \n  40m 30s \n   \n\n\n\n\n\n\n\n\n\n Subsystem \n Report/Notes \n\n\n JIRA Issue \n LUCENE-8542 \n\n\n JIRA Patch URL \n https://issues.apache.org/jira/secure/attachment/12950019/LUCENE-8542.patch \n\n\n Optional Tests \n  compile  javac  unit  ratsources  checkforbiddenapis  validatesourcepatterns  \n\n\n uname \n Linux lucene2-us-west.apache.org 4.4.0-112-generic #135-Ubuntu SMP Fri Jan 19 11:48:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux \n\n\n Build tool \n ant \n\n\n Personality \n /home/jenkins/jenkins-slave/workspace/PreCommit-LUCENE-Build/sourcedir/dev-tools/test-patch/lucene-solr-yetus-personality.sh \n\n\n git revision \n master / 75b1831 \n\n\n ant \n version: Apache Ant(TM) version 1.9.6 compiled on July 20 2018 \n\n\n Default Java \n 1.8.0_191 \n\n\n  Test Results \n https://builds.apache.org/job/PreCommit-LUCENE-Build/127/testReport/ \n\n\n modules \n C: lucene lucene/core lucene/facet U: lucene \n\n\n Console output \n https://builds.apache.org/job/PreCommit-LUCENE-Build/127/console \n\n\n Powered by \n Apache Yetus 0.7.0   http://yetus.apache.org \n\n\n\n\n\n\nThis message was automatically generated.\n ",
            "date": "2018-11-30T05:59:08+0000"
        },
        {
            "id": "comment-16704391",
            "author": "Christoph Kaser",
            "content": "I think it would be nice to have the option to grow the heap dynamically. However the way TopScoreDocCollector and TopDocsCollector are currently built, for a lucene user that would mean copying the complete source code for those classes and adopting them to use a java.util.PriorityQueue (probably with worse performance than org.apache.lucene.util.PriorityQueue).\n\nThis is certainly possible, but would mean a lot of code duplication (from the perspective of a lucene user, because the used priority queue can't be changed easily),\n\nI think that this patch makes sense anyway: The size of segments has a very wide range in a typical index, and usually there are a lot more small segments than large ones. Given that the default implementation of IndexSearcher.slices() returns one slice per segment, that means a lot of wasted memory for all queries that have a numHits greater than the typical size of a small segment. I don't think it has any negative impact on queries with a small value of numHits, because it only adds one Math.min per segment.\n\nIt also helps with my problem: for an index with 28 segments and 13,360,068 documents and a search with numhits=5,000,000, it makes the difference between creating priority queues with a combined size of 140,000,000 vs 13,360,068. As you can see in the following table, there are benefits for searches with a more reasonable numHits value as well (all against my index):\n\n\u00a0\n\n\n\nnumHits\nCombined size w/o patch\nCombined size with patch\n\n\n10,000,000\n280,000,000\n13,360,068\n\n\n5,000,000\n140,000,000\n13,360,068\n\n\n1,000,000\n28,000,000\n6,870,854\n\n\n100,000\n2,800,000\n1,632,997\n\n\n50,000\n1,400,000\n1,015,274\n\n\n10,000\n280,000\n252,528\n\n\n\n\n\n\u00a0 ",
            "date": "2018-11-30T08:11:20+0000"
        }
    ]
}