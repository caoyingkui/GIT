{
    "id": "SOLR-10163",
    "title": "CheckHdfsIndexTest fail.",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Test",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "[junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=CheckHdfsIndexTest -Dtests.method=doTest -Dtests.seed=C045205F24FEF89C -Dtests.slow=true -Dtests.locale=es-NI -Dtests.timezone=Etc/GMT+8 -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1\n   [junit4] ERROR   10.2s J8  | CheckHdfsIndexTest.doTest <<<\n   [junit4]    > Throwable #1: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=1637, name=Thread-391, state=RUNNABLE, group=TGRP-CheckHdfsIndexTest]\n   [junit4]    > Caused by: java.lang.NullPointerException\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([C045205F24FEF89C]:0)\n   [junit4]    > \tat org.apache.hadoop.fs.DU.<init>(DU.java:74)\n   [junit4]    > \tat org.apache.hadoop.fs.DU.<init>(DU.java:95)\n   [junit4]    > \tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.<init>(BlockPoolSlice.java:140)\n   [junit4]    > \tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.addBlockPool(FsVolumeImpl.java:827)\n   [junit4]    > \tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:405)Throwable #2: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=1635, name=Thread-389, state=RUNNABLE, group=TGRP-CheckHdfsIndexTest]\n   [junit4]    > Caused by: java.lang.NullPointerException\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([C045205F24FEF89C]:0)\n   [junit4]    > \tat org.apache.hadoop.fs.DU.<init>(DU.java:74)\n   [junit4]    > \tat org.apache.hadoop.fs.DU.<init>(DU.java:95)\n   [junit4]    > \tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice.<init>(BlockPoolSlice.java:140)\n   [junit4]    > \tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.addBlockPool(FsVolumeImpl.java:827)\n   [junit4]    > \tat org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeList$2.run(FsVolumeList.java:405)",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2017-02-19T20:36:30+0000",
            "content": "Mike Drob, here is a fail I just hit. Check if it's reproducible. ",
            "author": "Mark Miller",
            "id": "comment-15873834"
        },
        {
            "date": "2017-02-19T20:42:59+0000",
            "content": "Bummer, the reproduce line passed: \nreproduce with: ant test  -Dtestcase=CheckHdfsIndexTest -Dtests.method=doTest -Dtests.seed=C045205F24FEF89C -Dtests.slow=true -Dtests.locale=es-NI -Dtests.timezone=Etc/GMT+8 -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1 ",
            "author": "Mark Miller",
            "id": "comment-15873836"
        },
        {
            "date": "2017-02-19T20:43:34+0000",
            "content": "We might just have to add exceptions for these if we can. ",
            "author": "Mark Miller",
            "id": "comment-15873837"
        },
        {
            "date": "2017-02-21T21:14:18+0000",
            "content": "Hadoop's DU changed a bunch in 2.8.0 from HADOOP-12973 - might not be a problem once we upgrade. Looking at the 2.7.2 code, I I don't even understand what is causing that NPE, though. ",
            "author": "Mike Drob",
            "id": "comment-15876731"
        }
    ]
}