{
    "id": "SOLR-9446",
    "title": "Leader failure after creating a freshly replicated index can send nodes into recovery even if index was not changed",
    "details": {
        "components": [
            "replication (java)"
        ],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [
            "6.3",
            "7.0"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Minor"
    },
    "description": "We noticed this issue while migrating solr index from machines A1, A2 and A3 to B1, B2, B3. We followed following steps (and there were no updates during the migration process).\n\n\n\tIndex had replicas on machines A1, A2, A3. Let's say A1 was the leader at the time\n\tWe added 3 more replicas B1, B2 and B3. These nodes synced with the by replication. These fresh nodes do not have tlogs.\n\tWe shut down one of the old nodes (A3).\n\tWe then shut down the leader (A1)\n\tNew leader got elected (let's say A2) became the new leader\n\tLeader asked all the replicas to sync with it\n\tFresh nodes (ones without tlogs), first tried PeerSync but since there was no frame of reference, PeerSync failed and fresh nodes fail back on to try replication\n\n\n\n\nAlthough replication would not copy all the segments again, it seems like we can short circuit sync to put nodes back in active state as soon as possible. \n\nIf in case freshly replicated index becomes leader for some reason, it can still send nodes (both other freshly replicated indexes and old replicas) into recovery. Here is the scenario\n\n\n\tFreshly replicated becomes the leader.\n\tNew leader however asks all the replicas to sync with it.\n\tReplicas (including old one) ask for versions from the leader, but the leader has no update logs, hence replicas can not compute missing versions and falls back to replication",
    "attachments": {
        "SOLR-9446.patch": "https://issues.apache.org/jira/secure/attachment/12829630/SOLR-9446.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-08-26T15:50:04+0000",
            "author": "Pushkar Raste",
            "content": "I can think of couple of ways two solve it using fingerprint comparsion\n\n\tAdd a fingerprint check in SyncStratergy.syncToMe() and request replica to sync only if fingperint does not match\n\tAdd a fingerprint check in RecoveryStratergy.doRecovery() and initiate recovery only if fingerprint check does not match\n\tAdd a fingerprint check in PeerSync.sync() to check if we are already in sync\n\n\n\nI think we almost always try PeerSync before trying replication so #3, should work. ",
            "id": "comment-15439296"
        },
        {
            "date": "2016-08-26T18:13:52+0000",
            "author": "Pushkar Raste",
            "content": "It also seems like if I take either of approach #1 or approach #2 I will have add check at more than one place to cover multiple scenario (e.g. LIR, node coming out of long GC etc, getVersions call to RealTImeGetComponent with sync) \n\nAs I mentioned in the last comment, since we always try PeerSync first, adding a a check in PeerSync.sync(), seems easiest/cleanest way to fix it. ",
            "id": "comment-15439491"
        },
        {
            "date": "2016-08-26T21:36:44+0000",
            "author": "ASF GitHub Bot",
            "content": "GitHub user praste opened a pull request:\n\n    https://github.com/apache/lucene-solr/pull/73\n\n    SOLR-9446 Do a fingerprint check before starting PeerSync\n\n\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/praste/lucene-solr SOLR-9446\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/lucene-solr/pull/73.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #73\n\n\ncommit 82e2fb5914a202f7577b92b999370cfb6fcc605b\nAuthor: Pushkar Raste <praste@bloomberg.net>\nDate:   2016-08-26T17:50:40Z\n\n    SOLR-9446 Do a fingerprint check before starting PeerSync\n\n ",
            "id": "comment-15439918"
        },
        {
            "date": "2016-09-12T07:30:48+0000",
            "author": "Noble Paul",
            "content": "I find the following assertion commented out in the testcase\n\n\n// assertEquals(\"FreshNode went into recovery\", numRequestsBefore, numRequestsAfter);\n\n\n\nI tested by uncommenting it. it is passing anyway. ",
            "id": "comment-15483355"
        },
        {
            "date": "2016-09-21T18:26:18+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 15cee3141c160c38756ceed73bd1cd88002c01c9 in lucene-solr's branch refs/heads/master from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=15cee31 ]\n\nSOLR-9446: Leader failure after creating a freshly replicated index can send nodes into recovery even if index was not changed ",
            "id": "comment-15510764"
        },
        {
            "date": "2016-09-21T18:27:39+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 8502995e3b1ce66db49be26b23a3fa3c169345a8 in lucene-solr's branch refs/heads/branch_6x from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=8502995 ]\n\nSOLR-9446: Leader failure after creating a freshly replicated index can send nodes into recovery even if index was not changed ",
            "id": "comment-15510768"
        },
        {
            "date": "2016-10-27T19:33:56+0000",
            "author": "Jim Musil",
            "content": "FWIW, This was a particularly bad problem for us. In the scenario outlined in the description, our old nodes were going down at different times but generally while the new nodes were in recovery. This produced a situation where all the live nodes were in recovery, but could never recover. The new nodes did not serve requests and the collection was dead in the water.\n\n\n\n ",
            "id": "comment-15612935"
        },
        {
            "date": "2016-11-09T08:38:53+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Closing after 6.3.0 release. ",
            "id": "comment-15650298"
        }
    ]
}