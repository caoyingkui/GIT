{
    "id": "SOLR-303",
    "title": "Distributed Search over HTTP",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "1.3"
        ],
        "components": [
            "search"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Searching over multiple shards and aggregating results.\nMotivated by http://wiki.apache.org/solr/DistributedSearch",
    "attachments": {
        "distributed_add_tests_for_intended_behavior.patch": "https://issues.apache.org/jira/secure/attachment/12378146/distributed_add_tests_for_intended_behavior.patch",
        "solr-dist-faceting-non-ascii-all.patch": "https://issues.apache.org/jira/secure/attachment/12382364/solr-dist-faceting-non-ascii-all.patch",
        "distributed_pjaol.patch": "https://issues.apache.org/jira/secure/attachment/12373211/distributed_pjaol.patch",
        "shards_qt.patch": "https://issues.apache.org/jira/secure/attachment/12381157/shards_qt.patch",
        "distributed_facet_count_bugfix.patch": "https://issues.apache.org/jira/secure/attachment/12378136/distributed_facet_count_bugfix.patch",
        "fedsearch.patch": "https://issues.apache.org/jira/secure/attachment/12361878/fedsearch.patch",
        "distributed.patch": "https://issues.apache.org/jira/secure/attachment/12372134/distributed.patch",
        "shards.start_rows.patch": "https://issues.apache.org/jira/secure/attachment/12385455/shards.start_rows.patch",
        "fedsearch.stu.patch": "https://issues.apache.org/jira/secure/attachment/12366220/fedsearch.stu.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Sharad Agarwal",
            "id": "comment-12512892",
            "date": "2007-07-16T08:41:09+0000",
            "content": "To do a quick test of the patch, try adding:\nshards=local,localhost:8080\nas a request parameter to the search url "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12512983",
            "date": "2007-07-16T18:01:24+0000",
            "content": "Thanks for kicking this off Sharad!\n\n> \"Index view consistency between multiple requests\" requirement is relaxed in this implementation. \n\nDo you have plans to remedy that?  Or do you think that most people are OK with inconsistencies that could arise?\n\n> Load-balancing and Fail-over taken care by VIP as usual\n\nIn a static configuration, this works OK, but it might be nice to support a more dynamic environment where extra shards could be easily added.  It might also be the case that a custom partitioning function could be implemented (such as improving caching by partitioning queries, etc) or it may be more efficient to do the second phase of a query on the same shard copy as the first phase.\nIn that case it might make sense load balancing across shards from Solr . The  VIP solution would map to the simplest case of a single copy of each shard, thus a LB could still be used if desired.\n\n> STEP 1: The query is built, terms are extracted. \n\nWhere are terms extracted from (some queries require index access)?  This should be delegated to the shards, no?  It can be the same step that gets the docFreqs from the shards (pass the query, not the terms).  Step 1 should also be optional for those that can make do with local idf factors.\n\nIn order to facilitate custom logic in a distributed environment,\nI think we should base the solution on something like\nhttps://issues.apache.org/jira/browse/SOLR-281\nWith additional hooks for distributed search.\nThis should allow relatively independent parts of query processing to piggyback in the same network request (for example, the first steps to querying and faceting can be added to a single request, and highlighting and stored field retrieval can be done in conjunction).\n\nAny thoughts on RMI vs HTTP for the searcher-subsearcher interface?  \n "
        },
        {
            "author": "Sharad Agarwal",
            "id": "comment-12513167",
            "date": "2007-07-17T07:25:24+0000",
            "content": "> \"Index view consistency between multiple requests\" requirement is relaxed in this implementation.\n>>Do you have plans to remedy that? Or do you think that most people are OK with inconsistencies that could arise?\nThe thing to note here is that currently multi phase execution is based on document unique fields, NOT on doc internal ids. So there wont be much inconsistencies between requests; as it does not depend on changing internal doc ids. \nThe possibility is that a particular document may have been deleted when the second phase executes.; which in my opinion should be OK to live with.\nOther possibility could be the document is changed and original query terms are not present in the document anymore. This can be solved by doing a AND with the original query and uniq field document query.\n\nIf people think it is really crucial to have index view consistency, then it should be easy to implement \"Consistency via Retry\" as mentioned in http://wiki.apache.org/solr/FederatedSearch \n\n>>It might also be the case that a custom partitioning function could be implemented (such as improving caching by partitioning queries, etc) or it may >>be more efficient to do the second phase of a query on the same shard copy as the first phase.\n>>In that case it might make sense load balancing across shards from Solr. \nFor second phase of a query to execute on the same shard copy, third party \"Sticky load balancers\" can be used. I believe Apache already does that. All copies of a single partition can sit behind the Apache load balancer (doing the \"Sticky\"). The merger just needs to know about the Load-balancer ip/port for each partition. Now based on the query, merger can search the appropriate partitions only.\n\nTo improve the caching, Solr itself has to do the load balancing. Other option could be to introduce the query result cache at the merger itself.\n\n>>Where are terms extracted from (some queries require index access)? This should be delegated to the shards, no?It can be the same step that gets >>the docFreqs from the shards (pass the query, not the terms). \nyes, if thats the case, should be easy to implement as you have suggested.\n\n>>I think we should base the solution on something like https://issues.apache.org/jira/browse/SOLR-281 \ncool, I was looking for something like this. This looks like the way to go.\n\n>>Any thoughts on RMI vs HTTP for the searcher-subsearcher interface? \nRMI could be supported as an option by enhancing the ResponseParser (better name ??) interface. The remote search server can directly return the SolrQueryResponse object. I understand that there will be some performance benefit if doing the native java marshalling/unmarshalling of object; instead of Solr response writing and then parsing (if done the HTTP way). The question we need to answer is: Is the effort/complexity worth it?\n\nIn our organization we made a conscious decision to go for HTTP. The operation folks like HTTP as it is standard stuff, load balancing, monitoring etc. Lot of tools already available for it. With RMI, I am not sure external Sticky load-balancing is possible; the merger itself has to build the logic.\nMoreover, I think HTTP fits more naturally with Solr in its Request handler model.\n\n\n "
        },
        {
            "author": "Sharad Agarwal",
            "id": "comment-12513488",
            "date": "2007-07-18T08:48:20+0000",
            "content": "Making this issue blocked by SOLR-281 "
        },
        {
            "author": "Sharad Agarwal",
            "id": "comment-12516651",
            "date": "2007-07-31T09:05:09+0000",
            "content": "Added support for sorting. "
        },
        {
            "author": "Mike Klaas",
            "id": "comment-12523925",
            "date": "2007-08-30T20:10:40+0000",
            "content": "Great stuff!\n\nI think asynchronous/parallel requests are a central feature to this kind of result aggregator.  In my similar python implementation, I fire off all the requests and collect the responses in a select() loop.  Threads are possible but get somewhat weighty when you have many shards (I've used up to 90).  An easier alternative to select() is to simply fire off all the requests and then wait for the responses sequentially (assuming java has an api that allows this).  This is almost as good as the select() loop but does not have the same complexity. "
        },
        {
            "author": "Sharad Agarwal",
            "id": "comment-12523996",
            "date": "2007-08-31T04:25:39+0000",
            "content": "Recently I have added a feature of parallel requests to shards using a thread pool. (not yet uploaded the patch)\nAsync IO would be the next thing but dont want to bring in its complexity so early.\nPerhaps, we can benchmark the performance of the thread pool/parallel requests implementation. Later based on the numbers, work towards having Async IO. "
        },
        {
            "author": "Stu Hood",
            "id": "comment-12526563",
            "date": "2007-09-11T19:11:55+0000",
            "content": "Sharad, what Solr revision have you applied the latest copy of this this patch against? I know that the r573893 commit caused all kinds of havoc in the source tree, but I'd really like to try it out, and I don't mind using an older revision to get it working.\n\nAlso, do you have any newer versions of the patch?\n\nThanks a lot! "
        },
        {
            "author": "Sharad Agarwal",
            "id": "comment-12526669",
            "date": "2007-09-12T05:00:19+0000",
            "content": "Updated to do following:\n1. Fed search query being executed via different components\n-GlobalCollectionStatComponent (optional)\n-FirstQPhaseComponent\n-SecondQPhaseComponent (optional)\nThe user can use 'skip' request param to tell which component to skip\n\n2. Sub searcher requests are executed in parallel threads using thread pool.\n\n3. work against the trunk revision 574785.\n\nI am working on further refactoring the code and make it work with SOLR-281, which should make the code really clean with pluggable components. "
        },
        {
            "author": "Stu Hood",
            "id": "comment-12527570",
            "date": "2007-09-14T17:01:04+0000",
            "content": "Thanks Sharad, the last patch applied cleanly as you said.\n\nI've run into some errors that should be quick fixes for your next revision:\n\n\n\tI had to modify the code not to assume that shard names end in '/solr' so that I could specify an instance name, like: 'blah.com:8080/instance_name'.\n\tThe parameters for your subqueries are not (always?) getting escaped. My document ids contain some colons (':'), and so its throwing a null pointer error during the SecondQueryphase, and then again in SolrCore execute.\n\n\n\n\nThanks a lot for your work! "
        },
        {
            "author": "Stu Hood",
            "id": "comment-12527637",
            "date": "2007-09-14T21:22:14+0000",
            "content": "For the second issue above, I did the following:\n\n*Added 'static String escape(string, field, schema)' to QueryParsing, that uses SolrQueryParser's escape method. I run this across all key values as they are being iterated in the beginning of 'SecondQPhaseComponent.createSecondPhaseParams' "
        },
        {
            "author": "Stu Hood",
            "id": "comment-12527647",
            "date": "2007-09-14T22:08:18+0000",
            "content": "I'm also seeing the following issue, but I haven't have time to investigate:\n\n\nWARNING: Exception while querying shard crc10:8080/solr_postfix09092000-09112000 :java.lang.ClassCastException: com.sun.org.apache.xerces.internal.dom.DeferredTextImpl cannot be cast to org.w3c.dom.Element "
        },
        {
            "author": "Stu Hood",
            "id": "comment-12528083",
            "date": "2007-09-17T14:40:47+0000",
            "content": "I was trying to use the PHP serialized response writer with the federate search patch, and ran into some trouble. Then I noticed that you had made some changes in XMLWriter to support the federated.ResponseDocs class.\n\nIs there any way ResponseDocs could extend Doclist so that all of the writers don't need to be modified? "
        },
        {
            "author": "Sharad Agarwal",
            "id": "comment-12528253",
            "date": "2007-09-18T05:20:10+0000",
            "content": ">Is there any way ResponseDocs could extend Doclist so that all of the writers don't need to be modified?\nResonseDocs are based on document unique key while DocList is based on internal doc id. \nThe purpose of ResponseDocs is to represent documents lying in remote index while DocList are meant for local internal doc id.\n\nI dont think there is an easy way to avoid modifying writers. Currently writers retrieve document data based on local internal doc id. But for remote index, this has to be done differently. "
        },
        {
            "author": "Stu Hood",
            "id": "comment-12528260",
            "date": "2007-09-18T05:50:20+0000",
            "content": "Yea, that is a bit of a problem isn't it...\n\nIt looks like if you subclassed SolrIndexSearcher and DocList, you could generate fake Lucene document ids that map back to actual unique keys. Unfortunately, SolrIndexSearcher is intimidatingly long, so depending on how people feel about adding to the writers, it might not be necessary to modify it. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12528494",
            "date": "2007-09-18T18:27:06+0000",
            "content": "FWIW: I haven't really been able to follow this issue much (it's way out of my area of expertise) but seeing some comments go by in email i wanted to mention two things...\n\n> ResonseDocs are based on document unique key while DocList is based on internal doc id.\n> The purpose of ResponseDocs is to represent documents lying in remote index while DocList are\n> meant for local internal doc id.\n\nOne thing to keep in mind is the way MultiReader deals with this in Lucene ... if you know the maxDoc of each of your sub-indexes, then you can compute internal docIds ... that may be one way to preserve the DocList abstraction (and allow for supporting schemas without uniqueKey fields) when dealing with federated search  (allthough it may open up new problems if you need to rely on havingsome form of an identifier that doesn't change .. i'm not sure if the approach being taken makes multiple requests to the shards)\n\nThat said...\n\nFederated Search is a complex enough concept that if it requires additions to the ResponseWriter API to be done effeciently, I don't think that would be the end of the world \u2013 the key thing would be to find ways to minimize the impact on existing clients \u2013 if things work for you now, they should keep working for you; if you want to start using federated search, then it's fair to expect that you may have to change a few things, or deal with a few limitations.  Off hte top of my head: one option may be to add a FederatableResponseWriter subclass such that if a request is federated, then the writer being used must implement that interface or it's a runtime error. "
        },
        {
            "author": "Stu Hood",
            "id": "comment-12528593",
            "date": "2007-09-18T22:24:53+0000",
            "content": "I've been working with the most recent version of the patch some more, and have run into some more issues. Since I'm sure that you have been working on the patch on your own, I don't want you to have to dig through my changes as a diff. Instead I'll just try and point them out for your revision.\n\nWe have a few fields that are indexed as strings that contain characters like '@' and ':'. There are still a few places having to do with the 'df' parameter where these need to be escaped/worked around, but here is what I've found so far:\n\n\tDuring the iteration over the document's uniqFields in SecondQPhaseComponent.createSecondPhaseParams\n\t\n\t\tSurrounded the value in \"quotes\"\n\t\n\t\n\tDuring the iteration over strTerms in MultiSearchRequestHandler.buildQuery\n\t\n\t\tModified the split on '@' to only split on the last '@' in the string.\n\t\tModified the split on ':' to split into a maximum of 2 pieces.\n\t\n\t\n\tDuring the iteration over extractedTerms in GlobalCollectionStatComponent.calcuateGlobalCollectionStat\n\t\n\t\tModified the split on ':' to split into a maximum of 2 pieces.\n\t\n\t\n\n\n\n\nI also ran into some problems in other areas:\n\n\tXMLResponseParser.parse(url, params) fails to parse a response if it is indented using the 'indent=on' parameter, which gets passed through to the subqueries\n\t\n\t\tStripped out 'indent' during the iteration over the params (but there is probably a better solution to this issue)\n\t\n\t\n\tSecondQPhaseComponent.createSecondPhaseParams passes the 'start' parameter through to the subqueries, which leads to a null pointer when we are querying for specific unique ids.\n\t\n\t\tStripped out 'start' during the iteration over the params\n\t\n\t\n\n\n\n\nI'll keep looking for the last few 'df' issues. Thanks a lot for the patch! "
        },
        {
            "author": "Sharad Agarwal",
            "id": "comment-12528674",
            "date": "2007-09-19T06:55:41+0000",
            "content": "Thanks much Stu for pointing the issues. Will take care of these in next update. "
        },
        {
            "author": "Stu Hood",
            "id": "comment-12528837",
            "date": "2007-09-19T17:35:32+0000",
            "content": "I got the rest of the DF issues resolved: please refer to the attached and ignore my earlier comments (some of them were faulty).\n\nHere is a patch that is very similar to your last patch, but with my fixes included. If you `diff fedsearch.stu.patch fedsearch.patch` you should be able to see what I did.\n\nThe final (minor) issue I've found, is that when I strip the 'start' parameter in SecondQPhaseComponent.createSecondPhaseParams, it gets stripped from the response that is returned to the user as well (although it is honored in the results).\n\nThanks again! "
        },
        {
            "author": "Stu Hood",
            "id": "comment-12529559",
            "date": "2007-09-21T21:35:04+0000",
            "content": "Here is another revision of the latest patch (I've still only tried it with r574785: I'm a bit crunched for time).\n\nResolved issues:\n\n\tWe were forgetting to increment a counter during the last step in SecondQPhaseComponent.process, and so we weren't getting results from all shards.\n\tSecondQPhaseComponent.merge was throwing away any fields that already existed in a document, and so it was throwing away parts of multi-value fields. Fixing this exposed the first issue listed below.\n\tMultiSearchRequestHandler was creating non-daemon threads (the default) for the thread pool. This meant that when the JVM died, the threads were sticking around. I added a ThreadFactory that creates daemonized threads.\n\n\n\nOpen issues:\n\n\tThe 'local' shard is ignoring the 'FL' parameter during the FirstQueryPhase, and returning the entire document. We then try and merge the document into itself in SecondQPhaseComponent.merge, causing a ConcurrectMod exception. For now, I put a check for \"newDoc != oldDoc\", but I think we need to figure out why the local query is returning full documents.\n\tRange queries are broken (probably due to the extract terms phase failing)\n\t'start' and 'numfound' are incorrect when returned to the user\n\t\n\t\tstart is getting wiped out somewhere\n\t\tnumfound is counting all copies of matches for a uniqKey towards the total\n\t\n\t\n\tMultiSearchRequestHandler.THREAD_POOL_SIZE and MultiSearchRequestHandler.REQUEST_TIME_OUT_IN_MS should be configuration parameters in solrconfig.xml.\n\n\n\nThanks a lot! "
        },
        {
            "author": "Sharad Agarwal",
            "id": "comment-12531419",
            "date": "2007-10-01T06:52:31+0000",
            "content": "Hi Stu, I have merged the issues fixed by you in my version of patch.\n\nAlso the following changes:\n\n->Based the solution on SOLR-281. Got away with the MultiSearchRequestHandler base class. Now federated features are just pure components which can be plugged along with other regular components like QueryComponent, HighlightComponent etc.\nThis way it would be very easy to override the core federated functionality.\n\n->Renamed the Federated components to :\nGlobalCollectionStatComponent\nMainQPhaseComponent\nAuxiliaryQPhaseComponent\n\n-> Doing url encoding for the request params in XMLResponseParser\n\n\n "
        },
        {
            "author": "Stu Hood",
            "id": "comment-12531565",
            "date": "2007-10-01T17:01:06+0000",
            "content": "\n->Based the solution on SOLR-281. Got away with the MultiSearchRequestHandler base class.\nDoes this mean that this patch requires SOLR-281 to be applied first? Also, what revision should it be applied to, or will HEAD work?\n\n\n\n-> Doing url encoding for the request params in XMLResponseParser\nAh yea, I ran into that one a few days ago as well. Additionally, I had XMLResponseParser strip the \"WT\" parameter off its queries: 'extractterms' was passing through the user's wt, which caused the XML parsing to fail (obviously =) ).\n\n\nCan't wait to try it out... Thanks a lot! "
        },
        {
            "author": "Sharad Agarwal",
            "id": "comment-12531997",
            "date": "2007-10-03T04:27:49+0000",
            "content": ">> Does this mean that this patch requires SOLR-281 to be applied first?\nNo. Current patch has all files. When SOLR-281 gets in to the trunk then this patch needs to be reworked. "
        },
        {
            "author": "Stu Hood",
            "id": "comment-12534213",
            "date": "2007-10-12T04:06:34+0000",
            "content": "I really like where you are headed with the 'componentized' version of the patch: it much more elegant.\n\nBut: I'm still having the problem where multi-valued fields only get one value returned. During AuxiliaryQPhaseComponent.merge(SolrQueryResponse rsp, SolrQueryResponse auxPhaseRes), you check whether the field already exists before adding it, but multi-value fields can exist multiple times.\n\nAlso, I'm considering disabling the AuxiliaryQPhase and just letting the MainQPhase fetch the document fields. All of my documents are small ( < 1k on average with 10ish fields), so I think making another call across the network to fetch the remaining fields is probably a waste for our indexes. What do you think?\n\nThanks! "
        },
        {
            "author": "Sharad Agarwal",
            "id": "comment-12534268",
            "date": "2007-10-12T10:48:26+0000",
            "content": ">>But: I'm still having the problem where multi-valued fields only get one value returned. During AuxiliaryQPhaseComponent.merge(SolrQueryResponse rsp, SolrQueryResponse auxPhaseRes), you check whether the field already exists before adding it, but multi-value fields can exist multiple times.\n\nyeah, may be I have missed those scenarios. If you have the fix, pl feel free to update the patch.\n\n>>Also, I'm considering disabling the AuxiliaryQPhase and just letting the MainQPhase fetch the document fields. All of my documents are small ( < 1k on average with 10ish fields), so I think making another call across the network to fetch the remaining fields is probably a waste for our indexes. What do you think?\nHaving AuxiliaryQPhase saves primarily on following counts:-\n1) fetching doc fields \n2) generating snippets \n3) more like this query etc\n-> for only the merged docs.\n\nFrom my experience generating snippets is very CPU intensive and if the no of shards are large, there would be lot of CPU wastage (if snippets are generated in MainQPhase) => CPU wastage proportional to (n-1)/n  => n being no of shards\nSo, having extra network calls saves on CPU. Hence there being trade-off between two. "
        },
        {
            "author": "Stu Hood",
            "id": "comment-12534731",
            "date": "2007-10-15T04:44:55+0000",
            "content": "\nyeah, may be I have missed those scenarios. If you have the fix, pl feel free to update the patch.\nUnfortunately, my fix was more of a workaround: I allow any field that is not the unique key to be added multiple times. But, the local shard always returns all the fields  of the document, so if the local shard is queried directly, some fields are duplicated. And so I don't query the local shard directly =/ "
        },
        {
            "author": "Stu Hood",
            "id": "comment-12536064",
            "date": "2007-10-18T21:55:56+0000",
            "content": "I'm still working on wrapping my head around the fedsearch phases, but I noticed the following stacktrace showing up in the logs every now and then:\n\nSEVERE: java.lang.NullPointerException\n        at org.apache.solr.handler.federated.component.GlobalCollectionStatComponent.prepare(GlobalCollectionStatComponent.java:81)\n        at org.apache.solr.handler.SearchHandler.handleRequestBody(SearchHandler.java:116)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:78)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:807)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:206)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:174)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:175)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:263)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:844)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:584)\n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:447)\n        at java.lang.Thread.run(Thread.java:619)\n\n\n\n... that is probably caused by the following statements around line 81 in GlobalCollectionStatComponent.prepare. We only enter the if statement if terms is null, and then we dereference it...\n\n    String terms = req.getParams().get(ResponseBuilder.DOCFREQS);\n    if (numDocs != null && terms == null) {\n      // the build query has to be over-written to take into\n      //account global numDocs and docFreqs\n\n      //extract the numDocs and docFreqs from request params\n      Map<Term, Integer> dfMap = new HashMap<Term, Integer>();\n      String[] strTerms = terms.split(\",\");\n\n "
        },
        {
            "author": "Sabyasachi Dalal",
            "id": "comment-12543551",
            "date": "2007-11-19T13:08:19+0000",
            "content": "I have updated the patch to remove the code pertaining to SOLR-281, because 281 has been committed. "
        },
        {
            "author": "Sabyasachi Dalal",
            "id": "comment-12543553",
            "date": "2007-11-19T13:10:46+0000",
            "content": "I mean i removed the files pertaining to 281. If you follow the development above, the files pertaining to 281 were added to this patch to make it easier to apply this patch. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12543774",
            "date": "2007-11-20T04:57:24+0000",
            "content": "I'm really just starting to dig into this again, but here are a couple of thoughts:\n\nIt looks like there is a monolithic main federated query component that does all the work... It would be nice if there were a way to turn this around so that a user could write a query component that could participate in a distributed search call.  It seems like query info should be able to be gathered from multiple components and then a single request to a shard could be made.  This entails multiple methods on QueryComponent for use in a distributed request.\n\nAnother observation is that the number of \"phases\" may be unpredictable.  For example when faceting, if one wants \"exact\" results, more information may be required from certain nodes.  This means that components need a way to say if they are done or not, and a way to send different requests to different shards.  Then when responses are received, it should be possible to optionally handle them one-by-one as they come in, or alternately all at once to merge the results.\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12543931",
            "date": "2007-11-20T16:16:19+0000",
            "content": "Note: there has been discussion recently about the terminology distinction between \"federated search\" and \"distributed search\" (which ken recently updated on the wiki) ... this issue is tracking \"distributed search\" and not \"federated search\" correct?\n\nif so, the issue summary should be updated\n\nhttp://wiki.apache.org/solr/FederatedSearch\nhttp://wiki.apache.org/solr/DistributedSearch\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12543936",
            "date": "2007-11-20T16:31:10+0000",
            "content": "Original description by Sharad, moved to this comment because a JIRA \"Description\" is sent to the email list every time there is an update to the issue.\n\n\nMotivated by http://wiki.apache.org/solr/DistributedSearch\n\"Index view consistency between multiple requests\" requirement is relaxed in this implementation.\n\nDoes the federated search query side. Update not yet done.\n\nTries to achieve:-\n------------------------\n\n\tThe client applications are totally agnostic to federated search. The federated search and merging of results are totally behind the scene in Solr in request handler . Response format remains the same after merging of results.\nThe response from individual shard is deserialized into SolrQueryResponse object. The collection of SolrQueryResponse objects are merged to produce a single SolrQueryResponse object. This enables to use the Response writers as it is; or with minimal change.\n\n\n\n\n\tEfficient query processing with highlighting and fields getting generated only for merged documents. The query is executed in 2 phases. First phase gets the doc unique keys with sort criteria. Second phase brings all requested fields and highlighting information. This saves lot of CPU in case there are good number of shards and highlighting info is requested.\nShould be easy to customize the query execution. For example: user can specify to execute query in just 1 phase itself. (For some queries when highlighting info is not required and number of fields requested are small; this can be more efficient.)\n\n\n\n\n\tAbility to easily overwrite the default Federated capability by appropriate plugins and request parameters. As federated search is performed by the RequestHandler itself, multiple request handlers can easily be pre-configured with different federated search settings in solrconfig.xml\n\n\n\n\n\tGlobal weight calculation is done by querying the terms' doc frequencies from all shards.\n\n\n\n\n\tFederated search works on Http transport. So individual shard's VIP can be queried. Load-balancing and Fail-over taken care by VIP as usual.\n\n\n\n-Sub-searcher response parsing as a plugin interface. Different implementation could be written based on JSON, xml SAX etc. Current one based on XML DOM.\n\n\nHOW:\n-------\nA new RequestHandler called MultiSearchRequestHandler does the federated search on multiple sub-searchers, (referred as \"shards\" going forward). It extends the RequestHandlerBase. handleRequestBody method in RequestHandlerBase has been divided into query building and execute methods. This has been done to calculate global numDocs and docFreqs; and execute the query efficiently on multiple shards.\nAll the \"search\" request handlers are expected to extend MultiSearchRequestHandler class in order to enable federated capability for the handler. StandardRequestHandler and DisMaxRequestHandler have been changed to extend this class.\n\nThe federated search kicks in if \"shards\" is present in the request parameter. Otherwise search is performed as usual on the local index. eg. shards=local,host1:port1,host2:port2 will search on the local index and 2 remote indexes. The search response from all 3 shards are merged and serviced back to the client. \n\nThe search request processing on the set of shards is performed as follows:\n\nSTEP 1: The query is built, terms are extracted. Global numDocs and docFreqs are calculated by requesting all the shards and adding up numDocs and docFreqs from each shard.\n\nSTEP 2: (FirstQueryPhase) All shards are queried. Global numDocs and docFreqs are passed as request parameters. All document fields are NOT requested, only document uniqFields and sort fields are requested. MoreLikeThis and Highlighting information are NOT requested.\n\nSTEP 3: Responses from FirstQueryPhase are merged based on \"sort\", \"start\" and \"rows\" params. Merged doc uniqField and sort fields are collected. Other information like facet and debug is also merged.\n\nSTEP 4: (SecondQueryPhase) Merged doc uniqFields and sort fields are grouped based on shards. All shards in the grouping are queried for the merged doc uniqFields (from FirstQueryPhase), highlighting and moreLikeThis info.\n\nSTEP 5: Responses from all shards from SecondQueryPhase are merged.\n\nSTEP 6: Document fields , highlighting and moreLikeThis info from SecondQueryPhase are merged into FirstQueryPhase response.\n\n\n\n\nTODO:\n-Support sort field other than default score\n-Support ResponseDocs in writers other than XMLWriter\n-Http connection timeouts\n\nOPEN ISSUES;\n-Merging of facets by \"top n terms of field f\" \n\nScope for Performance optimization:-\n-Search shards in parallel threads\n-Http connection Keep-Alive ?\n-Cache global numDocs and docFreqs\n-Cache Query objects in handlers ??\n\nWould appreciate feedback on my approach. I understand that there would be lot things I might have over-looked.  "
        },
        {
            "author": "zhang.zuxin",
            "id": "comment-12544684",
            "date": "2007-11-22T03:21:56+0000",
            "content": "to Sabyasachi Dalal:\nI update solr trunk to version 597284. And I patch it cleanly.But it does't work,just like it doesn't support distributed search.\nAlternately,it works when I used Sharad Agarwal 's patch.I don't know what's wrong, or maybe you change anything? "
        },
        {
            "author": "Sabyasachi Dalal",
            "id": "comment-12545143",
            "date": "2007-11-24T02:00:02+0000",
            "content": "Can you please some more details about the error ? Are you seeing\nany exceptions ? How are your partitions set up and what is the\nrequest you are sending ? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12545154",
            "date": "2007-11-24T04:16:07+0000",
            "content": "It doesn't seem like there is any request handler set up that references the distributed search components. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12548032",
            "date": "2007-12-03T22:58:02+0000",
            "content": "I've been prototyping distributed search in python...\nThe current methods I have for a component are something like\n\n\n  // returns the current stage this component is at... stage starts at -1 and the next stage is the minimum returned\n  // by all components on the previous calls to process()\n  int process(RequestBuilder rb, int stage);\n\n   // callback for a single response received (optional... this could be left out)\n   // all components have this called, regardless of who queued the request\n   void singleResponse(ResponseBuilder rb, int stage, Request req, Response rsp);\n\n   // callback when all responses (from all shards) to a request have been received\n   void allResponses(ResponseBuilder rb, int stage, Request req);\n\n\n\nAny of these methods can add another request to the outgoing queue.  The current stage is only over after all\nrequests have been sent, responses received, and the outgoing queue is empty.\nWhen all components return maxint from process(), we are done. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12548044",
            "date": "2007-12-03T23:22:04+0000",
            "content": "Are you suggesting changing the main control loop from:\n\n      for( SearchComponent c : components ) {\n        c.process( req, rsp );\n      }\n\n\n\nto something that knows \"stages\"? \n\nOr are you discussing something that would happen within a single 'c.process( req, rsp );? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12549573",
            "date": "2007-12-07T22:04:05+0000",
            "content": "Yes, I'm suggesting changing the main control loop.\nNormal non-distributed requests don't necessarily need stages (but could be added to be more consistent with the distributed methods... with stages, I don't think there would be a \"prepare\" method).\nRight now, my private copy of SearchComponent looks like\n\npublic abstract class SearchComponent implements SolrInfoMBean\n{\n  public abstract void prepare( SolrQueryRequest req, SolrQueryResponse rsp ) throws IOException, ParseException;\n  public abstract void process( SolrQueryRequest req, SolrQueryResponse rsp ) throws IOException;\n\n  public int distributedProcess(ResponseBuilder rb) throws IOException {\n    return ResponseBuilder.STAGE_END;\n  }\n\n  public void handleResponses(ResponseBuilder rb, ShardRequest sreq) {\n  }\n\n "
        },
        {
            "author": "Sabyasachi Dalal",
            "id": "comment-12549970",
            "date": "2007-12-10T08:54:35+0000",
            "content": "I fixed the issue with the patch and it works with version 594268. \nNow, i am trying to make it work with the latest trunk.  I am facing a problem. The  FedSearchComponent needs a handle to the \"handler\" in order to execute on the local shard. I am trying to figure out how to pass the handler during component initialization. "
        },
        {
            "author": "Sabyasachi Dalal",
            "id": "comment-12550896",
            "date": "2007-12-12T10:03:48+0000",
            "content": "I made a mistake and uploaded the wrong patch file. Now uploading the correct file.\n\nI have fixed and updated the patch with trunk version 600419. It is integrated with the re-opened SOLR-281 patch.\nI have added the configuration for the three distributed-search components in the solrconfig.xml, under \"/search\" request handler. So, the distributed search works with /search request only.\n\nCouple of issues :\n1. The dist search components need the reference to the SearchHandler. So for now , i have hard coded the \"/search\" pattern in the FedSearchComponent.\n2. Need a clean way to load common init params for the dist search components, such as timeout, thread pool size and search handler pattern. "
        },
        {
            "author": "Sabyasachi Dalal",
            "id": "comment-12551225",
            "date": "2007-12-13T06:02:15+0000",
            "content": "Removed the commented line from SolrCore.loadSearchComponents and couple of debug statements. "
        },
        {
            "author": "Gereon Steffens",
            "id": "comment-12551457",
            "date": "2007-12-13T10:20:29+0000",
            "content": "I started experimenting with this patch and have a couple of issues.\n\nFirst, the patch did not apply cleanly to the latest trunk (603869), so I reverted to 600419 - no big deal.\n\nI then set up two separate tomcat/solr instances using identical schemas (on ports 8080 and 8090) and tried querying both using solr/search requests and can't any of my queries to work.\n\nFor example, there is a document with field \"id\" = 1527426 in the database on port 8090. \"id\" is defined as a \"sint\" field. The 8080 instance has no such id.\nWhen querying \"http://localhost/8080/solr/search?q=id:1527426&shards=local,localhost:8090/solr\", I get the following in the tomcat logs:\n\n\ncatalina.out on the 8080 instance:\n\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.SimpleOrderedMap\nDec 13, 2007 10:55:04 AM org.apache.solr.handler.component.ResponseBuilder <init>\nINFO: ### *** shards len 2\nDec 13, 2007 10:55:04 AM org.apache.solr.handler.federated.component.GlobalCollectionStatComponent extractTerms\nINFO: --------Extract terms starting----------- :\nDec 13, 2007 10:55:04 AM org.apache.solr.handler.federated.component.GlobalCollectionStatComponent extractTerms\nINFO: ### *** is shards null false\nDec 13, 2007 10:55:04 AM org.apache.solr.handler.federated.component.GlobalCollectionStatComponent extractTerms\nINFO: ### *** SHARDS len 2\nDec 13, 2007 10:55:04 AM org.apache.solr.handler.federated.XMLResponseParser parse\nINFO: ->Request http://localhost:8090/solr/select?q=id%3A1527426&shards=local%2Clocalhost%3A8090%2Fsolr&eqt=true&\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.NamedList\nDec 13, 2007 10:55:04 AM org.apache.solr.handler.federated.component.GlobalCollectionStatComponent execute\nWARNING: Exception while querying shard localhost:8090/solr :java.lang.NullPointerException\nDec 13, 2007 10:55:04 AM org.apache.solr.handler.federated.component.GlobalCollectionStatComponent calcuateGlobalCollectionStat\nINFO: --------getGlobalCollectionStat starting----------- :\nDec 13, 2007 10:55:04 AM org.apache.solr.handler.federated.XMLResponseParser parse\nINFO: ->Request http://localhost:8090/solr/federated/collectionstats?terms=id%3A%C2%80%C5%B4%E0%BA%82%2C&\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.NamedList\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values nd : java.lang.Integer\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values tdf : org.apache.solr.common.util.NamedList\nDec 13, 2007 10:55:04 AM org.apache.solr.handler.federated.component.MainQPhaseComponent process\nINFO: --------MainQPhaseComponent starting----------- :\nDec 13, 2007 10:55:04 AM org.apache.solr.handler.federated.component.FedSearchComponent executeOnLocal\nINFO: ->Local request params: {fl=id,score,,q=id:1527426,nd=74621,tdf=id:\u0174\u0e82@1,,fsv=true}\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.SimpleOrderedMap\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values response : org.apache.solr.search.DocSlice\nDec 13, 2007 10:55:04 AM org.apache.solr.core.SolrCore execute\nINFO: null nd=74621&fsv=true&tdf=id:\u0174\u0e82@1,&q=id:1527426&fl=id,score, 0 1\nDec 13, 2007 10:55:04 AM org.apache.solr.handler.federated.XMLResponseParser parse\nINFO: ->Request http://localhost:8090/solr/select?fl=id%2Cscore%2C&q=id%3A1527426&nd=74621&tdf=id%3A%C2%80%C5%B4%E0%BA%82%401%2C&fsv=true&\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.NamedList\nDec 13, 2007 10:55:04 AM org.apache.solr.handler.federated.component.MainQPhaseComponent process\nWARNING: Exception while querying shard localhost:8090/solr :java.lang.ClassCastException: java.lang.Integer\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values response : org.apache.solr.handler.federated.ResponseDocs\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.NamedList\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values response : org.apache.solr.handler.federated.ResponseDocs\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.NamedList\nDec 13, 2007 10:55:04 AM org.apache.solr.handler.federated.component.AuxiliaryQPhaseComponent process\nINFO: --------AuxiliaryQPhaseComponent starting----------- :\nDec 13, 2007 10:55:04 AM org.apache.solr.core.SolrCore execute\nINFO: /search q=id:1527426&shards=local,localhost:8090/solr 0 60\n\n\n\n\ncatalina.out on the 8090 instance\n\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.SimpleOrderedMap\nDec 13, 2007 10:55:04 AM org.apache.solr.handler.component.ResponseBuilder <init>\nINFO: ### *** shards len 2\nDec 13, 2007 10:55:04 AM org.apache.solr.core.SolrCore execute\nINFO: /select q=id:1527426&eqt=true&shards=local,localhost:8090/solr 0 3\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.SimpleOrderedMap\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values nd : java.lang.Integer\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values tdf : org.apache.solr.common.util.NamedList\nDec 13, 2007 10:55:04 AM org.apache.solr.core.SolrCore execute\nINFO: /federated/collectionstats terms=id:\u0174\u0e82, 0 3\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.SimpleOrderedMap\nDec 13, 2007 10:55:04 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values response : org.apache.solr.search.DocSlice\nDec 13, 2007 10:55:04 AM org.apache.solr.core.SolrCore execute\nINFO: /select nd=74621&fsv=true&fl=id,score,&q=id:1527426&tdf=id:\u0174\u0e82@1, 0 1\n\n\n\nSo the request does reach the 8090 instance, but triggers a CastException on the 8080 instance. The XML output is\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response>\n<lst name=\"responseHeader\">\n  <int name=\"status\">0</int>\n  <int name=\"QTime\">135</int>\n  <lst name=\"params\">\n    <str name=\"q\">id:1527426</str>\n    <str name=\"shards\">local,localhost:8090/solr</str>\n  </lst>\n</lst>\n<result name=\"response\" numFound=\"0\" start=\"0\"/>\n  <lst name=\"responseHeader\">\n    <lst name=\"local\">\n      <int name=\"status\">0</int>\n      <int name=\"QTime\">4</int>\n      <lst name=\"params\">\n      <str name=\"nd\">74621</str>\n      <str name=\"fsv\">true</str>\n      <str name=\"tdf\">id:\u0080\u0174\u0e82@1,</str>\n      <str name=\"q\">id:1527426</str>\n      <str name=\"fl\">id,score,</str>\n    </lst>\n  </lst>\n</lst>\n</response>\n\n\n\nThe \"reverse\" request for \"http://localhost:8090/solr/search?q=id:1527426&shards=local,localhost:8080/solr\" produces an HTTP Status 500 - null java.lang.NullPointerException response, the logs are:\n\n\ncatalina.out on the 8080 instance\n\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.SimpleOrderedMap\nDec 13, 2007 11:07:33 AM org.apache.solr.handler.component.ResponseBuilder <init>\nINFO: ### *** shards len 2\nDec 13, 2007 11:07:33 AM org.apache.solr.core.SolrCore execute\nINFO: /select q=id:1527426&eqt=true&shards=local,localhost:8080/solr 0 2\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.SimpleOrderedMap\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values nd : java.lang.Integer\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values tdf : org.apache.solr.common.util.NamedList\nDec 13, 2007 11:07:33 AM org.apache.solr.core.SolrCore execute\nINFO: /federated/collectionstats terms=id:\u0174\u0e82, 0 5\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.SimpleOrderedMap\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values response : org.apache.solr.search.DocSlice\nDec 13, 2007 11:07:33 AM org.apache.solr.core.SolrCore execute\nINFO: /select nd=74621&fsv=true&fl=id,score,&q=id:1527426&tdf=id:\u0174\u0e82@1, 0 1\n\n\n\n\ncatalina.out on the 8090 instance\n\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.SimpleOrderedMap\nDec 13, 2007 11:07:33 AM org.apache.solr.handler.component.ResponseBuilder <init>\nINFO: ### *** shards len 2\nDec 13, 2007 11:07:33 AM org.apache.solr.handler.federated.component.GlobalCollectionStatComponent extractTerms\nINFO: --------Extract terms starting----------- :\nDec 13, 2007 11:07:33 AM org.apache.solr.handler.federated.component.GlobalCollectionStatComponent extractTerms\nINFO: ### *** is shards null false\nDec 13, 2007 11:07:33 AM org.apache.solr.handler.federated.component.GlobalCollectionStatComponent extractTerms\nINFO: ### *** SHARDS len 2\nDec 13, 2007 11:07:33 AM org.apache.solr.handler.federated.XMLResponseParser parse\nINFO: ->Request http://localhost:8080/solr/select?q=id%3A1527426&shards=local%2Clocalhost%3A8080%2Fsolr&eqt=true&\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.NamedList\nDec 13, 2007 11:07:33 AM org.apache.solr.handler.federated.component.GlobalCollectionStatComponent execute\nWARNING: Exception while querying shard localhost:8080/solr :java.lang.NullPointerException\nDec 13, 2007 11:07:33 AM org.apache.solr.handler.federated.component.GlobalCollectionStatComponent calcuateGlobalCollectionStat\nINFO: --------getGlobalCollectionStat starting----------- :\nDec 13, 2007 11:07:33 AM org.apache.solr.handler.federated.XMLResponseParser parse\nINFO: ->Request http://localhost:8080/solr/federated/collectionstats?terms=id%3A%C2%80%C5%B4%E0%BA%82%2C&\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.NamedList\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values nd : java.lang.Integer\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values tdf : org.apache.solr.common.util.NamedList\nDec 13, 2007 11:07:33 AM org.apache.solr.handler.federated.component.MainQPhaseComponent process\nINFO: --------MainQPhaseComponent starting----------- :\nDec 13, 2007 11:07:33 AM org.apache.solr.handler.federated.XMLResponseParser parse\nINFO: ->Request http://localhost:8080/solr/select?fl=id%2Cscore%2C&q=id%3A1527426&nd=74621&tdf=id%3A%C2%80%C5%B4%E0%BA%82%401%2C&fsv=true&\nDec 13, 2007 11:07:33 AM org.apache.solr.handler.federated.component.FedSearchComponent executeOnLocal\nINFO: ->Local request params: {fl=id,score,,q=id:1527426,nd=74621,tdf=id:\u0174\u0e82@1,,fsv=true}\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.SimpleOrderedMap\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values response : org.apache.solr.search.DocSlice\nDec 13, 2007 11:07:33 AM org.apache.solr.core.SolrCore execute\nINFO: null nd=74621&fsv=true&tdf=id:\u0174\u0e82@1,&q=id:1527426&fl=id,score, 0 4\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.NamedList\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values response : org.apache.solr.handler.federated.ResponseDocs\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values response : org.apache.solr.handler.federated.ResponseDocs\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.NamedList\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values response : org.apache.solr.handler.federated.ResponseDocs\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.NamedList\nDec 13, 2007 11:07:33 AM org.apache.solr.handler.federated.component.AuxiliaryQPhaseComponent process\nINFO: --------AuxiliaryQPhaseComponent starting----------- :\nDec 13, 2007 11:07:33 AM org.apache.solr.handler.federated.component.FedSearchComponent executeOnLocal\nINFO: ->Local request params: {dq=id:\"\u0174\u0e82\" ,q=id:1527426}\nDec 13, 2007 11:07:33 AM org.apache.solr.request.SolrQueryResponse add\nINFO: adding into values responseHeader : org.apache.solr.common.util.SimpleOrderedMap\nDec 13, 2007 11:07:33 AM org.apache.solr.common.SolrException log\nSEVERE: java.lang.NumberFormatException: For input string: \"\u0174\u0e82\"\n        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)\n        at java.lang.Integer.parseInt(Integer.java:447)\n        at java.lang.Integer.parseInt(Integer.java:497)\n        at org.apache.solr.util.NumberUtils.int2sortableStr(NumberUtils.java:36)\n        at org.apache.solr.schema.SortableIntField.toInternal(SortableIntField.java:52)\n        at org.apache.solr.schema.FieldType$DefaultAnalyzer$1.next(FieldType.java:315)\n        at org.apache.lucene.queryParser.QueryParser.getFieldQuery(QueryParser.java:437)\n        at org.apache.solr.search.SolrQueryParser.getFieldQuery(SolrQueryParser.java:97)\n        at org.apache.lucene.queryParser.QueryParser.getFieldQuery(QueryParser.java:515)\n        at org.apache.lucene.queryParser.QueryParser.Term(QueryParser.java:1227)\n        at org.apache.lucene.queryParser.QueryParser.Clause(QueryParser.java:979)\n        at org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:907)\n        at org.apache.lucene.queryParser.QueryParser.TopLevelQuery(QueryParser.java:896)\n        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:146)\n        at org.apache.solr.search.QueryParsing.parseQuery(QueryParsing.java:101)\n        at org.apache.solr.handler.federated.component.AuxiliaryQPhaseComponent.prepare(AuxiliaryQPhaseComponent.java:71)\n        at org.apache.solr.handler.SearchHandler.handleRequestBody(SearchHandler.java:152)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:117)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:866)\n        at org.apache.solr.handler.federated.component.FedSearchComponent.executeOnLocal(FedSearchComponent.java:87)\n        at org.apache.solr.handler.federated.component.AuxiliaryQPhaseComponent$1.call(AuxiliaryQPhaseComponent.java:115)\n        at org.apache.solr.handler.federated.component.AuxiliaryQPhaseComponent$1.call(AuxiliaryQPhaseComponent.java:114)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:269)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:123)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:417)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:269)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:123)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:65)\n        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:168)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:650)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:675)\n        at java.lang.Thread.run(Thread.java:595)\n\nDec 13, 2007 11:07:33 AM org.apache.solr.core.SolrCore execute\nINFO: null q=id:1527426&dq=id:\"\u0174\u0e82\"+ 0 2\nDec 13, 2007 11:07:33 AM org.apache.solr.common.SolrException log\nSEVERE: java.lang.NullPointerException\n        at org.apache.solr.handler.federated.SearchResponseMerger.mergeResponseDocs_NoSort(SearchResponseMerger.java:215)\n        at org.apache.solr.handler.federated.SearchResponseMerger.merge(SearchResponseMerger.java:83)\n        at org.apache.solr.handler.federated.component.AuxiliaryQPhaseComponent.process(AuxiliaryQPhaseComponent.java:156)\n        at org.apache.solr.handler.SearchHandler.handleRequestBody(SearchHandler.java:158)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:117)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:866)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:206)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:174)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:210)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:174)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:117)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:108)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:151)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:870)\n        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection(Http11BaseProtocol.java:665)\n        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:528)\n        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:81)\n        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:685)\n        at java.lang.Thread.run(Thread.java:595)\n\nDec 13, 2007 11:07:33 AM org.apache.solr.core.SolrCore execute\nINFO: /search q=id:1527426&shards=local,localhost:8080/solr 0 95\nDec 13, 2007 11:07:33 AM org.apache.solr.common.SolrException log\nSEVERE: java.lang.NullPointerException\n        at org.apache.solr.handler.federated.SearchResponseMerger.mergeResponseDocs_NoSort(SearchResponseMerger.java:215)\n        at org.apache.solr.handler.federated.SearchResponseMerger.merge(SearchResponseMerger.java:83)\n        at org.apache.solr.handler.federated.component.AuxiliaryQPhaseComponent.process(AuxiliaryQPhaseComponent.java:156)\n        at org.apache.solr.handler.SearchHandler.handleRequestBody(SearchHandler.java:158)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:117)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:866)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:206)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:174)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:215)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:188)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:210)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:174)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:117)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:108)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:151)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:870)\n        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection(Http11BaseProtocol.java:665)\n        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:528)\n        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:81)\n        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:685)\n        at java.lang.Thread.run(Thread.java:595)\n\n\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12551514",
            "date": "2007-12-13T14:22:20+0000",
            "content": "Bear with me... I'm working on this from a bit of a different angle.\n\n\tmultiple stages, defined by components themselves, and a stage doesn't end until an outgoing request queue is empty.\n\tmaking components responsible for turning on/off their own options in the query phases, rather than having the distributed search component have to know all the different options.\n\tusing SolrJ/HttpClient for communication\n\torganizational: moved SearchHandler into the component package, along with distributed search stuff.  It's all related and allows us to keep things private that should be kept private.\n\n\n\nI understand the original author is no longer involved with this issue, so I'm basing things on his code in some places, but not others.  Hopefully I'll have something  "
        },
        {
            "author": "Stu Hood",
            "id": "comment-12553838",
            "date": "2007-12-21T01:08:45+0000",
            "content": "I recognize the advantage of the AuxiliaryQPhase, but I'm not quite sure about GlobalCollectionStat. Is its purpose just to normalize weights from the shards?\n\nI had to make some changes to the MainQPhase parameter building, and to the PriorityQueue that SearchResponseMerger uses to get sorting working properly. Yonik, if you aren't planning on re-writing those from scratch, would you prefer a patch, or an explanation of what I needed to change? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12553841",
            "date": "2007-12-21T01:42:35+0000",
            "content": "I'm not quite sure about GlobalCollectionStat. Is its purpose just to normalize weights from the shards?\n\nIt's to make a distributed search score the same as it would if everything was in a single index.\nidf (inverse document frequency) is part of the scoring, so that component essentially does a distributed idf.\n\nI still use the PriorityQueue, but it's been modified since SolrJ returns objects rather than strings.\nI'll try to post a draft soon... if you understood the old code, it will be great for you to look at the new stuff to see what I'm missing. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12554145",
            "date": "2007-12-22T19:07:53+0000",
            "content": "OK, here is a draft that mostly works for searches and highlighting.\n\nThere are stages in the request:\n\n  public static int STAGE_START           = 0;\n  public static int STAGE_PARSE_QUERY     = 1000;\n  public static int STAGE_EXECUTE_QUERY   = 2000;\n  public static int STAGE_GET_FIELDS      = 3000;\n  public static int STAGE_DONE            = Integer.MAX_VALUE;\n\n\n\nWhen a component wants to send a request, it adds it to \"outgoing\" queue.\nOther components can inspect and modify these shard requests.\nAll components get a callback when the shard response is received.\n\nAll shard responses purposes (to aid in both correlation and inspection/modification by other components).\nThis is what a ShardRequest looks like:\n\npublic class ShardRequest {\n  public final static String[] ALL_SHARDS = null;\n\n  public final static int PURPOSE_PRIVATE         = 0x01;\n  public final static int PURPOSE_GET_TERM_DFS    = 0x02;\n  public final static int PURPOSE_GET_TOP_IDS     = 0x04;\n  public final static int PURPOSE_REFINE_TOP_IDS  = 0x08;\n  public final static int PURPOSE_GET_FACETS      = 0x10;\n  public final static int PURPOSE_REFINE_FACETS   = 0x20;\n  public final static int PURPOSE_GET_FIELDS      = 0x40;\n  public final static int PURPOSE_GET_HIGHLIGHTS  = 0x80;\n\n  public int purpose;  // the purpose of this request\n\n  public String[] shards;  // the shards this request should be sent to\n// TODO: how to request a specific shard address?\n\n  public ModifiableSolrParams params;\n\n  public List<ShardResponse> responses = new ArrayList<ShardResponse>();\n}\n\n\n\n\nComponents are responsible for themselves... the highlighting component is responsible for turning itself on/off at the appropriate time... the query component has no knowledge of the highlight component.  This will make it so that custom components can be developed that can work in a distributed environment w/o explicit support for that component baked into the other components.\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12554322",
            "date": "2007-12-25T04:40:54+0000",
            "content": "attaching updated patch (distributed.patch) that fixes some sorting issues. "
        },
        {
            "author": "Stu Hood",
            "id": "comment-12554446",
            "date": "2007-12-26T18:19:24+0000",
            "content": "Thanks for the new patch Yonik! It doesn't apply cleanly because of the way you generated the test files, but after those have been removed, it looks good. It seems you figured out the sorting issue that I had mentioned: thanks. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12554513",
            "date": "2007-12-27T03:57:50+0000",
            "content": "I just took a quick look...  a few observations:\n\nWe should extract out a few simple things and commit them quickly to make this go more smoothly:\n\n\tmove SearchHandler to o.a.s.handler.component \u2013 I vote you go ahead and commit that change.\n\tCreate a separate issue for adding SolrDocument to XMLWriter\n\tMove solrj into the main source tree.  I'm not sure the best way to do this, but I don't think solrj should sit in its own source folder if the core depends on it.\n\n\n\n\nIs there a good reason to use the same handler for distributed search?  Why not have a DistributedSearchHandler that extends SearchHandler and skip the if {} else {} checking?  Likewise, I wonder if a DistributedResponseBuilder could/should extend ResponseBuilding and add the necessary logic.\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12554519",
            "date": "2007-12-27T04:43:53+0000",
            "content": "{quote}}We should extract out a few simple things and commit them quickly to make this go more smoothly:\n\n   1. move SearchHandler to o.a.s.handler.component - I vote you go ahead and commit that change.\n   2. Create a separate issue for adding SolrDocument to XMLWriter\n   3. Move solrj into the main source tree. I'm not sure the best way to do this, but I don't think solrj should sit in its own source folder if the core depends on it.\n\n\nDefinitely agree on #1 and #2.\nFor #3, are there SolrJ parts (or future parts) that we wouldn't want automatically bundled with Solr?\nIs there a good reason to use the same handler for distributed search?\n\nIt seems like a single search component should be able to handle distributed search.\nIf that's the case, what separates a handler that is distributed and one that isn't?\nThe first thing that occured to me was to just detect the presence of shards[] after the prepare phase.\nThere is a side benefit in that a component can control whether a request is distributed or not (all solrconfig could be the same for systems in a cluster, with some sort of external system controlling topology). \n\nOne could have a distributed handler that could delegate or handle non-distributed requests, but it seems to amount to the same thing (a single handler that can do both on the fly).\n\nSaving an if() doesn't seem too compelling (the current code could certainly be refactored to be cleaner anyway).  Are there other benefits to having a separate DistributedSearchHandler though?\n.\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12554524",
            "date": "2007-12-27T05:31:27+0000",
            "content": "\n\n\nFor #3, are there SolrJ parts (or future parts) that we wouldn't want automatically bundled with Solr?\n\n\n\nI don't think so.  The thing I want to make sure is still possible is that solrj can be distributed independently (without the lucene dependencies)\n\nThe existing artifact topology makes sense as is: common, solrj, core.  \n\nCurrently we have:\n\n+ common\n  + solrj\n  + core\n\n\nwe need\n\n+ common\n  + solrj  \n      +core\n\n\nor\n\n+ common & solrj  \n  + core\n\n\n\nThis issue is essentially independent of SOLR-303, but we should try to make our source directory structures consistent with standard practice. \n\n\n\nSaving an if() doesn't seem too compelling (the current code could certainly be refactored to be cleaner anyway). Are there other benefits to having a separate DistributedSearchHandler though?\n\n\nIf there is a good reason to keep it the same handler then that is a reason enough.  \n\nI just looked at it  (without really grocking how it works) and it seemed a bit bloated with distribution lifecycle stuff.  As long as the non-distributed request cycle isn't tied to the distributed stuff, I'm sure it is fine.\n\n\n\nBTW, where does the term \"shard\" come from?  What specifically does it refer to? "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12554534",
            "date": "2007-12-27T06:38:55+0000",
            "content": "Shard is what you call a small(er) index that is a part of a large(r) cluster of indices.  These smaller shards together form one large logical index.\n\nSee http://www.scribd.com/doc/312186/THE-GOOGLE-CLUSTER-ARCHITECTURE\n\nI wish Nutch used the same (shard) nomenclature instead of using \"segments\", so there is no confusion with Lucene index segments.... but that's another issue. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12555711",
            "date": "2008-01-03T23:21:41+0000",
            "content": "Small update, mostly to sorting\n\n\tThis changes sorting to get values from the Sort comparators (thus supporting custom sorts)\n\tuses external values that can be supported by XML, also nicer for debugging\n\treturns sort field values in an array per-field \n{price=[10,20,30,40,50]}\n\tmerging should be faster... lookup of sort values is by index number instead of searching\n  for the field name.\n\tmerging short-circuits comparisons for docs in the same shard\n\tsorting null values now works & respects sortMissingFirst/Last, etc\n\tif a shard request, don't pre-fetch docs for highlighter\n\n "
        },
        {
            "author": "Gereon Steffens",
            "id": "comment-12556575",
            "date": "2008-01-07T14:52:48+0000",
            "content": "Yonik, no matter what I try, I keep getting exceptions when querying anything that uses shards. \nIs the correct query URL still what I've used in my previous comment?\n\nExcerpt from my logs:\n\n\nSEVERE: org.apache.solr.client.solrj.SolrServerException: Error executing query\n        at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:86)\n[...]\nCaused by: org.apache.solr.common.SolrException: /select\n\n/select\n\nrequest: http://localhost:8090/select?echoParams=explicit&q=id:1527426&start=0&rows=10&fsv=true&fl=id,score&isShard=true&wt=xml&version=2.2\n        at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:243)\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12556612",
            "date": "2008-01-07T16:26:15+0000",
            "content": "There is currently no \"local\" shard... is that causing your problem?\nUse something like shards=localhost:8983/solr,localhost:8080/solr "
        },
        {
            "author": "patrick o'leary",
            "id": "comment-12556644",
            "date": "2008-01-07T18:07:17+0000",
            "content": "Hey Yonik\n\nAre you applying the federated search patch first before the distributed search?\nThe patch itself won't apply cleanly against trunk \n\nThanks\nP "
        },
        {
            "author": "Gereon Steffens",
            "id": "comment-12556881",
            "date": "2008-01-08T12:34:12+0000",
            "content": "Yonik - thanks, that's what caused it.\n\nPatrick - as far as I can tell, you can ignore the error messages from patch. "
        },
        {
            "author": "patrick o'leary",
            "id": "comment-12556949",
            "date": "2008-01-08T17:14:08+0000",
            "content": "This might help, merged the distributed & federated patchs with trunk last night, fixed the rejects. Appears to work.\nThe only things not included are the distributed searcher unit tests from the previous patch. Only the deltas were in the patch, so I had no way to rebuild them.\n\nHope this helps\nP "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12556950",
            "date": "2008-01-08T17:23:09+0000",
            "content": "I'm in the middle of implementing some distributed faceting... but I'll try to get a better patch the next time around.\nI think some of Ryan's suggestions are good (a separate patch to move SearchHandler, put solrj in core, implement ResponseWriter support for SolrJ objects).\n "
        },
        {
            "author": "patrick o'leary",
            "id": "comment-12557012",
            "date": "2008-01-08T20:03:33+0000",
            "content": "Was missing a file from an svn add, so the patch I put in there misses out on SolrFieldSortedHitQueue\nI'll remove it to reduce confusion. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12557017",
            "date": "2008-01-08T20:19:59+0000",
            "content": "yonik, if you say \"go\", I'll add SOLR-446 "
        },
        {
            "author": "patrick o'leary",
            "id": "comment-12557324",
            "date": "2008-01-09T16:57:49+0000",
            "content": "Small thing but if you update org.apache.solr.handler.component.ResponseBuilder\nand set the stages to final, you can use a switch statement in the distributedProcess phase.\n\n\npublic class ResponseBuilder \n{\n  public static final int STAGE_START           = 0;\n  public static final int STAGE_PARSE_QUERY     = 1000;\n  public static final int STAGE_EXECUTE_QUERY   = 2000;\n  public static final int STAGE_GET_FIELDS      = 3000;\n  public static final int STAGE_DONE            = Integer.MAX_VALUE;\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12557332",
            "date": "2008-01-09T17:15:39+0000",
            "content": "WRT a switch, I left room for other components to insert stages between the well defined ones.\nI'm not sure if this will be useful in the future or not.  Much of that seems like it would depend on the contracts between the components and the ResponseBuilder, and thus how other unknown custom coponents would be able to change things. That's still very immature, as I've really just been focusing on getting things working. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12557522",
            "date": "2008-01-10T02:39:14+0000",
            "content": "OK, this version patches cleanly and includes some distributed faceting code.\n\n\tfacet.query and facet.field sorted by count is mostly handled\n\tbreaking ties by natural (index) sort order is not yet implemented\n\tdate faceting and unsorted (index order) facet.field is not implemented\n\n\n\nAssuming the user asks for the top 10 terms of a field:\n1) The first facet queries piggyback on the queries to get the top ids and sort field values.\n2) counts are merged, and new \"refinement\" requests are send out for those terms in the top 10 where a count was not received from some shards.  Also, for terms below the top 10, we calculate the maximum it could have based on shards we have not heard from, and if that boosts it into the top 10, we include that term for \"refinement\".\n3) refinement responses are used to adjust the counts, and we are done.\n\nNote that it is theoretically possible to miss terms.  A term could be just below the threshold of each shard (and thus not returned by any shard), but the total count could boost it in the top.  This could be rectified by retrieving all terms above a specified count, but it could be expensive.  The counts that are currently returned are exact.\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12557525",
            "date": "2008-01-10T02:42:14+0000",
            "content": "Note that for a normal facet query, this could result in 3 waves of requests.\n1) query + facet\n2) facet refinements\n3) retrieve stored fields + highlight\n\nWe probably want to allow #2 to piggyback on #3 requests, provided that nothing needs final facet values before retrieving the stored fields. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12557531",
            "date": "2008-01-10T03:09:17+0000",
            "content": "OK, this version patches cleanly and includes some distributed faceting code.\n\nI haven't looked at it ... but holy freaking cow that's cool.\n\nNote that it is theoretically possible to miss terms. A term could be just below the threshold of each shard (and thus not returned by any shard), but the total count could boost it in the top. This could be rectified by retrieving all terms above a specified count, but it could be expensive. The counts that are currently returned are exact.\n\none solution i've seen to mitigate problems like this in the past is to compute a higher \"limit\" when querying the individual shards, someone somewhere suggested that n**2 is a good approach (but they may have been talking out of their ass) so if the initial request says facet.limit=5, the individual shards would be queried with facet.limit=25 ... but you'd also still want to use refinement requests.\n "
        },
        {
            "author": "Ian Holsman",
            "id": "comment-12557533",
            "date": "2008-01-10T03:32:17+0000",
            "content": "Hoss.. \n\nI'm not sure about n**2. \n\nI would think it would be n * number of shards. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12557535",
            "date": "2008-01-10T03:44:54+0000",
            "content": "> one solution i've seen to mitigate problems like this in the past is to compute a higher \"limit\" when querying the individual shards\n\nYep.  Eventually should be configurable too.  We should definitely do some \"over requesting\" for very small limits.  Expanding the limit too much can be expensive though (CPU cost partially depends on the algorithm).  I think users should even be able to disable refinement queries if they just want an estimate.\n\nNote that it's possible to tell if there even could be stealth terms out there... we maintain the smallest count we get from each shard, so that serves as the largest count any unknown term could have.  Add all those together to see if it's possible an unknown term could make it to the top terms.   This means you could do a request with a smaller limit, and then re-request with a larger limit if necessary.\n\nBeyond that, it becomes unclear what the best strategy is.  Worst case scenario: If the top N facets get down to a count of 1, then any unknown term could bump another higher.  Requesting all terms with count>=1 from each shard isn't something I want to ponder. \n\nAnyway, a colleague informs me that this is the way at least one other major search vendor does things (counts are exact for terms shown, but it is theoretically possible to miss a term).  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12557537",
            "date": "2008-01-10T03:59:00+0000",
            "content": "> I would think it would be n * number of shards.\n\nThat would make the number of terms to transfer over the network and to merge O(n_shards**2)... not great for scalability  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12557554",
            "date": "2008-01-10T06:21:15+0000",
            "content": "New patch attached...\n\nI just discovered that refinement queries weren't working because filter.query doesn't accept the new query syntax I was using to avoid having to escape field values: <!field f=myfield>value\n(this should probably be committed separately, but it's in this patch for now).\n\nI put in code to over-request facet.field limit, but then commented it out for now since it too easily covers up bugs because it often prevents any refinement query logic from being exercized.\n\nAlso corrected the code that always used the last element as the max possible missing count.  If we requested 10 terms and only got 6, then we know that the max possible missing count is zero. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12557899",
            "date": "2008-01-11T04:52:00+0000",
            "content": "Now patch attached... this one implements count tiebreaking by index order (to match the non-distributed faceting). "
        },
        {
            "author": "Dima Brodsky",
            "id": "comment-12559259",
            "date": "2008-01-15T22:41:58+0000",
            "content": "Hey,\n\nQuick question from a solr newbie.  I'd love to be able to play/test out the distributed functionality of this patch.  Are there some user level instructions as to how to configure and run?\n\nThanks!\nttyl\nDima\n "
        },
        {
            "author": "patrick o'leary",
            "id": "comment-12559269",
            "date": "2008-01-15T22:55:16+0000",
            "content": "Hey Yonik\nNeeded to make a couple of updates to ShardDoc as the nested outer classes were preventing me from using the patch.\nAlso included SOLR-457, with a multi threaded implementation of solrj to query the shards.\nwith this patch.\n\nP "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12563701",
            "date": "2008-01-29T22:34:06+0000",
            "content": "This update adds parallel requests.\n\n\ta singleton communications thread pool (executor) is added... currently static, but it should be per core and have a way of shutting down.\n\ta singleton HttpClient for use by all SolrServer instances, currently static, probably fine to remain so (unless there needs to be core specific config?)\n\tan exception causes everything to be aborted\n\tall requests in a phase are sent out in parallel\n\ta completion service is used for grabbing completed requests, so the first requests back can start being processed.\n\twhile receiving responses, if any new requests are put on the outgoing queue, they are immediately sent out before waiting for any further responses.\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12565638",
            "date": "2008-02-05T04:33:23+0000",
            "content": "Updated patch:\n\n\tface refinement requests piggyback on the requests to retrieve stored fields where possible.\n\tfixed bug when requesting scores... don't include scores even if requested if they are not in the given DocList\n\tfixed HTTP error codes for query parse errirs\n\tadded double/long support in sorting since we've upgraded to lucene 2.3, and changed aggregate numFound to handle long\n\tescape&unescape comma separated \"ids\" string using backslash escaping (used to specify docs from each shard to retrieve)\n\tother misc cleanups\n\n "
        },
        {
            "author": "patrick o'leary",
            "id": "comment-12567953",
            "date": "2008-02-12T04:47:18+0000",
            "content": "It looks pretty good, I really need the ShardDoc's classes to be split up into public classes so I can use\nthem. \nIt would also be fantastic to open up QueryComponent, my component only needs to over ride\na few functions, and it would so much cleaner to just extend QueryComponent rather than duplicate the code.\n\nAlso through testing, it might be worth while to apply a few negative edge cases.\ne.g. duplicate documents in different shards. As systems get larger this is a huge possibility. Only fixed hash indexing could ensure you don't get duplicates, but if you try to have an extend-able  environment that might not be an option.\n\nTook me a while to realize I had duplicated documents during indexing, but it causes NPEs in the query response writers, so not obvious or easy to figure out.\n\nA solution would be to maintain map of unique fields as adding the ShardDocs to the priority queue, and continue on duplicates. You might also want to put some logic in there to ensure same shard doc is used for each duplicate doc, simple because the scores for identical doc's will be different across shards, and could change based upon order of which Shard responds first. This should eliminate that\n\n\nSo something like\nQueryComponent.mergeIds\n\nMap<Object, String> uniqueDoc = new HashMap<Object, String>();\n      \n      for (ShardResponse srsp : sreq.responses) {\n        SolrDocumentList docs = srsp.rsp.getResults();\n         ................\n         ................\n         // go through every doc in this response, construct a ShardDoc, and\n        // put it in the priority queue so it can be ordered.\n        for (int i=0; i<docs.size(); i++) {\n          SolrDocument doc = docs.get(i);\n          ..................\n          ..................\n          Object uniqueField = doc.getFieldValue(uniqueKeyField.getName());\n          \n          if(! uniqueDoc.containsKey(uniqueField)) {\n        \t  shardDoc.setId(uniqueField);\n        \t  uniqueDoc.put(uniqueField, shardDoc.shard);\n          } else{\n        \t  numFound--;\n        \t  if(uniqueDoc.get(uniqueField).compareTo(shardDoc.shard) >0){\n        \t\t continue;\n        \t  }\n          }\n\n          ..........................\n          queue.insert(shardDoc);\n        } // end for-each-doc-in-response\n      } // end for-each-response\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12569711",
            "date": "2008-02-17T15:47:27+0000",
            "content": "updated patch:\n\n\trefactored some distributed search code to make things easier (added modifyRequest, etc)\n\tadded merging of debugging info timing info (including timing info, via generic recursive merging)\n\tmerge explain info, drops internal id from explain key for easier merging\n\tMany small changes: don't return scores if they aren't requested (even if needed for shard requests to merge), return maxScore\n  if scores are requested, enable escaping for shards parameter.\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12569712",
            "date": "2008-02-17T15:59:22+0000",
            "content": "> I really need the ShardDoc's classes to be split up into public classes\n\nShardDoc is public already... can you elaborate?\n\n> It would also be fantastic to open up QueryComponent, my component only needs to override a few functions\n\nWhat is yours trying to accomplish?\n\n> A solution would be to maintain map of unique fields as adding the ShardDocs to the priority queue, and continue on duplicates.\n\nAgree.  It should fall into the category of robustness though, rather than a duplicates detection feature (since it will mean that facets will be off, and it will be possible to get fewer docs than requested if duplicates do exist).\n\nWe also need to be robust in the face of a commit on a shard happening between phases of a request (a doc that we request info for may no longer exist, etc).  That would probably cause us to blow up currently.\n\nHopefully this can be committed after some basic tests are added, and that will make it much easier for others to contribute patches.  In the future maybe we should try a branch for changes this large. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12569717",
            "date": "2008-02-17T17:01:27+0000",
            "content": "New patch attached... last one had an unfinished change that prevented compilation (using the generic SolrResponse instead of SolrQueryResponse). "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12569728",
            "date": "2008-02-17T17:36:58+0000",
            "content": "fixed test cases that relied on parsing previous explain format "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12571992",
            "date": "2008-02-25T00:19:05+0000",
            "content": "Patrick, I've reproduced your null pointer exception on accidental duplicates (I've been working on tests).  I'll look into a fix along the lines of what you suggested. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12572261",
            "date": "2008-02-25T21:53:16+0000",
            "content": "New patch:\n\n\ttest framework using multiple embedded jetty servers that adds documents to multiple servers, and also to a control server, then executes both distributed and non-distributed queries and compares the results.\n\tfixed merging for non-string uniqueKeyFields\n\tfixed issue when id field was not selected by client\n\tbreak facet count ties by label\n\tadded rudimentary duplicate detection in case one accidentally adds the same doc to different shards\n\tadd code to handle index changes between query phases (docs may no longer exist)\n\n\n\nGiven that most of this is new functionality, I think things are in good enough shape to commit now (making it much easier for others to generate patches against it). "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12572542",
            "date": "2008-02-26T16:12:33+0000",
            "content": "\nGiven that most of this is new functionality, I think things are in good enough shape to commit now (making it much easier for others to generate patches against it).\n\n+1 (But have only checked that it does not break anything I'm working with) \u2013 I think this should get committed soon.  Since it is large and mostly discrete from existing functions, it will be much easier to refine with smaller patches. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12572633",
            "date": "2008-02-26T20:02:12+0000",
            "content": "OK, I've committed this!  Thanks everyone!\nI'll leave this bug open for now as a place to accumulate patches.\n\nSome things that are missing (but optional and not currently high on my TODO list):\n\n\tfield faceting when facet.sort=false\n\tdistributed idf... this has a performance cost, and should matter little in a well mixed index.\n\n "
        },
        {
            "author": "Henri Biestro",
            "id": "comment-12574984",
            "date": "2008-03-04T13:18:22+0000",
            "content": "Nothing functional , just noticed reading the code that Shard\n{Doc,Request}\n are missing the Apache license header. "
        },
        {
            "author": "Jayson Minard",
            "id": "comment-12579926",
            "date": "2008-03-18T17:04:48+0000",
            "content": "Attached patch to fix issue with distributed search.  If you specified a facet.field that was valid for the schema but not contained in a shard, an unintentional exception (array index out of bounds) would be thrown instead of returning the facet as empty. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12579931",
            "date": "2008-03-18T17:15:20+0000",
            "content": "I just committed this bugfix... thanks Jayson! "
        },
        {
            "author": "Jayson Minard",
            "id": "comment-12579966",
            "date": "2008-03-18T18:06:48+0000",
            "content": "A few more tests to show intended behavior when facets differ between shards which is likely in the wild (missing from all but valid in schema, missing from some, and invalid field not in schema).  The last test  is just to ensure error behavior matches non-distributed searches. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12580024",
            "date": "2008-03-18T19:40:22+0000",
            "content": "committed addition tests... thanks! "
        },
        {
            "author": "Jayson Minard",
            "id": "comment-12580188",
            "date": "2008-03-19T00:18:20+0000",
            "content": "Would it be interesting to others to have an extended response format for distributed queries that would bring back the list of shards numbered, and then code each element of the response with the source list of shards that contributed to the element appearing in the results?  For example, which shard was the source of a document?  Or which shards had the facet value present?  And so on.\n\nIn really high shard counts it is more efficient if you can trim follow-on queries and pivots to only shards that matter.  This information would help that effort.  \n\nRegardless, it is useful for debugging. "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12580392",
            "date": "2008-03-19T14:43:14+0000",
            "content": "Jayson--\n\nI agree.  I've been meaning to recommend that be added.  We've found it invaluable in the past (mostly with debugging) when doing federated and distributed search.  I would like to see a \"shard\" field added which would contain the base URI of the shard where the result originated as provided in the request.  The index of each result is less important to me, but I can see how that would be useful. "
        },
        {
            "author": "Jayson Minard",
            "id": "comment-12580403",
            "date": "2008-03-19T15:22:31+0000",
            "content": "I'll see if I can work up a patch tonight on the extended response... "
        },
        {
            "author": "Stu Hood",
            "id": "comment-12581747",
            "date": "2008-03-25T00:44:56+0000",
            "content": "Because the subqueries to Solr shards use GET requests (via SolrJ), they are limited in the number of documents they can request during the second phase by the maximum length of the query string.\n\nOne (API preserving) solution would be to modify SolrJ to use a POST request for queries if the query string is longer than some constant value. "
        },
        {
            "author": "Thomas Peuss",
            "id": "comment-12581885",
            "date": "2008-03-25T11:57:27+0000",
            "content": "they are limited in the number of documents they can request during the second phase by the maximum length of the query string.\nFor Tomcat you can increase the allowed length of the query string by adding for example maxHttpHeaderSize=\"65536\" to the Connector entries in server.xml. This increases the max. allowed GET request size to 64KB (standard is 4KB). "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12593196",
            "date": "2008-04-30T01:17:09+0000",
            "content": "Attaching shards_qt.patch, which uses \"shards.qt\" as \"qt\" for sub-requests to avoid infinite recursion when setting \"shards\" as a default in the request handler. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12593574",
            "date": "2008-05-01T16:38:10+0000",
            "content": "I just committed shards_qt.patch "
        },
        {
            "author": "Lars Kotthoff",
            "id": "comment-12598230",
            "date": "2008-05-20T09:32:59+0000",
            "content": "I've had a couple of issues with the current version. First, the facet queries which are sent to the other shards are posted in the URL, but aren't URL encoded, i.e. during the refine stage anything non-ascii results in facet counts for \"new\" values (i.e. the garbled version) coming back and causing NPEs when trying to update the counts.\n\nFurthermore, facet.limit=<negative value> isn't working as expected, i.e. instead of all facets it returns none. Also facet.sort is not automatically enabled for negative values.\n\nI've attached \"solr-dist-faceting-non-ascii-all.patch\" which fixes the above issues. Somebody who understands what everything is supposed to do should have a look over it though \nFor example I've found two linked hash maps in FacetInfo, topFacets and listFacets, which seem to serve the same purpose. Therefore I replaced them by a single hash map. It seems to work just fine this way. "
        },
        {
            "author": "Lars Kotthoff",
            "id": "comment-12598548",
            "date": "2008-05-21T04:06:37+0000",
            "content": "On closer inspection of the code, are the fields \"sort\" and \"prefix\" of FieldFacet used anywhere at all? They don't seem to be referenced anywhere in the code and just removing them doesn't seem to have any obvious effect. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12598826",
            "date": "2008-05-21T22:20:33+0000",
            "content": "marking as intended for 1.3 ... i'm not overly familiar with the state of this issue, but i do know that large chunks of functionality have already been committed, so i want to make sure that before 1.3 is released someone conciously decides between:\n\n\t\"DONE\" ...resolving this issue\n\t\"NOT DONE BUT OK\" ... leaving the issue unresolved and removing the 1.3 designation\n\t\"NOT DONE AND NOT OK\" ... rolling back any/all committed code that is considered detrimental for the 1.3 release.\n\n "
        },
        {
            "author": "Brian Whitman",
            "id": "comment-12605650",
            "date": "2008-06-17T16:38:08+0000",
            "content": "When I give the following request:\n\nhttp://localhost:8983/solr/select?shards=localhost:8983/solr,localhost:8984/solr&q=woof\n\nWith no server running on 8984 I get a error 500 (naturally.)\n\nBut shouldn't there be an option to skip over servers that aren't responding or time out? Envisioning a scenario in which this is used to search across possibly redundant uniqueIDs and a server being down is not cause for exception.\n\n "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12605653",
            "date": "2008-06-17T16:48:27+0000",
            "content": "Ah, yes, I agree with Brian.  I did see this, too, fut forgot to report it as a problem that needs a fix. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12605660",
            "date": "2008-06-17T17:03:40+0000",
            "content": "> But shouldn't there be an option to skip over servers that aren't responding or time out?\n\nThat does sound like it would be a useful option (but I think it should be false by default though).\n\nFYI, I'm currently looking into Lars' facet changes. "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12605666",
            "date": "2008-06-17T17:28:28+0000",
            "content": "In SOLR-502, there is the notion of partialResults.  It seems that the same flag could be used in this case.  Perhaps a string should also be added indicating why all results were not able to be returned. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12605670",
            "date": "2008-06-17T17:33:31+0000",
            "content": "Lars: I committed your fix to the facet.limit value sent to shards, and instead of changing ntop when facet.limit<=0, I simply short-circuited checking if refinement is needed at all.\n\nNext up: investigate this URL encoding (or lack of it) in the POST body. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12605699",
            "date": "2008-06-17T19:18:41+0000",
            "content": "Lars: I'm not yet able to reproduce an issue with SolrJ not encoding the parameters properly.\n\nThe following code finds the sample solr document:\n\n    SolrServer server = new CommonsHttpSolrServer(\"http://localhost:8983/solr\");\n    ModifiableSolrParams params = new ModifiableSolrParams();\n    params.set(\"echoParams\",\"all\");\n    params.set(\"q\",\"+h\\u00E9llo\");\n    QueryRequest req = new QueryRequest(params);\n    req.setMethod(SolrRequest.METHOD.POST);\n     System.out.println(server.request(req));\n\n\n\nAnd netcat confirms the encoding looks good, and is in fact using POST\n\n$ nc -l -p 8983\nPOST /solr/select HTTP/1.1\nUser-Agent: Solr[org.apache.solr.client.solrj.impl.CommonsHttpSolrServer] 1.0\nHost: localhost:8983\nContent-Length: 53\nContent-Type: application/x-www-form-urlencoded\n\nechoParams=all&q=%2Bh%C3%A9llo&wt=javabin&version=2.2\n\n\n\nI'll see if I can reproduce anything with TestDistributedSearch "
        },
        {
            "author": "Lars Kotthoff",
            "id": "comment-12605868",
            "date": "2008-06-18T06:58:33+0000",
            "content": "Yonik, thanks for taking a look at it.\n\nI've investigated this issue further and I believe I know what the root cause is now. The line\no.a.s.client.solrj.impl.CommonsHttpSolrServer.java\n...\npost.getParams().setContentCharset(\"UTF-8\");\n...\n\n\ntells the sender to encode the data as UTF-8. The way the receiver decodes the data depends on whatever is set as charset in the Content-Type header. This header is currently automatically added by httpclient and, as you can see in the netcat log, \"application/x-www-form-urlencoded\", i.e. without a charset. The default charset is ISO-8859-1 (cf. http://hc.apache.org/httpclient-3.x/charencodings.html). So the data is encoded as UTF-8 but decoded as ISO-8859-1, which causes the effect I described earlier.\n\nI tried to reproduce this with TestDistributedSearch myself, but for some reason it seems to be fine. Perhaps the Jetty configuration is different to my Tomcat configuration. I didn't find any parameter to tell Tomcat the default encoding if the Content-Type header doesn't specify one though.\n\nThe minimal change I had to make to make it work was add a line to set the Content-Type header explicitly, i.e.\no.a.s.client.solrj.impl.CommonsHttpSolrServer.java\n...\npost.getParams().setContentCharset(\"UTF-8\");\npost.setRequestHeader(\"Content-Type\", \"application/x-www-form-urlencoded; charset=UTF-8\");\n...\n\n\nThis probably won't work with multi-part requests though. I'm not sure what the right way to handle this would be. The stub Content-Type header is set by httpclient when the method is executed, i.e. there's no way to let httpclient figure out the first part and then append the charset in CommonsHttpSolrServer.\n\nSome other things I've noticed:\n\n\tJust before the content charset is set, the parameters of the POST request are populated. If the value for a parameter is null, the code attempts to to add a null parameter. This however will cause an IllegalArgumentException from httpclient (cf. http://hc.apache.org/httpclient-3.x/apidocs/org/apache/commons/httpclient/methods/PostMethod.html#addParameter(java.lang.String, java.lang.String)).\n\tTestDistributedSearch does not exercise the code to refine facet counts. Adding another facet request with facet.limit=1 redresses this.\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12605931",
            "date": "2008-06-18T13:08:33+0000",
            "content": "I forgot we've already gone a few rounds on charset in POST bodies:\nhttps://issues.apache.org/jira/browse/SOLR-443\nhttp://markmail.org/message/gtzbtwzqa6zranur?q=POST+body+charset#query:POST%20body%20charset+page:1+mid:fkragfatbox5fff5+state:results "
        },
        {
            "author": "Lars Kotthoff",
            "id": "comment-12606242",
            "date": "2008-06-19T04:20:18+0000",
            "content": "Making this issue depend on SOLR-443 as distributed faceting of non-ascii values won't work properly without it. Please also see my comment on that issue. "
        },
        {
            "author": "Brian Whitman",
            "id": "comment-12607097",
            "date": "2008-06-22T14:38:06+0000",
            "content": "If the user is going to be splitting their index over N shards, it's going to be crucial to have the distributed search (optionally) return the docid->shard map in the response. Is that tricky to add as part of this issue?  "
        },
        {
            "author": "Brian Whitman",
            "id": "comment-12607106",
            "date": "2008-06-22T16:48:10+0000",
            "content": "Putting &debugQuery on a query with shards that returns 0 results will NPE:\n\n(removing NPE code block so it stops wrapping the page) "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12611230",
            "date": "2008-07-07T16:46:52+0000",
            "content": "Fixed \"debugQuery on a query with shards that returns 0 results will NPE\".\nThere are still some issues with debugQuery=true, but it's not critical since it is just debugging.  I'll open another issue for that. "
        },
        {
            "author": "Brian Whitman",
            "id": "comment-12611368",
            "date": "2008-07-07T21:50:08+0000",
            "content": "Anyone notice something like this:\n\nhttp://localhost:8983/solr/select?shards=\n{4 shards}\n&q=:&start=5000&rows=1000\n\nSeems to request &rows=6000 from all the shards? (likewise, start=10000&rows=1000 sends rows=11000 to all the shards?) \n\nThe shards all say:\nINFO: webapp=/solr path=/select params=\n{fl=id,score&start=0&q=*:*&isShard=true&wt=javabin&fsv=true&rows=6000&version=2.2}\n hits=6000 status=0 QTime=175 \n\nAnd the host I called select on says:\nINFO: webapp=/solr path=/search params=\n{start=5000&q=*:*&rows=1000}\n status=0 QTime=1192 \n\nAnd the QTime goes up the higher &start goes. (QTime for start=5000 was 200, QTime for start=50000 was 4500, start=500000 had 35000!)\n\nBug or feature?\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12611372",
            "date": "2008-07-07T22:08:20+0000",
            "content": "\nhttp://localhost:8983/solr/select?shards=[4 shards]&q=:&start=5000&rows=1000\nSeems to request &rows=6000 from all the shards?\n\nIt's a feature.\n\nTo retrieve documents 5000-6000, one must find the first 6000 documents then take the last 1000.\nSince it's possible that all top 6000 documents could come from a single shard, the top 6000 documents must be collected from each and merged.\n\nThere are alternatives:\n1) Optimistically request less than 6000 documents per shard and re-query if we are wrong\n2) Add an optional mode that treats documents across shards in the same position as equal, so if you had 10 shards, you would simply get the top 100 docs starting at 500.  This might be OK for some applications.\n\nIn general, search engines are optimized at retrieving the top 10 of something, and bad at retrieving the top 10 starting at a big number.  Limit the depth people can page, or restructure queries to avoid the latter case. "
        },
        {
            "author": "Brian Whitman",
            "id": "comment-12611376",
            "date": "2008-07-07T22:15:45+0000",
            "content": "Understood. Can I suggest a third alternative?\n\ntwo new params: named &d.rows and &d.start with the implication that these get sent unchanged to each of the shards. You may get back up to N*d.rows, where N is the # of shards. That leaves the paging management up to the client.\n\nOur use case is millions of documents across many shards, and we often do queries that are \"get all document of type X.\" There may be 5m type X documents. Doing a &rows=5000000 is unpredictable so we've previously done a loop of incrementing start by a 1000 and getting 1000 rows each time. But with this distributed setup, each successive batch query takes slightly longer, and by the time we've gotten to the 5,001,000 batch queries are timing out and breaking anyway. \n\n\n "
        },
        {
            "author": "Brian Whitman",
            "id": "comment-12611414",
            "date": "2008-07-08T00:41:07+0000",
            "content": "Attaching patch to add a &shards.start and &shards.rows optional parameter. If set, they override distributed search's intelligence on setting start and rows per shard. If you set &shards.start=10 and &shards.rows=10, each shard will be queried with &start=10 and &rows=10 and you'll get back N*10 results (set &rows on the main query to get it all.)\n\n[Not a java developer, my patch works but may violate good taste/style] "
        },
        {
            "author": "Sean Timm",
            "id": "comment-12611758",
            "date": "2008-07-08T19:19:22+0000",
            "content": "Another option is to pass state on the number of documents and positions retrieved from each shard.  I have  a client layer that can do that, so it works, but it is complicated, maintaining state is messy, and the vast majority of requests are first page requests so in practice we almost never use that feature, but instead do exactly as is implemented here and request the full document count from each shard. "
        },
        {
            "author": "Brian Whitman",
            "id": "comment-12613969",
            "date": "2008-07-16T14:26:42+0000",
            "content": "Getting \"Form too large\" from jetty while doing normal but large rows= (40000) shards requests. Is this related to SOLR-612 ?\n\nQuery was : http://x.x.x.x/solr/search?q=*:*&sort=indexed%20desc&fl=indexed&rows=40000 , where x.x.x.x is a single shard and /search has the shards ivars mapped to it in solrconfig.\n\n(Sorry for the mess, but that's how it appears)\n\nForm_too_large_javalangIllegalStateException\nForm_too_large_at_orgmortbayjettyRequestextractParametersRequestjava1273at\norgmortbayjettyRequestgetParameterMapRequestjava650_at\norgapachesolrrequestServletSolrParamsinitServletSolrParamsjava29_at\norgapachesolrservletStandardRequestParserparseParamsAndFillStreamsSolrRequestParsersjava392_at\norgapachesolrservletSolrRequestParsersparseSolrRequestParsersjava113_at\norgapachesolrservletSolrDispatchFilterdoFilterSolrDispatchFilterjava240_at\norgmortbayjettyservletServletHandler$CachedChaindoFilterServletHandlerjava1089_at\norgmortbayjettyservletServletHandlerhandleServletHandlerjava365_at\norgmortbayjettysecuritySecurityHandlerhandleSecurityHandlerjava216_at\norgmortbayjettyservletSessionHandlerhandleSessionHandlerjava181_at\norgmortbayjettyhandlerContextHandlerhandleContextHandlerjava712_at\norgmortbayjettywebappWebAppContexthandleWebAppContextjava405_at\norgmortbayjettyhandlerContextHandlerCollectionhandleContextHandlerCollectionjava211_at\norgmortbayjettyhandlerHandlerCollectionhandleHandlerCollectionjava114_at\norgmortbayjettyhandlerHandlerWrapperhandleHandlerWrapperjava139_at\norgmortbayjettyServerhandleServerjava285_at\norgmortbayjettyHttpConnectionhandleRequestHttpConnectionjava502_at\norgmortbayjettyHttpConnection$RequestHandlercontentHttpConnectionjava835_at\norgmortbayjettyHttpParserparseNextHttpParserjava641_at\norgmortbayjettyHttpParserparseAvailableHttpParserjava202_at\norgmortbayjettyHttpConnectionhandleHttpConnectionjava378_at\norgmortbayjettybioSocketConnector$ConnectionrunSocketConnectorjava226_at\norgmortbaythreadBoundedThreadPool$PoolThreadrunBoundedThreadPooljava442_\n\nrequest: http://x.x.x.x.y/solr/select (ed: this was a different shard than the one I called)\n\nrequest: http://x.x.x.y/solr/select\n\tat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:343)\n\tat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:183)\n\tat org.apache.solr.handler.component.HttpCommComponent$1.call(SearchHandler.java:371)\n\tat org.apache.solr.handler.component.HttpCommComponent$1.call(SearchHandler.java:345)\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:138)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:138)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)\n\tat java.lang.Thread.run(Thread.java:619) "
        },
        {
            "author": "Lars Kotthoff",
            "id": "comment-12614181",
            "date": "2008-07-17T01:16:07+0000",
            "content": "The default limit for form submissions is 200000 bytes with Jetty. I'm not sure why Solr is trying to send such large amounts of data to the shards though, the only case I've seen this happening is with faceting \u2013 Solr has to request facet counts for specific values from the shards to get exact counts. Maybe because of the sorting?\n\nAnyway, you can change the limit by setting the org.mortbay.http.HttpRequest.maxFormContentSize system property. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12614186",
            "date": "2008-07-17T01:24:36+0000",
            "content": "I'm not sure why Solr is trying to send such large amounts of data to the shards though\n\nSpecifying 40,000 ids to be retrieved I imagine.  The average id length must be over 50 bytes.\n\nBrian: if ordering isn't important for some of these big bulk queries, you might want to consider directly querying the shards. "
        },
        {
            "author": "Brian Whitman",
            "id": "comment-12614397",
            "date": "2008-07-17T15:38:01+0000",
            "content": "Yonik, sure-- but I think we should probably handle the case better than a 500 error. maybe a solr warning about per-shard row limits?\n\nLars \u2013 I am having trouble getting that maxFormContentSize property set. I am running jetty like:\n\n/usr/local/java/bin/java -Dorg.mortbay.http.HttpRequest.maxFormContentSize=1000000 -Xmx7000m -Xms1024m -jar start.jar\n\n(I've also tried 0 and -1, per the jetty docs this means \"unlimited.\") \n\nbut the same distributed query gives the same error. How are you setting that property?\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12614400",
            "date": "2008-07-17T15:45:54+0000",
            "content": "but I think we should probably handle the case better than a 500 error. maybe a solr warning about per-shard row limits?\n\nThat's a jetty limit you hit, the exception was understandable, and an unknown exception like that (from solr's perspective) seems like it should map to a 500 error code. "
        },
        {
            "author": "Lars Kotthoff",
            "id": "comment-12614402",
            "date": "2008-07-17T15:51:10+0000",
            "content": "I think we should probably handle the case better than a 500 error. maybe a solr warning about per-shard row limits?\n\nThat's specific to the configuration of your container, I think there's nothing that Solr can do about it.\n\nAs for the form content size, I haven't actually tried that myself I must admit. I'm running Tomcat and just got that parameter from the Jetty documentation. I'd take a wiredump with something like tcpdump to see what the actual size of the request is. Maybe it's even larger than 1000000 bytes? "
        },
        {
            "author": "Brian Whitman",
            "id": "comment-12614410",
            "date": "2008-07-17T16:17:16+0000",
            "content": "My ids are 32-character MD5s, and the break happens around 23000 rows. The maxFormContentSize doesn't seem to make any difference whether I set it or not-- with it set at 0, -1, 10000000 or not set at all I can query &rows=22300 but not &rows=22400. Obviously this is an edge case but I'm posting this here for the next person who runs into this... but since I can work around it I'll stop messing with it.\n "
        },
        {
            "author": "Lars Kotthoff",
            "id": "comment-12614602",
            "date": "2008-07-18T02:01:32+0000",
            "content": "Which version of Jetty are you using? The org.mortbay.http.HttpRequest.maxFormContentSize system property seems to be specific to Jetty 5 \u2013 I didn't find any information on how to set the limit with Jetty 6 (or indeed if it exists at all). "
        },
        {
            "author": "Brian Whitman",
            "id": "comment-12614708",
            "date": "2008-07-18T12:19:50+0000",
            "content": "Lars- I'm using the jetty that comes with solr-trunk, jetty-6.1.3.\n\nI found this: http://webteam.archive.org/jira/browse/HER-1173#action_14736\n\nWhich indicates the Jetty 6 concordant property is org.mortbay.jetty.Request.maxFormContentSize.\n\nI set that to 1000000, restarted my shards, and queries of &rows=40000 works. So for those who have this problem, start jetty with:\n\njava -Dorg.mortbay.jetty.Request.maxFormContentSize=1000000 -jar start.jar\n\nI would suggest only that the jetty.xml included in the solr example somehow get this parameter hardcoded (I don't know how personally.) I understand this is not a solr issue but it does cause a non-obvious result to an obvious query.\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12615461",
            "date": "2008-07-21T23:00:55+0000",
            "content": "Closing this issue (finally!).  Specific bugs or improvements can get their own new issues.\nThanks to everyone who contributed to this! "
        }
    ]
}