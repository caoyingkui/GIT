{
    "id": "LUCENE-4609",
    "title": "Write a PackedIntsEncoder/Decoder for facets",
    "details": {
        "components": [
            "modules/facet"
        ],
        "fix_versions": [],
        "affect_versions": "None",
        "priority": "Minor",
        "labels": "",
        "type": "New Feature",
        "resolution": "Not A Problem",
        "status": "Resolved"
    },
    "description": "Today the facets API lets you write IntEncoder/Decoder to encode/decode the category ordinals. We have several such encoders, including VInt (default), and block encoders.\n\nIt would be interesting to implement and benchmark a PackedIntsEncoder/Decoder, with potentially two variants: (1) receives bitsPerValue up front, when you e.g. know that you have a small taxonomy and the max value you can see and (2) one that decides for each doc on the optimal bitsPerValue, writes it as a header in the byte[] or something.",
    "attachments": {
        "SemiPackedEncoder.patch": "https://issues.apache.org/jira/secure/attachment/12568423/SemiPackedEncoder.patch",
        "LUCENE-4609.patch": "https://issues.apache.org/jira/secure/attachment/12561747/LUCENE-4609.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-12-10T17:04:25+0000",
            "content": "I just looked at IntDecoder/IntEncoder and wrapping a PackedInts ReaderIterator/Writer (or maybe directly a Decoder/Encoder) looks easy to implement. Don't hesitate to let me know if you have questions regarding the PackedInts API! ",
            "author": "Adrien Grand",
            "id": "comment-13528068"
        },
        {
            "date": "2012-12-10T17:15:24+0000",
            "content": "Thanks Adrien. I would like to implement a specialized Encoder/Decoder, rather than wrapping them w/ PackedInts Reder/Writer. I sure would appreciate your review once I have a patch ! ",
            "author": "Shai Erera",
            "id": "comment-13528077"
        },
        {
            "date": "2012-12-19T18:07:46+0000",
            "content": "Attached a PackedEncoder, which is based on PackedInts. Currently only the approach of a 'per-document' bits-per-value is implemented.\n\nI'm not convinced the header could be spared, as at the very least, the number of bits to neglect at the end of the stream should be written. E.g if there are 2 bits per value, and there are 17 values, there's a need for 34 bits, but everything is written in (at least) bytes, so 6 bits should be neglected.\n\nUpdated EncodingTest and EncodingSpeed, and found out that the compression factor is not that good, probably due to large numbers which bumps the amount of required bits to higher value. \n\nStarted to look into a semi-packed encoder, which could encode most values in a packed manner, but could also add large values as, e.g., vints.\nExample: for 6 bits per value, all values 0-62 are packed, while a packed value of 63 (packed all 1' s) is a marker that the next value is written in a non-packed manner (say vint, Elias delta, whole 32 bits.. ). \nThis should improve the compression factor when most ints are small, and only a few are large. \nImpact on encoding/decoding speed remains to be seen.. ",
            "author": "Gilad Barkai",
            "id": "comment-13536210"
        },
        {
            "date": "2012-12-19T19:09:44+0000",
            "content": "Cool!  Do you encode the gaps or the straight up ords?  (Looks like straight up ords?).\n\nStarted to look into a semi-packed encoder, which could encode most values in a packed manner, but could also add large values as, e.g., vints.\n\nThis is PForDelta compression (the outliers are encoded separately) I think?  We can test it and see if it helps ... but we weren't so happy with it for encoding postings (it adds complexity, slows down decode, and didn't seem to help that much in reducing the size).\n\nHow much worse was compression with straight up packed ints vs vInt(gap)?  The byte-header-per-doc is annoying but I don't think we can do much about it today ... really we need a DocValues that allows multiple ints per field (int[]).  It would write the bpv once in the header (for the entire segment) and then all docs would use that bpv (and we'd separately have to write doc -> offset).  But this is a big change (we need multi-valued DV first)...\n\nHmm, it seems like you are writing the full header per field (which is very wasteful)?  We should be able to write only one byte (the bpv), if you use getWriterNoHeader instead? ",
            "author": "Michael McCandless",
            "id": "comment-13536321"
        },
        {
            "date": "2012-12-19T19:34:28+0000",
            "content": "Do you encode the gaps or the straight up ords?\n\nWell, It's a 'end point' encoder, meaning it encodes whatever values are received directly to the output.\nOne could create an encoder as: new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new PackedEncoder()))), so the values the packed encoder would receive are already after sort, unique and dgap. \n\n\nThis is PForDelta compression (the outliers are encoded separately) I think? We can test it and see if it helps ... but we weren't so happy with it for encoding postings (it adds complexity, slows down decode, and didn't seem to help that much in reducing the size).\n\nPForDelta is indeed slower. But we've met scenarios in which most dgaps are small - hence the NOnes, and the Four/Eight Flag encoders. If indeed most values are small, say, could fit in 4 bits, but there's also one or two larger values which would require 12 or 14 bits, we could benefit hear greatly.\nThis is all relevant only where there are large amount of categories per document.\n\nit seems like you are writing the full header per field\nThat is right. To be frank, I'm not 100% sure what PackedInts does.. nor how large its header is.. \nBut I think perhaps some header per doc is required anyway? For bits-per-value smaller than the size of a byte, there's a need to know how many bits should be left out from the last read byte. \n\nI started writing my own version as a first step toward the 'mixed' version, in which a 1 byte header is written, that contained both the the 'bits per value' as the first 5 bits, and the amount of extra bits in the last 3 bits. I'm still playing with it, hope to share it soon. ",
            "author": "Gilad Barkai",
            "id": "comment-13536347"
        },
        {
            "date": "2012-12-19T20:13:02+0000",
            "content": "Well, It's a 'end point' encoder, meaning it encodes whatever values are received directly to the output.\n\nAhh right, OK.\n\nPForDelta is indeed slower. But we've met scenarios in which most dgaps are small - hence the NOnes, and the Four/Eight Flag encoders. \n\nOK makes sense.\n\n\nIf indeed most values are small, say, could fit in 4 bits, but there's also one or two larger values which would require 12 or 14 bits, we could benefit hear greatly.\nThis is all relevant only where there are large amount of categories per document.\n\nRight ... I'm just wondering how often this happens in \"typical\" (if there is such a thing) facet aps.  Decode speed trumps compression ratios here, I think.\n\nThat is right. To be frank, I'm not 100% sure what PackedInts does.. nor how large its header is.. \n\nThe header is very large ... really you should only need 1) bpv, and 2) bytes.length (which I think you already have, via both payloads and DocValues).  If the PackedInts API isn't flexible enough for you to feed it bpv and bytes.length then let's fix that!\n\nFor bits-per-value smaller than the size of a byte, there's a need to know how many bits should be left out from the last read byte.\n\nHopefully you don't need to separately encode \"leftover unused bits\" ... ie byte[].length (which is \"free\" here, since codec already stores this) should suffice. ",
            "author": "Michael McCandless",
            "id": "comment-13536379"
        },
        {
            "date": "2012-12-19T20:16:44+0000",
            "content": "Hopefully you don't need to separately encode \"leftover unused bits\" ... ie byte[].length (which is \"free\" here, since codec already stores this) should suffice.\n\nI'm missing something.. if there are 2 bits per value, and the codec knows its only 1 byte, there could be either 1, 2, 3 or 4 values in that single byte. How could the decoder know when to stop without knowing how many bits should not be encoded at the end?  ",
            "author": "Gilad Barkai",
            "id": "comment-13536384"
        },
        {
            "date": "2012-12-19T20:21:18+0000",
            "content": "\nI'm missing something.. if there are 2 bits per value, and the codec knows its only 1 byte, there could be either 1, 2, 3 or 4 values in that single byte. How could the decoder know when to stop without knowing how many bits should not be encoded at the end?\n\nAhh you're right!  OK.  So I think the custom header must include both bpv and \"wasted bits\".  Hmm, but only if bpv is \"small enough\" to be ambiguous right?\n\nI guess another option is to leave those bits as 0s so that the decoded ord is 0, which is the \"reserved\" root ord and so counting that is harmless maybe?  Tricky ... ",
            "author": "Michael McCandless",
            "id": "comment-13536386"
        },
        {
            "date": "2012-12-19T20:24:43+0000",
            "content": "In a unique encoding dgap, there's no zero, so in order to save that little extra bit, every gap could be encoded as a (gap - 1). Tricky is the word  ",
            "author": "Gilad Barkai",
            "id": "comment-13536391"
        },
        {
            "date": "2012-12-20T10:36:13+0000",
            "content": "Attached a PackedEncoder, which is based on PackedInts.\n\nNice! You could probably improve memory efficiency and speed of the decoder by using a ReaderIterator instead of a Reader:\n\n\tgetReader: consumes the packed array stream and returns an in-memory packed array,\n\tgetDirectReader: does not consume the whole stream and return an impl that uses IndexInput.seek to look up values,\n\tgetReaderIterator: returns a sequential iterator which bulk-decodes values (the \"mem\" parameter allows you to control the speed/memory-efficiency trade-off), so it will be much faster than iterating over the values of getReader.\n\n\n\nFor improved speed, getReaderIterator has the next(int count) method which returns several values in a single call, this proved to be faster. Another option could be to directly use PackedInts.Encoder/Decoder similarly to Lucene41PostingsFormat (packed writers and reader iterators also use them under the hood).\n\nThis is PForDelta compression (the outliers are encoded separately) I think? We can test it and see if it helps ... but we weren't so happy with it for encoding postings\n\nIf the packed stream is very large, another option is to split it into blocks that all have the same number of values (but different number of bits per value). This should prevent the whole stream from growing because of rare extreme values. This is what the stored fields index (with blocks of 1024 values) and Lucene41PostingsFormat (with blocks of 128 values) do. Storing the min value at the beginning of the block and then only encoding deltas could help too.\n\nThe header is very large ... really you should only need 1) bpv, and 2) bytes.length (which I think you already have, via both payloads and DocValues). If the PackedInts API isn't flexible enough for you to feed it bpv and bytes.length then let's fix that!\n\nMost PackedInts method have a \"*NoHeader\" variant that does the exact same job whithout relying on a header at the beginning of the stream (LUCENE-4161), I think this is what you are looking for. We should probably make this header stuff opt-in rather than opt-out (by replacing getWriter/Reader/ReaderIterator with the NoHeader methods and adding a method dedicated to reading/writing a header). ",
            "author": "Adrien Grand",
            "id": "comment-13536928"
        },
        {
            "date": "2012-12-21T15:37:54+0000",
            "content": "Gilad, I created LUCENE-4643 which I assume should be better than PackedInts.Writer and PackedInts.ReaderIterator for your use-case? It doesn't write heavyweight headers (meaning that you need to know the PackedInts version and the size of the stream otherwise) and encodes data in fixed-size blocks. ",
            "author": "Adrien Grand",
            "id": "comment-13538143"
        },
        {
            "date": "2012-12-21T16:37:39+0000",
            "content": "Thank you Adrian!\nI'll will look into it. ",
            "author": "Gilad Barkai",
            "id": "comment-13538193"
        },
        {
            "date": "2013-01-16T22:09:21+0000",
            "content": "Patch, w/ a \"custom\" (not using our PackedInts APIs) packed ints encoder/decoder.  It only uses as many bytes as are necessary, and packs bpv & \"leftoverBits\" into a single byte header.\n\nI tested on first 1M Wikipedia docs ... and performance is much worse than current default in trunk... admittedly it's not quite fair (trunk has specialized vInt/dGap decoder, but patch leaves dGap separate from packed int decode), and admittedly this decoder will be slower than the optimized oal.util.PackedInts ... but perf is so far off that I find it hard to believe PackedInts can match vInt even after optimizing.\n\nTrunk gets these results:\n\n                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff\n                PKLookup      203.77      (1.8%)      202.25      (1.8%)   -0.7% (  -4% -    2%)\n                HighTerm       20.43      (1.8%)       20.53      (0.8%)    0.5% (  -2% -    3%)\n                 MedTerm       33.12      (1.7%)       33.30      (0.9%)    0.5% (  -2% -    3%)\n                 LowTerm       87.55      (3.0%)       88.59      (2.5%)    1.2% (  -4% -    6%)\n\n\n\nPatch gets this:\n\n                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff\n                HighTerm       10.82      (3.6%)       10.69      (4.4%)   -1.2% (  -8% -    7%)\n                 MedTerm       19.33      (3.2%)       19.10      (4.0%)   -1.2% (  -8% -    6%)\n                 LowTerm       67.75      (2.8%)       67.11      (3.0%)   -0.9% (  -6% -    5%)\n                PKLookup      196.49      (1.0%)      196.24      (1.9%)   -0.1% (  -3% -    2%)\n\n\n\n(NOTE: base/comp are the same in each run, so ignore the differences w/in each run (it's noise) and compare absolute across the two runs ... ie HighTerm gets ~20.43 QPS with trunk but ~10.82 with patch).\n\nAlso: trunk took ~63 MB for the DV files while patch took ~84 MB.  Net/net I think postings compress better with PackedInts than facet ords (at least for these 9 facet fields I'm using in Wikipedia)... ",
            "author": "Michael McCandless",
            "id": "comment-13555517"
        },
        {
            "date": "2013-01-17T07:27:36+0000",
            "content": "How come the order of the lines in the two tables is different? At first I thought you misread the table because the results don't look that bad .\n\nYou can very easily push DGap into the encoder? But judging from LUCENE-4686, this got us there 6% improvement, so not likely it will double the decoding speed here.\nAlso, I see that you have a static MASK table. I wonder, given the recent thread about FixedBitSet and static mask tables (people were against it because a couple of bitwise ops would do better), if getting rid of it would improve perf.\n\nI think that the main problem with these ordinals encoding is that we encode small groups of integers, usually. So just for fun, I ran EncodingSpeed with 4 encoders/decoders, encoding 2430 values in one go (single call made to encode). The results are below:\n\n\nEncoder                            Bits/Int          Encode Time                Encode Time          Decode Time                Decode Time\n                                                   [milliseconds]        [microsecond / int]       [milliseconds]        [microsecond / int]\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\nSorting(Unique(DGap(PackedInts)))  11.0189                 8561                    85.6105                  590                     5.9000\nSorting(Unique(DGap(Packed)))      11.0058                 8206                    82.0605                  806                     8.0601\nSorting(Unique(DGap(VInt8)))        8.5597                 7133                    71.3305                  263                     2.6300\nSorting(Unique(DGapVInt8))          8.5597                 7208                    72.0805                  251                     2.5100\n\n\n\n\n\tPackedInts is an encoder similar to what Gilad wrote here, which uses PackedInts.Writer/ReaderNoHeader (writes a 1-byte header itself).\n\tPacked is Mike's encoder\n\tI wrapped all encoders with DGap (except for the last one which is specialized)\n\n\n\nNotice that:\n\n\tBoth Packed versions achieve worse compression than VInt (confirms Mike's DV files results above)\n\tBoth DGap perform much faster at decode that the Packed versions.\n\tSurprisingly, but this may be a result of micro-benchmkaring, Packed (Mike's specialized) decodes slower than PackedInts. I wonder if luceneutil would confirm that too.\n\n\n\nI think that if we had some statistics about the result Wikipedia index, e.g. min/max ord per document (the final integer, after dgap), we could tell better if there's any point to continue these investigations further. Since luceneutil search perf is happier w/ VInt and more so, the result index is smaller by ~25%, I tend to think that packed ints won't get us far here.\n\nWith postings, I think that the numbers are smaller? e.g. for dense posting lists, with docID gaps, you'll get a small bpv?\nMike, is it possible to print the min/max bpv used during the indexing? For instance, I printed that info when running EncodingSpeed, and got 11 (there's a single chunk to encode, so min=max). I wonder what the result will be for the index. I added these as public static members to PackedIntEncoder, so I think you can do the same for the index.\n\nAlso, 11 bpv means that every value is encoded int 1.5 bytes + 1 header byte. So for 25 ords, we're looking at ~39 bytes per document. If most of the values are small though (and this happens with ordinals, even with dgap, because the first value is the largest), we pay a high penalty for each value, because bpv will be large.\n\nMaybe in order to beat VInt we need a smarter encoder. E.g. the facets package have a FourFlags and EightFlags encoders which are very good for really low numbers (Four for 1,2,3 and Eight for 1). Gilad had an idea above to use a static bpv=6 and encode all values that are 1-63 as 0-62 in 6 bits, and values that are 64+ are encoded as 63 (all 1's) followed by a VInt. I ran a short analysis using EncodingSpeed (I should say here that these 2430 integers are actual category ordinals of a real document from one application, so these are not just made up numbers):\n\n\n\tmaxValue=1928 (requires 11 bits)\n\tnumValuesLessThan64=2169 (out of 2430!)\n\n\n\nSo if we had encoded using Gilad's idea, we would end up with\n\n\t261 \"large\" values, at most 2 bytes-per-value = 522 bytes (4176 bits)\n\t2169 values encoded with 6 bpv = 135.5625 longs = 1084.5 bytes (8676 bits)\n\tavg bits/int = 12852 / 2430 = 5.28\n\n\n\nWhich is better than VInt. I omitted the 1 byte header here, because of the large number of values that are encoded at once, but it should add some overhead for documents with very few categories (e.g. OrdinalPolicy.NO_PARENTS). Also, this is the \"best\" compression that we could get for these numbers, but if we really encoded them I suspect it would be worse. Because we cannot reorder after sorting+dgap, so e.g. if we have a series of 5-1 values (5 small, 1 large), we would waste 2 bits on the 5 small ones. 2 bits here, 2 bits there, they add up.\n\nStill, I wonder what would be the speed of such decoder. And what are the statistics for the real Wikipedia index, with its 25 ords per doc. Encoders like the one above greatly benefit when there are a large number of values (because there are more chances for small values). ",
            "author": "Shai Erera",
            "id": "comment-13555948"
        },
        {
            "date": "2013-01-22T16:11:31+0000",
            "content": "Here's another attempt (totally prototype / not committable) at using PackedInts to hold the ords ...\n\nIt's hacked up: it visits all byte[] from DocValues in the index and converts to in-RAM PackedInts arrays, and then does all facet counting from those arrays.\n\nBut, the performance is sort of 'meh':\n\n\n                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff\n                 MedTerm      109.40      (1.5%)      102.06      (1.5%)   -6.7% (  -9% -   -3%)\n              AndHighLow      374.95      (3.0%)      361.19      (2.6%)   -3.7% (  -8% -    1%)\n              AndHighMed      172.57      (1.5%)      169.35      (1.1%)   -1.9% (  -4% -    0%)\n                 Prefix3      177.54      (6.2%)      174.26      (8.0%)   -1.8% ( -15% -   13%)\n                  IntNRQ      116.07      (7.5%)      113.97      (9.3%)   -1.8% ( -17% -   16%)\n                  Fuzzy2       86.19      (2.4%)       85.16      (2.8%)   -1.2% (  -6% -    4%)\n             AndHighHigh       46.76      (1.4%)       46.36      (1.1%)   -0.8% (  -3% -    1%)\n                 LowTerm      146.56      (1.8%)      145.58      (1.4%)   -0.7% (  -3% -    2%)\n                HighTerm       26.35      (2.0%)       26.20      (2.1%)   -0.6% (  -4% -    3%)\n             MedSpanNear       64.98      (2.3%)       64.62      (2.8%)   -0.5% (  -5% -    4%)\n         LowSloppyPhrase       67.07      (2.3%)       66.80      (3.6%)   -0.4% (  -6% -    5%)\n               OrHighMed       25.18      (1.6%)       25.10      (2.1%)   -0.3% (  -3% -    3%)\n                Wildcard      256.33      (3.1%)      255.56      (3.5%)   -0.3% (  -6% -    6%)\n                PKLookup      305.42      (2.3%)      304.72      (2.1%)   -0.2% (  -4% -    4%)\n               OrHighLow       24.59      (1.3%)       24.54      (2.2%)   -0.2% (  -3% -    3%)\n                  Fuzzy1       81.38      (3.0%)       81.60      (2.7%)    0.3% (  -5% -    6%)\n                 Respell      141.17      (3.8%)      141.87      (3.9%)    0.5% (  -6% -    8%)\n             LowSpanNear       38.34      (3.2%)       38.78      (3.0%)    1.1% (  -4% -    7%)\n         MedSloppyPhrase       63.80      (2.1%)       64.53      (3.5%)    1.1% (  -4% -    6%)\n            HighSpanNear       10.20      (2.8%)       10.32      (3.1%)    1.2% (  -4% -    7%)\n               MedPhrase      103.16      (4.5%)      104.72      (2.1%)    1.5% (  -4% -    8%)\n              OrHighHigh       17.81      (1.5%)       18.18      (2.7%)    2.1% (  -2% -    6%)\n               LowPhrase       58.77      (5.5%)       60.49      (3.0%)    2.9% (  -5% -   12%)\n              HighPhrase       38.68     (10.0%)       40.46      (5.6%)    4.6% ( -10% -   22%)\n        HighSloppyPhrase        2.97      (7.9%)        3.22     (12.6%)    8.3% ( -11% -   31%)\n\n\n\n\nMaybe if I used the bulk read PackedInts APIs instead it would be better... ",
            "author": "Michael McCandless",
            "id": "comment-13559738"
        },
        {
            "date": "2013-01-22T18:17:06+0000",
            "content": "Perhaps this should belong to a different issue? I mean, this one is focused on a PackedInts encoder/decoder (or any other encoder/decoder that's better than VInt).\n\nSeparately, it's interesting that this performs worse than DV Source + decoding. I mean, the results don't factor in reading and populating the cache, right? The test is already \"hot\" when it's measured?\n\nI must say that I'm not entirely surprised ... having recently looked at PackedInts API, it doesn't look so optimized (working w/ DataInput for example), where the dgap+vint that we have in CountingFC is very optimized. I think that what we need is a custom cache, which encodes things similar to PackedInts ... or maybe as you say, the bulk read methods would do better.\n\nBut we should explore that on another issue? ",
            "author": "Shai Erera",
            "id": "comment-13559833"
        },
        {
            "date": "2013-01-22T19:21:30+0000",
            "content": "Also, the on-disk size of the vInt(dGap) encoded ords is 63880 KB while the in-RAM packed ints size was 74501 KB.  Maybe if we block-coded the packed ints parts we'd get better compression ... ",
            "author": "Michael McCandless",
            "id": "comment-13559912"
        },
        {
            "date": "2013-01-22T19:25:08+0000",
            "content": "Perhaps this should belong to a different issue?\n\nI think this is the right issue to explore whether packed ints can be smaller/faster for facets?\n\nIe I think we should iterate on this prototype/specialized collector, and if we don't see net gains with it then I don't think we should pursue packed ints encoder/decoder.  This serves as the litmus test. ",
            "author": "Michael McCandless",
            "id": "comment-13559913"
        },
        {
            "date": "2013-01-22T19:52:18+0000",
            "content": "On the DV 2.0 branch the on-disk size is 55288 KB (~16% smaller): cool! ",
            "author": "Michael McCandless",
            "id": "comment-13559943"
        },
        {
            "date": "2013-01-22T20:47:10+0000",
            "content": "and if we don't see net gains with it then I don't think we should pursue packed ints encoder/decoder\n\nThat's right. But if we'll see net gains, it doesn't mean anything about how it will perform on small set of integers.\nThat's why I think this test has nothing to do w/ the Encoder/Decoder.\n\nBut I don't mind if this experiment is done here anyway. ",
            "author": "Shai Erera",
            "id": "comment-13560000"
        },
        {
            "date": "2013-01-22T20:56:56+0000",
            "content": "New prototype collector, this time using simple int[] instead of PackedInts.\n\nTrunk (base) vs prototype collector (comp):\n\n                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff\n                  IntNRQ      114.81      (6.2%)      112.35      (8.4%)   -2.1% ( -15% -   13%)\n                 Prefix3      176.77      (4.7%)      173.10      (7.4%)   -2.1% ( -13% -   10%)\n                Wildcard      254.90      (3.2%)      250.81      (3.3%)   -1.6% (  -7% -    5%)\n              AndHighLow      371.35      (2.6%)      366.23      (2.3%)   -1.4% (  -6% -    3%)\n                PKLookup      302.90      (1.7%)      299.45      (1.7%)   -1.1% (  -4% -    2%)\n                 Respell      143.44      (3.1%)      143.18      (3.4%)   -0.2% (  -6% -    6%)\n                  Fuzzy2       86.16      (2.0%)       88.32      (3.1%)    2.5% (  -2% -    7%)\n         LowSloppyPhrase       67.41      (1.8%)       69.45      (2.9%)    3.0% (  -1% -    7%)\n             LowSpanNear       37.85      (2.6%)       39.38      (3.0%)    4.0% (  -1% -    9%)\n            HighSpanNear       10.19      (2.6%)       10.62      (3.2%)    4.2% (  -1% -   10%)\n                 MedTerm      111.19      (1.4%)      117.18      (1.6%)    5.4% (   2% -    8%)\n                  Fuzzy1       83.60      (2.5%)       88.65      (2.8%)    6.0% (   0% -   11%)\n              AndHighMed      171.63      (1.4%)      182.81      (2.0%)    6.5% (   3% -   10%)\n             MedSpanNear       64.59      (2.0%)       69.13      (2.1%)    7.0% (   2% -   11%)\n               LowPhrase       57.89      (5.3%)       63.54      (4.5%)    9.8% (   0% -   20%)\n              HighPhrase       37.97     (11.0%)       41.79      (8.3%)   10.1% (  -8% -   32%)\n         MedSloppyPhrase       63.51      (2.0%)       70.31      (3.2%)   10.7% (   5% -   16%)\n                 LowTerm      145.85      (1.5%)      169.28      (1.6%)   16.1% (  12% -   19%)\n        HighSloppyPhrase        2.97      (8.4%)        3.47     (12.4%)   16.6% (  -3% -   40%)\n             AndHighHigh       46.49      (1.0%)       54.30      (1.2%)   16.8% (  14% -   19%)\n               MedPhrase      101.99      (4.1%)      128.31      (4.7%)   25.8% (  16% -   36%)\n               OrHighMed       24.97      (1.7%)       35.04      (3.6%)   40.3% (  34% -   46%)\n                HighTerm       26.22      (1.2%)       37.55      (3.6%)   43.2% (  38% -   48%)\n               OrHighLow       24.31      (1.5%)       34.89      (3.8%)   43.5% (  37% -   49%)\n              OrHighHigh       17.72      (1.4%)       26.44      (4.5%)   49.3% (  42% -   55%)\n\n\n\nSo this is at least good news ... it means if we can speed up decode there are gain to be had ... but RAM usage is now 105231 KB (hmm not THAT much larger than 63880 KB ... interesting). ",
            "author": "Michael McCandless",
            "id": "comment-13560012"
        },
        {
            "date": "2013-01-22T21:00:24+0000",
            "content": "\nThat's right. But if we'll see net gains, it doesn't mean anything about how it will perform on small set of integers.\nThat's why I think this test has nothing to do w/ the Encoder/Decoder.\n\nAhh, I see: you're right.\n\nIf this prototype collector is faster it's not clear how we'd \"productize\" it.  Maybe as multi-valued DV (int[] per doc), which could then use big packed ints array under the hood or something ... ",
            "author": "Michael McCandless",
            "id": "comment-13560014"
        },
        {
            "date": "2013-01-22T21:32:55+0000",
            "content": "The above results were 1M index; here's the full wikipedia en (6.6M docs) results:\n\n                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff\n            HighSpanNear        2.91      (2.1%)        2.90      (2.4%)   -0.6% (  -5% -    4%)\n                 Prefix3       46.35      (4.0%)       46.07      (3.9%)   -0.6% (  -8% -    7%)\n                PKLookup      240.11      (1.4%)      238.95      (1.9%)   -0.5% (  -3% -    2%)\n                Wildcard       73.79      (2.2%)       73.48      (2.3%)   -0.4% (  -4% -    4%)\n                  IntNRQ       18.05      (6.1%)       18.01      (5.9%)   -0.2% ( -11% -   12%)\n                 Respell       96.78      (3.1%)       98.09      (3.3%)    1.3% (  -4% -    7%)\n         LowSloppyPhrase       17.63      (4.4%)       17.91      (3.8%)    1.6% (  -6% -   10%)\n              AndHighLow      108.80      (2.8%)      110.58      (4.2%)    1.6% (  -5% -    8%)\n             LowSpanNear        7.53      (4.8%)        7.67      (5.6%)    1.8% (  -8% -   12%)\n        HighSloppyPhrase        0.87     (10.1%)        0.90      (9.6%)    3.2% ( -14% -   25%)\n                  Fuzzy2       42.22      (2.5%)       43.90      (2.7%)    4.0% (  -1% -    9%)\n              HighPhrase       15.32      (7.5%)       15.93      (5.4%)    4.0% (  -8% -   18%)\n               LowPhrase       17.09      (4.3%)       18.10      (2.9%)    5.9% (  -1% -   13%)\n              AndHighMed       52.60      (1.4%)       55.90      (2.1%)    6.3% (   2% -    9%)\n             MedSpanNear       20.09      (2.0%)       21.44      (1.8%)    6.7% (   2% -   10%)\n         MedSloppyPhrase       18.69      (3.0%)       20.00      (2.7%)    7.0% (   1% -   13%)\n                  Fuzzy1       33.68      (2.0%)       37.26      (2.2%)   10.6% (   6% -   15%)\n               MedPhrase       57.00      (2.9%)       63.56      (3.3%)   11.5% (   5% -   18%)\n                 MedTerm       19.22      (1.2%)       21.70      (1.1%)   12.9% (  10% -   15%)\n                 LowTerm       41.98      (1.2%)       48.26      (1.8%)   15.0% (  11% -   18%)\n             AndHighHigh       12.09      (1.0%)       13.98      (1.2%)   15.7% (  13% -   18%)\n                HighTerm        7.11      (2.1%)        9.11      (2.0%)   28.1% (  23% -   32%)\n               OrHighMed        6.67      (2.4%)        8.55      (2.1%)   28.2% (  23% -   33%)\n               OrHighLow        6.76      (2.1%)        8.70      (2.3%)   28.6% (  23% -   33%)\n              OrHighHigh        3.84      (2.5%)        5.33      (2.7%)   38.7% (  32% -   45%)\n\n\n\nOn-disk size of _dv* is 464768 KB and in memory int[] is 669428 KB (44% more).\n\nNext I'll try NO_PARENTS ord policy... ",
            "author": "Michael McCandless",
            "id": "comment-13560048"
        },
        {
            "date": "2013-01-22T22:44:46+0000",
            "content": "Ugh!  My DV total bytes numbers were too high: luceneutil also indexes\ntitle field as DV.  So ignore past byte sizes ... here's the [correct,\nI hope!] byte sizes for the NO_PARENTS case, full 6.6M Wikipedia en\nindex: DV (index) 151208 KB, int[] (in RAM): 305889 KB.  And\nNO_PARENTS perf (base = trunk, comp = int[] collector):\n\n\n                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff\n                Wildcard       74.70      (3.3%)       74.32      (1.9%)   -0.5% (  -5% -    4%)\n                PKLookup      245.87      (1.8%)      244.80      (2.0%)   -0.4% (  -4% -    3%)\n              HighPhrase       15.68      (5.7%)       15.72      (6.4%)    0.2% ( -11% -   12%)\n                 Respell      111.09      (3.5%)      111.33      (3.7%)    0.2% (  -6% -    7%)\n              AndHighLow       97.90      (1.6%)       98.16      (1.4%)    0.3% (  -2% -    3%)\n             LowSpanNear        7.62      (3.8%)        7.67      (3.5%)    0.7% (  -6% -    8%)\n                 Prefix3       45.94      (5.6%)       46.34      (2.7%)    0.9% (  -6% -    9%)\n                  IntNRQ       18.04      (8.2%)       18.20      (4.6%)    0.9% ( -11% -   14%)\n         LowSloppyPhrase       17.77      (2.9%)       17.94      (4.8%)    1.0% (  -6% -    8%)\n                  Fuzzy2       41.36      (2.4%)       42.68      (2.3%)    3.2% (  -1% -    8%)\n               LowPhrase       16.94      (2.4%)       17.65      (3.5%)    4.1% (  -1% -   10%)\n            HighSpanNear        2.98      (2.8%)        3.14      (2.1%)    5.3% (   0% -   10%)\n              AndHighMed       49.18      (1.0%)       51.97      (0.7%)    5.7% (   3% -    7%)\n        HighSloppyPhrase        0.90      (6.7%)        0.97     (12.6%)    6.8% ( -11% -   27%)\n         MedSloppyPhrase       18.54      (1.8%)       19.91      (3.0%)    7.4% (   2% -   12%)\n             MedSpanNear       19.86      (1.6%)       21.36      (2.0%)    7.5% (   3% -   11%)\n               MedPhrase       55.57      (2.2%)       60.31      (2.3%)    8.5% (   3% -   13%)\n                  Fuzzy1       33.38      (1.4%)       37.19      (1.9%)   11.4% (   8% -   14%)\n             AndHighHigh       12.58      (1.2%)       14.66      (0.9%)   16.6% (  14% -   18%)\n                 LowTerm       40.41      (1.2%)       47.14      (1.4%)   16.6% (  13% -   19%)\n                 MedTerm       23.00      (1.4%)       27.14      (3.0%)   18.0% (  13% -   22%)\n               OrHighMed        7.50      (2.2%)       10.16      (2.3%)   35.6% (  30% -   40%)\n               OrHighLow        7.55      (2.0%)       10.30      (2.8%)   36.3% (  30% -   41%)\n                HighTerm        7.92      (1.9%)       10.98      (2.8%)   38.6% (  33% -   44%)\n              OrHighHigh        4.30      (2.7%)        6.39      (3.0%)   48.6% (  41% -   55%)\n\n ",
            "author": "Michael McCandless",
            "id": "comment-13560129"
        },
        {
            "date": "2013-01-26T20:57:23+0000",
            "content": "I tried to hack up a patch to make packed ints behave better for this use-case. I made byte[] -> int[] encoding and decoding byte-aligned (it was still long-aligned) and wrote a quick class which can decode an arbitrary number of packed ints at any offset. I've tried to run the benchmark but it looks like it didn't use this CountingFacetCollector (I added doFacets=True to my localrun.py), do I need to modify some code in the perf package? ",
            "author": "Adrien Grand",
            "id": "comment-13563631"
        },
        {
            "date": "2013-01-26T22:13:37+0000",
            "content": "Ooh that patch looks good!  Thanks Adrien.  I'll test it.\n\nDid you add doFacets=True to both the Index and the Competitor?  Also, you need to use a tasks file that adds +dateFacets or +allFacets to each task ... and if you want to use +allFacets you need to pull down the latest line file docs that has the added facet dimensions ... ",
            "author": "Michael McCandless",
            "id": "comment-13563654"
        },
        {
            "date": "2013-01-26T23:38:10+0000",
            "content": "I had to change the PackedBytes.get to take a [reused] IntsRef in, else I was hitting thread-safety issues (AIOOBE)...\n\nbase = trunk, comp = patch, index = full 6.6M Wikpedia English with 9 facet dims counted:\n\n\n                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff\n                HighTerm        7.30      (1.5%)        7.60      (1.6%)    4.1% (   0% -    7%)\n                 MedTerm       16.22      (0.7%)       17.25      (0.8%)    6.4% (   4% -    7%)\n                 LowTerm       37.87      (0.8%)       41.08      (0.4%)    8.5% (   7% -    9%)\n\n\n\nSo we finally have something faster than dGap(vInt)!\n\nBut PackedInts takes ~2X the storage (base is 151208 KB, by summing DV on disk; comp is 305889 KB by measuring RAM of the PackedInts structures). ",
            "author": "Michael McCandless",
            "id": "comment-13563685"
        },
        {
            "date": "2013-01-26T23:54:15+0000",
            "content": "Also, you need to use a tasks file that adds +dateFacets or +allFacets to each task ... and if you want to use +allFacets you need to pull down the latest line file docs that has the added facet dimensions ...\n\nOK, this is what I was missing. \n\nI had to change the PackedBytes.get to take a [reused] IntsRef in, else I was hitting thread-safety issues (AIOOBE)...\n\nOops, I didn't know it would be called from multiple threads.\n\nSo we finally have something faster than dGap(vInt)!\n\nGood news! However the patch has a nocommit because it uses a byte[] to store data, so it cannot grow beyond 2G. I hope that the paging won't make it too much slower. (But maybe it could help reduce memory usage, if each page can have a different number of bits per value?)\n\nI think I'll open a separate issue to make encoding to byte[] and decoding from byte[] byte-aligned (it is long-aligned today). I ran a benchmark with the Lucene41 PF and the deltas were small (probably noise). ",
            "author": "Adrien Grand",
            "id": "comment-13563689"
        },
        {
            "date": "2013-01-28T14:54:26+0000",
            "content": "Thanks Adrien. From what I can tell, you implemented a PackedInts version of what used to be CategoryListCache (per-document ordinals). The patch has many changes to what I think is unrelated to facets code, but I get the basic idea. So if we had a CategoryListCache interface, we'd have several implementations thus far: StraightIntsCache (Mike's int[] version with offsets), PackedIntsCache (regular PackedInts) and AdriensEfficientPackedIntsCache (your version ). Right?\n\nAlso, this issue started in order to explore an alternative encoder/decoder for per-document category ordinals. Is it ok to conclude that none seemed more efficient than dgap+vint?\n\nRegarding caching, it seems that the Straight impl beats everything so far, even packed-ints? I mean, it achieved 50% gains for some queries, while comparing HighTerm of both versions, straight achieves 38%, while packed only 4%. Yet both straight and packed versions consume exactly the same amount of RAM, so there's no real tradeoff here. It doesn't look like there's any advantage to using the packed version? ",
            "author": "Shai Erera",
            "id": "comment-13564314"
        },
        {
            "date": "2013-02-07T15:27:39+0000",
            "content": "Finally figured out I was doing things completely wrong.. instead of having a super smart optimizing code for semi-packed encoder - there's now a strait forward semi-packed encoder:\nValues smaller than 256 use only one byte (the value itself) and larger values are encoded as VInt plus a leading zero byte. Worst case it can be 6 bytes per value (zero marker + 5 of VInt).\n\nThe idea, is to pay the penalty for variable length encoding for large values which should be less common in a sort-uniq-dgap scenario.\n\nWrote two versions w and w/o dgap specialization, though I'm not sure how useful is the non-specialized code.\n\nI do not currently have the means to run the LuceneUtil (nor the wikipedia index with the categories) - but I ran the EncodingSpeed test - and was surprised.\nWhile the encoding is on a little worse (or on par) with dgap-vint, the decoding speed is significantly faster. The new encode is the only (?!) encoder to beat SimpleIntEncoder (which writes plain 32 bits per value) in decoding time.\n\nThose values being used in the EncodingSpeed are real scenario, but I'm not sure how much they represent a \"common\" case (e.g wikipedia). \n\nMike - could you please try this encoder? I guess it only makes sense to run the specialized DGapSemiPackedEncoder.\nAlso, I'm not sure SimpleIntEncoder was ever used (without any sorting, or unique). It would be interesting to test it as well. We will pay in more I/O and much larger file size (~4 times larger..) but it doesn't mean it will be any slower.\n\nHere are the results of the EncodingSpeed test:\n\nEstimating ~100000000 Integers compression time by\nEncoding/decoding facets' ID payload of docID = 3630 (unsorted, length of: 2430) 41152 times.\n\nEncoder                                                        Bits/Int          Encode Time                Encode Time          Decode Time                Decode Time\n                                                                              [milliseconds]        [microsecond / int]       [milliseconds]        [microsecond / int]\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\nSimple                                                          32.0000                  190                     1.9000                  165                     1.6500\nVInt8                                                           18.4955                  436                     4.3600                  359                     3.5900\nSorting(Unique(VInt8))                                          18.4955                 3557                    35.5702                  314                     3.1400\nSorting(Unique(DGap(VInt8)))                                     8.5597                 3485                    34.8502                  270                     2.7000\nSorting(Unique(DGapVInt8))                                       8.5597                 3434                    34.3402                  192                     1.9200\nSorting(Unique(DGap(SemiPacked)))                                8.6453                 3386                    33.8602                  156                     1.5600\nSorting(Unique(DGapSemiPacked))                                  8.6453                 3397                    33.9702                   99                     0.9900\nSorting(Unique(DGap(EightFlags(VInt))))                          4.9679                 4002                    40.0203                  381                     3.8100\nSorting(Unique(DGap(FourFlags(VInt))))                           4.8198                 3972                    39.7203                  399                     3.9900\nSorting(Unique(DGap(NOnes(3) (FourFlags(VInt)))))                4.5794                 4448                    44.4803                  645                     6.4500\nSorting(Unique(DGap(NOnes(4) (FourFlags(VInt)))))                4.5794                 4461                    44.6103                  641                     6.4100\n\n\nEstimating ~100000000 Integers compression time by\nEncoding/decoding facets' ID payload of docID = 9910 (unsorted, length of: 1489) 67159 times.\n\nEncoder                                                        Bits/Int          Encode Time                Encode Time          Decode Time                Decode Time\n                                                                              [milliseconds]        [microsecond / int]       [milliseconds]        [microsecond / int]\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\nSimple                                                          32.0000                  169                     1.6900                  163                     1.6300\nVInt8                                                           18.2673                  421                     4.2100                  374                     3.7400\nSorting(Unique(VInt8))                                          18.2673                 2230                    22.3001                  337                     3.3700\nSorting(Unique(DGap(VInt8)))                                     8.9456                 2257                    22.5701                  273                     2.7300\nSorting(Unique(DGapVInt8))                                       8.9456                 2214                    22.1401                  192                     1.9200\nSorting(Unique(DGap(SemiPacked)))                                9.2787                 2162                    21.6201                  180                     1.8000\nSorting(Unique(DGapSemiPacked))                                  9.2787                 2148                    21.4801                  120                     1.2000\nSorting(Unique(DGap(EightFlags(VInt))))                          5.7542                 2937                    29.3701                  395                     3.9500\nSorting(Unique(DGap(FourFlags(VInt))))                           5.5447                 2768                    27.6801                  407                     4.0700\nSorting(Unique(DGap(NOnes(3) (FourFlags(VInt)))))                5.3566                 3294                    32.9401                  651                     6.5100\nSorting(Unique(DGap(NOnes(4) (FourFlags(VInt)))))                5.3996                 3318                    33.1801                  662                     6.6200\n\n\nEstimating ~100000000 Integers compression time by\nEncoding/decoding facets' ID payload of docID = 10000 (unsorted, length of: 18) 5555555 times.\n\nEncoder                                                        Bits/Int          Encode Time                Encode Time          Decode Time                Decode Time\n                                                                              [milliseconds]        [microsecond / int]       [milliseconds]        [microsecond / int]\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\nSimple                                                          32.0000                  316                     3.1600                  318                     3.1800\nVInt8                                                           20.8889                  569                     5.6900                  496                     4.9600\nSorting(Unique(VInt8))                                          20.8889                 1196                    11.9600                  488                     4.8800\nSorting(Unique(DGap(VInt8)))                                    12.0000                 1151                    11.5100                  481                     4.8100\nSorting(Unique(DGapVInt8))                                      12.0000                 1100                    11.0000                  352                     3.5200\nSorting(Unique(DGap(SemiPacked)))                               14.6667                 1107                    11.0700                  507                     5.0700\nSorting(Unique(DGapSemiPacked))                                 14.6667                 1037                    10.3700                  439                     4.3900\nSorting(Unique(DGap(EightFlags(VInt))))                         10.2222                 1315                    13.1500                  656                     6.5600\nSorting(Unique(DGap(FourFlags(VInt))))                          10.2222                 1408                    14.0800                  675                     6.7500\nSorting(Unique(DGap(NOnes(3) (FourFlags(VInt)))))                9.7778                 1617                    16.1700                  990                     9.9000\nSorting(Unique(DGap(NOnes(4) (FourFlags(VInt)))))               10.2222                 1708                    17.0800                  992                     9.9200\n\n\nEstimating ~100000000 Integers compression time by\nEncoding/decoding facets' ID payload of docID = 501871 (unsorted, length of: 957) 104493 times.\n\nEncoder                                                        Bits/Int          Encode Time                Encode Time          Decode Time                Decode Time\n                                                                              [milliseconds]        [microsecond / int]       [milliseconds]        [microsecond / int]\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------\nSimple                                                          32.0000                  181                     1.8100                  168                     1.6800\nVInt8                                                           16.5768                  428                     4.2800                  351                     3.5100\nSorting(Unique(VInt8))                                          16.5768                 1586                    15.8600                  330                     3.3000\nSorting(Unique(DGap(VInt8)))                                     8.4848                 1477                    14.7700                  267                     2.6700\nSorting(Unique(DGapVInt8))                                       8.4848                 1435                    14.3500                  193                     1.9300\nSorting(Unique(DGap(SemiPacked)))                                8.6771                 1424                    14.2400                  159                     1.5900\nSorting(Unique(DGapSemiPacked))                                  8.6771                 1402                    14.0200                  100                     1.0000\nSorting(Unique(DGap(EightFlags(VInt))))                          4.4138                 1603                    16.0300                  376                     3.7600\nSorting(Unique(DGap(FourFlags(VInt))))                           4.1797                 1700                    17.0000                  382                     3.8200\nSorting(Unique(DGap(NOnes(3) (FourFlags(VInt)))))                3.8955                 2011                    20.1100                  602                     6.0200\nSorting(Unique(DGap(NOnes(4) (FourFlags(VInt)))))                3.8871                 2024                    20.2400                  594                     5.9400\n\n\n\n\n\n\nI have another version which uses two bytes before using the variable length (e.g values smaller than 0x10000 are encoded as is in two bytes, other values are encoded as two leading zero bytes and the VInt) - but I did not optimize it yet, nor I'm sure it's very useful.  ",
            "author": "Gilad Barkai",
            "id": "comment-13573590"
        },
        {
            "date": "2013-02-07T16:14:10+0000",
            "content": "That will be interesting to test. In order to test it \"fairly\", we should either test the decoder (that's what we usually test) through the abstracted code (i.e. via CategoryListIterator), or Gilad, if you can, please copy CountingFacetsCollector and inline the decoder code instead of the dgap+vint code? That will be simpler to test, with the least noise.\n\nI'm not sure SimpleIntEncoder was ever used\n\nMike and I tested it ... at some point . I don't remember where we posted the results though, whether it was in email, GTalk or some issue. But I do remember that the results were less good than DGapVInt. We were always surprised by how fast DGapVInt is .. all the while we thought VInt is expensive, but it may not be so expensive ... at least not on the Wikipedia collection.\n\nthe decoding speed is significantly faster\n\nThat's good, but Mike and I have already concluded that EncodingSpeed just .. lies . It's a micro-benchmark, and while it showed significant improvements after I moved the encoders to bulk-API, on the real-world scenario it performed worse. I had to inline stuff and specialize it even more for it to beat the previous way things worked.\n\nI will be glad if SemiPacked is faster .. but judging from past experience, I don't get my hopes too high .\n\nAs for this encoding algorithm, it all depends on how many values actually fall into the 256 range. That's another problem w/ EncodingSpeed \u2013 it uses a real-world scenario of a crazy application which encoded 2430 ordinals for a single document! You can see that the values that are encoded are small, by e.g. looking at the NOnes bits/int. I suspect that in real-life, there won't be many values that fall into that range, at least after some documents have been indexed, because when you have a single category per-dimension in a document, then there are not too many chances that their values will be \"close\".\n\nBut .. we should let luceneutil be the judge of that . So Gilad, can you make a patch with a SemiPackedCountingCollector? And also modify the default that FacetCollector.create returns, so that it's easy to compare base (CountinFC) to comp (SemiPackedCFC). If you want to test the collector, then run TestDemoFacets (as-is) and CountingFCTest (modify the collector though) to make sure the Collector works. ",
            "author": "Shai Erera",
            "id": "comment-13573633"
        },
        {
            "date": "2013-02-07T16:24:10+0000",
            "content": "Few comments about the patch:\n\n\n\tinstead of iterating from 0 to length and doing values.ints[values.offset + i], you can compute 'upto' once and iterate from values.offset to upto?\n\n\n\n\n\tis if (v & 0xFF) == 0) better than if (v <= 256)? we could test of course ...\n\n\n\n\n\tI think that doing v - 256 is a good idea? You anyway do dgap, so what's another '-' (and '+' in decoder)?\n\n\n\n\n\tMaybe add a test to EncodingTest? Just to be sure it passes the random testing?\n\n ",
            "author": "Shai Erera",
            "id": "comment-13573643"
        },
        {
            "date": "2013-02-08T13:35:22+0000",
            "content": "OK the new format doesn't do very well.  This is all wikipedia (6.6M \"big\" docs), 7 facet dims:\n\n\n                    Task    QPS base      StdDev    QPS comp      StdDev                Pct diff\n                 MedTerm       46.85      (2.4%)       28.22      (0.7%)  -39.8% ( -41% -  -37%)\n                HighTerm       19.09      (2.5%)       12.27      (0.9%)  -35.7% ( -38% -  -33%)\n               OrHighLow       16.83      (2.8%)       11.21      (1.0%)  -33.4% ( -36% -  -30%)\n               OrHighMed       16.35      (2.8%)       11.00      (1.0%)  -32.7% ( -35% -  -29%)\n                 Prefix3       12.87      (2.8%)        8.81      (0.9%)  -31.5% ( -34% -  -28%)\n                Wildcard       27.22      (2.2%)       18.68      (0.7%)  -31.4% ( -33% -  -29%)\n                 LowTerm      110.58      (1.8%)       79.25      (0.6%)  -28.3% ( -30% -  -26%)\n              OrHighHigh        8.61      (2.9%)        6.19      (1.3%)  -28.1% ( -31% -  -24%)\n                  IntNRQ        3.54      (2.9%)        2.55      (1.2%)  -27.9% ( -31% -  -24%)\n             AndHighHigh       23.19      (1.4%)       17.67      (0.7%)  -23.8% ( -25% -  -22%)\n                  Fuzzy1       46.94      (1.7%)       40.34      (1.6%)  -14.1% ( -17% -  -10%)\n               MedPhrase      110.00      (5.6%)       98.08      (4.2%)  -10.8% ( -19% -   -1%)\n         MedSloppyPhrase       25.93      (2.5%)       23.37      (1.6%)   -9.9% ( -13% -   -5%)\n             MedSpanNear       28.43      (2.5%)       25.68      (1.2%)   -9.7% ( -13% -   -6%)\n              AndHighMed      105.06      (0.9%)       95.74      (1.0%)   -8.9% ( -10% -   -7%)\n               LowPhrase       21.26      (6.2%)       19.86      (5.3%)   -6.6% ( -16% -    5%)\n            HighSpanNear        3.53      (2.0%)        3.30      (1.2%)   -6.5% (  -9% -   -3%)\n                  Fuzzy2       52.61      (2.6%)       49.64      (2.5%)   -5.6% ( -10% -    0%)\n              HighPhrase       17.44     (10.2%)       16.66      (9.5%)   -4.5% ( -21% -   16%)\n        HighSloppyPhrase        0.92      (7.3%)        0.88      (5.7%)   -4.5% ( -16% -    9%)\n         LowSloppyPhrase       20.28      (3.1%)       19.59      (2.0%)   -3.4% (  -8% -    1%)\n                 Respell       46.30      (3.2%)       45.27      (3.4%)   -2.2% (  -8% -    4%)\n             LowSpanNear        8.36      (2.8%)        8.20      (1.9%)   -1.9% (  -6% -    2%)\n              AndHighLow      578.66      (3.0%)      569.71      (3.1%)   -1.5% (  -7% -    4%)\n\n\n\nAlso it's quite a bit more RAM / disk consuming: 306 MB of .dvm/d files on disk vs 178 MB for trunk (and remember that part of this is the title SortedDV field. ",
            "author": "Michael McCandless",
            "id": "comment-13574481"
        },
        {
            "date": "2013-02-08T13:42:09+0000",
            "content": "Well ... this encoding may be better if really you had many more values in the range 128-256 (which w/ vint are encoded w/ 2 bytes). But that seems arbitrary .. why those values?\n\nI am not sure if we should keep this encoder, b/c it consumes more space and even in EncodingSpeed, which indexes categories that are very close to each other, vint achieved slightly better compression.\n\nI am not sure anymore that for an arbitrary set of integers, we can do better than VInt (well, just b/c of all the attempts we've had ). ",
            "author": "Shai Erera",
            "id": "comment-13574484"
        },
        {
            "date": "2013-07-21T12:14:49+0000",
            "content": "Closing as \"Not A Problem\". All the attempts didn't show that there's a better encoding than VInt for categories, because their indexing nature (including range of values) is unexpected. VInt is fast and achieves good compression. We can re-open it in the future if we want. ",
            "author": "Shai Erera",
            "id": "comment-13714690"
        }
    ]
}