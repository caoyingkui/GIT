{
    "id": "SOLR-8371",
    "title": "Try and prevent too many recovery requests from stacking up and clean up some faulty logic.",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [
            "5.5",
            "6.0"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "",
    "attachments": {
        "SOLR-8371.patch": "https://issues.apache.org/jira/secure/attachment/12775798/SOLR-8371.patch",
        "SOLR-8371-2.patch": "https://issues.apache.org/jira/secure/attachment/12779585/SOLR-8371-2.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-12-04T16:54:33+0000",
            "author": "Mark Miller",
            "content": "patch ive started playing with attatched ",
            "id": "comment-15041762"
        },
        {
            "date": "2015-12-09T02:53:55+0000",
            "author": "Mark Miller",
            "content": "This is a lot uglier than it once was. I don't think cancel is actually async in this case, so I'm not sure that we are bailing on recovery early to try the next one, and we have the 10 sec between recoveries throttle. If you stack up a bunch of them, it can take a long time to unwind.\n\nNot sure this patch is quite right yet, but trying to make it so that if cancel is actually async and if there is a recovery attempt waiting and more come in, we try and only do one or as few as we can. ",
            "id": "comment-15047919"
        },
        {
            "date": "2015-12-17T14:22:50+0000",
            "author": "Mark Miller",
            "content": "Updated patch, fixed to work correctly. ",
            "id": "comment-15062097"
        },
        {
            "date": "2015-12-17T15:32:52+0000",
            "author": "Mark Miller",
            "content": "And another pass to make parts that were async, async again. ",
            "id": "comment-15062191"
        },
        {
            "date": "2015-12-17T17:01:00+0000",
            "author": "Mark Miller",
            "content": "Still doing test runs to see if anything random falls out, but I think the latest patch is good. ",
            "id": "comment-15062336"
        },
        {
            "date": "2015-12-17T20:50:39+0000",
            "author": "Mark Miller",
            "content": "New patch. We have actually been using the update executor for recovery threads - those threads can actually lead to IO (in tests im mostly seeing it as jmx getStats calls on close) and we now interrupt the update executor. I've made a new 'recoveryExecutor' to handle the main Recovery threads. ",
            "id": "comment-15062780"
        },
        {
            "date": "2015-12-18T04:01:05+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1720718 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1720718 ]\n\nSOLR-8371: Try and prevent too many recovery requests from stacking up and clean up some faulty cancel recovery logic. ",
            "id": "comment-15063388"
        },
        {
            "date": "2015-12-18T04:02:35+0000",
            "author": "Mark Miller",
            "content": "I'll subject this to Jenkins for a bit before backport. Reviews welcome. ",
            "id": "comment-15063391"
        },
        {
            "date": "2015-12-18T04:05:23+0000",
            "author": "Mark Miller",
            "content": "The future could be a local variable now instead of a field. ",
            "id": "comment-15063395"
        },
        {
            "date": "2015-12-21T14:00:39+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1721158 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1721158 ]\n\nSOLR-8371: The Future field should now be a local variable. ",
            "id": "comment-15066470"
        },
        {
            "date": "2015-12-26T21:35:20+0000",
            "author": "Ramkumar Aiyengar",
            "content": "Looks good functionally. But I don't fully understand why you need the locking/queue-checks.. Can't you have a bounded recovery queue (of say 1 or 2 \u2013 not sure of the semantics), and just return if the queue limit is exceeded? ",
            "id": "comment-15071983"
        },
        {
            "date": "2015-12-27T13:16:56+0000",
            "author": "Mark Miller",
            "content": "Perhaps, I have not worked through it. I came more from an approach of molding what was there, going down a couple paths until I hit something that worked right. Anyhow, I'd be happy to examine an alternate impl patch, but my effort has been on testing and is still on working out any changes in behavior that are affecting our tests on trunk (either because of this or to rule this out). ",
            "id": "comment-15072152"
        },
        {
            "date": "2015-12-27T15:39:02+0000",
            "author": "Ramkumar Aiyengar",
            "content": "Here's a patch of what I meant, on top of your changes. I haven't tested it extensively though, beyond a run or two of the existing TestRecovery.. ",
            "id": "comment-15072173"
        },
        {
            "date": "2016-01-21T18:58:22+0000",
            "author": "Mark Miller",
            "content": "Thanks, I'll merge this back to 5x and take a look. ",
            "id": "comment-15111096"
        },
        {
            "date": "2016-01-21T19:14:12+0000",
            "author": "Mark Miller",
            "content": "Here's a patch of what I meant, on top of your changes. I haven't tested it extensively though, beyond a run or two of the existing TestRecovery..\n\nHmm...perhaps I'm missing something, I just browsed over it, but does this fit the requirements?\n\nWhen a recovery is running, we don't want to drop any recovery attempts necessarily. We need the last recovery request to actually trigger a recovery, not just be sure one is in progress. ",
            "id": "comment-15111124"
        },
        {
            "date": "2016-01-21T19:49:14+0000",
            "author": "Mark Miller",
            "content": "We also want, if a 1000 recovery requests come in really quick, for most of them to dropped - but we still need to be sure a recovery starts after the last one hits. This looks like it won't drop recoveries? It will block and execute all of them? ",
            "id": "comment-15111185"
        },
        {
            "date": "2016-01-21T19:52:59+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1726076 from Mark Miller in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1726076 ]\n\nSOLR-8371: Try and prevent too many recovery requests from stacking up and clean up some faulty cancel recovery logic. ",
            "id": "comment-15111191"
        },
        {
            "date": "2016-01-21T19:54:08+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1726077 from Mark Miller in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1726077 ]\n\nSOLR-8371: The Future field should now be a local variable. ",
            "id": "comment-15111193"
        },
        {
            "date": "2016-01-21T19:55:14+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1726079 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1726079 ]\n\nSOLR-8371: Move 6.0 changes entry to 5.5 ",
            "id": "comment-15111196"
        },
        {
            "date": "2016-01-28T09:53:17+0000",
            "author": "Stephan Lagraulet",
            "content": "Can you update Fix Version to 5.5 if it is going to be shipped with this version ? ",
            "id": "comment-15121133"
        },
        {
            "date": "2016-02-01T10:10:46+0000",
            "author": "Stephan Lagraulet",
            "content": "I'm trying to gather all issues related to SolrCloud that affects Solr 5.4. Can you affect SolrCloud component to this issue ? ",
            "id": "comment-15126046"
        },
        {
            "date": "2016-02-12T22:39:35+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Mark Miller \u2013 Looking at the following code in doRecovery:\n\n          // if we can't get the lock, another recovery is running\n          // we check to see if there is already one waiting to go\n          // after the current one, and if there is, bail\n          boolean locked = recoveryLock.tryLock();\n          try {\n            if (!locked) {\n              if (recoveryWaiting.get() > 0) {\n                return;\n              }\n              recoveryWaiting.incrementAndGet();\n            } else {\n              recoveryWaiting.incrementAndGet();\n              cancelRecovery();\n            }\n            \n            recoveryLock.lock();\n            try {\n              recoveryWaiting.decrementAndGet();\n              ...\n              ...\n\n\n\nIn the case where the tryLock fails, you bail out if the recoveryWaiting > 0 but in case it is not \u2013 we should increment recoveryWaiting and then again bail out if recoveryWaiting > 1 (and decrement recoveryWaiting). The idea is to run only the latest recovery request which has come in and no more. What do you think? ",
            "id": "comment-15145457"
        },
        {
            "date": "2016-02-13T00:06:06+0000",
            "author": "Keith Laban",
            "content": "I see that this part of the code is being ported from the previous implementation, but, what are the effects of canceling recovery and then throttling the next recovery? Would it be more efficient to let to original recovery finish and have the next pending recovery fired once the original one is done? It seems counter intuitive to cancel the current progress then wait to start it again if the throttling strategy says so. With these changes there should always be one pending recovery as long there has been a recovery request since the currently running one has started.\n\nMy depth of knowledge here is pretty limited so I'm not sure if finishing current recovery and then picking up the missing pieces would be better than stopping and starting, just throwing in some thoughts. ",
            "id": "comment-15145596"
        },
        {
            "date": "2016-02-13T02:08:22+0000",
            "author": "Mark Miller",
            "content": "Yeah, I think we can work on improvements but let's open some new JIRA's and relate them. Gregory Chanan and Ramkumar Aiyengar have also both expressed some ideas. These changes take a fair amount of testing and time, and this improvement over what we had just went out in 5.5. I'll help implement, test, and make any further ideas in new improvement JIRAs.\n\nthrottling strategy\n\nRecovery throttling was put in as a stop gap to handle recovery stack up overload. We should reconsider it and it's implementation in light of these other improvements. ",
            "id": "comment-15145714"
        },
        {
            "date": "2016-02-13T02:13:41+0000",
            "author": "Mark Miller",
            "content": "more efficient to let to original recovery finish\n\nI think it really depends. If you cancel before doing what could be a long replication, on a large index or if updates are coming in fast relative to how fast they can be replayed, it can be a huge time savings. But yes, again, we should re-examine throttling with this change and related improvements. ",
            "id": "comment-15145718"
        },
        {
            "date": "2016-02-13T23:08:54+0000",
            "author": "Ramkumar Aiyengar",
            "content": "Sorry Mark, I completely missed your comments on my patch. The intent there was to have at least one in the queue after the currently processing one (hence a fixed queue of size 1), so yes, there will be at least one more attempt after the last request. And any other attempt will just be rejected. But as I said, I haven't tested it \u2013 I know the ArrayBlockingQueue will block any queueing when it hits capacity, but the ThreadpoolExecutor docs say that when a bound queue size is hit, an exception is thrown. The exception was my intention (so all jobs won't be run), perhaps testing will show otherwise.. ",
            "id": "comment-15146242"
        },
        {
            "date": "2016-02-19T04:48:57+0000",
            "author": "Mark Miller",
            "content": "Thanks Ramkumar Aiyengar. I'll open a new issue shortly and ping those interested. ",
            "id": "comment-15153714"
        },
        {
            "date": "2016-02-19T13:14:57+0000",
            "author": "Mark Miller",
            "content": "I created SOLR-8702 Improve our strategy for preventing recovery requests from stacking up. ",
            "id": "comment-15154194"
        }
    ]
}