{
    "id": "SOLR-9764",
    "title": "Design a memory efficient DocSet if a query returns all docs",
    "details": {
        "components": [],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [
            "6.5",
            "7.0"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "In some use cases, particularly use cases with time series data, using collection alias and partitioning data into multiple small collections using timestamp, a filter query can match all documents in a collection. Currently BitDocSet is used which contains a large array of long integers with every bits set to 1. After querying, the resulted DocSet saved in filter cache is large and becomes one of the main memory consumers in these use cases.\n\nFor example. suppose a Solr setup has 14 collections for data in last 14 days, each collection with one day of data. A filter query for last one week data would result in at least six DocSet in filter cache which matches all documents in six collections respectively.   \n\nThis is to design a new DocSet that is memory efficient for such a use case.  The new DocSet removes the large array, reduces memory usage and GC pressure without losing advantage of large filter cache.\n\nIn particular, for use cases when using time series data, collection alias and partition data into multiple small collections using timestamp, the gain can be large.\n\nFor further optimization, it may be helpful to design a DocSet with run length encoding. Thanks Mostafa Mokhtar for suggestion.",
    "attachments": {
        "SOLR_9764_no_cloneMe.patch": "https://issues.apache.org/jira/secure/attachment/12839875/SOLR_9764_no_cloneMe.patch",
        "SOLR-9764.patch": "https://issues.apache.org/jira/secure/attachment/12838871/SOLR-9764.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-11-14T23:49:57+0000",
            "author": "Michael Sun",
            "content": "Upload a prototype. For further optimization, the MatchAllDocSet can also override intersection() etc. ",
            "id": "comment-15665419"
        },
        {
            "date": "2016-11-15T16:42:48+0000",
            "author": "Mark Miller",
            "content": "Nice Michael, looks interesting.\n\nLooks like you need to handle intersection overloads to avoid an infinite loop of method call backs?\n\nGuessing that is due to:\n\nBitDocSet.java\n\n      // they had better not call us back!\n      return other.intersectionSize(this);\n\n\n ",
            "id": "comment-15667611"
        },
        {
            "date": "2016-11-17T20:42:03+0000",
            "author": "Michael Sun",
            "content": "Thanks Mark Miller. Let me fix it.\n\nCan you also send me the test case or query that you discovered the issue? Thanks. ",
            "id": "comment-15674749"
        },
        {
            "date": "2016-11-18T22:06:23+0000",
            "author": "Michael Sun",
            "content": "Upload a patch with more overload methods and tests. \n\nThere is still some room for optimization. There are some clever shortcuts in DocSet subclasses, probably for performance reason. Need to take advantage of it. ",
            "id": "comment-15677905"
        },
        {
            "date": "2016-11-19T20:19:10+0000",
            "author": "Shawn Heisey",
            "content": "I looked at the patch, and I find that I don't understand anything that I'm looking at, except that the bigger newer patch looks like it might be a reverse patch that removes all the new changes.\n\nAn orthogonal idea: One thing I've wondered about is whether filter results might sometimes benefit from one of the most simple compression techniques there is \u2013 run-length encoding.  There are obviously some cases where it would really suck, such as a filter where every other Lucene document matches, but I suspect that in many circumstances it would produce something much smaller than a simple bitset.  On the time series data that was mentioned in the description, a timestamp filter would probably benefit greatly. ",
            "id": "comment-15679804"
        },
        {
            "date": "2016-11-20T17:48:29+0000",
            "author": "Michael Sun",
            "content": "Shawn Heisey Thanks for reviewing. The patch was wrong. I uploaded an updated one. There was a mistake in git command in patch creation. Apologize for it.\n\nFor run length encoding, it can be a good direction for further memory optimization. Mostafa Mokhtar initially suggested this idea, as mentioned in JIRA description. I am trying to gather some supporting data meanwhile to justify the effort and potential risk. Any help would be great.\n\n\n ",
            "id": "comment-15681560"
        },
        {
            "date": "2016-11-20T22:32:29+0000",
            "author": "David Smiley",
            "content": "Patch looks good to me.  Just eyeing the patch, one thing isn't clear.  Why both clone() & cloneMe() methods?\n\nI suggest not incorporating stuff like RLE into this patch; that feels like incredible scope creep. ",
            "id": "comment-15681947"
        },
        {
            "date": "2016-11-21T14:03:02+0000",
            "author": "Shawn Heisey",
            "content": "I suggest not incorporating stuff like RLE into this patch; that feels like incredible scope creep.\n\nGood plan.  I wasn't suggesting it for this issue, it was merely an idea that's been brewing but hadn't quite surfaced completely.  This issue brought it up to more conscious thought. ",
            "id": "comment-15683638"
        },
        {
            "date": "2016-11-21T17:45:45+0000",
            "author": "Mark Miller",
            "content": "Nice patch Michael Sun. What is the issue with intDocSet? ",
            "id": "comment-15684214"
        },
        {
            "date": "2016-11-21T19:08:21+0000",
            "author": "Michael Sun",
            "content": "Thanks David Smiley Mark Miller for reviewing. Here are some of my thoughts.\n\nWhy both clone() & cloneMe() methods?\nWhat I try to achieve is make clone() public (protected by default). Meanwhile it need to be public at DocSet level which is the main interface used. Unfortunately Java seems not allow this visibility change in interface definition (can change in class). Therefore the current implementation is a small workaround for this problem.\n\nThere are some discussion online for other workarounds. Also another alternative is to override clone() in DocSetBase and convert DocSet to DocSetBase when clone() is used. But I thought the current implementation is easiest to understand. With that said, it's still a workaround. Any suggestion is welcome. \n\nWhat is the issue with intDocSet?\nIntDocSet actually works fine. The issue is DocSetBase.equals(), which is marked as for test purposed only. The equals() can't figure out two equal DocSet's are equal sometimes. Some work is need in the DocSetBase.equals() to get this test pass. I would add more details in patch comment for it meanwhile. ",
            "id": "comment-15684442"
        },
        {
            "date": "2016-11-21T19:47:55+0000",
            "author": "David Smiley",
            "content": "Simply define clone() as public in DocSet and thus all implementations.  Mention in javadoc it's a deep copy (safe to mutate).  Implement Cloneable.  I did these things locally and had no trouble.  I attached the patch; it compiles. ",
            "id": "comment-15684538"
        },
        {
            "date": "2016-11-21T21:49:29+0000",
            "author": "Michael Sun",
            "content": "Cool! Thanks David Smiley, Let me check it out. ",
            "id": "comment-15684837"
        },
        {
            "date": "2016-11-22T00:17:47+0000",
            "author": "Michael Sun",
            "content": "Here are some single user test results for the amount of memory saved.\n\nSetup: Solr with a collection alias mapping to two collections, each with 4 days of data. \nTest: Restart Solr, run query with filter for last 7 days and collect memory histogram on one server afterwards. The filter hits both collections, with one match all and the other match partially.\nResult (extracted from  histogram)\n\n\n\nPatched\n#BitDocSet instances\n#MatchAllDocSet instances\nbytes for [J\nMem Saving\n\n\nY\n1\n1\n9998408496\n3.4M\n\n\nN\n2\n0\n10001843704\n\u00a0\n\n\nY\n2\n2\n10001833664\n6.9M or 3.4M per hit\n\n\nN\n4\n0\n10008701640\n\u00a0\n\n\n\n\n\nAnalysis:\nThe difference of bytes for long[] is 3435208 bytes(3.4M) if one MatchAllDocSet is hit.. That's the total amount of memory saved by this patch for one query per server per matched collection. The the other side, The core under study has 27M documents. A BitDocSet would require a long[] at the size of 3.4M (27M/8) without patch, which is aligned with the memory saved.\n\n ",
            "id": "comment-15685203"
        },
        {
            "date": "2016-11-22T01:39:56+0000",
            "author": "Michael Sun",
            "content": "Ah, I see. The implementation of clone() in DocSetBase makes the difference. It's good to know. Thanks David Smiley for help. \n\nUploaded an updated patch with cloneMe() removed, using David Smiley's code as example. The DocSetBase.clone() is simplified though.\n\nJust curious, what logic in JVM requires clone() to be implemented in DocSetBase in this case. DocSetBase is an abstract class which normally is not required to implement a method. ",
            "id": "comment-15685364"
        },
        {
            "date": "2016-11-22T03:24:14+0000",
            "author": "David Smiley",
            "content": "Just curious, what logic in JVM requires clone() to be implemented in DocSetBase in this case. DocSetBase is an abstract class which normally is not required to implement a method.\n\nI suspect it's because DocSetBase actually does implement clone() \u2013 albeit indirectly via Object.  But Object's definition is incompatible with the DocSet interface in three ways: visibility & throws & co-variant return ",
            "id": "comment-15685550"
        },
        {
            "date": "2016-11-22T03:36:00+0000",
            "author": "Michael Sun",
            "content": "Yeah. Make sense. Now I can understand the error message when building without clone() implementation in DocSetBase, \" attempting to assign weaker access privileges; was public\". As you said, Javac probably consider clone() is implemented in DocSetBase as a protected method.\n\nThat's a good discussion. Your help is appreciated. ",
            "id": "comment-15685573"
        },
        {
            "date": "2016-11-28T21:26:53+0000",
            "author": "Michael Sun",
            "content": "Uploaded a new patch with all tests passed.\n\nWhat is the issue with intDocSet?\nBasic in DocSetBase.equals(), both DocSet are converted to FixedBitSet and then both FixedBitSet are compared. However, both DocSet may go through different code path and resize differently in conversion even these two DocSet are equal. The result is that one FixedBitSet has more zero paddings than the other which makes FixedBitSet.equals() think they are different. \n\nThe fix is to resize both FixedBitSet to the same larger size before comparison in DocSetBase.equals(). Since DocSetBase.equals() is marked for test purpose only, the efficiency of the extra sizing would not be a problem. ",
            "id": "comment-15703181"
        },
        {
            "date": "2016-12-01T16:00:15+0000",
            "author": "Shawn Heisey",
            "content": "Something just mentioned for memory efficiency on the mailing list was a roaring bitmap.  Lucene does have this implemented already \u2013 RoaringDocIdSet.  I do not know how it would perform when actually used as a filterCache entry, compared to the current bitset implementation. ",
            "id": "comment-15712339"
        },
        {
            "date": "2016-12-01T16:29:51+0000",
            "author": "Dorian",
            "content": "Elasticsearch uses it for filter caching. Some comparisons here: https://www.elastic.co/blog/frame-of-reference-and-roaring-bitmaps ",
            "id": "comment-15712406"
        },
        {
            "date": "2016-12-01T19:38:03+0000",
            "author": "Michael Sun",
            "content": "Lucene does have this implemented already \u2013 RoaringDocIdSet\nThat's cool. Let me check it out to understand pros and cons. Thanks Shawn Heisey to point it out.\n\nI do not know how it would perform when actually used as a filterCache entry, compared to the current bitset implementation.\nFor this particular use case (a query matches all docs), the approach in patch should be better than roaring bitmap. The patch designed a MatchAllDocSet for this use case, which uses no memory other than storing the size. In addition, MatchAllDocSet would be faster in creating DocSet, union, intersect etc. since no real bit manipulation is required.  ",
            "id": "comment-15712868"
        },
        {
            "date": "2016-12-02T01:53:04+0000",
            "author": "Yonik Seeley",
            "content": "Since Solr DocSets don't currently match deleted documents, even a single deletion in the index would circumvent this optimization.\n\nAlso, note that \"all non-deleted docs\" is a special case that is cached if requested... see SolrIndexSearcher.getLiveDocs() (this is used in a few places).\nSo another optimization (inspired by Michael's insight into  the size==maxDoc case) would be: if the DocSet just produced has size==numDocs, then just use liveDocs.  This would have the effect of making all queries that map onto all documents share the resulting DocSet (as well as working when there were deleted docs in the index).\n\nWhether it's worth trying to compress that single set (and the best way to do it) is an independent decision. ",
            "id": "comment-15713723"
        },
        {
            "date": "2016-12-02T21:09:47+0000",
            "author": "Michael Sun",
            "content": "This would have the effect of making all queries that map onto all documents share the resulting DocSet\nAh, I see. That's a good idea.  Let me check it out. Thanks Yonik Seeley for suggestion.  ",
            "id": "comment-15716529"
        },
        {
            "date": "2016-12-02T21:28:30+0000",
            "author": "Michael Sun",
            "content": "I do not know how it would perform when actually used as a filterCache entry, compared to the current bitset implementation.\nRoaringDocIdSet looks pretty interesting. From the link in comments,  https://www.elastic.co/blog/frame-of-reference-and-roaring-bitmaps, however, it looks RoaringDocIdSet doesn't save any memory in case a query match all docs.\n\nBasically the idea of RoaringDocIdSet is to divide the entire bitmap into multiple chunks. For each chunk, either a bitmap or a integer array (using diff compression) can be used depending on number of matched docs in that chunk. If matched doc is higher than a certain number in a chunk, a bitmap is used for that chunk. Otherwise integer array is used. It can help in some use cases but it would fall back to something equivalent to FixedBitMap in this use case.\n\nIn addition, the 'official' website for roaring bitmaps  http://roaringbitmap.org mentioned roaring bitmaps can also use run length encoding to store the bitmap chunk but also mentioned one of the main goals of roaring bitmap is to solve the problem of run length encoding, which is expensive random access. Need to dig into source code to understand it better. Any suggestion is welcome. ",
            "id": "comment-15716573"
        },
        {
            "date": "2016-12-03T16:58:31+0000",
            "author": "Shawn Heisey",
            "content": "How much of a performance speedup (forgetting for a moment about memory savings) are we talking about for the \"match all docs\" enhancement?  For my environment, it would only apply to manual queries and the load balancer ping requests (every five seconds), but NOT to queries made by users.  The ping handler does a distributed query using q=*:* with no filters and rows=1.  If the speedup is significant, then my load balancer health checks might get faster, which would be a good thing. ",
            "id": "comment-15718383"
        },
        {
            "date": "2016-12-03T17:03:11+0000",
            "author": "Shawn Heisey",
            "content": "It's worth noting that the ping queries are already VERY fast. ",
            "id": "comment-15718391"
        },
        {
            "date": "2016-12-05T05:25:23+0000",
            "author": "Varun Thacker",
            "content": "Hi Michael,\n\nI was reading through the blog and the lucene implementation \"inverses its encoding when the set becomes very dense\" . It's also documented in the Javadocs : https://lucene.apache.org/core/6_3_0/core/org/apache/lucene/util/RoaringDocIdSet.html\n\nDo you think this will be good enough for this case? ",
            "id": "comment-15721274"
        },
        {
            "date": "2016-12-05T18:44:14+0000",
            "author": "Michael Sun",
            "content": "if the DocSet just produced has size==numDocs, then just use liveDocs\nYonik Seeley Can you give me some more details how to implement this check. Somehow I can't find a clean way to do it. Thanks. ",
            "id": "comment-15723002"
        },
        {
            "date": "2016-12-05T22:30:57+0000",
            "author": "Michael Sun",
            "content": "Ah, yes, you are right. Thanks Varun Thacker for suggestion. The 'inverse encoding' is a good idea.\n\nDo you think this will be good enough for this case\nOn memory saving side, RoaringIdDocSet looks a good solution. It would only use a small amount of memory in this use case.\n\nOn the other hand, there are some implication on CPU usage, mainly in constructing the DocSet. RoaringIdDocSet saves memory by choosing different data structure based on matched documents in a chunk. However, the code doesn't know what data structure to use before it iterate all documents in a chunk and can result in some expensive 'shift' in data structure and 'resizing'. For example, in this use case, for each chunk, the code basically starts fill a large short[], then shift it to a bitmap, and convert data from short[] to bitmap, then fill bitmap, then later switch back to a small short[]. All these steps can be expensive unless it's optimized for some use cases. In addition, all these steps use iterator to get matched doc one by one.\n\nThe union and intersection using RoaringIdDocSet can be more expensive too in addition the cost of constructing. Of course, it's hard to fully understand the performance implication without testing on a prototype. Any suggestion is welcome.\n ",
            "id": "comment-15723569"
        },
        {
            "date": "2016-12-05T22:36:32+0000",
            "author": "Michael Sun",
            "content": "Inspired by all nice discussions, another good optimization would be to store an inverse of the matched docSet if all or most of docs are matched by a query. If the number of docs matched is close to maxDocs, a HashDocSet would be very efficient. (Thanks Yonik Seeley for suggestion.) ",
            "id": "comment-15723591"
        },
        {
            "date": "2016-12-05T22:42:44+0000",
            "author": "Michael Sun",
            "content": "hmmm, I think query with q=: doesn't use MatchAllDocSets with the current patch. Let me see if there is a way to optimize this use case as well. ",
            "id": "comment-15723605"
        },
        {
            "date": "2016-12-22T18:55:03+0000",
            "author": "Michael Sun",
            "content": "Uploaded an updated patch. In this patch, if a query matches all docs,  MatchAllDocSet is used if there is no deleted docs in collection or LiveDocs if there is deleted docs. Thanks Yonik Seeley for suggestion.\n\n ",
            "id": "comment-15770798"
        },
        {
            "date": "2016-12-22T18:59:46+0000",
            "author": "Michael Sun",
            "content": "Other than the patch, there are some good ideas for memory optimization, such as:\n\n1. run length encoding.\n2. RoaringBitSet\n3. Inverse DocSet\n\nEach of them has advantages as well as constraints and limitations. We can open separate JIRAs for each to evaluate these solutions and optimize as need.\n ",
            "id": "comment-15770810"
        },
        {
            "date": "2017-01-27T17:36:02+0000",
            "author": "Yonik Seeley",
            "content": "tldr; the attached patch should make all queries that end up matching all documents use the same DocSet instead of caching different sets.\n\nNotes on changes from the previous patch:\n\n\tI'm not sure what version this patch was made for, but it won't compile on either trunk or 6x.\n\n[javac] /opt/code/lusolr/trunk/solr/core/src/java/org/apache/solr/query/SolrRangeQuery.java:157: error: incompatible types: DocSet cannot be converted to BitDocSet\n[javac]     BitDocSet liveDocs = searcher.getLiveDocs();\n\n\n\tSome code currently explicitly relies on BitDocSet from liveDocs (hence the compilation error above)\n\n\n\n\n\thopefully the following change is just an optimization, and not meant to ensure that the same amount of 0 padding is used for each bitset?\n  it's buggy in the general case (size is cardinality, not capacity)\n\n   protected FixedBitSet getBits() {\n-    FixedBitSet bits = new FixedBitSet(64);\n+    FixedBitSet bits = new FixedBitSet(size());\n\n\n\n\n\n\n\tif MatchAllDocs is used as a lucene filter, it can cause a lot of unnessesary memory use... in fact it would end up creating a new bit set\n  for each use.  This is one instance of a more generic problem... the BaseDocSet implementations are often very inefficient and should be\n  overridden by any DocSet meant for use in any common case.  Much of the code was also written for best performance with the knowledge that\n  there were only 2 common sets (small and large)... that code will need to be revisited / re-reviewed when adding a 3rd into the mix.\n\n\n\n\n\tGiven the current problems with MatchAllDocs, I've backed out that part of the patch for now.\n  As detailed above, this is more a problem of the fragility of the current code base than with your class.\n  It's probably best handled in a separate issue, and perhaps in a more general way that can handle more cases (like most docs matching or segments with deleted docs),\n  or if the robustness of the DocSet hierarchy can be improved, we could even add multiple new implementations (Roaring, MatchMost, MatchAll, etc)\n\n\n\n\n\tThe setting of liveDocs was not thread-safe (unsafe object publishing)\n\n\n\n\n\tadded size() to DocSetCollector in favor of the more specific isMatchLiveDocs\n\n\n\n\n\tThe check for liveDocs was only done in createDocSetGeneric, meaning many instances wouldn't be detected (term query, range query on string,\n  non-fq parameters like base queries, etc). I created some DocSetUtil methods to handle these cases and called them\n  from most of the appropriate places.\n\n\n\n\n\tjust a draft patch, but if people agree, we can add tests for sharing/deduplication of liveDocs (which includes the MatchAllDocs case) and commit.\n\n\n ",
            "id": "comment-15843157"
        },
        {
            "date": "2017-01-30T22:00:42+0000",
            "author": "Yonik Seeley",
            "content": "Here's an updated patch that has tests for the sharing of DocSets that match all documents. ",
            "id": "comment-15846018"
        },
        {
            "date": "2017-01-31T01:43:53+0000",
            "author": "Michael Sun",
            "content": "if the robustness of the DocSet hierarchy can be improved, we could even add multiple new implementations (Roaring, MatchMost, MatchAll, etc)\nThanks for Yonik Seeley for reviewing and improving. I open new JIRAs to evaluate and implement these optimizations.\n\nSOLR-10057 Evaluate/Design a MatchMost/MatchAll doc set (Use one JIRA since MatchAll can be a special case for MatchMost)\nSOLR-10058 Evaluate/Design a DocSet using Lucene RoaringIdDocSet  ",
            "id": "comment-15846275"
        },
        {
            "date": "2017-01-31T16:52:17+0000",
            "author": "ASF subversion and git services",
            "content": "Commit a43ef8f480c6a32dd77e43d02c2dca2299a05eb3 in lucene-solr's branch refs/heads/master from Yonik Seeley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a43ef8f ]\n\nSOLR-9764: share liveDocs for any DocSet of size numDocs ",
            "id": "comment-15847113"
        },
        {
            "date": "2017-01-31T17:17:34+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 6a3d7bf37f1b0b6a6ec03ecc20367f8a121ddb81 in lucene-solr's branch refs/heads/branch_6x from Yonik Seeley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6a3d7bf ]\n\nSOLR-9764: share liveDocs for any DocSet of size numDocs ",
            "id": "comment-15847147"
        },
        {
            "date": "2017-01-31T17:20:35+0000",
            "author": "Yonik Seeley",
            "content": "Committed, thanks! ",
            "id": "comment-15847152"
        },
        {
            "date": "2017-02-02T21:44:43+0000",
            "author": "David Smiley",
            "content": "Minor feedback:\n\n\tI think getLiveDocs as implemented here will do a volatile-read twice; it could be improved to do once.\n\tIt's not obvious that one should use DocSetUtil.getDocSet(docSetCollector, searcher); perhaps instead DocSetCollector.getDocSet should demand an indexSearcher argument?  It could even be optional (null) I guess.\n\n ",
            "id": "comment-15850557"
        },
        {
            "date": "2017-02-03T02:49:19+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 98d1dabcd8c851be507bc374c565a41a829e2c72 in lucene-solr's branch refs/heads/master from Yonik Seeley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=98d1dab ]\n\nSOLR-9764: change getLiveDocs to do a single volatile read ",
            "id": "comment-15850974"
        },
        {
            "date": "2017-02-03T02:49:42+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 64b1d24819371a4e51fb525a4564905b155f41f1 in lucene-solr's branch refs/heads/branch_6x from Yonik Seeley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=64b1d24 ]\n\nSOLR-9764: change getLiveDocs to do a single volatile read ",
            "id": "comment-15850976"
        },
        {
            "date": "2017-02-03T02:55:55+0000",
            "author": "Yonik Seeley",
            "content": "I think getLiveDocs as implemented here will do a volatile-read twice; it could be improved to do once.\n\nDone.\n\nIt's not obvious that one should use DocSetUtil.getDocSet(docSetCollector, searcher); perhaps instead DocSetCollector.getDocSet should demand an indexSearcher argument? It could even be optional (null) I guess.\n\nI could go either way on this one...  I'll leave it to you to change if you like. ",
            "id": "comment-15850982"
        },
        {
            "date": "2017-02-03T11:01:36+0000",
            "author": "Varun Thacker",
            "content": "I tried running a small benchmark to see how much memory does this save:\n\nIndexed 10M documents and started solr with 4G of heap. Then on this static index I fired 10k queries \n\n{!cache=false}*:*\n\nFreed memory was calculated by firing 10k queries then forcing a GC and reading the freed memory in GC viewer.\n\nFreed Memory:\nTrunk with this patch:   1301MB\nSolr 6.3             :           1290MB\n\nA FixedBitSet of 10M entries translates to a long array of size=156250 = 1.2 MB\n\nThe filterCache/queryResultCache didn't have any entries but maybe I'm missing something here. I'll look into the test setup over the next couple of days to see what's wrong ",
            "id": "comment-15851332"
        },
        {
            "date": "2017-02-03T13:03:53+0000",
            "author": "Yonik Seeley",
            "content": "The filterCache/queryResultCache didn't have any entries \n\nThe sharing of liveDocs is mostly for the filterCache.  It doesn't prevent the building of the set in the first place... it prevents it from being redundantly cached as a separate set. ",
            "id": "comment-15851464"
        },
        {
            "date": "2017-02-23T20:58:45+0000",
            "author": "Steve Rowe",
            "content": "@yonik: The original commit on this issue included the following CHANGES entry:\n\nSOLR-9764: All filters that which all documents in the index now share the same memory (DocSet).\n\nI think that the \"which\" in that sentence should instead be \"match\"? ",
            "id": "comment-15881245"
        },
        {
            "date": "2017-02-23T21:43:35+0000",
            "author": "Yonik Seeley",
            "content": "Hmmm, yep.  I'll fix... ",
            "id": "comment-15881330"
        },
        {
            "date": "2017-02-23T21:55:44+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 05c17c9a516d8501b2dcce9b5910a3d0b5510bc4 in lucene-solr's branch refs/heads/master from Yonik Seeley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=05c17c9 ]\n\nSOLR-9764: fix CHANGES entry ",
            "id": "comment-15881361"
        },
        {
            "date": "2017-02-23T21:57:12+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 92e619260cc89b4725c2e5e971fc3cb7bbb339cc in lucene-solr's branch refs/heads/branch_6x from Yonik Seeley\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=92e6192 ]\n\nSOLR-9764: fix CHANGES entry ",
            "id": "comment-15881362"
        },
        {
            "date": "2017-04-12T05:23:00+0000",
            "author": "David Smiley",
            "content": "I'm looking at this closer again.  Comments/Questions:\n\n\tThe change in DocSetBase.getBits from '64' vs 'size()' seems odd to me.   Wouldn't, say Math.max(64,size()) (or perhaps use a larger number like 1024) make more sense?  size() is almost certainly too small; no?\n\tPerhaps DocSetCollector.getDocSet should return DocSet.EMPTY?  Or perhaps this should be the job of DocSetUtil.getDocSet since it already optimizes to a shared reference for the live docs.  That is quite minor though; it's cheap & light-weight.\n\tSolrIndexSearcher.getDocSetBits will call getDocSet which will ensure the query gets put into the filter cache.  Yet it also upgrades it to a BitDocSet if it isn't and will put it in again, overriding the existing SortedIntSet (like that's what it is).  Why?  What if it's a match-no-docs?  If this is deliberate it deserves a comment; if not then probably a minor perf bug.\n\n\n\nThe main thing I'm investigating, however, is how might the filterCache's maxRamMB setting not over-count the shared liveDocs: either count it zero times, one times (both fine possibilities), but definitely not more than once.  Without resorting to the cache knowing about live docs (ugh; pretty ugly), I think this requires a MatchAll instance like Michael had since created.  The match-all (live docs) can easily be a common cache entry for range faceting on time, especially with time based shards. ",
            "id": "comment-15965394"
        }
    ]
}