{
    "id": "LUCENE-2723",
    "title": "Speed up Lucene's low level bulk postings read API",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.1"
        ],
        "affect_versions": "None",
        "resolution": "Won't Fix",
        "status": "Closed"
    },
    "description": "Spinoff from LUCENE-1410.\n\nThe flex DocsEnum has a simple bulk-read API that reads the next chunk\nof docs/freqs.  But it's a poor fit for intblock codecs like FOR/PFOR\n(from LUCENE-1410).  This is not unlike sucking coffee through those\ntiny plastic coffee stirrers they hand out airplanes that,\nsurprisingly, also happen to function as a straw.\n\nAs a result we see no perf gain from using FOR/PFOR.\n\nI had hacked up a fix for this, described at in my blog post at\nhttp://chbits.blogspot.com/2010/08/lucene-performance-with-pfordelta-codec.html\n\nI'm opening this issue to get that work to a committable point.\n\nSo... I've worked out a new bulk-read API to address performance\nbottleneck.  It has some big changes over the current bulk-read API:\n\n\n\tYou can now also bulk-read positions (but not payloads), but, I\n     have yet to cutover positional queries.\n\n\n\n\n\tThe buffer contains doc deltas, not absolute values, for docIDs\n    and positions (freqs are absolute).\n\n\n\n\n\tDeleted docs are not filtered out.\n\n\n\n\n\tThe doc & freq buffers need not be \"aligned\".  For fixed intblock\n    codecs (FOR/PFOR) they will be, but for varint codecs (Simple9/16,\n    Group varint, etc.) they won't be.\n\n\n\nIt's still a work in progress...",
    "attachments": {
        "LUCENE-2723-BulkEnumWrapper.patch": "https://issues.apache.org/jira/secure/attachment/12469300/LUCENE-2723-BulkEnumWrapper.patch",
        "LUCENE-2723.patch": "https://issues.apache.org/jira/secure/attachment/12457994/LUCENE-2723.patch",
        "LUCENE-2723-termscorer.patch": "https://issues.apache.org/jira/secure/attachment/12466307/LUCENE-2723-termscorer.patch",
        "LUCENE-2723_bulkvint.patch": "https://issues.apache.org/jira/secure/attachment/12466660/LUCENE-2723_bulkvint.patch",
        "LUCENE-2723_openEnum.patch": "https://issues.apache.org/jira/secure/attachment/12466541/LUCENE-2723_openEnum.patch",
        "LUCENE-2723_termscorer.patch": "https://issues.apache.org/jira/secure/attachment/12466265/LUCENE-2723_termscorer.patch",
        "LUCENE-2723_wastedint.patch": "https://issues.apache.org/jira/secure/attachment/12466465/LUCENE-2723_wastedint.patch",
        "LUCENE-2723_facetPerSeg.patch": "https://issues.apache.org/jira/secure/attachment/12466575/LUCENE-2723_facetPerSeg.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-10-25T19:35:27+0000",
            "content": "Attached patch, which also includes FrameOfRef/PatchedFrameOfRef\ncodecs modified some from LUCENE-1410.\n\nAll tests pass (at least once, heh) for all core/test codecs, but,\nthere are still many nocommits to work through...\n\nI've cutover a few places to the new API (the old one is still there\nbut I plan to fully cutover and remove it before committing). ",
            "author": "Michael McCandless",
            "id": "comment-12924690"
        },
        {
            "date": "2010-11-02T22:20:15+0000",
            "content": "New patch, sync'd to trunk. ",
            "author": "Michael McCandless",
            "id": "comment-12927650"
        },
        {
            "date": "2010-11-03T11:56:13+0000",
            "content": "New patch, sync'd to trunk.\nAmazing mike - what a huge patch.  Unfortunately there is a file missing in the patch - benchmark can not find GatherFieldValuesTask and o.a.l.u.pfor.Pfor.java uses Java 6 Arrays.copyOfRange(..) \n\nI will hack around and run some tests ",
            "author": "Simon Willnauer",
            "id": "comment-12927822"
        },
        {
            "date": "2010-11-03T12:10:21+0000",
            "content": "Ugh, sorry \u2013 that GatherFieldValues isn't going to be committed; you can discard all mods under contrib/benchmark in the patch.\n\nAnd we'll have to fix the copyOfRange before committing.  I'll stuck a nocommit on it  ",
            "author": "Michael McCandless",
            "id": "comment-12927824"
        },
        {
            "date": "2010-11-03T13:30:49+0000",
            "content": "And we'll have to fix the copyOfRange before committing. I'll stuck a nocommit on it \n\nThe copyOfRange methods in Java 6 are 3liners in src.zip/harmony. We can add the harmony versions to ArrayUtil and we are fine. It is for native types veeeeery easy and for non-native types (? extends Object) it uses the Array reflection class to generate a target array of same type String[] -> String[].\n\nI could supply a patch. Which variants (datatypes) do you need? ",
            "author": "Uwe Schindler",
            "id": "comment-12927848"
        },
        {
            "date": "2010-12-09T19:28:41+0000",
            "content": "Mike, this patch is so impressive that I didn't even make it to the bottom. I think that is bigger than the docValues branch changes, so here is my take on it.... I think this is extremely valuable but we really should try to split that up into multiple patches. The easiest take on it would be moving PFoR and FoR out of it to make it at least reviewable.\nI would volunteer to do so if we agree on it. I am actually close to proposing a branch for that since its truely huge and it would be easier to keep up with trunk really.  Either way, let me know and I get it started....\n\nsimon ",
            "author": "Simon Willnauer",
            "id": "comment-12969885"
        },
        {
            "date": "2010-12-09T20:03:57+0000",
            "content": "Definitely, let's move FOR/PFOR out onto a separate issue \u2013 they don't need to be included in the cutover.  I just use them for testing the bulk API. ",
            "author": "Michael McCandless",
            "id": "comment-12969908"
        },
        {
            "date": "2010-12-11T17:54:04+0000",
            "content": "here is a patch without all FoR and PFoR  ~ 150kb sounds more reasonable though. All tests pass on trunk\n\nwe have 55 nocommit left in that patch but it actually seems to be quite close to me. I will start iterating next week I guess to mike if you feel like it go ahead start iterating.....\n\nsimon ",
            "author": "Simon Willnauer",
            "id": "comment-12970488"
        },
        {
            "date": "2010-12-12T15:51:46+0000",
            "content": "here's an updated patch... i did the easy stuff, 41 nocommits left (i added one).\n\nSome notes:\n\n\twe should remove the old API everywhere... contrib/misc/HighFreqTerms and solr/faceting/UninvertedField are the only users left?\n\tTermScorer is very scary, we gotta simplify. I don't like not being able to understand it.\n\tFor MultiTermQueries, TermQuery shouldnt calculate that per-segment docFreq... MTQ \"knows\" this docFreq, so maybe we need to solve the TermState first. i added a nocommit here as it will hurt performance.\n\texposing the MMap buffer i dont think is helpful, and dangerous. The Pfor/For should work on DataInput.readInt() instead of wrapping bytebuffers as intbuffers? anyway this can be dealt with on the pfor/for issue.\n\n ",
            "author": "Robert Muir",
            "id": "comment-12970610"
        },
        {
            "date": "2010-12-13T17:13:23+0000",
            "content": "here is a frist try to clean up the TermScorer a little bit. I introduced a helper class BulkReaderConsumer.java that generalizes some of the operations. Before we shoot for something like this super tough TermScorer I would really be interested if it isn't premature optimization and if hotspot could do a good job with small and simple methods.\n\n\nmike can you give it a try on beast? ",
            "author": "Simon Willnauer",
            "id": "comment-12970899"
        },
        {
            "date": "2010-12-14T00:57:21+0000",
            "content": "Simon, the benchmarker barfs on conjunction queries w/ the improvement to TermScorer (eg +nebraska +states gets different results). ",
            "author": "Michael McCandless",
            "id": "comment-12971110"
        },
        {
            "date": "2010-12-14T10:55:24+0000",
            "content": "I disabled the conjunction queries and then ran perf test w/ the simplified TermScorer:\n\n\n\n\nQuery\nQPS base\nQPS simon\nPct diff\n\n\nstate\n32.53\n29.43\n-9.5%\n\n\nunit~2.0\n16.88\n15.83\n-6.2%\n\n\nunit~1.0\n17.17\n16.21\n-5.6%\n\n\nunit state\n9.64\n9.12\n-5.4%\n\n\nspanFirst(unit, 5)\n17.09\n16.79\n-1.7%\n\n\nunited~1.0\n62.01\n60.95\n-1.7%\n\n\nunited~2.0\n15.86\n15.70\n-1.0%\n\n\nu*d\n31.09\n30.77\n-1.0%\n\n\nspanNear([unit, state], 10, true)\n4.17\n4.13\n-1.0%\n\n\nuni*\n14.38\n14.27\n-0.7%\n\n\nunit*\n28.45\n28.24\n-0.7%\n\n\nun*d\n67.19\n66.78\n-0.6%\n\n\n\"unit state\"\n8.59\n8.56\n-0.3%\n\n\n\n\n\nUnfortunately the extra abstraction (BlockReaderConsumer) made things a good amount slower...  ",
            "author": "Michael McCandless",
            "id": "comment-12971229"
        },
        {
            "date": "2010-12-14T11:49:52+0000",
            "content": "I think we should cut a branch starting w/ Robert's latest patch... I'll do that soon. ",
            "author": "Michael McCandless",
            "id": "comment-12971236"
        },
        {
            "date": "2010-12-14T17:21:25+0000",
            "content": "OK I created the branch at  https://svn.apache.org/repos/asf/lucene/dev/branches/bulkpostings ... all tests pass now. ",
            "author": "Michael McCandless",
            "id": "comment-12971324"
        },
        {
            "date": "2010-12-14T21:08:54+0000",
            "content": "FYI, just so we don't duplicate work, I'm in the middle of converting some of the solr uses. ",
            "author": "Yonik Seeley",
            "id": "comment-12971411"
        },
        {
            "date": "2010-12-14T22:37:59+0000",
            "content": "Here's a patch to make TermScorer more readable: advance() is still scary\nbut the rest starts to look reasonable.\n\nI pulled out the omitTF case into a MatchOnlyTermScorer.\nHere's the benchmark with luceneutil.\n\n\n               Query  QPS branch   QPS patch  Pct diff\nspanNear([unit, state], 10, true)        2.91        2.87     -1.3%\n                uni*       11.36       11.31     -0.4%\n               unit*       20.89       20.81     -0.4%\n        \"unit state\"        6.14        6.13     -0.2%\n                 u*d       17.30       17.28     -0.1%\n          unit state        7.47        7.46     -0.1%\n                un*d       55.42       55.69      0.5%\n  spanFirst(unit, 5)       12.27       12.34      0.6%\n          united~2.0       13.51       13.61      0.7%\n          united~1.0       49.88       50.30      0.8%\n            unit~1.0       13.00       13.27      2.0%\n               state       27.67       28.32      2.4%\n            unit~2.0       12.46       12.79      2.6%\n    +nebraska +state       75.91       79.97      5.3%\n        +unit +state        8.63        9.25      7.1%\n\n ",
            "author": "Robert Muir",
            "id": "comment-12971450"
        },
        {
            "date": "2010-12-14T23:28:34+0000",
            "content": "by the way, i tried an experiment... i don't much trust the +nebraska +state results in general,\nbut the +unit +state is more stable and I ran this one twice to be sure... i think we should investigate more\n\nin advance() I changed:\n\n\n    // not found in current block, seek underlying stream\n    BulkPostingsEnum.JumpResult jumpResult = docsEnum.jump(target, count);\n    if (jumpResult != null) {\n\n\n\nto:\n\n\n    // not found in current block, seek underlying stream\n    BulkPostingsEnum.JumpResult jumpResult;\n    if (target - doc > docDeltas.length*4 && (jumpResult = docsEnum.jump(target, count)) != null) {\n\n\n\n\nQuery  QPS branch   QPS patch  Pct diff\n    +nebraska +state       75.91       84.46     11.3%\n        +unit +state        8.67       10.01     15.4%\n\nQuery  QPS branch   QPS patch  Pct diff\n    +nebraska +state       73.23       80.27      9.6%\n        +unit +state        8.66       10.00     15.5%\n\n ",
            "author": "Robert Muir",
            "id": "comment-12971465"
        },
        {
            "date": "2010-12-15T04:08:42+0000",
            "content": "ok after a lot of benchmarking with different \"short jump distances\" i found the best thing\nto do was the most conservative: just avoid useless jumps when the target has to be in the \nnext fill()\n\nanything else was just in the wind: it only matters for high freq terms anyway.\n\nI committed this in r1049411, since its always a win and independent of the refactoring. ",
            "author": "Robert Muir",
            "id": "comment-12971541"
        },
        {
            "date": "2010-12-15T10:16:48+0000",
            "content": "That's awesome speedup Robert! ",
            "author": "Michael McCandless",
            "id": "comment-12971605"
        },
        {
            "date": "2010-12-15T10:18:35+0000",
            "content": "I think we should commit the prototype FOR/PFOR codec onto the branch so that we can run perf tests and properly stretch the new API...\n\nif these codecs aren't ready (but the core bulk API changes are) by the time we want to land back then we can always drop them on merging back.\n\nSo I'll go commit them onto the branch... ",
            "author": "Michael McCandless",
            "id": "comment-12971606"
        },
        {
            "date": "2010-12-15T10:44:40+0000",
            "content": "OK I brought PFOR/FOR back to life on the branch; all tests pass w/ each codec. ",
            "author": "Michael McCandless",
            "id": "comment-12971617"
        },
        {
            "date": "2010-12-15T11:05:54+0000",
            "content": "I played around a little with recommended actions from hotspot wiki and factored some methods out on TemScorer and MatchOnlyTermScorer. I merged my changes with the ones from Robert from the latest patch. \nAll tests pass and I checked with a10M docs wikipedia index if base (branch) and spec (branch + patch). I actually was surprised about the results though:\n\n\n               Query    QPS base    QPS spec  Pct diff\n        \"unit state\"        1.71        1.70     -0.6%\n  spanFirst(unit, 5)        4.81        4.79     -0.5%\n                un*d       14.94       14.88     -0.4%\n          united~1.0        8.79        8.76     -0.4%\n               unit*        8.70        8.66     -0.4%\n                uni*        4.16        4.14     -0.3%\n                 u*d        4.01        4.01     -0.1%\n          united~2.0        1.88        1.88      0.0%\nspanNear([unit, state], 10, true)        0.94        0.96      2.8%\n            unit~1.0        4.32        4.49      4.0%\n            unit~2.0        4.20        4.38      4.4%\n    +nebraska +state       24.90       26.11      4.8%\n        +unit +state        4.60        4.97      8.0%\n          unit state        3.60        3.93      9.1%\n               state        9.83       10.98     11.7%\n\n \n\nI ran those twice with very similar results.... \n3 iters, 40 iters per JVM and 2 threads on delmulti index\n\nhers is my JVM\n\njava version \"1.6.0_22\"\nJava(TM) SE Runtime Environment (build 1.6.0_22-b04)\nJava HotSpot(TM) Server VM (build 17.1-b03, mixed mode)\n\n\nstarted with:\n\n\n  java -Xbatch -Xms2g -Xmx2g -server \n\n ",
            "author": "Simon Willnauer",
            "id": "comment-12971623"
        },
        {
            "date": "2010-12-15T12:01:42+0000",
            "content": "updated patch to work with mikes simplification with pre-filled buffer on Jump ",
            "author": "Simon Willnauer",
            "id": "comment-12971653"
        },
        {
            "date": "2010-12-15T12:06:40+0000",
            "content": "in the MatchOnly advance() we should avoid the useless jumps too? ",
            "author": "Robert Muir",
            "id": "comment-12971654"
        },
        {
            "date": "2010-12-15T12:10:41+0000",
            "content": "fyi: i'll be working on removing the mmap hack (i think we shouldnt expose its buffer \nand create intbuffer views over it, and i think simply working on datainput in for/pfor will actually be faster). ",
            "author": "Robert Muir",
            "id": "comment-12971655"
        },
        {
            "date": "2010-12-15T12:39:00+0000",
            "content": "before resolving this issue I think we are going to need at least basic unit tests for PFor/FOR,\nI don't think we should rely exclusively on randomly using PatchedFrameOfRef in unit tests.\n\nI looked at LUCENE-1410 but it seemed the unit tests there are really performance tests... ",
            "author": "Robert Muir",
            "id": "comment-12971664"
        },
        {
            "date": "2010-12-15T12:56:26+0000",
            "content": "linking issues as we need to resolve the TermState patch before this can go to trunk ",
            "author": "Simon Willnauer",
            "id": "comment-12971667"
        },
        {
            "date": "2010-12-15T13:20:40+0000",
            "content": "Patch looks great Simon \u2013 much simpler!  And faster  ",
            "author": "Michael McCandless",
            "id": "comment-12971675"
        },
        {
            "date": "2010-12-15T13:24:09+0000",
            "content": "added roberts prevent-useless-jump improvement to MatchOnlyScorer\nI think this is ready - I will commit shortly ",
            "author": "Simon Willnauer",
            "id": "comment-12971677"
        },
        {
            "date": "2010-12-15T19:36:36+0000",
            "content": "I did some faceting performance tests of solr trunk vs branch.\nThis is a 10M doc index w/ standard codec.  It looks like terms that match many docs got a little speedup, but terms that match few docs were slowed down.\n\nTrunk\n\n\n\nunique values in field\ncache.minDf=0\ncache.minDf=maxdoc\n\n\n10\n133\n208\n\n\n100\n176\n256\n\n\n1000\n214\n314\n\n\n10000\n433\n516\n\n\n100000\n2166\n2096\n\n\n10000000\n30974\n21093\n\n\n\n\n\nBranch:\n\n\n\nunique values in field\ncache.minDf=0\ncache.minDf=maxdoc\n\n\n10\n132\n187\n\n\n100\n171\n237\n\n\n1000\n223\n292\n\n\n10000\n487\n547\n\n\n100000\n3190\n3123\n\n\n10000000\n45071\n36504\n\n\n\n\n\nOne think trunk had was specialized per-segment code - it used MultiDocsEnum.getSubs(), so this is not necessarily a problem with the new bulk codec.\n\nupdate: to see if it was MultiFields related (i.e. that Solr's branch code does not yet work per-segment where trunk does) I optimized the index and reran the worst-case (10M unique values).  result: trunk=1.4sec, branch=25sec. ",
            "author": "Yonik Seeley",
            "id": "comment-12971800"
        },
        {
            "date": "2010-12-16T15:43:41+0000",
            "content": "I tested the optimized index with mike's latest patches (since that's \"per segment\" on both branch and trunk).  Things are much more in line now... with the branch being anywhere from 2.3% to 5.4% slower, depending on the exact field tested. ",
            "author": "Yonik Seeley",
            "id": "comment-12972119"
        },
        {
            "date": "2010-12-17T15:16:51+0000",
            "content": "patch with more refactoring of For/Pfor decompression:\n\n\tThe decompressors take DataInput, but still use the IntBuffer technique for now.\n\tI removed the wasted int-per-block in For.\n\n ",
            "author": "Robert Muir",
            "id": "comment-12972514"
        },
        {
            "date": "2010-12-18T17:04:51+0000",
            "content": "Here's a small patch that may be sufficient to enable dropping down to per-segment work while still using MultiTerms/MultiTermsEnum to traverse terms in order.  It basically makes the TermsEnumWithSlice members public, and adds a bulkPostings member for reuse.\n\nIs this the right approach? ",
            "author": "Yonik Seeley",
            "id": "comment-12972827"
        },
        {
            "date": "2010-12-18T21:17:47+0000",
            "content": "Looks good Yonik! ",
            "author": "Michael McCandless",
            "id": "comment-12972869"
        },
        {
            "date": "2010-12-19T13:53:30+0000",
            "content": "Here's a patch that changes just one pace in faceting to per-segment bulk.\nSame 10M doc index, testing w/ cache.minDf=maxdoc only (no use of filterCache since I haven't changed that to per-seg yet).  Time in ms.\n\n\n\n\nunique values in field\ntrunk per-seg\nbranch per-seg\nspeedup\n\n\n10\n161\n173\n-7%\n\n\n100\n217\n218\n-0%\n\n\n1000\n267\n262\n2%\n\n\n10000\n465\n325\n43%\n\n\n100000\n2025\n678\n199%\n\n\n10000000\n21061\n4393\n379%\n\n\n\n\n\nNow, facet.method=enum wasn't even designed for many unique values in a field, but this more efficient per-segment bulk code certainly expands the range where it's feasible.  My guess is that the speedup is due to us dropping to per-segment quicker with this patch (trunk get's a bulk enum, and then drops to per-segment).\n\nThe drop in performance for the high df field (each value will match ~1M docs) is curious.  Seems like this should be a more efficient inner loop, but I guess hotspot just optimized it differently.\n\nBased on these results, I'll re-convert the rest of the code to go per-segment too. ",
            "author": "Yonik Seeley",
            "id": "comment-12972989"
        },
        {
            "date": "2010-12-19T20:16:22+0000",
            "content": "Here's an update to faceting per seg w/ the new bulk API. ",
            "author": "Yonik Seeley",
            "id": "comment-12973042"
        },
        {
            "date": "2010-12-20T21:41:39+0000",
            "content": "attached is a silly codec, but i think useful when benchmarking:\nan intblock codec that encodes blocks with vint, with a single vint header\ncontaining the uncompressed length in bytes.\n\nI think this is useful because when comparing StandardCodec to FixedIntBlockCodecs such as FOR/PFOR, and also considering the bulk read API, there is a lot of variables in play... especially different I/O access patterns.\n\nso I think we should try to separate some of these variables out, and a FixedIntBlockCodec that uses Vint is useful to having a real \"baseline\" when looking at other compression algorithms, because FixedIntBlock is very different from standard already.\n\nhere's a comparison between Standard (bulkpostings branch) and VintBlock (bulkpostings branch):\nI used mmap on windows (but also tried simplefs with similar results)\n\nanyway, i think its interesting enough for future benchmarking i'd like to commit to the branch. we can\nlater delete this codec or simply change MockFixedIntBlock to work like this one.\n\n\n\n\nQuery\nQPS Standard\nQPS BulkVInt\nPct diff\n\n\nu*d\n12.43\n10.55\n-15.1%\n\n\nun*d\n45.05\n39.70\n-11.9%\n\n\n\"unit state\"~3\n3.75\n3.43\n-8.5%\n\n\n+nebraska +state\n69.97\n65.92\n-5.8%\n\n\nspanNear([unit, state], 10, true)\n3.01\n2.92\n-2.7%\n\n\nspanFirst(unit, 5)\n11.39\n11.35\n-0.4%\n\n\n\"unit state\"\n5.99\n5.99\n-0.1%\n\n\nunited~1.0\n9.27\n9.70\n4.6%\n\n\nunited~2.0\n1.95\n2.04\n4.9%\n\n\nunit~1.0\n5.89\n6.34\n7.6%\n\n\nunit~2.0\n5.76\n6.20\n7.8%\n\n\nunit state\n7.00\n8.05\n15.0%\n\n\n+unit +state\n9.11\n11.09\n21.7%\n\n\nunit*\n23.28\n29.50\n26.7%\n\n\nuni*\n13.29\n16.93\n27.4%\n\n\nstate\n21.43\n28.79\n34.3%\n\n\n\n\n ",
            "author": "Robert Muir",
            "id": "comment-12973371"
        },
        {
            "date": "2010-12-20T22:08:13+0000",
            "content": "looks like the culprit to the slower low-freq terms (and the culprit to larger index) is this in SepPostingsWriter:\n\n\n      // nocommit -- only write if docFreq > skipInterval?\n      docOut.writeVLong(skipOut.getFilePointer());\n\n\n\nI think this hurts the low freq terms and why we see slow wildcards etc. ",
            "author": "Robert Muir",
            "id": "comment-12973381"
        },
        {
            "date": "2010-12-20T22:27:12+0000",
            "content": "also the payloads pointer... we need this out of the .doc! ",
            "author": "Robert Muir",
            "id": "comment-12973389"
        },
        {
            "date": "2010-12-22T14:39:28+0000",
            "content": "Should we keep MultiBulkPostingsEnum?\nEven when someone writes their code to work per-segment, not all IndexReader implementations may be able to provide segment-level readers.  ParallelReader is one that can't currently? ",
            "author": "Yonik Seeley",
            "id": "comment-12974221"
        },
        {
            "date": "2010-12-22T16:41:50+0000",
            "content": "Should we keep MultiBulkPostingsEnum?\n\nI think we have to keep it.  EG if someone makes a SlowMultiReaderWrapper and then run searches on it...\n\nParallelReader is one that can't currently?\n\nParallelReader is a tricky one.\n\nIf your ParallelReader only contains SegmentReaders (and eg you make a MultiReader on top), then everything's great, because ParallelReader dispatches by field to a unique SegmentReader.\n\nBut if instead you make a ParallelReader whose child readers are themselves MultiReaders, then, yes it's basically the same as wrapping all of these subs in a SlowMultiReaderWrapper. ",
            "author": "Michael McCandless",
            "id": "comment-12974272"
        },
        {
            "date": "2011-01-12T21:42:04+0000",
            "content": "the blocker has been committed - we should merge though! ",
            "author": "Simon Willnauer",
            "id": "comment-12980949"
        },
        {
            "date": "2011-01-13T02:11:06+0000",
            "content": "I merged us up to yesterday (1052991:1057836), but stopped at the Pulsing codec rewrite \n\nMike can you assist in merging r1057897? Besides requiring a lot of beer there is a \ndanger of screwing it up, since we have to re-implement its bulk postings enum. ",
            "author": "Robert Muir",
            "id": "comment-12981083"
        },
        {
            "date": "2011-01-13T11:29:47+0000",
            "content": "I merged us up to yesterday (1052991:1057836),\n\nAwesome, thanks!!\n\nMike can you assist in merging r1057897? \n\nWill do. ",
            "author": "Michael McCandless",
            "id": "comment-12981222"
        },
        {
            "date": "2011-01-14T13:59:12+0000",
            "content": "Ok, we are caught up to trunk... but we need to integrate getBulkPostingsEnum with termstate to fix the nocommits in TermQuery.\n\nThis should also finally allow us to fix the cost of that extra per-segment docFreq. ",
            "author": "Robert Muir",
            "id": "comment-12981762"
        },
        {
            "date": "2011-01-14T14:44:42+0000",
            "content": "here is a fix for the nocommit robert put into TermQuery. All tests pass, i will commit in a bit ",
            "author": "Simon Willnauer",
            "id": "comment-12981776"
        },
        {
            "date": "2011-01-25T17:18:48+0000",
            "content": "This patch adds a BulkPostingsEnumWrapper that implement DocsEnumAndPositions by using the bulkpostings. I first just added this as a class to ease testsing for PositionDeltaBulks but it seems that this could be useful for more than just testing. Codecs that don't want to implement the DocsEnumAndPositions API can just use this wrapper to provide the functionality.\nI also added a testcase for MemoryIndex that uses this wrapper ",
            "author": "Simon Willnauer",
            "id": "comment-12986525"
        },
        {
            "date": "2011-01-25T18:11:11+0000",
            "content": "Simon, just took a quick glance (not a serious review, all the bulkpostings stuff is heavy).\n\nI agree with the idea that Codecs should only need to implement the bulk api at a minimum:\nif all serious stuff (queries) is using these bulk apis, then the \"friendly\" iterator methods\ncan simply be a wrapper over it.\n\nbut separately, i know there are some performance degradations with the bulk APIs today\nversus trunk... (with the same index). I know if i use other fixed-int codecs i see these same\nproblems, so I dont think its just Standard's implementation: pretty sure the issue is somewhere\nwith advance()/jump().\n\nI really wish we could debug whatever this performance problem is, just in case the bulk APIs\nthemselves need changing... a little concerned about them at the moment thats all...\nnot sure it should stand in the way of your patch, just saying I don't like the performance\nregression.\n ",
            "author": "Robert Muir",
            "id": "comment-12986568"
        },
        {
            "date": "2011-01-26T08:30:22+0000",
            "content": "\nI really wish we could debug whatever this performance problem is, just in case the bulk APIs\nthemselves need changing... a little concerned about them at the moment thats all...\nnot sure it should stand in the way of your patch, just saying I don't like the performance\nregression\n\nyeah I agree - I think we should open a separate issue to figure out what the problem here is. Unrelated to this, the wrapper gives me the ability to test the bulks apis easily together with the enum API which is valuable in any case. I am opening a new issue and commit that latest patch to the branch with that wrapper moved to /src/test. We can still move it to /src/java later though.\n ",
            "author": "Simon Willnauer",
            "id": "comment-12986891"
        },
        {
            "date": "2011-02-01T20:49:12+0000",
            "content": "Simon, the bulk enum wrapper looks great!  But maybe we should merge in the BulkPayload additions (from LUCENE-2878), and that way the wrapper is fully functional? ",
            "author": "Michael McCandless",
            "id": "comment-12989377"
        },
        {
            "date": "2012-09-11T16:23:54+0000",
            "content": "We nuked the low level bulk postings API... and BlockPostingsFormat now does bulk reads under the hood and gives great performance ... ",
            "author": "Michael McCandless",
            "id": "comment-13453149"
        },
        {
            "date": "2013-05-10T10:33:25+0000",
            "content": "Closed after release. ",
            "author": "Uwe Schindler",
            "id": "comment-13653920"
        }
    ]
}