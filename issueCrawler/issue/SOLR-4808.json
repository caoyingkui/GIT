{
    "id": "SOLR-4808",
    "title": "Persist and use router,replicationFactor and maxShardsPerNode at Collection and Shard level",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.5",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "The replication factor for a collection as of now is not persisted and used while adding replicas.\nWe should save the replication factor at collection factor as well as shard level.",
    "attachments": {
        "SOLR-4808.patch": "https://issues.apache.org/jira/secure/attachment/12589619/SOLR-4808.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-13654660",
            "date": "2013-05-10T17:49:47+0000",
            "content": "Yeah, this is mainly because this seems like a largish issue to deal with to make good use of it, so it's sort of been a long term plan.\n\nBecause you can create collections and add/remove from them without the collections api or overseer right now, you could persist a replicationFactor that is simply wrong and not adjusted for. \n\nMy long term plan for this has been:\n\nAs the collections api gets better, stop supporting creation of collections by configuration.\n\nHave the Overseer optionally enforce the replicationFactor over time - if too few nodes are up, it creates new ones, if for some reason too many are up, it removes some - using some plugable algorithm.\n\nAs part of this, we can also make ZooKeeper the truth for cluster state - so for example, if you delete a collection and then bring back an old node with a core for that collection, the Solr instance can know that it should simply remove that core and not bring the collection back to life.\n\nOne step at a time, but I think there is a lot to do to take advantage of the persisted replicationFactor in a way that makes sense - it wouldn't be too cool if we persisted a repilcation factor of 4 and then the core admin api was used to make it actually 6 and the spec never matched with the model again. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13690216",
            "date": "2013-06-21T12:13:20+0000",
            "content": "We need to redefine replicationFactor as the minimum no:of replicas for a given shard. The cluster will have atleast those many no:of replicas for that given shard. If the number falls below , that the overseer should attempt to bring the number upto that.\n\nWhat if a node is asked to join a shard which has enough replicas according to the replicationFactor? It should be allowed to do so but the replicationFactor should remain same till it is changed through the cluster admin command.\n\nreplicationFactor value must be persisted per shard . We should be able to manipulate the value\n\n\tper shard : relevant for custom sharding\n\tentire cluster : for hash based setup the user would just update it cluster wide\n\tshard.keys values : for composite id collections it make sense to change the value by providing just the keys and the command can identify the appropriate shards\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13690274",
            "date": "2013-06-21T13:43:25+0000",
            "content": "We need to redefine replicationFactor as the minimum no:of replicas for a given shard. The cluster will have atleast those many no:of replicas for that given shard. If the number falls below , that the overseer should attempt to bring the number upto that.\n\nI don't think it needs to necessarily be redefined - you are specifying the replicationFactor you would like, not the minimum replicationFactor you'd like.\n\nIf the number falls below , that the overseer should attempt to bring the number upto that.\n\nI think any auto cluster changes should be optional - by default we should not meddle. And one of those options should allow trying to reduce to the right replicationFactor if you have too many the same way you might raise to it.\n\nWhat if a node is asked to join a shard which has enough replicas according to the replicationFactor?\n\nFor this mode, we should not allow preconfigured cores - so this shouldn't happen - if it does for some weird reason, depending on your config, the node should be removed shortly after by the overseer - I think the way you should raise the number of replicas is to use the collections api to change the replicationFactor.\n\nOr don't use this mode where the Overseer keeps things in line and it's up to you to manually do what you want. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13690289",
            "date": "2013-06-21T13:55:25+0000",
            "content": "For this mode, we should not allow preconfigured cores - so this shouldn't happen - if it does for some weird reason, depending on your config, the node should be removed shortly after by the overseer - I think the way you should raise the number of replicas is to use the collections api to change the replicationFactor.\n\nNoble and I had a discussion about this. What you say is right but the problem is about back-compat at least until the 4.x releases continue. In 5.0 we can stop supporting collection creation via startup configuration. But until that time we need to decide whether we throw an exception if a user starts up a pre-configured core or treat replication factor as a minimum value and let the core be added to the shard. After our discussion Noble and I were leaning towards the latter option.\n\nI have started to work on this. I'm going to create some issues around APIs to modify replication factor and maxShardsPerNode. I also plan to store replication factor (at a shard level only) and start using these values in Overseer. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13690305",
            "date": "2013-06-21T14:08:47+0000",
            "content": "What you say is right but the problem is about back-compat at least until the 4.x releases continue.\n\nI think we should have a simple config to deal with back compat - if you turn it on, you are in this smarter mode. By default, predefined cores work, you don't get this feature set - when you turn it on, predefined cores are out the window and you get this feature set. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13690307",
            "date": "2013-06-21T14:09:35+0000",
            "content": "It seems like we want a collection level replicationFactor so it's easy to change for the whole collection (if you want to scale up/down to meet increased query traffic for instance), and to remove the burden of having to specify it for each shard created. Consequently, we probably shouldn't store a replicationFactor at the shard level unless it has been explicitly set.  "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13690315",
            "date": "2013-06-21T14:16:13+0000",
            "content": "It seems like we want a collection level replicationFactor so it's easy to change for the whole collection\n\nThe API to change replication factor can work at both levels. If you specify a shard name we change the factor for that shard otherwise we change it for the whole collection. Considering that it is nice to have a replication factor per shard for custom sharding use-cases, I think it is best that we store it only at a per-shard level. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13690318",
            "date": "2013-06-21T14:18:24+0000",
            "content": "I think we should have a simple config to deal with back compat - if you turn it on, you are in this smarter mode. By default, predefined cores work, you don't get this feature set - when you turn it on, predefined cores are out the window and you get this feature set.\n\nOkay that sounds good to me. It is easier to transition to 5.x this way \u2013 just throw away the non-smart mode. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13690322",
            "date": "2013-06-21T14:23:03+0000",
            "content": "We need to redefine replicationFactor\n\nreplicationFactor has always been the \"target\".  The property does not fully define how that target is generally met, and \nit doesn't not mean that if the target isn't exactly met that the cluster is invalid somehow.\n\nWhat if a node is asked to join a shard which has enough replicas according to the replicationFactor? It should be allowed to do so but the replicationFactor should remain same till it is changed through the cluster admin command.\n\nAgreed - if it's explicit, we should allow it.  I'm not sure I see the problem here. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13690522",
            "date": "2013-06-21T17:57:14+0000",
            "content": "Agreed - if it's explicit, we should allow it. I'm not sure I see the problem here.\n\nSeems like we have two conflicting opinions here? Mark wants to have two different options to allow/disallow pre-configured cores.\n\nWhich one should we go for? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13690597",
            "date": "2013-06-21T18:57:03+0000",
            "content": "After a little chatting on #solr-dev, I think I understand the source of my confusion...\nI took this \"What if a node is asked to join a shard which has enough replicas according to the replicationFactor?\" to mean there was an explicit operator request for a node to join a shard (which should always succeed if possible IMO).\n\nThe issue seems to be more that when a node is brought up, we don't know if the shards it has locally were explicitly configured by the user, or just the result of previous automatic shard assignments.  It seems like we should assume ZK has the truth (i.e. assume the latter).  We also don't want shards coming back from the dead (say we deleted a shard, but one of the replicas was down at the time).  So I think I'm agreeing with Mark's \"we should not allow preconfigured cores\".\n\nIt also seems like it would be a very useful feature to be able to dynamically tell the cluster \"stop trying to fix things temporarily\" via API (i.e. it shouldn't just be a back compat thing).\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13691069",
            "date": "2013-06-22T06:41:11+0000",
            "content": "It also seems like it would be a very useful feature to be able to dynamically tell the cluster \"stop trying to fix things temporarily\" via API.(i.e. it shouldn't just be a back compat thing).\n\nyes . Having this property as a feature will help a lot . We should have a set of cluster-wide properties which are editable via an EDITCOLLECTION command. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13692098",
            "date": "2013-06-24T16:11:25+0000",
            "content": "Here's what I have in mind:\n\nSolrCloud shall store the following two properties in addition to the ones it already does:\n\n\tsmartCloud (boolean) - Stored per collection\n\tautoManageCluster - Stored per collection\n\tmaxShardsPerNode - Stored per collection\n\treplicationFactor - Stored per slice\n\n\n\nThe smartCloud parameter must be specified in the create collection API. The smartCloud mode will default to false for all existing collections. The autoManageCluster, maxShardsPerNode and replicationFactor properties will be used in SolrCloud only if smartCloud=true for that collection.\n\nIf a collection is running in smart mode then it will assume the cluster state in ZK to be the truth i.e. it may re-assign pre-configured cores when they join/re-join the cluster.\n\nIf autoManageCluster=true then Overseer will attempt to keep the cluster running according to the values of replication factor and maxShardsPerNode i.e. it will increase/decrease replicas to match replication factor and maxShardsPerNode\n\nA new \u201ceditcollection\u201d API will be introduced to:\nModify value of smartCloud\nModify value of autoManageCluster\nModify values of maxShardsPerNode for the collection\nModify value of replicationFactor for entire collection (apply to each and every slice)\nModify values of replicationFactor on a per-slice basis\n\nThe way to add a node to the collection would be to 1) increase the replication factor (of a collection or shard) and then 2) start a node\n\nThe smartCloud property can be turned off via editcollection to allow users to pre-configure nodes if they want to.\n\nI'm not working on the \"autoManageCluster\" right now. I welcome suggestions for alternate names of the \"smartCloud\" property. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13692104",
            "date": "2013-06-24T16:20:33+0000",
            "content": "smartCloud (boolean) - Stored per collection \nautoManageCluster - Stored per collection\n\nI don't see a reason to have two parameters. It is quite confusing. We should have only one property . "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13692117",
            "date": "2013-06-24T16:31:20+0000",
            "content": "I don't see a reason to have two parameters. It is quite confusing. We should have only one property \n\nThere are two things that we want:\n\n\tUse ZK as the truth and assign nodes accordingly as and when they join the cluster\n\tIncrease/decrease replicas on-the-fly automatically using the replication factor and maxShardsPerNode properties\n\n\n\nWhile the former affects only those nodes that are joining the cluster, the latter affects active and available nodes. It makes sense to have a switch to disable #2. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13692123",
            "date": "2013-06-24T16:37:31+0000",
            "content": "Another idea that I've been thinking about for some time is to use the Solr Admin GUI to give hints to users instead of automatically managing the cluster completely. This can help us get rid of an additional config param (autoManageCluster) and reduce growing pains of such a feature by letting it \"bake\" for a while in real world installations without causing them damage. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13692124",
            "date": "2013-06-24T16:38:04+0000",
            "content": "Use ZK as the truth and assign nodes accordingly as and when they join the cluster\nIncrease/decrease replicas on-the-fly automatically using the replication factor and maxShardsPerNode properties\n\nWhy don't we have explicit names if that is the objective. terms like 'smart' 'auto' are very easy to be misinterpreted. An in the future , we may add a few more properties and we will always be wondering whether it is a part of the 'smart' thing or not "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13692130",
            "date": "2013-06-24T16:42:33+0000",
            "content": "autoManageCluster - Stored per collection\n\nWe might want to be more explicit since there will likely be more and more aspects to managing a cluster going forward.\nPerhaps \"autoRebalance\", or if there is a use-case for turning off one side or the other, \"autoCreateReplicas\" and \"autoDestroyReplicas\"? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13692199",
            "date": "2013-06-24T18:10:44+0000",
            "content": "Why don't we have explicit names if that is the objective. terms like 'smart' 'auto' are very easy to be misinterpreted. An in the future , we may add a few more properties and we will always be wondering whether it is a part of the 'smart' thing or not\n\n+1. I used this only as a place holder. Any suggestions?\n\nWe might want to be more explicit since there will likely be more and more aspects to managing a cluster going forward. Perhaps \"autoRebalance\", or if there is a use-case for turning off one side or the other, \"autoCreateReplicas\" and \"autoDestroyReplicas\"?\n\n+1 for autoCreateReplicas and autoDestroyReplicas.\n\nHow do people feel about my suggestion on giving hints via GUI for a start (instead of full-blown automatic cluster management)? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13693141",
            "date": "2013-06-25T16:05:56+0000",
            "content": "This patch changes the way the collection creation is perfromed. The OverseerCollectionProcessor sends a message to Overseer to create a collection with all the parameters it received via the collection 'create' command . After the empty collection is created it proceeds to create the nodes.\n\nThe core create no more needs numShards parameter , nor does it send it to Overseer when it has to register itself\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13696952",
            "date": "2013-07-01T16:52:07+0000",
            "content": "Patch builds over Noble's work.\n\nChanges:\n\n\tNew mandatory \"collectionApiMode\" parameter during create collection command (we can think of a better name)\n\tWhen api mode is enabled, zk is used as truth and shardId is always looked up from cluster state\n\treplicationFactor is persisted at slice level\n\tmaxShardsPerNode is persisted at collection level\n\tBasic testing in CollectionAPIDistributedZkTest\n\n\n\nI'm working on more tests and a modifyCollection API which can be used to change these values. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13703627",
            "date": "2013-07-09T18:49:33+0000",
            "content": "New mandatory \"collectionApiMode\" parameter during create collection command (we can think of a better name)\n\nEh, internally mandatory I hope (as in, the user should not have to specify it?)\n\nreplicationFactor is persisted at slice level\n\nIt still feels like this should be a collection level property that we have the ability to store/override on a per-shard level.\nThe reasons off the top of my head:\n\n\twould be nice to be able to create a new shard w/o having to know/specify what the replication factor currently is\n\tpossible to completely lose the replication factor if we delete a shard and re-add a new one\n\tthere may be one shard that has a lot of demand and you set it's replication level high... so you override the replicationFactor for that shard only.  It would still be nice to be able to adjust the replication factor for everyone else (by adjusting the collection level replicationFactor)\n\n\n\n\"maxShardsPerNode\" - should that be maxReplicasPerNode, or are we really talking logical shards? "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13725495",
            "date": "2013-07-31T17:53:04+0000",
            "content": "Commit 1508968 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1508968 ]\n\nSOLR-4221 SOLR-4808 SOLR-5006 SOLR-5017 SOLR-4222 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13725550",
            "date": "2013-07-31T18:28:05+0000",
            "content": "Commit 1508981 from Noble Paul in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1508981 ]\n\nSOLR-4221 SOLR-4808 SOLR-5006 SOLR-5017 SOLR-4222 "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13725568",
            "date": "2013-07-31T18:35:49+0000",
            "content": "The 'collectionApiMode' feature will be tracked as apart of another issue "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13725586",
            "date": "2013-07-31T18:45:58+0000",
            "content": "I opened SOLR-5096 "
        },
        {
            "author": "Adrien Grand",
            "id": "comment-13787144",
            "date": "2013-10-05T10:19:26+0000",
            "content": "4.5 release -> bulk close "
        }
    ]
}