{
    "id": "SOLR-3881",
    "title": "frequent OOM in LanguageIdentifierUpdateProcessor",
    "details": {
        "affect_versions": "4.0",
        "status": "Closed",
        "fix_versions": [
            "4.10.4",
            "5.0",
            "6.0"
        ],
        "components": [
            "update"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "We are seeing frequent failures from Solr causing it to OOM. Here is the stack trace we observe when this happens:\n\n\nCaused by: java.lang.OutOfMemoryError: Java heap space\n        at java.util.Arrays.copyOf(Arrays.java:2882)\n        at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:100)\n        at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:390)\n        at java.lang.StringBuffer.append(StringBuffer.java:224)\n        at org.apache.solr.update.processor.LanguageIdentifierUpdateProcessor.concatFields(LanguageIdentifierUpdateProcessor.java:286)\n        at org.apache.solr.update.processor.LanguageIdentifierUpdateProcessor.process(LanguageIdentifierUpdateProcessor.java:189)\n        at org.apache.solr.update.processor.LanguageIdentifierUpdateProcessor.processAdd(LanguageIdentifierUpdateProcessor.java:171)\n        at org.apache.solr.handler.BinaryUpdateRequestHandler$2.update(BinaryUpdateRequestHandler.java:90)\n        at org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readOuterMostDocIterator(JavaBinUpdateRequestCodec.java:140)\n        at org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readIterator(JavaBinUpdateRequestCodec.java:120)\n        at org.apache.solr.common.util.JavaBinCodec.readVal(JavaBinCodec.java:221)\n        at org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readNamedList(JavaBinUpdateRequestCodec.java:105)\n        at org.apache.solr.common.util.JavaBinCodec.readVal(JavaBinCodec.java:186)\n        at org.apache.solr.common.util.JavaBinCodec.unmarshal(JavaBinCodec.java:112)\n        at org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec.unmarshal(JavaBinUpdateRequestCodec.java:147)\n        at org.apache.solr.handler.BinaryUpdateRequestHandler.parseAndLoadDocs(BinaryUpdateRequestHandler.java:100)\n        at org.apache.solr.handler.BinaryUpdateRequestHandler.access$000(BinaryUpdateRequestHandler.java:47)\n        at org.apache.solr.handler.BinaryUpdateRequestHandler$1.load(BinaryUpdateRequestHandler.java:58)\n        at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:59)\n        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:129)\n        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1540)\n        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:435)\n        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:256)\n        at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1337)\n        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:484)\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:119)\n        at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:524)\n        at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:233)\n        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1065)\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:413)\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:192)\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:999)",
    "attachments": {
        "SOLR-3881.patch": "https://issues.apache.org/jira/secure/attachment/12551566/SOLR-3881.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Rob Tulloh",
            "id": "comment-13462943",
            "date": "2012-09-25T16:44:39+0000",
            "content": "One possible solution is to limit the size of the string that is selected for concatenation. I don't know if there is a good way to guess which part of the string to select. Just picking the first 64K seems like a reasonable solution with the lack of any heuristics to suggest otherwise. Thoughts?\n\nIn LanguageIdentifierUpdateProcessor.concatFields:\n\n          String fieldValue = (String) doc.getFieldValue(fieldName);\n          if (fieldValue != null && fieldValue.length() > 0) {\n             fieldValue = fieldValue.substring(0,Math.min(65536,fieldValue.length()));\n          }\n          sb.append(fieldValue);\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13476584",
            "date": "2012-10-15T23:49:53+0000",
            "content": "One possible solution is to limit the size of the string that is selected for concatenation.\n\nI don't know if there is anyway to make LanguageIdentifierUpdateProcessor more memory efficient (in particular, i'm not sure why it needs to concat the field values instead of operating on them directly) but if you want to give langId just the first N characters of another field: that should already be possible w/o cod changes by wiring together the  CloneFieldUpdateProcessorFactory with the TruncateFieldUpdateProcessorFactory.\n\nSomething like this should work...\n\n\n ...\n <processor class=\"solr.CloneFieldUpdateProcessorFactory\">\n   <str name=\"source\">GIANT_HONKING_STRING_FIELD</str>\n   <str name=\"dest\">truncated_string_field_for_lang_detect</str>\n </processor>\n <processor class=\"solr.TruncateFieldUpdateProcessorFactory\">\n   <str name=\"fieldName\">truncated_string_field_for_lang_detect</str>\n   <int name=\"maxLength\">65536</int>\n </processor>\n <processor class=\"solr.LangDetectLanguageIdentifierUpdateProcessorFactory\">\n   <!-- <str name=\"langid.fl\">title,subject,GIANT_HONKING_STRING_FIELD</str> -->\n   <str name=\"langid.fl\">title,subject,truncated_string_field_for_lang_detect</str>\n   ...\n </processor>\n <processor class=\"solr.IgnoreFieldUpdateProcessorFactory\">\n   <str name=\"fieldName\">truncated_string_field_for_lang_detect</str>\n </processor>\n ...\n\n\n\nNeither CloneFieldUpdateProcessorFactory nor TruncateFieldUpdateProcessorFactory will make a full copy of the original String value, and TruncateFieldUpdateProcessorFactory will only make a truncated copy if the sources is longer then the configured max (and even then wether any copy is actaully made really just depends on how the JVM implements substring). IgnoreFieldUpdateProcessorFactory will ensure that the truncated copy is freed up for GC as soon as you are done with LangId. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13476942",
            "date": "2012-10-16T11:47:33+0000",
            "content": "I'm sure it's possible to optimize memory footprint somehow. The reason why we concat all \"fl\" fields before detection was originally because Tika's detector gets better and better the longer input text you have. So while detection for individual short fields have a high risk of mis-detection, the resulting concatenated string has a better chance.\n\nA configurable max-cap in the concatenation may make sense, as the detection accuracy flattens out after some threshold.\n\nPerhaps we could also avoid the expandCapacity() and Ararys.copyOf() calls if we pre-allocate the StringBuffer with the theoretical max size, being the size of our SolrInputDoc. If StringBuffer is at 10kb and needs an extra 10b for an append, it will allocate a new buffer of (10kb+1)*2 capacity which is a waste. We should also switch to StringBuilder which is more performant. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13477104",
            "date": "2012-10-16T15:55:29+0000",
            "content": "The reason why we concat all \"fl\" fields before detection was originally because Tika's detector gets better and better the longer input text you have.\n\nBut is it possible to give Tika a String[] or List<String> instead of concating everything into a single String? "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13477123",
            "date": "2012-10-16T16:18:10+0000",
            "content": "The langdetect implementation can append each piece at a time.\n\nIt can also take reader: append(Reader), but that is really just syntactic sugar forwarding to append(String)\nand not exceeding the Detector.max_text_length.\n\nSeems like the concatenating stuff should be pushed out of the base class into the Tika impl. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13477426",
            "date": "2012-10-16T22:45:31+0000",
            "content": "Probably built-in truncation is enough to avoid the OOMs, and we could refactor the multi string append if neccesary later. "
        },
        {
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-13486862",
            "date": "2012-10-30T13:33:00+0000",
            "content": "Do you think it would make more sense to limit each append (for the different fields) or to limit the total size of the buffer/builder (stop appending fields when the maximum was reached)? Both ways would prevent OOM, however they could give different results. "
        },
        {
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-13487865",
            "date": "2012-10-31T15:36:34+0000",
            "content": "Added the proposed changes "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13717135",
            "date": "2013-07-23T18:47:13+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 "
        },
        {
            "author": "Vitaliy Zhovtyuk",
            "id": "comment-13961420",
            "date": "2014-04-06T14:39:40+0000",
            "content": "Updated to latest trunk.\nFixed multivalue support.\nAdded string size calculation as string builder capacity. Used to prevent multiple array allocation on append. (Maybe also need to be configurable - for large documents only)  "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13971157",
            "date": "2014-04-16T12:57:19+0000",
            "content": "Move issue to Solr 4.9. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-14062122",
            "date": "2014-07-15T14:28:19+0000",
            "content": "Added string size calculation as string builder capacity. Used to prevent multiple array allocation on append. (Maybe also need to be configurable - for large documents only)\n\nVitaliy Zhovtyuk, I agree - I think we should have two configurable limits: max chars per field value (already in Tom\u00e1s Fern\u00e1ndez L\u00f6bbe and your updated patches), and a max total chars (not there yet).\n\nTom\u00e1s wrote:\nDo you think it would make more sense to limit each append (for the different fields) or to limit the total size of the buffer/builder (stop appending fields when the maximum was reached)? Both ways would prevent OOM, however they could give different results.\n\nI think we should have both limits.\n\nI think it's more important, though, to do as Robert Muir said earlier in this issue: \n\n\nThe langdetect implementation can append each piece at a time.\n\nIt can also take reader: append(Reader), but that is really just syntactic sugar forwarding to append(String)\nand not exceeding the Detector.max_text_length.\n\nSeems like the concatenating stuff should be pushed out of the base class into the Tika impl.\n\nSee http://language-detection.googlecode.com/svn/trunk/doc/com/cybozu/labs/langdetect/Detector.html#setMaxTextLength(int) - the default is 10K chars - we can pass the configured max total chars here.\n\nWe should also set default maxima for both per-value and total chars, rather than MAX_INT, as in the current patch. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-14062157",
            "date": "2014-07-15T14:52:34+0000",
            "content": "See http://language-detection.googlecode.com/svn/trunk/doc/com/cybozu/labs/langdetect/Detector.html#setMaxTextLength(int) - the default is 10K chars - we can pass the configured max total chars here.\n\nThe default is actually 10K bytes, not chars, so we'd need to divide by two when passing the configured max total chars.\n\nedit Disregard the above comment; the javadocs refer to \"10KB\" as the default max text length, but Detector.append() uses the max_text_length config as a max number of chars. "
        },
        {
            "author": "Vitaliy Zhovtyuk",
            "id": "comment-14070862",
            "date": "2014-07-22T20:43:54+0000",
            "content": "Added global limit to concatenated string\nAdded limit to detector to detector.setMaxTextLength(maxTotalAppendSize);\n\nAbout moving org.apache.solr.update.processor.LanguageIdentifierUpdateProcessor#concatFields to org.apache.solr.update.processor.TikaLanguageIdentifierUpdateProcessor it's a bit unclear because concatFields is used in both org.apache.solr.update.processor.TikaLanguageIdentifierUpdateProcessor and org.apache.solr.update.processor.LangDetectLanguageIdentifierUpdateProcessor "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-14072529",
            "date": "2014-07-23T23:14:29+0000",
            "content": "Vitaliy, those changes look good.\n\nAbout moving concatFields() to the tika language identifier: I think the way to go is just move the whole method there, then change the detectLanguage() method to take the SolrInputDocument instead of a String.  You don't need to carry over the field[] parameter from concatFields(), since data member inputFields will be accessible everywhere it's needed. \n\nI should have mentioned previously: I don't like the maxAppendSize and maxTotalAppendSize names - \"size\" is ambiguous (could refer to bytes, chars, whatever), and \"append\" refers to an internal operation... I'd like to see \"append\"=>\"field value\" and \"size\"=>\"chars\": maxFieldValueChars, and maxTotalChars (since appending doesn't need to be mentioned for the global limit).  The same thing goes for the default constants and the test method names.\n\nSome minor issues I found with your patch:\n\n\n\tAs I said previously: \"We should also set default maxima for both per-value and total chars, rather than MAX_INT, as in the current patch.\"\n\tThe total chars default should be its own setting; I was thinking we could make it double the per-value default?\n\tIt's better not to reorder import statements unless you're already making significant changes to them; it distracts from the meat of the change.  (You reordered them in LangDetectLanguageIdentifierUpdateProcessor and LanguageIdentifierUpdateProcessorFactoryTestCase)\n\tIn LanguageIdentifierUpdateProcessor.concatFields(), when you trim the concatenated text to maxTotalAppendSize, I think StringBuilder.setLength(maxTotalAppendSize); would be more efficient than StringBuilder.delete(maxTotalAppendSize, sb.length() - 1);\n\tIn addition to the test you added for the global limit, we should also test using both the per-value and global limits at the same time.\n\n\n\n "
        },
        {
            "author": "Vitaliy Zhovtyuk",
            "id": "comment-14086730",
            "date": "2014-08-05T20:50:51+0000",
            "content": "About moving concatFields() to the tika language identifier: I think the way to go is just move the whole method there, then change the detectLanguage() method to take the SolrInputDocument instead of a String. You don't need to carry over the field[] parameter from concatFields(), since data member inputFields will be accessible everywhere it's needed.\n[VZ] This call looks more cleaner now, i changed inputFields to private now to reduce visibility scope\n\nI should have mentioned previously: I don't like the maxAppendSize and maxTotalAppendSize names - \"size\" is ambiguous (could refer to bytes, chars, whatever), and \"append\" refers to an internal operation... I'd like to see \"append\"=>\"field value\" and \"size\"=>\"chars\": maxFieldValueChars, and maxTotalChars (since appending doesn't need to be mentioned for the global limit). The same thing goes for the default constants and the test method names.\n[VZ] Renamed parameters and test methods\n\nSome minor issues I found with your patch:\nAs I said previously: \"We should also set default maxima for both per-value and total chars, rather than MAX_INT, as in the current patch.\"\nThe total chars default should be its own setting; I was thinking we could make it double the per-value default?\n[VZ] added default value to maxTotalChars and changed both to 10K like in com.cybozu.labs.langdetect.Detector.maxLength\n\nIt's better not to reorder import statements unless you're already making significant changes to them; it distracts from the meat of the change. (You reordered them in LangDetectLanguageIdentifierUpdateProcessor and LanguageIdentifierUpdateProcessorFactoryTestCase)\n[VZ] This is IDE optimization to put imports in alphabetical order - restored it to original order\n\nIn LanguageIdentifierUpdateProcessor.concatFields(), when you trim the concatenated text to maxTotalAppendSize, I think StringBuilder.setLength(maxTotalAppendSize); would be more efficient than StringBuilder.delete(maxTotalAppendSize, sb.length() - 1);\n[VZ] Yep, cleaned that \n\nIn addition to the test you added for the global limit, we should also test using both the per-value and global limits at the same time.\n[VZ] Tests for both limits added "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-14088223",
            "date": "2014-08-06T20:44:28+0000",
            "content": "Vitaliy, thanks for the changes.\n\nI see a few more issues in your latest patch:\n\n\n\tLangDetectLanguageIdentifierUpdateProcessor.detectLanguage() still uses concatFields(), but it shouldn't \u2013 that was the whole point about moving it to TikaLanguageIdentifierUpdateProcessor; instead,  LangDetectLanguageIdentifierUpdateProcessor.detectLanguage() should loop over inputFields and call detector.append() (similarly to what concatFields() does).\n\tconcatFields() and getExpectedSize() should move to TikaLanguageIdentifierUpdateProcessor.\n\tLanguageIdentifierUpdateProcessor.getExpectedSize() still takes a maxAppendSize, which didn't get renamed, but that param could be removed entirely, since maxFieldValueChars is available as a data member.\n\tThere are a bunch of whitespace changes in LanguageIdentifierUpdateProcessorFactoryTestCase.java - it makes reviewing patches significantly harder when they include changes like this.  Your IDE should have settings that make it stop doing this.\n\tThere is still some import reordering in TikaLanguageIdentifierUpdateProcessor.java.\n\n\n\nOne last thing:\n\n\nThe total chars default should be its own setting; I was thinking we could make it double the per-value default?\n\n[VZ] added default value to maxTotalChars and changed both to 10K like in com.cybozu.labs.langdetect.Detector.maxLength\n\nThanks for adding the total chars default, but you didn't make it double the field value chars default, as I suggested.  Not sure if that's better - if the user specifies multiple fields and the first one is the only one that's used to determine the language because it's larger than the total char default, is that an issue?  I was thinking that it would be better to visit at least one other field (hence the idea of total = 2 * per-field), but that wouldn't fully address the issue.  What do you think? "
        },
        {
            "author": "Vitaliy Zhovtyuk",
            "id": "comment-14230080",
            "date": "2014-12-01T17:29:04+0000",
            "content": "1. LangDetectLanguageIdentifierUpdateProcessor.detectLanguage() still uses concatFields(), but it shouldn't \u2013 that was the whole point about moving it to TikaLanguageIdentifierUpdateProcessor; instead, LangDetectLanguageIdentifierUpdateProcessor.detectLanguage() should loop over inputFields and call detector.append() (similarly to what concatFields() does).\n[VZ] LangDetectLanguageIdentifierUpdateProcessor.detectLanguage() changed to use old flow with limit on field and max total on detector.\nEach field value appended to detector.\n\n2. concatFields() and getExpectedSize() should move to TikaLanguageIdentifierUpdateProcessor.\n[VZ] Moved to TikaLanguageIdentifierUpdateProcessor. Tests using concatFields() moved to TikaLanguageIdentifierUpdateProcessorFactoryTest.\n\n3. LanguageIdentifierUpdateProcessor.getExpectedSize() still takes a maxAppendSize, which didn't get renamed, but that param could be removed entirely, since maxFieldValueChars is available as a data member.\n[VZ] Argument removed.\n\n4. There are a bunch of whitespace changes in LanguageIdentifierUpdateProcessorFactoryTestCase.java - it makes reviewing patches significantly harder when they include changes like this. Your IDE should have settings that make it stop doing this.\n[VZ] Whitespaces removed.\n\n5. There is still some import reordering in TikaLanguageIdentifierUpdateProcessor.java.\n[VZ] Fixed.\n\nOne last thing:\nThe total chars default should be its own setting; I was thinking we could make it double the per-value default?\n[VZ] added default value to maxTotalChars and changed both to 10K like in com.cybozu.labs.langdetect.Detector.maxLength\nThanks for adding the total chars default, but you didn't make it double the field value chars default, as I suggested. Not sure if that's better - if the user specifies multiple fields and the first one is the only one that's used to determine the language because it's larger than the total char default, is that an issue? I was thinking that it would be better to visit at least one other field (hence the idea of total = 2 * per-field), but that wouldn't fully address the issue. What do you think?\n[VZ] i think in most cases it will be only one field, but since both parameters are optional we should not restrict result if only per field specified more then 10K.\nUpdated total default value to 20K.  "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-14233220",
            "date": "2014-12-03T17:35:23+0000",
            "content": "Thanks Vitaliy, looks great.\n\n[VZ] ... since both parameters are optional we should not restrict result if only per field specified more then 10K.\n\nI agree - I added code to LanguageIdentifierUpdateProcessor.initParams() to check if maxFieldValueChars > maxTotalChars, and if so, set maxTotalChars to the value of maxFieldValueChars and log a warning.\n\nAttaching a version of the patch that includes the above change, as well as a CHANGES.txt entry.\n\nI'll commit when Subversion lets me (can't connect right now). "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-14233237",
            "date": "2014-12-03T17:47:06+0000",
            "content": "I added code to LanguageIdentifierUpdateProcessor.initParams() to check if maxFieldValueChars > maxTotalChars, and if so, set maxTotalChars to the value of maxFieldValueChars and log a warning.\n\nThe above was too simplistic.  If maxFieldValueChars > maxTotalChars and the user only set maxTotalChars, then the reverse should happen: maxFieldValueChars should be set maxTotalChars.\n\nThe attached patch fixes this, and allows the tests to pass. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14235843",
            "date": "2014-12-05T18:22:39+0000",
            "content": "Commit 1643377 from Use account \"steve_rowe\" instead in branch 'dev/trunk'\n[ https://svn.apache.org/r1643377 ]\n\nSOLR-3881: Avoid OOMs in LanguageIdentifierUpdateProcessor:\n\n\tAdded langid.maxFieldValueChars and langid.maxTotalChars params to limit\n  input, by default 10k and 20k chars, respectively.\n\tMoved input concatenation to Tika implementation; the langdetect\n  implementation instead appends each input piece via the langdetect API.\n\n "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14235896",
            "date": "2014-12-05T18:54:02+0000",
            "content": "Commit 1643390 from Use account \"steve_rowe\" instead in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1643390 ]\n\nSOLR-3881: Avoid OOMs in LanguageIdentifierUpdateProcessor:\n\n\tAdded langid.maxFieldValueChars and langid.maxTotalChars params to limit\n  input, by default 10k and 20k chars, respectively.\n\tMoved input concatenation to Tika implementation; the langdetect\n  implementation instead appends each input piece via the langdetect API. \n(merged trunk r1643377)\n\n "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-14235899",
            "date": "2014-12-05T18:55:38+0000",
            "content": "Committed to trunk and branch_5x.\n\nThanks all! "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-14332777",
            "date": "2015-02-23T05:01:51+0000",
            "content": "Bulk close after 5.0 release. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14338335",
            "date": "2015-02-26T12:43:04+0000",
            "content": "Reopening to backport to 4.10.4 "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14338338",
            "date": "2015-02-26T12:45:32+0000",
            "content": "Commit 1662436 from shalin@apache.org in branch 'dev/branches/lucene_solr_4_10'\n[ https://svn.apache.org/r1662436 ]\n\nSOLR-3881: Avoid OOMs in LanguageIdentifierUpdateProcessor:\n\n\tAdded langid.maxFieldValueChars and langid.maxTotalChars params to limit\n  input, by default 10k and 20k chars, respectively.\n\tMoved input concatenation to Tika implementation; the langdetect\n  implementation instead appends each input piece via the langdetect API.\n\n "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-14348922",
            "date": "2015-03-05T15:36:31+0000",
            "content": "Bulk close for 4.10.4 release "
        }
    ]
}