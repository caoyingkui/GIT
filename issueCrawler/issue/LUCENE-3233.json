{
    "id": "LUCENE-3233",
    "title": "HuperDuperSynonymsFilter\u2122",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [
            "3.4",
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "The current synonymsfilter uses a lot of ram and cpu, especially at build time.\n\nI think yesterday I heard about \"huge synonyms files\" three times.\n\nSo, I think we should use an FST-based structure, sharing the inputs and outputs.\nAnd we should be more efficient with the tokenStream api, e.g. using save/restoreState instead of cloneAttributes()",
    "attachments": {
        "LUCENE-3223.patch": "https://issues.apache.org/jira/secure/attachment/12483689/LUCENE-3223.patch",
        "LUCENE-3233.patch": "https://issues.apache.org/jira/secure/attachment/12483597/LUCENE-3233.patch",
        "synonyms.zip": "https://issues.apache.org/jira/secure/attachment/12485378/synonyms.zip"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2011-06-23T14:04:47+0000",
            "content": "here's a rough start to building a datastructure that I think makes good tradeoffs between RAM and processing.\n\nNo matter what, the processing on the filter-side will be hairy because of the 'interleaving' with the tokenstream.\n\nThis one is just an FST<CharsRef,Int[]>(BYTE4) where Int is an ord to a BytesRefHash, containing the output Bytes for each term.\n\nThis way, at input time we can walk the FST with codePointAt()\n\nOn both sides, the Chars/Bytes are actually phrases, using \\u0000 as a word separator. ",
            "author": "Robert Muir",
            "id": "comment-13053869"
        },
        {
            "date": "2011-06-24T10:48:02+0000",
            "content": "Dumping my current state on FSTSynonymFilter \u2013 it compiles but it's got tons of bugs I'm sure!  I added a trivial initial test. ",
            "author": "Michael McCandless",
            "id": "comment-13054356"
        },
        {
            "date": "2011-07-04T17:42:11+0000",
            "content": "New patch w/ current state.\n\nI think it's closer; the test has more cases now (but I'd still like to make a random test), fewer nocommits, etc. ",
            "author": "Michael McCandless",
            "id": "comment-13059523"
        },
        {
            "date": "2011-07-05T13:25:30+0000",
            "content": "patch with a first random test, this one currently does 10 iterations where it adds random shit to the synonym map, then it analyzes 10k random strings (each time capturing the output, and replaying it back to ensure the thing is deterministic and doesn't have reuse bugs).\n\ni also added the ignoreCase support.\n\nthe filter might have a reuse bug, see ant test -Dtestcase=TestFSTSynonymMapFilter -Dtestmethod=testRandom -Dtests.seed=-4122723628721952592:244824441557739968 ",
            "author": "Robert Muir",
            "id": "comment-13059891"
        },
        {
            "date": "2011-07-05T19:49:05+0000",
            "content": "New patch, folding in Robert's changes and the random stress test.  All tests pass.  I think it's now functionally correct, but I still need to compare perf vs existing syn filter, and there are still a few minor nocommits to work out.\n\nIdeally, if we get perf close enough, since RAM is much much less w/ this new syn filter, I think we should replace the old one with this new one. ",
            "author": "Michael McCandless",
            "id": "comment-13060095"
        },
        {
            "date": "2011-07-05T21:46:55+0000",
            "content": "New patch, adding dedup option to the builder, removing a couple nocommits, cutting back on iters/counts in testRandom2. ",
            "author": "Michael McCandless",
            "id": "comment-13060145"
        },
        {
            "date": "2011-07-06T05:25:54+0000",
            "content": "Updated patch:\n\n\tadded a SolrSynonymsParser and test to the analyzers module, that parses the existing solr synonyms format.\n\tadded a Solr factory for this thing (untested!) that uses this when \"format=solr\" (the default)\n\n\n\nThis way, the idea is the factory would be more extensible, e.g. you could load syns from a database, or we could add parsers for wordnet and nuke contrib/wordnet, etc etc.\n\nStill need to do some basic benchmarking. ",
            "author": "Robert Muir",
            "id": "comment-13060296"
        },
        {
            "date": "2011-07-06T06:49:59+0000",
            "content": "fixed some bugs, added some tests, but there is a problem, I started to add a little benchmark and I hit this on my largish synonyms file:\n\njava.lang.IllegalStateException: max arc size is too large (445)\n\n\n\nJust run the TestFSTSynonymFilterFactory and you will see it, i enabled some prints and it doesn't appear like anything totally stupid is going on... giving up for the night  ",
            "author": "Robert Muir",
            "id": "comment-13060322"
        },
        {
            "date": "2011-07-06T06:51:52+0000",
            "content": "attaching my synonyms.txt test file that i was using: its derived from wordnet ",
            "author": "Robert Muir",
            "id": "comment-13060323"
        },
        {
            "date": "2011-07-06T10:52:41+0000",
            "content": "java.lang.IllegalStateException: max arc size is too large (445)\n\nAhh \u2013 to fix this we have to call Builder.setAllowArrayArcs(false), ie, disable the array arcs in the FST (and this binary search lookup for finding arcs!).  I had to do this also for MemoryCodec, since postings encoded as output per arc can be more than 256 bytes, in general.\n\nThis will hurt perf, ie, the arc lookup cannot use a binary search; it's because of a silly limitation in the FST representation, that we use a single byte to hold the max size of all arcs, so that if any arc is > 256 bytes we are unable to encode it as an array.  We could fix this (eg, use vInt), however, arcs with such widely varying sizes (due to widely varying outputs on each arc) will be very wasteful in space because all arcs will use up a fixed number of bytes when represented as an array.\n\nFor now I think we should just call the above method, and then test the resulting perf. ",
            "author": "Michael McCandless",
            "id": "comment-13060471"
        },
        {
            "date": "2011-07-06T13:45:29+0000",
            "content": "Actually, maybe a better general fix for FST would be for it to dynamically decide whether to make an array based on how many bytes will be wasted (in addition to the number of arcs / depth of the node).  This way we could turn on arcs always, and FST would pick the right times to use it.  If we stick to only 1 byte for the number of bytes per arc, the FST could simply not use the array when an arc is > 256 bytes. ",
            "author": "Michael McCandless",
            "id": "comment-13060578"
        },
        {
            "date": "2011-07-06T14:46:17+0000",
            "content": "Thanks Mike, I will set the option for now, we can address any potential perf hit in a number of different ways here (besides modifying FST itself). ",
            "author": "Robert Muir",
            "id": "comment-13060613"
        },
        {
            "date": "2011-07-06T17:00:36+0000",
            "content": "I ran some quick numbers, using the syn file example here, just best of 3 runs:\n\n\n\n\nImpl\nBuild time\nRAM usage\n\n\nSynonymFilterFactory\n6619ms\n207.92 mb\n\n\nFSTSynonymFilterFactory\n463 ms\n3.51 mb\n\n\n\n\n\nI modified the builder slightly to build the FST more efficiently for this, will upload the updated patch.\n\nSo i think the build-time and RAM consumption are really improved, the next thing is to benchmark the runtime perf. ",
            "author": "Robert Muir",
            "id": "comment-13060686"
        },
        {
            "date": "2011-07-06T17:16:53+0000",
            "content": "Wow that's striking; nice work guys.  FSTs are definitely one of those killer pieces of technology in Lucene.\n\nThe difference in build time is surprising to me.  Any theory why SynonymFilterFactory takes so much more time to build? ",
            "author": "David Smiley",
            "id": "comment-13060698"
        },
        {
            "date": "2011-07-06T17:28:07+0000",
            "content": "\nThe difference in build time is surprising to me. Any theory why SynonymFilterFactory takes so much more time to build?\n\nYes, its the n^2 portion where you have a synonym entry like this: a, b, c, d\nin reality this is creating entries like this:\na -> a\na -> b\na -> c\na -> d\nb -> a\nb -> b\n...\n\nin the current impl, this is done using some inefficient datastructures (like nested chararraymaps with Token),\nas well as calling merge().\n\nIn the FST impl, we don't use any nested structures (instead input and output entries are just phrases), and we explicitly \ndeduplicate both inputs and outputs during construction, the FST output is just a\nList<Integer> basically pointing to ords in the deduplicated bytesrefhash.\n\nso during construction when you add() its just a hashmap lookup on the input phrase, a bytesrefhash get/put on the UTF16toUTF8WithHash\nto get the output ord, and an append to an arraylist.\n\nthis code isn't really optimized right now and we can definitely speed it up even more in the future. but the main thing\nright now is to ensure the filter performance is good. ",
            "author": "Robert Muir",
            "id": "comment-13060705"
        },
        {
            "date": "2011-07-06T18:35:48+0000",
            "content": "here is a patch with a little microbenchmark... so we have some tuning to do. \n\nthe benchmark analyzes a short string a million times, that doesn't match any synonyms (actually hte solr default)\n\n\n\n\nimpl\nms\n\n\nSynonymsFilter\n1692\n\n\nFST with array arcs\n2794\n\n\nFST with no array arcs\n8823\n\n\n\n\n\nso, disabling the array arcs is a pretty crucial hit here. but we could do other options to speed up this common case, e.g. with daciuk we could build a charrunautomaton of the K-prefixes of the synonyms, this would be really fast to reject these terms that don't match any syns.\n\nor we could explicitly put our bytesref output in a byte[], and use long pointers as outputs.\n\nor we could speed up FST! But i think its interesting to see how important this parameter is. ",
            "author": "Robert Muir",
            "id": "comment-13060749"
        },
        {
            "date": "2011-07-06T18:59:21+0000",
            "content": "Wow, it's very important to allow arcs to be encoded as arrays (for the binary search on lookup).  I think we should just fix FST... I'll think about it.  MemoryCodec would also get big gains here. ",
            "author": "Michael McCandless",
            "id": "comment-13060764"
        },
        {
            "date": "2011-07-06T19:24:40+0000",
            "content": "I agree, this would be the best solution. Maybe we should just open a separate issue for that? \n\nwe can let this one be for now until that is resolved, can even continue working on other parts of it. ",
            "author": "Robert Muir",
            "id": "comment-13060781"
        },
        {
            "date": "2011-07-06T21:18:47+0000",
            "content": "New patch, including some optimizing to FST (which we can commit under a separate issue): array arcs can now be any size, and I re-use the BytesReader inner class that's created for parsing arcs. ",
            "author": "Michael McCandless",
            "id": "comment-13060848"
        },
        {
            "date": "2011-07-06T23:09:03+0000",
            "content": "New patch, including some optimizing to FST (which we can commit under a separate issue)\n\nworks! I don't think we need to open a new issue, I didn't think you would come back with a patch in just two hours!\n\nI'll play with the patch some now and see what I can do with it. ",
            "author": "Robert Muir",
            "id": "comment-13060905"
        },
        {
            "date": "2011-07-07T01:13:20+0000",
            "content": "updated patch, this tableizes the first FST arcs for latin-1.\n\nprecomputing this tiny table speeds up this filter a ton (~3000ms -> ~2000ms) and I think is a cheap easy win for the terms index too. ",
            "author": "Robert Muir",
            "id": "comment-13060973"
        },
        {
            "date": "2011-07-07T02:17:54+0000",
            "content": "Benchmark with the synonyms.zip attached to this issue (so that we are actually matching synonyms):\nin this case i only analyzed the text 100,000 times, as its a lot more output.\nI also checked they are emitting the same stuff:\n\n\n\n\nImpl\nms\n\n\nSynonymsFilter\n112527\n\n\nFST\n22872\n\n\n\n\n\nSo, thats 5x faster, I think probably due to avoiding the expensive cloning.\n\nI think we are fine on performance. ",
            "author": "Robert Muir",
            "id": "comment-13061001"
        },
        {
            "date": "2011-07-07T11:57:02+0000",
            "content": "so i don't forget, lets not waste an arc bitflag marking an arc as 'first'... \n\nI hear the secret is instead arc.target == startNode ",
            "author": "Robert Muir",
            "id": "comment-13061232"
        },
        {
            "date": "2011-07-07T13:07:36+0000",
            "content": "New patch, moving the root arcs cache into FST, not using up our last precious arc bit. ",
            "author": "Michael McCandless",
            "id": "comment-13061275"
        },
        {
            "date": "2011-07-07T18:54:37+0000",
            "content": "Another rev of the patch: I did a hard bump the FST version (so\nexisting trunk indices must be rebuilt), and added NOTE in suggest's\nFST impl that the file format is experimental; removed\nmaxVerticalContext; fixed false test failure. ",
            "author": "Michael McCandless",
            "id": "comment-13061486"
        },
        {
            "date": "2011-07-09T13:33:21+0000",
            "content": "I think this is ready to commit, but I'd like to rename existing syn filter to SlowSynonymFilter and rename the new one to SynonymFilter.\n\nBecause there are some minor diffs (deduping rules, lowercasing), for Solr to cutover I think we need some back compat logic; I'll open a separate issue for this. ",
            "author": "Michael McCandless",
            "id": "comment-13062375"
        },
        {
            "date": "2011-07-09T13:50:45+0000",
            "content": "I'll try to add some not-so-sophisticated backwards here! ",
            "author": "Robert Muir",
            "id": "comment-13062380"
        },
        {
            "date": "2011-07-09T13:58:46+0000",
            "content": "but I'd like to rename existing syn filter to SlowSynonymFilter and rename the new one to SynonymFilter.\n\nBut the lookup on the original is still faster, right?  And if someone has small synonym dicts (actually pretty common in my experience since SynonymFilter isn't necessary used to inject synonyms in the traditional sense, but for any mapping task) then build time and mem use won't be much of an issue (esp if the input to match is mostly single words).\n\nThis looks great for large synonym maps, but perhaps instead of Slow* or Fast* we could name them for the implementation and either name the new one FSTSynonymFilter or rename the current one to MapSynonymFilter?  Or is the plan to actually deprecate the current SynonymFilter?  ",
            "author": "Yonik Seeley",
            "id": "comment-13062382"
        },
        {
            "date": "2011-07-09T14:09:14+0000",
            "content": "But the lookup on the original is still faster, right?\n\nThat was before we optimized FST for this usage case.\n\nNow, from the testing above, it looks like we are faster when syns actually match; if no syns match the two are around the same speed.\n\nSeparately: shouldn't we not have any syns in the default text_en field type?  Like we can have a synonyms.txt but comment out all the rules in there?\n\nI don't think we should keep the old one around, ie, we should [eventually] replace it with the new one. ",
            "author": "Michael McCandless",
            "id": "comment-13062383"
        },
        {
            "date": "2011-07-09T14:12:32+0000",
            "content": "This one is faster for the simple reason that the TokenFilter uses captureState() and restoreState() [and has logic to minimize cloning in more cases] instead of AttributeSource.cloneAttributes()/copyTo() ",
            "author": "Robert Muir",
            "id": "comment-13062384"
        },
        {
            "date": "2011-07-09T14:20:26+0000",
            "content": "Now, from the testing above, it looks like we are faster when syns actually match; if no syns match the two are around the same speed.\n\nOh cool!  I was looking at \"1692\" for the SynonymsFilter and a drop from \"~3000ms -> ~2000ms\" for the FST version.   I assumed Robert's last benchmark was building and not lookup (the 112527/22872).\n\nSeparately: shouldn't we not have any syns in the default text_en field type?\n\nI dunno... it's nice for both demonstration and testing (and it's in the current tutorial). ",
            "author": "Yonik Seeley",
            "id": "comment-13062386"
        },
        {
            "date": "2011-07-09T19:13:46+0000",
            "content": "Updated patch: \n\n\trenamed to SynonymFilter\n\tadded not-so-sophisticated backwards layer\n\tadded tests\n\tadded parser for format=\"wordnet\"\n\tremoved contrib/wordnet\n\n\n\nbut i found some bugs (well one, surely is) in the new tests, so i added nocommits here. ",
            "author": "Robert Muir",
            "id": "comment-13062603"
        },
        {
            "date": "2011-07-10T08:53:10+0000",
            "content": "just updating patch to trunk, the nocommits remain... ",
            "author": "Robert Muir",
            "id": "comment-13062705"
        },
        {
            "date": "2011-07-10T20:31:05+0000",
            "content": "New patch, also setting offsets in the produced tokens (the wordnet test passes), and adding adding a NOTE about the dup output words issue.\n\nI think it's finally ready to commit! ",
            "author": "Michael McCandless",
            "id": "comment-13062800"
        },
        {
            "date": "2011-08-22T06:23:57+0000",
            "content": "Does this support multi word synonyms? I tried one of the 3.4 builds and I was getting odd results on multi word synonyms. ",
            "author": "Glen Stampoultzis",
            "id": "comment-13088540"
        },
        {
            "date": "2011-08-22T12:48:39+0000",
            "content": "It does support multi-word synonyms \u2013 can you give some details on the odd behavior?  Maybe email dev@lucene.apache.org, or open a new Jira issue? ",
            "author": "Michael McCandless",
            "id": "comment-13088666"
        },
        {
            "date": "2011-08-23T07:16:05+0000",
            "content": "I'll try and put together a simple test case just to make sure I've got something repeatable and post to the list. I think it might have been related to synonym fields tokenized using KeywordTokenizerFactory. ",
            "author": "Glen Stampoultzis",
            "id": "comment-13089319"
        },
        {
            "date": "2011-08-26T15:48:39+0000",
            "content": "Looks like that when Solr's synonym parsing was moved to the analysis module, it was also rewritten, introducing escaping bugs.\n\nExamples:\na\\,a is no longer treated as a single token\na\\=>a is no longer treated as a single token\na\\ta is treated as \"ata\" instead of containing a tab character\n\nI didn't do a full review, so I'm not sure if there are other differences in behavior. ",
            "author": "Yonik Seeley",
            "id": "comment-13091834"
        },
        {
            "date": "2011-08-26T16:00:18+0000",
            "content": "Do you have a test?\n\nBecause we have tests for this:\n\n    String testFile = \n      \"a\\\\=>a => b\\\\=>b\\n\" +\n      \"a\\\\,a => b\\\\,b\";\n...\n    assertAnalyzesTo(analyzer, \"a=>a\",\n        new String[] { \"b=>b\" },\n        new int[] { 1 });\n    \n    assertAnalyzesTo(analyzer, \"a,a\",\n        new String[] { \"b,b\" },\n        new int[] { 1 });\n\n ",
            "author": "Robert Muir",
            "id": "comment-13091843"
        },
        {
            "date": "2011-08-26T18:11:35+0000",
            "content": "I just tested by hand:\nI added a line to synonyms.txt \"a\\,a => b\\,b\", fired up the example server and then executed the following query:\nhttp://localhost:8983/solr/select?q=a,a&debugQuery=true\n\nI then verified that the synonyms were in effect in general, via:\nhttp://localhost:8983/solr/select?q=fooaaa&debugQuery=true ",
            "author": "Yonik Seeley",
            "id": "comment-13091919"
        },
        {
            "date": "2011-08-28T16:41:57+0000",
            "content": "User error - the field type changes in the example schema tripped up the user (and me too).  The standard tokenizer does not keep \"a,a\" as a single token. ",
            "author": "Yonik Seeley",
            "id": "comment-13092512"
        }
    ]
}