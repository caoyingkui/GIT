{
    "id": "LUCENE-6339",
    "title": "[suggest] Near real time Document Suggester",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "5.0",
        "components": [
            "core/search"
        ],
        "labels": "",
        "fix_versions": [
            "5.1",
            "6.0"
        ],
        "priority": "Major",
        "status": "Closed",
        "type": "New Feature"
    },
    "description": "The idea is to index documents with one or more SuggestField(s) and be able to suggest documents with a SuggestField value that matches a given key.\nA SuggestField can be assigned a numeric weight to be used to score the suggestion at query time.\n\nDocument suggestion can be done on an indexed SuggestField. The document suggester can filter out deleted documents in near real-time. The suggester can filter out documents based on a Filter (note: may change to a non-scoring query?) at query time.\n\nA custom postings format (CompletionPostingsFormat) is used to index SuggestField(s) and perform document suggestions.\n\nUsage\n\n  // hook up custom postings format\n  // indexAnalyzer for SuggestField\n  Analyzer analyzer = ...\n  IndexWriterConfig config = new IndexWriterConfig(analyzer);\n  Codec codec = new Lucene50Codec() {\n    PostingsFormat completionPostingsFormat = new Completion50PostingsFormat();\n\n    @Override\n    public PostingsFormat getPostingsFormatForField(String field) {\n      if (isSuggestField(field)) {\n        return completionPostingsFormat;\n      }\n      return super.getPostingsFormatForField(field);\n    }\n  };\n  config.setCodec(codec);\n  IndexWriter writer = new IndexWriter(dir, config);\n  // index some documents with suggestions\n  Document doc = new Document();\n  doc.add(new SuggestField(\"suggest_title\", \"title1\", 2));\n  doc.add(new SuggestField(\"suggest_name\", \"name1\", 3));\n  writer.addDocument(doc)\n  ...\n  // open an nrt reader for the directory\n  DirectoryReader reader = DirectoryReader.open(writer, false);\n  // SuggestIndexSearcher is a thin wrapper over IndexSearcher\n  // queryAnalyzer will be used to analyze the query string\n  SuggestIndexSearcher indexSearcher = new SuggestIndexSearcher(reader, queryAnalyzer);\n  \n  // suggest 10 documents for \"titl\" on \"suggest_title\" field\n  TopSuggestDocs suggest = indexSearcher.suggest(\"suggest_title\", \"titl\", 10);\n\n\n\nIndexing\nIndex analyzer set through IndexWriterConfig\n\nSuggestField(String name, String value, long weight) \n\n\n\nQuery\nQuery analyzer set through SuggestIndexSearcher.\nHits are collected in descending order of the suggestion's weight \n\n// full options for TopSuggestDocs (TopDocs)\nTopSuggestDocs suggest(String field, CharSequence key, int num, Filter filter)\n\n// full options for Collector\n// note: only collects does not score\nvoid suggest(String field, CharSequence key, int num, Filter filter, TopSuggestDocsCollector collector) \n\n\n\nAnalyzer\nCompletionAnalyzer can be used instead to wrap another analyzer to tune suggest field only parameters. \n\nCompletionAnalyzer(Analyzer analyzer, boolean preserveSep, boolean preservePositionIncrements, int maxGraphExpansions)",
    "attachments": {
        "LUCENE-6339.patch": "https://issues.apache.org/jira/secure/attachment/12702633/LUCENE-6339.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14347725",
            "author": "Areek Zillur",
            "date": "2015-03-04T22:55:05+0000",
            "content": "Initial patch. needs more unit tests. \nThe custom postings format was originally a fork of Completion090PostingsFormat  . Document suggestion uses the same TopNSearcher as that of AnalyzingSuggester.\n\nWould be awesome to get some feedback on the patch! "
        },
        {
            "id": "comment-14349653",
            "author": "Michael McCandless",
            "date": "2015-03-06T00:02:11+0000",
            "content": "This looks really nice!\n\nI think AutomatonUtil is (nearly?) the same thing as\nTokenStreamToAutomaton?  Can we somehow consolidate the two?\n\nWhen I try to \"ant test\" with the patch on current 5.x some things are\nangry:\n\n\n    [mkdir] Created dir: /l/areek/lucene/build/suggest/classes/java\n    [javac] Compiling 65 source files to /l/areek/lucene/build/suggest/classes/java\n    [javac] /l/areek/lucene/suggest/src/java/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.java:597: warning: [cast] redundant cast to TopFieldDocs\n    [javac]       TopFieldDocs hits = (TopFieldDocs) c.topDocs();\n    [javac]                           ^\n    [javac] /l/areek/lucene/suggest/src/java/org/apache/lucene/search/suggest/document/NRTSuggester.java:208: error: local variable collector is accessed from within inner class; needs to be declared final\n    [javac]               collector.collect(docID);\n    [javac]               ^\n    [javac] /l/areek/lucene/suggest/src/java/org/apache/lucene/search/suggest/document/CompletionFieldsProducer.java:164: error: CompletionFieldsProducer.CompletionsTermsReader is not abstract and does not override abstract method getChildResources() in Accountable\n    [javac]   private class CompletionsTermsReader implements Accountable {\n    [javac]           ^\n    [javac] Note: Some input files use or override a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] 2 errors\n    [javac] 1 warning\n\n\n\nNot sure why we need an FSTBuilder inside the NRTSuggesterBuilder;\ncan't the first be absorbed into the latter?  Can NRTSuggesterBuilder\nbe package private?  Ie the public API here is the postings format and\nSuggestIndexSearcher / SuggestTopDocs?  I think other things can be\nprivate, e.g. CompletionTokenStream.\n\nCan you use CodecUtil.writeIndexHeader when storing the FST?  It also\nstores the segment ID and file extension in the header.  And then\nCodecUtil.checkIndexHeader at read-time.\n\nCompletionTermsReader.lookup() should be sync'd?   Else two threads\ncould try to use the IndexInput (dictIn) at once?\n\nMaybe we should move the code in SuggestIndexSearcher.suggest into\na new TopSuggestDocs.merge method?\n\nDo we really need the separate SegmentLookup interface?  Seems like we\ncan just invoke lookup method directly on CompletionTerms?\n\nWhy do we allow -1 weight?  And why do we restrict to int not long\n(other suggesters are long I think, though it does seem like\noverkill!). "
        },
        {
            "id": "comment-14350351",
            "author": "Simon Willnauer",
            "date": "2015-03-06T13:55:13+0000",
            "content": "Hey Areek, I agree with mike this looks awesome... lemme give you some comments\n\n\n\tcan we make CompletionAnalyzer immutable by any chance? I'd really like to not have setters if possible? For that I guess it's constants need to be public as well?\n\tis private boolean isReservedInputCharacter(char c) }} needed since we then afterwards check it again in the {{checkKey method, maybe you just wanna use a switch here?\n\tIn CompletionFieldsConsumer#close() I think we need to make sure IOUtils.close(dictOut); is also called if an exception is hit?\n\tdo we need the extra InputStreamDataInput in CompletionTermWriter#parse, I mean it's a byte input stream so we should be able to read all of the bytes?\n\tSuggestPayload doesn't need a default ctor\n\tcan we use {{ if (success == false) }} instead of {{ if (!success) }}  as a pattern in general?\n\tuse try / finally in CompletionFieldsProducer#close() to ensure all resource are closed or pass both the dict and {{ delegateFieldsProducer }} to IOUtils#close()?\n\tyou fetch the checksum for the dict file in {{ CompletionFieldsProducer#ctor }} via  {{ CodecUtil.retrieveChecksum(dictIn); } but you ignore it's return value, was this intended? I think you don't wanna do that here? Did you intend to check the entire file?\n\tI wonder if we should just write one file for both, the index and the FSTs? What's the benefit from having two?\n\n\n\nFor loading the dict you put a comment in there sayingm {{ // is there a better way of doing this?}}\n\nI think what you need to do is this:\n\n\npublic synchronized SegmentLookup lookup() throws IOException {\n  if (lookup == null) {\n     try (IndexInput dictClone = dictIn.clone()) { // let multiple fields load concurrently\n         dictClone.seek(offset); // this is your field private clone \n         lookup = NRTSuggester.load(dictClone);\n     }\n  }\n  return lookup;\n}\n\n\n\nI'd appreciate a tests that this works just fine ie. loading multiple FSTs concurrently.\n\nI didn't get further than this due to the lack of time but I will come back to this either today or tomorrow. Good stuff Areek "
        },
        {
            "id": "comment-14351169",
            "author": "Areek Zillur",
            "date": "2015-03-07T00:02:26+0000",
            "content": "Thanks Michael McCandless and Simon Willnauer for the feedback!\n\nWhen I try to \"ant test\" with the patch on current 5.x some things are\nangry\nThis is fixed.\nHmm interestingly enough those errors do not show up for me, using java8.\n\nUpdated Patch:\n\n\tremoved private boolean isReservedInputCharacter(char c) and moved reserved input char check to toAutomaton(CharSequence key)\n\tuse CodecUtil.checkIndexHeader && CodecUtil.writeIndexHeader for all files in custom postings format\n\tuse if (success== false) instead of if(!success)\n\tproper sync for loading FSTs concurrently\n\tadded TopSuggestDocs.merge method\n\tmake sure CompletionFieldsConsumer#close() and CompletionFieldsProducer#close() properly handle closing resources\n\tremoved SegmentLookup interface; use NRTSuggester directly\n\tfixed weight check to not allow negative weights; allow long values\n\tremoved FSTBuilder and made NRTSuggesterBuilder & CompletionTokenStream package-private\n\n\n\nStill TODO:\n\n\tconsolidate AutomatonUtil and TokenStreamToAutomaton\n\tmake CompletionAnalyzer immutable\n\tremove use of extra InputStreamDataInput in CompletionTermWriter#parse\n\ttest loading multiple FSTs concurrently\n\tmore unit tests\n\n "
        },
        {
            "id": "comment-14351191",
            "author": "Areek Zillur",
            "date": "2015-03-07T00:15:23+0000",
            "content": "\nyou fetch the checksum for the dict file in {{ CompletionFieldsProducer#ctor }} via {{ CodecUtil.retrieveChecksum(dictIn); } but you ignore it's return value, was this intended? I think you don't wanna do that here? Did you intend to check the entire file?\nI wonder if we should just write one file for both, the index and the FSTs? What's the benefit from having two?\nThis was intentional, used the same convention for BlockTreeTermsReader#termsIn here. The thought was doing the checksum check would be very costly, in most cases the dict file would be large?\nIf we write one file instead of two, then the checksum check would be more expensive for the index then now? "
        },
        {
            "id": "comment-14362740",
            "author": "Areek Zillur",
            "date": "2015-03-16T04:53:05+0000",
            "content": "Updated Patch:\n\n\tnuke AutomatonUtil\n\tmake CompletionAnalyzer immutable\n\tadd tests\n\tminor fixes\n\n "
        },
        {
            "id": "comment-14377513",
            "author": "Michael McCandless",
            "date": "2015-03-24T08:50:12+0000",
            "content": "New patch looks great, thanks Areek Zillur!\n\nIn TopSuggestDocsCollector:\n\n\n\tIn collect, we seem to assume the suggest searcher will never call\n    collect more than num times?  How is that?  If so, can you add that to\n    the javadocs, and maybe add an assert upto < num in collect?\n\n\n\n\n\tCan we just allocate scoreDocs up front instead of lazily?\n\n\n\n\n\tIn the javadocs, instead of \"one hit can be...\" maybe \"one doc can\n    be...\"?  Hit is a tricky word in this context since it could be a doc\n    or a suggestion...\n\n\n\nIn SuggestIndexSearcher, does it really ever make sense to take a\ngeneric Collector/LeafCollector?  Can we instead just strongly type\nthe params to all the methods to be TopSuggestDocsCollector?\n\n\"In case a filter has to be applied, the queue size is doubled\" is not\nquite correct?  Maybe change the logic there so the int queueSize is\nfirst computed, and then if filter is enabled, it's doubled?\n\nCan we remove the separate WeightProcessor class and just make\nencode/decode static methods on NRTSuggester?  We can add back\nabstractions later if users somehow need control over weight\nencoding...\n\nCan we add a test that tests the extreme case of nearly all docs\nfiltered out and another test with nearly all docs deleted? "
        },
        {
            "id": "comment-14381062",
            "author": "Areek Zillur",
            "date": "2015-03-25T23:55:11+0000",
            "content": "Thanks Michael McCandless for the review!\n\nIn TopSuggestDocsCollector:\nIn collect, we seem to assume the suggest searcher will never call\ncollect more than num times? How is that? If so, can you add that to\nthe javadocs, and maybe add an assert upto < num in collect?\nCan we just allocate scoreDocs up front instead of lazily?\nIn the javadocs, instead of \"one hit can be...\" maybe \"one doc can\nbe...\"? Hit is a tricky word in this context since it could be a doc\nor a suggestion...\n\nI have re written TopSuggestDocsCollector to have a priority queue at the top-level instead, somewhat similar to TopDocsCollector.\nNow completions across segments are collected in the same pq, this allows early termination for suggesters at the segment level \n(when a collected completion overflows the pq, we can disregard the rest of the completions for that segment, \nas completions are collected in order of their scores).\n\n\nIn SuggestIndexSearcher, does it really ever make sense to take a\ngeneric Collector/LeafCollector? Can we instead just strongly type\nthe params to all the methods to be TopSuggestDocsCollector?\nThanks for the suggestion! the generic Collector/LeafCollector is removed.\nCurrent API:\n\npublic void suggest(String field, CharSequence key, int num, Filter filter, TopSuggestDocsCollector collector) \n\n\n\n\n\"In case a filter has to be applied, the queue size is doubled\" is not\nquite correct? Maybe change the logic there so the int queueSize is\nfirst computed, and then if filter is enabled, it's doubled?\nNow the queueSize is increased by half the # of live docs in the segment instead. If a filter is applied, the queue size should \nbe increased w.r.t. to # of documents.\nif the applied filter filters out <= half of the top scoring documents for a query prefix, then the search is admissible.\nif a filter is too restrictive, then the search is inadmissible. a work around would be to multiply num by some factor, \nin this case early termination might help (if TopSuggestDocsCollector is initialized with the original num). thoughts?\n\nUpdated Patch:\n\n\tSuggestIndexSearcher cleanup\n\tTopSuggestDocsCollector re-write\n\tremove WeightProcessor from NRTSuggester\n\tadded more tests (including boundary cases for deleted/filtered out documents)\n\n "
        },
        {
            "id": "comment-14381112",
            "author": "Areek Zillur",
            "date": "2015-03-26T00:29:56+0000",
            "content": "Updated Patch:\n\n\tminor fixes\n\n "
        },
        {
            "id": "comment-14382992",
            "author": "Michael McCandless",
            "date": "2015-03-26T23:57:01+0000",
            "content": "Patch looks great!\n\nCan we pull out SuggestScoreDocPQ into its own .java source?  Should its lessThan method tie break by docID?\n\nI think the logic to compute maxQueueSize in getMaxTopNSearcherQueueSize could possibly overflow int?  Maybe use long, and then cast back to int after the Math.min? "
        },
        {
            "id": "comment-14384119",
            "author": "Areek Zillur",
            "date": "2015-03-27T16:43:53+0000",
            "content": "Thanks Michael McCandless for the suggestions!\n\nUpdated Patch:\n\n\tseparate out SuggestScoreDocPriorityQueue (break ties with docID)\n\tuse long to calculate maxQueueSize\n\tminor changes\n\n "
        },
        {
            "id": "comment-14384405",
            "author": "Michael McCandless",
            "date": "2015-03-27T19:06:38+0000",
            "content": "I think the tie break should be a.doc > b.doc, for consistency with Lucene?\n\nI.e., on a score tie, the smaller doc ID should sorter earlier than the bigger doc ID?\n\nOtherwise +1 to commit!  Thanks Areek Zillur! "
        },
        {
            "id": "comment-14384583",
            "author": "Uwe Schindler",
            "date": "2015-03-27T20:38:54+0000",
            "content": "I just reviewed the patch, too. I like the API, but have not yet looked into it closely like Mike - I just skimmed it.\n\nJust one question: What happens if 2 documents have the same SuggestField and same suggestion presented to user? This would now produce duplicates, right? I was just thinking about how to prevent that (coming from Elasticsearch world). "
        },
        {
            "id": "comment-14384597",
            "author": "Areek Zillur",
            "date": "2015-03-27T20:44:52+0000",
            "content": "Updated Patch:\n\n\tSuggestScoreSocPQ prefers smaller doc id\n\tdocumentation fixes\n\n\n\nI will commit this shortly, Thanks for all the feedback Michael McCandless & Simon Willnauer "
        },
        {
            "id": "comment-14384604",
            "author": "Uwe Schindler",
            "date": "2015-03-27T20:46:28+0000",
            "content": "+1 "
        },
        {
            "id": "comment-14384631",
            "author": "Areek Zillur",
            "date": "2015-03-27T20:55:21+0000",
            "content": "Hi Uwe Schindler,\nThanks for the review!\nIf two documents do have the same suggestion for the same SuggestField, it will produce duplicates in terms of the suggestion, but because they are from two documents (different doc ids) they are not considered as duplicates.\nMaybe we can add a boolean flag in the NRTSuggester to only collect unique suggestions, but then we will have to decide on which suggestion to throw out, as they are now tied to doc ids? "
        },
        {
            "id": "comment-14384807",
            "author": "Uwe Schindler",
            "date": "2015-03-27T22:27:09+0000",
            "content": "If two documents do have the same suggestion for the same SuggestField, it will produce duplicates in terms of the suggestion, but because they are from two documents (different doc ids) they are not considered as duplicates.\n\nYeah that's what I mean by duplicate. The suggester only returns doc ids. For display to user, it would read a stored field like you do when presenting search results (the actual suggestion) and this produces the duplicate. I am not sure how to solve that. It was just an idea. If this is really an issue, one could filter the duplicates afterwards. "
        },
        {
            "id": "comment-14384823",
            "author": "ASF subversion and git services",
            "date": "2015-03-27T22:37:51+0000",
            "content": "Commit 1669698 from Areek Zillur in branch 'dev/trunk'\n[ https://svn.apache.org/r1669698 ]\n\nLUCENE-6339: Added Near-real time Document Suggester via custom postings format "
        },
        {
            "id": "comment-14384857",
            "author": "Uwe Schindler",
            "date": "2015-03-27T23:03:11+0000",
            "content": "Indeed the suggestion does not need to come from a stored field of the result document, nice! But one could use that to add additional suggestion information, right - instead of the payload? "
        },
        {
            "id": "comment-14384859",
            "author": "ASF subversion and git services",
            "date": "2015-03-27T23:04:26+0000",
            "content": "Commit 1669703 from Areek Zillur in branch 'dev/trunk'\n[ https://svn.apache.org/r1669703 ]\n\nLUCENE-6339: move changes entry from 6.0.0 to 5.1.0 "
        },
        {
            "id": "comment-14384865",
            "author": "Areek Zillur",
            "date": "2015-03-27T23:07:23+0000",
            "content": "Yes Uwe Schindler that is the idea . the payload option has been removed entirely, now instead of using payloads one can grab any associated values from the document with each suggestion "
        },
        {
            "id": "comment-14384976",
            "author": "Areek Zillur",
            "date": "2015-03-28T00:27:32+0000",
            "content": "Committed to branch_5x with revision r1669715 (missed out on prepending the commit message with jira #) "
        },
        {
            "id": "comment-14393425",
            "author": "ASF subversion and git services",
            "date": "2015-04-02T21:12:04+0000",
            "content": "Commit 1670969 from Areek Zillur in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1670969 ]\n\nLUCENE-6339: fix test bug (ensure opening nrt reader with applyAllDeletes) "
        },
        {
            "id": "comment-14393438",
            "author": "ASF subversion and git services",
            "date": "2015-04-02T21:17:36+0000",
            "content": "Commit 1670972 from Areek Zillur in branch 'dev/trunk'\n[ https://svn.apache.org/r1670972 ]\n\nLUCENE-6339: fix test bug (ensure opening nrt reader with applyAllDeletes) "
        },
        {
            "id": "comment-14393474",
            "author": "ASF subversion and git services",
            "date": "2015-04-02T21:31:31+0000",
            "content": "Commit 1670978 from Areek Zillur in branch 'dev/branches/lucene_solr_5_1'\n[ https://svn.apache.org/r1670978 ]\n\nLUCENE-6339: fix test bug (ensure opening nrt reader with applyAllDeletes) "
        },
        {
            "id": "comment-14395174",
            "author": "ASF subversion and git services",
            "date": "2015-04-03T22:26:45+0000",
            "content": "Commit 1671187 from Areek Zillur in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1671187 ]\n\nLUCENE-6339: fix test (ensure the maximum requested size is bounded to 1000) "
        },
        {
            "id": "comment-14395176",
            "author": "ASF subversion and git services",
            "date": "2015-04-03T22:27:00+0000",
            "content": "Commit 1671189 from Areek Zillur in branch 'dev/branches/lucene_solr_5_1'\n[ https://svn.apache.org/r1671189 ]\n\nLUCENE-6339: fix test (ensure the maximum requested size is bounded to 1000) "
        },
        {
            "id": "comment-14395194",
            "author": "ASF subversion and git services",
            "date": "2015-04-03T22:40:40+0000",
            "content": "Commit 1671196 from Areek Zillur in branch 'dev/trunk'\n[ https://svn.apache.org/r1671196 ]\n\nLUCENE-6339: fix test (ensure the maximum requested size is bounded to 1000) "
        },
        {
            "id": "comment-14483734",
            "author": "ASF subversion and git services",
            "date": "2015-04-07T18:36:37+0000",
            "content": "Commit 1671914 from Areek Zillur in branch 'dev/trunk'\n[ https://svn.apache.org/r1671914 ]\n\nLUCENE-6339: fix test (take into account inadmissible filtered search for multiple segments) "
        },
        {
            "id": "comment-14483735",
            "author": "ASF subversion and git services",
            "date": "2015-04-07T18:36:41+0000",
            "content": "Commit 1671915 from Areek Zillur in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1671915 ]\n\nLUCENE-6339: fix test (take into account inadmissible filtered search for multiple segments) "
        },
        {
            "id": "comment-14483736",
            "author": "ASF subversion and git services",
            "date": "2015-04-07T18:36:44+0000",
            "content": "Commit 1671916 from Areek Zillur in branch 'dev/branches/lucene_solr_5_1'\n[ https://svn.apache.org/r1671916 ]\n\nLUCENE-6339: fix test (take into account inadmissible filtered search for multiple segments) "
        },
        {
            "id": "comment-14488015",
            "author": "ASF subversion and git services",
            "date": "2015-04-09T19:07:43+0000",
            "content": "Commit 1672458 from Steve Rowe in branch 'dev/trunk'\n[ https://svn.apache.org/r1672458 ]\n\nLUCENE-6339: Maven config: add resource dir src/resources/ to the POM. "
        },
        {
            "id": "comment-14488021",
            "author": "ASF subversion and git services",
            "date": "2015-04-09T19:08:57+0000",
            "content": "Commit 1672459 from Steve Rowe in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1672459 ]\n\nLUCENE-6339: Maven config: add resource dir src/resources/ to the POM. (merged trunk r1672458) "
        },
        {
            "id": "comment-14488027",
            "author": "ASF subversion and git services",
            "date": "2015-04-09T19:12:03+0000",
            "content": "Commit 1672461 from Steve Rowe in branch 'dev/branches/lucene_solr_5_1'\n[ https://svn.apache.org/r1672461 ]\n\nLUCENE-6339: Maven config: add resource dir src/resources/ to the POM. (merged trunk r1672458) "
        },
        {
            "id": "comment-14495395",
            "author": "Timothy Potter",
            "date": "2015-04-15T00:30:56+0000",
            "content": "Bulk close after 5.1 release "
        }
    ]
}