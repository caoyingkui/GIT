{
    "id": "SOLR-2808",
    "title": "Node Recovery",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Sub-task",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Node recovery encompasses everything involved in getting a node to the \"active\" state after coming (back) up.",
    "attachments": {
        "TEST-org.apache.solr.cloud.RecoveryZkTest.xml": "https://issues.apache.org/jira/secure/attachment/12505844/TEST-org.apache.solr.cloud.RecoveryZkTest.xml"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Yonik Seeley",
            "id": "comment-13120151",
            "date": "2011-10-04T14:15:02+0000",
            "content": "When a node comes up, it starts in \"recovering\" state (actually, this is probably a per-shard state, but for now, one core == one shard).\nWhile in the recovering state, updates should still be sent to the node, but these updates should be buffered (most likely using transaction logs) to be applied later.\nThe node should ask peer(s) (or the leader?) for updates it missed.  If peers can't provide those updates (because the transaction logs only go back so far), then the node needs to initiate a full index copy first (probably re-using java replication code).\nAfter the new index is in place, updates from peers have been applied, and buffered updates have been applied, the state can move to \"active\". "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13151280",
            "date": "2011-11-16T15:54:56+0000",
            "content": "Thinking about the full index copy, we need to ensure there's not a window for potentially missed updates.  One way to do this is:\n 1. leader should ensure that future updates will be forwarded to the recovering node (i.e. recovering node should be visible in ZK and the leader's model should contain that node).\n 2. leader should then do a hard commit\n 3. index copy from leader to recovering node can commence "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13161370",
            "date": "2011-12-02T02:40:10+0000",
            "content": "RecoveryZkTest is sometimes failing for me.\nHere's one log. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13161376",
            "date": "2011-12-02T02:49:17+0000",
            "content": "I think i just fixed that a commit or two ago - i saw it once or twice - looked like it was due to the jetty shutdown i was doing - it was being cut off with an interrupt before properly shutting down - i started calling destroy on the solr filter before shutting down jetty and i think that should clean it up... "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13161384",
            "date": "2011-12-02T03:00:16+0000",
            "content": "My bad - thought i had committed it but had only made the change - in now. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13161388",
            "date": "2011-12-02T03:04:06+0000",
            "content": "To update what's done here:\n\nI've taken what Yonik has done in SOLR-2700 and implemented basic replication recovery.\n\nWhen a new replica comes up, or a node goes down and comes back, recovery is triggered. The recovery process issues a commit to the leader, starts buffering updates to the transaction log, and does a replication. Then it replays the buffered updates.\n\nThis gives us working recovery. Eventually, we will want to use realtime-get to do a faster recovery when the node has not been down for long - that's really just an optimization though. \n\nI'm still slaving away on getting good tests for this, but its up and working which is pretty sweet. "
        }
    ]
}