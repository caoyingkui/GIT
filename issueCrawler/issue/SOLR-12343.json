{
    "id": "SOLR-12343",
    "title": "JSON Field Facet refinement can return incorrect counts/stats for sorted buckets",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Bug",
        "fix_versions": [
            "7.5",
            "master (8.0)"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "The way JSON Facet's simple refinement \"re-sorts\" buckets after refinement can cause refined buckets to be \"bumped out\" of the topN based on the refined counts/stats depending on the sort - causing unrefined buckets originally discounted in phase#2 to bubble up into the topN and be returned to clients with inaccurate counts/stats\n\nThe simplest way to demonstrate this bug (in some data sets) is with a sort: 'count asc' facet:\n\n\tassume shard1 returns termX & termY in phase#1 because they have very low shard1 counts\n\t\n\t\tbut not returned at all by shard2, because these terms both have very high shard2 counts.\n\t\n\t\n\tAssume termX has a slightly lower shard1 count then termY, such that:\n\t\n\t\ttermX \"makes the cut\" off for the limit=N topN buckets\n\t\ttermY does not make the cut, and is the \"N+1\" known bucket at the end of phase#1\n\t\n\t\n\ttermX then gets included in the phase#2 refinement request against shard2\n\t\n\t\ttermX now has a much higher known total count then termY\n\t\tthe coordinator now sorts termX \"worse\" in the sorted list of buckets then termY\n\t\twhich causes termY to bubble up into the topN\n\t\n\t\n\ttermY is ultimately included in the final result with incomplete count/stat/sub-facet data instead of termX\n\t\n\t\tthis is all indepenent of the possibility that termY may actually have a significantly higher total count then termX across the entire collection\n\t\tthe key problem is that all/most of the other terms returned to the client have counts/stats that are the cumulation of all shards, but termY only has the contributions from shard1\n\t\n\t\n\n\n\nImportant Notes:\n\n\tThis scenerio can happen regardless of the amount of overrequest used. Additional overrequest just increases the number of \"extra\" terms needed in the index with \"better\" sort values then termX & termY in shard2\n\tsort: 'count asc' is not just an exceptional/pathelogical case:\n\t\n\t\tany function sort where additional data provided shards during refinement can cause a bucket to \"sort worse\" can also cause this problem.\n\t\tExamples: sum(price_i) asc , min(price_i) desc , avg(price_i) asc|desc , etc...",
    "attachments": {
        "SOLR-12343.patch": "https://issues.apache.org/jira/secure/attachment/12922918/SOLR-12343.patch",
        "__incomplete_processEmpty_microfix.patch": "https://issues.apache.org/jira/secure/attachment/12931206/__incomplete_processEmpty_microfix.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2018-05-10T21:38:03+0000",
            "content": "Ultimately what seems to be at issue here is a discrepency between how Yonik designed the \"simple\" facet algorithm, and how it's implemented \u2013 but its only problematic in these \"additional information from refinement can make sort values 'worse'\" type situations.\n\nAs Yonik noted in SOLR-11733 regarding the design of refine:simple|true ...\n[compared to facet.field] ...the refinement algorithm being different (and for a single-level facet field, simpler).\n It can be explained as:\n 1) find buckets to return as if you weren't doing refinement\n 2) for those buckets, make sure all shards have contributed to the statistics\n i.e. simple refinement doesn't change the buckets you get back.\nBut in actuality, adding refine:true can change the buckets you get back. In my example above, if refine:false was used, termX would have ultimately been returned (with an unrefined count) \u2013 but because of refinement it's not returned, and termY is returned in it's place.\n\nI've attached a simple test patch demonstrating the problem but I haven't yet dug into the code to figure out the best fix.\n\nI suspect what's needed (to stick to the intent of refine:simple ) is that after the coordinator picks buckets that need refined, it should prune down the list of \"all known\" (size limit=N + overrequest=R) buckets to just the \"buckets to return\" (size limit=N) so that once the refinement values come in the set of buckets desn't change, even if the order or the buckets does. ",
            "author": "Hoss Man",
            "id": "comment-16471156"
        },
        {
            "date": "2018-05-11T00:46:04+0000",
            "content": "My initial thinking was that FacetRequestSortedMerger.sortBuckets() should go ahead and truncate the list of bucket based on the limit+offset as the very last thing it does \u2013 for the \"pre-refinement\" call to sortBuckets() this wouldn't change anything about the buckets selected for refinement, and for the \"post-refinement\" call to sortBuckets() it would only change the order of the buckets already refined \u2013 bug goes away. There's even a comment on the pre-refinement call that says // todo: make sure this filters buckets as well which seemed to be directly on point.\n\nExcept... looking at the post-refinement use of sortBuckets() in FacetFieldMerger, I realize that the mincount type \"filtering\" (which is probably what that '// todo' actually refered to) needs to be applied after the buckets are sorting, but before pruning down down bsad on the offset+limit.\n\nWith something like count desc it wouldn't matter if we \"pre-truncate\" the list, because if any of the refined buckets don't have a count>mincount, then there's no chance any of the un-refined buckets will satisfy that mincount either ... but for things like index asc|desc or sorting by functions: it definitely matters in order to ensure we return the full \"limit\" # of buckets.\n\nAlthough, I guess a key question i have is: if the user has explicitly requested refinement, then is there really any value in returning the full \"limit\" # of buckets if some of those buckets aren't refined?\n\nThat really seems like the crux of this bug: to me, it seems like when refinement is requested we should NEVER return an unrefined bucket (ie: a bucket that is lying about it's count/stats) ... but I can imagine other folks might feel differently.\n\nAnyone have strong opinions?\n\n\u00a0\n\nFor now, I'll assume the current behavior is considered desirable by some, and brain storm potential enhancements to make it optional...\n\nPerhaps we should add a new refine:required variant? If the user says refinement is required, then sortBuckets() could pre-truncate.\n\nOr maybe better still:\n\n\twe add an int numShardsContributing = 1 to FacetBucket that gets incremented every time a shard is merged in.\n\tAdd the new refine:required option but implemented differently...\n\t\n\t\tsortBuckets() doesn't change \u2013 leave all the un-refined buckets in sortedBuckets all the the time\n\t\tConsumers of sortedBuckets (like FacetFieldMerger.getMergedResult() ) are responsible for checking the type of refinement:\n\t\t\n\t\t\tif it was required , then filter the buckets on numShardsContributing just like the existing filtering on mincount in the same loop\n\t\t\n\t\t\n\t\n\t\n\tAdditionally: add a new overrefine:N option that can be use in conjunction with, or independently from refine:required\n\t\n\t\tDefault to '0' for back compat\n\t\tused during refinement phase similar to how \"overrequest\" is used during the initial request\n\t\t\n\t\t\tie: FacetRequestSortedMerger would add it to the limit when computing numBucketsToCheck\n\t\t\n\t\t\n\t\n\t\n\n\n\nThis way, clients that are willing to \"pay extra\" during refinement can request that additional terms get refined\u00a0\u2013 which can be useful for non-trivial sorts to ensure that the \"best\" buckets really are returned.\u00a0 Independently clients can indicate if they are unwilling to accept un-refined buckets in the response because they care about accuracy, or would rather have as many buckets (up to limit) returned as possible, even if they couldn't be refined.\n\n\u00a0\n\nWhat do folks think?\n\nYonik Seeley do you see any problems with this approach? or have alternative suggestions?\n\n\u00a0\n\n\u00a0 ",
            "author": "Hoss Man",
            "id": "comment-16471335"
        },
        {
            "date": "2018-05-18T23:55:56+0000",
            "content": "I think the most important thing here is that individual buckets should have correct stats.\u00a0 The behavior uncovered here was not intentional and isn't useful, so I think it should just be considered a bug.\n\nTruncating the list of buckets to N before the refinement phase would fix the bug, but it would also throw away complete buckets that could make it into the top N after refinement.\u00a0 One could tweak to only throw away incomplete buckets after the top N, but that still leaves the filtering\u00a0complications you brought up. In the long term, perhaps a cursorMark approach would work better in conjunction with filtering? Although it does feel like paging facets is a less important feature in general.\n\n\u00a0Exactly which buckets we chose to refine\u00a0(and exactly how many) can remain an implementation detail. The essence of the simple refinement algorithm is:\n 1) collect top buckets from each shard\n 2) refine some subset of those buckets (refinement == ensure every shard that can contribute to that bucket has)\n 3) return only refined buckets\n\n\u00a0 ",
            "author": "Yonik Seeley",
            "id": "comment-16481366"
        },
        {
            "date": "2018-05-23T18:47:41+0000",
            "content": "... I think it should just be considered a bug.\nThat's pretty much my feeling, but I wasn't sure.\nTruncating the list of buckets to N before the refinement phase would fix the bug, but it would also throw away complete buckets that could make it into the top N after refinement.\noh right ... yeah, i was forgetting about buckets that got data from all shards in phase #1.\nExactly which buckets we chose to refine (and exactly how many) can remain an implementation detail. ...\nright ... it can be heuristically determined, and very conservative in cases where we know it doesn't matter \u2013 but i still think there should be an explicit option...\n\nI worked up a patch similar to the straw man i outlined above \u2013 except that i didn't add the refine:required variant since we're in agreement that this is a bug.\n\nIn the new patch:\n\n\tbuckets now keep track of how many shards contributed to them\n\t\n\t\tI did this with a quick and dirty BitSet instead of an int numShardsContributing counter since we have to handle the possibility that mergeBuckets() will get called more then once for a single shard when we have partial refinement of sub-facets\n\t\tthere's a nocommit in here about the possibility of re-using the Context.sawShard BitSet instead \u2013 but i couldn't wrap my head around an efficient way to do it so i punted\n\t\n\t\n\tduring the final \"pruning\" in FacetFieldMerger.getMergedResult() buckets are excluded if a bucket doesn't have contributions from as many shards as the FacetField\n\t\n\t\tagain, i needed a new BitSet in at the FacetField level to count the shards \u2013 because Context.numShards may include shards that never return any results for the facet (ie: empty shard) so they never merge any data at all)\n\t\n\t\n\tthere is a new overrefine:N option which works similar to overrequest \u2013 but instead of determining how many \"extra\" terms to request in phase#1, it determines how many \"extra\" buckets should be in numBucketsToCheck for refinement in phase #2 (but if some buckets are already fully populated in phase #2, then the actual number \"refined\" in phase#2 can be lower then limit+overrefine)\n\t\n\t\tthe default hueristic currently pays attention to the sort \u2013 since (IIUC) count desc and index asc|desc should never need any \"over refinement\" unless mincount > 1\n\t\tif we have a non-trivial sort, and the user specified an explicit overrequest:N then the default hueristic for overrefine uses the same value N\n\t\t\n\t\t\tbecause i'm assuming if people have explicitly requested sort:SPECIAL, refine:true, overrequest:N then they care about the accuracy of the the terms to some degree N, and the bigger N is the more we should care about over-refinement as well.\n\t\t\n\t\t\n\t\tif neither overrequest or overrefine are explicitly set, then we use the same limit * 1.1 + 4 type hueristic as overrequest\n\t\tthere's another nocommit here though: if we're using a hueritic, should we be scaling the derived numBucketsToCheck based on mincount ? ... if mincount=M > 1 should we be doing something like numBucketsToCheck *= M ??\n\t\t\n\t\t\talthough, thinking about it now \u2013 this kind of mincount based factor would probably make more sense in the overrequest hueristic? maybe for overrefine we should look at how many buckets were already fully populated in phase#1 AND meet the mincount, and use the the difference between that number and the limit to decide a scaling factor?\n\t\t\teither way: can probably TODO this for a future enhancement.\n\t\t\n\t\t\n\t\n\t\n\tTesting wise...\n\t\n\t\tThese changes fix the problems in previous test patch\n\t\tI've also added some more tests, but there's nocommit's to add a lot more including verification of nested facets\n\t\tI didn't want to go too deep down the testing rabbit hole until i was sure we wanted to go this route.\n\t\n\t\n\n\n\nwhat do you think? ",
            "author": "Hoss Man",
            "id": "comment-16487839"
        },
        {
            "date": "2018-05-25T00:04:33+0000",
            "content": "there is a new overrefine:N option which works similar to overrequest \u2013 but instead of determining how many \"extra\" terms to request in phase#1, it determines how many \"extra\" buckets should be in numBucketsToCheck for refinement in phase #2 ...\nIt occurs to me now, that adding this option should also provide a \"solution\" for SOLR-11733 ... people who are concerned about refining long tail terms can set overrefine really high. ",
            "author": "Hoss Man",
            "id": "comment-16490020"
        },
        {
            "date": "2018-06-19T00:53:34+0000",
            "content": "\nUpdated patch with more tests and some code tweaks based on a few things the new tests caught.\n\nStill outstanding is the question of the new BitSets I added...\n\n\n\n\tbuckets now keep track of how many shards contributed to them ...\n\t\n\t\tthere's a nocommit in here about the possibility of re-using the Context.sawShard BitSet instead \u2013 but i couldn't wrap my head around an efficient way to do it so i punted\n\t\n\t\n\t...buckets are excluded if a bucket doesn't have contributions from as many shards as the FacetField...\n\t\n\t\tagain, i needed a new BitSet in at the FacetField level to count the shards \u2013 because Context.numShards may include shards that never return any results for the facet (ie: empty shard) so they never merge any data at all)\n\t\n\t\n\n\n\nI think it should be possible to re-implement the FacetBucket.getNumShardsMerged() method (i added) using Context.sawShard by using sawShard.get(bucketNum * numShards, bucketNum * numShards + numShards) to take a \"slice\" of the BitSet just for the current bucket and then look at it's cardinality.  the added cost of taking the slice only for buckets being considered in sorted order is probably a better trade off them the overhead of creating a new BitSet for every FacetBucket even if they are never considered for the response.\n\nBut I still don't see anyway to efficiently figure out the \"shards that participated\" info needed at the FacetField level using the existing sawShard BitSet \u2013 particularly with the changes I had to make to account for the case where a shard has docs participating in a facet, but not matching any buckets (see testSortedSubFacetRefinementWhenParentOnlyReturnedByOneShard ).  Fortunately that's just one new BitSet per FacetField instance (not per bucket).\n\n\n\nI'll look at refactoring FacetBucket.getNumShardsMerged() to use Context.sawShard soon. ",
            "author": "Hoss Man",
            "id": "comment-16516506"
        },
        {
            "date": "2018-06-20T22:56:28+0000",
            "content": "Not sure if it relates to this bug \u2013 please move/add if not \u2013 but my Jenkins found a reproducing failure for TestCloudJSONFacetSKG.testBespoke():\n\n\nChecking out Revision 008bc74bebef96414f19118a267dbf982aba58b9 (refs/remotes/origin/master)\n[...]\nant test  -Dtestcase=TestCloudJSONFacetSKG -Dtests.method=testBespoke -Dtests.seed=5D223D88BF5BF89 -Dtests.slow=true -Dtests.locale=bg-BG -Dtests.timezone=America/Asuncion -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1\n   [junit4] FAILURE 0.11s J0  | TestCloudJSONFacetSKG.testBespoke <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: Didn't check a single bucket???\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([5D223D88BF5BF89:E09A7E14375787E]:0)\n   [junit4]    > \tat org.apache.solr.cloud.TestCloudJSONFacetSKG.testBespoke(TestCloudJSONFacetSKG.java:219)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:748)\n[...]\n   [junit4]   2> NOTE: test params are: codec=FastCompressingStoredFields(storedFieldsFormat=CompressingStoredFieldsFormat(compressionMode=FAST, chunkSize=4, maxDocsPerChunk=1, blockSize=332), termVectorsFormat=CompressingTermVectorsFormat(compressionMode=FAST, chunkSize=4, blockSize=332)), sim=Asserting(org.apache.lucene.search.similarities.AssertingSimilarity@4052d535), locale=el, timezone=Indian/Antananarivo\n   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_151 (64-bit)/cpus=16,threads=1,free=213710424,total=526909440\n\n ",
            "author": "Steve Rowe",
            "id": "comment-16518698"
        },
        {
            "date": "2018-06-21T18:44:20+0000",
            "content": "Not sure if it relates to this bug...\nNo, that's an unrelated stupid test mistake that i've fixed locally and am currently hammering on \u2013 but that's for pointing it out! ",
            "author": "Hoss Man",
            "id": "comment-16519673"
        },
        {
            "date": "2018-07-03T01:19:35+0000",
            "content": "I think some of what I just worked on for SOLR-12326 is related to (or can be used by) this issue.\nFacetRequestSortedMerger now has a \"BitSet shardHasMoreBuckets\" to help deal with the fact that complete buckets do not need participation from every shard.  That info in conjunction with Context.sawShard should be enough to tell if a bucket is already \"complete\".\nFor every bucket that isn't complete, we can either refine it, or drop it. ",
            "author": "Yonik Seeley",
            "id": "comment-16530657"
        },
        {
            "date": "2018-07-05T17:35:17+0000",
            "content": "Here's an updated patch... I started with the last patch here and reimplemented just part of it to implement a isBucketComplete function, and then used that to screen out non-refined buckets in the same place that filters mincount.  All tests pass, and I plan on committing shortly. ",
            "author": "Yonik Seeley",
            "id": "comment-16533938"
        },
        {
            "date": "2018-07-05T17:37:02+0000",
            "content": "yonik: hold up ... i put this on the backburner because of SOLR-12516 (which i'm currently actively working on)\n\nFixing SOLR-12343 before SOLR-12516 will make SOLR-12516  a lot worse in the common case (it will stop returning the facet range \"other\" buckets completely since currently no code refines them at all) ",
            "author": "Hoss Man",
            "id": "comment-16533943"
        },
        {
            "date": "2018-07-06T02:18:16+0000",
            "content": "it will stop returning the facet range \"other\" buckets completely since currently no code refines them at all\n\nHmmm, so the patch I attached seems like it would only remove incomplete buckets in field facets under \"other\" buckets (i.e. if they don't actually need refining to be complete, they won't be removed by the current patch).  But this could still be worse in some cases (missing vs incomplete when refinement is requested), so I agree this can wait until  SOLR-12516 is done.  ",
            "author": "Yonik Seeley",
            "id": "comment-16534377"
        },
        {
            "date": "2018-07-06T19:09:03+0000",
            "content": "I think some of what I just worked on for SOLR-12326 is related to (or can be used by) this issue. ...\n\nHere's an updated patch... I started with the last patch here and reimplemented just part of it to implement a isBucketComplete function ...\n\nI'm still not caught up on exactly how SOLR-12326 works, but i do like how nice and clean this patch is now.\n\nI've updated it to resolve the test conflicts introduced by SOLR-12516 and to update the ref guide with an explanation of overrefine vs overrequest.\n\nI'm still running tests locally, but if you're confident in this feel free to commit. ",
            "author": "Hoss Man",
            "id": "comment-16535244"
        },
        {
            "date": "2018-07-08T10:40:15+0000",
            "content": "\n\n\n  -1 overall \n\n\n\n\n\n\n\n\n\n Vote \n Subsystem \n Runtime \n Comment \n\n\n\u00a0\n\u00a0\n\u00a0\n  Prechecks  \n\n\n +1 \n  test4tests  \n   0m  0s \n  The patch appears to include 1 new or modified test files.  \n\n\n\u00a0\n\u00a0\n\u00a0\n  master Compile Tests  \n\n\n +1 \n  compile  \n   1m 59s \n  master passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Patch Compile Tests  \n\n\n +1 \n  compile  \n   1m 49s \n  the patch passed  \n\n\n +1 \n  javac  \n   1m 49s \n  the patch passed  \n\n\n +1 \n  Release audit (RAT)  \n   1m 55s \n  the patch passed  \n\n\n +1 \n  Check forbidden APIs  \n   1m 49s \n  the patch passed  \n\n\n +1 \n  Validate source patterns  \n   1m 49s \n  the patch passed  \n\n\n +1 \n  Validate ref guide  \n   1m 49s \n  the patch passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Other Tests  \n\n\n -1 \n  unit  \n  65m 33s \n  core in the patch failed.  \n\n\n  \n   \n  73m  9s \n   \n\n\n\n\n\n\n\n\n\n Reason \n Tests \n\n\n Failed junit tests \n solr.cloud.autoscaling.IndexSizeTriggerTest \n\n\n\u00a0\n solr.cloud.api.collections.ShardSplitTest \n\n\n\u00a0\n solr.cloud.ForceLeaderTest \n\n\n\u00a0\n solr.cloud.api.collections.TestCollectionsAPIViaSolrCloudCluster \n\n\n\u00a0\n solr.cloud.autoscaling.sim.TestLargeCluster \n\n\n\n\n\n\n\n\n\n Subsystem \n Report/Notes \n\n\n JIRA Issue \n SOLR-12343 \n\n\n JIRA Patch URL \n https://issues.apache.org/jira/secure/attachment/12930572/SOLR-12343.patch \n\n\n Optional Tests \n  compile  javac  unit  ratsources  checkforbiddenapis  validatesourcepatterns  validaterefguide  \n\n\n uname \n Linux lucene1-us-west 3.13.0-88-generic #135-Ubuntu SMP Wed Jun 8 21:10:42 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux \n\n\n Build tool \n ant \n\n\n Personality \n /home/jenkins/jenkins-slave/workspace/PreCommit-SOLR-Build/sourcedir/dev-tools/test-patch/lucene-solr-yetus-personality.sh \n\n\n git revision \n master / b7d14c5 \n\n\n ant \n version: Apache Ant(TM) version 1.9.3 compiled on April 8 2014 \n\n\n Default Java \n 1.8.0_172 \n\n\n unit \n https://builds.apache.org/job/PreCommit-SOLR-Build/140/artifact/out/patch-unit-solr_core.txt \n\n\n  Test Results \n https://builds.apache.org/job/PreCommit-SOLR-Build/140/testReport/ \n\n\n modules \n C: solr/core solr/solr-ref-guide U: solr \n\n\n Console output \n https://builds.apache.org/job/PreCommit-SOLR-Build/140/console \n\n\n Powered by \n Apache Yetus 0.7.0   http://yetus.apache.org \n\n\n\n\n\n\nThis message was automatically generated.\n ",
            "author": "Lucene/Solr QA",
            "id": "comment-16536065"
        },
        {
            "date": "2018-07-08T12:55:32+0000",
            "content": "I'm occasionally getting a failure in testSortedFacetRefinementPushingNonRefinedBucketBackIntoTopN\nI haven't tried digging into it yet though. ",
            "author": "Yonik Seeley",
            "id": "comment-16536095"
        },
        {
            "date": "2018-07-09T01:55:43+0000",
            "content": "which assertion?  stacktrace? reproduce line? .. does the seed actually reproduce?\n\nThere's virtually no randomization in the test at all, except for the number of fillter termss/overrequest.\n\nIf you're seeing you're seeing a seed that reproduces, it makes me wonder if there is an edge case / off by one error based on the number of buckets ... if the seed doesn't reproduce (reliably) then it makes me wonder if it's an edge case that has to do with with which order the shards respond (ie: how the merger initializes the datastructs that get merged) ",
            "author": "Hoss Man",
            "id": "comment-16536481"
        },
        {
            "date": "2018-07-09T02:27:56+0000",
            "content": "Found one \u2013 it seems to be specific to the situation where overrequest==0, and the facet is nested under another facet?\n\nplaying the with values of top_over and top_refine it doesn't seem to matter if parent facet is refined, but the key is wether the top facet also uses\u00a0overrequest:0 (fails) or\u00a0overrequest:999 (passes)\n\n\u00a0\n\n   [junit4]   2> 9990 INFO  (qtp1276305453-48) [    x:collection1] o.a.s.c.S.Request [collection1]  webapp=/solr path=/select params={df=text&distrib=false&_facet_={}&fl=id&fl=score&shards.purpose=1048580&start=0&fsv=true&shard.url=127.0.0.1:47372/solr/collection1&rows=0&version=2&q=*:*&json.facet={+all:{+type:terms,+field:all_ss,+limit:1,+refine:true,+overrequest:0+++++++,+facet:{+++cat_count:{+type:terms,+field:cat_s,+limit:3,+overrequest:0+++++++++++++++,+refine:true,+sort:'count+asc'+},+++cat_price:{+type:terms,+field:cat_s,+limit:3,+overrequest:0+++++++++++++++,+refine:true,+sort:'sum_p+asc'++++++++++++++++,+facet:+{+sum_p:+'sum(price_i)'+}+}}+}+}&NOW=1531102182236&isShard=true&wt=javabin} hits=9 status=0 QTime=17\n   [junit4]   2> 9994 INFO  (qtp1276305453-49) [    x:collection1] o.a.s.c.S.Request [collection1]  webapp=/solr path=/select params={df=text&distrib=false&_facet_={\"refine\":{\"all\":{\"_p\":[[\"z_all\",{\"cat_count\":{\"_l\":[\"A\",\"B\",\"C\"]},\"cat_price\":{\"_l\":[\"A\",\"B\",\"C\"]}}]]}}}&shards.purpose=2097152&shard.url=127.0.0.1:47372/solr/collection1&rows=0&version=2&q=*:*&json.facet={+all:{+type:terms,+field:all_ss,+limit:1,+refine:true,+overrequest:0+++++++,+facet:{+++cat_count:{+type:terms,+field:cat_s,+limit:3,+overrequest:0+++++++++++++++,+refine:true,+sort:'count+asc'+},+++cat_price:{+type:terms,+field:cat_s,+limit:3,+overrequest:0+++++++++++++++,+refine:true,+sort:'sum_p+asc'++++++++++++++++,+facet:+{+sum_p:+'sum(price_i)'+}+}}+}+}&NOW=1531102182236&isShard=true&facet=false&wt=javabin} hits=9 status=0 QTime=1\n   [junit4]   2> 9996 INFO  (qtp1503674478-65) [    x:collection1] o.a.s.c.S.Request [collection1]  webapp=/solr path=/select params={shards=127.0.0.1:54950/solr/collection1,127.0.0.1:47372/solr/collection1,127.0.0.1:52833/solr/collection1&shards=debugQuery&shards=true&q=*:*&json.facet={+all:{+type:terms,+field:all_ss,+limit:1,+refine:true,+overrequest:0+++++++,+facet:{+++cat_count:{+type:terms,+field:cat_s,+limit:3,+overrequest:0+++++++++++++++,+refine:true,+sort:'count+asc'+},+++cat_price:{+type:terms,+field:cat_s,+limit:3,+overrequest:0+++++++++++++++,+refine:true,+sort:'sum_p+asc'++++++++++++++++,+facet:+{+sum_p:+'sum(price_i)'+}+}}+}+}&indent=true&rows=0&wt=json&version=2.2} hits=19 status=0 QTime=25\n   [junit4]   2> 9997 ERROR (TEST-TestJsonFacetRefinement.testSortedFacetRefinementPushingNonRefinedBucketBackIntoTopN-seed#[775BF43EF8268D50]) [    ] o.a.s.SolrTestCaseHS query failed JSON validation. error=mismatch: 'X'!='C' @ facets/all/buckets/[0]/cat_count/buckets/[2]/val\n   [junit4]   2>  expected =facets=={ count: 19,all:{ buckets:[   { val:z_all, count: 19,    cat_count:{ buckets:[                  {val:A,count:1},                 {val:B,count:1},                 {val:X,count:4},    ] },    cat_price:{ buckets:[                  {val:A,count:1,sum_p:1.0},                 {val:B,count:1,sum_p:1.0},                 {val:X,count:4,sum_p:4.0},    ] }} ] } }\n   [junit4]   2>  response = {\n   [junit4]   2>   \"responseHeader\":{\n   [junit4]   2>     \"status\":0,\n   [junit4]   2>     \"QTime\":25},\n   [junit4]   2>   \"response\":{\"numFound\":19,\"start\":0,\"maxScore\":1.0,\"docs\":[]\n   [junit4]   2>   },\n   [junit4]   2>   \"facets\":{\n   [junit4]   2>     \"count\":19,\n   [junit4]   2>     \"all\":{\n   [junit4]   2>       \"buckets\":[{\n   [junit4]   2>           \"val\":\"z_all\",\n   [junit4]   2>           \"count\":19,\n   [junit4]   2>           \"cat_price\":{\n   [junit4]   2>             \"buckets\":[{\n   [junit4]   2>                 \"val\":\"A\",\n   [junit4]   2>                 \"count\":1,\n   [junit4]   2>                 \"sum_p\":1.0},\n   [junit4]   2>               {\n   [junit4]   2>                 \"val\":\"B\",\n   [junit4]   2>                 \"count\":1,\n   [junit4]   2>                 \"sum_p\":1.0},\n   [junit4]   2>               {\n   [junit4]   2>                 \"val\":\"C\",\n   [junit4]   2>                 \"count\":6,\n   [junit4]   2>                 \"sum_p\":6.0}]},\n   [junit4]   2>           \"cat_count\":{\n   [junit4]   2>             \"buckets\":[{\n   [junit4]   2>                 \"val\":\"A\",\n   [junit4]   2>                 \"count\":1},\n   [junit4]   2>               {\n   [junit4]   2>                 \"val\":\"B\",\n   [junit4]   2>                 \"count\":1},\n   [junit4]   2>               {\n   [junit4]   2>                 \"val\":\"C\",\n   [junit4]   2>                 \"count\":6}]}}]}}}\n   [junit4]   2> \n   [junit4]   2> 10000 INFO  (TEST-TestJsonFacetRefinement.testSortedFacetRefinementPushingNonRefinedBucketBackIntoTopN-seed#[775BF43EF8268D50]) [    ] o.a.s.SolrTestCaseJ4 ###Ending testSortedFacetRefinementPushingNonRefinedBucketBackIntoTopN\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestJsonFacetRefinement -Dtests.method=testSortedFacetRefinementPushingNonRefinedBucketBackIntoTopN -Dtests.seed=775BF43EF8268D50 -Dtests.slow=true -Dtests.badapples=true -Dtests.locale=pl-PL -Dtests.timezone=Africa/Bamako -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] ERROR   4.32s | TestJsonFacetRefinement.testSortedFacetRefinementPushingNonRefinedBucketBackIntoTopN <<<\n   [junit4]    > Throwable #1: java.lang.RuntimeException: mismatch: 'X'!='C' @ facets/all/buckets/[0]/cat_count/buckets/[2]/val\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([775BF43EF8268D50:DB8655EB2671818E]:0)\n   [junit4]    > \tat org.apache.solr.SolrTestCaseHS.matchJSON(SolrTestCaseHS.java:161)\n   [junit4]    > \tat org.apache.solr.SolrTestCaseHS.assertJQ(SolrTestCaseHS.java:143)\n   [junit4]    > \tat org.apache.solr.SolrTestCaseHS$Client$Tester.assertJQ(SolrTestCaseHS.java:255)\n   [junit4]    > \tat org.apache.solr.SolrTestCaseHS$Client.testJQ(SolrTestCaseHS.java:297)\n   [junit4]    > \tat org.apache.solr.search.facet.TestJsonFacetRefinement.testSortedFacetRefinementPushingNonRefinedBucketBackIntoTopN(TestJsonFacetRefinement.java:568)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:748)\n   [junit4]   2> 10016 INFO  (SUITE-TestJsonFacetRefinement-seed#[775BF43EF8268D50]-worker) [    ] o.e.j.s.Abs\n\n\n\n\n...i haven't worked through it yet to figure out the problem, but my initial impression is that i made this test too aggressive? I'm not sure it's safe to assert correct results with top_over=1 ... but i'm not sure why it matters what the sub-facet overrequest is in that case? ",
            "author": "Hoss Man",
            "id": "comment-16536494"
        },
        {
            "date": "2018-07-09T17:32:05+0000",
            "content": "Ok ... fresh eyes and i see the problem.\n\nWhen final int overreq = 0 we don't add any \"filler\" docs, which means that when the nested facet test happens, shardC0 and shardC1 disagree about the \"top term\" for the parent facet on the all_ss field \u2013 shardC0 only knows about z_al while shardC1 has a tie between z_all} and {{some and some wins the tie due to index order \u2013 so when that parent facet uses overrequest:0 the initial merge logic doesn't have any contributions from shardC1 for the chosen all_ss:z_all bucket ... so it only knows to ask to refine the top3 child buckets it does know about (from shardC0): \"A,B,C\".  If the parent facet uses any overrequest larger then 0, then it would get the all_ss:z_all bucket from shardC1 as well, and have some child buckets to consider to know that C is a bad candidate, and it should be refining X instead.\n\nOn the flip side, when final int overreq = 1 (or anything higher) the addition of even a few filler docs is enough to skew the all_ss term stats on shardC1, such that it also thinkgs z_all is the top term, so regardless of the amount of overrequest on the top facet, the phase #1 merge has buckets from both shards for the child facet to consider.\n\n\n\nI remember when i was writing this test, and i include the some terms the entire point was to stress the case where the 2 shards disagree about the \"top\" term term from the parent facet \u2013 but apparently when adding the filler docs/terms randomization i broke that so that it's not always true, it only happens when there are no filler docs.  But it also seems like an unfair test, because when they do disagree, there's no reason for hte merge logic to think X is a worthwhile term to refine. what mattes is that in this case, C is accurately refined\n\nI'm working up a test fix... ",
            "author": "Hoss Man",
            "id": "comment-16537306"
        },
        {
            "date": "2018-07-09T18:13:14+0000",
            "content": "updated patch with fixed test.\n\nYonik Seeley \u2013 look good? ",
            "author": "Hoss Man",
            "id": "comment-16537347"
        },
        {
            "date": "2018-07-10T02:27:27+0000",
            "content": "Looks good, thanks for tracking that down! ",
            "author": "Yonik Seeley",
            "id": "comment-16537904"
        },
        {
            "date": "2018-07-11T16:23:13+0000",
            "content": "Yonik Seeley - i've been testing this out with the SKG (relatedness()) function \u2013 where i initially discovered bug \u2013 and trying to remove the workarounds for this that are currently in TestCloudJSONFacetSKG (grep for SOLR-12343) but i'm seeing some failures that I think i've traced back to a mistake in isBucketComplete() that only affects facets using processEmpty:true ...\n\n\nin getRefinement() you've got returnedAllBuckets taking into consideration processEmpty:true \u2013 so that even if a shardA doesn't say it has more:true we will still send it candidate bucketX for refinement if we didn't explicitly saw bucketX on shardA.  so far so good.\n\nbut then, once all the refinement is done, and we have a fully refined bucketX it might now sort \"lower\" then an incomplete bucketY ... and isBucketComplete doesn't pay any attention to processEmpty:true ... so it sees that shardA does not have more:true and thinks (the incomplete) bucketY is ok to return.\n\n\n...I'll work up an isolated test case ",
            "author": "Hoss Man",
            "id": "comment-16540330"
        },
        {
            "date": "2018-07-11T18:52:57+0000",
            "content": "I've attached an isloated __incomplete_processEmpty_microfix.patch which needs to be applied on top of the previous patch to try and give a quick and dirty demonstration of  the problem (w/o using the relatedness() function) by adding a new mode to the debug() agg function that will count the shards that contribute to a bucket.\n\nI was hoping there would be a straight forward fix (also in the patch) by just checking processEmpty anytime we check shardHasMoreBuckets \u2013 but when dealing with nested facets the other piece of the puzzle is mcontext.bucketWasMissing() but that info ephemeral as the merge logic loops over the shards, so we can't look it up for an arbitrary shard after the fact ... even then i'm not sure the new processEmpty test code i wrote can ever pass the way i initially thought it should given how missing (parent) buckets are refined via \"_l\" w/o specifying any child buckets.\n\nI need to think about this some more ... and i'll also try to write a simple relatedness() based test that more directly shows the \"unrefined bucket can still bubble up because of processEmpty\" (I set out with the debug() approach thinking it would be simpler to understand, but i don't think it is) ",
            "author": "Hoss Man",
            "id": "comment-16540521"
        },
        {
            "date": "2018-07-11T19:29:10+0000",
            "content": "\n\n\n  -1 overall \n\n\n\n\n\n\n\n\n\n Vote \n Subsystem \n Runtime \n Comment \n\n\n\u00a0\n\u00a0\n\u00a0\n  Prechecks  \n\n\n +1 \n  test4tests  \n   0m  0s \n  The patch appears to include 1 new or modified test files.  \n\n\n\u00a0\n\u00a0\n\u00a0\n  master Compile Tests  \n\n\n +1 \n  compile  \n  13m 36s \n  master passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Patch Compile Tests  \n\n\n +1 \n  compile  \n  16m 20s \n  the patch passed  \n\n\n +1 \n  javac  \n  16m 20s \n  the patch passed  \n\n\n +1 \n  Release audit (RAT)  \n  17m 13s \n  the patch passed  \n\n\n +1 \n  Check forbidden APIs  \n  16m 20s \n  the patch passed  \n\n\n +1 \n  Validate source patterns  \n  16m 20s \n  the patch passed  \n\n\n +1 \n  Validate ref guide  \n  16m 20s \n  the patch passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Other Tests  \n\n\n -1 \n  unit  \n 153m 34s \n  core in the patch failed.  \n\n\n  \n   \n 213m 21s \n   \n\n\n\n\n\n\n\n\n\n Reason \n Tests \n\n\n Failed junit tests \n solr.cloud.api.collections.TestCollectionsAPIViaSolrCloudCluster \n\n\n\u00a0\n solr.cloud.cdcr.CdcrBidirectionalTest \n\n\n\u00a0\n solr.cloud.autoscaling.sim.TestExecutePlanAction \n\n\n\u00a0\n solr.cloud.autoscaling.SearchRateTriggerIntegrationTest \n\n\n\u00a0\n solr.cloud.api.collections.ShardSplitTest \n\n\n\u00a0\n solr.cloud.autoscaling.sim.TestLargeCluster \n\n\n\n\n\n\n\n\n\n Subsystem \n Report/Notes \n\n\n JIRA Issue \n SOLR-12343 \n\n\n JIRA Patch URL \n https://issues.apache.org/jira/secure/attachment/12930878/SOLR-12343.patch \n\n\n Optional Tests \n  compile  javac  unit  ratsources  checkforbiddenapis  validatesourcepatterns  validaterefguide  \n\n\n uname \n Linux lucene2-us-west.apache.org 4.4.0-112-generic #135-Ubuntu SMP Fri Jan 19 11:48:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux \n\n\n Build tool \n ant \n\n\n Personality \n /home/jenkins/jenkins-slave/workspace/PreCommit-SOLR-Build/sourcedir/dev-tools/test-patch/lucene-solr-yetus-personality.sh \n\n\n git revision \n master / fe180bb \n\n\n ant \n version: Apache Ant(TM) version 1.9.6 compiled on July 8 2015 \n\n\n Default Java \n 1.8.0_172 \n\n\n unit \n https://builds.apache.org/job/PreCommit-SOLR-Build/142/artifact/out/patch-unit-solr_core.txt \n\n\n  Test Results \n https://builds.apache.org/job/PreCommit-SOLR-Build/142/testReport/ \n\n\n modules \n C: solr/core solr/solr-ref-guide U: solr \n\n\n Console output \n https://builds.apache.org/job/PreCommit-SOLR-Build/142/console \n\n\n Powered by \n Apache Yetus 0.7.0   http://yetus.apache.org \n\n\n\n\n\n\nThis message was automatically generated.\n ",
            "author": "Lucene/Solr QA",
            "id": "comment-16540576"
        },
        {
            "date": "2018-07-11T21:44:37+0000",
            "content": "...test that more directly shows the \"unrefined bucket can still bubble up because of processEmpty\" (I set out with the debug() approach thinking it would be simpler to understand, but i don't think it is)\nI realized at lunch that this is actaully very easy to do just be sorting on \" debug asc \" ... i encourge folks to completley ignore my half-assed non-functional changes in __incomplete_processEmpty_microfix.patch and instead just checkout the latest SOLR-12343.patch\u00a0\u2013 it has no new code changes, just new tests in TestJsonFacetRefinement:\n\n\ttestProcessEmptyRefinement\n\t\n\t\tcompletley new method\n\t\tshows how processEmpty can cause unrefined bucket to \"bubble up\" after a refined bucket's sort value changes\n\t\n\t\n\ttestSortedSubFacetRefinementWhenParentOnlyReturnedByOneShard\n\t\n\t\tupdated method from previous patches\n\t\tnew assertions here show similar situation to testProcessEmptyRefinement but with a nested facet\n\t\tsee nocommits comments for details of the concerns i mentioned in my last comment about whether this test can ever be viable\n\t\n\t\n\n ",
            "author": "Hoss Man",
            "id": "comment-16540718"
        },
        {
            "date": "2018-07-12T01:40:52+0000",
            "content": "\n\n\n  -1 overall \n\n\n\n\n\n\n\n\n\n Vote \n Subsystem \n Runtime \n Comment \n\n\n\u00a0\n\u00a0\n\u00a0\n  Prechecks  \n\n\n +1 \n  test4tests  \n   0m  0s \n  The patch appears to include 2 new or modified test files.  \n\n\n\u00a0\n\u00a0\n\u00a0\n  master Compile Tests  \n\n\n +1 \n  compile  \n   4m  7s \n  master passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Patch Compile Tests  \n\n\n +1 \n  compile  \n   2m 41s \n  the patch passed  \n\n\n +1 \n  javac  \n   2m 41s \n  the patch passed  \n\n\n +1 \n  Release audit (RAT)  \n   2m 55s \n  the patch passed  \n\n\n +1 \n  Check forbidden APIs  \n   2m 41s \n  the patch passed  \n\n\n -1 \n  Validate source patterns  \n   2m 41s \n  Validate source patterns validate-source-patterns failed  \n\n\n -1 \n  Validate ref guide  \n   2m 41s \n  Validate source patterns validate-source-patterns failed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Other Tests  \n\n\n -1 \n  unit  \n  94m 19s \n  core in the patch failed.  \n\n\n  \n   \n 104m 44s \n   \n\n\n\n\n\n\n\n\n\n Reason \n Tests \n\n\n Failed junit tests \n solr.cloud.autoscaling.IndexSizeTriggerTest \n\n\n\u00a0\n solr.cloud.api.collections.ShardSplitTest \n\n\n\u00a0\n solr.search.facet.TestJsonFacetRefinement \n\n\n\n\n\n\n\n\n\n Subsystem \n Report/Notes \n\n\n JIRA Issue \n SOLR-12343 \n\n\n JIRA Patch URL \n https://issues.apache.org/jira/secure/attachment/12931226/SOLR-12343.patch \n\n\n Optional Tests \n  compile  javac  unit  ratsources  checkforbiddenapis  validatesourcepatterns  validaterefguide  \n\n\n uname \n Linux lucene2-us-west.apache.org 4.4.0-112-generic #135-Ubuntu SMP Fri Jan 19 11:48:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux \n\n\n Build tool \n ant \n\n\n Personality \n /home/jenkins/jenkins-slave/workspace/PreCommit-SOLR-Build/sourcedir/dev-tools/test-patch/lucene-solr-yetus-personality.sh \n\n\n git revision \n master / fe180bb \n\n\n ant \n version: Apache Ant(TM) version 1.9.6 compiled on July 8 2015 \n\n\n Default Java \n 1.8.0_172 \n\n\n Validate source patterns \n https://builds.apache.org/job/PreCommit-SOLR-Build/143/artifact/out/patch-validate-source-patterns-root.txt \n\n\n Validate ref guide \n https://builds.apache.org/job/PreCommit-SOLR-Build/143/artifact/out/patch-validate-source-patterns-root.txt \n\n\n unit \n https://builds.apache.org/job/PreCommit-SOLR-Build/143/artifact/out/patch-unit-solr_core.txt \n\n\n  Test Results \n https://builds.apache.org/job/PreCommit-SOLR-Build/143/testReport/ \n\n\n modules \n C: solr/core solr/solr-ref-guide U: solr \n\n\n Console output \n https://builds.apache.org/job/PreCommit-SOLR-Build/143/console \n\n\n Powered by \n Apache Yetus 0.7.0   http://yetus.apache.org \n\n\n\n\n\n\nThis message was automatically generated.\n ",
            "author": "Lucene/Solr QA",
            "id": "comment-16541000"
        },
        {
            "date": "2018-07-16T18:40:36+0000",
            "content": "but then, once all the refinement is done, and we have a fully refined bucketX it might now sort \"lower\" then an incomplete bucketY ... and isBucketComplete doesn't pay any attention to processEmpty:true ... so it sees that shardA does not have more:true and thinks (the incomplete) bucketY is ok to return.\n\nI haven't been able to come up with a better solution for this, and since processEmpty is pretty special case, I think i'm just going to break it out into it's own Jira, and revise the patch so that the current assertion failures are confined to test methods that are @AwaitsFix'ed on that issue \u2013 that way we can move forward with the existing fix that likely impacts more people. ",
            "author": "Hoss Man",
            "id": "comment-16545604"
        },
        {
            "date": "2018-07-16T18:47:30+0000",
            "content": "I think i'm just going to break it out into it's own Jira, and revise the patch so that the current assertion failures are confined to test methods that are @AwaitsFix'ed on that issue \u2013 that way we can move forward with the existing fix that likely impacts more people.\nspun off processEmpty issue to SOLR-12556 and updated this patch with slightly tweaked tests so the effected assertions are isolated in their own (disabled) test methods.\n\n\n\nI think this is good to go. ",
            "author": "Hoss Man",
            "id": "comment-16545607"
        },
        {
            "date": "2018-07-16T21:59:08+0000",
            "content": "Updated patch to fix a small javadoc problem and to update comments in TestCloudJSONFacetSKG to refer to SOLR-12556 instead of this issue.\n\n\u00a0\n\nYonik Seeley: any concerns about the current state of this patch? can i go ahead and commit? ",
            "author": "Hoss Man",
            "id": "comment-16545783"
        },
        {
            "date": "2018-07-17T17:43:28+0000",
            "content": "\n\n\n  -1 overall \n\n\n\n\n\n\n\n\n\n Vote \n Subsystem \n Runtime \n Comment \n\n\n\u00a0\n\u00a0\n\u00a0\n  Prechecks  \n\n\n +1 \n  test4tests  \n   0m  0s \n  The patch appears to include 3 new or modified test files.  \n\n\n\u00a0\n\u00a0\n\u00a0\n  master Compile Tests  \n\n\n +1 \n  compile  \n   2m 21s \n  master passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Patch Compile Tests  \n\n\n +1 \n  compile  \n   2m 10s \n  the patch passed  \n\n\n +1 \n  javac  \n   2m 10s \n  the patch passed  \n\n\n +1 \n  Release audit (RAT)  \n   2m 18s \n  the patch passed  \n\n\n +1 \n  Check forbidden APIs  \n   2m 10s \n  the patch passed  \n\n\n +1 \n  Validate source patterns  \n   2m 10s \n  the patch passed  \n\n\n +1 \n  Validate ref guide  \n   2m 10s \n  the patch passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Other Tests  \n\n\n -1 \n  unit  \n 179m 55s \n  core in the patch failed.  \n\n\n  \n   \n 189m 14s \n   \n\n\n\n\n\n\n\n\n\n Reason \n Tests \n\n\n Failed junit tests \n solr.cloud.autoscaling.IndexSizeTriggerTest \n\n\n\u00a0\n solr.cloud.api.collections.ShardSplitTest \n\n\n\u00a0\n solr.cloud.autoscaling.sim.TestGenericDistributedQueue \n\n\n\u00a0\n solr.handler.component.InfixSuggestersTest \n\n\n\n\n\n\n\n\n\n Subsystem \n Report/Notes \n\n\n JIRA Issue \n SOLR-12343 \n\n\n JIRA Patch URL \n https://issues.apache.org/jira/secure/attachment/12931845/SOLR-12343.patch \n\n\n Optional Tests \n  compile  javac  unit  ratsources  checkforbiddenapis  validatesourcepatterns  validaterefguide  \n\n\n uname \n Linux lucene1-us-west 3.13.0-88-generic #135-Ubuntu SMP Wed Jun 8 21:10:42 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux \n\n\n Build tool \n ant \n\n\n Personality \n /home/jenkins/jenkins-slave/workspace/PreCommit-SOLR-Build/sourcedir/dev-tools/test-patch/lucene-solr-yetus-personality.sh \n\n\n git revision \n master / d730c8b \n\n\n ant \n version: Apache Ant(TM) version 1.9.3 compiled on April 8 2014 \n\n\n Default Java \n 1.8.0_172 \n\n\n unit \n https://builds.apache.org/job/PreCommit-SOLR-Build/144/artifact/out/patch-unit-solr_core.txt \n\n\n  Test Results \n https://builds.apache.org/job/PreCommit-SOLR-Build/144/testReport/ \n\n\n modules \n C: solr/core solr/solr-ref-guide U: solr \n\n\n Console output \n https://builds.apache.org/job/PreCommit-SOLR-Build/144/console \n\n\n Powered by \n Apache Yetus 0.7.0   http://yetus.apache.org \n\n\n\n\n\n\nThis message was automatically generated.\n ",
            "author": "Lucene/Solr QA",
            "id": "comment-16546927"
        },
        {
            "date": "2018-07-19T17:30:43+0000",
            "content": "Commit a7fe950074a834edc070c265df1394181b268683 in lucene-solr's branch refs/heads/branch_7x from Chris Hostetter\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=a7fe950 ]\n\nSOLR-12343: Fixed a bug in JSON Faceting that could cause incorrect counts/stats when using non default sort options\n\nThis also adds a new configurable \"overrefine\" option\n\n(cherry picked from commit 3a5d4a25df310d2021fa947ea593cc9b3c93a386) ",
            "author": "ASF subversion and git services",
            "id": "comment-16549602"
        },
        {
            "date": "2018-07-19T17:30:45+0000",
            "content": "Commit 3a5d4a25df310d2021fa947ea593cc9b3c93a386 in lucene-solr's branch refs/heads/master from Chris Hostetter\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=3a5d4a2 ]\n\nSOLR-12343: Fixed a bug in JSON Faceting that could cause incorrect counts/stats when using non default sort options\n\nThis also adds a new configurable \"overrefine\" option ",
            "author": "ASF subversion and git services",
            "id": "comment-16549603"
        }
    ]
}