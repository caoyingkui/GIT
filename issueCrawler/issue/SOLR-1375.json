{
    "id": "SOLR-1375",
    "title": "BloomFilter on a field",
    "details": {
        "affect_versions": "1.4",
        "status": "Open",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "components": [
            "update"
        ],
        "type": "New Feature",
        "priority": "Minor",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "A bloom filter is a read only probabilistic set. Its useful\nfor verifying a key exists in a set, though it returns false\npositives. http://en.wikipedia.org/wiki/Bloom_filter \n\n\n\n\n\tThe use case is indexing in Hadoop and checking for duplicates\nagainst a Solr cluster (which when using term dictionary or a\nquery) is too slow and exceeds the time consumed for indexing.\nWhen a match is found, the host, segment, and term are returned.\nIf the same term is found on multiple servers, multiple results\nare returned by the distributed process. (We'll need to add in\nthe core name I just realized). \n\n\n\n\n\tWhen new segments are created, and commit is called, a new\nbloom filter is generated from a given field (default:id) by\niterating over the term dictionary values. There's a bloom\nfilter file per segment, which is managed on each Solr shard.\nWhen segments are merged away, their corresponding .blm files is\nalso removed. In a future version we'll have a central server\nfor the bloom filters so we're not abusing the thread pool of\nthe Solr proxy and the networking of the Solr cluster (this will\nbe done sooner than later after testing this version). I held\noff because the central server requires syncing the Solr\nservers' files (which is like reverse replication). \n\n\n\n\n\tThe patch uses the BloomFilter from Hadoop 0.20. I want to jar\nup only the necessary classes so we don't have a giant Hadoop\njar in lib.\nhttp://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/util/bloom/BloomFilter.html\n\n\n\n\n\tDistributed code is added and seems to work, I extended\nTestDistributedSearch to test over multiple HTTP servers. I\nchose this approach rather than the manual method used by (for\nexample) TermVectorComponent.testDistributed because I'm new to\nSolr's distributed search and wanted to learn how it works (the\nstages are confusing). Using this method, I didn't need to setup\nmultiple tomcat servers and manually execute tests.\n\n\n\n\n\tWe need more of the bloom filter options passable via\nsolrconfig\n\n\n\n\n\tI'll add more test cases",
    "attachments": {
        "SOLR-1375.patch": "https://issues.apache.org/jira/secure/attachment/12417206/SOLR-1375.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Jason Rutherglen",
            "id": "comment-12745752",
            "date": "2009-08-21T00:51:18+0000",
            "content": "Patch file "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12745774",
            "date": "2009-08-21T03:03:30+0000",
            "content": "At my previous job, we were attempting to add the same document up to 100x per day. We used an MD5 signature for the id and made a bitmap file to pre-check ids before attempting to add them. Because we did not created a bitmap file with 2^32 bits (512M) instead of (2^128) we also had a false positive problem which we were willing to put up with. (It was ok if we did not add all documents. )\n\nWe also had the advantage that different feeding machines pulled documents from different sources, and so machine A's set of repeated documents was separate from machine B's. Therefore, each could keep its own bitmap file and the files could be OR'd together periodically in background. \n\nI can't recommend what we did. If you like the Bloom Filter for this problem, that's great. \n\nThis project: FastBits IBIS claims to be super-smart about compressing bits in a disk archive. It might be a better technology than the Nutch Bloom Filter, but there is no Java and the C is a different license.\n\nI would counsel against making a central server ; Solr technologies should be distributed and localized (close to the Solr instance) as possible.\n\n\n\n "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12745800",
            "date": "2009-08-21T05:21:05+0000",
            "content": "Lance,\n\nThanks for the info. BloomFilters can be ORed together. Hadoop\nuses BFs for map side joins, which is similar to this use case.\n\nCentralizing will help when performing millions of id membership\ntests, though I'm going to benchmark first and see if the\ncurrent patch is good enough.\n\n-J "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12746104",
            "date": "2009-08-21T17:35:20+0000",
            "content": "\n\tThe Hadoop BloomFilter code is included in the patch\n\n\n "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12746371",
            "date": "2009-08-21T23:48:32+0000",
            "content": "\n\tBug fixes\n\n\n\n\n\tCore name included in response\n\n\n\n\n\tWiki is located at http://wiki.apache.org/solr/BloomIndexComponent\n\n "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-12746591",
            "date": "2009-08-23T11:49:12+0000",
            "content": "See here for a Java impl. of FastBits: http://code.google.com/p/compressedbitset/ .\n\nRe: BloomFilters - in BloomIndexComponent you seem to assume that when BloomKeySet.contains(key) returns true then a key exists in a set. This is not strictly speaking true. You can only be sure with 1.0 probability that a key does NOT exist in a set, for other key when the result is true you only have a (1.0 - eps) probability that the answer is correct, i.e. the BloomFilter will return a false positive result for non-existent keys, with (eps) probability. You should take this into account when writing client code. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12747733",
            "date": "2009-08-26T01:36:32+0000",
            "content": "Patch which allows shards to be set in defaults, and not cause an infinite loop if the shard is the same as the host. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12747734",
            "date": "2009-08-26T01:37:36+0000",
            "content": "The other attribute to add is the ability to set the hash function to use (i.e. Murmur) "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12748202",
            "date": "2009-08-26T23:25:35+0000",
            "content": "\n\tRemoves the need to set the shards.qt in solrconfig.xml.\n\n\n\n\n\tThe test case verifies the correct .blm files are created\n\n "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12838446",
            "date": "2010-02-25T17:57:30+0000",
            "content": "\nWhen new segments are created, and commit is called, a new\nbloom filter is generated from a given field (default:id) by\niterating over the term dictionary values. There's a bloom\nfilter file per segment, which is managed on each Solr shard.\nWhen segments are merged away, their corresponding .blm files is\nalso removed. \n\nDoesn't this hint at some of this stuff (haven't looked at the patch) really needing to live in Lucene index segment files merging land? "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-12846139",
            "date": "2010-03-16T21:41:42+0000",
            "content": "Heh, with the Lucene/Solr merge taking place now, my previous comment above makes even more sense.  What do you think? "
        },
        {
            "author": "Ted Dunning",
            "id": "comment-12846166",
            "date": "2010-03-16T22:08:00+0000",
            "content": "Sorry to comment late here, but when indexing in hadoop, it is really nice to avoid any central dependence.  It is also nice to focus the map-side join on items likely to match.  Thirdly, reduce side indexing is typically really important.\n\nThe conclusions from these three considerations vary by duplication rate.  Using reduce-side indexing gets rid of most of the problems of duplicate versions of a single document (with the same sort key) since the reducer can scan to see whether it has the final copy handy before adding a document to the index.\n\nThere remain problems where we have to not index documents that already exist in the index or to generate a deletion list that can assist in applying the index update.  The former problem is usually the more severe one because it isn't unusual for data sources to just include a full dump of all documents and assume that the consumer will figure out which are new or updated.  Here you would like to only index new and modified documents.  \n\nMy own preference for this is to avoid the complication of the map-side join using Bloom filters and simply export a very simple list of stub documents that correspond to the documents in the index.  These stub documents should be much smaller than the average document (unless you are indexing tweets) which makes passing around great masses of stub documents not such a problem since Hadoop shuffle, copy and sort times are all dominated by Lucene index times.  Passing stub documents allows the reducer to simply iterate through all documents with the same key keeping the latest version or any stub that is encountered.  For documents without a stub, normal indexing can be done with the slight addition exporting a list of stub documents for the new additions.\n\nThe same thing could be done with a map-side join, but the trade-off is that you now need considerably more memory for the mapper to store the entire bitmap in memory as opposed needing (somewhat) more time to pass the stub documents around.  How that trade-off plays out in the real world isn't clear.  My personal preference is to keep heap space small since the time cost is pretty minimal for me.\n\nThis problem also turns up in our PDF conversion pipeline where we keep check-sums of each PDF that has already been converted to viewable forms.   In that case, the ratio of real document size to stub size is even more preponderate. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12851637",
            "date": "2010-03-30T22:20:32+0000",
            "content": "Doesn't this hint at some of this stuff (haven't looked at the patch) really needing to live in Lucene index segment files merging land?\n\nAdding this to Lucene is out of the scope of what I require, however I don't have time unless it's going to be committed. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12872432",
            "date": "2010-05-27T22:05:08+0000",
            "content": "Bulk updating 240 Solr issues to set the Fix Version to \"next\" per the process outlined in this email...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E\n\nSelection criteria was \"Unresolved\" with a Fix Version of 1.5, 1.6, 3.1, or 4.0.  email notifications were suppressed.\n\nA unique token for finding these 240 issues in the future: hossversioncleanup20100527 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043763",
            "date": "2011-06-03T16:46:56+0000",
            "content": "Bulk move 3.2 -> 3.3 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13106341",
            "date": "2011-09-16T14:50:40+0000",
            "content": "3.4 -> 3.5 "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13234682",
            "date": "2012-03-21T18:08:52+0000",
            "content": "Bulk of fixVersion=3.6 -> fixVersion=4.0 for issues that have no assignee and have not been updated recently.\n\nemail notification suppressed to prevent mass-spam\npsuedo-unique token identifying these issues: hoss20120321nofix36 "
        },
        {
            "author": "Mark Harwood",
            "id": "comment-13275802",
            "date": "2012-05-15T15:14:03+0000",
            "content": "FYI, I came across this issue while exploring a different use case but adopting a similar implementation. I thought it might be useful to share some numbers:\n\nI found I could reduce primary key lookup costs significantly if each segment-level reader cached a Bloom filter to avoid searching segments that showed no trace of the key in the accompanying filter.\nOn a 1.5GB index with 20 segments 1,000 key lookups took 6 seconds with segment-level Bloom filters compared to 44 seconds if unfiltered.\nEach segment had a BloomFilter represented by an OpenBitset sized for 1 million bits and constructed from a TermEnum walk of terms which were then hashed with MurmurHash. This cost only ~2mb of RAM (20 segs x 1m bits). One advantage of using a fixed size bitset is that these could be happily ORed together if we could only hook into Lucene's segment merge process - this would avoid the cost of another TermEnum walk-and-hash run when opening newly merged segments. Of course if these Bitsets are serialized as \".blm\" files as part of core Lucene then merge operations would maintain these in the background and always ensure faster start up from cold.\n "
        },
        {
            "author": "Mark Harwood",
            "id": "comment-13279685",
            "date": "2012-05-20T07:23:03+0000",
            "content": "4069 is an implementation of Otis' suggestion - core Lucene maintaining a Bloom filter for terms in a segment. "
        },
        {
            "author": "Isaac Hebsh",
            "id": "comment-13660354",
            "date": "2013-05-17T05:43:16+0000",
            "content": "Hi, I know that this issue is not active for a long while, But I could not understand why. What work should be done in order to make the issue resolved? "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-13660759",
            "date": "2013-05-17T14:41:19+0000",
            "content": "Isaac Hebsh Lucene has bloomfilter functionality, but it isn't a complete postings format - it wraps another format.  To get it into Solr, a new class needs to be created.\n\nIt's on my todo list via SOLR-3950.  This issue is about including Hadoop's functionality in Solr, which I am pretty sure we won't be doing.  I think this needs to be closed as a duplicate of SOLR-3950, even though it is an older issue. "
        },
        {
            "author": "Isaac Hebsh",
            "id": "comment-13661483",
            "date": "2013-05-19T03:51:57+0000",
            "content": "Thanks Shawn Heisey. You are probably right. Maybe the wiki page, which links to here, should be marked properly... "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13717288",
            "date": "2013-07-23T18:47:45+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13971060",
            "date": "2014-04-16T12:57:02+0000",
            "content": "Move issue to Solr 4.9. "
        }
    ]
}