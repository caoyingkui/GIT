{
    "id": "SOLR-6126",
    "title": "MapReduce's GoLive script should support replicas",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [],
        "components": [
            "contrib - MapReduce"
        ],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Not A Problem"
    },
    "description": "The GoLive feature of the MapReduce contrib module is pretty cool.  But a comment in there indicates that it doesn't support replicas.  Every production SolrCloud setup I've seen has had replicas!\n\nI wonder what is needed to support this.  For GoLive to work, it assumes a shared file system (be it HDFS or whatever, like a SAN).  If perhaps the replicas in such a system read from the very same network disk location, then all we'd need to do is send a commit() to replicas; right?",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "wolfgang hoschek",
            "id": "comment-14015092",
            "date": "2014-06-01T21:21:54+0000",
            "content": "The comment in the code is a bit outdated. The code does actually support replicas. "
        },
        {
            "author": "David Smiley",
            "id": "comment-14015159",
            "date": "2014-06-02T03:07:01+0000",
            "content": "Thanks Wolfgang.  Can you comment how that is so?  (how does it work?) "
        },
        {
            "author": "wolfgang hoschek",
            "id": "comment-14015266",
            "date": "2014-06-02T08:49:13+0000",
            "content": "David Smiley It uses the --zk-host CLI options to fetch the solr URLs of each replica from zk - see extractShardUrls(). This info gets passed via the Options.shardUrls parameter into the go-live phase. In the go-live phase the segments of each shard are explicitly merged via a separate REST merge request per replica into the corresponding replica. The result is that each input segment is explicitly merged N times where N is the replication factor. Each such merge reads from HDFS and writes to HDFS.\n\n(BTW, I'll be unreachable on an transatlantic flight very soon) "
        },
        {
            "author": "David Smiley",
            "id": "comment-14015445",
            "date": "2014-06-02T15:02:42+0000",
            "content": "Oooh, ok.  I guess since in this mode of operation no documents are sent to any replica, all that needs to be done is to merge additional segments on each replica (leader or not is irrelevant).  Interestingly, this GoLive script looks fairly generic.  It's nothing more than a distributed addIndexes() call followed by a distributed commit.  There's nothing Hadoop/HDFS oriented about it, even if it's only used in such contexts. "
        }
    ]
}