{
    "id": "LUCENE-537",
    "title": "Refactor of spell check",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Won't Fix",
        "status": "Closed"
    },
    "description": "I use the same ngram index for multiple categories, but only want to spell check per category. The old implementation did not support this as it used docFreq as controller source.\n\nThe spell check returns suggestions with score and not just the suggested word.\n\nTokenFrequencyVector replace the IndexReader used for docFreq. \n\nLuceneTokenFrequencyVector wraps an IndexReader and works just as the old implementation.\n\nLuceneQueryDictionary creates an ngram dictionary based on a query and not the whole index.\n\nMultiTokenFrequencyVector treats a number of TokenFrequencyVector:s as one. I.e. for use when spell checking in multiple contexts.\n\nTokenFrequencyVectorMap is a HashMap facade. Comes with static factory to create the vector based on the the tokens in a specific field from a search.\n\nI use the TokenFrequencyVectorMap to build one vector per category and instanciate a MultiTokenFrequencyVector for each  user query. Could probably save a couple of clock ticks by buffering MultiVectors rather than creating new once all the time.\n\nAlso it seems as the ngram-code might not be thread safe. This also include the source in CVS. Have not succeded to prove it when when testing, only in the live environment. Each instance of Spellchecker only suggest once. And it takes quite some resources to create new instances of the spellchecker as it is designed today. Might get back on that subject.",
    "attachments": {
        "ngram_spellcheck_karl_v3.tar": "https://issues.apache.org/jira/secure/attachment/12325090/ngram_spellcheck_karl_v3.tar",
        "lucene_spellcheck.tar.gz": "https://issues.apache.org/jira/secure/attachment/12324801/lucene_spellcheck.tar.gz"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2006-03-31T17:20:38+0000",
            "content": "the code ",
            "author": "Karl Wettin",
            "id": "comment-12372645"
        },
        {
            "date": "2006-04-01T10:12:32+0000",
            "content": "This just came on Lucene-users and might explain what I thought was thread safty. I'll take a look at update my refactored code some time soon.\n\n\tFr\u00e5n: \t  jenaluan@yahoo.com\n\t\u00c4mne: \tSpellchecker bug (or feature?)\n\tDatum: \tl\u00f6rdag 1 apr 2006 00.20.08 GMT+02:00\n\tTill: \t  java-user@lucene.apache.org\n\tSvara till: \t  java-user@lucene.apache.org\n\nNot sure if this is the right place to report this issue:\n\n  The accuracy value, which can be set via setAccuracy(), is being modified in SpellChecker.java when a word is checked. As a result, the \"min\" may be pushed\n  very high and will not suggest anything for later requests.\n\n  One workaround would be to call setAccuracy() each time before a word is checked, I'm not sure if this is a feature (intended behavior) or a bug.\n  By the way, I'm using spellchecker 1.9.1 that comes with Lucene 1.9.1.\n\n  Thanks,\n\n  Xiaocheng ",
            "author": "Karl Wettin",
            "id": "comment-12372746"
        },
        {
            "date": "2006-04-09T03:21:06+0000",
            "content": "This update include same name changes, small optimizations of logic and a fix to the evil bug that rendered my and the SVN-version inusable, mentioned in earlier comment.\n\nIt might be worth to mention that I in my derivate of this code cache all suggestions in a Map. Really really really speeds things up, and does not consume that much RAM.\n\nAs a side note, I feel that the \"suggest only more frequent terms\" is not satifactory. The threashold should be a strategy, and I think there must be a better one than what is available.\n\nI  do however think this is the final version of my changes to the ngram spell checker. I Will start working on a new suggestion scheme based on A-stared markov chain that analyses the relation between multiple words, as this ngrammer really only is good at one word at the time. Perhaps it can be a base for the new one. Levenstein is more compelling to me. ",
            "author": "Karl Wettin",
            "id": "comment-12373746"
        },
        {
            "date": "2006-06-03T06:08:39+0000",
            "content": "This code is no good. Please ignore it. ",
            "author": "Karl Wettin",
            "id": "comment-12414534"
        },
        {
            "date": "2006-06-06T03:26:47+0000",
            "content": "You sure, Karl?  Want to just close the issue?  I haven't looked at your code. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12414846"
        },
        {
            "date": "2006-06-06T03:43:14+0000",
            "content": "Leave it open. But don't use the code.\n\nI'm in the progress of redoing the refactor. Hopefully the A*/Markov/Levenstein implementation I talked about earlier will be around sometime this summer. It will use a derivate of the code in this issue for single word suggestions. ",
            "author": "Karl Wettin",
            "id": "comment-12414851"
        },
        {
            "date": "2006-06-06T08:34:51+0000",
            "content": "Not sure if you saw LUCENE-285, but I thought I'd point it out.  I think that code was never committed. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12414889"
        },
        {
            "date": "2006-07-13T20:36:43+0000",
            "content": "Jira admins: you are welcome to kill this issue in reference to \nhttp://issues.apache.org/jira/browse/LUCENE-626 ",
            "author": "Karl Wettin",
            "id": "comment-12420873"
        },
        {
            "date": "2006-07-13T23:57:40+0000",
            "content": "Closing as requested by the original reporter, Karl Wettin. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12420896"
        }
    ]
}