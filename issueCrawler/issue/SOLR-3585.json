{
    "id": "SOLR-3585",
    "title": "processing updates in multiple threads",
    "details": {
        "affect_versions": "4.0-ALPHA,                                            6.0",
        "status": "Open",
        "fix_versions": [],
        "components": [
            "update"
        ],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "Hello,\n\nI'd like to contribute update processor which forks many threads which concurrently process the stream of commands. It may be beneficial for users who streams many docs through single request.",
    "attachments": {
        "multithreadupd.patch": "https://issues.apache.org/jira/secure/attachment/12534128/multithreadupd.patch",
        "report.tar.gz": "https://issues.apache.org/jira/secure/attachment/12535562/report.tar.gz",
        "SOLR-3585-oome-and-default-tests-chain.patch": "https://issues.apache.org/jira/secure/attachment/12651414/SOLR-3585-oome-and-default-tests-chain.patch",
        "SOLR-3585.patch": "https://issues.apache.org/jira/secure/attachment/12534982/SOLR-3585.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13406060",
            "date": "2012-07-03T21:22:47+0000",
            "content": "Folks, why are you so skeptic? Don't you need utilize your boxes to get indexing happen faster? \nThe also potential usage which I see is returning multithreading to DIH back by keeping the old core single threaded, then putting this multithread consumer behind and pushing into from improved datasources which will have buffer and separate threads. \n\nok. here is java doc from the recent patch. most update is correct and comprehensive halting implementation.\n\n/**\n\n\tForks incoming adds into multiple threads.\n\tInstead of regular chaining it chains on it's own with chain specified in\n\t\"backing.chain\" update parameter.\n\tsupports two solrconfig.xml parameters <ul>\n\t<li>\n\t\"bufferSize\" length of the buffer for add commands\n\t</li>\n\t<li>\"pipesNumber\" number of parallel threads for handling updates</li>\n\t</ul>\n\tit spawns own cached thread pool, and shutdown it on core closing.\n\tAddUpdate commands are queued for background execution by spawned processors.\n\tAll other commands awaits until all queued tasks are completed, and then are\n\thandled in the calling thread (request).\n\tif one of the update processors threads (aka pipes) catches exception, all other pipes\n\tare stopped synchronously, but not in the same moment, and the root cause is propagated\n\tinto request thread.\n\tfor queuing spin-delay pattern is used to prevent dead lock when buffer is full, and\n\tqueuing thread is blocked, but buffer can't be processed due to dead pipes.\n\t*/\n\n "
        },
        {
            "author": "Dmitry Kan",
            "id": "comment-13408234",
            "date": "2012-07-06T19:12:56+0000",
            "content": "Mikhail, this sounds interesting to me. Have you tested this already to prove that there is a gain in time using your approach? Also did you find some optimal parameters, like amount of threads, so that some sensible default values could be set? "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13408872",
            "date": "2012-07-08T08:26:48+0000",
            "content": "Dmitry,\n\nI've took 3M rows tsv from http://www.freebase.com/view/book/book_edition\n\nslightly updated solr 4.0 examples config to allow concurrency (see patch from report.tar.gz)\n\nin report.tar.gz you can see rate of  utilization in iostat outputs. \n\nsummary: \non MacBookPro core i5\n233/183/138 sec for 1/2/4 threads.\n3M records, index size is slightly less than 1 G \n\nsingle thread (solr as-is)\n    KB/t tps  MB/s     KB/t tps  MB/s  us sy id   1m   5m   15m\n 1024.00   6  5.99     0.00   0  0.00  36  2 62  2.62 2.16 2.10\n\n233756 millis\n\nJul 8, 2012 11:41:32 AM org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] webapp=/solr path=/update params=\n{commit=true&Zupdate.chain=threads&Zbacking.chain=logrun&stream.contentType=text/csv;charset%3Dutf-8&separator=%09&escape=\\&stream.file=/Users/mkhl/Downloads/book_edition.tsv}\n \n{add=[/m/08s9170, /m/08s7myj, /m/08s7nfb, /m/08s912p, /m/08s7nqy, /m/08s7rkg, /m/08s7vmn, /m/08s7yzd, /m/08s7zlw, /m/08s7zw3, ... (3401073 adds)],commit=}\n 0 233756 \n\ntwo threads:\n\n          disk0           disk2       cpu     load average\n    KB/t tps  MB/s     KB/t tps  MB/s  us sy id   1m   5m   15m\n  104.09 128 13.01     0.00   0  0.00  46  6 48  4.53 2.94 2.30\n\n183157 millis\n\nJul 8, 2012 11:25:58 AM org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] webapp=/solr path=/update params=\n{backing.chain=logrun&commit=true&stream.contentType=text/csv;charset%3Dutf-8&separator=%09&escape=\\&stream.file=/Users/mkhl/Downloads/book_edition.tsv&update.chain=threads} {add=[/m/08s7myj, /m/08s7nfb, /m/08s912p, /m/08s7rkg, /m/08s7zlw, /m/08s8127, /m/08s8wx0, /m/08s8_cg, /m/08s8cd2, /m/08s8wjv, ... (1658583 adds)]} 0 183157\nJul 8, 2012 11:25:58 AM org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] {add=[/m/08s9170, /m/08s7nqy, /m/08s7vmn, /m/08s7yzd, /m/08s7zw3, /m/08s82t3, /m/08s8dcy, /m/08s8hnz, /m/08s8j3x, /m/08s8mfs, ... (1742490 adds)]} 0 183157\n\nfour threads\n\n          disk0           disk2       cpu     load average\n    KB/t tps  MB/s     KB/t tps  MB/s  us sy id   1m   5m   15m\n   91.19 134 11.91     0.00   0  0.00  93  5  2  5.29 3.13 2.51\n\n138413 millis\n\nJul 8, 2012 11:53:27 AM org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] webapp=/solr path=/update params={backing.chain=logrun&commit=true&stream.contentType=text/csv;charset%3Dutf-8&separator=%09&escape=&stream.file=/Users/mkhl/Downloads/book_edition.tsv&update.chain=threads}\n \n{add=[/m/08s912p, /m/08s7yzd, /m/08s82t3, /m/08s8wjv, /m/08s8nx4, /m/08s8txn, /m/08z05sg, /m/08z05jm, /m/08yzqg0, /m/08yzkh2, ... (949997 adds)]}\n 0 138413\nJul 8, 2012 11:53:27 AM org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] \n{add=[/m/08s9170, /m/08s7nqy, /m/08s7vmn, /m/08s8127, /m/08s8wx0, /m/08s8dcy, /m/08s8mfs, /m/08s8v_7, /m/08z06nt, /m/08z05c5, ... (848935 adds)]}\n 0 138413\nJul 8, 2012 11:53:32 AM org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] \n{add=[/m/08s7nfb, /m/08s7zw3, /m/08s8_cg, /m/08s8j3x, /m/08s8szc, /m/08z09lt, /m/08yzf7_, /m/08yz2b1, /m/08yz24n, /m/08yyz1r, ... (777467 adds)]}\n 0 138413\nJul 8, 2012 11:53:32 AM org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] \n{add=[/m/08s7myj, /m/08s7rkg, /m/08s7zlw, /m/08s8cd2, /m/08s8hnz, /m/08s8mrk, /m/08s8v92, /m/08s8tz0, /m/08z097y, /m/08z047g, ... (824674 adds)]}\n 0 138413\n\nurl \n\nhttp://localhost:8983/solr/update?commit=true&separator=%09&escape=\\&update.chain=threads&backing.chain=logrun&stream.file=/Users/mkhl/Downloads/book_edition.tsv&stream.contentType=text/csv;charset=utf-8\n\nFYI 0.5G heap \n$ java -Xmx512M -Xms512M -jar start.jar "
        },
        {
            "author": "David Smiley",
            "id": "comment-13409046",
            "date": "2012-07-08T19:36:51+0000",
            "content": "+1 (disclaimer: I haven't looked at the patch)\n\nI've always thought Solr's indexing concurrency model to be strange. It doesn't have one, except to say that each connection is on its own thread and thus happens concurrently with any other connection(s).  Based on this model, SolrJ (and the DIH) has an unfair advantage (compared to non-SolrJ) via StreamingUpdateSolrServer which can load Solr via multiple connections, when this really should be a server-side setting since the ability of the server to handle X number of loading threads is not something the client could/should know.  Solr should have a configurable number of threads that pull Documents off of a common incoming queue.  This patch is a step in that direction although it's not quite equivalent to my idealized vision. "
        },
        {
            "author": "Dmitry Kan",
            "id": "comment-13409329",
            "date": "2012-07-09T11:07:45+0000",
            "content": "Mikhail, thanks for the stats. They look good to me! And prove that the patch should help increasing the indexing throughput. In about 2,5 weeks I should be able to try your patch and tell you the results on my hardware. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13409778",
            "date": "2012-07-09T19:38:01+0000",
            "content": "new attach. silly typo fixed. added log debug for diagnosing slow producer issue.  "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13409790",
            "date": "2012-07-09T19:55:53+0000",
            "content": "David,\n\nBeside of the subj. I can't get your point. Solr indeed (almost) already has a kind of \"a configurable number of threads that pull Documents off of a common incoming queue\". This \"queue\" is a servlet container's pool, and request buffer. But if you want to have different number of threads in indexing pool and let's say for searching you can try to deploy solr war twice bind to different connectors.\n\nIn this model a client have to be multithreaded, otherwise it will wait for every <add><doc>..</doc></add> response. if it sends many docs in a single huge sequence it looses concurrency again. \n\nI'm not getting what's unfair in StreamingUpdateSolrServer? It's just a low level REST impl detail or we can rephrase it as S(t)AX vs DOM style of coding/thinking. Any client even telnet and stdio is able to form a stream of <doc>..</doc> which will be consequently processed an the server side. \n\n\n "
        },
        {
            "author": "David Smiley",
            "id": "comment-13410065",
            "date": "2012-07-10T05:41:33+0000",
            "content": "I wasn't clear at all, sorry.  Let me try again:\n\nI like your patch because it puts indexing concurrency control into the server where it belongs.  Presently, to take advantage of a indexing to a beefy Solr server as fast as possible, it is basically up to the client to do it in multiple concurrent connections.  StreamingUpdateSolrServer can do this (and the DIH can too although it's deprecated) but if you're using something else (e.g. ruby) then you would have to code it yourself to load documents faster, which is challenging \u2013 concurrency is hard.  And furthermore, the client shouldn't be making assumptions as to how many indexing threads Solr can handle \u2013 that should be in Solr's configuration.\n\nI think it would be nicer if this concurrency behavior you coded as an UpdateRequestProcessor was instead something internal to Solr, somewhere in-between where an UpdateRequestHandler receives its data and hands it off to an URP chain.  This currently happens in ContentStreamHandlerBase.  Maybe UpdateRequestProcessorChain.createProcessor() would do it; I'm not sure \u2013 there isn't a perfectly clean place for it based on Solr's architecture, unfortunately. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13410148",
            "date": "2012-07-10T09:07:31+0000",
            "content": "I think it would be nicer if this ...\n\nDavid, please let me know what issues with current design you see?\n\nFrom my perspective UpdateRequestProcessorFactory's way advantage is pluggability, you can cofigure one or several of them, and then you are able to choose whether to use it or not, even in runtime e.g. cient can send small updates into regular chain, and choose parallel chain for huge bulks, or failover during spike periods etc. \n\nContentStreamHandlerBase.handleRequestBody() is had to be single threaded. Even it creates several processor chains (which are single threaded \"prototypes\"), it's hard to separate content stream onto substreams per several processing threads. Even it's possible how to make sure that this distribution is done evenly?  "
        },
        {
            "author": "Dan Kogan",
            "id": "comment-13420881",
            "date": "2012-07-23T19:10:54+0000",
            "content": "Michael, would this patch work with version 3.6? "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13420934",
            "date": "2012-07-23T20:19:53+0000",
            "content": "The code itself (solr/core/src/java/) should be much compatible with 3.x you can try to build is as drop-in - it's just yet another one update processor.\nMinor test framework amendments (solr/test-framework/src/java/) will be a little bit harder to apply. Should not be a big deal anyway. \nif someone from committers wants to apply it, I could port the patch. \nBtw, did you meet any difficulties in back porting?  "
        },
        {
            "author": "Dan Kogan",
            "id": "comment-13420988",
            "date": "2012-07-23T22:05:24+0000",
            "content": "I'm a part of the web team. We are using Solarium to index and query Solr. I'm very much interested to have a drop-in for 3.6 to take advantage of the multi-threaded indexation, which I would expect to be the default behavior. If I can't use your patch with 3.6 I would love to get some help to make it compatible. +10 Votes from me and my team. Dan "
        },
        {
            "author": "Dmitry Kan",
            "id": "comment-13445246",
            "date": "2012-08-30T19:59:00+0000",
            "content": "Summary:\n\n1/2/4/8 threads\n\nThere was a gain for 2 threads, after that increasing amount of threads didn't matter for the indexing speed (again, can be too little data, too slow machine vs server)\n\nURL:\n\nhttp://localhost:8983/solr/update?commit=true&separator=%09&escape=\\&update.chain=threads&backing.chain=logrun&stream.file=d:\\Projects\\information_retrieval\\solr\\apache-solr-4.0.0-BETA\\solr\\example\\data\\book_edition.tsv&stream.contentType=text/csv;charset=utf-8\n\nIntel(R) Core2 Duo CPU T6600 @ 2.20GHz\nRAM: 4 GB\nOS: Windows 7 64 bit\n\nPC was moderately used during the indexing (Internet surfing mostly)\n\nSolr started with:\njava -Xmx512M -Xms512M -jar start.jar\n\nStats and Log extract:\n\n-------------------\none thread\n-------------------\n\n565576 milliseconds (9.43 seconds)\nsize of data/index: 1.61 GB\n\n30.08.2012 22:34:10 org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] webapp=/solr path=/update params=\n{backing.chain=logrun&commi\nt=true&stream.contentType=text/csv;charset%3Dutf-8&separator=%09&escape=\\&stream\n.file=d:\\Projects\\information_retrieval\\solr\\apache-solr-4.0.0-BETA\\solr\\example\n\\data\\book_edition.tsv&update.chain=threads}\n \n{add=[/m/0g9nk5p, /m/0g9rf0q, /m/0g\nj6_r3, /m/0gj702y, /m/0gk99b7, /m/0g461_s, /m/0g4thbr, /m/0g4vp__, /m/0gkgw7x, /\nm/0gb390f, ... (3401498 adds)]}\n 0 565576\n\n-------------------\ntwo threads\n-------------------\n\n400085 milliseconds (6.67 seconds)\nsize of data/index: 916MB\n\n30.08.2012 22:09:16 org.apache.solr.core.SolrDeletionPolicy updateCommits\nINFO: newest commit = 1\n\n30.08.2012 22:15:56 org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] webapp=/solr path=/update params=\n{backing.chain=logrun&commit=true&stream.contentType=text/csv;charset%3Dutf-8&separator=%09&escape=\\&stream.file=d:\\Projects\\information_retrieval\\solr\\apache-solr-4.0.0-BETA\\solr\\example\\data\\book_edition.tsv&update.chain=threads}\n \n{add=[/m/0g9nk5p, /m/0gj6_r3, /m/0gkgw7x, /m/0g9_qhd, /m/0g9_r1t, /m/0g9jxyt, /m/0g4wdtq, /m/0d0s9y1, /m/0d9pb_v, /m/0d0tfz7, ... (1838414 adds)]}\n 0 400085\n30.08.2012 22:15:56 org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] webapp=/solr path=/update params=\n{backing.chain=logrun&commit=true&stream.contentType=text/csv;charset%3Dutf-8&separator=%09&escape=\\&stream.file=d:\\Projects\\information_retrieval\\solr\\apache-solr-4.0.0-BETA\\solr\\example\\data\\book_edition.tsv&update.chain=threads}\n \n{add=[/m/0g9rf0q, /m/0gj702y, /m/0gk99b7, /m/0g461_s, /m/0g4thbr, /m/0g4vp__, /m/0gb390f, /m/0gb34pf, /m/0h8fm59, /m/0g99vfk, ... (1563084 adds)]}\n 0 400085\n\n\n-------------------\nfour threads\n-------------------\n\n423969 milliseconds (7.07 seconds)\nsize of data/index: 915 MB\n\n30.08.2012 21:52:03 org.apache.solr.core.SolrDeletionPolicy updateCommits\n\nINFO: [collection1] webapp=/solr path=/update params=\n{backing.chain=logrun&commit=true&stream.contentType=text/csv;charset%3Dutf-8&separator=%09&escape=\\&stream.file=d:\\Projects\\information_retrieval\\solr\\apache-solr-4.0.0-BETA\\solr\\example\\data\\book_edition.tsv&update.chain=threads}\n \n{add=[/m/0g9nk5p, /m/0dgjnsn, /m/0d0s539, /m/0d0t8b3, /m/0d9n2sg, /m/0d0s18j, /m/07n7lbm, /m/07n7mh6, /m/07n7mq0, /m/07n7n_d, ... (844367 adds)]}\n 0 r\n30.08.2012 21:59:07 org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] \n{add=[/m/0gj702y, /m/0gk99b7, /m/0gkgw7x, /m/0gb390f, /m/0g9_qhd, /m/0h2ymt3, /m/0g4wdtq, /m/0d0s9y1, /m/0d0tfz7, /m/0d0tdf1, ... (815450 adds)]}\n 0 423969\n30.08.2012 21:59:07 org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] \n{add=[/m/0g9rf0q, /m/0g461_s, /m/0g4thbr, /m/0g4vp__, /m/0gb34pf, /m/0h8fm59, /m/0g99vfk, /m/0g9_r1t, /m/0g9jxyt, /m/0ghc2b5, ... (836534 adds)]}\n 0 423969\n30.08.2012 21:59:07 org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] webapp=/solr path=/update params=\n{backing.chain=logrun&commit=true&stream.contentType=text/csv;charset%3Dutf-8&separator=%09&escape=\\&stream.file=d:\\Projects\\information_retrieval\\solr\\apache-solr-4.0.0-BETA\\solr\\example\\data\\book_edition.tsv&update.chain=threads}\n \n{add=[/m/0gj6_r3, /m/0d0sfq_, /m/0d9mhx1, /m/07tc6lf, /m/07tc75v, /m/07tc7jq, /m/07tc8kz, /m/07tc8wr, /m/07tc_cn, /m/07tc_fl, ... (905147 adds)]}\n 0 423969\n\n-------------------\neight threads\n-------------------\n\n431710 milliseconds (7.20 seconds)\nsize of data/index: 1.00 GB\n\n\n30.08.2012 22:47:43 org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] webapp=/solr path=/update params=\n{backing.chain=logrun&commit=true&stream.contentType=text/csv;charset%3Dutf-8&separator=%09&escape=\\&stream\n.file=d:\\Projects\\information_retrieval\\solr\\apache-solr-4.0.0-BETA\\solr\\example\\data\\book_edition.tsv&update.chain=threads}\n \n{add=[/m/0gk99b7, /m/0d0vb6s, /m/07t8mw8, /m/07t8pvt, /m/07t8ygz, /m/07t8yr6, /m/07t904r, /m/07t90cq, /m/07t91l8, /m/07t91yl, ... (369913 adds)]}\n 0 431710\n30.08.2012 22:47:43 org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] \n{add=[/m/0g9rf0q, /m/0dgh_yx, /m/0d0t2jm, /m/0d0tkmp, /m/0d0tdjk, /m/0d0s4t_, /m/0d9mcxz, /m/0d0rzm4, /m/0d9mjyb, /m/0d9n806, ... (480781 adds)]}\n 0 431710\n30.08.2012 22:47:43 org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] \n{add=[/m/0g4thbr, /m/07tct8c, /m/07tcwx7, /m/07tcxx_, /m/07tczs9, /m/07td4wf, /m/07td5z1, /m/07tp81q, /m/07tpf2f, /m/07tphh7, ... (331760 adds)]}\n 0 431710\n30.08.2012 22:47:43 org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] \n{add=[/m/0gj6_r3, /m/0dghv3t, /m/0dghm0z, /m/0dghgk7, /m/0d0s99z, /m/0d9mkv1, /m/0dgj61b, /m/0d9mkns, /m/0dgj3st, /m/0d0v7p7, ... (368899 adds)]}\n 0 431710\n30.08.2012 22:47:43 org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] \n{add=[/m/0g9nk5p, /m/0dj5yc2, /m/0dghh9b, /m/0dghhyt, /m/0dghkcc, /m/0dgjcyy, /m/0d0sfq_, /m/0d9nbgx, /m/0d0s51w, /m/0d9mby3, ... (374343 adds)]}\n 0 431710\n30.08.2012 22:47:43 org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] \n{add=[/m/0g4vp__, /m/0g99vfk, /m/0h2ymt3, /m/0dghty2, /m/0d9pb_v, /m/0d0st88, /m/0dghh0h, /m/0dghh_g, /m/0d9nmdk, /m/0dghspk, ... (504140 adds)]}\n 0 431710\n30.08.2012 22:47:43 org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] \n{add=[/m/0gj702y, /m/0gkgw7x, /m/0gb390f, /m/0gb34pf, /m/0h8fm59, /m/0g9_qhd, /m/0g9_r1t, /m/0g9jxyt, /m/0ghc2b5, /m/0g4wdtq, ... (450096 adds)]}\n 0 431710\n30.08.2012 22:47:45 org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: [collection1] \n{add=[/m/0g461_s, /m/07tptf3, /m/07tzgzv, /m/07v1bqt, /m/07v1d9p, /m/07v1fbh, /m/07v1jgm, /m/07v663l, /m/07v6fbt, /m/07v6gt5, ... (521566 adds)]}\n 0 431710 "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13445363",
            "date": "2012-08-30T22:35:47+0000",
            "content": "Dmitry,\n\ntwo threads speedup limit is reasonable to me - you have two ways chip ark.intel.com/products/37255/Intel-Core2-Duo-Processor-T6600-(2M-Cache-2_20-GHz-800-MHz-FSB) .\nKey factor for enabling threads is cpu-idle time i.e. no gain is expected after 100% cpu usage. \nAlso, local streaming is a rare usage, I believe. Most time it's kind of remote producer-consumer eg solrj,  where proper level of utilization is challenging.  "
        },
        {
            "author": "Dmitry Kan",
            "id": "comment-13445749",
            "date": "2012-08-31T07:56:00+0000",
            "content": "Mikhail,\n\nTrue & thanks for link. In any case, the test proves that there is a gain, even for a non-server \"horse\". I might find a way to run this on a server + (possibly) play with solrj. In our use case, local streaming is used for larger batch (re-)processing and solrj for relatively tiny updates. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13445775",
            "date": "2012-08-31T08:42:00+0000",
            "content": "These seem to be small records. Try indexing large PDF files with the ExtractingRequestHandler- these spend a much longer time in the analysis phase and have more data to copy around. Try them with and without storing the field: stored fields have to be copied during merges. "
        },
        {
            "author": "Ted Sullivan",
            "id": "comment-14027908",
            "date": "2014-06-11T15:47:00+0000",
            "content": "Curious as to why this issue has not been picked up/scheduled for a release. I am working with a client that is using DataImportHandler to connect to Oracle and because of performance issues with the latter (vis Clobs) needs every speed improvement that they can get. They have 5 parallel DIH handlers and that helps. They recently installed this patch and report that they get a better than 2X improvement (~50 minutes for a load compared to ~20 minutes with the patch). Is there a reason why this submission has stalled? Given the removal of multithreading in DIH, this seems to be a very good replacement option, and since it is an UpdateRequestProcessor, it is of general use. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14028255",
            "date": "2014-06-11T19:20:41+0000",
            "content": "Ted Sullivan thanks for the feedback. \nBut how as I said I disappointed with this approach after I wrote it. I suppose threads are client side concern (at least Solr is j2ee war). \nI prefer to see DIH replatformed and supports concurrency and as other clients uses ConcurrentUpdateSolrServer and/or CloudSolrServer, rather than this hacks.\n\nAny other opinions? "
        },
        {
            "author": "David Smiley",
            "id": "comment-14028315",
            "date": "2014-06-11T19:58:01+0000",
            "content": "Mikhail Khludnev I disagree!  I think it's crazy that to load Solr faster you have to make it a client-side concern to know how many threads are a suitable number to transmit data.\n\n\tUnless you are using ConcurrentUpdateSolrServer then you have to write this concurrent code yourself as well as anybody else out there that is in the same boat; yet doing it Solr side does it once for everyone.\n\tI argue it isn't the client's concern to know the ideal number of threads to load Solr fastest.  Clients don't tell Solr how big Lucene's indexing RAM buffer should be.  Should it know how many threads are on the box and how concurrent its IO hardware is?\n\tPutting it Solr-side affords the possibility that configuring the threads might be changed dynamically.\n\n "
        },
        {
            "author": "Dmitry Kan",
            "id": "comment-14028325",
            "date": "2014-06-11T20:12:32+0000",
            "content": "I would agree with David Smiley. Every good api (and to some extent Solr is an api in the client view) takes advantage of multi-threading by itself. In this case a client can be as thin as possible and not care about threads. And if client has enough idle cpus, sure, it could post in parallel. For example, we run solr on pretty beefy machines with lots of cpu cores and most of the time those are idling.\n\nSome of the latest findings of ours with soft commits and high posting pressure show, that posting may sometimes even fail and failed docs re-posting fixes the issue. "
        },
        {
            "author": "Ted Sullivan",
            "id": "comment-14028363",
            "date": "2014-06-11T20:41:17+0000",
            "content": "Good discussion Dmitry and David - I agree with your comments. As I said in my initial comment, I came at this thread while looking for ways to speed up the DataImportHandler. Even though this is not directly related to the client-server issue that you are discussing, it does factor in because with DIH, Solr is pulling data from an outside source. With a SolrJ push, you do have the opportunity to load Solr by taking advantage of the multi-threaded capability built into the REST/RequestHandler. However, with DIH you don't have this option (except by running multiple DIH handlers which seems hacky to me) - so in this case the Solr side is the only place where you can add multithreading. As I said, my client is doing both (multiple DIH handlers and using the patch) and they both help.\n\nTo Mikhail's comment that with DIH it would be better to refactored to support concurrency - yes I agree - that would be good, but it was tried before, was found to be problematic and was removed in Solr 4.0.  I see that you contributed a patch to fix multithreading in 3.6 Mikhail - so I would not want to challenge your expertise here   My take on it is that a DIH process can get fairly complicated with nested event handlers and possibly transformers doing strange things so maybe there were just too many dependencies that are not or can't be guaranteed to be thread-safe. Whatever the reason, I find your idea to move this problem downstream to the update processor chain to be a good, clean solution. It makes threading configurable and pluggable which both sound like good things to me.  "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14028498",
            "date": "2014-06-11T22:27:53+0000",
            "content": "David Smiley \nI think it's crazy that to load Solr faster you have to make it a client-side concern to know how many threads\nI saw quite often that a producer is a slowpoke (single thread SELECT), and fast consumer doesn't help at all. That's why I prefer to burden client for threads. Look at SolrCloud if client doesn't care about performance, it can send update to any node, but if needs performance, it have to concern about cluster topology, and send to nodes concurrently;\nUnless you are using ConcurrentUpdateSolrServer then \nin this case I'd rather say, this guy doesn't bother about performance, or neither cure help here. \nShould it know how many threads are on the box and how concurrent its IO hardware is?\nok. here are two separate concerns: throttling and performance. the former one is addressed by j2ee container thread pool so far. However, I can agree that it might make sense to introduce separate thread pool for handling updates, but only for limiting number of threads to avoid unnecessary index segmentation due to concurrent flush here was the false speculation. Thanks to Adrien Grand for resolving it. Also, don't you think that this should be handled even deeper, somewhere around Lucene IndexWriter? Coming back to performance, I'm indented to blame client-side as slowpoke.\nthreads might be changed dynamically.\nagain. throttling interleaves with j2ee container functionality\n\nOk Guys, if somebody need it, let it be. One note, now the code here does the same as ConcurrentUpdateSolrServer, it's not good at all. I'd prefer to extract core thread code from CUSS, and reuse it in this update processor with in-process sink for sure. "
        },
        {
            "author": "David Smiley",
            "id": "comment-14028712",
            "date": "2014-06-12T01:44:15+0000",
            "content": "I saw quite often that a producer is a slowpoke (single thread SELECT), and fast consumer doesn't help at all. That's why I prefer to burden client for threads. Look at SolrCloud if client doesn't care about performance, it can send update to any node, but if needs performance, it have to concern about cluster topology, and send to nodes concurrently;\n\nSure; the bottleneck is sometimes the source, in which case it will benefit from multiple threads to get the data to increase performance.  But that is independent of the number of ideal threads to populate the target (Solr in this case).  It's quite possible that a given application might find that to transfer the data with maximum throughput, it needs to get the data from the source in X threads and populate Solr in Y threads, where X and Y are not equal and not 1. I'm just saying the half of this that populates Solr should have the pool on the Solr side.  I'm saying nothing of the data-source consumption end.\n\nAnd your comments about JEE (i.e. servlet-container) concurrent request limits are not applicable because it can't be targeted to just updates, which is where the real constraint is.  Not to mention the JEE container option is going away for v5 any way.\n\nI can agree that it might make sense to introduce separate thread pool for handling updates, but only for limiting number of threads to avoid unnecessary index segmentation due to concurrent flush\n\nAnother reason for the Solr-side pool.  A customer of mine sorta hit this because they failed to understand that Solr couldn't handle 100 pipes shoving data into it.  Solr shouldn't fall over in such a case; it should use as many threads as it's configured to use in its config file.  I propose it default to a mode in which consumer-thread == indexing thread (as is now) but limited to 2... and have other options of course such as a dedicated thread-pool.\n\nOne note, now the code here does the same as ConcurrentUpdateSolrServer, it's not good at all. I'd prefer to extract core thread code from CUSS, and reuse it in this update processor with in-process sink for sure.\n\nMakes sense. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14031102",
            "date": "2014-06-13T20:19:07+0000",
            "content": "One note, now the code here does the same as ConcurrentUpdateSolrServer, it's not good at all. I'd prefer to extract core thread code from CUSS, and reuse it in this update processor with in-process sink for sure.\n\nMark Miller what's your feeling about the proposal above? "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14031567",
            "date": "2014-06-14T13:06:12+0000",
            "content": "David Smiley don't you worry that while updates hangs in queue they are not persisted in updateLog? "
        },
        {
            "author": "David Smiley",
            "id": "comment-14032023",
            "date": "2014-06-15T21:20:05+0000",
            "content": "That's an excellent point.  In fact, anyone using ConcurrentUpdateSolrServer (CUSS) doesn't, in effect, get the benefit of the updateLog either.\n\nI think Solr should try to retain the same semantics one gets without using CUSS:  Once you close out the HTTP message you send to Solr (which on one extreme might be one document or an another a virtually endless stream of documents), that a successful HTTP response semantically means whatever you did is \"safe\" \u2013 in the updateLog at least.  If there is no updateLog then there is no guarantee (there never was before this proposal either) and it'll return as soon as it gets into the indexed ramBuffer (beyond text analysis).  At least then if there's a schema related problem with Solr accepting the document then you'll know.  It would be nice if the response could include any errors on a per-document basis (by id); but that'd be a bonus.\n\nI doubt you'd be able to easily re-use the CUSS logic in this implementation.  I wish I had time to partake \u2013 sounds like fun  "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14032171",
            "date": "2014-06-16T07:08:26+0000",
            "content": "I think Solr should try to retain the same semantics one gets without using CUSS...\n\nit's a well reasoning! \n\nI doubt you'd be able to easily re-use the CUSS logic in this implementation.\nwhy.. it's just a few clicks in Refactoring menu.\nCheck the scratch attached.\n\n\tI introduced oddly-named ThreadBurster, where CUSS concurrency logic has been moved;\n\tI moved CUSS and ThreadedUpdateProcessorFactory onto ThreadBurster. It seems work.\nout of scope so far:\n\ta few CUSS descendants, which is more strict in error handling, but they should be able to live on ThreadBurster as well as TUP\n\tnaming is completely mad, feel free to contribute a better taste ones\n\tit's TUP configuration/properties left from the earlier patch, I wish to revamp it to make it seems more like CUSS\n\tpassing the caller logger to ThreadBurster, might not be really reasonable.\n\n\n\nYour feedback is quite appreciated!    "
        },
        {
            "author": "David Smiley",
            "id": "comment-14033245",
            "date": "2014-06-17T00:30:32+0000",
            "content": "I reviewed your patch.  Overall I really like it!\n\n\n\tPlease don\u2019t include in the patch changes to files that appear to do have no effect like  java import modifications as seen in TestSolrJ. I suspect you did some refactoring and reverted and some harmless deltas were left.\n\tCUSS: have only one constructor that does the constructing; the other constructors can call the one.\n\tI think instead of having a Callback interface and createCallback() method, the user of ThreadBurster should simply subclass ThreadBurster and implement the handle(), finish() and handleError() methods. It's already abstract anyway.  After doing so, you could omit passing any arguments to handle() except for 'E'.  Then consider making more fields on ThreadBurster private, adding some convenience methods for adding & polling the queue (maybe).\n\tThis wasn't introduced by you but runnerLock is weird: it's a Lock but not used for locking, it's used in blockUnitlFinished() (which is a typo!) as a way to wait until there are no runners.  Furthermore, blockUnitlFinished appears to wait longer than ideal since \"pollQueueTime\" time must pass. At a minimum, it would be more clear if it were using CountDownLatch.  Further improvements would be nice in a separate issue, to include javadocs on blockUntilFinished.\n\tThreadedUpdateProcesor.processRollback() shouldn\u2019t call checkTrouble()\n\tI wonder if there\u2019s a nicer way to hook this in without a new \u201cbacking.chain\u201d param?  Like if you could simply list this in a series of other URPs.  Not a serious problem but it's a little awkward.\n\tI like your tests, particularly the use of DelayProcessorFactory.\n\tI think preClose is the wrong place to shut down the Executor; look at its docs. If you do it in postClose; it should be safe.\n\tI don\u2019t care for some of the variable names.  \u201cpipes\u201d should be something like \u201cnumPipes\u201d or \u201cpipeCount\u201d etc.  When I see a variable named as you have, I think it actually is the set of pipes, versus the number of them.  Likewise \u201cbuffer\u201d should be \u201cbufferSize\u201d or something.  Note that CUSS uses \u201cthreadCount\u201d and \u201cqueueSize\u201d which are more clear to at least me.  I also saw \u201cprm\u201d vs. \u201cparams\u201d though sometimes I prefer names like \u201cdefaultParams\u201d etc. if there is ambiguity as to which params.\n\tIt\u2019s a shame that commit, delete, etc. (except add) have to do a blockUntilFinished which effects other concurrent streams the client (potentially multiple asynchronous update clients) may be sending.  It\u2019s not a big deal but not ideal.  The separate buffers (per concurrent load) minimize that, at least.  But people need to be aware there is a buffer-per-concurrent-connection.  At this point (progress not perfection) it\u2019s probably not worth concerning ourselves with optimizing this to a single buffer and a smarter blockUntilFinished.\n\tIt\u2019s worth documenting that DistributedUpdateProcessorFactory ought to be before this URP.\n\tMasking all exceptions as IOException isn\u2019t right.  SolrException should pass through.  FYI Guava\u2019s Throwables may be handy here.\n\n "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14035542",
            "date": "2014-06-18T09:56:48+0000",
            "content": "CUSS: have only one constructor \ndone. shutdownExecutor left in CUSS only that removes constructor mess. \nthe user of ThreadBurster should simply subclass ThreadBurster and implement the handle(), finish() and handleError() methods.\nthe motivation of introducing Callback is to have single thread delegate. It's not necessary for CUSS (where stateless http client is used). But update processors shouldn't be shared across threads. if we go the way you propose we need to keep update processor chains in ThreadLocal, which I'm not really happy about.\nmaking more fields on ThreadBurster private, adding some convenience methods for adding & polling \ndone. it seems better now\nThis wasn't introduced by you but runnerLock is weird\nLet me keep unaware of the legacy. Let's fight them later.\nThreadedUpdateProcesor.processRollback() shouldn\u2019t call checkTrouble() \nlogically - yes, formally - might not so straightforward. here is the plain scenario: correct update w/o commit; then send request with wrong update and rollback, thus exception should prevent rollback, however if we don't checkTrouble() before processRollback() it rollbacks the first correct update request, and it's not what is expected. Of course, this case is subtle, and artificial. I'm ready to accept your approach, if you confirm. I reordered rows there so far.  \nI wonder if there\u2019s a nicer way to hook this in without a new \u201cbacking.chain\u201d param?\nIndeed! I just realized how to do that, it's little bit fragile, but it looks better\nI like your tests, particularly the use of DelayProcessorFactory.\nThanks. I have who to learn from. \nIf you do it in postClose; it should be safe.\nok. thanks. now it halts running threads by shutdownNow(), I suppose it's correct.\nI don\u2019t care for some of the variable names.. \nReasonable. Sorry for \"blah\" coding. fixed and applied CUSS names\nhave to do a blockUntilFinished which effects other concurrent streams the client...need to be aware there is a buffer-per-concurrent-connection...\neven if it's possible to solve I don't think it's a point\nIt\u2019s worth documenting that DistributedUpdateProcessorFactory ought to be before\nadded javadoc\nMasking all exceptions as IOException..\nThis is what I don't really get. Where it is? Isn't it another one legacy\n\nother improvements\n\n\trenamed to ConcurrentUpdateProcessorFactory and props to mimics CUSS\n\tI keep passing logger to ThreadBurster to keep logs informative, are you happy with it?\n\tdo you like the name ThreadBurster?\n\tI reviewed CUSS descendants SafeConcurrentUpdateSolrServer and StreamingSolrServers they should work fine.\n\n "
        },
        {
            "author": "David Smiley",
            "id": "comment-14035910",
            "date": "2014-06-18T16:37:31+0000",
            "content": "I like the new name to align with CUSS.  I like that you pass through the logger.  I like the name ThreadBurster.  I like your new processRollback logic.\n\nYou're right to not mess with CUSS logic (e.g. choice of locks); addressing any of that now is a bad idea.\n\nRE IOExceptions... see handleError() which does the wrapping in CUPF.  Furthermore, consider having an OOME pass straight through.\n\nOne thing not obvious to me (that should be clarified in comments/docs) is that the \"next\" URP is actually used in addition to the separately constructed backing chain.  Only added docs go through threads; everything else falls through to the next URP.  CUSS works that way too, it appears, but it's not obvious.\n\nI like how you figured out how to create the backing chain of remaining URPs!  Although it's unfortunate this requires the re-creation of them but no biggie.  Also, I think I see a bug.  UPRChain.createProcessor doesn't create any URPs prior to DistributingUpdateProcessorFactory (which is an interface) when receiving a distributed update.  You could fix this by inserting NoOpDistributingUpdateProcessorFactory at the head of the factories when creating the chain.\n\nTesting: Since its semantics are aligned with default behavior, it would be nice to try and (randomly) use this URP in Solr's tests. If it proves to be too awkward; you should instead write a distributed test.  To augment the existing testing, you could add to the default test solrconfig a chain that includes this URP along with the other URPs one normally gets by default.  Then, assuming there is some spot that a test submits an update request that has access to the parameters, give a ~10% chance to adding update.chain=concurrent if update.chain isn't present.  Hopefully this isn't gloriously hacky even if it it's \"just\" for tests.  Even if this specific aspect doesn't get committed, it'd be great to at least do locally to find more bugs.\n\nthe motivation of introducing Callback is to have single thread delegate. It's not necessary for CUSS (where stateless http client is used). But update processors shouldn't be shared across threads. if we go the way you propose we need to keep update processor chains in ThreadLocal, which I'm not really happy about.\n\nI understand that Update processors shouldn't be shared across threads (yet another Solr undocumented semantic; ugh), but It's still not apparent to me the way you coded it is necessary (and I agree on avoiding ThreadLocal).  I think I'll try and change it and see if the test breaks.  It's also unclear why you clone the AddUpdateCommand in processAdd().  There should be a so-called \"happens-before\" relationship between when the object is placed on the queue and after a thread takes it to run it.\n\nThanks for your continuing effort! I'll try and get it committed next week, assuming the remaining points are addressed.  I'll probably take a stab at improving the javadoc & comment wording prior to doing so. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14037321",
            "date": "2014-06-19T13:12:40+0000",
            "content": "incremental patch SOLR-3585-oome-and-default-tests-chain.patch \nfull patch SOLR-3585.patch is also attached.\n\nRE IOExceptions... see handleError() which does ...\nI reordered lines in ThreadBurster.Runner.run(). now it passes OOME into Callback.handleError(). CUPF.handleError() now accepts more throables.\n\nthat should be clarified in comments/docs) is that the \"next\" \nI refreshed CUPF javadoc, but rely on your sense of beauty on commit anyway \n\nAlso, I think I see a bug. UPRChain.createProcessor doesn't create any URPs prior to \nit's either not a problem, or I didn't get you, I don't think I need to ...\n... fix this by inserting NoOpDistributingUpdateProcessorFactory at the head of the factories when creating the chain.\n\nDUPF is inserted when chain is read from xml, it isn't inserted when I request \"tail-chain\" remaining after CUPF. \n\nTesting: Since its semantics are aligned with default behavior,..\nI added new chain named concurrent and mark it default (fortunately it's absent so far) in solrconfig.xml and solrconfig-tlog.xml. it should impact many tests. at least FullSolrCloudDistribCmdsTest and CloudSolrServerTest touches CUPF and stay green.\n\nis it enough in scope of this ticket? is it non-invasive enough? or you propose something particular.\n\nAs a followup (separate) ticket I can propose to add/and check that CURP is between DUPF and RUPF in UpdateRequestProcessorChain.init() and insert in default implicit chain SolrCore.loadUpdateProcessorChains().\n\nbut It's still not apparent to me the way you coded it is necessary (and I agree on avoiding ThreadLocal). I think I'll try and change it and see if the test breaks. \nI bet LogUpdateProcessor punish you if you push it concurrently enough. see SOLR-2694, SOLR-2804, SOLR-3484, SOLR-3314.\n\nIt's also unclear why you clone the AddUpdateCommand in processAdd(). \nno secret here. \nlook how XMLLoader.processUpdate() mutates same AddUpdateCommand instance (on every <doc> tag). It's curious that CUSS doesn't allow such way.\n\n\n\n\n\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14037379",
            "date": "2014-06-19T14:37:21+0000",
            "content": "I have not looked at the patch yet, but if we wanted to offer this, why wouldn't we just make the number of threads used by cuss configurable up from 1 in streamingsolrservers? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14037382",
            "date": "2014-06-19T14:42:07+0000",
            "content": "Perhaps for non cloud support? Doesn't seem worthwhile to add a lot of complexity for that to me though.  "
        },
        {
            "author": "Ted Sullivan",
            "id": "comment-14037469",
            "date": "2014-06-19T16:16:44+0000",
            "content": "[~hakeber] - another usage for this patch is to improve performance of DataImportHandler-initiated updates. The original patch has shown to improve indexing performance by at least a factor of 2-3. There is no opportunity to add client-side threads in this situation and since multi-threading support for DIH was removed, this provides an alternative way to increase throughput with the DIH. "
        },
        {
            "author": "David Smiley",
            "id": "comment-14038413",
            "date": "2014-06-20T04:06:11+0000",
            "content": "Mark Miller I'm thinking it would just be yet another URP that if you want to use you have to add it; not by default \u2013 let the masses try it first.  And you ask why not make StreamingSolrServer's use of CUSS configurable?  That's a good point.  To me it's a matter of principle; let the receiving end (the server) dictate the concurrency it can handle, not the sender.  And I like the idea of one peer-wise connection instead of 'N' as well. "
        }
    ]
}