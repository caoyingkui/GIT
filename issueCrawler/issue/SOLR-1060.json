{
    "id": "SOLR-1060",
    "title": "a new DIH EnityProcessor allowing text file lists of files to be indexed",
    "details": {
        "affect_versions": "1.4",
        "status": "Closed",
        "fix_versions": [
            "1.4"
        ],
        "components": [
            "contrib - DataImportHandler"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "I have finished a new DIH EntityProcessor. It is designed around the idea that whatever demon is used to maintain your content store it is likely to drop a report or log file explaining what has changed within your content store. I wish to use this report file to control the indexing of the new or changed content and the removal of old content. The report files, perhaps from un-tar or un-zip, are likely to reference jpegs and directory stubs which need to be ignored. I assumed a file based content repository but this should be expanded to handle URI's as well\n\nI feel that the current FileListEntityProcessor is poorly named. It should be called the dirWalkEntityProcessor or dirCrawlEntityProcessor or such. And this new EntityProcessor should have the name FileListEntityProcessor. However what is done is done. I then came up with manifestEnityProcessor which I thought suited, manifest files are all over the content sets I deal with and the dictionary definition seemed close enough (\"ships manifest\"). However how about ChangeListEntityProcessor\n\n       <entity name=\"jc\"\n               processor=\"ManifestEntityProcessor\"\n               baseDir=\"/Volumes/Techmore/ts/aaa/schema/data\"\n               rootEntity=\"false\"\n               dataSource=\"null\"\n\n               allowRegex=\"^.*\\.xml$\"\n               blockRegex=\"usc2009\"\n               manifestFileName=\"/Volumes/ts/man-find.txt\"\n               docAddRegex=\".*\"\n               >\n\n\n\nThe new entity fields are as follows.\n\n   manifestFileName is the required location of the manifest file. If this value is relative, it assumed to be relative to baseDir.\n\n   allowRegex is an optional attribute that if present discards any line which does not match the regExp\n\n   blockRegex is an optional attribute that is applied after any allowRegex and discards any line which matches the regExp\n\n   docAddRegex is a required regex to identify lines which when matched should cause docs to be added to the index. As well as matching the line it should also return the portion of the line which contains the filepath as group(1)\n\n   docDeleteRegex is an optional value of a regex to identify documents which when matched should be deleted from the index. As well as matching the line it should also return the portion of the line which contains the filepath as group(1) *PLANNED*",
    "attachments": {
        "regex-fix.patch": "https://issues.apache.org/jira/secure/attachment/12402658/regex-fix.patch",
        "SOLR-1060.patch": "https://issues.apache.org/jira/secure/attachment/12401948/SOLR-1060.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Noble Paul",
            "id": "comment-12680753",
            "date": "2009-03-11T04:34:18+0000",
            "content": "few comments. \n\n\n\tChangeListEntityProcessor is preferred over ManifestEntityProcessor. manifestFileName can be changed to fileName\n\t'allowRegex' and blockRegex can be renamed to something else .how about acceptLineRegex and omitLineRegex\n\tdataSource='null' is not required from 1.4 onwards.\n\n "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12680945",
            "date": "2009-03-11T17:49:41+0000",
            "content": "This is by no means the finished article. It has no test case and only deals with a list file on disk, further the list file can only refer to files that are also on disk. None of the URI stuff is there yet, I guess I ran out of brain power and could not find anything suitable to copy and rework. Having a baseDir feature that seamlessly applies to both www and disk cases is a pain.\n\nHowever I have implemented all the suggestions and given it a good testing, works for me. "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12681616",
            "date": "2009-03-13T05:32:20+0000",
            "content": "I am working on minor changes to the (now inappropriatly named) HttpDataSource to allow it to access file:// based resources more cleanly. It should IMHO have been called URIDataSource.\n\nI am also rewriting ChangeListEntityProcessor to allow it to cooperate with a child entity which is using either HttpDataSource or FileDataSource. This cooperation will not be automatic, the person setting up the data-config.xml will need to take account of which datasource the child is using when configuring the parent ChangeListEntityProcessor. And of course IMHO ... FileDataSource should actually have been called diskDataSource or filesysDataSource "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12681620",
            "date": "2009-03-13T05:49:52+0000",
            "content": "I haven't had a chance to look at the patch yet. But some general comments/questions:\n\n\tLooks like the ChangeListEntityProcessor is trying to do two things \u2013 read a file line by line and process according to add/delete instructions\n\tHow about we have a LineEntityProcessor (we can change the name) which just reads text files line by line and a ChangeListEntityProcessor which extends LineEntityProcessor and detects/processes changes according to the regex? This can enable someone to write a CSVTransformer on top of LineEntityProcessor for instance.\n\tIs it necessary to change the semantics of HttpDataSource? It was meant to read an HTTP response and one should use FileDataSource for reading files. Why mix the functionalities at all?\n\tWhat is the cooperation that needs to be built with ChangeListEntityProcessor and HTTP/File DataSource?\n\n "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12681629",
            "date": "2009-03-13T06:46:31+0000",
            "content": "\n\tCorrect.\n\tGreat idea I am fine with that; but the next patch I submit, will still do both. I will work on seperating them once it is working.\n\tSort of; it either needs a change or we need a new URLDataSource. FileDataSource is fine for reading files from filesystems but it does not handle the file:// syntax. HttpDataSource already half supports reading from file:// locations (if the URL does not contain the protocol string http:// then baseUrl can be used to add any protocol!). But after considering the input from Otis Gospodnetic, Paul Libbrecht and thinking for a bit; It seemed reasonable that the location of the ChangeList could be specified using file:/// of http:// syntax as well as using plain old filesystem syntax. Likewise the lines within the ChangeList could use either URL or filesystem syntax, this in turn also applied to the baseDir.\n\tI have been playing about with ChangeListEntityProcessor feeding rows to XPathEntityProcessor. ChangeListEntityProcessor currently supports use of either FileDataSource or HttpDataSource by XPathEntityProcessor. However depending which DataSource is specified within XPathEntityProcessor then ChangeListEntityProcessor needs to be configured to return rows with the approriatle syntax. (i Think!)\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12681630",
            "date": "2009-03-13T07:26:33+0000",
            "content": "My concern is that we have two data sources whose names identify their respective functionality. With this change FileDataSource becomes redundant and HttpDataSource does not give the impression that it can read files too. I assume that everyone will be generating the changeset using their own sweet tools/programs. Therefore it is a simple task for the changeset generator to generate http/file separately or mark them differently. Then one can use different root entities.\n\nThe ChangeListEntityProcessor should not care whether the changelist contains a filepath or url \u2013 they are all strings to it which should be added to a map and passed along. If it is a delete set, then it should set $deleteDocByQuery or $deleteDocById along with $skipDoc and return. The ChangeListEntityProcessor should not try to collaborate with any other entity. It does not need to know about them just as current EntityProcessor do not know about each other.\n\nFor example:\n\n<dataConfig>\n  <dataSource name=\"file\" type=\"FileDataSource\"/>\n  <document>\n    <entity name=\"changeSet\" processor=\"ChangeSetEntityProcessor\"\n            rootEntity=\"false\"\n            allowRegex=\"^.*\\.xml$\"\n            blockRegex=\"usc2009\"\n            manifestFileName=\"/Volumes/ts/man-find.txt\"\n            docAddRegex=\".*\">\n      <entity name=\"indexer\" processor=\"XPathEntityProcessor\"\n              dataSource=\"file\"\n              forEach=\"/root/a\"\n              url=\"${changeSet.filename}\">\n\n      </entity>\n    </entity>\n  </document>\n</dataConfig>\n\n\n\nHere the ${changeSet.filename} is just a normal key/val in the changeSet entity's row map. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12681637",
            "date": "2009-03-13T08:18:50+0000",
            "content": "How about we have a LineEntityProcessor (we can change the name) which just reads text files line by line and a ChangeListEntityProcessor which extends LineEntityProcessor\n\nDoes the ChangeListEntityProcessor have to be an EntityProcessor? EntityProcessors are heavyweight components. They are complex and ugly. What stops it from being a Transformer .  A Transformer is simple and it can do almost everything an EntityProcessor does .  The only thing a Transformer does not do is that it does not usually generate it's own data from a DataSource \n\nThis is a just a thought.\n\n. FileDataSource is fine for reading files from filesystems but it does not handle the file:// syntax. HttpDataSource already half supports reading from file:// locations (if the URL does not contain the protocol string http:// then baseUrl can be used to add any protocol!). \n\nIs it a possibility that the kind of data source is not known in advance? Then the user will be able to configure an appropriate data source for that entity? "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12681649",
            "date": "2009-03-13T09:17:27+0000",
            "content": "Oh dear, this is getting complicated!\n\n\"My concern is that we have two data sources whose names identify their respective functionality. With this change FileDataSource becomes redundant and HttpDataSource does not give the impression that it can read files too. I assume that everyone will be generating the changeset using their own sweet tools/programs. Therefore it is a simple task for the changeset generator to generate http/file separately or mark them differently. Then one can use different root entities.\"\n\nHmmmm, no I am not sure about this. \n\n\tFirstly I agree \"FileDataSource becomes redundant and HttpDataSource .. can read files\"; bit of a mess really. Ideally I think we need a new dataSource that can read from either a FileSystem or a URI.\n\tI the poor old content indexer am often presented with the manifest fait accompli. It comes as part of the update kit, I have little or no control of its format. I would have to organise some middle-ware to sort its format if we restrict DIH. Which would be a pity since the proposed changes should allow solr to directly handle every case I have seen, and I suspect it is well over 80% of the usecase.\n\tEven if the lines read from the changelist are simple filepaths, how we access those files will depend on other factors. They could be on a local or remote machine. The lines read from the file will not indicate this. As Nobel implies we may not know this ahead of time, we need to be able to pass parameters into the system which supplies that information.\n\n\n\n<thinking out loud>\n\n\tWe need to be able to read lines describing changes we may wish to make to our index from a file:// or a restful web service or URL.\n\tThe lines read will need analysed for two purposes. a) to identify the portion of the line we are interested in b) to reformat that portion such that it can be passed to the child entity which will in turn pass it to a dataSource.\n\tWe do not know which dataSource the child entity may be using which make the reformating stage 2b) a bit more tricky. Hence the required cooperation.\n\n\n\n1) and 2a) could be done by changeListEntityProcessor (As Noble says we need an EntityProcessor because it is generating data... without even a datasource!)\n2b) could be done by a transformer, information will need to be available to the transformer to allow it deal with local or remote access. \n3)?????\n</thinking out loud>\n\nFor the moment I was intending to build 1)2a)2b) into the ChangeListEntityProcessor, it does not appear to be bad. Once done perhaps we can look again at a need to lift 2b) into a separate EntityProcessor or Transformer. "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12681653",
            "date": "2009-03-13T09:25:35+0000",
            "content": "Oh and by the way. The \"Configuration of HttpDataSource\" section of the DIH wiki, where it describes the entity URL attribute,  says...\n\nurl (required) : The url used to invoke the REST API. (Can be templatized). if the data souce is file this must be the file location\n\nSo I guess the cat is already out of the bag about HttpDataSource reading file:// "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12681654",
            "date": "2009-03-13T09:34:05+0000",
            "content": "Fergus . it is not a bad idea to have a URIDataSource if that simplifies the problem. The UriDataSource can wrap FiledataSource/HttpDataSource (if that is convenient) .  "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12681665",
            "date": "2009-03-13T10:07:13+0000",
            "content": "A couple of lines changed to HttpDataSource and we have a UriDataSource!\n\n\n-      if (query.startsWith(\"http:\")) {\n-        url = new URL(query);\n-      } else {\n-        url = new URL(baseUrl + query);\n-      }\n+      if ( URIMETHOD.matcher(query).find()) url = new URL(query);\n+      else url = new URL(baseUrl + query);\n...\n+  private static final Pattern URIMETHOD = Pattern.compile(\"\\\\w{3,}:\");\n\n\n\nAs I said at the start. HttpDataSource already half supports other protocols, its just that it assumes http: when deciding whether to prepend the baseUrl. A minor bug probably. Is it worth wraping? Also; given the cat is already out of the bag, shouldnt we just tweak HttpDataSource? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12681682",
            "date": "2009-03-13T11:36:30+0000",
            "content": "OK. Let us do this\n\nmake the change to HttpDataSource and then create a new DataSource URIDataSource extends HttpDataSource.\n So users can use both in this release .and let us deprecate HttpDataSource  in favor of URIDataSource in the future "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12682320",
            "date": "2009-03-16T14:04:12+0000",
            "content": "I have rewritten and tested the ChangeListEntityProcessor such that it supports URL's. This allows the list of changes to fetched from a local file, a simple URL or any restful type web service. The list of changes must appear as one change per line read, the line can contain an absolute file:/// or http:// pathnames or it can be a relative pathname. The entity attribute baseLocation specifies a prefix to be used with relative pathnames. baseLocation must be a valid URL; file:/// or http:// for the moment. The entity attributes are as follows\n\n\n\tFileName is the required URL location of the change list. If this value is relative, it assumed to be relative to baseLocation.</li>\n\tacceptLineRegex is an optional attribute that if present discards any line read from the change list which does not match the regExp.</li>\n\tomitLineRegex is an optional attribute that is applied after any acceptLineRegex and discards any line read from the change list which matches the regExp.</li>\n\tdocAddRegex is an optional regex to identify lines which when matched should cause docs to be added to the index. As well as matching the line it should also return the portion of the line which is to be treated as the pathname, as group(1). If not specified the whole line is assumed to be valid pathname.</li>\n\tdocDeleteRegex is an optional value of a regex to identify documents which when matched should be deleted from the index. As well as matching the line it should also return the portion of the line which contains the filepath as group(1) PLANNED WORK see SOLR-1059</li>\n\tbaseLocation is a required prefix added to fileName or lines read from the change list which do not appear to be absolute http:// or file:/// URL's</li>\n\n\n\nHere is a sample of the way I used it:-\n\n       <entity name=\"jc\"\n               processor=\"ChangeListEntityProcessor\"\n               acceptLineRegex=\"^.*\\.xml$\"\n               omitLineRegex=\"usc2009\"\n               fileName=\"file:///Volumes/ts/man-findlsurl.txt\"\n               rootEntity=\"false\"\n               dataSource=\"null\"\n               baseLocation=\"http://localhost/ford/\"\n               docAddRegex=\"\\s+([^ ]*)$\"\n               >\n\n\n\nThis entity returns a row containing a single \"fileAbsolutePath\" field for each pathname accepted from the changelist. If the docDeleteRegex was matched then another fields will also be returned $deleteDocId=?? and  $deleteDocQuery=??. What do I need to set these values to?\n\nI have also created a URLDataSource, it seems to work. However \"an expert\" had better review what I have done; I am still very inexperienced re Java best practice. On that topic; why did we not rename the existing httpDataSource to URLDataSource and then make httpDataSource a wrapper for URLDataSource?\n\nTesting with my sample of 40000 documents reveals no noticible slowdown compared with FileListEntiryProcessor. "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12682325",
            "date": "2009-03-16T14:20:19+0000",
            "content": "Ooooups. Another version of patch, but formated how ASF like things formated. I forgot to mention that I also changed httpDataSource and fileDataSource as follows\n\n\n\tharmonised the LOG messages for the individual files processed. Making them equivilent and only outputing at DEBG level.\n\n\n\n\n\thttpDataSource was altered to making it a generic URL reader and of course URLDataSource it merly a wrapper around it.\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12682329",
            "date": "2009-03-16T15:01:19+0000",
            "content": "This is great!\n\n$deleteDocId=?? and $deleteDocQuery=??. What do I need to set these values to?\nSet them to a boolean as a string.\n\nwhy did we not rename the existing httpDataSource to URLDataSource and then make httpDataSource a wrapper for URLDataSource?\n\n+1. Let's do this. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12682382",
            "date": "2009-03-16T17:34:46+0000",
            "content": "Hi fergus ,\ndid you consider splitting the ChangeListEntityProcessor into two\n\n\tLineEntityprocessor and\n\tChangeListEntityprocessor extends LineEntityprocessor\n\n\n\nTomorrow some one is definitely going to ask for a Line EntityProcessor. \n\nwhy did we not rename the existing httpDataSource to URLDataSource\n\n+1 "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12682441",
            "date": "2009-03-16T20:06:56+0000",
            "content": "Yes, briefly, I did and could not see how it could be done nicely; however it is quite possible I am misunderstanding things..\n\nTo recap, the idea was to split \"ChangeListEntityProcessor\" into two halves. The first half would deal with reading lines from a file:/// or http:// locations with features to allow lines to be omitted or accepted. The second half would focus on analyzing the line turning it into add/delete instructions and identifying the portion of the lines which was to be operated on. Is this correct?\n\nIf my understanding is correct. Then if baseLocation was allowed to be empty and \"docAddRegex\" and \"docDeleteRegex\" are not supplied then the line from the changelist could be returned by the entity exactly as read from the file. Further; if \"acceptLineRegex\" and \"omitLineRegex\" are also undefined then the whole file is returned to the next entity. Would that make it the same as part one?\n\nI had looked at removing all my code for doing the second half described above, replacing it with a transformers. I guess as long as the templatetransformer can assign to the fields $deleteDocId and $deleteDocQuery then it is do-able. Is the following valid? In the following I always assign to $deleteDocQuery but make $deleteDocId true/false to control actual deletion.\n\n\n<entity name=\"jc\"\n           processor=\"ChangeListEntityProcessor\"\n           fileName=\"file:///Volumes/ts/man-findlsurl.txt\"\n           rootEntity=\"false\"\n           baseLocation=\"http://localhost/ford/\"\n           transformer=\"TemplateTransformer,RegexTransformer\">\n           >\n<field column=\"id\"                regex=\".*(-- find jucy bit--).*\" replaceWith=\"$1\" \\>\n<field column=\"$deleteDocQuery\"   regex=\".*(-- find jucy bit--).*\" replaceWith=\"$1\"      sourceColName=\"fileAbsolutePath\"/>\n<field column=\"$deleteDocId\"      template=\"false\"   regex=\".*(-- find add/del bit--).*\" replaceWith=\"true\" sourceColName=\"fileAbsolutePath\"/>\n\n\n\n? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12682551",
            "date": "2009-03-17T04:09:23+0000",
            "content": "To recap, the idea was to split \"ChangeListEntityProcessor\" into two halves. The first half would ....\n\nright\n\nThe second part may become a bit easier with SOLR-1061\n "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12682636",
            "date": "2009-03-17T11:20:16+0000",
            "content": "Ok. I will try and see if I can make do as outlined above.\n\nHowever I still dont think I have got the use of $deleteDocQuery $deleteDocId understood properly. Shalin says they are a boolean and a string. Yet Solr-1059 implies they are both strings.\n\nNow following the information from Solr-1059 I think I need to set $deleteDocQuery to a valid Solr query. If so my entity needs to know the solr field name to use and what it contains; I do not have control of these items. I either need to mandate to the user that for deletes to function the field, say \"fileAbsolutePath\", has to be defined in the schema.xml and it must be equal to the filename returned by the entity. I am trying to test this today... I am not sure the transformers provide the flexibility I need to do everything. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12682644",
            "date": "2009-03-17T11:39:58+0000",
            "content": "Shalin says they are a boolean and a string. Yet Solr-1059 implies they are both strings.\n\nSorry for the confusion. I meant that the value should be \"true\" or \"false\" (boolean values as a string type)\n\nI don't know what I was thinking when I wrote that. I'm still stuck in the boolean world of $skipDoc etc. Please disregard my mumblings. Yes, they are both strings. "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12682802",
            "date": "2009-03-17T21:01:38+0000",
            "content": "Hi,\n\nI have the following snippet from my data-config.xml. This is after removing all code from ChangeListEntityProcessor which deals with finding the juicy part of the line. However I get tracebacks when ever I start tomcat saying that my schema.xml has no mention of $deleteQuery. Do I have to declare a field $deleteQuery in my schema.xml; if so it is rather ugly!\n\nI was wondering if perhaps field/column names beginning with '$' could be considered magic, or some fashion local to data-config.xml and skip whatever check is causing tomcat to bomb. With the new power of transformers I could see a need for \"temporary variables\" within data-config.xml\n\n\nMar 17, 2009 8:54:31 PM org.apache.solr.handler.dataimport.DataImporter loadDataConfig\nINFO: Data Configuration loaded successfully\nMar 17, 2009 8:54:31 PM org.apache.solr.handler.dataimport.DataImportHandler inform\nSEVERE: Exception while loading DataImporter\norg.apache.solr.handler.dataimport.DataImportHandlerException: There are errors in the Schema\nThe field :$deleteQuery present in DataConfig does not have a counterpart in Solr Schema\n\n\tat org.apache.solr.handler.dataimport.DataImporter.<init>(DataImporter.java:109)\n\tat org.apache.solr.handler.dataimport.DataImportHandler.inform(DataImportHandler.java:96)\n\tat org.apache.solr.core.SolrResourceLoader.inform(SolrResourceLoader.java:388)\n\tat org.apache.solr.core.SolrCore.<init>(SolrCore.java:571)\n\tat org.apache.solr.core.CoreContainer$Initializer.initialize(CoreContainer.java:122)\n\tat org.apache.solr.servlet.SolrDispatchFilter.init(SolrDispatchFilter.java:69)\n\tat org.apache.catalina.core.ApplicationFilterConfig.getFilter(ApplicationFilterConfig.java:223)\n\n\n\n\n\n<dataConfig>\n  <dataSource name=\"myFILEreader\" type=\"FileDataSource\"/>    \n  <dataSource name=\"myURIreader\"  type=\"URLDataSource\" />    \n    <document>\n      <entity name=\"jc\"\n               processor=\"ChangeListEntityProcessor\"\n               acceptLineRegex=\"^.*\\.xml$\"\n               omitLineRegex=\"usc2009\"\n               fileName=\"file:///Volumes/spare/ts/man-findlsurl.txt\"\n\t       rootEntity=\"false\"\n\t       dataSource=\"null\"\n               baseLocation=\"file:///Volumes/spare/ts/ford\"\n\t       transformer=\"RegexTransformer\"\n\t       >\n\t<!-- the following columns are only defined if the regex matches -->\n\t<field column=\"fileAbsolutePath\"    regex=\"\\s+([^ ]*)$\" replaceWith=\"${jc.baseLocation}/$1\"  sourceColName=\"rawLine\"/>\n\t<field column=\"$deleteQuery\"        regex=\"^DELETE\\s+\"  replaceWith=\"${jc.fileAbsolutePath}\" sourceColName=\"rawLine\"/> \t       \n\n\t<entity name=\"x\"\n\t\tdataSource=\"myurireader\"\n\t\tprocessor=\"XPathEntityProcessor\"\n\t\turl=\"${jc.fileAbsolutePath}\"\n\t\trootEntity=\"true\"\n\t\tflatten=\"true\"\n\t\tstream=\"false\"\n\t\tforEach=\"/record | /record/mediaBlock\"\n\t\ttransformer=\"DateFormatTransformer,TemplateTransformer,RegexTransformer\">\n\n<field column=\"fileAbsolutePath\"                 template=\"${jc.fileAbsolutePath}\" />\n<field column=\"fileWebPath\"                      template=\"${jc.fileAbsolutePath}\" regex=\"${dataimporter.request.fordinstalldir}(.*)\" replaceWith=\"/ford$1\"/>\n<field column=\"fileWebDir\"                       regex=\"(.*)/.*\" replaceWith=\"$1\" sourceColName=\"fileWebPath\"/>\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12682897",
            "date": "2009-03-18T03:33:45+0000",
            "content": "However I get tracebacks when ever I start tomcat saying that my schema.xml has no mention of $deleteQuery. Do I have to declare a field $deleteQuery in my schema.xml; if so it is rather ugly!\n\nEntityProcessorBase should remove such variables after using them. Glancing at the code, I think we will see the same problem if we put $skipDoc as \"false\". We need to update the SOLR-1059 patch. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12682903",
            "date": "2009-03-18T03:55:24+0000",
            "content": "shalin we need to remove the check done in DataImporter for variables being present  "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12683014",
            "date": "2009-03-18T13:19:06+0000",
            "content": "Thanks for the changes to SOLR-1059. I am now attempting to test document deletion; it is not going too well!\n\n\nMar 18, 2009 1:12:48 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/apache-solr-1.4-dev path=/dataimport params={command=full-import&clean=false&entity=single-delete&single=/Volumes/spare/ts/schema/data/news/fdw2008/jn71796.xml} status=0 QTime=1 \nMar 18, 2009 1:12:48 PM org.apache.solr.handler.dataimport.SolrWriter readIndexerProperties\nINFO: Read dataimport.properties\nMar 18, 2009 1:12:48 PM org.apache.solr.handler.dataimport.DataImporter doFullImport\nINFO: Starting Full Import\nMar 18, 2009 1:12:48 PM org.apache.solr.handler.dataimport.SolrWriter readIndexerProperties\nINFO: Read dataimport.properties\nMar 18, 2009 1:12:48 PM org.apache.solr.handler.dataimport.SolrWriter upload\nWARNING: Error creating document : SolrInputDocument[{}]\norg.apache.solr.common.SolrException: Document [null] missing required field: vdkvgwkey\n\tat org.apache.solr.update.DocumentBuilder.toDocument(DocumentBuilder.java:292)\n\tat org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:59)\n\tat org.apache.solr.handler.dataimport.SolrWriter.upload(SolrWriter.java:67)\n\tat org.apache.solr.handler.dataimport.DataImportHandler$1.upload(DataImportHandler.java:274)\n\tat org.apache.solr.handler.dataimport.DocBuilder.buildDocument(DocBuilder.java:373)\n\tat org.apache.solr.handler.dataimport.DocBuilder.doFullDump(DocBuilder.java:221)\n\tat org.apache.solr.handler.dataimport.DocBuilder.execute(DocBuilder.java:163)\n\tat org.apache.solr.handler.dataimport.DataImporter.doFullImport(DataImporter.java:309)\n\tat org.apache.solr.handler.dataimport.DataImporter.runCmd(DataImporter.java:367)\n\tat org.apache.solr.handler.dataimport.DataImporter$1.run(DataImporter.java:348)\nMar 18, 2009 1:12:48 PM org.apache.solr.handler.dataimport.SolrWriter upload\nWARNING: Error creating document : SolrInputDocument[{}]\norg.apache.solr.common.SolrException: Document [null] missing required field: vdkvgwkey\n\tat org.apache.solr.update.DocumentBuilder.toDocument(DocumentBuilder.java:292)\n\tat org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:59)\n\tat org.apache.solr.handler.dataimport.SolrWriter.upload(SolrWriter.java:67)\n\tat org.apache.solr.handler.dataimport.DataImportHandler$1.upload(DataImportHandler.java:274)\n\tat org.apache.solr.handler.dataimport.DocBuilder.buildDocument(DocBuilder.java:373)\n\tat org.apache.solr.handler.dataimport.DocBuilder.doFullDump(DocBuilder.java:221)\n\tat org.apache.solr.handler.dataimport.DocBuilder.execute(DocBuilder.java:163)\n\tat org.apache.solr.handler.dataimport.DataImporter.doFullImport(DataImporter.java:309)\n\tat org.apache.solr.handler.dataimport.DataImporter.runCmd(DataImporter.java:367)\n\tat org.apache.solr.handler.dataimport.DataImporter$1.run(DataImporter.java:348)\nMar 18, 2009 1:12:48 PM org.apache.solr.handler.dataimport.SolrWriter upload\nWARNING: Error creating document : SolrInputDocument[{}]\norg.apache.solr.common.SolrException: Document [null] missing required field: vdkvgwkey\n\tat org.apache.solr.update.DocumentBuilder.toDocument(DocumentBuilder.java:292)\n\tat org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:59)\n\tat org.apache.solr.handler.dataimport.SolrWriter.upload(SolrWriter.java:67)\n\tat org.apache.solr.handler.dataimport.DataImportHandler$1.upload(DataImportHandler.java:274)\n\tat org.apache.solr.handler.dataimport.DocBuilder.buildDocument(DocBuilder.java:373)\n\tat org.apache.solr.handler.dataimport.DocBuilder.doFullDump(DocBuilder.java:221)\n\tat org.apache.solr.handler.dataimport.DocBuilder.execute(DocBuilder.java:163)\n\tat org.apache.solr.handler.dataimport.DataImporter.doFullImport(DataImporter.java:309)\n\tat org.apache.solr.handler.dataimport.DataImporter.runCmd(DataImporter.java:367)\n\tat org.apache.solr.handler.dataimport.DataImporter$1.run(DataImporter.java:348)\nMar 18, 2009 1:12:48 PM org.apache.solr.handler.dataimport.DocBuilder execute\nINFO: Time taken = 0:0:0.41\n\n\n\n\nmy entity is as follows:-\n\n     <entity name=\"single-delete\"\n\t\t dataSource=\"myFILEreader\"\n\t\t processor=\"XPathEntityProcessor\"\n\t\t url=\"${dataimporter.request.single}\"\n\t\t rootEntity=\"true\"\n\t\t flatten=\"true\"\n\t\t stream=\"false\"\n\t\t forEach=\"/record | /record/mediaBlock\"\n\t\t transformer=\"RegexTransformer\">\n\n      <!-- the following columns are only defined if the regex matches -->\n      <field column=\"fileAbsolutePath\"    template=\"${dataimporter.request.single}\" /> \n      <field column=\"$deleteQuery\"        template=\"fileAbsolutePath:${dataimporter.request.single}\" /> \t       \n      <field column=\"vdkvgwkey\"           template=\"${dataimporter.request.single}\" /> \n      </entity>\n\n\n\nAnd repeating what was shown in the traceback my test was:-\n\nget 'http://localhost:8080/apache-solr-1.4-dev/dataimport?command=full-import&entity=single-delete&clean=false&single=/Volumes/spare/ts/ford/schema/data/news/fdw2008/jn71796.xml' "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12683301",
            "date": "2009-03-19T03:54:10+0000",
            "content": "hi Fergus, \nThe issue here is that when there is a $deleteQuery /$deleteId is present the document is still tried to be inserted . One way is to do a $skipDoc in the same row. or we can add a check to DIH to avoid inserting doc if the uniqueKey is absent . \n\n "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12683346",
            "date": "2009-03-19T06:58:46+0000",
            "content": "I do like SOLR complaining if the ID is missing or not uniqueue. Or I guess I need to set $skipDoc=\"true\" (is that syntax correct?). However I think that $skipDoc should be invoked internally whenever $deleteQuery /$deleteId is present.\n\nFor the moment I will try $skipDoc=\"true\" using an extra transform. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12683362",
            "date": "2009-03-19T08:37:01+0000",
            "content": "I do like SOLR complaining if the ID is missing\n\nThe only problem with this solution is it may not work if <uniqueKey> is not specifie\n\nI guess I need to set $skipDoc=\"true\" (is that syntax correct?).\nyes.  "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12683435",
            "date": "2009-03-19T11:28:52+0000",
            "content": "Did you notice that besides <uniqueKey> not being specified, that the whole document was empty!\n\n\nMar 18, 2009 1:12:48 PM org.apache.solr.handler.dataimport.SolrWriter upload\nWARNING: Error creating document : SolrInputDocument[{}]\n\n "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12683532",
            "date": "2009-03-19T17:26:51+0000",
            "content": "I have applied the latest version of SOLR-1059 and I just cannot get delete to work!\n\n\n     <entity name=\"single-delete\"\n\t\t dataSource=\"myURIreader\"\n\t\t processor=\"XPathEntityProcessor\"\n\t\t url=\"${dataimporter.request.single}\"\n\t\t rootEntity=\"true\"\n\t\t flatten=\"true\"\n\t\t stream=\"false\"\n\t\t forEach=\"/record | /record/mediaBlock\"\n\t\t transformer=\"TemplateTransformer\">\n\n      <field column=\"$skipDoc\"            template=\"true\" /> \n      <field column=\"fileAbsolutePath\"    template=\"${dataimporter.request.single}\" /> \n      <field column=\"$deleteDocByQuery\"   template=\"fileAbsolutePath:${dataimporter.request.single}\" /> \t       \n      <field column=\"vdkvgwkey\"           template=\"${dataimporter.request.single}\" /> \n      </entity>\n\n\n\nAnd here is a section from the log file showing that after an attempt to wipe the file, it is still there; it was not removed.\n\n\nMar 19, 2009 5:24:52 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/apache-solr-1.4-dev path=/select params={wt=xml&q=fileAbsolutePath:file\\:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml} hits=3 status=0 QTime=10 \n\n\n\nMar 19, 2009 5:25:04 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/apache-solr-1.4-dev path=/dataimport params={command=full-import&clean=false&entity=single-delete&commit=true&single=file\\:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml} status=0 QTime=0 \nMar 19, 2009 5:25:04 PM org.apache.solr.handler.dataimport.SolrWriter readIndexerProperties\nINFO: Read dataimport.properties\nMar 19, 2009 5:25:04 PM org.apache.solr.handler.dataimport.DataImporter doFullImport\nINFO: Starting Full Import\nMar 19, 2009 5:25:04 PM org.apache.solr.handler.dataimport.SolrWriter readIndexerProperties\nINFO: Read dataimport.properties\nMar 19, 2009 5:25:04 PM org.apache.solr.handler.dataimport.URLDataSource getData\nSEVERE: Exception thrown while getting data\njava.net.MalformedURLException: no protocol: nullfile\\:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml\n\tat java.net.URL.<init>(URL.java:567)\n\tat java.net.URL.<init>(URL.java:464)\n\tat java.net.URL.<init>(URL.java:413)\n\tat org.apache.solr.handler.dataimport.URLDataSource.getData(URLDataSource.java:88)\n\tat org.apache.solr.handler.dataimport.URLDataSource.getData(URLDataSource.java:47)\n\tat org.apache.solr.handler.dataimport.XPathEntityProcessor.initQuery(XPathEntityProcessor.java:239)\n\tat org.apache.solr.handler.dataimport.XPathEntityProcessor.fetchNextRow(XPathEntityProcessor.java:182)\n\tat org.apache.solr.handler.dataimport.XPathEntityProcessor.nextRow(XPathEntityProcessor.java:165)\n\tat org.apache.solr.handler.dataimport.DocBuilder.buildDocument(DocBuilder.java:335)\n\tat org.apache.solr.handler.dataimport.DocBuilder.doFullDump(DocBuilder.java:221)\n\tat org.apache.solr.handler.dataimport.DocBuilder.execute(DocBuilder.java:163)\n\tat org.apache.solr.handler.dataimport.DataImporter.doFullImport(DataImporter.java:309)\n\tat org.apache.solr.handler.dataimport.DataImporter.runCmd(DataImporter.java:367)\n\tat org.apache.solr.handler.dataimport.DataImporter$1.run(DataImporter.java:348)\nMar 19, 2009 5:25:04 PM org.apache.solr.handler.dataimport.DocBuilder buildDocument\nSEVERE: Exception while processing: single-delete document : SolrInputDocument[{}]\norg.apache.solr.handler.dataimport.DataImportHandlerException: Exception in invoking url null Processing Document # 1\n\tat org.apache.solr.handler.dataimport.URLDataSource.getData(URLDataSource.java:112)\n\tat org.apache.solr.handler.dataimport.URLDataSource.getData(URLDataSource.java:47)\n\tat org.apache.solr.handler.dataimport.XPathEntityProcessor.initQuery(XPathEntityProcessor.java:239)\n\tat org.apache.solr.handler.dataimport.XPathEntityProcessor.fetchNextRow(XPathEntityProcessor.java:182)\n\tat org.apache.solr.handler.dataimport.XPathEntityProcessor.nextRow(XPathEntityProcessor.java:165)\n\tat org.apache.solr.handler.dataimport.DocBuilder.buildDocument(DocBuilder.java:335)\n\tat org.apache.solr.handler.dataimport.DocBuilder.doFullDump(DocBuilder.java:221)\n\tat org.apache.solr.handler.dataimport.DocBuilder.execute(DocBuilder.java:163)\n\tat org.apache.solr.handler.dataimport.DataImporter.doFullImport(DataImporter.java:309)\n\tat org.apache.solr.handler.dataimport.DataImporter.runCmd(DataImporter.java:367)\n\tat org.apache.solr.handler.dataimport.DataImporter$1.run(DataImporter.java:348)\nCaused by: java.net.MalformedURLException: no protocol: nullfile\\:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml\n\tat java.net.URL.<init>(URL.java:567)\n\tat java.net.URL.<init>(URL.java:464)\n\tat java.net.URL.<init>(URL.java:413)\n\tat org.apache.solr.handler.dataimport.URLDataSource.getData(URLDataSource.java:88)\n\t... 10 more\nMar 19, 2009 5:25:04 PM org.apache.solr.handler.dataimport.DataImporter doFullImport\nSEVERE: Full Import failed\norg.apache.solr.handler.dataimport.DataImportHandlerException: Exception in invoking url null Processing Document # 1\n\tat org.apache.solr.handler.dataimport.URLDataSource.getData(URLDataSource.java:112)\n\tat org.apache.solr.handler.dataimport.URLDataSource.getData(URLDataSource.java:47)\n\tat org.apache.solr.handler.dataimport.XPathEntityProcessor.initQuery(XPathEntityProcessor.java:239)\n\tat org.apache.solr.handler.dataimport.XPathEntityProcessor.fetchNextRow(XPathEntityProcessor.java:182)\n\tat org.apache.solr.handler.dataimport.XPathEntityProcessor.nextRow(XPathEntityProcessor.java:165)\n\tat org.apache.solr.handler.dataimport.DocBuilder.buildDocument(DocBuilder.java:335)\n\tat org.apache.solr.handler.dataimport.DocBuilder.doFullDump(DocBuilder.java:221)\n\tat org.apache.solr.handler.dataimport.DocBuilder.execute(DocBuilder.java:163)\n\tat org.apache.solr.handler.dataimport.DataImporter.doFullImport(DataImporter.java:309)\n\tat org.apache.solr.handler.dataimport.DataImporter.runCmd(DataImporter.java:367)\n\tat org.apache.solr.handler.dataimport.DataImporter$1.run(DataImporter.java:348)\nCaused by: java.net.MalformedURLException: no protocol: nullfile\\:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml\n\tat java.net.URL.<init>(URL.java:567)\n\tat java.net.URL.<init>(URL.java:464)\n\tat java.net.URL.<init>(URL.java:413)\n\tat org.apache.solr.handler.dataimport.URLDataSource.getData(URLDataSource.java:88)\n\t... 10 more\nMar 19, 2009 5:25:04 PM org.apache.solr.update.DirectUpdateHandler2 rollback\nINFO: start rollback\nMar 19, 2009 5:25:04 PM org.apache.solr.update.DirectUpdateHandler2 rollback\nINFO: end_rollback\nMar 19, 2009 5:25:04 PM org.apache.solr.update.DirectUpdateHandler2 commit\nINFO: start commit(optimize=false,waitFlush=false,waitSearcher=true)\nMar 19, 2009 5:25:04 PM org.apache.solr.search.SolrIndexSearcher <init>\nINFO: Opening Searcher@281e7e main\nMar 19, 2009 5:25:04 PM org.apache.solr.update.DirectUpdateHandler2 commit\nINFO: end_commit_flush\nMar 19, 2009 5:25:04 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming Searcher@281e7e main from Searcher@7740f6 main\n\tfieldValueCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\nMar 19, 2009 5:25:04 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming result for Searcher@281e7e main\n\tfieldValueCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\nMar 19, 2009 5:25:04 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming Searcher@281e7e main from Searcher@7740f6 main\n\tfilterCache{lookups=6,hits=6,hitratio=1.00,inserts=0,evictions=0,size=9,warmupTime=16,cumulative_lookups=25,cumulative_hits=25,cumulative_hitratio=1.00,cumulative_inserts=2,cumulative_evictions=0}\nMar 19, 2009 5:25:04 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming result for Searcher@281e7e main\n\tfilterCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=9,warmupTime=16,cumulative_lookups=25,cumulative_hits=25,cumulative_hitratio=1.00,cumulative_inserts=2,cumulative_evictions=0}\nMar 19, 2009 5:25:04 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming Searcher@281e7e main from Searcher@7740f6 main\n\tqueryResultCache{lookups=2,hits=2,hitratio=1.00,inserts=7,evictions=0,size=7,warmupTime=8,cumulative_lookups=9,cumulative_hits=7,cumulative_hitratio=0.77,cumulative_inserts=2,cumulative_evictions=0}\nMar 19, 2009 5:25:04 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming result for Searcher@281e7e main\n\tqueryResultCache{lookups=0,hits=0,hitratio=0.00,inserts=7,evictions=0,size=7,warmupTime=8,cumulative_lookups=9,cumulative_hits=7,cumulative_hitratio=0.77,cumulative_inserts=2,cumulative_evictions=0}\nMar 19, 2009 5:25:04 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming Searcher@281e7e main from Searcher@7740f6 main\n\tdocumentCache{lookups=18,hits=15,hitratio=0.83,inserts=26,evictions=0,size=26,warmupTime=0,cumulative_lookups=165,cumulative_hits=149,cumulative_hitratio=0.90,cumulative_inserts=16,cumulative_evictions=0}\nMar 19, 2009 5:25:04 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming result for Searcher@281e7e main\n\tdocumentCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=165,cumulative_hits=149,cumulative_hitratio=0.90,cumulative_inserts=16,cumulative_evictions=0}\nMar 19, 2009 5:25:04 PM org.apache.solr.core.QuerySenderListener newSearcher\nINFO: QuerySenderListener sending requests to Searcher@281e7e main\nMar 19, 2009 5:25:04 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=null path=null params={rows=10&start=0&q=solr} hits=0 status=0 QTime=6 \nMar 19, 2009 5:25:04 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=null path=null params={rows=10&start=0&q=rocks} hits=90 status=0 QTime=34 \nMar 19, 2009 5:25:04 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=null path=null params={q=static+newSearcher+warming+query+from+solrconfig.xml} hits=12327 status=0 QTime=98 \nMar 19, 2009 5:25:04 PM org.apache.solr.core.QuerySenderListener newSearcher\nINFO: QuerySenderListener done.\nMar 19, 2009 5:25:04 PM org.apache.solr.core.SolrCore registerSearcher\nINFO: [] Registered new searcher Searcher@281e7e main\nMar 19, 2009 5:25:04 PM org.apache.solr.search.SolrIndexSearcher close\nINFO: Closing Searcher@7740f6 main\n\tfieldValueCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\n\tfilterCache{lookups=6,hits=6,hitratio=1.00,inserts=0,evictions=0,size=9,warmupTime=16,cumulative_lookups=25,cumulative_hits=25,cumulative_hitratio=1.00,cumulative_inserts=2,cumulative_evictions=0}\n\tqueryResultCache{lookups=2,hits=2,hitratio=1.00,inserts=7,evictions=0,size=7,warmupTime=8,cumulative_lookups=9,cumulative_hits=7,cumulative_hitratio=0.77,cumulative_inserts=2,cumulative_evictions=0}\n\tdocumentCache{lookups=18,hits=15,hitratio=0.83,inserts=26,evictions=0,size=26,warmupTime=0,cumulative_lookups=165,cumulative_hits=149,cumulative_hitratio=0.90,cumulative_inserts=16,cumulative_evictions=0}\n\n\n\nMar 19, 2009 5:25:12 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/apache-solr-1.4-dev path=/select params={wt=xml&q=fileAbsolutePath:file\\:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml} hits=3 status=0 QTime=11 \n\n\n\nAny hints on what I should try next? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12683546",
            "date": "2009-03-19T18:00:05+0000",
            "content": "I have applied the latest version of SOLR-1059 and I just cannot get delete to work! \n\nSOLR-1059 is now committed to trunk so you do not need to apply the patch anymore. There was a slight change too \u2013 the delete flag variable\nhas been renamed to \"$deleteDocById\" and \"$deleteDocByQuery\".\n\nI see that you are escaping the ':' character. You need to escape it only if you are specifying it in the 'q' parameter. For any other parameter ('single' in this case)\nyou do not need to escape it. The escaped ':' is causing this exception. "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12683558",
            "date": "2009-03-19T18:35:56+0000",
            "content": "Yes, I spotted you had committed SOLR-1059. I backed out that patch and did a \"svn update\"  to get the new changes. I had changed my data-config.xml as shown above as was already using the $deleteDocByQuery. Removing the escaping of the : I get the following:-\n\n\nMar 19, 2009 6:31:27 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/apache-solr-1.4-dev path=/dataimport params={command=full-import&clean=false&entity=single-delete&commit=true&single=file:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml} status=0 QTime=0 \nMar 19, 2009 6:31:27 PM org.apache.solr.handler.dataimport.SolrWriter readIndexerProperties\nINFO: Read dataimport.properties\nMar 19, 2009 6:31:27 PM org.apache.solr.handler.dataimport.DataImporter doFullImport\nINFO: Starting Full Import\nMar 19, 2009 6:31:27 PM org.apache.solr.handler.dataimport.SolrWriter readIndexerProperties\nINFO: Read dataimport.properties\nMar 19, 2009 6:31:27 PM org.apache.solr.handler.dataimport.DocBuilder execute\nINFO: Time taken = 0:0:0.14\nMar 19, 2009 6:31:42 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/apache-solr-1.4-dev path=/select params={wt=xml&q=fileAbsolutePath:file\\:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml} hits=3 status=0 QTime=11 \n\n\n\nAny ideas or should I start adding log statements all over the place? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12683567",
            "date": "2009-03-19T18:55:40+0000",
            "content": "I'm not sure what to make of the above log. Are the documents not getting deleted? In your data-config I see that skipDoc is true. No documents will be added at all. "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12683577",
            "date": "2009-03-19T19:35:28+0000",
            "content": "Correct documents are not getting deleted. Line 2 from the log shows:-\n\n   path=/dataimport command=full-import&clean=false&entity=single-delete&commit=true&single=file:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml\n\nLine 12 is me dong a query for the same document:-\n\n   path=/select params=\n{wt=xml&q=fileAbsolutePath:file\\:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml}\n hits=3 status=0 QTime=11\n\nwhich returns three hits. So the documents have not been deleted! Removing the $skipDoc=true and rerunning the delete I get:-\n\n\nMar 19, 2009 7:33:34 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/apache-solr-1.4-dev path=/dataimport params={command=full-import&clean=false&entity=single-delete&commit=true&single=file:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml} status=0 QTime=0 \nMar 19, 2009 7:33:34 PM org.apache.solr.handler.dataimport.SolrWriter readIndexerProperties\nINFO: Read dataimport.properties\nMar 19, 2009 7:33:34 PM org.apache.solr.handler.dataimport.DataImporter doFullImport\nINFO: Starting Full Import\nMar 19, 2009 7:33:34 PM org.apache.solr.handler.dataimport.SolrWriter readIndexerProperties\nINFO: Read dataimport.properties\nMar 19, 2009 7:33:34 PM org.apache.solr.handler.dataimport.SolrWriter deleteByQuery\nINFO: Deleting documents from Solr with query: fileAbsolutePath:file:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml\nMar 19, 2009 7:33:34 PM org.apache.solr.common.SolrException log\nSEVERE: org.apache.lucene.queryParser.ParseException: Cannot parse 'fileAbsolutePath:file:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml': Encountered \" \":\" \": \"\" at line 1, column 21.\nWas expecting one of:\n    <EOF> \n    <AND> ...\n    <OR> ...\n    <NOT> ...\n    \"+\" ...\n    \"-\" ...\n    \"(\" ...\n    \"*\" ...\n    \"^\" ...\n    <QUOTED> ...\n    <TERM> ...\n    <FUZZY_SLOP> ...\n    <PREFIXTERM> ...\n    <WILDTERM> ...\n    \"[\" ...\n    \"{\" ...\n    <NUMBER> ...\n    \n\tat org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:177)\n\tat org.apache.solr.search.QueryParsing.parseQuery(QueryParsing.java:74)\n\tat org.apache.solr.search.QueryParsing.parseQuery(QueryParsing.java:63)\n\tat org.apache.solr.update.DirectUpdateHandler2.deleteByQuery(DirectUpdateHandler2.java:314)\n\tat org.apache.solr.update.processor.RunUpdateProcessor.processDelete(RunUpdateProcessorFactory.java:70)\n\tat org.apache.solr.handler.dataimport.SolrWriter.deleteByQuery(SolrWriter.java:153)\n\tat org.apache.solr.handler.dataimport.DocBuilder.addFields(DocBuilder.java:449)\n\tat org.apache.solr.handler.dataimport.DocBuilder.buildDocument(DocBuilder.java:358)\n\tat org.apache.solr.handler.dataimport.DocBuilder.doFullDump(DocBuilder.java:221)\n\tat org.apache.solr.handler.dataimport.DocBuilder.execute(DocBuilder.java:163)\n\tat org.apache.solr.handler.dataimport.DataImporter.doFullImport(DataImporter.java:309)\n\tat org.apache.solr.handler.dataimport.DataImporter.runCmd(DataImporter.java:367)\n\tat org.apache.solr.handler.dataimport.DataImporter$1.run(DataImporter.java:348)\n\nMar 19, 2009 7:33:34 PM org.apache.solr.handler.dataimport.DataImporter doFullImport\nSEVERE: Full Import failed\norg.apache.solr.handler.dataimport.DataImportHandlerException: org.apache.solr.common.SolrException: Error parsing Lucene query\n\tat org.apache.solr.handler.dataimport.DocBuilder.buildDocument(DocBuilder.java:400)\n\tat org.apache.solr.handler.dataimport.DocBuilder.doFullDump(DocBuilder.java:221)\n\tat org.apache.solr.handler.dataimport.DocBuilder.execute(DocBuilder.java:163)\n\tat org.apache.solr.handler.dataimport.DataImporter.doFullImport(DataImporter.java:309)\n\tat org.apache.solr.handler.dataimport.DataImporter.runCmd(DataImporter.java:367)\n\tat org.apache.solr.handler.dataimport.DataImporter$1.run(DataImporter.java:348)\nCaused by: org.apache.solr.common.SolrException: Error parsing Lucene query\n\tat org.apache.solr.search.QueryParsing.parseQuery(QueryParsing.java:84)\n\tat org.apache.solr.search.QueryParsing.parseQuery(QueryParsing.java:63)\n\tat org.apache.solr.update.DirectUpdateHandler2.deleteByQuery(DirectUpdateHandler2.java:314)\n\tat org.apache.solr.update.processor.RunUpdateProcessor.processDelete(RunUpdateProcessorFactory.java:70)\n\tat org.apache.solr.handler.dataimport.SolrWriter.deleteByQuery(SolrWriter.java:153)\n\tat org.apache.solr.handler.dataimport.DocBuilder.addFields(DocBuilder.java:449)\n\tat org.apache.solr.handler.dataimport.DocBuilder.buildDocument(DocBuilder.java:358)\n\t... 5 more\nCaused by: org.apache.lucene.queryParser.ParseException: Cannot parse 'fileAbsolutePath:file:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml': Encountered \" \":\" \": \"\" at line 1, column 21.\nWas expecting one of:\n    <EOF> \n    <AND> ...\n    <OR> ...\n    <NOT> ...\n    \"+\" ...\n    \"-\" ...\n    \"(\" ...\n    \"*\" ...\n    \"^\" ...\n    <QUOTED> ...\n    <TERM> ...\n    <FUZZY_SLOP> ...\n    <PREFIXTERM> ...\n    <WILDTERM> ...\n    \"[\" ...\n    \"{\" ...\n    <NUMBER> ...\n    \n\tat org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:177)\n\tat org.apache.solr.search.QueryParsing.parseQuery(QueryParsing.java:74)\n\t... 11 more\nMar 19, 2009 7:33:34 PM org.apache.solr.update.DirectUpdateHandler2 rollback\nINFO: start rollback\nMar 19, 2009 7:33:34 PM org.apache.solr.update.DirectUpdateHandler2 rollback\nINFO: end_rollback\nMar 19, 2009 7:33:34 PM org.apache.solr.update.DirectUpdateHandler2 commit\nINFO: start commit(optimize=false,waitFlush=false,waitSearcher=true)\nMar 19, 2009 7:33:34 PM org.apache.solr.search.SolrIndexSearcher <init>\nINFO: Opening Searcher@86b804 main\nMar 19, 2009 7:33:34 PM org.apache.solr.update.DirectUpdateHandler2 commit\nINFO: end_commit_flush\nMar 19, 2009 7:33:34 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming Searcher@86b804 main from Searcher@281e7e main\n\tfieldValueCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\nMar 19, 2009 7:33:34 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming result for Searcher@86b804 main\n\tfieldValueCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\nMar 19, 2009 7:33:34 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming Searcher@86b804 main from Searcher@281e7e main\n\tfilterCache{lookups=6,hits=6,hitratio=1.00,inserts=0,evictions=0,size=9,warmupTime=16,cumulative_lookups=31,cumulative_hits=31,cumulative_hitratio=1.00,cumulative_inserts=2,cumulative_evictions=0}\nMar 19, 2009 7:33:35 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming result for Searcher@86b804 main\n\tfilterCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=9,warmupTime=39,cumulative_lookups=31,cumulative_hits=31,cumulative_hitratio=1.00,cumulative_inserts=2,cumulative_evictions=0}\nMar 19, 2009 7:33:35 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming Searcher@86b804 main from Searcher@281e7e main\n\tqueryResultCache{lookups=2,hits=2,hitratio=1.00,inserts=7,evictions=0,size=7,warmupTime=8,cumulative_lookups=11,cumulative_hits=9,cumulative_hitratio=0.81,cumulative_inserts=2,cumulative_evictions=0}\nMar 19, 2009 7:33:35 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming result for Searcher@86b804 main\n\tqueryResultCache{lookups=0,hits=0,hitratio=0.00,inserts=7,evictions=0,size=7,warmupTime=9,cumulative_lookups=11,cumulative_hits=9,cumulative_hitratio=0.81,cumulative_inserts=2,cumulative_evictions=0}\nMar 19, 2009 7:33:35 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming Searcher@86b804 main from Searcher@281e7e main\n\tdocumentCache{lookups=18,hits=15,hitratio=0.83,inserts=26,evictions=0,size=26,warmupTime=0,cumulative_lookups=183,cumulative_hits=164,cumulative_hitratio=0.89,cumulative_inserts=19,cumulative_evictions=0}\nMar 19, 2009 7:33:35 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming result for Searcher@86b804 main\n\tdocumentCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=183,cumulative_hits=164,cumulative_hitratio=0.89,cumulative_inserts=19,cumulative_evictions=0}\nMar 19, 2009 7:33:35 PM org.apache.solr.core.QuerySenderListener newSearcher\nINFO: QuerySenderListener sending requests to Searcher@86b804 main\nMar 19, 2009 7:33:35 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=null path=null params={rows=10&start=0&q=solr} hits=0 status=0 QTime=3 \nMar 19, 2009 7:33:35 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=null path=null params={rows=10&start=0&q=rocks} hits=90 status=0 QTime=16 \nMar 19, 2009 7:33:35 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=null path=null params={q=static+newSearcher+warming+query+from+solrconfig.xml} hits=12327 status=0 QTime=96 \nMar 19, 2009 7:33:35 PM org.apache.solr.core.QuerySenderListener newSearcher\nINFO: QuerySenderListener done.\nMar 19, 2009 7:33:35 PM org.apache.solr.core.SolrCore registerSearcher\nINFO: [] Registered new searcher Searcher@86b804 main\nMar 19, 2009 7:33:35 PM org.apache.solr.search.SolrIndexSearcher close\nINFO: Closing Searcher@281e7e main\n\tfieldValueCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\n\tfilterCache{lookups=6,hits=6,hitratio=1.00,inserts=0,evictions=0,size=9,warmupTime=16,cumulative_lookups=31,cumulative_hits=31,cumulative_hitratio=1.00,cumulative_inserts=2,cumulative_evictions=0}\n\tqueryResultCache{lookups=2,hits=2,hitratio=1.00,inserts=7,evictions=0,size=7,warmupTime=8,cumulative_lookups=11,cumulative_hits=9,cumulative_hitratio=0.81,cumulative_inserts=2,cumulative_evictions=0}\n\tdocumentCache{lookups=18,hits=15,hitratio=0.83,inserts=26,evictions=0,size=26,warmupTime=0,cumulative_lookups=183,cumulative_hits=164,cumulative_hitratio=0.89,cumulative_inserts=19,cumulative_evictions=0}\n\n\n\n "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12683589",
            "date": "2009-03-19T19:50:04+0000",
            "content": "Lot of weirdness going on here, do not bother looking into this further ill I get my sort straight! "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12683740",
            "date": "2009-03-20T04:11:40+0000",
            "content": "hi fergus, \n\n\tif the document is empty it is not tried to be added.\n\tThere is a new LogTransformer checked into the trunk. you can use that to log any information\n\n\n\n\n "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12683839",
            "date": "2009-03-20T10:46:16+0000",
            "content": "Some of my weirdness resolved by a \"ant clean\". However I have got myself into a whole pile of regex problems. The RegexTransformer just does not allow me to do what I need, which is to conditionally populate the field $deleteDocByQuery. A snippet from my data-config follows:-\n\n\n   <entity name=\"jc\"\n\t     processor=\"ChangeListEntityProcessor\"\n\t     acceptLineRegex=\"^.*\\.xml$\"\n\t     omitLineRegex=\"usc2009\"\n\t     fileName=\"file:///Volumes/spare/ts/man-findlsurl.txt\"\n\t     rootEntity=\"false\"\n\t     dataSource=\"null\"\n\t     baseLocation=\"file:///Volumes/spare/ts/ford/schema/\"\n\t     transformer=\"RegexTransformer\"\n\t     >\n      <field column=\"fileAbsolutePath\"    regex=\"^.*\\s+([^ ]*)$\" replaceWith=\"${jc.baseLocation}/$1\"  sourceColName=\"rawLine\"/>\n      <field column=\"$deleteDocByQuery\"   regex=\"^DELETE.*\"      replaceWith=\"fileAbsolutePath:${jc.fileAbsolutePath}\" sourceColName=\"rawLine\"/> \t       \n\n      <entity name=\"x\"\n\t      dataSource=\"myURIreader\"\n\t      processor=\"XPathEntityProcessor\"\n\n\n\nThe trouble is that this \n\n     <field column=\"$deleteDocByQuery\"   regex=\"^DELETE.*\"      sourceColName=\"rawLine\"/>\n\nleaves $deleteDocByQuery undefined if the regex is unmatched; which is good. However\n\n     <field column=\"$deleteDocByQuery\"   regex=\"^DELETE.*\"      sourceColName=\"rawLine\"     replaceWith=\"fileAbsolutePath:${jc.fileAbsolutePath}\" />\n\nleaves $deleteDocByQuery equal to rawLine if the regex is unmatched. I do not think this is desirable. I tried a work around which, after reading the code, I thought I could get away with\n\n     <field column=\"$deleteDocByQuery\"   regex=\"^DELETE.*\"      sourceColName=\"rawLine\"     replaceWith=\"fileAbsolutePath:${jc.fileAbsolutePath}\" groupNames=\"$deleteDocByQuery\" />\n\nhowever the presence of the \"replaceWith\" attribute disables the \"groupNames\" functionality.\n\nI think something needs sorted; or at least the documentation needs further clarification.\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12683843",
            "date": "2009-03-20T11:06:41+0000",
            "content": "Hi fergus,\nI guess you are trying to do something with in-built Transformers which would be better handled by code (custom/new Transformer).  "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12683851",
            "date": "2009-03-20T11:44:56+0000",
            "content": "My original patch did all this in the ChangeListEntityProcessor, as an option! However as a seperate issue I do think we have a ambigutiy in the face value behaviour of the following code when a mismatch occurs.\n\n\n   <field column=\"$deleteDocByQuery\" regex=\"^DELETE.*\" sourceColName=\"rawLine\"/>\n   <field column=\"$deleteDocByQuery\" regex=\"^DELETE.*\" sourceColName=\"rawLine\" replaceWith=\"fileAbsolutePath:${jc.fileAbsolutePath}\" />\n\n\n\nWhile I do understand that under the hood one is a match and the other a replace. I think that we could to enhance the existing transformer somehow to streamline its interface. After all a new custom/new Transformer would just be a regex by another name. Not sure what to do for the best. 1) I could put my optional code back into ChangeListEntityProcessor? 2) I can also get around the problem with temporary fields, but it is rather ugly:-\n\n\n\n<entity name=\"jc\"\n\t     processor=\"ChangeListEntityProcessor\"\n\t     acceptLineRegex=\"^.*\\.xml$\"\n\t     omitLineRegex=\"usc2009\"\n\t     fileName=\"file:///Volumes/spare/ts/man-findlsurl.txt\"\n\t     rootEntity=\"false\"\n\t     dataSource=\"null\"\n\t     baseLocation=\"file:///Volumes/spare/ts/ford/schema/\"\n\t     transformer=\"RegexTransformer\"\n\t     >\n      <field column=\"fileAbsolutePath\"    regex=\"^.*\\s+([^ ]*)$\" replaceWith=\"${jc.baseLocation}/$1\"  sourceColName=\"rawLine\"/>\n      <field column=\"dummy\"                  regex=\"^DELETE.*\"      replaceWith=\"fileAbsolutePath:${jc.fileAbsolutePath}\" sourceColName=\"rawLine\"/> \t       \n      <field column=\"$deleteDocByQuery\"   regex=\"^fileAbsolutePath:\"  sourceColName=\"dummy\"/> \t       \n\n      <entity name=\"x\"\n\t      dataSource=\"myURIreader\"\n\t      processor=\"XPathEntityProcessor\"\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12683855",
            "date": "2009-03-20T12:01:20+0000",
            "content": "leaves $deleteDocByQuery equal to rawLine if the regex is unmatched\n\nis it a good idea to not do anything if the regex is not matched? that is do not do the replaceAll() "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12683861",
            "date": "2009-03-20T12:24:22+0000",
            "content": "I really dont know, I think it is the syntax that is confusing more than anything else. The first case is clearly a call to a plain olde matcher. The second case is not so clear or explicit; despite your wiki docs which do say you are doing a replace. How about a new RegexTransformer keyword \"matcher\"?\n\n\n1   <field column=\"$deleteDocByQuery\" matcher=\"^DELETE.*\" sourceColName=\"rawLine\"/>\n2   <field column=\"$deleteDocByQuery\"     regex=\"^DELETE.*\" sourceColName=\"rawLine\"/>\n3   <field column=\"$deleteDocByQuery\" matcher=\"^DELETE.*\" sourceColName=\"rawLine\" replaceWith=\"fileAbsolutePath:${jc.fileAbsolutePath}\" />\n4   <field column=\"$deleteDocByQuery\"     regex=\"^DELETE.*\" sourceColName=\"rawLine\" replaceWith=\"fileAbsolutePath:${jc.fileAbsolutePath}\" />\n\n\n\ncases 1) and 2) behave identically, with \"regex\" being deprecated\ncases 3) and 4) differ, case 4) is the existing behavior which boils down to a replace. Case 3) however performs a match first, if the match succeeds then the replace is performed. If the match fails $deleteDocByQuery is unchanged, if it was null, it stays null.  "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12683862",
            "date": "2009-03-20T12:25:53+0000",
            "content": "this should check if the regex matches if not it doesn't replace "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12683882",
            "date": "2009-03-20T13:43:31+0000",
            "content": "This is the latest version of my patch, the principle item of work was ChangeListEntityProcessor.java. However a new URLDataSource.java datasource was added which is almost identical to the now depreciated HttpDataSource.java. I also editied FileDataSource.java to make the log messages equivalent to that produced by URLDataSource.java.\n\nIt is as good as working however I cannot get deleteDocByQuery to function properly. "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12683886",
            "date": "2009-03-20T13:52:15+0000",
            "content": "Your patched version of regex seems great; thanks very much. I think that it is better more useful behavior. \n\nNext.... cant get delete to work. Using a data-config of \n\n<entity name=\"single-delete\"\n\t\t dataSource=\"myURIreader\"\n\t\t processor=\"XPathEntityProcessor\"\n\t\t url=\"${dataimporter.request.single}\"\n\t\t rootEntity=\"true\"\n\t\t stream=\"false\"\n\t\t forEach=\"/record | /record/mediaBlock\"\n\t\t transformer=\"TemplateTransformer\">\n\n      <field column=\"fileAbsolutePath\"    template=\"${dataimporter.request.single}\" /> \n      <field column=\"$deleteDocByQuery\"   template=\"fileAbsolutePath:${dataimporter.request.single}\" /> \t       \n      <field column=\"vdkvgwkey\"           template=\"${dataimporter.request.single}\" /> \n      </entity>\n\n\n\n\n\nIf I enter\n\n    get 'http://localhost:8080/apache-solr-1.4-dev/dataimport?command=full-import&entity=single-delete&clean=false&commit=true&single=file:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml'\n\nI get\n\n\nMar 20, 2009 1:33:36 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/apache-solr-1.4-dev path=/select params={wt=xml&q=\"sea+stallion\"&qt=} hits=10 status=0 QTime=54 \nMar 20, 2009 1:34:01 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/apache-solr-1.4-dev path=/dataimport params={command=full-import&clean=false&entity=single-delete&commit=true&single=file:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml} status=0 QTime=1 \nMar 20, 2009 1:34:01 PM org.apache.solr.handler.dataimport.SolrWriter readIndexerProperties\nINFO: Read dataimport.properties\nMar 20, 2009 1:34:01 PM org.apache.solr.handler.dataimport.DataImporter doFullImport\nINFO: Starting Full Import\nMar 20, 2009 1:34:01 PM org.apache.solr.handler.dataimport.SolrWriter readIndexerProperties\nINFO: Read dataimport.properties\nMar 20, 2009 1:34:01 PM org.apache.solr.handler.dataimport.SolrWriter deleteByQuery\nINFO: Deleting documents from Solr with query: fileAbsolutePath:file:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml\nMar 20, 2009 1:34:01 PM org.apache.solr.common.SolrException log\nSEVERE: org.apache.lucene.queryParser.ParseException: Cannot parse 'fileAbsolutePath:file:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml': Encountered \" \":\" \": \"\" at line 1, column 21.\nWas expecting one of:\n    <EOF> \n    <AND> ...\n    <OR> ...\n    <NOT> ...\n    \"+\" ...\n\n\n\n\nHowever if I escape the ':' and enter \n\n    get 'http://localhost:8080/apache-solr-1.4-dev/dataimport?command=full-import&entity=single-delete&clean=false&commit=true&single=file\\:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml'\n\nin this case it looks as though the deleteDocByQuery is being ignored!\n\n\nMar 20, 2009 1:34:22 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/apache-solr-1.4-dev path=/dataimport params={command=full-import&clean=false&entity=single-delete&commit=true&single=file\\:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml} status=0 QTime=0 \nMar 20, 2009 1:34:22 PM org.apache.solr.handler.dataimport.SolrWriter readIndexerProperties\nINFO: Read dataimport.properties\nMar 20, 2009 1:34:22 PM org.apache.solr.handler.dataimport.DataImporter doFullImport\nINFO: Starting Full Import\nMar 20, 2009 1:34:22 PM org.apache.solr.handler.dataimport.SolrWriter readIndexerProperties\nINFO: Read dataimport.properties\nMar 20, 2009 1:34:22 PM org.apache.solr.handler.dataimport.URLDataSource getData\nSEVERE: Exception thrown while getting data\njava.net.MalformedURLException: no protocol: nullfile\\:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml\n\tat java.net.URL.<init>(URL.java:567)\n\tat java.net.URL.<init>(URL.java:464)\n\tat java.net.URL.<init>(URL.java:413)\n\tat org.apache.solr.handler.dataimport.URLDataSource.getData(URLDataSource.java:88)\n\tat org.apache.solr.handler.dataimport.URLDataSource.getData(URLDataSource.java:47)\n\tat org.apache.solr.handler.dataimport.XPathEntityProcessor.initQuery(XPathEntityProcessor.java:239)\n\tat org.apache.solr.handler.dataimport.XPathEntityProcessor.fetchNextRow(XPathEntityProcessor.java:182)\n\tat org.apache.solr.handler.dataimport.XPathEntityProcessor.nextRow(XPathEntityProcessor.java:165)\n\tat org.apache.solr.handler.dataimport.DocBuilder.buildDocument(DocBuilder.java:335)\n\tat org.apache.solr.handler.dataimport.DocBuilder.doFullDump(DocBuilder.java:221)\n\tat org.apache.solr.handler.dataimport.DocBuilder.execute(DocBuilder.java:163)\n\tat org.apache.solr.handler.dataimport.DataImporter.doFullImport(DataImporter.java:309)\n\tat org.apache.solr.handler.dataimport.DataImporter.runCmd(DataImporter.java:367)\n\tat org.apache.solr.handler.dataimport.DataImporter$1.run(DataImporter.java:348)\nMar 20, 2009 1:34:22 PM org.apache.solr.handler.dataimport.DocBuilder buildDocument\nSEVERE: Exception while processing: single-delete document : SolrInputDocument[{}]\norg.apache.solr.handler.dataimport.DataImportHandlerException: Exception in invoking url null Processing Document # 1\n\tat org.apache.solr.handler.dataimport.URLDataSource.getData(URLDataSource.java:112)\n\tat org.apache.solr.handler.dataimport.URLDataSource.getData(URLDataSource.java:47)\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12688156",
            "date": "2009-03-22T18:48:00+0000",
            "content": "I was having trouble applying the patch as there were some conflicts in HttpDataSource. This patch is in sync with trunk.  "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12688159",
            "date": "2009-03-22T18:56:44+0000",
            "content": "The ParseException is because when we try to delete the document, the file path being given contains a ':' character. A valid solr query must cannot contain such characters. The best way is to use ClientUtils.escapeQueryChars on the value of the query. I see that you are creating the delete query through a template. Perhaps we need a Evaluator which can escape query characters using ClientUtils.escapeQueryChars method?\n\n<field column=\"$deleteDocByQuery\"   template=\"fileAbsolutePath:${dataimporter.functions.escapeQueryChars(dataimporter.request.single)}\" />\n\n "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12688280",
            "date": "2009-03-23T12:49:28+0000",
            "content": "Down loaded your version of my patch. Thanks for taking a look at it and making the improvements.\n\nHowever I still can get things to work. My solr-data.xml is now as follows:-\n\n     <entity name=\"single-delete\"\n\t\t dataSource=\"myURIreader\"\n\t\t processor=\"XPathEntityProcessor\"\n\t\t url=\"${dataimporter.request.single}\"\n\t\t rootEntity=\"true\"\n\t\t flatten=\"true\"\n\t\t stream=\"false\"\n\t\t forEach=\"/record | /record/mediaBlock\"\n\t\t transformer=\"TemplateTransformer\">\n\n      <field column=\"fileAbsolutePath\"    template=\"${dataimporter.request.single}\" /> \n      <field column=\"$deleteDocByQuery\"   template=\"fileAbsolutePath:${dataimporter.functions.escapeQueryChars(dataimporter.request.single)}\" /> \t       \n      <field column=\"vdkvgwkey\"           template=\"${dataimporter.request.single}\" /> \n      </entity>\n\n\n\n\nBut an attempt to delete a document produces the following..\n\nMar 23, 2009 12:45:42 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/apache-solr-1.4-dev path=/dataimport params={command=full-import&clean=false&entity=single-delete&commit=true&single=file:///Volumes/spare/ts/janes/schema/janesxml/data/news/jdw/jdw2008/jni71796.xml} status=0 QTime=1 \nMar 23, 2009 12:45:42 PM org.apache.solr.handler.dataimport.SolrWriter readIndexerProperties\nINFO: Read dataimport.properties\nMar 23, 2009 12:45:42 PM org.apache.solr.handler.dataimport.DataImporter doFullImport\nINFO: Starting Full Import\nMar 23, 2009 12:45:42 PM org.apache.solr.handler.dataimport.SolrWriter readIndexerProperties\nINFO: Read dataimport.properties\nMar 23, 2009 12:45:42 PM org.apache.solr.handler.dataimport.TemplateTransformer transformRow\nWARNING: Unable to resolve variable: dataimporter.functions.escapeQueryChars(dataimporter.request.single) while parsing expression: fileAbsolutePath:${dataimporter.functions.escapeQueryChars(dataimporter.request.single)}\nMar 23, 2009 12:45:42 PM org.apache.solr.core.SolrDeletionPolicy onInit\nINFO: SolrDeletionPolicy.onInit: commits:num=1\n\tcommit{dir=/Volumes/spare/ts/solrnightlyjanes/data/index,segFN=segments_3,version=1237809265075,generation=3,filenames=[_5.nrm, _5.tii, _5.tis, _5.fdx, _5.prx, _5.fdt, _5.fnm, segments_3, _5.frq]\nMar 23, 2009 12:45:42 PM org.apache.solr.core.SolrDeletionPolicy updateCommits\nINFO: last commit = 1237809265075\nMar 23, 2009 12:45:42 PM org.apache.solr.handler.dataimport.TemplateTransformer transformRow\nWARNING: Unable to resolve variable: dataimporter.functions.escapeQueryChars(dataimporter.request.single) while parsing expression: fileAbsolutePath:${dataimporter.functions.escapeQueryChars(dataimporter.request.single)}\nMar 23, 2009 12:45:42 PM org.apache.solr.handler.dataimport.TemplateTransformer transformRow\nWARNING: Unable to resolve variable: dataimporter.functions.escapeQueryChars(dataimporter.request.single) while parsing expression: fileAbsolutePath:${dataimporter.functions.escapeQueryChars(dataimporter.request.single)}\nMar 23, 2009 12:45:42 PM org.apache.solr.handler.dataimport.DocBuilder commit\nINFO: Full Import completed successfully\nMar 23, 2009 12:45:42 PM org.apache.solr.update.DirectUpdateHandler2 commit\nINFO: start commit(optimize=true,waitFlush=false,waitSearcher=true)\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12688281",
            "date": "2009-03-23T13:00:20+0000",
            "content": "However I still can get things to work. My solr-data.xml is now as follows:-\n\nSorry, I was not very clear. I meant that we need to create a new Evaluator which can escape query characters. I'll create an issue and give a patch. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12688583",
            "date": "2009-03-24T06:28:54+0000",
            "content": "OK I opened SOLR-1083 for this enhancement "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12689026",
            "date": "2009-03-25T08:22:14+0000",
            "content": "Here I am at apacheCon. I finally got email going and it looks as though it is going to stop raining.\n\nAnd to top it all I have been able to delete documents. Fantastic. Thanks very much. \n\nI now have very flexable and powerfull functionality that is IMHO miles better than the equivilent functionality in the commercial search engines I  have used.\n\nHowever, I tried deleting some documents that dont exist and was wondering if I the full trace back was required. A simple message should be enough?\n\nI will review and comment the patch and re-upload ASAP. "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12694431",
            "date": "2009-04-01T09:08:37+0000",
            "content": "A more complete version of the patch with docs and an expanded regex test case. Ready for submission? "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12697463",
            "date": "2009-04-09T10:08:04+0000",
            "content": "Now with test case for the ChangeListEntityProcessor  "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12697464",
            "date": "2009-04-09T10:09:58+0000",
            "content": "Ooups. Deleted my patch! Uploading again. "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12697466",
            "date": "2009-04-09T10:11:00+0000",
            "content": "This time it is right! "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12698710",
            "date": "2009-04-14T09:21:35+0000",
            "content": "Fergus, ChangeListEntityProcessor seems to duplicate URIDataSource's functionality instead of using it. Why is that? "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12698767",
            "date": "2009-04-14T13:23:27+0000",
            "content": "Hmmm,\n\nAre you referring to the fragment of code inside ChangeListEntityProcessor that opens the changelist, and its similarity to the functionality in URIDataSource?\n\nI had not thought about arranging some kind of nested use of URIDataSource... is that what you are thinking about?  "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12699273",
            "date": "2009-04-15T16:47:30+0000",
            "content": "Are you referring to the fragment of code inside ChangeListEntityProcessor that opens the changelist, and its similarity to the functionality in URIDataSource?\n\nYes.\n\nI had not thought about arranging some kind of nested use of URIDataSource... is that what you are thinking about? \n\nNot exactly. EntityProcessors do not access http/files directly. That's what DataSources are for. The ChangeListEntityProcessor should just use the context.getDataSource() instead of creating URLConnection directly. The only problem with that approach is that the baseLocation must be specified on the <dataSource>. If you really need it to be returned with the row, you can put a template field with its value, assuming the baseLocation is fixed.\n\nThe more I look at this, the more I feel that the name 'ChangeListEntityProcessor' is misleading. It doesn't really do any changes. It is actually what I imagined a LineEntityProcessor would be. It just streams lines one by one after accepting or rejecting some lines with regex. Whatever else you need to do (for your original use-case), can be done with nested entities and/or custom transformers.\n\nWhat are the changes to TestRegexTransformer that this patch includes? Are these tests that you wrote for the RegexTransformer improvements/fixes that you found earlier? If yes, we should commit them through a different issue. Same should be done for the URIDataSource and associated changes.\n\nWhat do you think? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12699275",
            "date": "2009-04-15T16:50:13+0000",
            "content": "Also, you should override EntityProcessorBase#destroy and close the reader object in it. "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12699309",
            "date": "2009-04-15T18:36:14+0000",
            "content": "Oh boy is this taking ages!. Taking the points in order\n\n1)OK. I will try and rewrite it again with this point in mind.\n\n2)This beast has changed its spots big time and I agree its name is now totally inappropriate. It could at a pinch process CSV, tab separated or many other line orientated text formats and the power provided by nested entities and/or custom transformers is considerable. LineEntityProcessor or LineFilterEntityProcessor are good names.\n\n3) The existing tests with TestRegexTransformer focused on patterns that matched and checked that the expected things happened. My extra tests tested the behavior when patterns failed to match. This was an issue earlier on. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12699311",
            "date": "2009-04-15T18:48:32+0000",
            "content": "Oh boy is this taking ages!. Taking the points in order \n\nI didn't mean to overwhelm you. I can pick it up from here. I have a half-cooked patch with the above changes. "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12699324",
            "date": "2009-04-15T19:22:10+0000",
            "content": "No! It is a learning process for me. It is just it is taking so long to get it sorted.\n\n    I will rename the processor\n\n    I am of course very interested in your patch.\n\n    I agree the included TestRegexTransformer patch could perhaps be a another JIRA issue. Should I open a new one or reopen SOLR-1080?\n\n    I will add an override for EntityProcessorBase#destroy "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12699348",
            "date": "2009-04-15T20:15:31+0000",
            "content": "OK, here's the patch. Completely untested, sorry.\n\n\n\tRemoved explicit URLConnection creation. Just use the entity's data source\n\tDoes not return baseLocation (because now only the data source knows about it)\n\tRemoves a lot of stuff that I think we don't need (and I may be wrong here)\n\tCloses the reader in destroy\n\tStill has TestRegexTransformer changes\n\n\n\nAll yours now. Big thanks for bearing with me! "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12699418",
            "date": "2009-04-15T22:15:31+0000",
            "content": "Hmmm How do I apply your patch\n\nI tried \"patch -p0 < SOLR-1060.patch\" and \"patch -p1 < SOLR-1060.patch\" and it just asked all kind of difficult qustions. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12699549",
            "date": "2009-04-16T06:34:37+0000",
            "content": "This one should work.\n\nI've removed the changes to HttpDataSource (the deprecation) which was causing this funniness. "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12699559",
            "date": "2009-04-16T07:24:52+0000",
            "content": "Thanks. It is working fine now.\n\nI have used it on my data and all seems fine, I am now trying to get the testcase working.\n\nAre you happy with the name \"LineFilterEntityProcessor\" or do you prefer \"LineEntityProcessor\"?\n\nAlso what should I do about the TestRegexTransformer testcase? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12699560",
            "date": "2009-04-16T07:27:56+0000",
            "content": "Are you happy with the name \"LineFilterEntityProcessor\" or do you prefer \"LineEntityProcessor\"?\n\nI prefer LineEntityProcessor\n\nAlso what should I do about the TestRegexTransformer testcase?\n\nAttach it to SOLR-1080. That can go in immediately. "
        },
        {
            "author": "Fergus McMenemie",
            "id": "comment-12699820",
            "date": "2009-04-16T19:25:01+0000",
            "content": "Your patch was almost perfect,\n\n\tI sorted comments and other details to suite the new model\n\tRenamed the entity to LineEntityProcessor\n\tFixed the unit test module\n\tMoved testing of regex stuff elsewhere\n\ttested with my own apps\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12700049",
            "date": "2009-04-17T07:33:20+0000",
            "content": "Thanks Fergus.\n\nEven though LineEntityProcessor was originally conceived by you for reading file/urls from a text file, it does not need to be mentioned in the javadocs. I think it can confuse users. The purpose of LineEntityProcessor is simple, just read line by and line, accept/reject and pass on. The documentation should not be more complicated than that.\n\nAlso look at SOLR-1120 that I just opened. There are just so many things in entity processor that even I cannot keep track of. It is a big change but very much needed so let me circle back to this issue after taking care of it.\n\nSome of the gotchas are:\n\n\tRight way to clean up. Contrary to my previous comments, destroy is not the right place to do the cleanup.\n\tapplyTransformer can return multiple rows which are cached in the entity processor base class\n\tonError attribute need to be handled correctly e.g. abort, skip, continue\n\n\n\nI'll take this forward from here. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12700736",
            "date": "2009-04-20T09:59:29+0000",
            "content": "\n\tRenamed fileName attribute to url to be consistent with XPathEntityProcessor\n\tRenamed omitLineRegex to skipLineRegex (renamed internal variables as well)\n\tUpdated javadocs to remove mentions of change lists (except in one place)\n\n\n\nAll tests pass. I'll commit this shortly. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12700743",
            "date": "2009-04-20T10:13:39+0000",
            "content": "Committed revision 766638.\n\nThanks Fergus! "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12775686",
            "date": "2009-11-10T15:51:59+0000",
            "content": "Bulk close for Solr 1.4 "
        }
    ]
}