{
    "id": "SOLR-1781",
    "title": "Replication index directories not always cleaned up",
    "details": {
        "affect_versions": "1.4",
        "status": "Closed",
        "fix_versions": [
            "4.0-BETA",
            "6.0"
        ],
        "components": [
            "replication (java)",
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "We had the same problem as someone described in http://mail-archives.apache.org/mod_mbox/lucene-solr-user/201001.mbox/%3c222A518D-DDF5-4FC8-A02A-74D4F232B339@snooth.com%3e. A partial copy of that message:\n\nWe're using the new replication and it's working pretty well. There's  \none detail I'd like to get some more information about.\n\nAs the replication works, it creates versions of the index in the data  \ndirectory. Originally we had index/, but now there are dated versions  \nsuch as index.20100127044500/, which are the replicated versions.\n\nEach copy is sized in the vicinity of 65G. With our current hard drive  \nit's fine to have two around, but 3 gets a little dicey. Sometimes  \nwe're finding that the replication doesn't always clean up after  \nitself. I would like to understand this better, or to not have this  \nhappen. It could be a configuration issue.",
    "attachments": {
        "0001-Replication-does-not-always-clean-up-old-directories.patch": "https://issues.apache.org/jira/secure/attachment/12436328/0001-Replication-does-not-always-clean-up-old-directories.patch",
        "SOLR-1781.patch": "https://issues.apache.org/jira/secure/attachment/12536865/SOLR-1781.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Terje Sten Bjerkseth",
            "id": "comment-12835770",
            "date": "2010-02-19T15:15:03+0000",
            "content": "This patch seems to fix the problem, mostly. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12836519",
            "date": "2010-02-22T05:51:13+0000",
            "content": "The problem is that while you are trying to delete the original index, there may be requests in pipleline which uses the old index. If the index files are deleted those requests may fail. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12872455",
            "date": "2010-05-27T22:05:39+0000",
            "content": "Bulk updating 240 Solr issues to set the Fix Version to \"next\" per the process outlined in this email...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E\n\nSelection criteria was \"Unresolved\" with a Fix Version of 1.5, 1.6, 3.1, or 4.0.  email notifications were suppressed.\n\nA unique token for finding these 240 issues in the future: hossversioncleanup20100527 "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-12971296",
            "date": "2010-12-14T16:32:01+0000",
            "content": "I have been having this problem too, but most of the time it's fine.  After a discussion today on the mailing list, I believe it is related to a replication initiated by swapping cores on the master. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043723",
            "date": "2011-06-03T16:46:45+0000",
            "content": "Bulk move 3.2 -> 3.3 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13106421",
            "date": "2011-09-16T14:51:00+0000",
            "content": "3.4 -> 3.5 "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13412136",
            "date": "2012-07-11T22:26:07+0000",
            "content": "bulk fixing the version info for 4.0-ALPHA and 4.0 all affected issues have \"hoss20120711-bulk-40-change\" in comment "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13416005",
            "date": "2012-07-17T08:32:07+0000",
            "content": "This happens almost daily on our SolrCloud (trunk) test cluster, we sometimes see four surpluss index directories created in a day. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13416169",
            "date": "2012-07-17T13:31:58+0000",
            "content": "Markus: on windows? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13416183",
            "date": "2012-07-17T13:43:49+0000",
            "content": "We don't have that, i should have included it in my comment. All servers run Debian GNU/Linux 6.0 and the cloud test cluster always runs with a very recent build from trunk. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13416231",
            "date": "2012-07-17T14:10:44+0000",
            "content": "I'll look into taking care of this. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13416255",
            "date": "2012-07-17T14:50:31+0000",
            "content": "I'm not sure who or when, but it looks like a form of the above patch has already been committed, except instead of:\n\n\n+        else if (successfulInstall) {\n+          delTree(indexDir);\n+        }\n\n\n\nit's\n\n\n+        else {\n+          delTree(indexDir);\n+        }\n\n\n\nI think there is a problem with both. Anyway, I can duplicate this issue easy enough - I'll figure out the right fix. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13416309",
            "date": "2012-07-17T15:46:08+0000",
            "content": "Hmm...so I think I have a fix...but it involves reloading the core even if you do not replicate conf files.\n\nThe problem is, unless you reload the core, it's very difficult to know when searchers are done with the old index. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13416318",
            "date": "2012-07-17T15:55:42+0000",
            "content": "Perhaps they could be cleaned up on core start or after some time has passed?  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13416330",
            "date": "2012-07-17T16:16:35+0000",
            "content": "after some time has passed?\n\nThis was tempting for a moment - but it feels dirty.\n\nYou could just say, hey, do it on the next core restart - but that may not come for a long time.\n\nYou'd really like to be able to tell when all the indexreaders on the old dir are done, but that is very difficult it turns out. The only easy way to do it is to reload the core.\n\nIf you replicate configuration files, you have to reload the core anyway. I'd say it's probably the way to go. It should not be that much more expensive than reloading a searcher in the larger scheme of replication. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13416453",
            "date": "2012-07-17T18:59:23+0000",
            "content": "Patch that starts reloading on replication even when config is not replicated.\n\nAlso, so that existing tests will pass, the dataDir is now consistant across reloads - it won't pick up a new setting from config. \nI conferred about this latter change with hossman in IRC, so for now, that's the way I'm going. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13417132",
            "date": "2012-07-18T14:43:27+0000",
            "content": "Spoke to yonik about this real quick and he brought up the ref counting we have for Directories as an alternative to using reload. It took a couple new methods to our DirectoryFactory, but I think I have something working using that now. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13417192",
            "date": "2012-07-18T15:55:07+0000",
            "content": "Latest patch - I'll commit this soon. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13417212",
            "date": "2012-07-18T16:19:44+0000",
            "content": "Committed. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13419032",
            "date": "2012-07-20T10:10:25+0000",
            "content": "Hi, is the core reloading still part of this? I get a lot of firstSearcher events on a test node now and it won't get online. Going back to July 18th (before this patch) build works fine. Other nodes won't come online with a build from the 19th (after this patch). "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13419099",
            "date": "2012-07-20T12:53:37+0000",
            "content": "No, no reload. Can you please elaborate on what not going online means. Can you share logs? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13419114",
            "date": "2012-07-20T13:19:27+0000",
            "content": "The node will never respond to HTTP requests, all ZK connections time out, very high resource consumption. I'll try provide a log snippet soon. I tried running today's build several times but one specific node refuses to `come online`. Another node did well and runs today's build.\n\nI cannot attach a file to a resolved issue. Send over mail? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13419116",
            "date": "2012-07-20T13:24:02+0000",
            "content": "Ill reopen - email is fine as well. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13419127",
            "date": "2012-07-20T13:31:35+0000",
            "content": "Log sent.\n\nThis node has two shards on it and executed 2x 512 warmup queries which adds up. It won't talk to ZK (tail of the log). Restarting the node with an 18th's build works fine. Did it three times today.\nThanks "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13419380",
            "date": "2012-07-20T17:44:44+0000",
            "content": "Hmm...i can't replicate this issue so far.\n\nAnother change around then was updating to ZooKeeper 3.3.5 (bug fix update).\n\nI wouldnt expect that to be an issue - but are you just upgrading one node and not all of them? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13419385",
            "date": "2012-07-20T17:50:11+0000",
            "content": "Strange indeed. I can/could replicate it on one machine consistently and not on others. Machines weren't upgraded at the same time to prevent cluster downtime.\n\nI'll check back monday, there are two other machines left to upgrade plus the bad node. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13419427",
            "date": "2012-07-20T18:46:44+0000",
            "content": "Machines weren't upgraded at the same time to prevent cluster downtime.\n\nYeah, makes sense, just wasn't sure how you went about it. I'd expect a bugfix  release of zookeeper to work no problem with the previous nodes, but it's the other variable I think. They recommend upgrading with rolling restarts, so it shouldn't be the problem... "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13420495",
            "date": "2012-07-23T08:05:06+0000",
            "content": "The problem is resolved. The `bad` node created several new index.[0-9] directories, even with this patch, and caused high I/O. I deleted the complete data directory so also the index.properties file. It loaded its index from the other nodes and no longer created many index dirs.\n\nThanks "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13421323",
            "date": "2012-07-24T11:00:13+0000",
            "content": "One of the nodes ended up with two index directories today. Later some other nodes also didn't clean up after they got restarted. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13421557",
            "date": "2012-07-24T17:20:01+0000",
            "content": "Odd - in the first case, you are sure the indexes were there over time? For brief periods, more than once can exist...it should just end up being cleaned up when no longer in use.\n\nI can try and dig in some more, but I'll have to think a little - I don't really know where to start.\n\nMy test for this issue is a test that runs a lot of instances and randomly starts and stop them. I then monitor the index directories for these 6-12 instances - I run these tests for a long time and monitor that each keeps ending up with one index dir. At some points, there are two indexes - but it always drops to one shortly later.\n\nSo there still may be some hole, but I don't know where or how.\n\nIf you look in the logs, perhaps you will see a bunch of \"Unable to delete directory : \" entries? It might be that it's trying but cannot. It might make sense to start using deleteOnExit as a last resort if delete fails - I just looked and the del call in SnapPuller does not do this. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13421565",
            "date": "2012-07-24T17:34:57+0000",
            "content": "Hi,\n\nI've deleted both today and yesterday indexes of more than a few hours old.\n\nSome are being removed indeed and some persist. I just restarted all nodes (introduced new fieldTypes and one field) and at least one node has three index directories. Others had two, some just one. Not a single node has a `unable to delete` string in the logs.\n\nThanks "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13421661",
            "date": "2012-07-24T19:06:49+0000",
            "content": "Could you tell me a little bit about the operations you are performing on your cluster - perhaps I am missing some activity that is needed to trigger the remaining issue(s)? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13421675",
            "date": "2012-07-24T19:16:01+0000",
            "content": "Besides search and index only the occasional restart when i change some config or deploy a new build. Sometimes i need to start ZK 3.4 again because it died for some reason. Restarting Tomcat a few times in a row may be a clue here. I'll check again tomorrow if whether it's consistent. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13421707",
            "date": "2012-07-24T19:49:43+0000",
            "content": "occasional restart when i change some config\n\nYou can also do that by reloading all the cores in the cluster - the latest collections api work has a collection reload command.\n\nRestarting Tomcat a few times in a row may be a clue here.\n\nYeah, if any docs where added while a node was down, it will replicate and perhaps get a new index dir. I'm restarting jetty in my tests literally hundreds of times in a row and have not seen this yet. I also just added the search thread, but so far no luck. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13422135",
            "date": "2012-07-25T10:16:47+0000",
            "content": "Hi,\n\nI'll restart one node with two cores.\n\n\n#cat cores/openindex_b/data/index.properties \n#index properties\n#Wed Jul 25 09:58:26 UTC 2012\nindex=index.20120725095644707\n\n\n\n\n# du -h cores/\n4.0K    cores/lib\n46M     cores/openindex_b/data/index.20120725095644707\n404K    cores/openindex_b/data/tlog\n46M     cores/openindex_b/data\n46M     cores/openindex_b\n98M     cores/openindex_a/data/index.20120725095843731\n124K    cores/openindex_a/data/tlog\n98M     cores/openindex_a/data\n98M     cores/openindex_a\n144M    cores/\n\n\n\n2012-07-25 10:01:09,176 WARN [solr.core.SolrCore] - [main] - : New index directory detected: old=null new=/opt/solr/cores/openindex_b/data/index.20120725095644707\n...\n2012-07-25 10:01:17,303 WARN [solr.core.SolrCore] - [main] - : New index directory detected: old=null new=/opt/solr/cores/openindex_a/data/index.20120725095843731\n...\n2012-07-25 10:01:55,016 WARN [solr.core.SolrCore] - [RecoveryThread] - : New index directory detected: old=/opt/solr/cores/openindex_b/data/index.20120725095644707 new=/opt/solr/cores/openindex_b/data/index.20120725100120496\n...\n2012-07-25 10:03:35,236 WARN [solr.core.SolrCore] - [RecoveryThread] - : New index directory detected: old=/opt/solr/cores/openindex_a/data/index.20120725100220706 new=/opt/solr/cores/openindex_a/data/index.20120725100321897\n\n\n\n# du -h cores/\n4.0K    cores/lib\n46M     cores/openindex_b/data/index.20120725095644707\n404K    cores/openindex_b/data/tlog\n46M     cores/openindex_b/data/index.20120725100120496\n91M     cores/openindex_b/data\n91M     cores/openindex_b\n98M     cores/openindex_a/data/index.20120725100321897\n98M     cores/openindex_a/data/index.20120725100220706\n124K    cores/openindex_a/data/tlog\n196M    cores/openindex_a/data\n196M    cores/openindex_a\n287M    cores/\n\n\n\nA few minutes later we still have multiple index directories. No updates have been sent to the cluster during this whole scenario. Each time another directory appears it comes with a lot of I/O, on these RAM limited machines it's almost trashing because of the additional directory. It does not create another directory on each restart but sometimes does, it restarted the same machine again and now i have three dirs for each core.\n\nI'll turn on INFO logging for the node and restart it again without deleting the surpluss dirs. The master and slave versions are still the same.\n\n\n# du -h cores/\n4.0K    cores/lib\n46M     cores/openindex_b/data/index.20120725100813961\n42M     cores/openindex_b/data/index.20120725101349376\n46M     cores/openindex_b/data/index.20120725095644707\n46M     cores/openindex_b/data/index.20120725101231289\n404K    cores/openindex_b/data/tlog\n46M     cores/openindex_b/data/index.20120725100120496\n223M    cores/openindex_b/data\n223M    cores/openindex_b\n98M     cores/openindex_a/data/index.20120725101252920\n98M     cores/openindex_a/data/index.20120725100220706\n124K    cores/openindex_a/data/tlog\n196M    cores/openindex_a/data\n196M    cores/openindex_a\n418M    cores/\n\n\n\nMaybe it cannot find the current index directory on start up (in my case).\n\n\n2012-07-25 10:13:36,125 WARN [solr.core.SolrCore] - [main] - : New index directory detected: old=null new=/opt/solr/cores/openindex_b/data/index.20120725101231289\n2012-07-25 10:13:45,840 WARN [solr.core.SolrCore] - [main] - : New index directory detected: old=null new=/opt/solr/cores/openindex_a/data/index.20120725101252920\n2012-07-25 10:15:41,393 WARN [solr.core.SolrCore] - [RecoveryThread] - : New index directory detected: old=/opt/solr/cores/openindex_b/data/index.20120725101231289 new=/opt/solr/cores/openindex_b/data/index.20120725101349376\n2012-07-25 10:15:46,895 WARN [solr.cloud.RecoveryStrategy] - [main-EventThread] - : Stopping recovery for core openindex_b zkNodeName=nl2.index.openindex.io:8080_solr_openindex_b\n2012-07-25 10:15:46,952 WARN [solr.core.SolrCore] - [RecoveryThread] - : [openindex_a] Error opening new searcher. exceeded limit of maxWarmingSearchers=1, try again later.\n2012-07-25 10:15:47,298 ERROR [solr.cloud.RecoveryStrategy] - [RecoveryThread] - : Error while trying to recover.\norg.apache.solr.common.SolrException: Error opening new searcher. exceeded limit of maxWarmingSearchers=1, try again later.\n        at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1365)\n        at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1157)\n        at org.apache.solr.update.DirectUpdateHandler2.commit(DirectUpdateHandler2.java:560)\n        at org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:316)\n        at org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:210)\n2012-07-25 10:15:47,299 ERROR [solr.cloud.RecoveryStrategy] - [RecoveryThread] - : Recovery failed - trying again...\n\nThis is crazy \n\nbtw: this is today's build. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13422257",
            "date": "2012-07-25T13:35:22+0000",
            "content": "Thanks for all the info - I think perhaps this is a different issue - I'll try and look into. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13422298",
            "date": "2012-07-25T14:18:48+0000",
            "content": "Can I get more of that log? If you are not doing updates, I'm surprised it's going into replication at all on restart... "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13422308",
            "date": "2012-07-25T14:28:48+0000",
            "content": "Ok, I purged the logs, enabled info and started a tomcat. New indexes are created shortly after :\n2012-07-25 10:13:36,125 WARN [solr.core.SolrCore] - [main] - : New index directory detected: old=null new=/opt/solr/cores/openindex_b/data/index.20120725101231289\n\nI'll send it right now. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13422353",
            "date": "2012-07-25T15:32:04+0000",
            "content": "Thanks - there is some sort of race here around opening new searches and what index dir appears. I'm not sure why you see it and I don't in my tests, but it's fairly clear in your logs.\n\nI think I can address it with a simple change that ensures we are picking up the right directory to remove. I'll commit that change in a moment. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13422526",
            "date": "2012-07-25T19:11:42+0000",
            "content": "It seems this fixes the issue. I'll double check tomorrow! "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13423022",
            "date": "2012-07-26T12:05:09+0000",
            "content": "Yes, the problem no longer occurs!\nGreat work! Thanks "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13423113",
            "date": "2012-07-26T14:38:05+0000",
            "content": "Thansk Markus! "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13423779",
            "date": "2012-07-27T10:17:57+0000",
            "content": "There's still a problem with old index directories not being cleaned up and strange replication on start up. I'll write to the ML for this, the problem is likely larger than just cleaning up. "
        },
        {
            "author": "zhuojunjian",
            "id": "comment-13586918",
            "date": "2013-02-26T08:17:39+0000",
            "content": "hi\nsure this issue is fixed in solr4.0 ?\nwe find the issue in solr4.0 in our enviroment . \nI have created a new JIRA \"SOLR-4506\". "
        }
    ]
}