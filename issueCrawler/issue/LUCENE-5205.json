{
    "id": "LUCENE-5205",
    "title": "SpanQueryParser with recursion, analysis and syntax very similar to classic QueryParser",
    "details": {
        "components": [
            "core/queryparser"
        ],
        "fix_versions": [],
        "affect_versions": "None",
        "priority": "Major",
        "labels": "",
        "type": "Improvement",
        "resolution": "Later",
        "status": "Resolved"
    },
    "description": "This parser extends QueryParserBase and includes functionality from:\n\n\n\tClassic QueryParser: most of its syntax\n\tSurroundQueryParser: recursive parsing for \"near\" and \"not\" clauses.\n\tComplexPhraseQueryParser: can handle \"near\" queries that include multiterms (wildcard, fuzzy, regex, prefix),\n\tAnalyzingQueryParser: has an option to analyze multiterms.\n\n\n\nAt a high level, there's a first pass BooleanQuery/field parser and then a span query parser handles all terminal nodes and phrases.\n\nSame as classic syntax:\n\n\tterm: test\n\tfuzzy: roam~0.8, roam~2\n\twildcard: te?t, test*, t*st\n\tregex: /[mb]oat/\n\tphrase: \"jakarta apache\"\n\tphrase with slop: \"jakarta apache\"~3\n\tdefault \"or\" clause: jakarta apache\n\tgrouping \"or\" clause: (jakarta apache)\n\tboolean and +/-: (lucene OR apache) NOT jakarta; +lucene +apache -jakarta\n\tmultiple fields: title:lucene author:hatcher\n\n\n\nMain additions in SpanQueryParser syntax vs. classic syntax:\n\n\tCan require \"in order\" for phrases with slop with the ~> operator: \"jakarta apache\"~>3\n\tCan specify \"not near\": \"fever bieber\"!~3,10 ::\n    find \"fever\" but not if \"bieber\" appears within 3 words before or 10 words after it.\n\tFully recursive phrasal queries with [ and ]; as in: [[jakarta apache]~3 lucene]~>4 ::\n    find \"jakarta\" within 3 words of \"apache\", and that hit has to be within four words before \"lucene\"\n\tCan also use [] for single level phrasal queries instead of \" as in: [jakarta apache]\n\tCan use \"or grouping\" clauses in phrasal queries: \"apache (lucene solr)\"~3 :: find \"apache\" and then either \"lucene\" or \"solr\" within three words.\n\tCan use multiterms in phrasal queries: \"jakarta~1 ap*che\"~2\n\tDid I mention full recursion: [[jakarta~1 ap*che]~2 (solr~ /l[ou]+[cs][en]+/)]~10 :: Find something like \"jakarta\" within two words of \"ap*che\" and that hit has to be within ten words of something like \"solr\" or that \"lucene\" regex.\n\tCan require at least x number of hits at boolean level: \"apache AND (lucene solr tika)~2\n\tCan use negative only query: -jakarta :: Find all docs that don't contain \"jakarta\"\n\tCan use an edit distance > 2 for fuzzy query via SlowFuzzyQuery (beware of potential performance issues!).\n\n\n\nTrivial additions:\n\n\tCan specify prefix length in fuzzy queries: jakarta~1,2 (edit distance =1, prefix =2)\n\tCan specifiy Optimal String Alignment (OSA) vs Levenshtein for distance <=2: (jakarta~1 (OSA) vs jakarta~>1(Levenshtein)\n\n\n\nThis parser can be very useful for concordance tasks (see also LUCENE-5317 and LUCENE-5318) and for analytical search.  \n\nUntil LUCENE-2878 is closed, this might have a use for fans of SpanQuery.\n\nMost of the documentation is in the javadoc for SpanQueryParser.\n\nAny and all feedback is welcome.  Thank you.\n\nUntil this is added to the Lucene project, I've added a standalone lucene-addons repo (with jars compiled for the latest stable build of Lucene)  on github.",
    "attachments": {
        "patch.txt": "https://issues.apache.org/jira/secure/attachment/12611418/patch.txt",
        "LUCENE-5205_improve_stop_word_handling.patch": "https://issues.apache.org/jira/secure/attachment/12633837/LUCENE-5205_improve_stop_word_handling.patch",
        "LUCENE-5205-date-pkg-prvt.patch": "https://issues.apache.org/jira/secure/attachment/12630291/LUCENE-5205-date-pkg-prvt.patch",
        "SpanQueryParser_v1.patch.gz": "https://issues.apache.org/jira/secure/attachment/12602839/SpanQueryParser_v1.patch.gz",
        "LUCENE-5205.patch.gz": "https://issues.apache.org/jira/secure/attachment/12626352/LUCENE-5205.patch.gz",
        "LUCENE-5205-cleanup-tests.patch": "https://issues.apache.org/jira/secure/attachment/12631821/LUCENE-5205-cleanup-tests.patch",
        "LUCENE_5205.patch": "https://issues.apache.org/jira/secure/attachment/12615661/LUCENE_5205.patch",
        "LUCENE-5205_smallTestMods.patch": "https://issues.apache.org/jira/secure/attachment/12630088/LUCENE-5205_smallTestMods.patch",
        "LUCENE-5205_dateTestReInitPkgPrvt.patch": "https://issues.apache.org/jira/secure/attachment/12630318/LUCENE-5205_dateTestReInitPkgPrvt.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2013-10-30T16:23:21+0000",
            "content": "If the community thinks this parser is worthwhile to add to Lucene, and if there's someone with the time and willingness to help me shape this into something worthy of the project, I'm more than happy to make modifications (e.g. use javacc and other refactoring).  Or, better yet, if there are mods that can be made to current code to include the above functionality, I'd be happy to work on those.\n\nI'll post updates as I add more tests and make bug fixes to my local version.\n\nThank you! ",
            "author": "Tim Allison",
            "id": "comment-13809285"
        },
        {
            "date": "2013-10-30T21:09:29+0000",
            "content": "I missed this originally, sorry about that, but I jst had a quick look at the patch.\n\nI think this has a lot more possibilities than the surround parser. So much more that this might actually replace the surround parser.\nYour target should be the queryparser module I think. Hopefully that will bring more users and perhaps even some maintainers.\n\nA few details:\n\nThere is no AND query, that is a pity, but I see the point. I remember the struggle I had to combine Boolean and Span queries in surround.\nA user interface that provides a QueryFilter might well be enough for most users.\n\nAre there test cases for the recursive queries? I may have overlooked them.\n\nThe source code indentation is not 2 spaces everywhere.\n ",
            "author": "Paul Elschot",
            "id": "comment-13809634"
        },
        {
            "date": "2013-10-31T16:03:56+0000",
            "content": "Paul,\n  Thank you for your feedback.  Updated version attached.\n\n1) Some recursion tests were scattered throughout, but I added several more in a new section devoted to this.\n2) Fixed the indentation (I think)\n\nAs for And query...the simplest hack that I can think of that uses only SpanQuery and Query would be the following. \n\nHave a simple parser that takes something like this:\n\nsq1 AND sq2 AND NOT sq3\n\nCreate a filter wrapper around a BooleanQuery that reflects the above for document retrieval and then create a SpanOr query (from sq1 and sq2) for the colorization/concordancing.  The return value would be a pair of SpanQuery and Filter (where filter could be null).  Or, if doc retrieval were the only goal, return the original BooleanQuery.  \n\nMy first attempt wouldn't allow grouping, but that should be easy enough to add.  By grouping, of course, I mean:\n\nsq1 AND sq2 AND NOT (sq3 AND sq4)\n\nAs an integration question...I just came across the test/dev code in the test branch of oal.queryparser.flexible.spans.  Is anyone working on that currently?  Is there an easy way to add my functionality to that framework?  ",
            "author": "Tim Allison",
            "id": "comment-13810379"
        },
        {
            "date": "2013-11-25T20:20:01+0000",
            "content": "Refactored to extend QueryParserBase and add boolean and fielded query parsing.  Added boosting and range queries.  Added nearly all tests from QueryParserTestBase. ",
            "author": "Tim Allison",
            "id": "comment-13831867"
        },
        {
            "date": "2014-01-31T19:19:15+0000",
            "content": "Added hooks for easier integration with Solr.\nFixed unbounded range queries.\nAdded \"negative only\" query.\nNumerous other small refactorings throughout.\n\nPaul Elschot, or any other kind committer, if you have a chance to review and help me get this into shape for committing, I'd greatly appreciate it.  Thank you! ",
            "author": "Tim Allison",
            "id": "comment-13888036"
        },
        {
            "date": "2014-02-06T19:39:23+0000",
            "content": "SImilar to previous efforts ",
            "author": "Tim Allison",
            "id": "comment-13893708"
        },
        {
            "date": "2014-02-18T18:49:08+0000",
            "content": "Thanks to Ahmet Arslan's recommendation, I added tests from TestComplexPhraseQuery.  This revealed LUCENE-5450.\n\nI had to make some small tweaks to the syntax of NOTs within phrases.  \n\nIf the community thinks this functionality is worthwhile, and if a kind committer would be willing to work with me to get this into shape, I'm happy to make modifications.\n\nThank you. ",
            "author": "Tim Allison",
            "id": "comment-13904397"
        },
        {
            "date": "2014-02-18T20:51:38+0000",
            "content": "FWIW my concern with any new query parser, particularly big ones like this which will have a non-trivial maintenance burden, is... how many query parsers do we want to ship & support with Lucene? Instead can we make an existing one better with the new capabilities you\u2019ve added? ",
            "author": "David Smiley",
            "id": "comment-13904569"
        },
        {
            "date": "2014-02-19T13:38:25+0000",
            "content": "David Smiley, I completely agree.  Ideally, at some point in the future, this may reduce the number of parsers required \u2013 Surround, ComplexPhrase and Analyzing \u2013 the functionality of these is covered by this one...I think.  \n\nIf I were to start with an existing parser, I'd probably target ComplexPhrase.  I have no experience with javacc, though, and I'm not sure I can justify the startup costs.  As with so much else, those startup costs will likely turn out to be minimal and the benefit worth the costs.  In general, I'd rather use a mature cc framework/tool than regexes...but I'm not sure I'll have the time or justification to get started.  \n\nAt the very least, the description of the functionality and the testcases from LUCENE-5205 could be used as a guide to others proficient in javacc who want to modify CPQP.  This code could also scamper off to github, but it would be great (from my perspective) to have the capability available as part of the standard distro.\n\nThoughts?\n ",
            "author": "Tim Allison",
            "id": "comment-13905426"
        },
        {
            "date": "2014-02-19T13:56:39+0000",
            "content": "I think its ok to add new parsers and slap experimental or some other label on them, and refactor them later?\n\nIf there are concerns about the code, there is the sandbox as an option, too, which makes things totally clear.\n\nI'll take a look at the patch Tim, thanks. ",
            "author": "Robert Muir",
            "id": "comment-13905461"
        },
        {
            "date": "2014-02-19T23:03:35+0000",
            "content": "Commit 1569953 from Robert Muir in branch 'dev/branches/lucene5205'\n[ https://svn.apache.org/r1569953 ]\n\nLUCENE-5205: create branch ",
            "author": "ASF subversion and git services",
            "id": "comment-13906252"
        },
        {
            "date": "2014-02-19T23:20:21+0000",
            "content": "Commit 1569969 from Robert Muir in branch 'dev/branches/lucene5205'\n[ https://svn.apache.org/r1569969 ]\n\nLUCENE-5205: latest patch, synced to trunk, whitespace and braces and javadocs fixes only ",
            "author": "ASF subversion and git services",
            "id": "comment-13906262"
        },
        {
            "date": "2014-02-19T23:26:50+0000",
            "content": "OK I took a look, i put the current state in a branch (https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5205). This way, we dont have to fling enormous patches around on this issue.\n\nA few things i noticed and fixed (or tried to):\n\n\tformatting: e.g. no space between parens or exception names and braces:\n\twhitespace: e.g. adding a line between each method, between apache license header and imports, and so on\n\tone test class didn't extend lucenetestcase\n\tjavadocs errors (these show up as compile errors in my IDE), e.g. @param foo with no actual text\n\n\n\nIn the process of going thru the code, i have some initial concerns, maybe we can figure out how to address:\n\n\tpublic classes with no javadocs. if its important enough to be a public class, we should at least have a javadoc on it saying what it does.\n\tshould we just nuke the spans.tokens package? This doesn't seem useful to any end user and folding this into .spans as package-private classes could greatly reduce the API surface area.\n\tlots of code copy-pasted from elsewhere (maybe with tweaks). this includes test code. we should try to do some refactoring of this. I can try to look at the tests tonight, and see if I can improve that side.\n\n\n\nAny patches welcome against the branch. ",
            "author": "Robert Muir",
            "id": "comment-13906267"
        },
        {
            "date": "2014-02-20T03:09:51+0000",
            "content": "Commit 1570063 from Robert Muir in branch 'dev/branches/lucene5205'\n[ https://svn.apache.org/r1570063 ]\n\nLUCENE-5205: clean up some test code dup ",
            "author": "ASF subversion and git services",
            "id": "comment-13906545"
        },
        {
            "date": "2014-02-20T03:18:35+0000",
            "content": "Commit 1570066 from Robert Muir in branch 'dev/branches/lucene5205'\n[ https://svn.apache.org/r1570066 ]\n\nLUCENE-5205: reduce visibility of internal classes to pkg-private ",
            "author": "ASF subversion and git services",
            "id": "comment-13906553"
        },
        {
            "date": "2014-02-20T03:29:30+0000",
            "content": "Commit 1570073 from Robert Muir in branch 'dev/branches/lucene5205'\n[ https://svn.apache.org/r1570073 ]\n\nLUCENE-5205: clean up some craziness in these asserts ",
            "author": "ASF subversion and git services",
            "id": "comment-13906561"
        },
        {
            "date": "2014-02-20T03:37:59+0000",
            "content": "Commit 1570074 from Robert Muir in branch 'dev/branches/lucene5205'\n[ https://svn.apache.org/r1570074 ]\n\nLUCENE-5205: clean up formatting, wildcard imports ",
            "author": "ASF subversion and git services",
            "id": "comment-13906566"
        },
        {
            "date": "2014-02-20T03:43:52+0000",
            "content": "Commit 1570075 from Robert Muir in branch 'dev/branches/lucene5205'\n[ https://svn.apache.org/r1570075 ]\n\nLUCENE-5205: uncomment+fix broken assertions ",
            "author": "ASF subversion and git services",
            "id": "comment-13906568"
        },
        {
            "date": "2014-02-20T03:51:34+0000",
            "content": "Commit 1570076 from Robert Muir in branch 'dev/branches/lucene5205'\n[ https://svn.apache.org/r1570076 ]\n\nLUCENE-5205: nuke ancient commented-out stuff, clean up exception tests ",
            "author": "ASF subversion and git services",
            "id": "comment-13906576"
        },
        {
            "date": "2014-02-20T04:04:28+0000",
            "content": "Commit 1570080 from Robert Muir in branch 'dev/branches/lucene5205'\n[ https://svn.apache.org/r1570080 ]\n\nLUCENE-5205: clean up dead code, asserts, remove warnings ",
            "author": "ASF subversion and git services",
            "id": "comment-13906583"
        },
        {
            "date": "2014-02-20T08:17:13+0000",
            "content": "GitHub user PaulElschot opened a pull request:\n\n    https://github.com/apache/lucene-solr/pull/38\n\n    Deprecate Surround parser \n\n    LUCENE-5205\n\n    Nothing much to say \n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/PaulElschot/lucene-solr depr-surround\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/lucene-solr/pull/38.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #38\n\n\ncommit 44b504f070c888bf7124939d352b2f47f77f2e8e\nAuthor: Robert Muir <rmuir@apache.org>\nDate:   2014-02-19T23:03:33Z\n\n    LUCENE-5205: create branch\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5205@1569953 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 97bebc0896fcb47c3f8059c32b25cc288e84b53d\nAuthor: Robert Muir <rmuir@apache.org>\nDate:   2014-02-19T23:20:19Z\n\n    LUCENE-5205: latest patch, synced to trunk, whitespace and braces and javadocs fixes only\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5205@1569969 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit cfb1f36875fd3643c84b3c495641692f6c5f9a26\nAuthor: Robert Muir <rmuir@apache.org>\nDate:   2014-02-20T03:09:47Z\n\n    LUCENE-5205: clean up some test code dup\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5205@1570063 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 7d3974d2cd4645a8933e4a667459f9baf0cda34f\nAuthor: Robert Muir <rmuir@apache.org>\nDate:   2014-02-20T03:18:33Z\n\n    LUCENE-5205: reduce visibility of internal classes to pkg-private\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5205@1570066 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit e5dda4a2e13dc9419f61871561a5ca9bb54f9940\nAuthor: Robert Muir <rmuir@apache.org>\nDate:   2014-02-20T03:29:28Z\n\n    LUCENE-5205: clean up some craziness in these asserts\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5205@1570073 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 020c91223f534a6ccb61745ff90bc273021bd60a\nAuthor: Robert Muir <rmuir@apache.org>\nDate:   2014-02-20T03:37:58Z\n\n    LUCENE-5205: clean up formatting, wildcard imports\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5205@1570074 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 53a9649d778e5887796980b51dfd6b644c27b443\nAuthor: Robert Muir <rmuir@apache.org>\nDate:   2014-02-20T03:43:50Z\n\n    LUCENE-5205: uncomment+fix broken assertions\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5205@1570075 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 068a17bf03620ccd4f25f619ff118495023f8ef5\nAuthor: Robert Muir <rmuir@apache.org>\nDate:   2014-02-20T03:51:33Z\n\n    LUCENE-5205: nuke ancient commented-out stuff, clean up exception tests\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5205@1570076 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 02c98224356d6b0b41acb572d559ab0f4e9dd716\nAuthor: Robert Muir <rmuir@apache.org>\nDate:   2014-02-20T04:04:27Z\n\n    LUCENE-5205: clean up dead code, asserts, remove warnings\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5205@1570080 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit a5921753a832b79e6c11e51f1af9ad6aa1d47c96\nAuthor: Paul Elschot <paul.j.elschot@gmail.com>\nDate:   2014-02-20T08:11:25Z\n\n    Deprecate Surround parser, use SpanQueryParser instead.\n\n ",
            "author": "ASF GitHub Bot",
            "id": "comment-13906751"
        },
        {
            "date": "2014-02-20T14:00:00+0000",
            "content": "Commit 1570190 from Robert Muir in branch 'dev/branches/lucene5205'\n[ https://svn.apache.org/r1570190 ]\n\nLUCENE-5205: commit paul's patch, deprecating surround for spans ",
            "author": "ASF subversion and git services",
            "id": "comment-13906977"
        },
        {
            "date": "2014-02-20T14:01:57+0000",
            "content": "Thanks Paul! ",
            "author": "Robert Muir",
            "id": "comment-13906978"
        },
        {
            "date": "2014-02-20T16:27:53+0000",
            "content": "Github user PaulElschot closed the pull request at:\n\n    https://github.com/apache/lucene-solr/pull/38 ",
            "author": "ASF GitHub Bot",
            "id": "comment-13907142"
        },
        {
            "date": "2014-02-20T16:29:43+0000",
            "content": "Wow, Robert, thank you!  \n\nThe attached contains some trivial mods.\n1) I uncommented the LUCENE-5450 test cases in TestComplexPhraseSpanQuery \n2) I modified new Searcher(reader) to newSearcher(reader) in a few places.\n\nPlease let me know if there are changes that I should make.  I'm somewhat afraid to get in your way.\n\nThank you, again. ",
            "author": "Tim Allison",
            "id": "comment-13907146"
        },
        {
            "date": "2014-02-20T16:38:06+0000",
            "content": "[~tallison@mitre.org] By the way, the ASF just recently improved the integration with GitHub and the Lucene project.  I suggest you fork and send a pull request, assuming both you and Robert Muir find that easier.  Just a suggestion. ",
            "author": "David Smiley",
            "id": "comment-13907156"
        },
        {
            "date": "2014-02-20T17:05:08+0000",
            "content": "patches of any sort are fine: svn patches, git patches, pull requests, to me its the same.\n\nSorry Tim, i just now saw your patch: thanks. I will commit this first and see what the branch looks like and then respond to your question. ",
            "author": "Robert Muir",
            "id": "comment-13907187"
        },
        {
            "date": "2014-02-20T17:34:40+0000",
            "content": "Commit 1570280 from Robert Muir in branch 'dev/branches/lucene5205'\n[ https://svn.apache.org/r1570280 ]\n\nLUCENE-5205: tim's test fixes ",
            "author": "ASF subversion and git services",
            "id": "comment-13907213"
        },
        {
            "date": "2014-02-20T18:06:03+0000",
            "content": "\nPlease let me know if there are changes that I should make. I'm somewhat afraid to get in your way.\n\nPlease, dont worry about being in my way. Actually thats why i was working on the tests part: i don't have to understand much about this specific parser to work on them, the changes likely wont be controversial, and i wont be in your way as far as the actual code.\n\nHere's a couple of things:\n\nAs far as concerns, i mentioned some things above, some are cleaned up, others are still problems i think. A big one is I think we should try to remove some of this code duplication here, from various places, as much as possible.\n\nA quick thing: which of the abstract classes in the package really needs to be public? Currently all are, if they currently really NEED to be public, can we make some of these package-private? At least, if they are public, they should have some javadocs (e.g. AbstractSpanQueryParser has none on its class).\n\nTestSpanQPBasedonQPTestBase.testDateRange is currently ignored with nocommit. It seems to sometimes fail based on the Locale. its highly possible that I caused this in my refactoring somehow, but i didnt run the tests enough times to know for sure... we should figure that out.\n\nI'm a little worried about stuff like SpanOnlyParser/SpanQueryParser.ReInit: this is confusing. Isn't it only called by the superclass' parse(String s), which is overridden here anyway? maybe we can just replace all that code with something like 'assert false'. \n\nill investigate more and try to think of more later, but maybe you have some thoughts on these things. ",
            "author": "Robert Muir",
            "id": "comment-13907256"
        },
        {
            "date": "2014-02-20T18:46:43+0000",
            "content": "Code duplication.  The biggest offenders are in test (I think, let me know if you disagree):\n     1) TestSpanQPBasedonQPTestBase...I can try to refactor this to extend QPTestBase, but that will require some reworking of QPTestBase as, and I didn't want to touch that (hence the duplication).  It would also help to add a getQuery() to SpanMultitermQueryWrapper to test for equality...again, I didn't want to touch anything outside of the parser at the cost of duplication. \n     2) TestMultiAnalyzer.  This relies on testing equality of string representations of queries.  Will have to modify TestMultiAnalyzer in way similar to QPTestBase.\n     3) TestComplexPhraseQuery.  Should be straightforward to extend the original, but will need to make checkMatches public so that I can override it.  I'll also have to move the tests with slightly different syntax into a different test, but that's easy and would help declutter.\n\nThere's other code duplication with AnalyzingQueryParser...should we break that functionality out into a helper class?\n\nAny other major duplication areas?\n\nY, I don't like the reinit at all.  The reason that's there was so that I could extend QueryParserBase, but I'm not sure that that decision buys much anymore.  As I remember, it buys date parsing in range queries (which I'm now not sure I actually want) and addBoolean; there may be more, but I'm not sure there is.\n\nIt would clean up a fair bit of code if I implement CommonQueryParserConfiguration instead of extending QueryParserBase.  I'd still have to leave in some things that don't make sense for the SpanQueryParser, though: lowerCaseExpandedTerms, enablePositionIncrements.  Another option would be to abandon CQPC, but I wanted this parser to at least implement that interface.  Let me know what makes sense.  \n\nAs for the public base classes, y, those can go private for now.  I made them public in case anyone wanted to extend them, but, as you point out, then I really ought to add javadocs and treat them as if they were public (which they are!).\n\nAs for date/locale issues, I'll take a look.     ",
            "author": "Tim Allison",
            "id": "comment-13907304"
        },
        {
            "date": "2014-02-20T19:10:47+0000",
            "content": "\nCode duplication. The biggest offenders are in test (I think, let me know if you disagree):\n\nI guess i wasn't even thinking about test code, i just see code like this method in AnalyzingQueryBase (as an example):\n\n\n  protected BytesRef analyzeMultitermTermParseEx(String field, String part) throws ParseException {\n\n\n\nI know this probably exists elsewhere, i swear there is something in QPBase doing this for range queries. \n\nAs far as test code:\n\n1) TestSpanQPBasedonQPTestBase...I can try to refactor this to extend QPTestBase, but that will require some reworking of QPTestBase as, and I didn't want to touch that (hence the duplication). It would also help to add a getQuery() to SpanMultitermQueryWrapper to test for equality...again, I didn't want to touch anything outside of the parser at the cost of duplication. \n\nI worked on this last night. Basically i split up QPTestBase into two parts:\n1. test harness, test analyzers, helper assertions, etc (QPTestCase)\n2. actual concrete test methods for historical lucene behavior.\n\nI fixed TestSpanQPBasedonQPTestBase to extend #1, I'm pretty happy with it? It removed all the duplicate test analyzers and assert methods and so on. Let me know if you have any concerns with what i did there.\n\n\nIt would also help to add a getQuery() to SpanMultitermQueryWrapper to test for equality...again, I didn't want to touch anything outside of the parser at the cost of duplication. \n\nHmm, i guess i'm of the opposite mentality. I can see your point about not touching existing code, as it makes this contribution a 'standalone' change, but personally i'd rather us fix stuff like this! That being said, i did in fact recently add such a method while working on an unrelated issue (LUCENE-5415):\n\n  /** Returns the wrapped query */\n  public Query getWrappedQuery() {\n    return query;\n  }\n\n\n\nDoes this work? Want to upload a patch that uses this?\n\n\n3) TestComplexPhraseQuery. Should be straightforward to extend the original, but will need to make checkMatches public so that I can override it. I'll also have to move the tests with slightly different syntax into a different test, but that's easy and would help declutter.\n\nOk, this sounds good.\n\n\nThere's other code duplication with AnalyzingQueryParser...should we break that functionality out into a helper class?\n\nSounds like a good idea, i havent looked at the two, but seems worthwhile.\n\n\nY, I don't like the reinit at all. The reason that's there was so that I could extend QueryParserBase, but I'm not sure that that decision buys much anymore. As I remember, it buys date parsing in range queries (which I'm now not sure I actually want) and addBoolean; there may be more, but I'm not sure there is.\n\nIt would clean up a fair bit of code if I implement CommonQueryParserConfiguration instead of extending QueryParserBase. I'd still have to leave in some things that don't make sense for the SpanQueryParser, though: lowerCaseExpandedTerms, enablePositionIncrements. Another option would be to abandon CQPC, but I wanted this parser to at least implement that interface. Let me know what makes sense. \n\nI think the current base class is fine. I guess my question is, who is calling reinit \n\n\nAs for the public base classes, y, those can go private for now. I made them public in case anyone wanted to extend them, but, as you point out, then I really ought to add javadocs and treat them as if they were public (which they are!).\n\nsounds good, maybe you want to upload a patch making the appropriate things pkg-private?  We can always open things back up later, if we need.\n\n ",
            "author": "Robert Muir",
            "id": "comment-13907339"
        },
        {
            "date": "2014-02-20T19:19:59+0000",
            "content": "\nI know this probably exists elsewhere, i swear there is something in QPBase doing this for range queries. \nBusted...yeah...I meant to include that in my list of duplication.  Sorry.  \n\nAs for Reinit, you're absolutely right...no one is calling that...let's go with documentation and assert false.\n\nWill start some patches on the smaller stuff.  Thank you, again! ",
            "author": "Tim Allison",
            "id": "comment-13907354"
        },
        {
            "date": "2014-02-20T21:05:03+0000",
            "content": "In TestSpanQPBasedOnQPTestBase, the date issue is caused by slightly different escaping requirements in SpanQueryParser.  The fix for that is overriding escapeDateString.\n\nWe could get rid of much more duplication if we were willing to add this to the various AssertQueryEquals in QueryParserTestCase:\n\n    if (q instanceof SpanMultiTermQueryWrapper){\n      q = ((SpanMultiTermQueryWrapper)q).getWrappedQuery();\n    }\n\n\n\nBetter to override in TestSpanQPBasedOnQPTestBase?\n\nPatch on other issues on way... ",
            "author": "Tim Allison",
            "id": "comment-13907496"
        },
        {
            "date": "2014-02-20T21:09:12+0000",
            "content": "\nIn TestSpanQPBasedOnQPTestBase, the date issue is caused by slightly different escaping requirements in SpanQueryParser. The fix for that is overriding escapeDateString.\n\nThanks for digging into this!\n\n\nBetter to override in TestSpanQPBasedOnQPTestBase?\n\nI would prefer that approach (override the asserts) ! ",
            "author": "Robert Muir",
            "id": "comment-13907504"
        },
        {
            "date": "2014-02-20T21:57:31+0000",
            "content": "Back on the issue of reinventing analyzeMultiterm...part of that reinvention was  because I was getting a setReader() in wrong state exception in one of my tests.  With analyzeMultiterm in QueryParserBase as it stands, the token stream is not consumed if an exception is thrown.  Therefore, next time you run the parser (with the same analyzer) you can get:\n\n   [junit4]    > Throwable #1: java.lang.AssertionError: setReader() called in wrong state: INCREMENT_FALSE\n   [junit4]    >        at __randomizedtesting.SeedInfo.seed([6E1DC3D6C716BC75:EDEE4C0E5E329586]:0)\n   [junit4]    >        at org.apache.lucene.analysis.MockTokenizer.setReaderTestPoint(MockTokenizer.java:266)\n   [junit4]    >        at org.apache.lucene.analysis.Tokenizer.setReader(Tokenizer.java:92)\n   [junit4]    >        at org.apache.lucene.analysis.Analyzer$TokenStreamComponents.setReader(Analyzer.java:304)\n   [junit4]    >        at org.apache.lucene.analysis.Analyzer.tokenStream(Analyzer.java:181)\n\n\n\nShould I fix this in QueryParserBase or did I not build the test analyzer correctly?  ",
            "author": "Tim Allison",
            "id": "comment-13907588"
        },
        {
            "date": "2014-02-20T22:04:27+0000",
            "content": "I havent looked at the code, but it seems its not consuming everything. Why is this the case? Why are there multiple tokens here?\n\nif someone does a wildcard like \"foo*\" and you analyze it, and it breaks it into multiple tokens, i think you should 1) consume the rest, 2) throw exception? ",
            "author": "Robert Muir",
            "id": "comment-13907598"
        },
        {
            "date": "2014-02-20T22:10:09+0000",
            "content": "Agreed.\n\nThis is how it stands in QueryParserBase:\n\n  protected BytesRef analyzeMultitermTerm(String field, String part, Analyzer analyzerIn) {\n    if (analyzerIn == null) analyzerIn = getAnalyzer();\n\n    try (TokenStream source = analyzerIn.tokenStream(field, part)) {\n      source.reset();\n      \n      TermToBytesRefAttribute termAtt = source.getAttribute(TermToBytesRefAttribute.class);\n      BytesRef bytes = termAtt.getBytesRef();\n\n      if (!source.incrementToken())\n        throw new IllegalArgumentException(\"analyzer returned no terms for multiTerm term: \" + part);\n      termAtt.fillBytesRef();\n      if (source.incrementToken())\n        throw new IllegalArgumentException(\"analyzer returned too many terms for multiTerm term: \" + part);\n      source.end();\n      return BytesRef.deepCopyOf(bytes);\n    } catch (IOException e) {\n      throw new RuntimeException(\"Error analyzing multiTerm term: \" + part, e);\n    }\n  }\n\n\n\nIn my duplicated version, I modified the if (source.incrementToken) to a while to consume all tokens.\n\nRaise separate issue or fix with this? ",
            "author": "Tim Allison",
            "id": "comment-13907608"
        },
        {
            "date": "2014-02-21T00:18:28+0000",
            "content": "Same issue in Solr's TextField. ",
            "author": "Tim Allison",
            "id": "comment-13907768"
        },
        {
            "date": "2014-02-21T12:03:22+0000",
            "content": "Small fixes to get date test to work.  Also made more classes package private.\n\nRobert, thanks to your lead in refactoring the common query parser test base, I'll do some more dramatic refactoring next week so that the tests for spanqueryparser extend QueryParserTestBase, not just ...Case.  Er...I'll see if that is possible and worth the effort. ",
            "author": "Tim Allison",
            "id": "comment-13908205"
        },
        {
            "date": "2014-02-21T12:04:43+0000",
            "content": "On the issue of token consumption on exception during multiterm analysis and on deduplicating code generally, I propose raising a separate issue that would:\n\n1) create a static util helper class to analyze multiterm and multitermwildcards...AnalyzingQueryParser and Solr's TextField as well as 5205's would all be calling the same multiterm analysis code.\n2) fold 5205's AnalyzingQueryParserBase's functionality into\nQueryParserBase\n3) add analysis as an option into ClassicQueryParser\n4) deprecate AnalyzingQueryParser\n5) fix the token consumption on exception issue centrally\n\nOnce this is committed and merged into 5205, we can get rid of AnalyzingQueryParserBase in 5205.\n\nThoughts? ",
            "author": "Tim Allison",
            "id": "comment-13908207"
        },
        {
            "date": "2014-02-21T13:10:31+0000",
            "content": "Sounds good Tim, lets do it. Feel free to break it down into more than 1 issue if you want too, that sounds like a lot at once. ",
            "author": "Robert Muir",
            "id": "comment-13908282"
        },
        {
            "date": "2014-02-21T15:05:17+0000",
            "content": "This includes the patch that I submitted earlier today.\n\n1) fixed date test\n2) made most classes package private\n3) cleaned up reinit and TopLevelQuery\n\nWill turn to other patches on Monday.  Thank you! ",
            "author": "Tim Allison",
            "id": "comment-13908413"
        },
        {
            "date": "2014-02-21T15:15:38+0000",
            "content": "Commit 1570602 from Robert Muir in branch 'dev/branches/lucene5205'\n[ https://svn.apache.org/r1570602 ]\n\nLUCENE-5205: make some abstract base classes pkg-private, fix date escaping/date test, clean up reInit ",
            "author": "ASF subversion and git services",
            "id": "comment-13908422"
        },
        {
            "date": "2014-02-21T15:15:59+0000",
            "content": "Thanks Tim, I just committed that patch. ",
            "author": "Robert Muir",
            "id": "comment-13908423"
        },
        {
            "date": "2014-02-28T20:17:05+0000",
            "content": "This patch focuses on reducing code duplication in the test cases.  Reducing duplication in the main part of code will be separate patch.\n\n1) TestSpanQPBasedOnQPTestBase (renamed to TestQPTestBaseSpanQuery)...now subclasses QueryParserTestBase.\n  There were a few handfuls of tests that I couldn't easily modify; mostly these were string equality tests for complex queries. The CJK examples where complex truth queries were built programmatically also had to be rewritten for SpanQueries. Those now exist in testParserSpecificQuery()...solution is not elegant, and I don't like the name.  It would be slightly cleaner to move those handfuls of tests up into TestQueryParser, but I want them to be available to the other subclasses of QueryParserTestBase.\n\n2) TestComplexPhraseSpanQuery now subclasses TestComplexPhraseQuery.  Again, had to add a testParserSpecificSyntax.  However, in this case the syntax btwn the two parsers is slightly different.\n\n3) TestMultiAnalyzer (renamed to TestMultiAnalyzerSpanQuery) now subclasses TestMultiAnalyzer.  These tests were mostly string equality tests on complex queries, and I couldn't easily keep any of the tests. \n\nFor the above, I'm sure there are more elegant solutions, but this is where the code is for now.\n\n\n\nSmall clean up in SpanQueryParserBase:\n1) got rid of special option to lowercase regex...treat like any other multiterm, and eventually get rid of lowercasing multiterms altogether!\n\n2) got rid of special rounding correction for fuzzy minSims.  Behavior is now the same as classic.\n\n\nBugs found:\n1) failed to set boost on MatchAllDocsQuery\n2) fixed analysis of range terms (the irony  )\n\nCapabilities not currently covered by SQP\n1) Known: && ! syntax\n2) Unknown: \"term phrase term\" ->\n        \"+term +(+phrase1 +phrase2) +term\" ",
            "author": "Tim Allison",
            "id": "comment-13916317"
        },
        {
            "date": "2014-03-06T17:31:12+0000",
            "content": "Robert Muir, if you have a chance to review and commit the Feb 28 patch for cleaning up the test cases, I'd greatly appreciate it!  Thank you, again. ",
            "author": "Tim Allison",
            "id": "comment-13922786"
        },
        {
            "date": "2014-03-06T19:54:47+0000",
            "content": "Sorry Tim! I'll try to get to this today. ",
            "author": "Robert Muir",
            "id": "comment-13922974"
        },
        {
            "date": "2014-03-06T19:57:13+0000",
            "content": "You've had far bigger fish to fry...np at all! ",
            "author": "Tim Allison",
            "id": "comment-13922978"
        },
        {
            "date": "2014-03-07T05:44:22+0000",
            "content": "Hi,\n\nPhrases with stop words in them are not getting searched whereas a phrase without it gets searched using SpanQueryParser/ComplexPhraseQueryParser.\nE.g. \"calculator for evaluating\" is not showing any result for an indexed document whereas a phrase without stop word like \"evaluating mathematical\" is getting searched.\n\nThe similar search works fine with classic parser which uses PhraseQuery as it includes the position of terms for all the terms including stop words in the query created.\nKindly provide your inputs.\n\nThanks,\nModassar ",
            "author": "Modassar Ather",
            "id": "comment-13923586"
        },
        {
            "date": "2014-03-07T14:36:12+0000",
            "content": "The root of this problem is that SpanNearQuery has no good way to handle stopwords in a way analagous to PhraseQuery.\n\nIn SpanQueryParser, this limitation should be well described in the javadocs to SpanQueryParser and in the test cases.  Let me know if it isn't.  You have the option of throwing an exception when a stopword is found to notify the user about stopwords, but that's exceedingly unsatisfactory.\n\nWithout digging into the internals of SpanNearQuery, we can still do better on this.  One proposal is to do what the basic highlighter does and risk false positives...behind the scenes modify \"calculator for evaluating\" to \"calculator evaluating\"~>1.  This would then falsely match \"calculator zebra evaluating.\"  PhraseQuery can have false positives, too, but it guarantees that the false hit has to be a stop word.  This solution would not do that.  So, is this better than no matches at all? ",
            "author": "Tim Allison",
            "id": "comment-13923915"
        },
        {
            "date": "2014-03-08T02:31:53+0000",
            "content": "PhraseQuery does not guarantee that a false hit will be a stop word - if the data contains 'calculator xyz evaluating' and we search for \"calculator for evaluating\", then it will match.\n\nI think that replacing the StopFilter with a SynonymFilter where all the stop words are synonyms of each other may work well. Initial tests by my team indicate that it behaves as expected. The index size does increases by about 20%, but we can live with that. I am wondering if there are any side effects that we are missing? ",
            "author": "Nikhil Chhaochharia",
            "id": "comment-13924671"
        },
        {
            "date": "2014-03-10T19:14:04+0000",
            "content": "\nPhraseQuery does not guarantee that a false hit will be a stop word - if the data contains 'calculator xyz evaluating' and we search for \"calculator for evaluating\", then it will match.\nUgh.  You are right.  Not sure how I got that wrong.  Thank you.\n\nIn the use case with a StopFilter, if we were to go with the proposal above to convert to a SpanNearQuery \"calculator evaluating\"~>1, there could be a false hit on \"calculator evaluating\"...adjacent terms.  But that shouldn't be too problematic?\n\nI can't think of any side effects by going with your proposal if you are ok with increased index size and query response time.  Out of curiosity, how much bigger is your index if you don't use a StopFilter or the SynonymFilter?   ",
            "author": "Tim Allison",
            "id": "comment-13926063"
        },
        {
            "date": "2014-03-11T01:57:30+0000",
            "content": "Nikhil Chhaochharia and Modassar Ather, this patch (based on current lucene5205 branch) adds the proposed change.  \"calculator for evaluating\" is modified to \"calculator evaluating\"~>1 behind the scenes.  I got rid of the option to throw a parse exception when encountering a stop word.\n\nI added extra tests in TestOverallSpanQueryParser and TestSpanOnlyParser. \n\nThese mods look good? ",
            "author": "Tim Allison",
            "id": "comment-13929868"
        },
        {
            "date": "2014-03-11T11:27:12+0000",
            "content": "Looks good - we will be testing this over the next few days and will report back if we find any issues.\n\nWith StopFilter removed, the index size increased by 20% and there was no appreciable increase in the indexing time.\nWith StopFilter replaced by a SynonymFilter (all stopwords as synonyms), the index size almost doubled and the indexing time more than tripled. We will probably not be going forward with this option. (I had mistakenly mentioned the stats for an index with the StopFilter removed in my earlier comment) ",
            "author": "Nikhil Chhaochharia",
            "id": "comment-13930253"
        },
        {
            "date": "2014-03-11T14:34:27+0000",
            "content": "Hi Nikhil Chhaochharia, Instead of \"all stopwords as synonyms\" what do you think reducing all stop words to the same token?\n\n\"the\" => \"ImpossibleToken\"\n\"a\" => \"ImpossibleToken\"\n\"for\" => \"ImpossibleToken\" ",
            "author": "Ahmet Arslan",
            "id": "comment-13930402"
        },
        {
            "date": "2014-03-12T00:24:28+0000",
            "content": "Nikhil Chhaochharia, ah, yes, that was a bit of a surprise.  Dropping the StopFilter has worked quite well on small to medium batches of documents. Thank you for clarifying.  \n\nPlease do let me know if you find any issues.  Are you using SOLR-5410 or straight Lucene?\n\nThere's still quite a bit of cleanup to do...Once the testcase patch is applied and there's some resolution to LUCENE-5470 and maybe LUCENE-5504, I should be able to get rid of quite a bit of code.\n\nThanks again. ",
            "author": "Tim Allison",
            "id": "comment-13931196"
        },
        {
            "date": "2014-03-12T09:37:56+0000",
            "content": "We will try reducing the stop words to some impossible token and report back in a few days.\n\nWe need the user fields and a few other features of the edismax parser, hence we have modified it to send only 'phrase' queries to SpanQueryParser. Its a huge hack but we would like include this functionality without the overhead of building our own parser from scratch. ",
            "author": "Nikhil Chhaochharia",
            "id": "comment-13931580"
        },
        {
            "date": "2014-03-13T11:40:23+0000",
            "content": "Interesting...let me know if I can make the hooks any easier for integration into edismax. ",
            "author": "Tim Allison",
            "id": "comment-13933132"
        },
        {
            "date": "2014-03-14T05:00:58+0000",
            "content": "We tried reducing all stopwords to an impossible token and it increased our indexing time as well as index size by about 10% when compared to using the StopFilter. We used a SynonymFilter to map all stop words to the impossible token and set expand=false  Initial tests show that the functionality is as expected and PhraseQuery / SpanQuery handle stop words properly. We will be running more tests to check if there are any unexpected side-effects, but this looks like a better option compared to using a StopFilter which sometimes leads to false matches. ",
            "author": "Nikhil Chhaochharia",
            "id": "comment-13934612"
        },
        {
            "date": "2014-03-14T08:55:24+0000",
            "content": "Hey [~tallison@mitre.org], can this parser handle \"tim *\" kind of queries? Google has this functionality where example query returns tim burton, tim hortons, etc.  ",
            "author": "Ahmet Arslan",
            "id": "comment-13934787"
        },
        {
            "date": "2014-03-14T13:07:28+0000",
            "content": "Y.  Make sure to set allowLeadingWildcard to true.\n\n\"t* *\" is parsed to:\nspanNear([SpanMultiTermQueryWrapper(name:t*), SpanMultiTermQueryWrapper(name:*)], 0, true)\n\nIf you want to see words that come immediately after \"tim\", you might want to take a look at LUCENE-5317, and if you want to see tf*idf weighted counts of words that come immediately after \"tim\" see LUCENE-5318.  I'll post screenshots to those today.  I'm in the process of refactoring both of those (code updates in two weeks?), and I'll start asking for help from committers on those once this issue is resolved....still lots to do here. ",
            "author": "Tim Allison",
            "id": "comment-13934964"
        },
        {
            "date": "2014-03-14T17:56:50+0000",
            "content": "But aren't Span queries going to get nuked?  See https://twitter.com/otisg/status/443227490753216512 ",
            "author": "Otis Gospodnetic",
            "id": "comment-13935335"
        },
        {
            "date": "2014-03-14T18:21:52+0000",
            "content": "Otis Gospodnetic, thank you for raising this point for discussion.  Yes, I acknowledged LUCENE-2878 in the original description of this issue, and that process has been ongoing since 2011.  My hope was that key SpanQuery functionality was going to be moved over to regular queries.  If this happens, I can modify this parser to handle those mods. \n\nWhatever functionality of Spans that is not moved over will have to disappear from this parser, and that could be painful depending on what functionality is not transitioned.\n\nWhen SpanQueries get nuked, what will happen to:\n1) in order and not in order phrases\n2) functionality of SpanNot\n3) searching for a phrase within a proximity of something \n\nI think those are the three main things that can't be handled by regular queries at this point. ",
            "author": "Tim Allison",
            "id": "comment-13935395"
        },
        {
            "date": "2014-03-17T10:00:51+0000",
            "content": "Robert Muir and community, given recent interest in LUCENE-2878, should we stop work on this? ",
            "author": "Tim Allison",
            "id": "comment-13937616"
        },
        {
            "date": "2014-03-17T10:02:19+0000",
            "content": "Tim I don't think so. I think actually it makes sense to have real current use cases for spans to ensure everything is really done correctly.\n\nThis is just my opinion. I've fallen behind on the issue only because I've been busy lately. ",
            "author": "Robert Muir",
            "id": "comment-13937617"
        },
        {
            "date": "2014-03-18T02:12:51+0000",
            "content": "Phew.  Thank you! ",
            "author": "Tim Allison",
            "id": "comment-13938693"
        },
        {
            "date": "2014-03-18T07:05:06+0000",
            "content": "Hey Tim,\nCan this parser handles query like \"hello world h*\"\nexpected results sentences like hello world hello,hello world hey etc.\nI am using ComplexPhraseQuery Parser now but not able to get expected results of the above query.\nThanks,\nAny help will be appreciated. ",
            "author": "Anonymous",
            "id": "comment-13938895"
        },
        {
            "date": "2014-03-18T23:45:35+0000",
            "content": "Y.  Make sure to set allowLeadingWildcard to true.  If you are using SOLR-5410, set ldwc = true.\n\nI quickly tested with ComplexQueryParser, and that appears to work too for your use case (again) if you set allowLeadingWildcard to true (straightforward in Lucene...not yet an available parameter in Solr, if I understand the recent commit correctly) ",
            "author": "Tim Allison",
            "id": "comment-13939977"
        },
        {
            "date": "2014-04-01T05:04:30+0000",
            "content": "Commit 1583533 from Robert Muir in branch 'dev/branches/lucene5205'\n[ https://svn.apache.org/r1583533 ]\n\nLUCENE-5205: Tim's test cleanup patch ",
            "author": "ASF subversion and git services",
            "id": "comment-13956110"
        },
        {
            "date": "2014-04-01T05:23:55+0000",
            "content": "Commit 1583537 from Robert Muir in branch 'dev/branches/lucene5205'\n[ https://svn.apache.org/r1583537 ]\n\nLUCENE-5205: merge trunk ",
            "author": "ASF subversion and git services",
            "id": "comment-13956120"
        },
        {
            "date": "2014-04-01T19:25:28+0000",
            "content": "Thank you, Robert!  Next steps: LUCENE-5470 and then LUCENE-5504? ",
            "author": "Tim Allison",
            "id": "comment-13956932"
        },
        {
            "date": "2014-06-02T13:35:38+0000",
            "content": "Robert Muir, would you have any interest/time to pick up work on this again?  Is there anything I can do to help?  Thank you! ",
            "author": "Tim Allison",
            "id": "comment-14015380"
        },
        {
            "date": "2014-06-10T20:25:28+0000",
            "content": "I'd like to extend this parser so it can be used to query positional joins, LUCENE-5627. This parser is a good fit for that because it provides span queries, and the positional joins are based on span queries, so integration should be doable.\n\nThis needs two changes in the parser here, one for the label-fragment join, and one for the label tree operations.\n\nFor the label-fragment join it would be necessary to allow a field in AbstractSpanQueryParser._parsePureSpanClause, basically at the point where it currently throws an exception \"Can't process field ...\". At that point a positional join query can be inserted to join from the new field to the original field of the span clause. This will have to be based on a  field schema that has the relations between the fields. This schema might also be used for indexing the documents.\n\nThe label tree will need an extension here to provide span queries in a label field that are based on the label tree info.\nThis is much the same as using the axes in XPath. I'd like to add the / for a named child, .. for parent, // for descendant-or-self, and maybe some form of child indexing.\n\nIt would be easier for me to try this from trunk rather than from the lucene5205 branch. ",
            "author": "Paul Elschot",
            "id": "comment-14026947"
        },
        {
            "date": "2014-06-10T20:36:41+0000",
            "content": "Interesting.  Sounds great to me.  How can I help?  What would the syntax of a query look like? ",
            "author": "Tim Allison",
            "id": "comment-14026958"
        },
        {
            "date": "2014-06-11T09:48:10+0000",
            "content": "How can I help?\n\nIn case you're familiar with git/github we could cooperate at github, otherwise we can post patches.\nShall I open another issue for this?\nCould you take a look at the javadocs of LUCENE-5627 ? I think that would help in understanding the connections to be made.\n\nWhat would the syntax of a query look like?\n\nThat is an open question to me. For the label-fragment joins this might work for example to find a label l1 in the lb: field that has a fragment in the fr: field containing f1:\n\nlb:[ l1 fr:f1]\n\nThis would add the join from the fr: field to the lb: field automatically.\nf1 might also be a nested query with brackets itself.\nThe allowed distance in the lb: field should normally be zero, the matches should be at the same label position.\nBasically that boils down to using something like SpanWithinQuery in the label field.\n\nThere are also some corner cases. For example this could make sense:\n\nlb:[ l1 fr:]\n\nto query for labels l1 that have a non empty fragment in the fr: field.\nThe other way around:\n\nfr:[ f1 lb:]\n\ndoes not make sense because labels are never empty.\n\nFor the label tree queries have a look here http://en.wikipedia.org/wiki/XPath#Axis_specifiers to get an idea of the possibilities.\nThe attribute axis is not needed, in the positional joins XML attributes and their values are indexed as fragments in separate fields.\nI don't know much about namespaces, so for the moment they are not needed either.\nAll the other axes might be implemented here, we could start by the ones I mentioned above: child by name, child by index, parent, descendant-or -self.\nThe slash / is used by XPath for the child axis, and here it is in use for regular expressions, so some other syntax might be needed here. ",
            "author": "Paul Elschot",
            "id": "comment-14027583"
        },
        {
            "date": "2014-06-11T16:42:49+0000",
            "content": "I'll take a look.  \n\nI'm not on github yet, but I've been meaning to get set up there, and this might give me reason to head that way.\n\nIs the plan to add a separate parser that requires modifications to SpanQueryParser (or its base classes) or should we add this new capability into SpanQueryParser? I guess it really doesn't matter.  The added capability is probably enough to open up a separate issue.\n\nShould we try to take care of LUCENE-5470 and LUCENE-5504 in trunk and the 4.x branch before we start aiming this towards trunk?  If possible, I'd like to address Robert's concerns about code duplication/bloat. ",
            "author": "Tim Allison",
            "id": "comment-14027992"
        },
        {
            "date": "2014-06-11T18:50:34+0000",
            "content": "Setting up github will take some time but I am not in a hurry, and git is worth your time.\n\nFor the label-fragment joins only a small change to the existing code might be necessary, basically an extra method to handle a field in  AbstractSpanQueryParser._parsePureSpanClause. That would throw an exception as it currently does, and hopefully only that would need to be overridden.\n\nFor the label tree queries I would hope that something like a getFieldQuery method could be overridden to do a special case for label fields. However, I have not looked that far into the code here yet.\n\nAnyway I'm hoping that you propose syntax extensions to the parser here, I lack the overview for that.\nFor now I'll concentrate on the field schema that should provide the positional joins to the query parser.\n\nOnce you're setup on github, I'll merge trunk, lucene5205, and the positional joins into a single branch so we can use that as a starting point.\nIf you prefer, we can also add the patches from LUCENE-5470 and LUCENE-5504 in that branch. ",
            "author": "Paul Elschot",
            "id": "comment-14028210"
        },
        {
            "date": "2014-06-12T18:17:30+0000",
            "content": "I merged branches lucene5205 (sha1 8a979e79) and trunk (sha1 d1274853) today locally, and got these conflicts in the test code in the queryparser module:\n\n    \tlucene/queryparser/src/test/org/apache/lucene/queryparser/classic/TestMultiAnalyzer.java\n    \tlucene/queryparser/src/test/org/apache/lucene/queryparser/complexPhrase/TestComplexPhraseQuery.java\n\nThe conflicts were resolvable, but I had to make some choices that I was not sure of. ",
            "author": "Paul Elschot",
            "id": "comment-14029543"
        },
        {
            "date": "2014-06-12T18:34:51+0000",
            "content": "Created LUCENE-5758 to extend SpanQueryParser with positional joins. ",
            "author": "Paul Elschot",
            "id": "comment-14029566"
        },
        {
            "date": "2014-06-12T19:41:49+0000",
            "content": "Sounds good.  I agree that a separate issue woud probably be best, and I think extending SpanQueryParser or one of its base classes might be the best route to go. We can make whatever changes are necessary in the base classes to make extension easier/actually work.  Things will get a bit more complicated if we have to change the lexer, which I think we will, but we can burn that bridge when we get to it.\n\nI looked over the javadocs last night, and wow, that is a great capability.  For new syntax, you're right / / are taken for regexes.  How about pipes to represent the hierarchical components: |a/b/c|\n\nI probably won't be on git until early next week...not as much extracurricular time available as I'd like, but I look forward to getting set up and collaborating on this.  \n\nI'd want to get a +1 from Robert Muir on LUCENE-5470 and LUCENE-5504 before making those changes to trunk/4.x, but I think that they would be quite useful.\n\nOn a related issue, I'm wondering if I should add FieldMaskingSpanQueries to the SpanOnlyParser.  The original reason that I required a single field was because I was using it for concordancing of a single field (LUCENE-5317)...re-analyzing to get character offsets for span hits.  I don't think that adding the FieldMaskingSpanQueries will wreck that.  Will need to take a look though. ",
            "author": "Tim Allison",
            "id": "comment-14029677"
        },
        {
            "date": "2014-06-15T10:17:25+0000",
            "content": "I'm wondering if I should add FieldMaskingSpanQueries to the SpanOnlyParser\n\nWhen two fields are indexed to allow a FieldMaskingSpanQuery (see  LUCENE-1494 for an example) such an addition makes sense:\n\nfield1:[ v1 field2:v2]\n\nHere the masking should be from field2 to field1.\n\nThere is a scoring issue for FieldMaskingSpanQuery, LUCENE-3723. So far I have avoided scoring in the label module...\n\nFor querying labeled fragments, a FieldMaskingSpanQuery should be used between two fragment fields that share their labeled positions, or when each fragment in one field consist of a single token. The first case happens in the label module for xml attribute names and attribute values. For the single token fragments case there is no special provision in the label module.\n\n ",
            "author": "Paul Elschot",
            "id": "comment-14031826"
        },
        {
            "date": "2014-06-18T19:31:19+0000",
            "content": "Paul Elschot, thanks to David Smiley's notes, I've at least pulled trunk via github and have that up and running (and David, yes, in Intellij!).  What are the next steps?  Will you create another branch in the main svn or will we work purely in github?  Is the overall plan to develop a monster patch/pull request or to make smallish modifications to trunk as we can?  Thank you, again. ",
            "author": "Tim Allison",
            "id": "comment-14036235"
        },
        {
            "date": "2014-06-18T20:56:15+0000",
            "content": "I cannot create branches in the main svn, we can work via github because that is integrated with issues.apache.org for lucene here.\n\nOne possible plan is to start from a merge between lucene5205 and recent trunk, and then add the positional joins there.\n(To locally see the branches available for tracking from upstream, use \"git ls-remote upstream\". To keep an overview of different local branches \"gitk\" has become indispensable for me.)\nI ran into a few small issues when merging the lucene5205 branch and a recent trunk (see above), so perhaps you can merge them in your local git repo first, resolve the conflicts, and then push that to your github repo.\nThat fits nicely at this issue, so you could create a pull request for that at this issue by mentioning LUCENE-5205 early in the pull request message.\n\nI have a field schema and a query factory more or less ready to be added to the positional joins. It should be possible to use these in the extended query parser. I'll push these additions to my github repo, and I'll add a pull request for that at LUCENE-5627.\n\nFrom that point we can use LUCENE-5758,  use your github repo and/or mine (git is distributed), and we can communicate with github pull requests and/or with comments and patches at LUCENE-5758.\n\nI normally do small commits locally (with \"git gui&\") and prepare these for pushing to github by using the squash option to git merge (from the command line, starting at a new branch at a trunk commit, and merging from the local feature branch that has the same trunk commit merged into it).\nThis squashing is quite similar to preparing a larger patch, and \"git diff --stat\" works nicely to check what is going on.\nIt might be good to base our pull requests on the same trunk commit, we'll see.\n\nThere may be easier ways to use git/github for this, in case someone knows one please let us know. ",
            "author": "Paul Elschot",
            "id": "comment-14036364"
        },
        {
            "date": "2014-06-30T12:01:11+0000",
            "content": "Your git or mine?   I think yours would be better. ",
            "author": "Tim Allison",
            "id": "comment-14047588"
        },
        {
            "date": "2014-06-30T15:39:22+0000",
            "content": "Any ideas on how to resolve the conflicts between lucene5205 and current trunk?\n\nAs you have probably seen, I updated LUCENE-5627 by posting a pull request on github from my github account to the main lucene-solr github repo.\n\nDo you have a github account? We could start by pushing to our own github repos and using 'git fetch' between them.\nOr I can add you on github as collaborator with write access here: https://github.com/PaulElschot/lucene-solr\nI have not done that before, so we'll see how that goes: https://help.github.com/articles/adding-collaborators-to-a-personal-repository ",
            "author": "Paul Elschot",
            "id": "comment-14047767"
        },
        {
            "date": "2014-07-18T18:10:34+0000",
            "content": "First attempt with github, merge from trunk to 5205.  Let me know if I did this backwards or otherwise botched it:\n\nhttps://github.com/apache/lucene-solr/pull/64  ",
            "author": "Tim Allison",
            "id": "comment-14066656"
        },
        {
            "date": "2014-07-19T16:07:19+0000",
            "content": "To have the github pull request message show up here, iirc one can add the lucene issue identier somewhere early in the message, see for example at LUCENE-5627.\n\nI did this locally:\ngit pull  https://github.com/tballison/lucene-solr lucene5205:lucene5205\n\nwhich ended in git commit 4d95fb5b69e667c0ec5d51bbe92096fe23d88f9c .\nThen, in directory lucene/queryparser, ant test failed to compile. This is the first error message:\n... lucene/queryparser/src/test/org/apache/lucene/queryparser/util/QueryParserTestBase.java:51: error: cannot find symbol\n    [javac] import org.apache.lucene.util.automaton.BasicAutomata;\n    [javac]                                        ^\n    [javac]   symbol:   class BasicAutomata\n\nAlso I think it would be better not to use lucene5205 as the branch name, because it is used in the upstream repository.\nShall we use for example lucene5205-ta and lucene5205-pe as branch names in our github repositories? ",
            "author": "Paul Elschot",
            "id": "comment-14067571"
        },
        {
            "date": "2014-07-19T18:15:45+0000",
            "content": "I wrote:\n\nShall we use for example lucene5205-ta and lucene5205-pe as branch names in our github repositories?\n\nActually, once we're done here solving the earlier merge conflict between lucene5205 and trunk, we can move to LUCENE-5758.\nSo the branch names could be lucene5758-ta and lucene5758-pe . ",
            "author": "Paul Elschot",
            "id": "comment-14067615"
        },
        {
            "date": "2014-07-21T15:01:49+0000",
            "content": "Agreed on way ahead.  \n\nLooks like I forgot to commit the few manual changes: BasicAutomata->Automata.\n\nI closed the broken pull request.\n\nLet me know if you have luck now.  Sorry about that! ",
            "author": "Tim Allison",
            "id": "comment-14068596"
        },
        {
            "date": "2014-07-22T17:09:59+0000",
            "content": "Unrelated to work on LUCENE-5758, I added a standalone package including a jar to track with current latest stable distro of Lucene here: \nhttps://github.com/tballison/lucene-addons/tree/master/lucene-5205\n\nFor trunk integration, see lucene-5205 branch of my fork on github. ",
            "author": "Tim Allison",
            "id": "comment-14070529"
        },
        {
            "date": "2014-07-22T18:56:23+0000",
            "content": "I pulled lucene5205 again as above, and merged in current trunk (commit afee841220c786055d28c95945646a7737a04d2a).\nThe tests in the lucene/queryparser module pass now.\nCould you make a pull request to here from your lucene5205 ? ",
            "author": "Paul Elschot",
            "id": "comment-14070697"
        },
        {
            "date": "2014-07-22T19:16:24+0000",
            "content": "Will do. I need to add the March 10 patch in as well (and remember to commit the changes!).  Do you mind if I roll in SOLR-5410? ",
            "author": "Tim Allison",
            "id": "comment-14070735"
        },
        {
            "date": "2014-07-22T20:07:34+0000",
            "content": "I don't mind rolling in SOLR-5410. ",
            "author": "Paul Elschot",
            "id": "comment-14070797"
        },
        {
            "date": "2014-07-24T16:02:13+0000",
            "content": "GitHub user tballison opened a pull request:\n\n    https://github.com/apache/lucene-solr/pull/68\n\n    Lucene5205\n\n    LUCENE-5205\n    1) merge from trunk\n    2) roll in March 10, 2014 LUCENE-5205 patch for improved stopword handling\n    3) roll in SOLR-5410\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/tballison/lucene-solr lucene5205\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/lucene-solr/pull/68.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #68\n\n\ncommit 3687d27902c3d993291a9f169f1c4a338c417327\nAuthor: Uwe Schindler <uschindler@apache.org>\nDate:   2014-06-11T17:50:45Z\n\n    SOLR-5940: post.jar reports back detailed error in case of error responses\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1601970 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 4f2da71473619def348518402cc567f429047cc0\nAuthor: Joel Bernstein <jbernste@apache.org>\nDate:   2014-06-11T19:35:19Z\n\n     SOLR-6150: Improving AnalyticsMergeStrategyTest\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1601997 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 109c4c47679a193ac3ca3a4a449d759dbad59725\nAuthor: shalin Shekhar Mangar <shalin@apache.org>\nDate:   2014-06-12T11:18:33Z\n\n    SOLR-6056: Don't publish recovery state until recovery runs to avoid overwhelming the overseer state queue\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602123 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit d553138492454798b9abeff7e610f0e8f3ddfb8b\nAuthor: Michael McCandless <mikemccand@apache.org>\nDate:   2014-06-12T11:54:20Z\n\n    fix typo\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602131 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 4dd3197621324234e77e741fd843c4d76df07719\nAuthor: Noble Paul <noble@apache.org>\nDate:   2014-06-12T12:18:21Z\n\n    SOLR-6048 the assert was not really failing the test\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602138 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 2cdb0941446628663849f56ffbe4b42c62d00e0c\nAuthor: Shai Erera <shaie@apache.org>\nDate:   2014-06-12T12:26:20Z\n\n    add comments to clarify code\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602140 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit d1274853919c1c9867e8e71117ff1303b6cc8816\nAuthor: shalin Shekhar Mangar <shalin@apache.org>\nDate:   2014-06-12T15:45:08Z\n\n    Fix typo, rf is actually 3 in code\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602210 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 0b9f7edd3109467052137004d36abb7f793e5835\nAuthor: Robert Muir <rmuir@apache.org>\nDate:   2014-06-12T19:40:36Z\n\n    LUCENE-5748: Add SORTED_NUMERIC docvalues type\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602277 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit e2f2c2fdaa77b4c17f6922fb9c5e25b02563855a\nAuthor: Uwe Schindler <uschindler@apache.org>\nDate:   2014-06-13T08:54:20Z\n\n    LUCENE-5754: Allow \"$\" as part of variable and function names in expressions module\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602344 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 40137f9162350a6281e0d3fba99898fd66be28b2\nAuthor: Adrien Grand <jpountz@apache.org>\nDate:   2014-06-13T11:39:43Z\n\n    LUCENE-5695: DocIdSet implements Accountable.\n\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602387 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit ccf0a812d1644e70b33157d5c33b34e78889f327\nAuthor: Simon Willnauer <simonw@apache.org>\nDate:   2014-06-13T11:41:19Z\n\n    LUCENE-5756: Implement Accountable from IndexWriter\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602388 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 0114c4e7292aa261996688b4f0813622d3ff99b3\nAuthor: Simon Willnauer <simonw@apache.org>\nDate:   2014-06-13T11:49:54Z\n\n    Add Import Layout Table to idea codestyle\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602389 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 0e92dc55e6293c26c020550742e2272547589df7\nAuthor: Robert Muir <rmuir@apache.org>\nDate:   2014-06-13T20:41:17Z\n\n    LUCENE-5757: move RamUsageEstimator reflector to test-framework\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602515 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 912e74424411c9055371924f403c0f66535c3066\nAuthor: Chris M. Hostetter <hossman@apache.org>\nDate:   2014-06-13T21:15:50Z\n\n    SOLR-5426: Fixed a bug in ReverseWildCardFilter that could cause InvalidTokenOffsetsException when highlighting\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602525 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit e9cb1382808cdd8f04dd837ce7fc473ed1e4a0b2\nAuthor: Robert Muir <rmuir@apache.org>\nDate:   2014-06-13T21:55:20Z\n\n    LUCENE-5760: Speed up BufferedIndexInput.randomAccessSlice\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602530 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 2df9ee28785e35e6e102b647b29e79660c10083f\nAuthor: shalin Shekhar Mangar <shalin@apache.org>\nDate:   2014-06-14T14:46:10Z\n\n    SOLR-6161: SolrDispatchFilter should throw java.lang.Error back even if wrapped in another exception\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602590 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 0b567b02453725acb47ed451b518a6103cc7a898\nAuthor: shalin Shekhar Mangar <shalin@apache.org>\nDate:   2014-06-14T14:57:30Z\n\n    SOLR-6153: ReplicationHandler backup response format should contain backup name\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602592 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 2ca52f0e5949f85427616ee2d3c6dfb836a80ff8\nAuthor: Alan Woodward <romseygeek@apache.org>\nDate:   2014-06-16T09:17:23Z\n\n    SOLR-6169: Properly remove deprecated CoreAdminHandler handleAlias action\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602825 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 9fe4454fdd1160a4df87b3f65512a00bf822c5d1\nAuthor: Robert Muir <rmuir@apache.org>\nDate:   2014-06-16T11:07:19Z\n\n    LUCENE-5762: Disable old codecs as much as possible\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602845 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 4525f7ab6e5289a1f5357a577b1fb5dca7f6d46f\nAuthor: Dawid Weiss <dweiss@apache.org>\nDate:   2014-06-16T12:17:57Z\n\n    SOLR-6151: Intermittent TestReplicationHandlerBackup failures.\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602854 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 7c506afc3bda28d056389e7679588cf8e6be0b94\nAuthor: David Wayne Smiley <dsmiley@apache.org>\nDate:   2014-06-16T12:38:43Z\n\n    LUCENE-5648: (NumberRangePrefixTree) Bug-fix in initIter optimization. Re-index required.\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602857 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 730c70a59d26f2bc5aedc474c317c277b4c14b9e\nAuthor: Robert Muir <rmuir@apache.org>\nDate:   2014-06-16T13:14:01Z\n\n    LUCENE-5761: Remove DiskDocValuesFormat\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602862 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit a55cbab679d5a88dbb36be7bf08d4dd27d39f602\nAuthor: Adrien Grand <jpountz@apache.org>\nDate:   2014-06-16T14:22:02Z\n\n    LUCENE-5759: Add PackedInts.unsignedBitsRequired.\n\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602873 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit c412a51bc386cb8636a96daaa4bf7ed4c63101bd\nAuthor: Adrien Grand <jpountz@apache.org>\nDate:   2014-06-16T14:25:12Z\n\n    LUCENE-5764: Add tests to DocIdSet.ramBytesUsed.\n\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602876 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 83c64d82de99a9180bb03f7648aca10412a513eb\nAuthor: Adrien Grand <jpountz@apache.org>\nDate:   2014-06-16T14:41:15Z\n\n    LUCENE-5765: Add tests to OrdinalMap.ramBytesUsed.\n\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602880 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit a28e98b142418347d260662e1b8c14f86536f9d5\nAuthor: Timothy Potter <thelabdude@apache.org>\nDate:   2014-06-16T16:44:15Z\n\n    SOLR-6157: Added some logging and re-opened the socket proxy to try to figure out why this test is hanging; reenabling temporarily to see if these changes help diagnose the cause of the hang.\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602924 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 5664fdd77a785b1c620527e1ecebb6659c457580\nAuthor: Timothy Potter <thelabdude@apache.org>\nDate:   2014-06-16T18:28:07Z\n\n    SOLR-6015: Moving change note to 4.9 bugfix section.\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602950 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit f233dd7698ea7f2134e86bc696868f7116d6140e\nAuthor: Timothy Potter <thelabdude@apache.org>\nDate:   2014-06-16T18:36:08Z\n\n    SOLR-5956: Use getInstanceDir instead of getRawInstanceDir as that was causing issues when trying to create a backup of an index on Linux when solr.solr.home is a symbolic link.\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602953 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit c1e8bb94e81eaa3c3188e81966ce311b624c0e96\nAuthor: Adrien Grand <jpountz@apache.org>\nDate:   2014-06-16T20:33:44Z\n\n    LUCENE-5767: OrdinalMap optimizations.\n\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1602997 13f79535-47bb-0310-9956-ffa450edef68\n\ncommit 2b985b580148ecb9f969a918c715f6488d92577b\nAuthor: Robert Muir <rmuir@apache.org>\nDate:   2014-06-16T21:56:05Z\n\n    LUCENE-5768: hunspell condition checks with character classes were buggy\n\n    git-svn-id: https://svn.apache.org/repos/asf/lucene/dev/trunk@1603007 13f79535-47bb-0310-9956-ffa450edef68\n\n ",
            "author": "ASF GitHub Bot",
            "id": "comment-14073323"
        },
        {
            "date": "2014-08-21T20:26:40+0000",
            "content": "I just tried the pull command:\ngit pull https://github.com/tballison/lucene-solr lucene5205\nfrom current trunk, commit 91dd8c4b1e430 .\n\nThis gave merge conflicts for TestComplexPhraseQuery.java and TestMultiAnalyzer.java, a.o. because of the recent removal of version arguments in the analysis code.\n\nAlso, the patch:\nhttps://github.com/apache/lucene-solr/pull/68.patch\ncontains some unrelated code, see the list of commits above.\n\nTim, could you resolve the conflicts and post a new pull request and/or patch?\nI'd like to have a starting point for LUCENE-5758. ",
            "author": "Paul Elschot",
            "id": "comment-14105906"
        },
        {
            "date": "2014-11-18T11:58:34+0000",
            "content": "I am trying following queries and facing an issue for which need your suggestions. The environment is 4 shard cluster with embedded zookeeper on one of them.\n\nq=field: (\"SEARCH TOOLS PROVIDER & CONSULTING COMPANY\") Gets transformed to following:\n+spanNear([field:search, field:tools, field:provider, field:, field:consulting, field:company], 0, true)\n\nfield: (\"SEARCH TOOL'S PROVIDER'S AND CONSULTING COMPANY\") Gets transformed to following:\n+spanNear([field:search, spanNear([field:s, field:provider], 0, true), field:s, field:and, field:consulting, field:company], 0, true)\n\nfield: (\"SEARCH TOOL'S SOLUTION PROVIDER TECHNOLOGY CO., LTD.\") Gets stuck and does not return. We have set query timeAllowed to 5 minutes but it seems that it is not reaching here and continues.\nDuring debug I found that it gets stuck at m.find(), Line 154 of SpanQueryLexer after it has created token for double quotes and term SEARCH.\n\nWhereas the above query without (') gets transformed to following\nfield: (\"SEARCH TOOLS SOLUTION PROVIDER TECHNOLOGY CO., LTD.\") => +spanNear([field:search, field:tools, field:solution, field:provider, field:technology, field:co, field:ltd], 0, true)\n\nNeed your help in understanding if I am not using the query properly or it can be an issue.\nNOTE: A space between the field: and query is added to avoid transformation to smileys. ",
            "author": "Modassar Ather",
            "id": "comment-14216113"
        },
        {
            "date": "2014-11-18T15:10:23+0000",
            "content": "Thank you for raising this, Modassar Ather.  The challenge is that the parser can use both \" and ' to mark the beginnings and endings of SpanNear.  As an initial hack, I was hoping that users would backslash single quotes within phrases, but that puts too much burden on users.  I'll see if I can add a bit more smarts so that if the parser knows that it is in a \" phrase, it will ignore ' and vice versa.  Are you using the my github standalone jars?  Or, how are you using this? ",
            "author": "Tim Allison",
            "id": "comment-14216267"
        },
        {
            "date": "2014-11-18T16:01:50+0000",
            "content": "Thanks Tim Allison for your response. I am using it from lucene5205 branch(http://svn.apache.org/repos/asf/lucene/dev/branches/lucene5205/) integrated as patch to latest Lucene core jar. ",
            "author": "Modassar Ather",
            "id": "comment-14216353"
        },
        {
            "date": "2014-11-18T16:54:49+0000",
            "content": "The permanent hang is surprising.  When I isolate the singlequote regex, I get a permanent hang in Java, but not Perl.\n\n\n      String s = \"SEARCH TOOL'S SOLUTION PROVIDER TECHNOLOGY CO., LTD\";\n      Matcher m = Pattern.compile(\"'((?:''|[^']+)+)'\").matcher(s);\n      while (m.find()) {\n          System.out.println(m.start());\n      }\n      System.out.println(\"done\");\n\n\n\n\nmy $s = \"SEARCH TOOL'S SOLUTION PROVIDER TECHNOLOGY CO., LTD\";\n\nwhile ($s =~/'((?:''|[^']+)+)'/g) {\n\tprint \"here\\n\";\n}\n\nprint \"done\\n\";\n\n ",
            "author": "Tim Allison",
            "id": "comment-14216415"
        },
        {
            "date": "2014-11-18T17:37:39+0000",
            "content": "Ha, turns out the hang isn't permanent, you just need to be patient.   The minimal code to reproduce this inefficiency:\n\n            String s = \"'S SOLUTION a PROVIDER TESTABCD\";\n            long start = new Date().getTime();\n            Matcher m = Pattern.compile(\"'(([^']+)+)'\").matcher(s);\n            while (m.find()) {\n                System.out.println(m.start());\n            }\n            System.out.println(\"elapsed:\" + (new Date().getTime()-start));\n\n\n\nWhen I ran this against different length strings, I got these times.  I did two runs for each string.\n\n\n\nString\n\tMILLIS_RUN1\n\tMILLIS_RUN2\n\n\n'S SOLUTION a PROVIDER TE\n\t937\n\t933\n\n\n'S SOLUTION a PROVIDER TES\n\t1671\n\t1310\n\n\n'S SOLUTION a PROVIDER TEST\n\t3165\n\t2643\n\n\n'S SOLUTION a PROVIDER TESTA\n\t5165\n\t5227\n\n\n'S SOLUTION a PROVIDER TESTAB\n\t9335\n\t9872\n\n\n'S SOLUTION a PROVIDER TESTABC\t\n19964\n\t18437\n\n\n'S SOLUTION a PROVIDER TESTABCD\n\t39387\n\t35961\n\n\n\n\n\nI fixed the regex inefficiency on my github site.  I set that up for standalone addons that track with the latest stable builds.\n\n I'll respond to your other issues shortly.  Thank you Modassar Ather for raising this issue! ",
            "author": "Tim Allison",
            "id": "comment-14216477"
        },
        {
            "date": "2014-11-18T18:41:54+0000",
            "content": "\nfield: (\"SEARCH TOOL'S PROVIDER'S AND CONSULTING COMPANY\") Gets transformed to following:\n +spanNear([field:search, spanNear([field:s, field:provider], 0, true), field:s, field:and, field:consulting, field:company], 0, true)\nUnfortunately, I can't think of a way around this.  In the SpanQueryParser, single quotes should be used to mark a token that should not be further parsed, i.e. '/files/a/b/c/path.html' should be treated as a string not a regex.  I toyed with requiring a space before the start ' and space after the ', but that seemed hacky.\n\nIf you escape your apostrophes, you should get the results you expect (this is with a whitespace analyzer, you may get different results with StandardAnalyzer):\n\n \"SEARCH TOOL\\\\'S SOLUTION PROVIDER\\\\'S TECHNOLOGY CO., LTD\"\n\nyields:\n\nf1:search f1:tool's f1:solution f1:provider's f1:technology f1:co., f1:ltd\n\n\n\nq=field: (\"SEARCH TOOLS PROVIDER & CONSULTING COMPANY\") Gets transformed to following:\n +spanNear([field:search, field:tools, field:provider, field:, field:consulting, field:company], 0, true)\nI think this is fixed on github.  What Analyzer chain are you using?\n ",
            "author": "Tim Allison",
            "id": "comment-14216571"
        },
        {
            "date": "2014-11-18T20:38:34+0000",
            "content": "Paul Elschot, I'm sorry for taking so long to get back to you.  I just merged trunk and made updates to my fork of the lucene5205 branch.  Let me know if that is of any use to you. ",
            "author": "Tim Allison",
            "id": "comment-14216757"
        },
        {
            "date": "2014-11-19T10:40:43+0000",
            "content": "Thanks Tim Allison for your response. I am using the SpanQuryParser and fix for query hanging issue from your github site as provided in your comment.\nI am using WhiteSpaceTokenizer.\n\nWith WhiteSpaceTokenizer:\nq=field: (\"SEARCH TOOLS PROVIDER & CONSULTING COMPANY\") still gets transformed to following:\n+spanNear([field:search, field:tools, field:provider, field:, field:consulting, field:company], 0, true)\n\nI am trying to find the possible cause of the removal of '&' in my config. ",
            "author": "Modassar Ather",
            "id": "comment-14217708"
        },
        {
            "date": "2014-11-19T11:54:34+0000",
            "content": "Good to hear the github workaround works.  If a committer has any interest in taking this on, it would be great to merge this into trunk...and then we could deprecate AnalyzingQueryParser, SurroundQueryParser and ComplexPhraseQueryParser just in time for 5.0. \n\nIn pure Lucene, with a WhitespaceAnalyzer, the '&' is still making it through the parsing process.\n\n\nspanNear([field:SEARCH, field:TOOLS, field:PROVIDER, field:&, field:CONSULTING, field:COMPANY], 0, true)\n\n\n\nWhen I use a StandardAnalyzer, the '&' is correctly dropped:\n\nspanNear([field:search, field:tools, field:provider, field:consulting, field:company], 1, true)\n\n\n\nWhat filters are you applying?  From your output, at least the LowerCaseFilterFactory, but anything else? ",
            "author": "Tim Allison",
            "id": "comment-14217793"
        },
        {
            "date": "2014-11-19T12:23:27+0000",
            "content": "It is solr.PatternReplaceFilterFactory in my analyzer chain which is replacing & with blank. Thanks for sharing the above details. ",
            "author": "Modassar Ather",
            "id": "comment-14217817"
        },
        {
            "date": "2014-11-19T12:26:32+0000",
            "content": "Ah, ok, so to confirm, no further action is required from me on the & issue?\n\nAre you ok with single quotes becoming operators?  Can you see a way of improving that behavior? ",
            "author": "Tim Allison",
            "id": "comment-14217823"
        },
        {
            "date": "2014-11-21T06:14:42+0000",
            "content": "Sorry for replying little late Tim Allison.\nAh, ok, so to confirm, no further action is required from me on the & issue?\nAs of now I see no issue with usage of & in the query as it is getting removed in my analyzer's chain.\n\nAre you ok with single quotes becoming operators? Can you see a way of improving that behavior?\nWe have been using double quotes with square brackets for phrase and nested phrase queries respectively. Have not used single quotes for the same. ",
            "author": "Modassar Ather",
            "id": "comment-14220574"
        },
        {
            "date": "2014-11-21T10:35:48+0000",
            "content": "Modassar Ather, no problem at all.  Thank you for your feedback.\n\nNote that there is a distinction between single and double quotes.  Double quotes and square brackets should be used for phrasal/near searches.  Single quotes are used for tokens that should not be parsed.\n\nFor example, if you wanted to search for a path '/the/quick/brown/fox.txt', you are telling the parser not to try to parse a regex within that term between the / and /.  To escape a single quote within a single quoted term, double it: \n\n'bob''s' \n\n\n\nis parsed as \n\nbob's\n\n\nThank you, again.  ",
            "author": "Tim Allison",
            "id": "comment-14220762"
        },
        {
            "date": "2014-12-02T11:37:49+0000",
            "content": "\nA query like following is throwing Exception.\nQuery: field: \"(term1* term2*) term3*\"~5 AND (field: (term4 OR term5 OR term6...termN)) where N > a couple of thousand e.g 6000-8000.\nException:\nException in thread \"main\" java.lang.StackOverflowError\n\tat java.util.regex.Pattern$GroupHead.match(Pattern.java:4554)\n\tat java.util.regex.Pattern$Loop.match(Pattern.java:4683)\n\tat java.util.regex.Pattern$GroupTail.match(Pattern.java:4615)\n\tat java.util.regex.Pattern$BranchConn.match(Pattern.java:4466)\n\tat java.util.regex.Pattern$CharProperty.match(Pattern.java:3694)\n\tat java.util.regex.Pattern$Branch.match(Pattern.java:4502)\n\tat java.util.regex.Pattern$GroupHead.match(Pattern.java:4556)\n\tat java.util.regex.Pattern$Loop.match(Pattern.java:4683)\n\nKindly let me know if I am using query in a wrong way.\nNOTE: A space between the field: and query is added to avoid transformation to smileys. ",
            "author": "Modassar Ather",
            "id": "comment-14231355"
        },
        {
            "date": "2014-12-02T13:39:39+0000",
            "content": "Will look into it.  To confirm, you have a query with 6000-8000 terms that you are OR'ing together?\n\nDavid Smiley, do I remember correctly that you recently added a query type for efficiently searching lots of terms? ",
            "author": "Tim Allison",
            "id": "comment-14231476"
        },
        {
            "date": "2014-12-02T14:32:59+0000",
            "content": "On the Solr end, I added a \"terms\" QParser that in turn uses one of the 4 differing options at the Lucene layer.  The option likely most suitable is a Lucene TermsFilter. ",
            "author": "David Smiley",
            "id": "comment-14231544"
        },
        {
            "date": "2014-12-03T03:54:45+0000",
            "content": "Tim Allison, yes the query have around 7500 OR'ed terms.  ",
            "author": "Modassar Ather",
            "id": "comment-14232537"
        },
        {
            "date": "2014-12-03T13:30:22+0000",
            "content": "David Smiley, thank you.  I'm sorry that I had my wires slightly crossed.  TermsFilter looks like the right candidate.\n\nModassar Ather, thank you for raising this issue.  I've made the fix on my github site.  I tested the parser with that many terms, and it is horrifically slow.  I'd recommend using a TermsFilter in combination with the SpanQuery and not trying to feed that many terms into the parser.\n\nThat said, I think that it might be time to redo the lexer and get rid of regexes, especially given the number of problems that you've found with their inefficiencies.\n\nIf someone with some JFlex/Antlr experience wants to beat me to a solution, I'd be very grateful! ",
            "author": "Tim Allison",
            "id": "comment-14232992"
        },
        {
            "date": "2014-12-03T19:44:27+0000",
            "content": "Modassar Ather et al., in refactoring the lexer, I'd like to make the following trivial syntax changes:\n\n\n\tto escape / in a regex, double it (as we currently do with single quotes within a single quoted run)\n\tto quote a term (i.e. treat this term as a literal), use single quote only (the current parser allows the option of double quotes, but this adds more code than I'd like).\n\tto quote a string that should be sent to a field without an analyzer (i.e. date fields in Solr), use single quotes.  Currently, the user has the option of using single or double quotes.\n\n\n\nIf there are any problems with the above, or if you'd like to see other changes, let me know. New lexer integration probably won't be ready for a few weeks, but it is far faster than the old lexer on large queries. ",
            "author": "Tim Allison",
            "id": "comment-14233416"
        },
        {
            "date": "2015-06-18T07:03:02+0000",
            "content": "Hi [~tallison@mitre.org]\nI migrated to Lucene/Solr 5.2.0 from Lucene/Solr 5.1.0. Lucene/Solr 5.2.0 has many changes related to Spans.\nWhile testing the SpanQueryParser I found a query like below is causing exception.\n\n Query: ft:\"([term1 term2]~>1 [term3 term4]~>1)\" \n\n\n Exception: \"msg\": \"Error from server at http:/<host>:8983/solr/collection: Less than 2 subSpans.size():1\", \n\nThe exception is triggering from org.apache.lucene.search.spans.ConjunctionSpans line number 38.\nKindly provide your suggestions. ",
            "author": "Modassar Ather",
            "id": "comment-14591367"
        },
        {
            "date": "2015-06-18T10:34:27+0000",
            "content": "Adding one more query for better clarification.\n\n\"[(term1 [term2 term3]~>1 [term4 term5]~>1) (term6 [term7 term8]~>1 term9)]~>2 (term10 [term11 term12])\"~4\n ",
            "author": "Modassar Ather",
            "id": "comment-14591616"
        },
        {
            "date": "2015-06-18T11:45:19+0000",
            "content": "While debugging I found following.\n\n\nfl:\"[term1 term2]~1 [term3 term4]~1\" is getting parsed as following:\nspanNear([spanNear([fl:term1, fl:term2], 1, false), spanNear([fl:term3, fl:term4], 1, false)], 0, true)\n\n\n\nfl:\"([term1 term2]~1 [term3 term4]~1)\"  is getting parsed as following:\nspanNear([spanOr([spanNear([fl:term1, fl:term2], 1, false), spanNear([fl:term3, fl:term4], 1, false)])], 0, true)\n\n\nThe second query is causing the exception as mentioned in my previous comments. As per my understanding it is the final SpanNear which gets an SpanOr which is only one span where as the SpanNear expects two subSpans.\nThe internal SpanNear gets this test passed as they have two sub-spans but the final SpanNear is failing.  \n\ne.g. spanNear([fl:term1, fl:term2], 1, false)\n\n\nPlease let me know your suggestions. ",
            "author": "Modassar Ather",
            "id": "comment-14591683"
        },
        {
            "date": "2015-06-18T12:14:09+0000",
            "content": "Wait, you're actually still using this?!  Ha!  That's great to hear.  I'll take a look.  Thank you for reporting this. ",
            "author": "Tim Allison",
            "id": "comment-14591700"
        },
        {
            "date": "2015-06-19T09:06:50+0000",
            "content": "Hi [~tallison@mitre.org]\nDid you get a chance to look at the issue? Please look into it and let me know your inputs. ",
            "author": "Modassar Ather",
            "id": "comment-14593237"
        },
        {
            "date": "2015-06-21T01:03:19+0000",
            "content": "Looking at it now.  I think I can add a step in the builder that will prevent a near clause from being created if there is only one child.  I'll give that a shot and see if there are unintended consequences.\n\nI just started a solr-collab-5x branch on my github site for this.\n\nOut of curiosity,  what code were you running that actually worked with 5.1?  I see that there are quite a few smallish changes that I'm having to make to my 4.x code.   ",
            "author": "Tim Allison",
            "id": "comment-14594900"
        },
        {
            "date": "2015-06-22T03:52:02+0000",
            "content": "I also had to do few changes during the migration from 4.x to 5.x in SpanQueryParser related code. I am looking forward for the fix.\nThanks for your help. ",
            "author": "Modassar Ather",
            "id": "comment-14595323"
        },
        {
            "date": "2015-06-22T07:01:17+0000",
            "content": "Adding one more test which will cause the issue.\n\"term1 <any stop word>\" E.g. \"plug in\" ",
            "author": "Modassar Ather",
            "id": "comment-14595442"
        },
        {
            "date": "2015-06-22T10:51:11+0000",
            "content": "Fixed in solr-collab-5x branch. \n\nOver the next week, I hope to finish the implementation of the new lexer (get rid of the ugly regex) and do some general clean up.\n\nModassar Ather, thank you for reporting this.  Let me know if there are any other surprises.\n\nOh, and on your most recent post about the stopwords, y, that caused at least one unit test to blow up with 5.2.1.  Thank you, again. ",
            "author": "Tim Allison",
            "id": "comment-14595763"
        },
        {
            "date": "2015-06-22T13:37:35+0000",
            "content": "There are now three branches for LUCENE-5205 and SOLR-5410 to track with 4.x, 5.x and trunk:\n\n1.solr-collab tracks with Lucene/Solr 4.x\n2.solr-collab-5x tracks with Lucene/Solr 5.x\n3.master tracks with Lucene/Solr 6.0.0-SNAPSHOT\n\nWill update this ticket with the new Lexer is ready. ",
            "author": "Tim Allison",
            "id": "comment-14595919"
        },
        {
            "date": "2015-06-22T20:28:30+0000",
            "content": "Cleaned up new lexer in lexer2 branch.  Pre-release compiled jars are available here ",
            "author": "Tim Allison",
            "id": "comment-14596593"
        },
        {
            "date": "2015-06-23T04:45:12+0000",
            "content": "Hi [~tallison@mitre.org] \nDid you mean the fix for the failing query with exception (Less than 2 subSpans.size() : 1) ? ",
            "author": "Modassar Ather",
            "id": "comment-14597157"
        },
        {
            "date": "2015-06-23T12:19:20+0000",
            "content": "Y, you should be good to go.  The basic code that you were using (with the old lexer) is available on the 5x branch.\n\nThe dev code with the new lexer for 5x is available as a pre-release or the on the lexer2 branch. ",
            "author": "Tim Allison",
            "id": "comment-14597567"
        },
        {
            "date": "2015-06-23T12:23:23+0000",
            "content": "Now that I'm back into this a bit, it looks like there have been some very cool things going on with Spans. \n\nIf there's any interest in adding syntax for some of the SpanQueries that aren't yet covered by this parser (e.g. FieldMaskingSpanQuery, SpanFirstQuery, SpanPositionRangeQuery, SpanWithQuery or SpanContainingQuery) let me know. ",
            "author": "Tim Allison",
            "id": "comment-14597573"
        },
        {
            "date": "2015-06-25T10:06:46+0000",
            "content": "Thanks [~tallison@mitre.org]\nWill integrate and start using it. ",
            "author": "Modassar Ather",
            "id": "comment-14600957"
        },
        {
            "date": "2015-09-08T06:56:34+0000",
            "content": "Hi [~tallison@mitre.org]\n\nThere is a document with following content in it which is indexed and stored.\n\nabout 2% growth\n\n\nIf following query is searched and the matched terms are highlighted then all the three terms of the document is highlighting.\nQuery: \n\n\"(growth* [term 2]) (about*)\"~2\n\nHighlighted text : \n\n<str><em>about</em> <em>2%</em> <em>growth</em></str>\n\n\nI tried to debug and found that scorer.getFieldWeightedSpanTerms() has entries for all the terms in it at PositionSpan (0, 2).\nPlease help me understand \n\n\tIf it is an issue?\n\tWhy \"term\" and \"2\" is present in the span terms although it should not match a document?\n\tWhy 2% is getting highlighted?\n\n\n\nRegards,\nModassar ",
            "author": "Modassar Ather",
            "id": "comment-14734335"
        },
        {
            "date": "2015-09-08T15:33:22+0000",
            "content": "This looks like a genuine issue in the Highlighter.  I was hoping that it was LUCENE-5503 so that would get some attention, but I don't think it is.\n\nThis is the minimal code to show the problem:\n\n  @Test\n  public void testEmbeddedSpanNearHighlighterIssue() throws Exception {\n    String field = \"f\";\n    Analyzer analyzer = new StandardAnalyzer();\n    String text = \"b c d\";\n\n//    SpanQueryParser p = new SpanQueryParser(field, analyzer);\n//    Query q = p.parse(\"\\\"(b [c z]) d\\\"~2\");\n    SpanQuery cz = new SpanNearQuery(\n        new SpanQuery[]{\n            new SpanTermQuery(new Term(field, \"c\")),\n            new SpanTermQuery(new Term(field, \"z\"))\n        }, 0, true\n    );\n    SpanQuery bcz = new SpanOrQuery(\n        new SpanTermQuery(new Term(field, \"b\")),\n            cz);\n    SpanQuery q = new SpanNearQuery(\n        new SpanQuery[]{\n            bcz,\n            new SpanTermQuery(new Term(field, \"d\"))\n        }, 2, false\n    );\n    QueryScorer scorer = new QueryScorer(q, field);\n    scorer.setExpandMultiTermQuery(true);\n\n\n    Fragmenter fragmenter = new SimpleFragmenter(1000);\n\n    Highlighter highlighter = new Highlighter(\n        new SimpleHTMLFormatter(),\n        new SimpleHTMLEncoder(),\n        scorer);\n    highlighter.setTextFragmenter(fragmenter);\n    String[] snippets = highlighter.getBestFragments(analyzer,\n        field, text,\n        3);\n    assertEquals(1, snippets.length);\n    assertFalse(snippets[0].contains(\"<B>c</B>\"));\n  }\n\n\n\nThis problem does not happen if \"c\" comes before \"a\" or after \"d\" in the text: \"c b d\" or \"b d c\". ",
            "author": "Tim Allison",
            "id": "comment-14734997"
        },
        {
            "date": "2015-09-08T18:56:38+0000",
            "content": "Can you please output what the output is now and what you expect it to be? ",
            "author": "David Smiley",
            "id": "comment-14735398"
        },
        {
            "date": "2015-09-10T06:21:31+0000",
            "content": "Please let me know if a new bug needs to be opened for this issue? ",
            "author": "Modassar Ather",
            "id": "comment-14738250"
        },
        {
            "date": "2015-09-10T17:11:39+0000",
            "content": "Y, I added it in a test case over on LUCENE-6796.  I'd expect: <B>b</B> c <B>d</B>, not <B>b</B> <B>c</B> <B>d</B>. ",
            "author": "Tim Allison",
            "id": "comment-14739156"
        },
        {
            "date": "2015-09-10T17:12:30+0000",
            "content": "I just opened LUCENE-6796 for this.  Thank you for raising it! ",
            "author": "Tim Allison",
            "id": "comment-14739159"
        },
        {
            "date": "2015-09-10T17:14:06+0000",
            "content": "Given that the momentum has disappeared for this parser, should I resolve this as \"won't fix\" and leave a pointer to github or should I leave the issue open? ",
            "author": "Tim Allison",
            "id": "comment-14739167"
        },
        {
            "date": "2015-09-11T04:11:25+0000",
            "content": "A fix for this issue will be a great help. Please provide your comments. ",
            "author": "Modassar Ather",
            "id": "comment-14740137"
        },
        {
            "date": "2015-09-11T11:32:24+0000",
            "content": "LUCENE-5205 or LUCENE-6796? ",
            "author": "Tim Allison",
            "id": "comment-14740611"
        },
        {
            "date": "2015-09-14T06:00:26+0000",
            "content": "Got the point. I think LUCENE-6796 will take care of this issue. ",
            "author": "Modassar Ather",
            "id": "comment-14742980"
        },
        {
            "date": "2015-10-05T06:07:01+0000",
            "content": "Hi [~tallison@mitre.org]\n\nThe patch of this feature is failing to build with compilation error with Lucene/Solr-5.3.1.\nI am trying to resolve it locally. public void fillBytesRef() method from TermToBytesRefAttribute.java has been removed.\nKindly look into it.\n\nThanks,\nModassar ",
            "author": "Modassar Ather",
            "id": "comment-14942956"
        },
        {
            "date": "2015-10-05T12:47:02+0000",
            "content": "Y, ha, upgraded code last week locally.  I'll push to a new lucene5.3on-0.1 branch shortly. ",
            "author": "Tim Allison",
            "id": "comment-14943324"
        },
        {
            "date": "2015-10-05T12:51:27+0000",
            "content": "Let me know if there are any surprise with lucene5.3on-0.1. \n\nThe solr-5410 (Solr parser wrapper for the SpanQueryParser works), but I haven't yet upgraded solr-5411 (Solr level concordance wrapper). ",
            "author": "Tim Allison",
            "id": "comment-14943327"
        },
        {
            "date": "2015-12-01T12:42:29+0000",
            "content": "Hi [~tallison@mitre.org]\n\nKindly let me know how to escape special character in SpanQueryParser. From the discussion above it seems that single quote is used. I thinks I am missing some understanding about it as normally \\ is used to escape special character.\ne.g \"understanding (span query)\"  \nTo escape brackets in above query what is the right character. I want to match the phrase where brackets are not considered as special character.\n\nThanks,\nModassar ",
            "author": "Modassar Ather",
            "id": "comment-15033625"
        },
        {
            "date": "2015-12-01T16:08:06+0000",
            "content": "Let's say you are using a whitespace tokenizer and you're trying to find (span, you'd surround that term with '' as in '(span'.\n\nTo search for the phrase above:\n\"understanding '(span' 'query)'\"\n\n\nIf your target token has a ' in it, (sp'an, double the token's single quotes: '(sp''an'\n ",
            "author": "Tim Allison",
            "id": "comment-15033952"
        },
        {
            "date": "2015-12-03T06:36:22+0000",
            "content": "Hi [~tallison@mitre.org]\n\nWildard * in double quotes is getting eaten up by the analyzer. E.g. f:\"term*\" is getting parsed to (+f:term). What I understand the wildcard queries are not analyzed.\nI noticed that the same works fine within a phrase. E.g. f:\"term1 term*\". Please let me know if it is an issue or it should not be used the way it is in the example i.e a single wildcard term within double quotes.\n\nThanks,\nModassar ",
            "author": "Modassar Ather",
            "id": "comment-15037358"
        },
        {
            "date": "2015-12-03T15:27:06+0000",
            "content": "Definitely a bug.  Thank you for reporting.  I was trying to allow the legacy behavior of double quotes around a single term (as determined by whitespace), but I think I should back off and require the use of single quotes for the classic parser's double quotes around a single term.\n\nThe fix for this example is easy, but I'd like to clean up other items related to this and add more tests.  Should have fix by tomorrow afternoon EDT.   ",
            "author": "Tim Allison",
            "id": "comment-15037929"
        },
        {
            "date": "2015-12-03T15:56:40+0000",
            "content": "Plan b... you can also escape with {{}}\n\n\"understanding (span query)\"\n\nor for a token sp'an:\n\n\"understanding (sp\\'an query)\"\n\nWas this not working for you? ",
            "author": "Tim Allison",
            "id": "comment-15037979"
        },
        {
            "date": "2015-12-03T16:23:28+0000",
            "content": "Thanks for your response. Just thought of checking if you are planning to add the fix to previous Solr/Lucene version too. I think a patch will be helpful. ",
            "author": "Modassar Ather",
            "id": "comment-15038025"
        },
        {
            "date": "2015-12-03T16:27:54+0000",
            "content": "Y.  Will backport for all 5.x.  Is there any need to make the fix in 4.x? ",
            "author": "Tim Allison",
            "id": "comment-15038032"
        },
        {
            "date": "2015-12-03T16:35:54+0000",
            "content": "Thanks for the quick response. Currently I am using 5.2.1 and will not be going back to older version. ",
            "author": "Modassar Ather",
            "id": "comment-15038049"
        },
        {
            "date": "2015-12-03T16:59:31+0000",
            "content": "Sorry for posting the same message more than one time. Some how the add button was not responding may be due to slow network which caused this. ",
            "author": "Modassar Ather",
            "id": "comment-15038100"
        },
        {
            "date": "2015-12-07T05:01:20+0000",
            "content": "Hi [~tallison@mitre.org]\n\nKindly share any update on the fix if any.\n\nThanks,\nModassar ",
            "author": "Modassar Ather",
            "id": "comment-15044377"
        },
        {
            "date": "2015-12-07T17:24:59+0000",
            "content": "Fixed in lexer2 branch, which is the most recent 5.0-5.2 branch and in lucene5.3on-0.1.\n\nThe current fix effectively ignores double quotes around a single term that is analyzed to a single term.  The user needs to use single quotes or use \\ to escape multiterm operators that should not be parsed (e.g. 'term*'...find the five letter term that ends with an asterisk...not a prefix query for words starting with term). ",
            "author": "Tim Allison",
            "id": "comment-15045288"
        },
        {
            "date": "2015-12-07T17:38:29+0000",
            "content": "Ticket is getting too long.  Development has moved to github, and I'll actively maintain it and respond to bug reports there:  https://github.com/tballison/lucene-addons.  \n\nIf a committer has an interest in incorporating this into Lucene proper, I'll be more than happy to collaborate on a new ticket.\n\nThank you, Robert Muir for all of your work on this!  Thank you, Paul Elschot for your many contributions, as well! ",
            "author": "Tim Allison",
            "id": "comment-15045315"
        },
        {
            "date": "2016-05-20T17:28:15+0000",
            "content": "Thanks to Luke Nezda for opening a request and for suggesting syntax to add SpanPositionRangeQueries to the parser.  These are now available in the 5.5-0.2 branch. ",
            "author": "Tim Allison",
            "id": "comment-15293781"
        },
        {
            "date": "2016-08-11T11:59:17+0000",
            "content": "Now available via Maven central.\n\n<!-- https://mvnrepository.com/artifact/org.tallison.lucene/lucene-5205 -->\n<dependency>\n    <groupId>org.tallison.lucene</groupId>\n    <artifactId>lucene-5205</artifactId>\n    <version>6.1-0.2</version>\n</dependency>\n\nIf anyone wants to help change the namespace back to org.apache.lucene, let me know.  ",
            "author": "Tim Allison",
            "id": "comment-15417095"
        }
    ]
}