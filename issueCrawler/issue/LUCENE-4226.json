{
    "id": "LUCENE-4226",
    "title": "Efficient compression of small to medium stored fields",
    "details": {
        "labels": "",
        "priority": "Trivial",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.1",
            "6.0"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "I've been doing some experiments with stored fields lately. It is very common for an index with stored fields enabled to have most of its space used by the .fdt index file. To prevent this .fdt file from growing too much, one option is to compress stored fields. Although compression works rather well for large fields, this is not the case for small fields and the compression ratio can be very close to 100%, even with efficient compression algorithms.\n\nIn order to improve the compression ratio for small fields, I've written a StoredFieldsFormat that compresses several documents in a single chunk of data. To see how it behaves in terms of document deserialization speed and compression ratio, I've run several tests with different index compression strategies on 100,000 docs from Mike's 1K Wikipedia articles (title and text were indexed and stored):\n\n\tno compression,\n\tdocs compressed with deflate (compression level = 1),\n\tdocs compressed with deflate (compression level = 9),\n\tdocs compressed with Snappy,\n\tusing the compressing StoredFieldsFormat with deflate (level = 1) and chunks of 6 docs,\n\tusing the compressing StoredFieldsFormat with deflate (level = 9) and chunks of 6 docs,\n\tusing the compressing StoredFieldsFormat with Snappy and chunks of 6 docs.\n\n\n\nFor those who don't know Snappy, it is compression algorithm from Google which has very high compression ratios, but compresses and decompresses data very quickly.\n\n\nFormat           Compression ratio     IndexReader.document time\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nuncompressed     100%                  100%\ndoc/deflate 1     59%                  616%\ndoc/deflate 9     58%                  595%\ndoc/snappy        80%                  129%\nindex/deflate 1   49%                  966%\nindex/deflate 9   46%                  938%\nindex/snappy      65%                  264%\n\n\n\n(doc = doc-level compression, index = index-level compression)\n\nI find it interesting because it allows to trade speed for space (with deflate, the .fdt file shrinks by a factor of 2, much better than with doc-level compression). One other interesting thing is that index/snappy is almost as compact as doc/deflate while it is more than 2x faster at retrieving documents from disk.\n\nThese tests have been done on a hot OS cache, which is the worst case for compressed fields (one can expect better results for formats that have a high compression ratio since they probably require fewer read/write operations from disk).",
    "attachments": {
        "LUCENE-4226.patch": "https://issues.apache.org/jira/secure/attachment/12536682/LUCENE-4226.patch",
        "SnappyCompressionAlgorithm.java": "https://issues.apache.org/jira/secure/attachment/12536683/SnappyCompressionAlgorithm.java",
        "CompressionBenchmark.java": "https://issues.apache.org/jira/secure/attachment/12536684/CompressionBenchmark.java"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-07-16T18:05:28+0000",
            "content": "Patch (applies against trunk and does not include the snappy codec).\n\nSee org.apache.lucene.codecs.compressing.CompressedStoredFieldsFormat javadocs for the format description.\n\nCompressionBenchmark.java and SnappyCompressionAlgorithm.java are the source files I used to compute differences in compression ratio and speed. To run it, you will need snappy-java from http://code.google.com/p/snappy-java/.\n\nThe patch is currently only for testing purposes: it hasn't been tested well and duplicates code from Lucene40's StoredFieldFormat. ",
            "author": "Adrien Grand",
            "id": "comment-13415464"
        },
        {
            "date": "2012-08-29T00:00:57+0000",
            "content": "New patch as well as the code I used to benchmark.\n\nDocuments are still compressed into chunks, but I removed the ability to select the compression algorithm on a per-field basis in order to make the patch simpler and to handle cross-field compression.\n\nI also added an index in front of compressed data using packed ints, so that uncompressors can stop uncompressing when enough data has been uncompressed.\n\nThe JDK only includes a moderately fast compression algorithm (deflate), but for this kind of use-case, we would probably be more interested in fast compression and uncompression algorithms such as LZ4 (http://code.google.com/p/lz4/) or Snappy (http://code.google.com/p/snappy/). Since lucene-core has no dependency, I ported LZ4 to Java (included in the patch, see o.a.l.util.compress).\n\nLZ4 has a very fast uncompressor and two compression modes\u00a0:\n\n\tfast scan, which looks for the last offset in the stream that has at least 4 common bytes (using a hash table) and adds a reference to it,\n\thigh compression, which looks for the last 256 offsets in the stream that have at least 4 common bytes, takes the one that has the longest common sequence, and then performs trade-offs between overlapping matches in order to improve the compression ratio.\n\n\n\n(In case you are curious about LZ4, I did some benchmarking with other compression algorithms in http://blog.jpountz.net/post/28092106032/wow-lz4-is-fast, unfortunately the high-compression Java impl is not included in the benchmark.)\n\nI ran a similar benchmark as for my first patch, but this time I only compressed and stored the 1kb text field (the title field being too small was unfair for document-level compression with deflate). Here are the results\u00a0:\n\n\nFormat           Chunk size  Compression ratio     IndexReader.document time\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nuncompressed                               100%                         100%\ndoc/deflate 1                               58%                         579%\ndoc/deflate 9                               57%                         577%\nindex/deflate 1          4K                 50%                        1057%\nindex/deflate 9          4K                 48%                        1037%\nindex/lz4 scan           4K                 70%                         329%\nindex/lz4 hc             4K                 66%                         321%\nindex/deflate 1           1                 60%                         457%\nindex/deflate 9           1                 59%                         454%\nindex/lz4 scan            1                 81%                         171%\nindex/lz4 hc              1                 79%                         176%\n\n\n\nNOTE: chunk size = 1 means that there was only one document in the chunk (there is a compress+flush every time the byte size of documents is >= the chunk size).\n\nNOTE: these number have been computed with the whole index fitting in the I/O cache. The performance should be more in favor of the compressing formats as soon as the index does not fit in the I/O cache anymore.\n\nThere are still a few nocommits in the patch, but it should be easy to get rid of them. I'd be very happy to have some feedback.  ",
            "author": "Adrien Grand",
            "id": "comment-13443676"
        },
        {
            "date": "2012-08-29T07:16:29+0000",
            "content": "Very cool. I skimmed through the patch, didn't look too carefully. This caught my attention:\n\n+  /**\n+   * Skip over the next <code>n</code> bytes.\n+   */\n+  public void skipBytes(long n) throws IOException {\n+    for (long i = 0; i < n; ++i) {\n+      readByte();\n+    }\n+  }\n\n\n\nyou may want to use an array-based read here if there are a lot of skips; allocate a static, write-only buffer of 4 or 8kb once and just reuse it. A loop over readByte() is nearly always a performance killer, I've been hit by this too many times to count.\n\nAlso, lucene/core/src/java/org/apache/lucene/codecs/compressing/ByteArrayDataOutput.java \u2013 there seems to be a class for this in\norg.apache.lucene.store.ByteArrayDataOutput? ",
            "author": "Dawid Weiss",
            "id": "comment-13443866"
        },
        {
            "date": "2012-08-29T08:14:04+0000",
            "content": "but I removed the ability to select the compression algorithm on a per-field basis in order to make the patch simpler and to handle cross-field compression.\n\nMaybe it is worth to keep it there for really short fields. Those general compression algorithms are great for bigger amounts of data, but for really short fields there is nothing like per field compression.   \nThinking about database usage, e.g. fields with low cardinality, or fields with restricted symbol set (only digits in long UID field for example).  Say zip code, product color...  is perfectly compressed using something with static dictionary approach (static huffman coder with escape symbol-s, at bit level, or plain vanilla dictionary lookup), and both of them are insanely fast and compress heavily. \n\nEven trivial utility for users is easily doable, index data without compression, get the frequencies from the term dictionary-> estimate e.g. static Huffman code table and reindex with this dictionary.  ",
            "author": "Eks Dev",
            "id": "comment-13443897"
        },
        {
            "date": "2012-08-29T10:01:54+0000",
            "content": "Thanks Dawid and Eks for your feedback!\n\nallocate a static, write-only buffer of 4 or 8kb once and just reuse it\n\nRight, sounds like a better default impl!\n\nByteArrayDataOutput.java \u2013 there seems to be a class for this in org.apache.lucene.store.ByteArrayDataOutput?\n\nI wanted to reuse this class, but I needed something that would grow when necessary (oal.store.BADO just throws an exception when you try to write past the end of the buffer). I could manage growth externally based on checks on the buffer length and calls to ArrayUtil.grow and BADO.reset but it was just as simple to rewrite a ByteArrayDataOutput that would manage it internally...\n\nMaybe it is worth to keep it there for really short fields. Those general compression algorithms are great for bigger amounts of data, but for really short fields there is nothing like per field compression.  Thinking about database usage, e.g. fields with low cardinality, or fields with restricted symbol set (only digits in long UID field for example). Say zip code, product color... is perfectly compressed using something with static dictionary approach (static huffman coder with escape symbol-s, at bit level, or plain vanilla dictionary lookup), and both of them are insanely fast and compress heavily.\n\nRight, this is exactly why I implemented per-field compression first. Both per-field and cross-field compression have pros and cons. Cross-field compression allows less fine-grained tuning but on the other hand it would probably be a better default since the compression ratio would be better out of the box. Maybe we should implement both?\n\nI was also thinking that some codecs such as this kind of per-field compression, but maybe even the bloom, memory, direct and pulsing postings formats might deserve a separate \"codecs\" module where we could put these non-default \"expert\" codecs. ",
            "author": "Adrien Grand",
            "id": "comment-13443939"
        },
        {
            "date": "2012-08-29T11:12:42+0000",
            "content": "\nI was also thinking that some codecs such as this kind of per-field compression, but maybe even the bloom, memory, direct and pulsing postings formats might deserve a separate \"codecs\" module where we could put these non-default \"expert\" codecs.\n\nWe have to do something about this soon!\n\nDo you want to open a separate issue for that (it need not block this issue)?\n\nI think we would try to get everything concrete we can out of core immediately\n(maybe saving only the default codec for that release), but use the other\nones for testing. Still we should think about it. ",
            "author": "Robert Muir",
            "id": "comment-13443962"
        },
        {
            "date": "2012-08-29T12:08:14+0000",
            "content": "Do you want to open a separate issue for that (it need not block this issue)?\n\nI created LUCENE-4340. ",
            "author": "Adrien Grand",
            "id": "comment-13443996"
        },
        {
            "date": "2012-08-30T15:37:04+0000",
            "content": "I just have a word of encouragement \u2013 this is awesome!  Keep up the good work Adrien. ",
            "author": "David Smiley",
            "id": "comment-13445022"
        },
        {
            "date": "2012-09-09T23:54:12+0000",
            "content": "Thanks for your kind words, David!\n\nHere is a new version of the patch. I've though a lot about whether or not to let users configure per-field compression, but I think we should just try to provide something simple that improves the compression ratio by allowing cross-field and cross-document compression\u00a0;  People who have very specific needs can still implement their own StoredFieldsFormat.\n\nMoreover I've had a discussion with Robert who argued that we should limit the number of classes that are exposed as a SPI because they add complexity (for example Solr needs to reload SPI registers every time it adds a core lib directory to the classpath). So I tried to make it simpler: there is no more CompressionCodec and people can choose between 3 different compression modes:\n\n\tFAST, that uses LZ4's fast compressors and uncompressors (for indices that have a high update rate),\n\tHIGH_COMPRESSION, that uses deflate (for people who want low compression ratios, no matter what the performance penalty is),\n\tFAST_UNCOMPRESSION that spends more time compressing using LZ4's compress_HC method but still has very fast uncompression (for indices that have a reasonnable update rate and need good read performance).\n\n\n\nI also added a test case and applied Dawid's advice to replace the default skipBytes implementation with a bulk-write into a write-only buffer.\n\nHere is a new benchmark that shows how this new codec can help compress stored fields. This time, I indexed some access.log files generated by Apache HTTP server. A document consists of a line from the log file and is typically between 100 and 300 bytes. Because every line contains the date of the request, its path and the user-agent of the client, there is a lot of redundancy across documents.\n\n\nFormat            Chunk size  Compression ratio     IndexReader.document time\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nuncompressed                               100%                         100%\ndoc/deflate 1                               90%                        1557%\ndoc/deflate 9                               90%                        1539%\nindex/FAST               512                50%                         197%\nindex/HIGH_COMPRESSION   512                44%                        1545%\nindex/FAST_UNCOMPRESSION 512                50%                         198%\n\n\n\nBecause documents are very small, document-level compression doesn't work well and only makes the .fdt file 10% smaller while loading documents from disk is more than 15 times slower on a hot OS cache.\n\nHowever, with this kind of highly redundant input, CompressionMode.FAST looks very interesting as it divides the size of the .fdt file by 2 and only makes IndexReader.document twice slower. ",
            "author": "Adrien Grand",
            "id": "comment-13451708"
        },
        {
            "date": "2012-09-10T10:25:45+0000",
            "content": "Otis shared a link to this issue on Twitter https://twitter.com/otisg/status/244996292743405571 and some people seem to wonder how it compares to ElasticSearch's block compression.\n\nElasticSearch's block compression uses a similar idea: data is compressed into blocks (with fixed sizes that are independent from document sizes). It is based on a CompressedIndexInput/CompressedIndexOutput: Upon closing, CompressedIndexOutput writes a metadata table at the end of the wrapped output that contains the start offset of every compressed block. Upon creation, a CompressedIndexInput first loads this metadata table into memory and can then use it whenever it needs to seek. This is probably the best way to compress small docs with Lucene 3.x.\n\nWith this patch, the size of blocks is not completely independent from document sizes: I make sure that documents don't spread across compressed blocks so that reading a document never requires more than one block to be uncompressed. Moreover, the LZ4 uncompressor (used by FAST and FAST_UNCOMPRESSION) can stop uncompressing whenever it has uncompressed enough data. So unless you need the last document of a compressed block, it is very likely that the uncompressor won't uncompress the whole block before returning.\n\nTherefore I expect this StoredFieldsFormat to have a similar compression ratio to ElasticSearch's block compression (provided that similar compression algorithms are used) but to be a little faster at loading documents from disk. ",
            "author": "Adrien Grand",
            "id": "comment-13451867"
        },
        {
            "date": "2012-09-26T00:00:52+0000",
            "content": "New version of the patch. It contains a few enhancements:\n\n\tMerge optimization: whenever possible the StoredFieldsFormat tries to copy compressed data instead of uncompressing it into a buffer before compressing back to an index output,\n\tNew options for the stored fields index: there are 3 strategies that allow different memory/perf trade-offs:\n\t\n\t\tleaving it fully on disk (same as Lucene40, relying on the O/S cache),\n\t\tloading the position of the start of the chunk for every document into memory (requires up to 8 * numDocs bytes, no disk access),\n\t\tloading the position of the start of the chunk and the first doc ID it contains for every chunk (requires up to 12 * numChunks bytes, no disk access, interesting if you have large chunks of compressed data).\n\t\n\t\n\tImproved memory usage and compression ratio (but a little slower) for CompressionMode.FAST (using packed ints).\n\tTry to save 1 byte per field by storing the field number and the bits together.\n\tMore tests.\n\n\n\nSo in the end, this StoredFieldsFormat tries to make disk seeks less likely by:\n\n\tgiving the ability to load the stored fields index into memory (you never need to seek to find the position of the chunk that contains you document),\n\treducing the size of the fields data file (.fdt) so that the O/S cache can cache more documents.\n\n\n\nOut of curiosity, I tested whether it could be faster for LZ4 to use intermediate buffers for compression and/or uncompression, and it is slower than accessing the index input/output directly (at least with MMapDirectory).\n\nI hope I'll have something committable soon. ",
            "author": "Adrien Grand",
            "id": "comment-13463379"
        },
        {
            "date": "2012-10-03T22:09:50+0000",
            "content": "New patch:\n\n\timproved documentation,\n\tI added CompressingCodec to the list of automatically tested codecs in test-framework,\n\ta few bug fixes.\n\n\n\nPlease let me know if you would like to review this patch before I commit. Otherwise, I'll commit shortly... ",
            "author": "Adrien Grand",
            "id": "comment-13468904"
        },
        {
            "date": "2012-10-03T22:20:08+0000",
            "content": "im on the phone but i have some questions. give me a few  ",
            "author": "Robert Muir",
            "id": "comment-13468921"
        },
        {
            "date": "2012-10-03T22:47:40+0000",
            "content": "Oh, I didn't mean THAT shortly  ",
            "author": "Adrien Grand",
            "id": "comment-13468945"
        },
        {
            "date": "2012-10-03T22:55:29+0000",
            "content": "Shouldnt ByteArrayDataInput override skip to just bump its 'pos'?\n\nCan we plugin various schemes into MockRandomCodec? ",
            "author": "Robert Muir",
            "id": "comment-13468949"
        },
        {
            "date": "2012-10-03T22:59:50+0000",
            "content": "OK MockRandom is just postings now... I think we should have a MockRandomCodec too! ",
            "author": "Robert Muir",
            "id": "comment-13468953"
        },
        {
            "date": "2012-10-04T00:03:26+0000",
            "content": "Almost the same patch. I removed ByteArrayDataInput.skipBytes(int) and removed \"throws IOException\" from ByteArrayDataInput.skipBytes(long).\n\nI think we should have a MockRandomCodec too!\n\nMaybe we should fix it in a separate issue? ",
            "author": "Adrien Grand",
            "id": "comment-13469002"
        },
        {
            "date": "2012-10-04T00:06:36+0000",
            "content": "Yeah: i opened another issue to try to straighten this out. We can just bring these frankenstein codecs upto speed there. ",
            "author": "Robert Muir",
            "id": "comment-13469008"
        },
        {
            "date": "2012-10-04T01:59:43+0000",
            "content": "I'm not a fan of the skipBytes on DataInput. Its not actually necessary or used for this patch?\n\nAnd today DataInput is always forward-only, i dont like the \"may or may not be bidirectional depending if the impl throws UOE\".\n\nI removed it locally and just left it on IndexInput, i think this is cleaner. ",
            "author": "Robert Muir",
            "id": "comment-13469079"
        },
        {
            "date": "2012-10-04T08:15:54+0000",
            "content": "I'm not a fan of the skipBytes on DataInput. Its not actually necessary or used for this patch?\n\nYou're right! I needed it in the first versions of the patch when I reused Lucene40StoredFieldsFormat, but it looks like it's not needed anymore. Let's get rid of it! ",
            "author": "Adrien Grand",
            "id": "comment-13469216"
        },
        {
            "date": "2012-10-04T20:36:49+0000",
            "content": "New patch that removes DataInput.skipBytes, this patch does not have any modifications in lucene-core anymore. ",
            "author": "Adrien Grand",
            "id": "comment-13469661"
        },
        {
            "date": "2012-10-05T00:19:08+0000",
            "content": "Slightly modified patch in order not so seek when writing the stored fields index. ",
            "author": "Adrien Grand",
            "id": "comment-13469862"
        },
        {
            "date": "2012-10-05T15:26:33+0000",
            "content": "I just committed to trunk. I'll wait for a couple of days to make sure Jenkins builds pass before backporting to 4.x. By the way, would it be possible to have one of the Jenkins servers to run lucene-core tests with -Dtests.codec=Compressing for some time? ",
            "author": "Adrien Grand",
            "id": "comment-13470392"
        },
        {
            "date": "2012-10-05T18:03:22+0000",
            "content": "By the way, would it be possible to have one of the Jenkins servers to run lucene-core tests with -Dtests.codec=Compressing for some time?\n\nFYI - http://builds.flonkings.com/job/Lucene-trunk-Linux-Java6-64-test-only-compressed/ ",
            "author": "Simon Willnauer",
            "id": "comment-13470509"
        },
        {
            "date": "2012-10-05T21:48:09+0000",
            "content": "Thanks, Simon! ",
            "author": "Adrien Grand",
            "id": "comment-13470699"
        },
        {
            "date": "2012-10-08T09:35:52+0000",
            "content": "lucene-core tests have passed the whole week-end so I just committed to branch 4.x as well. Thank you again for the Jenkins job, Simon. ",
            "author": "Adrien Grand",
            "id": "comment-13471470"
        },
        {
            "date": "2012-10-16T14:35:01+0000",
            "content": "is there example config provided? ",
            "author": "Radim Kolar",
            "id": "comment-13477038"
        },
        {
            "date": "2012-10-16T14:36:29+0000",
            "content": "@adrien I deleted the jenkins job for this. ",
            "author": "Simon Willnauer",
            "id": "comment-13477041"
        },
        {
            "date": "2012-10-16T14:50:22+0000",
            "content": "@radim you can have a look at CompressingCodec in lucene/test-framework\n@Simon ok, thanks! ",
            "author": "Adrien Grand",
            "id": "comment-13477062"
        },
        {
            "date": "2013-03-22T16:27:58+0000",
            "content": "[branch_4x commit] Adrien Grand\nhttp://svn.apache.org/viewvc?view=revision&revision=1395491\n\nLUCENE-4226: Efficient stored fields compression (merged from r1394578).\n ",
            "author": "Commit Tag Bot",
            "id": "comment-13610701"
        },
        {
            "date": "2013-05-10T10:34:12+0000",
            "content": "Closed after release. ",
            "author": "Uwe Schindler",
            "id": "comment-13654149"
        }
    ]
}