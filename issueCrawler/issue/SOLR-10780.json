{
    "id": "SOLR-10780",
    "title": "A new collection property autoRebalanceLeaders",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "SolrCloud"
        ],
        "type": "New Feature",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "In solrcloud , the first replica to get started in a given shard becomes the leader of that shard. This is a problem during cluster restarts. the first node to get started have al leaders and that node ends up being very heavily loaded. The solution we have today is to invoke a REBALANCELEADERS command explicitly so that the system ends up with  a uniform distribution of leaders across nodes. This is a manual operation and we can make the system do it automatically. \nso each collection can have an autoRebalanceLeaders flag . If it is set to true whenever a replica becomes ACTIVE in a shard , a REBALANCELEADER is invoked for that shard",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2017-05-31T05:37:41+0000",
            "content": "As the original author of all that REGALANCELEADERS stuff, I'll be happy to see it go away, it's always been arcane ....\n\nThe intent of the original was to prevent 100s of leaders being on the same Solr instance in cases where there were many, many shards spread across many machines and each machine would host a replica of each shard. In that case measurable performance degradation happened because, even though the extra work for the leader wasn't onerous, the cumulative extra work was.\n\nAnd since there is no use for BALANCESHARDUNIQUE other than preferredLeader (that I know of), this and the REBALANCELEADERS API commands are overkill.\n\nI think the intent of this functionality can be implemented much more simply. When a replica comes up and after it becomes active, if it examines the state of the collection and notes \"too many\" leaders on a particular node, if could simply request that it become the leader of its shard.\n\nBy waiting until it's active, we should avoid conditions where a replica wants to become the leader but hasn't synced.\n\nI think this is quite legitimate as part of the general autoscaling effort, the time is now.\n\nLet's say I have 100 nodes, 100 shards and 100 replicas/shard. That is, each node hosts one replica for each shard. Now I run around and start up all the nodes. How do we keep from unnecessary leadership changes? Maybe throttle this somehow?\n\nOr two replicas for the same shard request leadership at the same time....\n\nOr is this the Overseer's job? Something like a \"balancing thread\" that notices this condition and sends \"you should be leader\" messages to particular replicas. Or something that has a global view of what's happening cluster wide (as yet undefined)... ",
            "author": "Erick Erickson",
            "id": "comment-16030666"
        }
    ]
}