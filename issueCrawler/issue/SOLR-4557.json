{
    "id": "SOLR-4557",
    "title": "Fix broken CoreContainerTest.testReload",
    "details": {
        "affect_versions": "4.2,                                            6.0",
        "status": "Closed",
        "fix_versions": [
            "4.3",
            "6.0"
        ],
        "components": [],
        "type": "Test",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "I was chasing down a test failure, and it turns out that CoreContainerTest.testReload has only succeeded by chance. The test fires up 4 threads that go out and reload the same core all at once. This caused me to look at properly synchronizing reloading cores pursuant to SOLR-4196, on the theory that we should serialize loading, unloading and reloading cores; we shouldn't be doing any of those operations from different threads on the same core at the same time. It turns out that if you fire up multiple reloads at once without serializing them, an error is thrown instead of proper reloading occurring, and that's the only reason the test doesn't hang. The stack trace of the exception is below for reference, but it doesn't with the code I'll attach to this patch:\n\n[junit4:junit4]   2> \tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)\n[junit4:junit4]   2> \tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)\n[junit4:junit4]   2> \tat com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)\n[junit4:junit4]   2> \tat com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:536)\n[junit4:junit4]   2> \tat org.apache.solr.core.JmxMonitoredMap.put(JmxMonitoredMap.java:138)\n[junit4:junit4]   2> \tat org.apache.solr.core.JmxMonitoredMap.put(JmxMonitoredMap.java:51)\n[junit4:junit4]   2> \tat org.apache.solr.core.RequestHandlers.register(RequestHandlers.java:106)\n[junit4:junit4]   2> \tat org.apache.solr.core.RequestHandlers.initHandlersFromConfig(RequestHandlers.java:157)\n[junit4:junit4]   2> \tat org.apache.solr.core.SolrCore.<init>(SolrCore.java:757)\n[junit4:junit4]   2> \tat org.apache.solr.core.SolrCore.reload(SolrCore.java:408)\n[junit4:junit4]   2> \tat org.apache.solr.core.CoreContainer.reload(CoreContainer.java:1076)\n[junit4:junit4]   2> \tat org.apache.solr.core.TestCoreContainer$1TestThread.run(TestCoreContainer.java:90)",
    "attachments": {
        "SOLR-4557.patch": "https://issues.apache.org/jira/secure/attachment/12573118/SOLR-4557.patch",
        "SOLR-4557_posthshutdown_stack.txt": "https://issues.apache.org/jira/secure/attachment/12573213/SOLR-4557_posthshutdown_stack.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Erick Erickson",
            "id": "comment-13598984",
            "date": "2013-03-11T17:00:02+0000",
            "content": "NOCOMMITs in this patch, but with this patch in place on my Mac, TestCoreContainer.testReload hangs every time with zombie threads. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13599492",
            "date": "2013-03-11T23:46:17+0000",
            "content": "Erick: you didn't mention what threads you see in thread dumps when you see hang's with this patch, but when i try it is see...\n\n\n[junit4:junit4]   2> 4384 T10 oasc.SolrCore.closeSearcher [collection1] Closing main searcher on request.\n[junit4:junit4]   2> 4385 T10 oas.SolrTestCaseJ4.tearDown ###Ending testReload\n[junit4:junit4]   1> EOE: started thread 12\n[junit4:junit4]   1> EOE: started thread 13\n[junit4:junit4]   1> EOE: started thread 14\n[junit4:junit4]   1> EOE: started thread 15\n[junit4:junit4]   1> EOE: past join thread 12\n[junit4:junit4]   1> EOE: past join thread 13\n[junit4:junit4]   1> EOE: past join thread 14\n[junit4:junit4]   1> EOE: past join thread 15\n[junit4:junit4] OK      2.06s | TestCoreContainer.testReload\n[junit4:junit4]   2> 4390 T10 oas.SolrTestCaseJ4.deleteCore ###deleteCore\n[junit4:junit4]   2> 4390 T10 oasc.CoreContainer.shutdown Shutting down CoreContainer instance=2056171012\n\n\n\n...at which point there is a pause, and i took a threaddump with jstack (see SOLR-4557_posthshutdown_stack.txt attachment) then waiting a bit more...\n\n\n[junit4:junit4]   2> 125406 T10 oas.SolrTestCaseJ4.endTrackingSearchers SEVERE ERROR: SolrIndexSearcher opens=9 closes=5\n[junit4:junit4]   2> 125428 T9 ccr.ThreadLeakControl.checkThreadLeaks WARNING Will linger awaiting termination of 4 leaked thread(s).\n[junit4:junit4] HEARTBEAT J0 PID(13420@frisbee): 2013-03-11T16:38:37, stalled for  126s at: TestCoreContainer.testReload\n[junit4:junit4]   2> 145551 T9 ccr.ThreadLeakControl.checkThreadLeaks SEVERE 4 threads leaked from SUITE scope at org.apache.solr.core.TestCoreContainer: \n[junit4:junit4]   2> \t   1) Thread[id=16, name=searcherExecutor-5-thread-1, state=WAITING, group=TGRP-TestCoreContainer]\n[junit4:junit4]   2> \t        at sun.misc.Unsafe.park(Native Method)\n[junit4:junit4]   2> \t        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\n[junit4:junit4]   2> \t        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)\n[junit4:junit4]   2> \t        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:386)\n[junit4:junit4]   2> \t        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1069)\n[junit4:junit4]   2> \t        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1131)\n[junit4:junit4]   2> \t        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n[junit4:junit4]   2> \t        at java.lang.Thread.run(Thread.java:679)\n[junit4:junit4]   2> \t   2) Thread[id=17, name=searcherExecutor-8-thread-1, state=WAITING, group=TGRP-TestCoreContainer]\n[junit4:junit4]   2> \t        at sun.misc.Unsafe.park(Native Method)\n[junit4:junit4]   2> \t        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\n[junit4:junit4]   2> \t        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)\n[junit4:junit4]   2> \t        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:386)\n[junit4:junit4]   2> \t        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1069)\n[junit4:junit4]   2> \t        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1131)\n[junit4:junit4]   2> \t        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n[junit4:junit4]   2> \t        at java.lang.Thread.run(Thread.java:679)\n[junit4:junit4]   2> \t   3) Thread[id=11, name=searcherExecutor-2-thread-1, state=WAITING, group=TGRP-TestCoreContainer]\n[junit4:junit4]   2> \t        at sun.misc.Unsafe.park(Native Method)\n[junit4:junit4]   2> \t        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\n[junit4:junit4]   2> \t        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)\n[junit4:junit4]   2> \t        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:386)\n[junit4:junit4]   2> \t        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1069)\n[junit4:junit4]   2> \t        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1131)\n[junit4:junit4]   2> \t        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n[junit4:junit4]   2> \t        at java.lang.Thread.run(Thread.java:679)\n[junit4:junit4]   2> \t   4) Thread[id=18, name=searcherExecutor-11-thread-1, state=WAITING, group=TGRP-TestCoreContainer]\n[junit4:junit4]   2> \t        at sun.misc.Unsafe.park(Native Method)\n[junit4:junit4]   2> \t        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\n[junit4:junit4]   2> \t        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)\n[junit4:junit4]   2> \t        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:386)\n[junit4:junit4]   2> \t        at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1069)\n[junit4:junit4]   2> \t        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1131)\n[junit4:junit4]   2> \t        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n[junit4:junit4]   2> \t        at java.lang.Thread.run(Thread.java:679)\n\n\n\n...followed by errors from ThreadLeakControl.tryToInterruptAll that it was unable to terminate those searchExecuter threads.\n\nThe threaddump i got from jstack jives with the threaddump from the test framework as well as the error from SolrTestCaseJ4.endTrackingSearchers about \"opens=9 closes=5\" \u2013 it would appear that there is a SolrIndexSearcher leaking for each of the 4 reload commands executed.\n\nI have to run, but i would suggest starting by looking closely at how the SolrIndexSearcher refrences are tracked on core init/close and compare that with what's done on reload. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13599541",
            "date": "2013-03-12T00:50:49+0000",
            "content": "Right, thanks, I probably should have attached the jstack output too, sorry 'bout that.\n\nCrap! I thought I could write this off as a test artifact, but I was still trying it in the old code. Siiigggh.\n\nAnyway, back to the grind about looking at why we might not close searchers. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13599631",
            "date": "2013-03-12T02:42:32+0000",
            "content": "OK, I think I have a clue what's going on, but I need to sleep on it. Looks like it's in the new code I just put in to lock out multiple simultaneous operations, I think that code is doing an extra open on the core. Hmmm, new code is the problem; big surprise that <G>.. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13600154",
            "date": "2013-03-12T16:43:27+0000",
            "content": "trunk r: 1455606. fixed the root cause of the tests failing, also took more care with the core reloads so they don't happen simultaneously with loads/unloads. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13600164",
            "date": "2013-03-12T16:46:24+0000",
            "content": "Fix for trunk corresponding to the checkin. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13600475",
            "date": "2013-03-12T21:07:14+0000",
            "content": "tightened up the core sequencing operations load/unload/reload now are sequential for any individual core. Operations happen in parallel for different cores of course.\n\n4x:    r - 1455710\ntrunk: r - 1455606 "
        },
        {
            "author": "Commit Tag Bot",
            "id": "comment-13601107",
            "date": "2013-03-13T13:08:21+0000",
            "content": "[branch_4x commit] Erick Erickson\nhttp://svn.apache.org/viewvc?view=revision&revision=1455710\n\nFix for SOLR-4557, 'Fix broken CoreContinerTest.testReload' "
        },
        {
            "author": "Commit Tag Bot",
            "id": "comment-13601119",
            "date": "2013-03-13T13:08:37+0000",
            "content": "[trunk commit] Erick Erickson\nhttp://svn.apache.org/viewvc?view=revision&revision=1455606\n\nFix for SOLR-4557, fixes for reload testing. Also made a series of changes to make reloading cores more robust "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13654046",
            "date": "2013-05-10T10:33:50+0000",
            "content": "Closed after release. "
        }
    ]
}