{
    "id": "SOLR-10317",
    "title": "Solr Nightly Benchmarks",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Task",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "Currently hosted at: http://212.47.242.214/MergedViewCloud.html\n\n\nSolr needs nightly benchmarks reporting. Similar Lucene benchmarks can be found here, https://home.apache.org/~mikemccand/lucenebench/.\n\nPreferably, we need:\n\n\tA suite of benchmarks that build Solr from a commit point, start Solr nodes, both in SolrCloud and standalone mode, and record timing information of various operations like indexing, querying, faceting, grouping, replication etc.\n\tIt should be possible to run them either as an independent suite or as a Jenkins job, and we should be able to report timings as graphs (Jenkins has some charting plugins).\n\tThe code should eventually be integrated in the Solr codebase, so that it never goes out of date.\n\n\n\nThere is some prior work / discussion:\n\n\thttps://github.com/shalinmangar/solr-perf-tools (Shalin)\n\thttps://github.com/chatman/solr-upgrade-tests/blob/master/BENCHMARKS.md (Ishan/Vivek)\n\tSOLR-2646 & SOLR-9863 (Mark Miller)\n\thttps://home.apache.org/~mikemccand/lucenebench/ (Mike McCandless)\n\thttps://github.com/lucidworks/solr-scale-tk (Tim Potter)\n\n\n\nThere is support for building, starting, indexing/querying and stopping Solr in some of these frameworks above. However, the benchmarks run are very limited. Any of these can be a starting point, or a new framework can as well be used. The motivation is to be able to cover every functionality of Solr with a corresponding benchmark that is run every night.\n\nProposing this as a GSoC 2017 project. I'm willing to mentor, and I'm sure Shalin Shekhar Mangar and Mark Miller would help here.",
    "attachments": {
        "Screenshot from 2017-07-30 20-30-05.png": "https://issues.apache.org/jira/secure/attachment/12879536/Screenshot%20from%202017-07-30%2020-30-05.png",
        "solrconfig.xml": "https://issues.apache.org/jira/secure/attachment/12868197/solrconfig.xml",
        "changes-solr-20160907.json": "https://issues.apache.org/jira/secure/attachment/12868196/changes-solr-20160907.json",
        "Narang-Vivek-SOLR-10317-Solr-Nightly-Benchmarks-FINAL-PROPOSAL.pdf": "https://issues.apache.org/jira/secure/attachment/12861725/Narang-Vivek-SOLR-10317-Solr-Nightly-Benchmarks-FINAL-PROPOSAL.pdf",
        "changes-lucene-20160907.json": "https://issues.apache.org/jira/secure/attachment/12868195/changes-lucene-20160907.json",
        "SOLR-10317.patch": "https://issues.apache.org/jira/secure/attachment/12873019/SOLR-10317.patch",
        "managed-schema": "https://issues.apache.org/jira/secure/attachment/12868198/managed-schema",
        "Narang-Vivek-SOLR-10317-Solr-Nightly-Benchmarks.docx": "https://issues.apache.org/jira/secure/attachment/12861481/Narang-Vivek-SOLR-10317-Solr-Nightly-Benchmarks.docx"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-03-20T04:34:40+0000",
            "content": "This would be an awesome GSoC project. ",
            "author": "David Smiley",
            "id": "comment-15932153"
        },
        {
            "date": "2017-03-29T15:21:43+0000",
            "content": "Hi, I am interested in this project. I had one query, though. Ishan Chattopadhyaya while you have mentioned some operations above can you please provide me with a complete list of operations that you would want to be gauged for performance? The list that you should provide, will help me define my objectives in detail on the proposal and should be helpful in estimating if the project ultimately achieved its objectives or not. Thanks! ",
            "author": "Vivek Narang",
            "id": "comment-15947332"
        },
        {
            "date": "2017-03-29T16:42:03+0000",
            "content": "Here's a rough list of the top of my head. It would be good for a student to add to this list whatever I've missed out for the sake of completeness:\n\n\tIndexing benchmarks\n\t\n\t\tStandalone\n\t\tSolrCloud (various simple configurations (0) )\n\t\tnew replication mode (SOLR-9835) *\n\t\n\t\n\tVarious types of queries:\n\t\n\t\tQuerying on numeric fields (exact queries, range queries)\n\t\tQuerying on text fields\n\t\tQuerying on string fields\n\t\tSorting on numeric fields, string fields (with and without docValues)\n\t\tExtended Dismax queries\n\t\tHighlighting (Unified)\n\t\tSpatial search (using various strategies) *\n\t\n\t\n\tQuery (all the above) on\n\t\n\t\tStandalone Solr\n\t\tSolrCloud (on some simple configurations (0) )\n\t\tAlso, good if this can be tried out on the new replication mode (SOLR-9835). *\n\t\n\t\n\tPartial Updates benchmarks (atomic updates, in-place updates)\n\tFaceting (string fields, numeric fields, enum fields)\n\t\n\t\tJSON Faceting\n\t\tClassic faceting *\n\t\n\t\n\tGrouping (string fields, numeric fields, enum fields) *\n\tSpell check *\n\n\n\nA Wikipedia based dataset is usually available on all the Jenkins instances, and could be used for the purpose. Any other suitable dataset is also welcome. Steve Rowe, Uwe Schindler, can you please point to the downloadable link for the enwiki.random.lines.txt file? (I have it, but forgot where I got it from).\n\nIf I've missed out something, please feel free to comment. Vivek Narang, please feel free to ask any follow up question on any of the above, if you don't have clarity. I've added asterisks around items that are nice to have, but not strictly necessary (in terms of GSoC evaluation criteria) \u2013 something like stretch goals.\n\n(0) - Some simple SolrCloud configurations could be:\n\n\t1 shard, 2-3 replicas\n\t2 shards, 1 replica each\n\t2 shards, 2 replicas each\n\n ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15947470"
        },
        {
            "date": "2017-03-29T17:01:38+0000",
            "content": "A Wikipedia based dataset is usually available on all the Jenkins instances, and could be used for the purpose. Steve Rowe, Uwe Schindler, can you please point to the downloadable link for the enwiki.random.lines.txt file?\n\nThe target get-jenkins-line-docs in lucene/build.xml will download a compressed version of the file from Michael McCandless's home.apache.org space and decompress it for you:\n\n\n  <target name=\"get-jenkins-line-docs\" unless=\"enwiki.exists\">\n    <sequential>\n      <!-- TODO: can get .lzma instead (it's ~17% smaller) but there's no builtin ant support...? -->\n      <get src=\"http://home.apache.org/~mikemccand/enwiki.random.lines.txt.bz2\"\n           dest=\"enwiki.random.lines.txt.bz2\"/>\n      <bunzip2 src=\"enwiki.random.lines.txt.bz2\" dest=\"enwiki.random.lines.txt\"/>\n    </sequential>\n  </target>\n\n\n\nTo employ this target:\n\n\n$ cd lucene\n$ ant get-jenkins-line-docs\n\n ",
            "author": "Steve Rowe",
            "id": "comment-15947519"
        },
        {
            "date": "2017-03-29T17:50:44+0000",
            "content": "A Wikipedia based dataset is usually available on all the Jenkins instances, and could be used for the purpose. Any other suitable dataset is also welcome. Steve Rowe, Uwe Schindler, can you please point to the downloadable link for the enwiki.random.lines.txt file? (I have it, but forgot where I got it from).\n\nIt is available in SVN: http://svn.apache.org/repos/asf/lucene/test-data\n\nThis is where all jenkins servers get it from (nightly tests). I'd suggest to use that link also for downloading. ",
            "author": "Uwe Schindler",
            "id": "comment-15947601"
        },
        {
            "date": "2017-03-29T18:29:02+0000",
            "content": "If we want a nice dataset with nested entities, facetable fields and perhaps even graph-query ready stuff, the revision history of Lucene/Solr project itself might be something to think about. And the bonus is that it could be generated from the same source code we are building Solr. \n\nI've done preliminary generation of such a dataset from Git history at: https://github.com/arafalov/git-to-solr but it would likely need to be extended/improved. ",
            "author": "Alexandre Rafalovitch",
            "id": "comment-15947658"
        },
        {
            "date": "2017-03-31T15:36:37+0000",
            "content": "Thanks Ishan Chattopadhyaya, Steve Rowe, Uwe Schindler & Alexandre Rafalovitch for all your inputs.  ",
            "author": "Vivek Narang",
            "id": "comment-15951147"
        },
        {
            "date": "2017-03-31T17:46:38+0000",
            "content": "Hello, \nI am attaching the first draft version of my proposal. I would like to have your thoughts and valuable suggestions on my draft. Looking forward to the suggestions from the community. \n\nRegards \nVivek ",
            "author": "Vivek Narang",
            "id": "comment-15951382"
        },
        {
            "date": "2017-03-31T17:59:07+0000",
            "content": "Vivek Narang, please upload your draft on the GSoC website first. One of the mentors would review it and get back to you  ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15951401"
        },
        {
            "date": "2017-03-31T18:11:22+0000",
            "content": "Thanks Ishan Chattopadhyaya, Draft uploaded on the GSoC website. Regards ",
            "author": "Vivek Narang",
            "id": "comment-15951416"
        },
        {
            "date": "2017-03-31T18:46:08+0000",
            "content": "Thanks Ishan Chattopadhyaya for your prompt feedback on my draft for the proposal!! Regards ",
            "author": "Vivek Narang",
            "id": "comment-15951479"
        },
        {
            "date": "2017-03-31T18:49:41+0000",
            "content": "Vivek, I've added my initial comments on the proposal.\n\nAnshum Gupta, Mikhail Khludnev, Alexandre Rafalovitch, as I have mentioned in the mentors list, I have requested you to kindly co-mentor for this idea. Disclaimer (as I also mentioned in the mentors list), there is a potential conflict of interest here since I have worked with Vivek Narang in the past at Lucidworks (for 2-3 months) and know him personally. Having either of you as main mentor / co-mentor would help in evaluating his proposal and, if selected, mentoring him.\n ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15951486"
        },
        {
            "date": "2017-04-03T14:43:02+0000",
            "content": "Hello, \nI have uploaded my final proposal for this project. I would like to take this opportunity to thank all the members of the community especially Ishan Chattopadhyaya, Mikhail Khludnev & Alexandre Rafalovitch for their prompt feedback and review of my proposal. I am attaching a copy of the uploaded final proposal here. \n\nI look forward to working on this project. \n\nRegards\nVivek Narang ",
            "author": "Vivek Narang",
            "id": "comment-15953589"
        },
        {
            "date": "2017-04-03T14:44:23+0000",
            "content": "Final Proposal Attached ",
            "author": "Vivek Narang",
            "id": "comment-15953592"
        },
        {
            "date": "2017-05-03T00:26:00+0000",
            "content": "I have been working on Solr benchmarks during my work. While I continue to improve the current benchmark, I like to contribute it to community. Currently the benchmarks run nightly using Jenkins job and report result. Developer can also run benchmark for his own build to know performance for his change. There are other features, such as embedded instruments which can track Solr memory usage in addition to throughput and latency.\n\nThe framework in my benchmarks can be reused so that other benchmarks can take advantage of it. I can also co-mentor this project. Ishan Chattopadhyaya Vivek Narang Your proposal is great. My framework can help in some of the steps in proposal and improve the result. We can have a chat to go over details.\n ",
            "author": "Michael Sun",
            "id": "comment-15994096"
        },
        {
            "date": "2017-05-03T02:22:13+0000",
            "content": "Hi Michael Sun, Thank you for your comment. Yes! I would like to know more about your framework. Please let me know your availability for next week and your preferred medium of communication (Skype, phone etc.).  I am hoping that my proposal will get an official acceptance soon. I look forward to hearing from you soon. Regards ",
            "author": "Vivek Narang",
            "id": "comment-15994176"
        },
        {
            "date": "2017-05-03T18:01:54+0000",
            "content": "Vivek Narang That's great. I will ping you offline to set time. ",
            "author": "Michael Sun",
            "id": "comment-15995344"
        },
        {
            "date": "2017-05-03T18:04:04+0000",
            "content": "I can also co-mentor this project.\nThanks Michael! Final results are due on 4th May. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15995347"
        },
        {
            "date": "2017-05-03T18:08:40+0000",
            "content": "Final results are due on 4th May.\nThat's tomorrow. Cool!  ",
            "author": "Michael Sun",
            "id": "comment-15995357"
        },
        {
            "date": "2017-05-03T18:10:17+0000",
            "content": "preferred medium of communication (Skype, phone etc.).\nIf discussed over a non-public medium, please summarize the conversation here. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15995362"
        },
        {
            "date": "2017-05-03T18:40:25+0000",
            "content": "please summarize the conversation here.\nSure. Thanks for reminding. ",
            "author": "Michael Sun",
            "id": "comment-15995404"
        },
        {
            "date": "2017-05-07T13:40:19+0000",
            "content": "Mikhail Khludnev and I (mentors) warmly welcome Vivek Narang, whose proposal has been selected for GSoC 2017.\nAlso, many thanks to Michael Sun for your kind offer to co-mentor in this project. Your help will be valuable in taking this forward.\n\nVivek, please take this opportunity (the community bonding period) to interact with the community using JIRA, mailing lists, IRC (#solr-dev, #lucene-dev, #solr, #lucene). Please also take a moment to review & participate in SOLR-2646 and see how you could leverage the good work Mark is doing there. Your formal coding time begins 31 May, but feel free to start early if you want. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-15999845"
        },
        {
            "date": "2017-05-08T18:42:50+0000",
            "content": "Congratulations! Vivek Narang ",
            "author": "Michael Sun",
            "id": "comment-16001308"
        },
        {
            "date": "2017-05-08T18:51:30+0000",
            "content": "Thanks Michael Sun!  ",
            "author": "Vivek Narang",
            "id": "comment-16001326"
        },
        {
            "date": "2017-05-14T23:59:16+0000",
            "content": "Hello, Mikhail Khludnev, Ishan Chattopadhyaya. Please access http://162.243.101.83/CreateCollectionBenchmark.html. I created a sample where I am calculating the time taken for creating a collection, tested on a set of recent commits. Please hover on the graph to get more details. This uses the same graph API as used at https://home.apache.org/~mikemccand/lucenebench/indexing.html for Lucene nightly benchmarks. Please let me know what you think. I look forward to hearing from you soon. Thanks.  ",
            "author": "Vivek Narang",
            "id": "comment-16009889"
        },
        {
            "date": "2017-05-15T17:58:42+0000",
            "content": "Vivek Narang, looks promising!\n\n\tCan you please explain what x and y axes represent?\n\tCan you please point to the source code?\n\tAlso, can you please provide an update regarding the \"community bonding\" activities that you have taken up? As per the GSoC manual, this is the time when you engage with the community. You should participate in the discussions and issues, fix bugs, add tests and documentation, review code, help users (over IRC, mailing lists) etc. I may not have emphasized it enough, but this \"community bonding period\" is very important in order to make this GSoC project a success.\n\n ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16010999"
        },
        {
            "date": "2017-05-15T18:37:05+0000",
            "content": "Thanks Ishan Chattopadhyaya. The X axis represents day and time when the metric was recorded. The Y axis in this example is the time in nanoseconds that was registered for this activity against each commit. I will provide the link to the code soon. As far as community bonding activities are concerned I have been reading the Solr documentation and especially coding standards that I must adhere to. I have used also #solr-dev and #solr channels to ask few questions. I plan to be more active in the community bonding activities this week.  ",
            "author": "Vivek Narang",
            "id": "comment-16011080"
        },
        {
            "date": "2017-05-15T18:44:10+0000",
            "content": "The X axis represents day and time when the metric was recorded. The Y axis in this example is the time in nanoseconds that was registered for this activity against each commit.\nI think it would be better for every metric point to correspond to a commit, and the day/time be that of the commit. Nanoseconds feels too granular; for collection creation, seconds or milliseconds seems wiser. What do you think? ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16011097"
        },
        {
            "date": "2017-05-15T18:49:47+0000",
            "content": "Vivek Narang Good progress.\n\nA couple of suggestions\n1. There are nearly 30% difference between max value and min value in the report. I noticed they are run against different build but what is the reason for high variation.\n2. In report, it's useful to specify the test environment, test parameters used in test. These metadata make test result meaningful.\n3. Memory usage is probably not very important for this test but it would be good to think about it in your report. Some test such as faceting can be memory intensive.\n\nI strongly agree with Ishan Chattopadhyaya that this is a 'community bonding' project. One suggestion. A good way to start conversation with community would be to start a simply design document to describe the goals, challenges, high level design and how your work can address them. You can share it with community and get feedbacks.\n ",
            "author": "Michael Sun",
            "id": "comment-16011107"
        },
        {
            "date": "2017-05-15T18:53:56+0000",
            "content": "Ishan Chattopadhyaya Yes! I agree that nanosecond scale is too precise. I will convert it to milliseconds. Thanks. ",
            "author": "Vivek Narang",
            "id": "comment-16011113"
        },
        {
            "date": "2017-05-15T19:09:04+0000",
            "content": "Hi Ishan Chattopadhyaya I also wanted to share that I was trying to figure a way out to get the Solr commit history for indexing purposes. After searching I came up with a script https://gist.github.com/viveknarang/141ab289789b0fe55b09409f99d84c75 and with this, created a JSON file http://162.243.101.83/solrcommit.log having around 25K documents. Please let me know what you think about the data structure. Thanks!\n\n\u2014 Sample Record \u2014\n\n{\n  \"commit\": \"9c6279d439a231d9ec8c9564b0ab76f616d10076\",\n  \"author\": \"Joel Bernstein\",\n  \"date\": \"Sun May 14 15:54:32 2017 -0400\",\n  \"message\": \"SOLR-10663-Add-distance-Stream-Evaluator\",\n \"author email\": \"jbernste@apache.org\",\n \"timestamp\": \"1494791672\",\n \"committer name\": \"Joel Bernstein\",\n \"committer email\": \"jbernste@apache.org\",\n \"commit date\": \"Mon May 15 11:26:05 2017 -0400\"\n} ",
            "author": "Vivek Narang",
            "id": "comment-16011141"
        },
        {
            "date": "2017-05-15T19:15:17+0000",
            "content": "Solr commit history for indexing purposes\nLooks great for a start. Can you include the files touched in each commit and %age of the each file changed?\nAlso, if it can be joined with solr/CHANGES.txt and lucene/CHANGES.txt text, then contributor info from there can be gathered. That will be more meaningful, since one issue is committed by a committer, but various folks collaborate on that issue. Also, information as to whether a commit is a bugfix, feature, optimization or other change can be obtained from the CHANGES.txt. Also, the Solr version in which it was released can be leveraged from there.\n\nAlexandre Rafalovitch, any more thoughts? ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16011152"
        },
        {
            "date": "2017-05-15T19:25:09+0000",
            "content": "Hi Ishan Chattopadhyaya, A small additional update. Using this set of documents (~25K) and the data structure I am getting the following metrics against recent commits http://162.243.101.83/IndexingBenchmark.html. Y-Axis is in seconds and X-Axis is time. On hover, you will be able to see the commit ID.   ",
            "author": "Vivek Narang",
            "id": "comment-16011172"
        },
        {
            "date": "2017-05-15T19:37:18+0000",
            "content": "Vivek, as Michael Sun mentioned, \n\n2. In report, it's useful to specify the test environment, test parameters used in test. These metadata make test result meaningful.\n\nreports like these are meaningless, unless the test conditions are specified along with the report. Perhaps every graph should accompany those details?\n\nX-Axis doesn't seem to be commit time, as it should be. Time of metric collection is useless. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16011200"
        },
        {
            "date": "2017-05-15T19:41:29+0000",
            "content": "I've already mentioned my https://github.com/arafalov/git-to-solr in this JIRA back in March. It has full access to GIT information and produces nested records, so I think it might be better than the GIT-to-JSON route. Or at least worth comparing.\n\nI also did parsing of the CHANGES file, which was ugly and hacky, but did produce a nested dataset to play with. It is not publicly available now, but I could look for it in backups to see if that's useful. ",
            "author": "Alexandre Rafalovitch",
            "id": "comment-16011210"
        },
        {
            "date": "2017-05-16T00:37:38+0000",
            "content": "Attached are my experiment's outputs for Lucene and Solr's CHANGES.txt converted into nested Solr-ready input format. managed-schema and solrconfig.xml are also attached.\n\nIt's been a while since I retested this, but I can have another look if it is useful. ",
            "author": "Alexandre Rafalovitch",
            "id": "comment-16011555"
        },
        {
            "date": "2017-05-16T01:36:27+0000",
            "content": "Thanks Alexandre Rafalovitch  ",
            "author": "Vivek Narang",
            "id": "comment-16011599"
        },
        {
            "date": "2017-05-16T03:02:50+0000",
            "content": "Hello Ishan Chattopadhyaya I also found out another dataset which can be used for some scenarios/features. Please see https://www.kaggle.com/snap/amazon-fine-food-reviews. This data set is huge with over half a million records and ten fields. There is a good mix of text and numeric fields. The current indexing time as observed is 1222 seconds on a standalone node. Please access the file (~250MB) here: http://162.243.101.83/Reviews.csv. I think this is awesome! Please let me know what you think. Thanks. \n\n\u2014 Data Structure Details \u2014\nId\nProductId - unique identifier for the product\nUserId - unqiue identifier for the user\nProfileName\nHelpfulnessNumerator - number of users who found the review helpful\nHelpfulnessDenominator - number of users who indicated whether they found the review helpful\nScore - rating between 1 and 5\nTime - timestamp for the review\nSummary - brief summary of the review\nText - text of the review\n\u2014 ",
            "author": "Vivek Narang",
            "id": "comment-16011667"
        },
        {
            "date": "2017-05-16T03:06:24+0000",
            "content": "Amazon one looks interesting. And it looks like it could be made nested with product record as a parent. ",
            "author": "Alexandre Rafalovitch",
            "id": "comment-16011668"
        },
        {
            "date": "2017-05-16T03:47:06+0000",
            "content": "Yes Alexandre Rafalovitch  ",
            "author": "Vivek Narang",
            "id": "comment-16011691"
        },
        {
            "date": "2017-05-16T06:23:50+0000",
            "content": "Hi Ishan Chattopadhyaya I was trying to create an improved UI. Please access http://162.243.101.83/IndexingBenchmarkStandalone.html and let me know what you think. This is not yet final and I will be adding features/options/controls/data (Like an option to view test environment related information etc.) on this soon. Thanks.  ",
            "author": "Vivek Narang",
            "id": "comment-16011811"
        },
        {
            "date": "2017-05-16T07:57:10+0000",
            "content": "Exciting, keep it up! Still, some fundamental concerns remain: X-axis should be based on commit time, not time of collecting the metrics.\n\nHello Ishan Chattopadhyaya I also found out another dataset which can be used for some scenarios/features. Please see https://www.kaggle.com/snap/amazon-fine-food-reviews. \n\nThis dataset looks great! Seems like CC0 license, which might be fine. However, before we actually start using it, I'd like to get a clearance from ASF's legal team to make sure we're good. Give me a few days for this.\n ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16011935"
        },
        {
            "date": "2017-05-16T15:45:32+0000",
            "content": "Hi Ishan Chattopadhyaya A quick update. I was thinking that It would be nice to show the author information (Name, email, and date) for the commit (In case the viewer needs to reach out to him/her for some performance related questions). Please access:http://162.243.101.83/IndexingBenchmarkStandalone.html and now when you hover on a point on the graph you will also see the author details and his commit message. Please let me know what you think. Thanks! ",
            "author": "Vivek Narang",
            "id": "comment-16012606"
        },
        {
            "date": "2017-05-16T19:42:06+0000",
            "content": "Hi Ishan Chattopadhyaya, Michael Sun. I have added a \"Environment Details\" option on the top right-hand corner. Please access http://162.243.101.83/IndexingBenchmarkStandalone.html. While I am working towards adding a memory usage profile during a specific metric estimation, please let me know if the information that is currently shown is helpful. Thanks! ",
            "author": "Vivek Narang",
            "id": "comment-16012994"
        },
        {
            "date": "2017-05-16T19:55:21+0000",
            "content": "Cool! Looks pretty nice.\n\nI guess the number in the indexing chart is throughput but it needs to be specified in the report.\n\nThe test env details looks good. The other part of test metadata is test parameters, such as index # of threads, etc. The test result is meaningful with these parameters.\n\nA common way to express these parameters is to use key value pair. Key value pair also makes it easy for visualization and analysis.\n ",
            "author": "Michael Sun",
            "id": "comment-16013014"
        },
        {
            "date": "2017-05-16T23:46:01+0000",
            "content": "Hi Ishan Chattopadhyaya, Michael Sun. I have added an option to see the state of memory during a specific test. Please access http://162.243.101.83/IndexingBenchmarkStandalone.html, now when you click on a specific point on the graph, another graph will appear on a popup window with the record of how the main memory and swap memory availability changed during the test execution. Please let me know what you think. Thanks.  ",
            "author": "Vivek Narang",
            "id": "comment-16013277"
        },
        {
            "date": "2017-05-17T14:44:59+0000",
            "content": "The other part of test metadata is test parameters, such as index # of threads, etc. The test result is meaningful with these parameters.\n+1. Also, for the indexing graph, other meaningful information include number of shards, replicas, type of SolrJ client used, batch size.\n\nI have added an option to see the state of memory during a specific test. \nMemory profile looks good. Please use some human readable scale for Y-Axis and please specify the units. Also, how are you accessing the memory usage?\n\nI was thinking that It would be nice to show the author information (Name, email, and date) for the commit (In case the viewer needs to reach out to him/her for some performance related questions). \nGood to show as much information as possible, without cluttering the interface. However, I would prefer just the commit ID, name, date and the SOLR jira issue (email is useless) the commit IDs and links in this format: https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7830462d4b7da3acefff6353419e71cde62d5fee. Keep in mind that if this suite is run once a day, then each data point would be a result of all the intermediate commits between that day's run and previous day's run. Is it possible to include information about all the intermediate commits in the display? ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16014142"
        },
        {
            "date": "2017-05-17T17:05:47+0000",
            "content": "Hi Ishan Chattopadhyaya I am working on making these changes. For memory monitoring, a thread runs in parallel when the test is in progress and parses data from /proc/meminfo in specific intervals. Please let me know if you think that there is a better approach. Thanks. ",
            "author": "Vivek Narang",
            "id": "comment-16014418"
        },
        {
            "date": "2017-05-17T17:10:30+0000",
            "content": "You should use the Metrics API (https://cwiki.apache.org/confluence/display/solr/Metrics+Reporting).\nMemory, CPU utilization graphs would be very nice! ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16014429"
        },
        {
            "date": "2017-05-17T18:16:59+0000",
            "content": "Thanks Ishan Chattopadhyaya This API is Awesome !!!! ",
            "author": "Vivek Narang",
            "id": "comment-16014542"
        },
        {
            "date": "2017-05-17T18:34:09+0000",
            "content": "Thanks Ishan Chattopadhyaya This API is Awesome !!!!\nAll thanks to Andrzej Bialecki   ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16014569"
        },
        {
            "date": "2017-05-17T19:36:18+0000",
            "content": "Hi Andrzej Bialecki , Thank you for writing this awesome Metrics API. My life just got better with this  ",
            "author": "Vivek Narang",
            "id": "comment-16014663"
        },
        {
            "date": "2017-05-17T23:32:50+0000",
            "content": "Hi Ishan Chattopadhyaya, A quick update. Please access http://162.243.101.83/IndexingBenchmarkStandalone.html. Now when you click a point on the graph, a popup box appears having three tabs (Heap Space Usage, Process CPU load, and Test Environment Details). Now, the estimations are made using the Solr's Metrics API.The sections provide graphs for Heap Space usage and CPU usage. The third tab shows other environment information. Also, there is a link at the bottom (on the popup itself) which opens the page with details of file changes associated with each commit. Please let me know what you think. Regards. ",
            "author": "Vivek Narang",
            "id": "comment-16014937"
        },
        {
            "date": "2017-05-19T20:22:09+0000",
            "content": "Hi Ishan Chattopadhyaya, A quick update, now I have also added indexing performance for SolrCloud using the regular and the concurrent client. You can see the current running data by clicking the links provided. The current sample configuration for SolrCloud is 5 Solr nodes, 2 Shards, and 2 Replicas.  For the concurrent client, the configuration is queue size of 10 and threads 10. \nPlease access here: http://162.243.101.83/IndexingBenchmarkCloudConcurrent.html. \nAlso, I thought that it would be a great idea to install Jenkins http://162.243.101.83:8080/ on my server and make it invoke the nightly benchmark tests. Now, this benchmark package would be invoked every night on my server. Please let me know what you think. Regards.  ",
            "author": "Vivek Narang",
            "id": "comment-16017963"
        },
        {
            "date": "2017-05-20T02:54:02+0000",
            "content": "Hi Ishan Chattopadhyaya, After doing some refactoring the links in the previous comments may not work. Please access http://162.243.101.83/dev/CreateCollectionBenchmarkStandalone.html or http://162.243.101.83/production/CreateCollectionBenchmarkStandalone.html. \n\nOne of these links are for my dev environment the other is the simulated production environment where Jenkins on my server triggers the benchmark suite.\n\nRegards ",
            "author": "Vivek Narang",
            "id": "comment-16018271"
        },
        {
            "date": "2017-05-21T00:22:04+0000",
            "content": "Hi Ishan Chattopadhyaya I have one doubt. Please see the following graph http://162.243.101.83/dev/IndexingBenchmarkStandalone.html. What could be the cause of variation in performance? The environment is same (same configuration)  and the commits on this graph are consecutive and recent. Please let me know soon! Regards ",
            "author": "Vivek Narang",
            "id": "comment-16018655"
        },
        {
            "date": "2017-05-21T09:57:58+0000",
            "content": "What could be the cause of variation in performance?\nI have no insight into what you're referring to. I don't even know what the test does and how. \n\nPlease let me know soon!\nI'm currently recovering from a bad food poisoning, and unable to work  Can you please provide steps to reproduce, and describe what exactly you're doing and what exactly are you concerned about? Also, do you think your sample size of data points is sufficient to determine that the variation you're observing is an actual outlier, and not just statistical noise? You could discuss any usage related issues (like indexing performance, collection creation etc.) in the solr-users mailing list, and you would get more insightful answers. If you establish the degradation as a significant one, and/or as a result of a bug, feel free to file a JIRA for it.\n\nGSoC is about \"open source\" contributions, and this is your \"community bonding\" period. I neither see that you've provided any links to your source code, nor do I see your participation in any community bonding activities. I don't even know much about the underlying design of the tool that you're building. (Maybe you could start by contributing a patch here?) From an observer's point of view, all I see from you are just some updates about the development of a proprietary (non-free / non-open source) tool. This is contrary to the spirit of the GSoC program, and I would like to encourage you to undertake some course correction so as to make your participation in this program a success going forward.\n\nMikhail Khludnev, Michael Sun, any thoughts? ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16018774"
        },
        {
            "date": "2017-05-21T10:07:51+0000",
            "content": "Now, this benchmark package would be invoked every night on my server. Please let me know what you think.\nGreat idea. \n\nAlso, I thought that it would be a great idea to install Jenkins http://162.243.101.83:8080/ on my server\nA locked down Jenkins link is useless. Please take a look at the Apache Jenkins or Policeman Jenkins. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16018776"
        },
        {
            "date": "2017-05-21T17:11:36+0000",
            "content": "Hi Ishan Chattopadhyaya Please access the current code here: https://github.com/viveknarang/lucene-solr/tree/SolrNightlyBenchmarks/dev-tools/SolrNightlyBenchmarks.  ",
            "author": "Vivek Narang",
            "id": "comment-16018891"
        },
        {
            "date": "2017-05-22T00:29:32+0000",
            "content": "Hi Ishan Chattopadhyaya, I hope you get well soon. Regarding my doubt (as mentioned in the fourth last comment), I may have found out the cause of variation in performance. I am providing the details below. \n\nConcern: I have been noticing that there was a significant difference in the indexing (10000 documents) throughput (in documents/sec). Even for the same commit test executed at separate times, a significant difference in throughput was observed. Please see the following screenshot http://162.243.101.83/Capture_Throughput_drop_reason.PNG. I have marked the drops in throughput on the graph with red circles. You can observe the difference in throughput for example (322 - 263 doc/sec). This difference in throughput was reported by the system for the same commit id which was not making sense and should not be happening. After running a number of tests, I may have been able to pinpoint the cause of this abnormality in performance reporting. As guessed before this might be a memory issue. The server that I am using has limited resources (2 GB RAM). I added more than 5GB of swap space. These specific drops (as marked on the graph with red circles), might be happening when the test run required building the Solr server from fresh code before running the benchmark test. This additional activity (of building Solr from source) may have been causing the RAM to get utilized completely causing the OS to handle the situation with the swap space provided and this additional swap activity might be causing the degradation in performance.  ",
            "author": "Vivek Narang",
            "id": "comment-16019044"
        },
        {
            "date": "2017-05-22T08:12:11+0000",
            "content": "I see.. Seems like your DigitalOcean based VPS with 2GB RAM is insufficient for this project. I have setup a Scaleway based bare metal server (bare metal will be better than VPS due to lack of noisy neighbours), which has 32GB RAM, 8 CPU cores (Intel x86, 64-bit, 2.4GHz), 250GB SSD. Please mail/message me your SSH key, and I'll add it to the server so you can start using it asap. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16019256"
        },
        {
            "date": "2017-05-22T08:14:58+0000",
            "content": "Hello, Vivek Narang, I suppose it's preferable to disable swap at least for Solr jvm https://lucidworks.com/2013/05/21/mlockall-for-all/ .  ",
            "author": "Mikhail Khludnev",
            "id": "comment-16019265"
        },
        {
            "date": "2017-05-22T09:38:20+0000",
            "content": "Yes, see also SOLR-10306 ",
            "author": "Jan H\u00f8ydahl",
            "id": "comment-16019375"
        },
        {
            "date": "2017-05-22T19:22:53+0000",
            "content": "Thanks Ishan Chattopadhyaya, Mikhail Khludnev & Jan H\u00f8ydahl. Regards ",
            "author": "Vivek Narang",
            "id": "comment-16020044"
        },
        {
            "date": "2017-05-22T23:00:05+0000",
            "content": "Ishan Chattopadhyaya Hope you get better soon.\n\nVivek Narang A lot of good progress! For your question about variation, one suggestion is to report number of segments of the index created in test. The throughput can vary due to background merging activities. You can get number of segments using Solr admin/luke endpoint and verify the relationship.\n\nThe metric lib is great. One thing to keep in mind is the metric Dimensionality issue once all metrics are put into one datastore. For example, for index throughput, to specify index throughput for test with one shard or two shards, the only way is to add description in metric name. It can lead to long and hard to consume metric names, such as index.throughput.shard-1.replica-1.autocommit-30... There is a good general description in this netflix blog (https://medium.com/netflix-techblog/introducing-atlas-netflixs-primary-telemetry-platform-bd31f4d8ed9a, search Dimensionality).  ",
            "author": "Michael Sun",
            "id": "comment-16020355"
        },
        {
            "date": "2017-05-23T03:07:21+0000",
            "content": "Hi Ishan Chattopadhyaya I hope that you are well now. \n\nI think I was right when I assumed that there was a memory issue when I was testing on my previous server (with unsuitable hardware configuration for this purpose). Thanks to migrating to the new server that you provided for this project (8 Core 32 GB RAM), The indexing throughput is almost constant now, even across recent consecutive commits. Please see http://212.47.227.9/dev/IndexingBenchmarkStandalone.html. As you can observe now in comparison with the setup on the previous server, the performance on this server is consistent (with the difference of one second possibly due to rounding off, which may also be statistically insignificant depending on how precise we want to measure indexing performance). There is an exception noted though, the indexing on a cluster of 5 Solr nodes, two shards and two replicas using CloudSolrClient, is still registering a level of variation in indexing, please see http://212.47.227.9/dev/IndexingBenchmarkCloud.html. This might be due to the internal mechanism of indexing which could be explained by someone in our community who may have worked deeply on building/maintaining this client. \n\nHi Michael Sun Thank you for your appreciation, I am highly motivated and am looking forward to making this project a success and useful for the community. I will explore the option that you have recommended and will try to include that as well for a more meaningful estimation.\n\nThanks Ishan Chattopadhyaya again, for providing me this awesome server to work on!!! Please let me know if I can use this space for this project for the rest of the alloted time (till ~mid August) so that I can stop using my older server.\n\nRegards.  \nVivek ",
            "author": "Vivek Narang",
            "id": "comment-16020600"
        },
        {
            "date": "2017-05-23T08:27:08+0000",
            "content": "Hi Ishan Chattopadhyaya A quick short additional update. I thought that It will make sense if we add a \"moving average\" line for metric estimation. Please access http://212.47.227.9/dev/IndexingThroughputBenchmarkCloud.html. The \"moving average\" line acts as a dampening effect on the metric estimation that absorbs small fluctuations. In this way, if there is a big shift in the \"moving average\" line that is sure shot a red flag. Please let me know what you think. Regards.  ",
            "author": "Vivek Narang",
            "id": "comment-16020801"
        },
        {
            "date": "2017-05-23T22:49:56+0000",
            "content": "Hi Ishan Chattopadhyaya An update, now the following graph http://212.47.227.9/dev/IndexingThroughputBenchmarkCloudConcurrent.html shows the throughput performance with different thread configurations (2,4,6,8 and 10 Threads) on a Solr cluster of 5 Nodes with 2 Shards and 2 Replicas (as an example).  Regards.  ",
            "author": "Vivek Narang",
            "id": "comment-16022024"
        },
        {
            "date": "2017-05-24T03:12:12+0000",
            "content": "Guys, did you look at https://github.com/shalinmangar/solr-perf-tools ? Why is the motivation behind creating yet another benchmarking utility? ",
            "author": "Shalin Shekhar Mangar",
            "id": "comment-16022252"
        },
        {
            "date": "2017-05-24T18:36:22+0000",
            "content": "Hi Shalin Shekhar Mangar Thank you for sharing the link to your prior work. I briefly went through your work before, using the link to your work that was mentioned in the description section above. I think the purpose of this initiative is to have a comprehensive benchmarking suite based on the prior work done in this area and addition of many other features/tests that helps the Solr community. I would love to know your thoughts as to how we can come up with a better and much more comprehensive suite, based on the prior work done. Regards.  ",
            "author": "Vivek Narang",
            "id": "comment-16023417"
        },
        {
            "date": "2017-05-24T18:43:50+0000",
            "content": "Hi, I am trying to find a way to measure the QPS for query benchmarking. The way I understand it is that it is a way to measure the querying capacity for a Solr node or a cluster. Can anyone recommend me the correct mechanism of estimating QPS? Thanks for the help.  ",
            "author": "Vivek Narang",
            "id": "comment-16023430"
        },
        {
            "date": "2017-05-24T18:52:04+0000",
            "content": "Hi, I am trying to find a way to measure the QPS for query benchmarking. The way I understand it is that it is a way to measure the querying capacity for a Solr node or a cluster. Can anyone recommend me the correct mechanism of estimating QPS? Thanks for the help.\nPlease look at SolrMeter. https://github.com/tflobbe/solrmeter ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16023438"
        },
        {
            "date": "2017-05-24T18:53:43+0000",
            "content": "Hi Shalin Shekhar Mangar Thank you for sharing the link to your prior work. I briefly went through your work before, using the link to your work that was mentioned in the description section above. I think the purpose of this initiative is to have a comprehensive benchmarking suite based on the prior work done in this area and addition of many other features/tests that helps the Solr community. I would love to know your thoughts as to how we can come up with a better and much more comprehensive suite, based on the prior work done. Regards.\n\nThere were some projects that I listed in the description, including Shalin's and yours. I think the central question is why you chose one over the other? ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16023442"
        },
        {
            "date": "2017-05-24T20:15:41+0000",
            "content": "I would love to know your thoughts as to how we can come up with a better and much more comprehensive suite, based on the prior work done\n\nThe current solr-perf-tools has support for:\n\n\tIndexing JSON on a single node solr instance with schemaless configs\n\tIndexing wiki-1kb docs on single node solr instance with fixed schema\n\tIndexing wiki-4kb docs on single node solr instance with fixed schema\n\tIndexing wiki-1kb docs on 2 shard, 1 replica solr cloud\n\n\n\nSee the report attached at SOLR-9863. I like wiki data because the Lucene benchmarks also use it (but not exactly the same data) which gives us a sense how much overhead Solr has over Lucene. I also spent more time on non-cloud single node benchmarks because those are easier to reason about and debug. Troubleshooting cloud performance problems is much more difficult without establishing a baseline using consistent single node benchmarks.\n\nWe can go two ways from here:\n\n\tCleanup/refactor the code to make the tool easier to extend and add benchmarks e.g. instead of writing python code for testing variants of a test, perhaps a test description written in json or a DSL could be executed\n\tForget about the cleaning the code and just add more benchmarks both indexing and query and not worry about code duplication\n\n\n\nThere are arguments for both e.g. #1 above will encourage more people to contribute to benchmarks but #2 above will make your progress faster.\n\nAs for the benchmarks themselves, we already have basic indexing benchmarks there so we need to get started with query benchmarks. There are just so many possibilities here but we can start with uncached query performance first. For this you need to extract terms out of your data set, classify them according to frequency and test all combinations on a solr instance with query/filter cache disabled but ensure that we graph them separately. Reuse the indexes built by the indexing test. As an example, see the BooleanQuery section at https://home.apache.org/~mikemccand/lucenebench/ and the extracted terms data at https://github.com/mikemccand/luceneutil/blob/master/tasks/wikimedium.10M.tasks. Then repeat this with using both q and fq params. Then with dismax query parser and so on. Then iterate again with caches enabled this time. Then repeat with re-indexing data during the query tests (both cached and uncached cases). Use your imagination. Focus on correctness and repeatability. ",
            "author": "Shalin Shekhar Mangar",
            "id": "comment-16023607"
        },
        {
            "date": "2017-05-24T21:24:55+0000",
            "content": "We could also draw inspiration or code from https://github.com/elastic/rally which is really easy to use and quite extensible. No idea how much effort a solr-rally port would take or if it would be legally viable?  The tool supports some nice data sets ootb, see https://esrally.readthedocs.io/en/latest/race.html. A side effect would thus be that we could run same benchmarks to compare against ES as well \n ",
            "author": "Jan H\u00f8ydahl",
            "id": "comment-16023721"
        },
        {
            "date": "2017-05-26T17:35:29+0000",
            "content": "Hello Ishan Chattopadhyaya, After getting inspired by source code in SolrMeter I have come up with the logic to do a kind of a stress test where I am estimating the QPS for a set of numeric queries (as an example for now.). \nPlease access http://212.47.227.9/dev/NumericQueryBenchmarkCloud.html. I am observing a strange co-incidence - The QPS measured for a query looking for a specific number is lowest and the QPS measured for a query looking for all those numbers greater than a number. Is there an explanation for this?\n\n    QPS(Field:Number) < QPS (Field:[Number1 TO Number2]) < QPS(Field:[* TO Number]) < QPS(Field:[Number TO *])\n\nThis has been observed over a set of commits and not one commit.  ",
            "author": "Vivek Narang",
            "id": "comment-16026549"
        },
        {
            "date": "2017-05-26T17:45:37+0000",
            "content": "Can you please explain the test methodology for measuring QPS? What is the field type (Trie or Point / Int or Double or Long or Float)? Firing the same query again and again is useless. Is that what you're trying? Also, what is the latency?\n\nWhy is the motivation behind creating yet another benchmarking utility?\nCan you please answer this ^ ?\n\nI think the central question is why you chose one over the other?\nCan you please answer this ^ ? ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16026567"
        },
        {
            "date": "2017-05-26T23:44:23+0000",
            "content": "motivation behind creating yet another benchmarking utility\n\nThat's a good question. In fact it's one of the reasons that I suggested to start a scoping doc to start conversation early on. (https://issues.apache.org/jira/browse/SOLR-10317?focusedCommentId=16011107&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16011107)\n\nHere are my two cents for a few areas that can be improved in addition to increasing test coverage. Vivek Narang can articulate. \n\n1. Currently benchmark tells us how Solr perform but it can also help to tell why Solr perform in this way. A good example of effort in this direction is the telemetry (https://esrally.readthedocs.io/en/latest/telemetry.html) in rally framework. \n2. Provide baseline data for capacity planning. For capacity planning, it requires some data such as CPU, disk etc. for specific workloads and benchmark can provide that.\n3. Extensibility: a benchmark can be easily extended to include new components. For example, JMeter can be a good load generator for scalability study for Solr cluster with hundreds of nodes and it should be easy to extend current test case to use JMeter to replace existing load generator. This may require an object model at different abstraction level compared to existing benchmarks.\n4. Support more Solr setup and data type. For example, wiki data is a good but tweets data can be better in studying Solr performance for real time analytics use cases.\n5. Last but not least, as any engineering tool, I was hoping the benchmark suite can standardize Solr performance effort, promote code reuse and facilitate collaboration. This requires good understanding for all use cases and careful design.\n\nOf course, this doesn't need to be all done for GoC project. Not to scare Vivek Narang \n\nOverall, this project is a good initiative and a good venue to continue this discussion.\n\n\n\n ",
            "author": "Michael Sun",
            "id": "comment-16027030"
        },
        {
            "date": "2017-05-27T07:37:21+0000",
            "content": "In fact it's one of the reasons that I suggested to start a scoping doc to start conversation early on.\nThere exists one:\nhttps://issues.apache.org/jira/secure/attachment/12861725/Narang-Vivek-SOLR-10317-Solr-Nightly-Benchmarks-FINAL-PROPOSAL.pdf ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16027329"
        },
        {
            "date": "2017-05-30T19:29:46+0000",
            "content": "Hello,\nI have been down with illness for last three days, I will resume my activity shortly. Regards ",
            "author": "Vivek Narang",
            "id": "comment-16029990"
        },
        {
            "date": "2017-06-01T16:09:53+0000",
            "content": "Hi Shalin Shekhar Mangar Thanks for your suggestions above. Your code for indexing is excellent and I will be using that in the project. I was, however, trying to understand the IndexThreads class that you have written. Can you please explain its working when you have some free time? Thanks in advance. ",
            "author": "Vivek Narang",
            "id": "comment-16033224"
        },
        {
            "date": "2017-06-01T23:31:38+0000",
            "content": "Hello Ishan Chattopadhyaya, I have added a mechanism with which the failed tests sessions are identified and handled. The running processes, if any, (solr/zookeeper) from the last failed session are located and destroyed, any files/folders created during the failed session are located and removed and Metric data files are only updated with new statistics when the test session is completely successful. This mechanism will make this suite more self-sufficient. \n\nPlease access the latest code at https://github.com/viveknarang/lucene-solr/tree/SolrNightlyBenchmarks/dev-tools/SolrNightBenchmarks. Regards. ",
            "author": "Vivek Narang",
            "id": "comment-16033906"
        },
        {
            "date": "2017-06-02T00:45:00+0000",
            "content": "I was, however, trying to understand the IndexThreads class that you have written. Can you please explain its working when you have some free time?\n\nI've used IndexThreads in another project, and here's the reference: https://github.com/lintool/IR-Reproducibility/blob/master/systems/lucene/ingester/src/main/java/luceneingester/IndexThreads.java\n\nI remember getting that from Lucene's benchmark module, but I cannot find it now. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16033978"
        },
        {
            "date": "2017-06-02T00:49:40+0000",
            "content": "Hello Ishan Chattopadhyaya, I have added a mechanism with which the failed tests sessions are identified and handled. The running processes, if any, (solr/zookeeper) from the last failed session are located and destroyed, any files/folders created during the failed session are located and removed and Metric data files are only updated with new statistics when the test session is completely successful. This mechanism will make this suite more self-sufficient. \n\nSounds good. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16033983"
        },
        {
            "date": "2017-06-03T22:36:59+0000",
            "content": "Hi Ishan Chattopadhyaya I have restored Jenkins on the new server please access http://212.47.227.9/prod. At this moment you may not see any metric data but soon the numbers should become visible. Regards.  ",
            "author": "Vivek Narang",
            "id": "comment-16036110"
        },
        {
            "date": "2017-06-04T00:28:20+0000",
            "content": "Hi Shalin Shekhar Mangar Thanks for your suggestions above. Your code for indexing is excellent and I will be using that in the project.\n\nI'm very confused. Are you planning to use Shalin's benchmarking utility and extend upon it? If no, why not?\nPlease understand that Shalin or I are NOT asking you to use one platform/utility or the other, but simply asking you for a clarification on the motivations behind why you are building a new platform you are using instead of something that already exists. So far, I just see no clear answer, just beating around the bush.\n\nIn terms of the differences, two good things I like about the reports that Shalin's suite generates (https://issues.apache.org/jira/secure/attachment/12843260/indexing.html) are:\n\n\tall graphs can be viewed at once\n\tsupports tagging/addition of significant events\n\n\n\nIf you choose not to use Shalin's suite, can you please address these two issues in the new suite that you are building? ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16036128"
        },
        {
            "date": "2017-06-04T00:45:25+0000",
            "content": "This dataset looks great! Seems like CC0 license, which might be fine. However, before we actually start using it, I'd like to get a clearance from ASF's legal team to make sure we're good. Give me a few days for this.\nLEGAL-313 ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16036133"
        },
        {
            "date": "2017-06-06T02:23:59+0000",
            "content": "Hi Ishan Chattopadhyaya I think there is some confusion and I think I should give some explanation.\n\n\n\tI am not building a new framework, but I am extending the benchmarks framework that you created https://github.com/chatman/solr-upgrade-tests as mentioned in the proposal.\n\tThe reason behind me extending upon your framework is that it already has many flexible, ready to use resources and that it is written in one language. I am comfortable using one language over two languages together.\n\tFor the remaining things, I already am in the process of using the required resources from Shalin's work and adding to the framework that you created.\n\tAs far as tagging/addition of significant events go while I think that the current logic in Shalin's code base related to listing significant events is hard coded https://github.com/shalinmangar/solr-perf-tools/blob/master/src/python/bench.py#L32-L87, the closest that I am in making it dynamic and self-dependent is showing relevant commit messages with each metric point http://212.47.227.9/prod/NumericQueryBenchmarkStandalone.html (please hover over any point to see the relevant commit message).\n\tI will try to add a feature through which you would be able to view all the graphs together.\n\n\n\nPlease access the latest codebase https://github.com/viveknarang/lucene-solr/tree/SolrNightlyBenchmarks/dev-tools/SolrNightBenchmarks.\nAs per the agreement in the proposal, the code for benchmarks suite is under dev-tools framework and in the SolrNightlyBenchmarks branch. \n\nRegards ",
            "author": "Vivek Narang",
            "id": "comment-16038003"
        },
        {
            "date": "2017-06-06T09:55:31+0000",
            "content": "The reason behind me extending upon your framework is that it already has many flexible, ready to use resources and that it is written in one language. I am comfortable using one language over two languages together.\nThanks for the clarification. Exactly what I was looking for, i.e. reasons why you chose the current suite over Shalin's suite. Personally, I don't care whichever suite you are using so long as your stated goals are met, and we achieve parity with whatever prior work exists already, and the overall suite is flexible enough to add more benchmarks later.\n\nthe closest that I am in making it dynamic and self-dependent is showing relevant commit messages with each metric point http://212.47.227.9/prod/NumericQueryBenchmarkStandalone.html (please hover over any point to see the relevant commit message).\n\nMaintaining a separate JSON file containing significant commit and message sounds like a good way forward. Is it too difficult to plot that info on the graphs, like it is done in Shalin's or Mike's graphs?\n\nI will try to add a feature through which you would be able to view all the graphs together.\nSounds good.\n\nBtw, Shalin's suite has got some tests that I didn't see in your suite or proposal. I know that you've plotted some metrics on a per commit basis in a popup window for indexing (memory consumption over the course of indexing), but having an independent graph on GC while indexing and other similar graphs that Shalin has added to that suite would be good. Don't stretch yourself for them right now, but it would be awesome if you can add them at the end of your GSoC project, for the sake of completeness of the suite.\n\n Shalin Shekhar Mangar, Michael Sun, any thoughts, please? ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16038504"
        },
        {
            "date": "2017-06-07T01:46:04+0000",
            "content": "any thoughts, please?\n\nWell, this is a good topic. Framework is fun and always a tradeoff.  On one side, we should not over invest on framework because the tests and numbers produced by tests are things really matter. That's the target we should focus on. On the other hand, if framework is under invested, in long run there is high cost. One way to understand the cost is to look how many Solr performance frameworks there are already, not to mention a few not published (but I personally know).\n\nI am one of guys who built my own (going to be open source soon, adding one more into the collection). One of the motivation for me to build one is: developers only need to extend framework but not rebuild one for all Solr performance work in near future. In my option this is important and should be part of the goals of this project. And be part of the discussion about framework choice too (no matter which one is chosen). \n ",
            "author": "Michael Sun",
            "id": "comment-16039986"
        },
        {
            "date": "2017-06-08T02:10:05+0000",
            "content": "Hi Ishan Chattopadhyaya\nAn Update. I have added a mechanism with which a set of commits collected in a single day are processed for benchmarks at midnight now. Please view a sample run of this batch mode (that was processed today) here: http://212.47.227.9/prod/. The benchmark (bach-mode) cycle is now triggered by Jenkins every midnight [EST]. Regards ",
            "author": "Vivek Narang",
            "id": "comment-16042085"
        },
        {
            "date": "2017-06-10T03:50:04+0000",
            "content": "Hi Ishan Chattopadhyaya An Update. I have implemented a logic to calculate the Latency (for numeric queries) and corresponding percentiles (75th, 95th, 99th & 99.9th) for Latency. Additionally, I am in the process of adding mechanisms for AND and OR type queries for Numeric fields. These new features are not yet visible on the front end, however, their logic is already added. These new features will appear soon. Regards.   ",
            "author": "Vivek Narang",
            "id": "comment-16045363"
        },
        {
            "date": "2017-06-14T21:01:42+0000",
            "content": "Just uploaded the first cut of Solr benchmark I built during my work, as one more option for community for benchmarking. There are a few good benchmarks in the community for different use cases, using different frameworks. The goal of my benchmark, in short, is to design an extensible, standardized benchmark that can be used for a variety of common performance use cases. Nightly performance regression tests are very important. Meanwhile it would be good if we can reuse the same benchmark for capacity planning, scalability study, troubleshooting, etc., which has slightly different requirement to nightly tests. It would be a good saving for everyone in community if he only needs to extend the benchmark, not rebuild one, for his own use cases in near future.\n\nIn addition, the benchmark includes a variety of instruments to help understand why the performance is, in addition to what the performance is. One obvious reason is that answering why is the primary goal for some use cases, such as troubleshooting, scalability study. Meanwhile it also helps to build 'correct' performance tests. For example, performance bottleneck discovered in tests may not be a code defect but some setup issue. Being able to analyze a bit can make sure the performance tests are testing the right thing. \n\nDesigning a good benchmark is one of my primary jobs at work. So I will continue to elaborate the framework and add new tests. There are a few good benchmarks for Solr. Also Vivek Narang has done a great job in designing a few new test cases. I can help you in porting or adding new test cases with my framework if you like.\n\nThe patch mainly includes object model and a sample test to demonstrate object model. More components will follow. It's an option to community of course but I do think community can benefit from this contribution. Any feedback is appreciated.\n ",
            "author": "Michael Sun",
            "id": "comment-16049655"
        },
        {
            "date": "2017-06-17T01:38:27+0000",
            "content": "Hi Michael Sun. Like you, I was also deeply concerned about the fluctuations in the resulting metrics and after digging deep into the problem, for last two days, and applying critical fixes, I am happy to inform you that the fluctuations are now been contained. Please access http://212.47.227.9/prod/. Some important changes are still remaining though. Regards ",
            "author": "Vivek Narang",
            "id": "comment-16052624"
        },
        {
            "date": "2017-06-20T14:23:24+0000",
            "content": "the fluctuations are now been contained\nVivek Narang  That's cool. What is the change to solve the fluctuations? ",
            "author": "Michael Sun",
            "id": "comment-16055839"
        },
        {
            "date": "2017-06-21T10:00:34+0000",
            "content": "Hi Michael Sun It changed after identifying and handling resource contention. \n\nSadly for Indexing (using ConcurrentUpdateSolrClient) on the SolrCloud, there are still fluctuations noted (I am guessing because ConcurrentUpdateSolrClient uses HttpSolrClient instead of CloudSolrClient, see: https://github.com/apache/lucene-solr/blob/master/solr/solrj/src/java/org/apache/solr/client/solrj/impl/ConcurrentUpdateSolrClient.java#L105-L107) ",
            "author": "Vivek Narang",
            "id": "comment-16057279"
        },
        {
            "date": "2017-06-21T14:26:03+0000",
            "content": "It changed after identifying and handling resource contention\nWhat was the resource contention? ",
            "author": "Michael Sun",
            "id": "comment-16057599"
        },
        {
            "date": "2017-06-28T06:47:15+0000",
            "content": "Hi Ishan Chattopadhyaya & Mikhail Khludnev \nBelow are the action points (In order of importance), from the Round 1 evaluation meeting.\n\n\n\tSwitching from test data, currently used, to the use of Wikipedia data for index benchmarking.\n\tMerging menu items (eg. Merging standalone numeric query tests under category Standalone Metrics etc.)\n\tCode formatting (Removing spaces, indentation, missing javadocs etc.)\n\tAdding an option to put all the graps on one page (category wise)\n\tArgs parameter renaming.\n\tUse of log4j as applicable.\n\tAdjust labels on graphs/charts\n\n\n\nI am happy and thankful to have awesome Mentors like you!\n\nRegards\nVivek Narang\n\n\n ",
            "author": "Vivek Narang",
            "id": "comment-16066003"
        },
        {
            "date": "2017-07-05T23:28:06+0000",
            "content": "Just uploaded a second cut of Solr benchmark framework I built at work. In addition to object model, this patch has a working JSON facet benchmark using JMeter and measures CPU usage during benchmark. More importantly, the code for JMeter and CPU usage are easily reusable in other benchmarks.\n\nAs mentioned earlier, the idea of this framework is to avoid building a new framework for all Solr performance work of upcoming years, by carefully designed, extensible object model and reusable components. I am aware of several existing Solr benchmarks, inc. one actively being developed now. Any feedback is appreciated.  ",
            "author": "Michael Sun",
            "id": "comment-16075617"
        },
        {
            "date": "2017-07-05T23:30:54+0000",
            "content": "Vivek Narang I saw you made some good progress. Not sure what framework you decided to use. But if you wanted to use your own, make sure add CPU util metric in test result. You are welcome to use code in my patch for CPU measurement.\n ",
            "author": "Michael Sun",
            "id": "comment-16075623"
        },
        {
            "date": "2017-07-07T13:26:34+0000",
            "content": "Hi, Michael Sun Thanks. I will look into it soon. Regards. ",
            "author": "Vivek Narang",
            "id": "comment-16078068"
        },
        {
            "date": "2017-07-11T06:39:57+0000",
            "content": "Vivek Narang, please report on the status of these pending items.\nhttps://issues.apache.org/jira/browse/SOLR-10317?focusedCommentId=16066003&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16066003\n\nI'm mostly concerned about items 1 (using Wikipedia data in your benchmarks) and 4 (having a unified, one page view of your graphs). ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16081756"
        },
        {
            "date": "2017-07-12T14:16:15+0000",
            "content": "Hello Ishan Chattopadhyaya\n\nWikipedia data - done.\nUnified page view - pending.\nMerging menu items - done.\nCode formatting - in progress, partially done.\nArgs parameter renaming - done.\nLog4j - done.\nLabel adjustment - pending. \n\nRegards\nVivek ",
            "author": "Vivek Narang",
            "id": "comment-16084044"
        },
        {
            "date": "2017-07-22T05:56:39+0000",
            "content": "For Indexing benchmarks, we need multiple threads.\nFor querying benchmarks, we need multiple threads and the following metrics: Average latency, 90th %ile latency, 99th%ile latency, throughput. Please refer to JMeter for this functionality.\n\nThe current graph, http://212.47.242.214/IndexingThroughputBenchmarkStandalone.html#, is mostly useless since no information on threads or documents indexed are specified. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16097127"
        },
        {
            "date": "2017-07-22T06:03:05+0000",
            "content": "Also, add something like this [0] to help users download the data files used for benchmarking.\n\n[0] - https://da.gd/x0aR ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16097128"
        },
        {
            "date": "2017-07-22T17:26:29+0000",
            "content": "Hi Ishan Chattopadhyaya\n\nMultiple threads have already been implemented. Please see http://212.47.242.214/IndexingThroughputBenchmarkStandaloneConcurrent.html\nI think you got confused between two different tests. One test uses HTTP Solr client while the other with threads uses Concurrent Update client. \n\nThe ability to locate and download test data files is awesome and now the system will be able to check the data file presence and download the missing file(s) by itself as required. The source URL is configurable through the properties file. \n\nRegards\nVivek  ",
            "author": "Vivek Narang",
            "id": "comment-16097386"
        },
        {
            "date": "2017-07-26T20:19:06+0000",
            "content": "Thanks Vivek. As per our offline discussion, I can see that you have collapsed the \"serial\" benchmarks into the \"concurrent\" benchmarks with a threads=1 line. Looks much less confusing now!\n\nCan you please document the steps on how to run the benchmarks for past n days (not past n commits), with just one commit per day? Even if the steps are manual in nature, please document them. Going forward, it would be good to have some parameters to let us do that (perhaps also allow for a time of the day).\n\nAlso, I suggest that top bar menu be very simple: Standalone Metrics and Cloud Metrics. Both could link to the \"merged\" view (better call it \"unified\" view). Each of the graphs can be linked to the individual pages, and each page could have a back button that takes the user back to the \"unified\" page.\n\nAlso, can you please list down the major TODO items? ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16102215"
        },
        {
            "date": "2017-07-27T19:14:25+0000",
            "content": "Hi Ishan Chattopadhyaya\n\nSteps to run for N days are added in the README file. Yes, currently, it has to be done manually but I will try to add a new feature so this could be done using parameters very soon. \nMenu for standalone and Cloud have been re-arranged as you described above. Now a \"More Details\" link on each panel on the Unified view pages will take the user to the linked page. The back button is redundant since there are links on the top menu to browse around as required. If you still prefer it, please let me know and I will add it on the pages. Please check http://212.47.242.214\n\nI will compile and list the TODO items shortly.\n\nRegards\nVivek ",
            "author": "Vivek Narang",
            "id": "comment-16103731"
        },
        {
            "date": "2017-07-28T00:07:18+0000",
            "content": "Hi Ishan Chattopadhyaya\n\nI have added additional capability to archive performance data (archive parameter) and clear performance data (clear-data parameter). Please check the latest readme file.\n\nRegards\nVivek ",
            "author": "Vivek Narang",
            "id": "comment-16104154"
        },
        {
            "date": "2017-07-29T17:44:19+0000",
            "content": "Hi Ishan Chattopadhyaya\nI have fixed the label bug on graphs. Please see http://212.47.242.214/MergedViewStandalone.html\n\nRegards\nVivek ",
            "author": "Vivek Narang",
            "id": "comment-16106194"
        },
        {
            "date": "2017-07-29T20:12:15+0000",
            "content": "I saw this in the section for running last n days of commits.\n\n * While there is no feature implemented (yet), that supports automatic crunching of performance data for commits for last (n) days, there is still a way to do this.\n * Prepare and arrange (in order) a list of commit hashes for the (n) days, either one for each day or all of the commits for (n) days.\n * Now create empty files with name (no extension) EXACTLY equal to commit hashes in the sub-folder /data/commit_queue/ found in the directory as configured in the properties file for benchmarkAppDirectory parameter. The hashes will be picked in the order of their creation (FIFO).\n * Finally, in the section \"Steps to run\" follow all the steps but for step 8 above, use parameter --from-queue instead of --latest-commit\n\n\n\nJust use a shell script like this to do print last 30 days of commits (the last commit of every day):\n\n# Print the last commit of every day, for past 30 days, in the order of most recent day first\nfor n in {1..30}; do date=`date  --date=\"$n days ago\" +%Y-%m-%d`; git log --after=\"$date 00:00\" --before=\"$date 23:59\"|head -1|cut -f 2 -d \" \"; done\n\n\n\nSteps to run for N days are added in the README file. Yes, currently, it has to be done manually but I will try to add a new feature so this could be done using parameters very soon. \nNo need to add a parameter. Shell scripting can be your friend. All the steps that you mentioned can be automated using the above script; just modify the shell script to pass the commit to the --commit parameter.\nKindly update the steps in the README asap. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16106232"
        },
        {
            "date": "2017-07-29T20:17:13+0000",
            "content": "And please learn some markdown! Your README file [0] is unreadable due to horizontal scrollers in each section.\n\n[0] - https://github.com/viveknarang/lucene-solr/tree/SolrNightlyBenchmarks/dev-tools/SolrNightBenchmarks ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16106233"
        },
        {
            "date": "2017-07-29T20:33:28+0000",
            "content": "I have fixed the label bug on graphs. Please see http://212.47.242.214/MergedViewStandalone.html\nThanks\n\nI will compile and list the TODO items shortly.\nWhat is the status of this? As per GSoC guidelines, a student is expected to put in about 40 hours of work each week (which is approximately 8 hours a day). Couldn't you spare 5 minutes from those 8-16 hours (in last 2 days) to get to this?\n\nI requested you to share the weekly status with the mentors via email (which you have been doing, even though the level of detail can be improved). Going forward, can you kindly post the daily status here in JIRA and include sufficient details on work you have done each day? Daily status reporting has been something that other GSoC projects have benefited from, and I'd like to try this to ensure that we are on track to complete this project as well as possible.\n\nThe ability to locate and download test data files is awesome and now the system will be able to check the data file presence and download the missing file(s) by itself as required. The source URL is configurable through the properties file. \n\nThis is just unnecessary complexity to the system. Add a shell script to download the data files (similar to the one I pasted above) [0]. \n\nYour README file says:\n\nTest Data Files\n\n\tThe system has the ability to check and download required data files from the source, as specified in the properties file.\n\tIf you want to manually download files please use the link provided below.\n\n\n\nWhy are you calling it \"test data files\"? Also, as a user, I don't fucking care about what ability the system has. I want information on how to use whatever fucking ability the system has. You need to be specific in terms of steps the user needs to take.\n\nSecondly, \"please use the link provided below\" is equivalent to telling the user to \"go f*** yourself\". Just provide an exact command that will enable the user to download those files!\n\nPlease understand that if someone is not able to use your system due to poor documentation or poor interface, then your system is useless to him/her. As of now, this entire project is useless to anyone except you or me. I urge you to fix this aspect at the earliest.\n\n[0] - https://da.gd/x0aR ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16106234"
        },
        {
            "date": "2017-07-29T20:46:09+0000",
            "content": "So far, the project has relied on some manual steps to deploy a webapp on a webserver. This is unnecessary complexity on the part of the admin who is setting it up. Can you please use a Jetty based embedded web-server, which someone can run using an ant or mvn command? Upon starting this embedded web-server, it can serve all the UI and reports through a pre-configured port. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16106238"
        },
        {
            "date": "2017-07-29T20:51:57+0000",
            "content": "I'd like to add, however, that overall, the project looks to be in great shape, and you have done a good job at it so far. The second month evaluation has just completed, and you have passed that evaluation. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16106240"
        },
        {
            "date": "2017-07-30T03:08:33+0000",
            "content": "Hi Ishan Chattopadhyaya,\nI have sent a detailed status email for this week combined with last week a few minutes before. Since major current and future planned goals are already completed by now (as also noted in the email) the TODO items as of now are: Code refactoring, working on some of the points noted by you above and thorough testing of the platform. I will provide additional updates shortly.  ",
            "author": "Vivek Narang",
            "id": "comment-16106295"
        },
        {
            "date": "2017-07-30T07:19:36+0000",
            "content": "Hi Ishan Chattopadhyaya\n\nYour README file [0] is unreadable due to horizontal scrollers in each section.\n\nFixed now please check.\n\nhttps://github.com/viveknarang/lucene-solr/tree/SolrNightlyBenchmarks/dev-tools/SolrNightBenchmarks\n\nYou need to be specific in terms of steps the user needs to take.\n\nhttps://github.com/viveknarang/lucene-solr/tree/SolrNightlyBenchmarks/dev-tools/SolrNightBenchmarks#steps-to-launch\n\nThe steps have always existed on this file! Do you find any steps missing?\n\nWhy are you calling it \"test data files\"?\n\nPlease suggest what you want these files to be referred as. \n\nAlso, as a user, I don't fucking care about what ability the system has.\n\nThe referred part has been removed from the README file.\n\nCouldn't you spare 5 minutes from those 8-16 hours (in last 2 days) to get to this?\n\nI wanted to finish a group of important in-progress tasks before updating the TODO items.\n\nGoing forward, can you kindly post the daily status here in JIRA and include sufficient details on work you have done each day?\n\nSure! \n\nSecondly, \"please use the link provided below\" is equivalent to telling the user to \"go f*** yourself\"\n\nThe link has been provided as an option and is not a required step. When the user has configured the system to automatically download required files, there is no need for the user to manually download files.\n\nThis is just unnecessary complexity to the system. Add a shell script to download the data files (similar to the one I pasted above) [0].\n\nI don't think there is any complexity here. Checking the existence of these files is already an essential step. The files are only downloaded when missing and essentially a wget is done to download files please see code below. If you still insist I will create a shell script for this. \n\nhttps://github.com/viveknarang/lucene-solr/blob/SolrNightlyBenchmarks/dev-tools/SolrNightBenchmarks/src/main/java/org/apache/solr/tests/nightlybenchmarks/Util.java#L737 ",
            "author": "Vivek Narang",
            "id": "comment-16106352"
        },
        {
            "date": "2017-07-30T08:04:21+0000",
            "content": "\n>    You need to be specific in terms of steps the user needs to take.\n\nhttps://github.com/viveknarang/lucene-solr/tree/SolrNightlyBenchmarks/dev-tools/SolrNightBenchmarks#steps-to-launch\nThe steps have always existed on this file! Do you find any steps missing?\n\nWhat existed and exists now is \"The system has the ability to check and download required data files from the source, as specified in the properties file\". Again, why would a user care about the system's ability to download files? What he needs to know is, how to invoke that ability. Which \"properties file\", which property in that properties file needs to be adjusted? How to enable/disable this ability etc.?\n\n\nI don't think there is any complexity here. Checking the existence of these files is already an essential step. The files are only downloaded when missing and essentially a wget is done to download files please see code below. If you still insist I will create a shell script for this. \nChecking for existence is good. But automatically downloading it is unnecessary complexity. \"essentially a wget is done to download files\" <--- why the f*** would you want to invoke a shell command from within Java, when you can have the user/admin do it before launching your service? \n\nDownloading files requires a specialized software, and the benchmarking suite should not be concerned with that. Resuming partial downloads, multi-threaded downloading, etc. are special features that the downloader should take care of. I think the business of downloading files should be left for the user to deal with.\n\n\n>    Secondly, \"please use the link provided below\" is equivalent to telling the user to \"go f*** yourself\"\n\nThe link has been provided as an option and is not a required step. When the user has configured the system to automatically download required files, there is no need for the user to manually download files.\n\nThat link is useless. How will the user download all the files from that link?\n\n\n>     Why are you calling it \"test data files\"?\n\nPlease suggest what you want these files to be referred as. \nPerhaps \"Benchmarking data files\" or \"Data files\".\n ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16106364"
        },
        {
            "date": "2017-07-30T08:29:28+0000",
            "content": "Hi Ishan Chattopadhyaya\nPerhaps \"Benchmarking data files\" or \"Data files\".\n\nWill Rename.\n\nThat link is useless. How will the user download all the files from that link?\n\nLink Removed.\n\nDownloading files requires a specialized software, and the benchmarking suite should not be concerned with that. Resuming partial downloads, multi-threaded downloading, etc. are special features that the downloader should take care of. I think the business of downloading files should be left for the user to deal with.\n\nShell script coming up shortly. \n\n...which property in that properties file needs to be adjusted? How to enable/disable this ability etc.?\n\nMore detailed steps coming up shortly. ",
            "author": "Vivek Narang",
            "id": "comment-16106376"
        },
        {
            "date": "2017-07-30T08:36:58+0000",
            "content": "\n>    That link is useless. How will the user download all the files from that link?\n\nLink Removed.\nI think you are not understanding what I'm trying to convey above. I need you to provide clear instructions on how to download the data manually. \n\nIf father tells a son to go fetch some spare parts from a store and tells the address of the store to the son, the son still has no clue how to bring them in. Clear instructions should include, exact names of parts and their locations within the store.\n\nSimilarly here, exact URLs of all the individual files is needed. The user should be able to copy paste something from your readme file, run it and he would have his files ready. It could be a script invocation (that downloads all the files for him) or a list of wget commands.\n\nPreferably, even MD5 checksums should be added to make it easy to ensure that the files were downloaded completely and without corruption. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16106383"
        },
        {
            "date": "2017-07-30T08:42:03+0000",
            "content": "\n>    ...which property in that properties file needs to be adjusted? How to enable/disable this ability etc.?\n\nMore detailed steps coming up shortly.\nJust remove that ability. The benchmarking suite need not download files on its own. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16106384"
        },
        {
            "date": "2017-07-30T13:39:51+0000",
            "content": "How can I see logs for these runs? Are they stored somewhere (as they should be)?\n\nAlso, in the console logs, I see lines like these:\n\nEnding State: | 0| 0| 238842| 0| 0| 0| 0| 0| 0| 0| 0\nEnding State: | 0| 0| 238842| 0| 0| 0| 0| 0| 0| 0| 0\n\n\nWhat do they mean?\n\nAlso, what do the following mean?\n\norg.apache.solr.tests.nightlybenchmarks.QueryClient@427b6220** Getting out of critical section ...\norg.apache.solr.tests.nightlybenchmarks.QueryClient@25174eb0** Getting out of critical section ...\norg.apache.solr.tests.nightlybenchmarks.QueryClient@35ef7d35** Getting out of critical section ...\n\n\n\nAlso, I saw the following exceptions:\n\njava.lang.NullPointerException\n        at org.apache.solr.tests.nightlybenchmarks.QueryClient.run(QueryClient.java:342)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n        at java.lang.Thread.run(Thread.java:748)\n\n\nHowever, the execution continued without complaining. That shows that exception handling is not proper. Such exceptions should be fatal and the entire benchmarking suite should crash out after such exceptions. Silently ignoring them seems like a horrible thing to do. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16106499"
        },
        {
            "date": "2017-07-30T13:55:59+0000",
            "content": "My suggestion, as I have discussed with you before, would be to use Log4J (or any other similar logging framework). You can easily log INFO, DEBUG, ERROR and WARNING level messages. Also, the logs would be preserved even after the runs finish. It seems that currently, the only logging that is happening is at the console and hence the logs are not preserved after the run finishes. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16106507"
        },
        {
            "date": "2017-07-30T14:51:36+0000",
            "content": "Such exceptions should be fatal and the entire benchmarking suite should crash out after such exceptions. Silently ignoring them seems like a horrible thing to do.\nhttps://github.com/viveknarang/lucene-solr/blob/SolrNightlyBenchmarks/dev-tools/SolrNightBenchmarks/src/main/java/org/apache/solr/tests/nightlybenchmarks/QueryClient.java#L477-L479\n\nThe individual benchmarks should crash on failures. Failures indicate some problem with either Solr or the benchmarking suite. Failures due to the former should be discovered asap. Failures due to the latter must not exist, and lets fix all such possibilities.\n\nIn the above case, you're polling from a queue and not even checking if the poll was successful; hence the NPE. Please fix, and please stop ignoring exceptions! ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16106525"
        },
        {
            "date": "2017-07-30T15:03:36+0000",
            "content": "This multi-coloured console logging is extremely confusing. What do these different colours mean? My intuition tells me that red should be errors, green should be info and yellow should be warnings. However, there seems absolutely no logic behind this colouring scheme, and there are at least 6 colours in that screenshot.\n\n ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16106529"
        },
        {
            "date": "2017-07-30T17:10:46+0000",
            "content": "Had a discussion with Vivek over Hangout, and we discussed all these issues. Vivek will create Github issues (on his repository) to track all these issues, and fix all of them asap.\n\nhttps://github.com/viveknarang/lucene-solr/issues ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16106566"
        },
        {
            "date": "2017-07-30T18:44:50+0000",
            "content": "Hi Ishan Chattopadhyaya\nI have added issues on the list. Thanks for pointing out important gaps. I am working on them and will provide updates soon. \n\nRegards ",
            "author": "Vivek Narang",
            "id": "comment-16106602"
        },
        {
            "date": "2017-07-31T03:22:27+0000",
            "content": "Hi Ishan Chattopadhyaya,\nWork status as of now is as follows:\n\n\n\tRemove Internal queuing capability: Done\n\tRename Root and webapp directory: Done\n\tCreate a script for processing commits for last n days: In progress\n\tMissing exception chaining/handling: Done, pending verification.\n\tLog4j implementation: Done. (solrnightlybenchmarks.log file created in /logs folder)\n\tNull pointer exception handling across all class files: In progress\n\tProviding shell script for download: Done (download.sh script in /scripts folder)\n\tEmbedded Jetty server support, removing external HTTP server dependency: Done\n\n\n\nAs of now, I am currently testing against the changes made today.\n\nRegards ",
            "author": "Vivek Narang",
            "id": "comment-16106760"
        },
        {
            "date": "2017-08-01T03:32:44+0000",
            "content": "Hi Ishan Chattopadhyaya\nStatus for today:\nTesting, adding more logging and RCA for the observed errors. I see that the test is running fine for an older commit eg: 5f1c5ce81286ab2e0c0f98c47d332511ea57e8c1 while there are some errors noted when running for a recent commit eg: 3a405971b9e06e2004e0d66ae1b82f530de969f2. \n\nRegards\n\n ",
            "author": "Vivek Narang",
            "id": "comment-16108330"
        },
        {
            "date": "2017-08-02T03:31:14+0000",
            "content": "Hi Ishan Chattopadhyaya\nStatus For Today:\nConducted tests and added more logging & Null checks.\n\nAfter doing the RCA on the issue observed recently, I have come to a conclusion that a recent commit done in Solr, is the cause for this issue. \nPlease see this: https://github.com/apache/lucene-solr/commit/67b3d4e108b82b56ca45b9e78bcac52bd2280c8b. \nThis is the commit after which the issue started appearing (the ones you noted a few comments before). If you observe lines 128 and 149 (_default/conf/managed-schema), you see that *_pi is removed and now *_i must be used in instead of it. \n\nWork in progress:\n\n\tAdding an option for dev-tests where a small data set and small query set will allow for quicker execution of the benchmark framework, speeding up the development/debugging process.\n\tAdding more checks in the code.\n\n\n\nRegards   ",
            "author": "Vivek Narang",
            "id": "comment-16110222"
        },
        {
            "date": "2017-08-03T03:24:07+0000",
            "content": "Hi Ishan Chattopadhyaya,\nStatus for Today:\nTesting and refactoring QPS calculation logic - controlling the number of queries instead of the time period. \n\nRegards ",
            "author": "Vivek Narang",
            "id": "comment-16112115"
        },
        {
            "date": "2017-08-04T03:28:28+0000",
            "content": "Hi Ishan Chattopadhyaya,\nStatus for Today:\n\n\tTesting the new QPS calculation logic.\nRegards\n\n ",
            "author": "Vivek Narang",
            "id": "comment-16113867"
        },
        {
            "date": "2017-08-05T03:59:10+0000",
            "content": "Hi Ishan Chattopadhyaya,\nStatus:\n-Testing complete and bugs fixed in the new QPS calculation logic, the code will be pushed in one hour. \nRegards. ",
            "author": "Vivek Narang",
            "id": "comment-16115261"
        },
        {
            "date": "2017-08-06T10:20:43+0000",
            "content": "Kindly, be specific in your status reports. Phrases like \"new QPS calculation logic\" or \"bugs fixed\" have no meaning unless you link to a Github issue describing your \"new .. logic\" or the exact bugs you fixed. Ideally, those issues should be linked to your exact commits for fixing/implementing them.\n\nAlso, please fix the following parameter names:\n--commit-id could be as simple as --commit, --test-with-number-of-documents could be as simple as --num-docs, --use-sample-dataset could be as simple as --sample\n\nAlso, what is the difference between --use-sample-dataset and --test-with-number-of-documents? Does the latter only control the index size?\n\nThis description,\n\n     * --use-sample-dataset X.XX               Use this option when you want to work in dev-mode (i.e while enhancing\n                                               /debugging this project.). Please also pass a value in the range 0.01 to 1\n                                               with this parameter. This value is the percentage of data set that is used\n                                               in this mode. \n\n\ncould be:\n\n     * --use-sample-dataset X.XX               This value (0 - 1.0) is the fraction of dataset & queries used for indexing and querying. Useful in dev-mode. Default value, when this parameter is not specified, is 1.0\n\n ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16115745"
        },
        {
            "date": "2017-08-06T10:32:09+0000",
            "content": "A huge problem with the benchmarking suite is that it takes 6.5 hours to run one benchmarking run, end to end, irrespective of how fast the machine is! This clearly shows there's some major problem with how these benchmarks are implemented. I'd expect the benchmarks to finish quicker on a faster machine. For the benchmarks that the suite currently runs, I find it very odd that it takes so long to run, end to end. Can you please explain why this happens? I'm planning to take a deeper look into the code soon to figure out why. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16115749"
        },
        {
            "date": "2017-08-06T10:44:16+0000",
            "content": "This is just ridiculous:\n\n\tif (nodes == 2 && shards == \"1\" && replicas == \"2\") {\n...\n\t} else if (nodes == 2 && shards == \"2\" && replicas == \"1\") {\n...\n\t} else if (nodes == 3 && shards == \"1\" && replicas == \"3\") {\n...\n\t} else if (nodes == 4 && shards == \"2\" && replicas == \"2\") {\n\n\n\nhttps://github.com/viveknarang/lucene-solr/blob/SolrNightlyBenchmarks/dev-tools/solrnightlybenchmarks/src/main/java/org/apache/solr/tests/nightlybenchmarks/Tests.java#L301\n\nAll of these should be configurable and easily extendible (extensible?). ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16115754"
        },
        {
            "date": "2017-08-06T11:37:30+0000",
            "content": "https://github.com/viveknarang/lucene-solr/blob/SolrNightlyBenchmarks/dev-tools/solrnightlybenchmarks/src/main/java/org/apache/solr/tests/nightlybenchmarks/TestPlans.java#L32-L34\n\n\tpublic enum BenchmarkTestType {\n\t\tPROD_TEST, DEV_TEST\n}\n\n\n\nThere should be no concept of \"prod\" or \"dev\". It should be a benchmark that relies on configuration rather than assumed defaults like \"prod\" or \"dev\".\n\nAlso, I don't think we should be using terms like \"tests\" for individual benchmarks. There is no verification or assertions in these benchmarks, other than just timing data collection.\n\nThe way the entire code is laid out, it is extremely hard to add new benchmarks. It might require an entirely new GSoC project next year to make this useful for the community. Hard coded test scenarios, really? https://github.com/viveknarang/lucene-solr/blob/SolrNightlyBenchmarks/dev-tools/solrnightlybenchmarks/src/main/java/org/apache/solr/tests/nightlybenchmarks/MetricCollector.java#L31-L219 \nWe need to make this configurable at the earliest.\n\nI my opinion, the way the benchmarks in this suite should configured as:\n\n{\n  \"index-benchmarks\": [\n    {\n      \"name\": \"CLOUD_INDEXING\",\n      \"description\": \"some shit\",\n      \"replication-type\": \"cloud\",\n      \"dataset-file\": \"filename containing data\"\n      \"setups\": [\n        {\n          \"collection\": \"cloud_2x2\",\n          \"replicationFactor\": 2,\n          \"shards\": 2,\n          \"min-threads\": 1,\n          \"max-threads\": 16\n        },\n        {\n          \"collection\": \"cloud_1x1\",\n          \"replicationFactor\": 1,\n          \"shards\": 1,\n          \"min-threads\": 1,\n          \"max-threads\": 16\n        },\n        {\n          \"collection\": \"cloud_1x2\",\n          \"replicationFactor\": 2,\n          \"shards\": 1,\n          \"min-threads\": 1,\n          \"max-threads\": 16\n        }\n      ]\n    },\n    {\n      \"name\": \"CLOUD_PARTIAL_UPDATE\",\n      \"description\": \"some shit\",\n      \"replication-type\": \"cloud\",\n      \"dataset-file\": \"filename containing full documents\",\n      \"updates-file\": \"filename containing updates\",\n      \"setups\": [\n        {\n          \"collection\": \"partial_2x2\",\n          \"replicationFactor\": 2,\n          \"shards\": 2,\n          \"min-threads\": 1,\n          \"max-threads\": 16\n        },\n        {\n          \"collection\": \"partial_1x1\",\n          \"replicationFactor\": 1,\n          \"shards\": 1,\n          \"min-threads\": 1,\n          \"max-threads\": 16\n        },\n        {\n          \"collection\": \"partial_1x2\",\n          \"replicationFactor\": 2,\n          \"shards\": 1,\n          \"min-threads\": 1,\n          \"max-threads\": 16\n        }\n      ]\n    }, \n    ... more such benchmarks ...\n  ],\n  \"query-benchmarks\": [\n    {\n      \"name\": \"TERM_NUMERIC_QUERY_CLOUD_2T\",\n      \"description\": \"some shit describing the benchmark\",\n      \"replication-type\": \"cloud or standalone\"\"collection/core\": \"<name of the collection/core, that must've already been created during the indexing phase>\",\n      \"query-file\": \"name of file containing all the queries for this benchmark\",\n      \"client-type\": \"CUSC or CSC or HSC etc.\",\n      \"min-threads\": 1\"max-threads\": 8\n    }, \n    ... more such benchmarks ...\n  ]\n}\n\n\n\nBased on this, the suite will do the right thing. Various things to consider here:\n\n\tPartial updates benchmarks should:\n\nfor every replicationFactor, shards, thread combination:\n  Create a new collection with given name, and given replicationFactor and shards\n  Index the full dataset without timing.\n  Start timer\n  Update all documents\n  Stop timer, record difference in time\n  Delete this collection.\n\n\n\tFull document indexing benchmarks should:\n\nfor every replicationFactor, shards, thread combination:\n  Create a new collection with given name, and given replicationFactor and shards\n  Start timer\n  Index the documents\n  Stop timer, record difference in time\n  if (numThread != maxThread):\n     Delete this collection\n  else\n     // don't delete, since this collection needs to stay for query benchmarks\n\n\n\tFor every query benchmark:\n\nfor every collection, thread combination:\n  Stop and start all Solr nodes (so that caches are cleared)\n  Wait till all replicas for the given collection are \"active\"\n  Issue around 100-200 queries to warm up the searchers.\n  Start timer\n  Query the collection using the given numThreads\n  Stop timer, record difference in time\n\n\n\n\n\nIn case there's any information that the graphs need, but not covered here, please comment/discuss.\n\nWhat do you think of the above proposal (in general or in specific parts) to make the suite easier to configure/manage/extend? ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16115774"
        },
        {
            "date": "2017-08-08T18:17:25+0000",
            "content": "Unbxd Inc., the company I am doing Solr consulting for currently, has generously decided to support this effort by bearing the cost of bare metal servers that Vivek is currently using. These include the two C2L Scaleway servers (https://www.scaleway.com/pricing) that Vivek has been using for past two months, and shall be continuing to use for on-going development. Apart from that, Unbxd will also cover the cost of hardware to publicly host the nightly benchmarking service, on stable bare metal servers (Scaleway, Packet.net etc.),  going forward. The specifics of the latter, i.e. the type of boxes we need for this, coordination required with Apache's infra team etc. are TBD.\nFor a background, Unbxd uses Apache Solr and hosts a large number of collections (for e-commerce search and discovery) and is excited to support the continuous benchmarking of SolrCloud (along with the various plugins developed in-house). ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16118791"
        },
        {
            "date": "2017-08-08T18:19:02+0000",
            "content": "Vivek Narang, you have not posted the daily status for Monday, 7 August. Have you reviewed the above concerns and my comments about making the benchmarking suite more configurable? ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16118794"
        },
        {
            "date": "2017-08-08T18:21:37+0000",
            "content": "Unbxd Inc., the company I am doing Solr consulting for currently, has generously decided to support this effort by bearing the cost of bare metal servers that Vivek is currently using.\n\nAwesome and thanks!  I'm looking forward to seeing periodic benchmarks we can all access. ",
            "author": "David Smiley",
            "id": "comment-16118796"
        },
        {
            "date": "2017-08-08T19:06:04+0000",
            "content": "Hello Ishan Chattopadhyaya, \nI have been unwell for last three days, will try to resume work as soon as possible. ",
            "author": "Vivek Narang",
            "id": "comment-16118865"
        },
        {
            "date": "2017-08-08T19:18:07+0000",
            "content": "Thanks for informing, Vivek. \nI have been unwell for last three days\nHope you get well soon, since the suite is coming along really nicely so far. However, when you are unable to work, kindly inform as soon as possible.  ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16118880"
        },
        {
            "date": "2017-08-10T16:27:50+0000",
            "content": "Hi Ishan Chattopadhyaya,\nI am feeling better now, Thanks! I took this time to think about your suggestions and here is my opinion.\n\nYes, it is a good idea to make this configurable but there are some challenges to it. Even if the backend (leading to the point where CSVs are generated) is made dynamic (which is not as straight forward as it seems), the front end will have to be made dynamic which violates the initial agreement of keeping the front end static as discussed initially. Limitations on the front-end are one of the reasons why the implementation is the way it is. We need to make the front end dynamic for this idea to be realistic. The suite even in the current state is extensible but for that one will have to modify the classes and the front-end to add/modify benchmarks. However, one can always find better ways to do things and JSON based configurable suite is a good idea as an improvement! \nI want this benchmark project to be useful for the community and I would not recommend waiting for another year to make the suggested changes. However, since I anticipate these changes to be time taking and since the deadline is just over a week away, I recommend creating a duplicate branch of the current project and implement your suggestions over the coming weeks. \n\nRegards ",
            "author": "Vivek Narang",
            "id": "comment-16121865"
        },
        {
            "date": "2017-08-10T17:25:27+0000",
            "content": "Good to know you are better. Just do it. You have 19 days left in the project. https://developers.google.com/open-source/gsoc/timeline\n\nIf you need help in making the frontend dynamic, just give me a shout out. I'll get you some expert to guide you. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16121941"
        },
        {
            "date": "2017-08-10T17:31:38+0000",
            "content": "Discuss the design here before you start implementing. We don't want you to design another inflexible system. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16121960"
        },
        {
            "date": "2017-08-11T19:55:20+0000",
            "content": "Any update, please? Vivek Narang\nIf you need to reach me, please reach out to me over Hangout/e-mail. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16123940"
        },
        {
            "date": "2017-08-11T19:58:50+0000",
            "content": "> If you need help in making the frontend dynamic, just give me a shout out. I'll get you some expert to guide you.\nUpayavira, in case Vivek Narang needs some help with the UI of this tool, would it be possible for you to help him a bit (in advisory/mentoring capacity)? I am a bit challenged in frontend design and development, and I'm afraid that I'll be of very little help to him if he needs it. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16123952"
        },
        {
            "date": "2017-08-12T02:45:55+0000",
            "content": "Hello Ishan Chattopadhyaya,\nInitial steps for backend refactoring (up to the point of creating CSV files): \nTop layer - JSON meta data file, followed by creating a list of BenchmarkConfiguration objects based on parsed information. A BenchmarkConfiguration object holds the information on a specific benchmark which is then passed across several layers inside providing information at each level. Against each benchmark, there is a map object that holds the results that can then be passed to the point that creates the CSV files.  This approach will remove a major portion of partially redundant code in the Tests.java as well as remove the enums that were limiting configuration at various levels.\n\nPlease check sample code on a new branch that tries to showcase a part of this approach.\nhttps://github.com/viveknarang/lucene-solr/commit/0869a7c2aa131de6afdca38e04fa4627f87d6a46\n\nRegards. ",
            "author": "Vivek Narang",
            "id": "comment-16124417"
        },
        {
            "date": "2017-08-12T06:08:14+0000",
            "content": "Can you please elaborate:\n\n\twhy these CSV files are needed,\n\twhat is the format,\n\twho uses it\n\thow it is used?\n\n ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16124482"
        },
        {
            "date": "2017-08-12T06:54:49+0000",
            "content": "These CSV files are needed at the front-end side. \n\n1. Primary Consumer of CSV files - The dygraph API.\n2. Format changes with the required graph layout. [Col1 - X-axis, Col2,Col3 ... etc are the Y-axis]. The first line in the CSV files is the visible label as seen on the graphs.\n3. Dygraph uses these CSV data files.\n4. These CSV files are referenced to the calling JS functions from the dygraph API, and then they are handled by the dygraph API internally.\n ",
            "author": "Vivek Narang",
            "id": "comment-16124490"
        },
        {
            "date": "2017-08-12T06:59:37+0000",
            "content": "What is the plan for the frontend redesign, so as to make it work with your above backend redesign? ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16124494"
        },
        {
            "date": "2017-08-12T07:39:14+0000",
            "content": "The front-end plan is not clear yet. I will update as soon as I have more clarity on the approach.  ",
            "author": "Vivek Narang",
            "id": "comment-16124510"
        },
        {
            "date": "2017-08-12T07:47:20+0000",
            "content": "Thanks for the update, Vivek Narang. Would you available for a call to discuss the plan/approaches on Monday (or any other time) over a voice call/hangout? ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16124514"
        },
        {
            "date": "2017-08-13T02:54:37+0000",
            "content": "Hey Ishan Chattopadhyaya, I think an awesome dynamic UI mechanism idea just struck me. i'll summarize it soon. Regards ",
            "author": "Vivek Narang",
            "id": "comment-16124776"
        },
        {
            "date": "2017-08-14T14:23:28+0000",
            "content": "Hi Ishan Chattopadhyaya,\nLet's have a Google hangout meeting today (8/14) at 11 pm EST. Thanks ",
            "author": "Vivek Narang",
            "id": "comment-16125739"
        },
        {
            "date": "2017-08-14T14:52:23+0000",
            "content": "Sure, I'm ready.  ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16125786"
        },
        {
            "date": "2017-08-14T14:55:57+0000",
            "content": "Ah, 12h5m from now?  I'll stay up for the meeting. Ping me if you're available any time before that. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16125790"
        },
        {
            "date": "2017-08-14T15:12:54+0000",
            "content": "10-4 Captain! ",
            "author": "Vivek Narang",
            "id": "comment-16125810"
        },
        {
            "date": "2017-08-18T12:50:55+0000",
            "content": "Vivek, are there any updates, please? Full week as passed without any progress or updates. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16132135"
        },
        {
            "date": "2017-08-21T21:43:22+0000",
            "content": "Hi Ishan Chattopadhyaya,\nI was working on a big section of backend refactoring last week, I will send an update after committing the code tonight. Regards.  ",
            "author": "Vivek Narang",
            "id": "comment-16135892"
        },
        {
            "date": "2017-08-22T22:15:08+0000",
            "content": "Hello Ishan Chattopadhyaya, please switch to \"SolrNightlyBenchmarks-R2\" branch for the latest updates. Regards. ",
            "author": "Vivek Narang",
            "id": "comment-16137488"
        },
        {
            "date": "2017-08-25T05:25:47+0000",
            "content": "I see the branch just has pseudo code based on the approach (to make the suite configurable) we discussed last Sunday over the pair programming session (which is as per the comment here [0]). What's the status for actual working code?\n\nAlso, can you please document here the refactoring to frontend (to make this suite configurable) that we discussed on that call?\n\n[0] - https://issues.apache.org/jira/browse/SOLR-10317?focusedCommentId=16115774&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16115774 ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16141212"
        },
        {
            "date": "2017-08-25T06:14:15+0000",
            "content": "Refactoring is being done section by section, for now, two parts - QueryClient and Metric have been modified. Will add the required documentation soon.   ",
            "author": "Vivek Narang",
            "id": "comment-16141250"
        },
        {
            "date": "2017-08-29T16:37:22+0000",
            "content": "Hi Vivek Narang,\nDo you have any status, please? ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16145631"
        },
        {
            "date": "2017-08-29T17:13:56+0000",
            "content": "Vivek, unless I'm mistaken, the deadline of the GSoC has now passed. Can you please make a final submission?\nEither raise a Github PR or a attach a patch here. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16145707"
        },
        {
            "date": "2017-08-29T19:15:22+0000",
            "content": "GitHub user viveknarang opened a pull request:\n\n    https://github.com/apache/lucene-solr/pull/240\n\n    SOLR-10317\n\n    SOLR-10317: Integrate new benchmark suite\n    https://issues.apache.org/jira/browse/SOLR-10317\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/viveknarang/lucene-solr SolrNightlyBenchmarks\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/lucene-solr/pull/240.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #240\n\n\ncommit 2f6665b9ef8d218ffdd1116da7f65df1b183541d\nAuthor: Vivek Narang <vivek.narang10@gmail.com>\nDate:   2017-06-08T02:31:26Z\n\n    SolrNightlyBenchmarks font change ...\n\ncommit 1713a476f8c044ed47349bb22e32a673213272b4\nAuthor: Vivek Narang <vivek.narang10@gmail.com>\nDate:   2017-06-08T06:30:10Z\n\n    lastrun folder naming bug fix ...\n\ncommit 33e1b1839919e6f8ce367462636a18ed3d5aaee7\nAuthor: Vivek Narang <vivek.narang10@gmail.com>\nDate:   2017-06-08T07:12:47Z\n\n    duplicate commit in queue processing bug fix ...\n\ncommit eccf60ca2af17352b521cce6cd1b5e7400906d84\nAuthor: Vivek Narang <vivek.narang10@gmail.com>\nDate:   2017-06-09T00:47:29Z\n\n    Adding liscense header ...\n\ncommit 3aa6c6b4b1dac1fdf228a5c2caab6df3c3b36a48\nAuthor: Vivek Narang <vivek.narang10@gmail.com>\nDate:   2017-06-10T14:22:16Z\n\n    modifying Util class\n\ncommit ce3165f18d40a38e11534f46bd98ca97116b591b\nAuthor: Vivek Narang <vivek.narang10@gmail.com>\nDate:   2017-06-11T03:24:43Z\n\n    AND-OR numeric queries\n\ncommit d1f1ac75938c9d240cc21d38af26c7315ad1d65c\nAuthor: Vivek Narang <vivek.narang10@gmail.com>\nDate:   2017-06-11T07:53:50Z\n\n    CRITICAL CPU HEAP Grap Overlap bugs fixed batch ...\n\ncommit 0c91cc85c93e4f56a30595283a73db400e46a762\nAuthor: Vivek Narang <vivek.narang10@gmail.com>\nDate:   2017-06-11T10:59:32Z\n\n    Tab text change + First Commit: Qtime Percentile ...\n\ncommit 13c674b9523d508742702046dfbc25798f9c8064\nAuthor: Vivek Narang <vivek.narang10@gmail.com>\nDate:   2017-06-11T19:59:28Z\n\n    Formatting view for environment data for each point on popup ...\n\ncommit e90357be4696d55d3a9060dca924cc62148747d5\nAuthor: Vivek Narang <vivek.narang10@gmail.com>\nDate:   2017-06-11T20:18:26Z\n\n    Changing the legend on the pages ...\n\ncommit 721d913ae33594779a2f0b680d5556f5c81e7895\nAuthor: Vivek Narang <vivek.narang10@gmail.com>\nDate:   2017-06-12T00:28:24Z\n\n    MAJOR: Cloud Indexing various configurations (212, 221, 313, 422) implementation ...\n\ncommit 2d38444e948e8b926edd06ee347719388ddf7e1e\nAuthor: Vivek Narang <vivek.narang10@gmail.com>\nDate:   2017-06-12T00:39:16Z\n\n    Navigation divider update ...\n\ncommit 628780c918fd9e85251d1e4b955e09fb403dd698\nAuthor: Vivek Narang <vivek.narang10@gmail.com>\nDate:   2017-06-12T01:07:29Z\n\n    Webapp file bug fix ...\n\ncommit 406fa718504c26224b383237e3d67c78aabb25ab\nAuthor: Vivek Narang <vivek.narang10@gmail.com>\nDate:   2017-06-12T01:28:25Z\n\n    updating test plans uncommenting tests ...\n\ncommit 796d118ed896b078a7b5bf26602f43e29ab69ac6\nAuthor: Vivek Narang <vivek.narang10@gmail.com>\nDate:   2017-06-12T21:49:02Z\n\n    Folder creation bug fix. Some formatting ...\n\ncommit fc2ff1d329d862fdcbbf3890397db28458646dfa\nAuthor: Vivek Narang <vivek.narang10@gmail.com>\nDate:   2017-06-13T02:45:54Z\n\n    AND OR numeric query bug fix ...\n\ncommit 494555fa69a0592678544a077c77d7c0236a60de\nAuthor: Vivek Narang <vivek.narang@lucidworks.com>\nDate:   2017-06-14T05:02:11Z\n\n    Update README.md\n\ncommit 5c148c8ad8903da0735a9995e3f56dcd84c2179f\nAuthor: Vivek Narang <vivek.narang@lucidworks.com>\nDate:   2017-06-14T05:07:12Z\n\n    Update\n\ncommit 696e74cc4d021a7c870e1c01a751f0b4a8d03e9d\nAuthor: Vivek Narang <vivek.narang@lucidworks.com>\nDate:   2017-06-14T05:08:41Z\n\n    Create README.md\n\ncommit e57ebed359b3b0589539dfa4bfe29ee34d411616\nAuthor: Vivek Narang <vivek.narang@lucidworks.com>\nDate:   2017-06-14T05:11:42Z\n\n    Create README.md\n\ncommit 63eaa45ae9ff575914a0bfc52950c8747c511980\nAuthor: Vivek Narang <vivek.narang@lucidworks.com>\nDate:   2017-06-14T05:12:29Z\n\n    Create README.md\n\ncommit 79aaee63ca5ac5e5fcb5aab7fecd58509e0d587a\nAuthor: Vivek Narang <vivek.narang@lucidworks.com>\nDate:   2017-06-14T05:18:43Z\n\n    Create README.md\n\ncommit 506b6ae4481d5313b78ab47471bb20a50f535e41\nAuthor: Vivek Narang <vivek.narang@lucidworks.com>\nDate:   2017-06-14T05:20:52Z\n\n    Create README.md\n\ncommit 9e98cb2cc936905afefc67553bf1e0c07f4e99da\nAuthor: Vivek Narang <vivek.narang@lucidworks.com>\nDate:   2017-06-14T05:25:10Z\n\n    Create README.md\n\ncommit ce82a96727603b6c40b5f4b57366de25282a34c9\nAuthor: Vivek Narang <vivek.narang@lucidworks.com>\nDate:   2017-06-14T05:25:35Z\n\n    Create README.md\n\ncommit d27d4e5c3767408d7f5389e95ed91065ecdd3903\nAuthor: Vivek Narang <vivek.narang@lucidworks.com>\nDate:   2017-06-14T05:27:00Z\n\n    Create README.md\n\ncommit ebd2642168ffea61e6b1966e5abbe7375a79b302\nAuthor: Vivek Narang <vivek.narang@lucidworks.com>\nDate:   2017-06-14T05:27:18Z\n\n    Create README.md\n\ncommit fcfaeaf2c751f97c45d2af49664478bfb8b0809a\nAuthor: Vivek Narang <vivek.narang@lucidworks.com>\nDate:   2017-06-14T05:35:03Z\n\n    Create README.md\n\ncommit 103cb2ebad1e03037c88a657183ecba2aa359721\nAuthor: Vivek Narang <vivek.narang@lucidworks.com>\nDate:   2017-06-14T05:40:02Z\n\n    Create README.md\n\ncommit 9d5327d8c8dd5568244b2e279fdc59b38445b041\nAuthor: Vivek Narang <vivek.narang@lucidworks.com>\nDate:   2017-06-14T05:50:51Z\n\n    Create README.md\n\n ",
            "author": "ASF GitHub Bot",
            "id": "comment-16145941"
        },
        {
            "date": "2017-08-30T01:29:07+0000",
            "content": "Hello Ishan Chattopadhyaya,\nNow the configurable approach to indexing is working. Querying will also start working in the same way in a couple of hours from now. I would like to give you a quick demo soon. Regards.  ",
            "author": "Vivek Narang",
            "id": "comment-16146458"
        },
        {
            "date": "2017-08-30T04:37:36+0000",
            "content": "Sounds great. \n\nI would like to give you a quick demo soon. Regards. \nCan you write up something and/or record a screencast? ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16146591"
        },
        {
            "date": "2017-08-30T09:14:21+0000",
            "content": "Hello Ishan Chattopadhyaya,\nJSON based configurable querying is working now. Yes, I can provide a written summary of these updates. Regards. ",
            "author": "Vivek Narang",
            "id": "comment-16146931"
        },
        {
            "date": "2017-09-09T15:15:33+0000",
            "content": "An update on the project status here: The GSoC is over, and the project is not complete yet. Since the last month of the three month program was unproductive, the project has officially failed.\n\nHowever, I am working on the missing parts. The major piece is the configurability; every benchmark is currently hardcoded and extending/modifying them is extremely difficult. I shall try to have something for review in a few days. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16159974"
        },
        {
            "date": "2017-09-09T15:17:13+0000",
            "content": "Vivek Narang, thank you for your work on the project. I hope you can continue to contribute to this effort even after GSoC. ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16159975"
        },
        {
            "date": "2017-10-09T10:11:33+0000",
            "content": "I'm back from a vacation and now have some time to work on this. This is still not ready and needs about a week of dev effort (full time). I'm starting to work on this part time and without any help I should be able to do this in two weeks, i.e. by 21-22 October.\n\nBtw, Vivek and I spoke about the benchmarking suite last month:\nhttps://youtu.be/44v2WljG1R0?t=19m59s ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16196783"
        }
    ]
}