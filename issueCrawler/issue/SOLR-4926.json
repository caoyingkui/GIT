{
    "id": "SOLR-4926",
    "title": "I am seeing RecoveryZkTest and ChaosMonkeySafeLeaderTest fail often on trunk.",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.4",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Blocker",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "",
    "attachments": {
        "SOLR-4926.patch": "https://issues.apache.org/jira/secure/attachment/12589767/SOLR-4926.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-13683037",
            "date": "2013-06-14T03:02:44+0000",
            "content": "I've got some theories, but nothing hard. I do think this was caused very recently. I'm going to use fullmetaljenkins tomorrow to track down the issue. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13686775",
            "date": "2013-06-18T14:28:24+0000",
            "content": "It looks like replication thinks it's successful, then buffered replays are done - but only the buffered replays work - the replication is adding no docs. Somehow I think the compound file format stuff affected this, but no clue how yet. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13686814",
            "date": "2013-06-18T15:10:51+0000",
            "content": "Hmm...that may have just been what that one case looked like - looking at another case now that may not match. More digging... "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13686821",
            "date": "2013-06-18T15:23:01+0000",
            "content": "In some of the fails, I'm seeing some errors of this form:\n\n  2> Caused by: org.apache.solr.common.SolrException: Error opening Reader\n  2>    at org.apache.solr.search.SolrIndexSearcher.getReader(SolrIndexSearcher.java:174)\n  2>    at org.apache.solr.search.SolrIndexSearcher.<init>(SolrIndexSearcher.java:185)\n  2>    at org.apache.solr.search.SolrIndexSearcher.<init>(SolrIndexSearcher.java:181)\n  2>    at org.apache.solr.core.SolrCore.openNewSearcher(SolrCore.java:1487)\n  2>    ... 15 more\n  2> Caused by: java.lang.AssertionError: liveDocs.count()=4 info.docCount=6 info.getDelCount()=6\n  2>    at org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat.readLiveDocs(Lucene40LiveDocsFormat.java:92)\n\n\n\nedit: it looks like this type of error is appearing in about 20% of my fails. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13688146",
            "date": "2013-06-19T16:45:14+0000",
            "content": "I hacked the lucene IWC and MergePolicy classes to never use compound format, and then started ChaosMonkeySafeLeaderTest tests in a loop.\n11 passes in a row so far, so it definitely looks like these failures are related to the compound file format. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13688150",
            "date": "2013-06-19T16:52:46+0000",
            "content": "How does this test depend on CFS or not? So it looks like replication does not work correctly with CFS, which is a serious bug! "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13688151",
            "date": "2013-06-19T16:56:35+0000",
            "content": "How does this test depend on CFS or not?\n\nThat's the million dollar question   It does not, explicitly, but it seems like the use of CFS somehow causes replication to fail. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13688339",
            "date": "2013-06-19T19:22:43+0000",
            "content": "the use of CFS somehow causes replication to fail\n\nYeah, this is what I'm seeing - I just caught a really good sample case with decent logging.\n\nThe recovering replica commits on the leader and that leader then has 126 docs to replicate.\n\n16 documents end up on the relica after the replication - 110 short.\n\nBefore the replication, the leader is on gen 3, the replica on gen 1.\n\nPerhaps a red herring, but in the many cases of this I've looked at, oddly, no buffered docs are ever replayed after that - though I have seen buffered docs replayed in those same runs when the replication did not fail. Weird observation.\n\nAnyway, I need to turn on more replication level logging I think. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13688440",
            "date": "2013-06-19T20:53:17+0000",
            "content": "Reviewing some more sample fails of RecoveryZkTest:\n\nIt actually looks like after the replication we end up with one commit point back - eg we are trying to replicate gen 3 and replica moves from gen 1 to gen 2.\n\n\n\tMark\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13688522",
            "date": "2013-06-19T22:03:23+0000",
            "content": "In the case where the slave is on gen 2, it did just download the files for gen 3 - so it seems we are not picking up the latest commit point somehow.. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13688603",
            "date": "2013-06-19T23:09:04+0000",
            "content": "Somehow I think the compound file format stuff affected this, but no clue how yet.\n\nFYI: LUCENE-5038 inadvertantly caused the default compound file behavior in solr to change: it's suppose to be that in Solr compound files are not used unless explicitly configured.  SOLR-4941 will fix this, so once it's committed you will probably start getting \"false success\" from some of these tests again.\n\nstrongly suggest that the cloud & replication tests start randomizing <useCompoundFile> in the solrconfig.xml via a sys property.\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13688807",
            "date": "2013-06-20T02:39:34+0000",
            "content": "I've been focusing on the RecoveryZkTest case.\n\nEvery fail I've looked at has used the RAM dir. Odd because the safe leader test that fails is hard coded to not use ramdir I think. RecoveryZkTest also uses mock dir, but I don't think the safe leader test does because of the hard coding to standard dir.\n\nAnyway, more on what I'm seeing from the RecoveryZkTest fails:\n\nwe replicate gen 3 files, we reopen the writer and then the searcher using that writer - we get an index of gen 2 - the files from the searcher's directory don't contain the newly replicated files, just the gen 2 index files. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13689992",
            "date": "2013-06-21T03:11:48+0000",
            "content": "Still digging into RecoveryZkTest fails with compound file format...\n\nStill only seems to happen with RamDir.\n\nOddly, it seems that the searcher opened after the replication is somehow using a different Directory than the one the replication goes to and the writer is reopened on. Even though they are directories for the same path. I don't know how this can happen yet - and it would seem this may not be the same issue that affected the safe leader test considering that test looks hard coded to standard directory. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13692346",
            "date": "2013-06-24T20:31:03+0000",
            "content": "I started looking for failed assertions, and ran across one not related to index corruption.  This is prob not related to our current issues though.\n\n\n  2> 68103 T325 oasc.SolrException.log ERROR java.lang.AssertionError\n  2>            at org.apache.solr.update.SolrCoreState.decrefSolrCoreState(SolrCoreState.java:62)\n  2>            at org.apache.solr.core.SolrCore.close(SolrCore.java:1059)\n  2>            at org.apache.solr.handler.admin.CoreAdminHandler$2.run(CoreAdminHandler.java:814)\n  2>    \n  2> 68104 T206 C40 P47070 oasc.SolrException.log ERROR java.lang.AssertionError\n  2>            at org.apache.solr.update.SolrCoreState.decrefSolrCoreState(SolrCoreState.java:62)\n  2>            at org.apache.solr.core.SolrCore.close(SolrCore.java:1059)\n  2>            at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:389)\n\n\n\nedit: I first hit this 5 times within a 30 minute period... and then I added logging to try and nail it down and it hasn't happened yet after over an hour of running.  I'm going to give up soon and get back to the main task at hand (chaos compound-related failures).\n\nedit: lack of reproducibility was caused by a cut-n-paste bug - it's now back to being very reproducible.  Tracked it down to SOLR-4960.   "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13693090",
            "date": "2013-06-25T14:53:27+0000",
            "content": "Just to update:\n\nis somehow using a different Directory\n\nI was confused - better logging shows it's the same directory, just with completely different files in it. So I'm still confused.\n\nI see the index getting replicated into the directory and the right files in the directory.\nI see the IndexWriter open on the directory and all the right files in the directory.\nI see the IndexReader reopened with that IndexWriter - and boom, it's got a different set of files. It started with an index of gen1, replicated an index of gen 3, and ends up with a gen 2 index with totally different files. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13693265",
            "date": "2013-06-25T18:46:10+0000",
            "content": "It turns out this was all being caused by:\n\n\n[junit4:junit4]   2> org.apache.lucene.index.CorruptIndexException: invalid deletion count: 38 (resource: ChecksumIndexInput(RAMInputStream(name=segments_3)))\n[junit4:junit4]   2> \tat org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:340)\n[junit4:junit4]   2> \tat org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:374)\n\n\n\nThat causes the opening IW to roll back to gen 2 and remove the gen 3 files. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13693987",
            "date": "2013-06-26T14:17:49+0000",
            "content": "So one thing I see is that when the old IndexWriter is closed, a new segment file is written out.\n\n\n  2> 58339 T257 C69 P58203 oasu.DefaultSolrCoreState.newIndexWriter Closing old IndexWriter... core=collection1 dir=org.apache.lucene.store.MMapDirectory@/opt/code/lusolr_clean5/solr/build/solr-core/test/J0/org.apache.solr.cloud.ChaosMonkeySafeLeaderTest-1372252751336/jetty12/index lockFactory=org.apache.lucene.store.NativeFSLockFactory@42062bad\n  2> 58341 T257 C69 P58203 oasu.DefaultSolrCoreState.newIndexWriter PREFILES=[_0.fdx, segments_1, _0.cfs, _0.si, _0.cfe, _0.fdt, segments.gen, segments_4, _0_1.del, write.lock]\n\n\n  2> 58622 T257 C69 P58203 oasc.SolrDeletionPolicy.onCommit SolrDeletionPolicy.onCommit: commits DEBUG_INFO: num=2\n  2>            commit{dir=/opt/code/lusolr_clean5/solr/build/solr-core/test/J0/org.apache.solr.cloud.ChaosMonkeySafeLeaderTest-1372252751336/jetty12/index,segFN=segments_1,generation=1,filenam\nes=[segments_1]}\n  2>            commit{dir=/opt/code/lusolr_clean5/solr/build/solr-core/test/J0/org.apache.solr.cloud.ChaosMonkeySafeLeaderTest-1372252751336/jetty12/index,segFN=segments_2,generation=2,filenam\nes=[_0.cfs, _0.cfe, _0_1.del, segments_2, _0.si]}\n\n\n  2> 58630 T257 C69 P58203 oasu.DefaultSolrCoreState.newIndexWriter POSTFILES=[segments_2, _0.cfs, _0.si, _0.cfe, segments.gen, segments_4, _0_1.del]\n\n\n\n\nIt looks like this new segments file actually references all of the files we just replicated, which is of course wrong! "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13693993",
            "date": "2013-06-26T14:24:34+0000",
            "content": "Cool - this fits what I am seeing as well. I also checked to see that you could open a writer on the index we replicated and you can no problem. I was struggling to find what could cause the corruption from there - was suspecting the copy to the other dir somehow, but the writer close makes a lot of sense.\n\nAnd explains the mysterious segments_2 file when I only expected segments_1 and segments_3. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13693999",
            "date": "2013-06-26T14:38:06+0000",
            "content": "OK, looks like the close of the old IW is actually changing some of the segment files (the number in parens is the file size).\nNow the question is why...\n\n\n  2> 57648 T256 C30 P58775 oasu.DefaultSolrCoreState.showFiles PREFILES _0.cfe(266)_0.cfs(4375)_0.fdt(0)_0.fdx(0)_0.si(239)_0_1.del(32)segments.gen(20)segments_1(45)segments_4(99)write.lock(0)\n\n  2> 57887 T256 C30 P58775 oasu.DefaultSolrCoreState.showFiles POSTFILES _0.cfe(266)_0.cfs(2993)_0.si(239)_0_1.del(31)segments.gen(20)segments_2(70)segments_4(99)\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13694042",
            "date": "2013-06-26T15:57:53+0000",
            "content": "Running with a local modification that does a commit at the start of snappuller (as well as getting the latest commit point from the deletion policy).  Crossing my fingers... 12 successes in a row so far. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13694047",
            "date": "2013-06-26T16:09:33+0000",
            "content": "I have this competing approach attached - all tests pass and my zkrecoverytest loop is holding up pretty well (22 passes so far) "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13694055",
            "date": "2013-06-26T16:26:02+0000",
            "content": "I have this competing approach attached \n\nYeah, I like the rollback right before... safer in the event that somehow the IW acquires more changes.  I was initially worried about code that uses the IW to reopen the reader, but that code (openSearcher) doesn't force open a writer and has a fallback to reopen on the reader.\n\nI'll start looping the chaos test with your patch and see how it goes.\nedit: 20 passes so far... commit it! "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13694201",
            "date": "2013-06-26T19:55:42+0000",
            "content": "Thanks Yonik! "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13694211",
            "date": "2013-06-26T20:07:03+0000",
            "content": "The commit bot got knocked out because JIRA started responding that it was not authorized.\n\ncommitted:\n5x 1497020\n4x 1497022 "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13694213",
            "date": "2013-06-26T20:14:18+0000",
            "content": "Yonik follow up with\n\n4x 1497055 get latest commit from deletion policy\n5x 1497054 get latest commit from deletion policy "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13696930",
            "date": "2013-07-01T16:33:56+0000",
            "content": "Commit 1498554 from Yonik Seeley\n[ https://svn.apache.org/r1498554 ]\n\nSOLR-4926: SolrDeletionPolicy.getLatestCommit can be null before IW is used "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13696939",
            "date": "2013-07-01T16:44:15+0000",
            "content": "Commit 1498559 from Yonik Seeley\n[ https://svn.apache.org/r1498559 ]\n\nSOLR-4926: SolrDeletionPolicy.getLatestCommit can be null before IW is used "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13716866",
            "date": "2013-07-23T18:38:48+0000",
            "content": "Bulk close resolved 4.4 issues "
        }
    ]
}