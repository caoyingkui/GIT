{
    "id": "LUCENE-4072",
    "title": "CharFilter that Unicode-normalizes input",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/analysis"
        ],
        "type": "New Feature",
        "fix_versions": [
            "4.8",
            "6.0"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "I'd like to contribute a CharFilter that Unicode-normalizes input with ICU4J.\n\nThe benefit of having this process as CharFilter is that tokenizer can work on normalised text while offset-correction ensuring fast vector highlighter and other offset-dependent features do not break.\n\nThe implementation is available at following repository:\nhttps://github.com/ippeiukai/ICUNormalizer2CharFilter\n\nUnfortunately this is my unpaid side-project and cannot spend much time to merge my work to Lucene to make appropriate patch. I'd appreciate it if anyone could give it a go. I'm happy to relicense it to whatever that meets your needs.",
    "attachments": {
        "4072.patch": "https://issues.apache.org/jira/secure/attachment/12625389/4072.patch",
        "LUCENE-4072.patch": "https://issues.apache.org/jira/secure/attachment/12529724/LUCENE-4072.patch",
        "ippeiukai-ICUNormalizer2CharFilter-4752cad.zip": "https://issues.apache.org/jira/secure/attachment/12528690/ippeiukai-ICUNormalizer2CharFilter-4752cad.zip",
        "DebugCode.txt": "https://issues.apache.org/jira/secure/attachment/12529862/DebugCode.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-05-22T02:40:25+0000",
            "content": "This looks cool: it looked tricky to me to implement that iterative normalization api, I'm glad you tackled it \n\nIts a good idea, for some tokenizers like Ngrams etc you really need normalization as a charfilter.\n\nI'll see if i can help with the integration. ",
            "author": "Robert Muir",
            "id": "comment-13280687"
        },
        {
            "date": "2012-05-22T12:16:24+0000",
            "content": "Hello, if you are willing to contribute this under the Apache license (http://www.apache.org/licenses/LICENSE-2.0.html), would\nyou mind uploading a .zip file of your codebase (git has a button to create it), and check the box \"Grant license to ASF for inclusion in ASF works\"\nwhen doing the upload? \n\nThen I can help turn it into a patch. Thanks again! ",
            "author": "Robert Muir",
            "id": "comment-13280905"
        },
        {
            "date": "2012-05-23T02:50:35+0000",
            "content": "Robert,\nThe zipped copy of the repository is attached and licensed to ASF for inclusion.\nThanks! ",
            "author": "Ippei UKAI",
            "id": "comment-13281376"
        },
        {
            "date": "2012-05-25T14:45:40+0000",
            "content": "attached is the filter, turned into a patch.\n\nhowever, I added an additional random test and it currently fails... will look into this more. ",
            "author": "Robert Muir",
            "id": "comment-13283495"
        },
        {
            "date": "2012-05-26T09:29:11+0000",
            "content": "I've backported your additional test case of random string and reproduced the problem.\nHowever, I haven't managed to pin point the problem so far. Having no clue coming out of AssertionError, attacking LUCENE-3900 first might be more productive.\n\nThis final offset thing looks like a common occurrence among CharFilters (re LUCENE-3820 LUCENE-3971) but I'm yet to find a common theme in them. ",
            "author": "Ippei UKAI",
            "id": "comment-13283943"
        },
        {
            "date": "2012-05-26T13:17:34+0000",
            "content": "I've found a problem:\n\nIn ICUNormalizer2CharFilter#recordOffsetDiff(int,int), the positive diff case should look like following. \n<pre>\n      addOffCorrectMap(charCount + Math.min(1, outputLength), cumuDiff + diff);\n</pre>\n\nHopefully, there won't be more.\n\nI do grant license to ASF to include the above line in ASF works (as per the Apache License \u00a75) ",
            "author": "Ippei UKAI",
            "id": "comment-13283977"
        },
        {
            "date": "2012-05-26T13:22:21+0000",
            "content": "How I debugged for a reference. ",
            "author": "Ippei UKAI",
            "id": "comment-13283978"
        },
        {
            "date": "2013-10-18T18:59:06+0000",
            "content": "I'm available to help make this work. I updated Ippei UKAI's code to use 4.0 API (CharStream, CharReader, ReusableAnalyzerBase affected). I updated Robert Muir's random input test and it's not failing. I'm not sure if Ippei's last fix worked and this ought to have been closed then. I don't see this class in the Lucene library.\n\nLet me know if this helps. ",
            "author": "David Goldfarb",
            "id": "comment-13799412"
        },
        {
            "date": "2013-10-18T19:04:01+0000",
            "content": "Thanks David! I will look into this, I was unaware the previous problem had been resolved! ",
            "author": "Robert Muir",
            "id": "comment-13799418"
        },
        {
            "date": "2013-10-27T17:40:07+0000",
            "content": "I looked over the patch, and added license headers and so on.\n\nI also added some new tests, which currently fail. I think the problem is that the current logic iterates characters (e.g. passing charAt to hasBoundaryBefore and so on), when it should be passing codepoints to these methods. ",
            "author": "Robert Muir",
            "id": "comment-13806396"
        },
        {
            "date": "2013-10-30T20:25:32+0000",
            "content": "Indeed, changing the code to iterate over codepoints fixed a majority of the test failures.\n\nThe random tests still fail sometimes \u2013 I believe there's a bug in Normalizer2. I submitted a bug report here. ",
            "author": "David Goldfarb",
            "id": "comment-13809569"
        },
        {
            "date": "2013-11-25T15:31:29+0000",
            "content": "Is this considered blocked on ICU? Let me know if there's anything else I can contribute to this. ",
            "author": "David Goldfarb",
            "id": "comment-13831549"
        },
        {
            "date": "2013-11-26T15:32:41+0000",
            "content": "Hi David: I havent taken a look at the impact of the ICU bug (i'm not really that familiar with the incremental normalization API), but it seems rather serious.\n\nIs it possible to avoid use of hasBoundaryAfter? In addition to the bug you found, it has the warning that it may be significantly slower than hasBoundaryBefore: I'm wondering if we can dodge it. ",
            "author": "Robert Muir",
            "id": "comment-13832668"
        },
        {
            "date": "2013-12-23T21:43:30+0000",
            "content": "This patch dodges the use of hasBoundaryAfter, and the tests pass.\n\nNote in doTestMode there's a clause that checks if the normalized string has length zero. It seems the nfkc_cf-normalized output of some strings is empty. Examples I found:\n'\\uDB40\\uDCD9'\n'\\uDB43\\uDF86'\n'\\uFE04' ",
            "author": "David Goldfarb",
            "id": "comment-13855953"
        },
        {
            "date": "2013-12-24T04:46:59+0000",
            "content": "Thanks so much for attacking this David: I think that 0-length \"all default ignorables\" case makes sense (where it creates an empty string), because in that case there won't be a single token at all (MockTokenizer is not a perfect emulator of KeywordTokenizer here).\n\nI think this patch is close, but when running the test a few hundred times I hit a failure (see my added testCuriousString, which fails). I think this one is a bug in the logic.\n\nMotivated by this fail, I tried to beef up tests in general:\n\n\tfixed my typo where testNFD wasnt actually testing NFD\n\ttest strings > 20 characters, since this filter has an internal 128-char buffer.\n\n\n\nThe latter seems to expose a lot of bugs, I assume due to the internal buffering. I haven't yet looked into this. But it seems there are correctness issues for documents > 128 chars (as well as what I believe is a separate bug seen by testCuriousString, which I think is some bug in the logic related to ignorables). ",
            "author": "Robert Muir",
            "id": "comment-13856134"
        },
        {
            "date": "2013-12-24T04:54:14+0000",
            "content": "ok as for the testCuriousString bug, I enabled verbose (ant test -Dtestcase=TestICUNormalizer2CharFilter -Dtestmethod=testCuriousString -Dtests.verbose=true) and it seems to always fail when given a \"spoon-fed\" Reader. So Ill dig into this one, I think it involves how this charfilter consumes the reader api. ",
            "author": "Robert Muir",
            "id": "comment-13856140"
        },
        {
            "date": "2013-12-24T05:14:00+0000",
            "content": "One thing that certainly looks like a bug is this:\n\nThe input-processing side looks like this in pseudocode:\n\nwhile (read() some char[]s) {\n   normalize(char[]s) // (quick check/hasBoundary/etc)\n}\n\n\n\nBut read() works at char level, and these normalization apis want ints. \nSo I think readInputToBuffer() needs to keep reading, if possible, to ensure it fully consumes whole codepoints before returning. I added a little hack locally, but it didnt seem to clean up the test fails, so I think there are other bugs too, or I'm missing something?\n\n\n  private int readInputToBuffer() throws IOException {\n    final int len = input.read(tmpBuffer);\n    if (len == -1) {\n      inputFinished = true;\n      return 0;\n    }\n    inputBuffer.append(tmpBuffer, 0, len);\n    // nocommit: just a hack\n    // if buffer ends on high surrogate, keep reading before processing\n    if (len > 0 && Character.isHighSurrogate(tmpBuffer[len-1])) {\n      return len + readInputToBuffer();\n    }\n    // end hack\n    return len;\n  }\n\n ",
            "author": "Robert Muir",
            "id": "comment-13856149"
        },
        {
            "date": "2014-01-27T18:07:43+0000",
            "content": "Attaching a new patch - testCuriousString still fails. \n\nYou're right about readInputToBuffer. I think we also have to stop only on normalization boundaries. I see two options:\nuse normalizer.hasBoundaryAfter(tmpBuffer[len-1]) (straightforward)\nor\nuse normalizer.hasBoundaryBefore(tmpBuffer[len-1]) and use mark() and reset().\n\n\n  private int readInputToBuffer() throws IOException {\n    final int len = input.read(tmpBuffer);\n    if (len == -1) {\n      inputFinished = true;\n      return 0;\n    }\n    inputBuffer.append(tmpBuffer, 0, len);\n    if (len >= 2 && normalizer.hasBoundaryAfter(tmpBuffer[len-1]) && !Character.isHighSurrogate(tmpBuffer[len-1])) {\n        return len;\n    } else return len + readInputToBuffer();\n  }\n\n\n\n[edit]\nAnd the len >= 2 clause wasn't meant to be part of the patch, ignore that.\n\nif (normalizer.hasBoundaryAfter(tmpBuffer[len-1]) && !Character.isHighSurrogate(tmpBuffer[len-1])) {\n        return len;\n} else return len + readInputToBuffer();\n\n ",
            "author": "David Goldfarb",
            "id": "comment-13883032"
        },
        {
            "date": "2014-03-12T14:02:36+0000",
            "content": "Attaching a new patch. All tests pass. \n\nI'm using Normalizer2.isInert to check if we need to keep reading to the input buffer since it doesn't return false positives, even though it's not as fast as .hasBoundaryBefore(). ",
            "author": "David Goldfarb",
            "id": "comment-13931790"
        },
        {
            "date": "2014-03-20T00:10:31+0000",
            "content": "Whew, thank you!\n\nI did some minor cleanup: I toned down the tests i had added that were very slow (added multiplier, so they will do more work in jenkins), added testMassiveLigature (just to test the case where normalization increases the length), and removed the stuff around reset()... since mark isnt supported the default UOE is the right thing.\n\nI'll commit shortly ",
            "author": "Robert Muir",
            "id": "comment-13941218"
        },
        {
            "date": "2014-03-20T00:19:02+0000",
            "content": "Commit 1579488 from Robert Muir in branch 'dev/trunk'\n[ https://svn.apache.org/r1579488 ]\n\nLUCENE-4072: add ICUNormalizer2CharFilter ",
            "author": "ASF subversion and git services",
            "id": "comment-13941233"
        },
        {
            "date": "2014-03-20T00:43:50+0000",
            "content": "Commit 1579491 from Robert Muir in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1579491 ]\n\nLUCENE-4072: add ICUNormalizer2CharFilter ",
            "author": "ASF subversion and git services",
            "id": "comment-13941245"
        },
        {
            "date": "2014-03-20T00:44:54+0000",
            "content": "Thank you Ippei UKAI and David Goldfarb ! ",
            "author": "Robert Muir",
            "id": "comment-13941246"
        },
        {
            "date": "2014-03-20T01:30:22+0000",
            "content": "Commit 1579498 from Robert Muir in branch 'dev/trunk'\n[ https://svn.apache.org/r1579498 ]\n\nLUCENE-4072: add factory ",
            "author": "ASF subversion and git services",
            "id": "comment-13941285"
        },
        {
            "date": "2014-03-20T01:33:48+0000",
            "content": "Commit 1579499 from Robert Muir in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1579499 ]\n\nLUCENE-4072: add factory ",
            "author": "ASF subversion and git services",
            "id": "comment-13941290"
        },
        {
            "date": "2014-04-27T23:25:56+0000",
            "content": "Close issue after release of 4.8.0 ",
            "author": "Uwe Schindler",
            "id": "comment-13982607"
        }
    ]
}