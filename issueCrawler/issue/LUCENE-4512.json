{
    "id": "LUCENE-4512",
    "title": "Additional memory savings in CompressingStoredFieldsIndex.MEMORY_CHUNK",
    "details": {
        "components": [],
        "fix_versions": [
            "4.1",
            "6.0"
        ],
        "affect_versions": "None",
        "priority": "Minor",
        "labels": "",
        "type": "Improvement",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Robert had a great idea to save memory with CompressingStoredFieldsIndex.MEMORY_CHUNK: instead of storing the absolute start pointers we could compute the mean number of bytes per chunk of documents and only store the delta between the actual value and the expected value (avgChunkBytes * chunkNumber).\n\nBy applying this idea to every n(=1024?) chunks, we would even:\n\n\tmake sure to never hit the worst case (delta ~= maxStartPointer)\n\treduce memory usage at indexing time.",
    "attachments": {
        "LUCENE-4512.patch": "https://issues.apache.org/jira/secure/attachment/12551388/LUCENE-4512.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-10-29T20:24:33+0000",
            "content": "I do think we should use n=(some power of 2 or whatever) chunks, because e.g. just testing with that geonames dataset i saw the\ndeltas grow quite large at points... this caused it to use 24 bits per value (still better than 29), but with a tiny bit of \neffort I think it could be significantly less. ",
            "author": "Robert Muir",
            "id": "comment-13486311"
        },
        {
            "date": "2012-10-30T17:51:17+0000",
            "content": "Patch. I removed MEMORY_DOC and modified MEMORY_CHUNK to apply the proposed changes:\n\n\tit works with blocks of n=1024 (hard-wired) chunks,\n\tfor every block, doc bases and start pointers are compressed by only storing the delta from the average.\n\n ",
            "author": "Adrien Grand",
            "id": "comment-13487062"
        },
        {
            "date": "2012-10-30T18:15:54+0000",
            "content": "I tested this really fast on that geonames data again: 72 chunks with bpvs of 16-20 (avg 18 i think).\nSo this is quite a bit more savings than 29bpv with the trunk code.\n\nI didnt look at the code too much, but since we are computing the average at index-time (i think?),\ndo you think it still makes sense to encode the deltas from the previous value, or should we just\nup-front encode them at index-time as deltas from the average (if it makes things simpler?) ",
            "author": "Robert Muir",
            "id": "comment-13487084"
        },
        {
            "date": "2012-10-30T19:33:49+0000",
            "content": "72 chunks with bpvs of 16-20 (avg 18 i think).\n\nThat is good but I was expecting the distance from average (128kb here) to be less than the chunk size (16kb), which is clearly not the case. Is there anything in the dataset that could explain why chunk sizes vary so much? Or maybe we should just decrease the block size or the average is wrongly computed...\n\ndo you think it still makes sense to encode the deltas from the previous value\n\nGood question. Encoding deltas currently requires 14 or 15 bits per values (because it can grow a little larger than the chunk size which is 2^14) so it is still a little more compact, and it is less prone to worst cases I think? There is some overhead at read time to build the packed ints array instead of just deserializing it but I think this is negligible. If we manage to make bpvs smaller than 14 on \"standard\" datasets then I think it makes sense. ",
            "author": "Adrien Grand",
            "id": "comment-13487152"
        },
        {
            "date": "2012-10-30T20:16:46+0000",
            "content": "\nThat is good but I was expecting the distance from average (128kb here) to be less than the chunk size (16kb), which is clearly not the case. Is there anything in the dataset that could explain why chunk sizes vary so much? Or maybe we should just decrease the block size or the average is wrongly computed...\n\nProbably, i bet rows from the same country and even provinces within a country are typically grouped together?\nThough before this jira issue, i did experiments randomizing the dataset with sort -r and it didnt make much difference...\n\nIn all cases you can get it from http://download.geonames.org/export/dump/allCountries.zip\nIts UTF-8 and you can parse with split(\"\\t\")\n\n\nGood question. Encoding deltas currently requires 14 or 15 bits per values (because it can grow a little larger than the chunk size which is 2^14) so it is still a little more compact, and it is less prone to worst cases I think? There is some overhead at read time to build the packed ints array instead of just deserializing it but I think this is negligible. If we manage to make bpvs smaller than 14 on \"standard\" datasets then I think it makes sense.\n\nWell i wasnt really thinking about a few smaller bits on disk... if we want that, LZ4 this \"metadata stuff\" too (just kidding!).\n\nI was just thinking simpler code in the reader. ",
            "author": "Robert Muir",
            "id": "comment-13487181"
        },
        {
            "date": "2012-10-30T20:36:28+0000",
            "content": "And once you get all this baked in aren't you itching to do the vectors files too?  ",
            "author": "Robert Muir",
            "id": "comment-13487200"
        },
        {
            "date": "2012-10-31T00:37:24+0000",
            "content": "I did some tests with the 1K docs from the wikipedia dump:\n\n\talways 16 or 17 bpvs for start pointers, (my intuition was wrong! )\n\tthe CompressingStoredFieldsIndex instance is 185.3KB (measured with RamusageEstimator) for 1M docs (0.19 bytes per doc, 3.24 bytes per chunk).\n\n\n\nI tried some other block sizes:\n\n\t256\u00a0: 189.2KB\n\t4096\u00a0: 204.8 KB\n\n\n\n1024 looks like a good setting.\n\nI was just thinking simpler code in the reader.\n\nHmm good point. It is true that it is already complex enough... Here is a new patch.\n\nAnd once you get all this baked in aren't you itching to do the vectors files too?\n\nI started thinking to it but I'm not very familiar with the terms vectors file formats yet. There are probably other places that might benefit from compression (terms dictionary?). ",
            "author": "Adrien Grand",
            "id": "comment-13487411"
        },
        {
            "date": "2012-10-31T19:02:49+0000",
            "content": "This is the one you committed! Marking resolved. ",
            "author": "Robert Muir",
            "id": "comment-13488101"
        },
        {
            "date": "2013-03-22T16:21:35+0000",
            "content": "[branch_4x commit] Adrien Grand\nhttp://svn.apache.org/viewvc?view=revision&revision=1404216\n\nLUCENE-4512: Additional memory savings for CompressingStoredFieldsIndex.MEMORY_CHUNK (merged from r1404215).\n ",
            "author": "Commit Tag Bot",
            "id": "comment-13610616"
        }
    ]
}