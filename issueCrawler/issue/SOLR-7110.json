{
    "id": "SOLR-7110",
    "title": "Optimize JavaBinCodec to minimize string Object creation",
    "details": {
        "components": [],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [
            "5.2",
            "6.0"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Minor"
    },
    "description": "In JavabinCodec we already optimize on strings creation , if they are repeated in the same payload. if we use a cache it is possible to avoid string creation across objects as well.",
    "attachments": {
        "JavabinPerf.patch": "https://issues.apache.org/jira/secure/attachment/12725841/JavabinPerf.patch",
        "SOLR-7110.patch": "https://issues.apache.org/jira/secure/attachment/12698929/SOLR-7110.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-02-15T03:44:38+0000",
            "author": "Noble Paul",
            "content": "Yonik Seeley would be glad to know your comments on this ",
            "id": "comment-14321797"
        },
        {
            "date": "2015-02-15T12:44:24+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Is this change back-compatible? ",
            "id": "comment-14321967"
        },
        {
            "date": "2015-02-15T14:03:11+0000",
            "author": "Yonik Seeley",
            "content": "Background for others who don't know how this works, Solr (javabin format) internally avoids repeating String keys by allowing strings to be specified by number if it's already been seen in the current message.\n\nBut looking at the patch quickly, this isn't about reusing the \"external string\" across different messages.  This is simply about avoiding String creation.  Basically, one reads a sequence of UTF8 bytes off the stream and instead of creating a new String object, we check a cache may already have a String for those bytes.  This isn't unique to JavaBin either... one could use the same technique in any of our transports (including HTTP params).\n\nGut feel is that as written, this will be slower.  The extra work + overhead of our concurrent LRU cache should swamp any savings.  Has this been benchmarked? ",
            "id": "comment-14321994"
        },
        {
            "date": "2015-02-15T15:05:16+0000",
            "author": "Noble Paul",
            "content": "Is this change back-compatible?\n\nYes, There is no change to the serialization format. Only the deserialization logic is changed\n\nThis isn't unique to JavaBin either... one could use the same technique in any of our transports (including HTTP params).\n\nYes. The good part about javabin is possibly repeated strings are written as a different type in javabin . In other payloads we need to identify what is the 'cacheable' string\n\nThe extra work + overhead of our concurrent LRU cache should swamp any savings. Has this been benchmarked?\n\nI plan to do the benchmark. My hunch is that performance will be similar , a map.get() cannot be far more expensive than an Object creation and subsequent GC.\nEven if the performance is same this helps a lot on cutting down String objects and hence GC ",
            "id": "comment-14322012"
        },
        {
            "date": "2015-02-16T18:43:53+0000",
            "author": "Noble Paul",
            "content": "Here are the benchmark results\n\nNo: of objects in cache 10K, no:of strings created: 10million: no:of threads:10\n\n\n==============LRU 10 THREADS===============\n*************before test start***********\nUsed Memory:10\n*************after LRU cache init***********\nUsed Memory:14\n*************after cache test***********\nUsed Memory:16\ntime taken by LRUCACHE 709ms\n*************after new string test***********\nUsed Memory:70MB\ntime taken by string creation 668ms\n\n\n\nThe takeaways are . As expected the time taken by both is negligible. It does not really matter. Memory usage is dramatically higher for string creation. \n54MB vs 6MB = ~700% more memory used. This probably has a bigger impact on our GC pauses ",
            "id": "comment-14323125"
        },
        {
            "date": "2015-02-16T18:58:43+0000",
            "author": "Yonik Seeley",
            "content": "\nIt's not clear what's what here... did the test take 668ms without the patch and 709ms with the patch?\n\nMeasuring memory isn't enough here... GC is very efficient at cleaning up short lived objects, and the only downside to letting GC do it is some CPU time, which should show up in a throughput benchmark if this is a win. ",
            "id": "comment-14323145"
        },
        {
            "date": "2015-02-16T20:10:42+0000",
            "author": "Noble Paul",
            "content": "This is a micro benchmark. I'm attaching the perf test along with the patch. TestJavabinCodec.testPerf()\n\nMeasuring memory isn't enough here... GC is very efficient at cleaning up short lived objects, and the only downside to letting GC do it is some CPU time, which should show up in a throughput benchmark if this is a win.\n\nThe GC does not happen immediately at all . It may happen when it happens . But the real challenge is, Solr has an increasingly large memory footprint and as the heap grows , the probability of us pushing the JVM to do  a full GC keeps going up . The overall CPU cost of this operation is still negligible , even when GC happens. So, this is not really an optimization on CPU, it is a memory optimization. ",
            "id": "comment-14323227"
        },
        {
            "date": "2015-02-16T21:10:40+0000",
            "author": "Yonik Seeley",
            "content": "This should really only affect how fast the young gen fills up, which should not have an effect on full GCs.\n\nSo, this is not really an optimization on CPU, it is a memory optimization.\n\nThe question at hand is if the memory optimization is worth the CPU overhead here.  What's the long term decoding throughput before and after the patch? ",
            "id": "comment-14323296"
        },
        {
            "date": "2015-02-17T05:24:57+0000",
            "author": "Noble Paul",
            "content": "We are talking about ~100ms diff for a 10 million string objects . Assuming, there are a few dozen cached strings in every request , the CPU costs are not even quantifiable.  ",
            "id": "comment-14323663"
        },
        {
            "date": "2015-02-26T03:21:06+0000",
            "author": "Otis Gospodnetic",
            "content": "Possibly better ways to test:\n\n\tuse something like SPM or VisualVM or anything that gives you visualization of:\n\t\n\t\tvarious memory pools (size + utilization) in the heap\n\t\tGC activity (frequency, avg time, max time, size, etc.)\n\t\tCPU usage\n\t\n\t\n\tenable GC logging, grep for FullGC, or run jstat\n\n\n\n.... all of this over time - not just a few minutes, but longer runs before patch vs. after patch.  Then you can really see what difference this makes. ",
            "id": "comment-14337788"
        },
        {
            "date": "2015-02-26T07:03:37+0000",
            "author": "Noble Paul",
            "content": "My point is, the CPU costs of map lookups are zero (or near zero. may be a nanosecond or less in every request)  .Memory costs of are obvious from these simple tests.\n\n\nIMHO , Solr is a memory hog. We should proactively  try to bring it down before it becomes unmanageable ",
            "id": "comment-14338028"
        },
        {
            "date": "2015-03-23T18:20:29+0000",
            "author": "Noble Paul",
            "content": "Changed the scope to just add the functionality to JavabinCodec but not really use it anywhere. I guess this can go in and we can decide whether to use it later. So this currently will have no impact on performance till we use it directly in some component ",
            "id": "comment-14376327"
        },
        {
            "date": "2015-04-13T10:32:06+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1673149 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1673149 ]\n\nSOLR-7110: Optimize JavaBinCodec to minimize string Object creation ",
            "id": "comment-14492248"
        },
        {
            "date": "2015-04-13T10:33:24+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1673150 from Noble Paul in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1673150 ]\n\nSOLR-7110: Optimize JavaBinCodec to minimize string Object creation ",
            "id": "comment-14492249"
        },
        {
            "date": "2015-04-13T10:45:04+0000",
            "author": "Noble Paul",
            "content": "As of now , it is not used anywhere but the feature is in ",
            "id": "comment-14492263"
        },
        {
            "date": "2015-04-13T11:39:57+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1673161 from Yonik Seeley in branch 'dev/trunk'\n[ https://svn.apache.org/r1673161 ]\n\nSOLR-7110: reformat new code ",
            "id": "comment-14492298"
        },
        {
            "date": "2015-04-13T11:40:31+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1673162 from Yonik Seeley in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1673162 ]\n\nSOLR-7110: reformat new code ",
            "id": "comment-14492299"
        },
        {
            "date": "2015-04-13T13:49:07+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1673186 from Yonik Seeley in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1673186 ]\n\nSOLR-7110: fix break to 5x build ",
            "id": "comment-14492408"
        },
        {
            "date": "2015-04-13T15:24:07+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "I see that in JavaBinCodec.readExternString you have changed:\n\n-      String s = (String) readVal(fis);\n+      tagByte = fis.readByte();\n+      String s = readStr(fis, stringCache);\n\n\n\nBut I cannot find the corresponding change to writeExternString. How does this work? ",
            "id": "comment-14492507"
        },
        {
            "date": "2015-04-13T15:50:26+0000",
            "author": "Yonik Seeley",
            "content": "OK I did some performance testing (since I don't see the reason to commit something that won't be used).  I decoded random documents with the same set of fields (meaning there will be 100% hit rate on the string cache - a best case scenario for it).\n\nSingle threaded: using a string cache impacted decoding performance anywhere from 2.5-7 percent... median looked to be around 3.5% lower performance.\n2 core processor with 4 decoding threads: I saw decreases in performance ranging from 18% to 30%\n4 core processor with 4 decoding threads: I saw decreases in performance averaging about 23%\n\nSo in general, it seems like trying to cache relatively small objects with a relatively expensive cache is a lose. ",
            "id": "comment-14492535"
        },
        {
            "date": "2015-04-13T15:56:47+0000",
            "author": "Yonik Seeley",
            "content": "But I cannot find the corresponding change to writeExternString. How does this work?\n\nreadVal would normally read the tag byte then call readStr if it was a string, so hopefully it's equivalent. ",
            "id": "comment-14492542"
        },
        {
            "date": "2015-04-13T16:10:21+0000",
            "author": "Noble Paul",
            "content": "Which test did you use? Did u use the checked in tests or created new ones? \n\nWhat is important is probably the absolute numbers. if it is slower by a few of nanoseconds and you get better memory efficiency it should be worth it.\u00a0In %age of time taken in deserialization is already insignificant \n\na 100% or near 100% cache hit will not be unusual , if you have cache a few 1000 strings because our keys are mostly repeated ",
            "id": "comment-14492559"
        },
        {
            "date": "2015-04-13T16:13:44+0000",
            "author": "Noble Paul",
            "content": "yeah, both are equivalent. \n\nactually , even the old code using readVal() was redundant ",
            "id": "comment-14492565"
        },
        {
            "date": "2015-04-13T16:24:19+0000",
            "author": "Yonik Seeley",
            "content": "if it is slower by a few of nanoseconds\n\nI don't run things for a new nanoseconds in benchmarks.  In this case, I ran enough decode iterations to run for at least 30 seconds per run, more than enough time for the cache to have a positive effect on garbage collection times as well. \n\nI am -1 on enabling/using this cache. ",
            "id": "comment-14492578"
        },
        {
            "date": "2015-04-13T20:15:54+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1673270 from Yonik Seeley in branch 'dev/trunk'\n[ https://svn.apache.org/r1673270 ]\n\nSOLR-7110: tests - java7 compilable ",
            "id": "comment-14492995"
        },
        {
            "date": "2015-04-13T20:16:10+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1673271 from Yonik Seeley in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1673271 ]\n\nSOLR-7110: tests - java7 compilable ",
            "id": "comment-14492996"
        },
        {
            "date": "2015-04-16T11:05:28+0000",
            "author": "Noble Paul",
            "content": "I ran the tests and I consistently got better perf with cache. Attached is the patch to modify the tests to get this output\n\n\nTHREADS=1\n####### test started w/o cache\nreturn=0 THROUGHPUT=222965\n####### test started with cache\nreturn=0 THROUGHPUT=269978\ncache: hits=18999981 lookups=19000000 size=19\n%age improvement with cache :21\n\nTHREADS=2\n####### test started w/o cache\nreturn=0 THROUGHPUT=269396\n####### test started with cache\nreturn=0 THROUGHPUT=278086\ncache: hits=18999981 lookups=19000000 size=19\n%age improvement with cache :3\n\nTHREADS=3\n####### test started w/o cache\nreturn=0 THROUGHPUT=276090\n####### test started with cache\nreturn=0 THROUGHPUT=285062\ncache: hits=18999981 lookups=19000000 size=19\n%age improvement with cache :3\n\nTHREADS=4\n####### test started w/o cache\nreturn=0 THROUGHPUT=275633\n####### test started with cache\nreturn=0 THROUGHPUT=282246\ncache: hits=18999981 lookups=19000000 size=19\n%age improvement with cache :2\n\n\n ",
            "id": "comment-14497904"
        },
        {
            "date": "2015-04-16T11:53:41+0000",
            "author": "Yonik Seeley",
            "content": "Please don't commit this patch to the perf benchmark - you shouldn't be running different alternatives in the same JVM run. ",
            "id": "comment-14497970"
        },
        {
            "date": "2015-04-16T12:31:09+0000",
            "author": "Noble Paul",
            "content": "I'm not committing this. This is just to let you know that how I got these numbers.\n\nDo you mean to say the numbers are vastly different if you run in different JVMs ? ",
            "id": "comment-14497993"
        },
        {
            "date": "2015-04-16T12:47:35+0000",
            "author": "Yonik Seeley",
            "content": "Do you mean to say the numbers are vastly different if you run in different JVMs ?\n\nNope.  I'm just saying that running different variants of something in the same JVM throws mud in the water, and the results could be different.\n\n\tfirst variant run has to pay hotspot cost to optimize, things tend to get faster the longer they run (up to a limit of course)\n\thotspot will often specialize for the first variant run, and that can penalize later variants\n\tGC costs will leak from one variant to another variants\n\n ",
            "id": "comment-14498004"
        },
        {
            "date": "2015-04-16T12:58:45+0000",
            "author": "Noble Paul",
            "content": "first variant run has to pay hotspot cost to optimize, things tend to get faster the longer they run (up to a limit of course)\n\nSo, we should do some simple warm up before running all tests\n\nAnyway, Wha twere the numbers you got? ",
            "id": "comment-14498017"
        },
        {
            "date": "2015-04-16T13:05:29+0000",
            "author": "Noble Paul",
            "content": "Here is the result with adding a dry run in the beginning  and gc() between runs\n\nTHREADS=1\n####### test started w/o cache\nreturn=0 THROUGHPUT=225835\n####### test started with cache\nreturn=0 THROUGHPUT=273074\ncache: hits=18999981 lookups=19000000 size=19\n%age improvement with cache :20\n====DRY RUN ignore the last====\n\n\nTHREADS=1\n####### test started w/o cache\nreturn=0 THROUGHPUT=257665\n####### test started with cache\nreturn=0 THROUGHPUT=271296\ncache: hits=18999981 lookups=19000000 size=19\n%age improvement with cache :5\n\nTHREADS=2\n####### test started w/o cache\nreturn=0 THROUGHPUT=267881\n####### test started with cache\nreturn=0 THROUGHPUT=276090\ncache: hits=18999981 lookups=19000000 size=19\n%age improvement with cache :3\n\nTHREADS=3\n####### test started w/o cache\nreturn=0 THROUGHPUT=266737\n####### test started with cache\nreturn=0 THROUGHPUT=268744\ncache: hits=18999981 lookups=19000000 size=19\n%age improvement with cache :0\n\nTHREADS=4\n####### test started w/o cache\nreturn=0 THROUGHPUT=263782\n####### test started with cache\nreturn=0 THROUGHPUT=278862\ncache: hits=18999981 lookups=19000000 size=19\n%age improvement with cache :5\n\n ",
            "id": "comment-14498026"
        },
        {
            "date": "2015-04-16T13:06:17+0000",
            "author": "Yonik Seeley",
            "content": "So, we should do some simple warm up before running all tests\n\nNo... it's much more difficult to account for all the mud in the water by running different options in the same JVM run.  It's simplest to just not put mud in the water in the first place.  One needs to run each variant multiple times in different JVM runs as well since hotspot can sometimes optimize one run pretty well by luck.\n\nOh, and due to CPU speed changes due to thermal throttling, it's probably best to alternate  variants as well.  I did something like the following:\nfor i in 1 2 3 4 5 6 7 8 9 10; do test_variant1; test variant 2; done\n\nAnyway, Wha twere the numbers you got?\nI gave the aggregate results... I don't have time to re-run them all now. ",
            "id": "comment-14498028"
        },
        {
            "date": "2015-04-16T13:35:06+0000",
            "author": "Noble Paul",
            "content": "No... it's much more difficult to account for all the mud in the water by running different options in the same JVM run.\n\nIt is well known that the first run will be paying the penalty anyway. That is obvious from results I posted last. The non-cache run was 20% slower in the dry run and the next run with same set of params yield only a 5% advantage. \n\n\nI ran the same multiple times and I never saw a case where the cache was slower.\n\nAs we know that caching has better memory efficiency and the performance is marginally better, it is worth investigating what is the real performance before we ruling out this solution\n\nIt's simplest to just not put mud in the water in the first place\n\nI don't think you approach of fresh JVM is ideal because in reality we have a JVM that is warmed . According to me the better approach is to run this several times and look at the consistency of the numbers across different runs than running on a cold VM all the time. And my runs really show that caching consistently outperformed non caching (This is not into taking the GC costs into consideration at all) ",
            "id": "comment-14498048"
        },
        {
            "date": "2015-04-16T13:48:47+0000",
            "author": "Yonik Seeley",
            "content": "I don't think you approach of fresh JVM is ideal because in reality we have a JVM that is warmed . \n\na) \"warming\" the JVM with a different option of what you are testing is bad, it will lead to hotspot specialization and then de-specialization.\nb) run the test for longer... most of the time will be \nc) if you really want to warm the JVM, do the same variant twice and just time the second one\n\nIt is well known that [...]\n\nIf you insist on inferring performance rather than directly measuring it w/o throwing unnecessary mud in the water, then I'm just going to start ignoring your results and reiterate my -1 ",
            "id": "comment-14498052"
        },
        {
            "date": "2015-06-15T21:44:29+0000",
            "author": "Anshum Gupta",
            "content": "Bulk close for 5.2.0. ",
            "id": "comment-14586914"
        },
        {
            "date": "2015-09-18T14:47:48+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1703874 from shalin@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1703874 ]\n\nSOLR-7110: Added entry to CHANGES.txt under 5.2.0 ",
            "id": "comment-14875714"
        },
        {
            "date": "2015-09-18T14:48:46+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1703875 from shalin@apache.org in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1703875 ]\n\nAdded entry for SOLR-7110 and SOLR-7050 in the right places in CHANGES.txt ",
            "id": "comment-14875716"
        }
    ]
}