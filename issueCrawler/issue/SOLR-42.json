{
    "id": "SOLR-42",
    "title": "Highlighting problems with HTMLStripWhitespaceTokenizerFactory",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "3.6",
            "4.0-ALPHA"
        ],
        "components": [
            "highlighter"
        ],
        "type": "Bug",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Indexing content that contains HTML markup, causes problems with highlighting if the HTMLStripWhitespaceTokenizerFactory is used (to prevent the tag names from being searchable).\n\nExample title field:\n\n<SUP>40</SUP>Ar/<SUP>39</SUP>Ar laserprobe dating of mylonitic fabrics in a polyorogenic terrane of NW Iberia\n\nSearching for title:fabrics with highlighting on, the highlighted version has the <em> tags in the wrong place - 22 characters to the left of where they should be (i.e. the sum of the lengths of the tags).\n\nResponse from Yonik on the solr-user mailing-list:\n\nHTMLStripWhitespaceTokenizerFactory works in two phases...\nHTMLStripReader removes the HTML and passes the result to\nWhitespaceTokenizer... at that point, Tokens are generated, but the\noffsets will correspond to the text after HTML removal, not before.\n\nI did it this way so that HTMLStripReader  could go before any\ntokenizer (like StandardTokenizer).\n\nCan you open a JIRA bug for this?  The fix would be a special version\nof HTMLStripReader integrated with a WhitespaceTokenizer to keep\noffsets correct.",
    "attachments": {
        "htmlStripReaderTest.html": "https://issues.apache.org/jira/secure/attachment/12372537/htmlStripReaderTest.html",
        "SOLR-42.patch": "https://issues.apache.org/jira/secure/attachment/12372455/SOLR-42.patch",
        "TokenPrinter.java": "https://issues.apache.org/jira/secure/attachment/12375617/TokenPrinter.java",
        "HTMLStripReaderTest.java": "https://issues.apache.org/jira/secure/attachment/12372453/HTMLStripReaderTest.java",
        "HtmlStripReaderTestXmlProcessing.patch": "https://issues.apache.org/jira/secure/attachment/12375616/HtmlStripReaderTestXmlProcessing.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Hoss Man",
            "id": "comment-12462338",
            "date": "2007-01-04T22:21:50+0000",
            "content": "Suggestion from Mirko on solr-dev: change HTMLStripReader to replace striped HTML with equal length whitespace.\n\n(this could possibly be made a constructor option) "
        },
        {
            "author": "Paul Fryer",
            "id": "comment-12480554",
            "date": "2007-03-13T21:37:01+0000",
            "content": "Any update on this? I was hoping to use Solr to do full text search of HTML documents and provide snippets with search terms highlighted.  I assume I won't be able to provide accurate HTML snippet highlighting until this issue is resolved - correct? If so, what sort of time frame would be be looking at? Anything I can contribute to help? "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12480555",
            "date": "2007-03-13T21:40:12+0000",
            "content": "the most helpful contribution would be a patch containing a JUnit test demonstrating hte problem and the necessary fix \n\nhttp://wiki.apache.org/solr/HowToContribute "
        },
        {
            "author": "J.J. Larrea",
            "id": "comment-12532570",
            "date": "2007-10-05T04:37:27+0000",
            "content": "Here is the workaround I am using, along with a long comment explaining why:\n\nsolrconfig.xml\n\t\t<!--\n\t\t\tSpecial-case stuff for HTML tags in Abstract field:\n\t\t\tOriginally we had\n\t\t\t\t<tokenizer class=\"solr.HTMLStripWhitespaceTokenizerFactory\"/>\n\t\t\tbut pre-stripping destroys offsets needed for highiighting.\n\t\t\tTried an HTML tag-extraction RegEx as a post-process\n\t\t\t\t<tokenizer class=\"solr.WhitespaceTokenizerFactory\"/>\n\t\t\t\t<filter class=\"solr.PatternReplaceFilterFactory\"\n\t\t\t\t\tpattern=\"&lt;/?\\w+((\\s+\\w+(\\s*=\\s*(?:&quot;.*?&quot;|'.*?'|[^'&quot;>\\s]+))?)+\\s*|\\s*)/?&gt;\"\n\t\t\t\t\treplacement=\"\"\n\t\t\t\t\treplace=\"all\"/>\n\t\t\tbut it still doesn't adjust the offset and the subsequent WDF then created havoc.\n\t\t\tOne solution is to split on whitespace or tag delimiters (making tags\n\t\t\tinto text), and either index the tags or use StopFilter to remove 'em.\n\t\t\tBut the chosen solution is to swallow an entire chain of tags and any whitespace\n\t\t\twhich surrounds or separates them, leaving non-HTML < and > intact, or else runs\n\t\t\tof whitespace as normal.\n\n\t\t-->\n\t\t\t<tokenizer class=\"solr.PatternTokenizerFactory\"\n\t\t\t\t\tpattern=\"(?:\\s*&lt;/?\\w+((\\s+\\w+(\\s*=\\s*(?:&quot;.*?&quot;|'.*?'|[^'&quot;>\\s]+))?)+\\s*|\\s*)/?&gt;\\s*)++|\\s+\"/>\n\t\t\t<filter class=\"solr.ISOLatin1AccentFilterFactory\"/>\n\t\t\t...\n\n\n\nwithout the XMLEncoding the RegEx is:\n\n\t\t\t(?:\\s*</?\\w+((\\s+\\w+(\\s*=\\s*(?:\"?&\"'.?'|[^'\">\\s]))?)\\s*|\\s*)/?>\\s*)+|\\s\n\nand it will treat runs of \"things that look like HTML/XML open or close tags with optional attributes, optionally preceded or followed by spaces\" identically to \"runs of one or more spaces\" as token delimiters, and swallow them up, so the previous and following tokens have the correct offsets.\n\nOf course this is just a hack: It doesn't have any real understanding of HTML or XML syntax, so something invalid like </closing attr=\"x\"/> will get matched.  On the other hand, < and > in text will be left alone.  \n\nAlso note it doesn't decode XML or HTML numeric or symbolic entity references, as HTMLStripReader does (my indexer is pre-decoding the entity references before sending the text to Solr for indexing).\n\nSo fixing HTMLStripReader and its dependent HTMLStripXXXTokenizers to do the right thing with offsets would still be a worthy task.  I wonder whether recasting HTMLStripReader using the org.apache.lucene.analysis.standard.CharStream interface would make sense for this? "
        },
        {
            "author": "Stuart Sierra",
            "id": "comment-12542184",
            "date": "2007-11-13T17:58:12+0000",
            "content": "Another workaround is simply to strip HTML tags and entities before sending the document to Solr.  This gives correct highlights.  But if you want to retrieve the original HTML document you'll need to stash it somewhere else. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12555559",
            "date": "2008-01-03T12:54:51+0000",
            "content": "Might a solution to this be to replace the removed characters with whitespace, thus preserving the input offsets? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12555577",
            "date": "2008-01-03T14:15:27+0000",
            "content": "Here's the start of a test for the HTMLStripReader.  Note, in order to incorporate my suggestion of replacing HTML tags w/ whitespace, this test would need to be modified. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12555601",
            "date": "2008-01-03T16:20:52+0000",
            "content": "Patch that replaces HTML cruft with whitespace, thus preserving highlighting at the expense of extra characters.  Also included the a test file and test case\n\nNOTE: This is not backward-compatible with old HTMLStripReader, but seeing how it requires re-indexing to use anyway, I don't see that it is a concern. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12556098",
            "date": "2008-01-04T22:43:55+0000",
            "content": "Hmm, I don't know if this completely solves the problem even though the code seems like it is right.  I am still having trouble w/ the offsets.\n\nhas anyone else tried the patch? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12556104",
            "date": "2008-01-04T22:58:47+0000",
            "content": "Grant, I'm getting a test failure... did you forget to \"svn add\" some files?\n\n    <error message=\"src\\test\\test-files\\htmlStripReaderTest.html (The system ca\nnot find the path specified)\" type=\"java.io.FileNotFoundException\">java.io.File\notFoundException: src\\test\\test-files\\htmlStripReaderTest.html (The system cann\nt find the path specified)\n        at java.io.FileInputStream.open(Native Method)\n        at java.io.FileInputStream.<init>(FileInputStream.java:106)\n        at java.io.FileReader.<init>(FileReader.java:55)\n        at org.apache.solr.analysis.HTMLStripReaderTest.testHTML(HTMLStripReade\nTest.java:65) "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12556113",
            "date": "2008-01-04T23:24:28+0000",
            "content": "There is definitely still an issue w/ the HTMLStripReader in some circumstances, working on a test now. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12556115",
            "date": "2008-01-04T23:31:18+0000",
            "content": "Here is the file.  It shows it as being added in svn stat, but not sure why it wasn't included.\n\nIt goes in src/test/test-files\n\nIt's just a copy of site/index.html "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12556149",
            "date": "2008-01-05T01:12:25+0000",
            "content": "\nThere is definitely still an issue w/ the HTMLStripReader in some circumstances, working on a test now.\n\nRed herring.  The problem I was seeing is due to some tags that are valid syntax for my text are being stripped.  I think this patch can go ahead, although it might be useful to keep certain tags (i.e. a set of tags) "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12556158",
            "date": "2008-01-05T02:06:38+0000",
            "content": "Hmmm, I'm still getting the exception, even after adding the file.\nDid you test with \"ant clean test\", or with a GUI? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12556160",
            "date": "2008-01-05T02:09:25+0000",
            "content": "To elaborate, I think the CWD when running tests under ant is \"src\\test\\test-files\", so adding that to the filename when you try to open the file seems incorrect.  To match ant, I have intellij set up to use src\\test\\test-files as the CWD when running tests. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12556161",
            "date": "2008-01-05T02:44:39+0000",
            "content": "You are right.  I forgot that the CWD is different for Solr.  Feel  \nfree to strip off that initial path stuff, or I can generate a new  \npatch.\n\n-Grant\n\n\n\n--------------------------\nGrant Ingersoll\nhttp://lucene.grantingersoll.com\nhttp://www.lucenebootcamp.com\n\nLucene Helpful Hints:\nhttp://wiki.apache.org/lucene-java/BasicsOfPerformance\nhttp://wiki.apache.org/lucene-java/LuceneFAQ\n\n\n\n "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12556226",
            "date": "2008-01-05T13:55:00+0000",
            "content": "Here's a new version, with some more testing and the ability to preserve certain tags via a passed in set.  This is useful for text where some tags are meaningful or at least are useful further down the chain.  I am not sure why the html test file is not included.  svn stat shows:\n\nA  +   src/test/test-files/htmlStripReaderTest.html\nA      src/test/org/apache/solr/analysis/HTMLStripReaderTest.java\nM      src/java/org/apache/solr/analysis/HTMLStripReader.java\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12556247",
            "date": "2008-01-05T16:01:35+0000",
            "content": "committed.  Thanks Grant! "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12556634",
            "date": "2008-01-07T17:30:19+0000",
            "content": "Hmmm, still seems to be a problem with entities.  I think we need to replace the entity w/ the appropriate amount of whitespace.  I will work up test and patch. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12556636",
            "date": "2008-01-07T17:37:26+0000",
            "content": "Hmmm, this points out a deficiency in this approach... it could break up words or tokens (with whitespace) that were not originally separated (think international char in the middle of a word).\nSo I think this approach is probably OK for now, but a better approach would have the tokenizer get the offsets from the reader somehow (perhaps just a whitespace tokenizer with HTML stripping integrated). "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12556659",
            "date": "2008-01-07T19:21:24+0000",
            "content": "OK, here's a patch for the entity problem, but yes, I do agree Yonik, a better approach is needed.\n\nIt might be as simple as the SolrHighlighter being aware the the HTMLStripReader is being used. "
        },
        {
            "author": "Mike Klaas",
            "id": "comment-12556668",
            "date": "2008-01-07T19:51:35+0000",
            "content": "> It might be as simple as the SolrHighlighter being aware the the HTMLStripReader is being used.\n\nGrant, thanks for looking in to this issue.  What do you imagine the highlighter being able to do with that knowledge?  \n\nNote that the entity problem is an issue for search, not just highlighting.  The proper tokens will not be created in the case of international chars "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12556670",
            "date": "2008-01-07T20:02:38+0000",
            "content": "\nWhat do you imagine the highlighter being able to do with that knowledge?\n\nMy understanding of looking at the code is the disjoint comes from line 298.  In the call to Lucene's highlighter, we pass in the TokenStream, which has been stripped (or will be stripped if the the HTMLStripReader is employed) and the value from the stored field (docTexts[0]).  If, docTexts[0] was stripped first, then I think the offsets would be the same, no?  Of course, it would be really easy to test.  \n\nOf course, the real answer may be as suggested earlier and to apply the stripreader before sending to Solr. "
        },
        {
            "author": "Mike Klaas",
            "id": "comment-12556676",
            "date": "2008-01-07T20:12:41+0000",
            "content": "> Of course, the real answer may be as suggested earlier and to apply the stripreader before sending to Solr.\n\nHTMLStripTokenizer currently breaks the tokenizer contract, so it seems like the real answer is to fix the offsets.  I've glanced at the code, and it would be a significant amount of work to make the current implementation adhere to this contract.  The main problem is that no-one is really interested in doing this work.\n\n> > What do you imagine the highlighter being able to do with that knowledge?\n\n> My understanding of looking at the code is the disjoint comes from line 298. In the call to Lucene's highlighter, we pass in the TokenStream, which has > been stripped (or will be stripped if the the HTMLStripReader is employed) and the value from the stored field (docTexts[0]). If, docTexts[0] was stripped first, > then I think the offsets would be the same, no? Of course, it would be really easy to test.\n\nYou know, this is incredibly hacky, but I think that it is a great idea.\n\n-Mike\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12556729",
            "date": "2008-01-07T21:56:38+0000",
            "content": "I don't know much about unicode, but there are so many special characters in unicode, i just have to wonder if there is a special marker character that could be used instead of whitespace to \"fill in the gaps\" left when converting entities to real characters (or stripping tags).  ... something that isn't printable, and does't trigger any \"boundary\" logic (ie: note whitespace, punctuation, letter, digit, etc...)\n\n\n\tNUL perhaps?  (can you legally embed null in a string in java?)\n\tdoes anyone understand the definition of a \"nonspacing mark\" ?\n\tthe \"Invisible Separator\" character?\n\ta \"private use\" character?  (this actually seems like the most promising option)\n\n\n\nI say we just punt: have two options that allows users to specify characters: one for when tags are striped, one for when entities are converted to normal characters ... default both to an empty string (ie: current behavior) "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12556740",
            "date": "2008-01-07T22:15:12+0000",
            "content": "i just have to wonder if there is a special marker character that could be used instead of whitespace\nFor a hack, not a bad idea...\nThere could be a TokenFilter that removes any such characters in tokens, and it could even be automatically used by Tokenizers that use the html strip reader. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12556748",
            "date": "2008-01-07T22:35:07+0000",
            "content": "Hmmm.... I was assuming this would be an option on both the HTMLStripReader and the Tokenizers that use it (the tokenizers taking the option only to pass it on to the Reader) but i see what you mean ... once the Tokenizer knows the character positions of the \"words\" coming out of the text, it can then strip out those characters (Hmmm... strip the characters that are a placeholder for when other characters where already striped ... why do i feel like we're going to go to hell for this?)\n\nTf there were characters that we were certain would never appear in any unicode string, we could do it all under the covers by picking one of them ... but the safest thing to do would still be to have it as an option (but with a sensible default from the \"private use\" range instead of an empty string).  ... \n\nSo the HtmlStripReader would have a constructor that looks like...\n\n\n   /** \n   * @param entityFiller character to replace gaps made when entities are collapsed to real characters so that character positions still line up, may be null if no filler should be used \n   * @param tagFiller character to replace gaps made when entities are collapsed to real characters so that character positions still line up, may be null if no filler should be used \n   */\n  public HtmlStripReader(Reader input, Character entityFiller, Character tagFiller) { ... }\n\n\n\nand the Tokenizers could look like...\n\n\npublic class HTMLStripStandardTokenizerFactory extends BaseTokenizerFactory {\n  Pattern fillerPattern;\n  Character entityFiller, tagFiller; \n  public void init(...) {\n    entityFiller = getInitParam(...);\n    tagFiller = getInitParam(...)\n    fillerPattern = getInitParam(stripFiller) ? makePattern(entityFiller, tagFiller) : null;\n  }\n  public TokenStream create(Reader input) {\n    TokenStream s = new StandardTokenizer(new HTMLStripReader(input,entityFiller, tagFiller);\n    If (null != fillerPatterm) {\n      s = new PatternReplaceFiler(s, fillerPattern, \"\", true);\n    }\n    return s;\n  }\n}\n\n\n\nthat should totally work right? "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12556928",
            "date": "2008-01-08T16:19:26+0000",
            "content": "Yet another problem: \nIn certain circumstances, it is possible that restoreState() cannot be invoked b/c the mark has been lost due to moving well beyond it.  This is most noticeable in the while (true) loop inside of readProcessingInstruction() and can be caused by the following test:\n\npublic void testQuestionMark() throws Exception {\n    StringBuilder testBuilder = new StringBuilder(5020);\n    testBuilder.append(\"ah<?> \");\n    for (int i = 0; i < 5000; i++){\n      testBuilder.append('a');//tack on enough to go beyond the mark readahead limit, since <?> makes HTMLStripReader think it is a processing instruction\n    }\n    String test = testBuilder.toString();\n    Reader reader = new HTMLStripReader(new BufferedReader(new StringReader(test)));//force the use of BufferedReader\n    int ch = 0;\n    StringBuilder builder = new StringBuilder();\n    try {\n      while ((ch = reader.read()) != -1){\n        builder.append((char)ch);\n      }\n    } finally {\n      System.out.println(\"String: \" + builder.toString());\n    }\n    assertTrue(builder.toString() + \" is not equal to \" + test, builder.toString().equals(test) == true);\n  }\n\n\n\nIn this case, the final assert never gets hit, because there is an IOException in reader.read of:\n\njava.io.IOException: Mark invalid\n\tat java.io.BufferedReader.reset(BufferedReader.java:485)\n\tat org.apache.solr.analysis.HTMLStripReader.restoreState(HTMLStripReader.java:158)\n\tat org.apache.solr.analysis.HTMLStripReader.read(HTMLStripReader.java:731)\n\tat org.apache.solr.analysis.HTMLStripReaderTest.testQuestionMark(HTMLStripReaderTest.java:171)\n\n\n\nI will add that this is based off of text seen in the wild. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12556946",
            "date": "2008-01-08T17:06:58+0000",
            "content": "OK, I think the solution for these pieces is to change the \nwhile (true) loops inside of HTMLStripReader to only read up to READAHEAD chars so that it can always fall back to the last mark\n\n\nI will work up a patch. "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-12557256",
            "date": "2008-01-09T12:50:11+0000",
            "content": "OK, I'm 99.99% confident this fixes the issues w/ the while (true) loops and handles entities properly, etc.  and highlighting works correctly.  Added more tests, etc. "
        },
        {
            "author": "Erik Hatcher",
            "id": "comment-12557395",
            "date": "2008-01-09T19:41:44+0000",
            "content": "patch applied, all tests still pass, and committed.  Thanks Grant! "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12569035",
            "date": "2008-02-14T18:37:35+0000",
            "content": "The committed HtmlStripReader doesn't seem to handle offsets correctly for XML processing instructions such as this:\n\n    <?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n\nI'm attaching two files:\n\nHtmlStripReaderTestXmlProcessing.patch adds an HtmlStripReader test case to catch the problem. (The test currently fails.)\n\nTokenPrinter.java can help make it a little clearer what the problem actually is. Here is the output if I run it against against the analysis code in trunk. Note that the offsets are basically what one would expect, except in the XML processing instructions case, where the start position is off by one:\n\n-------------------------------------\nString to test: <uniqueKey>id</uniqueKey>\n Token info:\n   token 'id'\n     startOffset: 11\n     char at startOffset, and next few: 'id</u'\n-------------------------------------\nString to test: <!-- Unless this field is marked with required=\"false\", it will be a required field -->\n<uniqueKey>id</uniqueKey>\n Token info:\n   token 'id'\n     startOffset: 99\n     char at startOffset, and next few: 'id</u'\n-------------------------------------\nString to test: <!-- And now: two elements --> <element1>one</element1>\n <element2>two</element2>\n Token info:\n   token 'one'\n     startOffset: 41\n     char at startOffset, and next few: 'one</'\n   token 'two'\n     startOffset: 68\n     char at startOffset, and next few: 'two</'\n-------------------------------------\nString to test: <?xml version=\"1.0\" encoding=\"UTF-8\" ?><uniqueKey>id</uniqueKey>\n Token info:\n   token 'id'\n     startOffset: 49\n     char at startOffset, and next few: '>id</'\n------------------------------------- "
        },
        {
            "author": "Chris Harris",
            "id": "comment-12570338",
            "date": "2008-02-19T17:49:00+0000",
            "content": "Updating test case to reflect the fact that offset info still gets screwed up in the XML processing instruction case even if there are no XML elements in the source XML "
        },
        {
            "author": "solrize",
            "id": "comment-12620545",
            "date": "2008-08-07T08:09:39+0000",
            "content": "I am getting a ton of these errors in real documents with the 2008-07-30 nightly build.  Any advice?  Thanks. "
        },
        {
            "author": "Jim Murphy",
            "id": "comment-12625835",
            "date": "2008-08-26T19:41:08+0000",
            "content": "I've tracked down some background info on this issue - at least the way it was affecting me.  I could care less about highlighting - I'm using the HTMLStripWhitespaceTokenizerFactory during indexing to tokenize blog content - which obviously contains lots of html.  \n\nThe pathological case I've found with our input document set is:\n\nContent contains a malformed xml processing instruction in the first \"page\" of the buffer that contains more than one page of data.\n\nIt seems this is a fairly common (maybe MS Word XML?) form of invalid HTML.  Commonly it looks like this:\n\n...valid html...<?xml:namespace prefix = o />...valid html...\n\nNotice the PI starts with \"<?xml\" but terminates with a close tag., doh.\n\nThis issue is manifested in HTMLStripReader. It causes the following code to read too much off the buffer and invalidates the previous mark at the beginning of the tag.\n\nprivate int readProcessingInstruction() throws IOException {\n    // \"<?\" has already been read\n    while ((numRead - lastMark) < readAheadLimitMinus1) {\n      int ch = next();\n      if (ch=='?' && peek()=='>') \n{\n        next();\n        return MATCH;\n      }\n else if (ch==-1) \n{\n        return MISMATCH;\n      }\n\n    }\n    return MISMATCH;\n  }\n\nThe demoralizing part is the special treatment (readAheadLimitMinus1) isn't enough.  There is actually a \"over read\" by 2 chars.\n\nThe IOException - Invalid Mark happens when readProcessingInstruction() retuns (a mismatch because the entire buffer is read without finding the close PI) and restoreState(); is called to reset the marks - which fails.\n\nIf I tweak readAheadLimitMinus1 like this\n\nreadAheadLimitMinus1 -= 2 \n\nSo maybe the variable should be readAheadLimitMinus3 \n\nthen the buffer limits are preserved and the exception isn't found, parsing proceedes as expected.\n\nJim  \n "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12628473",
            "date": "2008-09-04T21:47:41+0000",
            "content": "A slight nit: Unicode contains three characters, fffD, fffE, and fffF, which are not supposed to appear in any document. One (fffD i think) is the offical \"not a character\" character, and should be used for the purpose of separating terms.\n "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12628475",
            "date": "2008-09-04T21:48:23+0000",
            "content": "Another bigger nit: the last time I studied this, the HTML stripper code did not pull alt-text fields as searchable tags. "
        },
        {
            "author": "Francisco Sanmart\u00edn",
            "id": "comment-12628650",
            "date": "2008-09-05T15:01:34+0000",
            "content": "The patch is for solr 1.3? \n\nI tried to apply the patch to solr 1.2 but it fails \n\nCan anybody complete the \"affected version\" and \"fix version\" field? thanks! "
        },
        {
            "author": "solrize",
            "id": "comment-12641883",
            "date": "2008-10-22T15:14:41+0000",
            "content": "This problem is still present in solr 1.3.0 and I'm getting a ton of those \"mark invalid\" exceptions.  Is this really a minor issue?!!!! "
        },
        {
            "author": "Mert Sakarya",
            "id": "comment-12717688",
            "date": "2009-06-09T14:26:52+0000",
            "content": "I think this is a problem of Microsoft Word. No one can say that;\n\n      ...valid html...<?xml:namespace prefix = o />...valid html...\n\nis a valid HTML. Any HTMLParser should look for a \"?>\" after a \"<?\"\n\nBUT! As a solution, I modified line 644 at HTMLStripReader.java as;\n\n      //if (ch=='?' && peek()=='>') {\n      if ((ch=='?' || ch=='/') && peek()=='>') { //This fixes Office Word problem, but might cause other problems!!! Be very careful.\n\nAnd created my own HTMLStripReader in another jar file. "
        },
        {
            "author": "Anders Melchiorsen",
            "id": "comment-12749421",
            "date": "2009-08-31T08:23:37+0000",
            "content": "I found the issue with entity replacement breaking up words, and created SOLR-1394.\n\nMy patch in that issue is to HTMLStripCharFilter. "
        },
        {
            "author": "David Smiley",
            "id": "comment-12848542",
            "date": "2010-03-23T04:33:02+0000",
            "content": "I was just reading this whole thread and SOLR-1394.  It seems this issue can be closed as a duplicate of SOLR-1394.  No? "
        },
        {
            "author": "Matthias Pigulla",
            "id": "comment-13027629",
            "date": "2011-05-02T12:00:05+0000",
            "content": "I don't think it's a duplicate and the issue is still unresolved at least in regard to comment-12625835 and the 1.4.1 release.\n\nThe input string \"<??>xx yy xx\" will have the start offsets for xx, yy and xx at 3, 6 and 9 respectively and is off by one.\n\n\"<? ?><? ?>xx yy xx\" [spaces added between question marks for JIRA display] will even have 6, 9 and 12, that is, every \"<??>\" (as a special \"degenerated\" kind of XML PI) will shift the offset by one. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13192226",
            "date": "2012-01-24T15:55:48+0000",
            "content": "Fixed by LUCENE-3690. "
        }
    ]
}