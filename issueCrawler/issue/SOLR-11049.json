{
    "id": "SOLR-11049",
    "title": "Solr in cloud mode silently fails uploading a big LTR model",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "contrib - LTR"
        ],
        "type": "Bug",
        "fix_versions": [
            "7.2",
            "master (8.0)"
        ],
        "affect_versions": "None",
        "resolution": "Workaround",
        "status": "Closed"
    },
    "description": "Hi,\n\nI'm using Solr in cloud mode, I have a MultipleAdditiveTreesModel with about 3MB in size. When I upload the model with\n\n\ncurl -v -XPUT 'http://localhost:8983/solr/tmdb/schema/model-store' --data-binary @/big-tree.model -H 'Content-type:application/json'\n\n\n\n\nI get the following response\n\n\n{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":24318}\n}\n\n\n\nThis looks kind of slow but without an error. When I check the config the model is not visible and when I try to run a query that uses the model I get the following error\n\n\n\"error\":{\n    \"metadata\":[\n      \"error-class\",\"org.apache.solr.common.SolrException\",\n      \"root-error-class\",\"org.apache.solr.common.SolrException\"],\n    \"msg\":\"cannot find model bigTreeModel\",\n    \"code\":400}\n\n\n\nWhen I upload the model to solr where I increased the zookeeper znode size limit with\n\n\n-Djute.maxbuffer=0x1ffffff\n\n\n\nthe same model upload succeeds much faster\n\n\n{\n  \"responseHeader\":{\n    \"status\":0,\n    \"QTime\":689}\n}\n\n\n\nThe model is visible in the configuration and queries that use it run without error.",
    "attachments": {
        "SOLR-11049.patch": "https://issues.apache.org/jira/secure/attachment/12879377/SOLR-11049.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-07-28T16:47:37+0000",
            "content": "Attaching draft patch to mention about jute.maxbuffer in the Learning-to-Rank Solr Reference Guide section, though I'm not entirely convinced about that being or not being 'right'. ",
            "author": "Christine Poerschke",
            "id": "comment-16105287"
        },
        {
            "date": "2017-07-29T01:38:59+0000",
            "content": "Zookeeper isn't suited to storing large data like this. I think you should explore using the blob store for this purpose. ",
            "author": "Shalin Shekhar Mangar",
            "id": "comment-16105969"
        },
        {
            "date": "2017-07-31T11:13:36+0000",
            "content": "Where can I find out more about this blob store? ",
            "author": "Stefan Langenmaier",
            "id": "comment-16107150"
        },
        {
            "date": "2017-07-31T14:29:21+0000",
            "content": "See https://lucene.apache.org/solr/guide/6_6/blob-store-api.html or https://cwiki.apache.org/confluence/display/solr/Blob+Store+API. ",
            "author": "Matthias Krueger",
            "id": "comment-16107364"
        },
        {
            "date": "2017-08-02T08:25:45+0000",
            "content": "Thanks for the pointers, but I seem to be a little bit lost. According to the documentation \"[the blob store] can be used to upload a jar file which contains standard solr components such as RequestHandlers, SearchComponents, or other custom code you have written for Solr.\"\n\nWhile the LTR model is used in request hanlders, as far as I understand it, it is uploaded and created through another mechanism and then Solr is repsonsible for storing it.\n\nI dont see yet how this is supposed to work. ",
            "author": "Stefan Langenmaier",
            "id": "comment-16110518"
        },
        {
            "date": "2017-08-02T10:22:33+0000",
            "content": "Stefan Langenmaier \u2013 my comment that you should explore the blob store (instead of increasing jute.maxBuffer setting) was meant for Christine and not you. The capability to load from the blob store is not yet implemented. ",
            "author": "Shalin Shekhar Mangar",
            "id": "comment-16110675"
        },
        {
            "date": "2017-08-02T10:59:38+0000",
            "content": "Shalin Shekhar Mangar - thanks for the clarification  ",
            "author": "Stefan Langenmaier",
            "id": "comment-16110708"
        },
        {
            "date": "2017-08-17T21:08:34+0000",
            "content": "Zookeeper isn't suited to storing large data like this. I think you should explore using the blob store for this purpose.\n\nThanks Shalin Shekhar Mangar for the blob store pointer!\n\nJust to cross-reference tickets here, Yuki Yano's SOLR-11250 takes a wrapper approach i.e. a wrapper model is still stored in ZooKeeper but the wrapped model's definition is stored externally. ",
            "author": "Christine Poerschke",
            "id": "comment-16131308"
        },
        {
            "date": "2017-12-01T15:52:30+0000",
            "content": "Thanks for reporting this issue and the -Djute.maxbuffer workaround!\n\nClosing this ticket with reference to the workaround and SOLR-11250 in 7.2 adding a DefaultWrapperModel class for loading of large and/or externally stored LTRScoringModel definitions. ",
            "author": "Christine Poerschke",
            "id": "comment-16274547"
        },
        {
            "date": "2018-03-09T23:10:25+0000",
            "content": "Just became aware of this issue due to the mailing list.\n\nIt's awesome that there's a workaround, and it does look like the reference guide was updated.\n\nBut anytime somebody performs an action and it doesn't work, Solr should not return a success status (0), and there should be at least one log entry explaining what went wrong.\n\nSeparately: Do we need to be worried about the fact that the failed upload took 24 seconds?  I'm guessing that there was at least one timeout involved with this.  I would have expected ZK to reject the upload quite quickly, and to do it in a way that Solr can detect as an error.  It would be good to figure out whether it was Solr or ZK that misbehaved here. ",
            "author": "Shawn Heisey",
            "id": "comment-16393741"
        }
    ]
}