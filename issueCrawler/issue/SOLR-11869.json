{
    "id": "SOLR-11869",
    "title": "Remote streaming UpdateRequestProcessor",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "UpdateRequestProcessors"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Won't Do",
        "status": "Closed"
    },
    "description": "When indexing documents from content management systems (or digital asset management systems) they usually have fields for metadata given by an editor and they in case of pdfs, docx or any other text formats\u00a0may also contain the binary content as well, which might be parsed to plain text using tika. This is whats currently supported by the ExtractingRequestHandler.\u00a0\n\nWe are now facing situations where we are indexing batches of documents using the UpdateRequestHandler and\u00a0want to send\u00a0the binary content of\u00a0the documents\u00a0mentioned above as part of the single request to the UpdateRequestHandler. As\u00a0those documents might\u00a0be of unknown size and\u00a0its difficult to send streams along the wire with javax.json APIs, I though about sending the url to the document itself, let solr fetch the document and let it\u00a0be parsed\u00a0by\u00a0tika - using a RemoteStreamingUpdateRequestProcessor.\u00a0\u00a0\n\nExample:\n\n{ \n \"add\": { \"id\": \"doc1\", \"meta\": \"foo\", \"meta\": \"bar\", \"text\": \"Short text\" }\n \"add\": { \"id\": \"doc2\", \"meta\": \"will become long\", \"text_ref\": \"http://...\" }\n}",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2018-01-18T13:12:37+0000",
            "content": "Basically its easy to implement that kind of UpdateRequestProcessor with the way UpdateRequestHandler works at the moment. The only missing piece is to prevent the entire document being loaded to memory. Its the DocumentBuilder#toDocument() that gets the\u00a0already processed\u00a0SolrInputDocument\u00a0at the end of an UpdateProcessorChain. It then calls\u00a0DocumentBuilder#addField() which gets the\u00a0schema defined field type and creates a new field for each of the values.\u00a0The FieldType#createField() method then does some processing on the value but at the end it\u00a0calls Field#Field(String, String, IndexableFieldType), so the value at the end is a string and therefor I would have to read the entire document into memory.\u00a0\n\nLuckily Field\u00a0has also a constructor accepting a Reader, so as a first step to make\u00a0it possible to read fields not only from in memory data but also streamed, I would like propose to\u00a0let FieldType#createField() properly handle Reader instances as value and then overwrite the behaviour of how a Reader is consumed for\u00a0the TextField and maybe StringField (not sure if it makes sense for others too).\u00a0\n\nThoughts? ",
            "author": "Dirk Rudolph",
            "id": "comment-16330492"
        },
        {
            "date": "2018-01-18T13:47:00+0000",
            "content": "If you wish to propose that Solr FieldType.createField and related plumbing work nicely with a Reader, then I think you should create an issue dedicated to that.  Also keep in mind that such a field cannot be \"stored\", since at the Lucene level it's required it be fully materialized to a String or BytesRef.  A further consequence of that is atomic-updates are not-possible.\n\nAnother thing that could be considered is using a BytesRef as the stored value, and wrapping a reader around it for Lucene Analyzer/TokenStream parts.  You wouldn't be truly streaming, but the RAM requirements should drop in half since you're working with UTF8 (usually 1-byte unicode characters) as opposed to a String (UTF16 usually 2-byte unicode characters).  This may have some gotchas, like highlighting and stored data retrieval which is anticipating a String from Lucene, not raw bytes.  BTW Lucene and Solr have code paths that recognize massive bytes<->char[] conversions and avoid over-allocating arrays by first computing how big the array on the other side needs to be by doing a preliminary pass to count the unicode chars. ",
            "author": "David Smiley",
            "id": "comment-16330528"
        },
        {
            "date": "2018-01-18T14:14:03+0000",
            "content": "I see.\u00a0So I will start without taking care of the document being fully read into memory or not.\n\nAnyway, would that kind of UpdateRequestProcessor be interesting for solr or\u00a0am I the only one facing that use case? ",
            "author": "Dirk Rudolph",
            "id": "comment-16330553"
        },
        {
            "date": "2018-01-18T18:01:48+0000",
            "content": "Just skimmed your first post. At a bit higher level, if you're running\nTika on the Solr server, that usually doesn't scale well for two\nreasons\n1> it puts a lot of CPU intensive work on the Solr box\n2> Tika sometimes hits OOMs, loops and the like. It has to deal with a\nton of wonky implementations of ill-defined specs.\n\nI'm not quite sure if this is germane to your question, but if so and\nyou can move your Tika processing off to an external client or service\nthat might be a better way to go... ",
            "author": "Erick Erickson",
            "id": "comment-16330877"
        },
        {
            "date": "2018-01-18T19:03:38+0000",
            "content": "I agree with Erick's sentiments \u2013 generally not a good idea.  Additionally if Solr can be instructed to fetch the contents of some remote URL, it will add a security concern. ",
            "author": "David Smiley",
            "id": "comment-16330991"
        },
        {
            "date": "2018-01-19T09:38:19+0000",
            "content": "Thanks for that valuable feedback. To summarise:\u00a0\n\n\tat the moment its not possible to create stored fields from a streamed/readable source\n\tprocessing with tika\u00a0might get expensive and brings certain risks to the box its running on\n\tfetching content from an URL\u00a0raise security concerns\n\n\n\nWith that in mind, I agree that it doesn't make much sense to spent time on that approach but better\u00a0look for\u00a0alternatives.\u00a0\n\nThanks again.\u00a0 ",
            "author": "Dirk Rudolph",
            "id": "comment-16332000"
        }
    ]
}