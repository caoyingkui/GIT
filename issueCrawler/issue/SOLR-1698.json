{
    "id": "SOLR-1698",
    "title": "load balanced distributed search",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "components": [],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Provide syntax and implementation of load-balancing across shard replicas.",
    "attachments": {
        "SOLR-1698.patch": "https://issues.apache.org/jira/secure/attachment/12429569/SOLR-1698.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Yonik Seeley",
            "id": "comment-12796410",
            "date": "2010-01-04T22:49:49+0000",
            "content": "This is related to the solr cloud branch, but is perhaps separable enough that I thought I'd try integrating into trunk.\nWe could add pipe delimiters between equivalent shards - useful for testing, troubleshooting, and ad hoc requests, even when shard info will be retrieved from zookeeper.\n\nExample shards param:\nshards=localhost:8983/solr|localhost:8985/solr,localhost:7574/solr|localhost:7576/solr "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12796417",
            "date": "2010-01-04T23:01:01+0000",
            "content": "Picking which shard replica to request can be random (round-robin or whatever, and customizable in the future), but a single distributed request should use the same replica for all phases of the request when possible. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12796585",
            "date": "2010-01-05T09:04:05+0000",
            "content": "is this related to SOLR-1431 . I though we can have custom ShardComponents for these things "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12796760",
            "date": "2010-01-05T16:51:02+0000",
            "content": "Another big question is: can we use LBHttpSolrServer for this, or are the needs too different?\n\nSome of the issues:\n\n\tneed control over order (so same server will be used in a single request)\n\tif we have a big cluster (100 shards), we don't want every node or every core to have 100 background threads checking liveness\n\tone request may want to hit addresses [A,B] while another may want [A,B,C] - a single LBHttpSolrServer can't currently do both at once, and separate instances wouldn't share liveness info.\n\n\n\nOne way: have many LBHttpSolrServer instances (one per shard group) but have them share certain things like the liveness of a shard and the background cleaning threads\n\nAnother way: have a single static LBHttpSolrServer instance that's shared for all requests, with an extra method that allows passing of a list of addresses on a per-request basis.\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12796837",
            "date": "2010-01-05T20:43:47+0000",
            "content": "Looking into LBHttpSolrServer more, it looks like we have some serious concurrency issues.  When a request does fail, a global lock is aquired to move from alive to zombie - but this same lock is used while making requests to check if zombies have come back (i.e. actual requests to zombies are being made with the lock held!).\n\nFor distributed search use (SearchHandler) I'm thinking of going with option #2 from my previous message (have a single static LBHttpSolrServer instance that's shared for all requests, with an extra method that allows passing of a list of addresses on a per-request basis.).  I'll address the concurrency issues at the same time. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12796838",
            "date": "2010-01-05T20:43:50+0000",
            "content": "Looking into LBHttpSolrServer more, it looks like we have some serious concurrency issues.  When a request does fail, a global lock is aquired to move from alive to zombie - but this same lock is used while making requests to check if zombies have come back (i.e. actual requests to zombies are being made with the lock held!).\n\nFor distributed search use (SearchHandler) I'm thinking of going with option #2 from my previous message (have a single static LBHttpSolrServer instance that's shared for all requests, with an extra method that allows passing of a list of addresses on a per-request basis.).  I'll address the concurrency issues at the same time. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-12797011",
            "date": "2010-01-06T06:48:06+0000",
            "content": "LBHttpSolrServer can have the concept of a sticky session and the session object can be used for all shard requests made in a single solr request. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12797271",
            "date": "2010-01-06T19:45:13+0000",
            "content": "Draft patch to  LBHttpSolrServer that lays the groundwork for being able to more easily use it in distributed search.  This also removes much of the locking.\n\nNext step will be to add a method that allows one to query an arbitrary server list in the given order.  This will involve the zombie list, but not the \"alive\" list.  We also need to avoid never-ending growth of the zombie list (and never-ending pinging of servers that are gone) by setting a ping limit. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12797809",
            "date": "2010-01-07T22:11:16+0000",
            "content": "Attaching new patch, still limited to LBHttpSolrServer at this point.\n\n\tincludes tests\n\tadds a new expert-level API:\n   public Rsp request(Req req) throws SolrServerException, IOException\n   I chose objects (Rsp and Req) since I imagine we will need to continue to add new parameters and controls to both the request and the response (esp the request... things like timeout, max number of servers to query, etc).  The Rsp also contains info about which server returned the response and will allow us to stick with the same server for all phases of a distributed request.\n\tadds the concept of \"standard\" servers (those provided by the constructor or addServer)... a server on the zombie list that isn't a standard server won't be added to the alive list if it wakes up, and will not be pinged forever.\n\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12798542",
            "date": "2010-01-10T20:36:48+0000",
            "content": "New patch that hooks in load balancing to distributed search.\nequivalent servers are separated by \"|\".\nExample: shards=localhost:8983,localhost:8985|localhost:9985 "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12798551",
            "date": "2010-01-10T22:31:40+0000",
            "content": "Another problem with the current load balancing algorithm is that if someone bounces both servers (or there is a temporary loss of network connectivity or whatever), they won't be marked up again until a successful ping (by default 1 minute later).\n\nInstead, it seems like we should first try all live servers, and then try all dead servers to see if they are still truly dead before failing a request. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12798558",
            "date": "2010-01-10T23:46:38+0000",
            "content": "Not sure what's going on... but it seems like if I have more than one load balancer instance that only one does zombie checks.  Really strange -  I've been banging my head against the wall for a while now. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12798841",
            "date": "2010-01-11T20:17:13+0000",
            "content": "Latest patch, everything back to working.  This fixes the load balancing algorithm to try dead servers if no live servers are available, adds tests for this case, and a bunch of other minor cleanups.\n\nI'll merge on the cloud branch very soon and give a little time for others to review before committing to trunk. "
        },
        {
            "author": "Uri Boness",
            "id": "comment-12798851",
            "date": "2010-01-11T20:35:51+0000",
            "content": "I think the patch doesn't work. I just checkout the trunk and applying the patch fails with a conflict for LBHttpSolrServer.java "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12798856",
            "date": "2010-01-11T20:52:00+0000",
            "content": "Hmmm, the rejection had $Id - that might be the cause.  I'll see if I can get rid of it first and generate a new patch. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12798858",
            "date": "2010-01-11T20:56:02+0000",
            "content": "OK, here's a new patch - update trunk first to get the removed $Id "
        },
        {
            "author": "Uri Boness",
            "id": "comment-12798870",
            "date": "2010-01-11T21:36:34+0000",
            "content": "yep.. that works "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12799250",
            "date": "2010-01-12T16:24:22+0000",
            "content": "This is now part of the solr_cloud branch.\nSeems like we should change the default number of retries from 3 to 1 in our HttpClient... otherwise failover can take a little while. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12799283",
            "date": "2010-01-12T17:41:24+0000",
            "content": "committed no-retries on solr_cloud branch.  Has the nice side-effect of speeding up some of the tests too. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12799367",
            "date": "2010-01-12T20:20:10+0000",
            "content": "note: it appears that hosts that don't exist (DNS failure) are loadbalanced fine on Windows, but cause the tests to fail (specifically TestDistributedSearch) on at least Ubuntu.  Looking into it...\n\nupdate: looks like the issue was down to the DNS provider returning a \"302 Found\" and an actual response for the bad host.  If I add a \".com\" to the bad host, everything starts working again.  That's what I've done for now. "
        },
        {
            "author": "patrick o'leary",
            "id": "comment-12799430",
            "date": "2010-01-12T22:17:58+0000",
            "content": "How does this work with search domains in resolv.conf ? "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13836545",
            "date": "2013-12-02T14:25:55+0000",
            "content": "I think this issue could be closed? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13836577",
            "date": "2013-12-02T14:59:04+0000",
            "content": "This has been incorporated in SolrCloud. "
        }
    ]
}