{
    "id": "LUCENE-831",
    "title": "Complete overhaul of FieldCache API/Implementation",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/search"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "Motivation:\n1) Complete overhaul the API/implementation of \"FieldCache\" type things...\n    a) eliminate global static map keyed on IndexReader (thus\n        eliminating synch block between completley independent IndexReaders)\n    b) allow more customization of cache management (ie: use \n        expiration/replacement strategies, disk backed caches, etc)\n    c) allow people to define custom cache data logic (ie: custom\n        parsers, complex datatypes, etc... anything tied to a reader)\n    d) allow people to inspect what's in a cache (list of CacheKeys) for\n        an IndexReader so a new IndexReader can be likewise warmed. \n    e) Lend support for smarter cache management if/when\n        IndexReader.reopen is added (merging of cached data from subReaders).\n2) Provide backwards compatibility to support existing FieldCache API with\n    the new implementation, so there is no redundent caching as client code\n    migrades to new API.",
    "attachments": {
        "fieldcache-overhaul.032208.diff": "https://issues.apache.org/jira/secure/attachment/12378441/fieldcache-overhaul.032208.diff",
        "LUCENE-831-trieimpl.patch": "https://issues.apache.org/jira/secure/attachment/12405292/LUCENE-831-trieimpl.patch",
        "ExtendedDocument.java": "https://issues.apache.org/jira/secure/attachment/12395557/ExtendedDocument.java",
        "LUCENE-831.patch": "https://issues.apache.org/jira/secure/attachment/12388649/LUCENE-831.patch",
        "LUCENE-831.03.31.2008.diff": "https://issues.apache.org/jira/secure/attachment/12378953/LUCENE-831.03.31.2008.diff",
        "LUCENE-831.03.28.2008.diff": "https://issues.apache.org/jira/secure/attachment/12378827/LUCENE-831.03.28.2008.diff",
        "fieldcache-overhaul.diff": "https://issues.apache.org/jira/secure/attachment/12353253/fieldcache-overhaul.diff",
        "LUCENE-831.03.30.2008.diff": "https://issues.apache.org/jira/secure/attachment/12378895/LUCENE-831.03.30.2008.diff"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2007-03-14T07:27:38+0000",
            "content": "\nThis is based on an idea i had a few months ago and was recently reminded of because of several mail threads about FieldCache .. so i started fleshing it out on the plane last week.\n\nI'm not entirely happy with it in it's current state, but I wanted to post it and see what people think of the overall approach.\n\nif people like the direction this is going, I would definitely appreciate some help with API critique and good unit tests (so far i've been relying solely on the existing Unit Tests to validate that i'm not breaking anything \u2013 but that doesn't really prove that the new APIs work the way they are intended to)\n\n\nTODO List\n\n\n\tlots of little :TODO:s in code, mainly about javadocs\n\tadd merge methods to StringIndexCacheKey\n   (a bit complicated, but should be possible/efficient)\n\tfigure out if there is any better way of dealing with\n   SortComparatorCacheKey and the visibility issues of \n   SortComparator.getComparable \n\tchange norm caching to use new caches (if not the same\n   main cache, then at least the same classes in a private cache)\n\twrite an ass load more tests\n\tis there a better way to deal with merging then to pass starts[] ?\n   (pass a new datastructure encapsulating starts/subReaders?)\n\tCacheFactory seemed like a good idea initially so that MultiReader\n   on a multi-segment index could cascade down, but what if people \n   only want caching at the outermost level (regardless of wether \n   the key is mergable) ... factory can't assuming anything if reader \n   is not an instance of MultiReader\n\taudit/change all core code using FieldCache to use new API\n\tperformance test that this doesn't hurt things in some way.\n\n\n ",
            "author": "Hoss Man",
            "id": "comment-12480676"
        },
        {
            "date": "2007-03-27T17:52:10+0000",
            "content": "I haven't looked at the patch yet.  However, I do know that a colleague of mine is about to start porting some FieldCache-based Filter stuff to Lucene's trunk.  Because that work may conflict with Hoss' changes here, we should see if this get applied first. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12484518"
        },
        {
            "date": "2007-06-04T03:39:31+0000",
            "content": "i haven't had any time to do further work on this issue ... partly because i haven't had a lot of time, but mainly because i'm hoping to get some feedback on the overall approach before any more serious effort investment.\n\nupdated patch to work against the trunk (r544035) ",
            "author": "Hoss Man",
            "id": "comment-12501086"
        },
        {
            "date": "2007-07-03T17:55:01+0000",
            "content": "I think this patch is great.  Not only does it make all of the sort caching logic much easier to decipher, being able to throw in a sophisticated cache manager like ehcache (which can let caches overflow to disk) could end up being pretty powerful. I am also very interested in the possibility of pre-warming a Reader.\n\nI will spend some time playing with what is here so far. ",
            "author": "Mark Miller",
            "id": "comment-12509958"
        },
        {
            "date": "2007-07-19T00:56:30+0000",
            "content": "thanks for the feedback mark ... i honestly haven't looked at this patch since the last time i updated the issue ... (i'm not sure if i've even thought about it once since then).  it's the kind of things that seemed really cool important at the time, but then ... you know, other things come up.\n\nby all means, feel free to update it.\n\nas i recall, the biggest thing about this patch that was really just pie in the sky and may not make any sense is the whole concept of merging and letting subreaders of MultiReader do their own caching which could then percolate up.  I did it on the assumption that it would come in handy when reopening an IndexReader that contains several segments \u2013 many of which may not have changed since the last time you opened the index.  but i really didn't have any idea how the whole reopening things would work.  i see now there is some reopen code in LUCENE-743, but frankly i'm still not sure wether the API makes sense, or is total overkill.\n\nit might be better to gut the merging logic from the patch and add it later if/when there becomes a more real use case for it (the existing mergeData and isMergable methods could always be re-added to the abstract base classes if it turns out they do make sense) ",
            "author": "Hoss Man",
            "id": "comment-12513777"
        },
        {
            "date": "2008-01-15T10:14:38+0000",
            "content": "I did some extensive tests with Lucene 2.3 today and was wondering, that IndexReader.reopen() in combination with FieldCache/Sorting does not bring a performance increase.\n\nUntil now, even if you reopen an Index using IndexReader.reopen(), you will get a new IndexReader instance which is not available in the default FieldCache. Because of that, all values from the cached fields must be reloaded. As reopen() only opens new/changed segments, a FieldCache implementation directly embedded into IndexReader as propsed by this issue would help here. Each segment would have its own FieldCache and reopening would be quite fast.\n\nAs with Lucene 2.3 the reopen is possible, how about this issue? Would this be possible for 2.4? ",
            "author": "Uwe Schindler",
            "id": "comment-12559008"
        },
        {
            "date": "2008-01-16T03:15:38+0000",
            "content": "\nAs with Lucene 2.3 the reopen is possible, how about this issue? Would this be possible for 2.4?\n\nYeah, this is a limitation currently of reopen(). I'm planning to work on it after the 2.3 release is\nout and LUCENE-584 is committed! ",
            "author": "Michael Busch",
            "id": "comment-12559341"
        },
        {
            "date": "2008-03-21T21:08:51+0000",
            "content": "I spent a little time getting this patch somewhat updated to the trunk and running various benchmarks with reopen. As expected, the sequence of searching a large index with a sort, adding a few docs and then reopening a Reader to perform a sorted search, can be 10 of times faster.\n\nI think the new API is great too - I really like being able to experiment with other caching strategies. I find the code easier to follow as well.\n\nDid you have any ideas for merging a String index? That is something that still needs to be done...\n\n\n\tMark\n\n ",
            "author": "Mark Miller",
            "id": "comment-12581189"
        },
        {
            "date": "2008-03-22T21:13:53+0000",
            "content": "Patch roughly moves things forward.\n\nI've added stuff for Long, Short, and Byte parsing, changed getAuto to a static cachkey method, switched core Lucene from using the FieldCache API to the new API, added some javadoc, and roughly have things going with the reopen method.\n\nIn short, still a lot to do, but this advances things enough so that you can apply it to trunk and check out the goodness that this can bring to sorting and reopen.\n\n\n\tMark\n\n ",
            "author": "Mark Miller",
            "id": "comment-12581291"
        },
        {
            "date": "2008-03-22T23:35:59+0000",
            "content": "Mark:\n\nI haven't looked at this issue or any of the code in my patch since my last updated, nor have i had a chance to look at your updated patch, but a few comments...\n\n1) you rock.\n\n2) I have no idea what i had in mind for dealing with StringIndex when i said \"a bit complicated, but should be possible/efficient\".  I do distinctly remember thinking that we should add support for just getting the string indexes (ie: the current int[numDocs]) for when you don't care what the strings are, just the sort order or just getting a String[numDocs] when you aren't doing sorting and just want an \"inverted inverted index\" on the field ... but obviously we still need to support the curent StringIndex (it's used in MultiSearcher right?) ",
            "author": "Hoss Man",
            "id": "comment-12581307"
        },
        {
            "date": "2008-03-23T12:47:24+0000",
            "content": "Right, I think its used in MultiSearcher and Parallel-MultSearcher and I believe its because you cannot compare simple ord arrays across Searchers and so you need the original sort term?\n\nI havn't been able to come up with anything that would be very efficient for merging a StringIndex, but I have not thought to much about it yet. Anyone out there have any ideas? Fastest way to merge two String[], each with an int[] indexing into the String[]? Is there a really fast way?\n\nI agree that it would be nice to skip the String[] if a MultiSearcher was not being used.\n\nI'll keep playing with it ",
            "author": "Mark Miller",
            "id": "comment-12581375"
        },
        {
            "date": "2008-03-25T01:19:42+0000",
            "content": "> I agree that it would be nice to skip the String[] if a MultiSearcher was not being used.\n\nIf you're going to incrementally update a FieldCache of a MultiReader, it's the same issue... can't merge the ordinals without the original (String) values. ",
            "author": "Yonik Seeley",
            "id": "comment-12581758"
        },
        {
            "date": "2008-03-26T20:43:00+0000",
            "content": "\nOne question here: should we switch to a method call, instead of a\nstraight array, to retrieve a cached value for a doc?\n\nIf we did that, then MultiSearchers would forward the request to the\nright IndexReader.\n\nThe benefit then is that reopen() of a reader would not have to\nallocate & bulk copy massive arrays when updating the caches.  It\nwould keep the cost of reopen closer to the size of the new segments.\nAnd this way the old reader & the new one would not double-allocate\nthe RAM required to hold the common parts of the cache.\n\nWe could always still provide a \"give me the full array\" fallback if\npeople really wanted that (and were willing to accept the cost).\n ",
            "author": "Michael McCandless",
            "id": "comment-12582422"
        },
        {
            "date": "2008-03-26T21:55:45+0000",
            "content": "\nThe benefit then is that reopen() of a reader would not have to\nallocate & bulk copy massive arrays when updating the caches. It\nwould keep the cost of reopen closer to the size of the new segments.\n\nI agree, Mike. Currently during reopen() the MultiSegmentReader \nallocates a new norms array with size maxDoc(), which is, as you said,\ninefficient if only some (maybe even small) segments changed.\n\nThe method call might be a little slower than the array lookup, but\nI doubt that this would be very significant. We can make this change for\nthe norms and run performance tests to measure the slowdown. ",
            "author": "Michael Busch",
            "id": "comment-12582443"
        },
        {
            "date": "2008-03-26T23:03:25+0000",
            "content": ">If you're going to incrementally update a FieldCache of a MultiReader, it's the same issue... can't merge the ordinals without the original (String) >values.\n\nThat is a great point.\n\n>should we switch to a method call, instead of a straight array, to retrieve a cached value for a doc?\n\nSounds like a great idea to me. Solves the StringIndex merge and eliminates all merge costs at the price of a method call per access. ",
            "author": "Mark Miller",
            "id": "comment-12582471"
        },
        {
            "date": "2008-03-26T23:44:57+0000",
            "content": "Hmm...how do we avoid having to pull the cached field values through a sync on every call? The field data has to be cached...and the method to return the single cached field value has to be multi-threaded... ",
            "author": "Mark Miller",
            "id": "comment-12582480"
        },
        {
            "date": "2008-03-27T09:41:51+0000",
            "content": "I think if we can finally move to having read-only IndexReaders then they would not sync on this method?\n\nAlso, we should still provide the \"give me the full array as of right now\" fallback which in a read/write usage would allow you to spend lots of RAM in order to not synchronize.  Of course you'd also have to update your array (or, periodically ask for a new one) if you are altering fields. ",
            "author": "Michael McCandless",
            "id": "comment-12582576"
        },
        {
            "date": "2008-03-28T23:13:19+0000",
            "content": "Here is a quick proof-of-concept type patch for using a method call rather than arrays. Speed pertaining to reopen.\n\nIn my quick test of 'open 500000 tiny docs index, repeat(3): add couple docs/sort search' the total time taken was:\n\nOrig FieldCache impl: 27 seconds\nNew impl with arrays: 12 seconds\nNew impl with method call: 3 seconds\n\nIts kind of a worse case scenerio, but much faster is much faster<g> The bench does not push through the point where method 3 would have to reload all of the segments, so that would affect it some...but method one is reloading all of the segments every single time...\n\nThis approach keeps the original approach for those that want to use the arrays. In that case everything still merges except for the StringIndex, so String sorting is slow. Lucene core is rigged to use the new method call though, so String sort is as sped up as the other field types when not using the arrays.\n\nNot sure everything is completely on the level yet, but all core tests pass (core sort tests can miss a lot).\n\nI lied about changing all core to use the new api...I havn't changed the function package yet.\n\n\n\tMark\n\n ",
            "author": "Mark Miller",
            "id": "comment-12583230"
        },
        {
            "date": "2008-03-30T12:35:32+0000",
            "content": "Another push forward:\n\n\n\tComparators are cached again (left it out before to think about).\n\tLots of new JavaDoc\n\tNaming, usage refinements\n\tStill doesn't touch the norms.\n\tCleanup, fixup, finishup, type stuff.\n\tDeprecated some function package methods that used FieldCache, but still need alternatives.\n\n ",
            "author": "Mark Miller",
            "id": "comment-12583443"
        },
        {
            "date": "2008-03-31T11:57:07+0000",
            "content": "-Further refinements and code changes.\n-Fixed ord comparisons across IndexReaders - as yonik pointed out. Standard sort tests didn't catch and I missed even with the reminder.\n-Added a bunch of CacheKey unit tests. ",
            "author": "Mark Miller",
            "id": "comment-12583650"
        },
        {
            "date": "2008-08-13T00:49:24+0000",
            "content": "I'm not sure how soon I'll have time to work on this; I don't want to block progress here, so I'm unassigning it for now in case someone else has time. ",
            "author": "Michael Busch",
            "id": "comment-12622052"
        },
        {
            "date": "2008-08-21T01:22:42+0000",
            "content": "Brings this patch back up to trunk level. ",
            "author": "Mark Miller",
            "id": "comment-12624217"
        },
        {
            "date": "2008-08-22T00:59:10+0000",
            "content": "Deprecating the function package is problematic. Doing it by method leaves the classes open to issues maintaining two possible states that could be mixed - the FieldCache and parsers are used as method params - you can't really replace the old internal representation with the new one. You really have to support the old method and new method and tell people not to use both or something. Doing it by class means duplicating half a dozen classes with new names. Neither is very satisfying. This stuff is all marked experimental and subject to change though - could it just be done clean sweep style? A little abrupt but...experimental is experimental <g> ",
            "author": "Mark Miller",
            "id": "comment-12624529"
        },
        {
            "date": "2008-08-22T14:07:15+0000",
            "content": "Would be nice to have TermVectorCache (if term vectors are stored in the index) ",
            "author": "Fuad Efendi",
            "id": "comment-12624835"
        },
        {
            "date": "2008-08-22T14:26:14+0000",
            "content": "That patch may have a goof , I'll peel off another soon. Unfortunately, it looks like this is slower than the orig implementation. I have to run more benchmarks, but it might even be in the 10-20% mark. My guess is that its the method calls - you may gain much faster reopen, but you appear to lose quite a bit on standard sort speed...\n\nCould give the choice of going with the arrays, if they prove as fast as orig, but then back to needing an efficient StringIndex merge... ",
            "author": "Mark Miller",
            "id": "comment-12624844"
        },
        {
            "date": "2008-08-23T17:54:35+0000",
            "content": "I've got the function package happily deprecated with replacement methods.\n\nI am going to ballpark the sort speed with method calls to be about 10% slower, but there are a lot of variables and I am still benchmarking.\n\nI've added a check for a system property so that by default, sorting will still use primitive arrays, but if you want to pay the small sort cost you can turn on the method calls and get faster reopen without cache merging.\n\nSo that wraps up everything I plan to do unless comments, criticisms bring up anything else.\n\nThe one piece that is missing and should be addressed is an efficient merge for the StringIndex.\n\nPatch to follow.\n\n\n\tmark\n\n ",
            "author": "Mark Miller",
            "id": "comment-12625084"
        },
        {
            "date": "2008-08-23T19:15:48+0000",
            "content": "Here is the patch - plenty to look over.\n\nThe method access might not be as much slower as I thought - might be closer to a couple to 5% than the 10% I first guessed.  ",
            "author": "Mark Miller",
            "id": "comment-12625089"
        },
        {
            "date": "2008-08-23T19:30:44+0000",
            "content": "change norm caching to use new caches (if not the same\nmain cache, then at least the same classes in a private cache)\n\n\nWhat benefit do you see to this? Does it offer anything over the simple map used now? ",
            "author": "Mark Miller",
            "id": "comment-12625098"
        },
        {
            "date": "2008-08-25T19:14:49+0000",
            "content": "What benefit do you see to this? Does it offer anything over the simple map used now?\n\nI think what i ment there was that if the norm caching used the same functionality then it could simplify the code, and norms would be optimized in the reopen case as well ... plus custom Cache Impls could report stats on norm usage (just like anything else) so people could  see when norms were getting used for fields they didn't expect them to be.\n\nBut as i've said before ... most of my early comments were pie in the sky theoretical brainstorming comments \u2013 don't read too much into them if they don't really make sense. ",
            "author": "Hoss Man",
            "id": "comment-12625470"
        },
        {
            "date": "2008-10-24T13:24:55+0000",
            "content": "Updated to trunk.\n\nTook out an optimization last patch - was using ordinals rather than strings to sort when Reader was not Multi - didn't like the isMulti on IndexReader though, so this patch and the last don't have it. ",
            "author": "Mark Miller",
            "id": "comment-12642459"
        },
        {
            "date": "2008-11-03T19:12:58+0000",
            "content": "This is missing a good way to manage the caching of the field values per cachekey (allowing for a more fine grained  weak or disk strategy for example). That type of stuff would have to be built into the cachkey currently - so to add a new strategy, youd have to implement a lot of new cachekeys and multiply your options to manage...for every int/long/byte/etc array and object array you would have to implement a new cachkey for a weakref solution. Is there an API that can overcome this?\n\nThe current cache that you can easily override, cachkey to data, is pretty course...I am not sure how many helpful things you can do easily. ",
            "author": "Mark Miller",
            "id": "comment-12644786"
        },
        {
            "date": "2008-11-19T22:10:05+0000",
            "content": "Another useful feature that seems like it would be pretty easy to implement on top of this patch is cache warming; that is, the ability to reopen an IndexReader and repopulate its caches before making it \"live\".  The main thing missing is a Cache.getKeys() method which could be used to discover what caches are already in use, in order to repopulate them after reopening the IndexReader.  This cache warming could be performed externally to the IndexReader (by calling getCacheData for each of the keys after reopening), or perhaps the reopen method could be overloaded with a boolean \"reloadCaches\" to perform this in the same method call.\n\nThe rationale for this I hope is clear; my application, like many I'm sure, keeps a single IndexReader for handling searches, and in a separate thread from search request handling commits writes and reopens the IndexReader before replacing the IndexReader reference for new searches (reference counting is then used to eventually close the old reader instances).  It would be ideal for that separate thread to also bear the expense of cache warming; even with this patch against our 4 GB indices, along with -Duse.object.array.sort=true,  a search request coming immediately after reopening the index will pause 20-40 seconds while the caches refill.  Preferably that could be done in the background and request handling threads would never be slowed down.\n ",
            "author": "Alex Vigdor",
            "id": "comment-12649180"
        },
        {
            "date": "2008-11-19T22:35:30+0000",
            "content": "Also - your reopen time will vary greatly depending on how many segments need to be reopened and what size they are...so I think a lower merge factor would help, while hurting overall search speed of course. ",
            "author": "Mark Miller",
            "id": "comment-12649188"
        },
        {
            "date": "2008-11-19T22:36:45+0000",
            "content": "You've tried the patch? Awesome!\n\nHow long did it take to fill the cache before the patch?\n\nI agree that we should be as friendly to warming as we can be. Keep in \nmind that you can warm the reader yourself by issuing a sorted search \nbefore putting the reader into duty - of course you don't get to warm \nfrom RAM like with what you suggest.\n\nKeep that feedback coming. I've been building momentum on coming back to \nthis issue, unless a commiter beats me to it.\n\n ",
            "author": "Mark Miller",
            "id": "comment-12649191"
        },
        {
            "date": "2008-11-20T14:56:03+0000",
            "content": "To be honest, the cache never successfully refilled before the patch - or at least I gave up after waiting 10 minutes.  I was about to give up on sorting.  It could have to do with the fact that we're running with a relatively modest amount of RAM (768M) given our index size. But with the patch at least sorting is a realistic option!\n\nI will look at adding the warming to my own code as you suggest; it is another peculiarity of this project that I can't know in the code what fields will be used for sorting, but I'll just track the searches coming through and aggregate any sorts they perform into a warming query. ",
            "author": "Alex Vigdor",
            "id": "comment-12649385"
        },
        {
            "date": "2008-11-20T15:24:12+0000",
            "content": "i haven't had any time to do further work on this issue ... partly because i haven't had a lot of time, but mainly because i'm hoping to get some feedback on the overall approach before any more serious effort investment. \n\nWheres that investment Hoss? You've orphaned your baby. There is a fairly decent amount of feedback here. ",
            "author": "Mark Miller",
            "id": "comment-12649391"
        },
        {
            "date": "2008-11-20T15:32:31+0000",
            "content": "I think this would actually be better if all cachekey types had to implement both ObjectArray access as well as primitive Array access. Makes the code cleaner and cuts down on the cachekey explosion. Should have done it this way to start, but couldnt see the forest through the trees back then i suppose. ",
            "author": "Mark Miller",
            "id": "comment-12649393"
        },
        {
            "date": "2008-12-04T23:38:23+0000",
            "content": "Updated to trunk.\n\nI've combined all of the dual (primitive array/ObjectArray) CachKeys into one. Each cache key can support both modes or throw UnsupportedException or something.\n\nI've also tried something a bit experimental to allow users to eventually use custom or alternate cachekeys (payload or sparse arrays or something) that work with internal sorting. A cache implementation can now supply a ComparatorFactory (name will prob be tweaked) that handles creating comparators. You can subclass ComparatorFactory and add new or override current supported CacheKeys.\n\nCustomComparators still needs to be twiddled with some.\n\nI've converted some of the sort tests to run with both primitive and object arrays as well.\n\n\n\tMark\nI\n\n ",
            "author": "Mark Miller",
            "id": "comment-12653546"
        },
        {
            "date": "2008-12-05T00:12:27+0000",
            "content": "Couple of needed tweaks and a test for a custom ComparatorFactory. ",
            "author": "Mark Miller",
            "id": "comment-12653560"
        },
        {
            "date": "2008-12-05T11:35:13+0000",
            "content": "change norm caching to use new caches (if not the same\n\nI think we could go even further, and [eventually] change norms to use an iterator API, which'd also have the same benefit of not requiring costly materialization of a full byte[] array for every doc in the index (ie, reopen() cost would be in proportion to changed segments not total index size).\n\nLikewise field cache / stored fields / column stride fields could eventually open up an iterator API as well.  This API would be useful if eg in a custom HitCollector you wanted to look at a field's value in order to do custom filtering/scoring. ",
            "author": "Michael McCandless",
            "id": "comment-12653757"
        },
        {
            "date": "2008-12-06T11:51:40+0000",
            "content": "\n[Note: my understanding of this area in general, and this patch in\nparticular, is still rather spotty... so please correct my\nmisconceptions in what follows...]\n\nThis change is a great improvement, since the cache management would\nbe per-IndexReader, and more public so that you could see what's\ncached, access the cache via the reader, swap in your own cache\nmanagement, etc.\n\nBut I'm concerned, because this change continues the \"materialize\nmassive array for entire index\" approach, which is the major remaining\ncost when (re)opening readers.  EG, isMergable()/mergeData() methods\nbuild up the whole array from sub readers.\n\nWhat would it take to never require materializing the full array for\nthe index, for Lucene's internal purposes (external users may continue\nto do so if they want)?  Ie, leave the array bound to the \"leaf\"\nIndexReader (ie, SegmentReader).  It was briefly touched on here:\n\n  https://issues.apache.org/jira/browse/LUCENE-1458?focusedCommentId=12650964#action_12650964\n\nI realize this is a big change, but I think we need to get there\neventually.\n\nEG I can see in this patch that MultiReader & MultiSegmentReader do\nexpose a CacheData that has get and get2 (why do we have get2?) that\ndelegate to child readers, which is good, but it's not good that they\nreturn Object (requires casting for every lookup).  We don't have\nper-atomic-type variants?  Couldn't we expose eg an IntData class (and\nall other types) that has int get(docID) abstract method, that\ndelegate to child readers?  (I'm also generally confused by why we\nhave the per-atomic-type switching happening in CacheKey subclasses\nand not CacheData.)\n\nThen... and probably the hardest thing to fix here: for all the\ncomparators we now materialize the full array.  I realize we use the\nfull array when sorting during a search of an\nIndexSearcher(MultiReader(...)), because FieldSortedHitQueue is called\nfor every doc visited and must be able to quickly make its comparison.\n\nHowever, stepping back, this is poor approach.  We should instead be\ndoing what MultiSearcher does, which is gather top results\nper-sub-reader, and then merge-sort the results.  At that point, to do\nthe merge, we only need actual field values for those docs in the top\nN.\n\nIf we could fix field-sorting like that (and I'm hazy on exactly how\nto do so), I think Lucene internally would then never need the full\narray?\n\nThis change also adds USE_OA_SORT, which is scary to me because Object\noverhead per doc can be exceptionally costly.  Why do we need to even\noffer that? ",
            "author": "Michael McCandless",
            "id": "comment-12654055"
        },
        {
            "date": "2008-12-06T12:10:11+0000",
            "content": "\nOne more thing here... while random-access lookup of a field's value\nvia MultiReader dispatch requires the binary search to find the right\nsub-reader, I think in most internal uses, the access could be\nswitched to an iterator instead, in which case the lookup should be\nfar faster.\n\nEG when sorting by field, we could pull say an IntData iterator from\nthe reader, and then access the int values in docID order as we visit\nthe docs.\n\nFor norms, which we should eventually switch to FieldCache +\ncolumn-stride fields, it would be the same story.\n\nAccessing via iterator should go a long ways to reducing the overhead\nof \"using a method\" instead of accessing the full array directly. ",
            "author": "Michael McCandless",
            "id": "comment-12654057"
        },
        {
            "date": "2008-12-06T13:13:51+0000",
            "content": "Ah, the dirty secret of 831 - there is plenty more to do  I've been pushing it down the path, but I've expected radical changes to be needed before it goes in.\n\nBut I'm concerned, because this change continues the \"materialize massive array for entire index\" approach, which is the major remaining cost when (re)opening readers. EG, isMergable()/mergeData() methods build up the whole array from sub readers.\n\nOriginally, 3.0 wasn't so close, so there was more concern with back compatibility than there might be now. I think the method call will be a slight slowdown no matter what as well...even with an iterator approach. Perhaps other \"wins\" will make up for it though. Its certainly cleaner to support 'one' mode.\n\nWhat would it take to never require materializing the full array for the index, for Lucene's internal purposes (external users may continue to do so if they want)? Ie, leave the array bound to the \"leaf\" IndexReader (ie, SegmentReader). \n\nI'm not sure I fully understand yet. If you use the ObjectArray mode, this is what happens right? Each sub array is bound to the IndexReader and MultiReader will distribute the requests to the right subreader. Only if you use the primitive arrays and merging do you get the full arrays (when not using USE_OA_SORT).\n\nI realize this is a big change, but I think we need to get there eventually.\n\nSounds good to me.\n\n(why do we have get2?) \n\nBecause a StringIndex needs to access both the array of Strings and a second array indexing into that. None of the other types need to access two arrays.\n\nCouldn't we expose eg an IntData class (and all other types) that has int get(docID) abstract method, that delegate to child readers?\n\nYeah, I think this would be possible. If casting does indeed cost so much, this may bring things closer to the primitive array speed.\n\nI'm also generally confused by why we have the per-atomic-type switching happening in CacheKey subclasses and not CacheData.\n\nFrom Hoss' original design. What are your concerns here? The right key gets you the right data  I've actually mulled this over some, buts its too early in the morning to remember I suppose. I'll look at it some more.\n\nIf we could fix field-sorting like that (and I'm hazy on exactly how to do so), I think Lucene internally would then never need the full array?\n\nThat would be cool. Again, I'll try to explore in this direction. It doesn't need the full array when using the ObjectArray stuff now though (well, it kind of does, just split up over the readers).\n\nThis change also adds USE_OA_SORT, which is scary to me because Object overhead per doc can be exceptionally costly. Why do we need to even offer that?\n\nAll this does at the moment (and I hate system properties, but for the moment, thats whats working) is switch between using the primitive arrays and merging or using the distributed ObjectArray for internal sorting. It defaults to using the primitive arrays and merging because its 5-10% faster than using the ObjectArrays. The ObjectArray approach is just an ObjectArray backed by an array for each Reader - a MultiReader distributes a requests for a doc field to the right Readers ObjectArray.\n\nTo your second comment...I'm gong to have to spend some more time \n\nNo worries though, this is very much a work in progress. I'd love to have it in by 3.0 though. Glad to see someone else taking more of an interest - very hard for me to find the time to dig into it all that often. I'll work with the code some as I can, thinking more about your comments, and perhaps I can come up with some better responses/ideas. \n ",
            "author": "Mark Miller",
            "id": "comment-12654064"
        },
        {
            "date": "2008-12-06T13:50:09+0000",
            "content": "This enhancement is particularly interesting to me (and the application my team is building). I'm not sure how much time I can donate but since I'd likely have to enhance this area of Lucene for our app anyway and it would be better to have it in core, I'd like to help out where I can.\n\nThe patch applies more or less cleanly against 2.4.0, but not trunk, btw. Is it possible to get the patch committed to a feature branch off of trunk perhaps?\n\nFinally, I'm most interested in the ability to make disk-backed caches. A very quick attempt to put the cache into JDBM failed as the CacheKey classes are not Comparable, which seems necessary for most kinds of disk lookup structures. SimpleMapCache uses a HashMap, which just needs equals/hashcode methods.\n\nThe other benefit to this approach is that it allows the data structures needed for sorting to be reused for range filtering. My application needs both, though on numeric field types (dates predominantly).\n\nFinally, this might also be a good time to add first class support for non-String field types. The formatting that NumberTools supplies is incompatible with SortField (the former outputs in base 36, the latter parses with parseLong), so there's clearly been several approaches to the general problem. In my case, I wrote a new Document class with addLong(name, value), etc. ",
            "author": "Robert Newson",
            "id": "comment-12654069"
        },
        {
            "date": "2008-12-06T14:10:53+0000",
            "content": "Hmmm - not sure what is up. There is already one small conflict for me (trunk is a rapidly changing target  ), but its a pretty simple conflict.\n\nThere are revision number issues (I was connected to an older revision apparently). If thats the problem, try this patch (which also resolves the new simple conflict). ",
            "author": "Mark Miller",
            "id": "comment-12654101"
        },
        {
            "date": "2008-12-06T14:16:50+0000",
            "content": "Maybe every asignee should tag his issues that are related to sorting to be related (or similar) to this one. I am thinking about the latest developments in LUCENE-1478 and LUCENE-1481 ",
            "author": "Uwe Schindler",
            "id": "comment-12654105"
        },
        {
            "date": "2008-12-06T14:24:42+0000",
            "content": "Maybe we need two trunks or branches or whatever . One for 2.9 and one for 3.0. This is a typical example for that in my opinion. ",
            "author": "Uwe Schindler",
            "id": "comment-12654106"
        },
        {
            "date": "2008-12-06T14:26:50+0000",
            "content": "\nThe conflict was easy to resolve, it was just an FYI, I appreciate trunk changes rapidly. I was just wondering if a feature branch would make synchronization easier.\n\nSome meta-task to combine or track these things would be great. I hit the problem that LUCENE-1478 describes. I'd previously indexed numeric values with NumberTools, which gives String-based sorting, the most memory-intensive one.\n\nIt seems with this field cache approach and the recent FieldCacheRangeFilter on trunk, that Lucene has a robust and coherent answer to performing efficient sorting and range filtering for float, double, short, int and long values, perhaps it's time to enhance Document. That might cut down the size of the API, which in turn makes it easy to test and tune. Document could preclude tokenization for such fields, I suspect I'm not the only one to build a type-safe replacement to Document.\n\nFor what it's worth, I'm currently indexed longs using String.format(\"%019d\") and treating dates as longs (getTime()) coupled with a long[] version of FieldCacheRangeFilter. It achieves a similar goal to this task, the long[] used for sorting is the same as for range filtering.  ",
            "author": "Robert Newson",
            "id": "comment-12654109"
        },
        {
            "date": "2008-12-08T13:44:49+0000",
            "content": "\nIt seems with this field cache approach and the recent FieldCacheRangeFilter on trunk, that Lucene has a robust and coherent answer to performing efficient sorting and range filtering for float, double, short, int and long values, perhaps it's time to enhance Document. That might cut down the size of the API, which in turn makes it easy to test and tune. Document could preclude tokenization for such fields, I suspect I'm not the only one to build a type-safe replacement to Document.\n\nThis is an interesting idea.  Say we create IntField, a subclass of\nField.  It could directly accept a single int value and not accept\ntokenization options.  It could assert \"not null\", if the field wanted\nthat.  FieldInfo could store that it's an int and expose more stronly\ntyped APIs from IndexReader.document as well.  If in the future we\nenable Term to be things-other-than-String, we could do the right\nthing with typed fields.  Etc.... ",
            "author": "Michael McCandless",
            "id": "comment-12654413"
        },
        {
            "date": "2008-12-08T14:03:11+0000",
            "content": "This is an interesting idea. Say we create IntField, a subclass of\nField. It could directly accept a single int value and not accept\ntokenization options. It could assert \"not null\", if the field wanted\nthat. FieldInfo could store that it's an int and expose more stronly\ntyped APIs from IndexReader.document as well. If in the future we\nenable Term to be things-other-than-String, we could do the right\nthing with typed fields. Etc....\n\nMaybe this new Document class could also manage the encoding of these fields to the index format. With that it would be possible to extend Document, to automatically use my trie-based encoding for storing the raw term values. On the otrher hand RangeQuery would be aware of the field encoding (from field metadata) and can switch dynamically to the correct search/sort algorithm. Great! ",
            "author": "Uwe Schindler",
            "id": "comment-12654417"
        },
        {
            "date": "2008-12-08T14:03:14+0000",
            "content": "\nYes, something like that. I made a Document class with an add method for each primitive type which allowed only the sensible choices for Store and Index. Field subclasses would achieve the same thing. A subclass per primitive type might be excessive, they'd be 99% identical to each other. A NumericField that could hold a single short, int, long, float, double or Date might be enough (new NumericField(name, 99.99F, true), the final boolean toggling YES/NO for Store, since Index is always UNANALYZED_NO_NORMS).\n\nAdding this to FieldInfo would change the on-disk format such that it remembers that a particular field is of a special type?  That way all the places that Lucene currently has a multiplicity of classes or constants (SortField.INT, etc) could be eliminated, replaced by first class support in Document/Field.\n\nA remaining question would be whether field name is sufficient for uniqueness, I suggest it becomes fieldname+type. This also implies changes to the Query and Filter hierarchy. \n\nIf it helps, I can post my Document class, which had helper methods for RangeFilter and TermQuery's for each type. It's not a complicated class, you can probably already picture it. ",
            "author": "Robert Newson",
            "id": "comment-12654418"
        },
        {
            "date": "2008-12-08T14:11:33+0000",
            "content": "\nType-safe Document-style object. Doesn't extend Document as it is final.\n ",
            "author": "Robert Newson",
            "id": "comment-12654421"
        },
        {
            "date": "2008-12-08T18:07:43+0000",
            "content": "M. McCandless:\n\n\"This is an interesting idea. Say we create IntField, a subclass of\nField. It could directly accept a single int value and not accept\ntokenization options. It could assert \"not null\", if the field wanted\nthat. FieldInfo could store that it's an int and expose more stronly\ntyped APIs from IndexReader.document as well. If in the future we\nenable Term to be things-other-than-String, we could do the right\nthing with typed fields. Etc....\"\n\n+1 For 3.0 this will be of great benefit in the effort to remove the excessive string creation\nthat happens right now with Lucene.  Term should also be more generic such that it\ncan also accept primitive or user defined types (and index format encodings).   ",
            "author": "Jason Rutherglen",
            "id": "comment-12654488"
        },
        {
            "date": "2008-12-09T15:15:51+0000",
            "content": "Marvin, does KS/Lucy have something like FieldCache?  If so, what API do you use?  Is it iterator-only? ",
            "author": "Michael McCandless",
            "id": "comment-12654820"
        },
        {
            "date": "2008-12-10T03:10:23+0000",
            "content": "> Marvin, does KS/Lucy have something like FieldCache? If so, what API do you\n> use? Is it iterator-only? \n\nAt present, KS only caches the docID -> ord map as an array.  It builds that\narray by iterating over the terms in the sort field's Lexicon and mapping the\ndocIDs from each term's posting list.\n\nBuilding the docID -> ord array is straightforward for a single-segment\nSegLexicon.  The multi-segment case requires that several SegLexicons be\ncollated using a priority queue.  In KS, there's a MultiLexicon class which\nhandles this; I don't believe that Lucene has an analogous class.\n\nRelying on the docID -> ord array alone works quite well until you get to the\nMultiSearcher case.  As you know, at that point you need to be able to\nretrieve the actual field values from the ordinal numbers, so that you can\ncompare across multiple searchers (since the ordinal values are meaningless).\n\n\nLex_Seek_By_Num(lexicon, term_num);\nfield_val = Lex_Get_Term(lexicon);\n\n\n\nThe problem is that seeking by ordinal value on a MultiLexicon iterator\nrequires a gnarly implementation and is very expensive.  I got it working, but\nI consider it a dead-end design and a failed experiment.\n\nThe planned replacement for these iterator-based quasi-FieldCaches involves\nseveral topics of recent discussion:\n\n  1) A \"keyword\" field type, implemented using a format similar to what Nate \n     and I came up with for the lexicon index.\n  2) Write per-segment docID -> ord maps at index time for sort fields.\n  3) Memory mapping.\n  4) Segment-centric searching.\n\nWe'd mmap the pre-composed docID -> ord map and use it for intra-segment\nsorting.  The keyword field type would be implemented in such a way that we'd\nbe able to mmap a few files and get a per-segment field cache, which we'd then\nuse to sort hits from multiple segments. ",
            "author": "Marvin Humphrey",
            "id": "comment-12655081"
        },
        {
            "date": "2008-12-12T12:12:09+0000",
            "content": "\n\n> At present, KS only caches the docID -> ord map as an array. It builds that\n> array by iterating over the terms in the sort field's Lexicon and mapping the\n> docIDs from each term's posting list.\n\nOK, that corresponds to the \"order\" array in Lucene's\nFieldCache.StringIndex class.\n\n\n> Building the docID -> ord array is straightforward for a single-segment\n> SegLexicon. The multi-segment case requires that several SegLexicons be\n> collated using a priority queue. In KS, there's a MultiLexicon class which\n> handles this; I don't believe that Lucene has an analogous class.\n\nLucene achieves the same functionality by using a MultiReader to read\nthe terms in order (which uses MultiSegmentReader.MultiTermEnum, which\nuses a pqueue under the hood) and building up StringIndex from that.\nIt's very costly.\n\n\n> Relying on the docID -> ord array alone works quite well until you get to the\n> MultiSearcher case. As you know, at that point you need to be able to\n> retrieve the actual field values from the ordinal numbers, so that you can\n> compare across multiple searchers (since the ordinal values are meaningless).\n\nRight, and we are trying to move towards pushing searcher down to the\nsegment.  Then we can use the per-segment ords for within-segment\ncollection, and then the real values for merging the separate pqueues\nat the end (but, initial results from LUCENE-1483 show that collecting\nN queues then merging in the end adds ~20% slowdown for N = 100\nsegments).\n\n\n> Lex_Seek_By_Num(lexicon, term_num);\n> field_val = Lex_Get_Term(lexicon);\n> \n> The problem is that seeking by ordinal value on a MultiLexicon iterator\n> requires a gnarly implementation and is very expensive. I got it working, but\n> I consider it a dead-end design and a failed experiment.\n\nOK.\n\n\n> The planned replacement for these iterator-based quasi-FieldCaches involves\n> several topics of recent discussion:\n> \n> 1) A \"keyword\" field type, implemented using a format similar to what Nate\n> and I came up with for the lexicon index.\n> 2) Write per-segment docID -> ord maps at index time for sort fields.\n> 3) Memory mapping.\n> 4) Segment-centric searching.\n> \n> We'd mmap the pre-composed docID -> ord map and use it for intra-segment\n> sorting. The keyword field type would be implemented in such a way that we'd\n> be able to mmap a few files and get a per-segment field cache, which we'd then\n> use to sort hits from multiple segments.\n\nOK so your \"keyword\" field type would expose random-access to field\nvalues by docID, to be used to merge the N segments' pqueues into a\nsingle final pqueue?\n\nThe alternative is to use iterator but pull the values into your\npqueues when they are inserted.  The benefit is iterator-only\nexposure, but the downside is likely higher net cost of insertion.\nAnd if the \"assumption\" is these fields can generally be ram resident\n(explicitly or via mmap), then the net benefit of iterator-only API is\nnot high. ",
            "author": "Michael McCandless",
            "id": "comment-12655988"
        },
        {
            "date": "2008-12-12T20:13:14+0000",
            "content": ">> Building the docID -> ord array is straightforward for a single-segment\n>> SegLexicon. The multi-segment case requires that several SegLexicons be\n>> collated using a priority queue. In KS, there's a MultiLexicon class which\n>> handles this; I don't believe that Lucene has an analogous class.\n> \n> Lucene achieves the same functionality by using a MultiReader to read\n> the terms in order (which uses MultiSegmentReader.MultiTermEnum, which\n> uses a pqueue under the hood) and building up StringIndex from that.\n> It's very costly.\n\nAh, you're right, that class is analogous.  The difference is that\nMultiTermEnum doesn't implement seek(), let alone seekByNum().  I was pretty\nsure you wouldn't have bothered, since by loading the actual term values into\nan array you eliminate the need for seeking the iterator.\n\n> OK so your \"keyword\" field type would expose random-access to field\n> values by docID, \n\nYes.  There would be three files for each keyword field in a segment.\n\n\n\tdocID -> ord map.  A stack of i32_t, one per doc.\n\tCharacter data.  Each unique field value would be stored as uncompressed\n    UTF-8, sorted lexically (by default).\n\tTerm offsets.  A stack of i64_t, one per term plus one, demarcating the\n    term text boundaries in the character data file.\n\n\n\nAssuming that we've mmap'd those files \u2013 or slurped them \u2013 here's the\nfunction to find the keyword value associated with a doc num:\n\n\nvoid\nKWField_Look_Up(KeyWordField *self, i32_t doc_num, ViewCharBuf *target)\n{\n    if (doc_num > self->max_doc) {\n        CONFESS(\"Doc num out of range: %u32 %u32\", \n    }\n    else {\n        i64_t offset      = self->offsets[doc_num];\n        i64_t next_offset = self->offsets[doc_num + 1];\n        i64_t len         = next_offset - offset;\n        ViewCB_Assign_Str(target, self->chardata + offset, len);\n    }\n}\n\n\n\nI'm not sure whether IndexReader.fetchDoc() should retrieve the values for\nkeyword fields by default, but I lean towards yes.  The locality isn't ideal,\nbut I don't think it'll be bad enough to contemplate storing keyword values\nredundantly alongside the other stored field values.\n\n> to be used to merge the N segments' pqueues into a\n> single final pqueue?\n\nYes, although I think you only need one two priority queues total: one\ndedicated to iterating intra-segment, which gets emptied out after each\nseg into the other, final queue.\n\n> The alternative is to use iterator but pull the values into your\n> pqueues when they are inserted. The benefit is iterator-only\n> exposure, but the downside is likely higher net cost of insertion.\n> And if the \"assumption\" is these fields can generally be ram resident\n> (explicitly or via mmap), then the net benefit of iterator-only API is\n> not high.\n\nIf I understand where you're going, you'd like to apply the design of the\ndeletions iterator to this problem?\n\nFor that to work, we'd need to store values for each document, rather than\nonly unique values... right?  And they couldn't be stored in sorted order,\nbecause we aren't pre-sorting the docs in the segment according to the value\nof a keyword field \u2013 which means string diffs don't help.  You'd have a\nsingle file, with each doc's values encoded as a vbyte byte-count followed by\nUTF-8 character data. ",
            "author": "Marvin Humphrey",
            "id": "comment-12656150"
        },
        {
            "date": "2008-12-16T20:27:21+0000",
            "content": "I was wondering if the next version of the patch could include a sample disk-based cache? It seems that CacheKey classes are fine for an in-memory HashMap (since SimpleMapCache works just fine) but I wonder if equals/hashCode is sufficient when the data is on disk? ",
            "author": "Robert Newson",
            "id": "comment-12657151"
        },
        {
            "date": "2008-12-17T14:28:55+0000",
            "content": "A couple things:\n\n\n\tLooking at the getCachedData method for MultiReader and MultiSegmentReader, it doesn't appear that the CacheData objects from merge operations are cached.  Is there any reason for this?\n\tI've written a merge method for StringIndexCacheKey. The process isn't all that complicated (apart from all of the off-by-ones), but it's expensive.\n\n\n\n\n  public boolean isMergable() {\n    return true;\n  }\n\n  private static class OrderNode {\n      int index;\n      OrderNode next;\n  }\n  \n  public CacheData mergeData(int[] starts, CacheData[] data) \n  throws UnsupportedOperationException {\n    int[] mergedOrder = new int[starts[starts.length - 1]];\n    // Lookup map is 1-based\n    String[] mergedLookup = new String[starts[starts.length - 1] + 1];\n    \n    // Unwrap cache payloads and flip order arrays\n    StringIndex[] unwrapped = new StringIndex[data.length];\n\n    /* Flip the order arrays (reverse indices and values)\n     * Since the ord map has a many-to-one relationship with the lookup table,\n     * the flipped structure must be one-to-many which results in an array of\n     * linked lists.\n     */\n    OrderNode[][] flippedOrders = new OrderNode[data.length][];\n    for (int i = 0; i < data.length; i++) {\n        StringIndex si = (StringIndex) data[i].getCachePayload();\n        unwrapped[i] = si;\n        flippedOrders[i] = new OrderNode[si.lookup.length];\n        for (int j = 0; j < si.order.length; j++) {\n            OrderNode a = new OrderNode();\n            a.index = j;\n            a.next = flippedOrders[i][si.order[j]];\n            flippedOrders[i][si.order[j]] = a;\n        }\n    }\n\n    // Lookup map is 1-based\n    int[] lookupIndices = new int[unwrapped.length];\n    Arrays.fill(lookupIndices, 1);\n\n    int lookupIndex = 0;\n    String currentVal;\n    int currentSeg;\n    while (true) {\n        currentVal = null;\n        currentSeg = -1;\n        int remaining = 0;\n        // Find the next ordered value from all the segments\n        for (int i = 0; i < unwrapped.length; i++) {\n            if (lookupIndices[i] < unwrapped[i].lookup.length) {\n                remaining++;\n                String that = unwrapped[i].lookup[lookupIndices[i]];\n                if (currentVal == null || currentVal.compareTo(that) > 0) {\n                    currentVal = that;\n                    currentSeg = i;\n                }\n            }\n        }\n        if (remaining == 1) {\n            break;\n        } else if (remaining == 0) {\n            /* The only way this could happen is if there are 0 segments or if\n             * all segments have 0 terms. In either case, we can return\n             * early.\n             */\n            return new CacheData(new StringIndex(\n                    new int[starts[starts.length - 1]], new String[1]));\n        }\n        if (!currentVal.equals(mergedLookup[lookupIndex])) {\n            lookupIndex++;\n            mergedLookup[lookupIndex] = currentVal;\n        }\n        OrderNode a = flippedOrders[currentSeg][lookupIndices[currentSeg]];\n        while (a != null) {\n            mergedOrder[a.index + starts[currentSeg]] = lookupIndex;\n            a = a.next;\n        }\n        lookupIndices[currentSeg]++;\n    }\n\n\n ",
            "author": "Jeremy Volkman",
            "id": "comment-12657401"
        },
        {
            "date": "2008-12-17T14:40:41+0000",
            "content": "Thanks Jeremey! Thats great news. \n\nI think the issues is going to heavily affected by LUCENE-1483, so once thats done, I hope I'll be able to come right back to this.\n\nLooking at the getCachedData method for MultiReader and MultiSegmentReader, it doesn't appear that the CacheData objects from merge operations are cached. Is there any reason for this?\n\nI think its just not done yet - I havn't looked at the merging since I started playing with the ObjectArrays - I think most of this stuff becomes moot with 1483 though - this will turn more into an API overhaul than an IndexReader reopen time saver. ",
            "author": "Mark Miller",
            "id": "comment-12657404"
        },
        {
            "date": "2008-12-17T14:47:11+0000",
            "content": "\n>  this will turn more into an API overhaul than an IndexReader reopen time saver.\n...and given the progress on LUCENE-1483 (copying values into the sort queues), I think this new FieldCache API should probably be primarily an iteration API. ",
            "author": "Michael McCandless",
            "id": "comment-12657409"
        },
        {
            "date": "2009-02-16T18:03:44+0000",
            "content": "Are there still things planned for this issue now that LUCENE-1483 has been committed?  ",
            "author": "Jeremy Volkman",
            "id": "comment-12674023"
        },
        {
            "date": "2009-02-17T16:09:37+0000",
            "content": "Are there still things planned for this issue now that LUCENE-1483 has been committed? \n\nGood question... I think it'd still be nice to 1) have the IndexReader\nexpose the API for accessing the FieldCache values, 2) allow for\ncustomization of the caching policy.\n\nThough maybe we should hold off on those changes until we do\nLUCENE-1231 (column stride fields), which I think would use exactly\nthe same API with the only difference being whether under-the-hood\nthere was a more efficient (column-stride storage) representation for\nthe field values vs the slower uninvert & resort (for StringIndex)\napproach that FieldCache does today.\n\nAlso, in the new API I'd like to make it not-so-easy to materialize\nthe full array.  I think it's OK to ask for the full array of a\nsub-reader, but if you want to access @ the MultiReader level, we\nshould encourage either random access getX(int docID), iteration or\nget-sub-arrays and append yourself. ",
            "author": "Michael McCandless",
            "id": "comment-12674255"
        },
        {
            "date": "2009-03-26T19:47:01+0000",
            "content": "Shall we try to gather all requirements we have for this feature here?\nI think recently new requirements were mentioned and they are now scattered accross different issues and email threads. ",
            "author": "Michael Busch",
            "id": "comment-12689620"
        },
        {
            "date": "2009-03-26T19:56:25+0000",
            "content": "I will attach my comments regarding the problem with the TrieRangeFilter and sorting (stop collecting terms into cache when lower precisions begin or only collect terms using a specific range (like a range filter). So you could fill a FieldCache and specify a starting term and ending term, all terms inbetween could be put into the cache, others outside left out. In this way, it would be possible to just use TrieUtils.prefixCodeLong() to specify the upper and lower integer bound encoded in the highest precision. ",
            "author": "Uwe Schindler",
            "id": "comment-12689624"
        },
        {
            "date": "2009-03-26T20:09:09+0000",
            "content": "One requirement i would like to request is the ability to attach an arbitrary object to each Segment.\nThis will allow people using lucene to store any arbitrary per segment caches and statistics that their application requires (fully free form)\n\nWould like to see the following:\n\n\tadd SegmentReader.setCustomCacheManager(CacheManager m) // mabye add a string for a CacheManager id (to allow registration of multiple cache managers)\n\tadd SegmentReader.getCustomCacheManager() // to allow accessing the manager\n\n\n\nCacheManager should be a very light interface (just a close() method that is called when the SegmentReader is closed)\n ",
            "author": "Tim Smith",
            "id": "comment-12689629"
        },
        {
            "date": "2009-03-27T09:45:54+0000",
            "content": "I'd like to see the new FieldCache API de-emphasize \"get me a single array holding all values for all docs in the index\" for a MultiReader. That invocation is exceptionally costly in the context of reopened readers, and providing the illusion that one can simply get this array is dangerous. It's a \"leaky API\", like how virtual memory API pretends you can use more memory than is physically available. \n\nI think it's OK to return an array-of-arrays (ie, one contiguous array per underlying segment); if the app really wants to make a massive array & concatenate it, they can do so outside of the FieldCache API. \n\nOr an method you call to get a given doc's value (like DocValues in o.a.l.search.function). Or an iterator API to step through all the values. \n\nWe should also set this API up as much as possible for LUCENE-1231. Ie, the current \"un-invert the field\" approach that FieldCache takes is merely one source of values per doc. Column stride fields in the future will be a different (faster) source of values, that should be able to \"just plug in\" under the hood somehow to this same exposure API. \n\nOn Uwe's suggestion for some flexibility on how the un-inversion takes place, I think allowing differing degrees of extension makes sense. EG we already allow you to provide a custom parser. We need to allow control on whether a given value replaces the already-seen value (LUCENE-1372), or whether to stop the looping early (Uwe's needs for improving Trie). We should also allow outright entire custom class that creates the value array. ",
            "author": "Michael McCandless",
            "id": "comment-12689863"
        },
        {
            "date": "2009-03-27T10:25:27+0000",
            "content": "Adding to Tim, I'd like to see the ability not only to be notified of SegmentReader destruction, but of SegmentReader creation (within reopen) too. And new FieldCache logic should be built on these notifications.\nThen it's possible to extend/replace Lucene's native FieldCache, then it's possible to create a cache specialized for trie-fields.\n\nLinking objects to each other with WeakHashmaps is insanely evil, especially in the case when object creation/destruction is clearly visible. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12689867"
        },
        {
            "date": "2009-03-28T16:05:42+0000",
            "content": "Let's make sure the new API fixes LUCENE-1579. ",
            "author": "Michael McCandless",
            "id": "comment-12693464"
        },
        {
            "date": "2009-04-09T12:02:07+0000",
            "content": "\nI'd like to see the new FieldCache API de-emphasize \"get me a single array holding all values for all docs in the index\" for a MultiReader. That invocation is exceptionally costly in the context of reopened readers, and providing the illusion that one can simply get this array is dangerous. It's a \"leaky API\", like how virtual memory API pretends you can use more memory than is physically available.\n\nI think it's OK to return an array-of-arrays (ie, one contiguous array per underlying segment); if the app really wants to make a massive array & concatenate it, they can do so outside of the FieldCache API. \n\nIs there much difference in one massive array or an array of arrays? Its just as much space and just as dangerous, right? Some apps will need random access to the field cache for any given document right? Don't we always have to support that in some way, and won't it always be severely limited by RAM (until IO is as fast)?\n\nI like the idea of an iterator API, but it seems we will still have to provide random access with all its problems, right?\n\n\nWe should also set this API up as much as possible for LUCENE-1231. Ie, the current \"un-invert the field\" approach that FieldCache takes is merely one source of values per doc. Column stride fields in the future will be a different (faster) source of values, that should be able to \"just plug in\" under the hood somehow to this same exposure API.\n\nDefinitely.\n\n\nOn Uwe's suggestion for some flexibility on how the un-inversion takes place, I think allowing differing degrees of extension makes sense. EG we already allow you to provide a custom parser. We need to allow control on whether a given value replaces the already-seen value (LUCENE-1372), or whether to stop the looping early (Uwe's needs for improving Trie). We should also allow outright entire custom class that creates the value array.\n\nAllow a custom field cache loader for each type? ",
            "author": "Mark Miller",
            "id": "comment-12697485"
        },
        {
            "date": "2009-04-09T12:18:56+0000",
            "content": "Is there much difference in one massive array or an array of arrays? Its just as much space and just as dangerous, right?\n\nOne massive array is far more dangerous during reopen() (ie that's why we did LUCENE-1483) since it's non-incremental.\n\nArray-per-segment I think is OK.\n\nBut yes both of them consume RAM, but I don't consider that \"dangerous\".\n\nI like the idea of an iterator API, but it seems we will still have to provide random access with all its problems, right?\n\nRight.\n\nSome apps will need random access to the field cache for any given document right?\n\nYes but I think such apps should move to the per-segment model (eg a Filter's getDocIdSet is called per segment reader).\n\nIf an app really wants to make a single massive array, they can certainly do so, outside of Lucene.\n\nAllow a custom field cache loader for each type?\n\nYes, possibly w/ different degrees of extension (much like Collector).  EG maybe you just want to override how you parse an int, or maybe you want to take control over the entire uninversion. ",
            "author": "Michael McCandless",
            "id": "comment-12697492"
        },
        {
            "date": "2009-04-09T12:38:41+0000",
            "content": "\nOne massive array is far more dangerous during reopen() (ie that's why we did LUCENE-1483) since it's non-incremental.\n\nArray-per-segment I think is OK.\n\nBut yes both of them consume RAM, but I don't consider that \"dangerous\".\n\nOkay, I got you now. We will force everyone to migrate to use fieldcache at the segment level rather than MR (or create their own array from the subarrays). ",
            "author": "Mark Miller",
            "id": "comment-12697495"
        },
        {
            "date": "2009-04-09T14:21:47+0000",
            "content": "But yes both of them consume RAM, but I don't consider that \"dangerous\".\n\nI guess you meant dangerous as in dangerous to reopen then? I actually thought you meant as in dangerous because it could require too may resources. Dangerous is a tough to pin down word \n\nSo what are the advantages of the iterator API again then? It not likely you are going to stream the values, and random access will likely still have a use as mentioned.\n\nJust trying to get a clearer picture in my head - I doubt I'll have time, but I'd love to put a little sweat into this issue.\n\nIt probably makes sense to start from one of Hoss's original patches or even from scratch. ",
            "author": "Mark Miller",
            "id": "comment-12697519"
        },
        {
            "date": "2009-04-09T15:42:14+0000",
            "content": "\nI guess you meant dangerous as in dangerous to reopen then? I actually thought you meant as in dangerous because it could require too may resources. Dangerous is a tough to pin down word \nDangerous is a dangerous word \n\nI meant: I don't like exposing non-performant APIs; they are sneaky traps.  (EG TermEnum.skipTo is another such API).\n\nSo what are the advantages of the iterator API again then?\n\nThe big advantage is the possibility of backing it with eg an IndexInput, so that the values need to all be in RAM for one segment.  Though, as Lucy is doing, we could leave things on disk and still have random access via mmap, which perhaps should be an option for Lucene as well.  However, iterator only messes up out-of-order scoring (BooleanScorer for OR queries), so I'm tentatively leaning against iterator only at this point.\n\nbut I'd love to put a little sweat into this issue.\n\nThat would be AWESOME (if you can somehow make time)!\n\nWe should hash out the design a bit before figuring out how/where to start. ",
            "author": "Michael McCandless",
            "id": "comment-12697540"
        },
        {
            "date": "2009-04-10T14:09:12+0000",
            "content": "Some random thoughts:\n\nIf we are going to allow random access, I like the idea of sticking with the arrays. They are faster than hiding behind a method, and it allows easier movement from the old API. It would be nice if we can still deprecate all of that by backing it with the new impl (as done with the old patch).\n\nThe current API (from this patch) still looks fairly good to me - a given cachekey gets your data, and knows how to construct it. You get data something like: return (byte[]) reader.getCachedData(new ByteCacheKey(field, parser)). It could be improved, but it seems a good start to me.\n\nThe immediate problem I see is how to handle multireader vs reader. Not being able to treat them the same is a real pain. In the segment case, you just want an array back, in the multi-segment perhaps an array of arrays? Or unsupported? I havn't thought of anything nice.\n\nWe have always been able to customize a lot of behavior with our custom sort types - I guess the real issue is making the built in sort types customizable. So I guess we need someway to say, use this \"cachekey\" for this built in type?\n\nWhen we load the new caches in FieldComparator, can we count on those being segmentreaders? We can Lucene wise, but not API wise right? Does that matter? I suppose its really tied in with the multireader vs reader API. ",
            "author": "Mark Miller",
            "id": "comment-12697812"
        },
        {
            "date": "2009-04-10T14:40:50+0000",
            "content": "\nIf we are going to allow random access, I like the idea of sticking\nwith the arrays. They are faster than hiding behind a method, and it\nallows easier movement from the old API.\n\nI agree.\n\n\nIt would be nice if we can\nstill deprecate all of that by backing it with the new impl (as done\nwith the old patch).\n\nThat seems fine?\n\nThe current API (from this patch) still looks fairly good to me - a given cachekey gets your data, and knows how to construct it. You get data something like: return (byte[]) reader.getCachedData(new ByteCacheKey(field, parser)). It could be improved, but it seems a good start to me.\n\nAgreed.\n\nThe immediate problem I see is how to handle multireader vs reader. Not being able to treat them the same is a real pain. In the segment case, you just want an array back, in the multi-segment perhaps an array of arrays? Or unsupported? I havn't thought of anything nice.\n\nI would lean towards throwing UOE, and suggesting that you call\ngetSequentialReaders instead.\n\nEg with the new getUniqueTermCount() we do that.\n\nWe have always been able to customize a lot of behavior with our custom sort types - I guess the real issue is making the built in sort types customizable. So I guess we need someway to say, use this \"cachekey\" for this built in type?\n\nI don't quite follow that last sentence.\n\nWe'll have alot of customizability here, ie, if you want to change how\nString is parsed to int, if you want to fully override how uninversion\nworks, etc.  At first the core will only support uninversion as a\nsource of values, but once CSF is online that should be an alternate\npluggable source, presumably plugging in the same way that\ncustomization would allow you to override uninversion.\n\nWhen we load the new caches in FieldComparator, can we count on those being segmentreaders? We can Lucene wise, but not API wise right? Does that matter? I suppose its really tied in with the multireader vs reader API.\n\nOnce getSequentialSubReaders() is called (and, recursively if needed),\nthen those \"atomic\" readers should be able to provide values.  I guess\nthat's the contract we require of a given IndexReader impl? ",
            "author": "Michael McCandless",
            "id": "comment-12697820"
        },
        {
            "date": "2009-04-10T14:49:55+0000",
            "content": "\nWe have always been able to customize a lot of behavior with our custom sort types - I guess the real issue is making the built in sort types customizable. So I guess we need someway to say, use this \"cachekey\" for this built in type?\n\nI don't quite follow that last sentence.\n\nWe'll have alot of customizability here, ie, if you want to change how\nString is parsed to int, if you want to fully override how uninversion\nworks, etc.  At first the core will only support uninversion as a\nsource of values, but once CSF is online that should be an alternate\npluggable source, presumably plugging in the same way that\ncustomization would allow you to override uninversion.\n\nRight - since a custom cachekey builds the array from a reader, you can pretty much do anything. What I meant was that you could do anything before with a custom sort type as well - the problem was that you could not say use this custom sort type when sorting on a built in type (eg INT, BYTE, STRING). So thats all we need, right? A way to say, use this builder (cachekey) for LONG, use this one for INT, etc. When we get CSF, you would set it to use cachekeys that built arrays from that data. ",
            "author": "Mark Miller",
            "id": "comment-12697822"
        },
        {
            "date": "2009-04-10T15:01:31+0000",
            "content": "A way to say, use this builder (cachekey) for LONG, use this one for INT, etc. When we get CSF, you would set it to use cachekeys that built arrays from that data.\n\nThat sounds right, though it'd presumably be field dependent rather than relying on only the native type?  Ie I may have 3 fields that should load long[]'s, but each has its own custom decoding to be done. ",
            "author": "Michael McCandless",
            "id": "comment-12697830"
        },
        {
            "date": "2009-04-10T15:06:19+0000",
            "content": "Yes, good point. Okay, I think I have a much clearer picture of what needs to be done - this may be less work than I thought - a lot of what has been done is probably still helpful. ",
            "author": "Mark Miller",
            "id": "comment-12697832"
        },
        {
            "date": "2009-04-11T02:19:31+0000",
            "content": "Here is fairly decent base to start from.\n\nStill needs a lot, but a surprising amount of tests pass. Essentially stripped out what isnt needed due to 1483, updated to trunk (new 1483 API), and added a bit of the new stuff we need.\n\nSome of the work to do:\n\nI think cache needs a clone for indexreader cloning.\n\nNeed to come up with a good way to specify type to cachekey configuration - I threw some paint on the wall. Once we go filed/type - cachekey, almost seems we are getting into schema territory...\n\nHave to deal with new Parser interface in FieldCache (back compat)\n\nWe may need the multireader to do the full array for back compat.\n\nI dont think any of the custom type stuff is setup to work yet.\n\nIterate iterate iterate I suppose. ",
            "author": "Mark Miller",
            "id": "comment-12698048"
        },
        {
            "date": "2009-04-11T12:57:40+0000",
            "content": "Iterate iterate iterate I suppose.\n\nHere here!  Ready, set, GO!\n\nWe may need the multireader to do the full array for back compat.\n\nCan't we just create the \"make massive array & copy sub arrays in\"\ninside the old FieldCache?  (And deprecate the old FieldCache\nentirely).\n\nI dont think any of the custom type stuff is setup to work yet.\n\nHow about we create a ValueSource abstract base class, that defines\nabstract byte[] getBytes(IndexReader r, String field),\nint[] getInts(IndexReader r, String field), etc.  (Just like\nExtendedFieldCache).\n\nThis is subclassed to things like UninversionValueSource (what\nFieldCache does today), CSFValueSource (in the future) both of which\ntake an IndexReader when created.\n\nUninversionValueSource should provide basic ways to customize the\nuninversion.  Hopefully, we can share mode code than the current\nFieldCacheImpl does (eg, a single \"enum terms & terms docs\" loop that\nswitches out to a \"handler\" to deal with each term & doc, w/\nsubclasses that handle to byte, int, etc.).\n\nAnd then I can also make MyFunkyValueSource (for extensibility) that\ndoes whatever to produce the values.\n\nThen we make CachingValueSource, that wraps any other ValueSource.\n\nAnd finally expose a way in IndexReader to set its ValueSource when\nyou open it?  It would default to\nCachedValueSource(UninversionValueSource()).  I think we should\nrequire that you set this on opening the reader, and you can't later\nchange it.\n\nThis would mean a single CachingValueSource can be used for more than\none reader, which is good because IndexReader.open would send it down\nto all SegmentReaders it opens.\n\nThis would then replace *CacheKey.\n\nThis approach is not that different from what we have today, but I\nthink there are important differences:\n\n\n\tDecouple value generation (ValueSource) from caching\n\n\n\n\n\tTell IndexReader what its ValueSource is, so eg when you do\n    sorting the sort pulls from your ValueSource and not a global\n    default one.\n\n\n\n\n\tHopefully don't duplicate so much code (eg uninversion)\n\n\n\nOther thoughts:\n\n\n\tPresumably, at this point, the arrays returned by field cache\n    should be considered readonly by the app, right?  So cloning\n    a reader should simply make a shallow clone of the cache.  (Longer\n    term, with CSF as the source, we think updating fields should be\n    possible, so we'd need a copy-on-write solution, like we now do w/\n    deleted docs).\n\n\n\n\n\tLooks like some accidental regressions snuck in, eg in\n    DirIndexReader:\n\n-    final String[] files = dir.listAll();\n+    final String[] files = dir.list();\n\n\n    and in IndexReader:\n\nprotected IndexReader(Directory directory) {\n     this();\n-    this.directory = directory;\n}\n\n\n\n\n\n\n\tDo we even need ComparatorFactory*?  Seems like this patch\n    shouldn't be be in the business of creating comparators.\n\n\n\n\n\tYou should hit UOE if you try to getXXX() on a MultiReader\n\n\n\n\n\tShouldn't FieldCache be deprecated entirely?  I would think, going\n    forward, I interact only w/ the IndexReader's default ValueSource?\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12698089"
        },
        {
            "date": "2009-04-11T14:01:02+0000",
            "content": "Thanks Mike! Everything makes sense on first read, so I'll work in that direction.\n\nIn regards to FieldCache - yes it def will be deprecated (pretty rough patch to start so I might have missed some of that - I know plenty is undone).\n\nAs far as back compat with it, I was trying to make it so that if you happened to have code that still used it, and you used the new cache, you woudn't double your mem needs (the original point of backing FieldCache with the new API). Thats pretty restrictive though - indeed, things look much nicer if we don't attempt that (and in the case of a MultiReader cache array, we wouldn't be able to avoid it anyway, so I guess it does make sense we don't worry about it so much).\n\nDo we even need ComparatorFactory*?\n\nProbably not then - I didn't really touch any of the custom type stuff yet. I mainly just got all of the sort tests except remote and custom to pass (though tests elsewhere are still failing).\n\nI'll make another push with your suggestions. ",
            "author": "Mark Miller",
            "id": "comment-12698096"
        },
        {
            "date": "2009-04-11T15:31:25+0000",
            "content": "\nHow about we create a ValueSource abstract base class, that defines\nabstract byte[] getBytes(IndexReader r, String field),\nint[] getInts(IndexReader r, String field), etc. (Just like\nExtendedFieldCache).\n\nThis is subclassed to things like UninversionValueSource (what\nFieldCache does today), CSFValueSource (in the future) both of which\ntake an IndexReader when created.\n\nUninversionValueSource should provide basic ways to customize the\nuninversion. Hopefully, we can share mode code than the current\nFieldCacheImpl does (eg, a single \"enum terms & terms docs\" loop that\nswitches out to a \"handler\" to deal with each term & doc, w/\nsubclasses that handle to byte, int, etc.).\n\nAnd then I can also make MyFunkyValueSource (for extensibility) that\ndoes whatever to produce the values.\n\nThen we make CachingValueSource, that wraps any other ValueSource.\n\nAnd finally expose a way in IndexReader to set its ValueSource when\nyou open it? It would default to\nCachedValueSource(UninversionValueSource()). I think we should\nrequire that you set this on opening the reader, and you can't later\nchange it.\n\nI like this idea, but i am a little bit concerned about only one ValueSource for the Reader. This makes plugging in different sources for different field types hard.\n\nE.g.: One have a CSF and a TrieField and several normal int/float fields. For each of these fields he needs another ValueSource. The CSF field can be loaded from Payloads, the TrieField by decoding the prefix encoded values and the others like it is now.\n\nSo the IndexReaders ValueSource should be a Map of FieldValueSources, so the user could register FieldValueSources for different field types.\n\nThe idea of setting the ValueSource for the IndexReader is nice, we then could simply remove the extra SortField constructors, I added in LUCENE-1478, as it would be possible to specify the type for Sorting when creating the IndexReader. Search code then would simply say, sort by fields a, b, c without knowing what type of field it is. The sort code would get the type and the arrays from the underlying IndexReaders.\n\nThe same with the current function query value sources (just a question: are the functions query value sources then obsolete and can be merged with the \"new\" ValueSource)? ",
            "author": "Uwe Schindler",
            "id": "comment-12698101"
        },
        {
            "date": "2009-04-11T16:15:04+0000",
            "content": "E.g.: One have a CSF and a TrieField and several normal int/float fields. For each of these fields he needs another ValueSource.\n\nCouldn't we make a \"PerFieldValueSource\" impl to handle this?  (And leave the switching logic out of IndexReader).\n\nI think another useful ValueSource would be one that first consults CSF and uses that, if present, else falls back to the uninversion source.\n\nThe sort code would get the type and the arrays from the underlying IndexReaders.\n\nI'm not sure this'll work \u2013 IndexReader still won't know what type to ask for, for a given field?\n\nare the functions query value sources then obsolete and can be merged with the \"new\" ValueSource\n\nWell, the API is a little different (this API returns int[], but the function query's ValueSource has int intVal(int docID)), so I think we'd wrap the new API to match function query's  (ie, cut over function query's use of FieldCache to this new FieldCache). ",
            "author": "Michael McCandless",
            "id": "comment-12698104"
        },
        {
            "date": "2009-04-11T21:11:51+0000",
            "content": "Any ideas on where parser fits in with valuesource? Its easy enough to kind of keep it how it is, but then what if CSF can be stored as a byte rep of an int or something? parse(String) won't make any sense. If we move Parser up to an Impl of valuesource, we have to special case things -\n\nAny thoughts? Just stick with allowing the passing of a 'String to type' Parser and worry about possible byte handling later? A different parser object of some kind?\n\nedit\n\nI guess its not so bad if parser is still first class and something that read bytes would just ignore the given parser? The parsing would just be built into something specialized like that, and it would be free to ignore a given String parser? ",
            "author": "Mark Miller",
            "id": "comment-12698135"
        },
        {
            "date": "2009-04-12T01:59:22+0000",
            "content": "or parsing is just done by the FieldValue implementation, with overrides or something? To change parsers you override UnivertedValuedSource returning your parsers in the callbacks (or something similiar?) ",
            "author": "Mark Miller",
            "id": "comment-12698151"
        },
        {
            "date": "2009-04-12T09:11:53+0000",
            "content": "Any ideas on where parser fits in with valuesource?\n\nI think the UninversionValueSource would accept a custom parser (String -> native type), like what's done today.\n\nMaybe it should also allow stopping the loop early (which Trie* uses), or perhaps outright overriding of the inversion loop itself (which if we make that class subclass-able should be simple). ",
            "author": "Michael McCandless",
            "id": "comment-12698195"
        },
        {
            "date": "2009-04-12T09:21:49+0000",
            "content": "> I like this idea, but i am a little bit concerned about only one ValueSource for the Reader. \n\nThinking more about this...\n\nOver in KS/Lucy, the approach Marvin is taking is something called a\nFieldSpec, to define the \"extended type\" for a field.  The idea is to\nstrongly decouple a field's type from its value, allowing that type to\nbe shared across different fields & instances of the same field.\n\nSo in KS/Lucy, presumably IndexReader would simply consult the\nFieldSpec for a given field, to determine which ValueSource impl is\nresponsible for producing values for this field.\n\nRight now details for a field are scattered about (PerFieldValueSource\nand PerFieldAnalyzerWrapper and Field.Index/Store/TermVector.*,\nFieldInfo, etc.). This then requires alot of app-level code to\nproperly use Trie* fields \u2013 you have to use Trie* to analyze the\nfield, use Trie* to construct the query, use PerFieldValueSource to\npopulate the FieldCache, etc.\n\nMaybe, as part of the cleanup of our three *Field classes, and index\nvs search time documents, we should make steps towards having a\nconsolidated class that represents the \"extended type\" of a field.\nThen in theory one could make a Field, attach a NumericFieldType() to\nit (after renaming Trie* -> Numeric*), and then everything would\ndefault properly. ",
            "author": "Michael McCandless",
            "id": "comment-12698196"
        },
        {
            "date": "2009-04-12T11:41:27+0000",
            "content": "\nI think the UninversionValueSource would accept a custom parser (String -> native type), like what's done today.\n\nMaybe it should also allow stopping the loop early (which Trie* uses), or perhaps outright overriding of the inversion loop itself (which if we make that class subclass-able should be simple).\n\nIts the accepting that seems tricky though. If the getInts() calls take the parser, you have to use instanceof code to work with ValueSource.  Thats why I was thinking maybe callbacks - if a new type is added you just add a new one returning a default parser. Then you can just extend and replace the parsers you want to. I wasn't a big fan of that idea, but I am not sure of a nice, clean, extensible way to specify a bunch of parsers to UninversionValueSource that allows the API to cleanly be used from ValueSource. It already kind of seemed annoying that you would have to set a new ValueSource on the reader just to specify different parsers. I guess at least that has to be accepted though. ",
            "author": "Mark Miller",
            "id": "comment-12698214"
        },
        {
            "date": "2009-04-12T11:47:10+0000",
            "content": "I'm using a similar approach.\n\nThere's a FieldType, that governs conversions from Java type into Lucene strings and declares 'abilities' of that type. Like - conversion is order-preserving (all numerics + some others), converted values can be meaningfully prefix-searched (like TreeId, that is essentially an int[], used to represent things like nested category trees). Some types can also declare themselves as derivatives of others, like DateType being derived from LongType.\n\nThen there's a FieldInfo, that defines field name, FieldType used for it, and actions we're going to take on the field. E.g. if we want to sort on it, build clusters with certain characteristics, load values for this field for each found document, use fast rangefilters, store/filter on field being null/notnull, apply transforms on the field before storing/searching, copy value of the field to another field (with probable transformation) when indexing, etc. From FieldType and desired actions, FieldInfo is able to deduce tokenize/index/store/cache behaviour, and can say that additional lucene fields are required (e.g. for handling null/notnull searches, or trie ranges, or a special sort-form).\n\nThen there's an interface that contains FieldInfo constants and a special constant FieldEnum FIELDS = fieldsOf(ResumeFields.class); that is essentially a navigable list of all FieldInfos defined in this interface and interfaces it extends (allows me to have CommonFields + ResumeFields extends CommonFields, VacancyFields extends CommonFields).\n\nFieldType, and consequently FieldInfo is type-parameterized with the java type associated with the field, so you get the benefit of type-safety when storing/loading/searching the field. All Filters/Queries/Sorters/Loaders/Document accept FieldInfo instead of String for field name, so for example Filters.Range(field, fromValue, fromInclusive, toValue, toInclusive) knows whether to use a simple range filter or a trie one, ensures from/toValues are of a proper type and converts them properly. Filters.IsSet(field) can consult an additional field created during indexation, or access a FieldCache. DocLoader will either get a value for the field from index or from the cache. etc, etc, etc.\n\nWhile I like resulting schema-style very much, I don't want to see the likes of it within Lucene core. Better to have some contrib/extension/whatever that builds on core-defined primitives. That way if one needs to build his own somewhat divergent schema, they can easily do it, instead of trying to fit theirs over Lucene's. For the very same reason I'd like to see fieldcaches moved away from the core, and depending on the same in-core IndexReader segment creation/deletion/whatever hooks that users will use to build their extensions.  ",
            "author": "Earwin Burrfoot",
            "id": "comment-12698215"
        },
        {
            "date": "2009-04-12T13:18:18+0000",
            "content": "\"FieldType\" is probably a better name than \"FieldSpec\", as it implies\nsubclasses with \"Type\" as a suffix: FullTextType, StringType, BlobType,\nFloat32Type, etc. ",
            "author": "Marvin Humphrey",
            "id": "comment-12698219"
        },
        {
            "date": "2009-04-12T14:46:38+0000",
            "content": "\"FieldType\" is probably a better name than \"FieldSpec\"\n\n+1 ",
            "author": "Michael McCandless",
            "id": "comment-12698224"
        },
        {
            "date": "2009-04-12T14:59:14+0000",
            "content": "\nHow about something like this (NOTE: not compiled/tested):\n\n\nabstract class Uninverter {\n  abstract void newTerm(String text);\n  abstract void handleDoc(int docID);\n  void go(IndexReader r) {\n    ... TermEnum/TermDocs uninvert code...\n  }\n}\n\n\n\nand then:\n\n\nclass IntUninverter extends Uninverter {\n  final int[] values;\n  IntUninverter(IndexReader r) {\n    values = new int[r.maxDoc()];\n  }\n\n  int currentVal;\n  void newTerm(String text) {\n    currentVal = Intger.parseInt(text);\n  }\n\n  void handleDoc(int docID) {\n    values[docID] = currentVal;\n  }\n}\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12698225"
        },
        {
            "date": "2009-04-12T15:04:50+0000",
            "content": "Looks good, one addition: newTerm() could return false to stop iterating (for trie).\n\nBut I do not know how performat this is with the class-global variable currentVal... ",
            "author": "Uwe Schindler",
            "id": "comment-12698226"
        },
        {
            "date": "2009-04-12T15:39:15+0000",
            "content": "This should give us some more concrete to start from. Still early and rough (I started to put nocommits, but too early to worry much yet). I have pushed things in general to the proposed API, but its still fairly rough. Core sort tests pass (no custom comparator, remote), but custom parsers are currently ignored. A bunch of the unfun back compat / little stuff is undone.\n\nHow about something like this (NOTE: not compiled/tested):\n\nThats kind of what I did (though not named as nicely, I'll update), but does that solve easily telling the IndexReader (valuesource) what parsers to use by a user? I suppose what your proposing is no custom parser? Instead a custom inverter - but still the problem of a user simply specifying inverters per field when initing the indexreader? Of course you can do a whole new UninversionVS imp, but it just seemed a little heavy handed compared to giving a parser the SortField... I'll update the code a bit and think some... ",
            "author": "Mark Miller",
            "id": "comment-12698229"
        },
        {
            "date": "2009-04-12T16:03:31+0000",
            "content": "I like this idea, but i am a little bit concerned about only one ValueSource for the Reader. This makes plugging in different sources for different field types hard.\n\nI guess you would just have to set a TrieEnabledValueSource when creating your IndexReaders? I suppose it could extend UninversionValueSource and for given fields do Trie unencoding, uninverting, and all other fields, yeld to the super impl? ",
            "author": "Mark Miller",
            "id": "comment-12698230"
        },
        {
            "date": "2009-04-12T16:12:18+0000",
            "content": "Note: I think we should add the option to sort nulls first or last with this. ",
            "author": "Mark Miller",
            "id": "comment-12698232"
        },
        {
            "date": "2009-04-12T16:14:17+0000",
            "content": "I guess you would just have to set a TrieEnabledValueSource when creating your IndexReaders? I suppose it could extend UninversionValueSource and for given fields do Trie unencoding, uninverting, and all other fields, yeld to the super impl?\nAnd then if you get some other XXX encoding, you'll end up with XXXVS extends UVS, TrieVS extends UVS, XXXAndTrieVS extends XXXVS or TrieVS + duplicate code from the other one. Ugly. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12698233"
        },
        {
            "date": "2009-04-12T16:26:46+0000",
            "content": "That is why I'd love to somehow move to a FieldType class that holds such per-field details.\n\nAs things stand now (or, soon) you have to do many separate things to use TrieXXX:\n\n\n\tCreate the right tokenStream for it, and stick that into your Field\n\n\n\n\n\tMake the right query type at search time\n\n\n\n\n\tMake the right sort-field parser\n\n\n\n\n\tMake the right ValueSource\n\n\n\nIt's crazy.  I should be able to make a TrieFieldType (SingleNumberFieldType, or something, after the rename), make that the type of my field when I add it to my document, and then have all these places that do range search, sorting, value retrieval consult the FieldInfo and see that this is a trie field, and act accordingly. ",
            "author": "Michael McCandless",
            "id": "comment-12698234"
        },
        {
            "date": "2009-04-12T16:29:08+0000",
            "content": "Note: I think we should add the option to sort nulls first or last with this.\n\nYou mean for getStringIndex()?  I agree!\n\nActually, it'd be nice to disallow nulls entirely, somehow, since this forces us to sprinkle null checks all over the place in StringOrdVarlComparator.  Maybe we would allow you to pass in a \"null equivalent\", eg you could use \"\", \"UNDEFINED\", whatever, as long as it's a valid string. ",
            "author": "Michael McCandless",
            "id": "comment-12698235"
        },
        {
            "date": "2009-04-12T16:41:31+0000",
            "content": "Another thing that'd be great to fix about FieldCache is its\nintermittent checking of the \"some docs had more than one token in the\nfield\" error.  The current check only catches it in limited cases,\nwhich is deadly because you can test like crazy and think you're OK\nonly in production months later to index slightly different content\nand hit the exception. RuntimeException\n\nBut I can't think of a cheap way to do it reliably.\n\nAt least, we should upgrade the exception from RuntimeException to a\nchecked exception.  Or, we could turn the check off entirely (which I\nthink is better than intermittently catching it).\n\nWe should also somehow allow turning off the check on a case by case\nbasis, since there are really times when it's OK.  (Though maybe you\njust make your own ValueSource, or maybe subclass Uninverter,\nor... something?). ",
            "author": "Michael McCandless",
            "id": "comment-12698238"
        },
        {
            "date": "2009-04-12T16:42:12+0000",
            "content": "Ugly.\n\nWell no worries yet  Still in early design mode, so if it can be made better, I'm sure it will. Of course I'd love to get to 'everything works right for the right field automagically' as well - not sure that will fit into the scope of this issue though (though nothing saying this issue can't be further delayed). We will do the best we can regardless though.\n\nI'm kind of worried that any change is going to hurt Apps like Solr - if you end up using the new built in API, but also have code that must stick for a while with the old API (for multireader fieldcache or something), you'll likely increase RAM usage more than before - how much of a concern that ends up being, I'm not sure. I suppose eventually it has to become up to the upgrader to consider and deal with it if we want to move to segment level caching.\n\nedit\n\nLittle behind on that worry I guess - we already pumped that problem out the door with 1483. Half in the clouds over here. ",
            "author": "Mark Miller",
            "id": "comment-12698239"
        },
        {
            "date": "2009-04-12T16:58:52+0000",
            "content": "Or, we could turn the check off entirely (which I think is better than intermittently catching it).\n\n+1 - agreed that the check is nasty - better to never trip it if your only going to trip it depending... ",
            "author": "Mark Miller",
            "id": "comment-12698241"
        },
        {
            "date": "2009-04-12T17:00:33+0000",
            "content": "> Another thing that'd be great to fix about FieldCache is its\n> intermittent checking of the \"some docs had more than one token in the\n> field\" error.\n\nAdd a FieldType that only allows one value per document. At index-time, \nverify when the doc is added that indeed, only one value was supplied.\n\nIn Lucy, I expect StringType to fill this role. FullTextType is for multi-token\nfields.\n\nOptionally, add a \"NOT NULL\" check to verify that each doc supplies a\nvalue, or allow the FieldType object to specify a default value that should\nbe inserted. ",
            "author": "Marvin Humphrey",
            "id": "comment-12698242"
        },
        {
            "date": "2009-04-12T17:09:30+0000",
            "content": "At least, we should upgrade the exception from RuntimeException to a checked exception.\nExceptions are for expected conditions that can be adequately handled by the caller. RuntimeExceptions are for possible, but unexpected conditions that can theoretically be handled by the caller, but most of the times caller will terminate anyway.\nHaving several values in place where only one should be at all times is obviously an unexpected indexer's fault. So by using checked exception here you'll only provoke some ugly rethrowing/wrapping code, or propagation of said checked exception up the method hierarchy, without gaining any benefit at all.\n\nOr, we could turn the check off entirely.\nYes! ",
            "author": "Earwin Burrfoot",
            "id": "comment-12698243"
        },
        {
            "date": "2009-04-12T17:51:20+0000",
            "content": "\n> Or, we could turn the check off entirely (which I think is better than intermittently catching it).\n\n+1 - agreed that the check is nasty - better to never trip it if your only going to trip it depending...\nOK \u2013 let's turn this check off entirely.\n\nI like Marvin's approach but we don't quite have a FieldType just yet... ",
            "author": "Michael McCandless",
            "id": "comment-12698247"
        },
        {
            "date": "2009-04-12T17:52:50+0000",
            "content": "I'm kind of worried that any change is going to hurt Apps like Solr - if you end up using the new built in API, but also have code that must stick for a while with the old API (for multireader fieldcache or something), you'll likely increase RAM usage more than before - how much of a concern that ends up being, I'm not sure.\nMy personal stance is that until you have one perfectly thought out API, nothing should restrain you from changing it. It's better to feel pain once or twice, when you adapt to API changes, then to feel it constantly, each time you're using that half-assed thing you're keeping around for back-compat. Look at google-collections. They did some really breaking changes since they released, but most of them eased my life after I made my project compile and run with the new version of their library.\n\nIn Lucy, I expect StringType to fill this role. FullTextType is for multi-token fields.\nIn our case multiplicity is defined on FieldInfo level. Because we can have one Int field that holds some value, and another Int field that holds several ids.\nSame goes for the String field - you might want tags on a document that are represented as untokenized strings, but each document can have 0..n of them. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12698248"
        },
        {
            "date": "2009-04-12T17:54:40+0000",
            "content": "Rolling forward with Uninverter ... ",
            "author": "Mark Miller",
            "id": "comment-12698249"
        },
        {
            "date": "2009-04-12T18:58:20+0000",
            "content": "I began to make the switch of allowing an Uninverter to be attached to a SortField (like Parser) with the idea of doing backcompat by wrapping the Parser with an Uninverter. I caught myself though, because the reason I wasn't doing that before is that Uninverter may not make sense for a given ValueSource. By forcing you to set the Uninverters per type on the UninversionValueSource instead (current posted patch), this issue is kind of avoided - the possible problem is that it seems somewhat less dynamic in that you set it once on the ValueSource and leave it instead of being able to pass any impl any time on a SortField. Perhaps not so bad. But then how do I set the Uninverter for back compat with an old Parser? It doesn't seem wise to allow arbitrary Uninverter updates to the UninversionValueSource does it? Thread safety issues, and ... but then how to handle back compat with the parser? ...  ",
            "author": "Mark Miller",
            "id": "comment-12698254"
        },
        {
            "date": "2009-04-12T19:17:50+0000",
            "content": "I guess we do need to have Uninverters settable per run somehow ... Then if a Parser comes in on SortField, we can downcast to UninversionValueSource and pass the Uninverter wrapping the Parser. I don't think we should pass the Uninverter on SortField though, going forward. Uninverter may not apply to ValueSource, and internally, things will work with ValueSource.\n\nSo a user cannot pass an Uninverter for internal sorting per sort, it has to init the UninversionValueSource with the right Uninverters, but for backcompat, FieldComparator would be able to pass an Uninverter that takes precedence?\n\nThat sounds somewhat alright to me, I'll roll that way a bit for now. ",
            "author": "Mark Miller",
            "id": "comment-12698255"
        },
        {
            "date": "2009-04-12T19:22:15+0000",
            "content": "How about SortField taking ValueSource? ",
            "author": "Michael McCandless",
            "id": "comment-12698257"
        },
        {
            "date": "2009-04-12T19:25:09+0000",
            "content": "This issue is fast moving!  Here're some thoughts on the last patch:\n\n\n\tNeed to pass in ValueSource impl to IndexReader.open, defaulting\n    to Cached(Uninverted)\n\n\n\n\n\tMaybe ValueSource should provide default \"throws UOE\" exceptions\n    methods (instead of abstract) since it's rather cumbersome for a\n    subclass that only intends to provide eg getInts().\n\n\n\n\n\tShould CachingValueSource really include IndexReader in its key?\n    I think it shouldn't?  The cache is already \"scoped\" to a reader\n    because each SegmentReader will have its own instance.  Also, one\n    might cache ValueSources that don't \"have\" a reader (eg pull from\n    a DB or custom file format or something).  Cloning an SR for now\n    should just copy over the same cache.\n\n\n\n\n\tI think we should back deprecated FieldCache with ValueSource for\n    the reader; one simple way to not ignore the Parser passed in is\n    to anonymously subclass Uninverter and invoke the passed in\n    ByteParser from its \"newTerm\" method?\n\n\n\n\n\tWhy do we need to define StringInverter, IntInverter, etc in\n    UninversionValueSource?  Couldn't someone instead subclass\n    UninversionValueSource, and override whichever getXXX's they want\n    using their own anonymous uninverter subclass?\n\n\n\n\n\tShould we deprecate function.FieldCacheSource, and make a new\n    function.ReaderValueSource (ie pulls its values from the\n    IndexReader's ValueSource)?\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12698258"
        },
        {
            "date": "2009-04-12T19:46:46+0000",
            "content": "How about SortField taking ValueSource? \n\nRight - theres the right thought I think. I'll play with that.\n\nNeed to pass in ValueSource impl to IndexReader.open, defaulting to Cached(Uninverted)\n\nYes - only havn't because that code is kind of unfirm - it already seems like it will prob be moved out to SortField  So I was just short-cut initing it.\n\nShould CachingValueSource really include IndexReader in its key?\n\nProbably not then  I'll be going over that tighter - I was in speed mode and havn't considered the CachingValueSource much yet - I kind of just banged out a quick impl (gotto love eclipses generate hashcode/equals), put it in and tested it.  To handle all of this 'do it for Long, repeat for Int, repeat for Byte, etc' I go somewhat into robot mode. Also, this early, I know you'll have me changing directions fast enough that I'm still in pretty rough hammer out mode.\n\nWhy do we need to define StringInverter, IntInverter, etc in UninversionValueSource? \n\nYes true. I was using anon classes in the prev patch, but upon switch to Uninverter, I just did mostly what came to mind quickest looking at your example code and what I had.\nIndeed, overriding getXXX is simple and effective. As I think about it - now I  am thinking I did it for the getArray call to return the right type (easy with anon class, but custom passed Uninverter ?). Could return object and cast though ...\n\nDo we need Uninverter if overriding getXXX is easy enough and we pass the ValueSource on SortField?\n\nShould we deprecate function.FieldCacheSource,\n\nYeah for sure. I'll take a closer look at the function package soon. Thus far, I've just got it compiling and the main sort tests passing. As soon as I feel the API is firming (which, sweet, it already is a bit I think), I'll start polishing and filling in the missing pieces, more thorough nocommits, comments. ",
            "author": "Mark Miller",
            "id": "comment-12698264"
        },
        {
            "date": "2009-04-12T20:43:04+0000",
            "content": "Should CachingValueSource really include IndexReader in its key?\n\nI see now - I wasnt always holding the ValueSource in the Reader - I was using getXXX(reader, field). Now I am back to that - and since the CachingValueSource holds the map, I am back to needing that. Being able to change the ValueSource on the fly now with SortField has implications for CachingValueSource. There has to be some kind of default ValueSource, which would likely do caching. Once you started using ValueSources from SortField, they won't share the cache. Get enough of them going and the RAM reqs jump. ",
            "author": "Mark Miller",
            "id": "comment-12698272"
        },
        {
            "date": "2009-04-12T21:45:36+0000",
            "content": "I really like CachingValueSource, but somehow the cache has to move to the segmentreader or something... (now that there is not a segmentreader -> valuesource instance mapping) ",
            "author": "Mark Miller",
            "id": "comment-12698286"
        },
        {
            "date": "2009-04-12T23:16:46+0000",
            "content": "Moves ValueSource to SortField, giving us a caching problem. I've got a DEFAULT_VALUESOURCE in IndexReader that needs movement to some refinement.\n\nNo custom field comparator support yet, but parser back compat test passes.\n\nI really am liking how its feeling now - have to think about the caching though. ",
            "author": "Mark Miller",
            "id": "comment-12698295"
        },
        {
            "date": "2009-04-13T00:43:16+0000",
            "content": "Might as well throw another one up with custom comparator test passing. Still a bunch to do beyond tests, but I think any left that are failing will be easily fixed with a resolution to how we are going to cache.\n\nThen javadoc, consider all the back compat issues a bit more, tweak API, consider what new tests make sense ... ",
            "author": "Mark Miller",
            "id": "comment-12698297"
        },
        {
            "date": "2009-04-13T00:51:49+0000",
            "content": "I did some tests with the new API and TrieRange. Without patch it did not work because the missing StopFillCacheException (but this was intended, as it was only a hack).\n\nI created a TrieValueSource, that is similar like the UninversionValueSource, but iterates only over shift=0 prefix encoded values.\n\nThe original patch only missed the following ctor of SortField:\n\n  public SortField (String field, int type, ValueSource valueSource, boolean reverse) {\n    initFieldType(field, type);\n    this.reverse = reverse;\n    this.valueSource = valueSource;\n  }\n\n\n\nAfter adding this, it worked (this ctor creates a \"normal\" sortfield but without custom comparator, but with a specific value-source). This ctor would replace the deprecated parser ctor. ",
            "author": "Uwe Schindler",
            "id": "comment-12698298"
        },
        {
            "date": "2009-04-13T00:58:55+0000",
            "content": "By the way: If this new API goes into 2.9, the SortField ctors with parser and the whole parser support in SortField/Search-code can be removed, if Parser itsself is deprecated (as support for LUCENE-1478 was not released until now). New code could always use ValueSource. ",
            "author": "Uwe Schindler",
            "id": "comment-12698300"
        },
        {
            "date": "2009-04-13T02:29:15+0000",
            "content": "I guess the caching is not the problem I thought. It's got to be per  \nvalue source anyway. I guess I just have to stick the default  \nvaluesource in a better place. Seems cachingvaluesource is still legit.\n\nStill, I introduced reader back into the caching , and you wanted to  \navoid that..\n\n\n\tMark\n\n\n\nhttp://www.lucidimagination.com\n\nOn Apr 12, 2009, at 8:45 PM, \"Mark Miller (JIRA)\" <jira@apache.org>  \n ",
            "author": "Mark Miller",
            "id": "comment-12698309"
        },
        {
            "date": "2009-04-13T14:10:36+0000",
            "content": "Also, this early, I know you'll have me changing directions fast enough that I'm still in pretty rough hammer out mode.\n\nYeah I hear you  Things're fast moving... I'm glad you're good with\nThe Eclipse.\n\nDo we need Uninverter if overriding getXXX is easy enough and we pass the ValueSource on SortField?\n\nGood question... though it is nice/important to not have to implement\nyour own TermEnum/TermDocs iteration logic.\n\nBeing able to change the ValueSource on the fly now with SortField has implications for CachingValueSource.\n\nI think an app would need to pay attention here, ie, if caching is\nneeded the app should always pass the same CachingValueSource to all\nsort fields.  It's up to the app to scope their ValueSources\ncorrectly.  You're right that if you have a bug and don't always use a\nconsistent CachingValueSource, you can use too much RAM since a given\nfield's int[] can be double cached; but I think that's a bug in the\napp?  It's analogous to using 2 IndexReaders instead of sharing?\n\nThough... I'm not sure.  Something's not yet right about our approach\nbut I can't think of how to fix it... will mull it over.\n\nI wonder if we can somehow fit norms handling into this?  Norms ought\nto be some sort of XXX.getBytes() somewhere, but I'm not sure how\nyet.  It's tricky because norms accept changes and thus must implement\ncopy-on-write.  So maybe we levae them out for now... ",
            "author": "Michael McCandless",
            "id": "comment-12698394"
        },
        {
            "date": "2009-04-13T14:18:08+0000",
            "content": "I'd like to get the cache back into the segment readers somehow ... or somehow get away from the large indexreader cache map.\n\nIndexReader plugin could come in useful there \n\nI've got the tests passing in general, but still have to figure out better/right caching (the way I allow you to pass in an Uninverter skips caching completely! plus the above), need to deal with ValueSource as a field on SortField - require it being serializable...ugg.\n\nI'll be away from this for while now prob though, so plenty of time to mull it over. ",
            "author": "Mark Miller",
            "id": "comment-12698397"
        },
        {
            "date": "2009-04-13T15:14:41+0000",
            "content": "Might as well post as far as I got in case anyone else ends up wanting to take a few swings.\n\nI think all tests pass except for one remote in testsort (probably related to caching?), and clone checks in an IndexReader test that try and ensure the fieldcache contents are the same instance (they are not because of the caching - havnt gotten that far).\n\nSo the major issue remaining I think (other than natural cleanup and back test thinking) is getting the caching right. There will probably be other changes too, but I think getting the caching right might drive them. ",
            "author": "Mark Miller",
            "id": "comment-12698409"
        },
        {
            "date": "2009-04-13T15:53:48+0000",
            "content": "I've got the tests passing in general, but still have to figure out better/right caching (the way I allow you to pass in an Uninverter skips caching completely! plus the above), need to deal with ValueSource as a field on SortField - require it being serializable...ugg.\n\nMh, FieldCache.Parser is also not serializable and the others like the Comparators are not serializable, too. Why not simply pass the ValueSource to SortField like the Parser or Locale? It worked until now without serialization and so I think we should remove serialization from SortField. The factory is silly. If we have a factory there, we should also have a factory for parsers...\n\nBy the way, can you add the ctor mentioned above to your patch, I need it to sucessfully test the new TrieValueSource I wrote yesterday (see patch). This is a good test case for the extensibility of your new API.\nBy the way: For TrieRange, I made the ValueSource a static variable. In general the ValueSources should be singletons (maybe this is why you created the factory). ",
            "author": "Uwe Schindler",
            "id": "comment-12698418"
        },
        {
            "date": "2009-04-13T16:00:40+0000",
            "content": "It worked until now without serialization and so I think we should remove serialization from SortField. \n\nI don't think we can right now because of remote searchable - I think?. I agree the factories are silly, but now I know why they exist! It had eluded me before.\n\nI can roll your patch in Uwe - sorry I missed it with that last one - I had meant to, but it slipped my mind.\n\nI've been waiting to pin down how ValueSource handles its cache (either lightning will strike my mind, or more likely, Mike will tell me what to do) - but the main reason for the factory at the moment is to get the remote tests to pass - since SortField is serializable, it allows us to pass the ValueSource without it being seriazable. ",
            "author": "Mark Miller",
            "id": "comment-12698419"
        },
        {
            "date": "2009-04-13T16:08:29+0000",
            "content": "I am just wondering why the parsers and locales work, which all are not serializable. But they are NULL per default. So in principle, if I do a remote search with a custom parser or comparator, it would fail, too. ",
            "author": "Uwe Schindler",
            "id": "comment-12698421"
        },
        {
            "date": "2009-04-13T16:13:41+0000",
            "content": "Okay, good point. I've got to take a closer look at what we are required to support for remote searching.\n\nI think your right though, we probably won't end up needing the factory. For right now, the way I set the DEFAULT, we need it to get remote tests to pass. But that is all very loose right now - when the caching mechanism is fixed up, I think the whole DEFAULT_VALUE_SOURCE will be cleaned right up - especially having to set it there on the SortField.\n\nI think we have to support remote for CustomFieldComparator, but I guess wouldnt have to for ValueSource. ",
            "author": "Mark Miller",
            "id": "comment-12698423"
        },
        {
            "date": "2009-04-15T01:26:35+0000",
            "content": "I was going to throw in that constructor Uwe, and I got caught up doing a few things. I'm not so sure we can stick with attaching the ValueSource on the SortField. In the end, we'd really like the cache to be held per segment in the reader, rather than a monster reader cache. So I have moved the standard ValueSource back to the reader. I've got a separate massive reader cache (as in, everything goes in one cache keyed by reader) for handling the backcompat issues with custom parsers in FieldCache (it would be awesome to get this out before having to deprecate the SortField parser stuff).\n\nThis doesnt really jive well with passing in ValueSources on the fly with SortField. Which sucks, because that was nice.\n\nWhat do you think about having to pull off Trie from the ValueSource set on your reader at reader init? I'm not thinking its super pretty - I guess you take which fields to override at init, and then do trie stuff for certain fields, but pass through to the default ValueSource impl for other fields?\n\nI hope that can be improved, but i don't see how at the moment... ",
            "author": "Mark Miller",
            "id": "comment-12699016"
        },
        {
            "date": "2009-04-15T02:19:15+0000",
            "content": "Or maybe we can just keep all 3 options? Backcompat parser stuff goes through a reader keyed cache, built in stuff goes through a segment level ValueSource, and custom stuff using SortField can do whatever to override the ValueSource - they would cache if they wanted, etc.\n\nSo you could either override the default ValueSource, and provide your own override on the fly.\n\nI guess that does seem to work out anyway ... ? ",
            "author": "Mark Miller",
            "id": "comment-12699028"
        },
        {
            "date": "2009-04-15T12:14:58+0000",
            "content": "Thinking a bit on this this morning:\n\nI think that will work out right. We would have 3 different attacks at ValueSource.\n\n1. A ValueSource per segment reader that handles all default ValueSource needs - you get it with IndexReader.getValueSource. Its an UninversionValueSource wrapped by a CachingValueSource by default.\n\n2. A singleton back compat value source that is wrapped by CacheByReaderValueSource. It has extra methods that takes Uninverters, allowing custom Uninverters and caching by Uninverter.\n\n3. You can override the ValueSource used for Sorting by attaching it to the SortField. Likely, you would wrap in a CacheByReaderValueSource and have your own singleton.\n\nI think that gives us back compat and the best of both worlds? ",
            "author": "Mark Miller",
            "id": "comment-12699166"
        },
        {
            "date": "2009-04-15T22:46:29+0000",
            "content": "I've added your SortField constructor Uwe.\n\nI've also switched things to what I proposed.\n\nYou can access a ValueSource from SegmentReaders with getValueSource. You get a USO Exception on non SegmentReaders with the hint to use getSequentialSubReaders.\nThe built in FieldCache usage uses this new API instead, allowing caching per segment without global caching keyed by IndexReader. Its hardwired at the moment, but you would be able to set it on IndexReader open.\n\nThere is a back compat CacheByReaderUninverterValueSource that caches in a WeakHashMap by IndexReader and Uninverter (Uninverter is actually keyed by the Parser it wraps). This\nworks as a Singleton. Deprecated code, like the FieldCache uses this. You wouldn't want to straddle both API's, because for identical types / uninversion you would duplicate data (eg if you asked both APIs for the same data type with\nthe same parser/uninverter and field, you would get twice the data in RAM). This method still works with MultiReaders, and so nicely solves back compat with regards to only allowing access to ValueSource from the SegmentReader.\n\nYou can also set a ValueSource on a SortField. This would override the ValueSource used at sort time to one provided. There is a convenience class called CacheByReaderValueSource that caches\nby Reader, field, type, key.\n\nI think that solves all three needs (and my caching concerns) in a pretty nice way. Patch is not done, but all tests now pass except for TrieRange (have not run back compat tests yet due to Trie failure - havnt looked into yet either). ",
            "author": "Mark Miller",
            "id": "comment-12699428"
        },
        {
            "date": "2009-04-15T22:59:35+0000",
            "content": "Bah, just wrote a bunch and hit cancel.\n\nAttacking this from the old incarnation of the patch has me trying to hard to back FieldCache with the new API. Looking from closer eyes now, I don't see that being necessary. We just want the SegmentReader level ValueSource and the option for SortField override. FieldCache can use its current implementation. The motivation to back it is to minimize the RAM reqs of straddling the two API's. If we want the new API to work at the SegmentReader level though, we can never really achieve that. Might as well not half *ss it. I'll return the FieldCache to as is at some point and just deprecate. ",
            "author": "Mark Miller",
            "id": "comment-12699433"
        },
        {
            "date": "2009-04-15T23:00:38+0000",
            "content": "Patch is not done, but all tests now pass except for TrieRange (have not run back compat tests yet due to Trie failure - havnt looked into yet either).\n\nThe TrieRange tests do not pass, because the FieldCache.StopFillCacheException is not handled in the uninverter code (part of LUCENE-1582, FieldCache.StopFillCacheException). When Trie gets switched to an own ValueSource, StopFillCacheException can be removed (you can do this in this patch, it was just a temporary hack). ",
            "author": "Uwe Schindler",
            "id": "comment-12699435"
        },
        {
            "date": "2009-04-15T23:11:57+0000",
            "content": "I do not get the patch applied to trunk (merging works) but it gots lots of compile failures (because of the changes of LUCENE-1575). Mostly because the comparators got new ctors and so on. ",
            "author": "Uwe Schindler",
            "id": "comment-12699439"
        },
        {
            "date": "2009-04-15T23:14:21+0000",
            "content": "Thanks Uwe. I had it in my mind to update to trunk about an hour ago and...\n\nI'll repost in a moment. ",
            "author": "Mark Miller",
            "id": "comment-12699440"
        },
        {
            "date": "2009-04-15T23:31:45+0000",
            "content": "By the way: In the patch, the ValueSource set in SortField seems to be never used when building the comparators. If it is used, when applying the patch attached by me some days ago (LUCENE-831-trieimpl.patch), the trie tests should also work. ",
            "author": "Uwe Schindler",
            "id": "comment-12699448"
        },
        {
            "date": "2009-04-16T00:22:08+0000",
            "content": "Okay, all fixed up and updated to trunk. All tests pass. Still some rough edges for sure. Still probably going to revert FieldCache to its former self (though internally, the API won't be used).\n\nBack compat test run hasnt finished yet. ",
            "author": "Mark Miller",
            "id": "comment-12699456"
        },
        {
            "date": "2009-04-16T06:23:13+0000",
            "content": "Hi, looks good:\n\nI am only not sure, what would be the right caching ValueSource. If you use a caching value source externally from IndexReader, what should I use? The original trie patch used the CachingValueSource (as when the patch was done, there only existed CacingValueSource):\n\n\n+  public static final ValueSource TRIE_VALUE_SOURCE = new CachingValueSource(new TrieValueSource());\n\n\n\nBut correct would be CacheByReaderValueSource as a per-JVM singleton? For the tests is its not a problem, because there is only one index with one segment. If I use CachingValueSurce as a singleton, it would cache all values from all index readers mixed together?\n ",
            "author": "Uwe Schindler",
            "id": "comment-12699545"
        },
        {
            "date": "2009-04-16T11:31:32+0000",
            "content": "Right, you really want to use CacheByReaderValueSource. Better would probably be to get that cache on the segment reader as well. But I think that would mean bringing back some sort of general cache to IndexReader. You would have to be able to attach arbitrary ValueSources to the reader. We will see what ends up materializing. I am agonizingly slow at understanding anything, but quick to move anyway  ",
            "author": "Mark Miller",
            "id": "comment-12699644"
        },
        {
            "date": "2009-04-16T11:41:46+0000",
            "content": "This was the idea behin the \"FieldType\": You register at the top-level IndexReader/MultiReader/whatever the parsers/valuesources (e.g. in a map coded by field), all subreaders would also get this map (passed through) and if one asks for cache values for a specific field, he would get the correctly decoded fields (from CSF, Univerter, TrieUniverter, Stored Fields [not really, but would be possible]). This was the original approach of this issue: attach caching to the single index/segmentreaders (with possibility to \"register\" valuesources for specific fields).\nIn this case the SortField ctors taking ValueSource or Parser can be cancelled (and we can do this for 2.9, as the Parser ctor of SortField was not yet released!). ",
            "author": "Uwe Schindler",
            "id": "comment-12699649"
        },
        {
            "date": "2009-04-16T12:02:18+0000",
            "content": "\nThats somewhat possible now (with the exception that you can't yet set the value source for the segment reader yet - it would likely become an argument to the static open methods): ValueSource gets a field as an argument, so it is also easy enough to set a ValueSource that does trie encoding for arbitrary fields on the SegmentReader, eg FieldTypeValueSource could take arguments to configure it per field and then you set it on the IndexReader when you open it. Thats all still in the patch - its just a bit more of a pain than being able to set it at any time on the SortField as an override.\n\nI guess I almost see things going just to the segment reader valuesource option though - once FieldCache goes back to standard, it might make sense to drop the SortField valuesource support too, and just do the segment ValueSource. Being able to init the SegmentReader with a ValueSource really allows for anything needed - I just wasn't sure if it was too much of a pain in comparison to also having a dynamic SortField override. ",
            "author": "Mark Miller",
            "id": "comment-12699663"
        },
        {
            "date": "2009-04-16T12:42:13+0000",
            "content": "So I'm flopping around on this, but I guess my latest take is that:\n\nI want to drop the SortField ValueSource override option. Everything would need to be handled by overriding the segment reader ValueSource.\n\nDrop the current back compat code for FieldCache - its mostly unnecessary I think. Instead, perhaps go back to orig FieldCache impl, except if the Reader is a segment reader, use the new ValueSource API ? Grrr - except if someone has mucked with the ValueSource or used a custom FieldCache Parser, it won't match correctly...thats it - you just can't straddle the two APIs. So I'll revert FieldCache to its former self and just deprecate. ",
            "author": "Mark Miller",
            "id": "comment-12699678"
        },
        {
            "date": "2009-04-16T21:34:43+0000",
            "content": "Okay, now that I half way understand this issue, I think I have to go back to the basic motivations. The original big win was taken away by 1483, so lets see if we really need a new API for the wins we have left.\n\nAdvantage of new API (kind of as it is in the patch)\nFieldCache is interface and it would be nice to move to abstract class, ExtendedFieldCache is ugly\nAvoid global sync by IndexReader to access cache\nits easier/cleaner to block caching by multireaders (though I am almost thinking I would prefer warnings/advice about performance and encouragement to move to per segment)\nIt becomes easier to share a ValueSource instance across readers.\n\nDisadvantages of new API\nIf we want only SegmentReaders to have a ValueSource, you can't efficiently back the old API with the new, causing RAM reqs jumps if you straddle the two APIs and ask for the same array data from each.\n\nIts probably a higher barrier to a custom Parser to implement and init a Reader with a ValueSource (presumably that works per field) than to simply pass the Parser on a SortField. However, Parser stops making sense if we end up being able to back ValueSource with column stride fields. We could allow ValueSource to be passed on the SortField (the current incarnation of this patch), but then you have to go back to a global cache by reader the ValueSources passed that way (you would also still have the per segment reader, settable ValueSource).\n\nAdvantages of staying with old API\nAvoid forcing large migration for users, with possible RAM req penalties if they don't switch from deprecated code (we are doing something similar with 1483 even without deprecated code though - if you were using an external multireader FieldCache that matched a sort FieldCache key, youd double your RAM reqs).\n\nThoughts\nIf we stayed with the old API, we could still allow a custom FieldCache to be supplied. We could still back FieldCacheImpl with Uninverter to reduce code. We could still have CachingFieldCache. Though CachingValueSource is much better  FieldCache implies caching, and so the name would be confusing. We could also avoid CachingFieldCache though, as just making a pluggable FieldCache would allow alternate caching implementations (with a bit more effort).\n\nWe could deprecate the Parser methods and force supplying a new FieldCache impl for custom uninversion to get to an API suitable to be backed by CSF.\n\nOr:\n\nWe could also move to ValueSource, but allow a ValueSource on multi-readers. That would probably make straddling the API's much more possible (and efficient) in the default case. We could advise that its best to work per segment, but leave the option to the user.\n\nConclusion\nI am not sure. I thought I was convinced we might as well not even move from FieldCache at all, but now that I've written a bit out, I'm thinking it would be worth going to ValueSource. I'm just not positive on what we should support. SortField ValueSource override keyed by reader? ValueSources on MultiReaders? ",
            "author": "Mark Miller",
            "id": "comment-12699880"
        },
        {
            "date": "2009-04-16T21:53:24+0000",
            "content": "We have the problem with the ValueSource-override not only with SortField. Also Functions Queries need the additional ValueSource-override and other places too. So a central place to register a \"ValueSource per field\" for a IndexReader (MultiReader,... passing down to segments) would really be nice.\n\nFor the caching problem: Possibly the ValueSource given to SortField etc. behaves like the current parser. The cache in IndexReader should also be keyed by the ValueSource. So the SortField/FunctionQuery ValueSource override is passed down to IndexReader's cache. If the IndexReader has an entry in its cache for same (field, ValueSource, ...) key, it could use the arrays from there, if not fill cache with an array from the overridden ValueSource. I would really make the ValueSource per-field.\n\nUniverter inner class should be made public and the Univerter should accept a starting term to iterate (overwrite \"\") and the newTerm() method should be able to return false to stop iterating (see my ValueSource example for trie). With that one could easily create a subclass of univerter with a own parser logic (like trie). ",
            "author": "Uwe Schindler",
            "id": "comment-12699893"
        },
        {
            "date": "2009-04-17T03:44:14+0000",
            "content": "I think we don't want to expose Uninverter though? The API should be neutral enough to naturally support loading from CSF, in which case Uninverter doesnt make sense...so we were going to go with having to override the value source to handle uninverter type stuff. ",
            "author": "Mark Miller",
            "id": "comment-12699993"
        },
        {
            "date": "2009-04-17T12:36:57+0000",
            "content": "I think, it would be a good idea to have the Univerter static class not package private, but \"protected\" (if possible) or \"public\". Then it would be simple, to subclass the UniverterValueSource (e.g. for the TriePackage) and then just use own univerter subclasses to fill the cache. The idea is to make this more simple.\nAs you may have seen in my TrieValueSource I had to redeclare the Univerter. ",
            "author": "Uwe Schindler",
            "id": "comment-12700154"
        },
        {
            "date": "2009-04-17T13:36:02+0000",
            "content": "Yes, I agree - will do. ",
            "author": "Mark Miller",
            "id": "comment-12700175"
        },
        {
            "date": "2009-04-17T13:43:46+0000",
            "content": "I've been struggling with the \"right\" way forward here... despite\nfollowing all comments and aggressive ongoing mulling, I still don't\nhave much clarity.\n\nIt feels like one of those features that just hasn't quite \"clicked\"\nyet (to me at least).  In fact, the more I try to think about it, the\nless clarity I get!\n\nI think there're some cncrete reasons to create a new API (some\noverlap w/ Mark's list above):\n\n\n\tMake caching \"external\"/public so you can control when things are\n    evicted\n\n\n\n\n\tCleaner API \u2013 it's just awkward that you now must call a separate\n    place (ExtendedFieldCache.EXT_DEFAULT) to getInts.  FieldCache &\n    ExtendedFieldCache are awkward, and they are interfaces.  It makes\n    more sense to ask the reader directly for ints (or a future\n    component of the reader).\n\n\n\n\n\tBetter extensibility on uninversion (either via \"you make your own\n    ValueSource entirely\", or \"you can subclass Uninverted and tweak\n    it\").  Trie needs this (though, we have a viable approach in field\n    cache).  Fields with more than one value want custom control to\n    pick one.\n\n\n\n\n\tMaking it not-so-easy to get all field values at the reader level\n    (don't set dangerous API traps)\n\n\n\nHonestly these reasons are not net/net compelling enough to warrant a\nwhole new API?  They are fairly minor.  And I agree: LUCENE-1483 has\nalready achieved the biggest step forward here.\n\nFurthermore, there are other innovations happening that may affect how\nwe do this. EG LUCENE-1597 introduces type information for fields (at\nleast at indexing time), and Earwin is working on \"componentizing\"\nSegmentReader.  Normally I don't like letting \"big distant future\nfeature X\" prevent progess on \"today's feature Y\", but since we lack\nclarity on Y...\n\nI can imagine a future when the FieldType would be the central place\nthat records all details for a field:\n\n\n\tThe analyzer to use (so we don't need PerFieldAnalyzerWrapper)\n\n\n\n\n\tThe ValueSource\n\n\n\n\n\tIt's \"native\" type (now \"switched\" in many places, like\n    FieldComparator, SortField, FieldCache, etc.)\n\n\n\n\n\tAll the index-time configuration\n\n\n\nAnd then instead of having ValueSource dispatch per field, we'd simply\nask the FieldType what it's source is.\n\nFinally, there are a number of future improvements we should take into\naccount.  We wouldn't try to accomplish these right now, but we ought\nto think about them (eg, not preclude them) in whatever approach we\nsettle on:\n\n\n\tWe need source pluggability for when CSF arrives (but, admittedly,\n    we could wait until CSF actually does arrive)\n\n\n\n\n\tAllowing values to change, just like we can call\n    IndexReader.setNorm/deleteDoc to change norms/deletes. We'd need a\n    copy-on-write approach, like norms & deleted docs.\n\n\n\n\n\tHow would norms be folded into this?  Ideally, each field could\n    choose to pull its norms from any source.  Document level norms\n    was discussed somewhere, and should easily \"fit\" as another norms\n    source.  We'd need to relax how per-doc-field boosting is computed\n    at runtime to pull from such \"arbitrary\" sources.\n\n\n\n\n\tDeleted docs could also be represented as a ValueSource?  Just one\n    bit per doc.  This way one could swap in whatever source for\n    \"deleted docs\" one wanted.\n\n\n\n\n\tAllowing for docs that have more than one value.  (We'd also need\n    to extend sorting to be able to compare multiple vlaues).\n\n\n\n\n\tAn mmap implementation (like Lucy/KS) \u2013 should feel just like CSF\n    or uninversion (ie, \"just another impl\").\n\n\n\n\n\tImpls of getStrings and getStringIndex that are based on offsets\n    into char[] (not actual individual String object).\n\n\n\n\n\tGood impls for the enum case (all strings could be considered\n    enums), eg if there are only 100 unique strings in that field, you\n    only need 7 bits per ord derefing into the char[] values.\n\n\n\n\n\tPossible future when Lucene computes sort cache (for text fields)\n    and stores in the index\n\n\n\n\n\tAllowing field sort to use an entirely external source of values\n\n\n\nThere's alot to think about  ",
            "author": "Michael McCandless",
            "id": "comment-12700177"
        },
        {
            "date": "2009-04-18T00:24:35+0000",
            "content": "I've got a bit of the same feeling. My list was more or less cherry picked from all of the above comments, and my initial feeling was their was not enough motivation as well. But the more I thought about it, the more kind of ugly field cache is. And we would want to lose exposing Parser so that CFS can be a seamless backing. That makes FieldCache even uglier for a while. Clickless thus far here too, but I think we have a good base to work with still.\n\nHonestly these reasons are not net/net compelling enough to warrant a\nwhole new API? They are fairly minor. And I agree: LUCENE-1483 has\nalready achieved the biggest step forward here.\n\nNot only that, but almost all of those reasons can be handled by allowing a custom FieldCache to be used, rather than just hard coding to the default singleton.\n\nA couple responses:\n\nWe need source pluggability for when CSF arrives (but, admittedly,\nwe could wait until CSF actually does arrive)\nWe have it? Just pass the CSFValueSource at IndexReader creation?\n\n\nAllowing values to change, just like we can call\nIndexReader.setNorm/deleteDoc to change norms/deletes. We'd need a\ncopy-on-write approach, like norms & deleted docs.\nGood point. We need a way to update, that can throw USO Exception?\n\n\nHow would norms be folded into this? Ideally, each field could\nchoose to pull its norms from any source. Document level norms\nwas discussed somewhere, and should easily \"fit\" as another norms\nsource. We'd need to relax how per-doc-field boosting is computed\nat runtime to pull from such \"arbitrary\" sources.\nGood point again. Getting norms under this API will add a bit more meat to this issue.\n\n\nDeleted docs could also be represented as a ValueSource? Just one\nbit per doc. This way one could swap in whatever source for\n\"deleted docs\" one wanted.\nYou've got me here at the moment. I don't know the delete code very well, but I will in time \n\n\n      Allowing for docs that have more than one value. (We'd also need\n      to extend sorting to be able to compare multiple values).\nThis is an interesting one, because I wonder if we can do it and stick with arrays? A multi dimensional array seems a bit much...\n\n\nAn mmap implementation (like Lucy/KS) - should feel just like CSF\nor uninversion (ie, \"just another impl\").\nThis is already fairly independent I think...\n\n\nGood impls for the enum case (all strings could be considered\nenums), eg if there are only 100 unique strings in that field, you\nonly need 7 bits per ord derefing into the char[] values.\n+1. Yes.\n\n\nPossible future when Lucene computes sort cache (for text fields)\nand stores in the index\nI'm not familiar with that idea, so not sure what affect this has...\n\n\nAllowing field sort to use an entirely external source of values\nI think both options allow that now - if you pass the ValueSource from the reader, it can get its values from everywhere. If you override the reader valuesource with the sortfield valuesource, it too can load from anywhere. I am just not sure both options are really needed. I am kind of liking Uwe's idea of assigning ValueSources per field, though that could probably get messy. Perhaps a default, and then per field overrides?  ",
            "author": "Mark Miller",
            "id": "comment-12700391"
        },
        {
            "date": "2009-04-18T07:53:46+0000",
            "content": "\nAllowing values to change, just like we can call\nIndexReader.setNorm/deleteDoc to change norms/deletes. We'd need a\ncopy-on-write approach, like norms & deleted docs.\nOn the other hand, maybe, we shouldn't?\nDeleted docs should definetly be mutable, but that's it.\nAnybody is updating norms on a regular basis on a serious project? But still everyone pays for the feature with running ugly synchronization code for norms. Let's dump it!\nAs for mutable fields, okay, users of Sphinx have them. They use them mostly.. hehehe.. for implementing deletions that Sphinx lacks. I bet there could exist some other usecases, but they can be handled with a custom ValueSource without the need to bring it into API everyone must implement.\n\n\nDeleted docs could also be represented as a ValueSource? Just one\nbit per doc. This way one could swap in whatever source for\n\"deleted docs\" one wanted.\nThat's why I think this is a misfeature. Deleted docs have different meaning from field values. They can be updated, and they should be checked against uberfast.\nSwapping in another impl is cool, while forcing everyone and his dog under the same usage API is not so cool. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12700430"
        },
        {
            "date": "2009-04-18T13:13:52+0000",
            "content": "Deleted docs could also be represented as a ValueSource? Just one\nbit per doc. This way one could swap in whatever source for\n\"deleted docs\" one wanted.\n\nSome of your comments seem to indicate you think we will need to end up with an object rather than raw arrays? ",
            "author": "Mark Miller",
            "id": "comment-12700479"
        },
        {
            "date": "2009-04-18T18:40:12+0000",
            "content": "Some of your comments seem to indicate you think we will need to end up with an object rather than raw arrays?\n\nWell, really I threw out all these future items to stir up the pot and\nsee if some clarity comes out of it  This is what I try to do\nwhenever I'm stuck on how to design something... some sort of defense\nmechanism.\n\nThat said, what requires object instead of array?  EG for binary\nfields (deleted docs) we'd have eg \"BitVector getBits(...)\".\n\nFor multi-valued fields, I'm not sure what's best.  I think Yonik did\nsomething neat with Solr for holding multi-valued fields but I can't\nfind it now.  But, with ValueSource, we have the freedom to use arrays\nfor simple cases and something else for interesting ones?  It's not\neither/or?\n\nAnd we would want to lose exposing Parser so that CFS can be a seamless backing. \n\nI see the CFS/CSF confusion has already struck!\n\nBut yes cleaner API would be a nice step forward...\n\nWe have it? Just pass the CSFValueSource at IndexReader creation?\n\nYes I think we have this one.\n\nThough... I feel like ValueSource should represent a single field's\nvalues, and something else (FieldType?) returns the ValueSource for\nthat field.  Ie, I think we are overloading ValueSource now?\n\nGood point. We need a way to update, that can throw USO Exception?\n\nMaybe... or we can defer for future.  We don't need full answers nor\nimpls for all of these now...\n\n\n> Possible future when Lucene computes sort cache (for text fields)\n> and stores in the index\n\nI'm not familiar with that idea, so not sure what affect this has...\n\nSort cache is just getStringIndex()... all other types just use the\nvalues directly (no need for separate ords).  If it's costly to\ncompute per-reopen we may want to store it in the index.  But\nhonestly, since we load the full thing into RAM, I wonder how\ndifferent the time'd really be loading it vs recomputing it.\n\nGood point again. Getting norms under this API will add a bit more meat to this issue.\n\nYeah I'm not sure whether norms/deleted docs \"fit\"; certainly we'd\nneed updatability first.  It's just that, from a distance, they are\nclearly a \"value per doc\" for every doc in the index.  If we had norms\n& deletions under this API then suddenly, [almost] for free, we'd get\npluggability of deleted docs & norms.\n\nI am kind of liking Uwe's idea of assigning ValueSources per field, though that could probably get messy. Perhaps a default, and then per field overrides?\n\nI'm also more liking \"per field\" to be somehow handled.  Whether\nIndexReader exposes that vs a FieldType (that also holds other\nper-field stuff), I'm not sure.\n\nAnybody is updating norms on a regular basis on a serious project?\n\nThis is a good question \u2013 I'd love to know too.\n\nBut I think updating CSFs would be compelling; having to reindex the\nentire doc because only 1 or 2 metadata fields had changed is a common\nannoyance.  Of course we'd have to figure out (or rule out) updating\nthe postings for such changes... ",
            "author": "Michael McCandless",
            "id": "comment-12700523"
        },
        {
            "date": "2009-04-18T19:38:56+0000",
            "content": "But, with ValueSource, we have the freedom to use arrays\nfor simple cases and something else for interesting ones? It's not\neither/or?\n\nGood point. I was also thinking that some of these issues could force back up to multi-reader support though. But I guess that is not such a worry now that we search per segment is it. A lot of that could probably be deprecated (though I really don't know how easily - I hope to spend a lot more time getting more familiar with IndexReader code).\n\n[almost] for free, we'd get\npluggability of deleted docs & norms.\nI like that idea as well. Plugability is nice.\n\nI'm also more liking \"per field\" to be somehow handled. Whether\nIndexReader exposes that vs a FieldType (that also holds other\nper-field stuff), I'm not sure.\nI want field handling to become easier in Lucene, but I hope we don't lose any of our super on the fly settings. +1 on making field handing easier, but I am much more weary of a fixed schema type thing.\n\nBut I think updating CSFs would be compelling; having to reindex the\nentire doc because only 1 or 2 metadata fields had changed is a common\nannoyance. \n\nI am very interested in having updatable CSF's (much too easy to mistype that). There are many cool things to use it for, especially in combination with near realtime search (tagging variations). ",
            "author": "Mark Miller",
            "id": "comment-12700529"
        },
        {
            "date": "2009-04-19T09:55:30+0000",
            "content": "I was also thinking that some of these issues could force back up to multi-reader support though. \n\nHopefully not...\n\nI want field handling to become easier in Lucene, but I hope we don't lose any of our super on the fly settings. +1 on making field handing easier, but I am much more weary of a fixed schema type thing.\n\nI think \"consolidating per-field details\" (FieldType) is well decoupled from \"forcing every occurrence of a field to be the same\" (fixed schema).  We can (and I think should) do FieldType without forcing a fixed schema.\n\nI am very interested in having updatable CSF's (much too easy to mistype that). There are many cool things to use it for, especially in combination with near realtime search (tagging variations).\n\nFor tags we'd presumably want multi-valued fields handled in ValueSource, plus updatability, plus NRT.\n\nUpdatability is tricky... ValueSource would maybe need a \"startChanges()\" API, which would copy the array (copy-on-write) if it's not already private.  The problem is... direct array access precludes more efficient data structures that amortize the copy-on-write cost (eg by block), which we are wanting to eventually get to for deleted docs & norms (it's likely a large cost in NRT reader turnaround, though I hven't yet measured just how costly). ",
            "author": "Michael McCandless",
            "id": "comment-12700571"
        },
        {
            "date": "2009-04-19T12:30:52+0000",
            "content": ">>I was also thinking that some of these issues could force back up to multi-reader support though. \n\n>Hopefully not...\n\nYes, I don't know enough yet to know for sure. My thought was things like norms and deletes that are available from multireader now will have to either still be, or straddle multi/segment for a while. I guess that doesnt become much of an issue if we go with the same method of just don't load from both single and multi or you will double your reqs? It just gets ugly trying to prevent multireader use with valuesource, but then have to support it due to all the back compat reqs.\n\nWe can (and I think should) do FieldType without forcing a fixed schema.\n\nFair enough, fair enough. I wasn't really taking this completely from this discussion, but from a variety of ideas about fields that have been spilling out on the list. Of course we can still get a lot better (easier) without hitting fixed.\n\nFor tags we'd presumably want multi-valued fields handled in ValueSource, plus updatability, plus NRT.\n\nWell I'm glad its a small order. Yonik did do some multi value faceting work that I never really looked at. I'll go dig it up.\n\nIt may just be best if this sits for a while and we see what happens with a couple other issues floating around it. I said I had sweat to pump into this, not intelligence  If we hit all this stuff (and yes, your not saying we need to or should, but) this ends up touching most things in IndexReader, than possibly writing and merging and what not in IndexWriter (pluggable norms etc still need to be written, merged, loaded, etc), and ... \n ",
            "author": "Mark Miller",
            "id": "comment-12700583"
        },
        {
            "date": "2009-04-19T12:45:20+0000",
            "content": "I am still thinking about the difference between function query's ValueSource and the new ValueSource and I would really like to combine both.\nI know for sorting, the array approach is faster, but maybe the new ValueSource could provide both ways to access. In the array approach, one would only get arrays for single segments, but the method-like access could still map the document ids to the correct segment, to have a uniform access even to multi readers.\nSo, maybe there is a possibility to merge both approaches and only provide one ValueSource supplying both access strategies. ",
            "author": "Uwe Schindler",
            "id": "comment-12700585"
        },
        {
            "date": "2009-04-19T13:07:18+0000",
            "content": "but maybe the new ValueSource could provide both ways to access\n\nYeah, this goes with with what Mike pointed out above - we can return arrays, objects, or anything and your grandmother. My main worry with that idea is the ValueSource API - it could have 10's of accessors, but only 1 or 2 are generally implemented and you have to know the right one to call - it could work of course, but on first thought, its fairly ugly. You could make a fair point that we are already a ways down that path with the design we already have I guess though.\n\nSo, maybe there is a possibility to merge both approaches and only provide one ValueSource supplying both access strategies. \n\nIts a good point. Something makes me think we will still be a bit hindered by back compat with deletes, norms though. ",
            "author": "Mark Miller",
            "id": "comment-12700586"
        },
        {
            "date": "2009-04-23T00:13:52+0000",
            "content": "I'm trying to figure out how to integrate Bobo faceting field\ncaches with this patch, I applied the patch, browsed the\nValueSource API and yeah, it's not what I expected. \"we can\nreturn arrays, objects, or anything and your grandmother\" not\nGrandma! But yeah we need to somehow support probably plain Java\nobjects rather than every primitive derivative? \n\n(In reference to Mark's post 2nd to last post) Bobo efficiently\nnicely calculates facets for multiple values per doc which is\nthe same thing as \"multi value faceting\"? \n\n> by back compat with deletes, norms though.\n\nAre norms and deletes implemented? These would just be byte\narrays in the current approach? If not how would they be\nrepresented? It seems like for deleted docs we'd want the\nBitVector returned from a ValueSource.get type of method?\n\nM.M.: \"Updatability is tricky... ValueSource would maybe need a\n\"startChanges()\" API, which would copy the array (copy-on-write)\nif it's not already private\"\n\nHmm... Does this mean we'd replace the current IndexReader\nmethod of performing updates on norms and deletes with this more\ngeneric update mechanism?\n\nIt would be cool to get CSF going? ",
            "author": "Jason Rutherglen",
            "id": "comment-12701751"
        },
        {
            "date": "2009-04-24T20:19:29+0000",
            "content": "\nGrandma! But yeah we need to somehow support probably plain Java\nobjects rather than every primitive derivative?\n\nYou mean big arrays (one per doc) of plain-java-objects?  Is Bobo doing that today?  Or do you mean a single Java obect that, internally, deals with lookup by docID?\n\n\n(In reference to Mark's post 2nd to last post) Bobo efficiently\nnicely calculates facets for multiple values per doc which is\nthe same thing as \"multi value faceting\"?\n\nNeat.  How do you compactly represent (in RAM) multiple values per doc?\n\n\nAre norms and deletes implemented? These would just be byte\narrays in the current approach? If not how would they be\nrepresented? It seems like for deleted docs we'd want the\nBitVector returned from a ValueSource.get type of method?\n\nThe current patch doesn't do this \u2013 but we should think about how this change could absorb norms/deleted docs, in the future.  We would add a \"bit\" variant of getXXX (eg that returns BitVector, BitSet, something).\n\n\nHmm... Does this mean we'd replace the current IndexReader\nmethod of performing updates on norms and deletes with this more\ngeneric update mechanism?\n\nProbably we'd still leave the \"sugar\" APIs in place, but under the hood their impls would be switched to this.\n\nIt would be cool to get CSF going?\n\nMost definitely!! ",
            "author": "Michael McCandless",
            "id": "comment-12702526"
        },
        {
            "date": "2009-05-12T02:56:32+0000",
            "content": "I won't likely be getting to this anytime soon if someone else wants to work on it. I'll get back at it at some point if not though.\n\nI believe the latest patch is a nice base to work from.\n\nI'm still not clear to me if its best to start merging using the ValueSource somehow, or do something where the ValueSource has a merge implementation (allowing for a more efficient private merge). It seems the merge code for fields, norms, dels, is fairly specialized now, but could become a bit more generic. Then perhaps you could add any old ValueSource (other than norms, fields, dels)  and easily hook into the merge process. Maybe even in RAM merges of RAM based ValueSources - FieldCache etc. Of course, I guess you could also still do things specialized as now, and just provide access to the files through a ValueSource. That really crimps the pluggability though.\n\nThe next step (in terms of the current patch) seems to be to start working ValueSource into norms, dels, possibly stored fields. Eventually they should become pluggable, but I'm not sure how best to plug them in. I was thinking you could set a default ValueSource by field for the FieldCache using the Reader open method with a new param. Perhaps it should take a ValueSourceFactory that can provide a variety of ValueSources based on field, norms, dels, stored fields, with variations for read-only? The proposed componentization of IndexReader could be another approach if it materializes, or worked into this issue.\n\nI don't think I'll understand whats needed for updatability until I'm in deeper. It almost seems like something like setInt(int doc, int n), setByte(int doc, byte b) on the ValueSource might work. They could possibly throw Unsupported. I know there are a lot of little difficulties involved in all of this though, so I'm not very sure of anything at the moment. The backing impl would be free to update in RAM (say synced dels), or do a copy on write, etc. I guess all methods would throw Unsupported by default, but if you override a getXXX you would have the option of  overriding a setXXX. \n\nValueSources also need the ability to be sharable across IndexReaders with the ability to do copy on write if they are shared and updatable.\n ",
            "author": "Mark Miller",
            "id": "comment-12708307"
        },
        {
            "date": "2013-07-23T18:44:54+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13717098"
        },
        {
            "date": "2014-04-16T12:54:52+0000",
            "content": "Move issue to Lucene 4.9. ",
            "author": "Uwe Schindler",
            "id": "comment-13970911"
        }
    ]
}