{
    "id": "LUCENE-3699",
    "title": "kuromoji dictionary could be more compact",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [
            "3.6",
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Reading thru the ipadic documentation, i realized we are storing a lot of redundant information,\nfor example the connection costs for bigram weights are based on POS+inflection data, so its redundant \nto also separately encode POS and inflection data for each entry.\n\nWith the patch the dictionary access is also faster and simpler, and TokenInfoDictionary is 1.5MB smaller.",
    "attachments": {
        "LUCENE-3699.patch": "https://issues.apache.org/jira/secure/attachment/12510721/LUCENE-3699.patch",
        "LUCENE-3699_more.patch": "https://issues.apache.org/jira/secure/attachment/12510786/LUCENE-3699_more.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-01-16T17:02:34+0000",
            "content": "here's a patch: all tests pass. ",
            "author": "Robert Muir",
            "id": "comment-13187027"
        },
        {
            "date": "2012-01-16T18:54:50+0000",
            "content": "Hey, another nice simplification, cool! You will see, next week we will have reduced the dict by another few megabytes  There is still possibilities to compress ConnectionCosts g.\n\nDict size started with 56 MB, right? ",
            "author": "Uwe Schindler",
            "id": "comment-13187078"
        },
        {
            "date": "2012-01-16T18:55:30+0000",
            "content": "I forgot: +1 to commit! ",
            "author": "Uwe Schindler",
            "id": "comment-13187079"
        },
        {
            "date": "2012-01-16T19:23:52+0000",
            "content": "thanks Uwe, we are now slightly under 5MB jar  ",
            "author": "Robert Muir",
            "id": "comment-13187105"
        },
        {
            "date": "2012-01-17T01:26:41+0000",
            "content": "here is a second patch removing ~ 1.3MB of the dictionary (jar file reduced to 4.5MB).\n\nThere are two changes:\n\n\tbaseform (stems) typically share prefixes with surface forms, so write a shared prefix amount and only the suffix.\n\tfor many entries (e.g. ones with no kanji), the reading is the same as the surface form if you map any hiragana to katakana, so just flag with a bit in that case.\n\n\n\nthere is a todo in the patch where we can save another 150KB, but i think it would complicate the reader at this time so I didn't do it. ",
            "author": "Robert Muir",
            "id": "comment-13187363"
        },
        {
            "date": "2012-01-17T01:35:50+0000",
            "content": "+1 ",
            "author": "Uwe Schindler",
            "id": "comment-13187368"
        },
        {
            "date": "2012-01-17T02:16:10+0000",
            "content": "Thanks for taking a look Uwe: i committed this one too.  ",
            "author": "Robert Muir",
            "id": "comment-13187385"
        },
        {
            "date": "2012-01-17T08:06:04+0000",
            "content": "If it's something that is statically compiled (in batch mode) then one could reorder states (nodes) to minimize vlength of arc pointers globally. This is something I did for fst5 automata and it worked very nice (because the distribution of in-node degrees is exponential-like so moving a few nodes with many in-links decreases the global automaton size in a significant way). \n\nI don't think there is any fast algorithm to do this. I used a simple heuristic: calculate in-link degree for each state, sort in descending order, then re-order N top-most nodes so that they're at the front of the serialized automaton. Pick N using any heuristic you like (constant, in-link cutoff, I used a sort of simulated annealing approach and probed around).\n\nThe presentation about the paper in question is here:\nhttp://ciaa-fsmnlp-2011.univ-tours.fr/ciaa/upload/files/Weiss-Daciuk.pdf\n\nI can't publish the PDF of the paper publicly (Springer below), but I can send a PDF copy if somebody is interested. The concept should be clear without the paper anyway \nhttp://www.springerlink.com/content/60r47952k610l822/ ",
            "author": "Dawid Weiss",
            "id": "comment-13187522"
        },
        {
            "date": "2012-01-17T08:35:57+0000",
            "content": "Dawid, currently the FST is not really the biggest culprit:\n\n\n-rw-r--r--   1 rmuir  staff    65568 Jan 16 16:35 CharacterDefinition.dat\n-rw-r--r--   1 rmuir  staff  2624540 Jan 16 16:35 ConnectionCosts.dat\n-rw-r--r--   1 rmuir  staff  4337216 Jan 17 03:22 TokenInfoDictionary$buffer.dat\n-rw-r--r--   1 rmuir  staff  1954846 Jan 16 16:35 TokenInfoDictionary$fst.dat\n-rw-r--r--   1 rmuir  staff    54870 Jan 16 16:35 TokenInfoDictionary$posDict.dat\n-rw-r--r--   1 rmuir  staff   392165 Jan 17 03:22 TokenInfoDictionary$targetMap.dat\n-rw-r--r--   1 rmuir  staff      311 Jan 17 03:22 UnknownDictionary$buffer.dat\n-rw-r--r--   1 rmuir  staff     4111 Jan 16 16:35 UnknownDictionary$posDict.dat\n-rw-r--r--   1 rmuir  staff       69 Jan 16 16:35 UnknownDictionary$targetMap.dat\n\n\n\nas far as the FST, our output is just an increasing ord (according to term sort order), \nso I think it should be pretty good? Is there something more efficient than this?\n\nBasically there are about 330k headwords, and 390k words. so some words have different\nparts of speech/reading etc for the same surface form.\n\nThe $fst.dat is currently FST<int> where int is just an ord into $targetMap.dat, which is\nreally a int[][] (it maps the output ord from the fst into an int[] containing the offsets\nof all word entries for that surface form). \n\nBut the 'meat' describing the entries is in $buffer.dat. for each word this is its cost,\npart of speech, base form (stem), reading, pronunciation, etc, etc. As you see we\nare down to about 11 bytes per lemma on average, but still this 'metadata' is the biggest,\nthats what i was working on shrinking in this issue. ",
            "author": "Robert Muir",
            "id": "comment-13187532"
        },
        {
            "date": "2012-01-17T09:01:34+0000",
            "content": "Oh, ok, sorry \u2013 I had the impression everything is encoded into a single automaton. The strategies of compression vary and there is hardly a single winner. What you did looks pretty compact to me (extracting the lookup data to separate data structures). ",
            "author": "Dawid Weiss",
            "id": "comment-13187538"
        },
        {
            "date": "2012-01-17T16:50:40+0000",
            "content": "Good work, Robert!\n\nUwe, the initial Kuromoji JAR was 11MB.  I believe Robert also has improved runtime memory usage. ",
            "author": "Christian Moen",
            "id": "comment-13187796"
        },
        {
            "date": "2012-01-17T16:57:53+0000",
            "content": "Uwe, the initial Kuromoji JAR was 11MB. I believe Robert also has improved runtime memory usage.\n\nI referred to the uncompressed size! ",
            "author": "Uwe Schindler",
            "id": "comment-13187801"
        },
        {
            "date": "2012-01-17T17:39:16+0000",
            "content": "Uwe improved the memory usage a lot too (e.g. parallel arrays)... thanks for this!\n\nOur uncompressed size is 9MB now, which i think is good for this dataset.\n\nMy motivation for improving the size stuff is not because kuromoji was ever really wasteful,\ninstead I think size comes with the territory for CJK (see your JVM/ICU if you don't believe me).\n\nOn the server, it doesn't really matter: but smaller size can make kuromoji more attractive for \nother use cases like integration into desktop. Also its also to make retrieval of attributes efficient \nfor the analysis chain (e.g. getting part of speech just means reading a short).\n\nFinally, languages aren't static and we can only anticipate dictionaries to grow in size in the future and\nmaybe have even more attributes (e.g. naist-jdic being 25% larger). ",
            "author": "Robert Muir",
            "id": "comment-13187824"
        }
    ]
}