{
    "id": "SOLR-1308",
    "title": "Cache docsets at the SegmentReader level",
    "details": {
        "affect_versions": "1.4",
        "status": "Closed",
        "fix_versions": [
            "3.2"
        ],
        "components": [],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Won't Fix"
    },
    "description": "Solr caches docsets at the top level Multi*Reader level. After a\ncommit, the filter/docset caches are flushed. Reloading the\ncache in near realtime (i.e. commits every 1s - 2min)\nunnecessarily consumes IO resources when reloading the filters,\nespecially for largish indexes.\n\nWe'll cache docsets at the SegmentReader level. The cache key\nwill include the reader.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Yonik Seeley",
            "id": "comment-12735159",
            "date": "2009-07-24T21:07:28+0000",
            "content": "Absolutely!  We need to get 1.4 out of the way first of course.\n\nOne interesting question is the structure of the cache and how to size caches.\n\nOne way: if someone specifies a document cache of 128 docs, and we have a cache per segment, how big should each segment cache be?\nOne answer is that if a segment represents 10% of the total index, then it should get 10% of the cache.  There are downsides to that though - it fails to take into account non-uniform access in the index (hotspots). "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12735466",
            "date": "2009-07-27T05:26:29+0000",
            "content": "One interesting question is the structure of the cache and how to size caches.\n\ni feel like i'm missing something here ... wouldn't the simplest approach still be the best?\n\nif i currently have a single filterCache of size=1024, and 1million docs then that uses up some quantity of memory =~ func(1024,1mil) (based on sparseness of each query)\n\nif i start having per segment caches, and there are 22 segments each with a filterCache of size=1024, then the amount of memory used by all the caches will be ~22*func(1024,(1mil/22)) ... which should wind up being roughtly the same as before.\n\nsmaller segments will wind up using less ram for their caches, even if the \"size\" of the cache is the same for each segment. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12735679",
            "date": "2009-07-27T17:40:31+0000",
            "content": "Perhaps in another issue we can implement a cache that is RAM\nusage aware. Implement sizeof(bitset), and keep the cache below\na predefined limit? \n\nDo we need to have a cache per reader or can the cachekey\ninclude the reader? If segments are created rapidly, we may not\nwant the overhead of creating a new cache and managing it's\nsize, especially if we move to a RAM usage model.   "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12784668",
            "date": "2009-12-02T06:59:17+0000",
            "content": "I'm taking a look at this, it's straightforward to cache and\nreuse docsets per reader in SolrIndexSearcher, however, we're\npassing docsets all over the place (i.e. UnInvertedField). We\ncan't exactly rip out DocSet without breaking most unit tests,\nand writing a bunch of facet merging code. We'd likely lose\nfunctionality? \n\nWill the MultiDocSet concept SOLR-568 as an easy way to get\nsomething that works up and running? Then we can benchmark and\nsee if we've lost performance? "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12785433",
            "date": "2009-12-03T18:22:06+0000",
            "content": "I realized because of UnInvertedField, we'll need to merge facet\nresults from UIF per reader, so using a MultiDocSet won't help. Can we\nleverage the distributed merging FacetComponent implements\n(i.e. reuse and/or change the code to work in both the\ndistributed and local cases)? Ah well, I was hoping for an easy\nsolution for realtime facets.  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12785435",
            "date": "2009-12-03T18:30:16+0000",
            "content": "we'll need to merge facet results from UIF per reader\n\nYeah... that's a pain.\nWe could easily do per-segment faceting for non-string types though (int, long, etc) since they don't need to be merged. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12785462",
            "date": "2009-12-03T19:26:34+0000",
            "content": "I changed the title because we're not going to cache docs in\nthis issue (though I think it's possible to cache docs by the\ninternal id, rather than the doc id). \n\nPer-segment facet caching and merging per segment can go into a\ndifferent issue. "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12786240",
            "date": "2009-12-04T23:35:13+0000",
            "content": " Yeah... that's a pain. We could easily do per-segment\nfaceting for non-string types though (int, long, etc) since they\ndon't need to be merged. \n\nI opened SOLR-1617 for this. I think doc sets can be handled\nwith a multi doc set (hopefully). Facets however, argh,\nFacetComponent is really hairy, though I think it boils down to\nsimply adding field values of the same up? Then there seems to\nbe edge cases which I'm scared of. At least it's easy to test\nwhether we're fulfilling todays functionality by randomly unit\ntesting per-segment and multi-segment side by side (i.e. if the\nresults of one are different than the results of the other, we\nknow there's something to fix).\n\nPerhaps we can initially add up field values, and test that\n(which is enough for my project), and move from there. I'd still\nlike to genericize all of the distributed processes to work over\nmultiple segments (like Lucene distributed search uses a\nMultiSearcher which also works locally), so that local or\ndistributed is the same API wise. However given I've had trouble\nfiguring out the existing distributed code (SOLR-1477 ran into a\nwall). Maybe as part of SolrCloud\nhttp://wiki.apache.org/solr/SolrCloud, we can rework the\ndistributed APIs to be more user friendly (i.e. *MultiSearcher\nis really easy to understand). If Solr's going to work well in\nthe cloud, distributed search probably needs to be easy to multi\ntier for scaling (i.e. if we have 1 proxy server and 100 nodes,\nwe could have 1 top proxy, and 1 proxy per 10 nodes, etc).  "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12872611",
            "date": "2010-05-27T22:09:10+0000",
            "content": "Bulk updating 240 Solr issues to set the Fix Version to \"next\" per the process outlined in this email...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E\n\nSelection criteria was \"Unresolved\" with a Fix Version of 1.5, 1.6, 3.1, or 4.0.  email notifications were suppressed.\n\nA unique token for finding these 240 issues in the future: hossversioncleanup20100527 "
        },
        {
            "author": "Jason Rutherglen",
            "id": "comment-12985998",
            "date": "2011-01-24T21:12:18+0000",
            "content": "Sorry if this spam's things, however it's unlikely that I'll work on these. "
        }
    ]
}