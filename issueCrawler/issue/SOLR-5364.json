{
    "id": "SOLR-5364",
    "title": "SolrCloud stops accepting updates",
    "details": {
        "affect_versions": "4.4,                                            4.5,                                            4.6",
        "status": "Resolved",
        "fix_versions": [
            "4.6"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Blocker",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "I'm attempting to import data into a SolrCloud cluster. After a certain amount of time, the cluster stops accepting updates.\n\nI have tried numerous suggestions in IRC from Elyorag and others without resolve.\n\nI have had this issue with 4.4, and understood there was a deadlock issue fixed in 4.5, which hasn't resolved the issue, neither have the 4.6 snapshots.\n\nI've tried with Tomcat, various tomcat configuration changes to threading, and with Jetty. Tried with various index merging configurations as I initially thought there was a deadlock with concurrent merg scheduler, however same issue with SerialMergeScheduler.\n\nThe cluster stops accepting updates after some amount of time, this seems to vary and is inconsistent. Sometimes I manage to index 400k docs, other times ~1million . Querying  the cluster continues to work. I can reproduce the issue consistently, and is currently blocking our transition to Solr.\n\nI can provide stack traces, thread dumps, jstack dumps as required.\n\nHere are two jstacks thus far:\n\nhttp://pastebin.com/1ktjBYbf\nhttp://pastebin.com/8JiQc3rb\n\nI have got these jstacks from the latest 4.6 snapshot, also running solrj snapshot. The issue is also consistently reproducable with BinaryRequest writer.",
    "attachments": {
        "threaddump.201311180812.zip": "https://issues.apache.org/jira/secure/attachment/12614352/threaddump.201311180812.zip"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-13799254",
            "date": "2013-10-18T16:32:13+0000",
            "content": "This is likely a dupe of SOLR-5216. "
        },
        {
            "author": "Chris Geeringh",
            "id": "comment-13799258",
            "date": "2013-10-18T16:34:43+0000",
            "content": "Mark, could well be.\n\nMinor update, shalin built a 4.6 snap war with the patch from that ticket for me to test. It did not help the situation - I'm not surprised as I agree with your comment, it's simply doubling the semaphore count. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13799548",
            "date": "2013-10-18T21:41:06+0000",
            "content": "SOLR-5216 should be solved by SOLR-5232. "
        },
        {
            "author": "Chris Geeringh",
            "id": "comment-13799825",
            "date": "2013-10-19T07:50:51+0000",
            "content": "Mark, I am running with 4.6 snapshots from 13 Oct. From the comments and commit history, I should have that fix. Your recent commit is only a performance enhancement on that. So not sure that ticket is the solution. "
        },
        {
            "author": "Chris Geeringh",
            "id": "comment-13799829",
            "date": "2013-10-19T07:56:09+0000",
            "content": "Actually, I could be wrong with that last comment, was confused with another ticket. I'll grab the latest snapshot and test it.\n\nfwiw, as a desperate work around I wrote my own DIH processor, which too ran into this after ~1.4m docs were processed. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13800062",
            "date": "2013-10-20T02:13:40+0000",
            "content": "How are you adding documents? Which solrj impl? Using multiple threads? A document at a time or using the collection (bulk) add methods? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13800066",
            "date": "2013-10-20T02:54:24+0000",
            "content": "Also how many nodes and how many shards, replicas etc? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13800067",
            "date": "2013-10-20T02:55:48+0000",
            "content": "Also, please dump any stack traces from the latest code now that it has changed. "
        },
        {
            "author": "Chris Geeringh",
            "id": "comment-13800131",
            "date": "2013-10-20T12:00:55+0000",
            "content": "CloudSolrServer (also tried with CUSS)\nBulk - tried varying sizes, 10, 50, 250, 500\n4 nodes, 4 shards and 4 replicas per node\nImporting with a single thread, single CSS\n\nWill look for stack traces shortly. "
        },
        {
            "author": "Chris Geeringh",
            "id": "comment-13801622",
            "date": "2013-10-22T09:02:08+0000",
            "content": "I can confirm the same as \"Prasi S\" on the solr-user mailing list with regard to hitting the admin url and indexing continues. After hanging for around 40 minutes, I went to the admin UI (which is totally responsive) and then indexing started again - and my client continues to report posting up to the cloud. After an even shorter period of time indexing stops again, and hitting the admin UI wont start indexing again, until a few hours later.\n\nNeither Solr, or my indexing client are reporting errors. The client appears to be hanging, but it looks like it's CloudSolrServer not returning a response and sitting in the .add(docs) method. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13801895",
            "date": "2013-10-22T15:10:06+0000",
            "content": "I indexed a few million docs using cuss, cloudsolrserver, single docs and bulk add, 4 nodes, 4 shards, 4 replicas, and I've yet to see any update lock up myself with the latest code. "
        },
        {
            "author": "Chris Geeringh",
            "id": "comment-13802172",
            "date": "2013-10-22T19:36:18+0000",
            "content": "Out of interest, what Java version are you using, and any specific JAVA_OPTS?\n\nI noticed a substantial improvement in insertion rate (3x) when altering garbage collections settings. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13802907",
            "date": "2013-10-23T14:42:41+0000",
            "content": "java version \"1.7.0_25\"\n\nAll the default java command line args other than more heap. "
        },
        {
            "author": "Chris Geeringh",
            "id": "comment-13805172",
            "date": "2013-10-25T08:41:59+0000",
            "content": "Mark, thanks for taking the time to test this.\n\nIt turns out it is a garbage collection issue, I was running the default java args too, however my machines have 128GB RAM, and had been allocated 20GB heap. It seems this really needs tuning with Java 7. Since updating my GC collection settings, I have not had any issues. For reference these are the GC settings I'm running with:\n\nJAVA_OPTS=\"$JAVA_OPTS -server -XX:NewRatio=1 -XX:SurvivorRatio=6 \\\n-XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode\n-XX:CMSIncrementalDutyCycleMin=0 \\\n-XX:CMSIncrementalDutyCycle=10 -XX:+CMSIncrementalPacing \\\n-XX:+CMSClassUnloadingEnabled -XX:+DisableExplicitGC \\\n-XX:ConcGCThreads=10 \\\n-XX:ParallelGCThreads=10 \\\n-XX:MaxGCPauseMillis=30000\"\n\nI've also set heap to 12g, eden to 5g:  -Xmx12g -Xms12g -Xmn5g "
        },
        {
            "author": "Vadym Lotar",
            "id": "comment-13823686",
            "date": "2013-11-15T14:22:56+0000",
            "content": "Greetings,\n\nWe are experiencing completely the same problem with our environment:\nSOLR 4.5.1\nOpen JDK version \"1.7.0_25\"\n30Gb RAM\nDefault JAVA_OPTS\n2 shards\n2 slaves for each shard\n3.5.0-36-generic #57~precise1-Ubuntu\n\nOur first step would be changing GC algorithm, however it seems that we have a deadlock somewhere during writing the index.\nCould we provide any information which can be useful for investigating this problem?\n\nThanks in advance. "
        },
        {
            "author": "Chris Geeringh",
            "id": "comment-13823689",
            "date": "2013-11-15T14:29:14+0000",
            "content": "Vadym, I have isolated the issue down to using UAX29URLEmailTokenizer, are you using it by any chance? "
        },
        {
            "author": "Vadym Lotar",
            "id": "comment-13823698",
            "date": "2013-11-15T14:46:07+0000",
            "content": "Hi Chris,\n\nThanks for your response. \n\nWe are NOT using UAX29URLEmailTokenizer. Having 5 cores in the cluster, we are normally using only: \nStandardTokenizerFactory\nLengthFilterFactory\nStandardFilterFactory\nStopFilterFactory\nLowerCaseFilterFactory\nSynonymFilterFactory\nSnowballPorterFilterFactory\nWhitespaceTokenizerFactory\nWordDelimiterFilterFactory\nRemoveDuplicatesTokenFilterFactory\nPatternTokenizerFactory\nKeywordTokenizerFactory\n\nThanks for your help. "
        },
        {
            "author": "Chris Geeringh",
            "id": "comment-13823704",
            "date": "2013-11-15T14:50:45+0000",
            "content": "Purely in the interest of testing, could you try without the StandardTokenizerFactory - and see if you hit the issue? The UAX29URLEmailTokenizer is an extension on StandardTokenizerFactory as far as I'm aware.\n\nSo only run with the WhiteSpaceTokenizerFactory for a test. "
        },
        {
            "author": "Vadym Lotar",
            "id": "comment-13823714",
            "date": "2013-11-15T15:02:38+0000",
            "content": "Interesting...\n\nFrom the documentation:\n\nsolr.StandardTokenizerFactory\nCreates org.apache.lucene.analysis.standard.StandardTokenizer.\nA good general purpose tokenizer that strips many extraneous characters and sets token types to meaningful values. Token types are only useful for subsequent token filters that are type-aware of the same token types. There aren't any filters that use StandardTokenizer's types. Word boundary rules from Unicode standard annex UAX#29\n\nSo this UAX#29 is also mentioned here, however UAX29URLEmailTokenizer extends abstract class Tokenizer and not related to StandardTokenizer.\n\nThe problem is that we actually cannot remove it from the configuration because it does what we really want to have.\n\nDo you think it's not related to JVM parameters configuration or to a deadlock somewhere in the index writer? "
        },
        {
            "author": "Chris Geeringh",
            "id": "comment-13823737",
            "date": "2013-11-15T15:37:17+0000",
            "content": "Vadym, it does what I want and need too. However, to verify this is the same issue, it would be helpful if you ran without the tokenizer and didn't see the issue. At least we can say definitely that's where the problem lies. "
        },
        {
            "author": "Vadym Lotar",
            "id": "comment-13825165",
            "date": "2013-11-18T08:10:36+0000",
            "content": "SOLR got stuck again both on Staging and Production environment. I attached thread dump taken from the production env.\n\nHere is the main interesting part:\n\nThread 14813: (state = BLOCKED)\n\n\tsun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)\n\tjava.util.concurrent.locks.LockSupport.park(java.lang.Object) @bci=14, line=186 (Compiled frame)\n\tjava.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt() @bci=1, line=834 (Compiled frame)\n\tjava.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(int) @bci=72, line=994 (Compiled frame)\n\tjava.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(int) @bci=24, line=1303 (Compiled frame)\n\tjava.util.concurrent.Semaphore.acquire() @bci=5, line=317 (Compiled frame)\n\torg.apache.solr.util.AdjustableSemaphore.acquire() @bci=4, line=61 (Compiled frame)\n\torg.apache.solr.update.SolrCmdDistributor.submit(org.apache.solr.update.SolrCmdDistributor$Request) @bci=22, line=418 (Compiled frame)\n\torg.apache.solr.update.SolrCmdDistributor.submit(org.apache.solr.client.solrj.request.UpdateRequest, org.apache.solr.update.SolrCmdDistributor$Node) @bci=20, line=368 (Compiled frame)\n\torg.apache.solr.update.SolrCmdDistributor.flushAdds(int) @bci=223, line=300 (Compiled frame)\n\torg.apache.solr.update.SolrCmdDistributor.finish() @bci=2, line=96 (Compiled frame)\n\torg.apache.solr.update.processor.DistributedUpdateProcessor.doFinish() @bci=4, line=499 (Compiled frame)\n\torg.apache.solr.update.processor.DistributedUpdateProcessor.finish() @bci=8, line=1256 (Compiled frame)\n\torg.apache.solr.update.processor.LogUpdateProcessor.finish() @bci=48, line=179 (Compiled frame)\n\torg.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(org.apache.solr.request.SolrQueryRequest, org.apache.solr.response.SolrQueryResponse) @bci=155, line=83 (Compiled frame)\n\torg.apache.solr.handler.RequestHandlerBase.handleRequest(org.apache.solr.request.SolrQueryRequest, org.apache.solr.response.SolrQueryResponse) @bci=43, line=135 (Compiled frame)\n\torg.apache.solr.core.SolrCore.execute(org.apache.solr.request.SolrRequestHandler, org.apache.solr.request.SolrQueryRequest, org.apache.solr.response.SolrQueryResponse) @bci=115, line=1859 (Compiled frame)\n\torg.apache.solr.servlet.SolrDispatchFilter.execute(javax.servlet.http.HttpServletRequest, org.apache.solr.request.SolrRequestHandler, org.apache.solr.request.SolrQueryRequest, org.apache.solr.response.SolrQueryResponse) @bci=31, line=703 (Compiled frame)\n\torg.apache.solr.servlet.SolrDispatchFilter.doFilter(javax.servlet.ServletRequest, javax.servlet.ServletResponse, javax.servlet.FilterChain, boolean) @bci=1348, line=406 (Interpreted frame)\n\torg.apache.solr.servlet.SolrDispatchFilter.doFilter(javax.servlet.ServletRequest, javax.servlet.ServletResponse, javax.servlet.FilterChain) @bci=5, line=195 (Compiled frame)\n\torg.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(javax.servlet.ServletRequest, javax.servlet.ServletResponse) @bci=100, line=1419 (Interpreted frame)\n\torg.eclipse.jetty.servlet.ServletHandler.doHandle(java.lang.String, org.eclipse.jetty.server.Request, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) @bci=197, line=455 (Interpreted frame)\n\torg.eclipse.jetty.server.handler.ScopedHandler.handle(java.lang.String, org.eclipse.jetty.server.Request, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) @bci=25, line=137 (Compiled frame)\n\torg.eclipse.jetty.security.SecurityHandler.handle(java.lang.String, org.eclipse.jetty.server.Request, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) @bci=540, line=557 (Compiled frame)\n\torg.eclipse.jetty.server.session.SessionHandler.doHandle(java.lang.String, org.eclipse.jetty.server.Request, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) @bci=68, line=231 (Interpreted frame)\n\torg.eclipse.jetty.server.handler.ContextHandler.doHandle(java.lang.String, org.eclipse.jetty.server.Request, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) @bci=206, line=1075 (Interpreted frame)\n\torg.eclipse.jetty.servlet.ServletHandler.doScope(java.lang.String, org.eclipse.jetty.server.Request, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) @bci=277, line=384 (Interpreted frame)\n\torg.eclipse.jetty.server.session.SessionHandler.doScope(java.lang.String, org.eclipse.jetty.server.Request, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) @bci=233, line=193 (Interpreted frame)\n\torg.eclipse.jetty.server.handler.ContextHandler.doScope(java.lang.String, org.eclipse.jetty.server.Request, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) @bci=437, line=1009 (Interpreted frame)\n\torg.eclipse.jetty.server.handler.ScopedHandler.handle(java.lang.String, org.eclipse.jetty.server.Request, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) @bci=13, line=135 (Compiled frame)\n\torg.eclipse.jetty.server.handler.ContextHandlerCollection.handle(java.lang.String, org.eclipse.jetty.server.Request, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) @bci=399, line=255 (Compiled frame)\n\torg.eclipse.jetty.server.handler.HandlerCollection.handle(java.lang.String, org.eclipse.jetty.server.Request, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) @bci=42, line=154 (Compiled frame)\n\torg.eclipse.jetty.server.handler.HandlerWrapper.handle(java.lang.String, org.eclipse.jetty.server.Request, javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) @bci=23, line=116 (Compiled frame)\n\torg.eclipse.jetty.server.Server.handle(org.eclipse.jetty.server.AbstractHttpConnection) @bci=146, line=368 (Compiled frame)\n\torg.eclipse.jetty.server.AbstractHttpConnection.handleRequest() @bci=323, line=489 (Compiled frame)\n\torg.eclipse.jetty.server.BlockingHttpConnection.handleRequest() @bci=1, line=53 (Compiled frame)\n\torg.eclipse.jetty.server.AbstractHttpConnection.content(org.eclipse.jetty.io.Buffer) @bci=13, line=953 (Compiled frame)\n\torg.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(org.eclipse.jetty.io.Buffer) @bci=5, line=1014 (Compiled frame)\n\torg.eclipse.jetty.http.HttpParser.parseNext() @bci=2044, line=622 (Compiled frame)\n\torg.eclipse.jetty.http.HttpParser.parseAvailable() @bci=51, line=240 (Compiled frame)\n\torg.eclipse.jetty.server.BlockingHttpConnection.handle() @bci=51, line=72 (Interpreted frame)\n\torg.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run() @bci=100, line=264 (Interpreted frame)\n\torg.eclipse.jetty.util.thread.QueuedThreadPool.runJob(java.lang.Runnable) @bci=1, line=608 (Interpreted frame)\n\torg.eclipse.jetty.util.thread.QueuedThreadPool$3.run() @bci=47, line=543 (Interpreted frame)\n\tjava.lang.Thread.run() @bci=11, line=724 (Interpreted frame)\n\n\n\n\nSo, it seems like synchronization issue inside of SOLR. Today I'm gonna go deeply inside the SOLR code and most likely try to check Chris's suggestion regarding WhitespaceTokenizer. "
        },
        {
            "author": "Vadym Lotar",
            "id": "comment-13826596",
            "date": "2013-11-19T15:29:45+0000",
            "content": "Replacing StandardTokenizerFactory by WhitespaceTokenizerFactory didn't help. \nIt got stuck again. \nThe situation is completely the same and thread dump looks pretty similar.\n\nDiving in to the code we saw that it waits till all nodes is notified about the changes... "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13826605",
            "date": "2013-11-19T15:37:05+0000",
            "content": "Vadym, it looks like you are hitting SOLR-5216. That should be address in 4.6, which we are voting on today. You will find the current RC here if you are able to help with testing: http://people.apache.org/~simonw/staging_area/lucene-solr-4.6.0-RC4-rev1543363/ "
        },
        {
            "author": "Vadym Lotar",
            "id": "comment-13828570",
            "date": "2013-11-21T07:28:40+0000",
            "content": "Hello Mark,\n\nThanks for your response. The problem with testing is that it's really hard to reproduce this issue on staging, I guess because we have small number of slaves there. If we will find a way how to test it on staging we will definitely do that.\n\nThanks again and hope your fix will solve the issue.\n\nBest regards. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13834890",
            "date": "2013-11-28T14:39:04+0000",
            "content": "Since the two SOLR tickets above pretty much change how all this is done (especially SOLR-5232 when using DIH), I'll mark this closed. Of course if anyone sees this kind of behavior again we'll re-open. "
        }
    ]
}