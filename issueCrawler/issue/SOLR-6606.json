{
    "id": "SOLR-6606",
    "title": "In cloud mode the leader should distribute autoCommits to it's replicas",
    "details": {
        "components": [],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [
            "5.0",
            "6.0"
        ],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "Today in SolrCloud different replicas of a shard can trigger auto (hard) commits at different times. Although the documents which get added to the system remain consistent the way the segments gets formed can be different because of this.\n\nThe downside of segments not getting formed in an identical fashion across replicas is that when a replica goes into recovery chances are that it has to do a full index replication from the leader. This is time consuming and we can possibly avoid this if the leader forwards auto (hard) commit commands to it's replicas and the replicas never explicitly trigger an auto (hard) commit.\n\nI am working on a patch. Should have it up shortly",
    "attachments": {
        "SOLR-6606.patch": "https://issues.apache.org/jira/secure/attachment/12673633/SOLR-6606.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2014-10-08T17:30:38+0000",
            "author": "Varun Thacker",
            "content": "Patch where the auto (hard) commit command is executed only on the leader and subsequently forwarded to it's replicas.\n\nThe current tests pass but I am unsure how many of them actually triggered this code path.\n\nI wanted to get some feedback as to what everyone feels about this approach before writing additional tests ",
            "id": "comment-14163823"
        },
        {
            "date": "2014-10-08T18:30:54+0000",
            "author": "Varun Thacker",
            "content": "Regarding the two nocommits - \n\nFirst one - If a replica can't determine who its leader is then we should fail the commit and log an exception\nSecond one - We should just log an exception and do nothing else. Currently we don't do anything if a commit fails locally so we don't need to do anything special in this case either.\n\nI will start writing tests for this tomorrow. ",
            "id": "comment-14163926"
        },
        {
            "date": "2014-10-08T19:06:58+0000",
            "author": "Erick Erickson",
            "content": "I admit I don't know the code well (at all, really). What I wonder is whether we can really count on all the segments being aligned. 'cause if there's any chance at all that segment 1 on any node has different docs than segment 1 on any other node, then it seems like things aren't going to work. So can we really guarantee that the segments are identical for all members of a slice under this scenario?\n\nThat said, it would be really cool to not have to replicate the entire index when synching.... ",
            "id": "comment-14163992"
        },
        {
            "date": "2014-10-08T19:13:08+0000",
            "author": "Mark Miller",
            "content": "I don't think whether it is guaranteed or not really matters. Just making it much more common is certainly a worthwhile improvement. ",
            "id": "comment-14164002"
        },
        {
            "date": "2014-10-09T19:56:37+0000",
            "author": "Varun Thacker",
            "content": "So can we really guarantee that the segments are identical for all members of a slice under this scenario?\n\nI think Mark put it well. With the patch the chances that the segments be the same across replicas are higher but cannot guaranteed.\n\nDoes someone know why the NOTE in the test fails?\n\nSecondly is writing commits with the same meta 'commitTimeMSec' correct? Or should change the ReplicationHandler 'fetchindex' logic to use the work done on LUCENE-5895 instead?\n\nTests seems to be happy ",
            "id": "comment-14165636"
        },
        {
            "date": "2014-10-10T19:26:13+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "I don't think whether it is guaranteed or not really matters. Just making it much more common is certainly a worthwhile improvement.\n\n+1\n\nThere are three problems that we can solve for faster recoveries (there might be others):\n\n\tReplicas assign their own commit time stamp causing almost identical segments to be downloaded again (which is solved by this issue)\n\tSegments merged away by leader that might still exist on an out-of-date replica\n\tSingle-threaded execution of transaction log replay (thanks Mark for pointing this out to me sometime back)\n\n\n\nReplicas can potentially ask other replicas too for segments with a particular commit id thereby spreading the load away from the leader.\n\nSecondly is writing commits with the same meta 'commitTimeMSec' correct? Or should change the ReplicationHandler 'fetchindex' logic to use the work done on LUCENE-5895 instead?\n\nI think both approaches are fine but if we go for the approach in LUCENE-5895 then we will need some careful back-compat logic in SnapPuller to make sure that we don't force replicas to re-download existing segments just because we started using another id.\n ",
            "id": "comment-14167353"
        },
        {
            "date": "2014-10-13T09:48:52+0000",
            "author": "Jim Ferenczi",
            "content": "Is is intended for partial recovery or only to distribute the downloads on a full recovery (when the Replica downloads a full index from the master) ? I am saying this because for the partial recovery you need to handle the deletes as well. For instance if one replica missed the last commit he could download the segment from the master but he would loose all the deletes related to this update. Keeping the list of every delete per commit seems mandatory but also very expensive unless we can garbage collect them at some point.  ",
            "id": "comment-14169142"
        },
        {
            "date": "2014-11-24T21:04:40+0000",
            "author": "Timothy Potter",
            "content": "How does merge policy affect SOLR-6606? It seems to me that even if you synchronize the commit boundaries between leader and replica, the merge schedulers will be running at different times on leader / replica so the segments get out-of-sync anyway, right? Admittedly, I don't know too much about how the replication handler determines which segments to pull from the leader ... just wondering if you investigated this question any? ",
            "id": "comment-14223508"
        },
        {
            "date": "2014-11-27T03:23:15+0000",
            "author": "Mark Miller",
            "content": "I think you would really have to test out how well segments stay in sync - certainly they can still go out of sync for various reasons, and you may have to use flush by document rather than RAM to improve things (someone should do some real world testing), but otherwise, I think you have an okay chance at some more segments lining up. ",
            "id": "comment-14227212"
        },
        {
            "date": "2014-12-15T14:58:48+0000",
            "author": "Varun Thacker",
            "content": "This is how I started testing on my local machine:\n1. created a collection with one shard and two replicas\n2. Keep indexing new documents. Batch size=1000. autoCommit every 10k docs.\n3. Now every few minutes, I ran this command - \n./solr stop -p 7574; sleep 100;./solr start -cloud -d node2 -p 7574 -z localhost:9983\n4. Stopped indexing.\n\nWhen we kill a server and bring it back up, replication handler will pull all the missing segment files which are missing, so both replicas will have same segment files after recovery. Now both replicas keep creating segment files in a similar fashion even without the leader distributing auto-commmit.\n\nFrom what I understand since replication checks if the file name and size is the same ( and not segment ids or anything like that ) we get away with it.\n\nI think since moving replication to use segment ids is something we are considering given SOLR-6640 , I am tempted to explore that first and revisit this. \n\nAny thought? Am I missing something during the tests or interpreting incorrectly? ",
            "id": "comment-14246712"
        },
        {
            "date": "2016-11-24T00:50:40+0000",
            "author": "Cao Manh Dat",
            "content": "Updated patch for this issue. This patch updated to lastest source and fixed some bugs :\n\n\tset DistributedUpdateProcessor.COMMIT_END_POINT flag to distributed commit, so replica won't resend the commit back to leader\n\tincrease retryCount in consistencyCheck()\n\tadded a test ( which currently failed ) to make sure segments are line up in very simple case of adding one by one new documents\n\n\n\nThe test is failed because between the IW.commit() and open a new searcher in the replicas side. They can received some new documents so when they open a new searcher they may create a new segment ( which have different of documents for each replicas ) so segments between replicas is never be lined up. \n\nThe test is failed so often, hence I think in the real world we won't never get a single benefit from this patch. ",
            "id": "comment-15691814"
        }
    ]
}