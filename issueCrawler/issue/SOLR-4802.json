{
    "id": "SOLR-4802",
    "title": "Atomic Updated for large documents is very slow and at some point Server stops responding.",
    "details": {
        "affect_versions": "4.2.1",
        "status": "Open",
        "fix_versions": [],
        "components": [
            "update"
        ],
        "type": "Bug",
        "priority": "Critical",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "I am updating three fields in the document which are of type long and float. This is an atomic update. Updating around 30K doucments and i always get stuck after 80%.\nUpdate 200 docs per request in a thread and i execute 5 such thread in parallel. \nThe current work around is that i have auto-commit setup for 10000 docs with openSearcher = false.\n\ni guess that this is related to SOLR-4589\n\nSome additional information \nthe machine is 6core with 5GB Heap. ramBuffer=1024MB",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Erick Erickson",
            "id": "comment-13652006",
            "date": "2013-05-08T15:54:35+0000",
            "content": "Are you seeing any out of memory errors? "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13652632",
            "date": "2013-05-09T01:20:21+0000",
            "content": "The current work around is that i have auto-commit setup for 10000 docs with openSearcher = false. \n\nA lack of commits or a lack of any autocommits is not really valid with transaction logging enabled.\nThis comes up enough that it seems like we should have some mechanism to prevent unbounded tlog growth regardless of autocommit settings (such as enabling an autocommit even if one is not configured, or some other similar mechanism) "
        },
        {
            "author": "Aditya",
            "id": "comment-13652640",
            "date": "2013-05-09T01:30:09+0000",
            "content": "@Eric\nYes the server stops responding and then eventually get to OOM.\n\nThe Document that we perform updates have large multiValued fields.  "
        },
        {
            "author": "Aditya",
            "id": "comment-13670090",
            "date": "2013-05-30T06:07:07+0000",
            "content": "I am not sure if this issue is active. But we recently had an incident where we had to rollback to 3.5 on Production. There is seriously some issue with High memory utilization in Solr with large multivalued field when retrieved. Fetching large size multivalued field is causing System being unresponsive and then eventually throw OOM. \n\nI will try collect more data and update the Issue. Planning to raise a ticket with Lucid Works "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13836114",
            "date": "2013-12-01T20:47:06+0000",
            "content": "Aditya, is this still a problem? Can we close this issue? "
        }
    ]
}