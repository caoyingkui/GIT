{
    "id": "LUCENE-3972",
    "title": "Improve AllGroupsCollector implementations",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/grouping"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "I think that the performance of TermAllGroupsCollectorm, DVAllGroupsCollector.BR and DVAllGroupsCollector.SortedBR can be improved by using BytesRefHash to store the groups instead of an ArrayList.",
    "attachments": {
        "LUCENE-3972.patch": "https://issues.apache.org/jira/secure/attachment/12522329/LUCENE-3972.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-04-11T22:21:39+0000",
            "content": "Attached initial patch. The TermAllGroupsCollector variant attached in this patch seems to be 2 times faster then the one that is already in subversion. ",
            "author": "Martijn van Groningen",
            "id": "comment-13251992"
        },
        {
            "date": "2012-04-12T09:56:47+0000",
            "content": "Curious that it's so much faster ... BytesRefHash operates on the byte[] term while the current approach operates on int ord.\n\nHow large was the index?  If it was smallish, maybe the time was dominated by re-ord'ing after each reader...? ",
            "author": "Michael McCandless",
            "id": "comment-13252315"
        },
        {
            "date": "2012-04-12T13:06:27+0000",
            "content": "Tested the current patch on a index containing 10M documents that has ~5M unique groups. The already existing implementation needs ~15.3 seconds to get the total group count and the new impl in the patch needs ~4.4 seconds to get the total group count. ",
            "author": "Martijn van Groningen",
            "id": "comment-13252402"
        },
        {
            "date": "2012-04-12T13:51:37+0000",
            "content": "This is curious indeed. One thing to check would be this: SentinelIntSet uses no key rehashing (rehash simply returns the key). This resulted in very poor performance for certain regular integer sets (my experience from implementing HPPC). So while rehashing may seem like an additional overhead, it actually boosts performance.\n\nMartijn \u2013 could you patch the trunk's SentinelIntSet#rehash with, for example, this (murmur hash3 tail):\n\n    public static int rehash(int k)\n    {\n        k ^= k >>> 16;\n        k *= 0x85ebca6b;\n        k ^= k >>> 13;\n        k *= 0xc2b2ae35;\n        k ^= k >>> 16;\n        return k;\n    }\n\n\nand retry your test? Btw. I'm not saying it'll be faster  ",
            "author": "Dawid Weiss",
            "id": "comment-13252427"
        },
        {
            "date": "2012-04-12T14:04:15+0000",
            "content": "Dawid - Do you mean the hash method instead of rehash method in SentinelIntSet? The rehash methods doesn't have any parameters.  ",
            "author": "Martijn van Groningen",
            "id": "comment-13252437"
        },
        {
            "date": "2012-04-12T14:32:48+0000",
            "content": "The major difference between the committed TermAllGroupsCollector and one in the patch is that committed TermAllGroupsCollector creates a BytesRef instance for each unique group and puts it in a ArrayList (5M in my case). The version in the patch reuses a single BytesRef instance. The BytesRef bytes are copied into the BytesRefHash big bytes array via the BytesRefHash#add(...) method. I think the many BytesRef instances makes the committed TermAllGroupsCollector slow. ",
            "author": "Martijn van Groningen",
            "id": "comment-13252458"
        },
        {
            "date": "2012-04-12T14:38:37+0000",
            "content": "Yes, sorry \u2013 hash of course. The hash method that should redistribute keys space into buckets (but currently doesn't).\n\nAs for BytesRefHash vs. BytesRef instances \u2013 maybe it's the source of the speedup, who knows. I would try the hash method though, if nothing else just for curiosity. I would also patch it for the future in either case. Not rehashing input keys is a flaw in my opinion (again \u2013 backed by real life experience from HPPC). ",
            "author": "Dawid Weiss",
            "id": "comment-13252463"
        },
        {
            "date": "2012-04-12T14:47:05+0000",
            "content": "I ran a test with the changes to SentinelIntSet and one without. Performance difference between the two tests are minimal. The test with the changes is somewhat slower (15.7 s) than the one without the changes (15.3 s). ",
            "author": "Martijn van Groningen",
            "id": "comment-13252467"
        },
        {
            "date": "2012-04-12T14:47:55+0000",
            "content": "I would also patch it for the future in either case. \n\nHmmm, if you don't know anything about the integer keys, then a little extra hashing by default is a good idea.\nBut we know a lot about docids, and extra hashing should just lead to an average-case slowdown. ",
            "author": "Yonik Seeley",
            "id": "comment-13252469"
        },
        {
            "date": "2012-04-12T15:25:46+0000",
            "content": "Hmmm... it's not collisions then, it was worth a try. I still find the difference puzzling \u2013 I can't justify your version being 3x faster. Curious what it might be.\n\nBut we know a lot about docids, and extra hashing should just lead to an average-case slowdown.\n\nOk. ",
            "author": "Dawid Weiss",
            "id": "comment-13252486"
        },
        {
            "date": "2012-04-12T16:01:20+0000",
            "content": "Actually, we are storing term ords here, not docIDs.\n\nI think the high number of unique groups explains why the new patch is\nfaster: the time is likely dominated by re-ord'ing for each segment?\n\nIf you have fewer unique groups (and as the number of docs collected goes up),\nI think the current impl should be faster...? ",
            "author": "Michael McCandless",
            "id": "comment-13252518"
        },
        {
            "date": "2012-04-12T16:56:07+0000",
            "content": "If you have fewer unique groups (and as the number of docs collected goes up), I think the current impl should be faster...?\nThis is true. I ran a few tests on an index containing 10M docs:\n\n\n\nRun\nNum unique groups\nPerf. collector in patch\nPerf. committed collector\n\n\n1\n~65000\n892 ms\n132 ms\n\n\n2\n~645000\n1183 ms\n878 ms\n\n\n3\n~953000\n1291 ms\n1517 ms\n\n\n4\n~1819000\n1783 ms\n3762 ms\n\n\n5\n~3332000\n2703 ms\n4882 ms\n\n\n6\n~6668000\n4840 ms\n18989 ms\n\n\n\n\n\nAll the times are the average over 10 executions with a match all query.\n\nthe time is likely dominated by re-ord'ing for each segment?\nDuring run 4 I noticed that 3470 ms of the total 3762 ms was spend on re-ord'ing groups for segments.\n\nIt seems that the implementation in the patch is only usable if a search yields many unique groups as result.   ",
            "author": "Martijn van Groningen",
            "id": "comment-13252581"
        },
        {
            "date": "2013-07-05T18:27:34+0000",
            "content": "(e-commerce Solr user here)\n\nWe hit the very same performance hit with pathological queries with\n1M+ unique groups and need to solve this issue for our business.\n\nWould an hybrid approach switching implementation half-ways when\nthe number of unique groups detected gets too high be welcomed?\n\nI also wonder whether the number of segments plays a great role in this.\nDid you observe that in your benchmarking? ",
            "author": "Paul Masurel",
            "id": "comment-13701060"
        },
        {
            "date": "2013-07-07T21:13:28+0000",
            "content": "I noticed that the real cost was in the setNextReader method, in that method the collected group values are re-based into ordinals that are valid for the new upcoming segment (binary search for each collected term). But this is what makes the ordinals comparison in the collect method actually work.\n\nReducing the number of segments, reduces the number of setNextReader invocations and makes this feature faster. If you only have one segment (optimized index), then the rebasing of the group values doesn't occur, and the AllGroupsCollector should be much faster.\n\nI think that having a hybrid solution that would change the impl when a predefined number of groups have been found would make the current approach better, but this would never be faster as just having one segment (or global/index level ordinals). ",
            "author": "Martijn van Groningen",
            "id": "comment-13701669"
        },
        {
            "date": "2013-07-07T21:41:44+0000",
            "content": "I think it would be better if we changed these to use OrdinalMap so that O(#terms) is only computed once per reopen instead of this rebasing which is O(#terms) per query. ",
            "author": "Robert Muir",
            "id": "comment-13701676"
        },
        {
            "date": "2013-07-16T14:20:25+0000",
            "content": "I think so as well. What would be the best way to use the OrdinalMap? Just create an OrdinalsMap from a top level reader via SlowCompositeReaderWrapper#getSortedSetDocValues()? This seems the only place where OrdinalsMaps are cached. ",
            "author": "Martijn van Groningen",
            "id": "comment-13709801"
        },
        {
            "date": "2013-07-16T17:41:33+0000",
            "content": "Yes you are correct. its currently pretty ugly. But, i don't think we should really cache this anywhere \nelse other than a slow-wrapper (it would be wrong to do so)...\n ",
            "author": "Robert Muir",
            "id": "comment-13709974"
        }
    ]
}