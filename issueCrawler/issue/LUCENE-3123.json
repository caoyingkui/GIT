{
    "id": "LUCENE-3123",
    "title": "TestIndexWriter.testBackgroundOptimize fails with too many open files",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Bug",
        "fix_versions": [
            "3.2",
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Recreate with this line:\n\nant test -Dtestcase=TestIndexWriter -Dtestmethod=testBackgroundOptimize -Dtests.seed=-3981504507637360146:51354004663342240\n\nMight be related to LUCENE-2873 ?",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2011-05-19T12:56:01+0000",
            "content": "This is on Ubuntu btw.\n\nRun log:\n\nNOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testBackgroundOptimize -Dtests.seed=-3981504507637360146:51354004663342240\nNOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testBackgroundOptimize -Dtests.seed=-3981504507637360146:51354004663342240\nThe following exceptions were thrown by threads:\n*** Thread: Lucene Merge Thread #0 ***\norg.apache.lucene.index.MergePolicy$MergeException: java.io.FileNotFoundException: /tmp/test4907593285402510583tmp/_51_0.sd (Too many open files)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:507)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)\nCaused by: java.io.FileNotFoundException: /tmp/test4907593285402510583tmp/_51_0.sd (Too many open files)\n\tat java.io.RandomAccessFile.open(Native Method)\n\tat java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)\n\tat org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput$Descriptor.<init>(SimpleFSDirectory.java:69)\n\tat org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.<init>(SimpleFSDirectory.java:90)\n\tat org.apache.lucene.store.SimpleFSDirectory.openInput(SimpleFSDirectory.java:56)\n\tat org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:337)\n\tat org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:402)\n\tat org.apache.lucene.index.codecs.mockrandom.MockRandomCodec.fieldsProducer(MockRandomCodec.java:236)\n\tat org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader.<init>(PerFieldCodecWrapper.java:113)\n\tat org.apache.lucene.index.PerFieldCodecWrapper.fieldsProducer(PerFieldCodecWrapper.java:210)\n\tat org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:131)\n\tat org.apache.lucene.index.SegmentReader.get(SegmentReader.java:495)\n\tat org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:635)\n\tat org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3260)\n\tat org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2930)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:379)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:447)\nNOTE: test params are: codec=RandomCodecProvider: {field=MockRandom}, locale=nl_NL, timezone=Turkey\nNOTE: all tests run in this JVM:\n[TestIndexWriter]\nNOTE: Linux 2.6.32-31-generic i386/Sun Microsystems Inc. 1.6.0_20 (32-bit)/cpus=1,threads=2,free=26480072,total=33468416\n\n ",
            "author": "Doron Cohen",
            "id": "comment-13036163"
        },
        {
            "date": "2011-05-19T17:21:51+0000",
            "content": "Does that repro line reproduce the failure for you Doron?  It's odd because that test doesn't make that many fields... oh I see it makes a 100 segment index. I'll drop that to 50...\n\nThe nightly build also hits too-many-open-files every so often, I suspect because our random-per-field-codec is making too many codecs... I wonder if we should throttle it?  Ie if it accumulates too many codecs, to start sharing them b/w fields? ",
            "author": "Michael McCandless",
            "id": "comment-13036307"
        },
        {
            "date": "2011-05-19T17:25:00+0000",
            "content": "I dropped it from 100 to 50 segs.  Can you test if that works in your env Doron? ",
            "author": "Michael McCandless",
            "id": "comment-13036308"
        },
        {
            "date": "2011-05-19T17:51:23+0000",
            "content": "Yes, thanks, now it passes (trunk) - with this seed as well quite a few times without specifying a seed. \nI'll now verify on 3x. ",
            "author": "Doron Cohen",
            "id": "comment-13036322"
        },
        {
            "date": "2011-05-19T18:02:23+0000",
            "content": "I fact in 3x this is not reproducible with same seed (expected as Robert once explained) and I was not able to reproduce it with no seed, tried with -Dtest.iter=100 as well (though I am not sure, would a new seed be created in each iteration? Need to verify this...)\nAnyhow in 3x the test passes also after svn up with this fix.\nSo I think this can be resolved... ",
            "author": "Doron Cohen",
            "id": "comment-13036331"
        },
        {
            "date": "2011-05-19T18:05:24+0000",
            "content": "Fixed by Mike, thanks Mike! ",
            "author": "Doron Cohen",
            "id": "comment-13036335"
        },
        {
            "date": "2011-05-19T21:24:37+0000",
            "content": "Thanks for raising it Doron! ",
            "author": "Michael McCandless",
            "id": "comment-13036482"
        },
        {
            "date": "2011-06-03T16:37:18+0000",
            "content": "Bulk closing for 3.2 ",
            "author": "Robert Muir",
            "id": "comment-13043513"
        }
    ]
}