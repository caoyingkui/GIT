{
    "id": "LUCENE-6894",
    "title": "Improve DISI.cost() by assuming independence for match probabilities",
    "details": {
        "resolution": "Not A Problem",
        "affect_versions": "None",
        "components": [
            "core/search"
        ],
        "labels": "",
        "fix_versions": [],
        "priority": "Minor",
        "status": "Resolved",
        "type": "Improvement"
    },
    "description": "The DocIdSetIterator.cost() method returns an estimation of the number of matching docs. Currently conjunctions use the minimum cost, and disjunctions use the sum of the costs, and both are too high.\n\nThe probability of a match is estimated by dividing available cost() by the number of docs in a segment.\n\nThe conjunction probability is then the product of the inputs, and the disjunction probability follows from De Morgan's rule:\n\"not (A and B)\" is the same as \"(not A) or (not B)\"\nwith the probability for \"not\" computed as 1 minus the input probability.\n\nThe independence that is assumed is normally not there. However, the cost() results are only used to order the input DISIs/Scorers for optimization, and for that I expect this assumption to work nicely.",
    "attachments": {
        "LUCENE-6894.patch": "https://issues.apache.org/jira/secure/attachment/12771842/LUCENE-6894.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15001140",
            "author": "Paul Elschot",
            "date": "2015-11-11T21:30:23+0000",
            "content": "Patch of 11 Nov 2015.\nMost of the changes are to pass numDocs down to where it is actually used:\nConjunctionDISI, DisjunctionDISIApproximation, DisjunctionScorer, ConjunctionSpans, SpanOrQuery.\n\n\nThis is incomplete, there no tests.\nMinShouldMatchSumScorer only has the disjunctions done.\nFor un/ordered NearSpans there is a division by 4 (unordered) and by 8 (ordered) for zero allowed slop, something like this should also be done for the PhraseQueries.\nSpanContaining and SpanWithin use the conjunction estimation, these should also be smaller. "
        },
        {
            "id": "comment-15001241",
            "author": "Adrien Grand",
            "date": "2015-11-11T22:24:24+0000",
            "content": "The independence that is assumed is normally not there. However, the cost() results are only used to order the input DISIs/Scorers for optimization, and for that I expect this assumption to work nicely.\n\nBut so would the current worst-case approach? "
        },
        {
            "id": "comment-15001269",
            "author": "Paul Elschot",
            "date": "2015-11-11T22:44:54+0000",
            "content": "Suppose the query is a conjunction of 2 phrases \"a b\" and \"c d\". a and b each occur in 10% of the docs, c in 9.7% and d in 50%.\nWhich phrase should lead the top level conjunction? "
        },
        {
            "id": "comment-15001272",
            "author": "Paul Elschot",
            "date": "2015-11-11T22:46:38+0000",
            "content": "That one is actually solved nowadays by the two phase approach.\nI'll think of a better example later. "
        },
        {
            "id": "comment-15001984",
            "author": "Stefan Pohl",
            "date": "2015-11-12T11:24:50+0000",
            "content": "There are very likely situations where you can still decrease query runtime even further with a different order of clauses than the one based on current worst-case estimates, and I agree that the naming 'cost()' doesn't really reflect the conservative estimates. However, any other non-worst-case estimate might err very badly and make queries that are currently reasonably fast extremely slow.\n\nIt comes down to trading in worst-case behavior to gain average/throughput, but usually people care more about the slowest/hardest queries. However, maybe we can have worst-case and other estimates too and choose to use the latter only in cases where even making the wrong decision won't be too bad, so that you're speculative on the fast queries to gain throughput, but conservative on potentially slow queries. "
        },
        {
            "id": "comment-15003039",
            "author": "Paul Elschot",
            "date": "2015-11-12T22:06:31+0000",
            "content": "Another reason why I started this is that the result of cost() is also used as weights for matchCost() at LUCENE-6276, and I'd prefer those weights to be as accurate as reasonably possible.\n\nI think we can keep this (assuming independence for conjunctions and disjunctions) as a possible alternative until the current implementation gives a bad result.\n\nFor the proximity queries (Phrases, Spans) this reduces the conjunction cost() using the allowed slop.\nWould it be worthwhile to open a separate issue for that? "
        },
        {
            "id": "comment-15006050",
            "author": "Paul Elschot",
            "date": "2015-11-15T21:54:57+0000",
            "content": "Patch of 15 Nov 2015.\nResolve conflicts after LUCENE-6276.\n\nI tried this patch with the wikimedium5m benchmark. This showed no significant differences to current trunk, the differences where never bigger than 2.1% either way, and well within the standard deviations.\nThis could be because the patch here should have influence for more complex queries than the one in the benchmark. I might try to add more complex queries to the benchmark later. "
        },
        {
            "id": "comment-15006052",
            "author": "Paul Elschot",
            "date": "2015-11-15T21:58:19+0000",
            "content": "Here is the benchmark output, it might be good for future reference:\n\n                    TaskQPS baseline      StdDevQPS my_modified_version      StdDev                Pct diff\n                HighTerm      178.20      (1.8%)      174.50      (5.6%)   -2.1% (  -9% -    5%)\n                 MedTerm      641.36      (1.6%)      630.32      (4.9%)   -1.7% (  -8% -    4%)\n              OrHighHigh       57.02      (5.5%)       56.32      (6.6%)   -1.2% ( -12% -   11%)\n               OrHighMed      107.80      (5.2%)      106.89      (6.2%)   -0.8% ( -11% -   11%)\n             AndHighHigh      100.02      (2.3%)       99.34      (0.7%)   -0.7% (  -3% -    2%)\n                 LowTerm     2477.28      (3.0%)     2463.27      (5.5%)   -0.6% (  -8% -    8%)\n              AndHighMed      627.58      (1.5%)      625.22      (1.2%)   -0.4% (  -3% -    2%)\n              HighPhrase       81.21      (4.2%)       80.98      (4.3%)   -0.3% (  -8% -    8%)\n               OrHighLow      136.70      (3.1%)      136.35      (2.1%)   -0.3% (  -5% -    5%)\n               LowPhrase      181.55      (2.2%)      181.09      (2.0%)   -0.3% (  -4% -    4%)\n         MedSloppyPhrase       56.03      (2.9%)       55.93      (3.1%)   -0.2% (  -5% -    5%)\n             MedSpanNear       52.77      (1.7%)       52.68      (2.6%)   -0.2% (  -4% -    4%)\n         LowSloppyPhrase      106.15      (2.9%)      106.01      (3.1%)   -0.1% (  -6% -    6%)\n               MedPhrase       39.38      (3.8%)       39.36      (3.3%)   -0.1% (  -6% -    7%)\n                  Fuzzy1      137.14      (2.1%)      137.06      (1.5%)   -0.1% (  -3% -    3%)\n                  Fuzzy2       79.28      (1.9%)       79.25      (1.5%)   -0.0% (  -3% -    3%)\n             LowSpanNear       94.38      (1.7%)       94.35      (2.8%)   -0.0% (  -4% -    4%)\n            OrNotHighMed      444.12      (1.7%)      444.36      (1.2%)    0.1% (  -2% -    2%)\n              AndHighLow     1878.59      (2.0%)     1880.20      (1.9%)    0.1% (  -3% -    4%)\n                 Respell      106.47      (1.9%)      106.62      (1.7%)    0.1% (  -3% -    3%)\n            OrNotHighLow     1831.85      (1.7%)     1834.68      (1.3%)    0.2% (  -2% -    3%)\n           OrNotHighHigh       69.75      (1.6%)       69.91      (1.4%)    0.2% (  -2% -    3%)\n            HighSpanNear       36.38      (2.8%)       36.47      (3.8%)    0.3% (  -6% -    7%)\n        HighSloppyPhrase       45.58      (3.6%)       45.70      (3.5%)    0.3% (  -6% -    7%)\n            OrHighNotLow       65.78      (7.0%)       66.03      (8.4%)    0.4% ( -14% -   16%)\n                 Prefix3      448.85      (3.5%)      450.67      (3.8%)    0.4% (  -6% -    8%)\n                Wildcard      114.35      (4.8%)      115.02      (4.6%)    0.6% (  -8% -   10%)\n                  IntNRQ       23.48      (7.4%)       23.71      (7.7%)    1.0% ( -13% -   17%)\n                PKLookup      360.70      (1.7%)      364.91      (3.1%)    1.2% (  -3% -    6%)\n            OrHighNotMed      178.99      (7.2%)      181.91      (8.2%)    1.6% ( -12% -   18%)\n           OrHighNotHigh       39.78      (7.1%)       40.63      (7.5%)    2.1% ( -11% -   18%)\n\n\n "
        }
    ]
}