{
    "id": "LUCENE-6031",
    "title": "TokenSources optimization, avoid sort",
    "details": {
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed",
        "components": [
            "modules/highlighter"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "5.0",
            "6.0"
        ]
    },
    "description": "TokenSources.java, in the highlight module, is a facade that returns a TokenStream for a field by either un-inverting & converting the TermVector Terms, or by text re-analysis if TermVectors are unavailable or don't have the right options.  TokenSources is used by the default highlighter, which is the most accurate highlighter we've got.  When documents are large (say hundreds of kilobytes on up), I found that most of the highlighter's activity was up-front spent un-inverting & converting the term vector to a TokenStream, not on the actual/real highlighting that follows.  Much of that time was on a huge sort of hundreds of thousands of Tokens.  Time was also spent doing lots of String conversion and char copying, and it used a lot of memory, too.\n\nIn this patch, I overhauled TokenStreamFromTermPositionVector.java, and I removed similar logic in TokenSources that was used in circumstances when positions weren't available but offsets were.  This class can un-invert term vectors that have positions and/or offsets (at least one).  It doesn't sort.  It places Tokens directly into an array of tokens directly indexed by position.  When positions aren't available, the startOffset/8 is a substitute.  I've got a more light-weight Token inner class used in place of the former and deprecated Token that ultimately forms a linked-list when the process is done.  There is no string conversion; character copying is minimized.  The Token array is GC'ed after initialization, it's only needed during construction.\n\nMisc:\n\n\tIt implements reset() efficiently so it need not be wrapped in CachingTokenFilter (I'll supply a patch later on this).\n\tIt only fetches payloads if you ask for them by adding the attribute (the default highlighter won't add the attribute).\n\tIt exposes the underlying TermVector terms via a getter too, which is needed by another patch to follow later.\n\n\n\nA key assumption is that the position increment gap or first position isn't gigantic, as that will create wasted space and the linked-list formation ultimately has to visit all the slots.  We also assume that there aren't a ton of tokens at the same position, since inserting new tokens in sorted order is O(N^2) where 'N' is the average co-occurring token length.\n\nMy performance testing using Lucene's benchmark module on a megabyte document showed >5x speedup, in conjunction with some other patches to be posted separately. This patch made the most difference.",
    "attachments": {
        "LUCENE-6031.patch": "https://issues.apache.org/jira/secure/attachment/12677988/LUCENE-6031.patch",
        "LUCENE-6031_lazy_init_at_incrementToken,_and_optimize_payloads.patch": "https://issues.apache.org/jira/secure/attachment/12687493/LUCENE-6031_lazy_init_at_incrementToken%2C_and_optimize_payloads.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14188942",
            "author": "David Smiley",
            "content": "Here's the patch.\n\nThere are a couple no-commits:\n\n\tI want to rename TokenStreamFromTermPositionVector to TokenStreamFromTermVector\n\tI think TokenSources.getTokenStreamWithOffsets should relax it's insistence that the term vector have positions.  If you have control of your index options (and you do!), then you can choose not to put in positions and then highlight with the consequences of that decision, which is that highlighting will ignore stop-words, thus a query \"Sugar and Spice\" would not match \"sugar space\" and a query of \"sugar spice\" would match \"sugar and spice\" indexed.  If you don't even have stop-words then why put positions in the term vector.\n\n ",
            "date": "2014-10-29T20:30:40+0000"
        },
        {
            "id": "comment-14225774",
            "author": "David Smiley",
            "content": "Here's an updated patch.\n\n\tEnsures nobody calls the now-deprecated getTokenSources overloaded version that took the tokenPositionsGuaranteedContiguous argument.  The only callers were tests.\n\tRelaxed the requirement for the term vector to have positions.  It may be worth adding an entry in CHANGES.txt for upgraders but it seems it would only effect someone who had offsets but forgot to add positions to term vectors (why else would they not be there?), and now they may see less ideal highlighting related to positions. The fix is to add positions.\n\tRemoved the \"nocommit\" for the rename I want to do.  I'l have to remember to do a rename in a second commit.\n\n\n\nI'll commit in a couple days or so to trunk & 5x. ",
            "date": "2014-11-26T05:27:19+0000"
        },
        {
            "id": "comment-14228292",
            "author": "ASF subversion and git services",
            "content": "Commit 1642294 from David Smiley in branch 'dev/trunk'\n[ https://svn.apache.org/r1642294 ]\n\nLUCENE-6031: Optimize TokenSources term vector to TokenStream ",
            "date": "2014-11-28T13:35:24+0000"
        },
        {
            "id": "comment-14228293",
            "author": "ASF subversion and git services",
            "content": "Commit 1642295 from David Smiley in branch 'dev/trunk'\n[ https://svn.apache.org/r1642295 ]\n\nLUCENE-6031: Rename TokenStreamFromTermPositionVector to TokenStreamFromTermVector ",
            "date": "2014-11-28T13:38:18+0000"
        },
        {
            "id": "comment-14228298",
            "author": "ASF subversion and git services",
            "content": "Commit 1642297 from David Smiley in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1642297 ]\n\nLUCENE-6031: Optimize TokenSources term vector to TokenStream ",
            "date": "2014-11-28T13:46:29+0000"
        },
        {
            "id": "comment-14228299",
            "author": "ASF subversion and git services",
            "content": "Commit 1642298 from David Smiley in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1642298 ]\n\nLUCENE-6031: Rename TokenStreamFromTermPositionVector to TokenStreamFromTermVector ",
            "date": "2014-11-28T13:48:08+0000"
        },
        {
            "id": "comment-14228791",
            "author": "Yonik Seeley",
            "content": "Reopening - Solr highlighting tests do not pass after this commit. ",
            "date": "2014-11-29T15:32:53+0000"
        },
        {
            "id": "comment-14229002",
            "author": "David Smiley",
            "content": "Ouch; so sorry I failed the build!  In my checkout I have several pending issues related to highlighting, and apparently the Solr one, SOLR-6680, is dependent.  I should have monitored the dev list closely; I recall getting a nastygram from Jenkins when I failed the build in the past and thought I was in the clear since I didn't get one this time.\n\nThe coupling between this and SOLR-6680 is that TokenSources, prior to my commit here, did not require that you call reset().  This is of course a violation of the TokenSources contract which is unacceptable.  The patch to SOLR-6680 does several things to DefaultSolrHighlighter, one of which is ensuring reset() is called appropriately.  Since I've posted SOLR-6680 some time ago, I will commit within an hour or so, and thus fix the build.  I will also add a note in the upgrading section of LUCENE in case someone else might be forgetting to reset the stream returned from TokenStream. ",
            "date": "2014-11-30T03:22:55+0000"
        },
        {
            "id": "comment-14229013",
            "author": "ASF subversion and git services",
            "content": "Commit 1642512 from David Smiley in branch 'dev/trunk'\n[ https://svn.apache.org/r1642512 ]\n\nLUCENE-6031: don't call deprecated TokenSources method (in tests) ",
            "date": "2014-11-30T04:34:16+0000"
        },
        {
            "id": "comment-14229014",
            "author": "ASF subversion and git services",
            "content": "Commit 1642513 from David Smiley in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1642513 ]\n\nLUCENE-6031: don't call deprecated TokenSources method (in tests) ",
            "date": "2014-11-30T04:35:10+0000"
        },
        {
            "id": "comment-14229016",
            "author": "David Smiley",
            "content": "Hmm; Solr's CHANGES.txt is the one with an upgrading section, not Lucene. Anyway, I think I won't add the note after all; Solr was in error by not calling reset() in the first place.  Though if you/anyone think adding a note in CHANGES.txt that reset() wasn't but now is required then I will. ",
            "date": "2014-11-30T04:48:08+0000"
        },
        {
            "id": "comment-14248267",
            "author": "David Smiley",
            "content": "The attached patch further improves this by:\n\n\tdelaying lazy-init to first incrementToken() call instead of at reset().  It seems people call addAttributes before and after reset() and so the only way to know if consumers wants payloads is to wait till incrementToken().\n\t\n\t\ta consequence of the implementation is that reset() is no longer required, which is how it used to be for this TokenStream.  Obviously it should be called generally, it's just that this one will still work if you don't.\n\t\n\t\n\tPayload data is managed in a BytesRefArray which reduces overall memory use and object count (reduces GC burden).\n\n ",
            "date": "2014-12-16T14:00:54+0000"
        },
        {
            "id": "comment-14253767",
            "author": "David Smiley",
            "content": "This patch further improves slightly by using an AttributeFactory WITH PackedTokenAttributeImpl, and thus reducing the memory overhead involved in captureState() since there will be one attribute impl (2 if payloads).\n\n//This attribute factory uses less memory when captureState() is called.\n  public static final AttributeFactory ATTRIBUTE_FACTORY =\n      AttributeFactory.getStaticImplementation(\n          AttributeFactory.DEFAULT_ATTRIBUTE_FACTORY, PackedTokenAttributeImpl.class);\n\n\nI pass that via the constructor, super(ATTRIBUTE_FACTORY).\n\nI'll commit in a couple days or so. ",
            "date": "2014-12-19T18:43:59+0000"
        },
        {
            "id": "comment-14253770",
            "author": "Robert Muir",
            "content": "This issue is marked resolved. Can you please open a new issue for other changes? ",
            "date": "2014-12-19T18:45:16+0000"
        },
        {
            "id": "comment-14253778",
            "author": "David Smiley",
            "content": "Hi Rob.\nSince it hasn't been released yet, I'll simply re-open.  The patch is an incremental improvement; no change in scope. ",
            "date": "2014-12-19T18:47:51+0000"
        },
        {
            "id": "comment-14256582",
            "author": "ASF subversion and git services",
            "content": "Commit 1647479 from David Smiley in branch 'dev/trunk'\n[ https://svn.apache.org/r1647479 ]\n\nLUCENE-6031: TokenStreamFromTermVector: move lazy init to incrementToken, hold payloads more efficiently; use PackedTokenAttributeImpl (save mem) ",
            "date": "2014-12-23T04:14:16+0000"
        },
        {
            "id": "comment-14256583",
            "author": "ASF subversion and git services",
            "content": "Commit 1647480 from David Smiley in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1647480 ]\n\nLUCENE-6031: TokenStreamFromTermVector: move lazy init to incrementToken, hold payloads more efficiently; use PackedTokenAttributeImpl (save mem) ",
            "date": "2014-12-23T04:17:02+0000"
        },
        {
            "id": "comment-14332732",
            "author": "Anshum Gupta",
            "content": "Bulk close after 5.0 release. ",
            "date": "2015-02-23T05:01:37+0000"
        }
    ]
}