{
    "id": "LUCENE-3888",
    "title": "split off the spell check word and surface form in spell check dictionary",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/spellchecker"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "The \"did you mean?\" feature by using Lucene's spell checker cannot work well for Japanese environment unfortunately and is the longstanding problem, because the logic needs comparatively long text to check spells, but for some languages (e.g. Japanese), most words are too short to use the spell checker.\n\nI think, for at least Japanese, the things can be improved if we split off the spell check word and surface form in the spell check dictionary. Then we can use ReadingAttribute for spell checking but CharTermAttribute for suggesting, for example.",
    "attachments": {
        "LUCENE-3888.patch": "https://issues.apache.org/jira/secure/attachment/12519038/LUCENE-3888.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-03-20T07:54:28+0000",
            "content": "The patch cannot be compiled now because I changed the return type of the method in Dictionary interface but all implemented classes have not been changed.\n\nPlease give some comment because I'm new to spell checker. If no problem to go, I'll continue to work. ",
            "author": "Koji Sekiguchi",
            "id": "comment-13233286"
        },
        {
            "date": "2012-03-20T08:07:11+0000",
            "content": "Koji: hmm I think the problem is not in the Dictionary interface (which is actually ok),\nbut instead in the spellcheckers and suggesters themselves?\n\nFor spellchecking, I think we need to expose more Analysis options in Spellchecker:\ncurrently this is actually hardcoded at KeywordAnalyzer (it uses NOT_ANALYZED). \nInstead I think you should be able to pass Analyzer: we would also\nhave a TokenFilter for Japanese that replaces term text with Reading from ReadingAttribute.\n\nIn the same way, suggest can analyze too. (LUCENE-3842 is already some work for that, especially\nwith the idea to support Japanese this exact same way).\n\nSo in short I think we should:\n\n\tcreate a TokenFilter (similar to BaseFormFilter) which copies ReadingAttribute into termAtt.\n\trefactor the 'n-gram analysis' in spellchecker to work on actual tokenstreams (this can\n  also likely be implemented as tokenstreams), allowing user to set an Analyzer on Spellchecker\n  to control how it analyzes text.\n\tcontinue to work on 'analysis for suggest' like LUCENE-3842.\n\n\n\nNote this use of analyzers in spellcheck/suggest is unrelated to Solr's current use of 'analyzers' \nwhich is only for some query manipulation and not very useful. ",
            "author": "Robert Muir",
            "id": "comment-13233291"
        },
        {
            "date": "2012-03-20T08:37:10+0000",
            "content": "Here is a simple prototype of what I was suggesting, allows you to specify Analyzer to SpellChecker.\n\nThis Analyzer converts the 'surface form' into 'analyzed form' at index and query time: at index-time it forms n-grams based on the analyzed form, but stores the surface form for retrieval.\n\nAt query-time we have a similar process: the docFreq() etc checks are done on the surface form, but the actual spellchecking on the analyzed form.\n\nThe default Analyzer is null which means do nothing, and the patch has no tests, refactoring, or any of that. ",
            "author": "Robert Muir",
            "id": "comment-13233305"
        },
        {
            "date": "2012-03-20T08:38:45+0000",
            "content": "fix the obvious reset() problem... the real problem is I need to reset() my coffee mug. ",
            "author": "Robert Muir",
            "id": "comment-13233306"
        },
        {
            "date": "2012-03-24T06:18:47+0000",
            "content": "I added a test for the surface analyzer. I also added code for the analyzer in Solr.\n\nCurrently, due to classpath problem, the test cannot be compiled. I should dig in, but if someone could, it would be appreciated. ",
            "author": "Koji Sekiguchi",
            "id": "comment-13237452"
        },
        {
            "date": "2012-03-24T06:30:14+0000",
            "content": "This is excellent, Koji and Robert.  We should be able to do basic spellchecking for Japanese with this. ",
            "author": "Christian Moen",
            "id": "comment-13237457"
        },
        {
            "date": "2012-03-24T06:30:42+0000",
            "content": "The test itself is not good. ",
            "author": "Koji Sekiguchi",
            "id": "comment-13237458"
        },
        {
            "date": "2012-03-24T17:44:05+0000",
            "content": "lemme see if I can help with the test. I feel bad I didn't supply one with the prototype patch.\n\nAbout the Solr integration: this looks good! We can use a similar approach for autosuggest, too,\nso this could configure the analyzer for LUCENE-3842.\n\nI wonder if we should allow separate configuration of \"index\" and \"query\" analyzers? I know\nI came up with some use-cases for that for autosuggest, but I'm not sure about spellchecking.\nI guess it wouldn't be overkill to allow it though. ",
            "author": "Robert Muir",
            "id": "comment-13237614"
        },
        {
            "date": "2012-03-25T15:46:33+0000",
            "content": "I updated the patch and fixed Koji's test, its passing BUT there is a nocommit:\n\n// nocommit: we need to fix SuggestWord to separate surface and analyzed forms.\n// currently the 're-rank' is based on the surface forms!\nspellChecker.setAccuracy(0F);\n\n\n\nTo explain with the Japanese case how the patch currently works, the spellchecker has two phases:\n\n\tPhase 1: n-gram approximation phase. Here we generate a n-gram boolean query on the Readings. This is working fine.\n\tPhase 2: re-rank phase. Here we take the candidates from Phase 1 and do a real comparison (e.g. Levenshtein) to give them the final score. The problem is this currently uses surface form!\n\n\n\nI think phase 2 should re-rank based on the 'analyzed form' too? Inside spellchecker itself, I don't think this is very difficult, when analyzed != surface, we just store it for later retrieval.\n\nThe problem is the spellcheck comparison APIs such as SuggestWord don't even have any getters or setters and present no way for me to migrate to surface+analyzed in any backwards compatible way...\n\nI'll think about this in the meantime. Maybe we should just break and cleanup these APIs since its a contrib module and they are funky?  ",
            "author": "Robert Muir",
            "id": "comment-13237895"
        },
        {
            "date": "2012-03-25T16:22:05+0000",
            "content": "updated patch (note with this one: Solr does not yet compile).\n\nI went the route of trying to clean up these apis correctly: I think there are serious problems here.\n\nThe biggest violation is stuff like:\n\n// convert to array string: \n// nocommit: why don't we just return SuggestWord[] with all the information?\n// consumers such as Solr must be recomputing this stuff again?!\nString[] list = new String[sugQueue.size()];\nfor (int i = sugQueue.size() - 1; i >= 0; i--) {\n list[i] = sugQueue.pop().getSurface();\n}\n\nreturn list;\n\n\n\nDirectSpellChecker already returns all this data, I think its doing the right thing, but I think SpellChecker should be fixed. Even for the normal case surely we are recomputing docFreq etc on all the candidates which is wasteful.\n\nI'll keep plugging away but it seems like this will be a pretty serious refactoring (including e.g. distributed spellcheck refactoring) and difficult for 3.6. ",
            "author": "Robert Muir",
            "id": "comment-13237907"
        },
        {
            "date": "2012-03-25T17:06:25+0000",
            "content": "In my opinion we should set this as fix for 4.0\n\nThe only option for 3.6 would be something like my previous patch \n(https://issues.apache.org/jira/secure/attachment/12519860/LUCENE-3888.patch) which \nhas the disadvantages of doing the second-phase re-ranking on surface forms.\n\nAny other opinions? ",
            "author": "Robert Muir",
            "id": "comment-13237934"
        },
        {
            "date": "2012-03-26T07:42:50+0000",
            "content": "Thanks Robert for giving some patches and comment.\n\n\nThe only option for 3.6 would be something like my previous patch\n(https://issues.apache.org/jira/secure/attachment/12519860/LUCENE-3888.patch) which\nhas the disadvantages of doing the second-phase re-ranking on surface forms.\n\nWith the disadvantages, the spell checker won't work well for Japanese anyway. I give up this for 3.6. ",
            "author": "Koji Sekiguchi",
            "id": "comment-13238178"
        },
        {
            "date": "2012-03-26T13:55:51+0000",
            "content": "Thanks for the feedback Koji.\n\nI'm not happy with the situation: I thought it would be easy to support\nsome rough Japanese spellcheck in 3.6 \n\nBut it just seems like we need to do a lot of cleanup to make it work,\nI would rather fix all of these APIs and do it right the first time so\nthat things like distributed support work too. ",
            "author": "Robert Muir",
            "id": "comment-13238397"
        },
        {
            "date": "2012-07-11T23:03:44+0000",
            "content": "bulk cleanup of 4.0-ALPHA / 4.0 Jira versioning. all bulk edited issues have hoss20120711-bulk-40-change in a comment ",
            "author": "Hoss Man",
            "id": "comment-13412298"
        },
        {
            "date": "2012-08-07T03:41:22+0000",
            "content": "rmuir20120906-bulk-40-change ",
            "author": "Robert Muir",
            "id": "comment-13429699"
        },
        {
            "date": "2013-07-23T18:44:53+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13717088"
        },
        {
            "date": "2014-04-16T12:54:26+0000",
            "content": "Move issue to Lucene 4.9. ",
            "author": "Uwe Schindler",
            "id": "comment-13970753"
        }
    ]
}