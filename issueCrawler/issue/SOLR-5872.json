{
    "id": "SOLR-5872",
    "title": "Eliminate overseer queue",
    "details": {
        "affect_versions": "None",
        "status": "Open",
        "fix_versions": [],
        "components": [
            "SolrCloud"
        ],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "The overseer queue is one of the busiest points in the entire system. The raison d'\u00eatre of the queue is\n\n\tProvide batching of operations for the main clusterstate,json so that state updates are minimized\n\tAvoid race conditions and ensure order\n\n\n\nNow , as we move the individual collection states out of the main clusterstate.json, the batching is not useful anymore.\n\nRace conditions can easily be solved by using a compare and set in Zookeeper. \n\nThe proposed solution  is , whenever an operation is required to be performed on the clusterstate, the same thread (and of course the same JVM)\n\n\n\tread the fresh state and version of zk node\n\tconstruct the new state\n\tperform a compare and set\n\tif compare and set fails go to step 1\n\n\n\nThis should be limited to all operations performed on external collections because batching would be required for others",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Yonik Seeley",
            "id": "comment-13937842",
            "date": "2014-03-17T14:11:28+0000",
            "content": "as we move the individual collection states out of the main clusterstate.json [...]\n\nThis will make a difference on clusters with many smaller collections, but not on the single big collection.\nIt seems like we still want scalability in both directions (wrt number of collections, and the size a single collection can be). "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13937901",
            "date": "2014-03-17T15:12:23+0000",
            "content": "I'm not fully sold on this yet. Compare and set is how this was first implemented and it has it's own issues - hence the work Sami did to move to the queue. \n\nPotter has noticed the overseer is fairly slow at working through state updates. I think that should be investigated first.  "
        },
        {
            "author": "Jessica Cheng Mallet",
            "id": "comment-13938075",
            "date": "2014-03-17T17:27:13+0000",
            "content": "Seems like everyone is worried about batching. I think it'd be interesting to add logging/ stats tracking and experiment on a large cluster to see how much batching is actually achieved.\n\nThere are a few things I worry about with the current implementation:\n\n\tWith the overseer queues, each state update is 4+ zookeeper writes: 1 enqueue to stateUpdateQueue, 1 enqueue to workqueue, 1 state update write (potentially batched), 1 dequeue from stateUpdateQueue, and 1 dequeue from workqueue--not to mention that each core going through a restart could generate quite a few state updates (down, potentially isLeader switch, recovering, up) and each node can contain multiple cores.\n\tEmpirically, we have definitely seen the workqueue back up with lots of items during a node bounce--but of course this can be due to some bug that's causing Potter to notice the slowness.\n\tIf batching really is so important, there's no batching for external collection state updates.\n\tIn a \"normal\" rolling bounce where instances are restarted one-by-one, in the same order each time, the Overseer is killed at each instance restart, thus hindering the recovery process by gating state transition. (Here there are workarounds by playing with bounce orders, etc., but I would argue that in any organization that would have a cluster large enough to worry about this, there is most likely a system that governs the machines and normally does instance 1 to N bounces, and a general-purpose ops team that eschews service-/app-specific bounce instructions.)\n\n\n\nWith all that said, I would really appreciated it to have more background details about what problems Mark and Sami has seen in the old implementation, and exactly what that old implementation was. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13938116",
            "date": "2014-03-17T17:58:14+0000",
            "content": "Thanks Jessica - your comment is more along the lines of what you need to argue to make a large change like this. Specifics.\n\nI don't have time to write a detailed answer at the moment, but a lot of my reservations are around the large refactoring that is being attempted to support tons of collections. So far, I have not been super happy with a lot of the work that has been done. Much of it seems hurried, existing tests have not been beefed up in critical areas, new tests have been fairly minimal, and so I'm likely to push back on many of these issues. We have too many stability issues to tackle as it is. Abandoning code that has been getting hardened for over a year now for a approach that was already abandoned should not be done lightly. \n\nIf someone makes a thoughtful and clear argument with specifics and then makes a thoughtful, well tested implementation, I'm much more likely to get on board.\n\nI'll respond to the technical points when I get some time. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13938185",
            "date": "2014-03-17T18:39:28+0000",
            "content": "People suggest new changes to the system when /where they think it is required. It is important that we counter suggestions on their own merits/demerits. \n\nI'm sure you Mark Miller /Sami would have abandoned the idea because of some real issues. I would love to hear them out (when you have time) .The issues may not me insurmountable  . But , the point is , looking at the code the Overseer queue is seen as quite a bottleneck and this is the solution that immediately comes to ones mind. \n\nAnyone who can build up a patch will be a good demonstration of the possibility of such a solution. People who are testing out their systems in real test environment will be able to provide invaluable feedback on the viability/issues with the solution. As developers,  we need to guide/handhold the users who are pushing the envelope . At some point when we develop enough confidence we can integrate it into the product itself . \n\nIt seems like we still want scalability in both directions (wrt number of collections, and the size a single collection can be).\n\nYes, in the current system scaling with multiple collections is much simpler and a first baby step towards breaking the monolithic clusterstate.json . Eventually we would like to go to a state per slice so that we can support very large collections. But these new experiments need to be tried out first before we venture into larger ones "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13938498",
            "date": "2014-03-17T22:39:42+0000",
            "content": "\nPeople suggest new changes to the system when /where they think it is required. It is important that we counter suggestions on their own merits/demerits.\n\n\n\nOf course - and given this issue as presented, as I said \"I'm not fully sold on this yet.\" The other background I gave also applies to all of these issues. We won't just rip out tons of code and replace it just because someone has identified an issue and proposed a solution. The bar for this type of change should be high. Given the history of these changes, I'm going to have to be sold more than if the history was better. Each contributor is also judged on their merit - what have they contributed so far, what's the quality of their contributions, how much are they helping with test fails, etc. The more merit you have on SolrCloud, the more likely these large scale refactorings will receive support.\n\n\nAs developers, we need to guide/handhold the users who are pushing the envelope . At some point when we develop enough confidence we can integrate it into the product itself .\n\n\n\nWe don't necessarily need to hold hands - we need to take that on a case by case basis. We need to walk before we can run. We should probably jog before we run as well.\n\nIt's an issue by issue thing though, and Jessica has already begun providing a case for looking at this.\n\nAt some point when we develop enough confidence we can integrate it into the product itself .\n\nI've heard that before and I'm not totally sold that's how things will play out. Certainly they would not play out that way without some push back on these issues IMO. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13938539",
            "date": "2014-03-17T23:23:52+0000",
            "content": "With the overseer queues, each state update is 4+ zookeeper writes\n\nGiven the numbers I've seen published for ZK performance, it seems like that should not be a big deal in typical cases?\n\nEmpirically, we have definitely seen the workqueue back up with lots of items during a node bounce\n\nI'm not surprised - most of this code has not been optimized or investigated thoroughly. The original author of a lot of the Overseer code has moved on and it likely has not seen as much attention as would be nice over the past year. Until someone looks into the current issues closely though, it seems hard to recommend rewriting this whole very important piece.\n\nIf batching really is so important, there's no batching for external collection state updates.\n\nI'm not really fully up on \"external collections\" but AFAIK it's part of some other work to support tons of collections that I'm not fully sold on yet either \n\nIn a \"normal\" rolling bounce where instances are restarted one-by-one, in the same order each time, the Overseer is killed at each instance restart, thus hindering the recovery process by gating state transition.\n\nThis points out another issue that we might be able to address.\n\nWithout having looked closely at the issues brought up (and I don't see evidence anyone else has either), it's hard to draw the conclusion the whole thing just has to be replaced yet.\n\nA couple issues around the old implementation:\n\n\n\tWith every node updating the whole cluster state on state change, the clusterstate.json file is read far too much. The workaround you guys are proposing for that appears to be only having clients update the clusterstate when they run into an error - but I'm not sold that that is the best architecture for the future either. That's a complicated change to make, with many ramifications for future development.\n\n\n\n\n\tSome things that are in the clusterstate now and that could be in the future are not so easily handled with the non overseer strategy - like marking who is the leader. You have to have the Overseer running its own special thread to inject and remove information.\n\n\n\n\n\tAs things are, on something like cluster startup, there will be tons of reads and writes of the clusterstate.json - a flood of attempts and retries to update it in ZooKeeper.\n\n\n\nFor further discussion around the change, there should be background if you search the archives.\n\nThere is a strong argument to be made that we should first investigate the performance issues with the current strategy. ZooKeeper is pretty fast - these state updates are tiny and batched. It seems like we should be able to do a lot better without throwing out code that has been getting hardened for a long time now.\n "
        },
        {
            "author": "Jessica Cheng Mallet",
            "id": "comment-13938670",
            "date": "2014-03-18T01:43:59+0000",
            "content": "For further discussion around the change, there should be background if you search the archives.\nIf you wouldn't mind terribly, will you please paste the link of a few relevant threads in the archive? (Sorry, I'm not familiar with all the keywords and archives, etc., yet.)\n\nThere is a strong argument to be made that we should first investigate the performance issues with the current strategy. ZooKeeper is pretty fast - these state updates are tiny and batched. It seems like we should be able to do a lot better without throwing out code that has been getting hardened for a long time now.\nI see where your hesitation is now, and I can definitely agree. Sounds like there are a few points to be investigated for the current system before we attempt to change anything:\n\n\n\tWhy is the Overseer's so slow at updating cluster state/ What's causing the build-up of queue messages during a restart?\n\tWhat can we do to generally solve the problem of the Overseer being killed on every instance restart in a rolling bounce?\n\tHow much is actually batched?\n\n\n\nMy gut is that for external collections, batching won't be of that much benefit (except for that super-large collection case that Yoink mentioned), but I agree that if the current system can be hardened to work even for those, then the simplicity of one code path should be preferred over ultra-optimizing for a non-issue (assuming the first two points above can be \"fixed\"). "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13938687",
            "date": "2014-03-18T02:09:20+0000",
            "content": "\nSome things that are in the clusterstate now and that could be in the future are not so easily handled with the non overseer strategy - like marking who is the leader. You have to have the Overseer running its own special thread to inject and remove information.\n\n\n\nTo expand on this one a bit - you can obviously have each node essentially do what the overseer does now - to know the true shard leader that means things like going to ZooKeeper though - so for a large cluster, as each node takes on all the duties of the overseer and every node is now hitting zookeper for this and that, and then each node is trying update the clusterstate.json at the same time and retrying, and you have this contentious herd pilling onto this one zookeeper node.\n\nThe Overseer was seen as a fairly elegant way to avoid this herd effect and provide a less chatty solution. Rather than all the retries and reading the state on every state change, everyone writes to a non contentious zk node, the Overseer batches up the info and writes out the state.\n\nNow if we cannot make it fast enough because of fundamental limitations, that is one thing. But gosh, on the surface, these state updates are so small and ZK is fairly performant...\n\nWe should identify the bottlenecks.\n\nFor startup, one random idea is to look at using zk's multi call support to read the whole queue in one request and then batch it all.\n\nI've got some other common sense ideas as well, but will have to find out the choke points before it makes a lot of sense brainstorming solutions. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13938710",
            "date": "2014-03-18T02:40:05+0000",
            "content": "There is a strong argument to be made that we should first investigate the performance issues with the current strategy.\n\nThe current implementation of DistributedQueue.peek() is extremely expensive. It reads all the children and sort them and return one item from the head and discard all others.  There can be  a new method DistributedQueue.peek where n is the number of items and Overseer can process them all in one batch .\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13938717",
            "date": "2014-03-18T02:53:18+0000",
            "content": "I'm not really fully up on \"external collections\" but AFAIK it's part of some other work to support tons of collections\n\nThere is more and more information getting added to the cluster state . I'm sure no one would object to the point that splitting the clusterstate.json would be a more scalable solution and the right direction to take. Of course, this is not to be done in haste , but eventually that should be the way.  The eventual goal should be to support very large no:of collections (say 1000's) and support extremely large collections (with 1000's of slices) . Solr itself will not have any problem scaling like that but the Overseer/clusterstate strategy will go through a revamp before we reach there.   "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13938751",
            "date": "2014-03-18T03:34:58+0000",
            "content": "If it's the issue about breaking up clusterstate.json per collection, I don't necessarily think that's a bad idea. I didn't realize that would make it something called an \"external\" collection though. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13938755",
            "date": "2014-03-18T03:44:02+0000",
            "content": "The \"external\" collection is just a name. It really does not matter what we call them. The idea is to split the same data out to smaller state nodes .  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13938775",
            "date": "2014-03-18T04:01:08+0000",
            "content": "It's obviously just a name  I didn't know that it existed - that's all I was saying - I figured it meant something else. To me it doesn't make much sense. I think if we decide to split out the clusterstate.json per collection, that is the direction we should take, we should only support one clusterstate.json for back compat at most, and no such special name should exist. Solr 5.0 would no longer support the single clusterstate.json. Or, we might even decide to have the Overseer upgrade the format for you or something before 5.0.\n\nOther thoughts on Overseer performance:\n\n\n\tBecause only one process should be reading and removing items from the distributed queue at a time, seems like there are many cases we could read multiple nodes in one call.\n\n\n\n\n\tPerhaps 1500ms is not a great batch time - would be interesting if we made it configurable as well.\n\n\n\n\n\tSeems there might be a lot of room for parallelism - we probably only need to order within a collection if not simply per SolrCore.\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13938796",
            "date": "2014-03-18T04:26:10+0000",
            "content": "I think if we decide to split out the clusterstate.json per collection, that is the direction we should take\n\nYes, that is the plan\n\nwe would probably switch to that from 5.0 or something. But the challenge is to offer a smother migration path. Till then we need a name to differentiate both modes\n\n\tinitially , users would be able to switch to that mode when creating a collection (an opt In) SOLR-5473 does that\n\toffer an API to migrate to the new format  SOLR-5756\n\tMake it the default format (from say 5.0)\n\tdeprecate the old format\n\n\n "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13938943",
            "date": "2014-03-18T08:09:21+0000",
            "content": "\nas we move the individual collection states out of the main clusterstate.json [...]\n\nThis will make a difference on clusters with many smaller collections, but not on the single big collection.\nIt seems like we still want scalability in both directions (wrt number of collections, and the size a single collection can be).\n\nThe best solution that I see here is to move the replica states out into their own ZK nodes. This way the individual nodes can update them directly without the overseer via compare and set operations. The rest of the operations can continue to be processed in the overseer. If we do this, even the external collection changes may not be required. The leader information can also be read directly from the leader election nodes. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13939413",
            "date": "2014-03-18T15:58:07+0000",
            "content": "is to move the replica states out into their own ZK nodes.\n\nThat is also how I first implemented the clusterstate - it was super slow to read the state and required a ridiculous number of watchers. Now that they have some options to read multiple nodes in one call, it may be that you can work around some of the issues we had, but it was really only good for the case where you had small changes in state to read - users had real issues with performance otherwise and that is why we moved to clusterstate.json.\n\nIt's a similar issue - we have been there before, we moved because of tough issues, it's should be a high bar to go back. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13939422",
            "date": "2014-03-18T16:02:53+0000",
            "content": "That is also how I first implemented the clusterstate\n\nCan you throw some light on how was the ZK schema for your initial impl? If all nodes of a given slice is under one zk directory , one watch on the parent should be fine, right? "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-13939824",
            "date": "2014-03-18T21:24:14+0000",
            "content": "Wasn't one of the ideas considered in one of the other tickets to 'shard' the cluster state into N pieces so that we can hit a sweet spot between number of watchers and contention? Is that dead in the water now? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13940787",
            "date": "2014-03-19T18:23:01+0000",
            "content": "Is that dead in the water now?\n\nNo. It's got it's own issue, and it seems likely to happen to me.\n\nEven this issue is not \"dead in the water\". Things are generally determined via discussion and consensus. I'm arguing that we should look at simple performance bottleneck and improvements to the current system - there seems to be a lot of low hanging fruit.\n\n\nCan you throw some light on how was the ZK schema for your initial impl? If all nodes of a given slice is under one zk directory , one watch on the parent should be fine, right?\n\n\n\nIt's been a long time and we had a few variations, so I'd have to go back in the code to refresh my memory. For now, from my memory:\n\nInitially I had it to that we simply watched the parent - Loggly ran into performance issues with this - even when only one entry changed, they had so many entries that updating the state with so many nodes reading so many entries, the performance was a big problem for them. They hacked around it initially, and then we moved to watching each entry eventually - this made small updating state for small changes very efficient. But then another big early user was still hitting performance issues simply from having to read so many entries on startup and such. This is what prompted the move to a single clusterstate.json.\n\nIt's hard to remember it all perfectly - the info is spread across and around a lot of old JIRAs. Non of the changes were taken lightly, and a variety of developers and contributors were generally involved in the discussion or motivating changes via their needs.\n\nThere are tradeoffs with all of these approaches. "
        },
        {
            "author": "Scott Blum",
            "id": "comment-14658807",
            "date": "2015-08-05T20:11:17+0000",
            "content": "Now that SOLR-5756 is close to landed, I want to take a serious stab at making updates to format2 collections not go through overseer.  IE, anything that modifies clusterstate.json goes through overseer, but anything that modifies a /collection/foo/state.json would be handled by the local node with a CAS loop.  I realize that for a collection with a huge number of shards+replicas, there could be contention on that single node.  Worth nothing that the current implementation doesn't batch format2 updates anyway, it ends up doing a (non-contended) write for every individual mutation. "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14698435",
            "date": "2015-08-15T20:26:51+0000",
            "content": "Though I haven't done serious experiments on this as yet, I see the lack of batching in stateFormat=2 is a potential blocker to it's adoption. We need some benchmarks on a single collection with lots of cores (at least 1000), and see how it works with stateFormat=1, stateFormat=2, and this new approach. Keep in mind that hundreds of cores might change state at the same time, that's the real benefit to batching. I fear that without a batching approach, the system might choke due to the contention at that point.\n\nMy point here being that stateFormat=2 not doing batching isn't a convincing enough argument to eliminate overseer queue, may be the effort should be directed more towards getting batching for stateFormat=2 if that's more useful. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14698494",
            "date": "2015-08-15T23:56:00+0000",
            "content": "as Ramkumar Aiyengar said, batching offers serious benefits. stateformat=2 really does not matter . A collection with a lot of shards is more likely than a large no:of collections. Without batching , it will have the same bottleneck. The batching is not for writing to ZK, it is for reading from ZK. If there are 1000s of cores reading every single update to the state.json we are back to square one. \n\nWe will need to do some serious benchmarking to prove the performance of that it is worth it "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14699051",
            "date": "2015-08-17T06:00:01+0000",
            "content": "I see the lack of batching in stateFormat=2 is a potential blocker to it's adoption. We need some benchmarks on a single collection with lots of cores (at least 1000), and see how it works with stateFormat=1, stateFormat=2, and this new approach\n\nA single collection with lots of cores will perform similarly with both stateFormat=1 and stateFormat=2 because updates in stateFormat=2 are also batched as long as consecutive updates are for the same collection. "
        },
        {
            "author": "Scott Blum",
            "id": "comment-14700144",
            "date": "2015-08-17T20:08:22+0000",
            "content": "At the risk of creating two code paths, here's an idea.\n\n1) We could improve batching significantly at the Overseer level, to be able to batch even when the same collection isn't updated twice in a row.  We just need something like a dirty list instead of only tracking the last one and the shared clusterStateModified.  This could be an independent improvement.\n\n2) When performing updates on format=2, we could use a size heuristic to decide whether or not to go through the queue.  For collections with less than N shards, we could just do a local CAS loop for state update ops.  For collections with more than N shares we'd just always go through the queue. "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14700181",
            "date": "2015-08-17T20:39:40+0000",
            "content": "(1) sounds like a good idea..\n\nOn (2), to either have people decide on the approach, or have Solr do it, we would need to know the perf characteristics of both approaches. So maintaining two implementations or not really comes down to what we are trading off against. Again, only some serious benchmarking can answer that.. "
        },
        {
            "author": "Scott Blum",
            "id": "comment-14700191",
            "date": "2015-08-17T20:46:26+0000",
            "content": "Agreed.  The idea behind #2 is to serve two very different kind of configurations.\n\na) Avoiding the overseer queue for small shard count is an optimization for deployments that have a huge number of collections, but each collection has very few shards.  The process of starting up and shutting down is more efficient because each collection can be updated in a distributed manner.\n\nb) Using the overseer queue for big shard count is an optimization for deployments that have few collections, but each collection has a huge number of shards. "
        },
        {
            "author": "albert vico oton",
            "id": "comment-15890021",
            "date": "2017-03-01T11:44:42+0000",
            "content": "Hello, we are currently trying to do a deploy of around 200 collections and solrcloud can't handle it, it just  dies due update_status messages propagation everytime we try to add a new collection, each collection has 3 replicas, and sizes are not very large. Also, I do not see why collection A should be aware of collection B state.  \n\nBut moving to the topic, overseer node dies since he can not handle all the stress due the flooding of messages. IMHO we have here a single point of failure in a distributed system, which is not very recommended. \n\nsince it would be useful for big fat shards, my suggestion would be to make this optional behavior, so people like us, who need to have a more distributed approach, can make use of solrcloud. Since right now it is impossible to. and I'm not talking about \"thousands\" of collections actually with as few as 100 we are seeing very bad performance.\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-15900731",
            "date": "2017-03-08T05:23:39+0000",
            "content": "it's time to split the \"state\" from \"state.json\" into a separate file. replica-status.json\n\n\n{\n// 0:DOWN\n// 1: ACTIVE\n// 2: RECOVERING\n\n\"replica1\": 1\n\"replica2\": 1\n}\n\n\n\nSo every core watches 2 files instead of one and 99% of changes happen to the replica-status.json\n\nThis can help us scale to a very large no:of of shards "
        },
        {
            "author": "Mark Miller",
            "id": "comment-15900838",
            "date": "2017-03-08T07:25:56+0000",
            "content": "Hello, we are currently trying to do a deploy of around 200 collections\n\nWith which version? "
        },
        {
            "author": "albert vico oton",
            "id": "comment-15902928",
            "date": "2017-03-09T11:51:07+0000",
            "content": "We were doing our tests with solr 6.4.1 "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-15903303",
            "date": "2017-03-09T16:16:27+0000",
            "content": "bq: \"Also, I do not see why collection A should be aware of collection B state.\"\n\nWhat is your evidence of this? Because this was changed quite a while ago. Originally there was a single clusterstate.json that held all of the state information for all the collections, but that changed to having a state.json in each collections' Znode. So either you're misinterpreting something or somehow using an older style Zookeeper state. Or something really strange is happening.\n\nSee the collections API MIGRATESTATEFORMAT.\n "
        },
        {
            "author": "albert vico oton",
            "id": "comment-15903394",
            "date": "2017-03-09T17:00:06+0000",
            "content": "Already did that, but nodes still notify its state change, apparently collections need to know about other collections status in order to reroute queries to them, this amount of state change msg was killing our io in the ZK cluster, causing a lot of cpu wait time and effectively rendering the system unusable. But honestly that's as far as we went, we moved away from solrcloud and now we are using standalone solr inside each container for each of our collections and doing the balancing between replicas through nginx.  "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-15903557",
            "date": "2017-03-09T18:24:31+0000",
            "content": "We were doing our tests with solr 6.4.1\n\nThat version has other problems that may be clouding the issue (pun intended).  See SOLR-10130.  I advise an immediate upgrade to 6.4.2, to be sure that any problems you're encountering are actually caused by SolrCloud, not high CPU usage. "
        },
        {
            "author": "albert vico oton",
            "id": "comment-15903961",
            "date": "2017-03-09T22:17:48+0000",
            "content": "thanks for the advice, I'll take a look but the problem we were seeing was in Zookeeper cluster not in solr  "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-15905224",
            "date": "2017-03-10T15:10:52+0000",
            "content": "I don't know how this issue escaped my attention, especially since it's been around a few years.\n\nJessica Cheng Mallet mentioned early on in this issue that each state change results in four ZK writes.  When I opened SOLR-7191, I found that when any collection changed state, something was sent to the overseer queue for every collection.  If I remember right, this happens even when adding a new collection, which seems completely insane to me.\n\nWhen the number of collections gets large enough, Solr has a tendency to run into ZOOKEEPER-1162, because entries can be added to the overseer queue at a much faster rate than the overseer can process them.  During my testing on SOLR-7191 with version 5, Solr generated an overseer queue with 850,000 entries in it, resulting in a ZK packet size of 14 megabytes.\n\nI am not at all familiar with how SolrCloud's zookeeper code works.  Exploring that rabbit hole will take a pretty major time investment.  I've been reluctant to spend that time.  Other people do understand it, so I mostly just bounce ideas off of those people and ask questions.\n\nI'll take a look but the problem we were seeing was in Zookeeper cluster not in solr\n\nI don't see anything in your comment on 2017/03/01 that describes a problem with ZK.  It sounds like problems with Solr using ZK.  The overseer is a Solr component, it's not in ZK.  If SOLR-10130 is occurring on your system, then an upgrade to 6.4.2 will help. "
        },
        {
            "author": "albert vico oton",
            "id": "comment-15905254",
            "date": "2017-03-10T15:24:43+0000",
            "content": "Yea exactly, sorry if my comment was confusing but the problem is with the use that SolrCloud is doing of ZK not with ZK itself.\n\n When the number of collections gets large enough, Solr has a tendency to run into ZOOKEEPER-1162, because entries can be added to the overseer queue at a much faster rate than the overseer can process them. During my testing on SOLR-7191 with version 5, Solr generated an overseer queue with 850,000 entries in it, resulting in a ZK packet size of 14 megabytes. \n\nI believe that this is exactly what we were experiencing.  "
        },
        {
            "author": "Joshua Humphries",
            "id": "comment-15926351",
            "date": "2017-03-15T15:21:29+0000",
            "content": "I identified one issue with slowness in processing the overseer queue: a 'downnode' message can result in far more updates to ZK than necessary \u2013 mainly for clusters with many collections where any given collection only has shard-replicas on a small minority of the nodes. Our cluster has many thousands of collections, most of which have only one shard and one replica. So 'downnode' was updating about 40x more collections in ZK than actually necessary. Furthermore, all of the writes are done synchronously/sequentially which means we pay the RTT to ZK 40x more than necessary. (Also, writes across collections, even when state_format > 1, could be batched and pipelined, to further reduce latency here.)\n\nSee SOLR-10277 for more details. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-15927274",
            "date": "2017-03-16T01:19:45+0000",
            "content": "We actually try to batch the writes at overseer today if multiple subsequent update operations come for the same collection. because all the collections share the same queue, the benefits are not realized. The solution is to have a larger no:of of queues , say 1000 buckets (and as many threads). Each collection must be hashed to one of these buckets.This will help us improve the batching because there is a much higher probability of 2 subsequent events are for the same collection "
        },
        {
            "author": "Joshua Humphries",
            "id": "comment-15927301",
            "date": "2017-03-16T01:43:37+0000",
            "content": "Right, but when a node comes up and changes replica states to active, it is highly likely that the number of events for a single collection will be ~1. So breaking batches at collection boundaries results in effectively no batching.\n\nWith the current code, there's no benefit to combining writes for multiple collections into the same batch. But if the code pipelined all of the writes for a batch (instead of issuing each one synchronously, blocking for each result) then combining writes across collections would reduce latency.\n\nWhen you suggest partitioning the queue, do you mean multiple ZK queues? Seems simpler to just partition in memory: ingest the whole queue (or up to some limit) and push into in-memory queues (one per partition; could even explode a 'downnode' message into the multiple updates it implies and scatter those updates across partitions). After one of the in-memory partitions completes an item, it can delete the corresponding entry from ZK. So, from ZK's point of view, the operations can completing out-of-order instead of always polling the head of the queue. When partitions quiesce (or when some other policy allows more items to be polled \u2013 so we don't necessarily have to wait on all partitions to complete before grabbing more items), ingest another batch of items from ZK. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-15927335",
            "date": "2017-03-16T02:10:58+0000",
            "content": "When you suggest partitioning the queue, do you mean multiple ZK queues?\n\nIt helps in keeping the size of any given queue to be minimal if you have very large no:of collections. \n\nI'm open to the idea of doing an in memory partitioning. \n\nread large no:of items say 10000. put them into in memory buckets and feed them into overseer would be just fine as well and it could be an easy win  "
        },
        {
            "author": "David Smiley",
            "id": "comment-16106025",
            "date": "2017-07-29T04:19:12+0000",
            "content": "Is it really true that every Solr node subscribes/watches to ZK state changes to all collections all the time, even to Collections that have no replicas on the current node?  Albert's comment https://issues.apache.org/jira/browse/SOLR-5872?focusedCommentId=15890021&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15890021 indicate this is so.  I could understand doing this to query collections on different nodes but I think such watches should expire if not continuously utilized.  Is there another JIRA issue about this? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-16106028",
            "date": "2017-07-29T04:33:02+0000",
            "content": "Is it really true that every Solr node subscribes/watches to ZK state changes to all collections all the time\n\nA node only watches those states if it has a replica of that collection. "
        }
    ]
}