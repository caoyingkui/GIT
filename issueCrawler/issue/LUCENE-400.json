{
    "id": "LUCENE-400",
    "title": "NGramFilter -- construct n-grams from a TokenStream",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "fix_versions": [
            "2.4"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "This filter constructs n-grams (token combinations up to a fixed size, sometimes\ncalled \"shingles\") from a token stream.\n\nThe filter sets start offsets, end offsets and position increments, so\nhighlighting and phrase queries should work.\n\nPosition increments > 1 in the input stream are replaced by filler tokens\n(tokens with termText \"_\" and endOffset - startOffset = 0) in the output\nn-grams. (Position increments > 1 in the input stream are usually caused by\nremoving some tokens, eg. stopwords, from a stream.)\n\nThe filter uses CircularFifoBuffer and UnboundedFifoBuffer from Apache\nCommons-Collections.\n\nFilter, test case and an analyzer are attached.",
    "attachments": {
        "ASF.LICENSE.NOT.GRANTED--NGramAnalyzerWrapper.java": "https://issues.apache.org/jira/secure/attachment/12312647/ASF.LICENSE.NOT.GRANTED--NGramAnalyzerWrapper.java",
        "ASF.LICENSE.NOT.GRANTED--NGramAnalyzerWrapperTest.java": "https://issues.apache.org/jira/secure/attachment/12312649/ASF.LICENSE.NOT.GRANTED--NGramAnalyzerWrapperTest.java",
        "ASF.LICENSE.NOT.GRANTED--NGramFilter.java": "https://issues.apache.org/jira/secure/attachment/12312646/ASF.LICENSE.NOT.GRANTED--NGramFilter.java",
        "ASF.LICENSE.NOT.GRANTED--NGramFilterTest.java": "https://issues.apache.org/jira/secure/attachment/12312648/ASF.LICENSE.NOT.GRANTED--NGramFilterTest.java",
        "LUCENE-400.patch": "https://issues.apache.org/jira/secure/attachment/12373074/LUCENE-400.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2005-06-22T06:09:52+0000",
            "content": "Created an attachment (id=15504)\nNGramFilter ",
            "author": "Sebastian Kirsch",
            "id": "comment-12322452"
        },
        {
            "date": "2005-06-22T06:10:29+0000",
            "content": "Created an attachment (id=15505)\nNGramAnalyzerWrapper (wraps an NGramFilter around an analyzer.) ",
            "author": "Sebastian Kirsch",
            "id": "comment-12322453"
        },
        {
            "date": "2005-06-22T06:12:35+0000",
            "content": "Created an attachment (id=15506)\nJUnit TestCase for NGramFilter ",
            "author": "Sebastian Kirsch",
            "id": "comment-12322454"
        },
        {
            "date": "2005-06-29T10:26:46+0000",
            "content": "\n\t<p>For example, the sentence \"please divide this sentence into ngrams\" would be\n\ttokenized into the tokens \"please divide\", \"this sentence\", \"sentence into\", and\n\t\"into ngrams\".\n\n\n\nThe comment should read;\n\n\n\t<p>For example, the sentence \"please divide this sentence into ngrams\" would be\n\ttokenized into the tokens \"please divide\", \"divide this\", \"this sentence\",\n\"sentence into\", and\n\t\"into ngrams\".\n\n ",
            "author": "Robert Newson",
            "id": "comment-12322455"
        },
        {
            "date": "2005-07-29T21:56:59+0000",
            "content": "Created an attachment (id=15818)\nJUnit test class for NGramAnalyzerWrapper\n\nThe tests in this class are concerned with the interaction between QueryParser\nand an NGramAnalyzer, and whether searching works as expected on an index\nconstructed with an NGramAnalyzer.\n\nOne of the test cases throws an exception that I haven't investigated yet. So\nproceed with caution if you use the QueryParser with NGramAnalyzer. \n\n..E....\nTime: 1.771\nThere was 1 error:\n1)\ntestNGramAnalyzerWrapperPhraseQueryParsingFails(org.apache.lucene.analysis.NGramAnalyzerWrapperTest)java.lang.NullPointerException\n\n\tat\norg.apache.lucene.index.MultipleTermPositions.skipTo(MultipleTermPositions.java:178)\n\n\tat\norg.apache.lucene.search.PhrasePositions.skipTo(PhrasePositions.java:47)\n\tat org.apache.lucene.search.PhraseScorer.doNext(PhraseScorer.java:73)\n\tat org.apache.lucene.search.PhraseScorer.next(PhraseScorer.java:66)\n\tat org.apache.lucene.search.Scorer.score(Scorer.java:47)\n\tat\norg.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:102)\n\tat org.apache.lucene.search.Hits.getMoreDocs(Hits.java:65)\n\tat org.apache.lucene.search.Hits.<init>(Hits.java:44)\n\tat org.apache.lucene.search.Searcher.search(Searcher.java:40)\n\tat org.apache.lucene.search.Searcher.search(Searcher.java:32)\n\tat\norg.apache.lucene.analysis.NGramAnalyzerWrapperTest.queryParsingTest(NGramAnalyzerWrapperTest.java:75)\n\n\tat\norg.apache.lucene.analysis.NGramAnalyzerWrapperTest.testNGramAnalyzerWrapperPhraseQueryParsingFails(NGramAnalyzerWrapperTest.java:100)\n\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\n\tat\norg.apache.lucene.analysis.NGramAnalyzerWrapperTest.main(NGramAnalyzerWrapperTest.java:36)\n\n\nFAILURES!!!\nTests run: 6,  Failures: 0,  Errors: 1 ",
            "author": "Sebastian Kirsch",
            "id": "comment-12322456"
        },
        {
            "date": "2006-07-09T21:51:29+0000",
            "content": "Sebastian, ever figured out the problem?  Also, is there a way to get rid of the Commons Collections?  Lucene has no run-time dependencies on other libraries. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12419913"
        },
        {
            "date": "2006-08-07T21:14:35+0000",
            "content": "Hi Otis,\n\nI did not figure out the problem. Getting rid of Commons Collection should be no problem; I am just using them as FIFOs. However, I do not have the time at the moment to implement this.\n\nKind regards, Sebastian ",
            "author": "Sebastian Kirsch",
            "id": "comment-12426327"
        },
        {
            "date": "2008-01-12T22:56:43+0000",
            "content": "Lucene has NGram support ",
            "author": "Grant Ingersoll",
            "id": "comment-12558305"
        },
        {
            "date": "2008-01-13T05:36:33+0000",
            "content": "Lucene has character NGram support, but not word NGram support, which this filter supplies:\n\nThis filter constructs n-grams (token combinations up to a fixed size, sometimes called \"shingles\") from a token stream. ",
            "author": "Steve Rowe",
            "id": "comment-12558350"
        },
        {
            "date": "2008-01-13T13:33:59+0000",
            "content": "Good catch, Steve.  I will reopen, as a word based ngram filter is useful. ",
            "author": "Grant Ingersoll",
            "id": "comment-12558401"
        },
        {
            "date": "2008-01-14T04:15:11+0000",
            "content": "Repackaged these four files as a patch, with the following modifications to the code:\n\n\n\tRenamed files and variables to refer to \"n-grams\" as \"shingles\", to avoid confusion with the character-level n-gram code already in Lucene's sandbox\n\tPlaced code in the o.a.l.analysis.shingle package\n\tConverted commons-collections FIFO usages to LinkedLists\n\tRemoved @author from javadocs\n\tChanged deprecated Lucene API usages to alternate forms; addressed all compilation warnings\n\tChanged code style to conform to Lucene conventions\n\tChanged field setters to return null instead of a reference to the class instance, then changed instantiations to use individual setter calls instead of the chained calling style\n\tAdded ASF license to each file\n\n\n\nAll tests pass.\n\nAlthough I left in the ShingleAnalyzerWrapper and its test in the patch, no other Lucene filter (AFAICT) has such a filter wrapping facility.  My vote is to remove these two files. ",
            "author": "Steve Rowe",
            "id": "comment-12558495"
        },
        {
            "date": "2008-01-14T12:29:02+0000",
            "content": "Thanks, Steve.  I will mark this as 2.4 ",
            "author": "Grant Ingersoll",
            "id": "comment-12558585"
        },
        {
            "date": "2008-01-14T18:40:02+0000",
            "content": "Removed the duplicate link (to LUCENE-759), since that issue is about character-level n-grams, and this issue is about word-level n-grams. ",
            "author": "Steve Rowe",
            "id": "comment-12558717"
        },
        {
            "date": "2008-01-14T18:55:08+0000",
            "content": "Thanks for bringing this up to date.  I'll commit it after 2.3 is out. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12558728"
        },
        {
            "date": "2008-03-03T13:28:15+0000",
            "content": "ping, Otis, do you still plan to commit? ",
            "author": "Grant Ingersoll",
            "id": "comment-12574493"
        },
        {
            "date": "2008-03-18T17:20:42+0000",
            "content": "re-ping, Otis, do you still plan to commit? ",
            "author": "Steve Rowe",
            "id": "comment-12579933"
        },
        {
            "date": "2008-03-25T22:39:26+0000",
            "content": "Sorry for hogging.  Got some local compilation issues with the query builder in contrib, so assigning to Grant to get this in. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12582075"
        },
        {
            "date": "2008-03-29T21:09:54+0000",
            "content": "Committed revision 642612.\n\nThanks Sebastian and Steve ",
            "author": "Grant Ingersoll",
            "id": "comment-12583365"
        }
    ]
}