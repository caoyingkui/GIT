{
    "id": "LUCENE-7398",
    "title": "Nested Span Queries are buggy",
    "details": {
        "resolution": "Unresolved",
        "affect_versions": "5.5",
        "components": [
            "core/search"
        ],
        "labels": "",
        "fix_versions": [],
        "priority": "Critical",
        "status": "Open",
        "type": "Bug"
    },
    "description": "Example for a nested SpanQuery that is not working:\n\nDocument: Human Genome Organization , HUGO , is trying to coordinate gene mapping research worldwide.\n\nQuery: spanNear([body:coordinate, spanOr([spanNear([body:gene, body:mapping], 0, true), body:gene]), body:research], 0, true)\n\nThe query should match \"coordinate gene mapping research\" as well as \"coordinate gene research\". It does not match  \"coordinate gene mapping research\" with Lucene 5.5 or 6.1, it did however match with Lucene 4.10.4. It probably stopped working with the changes on SpanQueries in 5.3. I will attach a unit test that shows the problem.",
    "attachments": {
        "TestSpanCollection.java": "https://issues.apache.org/jira/secure/attachment/12820971/TestSpanCollection.java",
        "LUCENE-7398.patch": "https://issues.apache.org/jira/secure/attachment/12821673/LUCENE-7398.patch",
        "LUCENE-7398-20160924.patch": "https://issues.apache.org/jira/secure/attachment/12830193/LUCENE-7398-20160924.patch",
        "LUCENE-7398-20160814.patch": "https://issues.apache.org/jira/secure/attachment/12823627/LUCENE-7398-20160814.patch",
        "LUCENE-7398-20160925.patch": "https://issues.apache.org/jira/secure/attachment/12830231/LUCENE-7398-20160925.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15399414",
            "author": "Christoph Goller",
            "date": "2016-07-29T14:33:38+0000",
            "content": "Please find attatched an extended TestSpanCollection.java for Lucene 6.1 that shows the problem. "
        },
        {
            "id": "comment-15404518",
            "author": "Alan Woodward",
            "date": "2016-08-02T18:16:25+0000",
            "content": "I think this is a bug in SpanPositionQueue, where longer Spans should be sorted before shorter ones at the same position - will dig and see if I can get a fix in shortly. "
        },
        {
            "id": "comment-15404604",
            "author": "Alan Woodward",
            "date": "2016-08-02T19:01:51+0000",
            "content": "Patch with fix, incorporating Christoph's test - will commit tomorrow. "
        },
        {
            "id": "comment-15404699",
            "author": "Paul Elschot",
            "date": "2016-08-02T20:16:02+0000",
            "content": "The patch changes the order in SpanPositionQueue which is used by SpanOr.\nThis works to pass the test case, and it does not increase complexity.\n\nI think the problem is in NearSpansOrdered.stretchToOrder() which only does this:\n\nmatchEnd = subSpans[subSpans.length - 1].endPosition();\n\nWhat is should also do is lookahead to see whether there is an ordered match with a smaller slop.\n\nIt could be that there still is a failing case with a nested SpanOr, possibly containing another nested SpanNear, but I'm not sure, this is tricky.\n\nSince looking ahead increases the complexity (normal case runtime) I'd prefer to have the patch applied now, and see what the future brings. "
        },
        {
            "id": "comment-15404722",
            "author": "Paul Elschot",
            "date": "2016-08-02T20:33:08+0000",
            "content": "I applied the patch to master commit b3505298a5bef76ff83b269bf87a179d027da849 .\nWith the patch applied, TestSpanCollection does not compile, there is no applicable createWeight() method a few times.\nProbably there was a recent conflict there, so I could not actually verify the patch. "
        },
        {
            "id": "comment-15404758",
            "author": "Alan Woodward",
            "date": "2016-08-02T20:47:12+0000",
            "content": "Ah, looks like I made the patch against 6x, will re-up against master.\n\nThe specific problem here isn't to do with look-aheads, it's with overlapping Spans within a SpanOr when it's not the leading Span for a near query.  If two clauses start at the same position, but one is longer than the other, then we need to check the longer Span first, as it will cover both cases; if the shorter Span is checked first and fails, then the leading Span will be incremented, rather than the Or. "
        },
        {
            "id": "comment-15404763",
            "author": "Alan Woodward",
            "date": "2016-08-02T20:53:07+0000",
            "content": "Patch against master. "
        },
        {
            "id": "comment-15405698",
            "author": "Paul Elschot",
            "date": "2016-08-03T10:31:15+0000",
            "content": "The patch of 22:53 hrs works as expected.\n\nThis additional test case fails on doc 0, \"w1 w2 w3 w4 w5\":\n\n  @Test\n  public void testNestedOrQuerySlop() throws IOException {\n    SpanNearQuery snq = SpanNearQuery.newOrderedNearQuery(FIELD)\n        .setSlop(1)\n        .addClause(new SpanTermQuery(new Term(FIELD, \"w1\")))\n        .addClause(new SpanOrQuery(\n            new SpanTermQuery(new Term(FIELD, \"w2\")),\n            SpanNearQuery.newOrderedNearQuery(FIELD)\n                .addClause(new SpanTermQuery(new Term(FIELD, \"w3\")))\n                .addClause(new SpanTermQuery(new Term(FIELD, \"w4\")))\n                .build()\n        ))\n        .addClause(new SpanTermQuery(new Term(FIELD, \"w5\")))\n        .build();\n\n    Spans spans = snq.createWeight(searcher, false, 1).getSpans(searcher.getIndexReader().leaves().get(0), SpanWeight.Postings.POSITIONS);\n    assertEquals(0, spans.advance(0));\n    assertEquals(Spans.NO_MORE_DOCS, spans.nextDoc());\n  }\n\n\nand it passes when using setSlop(2) instead of setSlop(1).\n\nPerhaps we should also add the above test case with setSlop(2) and a comment that it actually should pass with setSlop(1). "
        },
        {
            "id": "comment-15405903",
            "author": "Christoph Goller",
            "date": "2016-08-03T13:17:41+0000",
            "content": "After thoroughly looking into SpanQueries my conclusion is, that we have a fundamental problem in the implementation of SpanNearQuery. The problem is not new, it probably existed already in the first version of SpanQueries which as far as I know were implemented by Doug Cutting himself. I remember some attempts to describe in which cases SpanQueries work correctly and in which they do not (discussions about overlapping), but those explanations and definitions were never completely convincing for me. \n\nMy best guess: NearSpansOrdered and NearSpansUnordered currently are only correct if for each clause of the SpanQuery we can guarantee, that all its matches have the same length. In this case it is clear that (for the ordered case) if a match is too long (sloppy) we can skip to the first clause and call nextPosition. No alternative matches of intermediate clauses could improve the overall match. If we have clauses with varying match length (SpanOr or SpanNear with sloppyness) we would have to backtrack to intermediate clauses and check whether there are e.g. longer matches that could reduce the overall match length. Pauls last test case shows that even a match of the second clause that advances its position can reduce the overall lenght if it is longer himself. A match of an intermediate clause at an advanced position could be considerably shorter than its first match requiring a reset of the spans of following clauses. To my opinion this bug can only be fixed by implementing a backtracking search on the subspans that also requires a limited possibilitxy to reposition Spans to previous positions.\n\nBy the way, shrinkToAfterShortestMatch() in NearSpansOrdered of Lucene 4_10_4 provided a kind of backtracking which was the reason why my queries worked in elasticsearch 1.7.x. However, I think the implementation also did not solve all cases:\n\n\n  /** The subSpans are ordered in the same doc, so there is a possible match.\n   * Compute the slop while making the match as short as possible by advancing\n   * all subSpans except the last one in reverse order.\n   */\n  private boolean shrinkToAfterShortestMatch() throws IOException {\n    matchStart = subSpans[subSpans.length - 1].start();\n    matchEnd = subSpans[subSpans.length - 1].end();\n    Set<byte[]> possibleMatchPayloads = new HashSet<>();\n    if (subSpans[subSpans.length - 1].isPayloadAvailable()) {\n      possibleMatchPayloads.addAll(subSpans[subSpans.length - 1].getPayload());\n    }\n\n    Collection<byte[]> possiblePayload = null;\n    \n    int matchSlop = 0;\n    int lastStart = matchStart;\n    int lastEnd = matchEnd;\n    for (int i = subSpans.length - 2; i >= 0; i--) {\n      Spans prevSpans = subSpans[i];\n      if (collectPayloads && prevSpans.isPayloadAvailable()) {\n        Collection<byte[]> payload = prevSpans.getPayload();\n        possiblePayload = new ArrayList<>(payload.size());\n        possiblePayload.addAll(payload);\n      }\n      \n      int prevStart = prevSpans.start();\n      int prevEnd = prevSpans.end();\n      while (true) { // Advance prevSpans until after (lastStart, lastEnd)\n        if (! prevSpans.next()) {\n          inSameDoc = false;\n          more = false;\n          break; // Check remaining subSpans for final match.\n        } else if (matchDoc != prevSpans.doc()) {\n          inSameDoc = false; // The last subSpans is not advanced here.\n          break; // Check remaining subSpans for last match in this document.\n        } else {\n          int ppStart = prevSpans.start();\n          int ppEnd = prevSpans.end(); // Cannot avoid invoking .end()\n          if (! docSpansOrderedNonOverlap(ppStart, ppEnd, lastStart, lastEnd)) {\n            break; // Check remaining subSpans.\n          } else { // prevSpans still before (lastStart, lastEnd)\n            prevStart = ppStart;\n            prevEnd = ppEnd;\n            if (collectPayloads && prevSpans.isPayloadAvailable()) {\n              Collection<byte[]> payload = prevSpans.getPayload();\n              possiblePayload = new ArrayList<>(payload.size());\n              possiblePayload.addAll(payload);\n            }\n          }\n        }\n      }\n\n      if (collectPayloads && possiblePayload != null) {\n        possibleMatchPayloads.addAll(possiblePayload);\n      }\n      \n      assert prevStart <= matchStart;\n      if (matchStart > prevEnd) { // Only non overlapping spans add to slop.\n        matchSlop += (matchStart - prevEnd);\n      }\n\n      /* Do not break on (matchSlop > allowedSlop) here to make sure\n       * that subSpans[0] is advanced after the match, if any.\n       */\n      matchStart = prevStart;\n      lastStart = prevStart;\n      lastEnd = prevEnd;\n    }\n    \n    boolean match = matchSlop <= allowedSlop;\n    \n    if(collectPayloads && match && possibleMatchPayloads.size() > 0) {\n      matchPayload.addAll(possibleMatchPayloads);\n    }\n\n    return match; // ordered and allowed slop\n  }\n  \n\n\n\nUnfortunately the patch provided by Alan does not solve the problem. It only reorders span-matches of a SpanOrQuery in a special case, since it cannot control the order of span-matches of its subspans. I consider the patch as potentially dangerous since SpanOrQuery with the patch provides an ordering of span-matches that differs form the general contract that holds for all spans.\n "
        },
        {
            "id": "comment-15405983",
            "author": "David Smiley",
            "date": "2016-08-03T14:33:13+0000",
            "content": "I consider the patch as potentially dangerous since SpanOrQuery with the patch provides an ordering of span-matches that differs form the general contract that holds for all spans.\n\nCould you elaborate on what ordering could now happen with Alan's patch that differs from the general contract? "
        },
        {
            "id": "comment-15406059",
            "author": "Christoph Goller",
            "date": "2016-08-03T15:25:22+0000",
            "content": "The whole idea of the patch is to change the order of the matches returned by SpanOrQuery.\n\n\nSpanTermQuery q2 = new SpanTermQuery(new Term(FIELD, \"w2\"));\nSpanTermQuery q3 = new SpanTermQuery(new Term(FIELD, \"w3\"));\nSpanNearQuery q23 = new SpanNearQuery(new SpanQuery[]{q2, q3}, 0, true);\nSpanOrQuery q223 = new SpanOrQuery(q2, q23);\n\n\n\nFor a document containing \"w1 w2 w3 w4\" query q223 now returns as first match \"w2 w3\" (the longer one) and then \"w2\" while formerly it was the other way round. Both matches have the same start position, but different end positions and the contract about spans says that if start positions equal we first get the match with the lower end position (Javadoc of spans). "
        },
        {
            "id": "comment-15407775",
            "author": "Paul Elschot",
            "date": "2016-08-04T13:39:38+0000",
            "content": "To complete the picture here for the ordered case, shrinkToAfterShortestMatch() was replaced by lazy iteration at LUCENE-6537. Some points from there:\n\n\tLazy iteration should return the same document matches, but it will return some extra Span hits within each document, so scores might be different.\n\tRepeated matches from non nested ordered span near occur only when the first term repeats and there is enough slop; for query t1 t2 with slop 1:\n  t1 t1 t2 matches twice,\n  t1 t2 t2 matches once.\n\n\n\nNevertheless, from the gene research example above one can see that the current lazy iteration misses a document that used to match.\n\nSo, is it possible to change the current implementation so that it matches more documents correctly, while still being lazy?\nHere lazy means that all subspans are only moved forward, and a test for a match is only done after at least one subspans was moved forward.\n\nThe current implementation is based on the first subspans moving forward followed by a stretchToOrder().\nAfter that, as long as there is no match (i.e. too much slop), we could add moving each of the intermediate subspans forward until the order is lost.\n(This would be somewhat similar to shrinkToAfterShortestMatch(), but based on the actual slop, and not on the length on the match.)\n\nWould that help?\nWhen so, in which order should the intermediate spans be moved forward? shrinkToAfterShortestMatch() used to work backwards, but forwards could also be done. "
        },
        {
            "id": "comment-15420316",
            "author": "Paul Elschot",
            "date": "2016-08-14T12:46:36+0000",
            "content": "Patch of 14 August 2016.\nThis uses the span position queue from master, and adds the above w1..w5 test case with slop 1.\n\nThis reintroduces a shrink in shrinkToDecreaseSlop:\n\n  /** The subSpans are ordered in the same doc and matchSlop is too big.\n   * Try and decrease the slop by calling nextStartPosition() on all subSpans except the last one in reverse order.\n   * Return true iff an ordered match was found with small enough slop.\n   */\n  private boolean shrinkToDecreaseSlop() throws IOException {\n  ...\n  }\n\n\n\nThis also adds a FIXME to collect():\n\n  /** FIXME: the subspans may be after the current match. */\n\n\nPayload collection can still be added, I left that to be done.\n\nIt only fails the test case TestSpanCollection.testNestedNearQuery reporting that a term was not collected properly, the other search tests pass.\n "
        },
        {
            "id": "comment-15458530",
            "author": "Tim Allison",
            "date": "2016-09-02T13:30:09+0000",
            "content": "The cause of this is different, I think (ordered vs unordered).  However, LUCENE-5331 offers another test case that is still failing for nested SpanQueries.\n\nBased on this, I found that if you switch Paul Elschot's unit tests in the 20160814 patch to unordered SpanNear's, the unit tests still fail.\n\nThis is not surprising given that Paul's patch focuses on NearSpansOrdered.  In short, please don't take this as a complaint. \n\nThank you, all, for your work on this! "
        },
        {
            "id": "comment-15465301",
            "author": "Christoph Goller",
            "date": "2016-09-05T16:02:52+0000",
            "content": "Paul's  20160814 patch  almost convinced me. Unfortunately, it does not fix the case when an intermediate span has a longer match that reduces overall sloppyness but overlaps with a match of a subsequent span and consequently requires advancing the subsequent span. Here is an example \n\nDocument: w1 w2 w3 w4 w5\nnear/0(w1, or(w2, near/0(w2, w3, w4)), or(w5, near/0(w4, w5)))\n\nAdd the following code to the end of TestSpanCollection.testNestedNearQuery()\n\n\nSpanNearQuery q234 = new SpanNearQuery(new SpanQuery[]{q2, q3, q4}, 0, true);\nSpanOrQuery q2234 = new SpanOrQuery(q2, q234);\nSpanTermQuery p5 = new SpanTermQuery(new Term(FIELD, \"w5\"));\nSpanNearQuery q45 = new SpanNearQuery(new SpanQuery[]{q4, p5}, 0, true);\nSpanOrQuery q455 = new SpanOrQuery(q45, p5);\n        \nSpanNearQuery q1q2234q445 = new SpanNearQuery(new SpanQuery[]{q1, q2234, q455}, 0, true);\nspans = q1q2234q445.createWeight(searcher, false, 1f).getSpans(searcher.getIndexReader().leaves().get(0),SpanWeight.Postings.POSITIONS);\nassertEquals(0, spans.advance(0));\n\n\n\nI think we can only fix it if we get give up lazy iteration. I don't think this is so bad for performance. If we implement a clever caching for positions in spans a complete backtracking would only consist of making a few additional int-comparisons. The expensive operation is iterating over all span positions (IO) and we do this already in advancePosition(Spans, int), aren't we.  "
        },
        {
            "id": "comment-15465336",
            "author": "Christoph Goller",
            "date": "2016-09-05T16:14:29+0000",
            "content": "Good idea to try the nested tests from TestSpanCollection for the unordered case. The example from LUCENE-5331 shows the problems of incomplete backtracking (not comparing all combinations of span matches of all subspans) for the unordered case. In the ordered case we only have a problem with spans that have matches of different lenght, in the unorderd case we also see a problem with overlapping span-matches, even if they all have length 1. "
        },
        {
            "id": "comment-15466888",
            "author": "Christoph Goller",
            "date": "2016-09-06T09:03:11+0000",
            "content": "After thoroughly reviewing the current implementations of SpanNearQuery, PhraseQuery and MultiPhraseQuery I see some problems and inconsistencies. I volunteer to fix at least some of these problems, but first I would like to have a consensus about the desired bahavior of SpanQuery. This ticket may not be the right place for such a discussion, so please point me to a better place if there is one. \n\n1) Missing Matches caused by lazy iteration:\n\nI think lazy iteration is not a new thing in Lucene SpanNearQuery. As far as I know there never was an implementation that compared all possible combinations of subspan matches for SpanNearQuery in Lucene. So SpanNearQuery always missed some matches.\n\n*) This ticket demonstrates missing matches for ordered SpanQuery. Documents that should match don't match. This is caused by subspans of SpanNearQuery having a variable match length. For these cases the lazy iteration implementation which tries to optimize the number of comparisons of subspan matches is not sufficient.\n\n*) Tim tried these examples with unorderd SpanQuery and got the same bahavior. I think this is caused by a similar kind of lazy iteration in the unordered case.\n\n*) In the unordered case lazy iteration also causes problems if the subspans do not have variable-length matches. This is demonstrated in LUCENE-5331 and LUCENE-2861. Tim, thanks for pointing to these tickets. In these examples all clauses of the SpanNearQuery were SpanTermQueries, but some occured more than once. For PhraseQuery and MultiPhraseQuery and their implementation in SloppyPhraseScore this seems to be a known problem that has been solved by a special complex treatment of repetitions that I currently don't understand in detail.\n\nMy current opinion: We should give up lazy iteration for the unordered and the ordered case to solve these problems. I think it can be done and the performance peanalty should not be too big. We already iterate over all positions of all subspans. So we already have done the expensive operation of reading them. Should some more comparisons of int-values (positions) really matter so much? At least for the ordered case I am optimistic that I could implement it efficiently.\n\n2) Inconsistent Scoring of SpanNearQuery\n\n*) Lazy iteration means that some \"redundant\" matches in a document are skipped in order to have a faster matching algorithm. I am not sure how redundant was defined exactly for the idea of lazy iteration. It referred to matches with the same start posisiton somehow. As long as different matches for the first clause are concerned, they are found, but not the all matches for intermediate subclauses are regarded. Skipping matches however reduces the frequency that is computed and consequently the score. See Javadoc of phraseFreq() in SloppyPhraseScore which mention the same phenomenon. This is quite important for my use case of SpanQueries. I have different versions/variants of the same term on the same position, e.g. one with case-normalization and one without and I want a higher score if the user-query matches for more than one variant, and I use this approach for clauses of SpanNearQuery.\n\n*) In NearSpansOrdered the method width() (it is used to compute sloppy frequency in SpanScore) returns the number of gaps between the matches. If you have a perfect match it returns 0 (no sloppyness). In NearSpansUnordered it returns the length of the match, not the number of gaps. See atMatch() for the difference. The reason is probably, that (maxEndPositionCell.endPosition() - minPositionCell().startPosition() - totalSpanLength) might even become negative if matches overlap. I would prefer something like Math.max(0, (maxEndPositionCell.endPosition() - minPositionCell().startPosition() - totalSpanLength))\n\n*) SpanOrQuery and SpanNearQuery completely ignore the scores of their subclauses  (subweights are always generated as non-scoring). A SpanOrQuery should give a Score similar to a BooleanQuery, shouldn't it? As long as we have this behavior, SpanBoostQuery does not make any sense, doese it? So to my opinion the existance of SpanBoostQuery shows that others also had the idea that a nested SpanQuery should somehow use the scores of their clauses for the computation of their own score. "
        },
        {
            "id": "comment-15470053",
            "author": "Christoph Goller",
            "date": "2016-09-07T08:52:08+0000",
            "content": "I just found that the LUCENE-2878 work/branch may contain some interesting ideas about scoring and proximity search / Span*Queries. "
        },
        {
            "id": "comment-15484910",
            "author": "Paul Elschot",
            "date": "2016-09-12T18:35:34+0000",
            "content": "As to missing matches due to lazy iteration, I'd prefer to add an option to allow choice between current behaviour, the above patch (because I think it is slightly better than previous 4.10 behaviour), one that misses no matches, and perhaps more.\nFor example, would anyone like a SpanWindowQuery that only uses span start positions? That would at least allow an easy complete implementation.\nAnd we need to document the current ordered - no overlap, and non ordered - overlap behaviour.\n\nTo improve scoring consistency, we could start by requiring that span near queries score the same as phrases.\nThere is a problem for nested span queries in that current similarities have a tf component over a complete document field, and this tf does not play well with the sloppy frequency for SpanNear over SpanOr. I'd like each term occurrence of a SpanTerm to contribute the same (idf like) weight to a SpanNear, but that can currently not be done because the spans of a SpanOr does not have a weight. So when mixing terms with SpanOr it will be hard to get the same scoring as a boolean Or over PhraseQueries. I don't know how to resolve this, we may have to add something to the similarities for this.\nSpanBoostQuery would only make sense when the individual Spans occrurences can carry a weight.\nI'd prefer span scoring consistency to have its own jira issue(s).\n "
        },
        {
            "id": "comment-15504663",
            "author": "Paul Elschot",
            "date": "2016-09-19T20:54:20+0000",
            "content": "I have started on working on a SpanNearQuery that contains this:\n\n  /** Specifies how clauses are to occur near each other in matching documents. */\n  public static enum MatchNear {\n\n    /** Use this method for clauses that match when they are not ordered,\n     * and the slop should be determined between the end and start positions of all clauses.\n     * When the subspans vary in length, some matches may not be found.\n     */\n    UNORDERED_LAZY,\n\n    /** Use this method for clauses that match when they are not ordered,\n     * and the slop should be determined between the start positions of the first and last matching clauses.\n     */\n    UNORDERED_STARTPOS,\n\n    /** Use this method for clauses that can match when they are ordered and span collection is needed,\n     * and the slop should be determined between the end and start positions of the clauses.\n     * When the subspans vary in length, some matches may not be found.\n     */\n    ORDERED_LAZY,\n\n    /** Use this method for clauses that can match when they are ordered and span collection is needed,\n     * and the slop should be determined between the end and start positions of the clauses.\n     * When the subspans vary in length, some matches may not be found,\n     * however this method finds more matches than {@link ORDERED_LAZY}.\n     */\n    ORDERED_LOOKAHEAD,\n\n    /** Use this method for clauses that match when they are ordered,\n     * and the slop should be determined between the start positions of the first and last matching clauses.\n     */\n    ORDERED_STARTPOS\n  }\n\n\n "
        },
        {
            "id": "comment-15504686",
            "author": "Paul Elschot",
            "date": "2016-09-19T21:03:06+0000",
            "content": "The idea is to allow full backward compatibility, as well as more matching methods:\n\nUNORDERED_LAZY is the current unordered,\nUNORDERED_STARTPOS is even simpler, it only uses span start positions, so it should be complete.\nORDERED_LAZY is the current ordered,\nORDERED_LOOKAHEAD is in the patch of 14 August 2016,\nORDERED_STARTPOS also only uses start positions, so it should be complete.\n\nThe complete ORDERED and UNORDERED cases that use start and end positions and need backtracking are left for later.\n\nComments? "
        },
        {
            "id": "comment-15519320",
            "author": "Paul Elschot",
            "date": "2016-09-24T17:11:58+0000",
            "content": "Patch of 24 Sep 2016, work in progress. Edit: superseded on 25 Sep, this can be ignored.\n\nThis introduces SpanNearQuery.MatchNear to choose the matching method.\n\nThe ORDERED_LAZY case is still the patch of 14 August, this should be changed back to the current implementation, and be used to implement ORDERED_LOOKAHEAD.\n\nThis implements MatchNear.UNORDERED_STARTPOS and uses that as the default implementation for the unordered case.\nThe implementation of UNORDERED_STARTPOS is in NearSpansUnorderedStartPos, which is simpler than the current NearSpansUnordered, there is no SpansCell.\nI'd expect this StartPos implementation to be a little faster, so I also implemented it as default for the unordered case.  In only one test case the UNORDERED_LAZY method is needed to pass the test.\n\nThe question is whether it is ok to change the default unordered implementation to only use the span start positions.\n\nThe collect() method is moved to the superclass ConjunctionSpans, this simplification might be done at another issue. "
        },
        {
            "id": "comment-15521487",
            "author": "Paul Elschot",
            "date": "2016-09-25T21:59:01+0000",
            "content": "Patch of 25 Sep 2016.\nCompared to the previous patch, this removes the ORDERED_STARTPOS case, because I don't know whether that is needed.\nAlso this restores backward compatibility.\n\nCompared to master, this has:\nFour MatchNear methods, two are the current ones, they are called ORDERED_LAZY and UNORDERED_LAZY, and these are used when the current builder and constructors use a boolean ordered argument.\n\nThe third case is ORDERED_LOOKAHEAD, which is from the patch of 18 August.\n\nThe last case is UNORDERED_STARTPOS, which is simpler than UNORDERED_LAZY, hopefully a little faster, and with better completeness of the result.\n\nJavadocs for all four cases have been added.\n\nAll test cases from here have been added, and where necessary they have been modified to use ORDERED_LOOKAHEAD and to not do span collection. These tests pass.\n\nFor the last case, UNORDERED_STARTPOS, no test cases have been added yet. This is still to be done. Does anyone have more difficult cases?\n\nMinor point: the collect() method was moved to the superclass ConjunctionSpans.\n\nFeedback welcome, especially on the javadocs of SpanNearQuery.MatchNear.\n\nInstead of adding backtracking methods, it might be better to do counting of input spans in a matching window. I'm hoping that the UNORDERED_STARTPOS case can be extended for that. Any ideas there? "
        },
        {
            "id": "comment-15546743",
            "author": "Paul Elschot",
            "date": "2016-10-04T21:36:30+0000",
            "content": "Patch of 4 Oct 2016.\n\nThis is the patch of 25 Sep 2016, but without the UNORDERED_STARTPOS case.\n\nIn a nutshell this:\n\n\tadds ORDERED_LOOKAHEAD,\n\tis backward compatible,\n\ttries to document the limitations of the matching methods for SpanNearQuery.\n\n "
        },
        {
            "id": "comment-15670478",
            "author": "Michael McCandless",
            "date": "2016-11-16T14:02:45+0000",
            "content": "I can't quite tell from the comments/iterations here: is this latest patch ready to be committed, or are there still known problems?\n\nAlternatively, should we maybe revert the lazy iteration change (LUCENE-6537) if it is the root cause that broke previous cases? "
        },
        {
            "id": "comment-15671305",
            "author": "Paul Elschot",
            "date": "2016-11-16T19:04:17+0000",
            "content": "is this latest patch ready to be committed, or are there still known problems?\n\nBoth actually, assuming that master has not had a conflicting update since.\nTo completely solve this backtracking is needed, and the patch does not provide that.\n\nTo allow collecting/payloads easily, I'd rather accept the limitations/bugs of the current lazy implementation.\nAs a minimum a reference to this issue could be added to the javadocs of the (un)ordered near spans.\n\nAFAIK:\n\n\ta complete solution that can be made with lazy iteration is a span near query that has two subqueries\nand that only checks the span starting positions,\n\tfor subqueries that are terms or that do not vary in length, completeness for two subqueries is already there.\n\n\n\nIn case there is interest in span near queries that only use starting positions, well, that should be easy.\n\n "
        },
        {
            "id": "comment-15673323",
            "author": "Michael McCandless",
            "date": "2016-11-17T10:15:38+0000",
            "content": "Paul Elschot I was able to apply the patch to current master (there was one minor conflict) and tests passed.\n\nI looked at the patch, and it looks like it generalizes the previous boolean ordered into a trilean, adding a new ORDERED_LOOKEAHEAD option that resolves some (not all) of the issues raised here, right?  And the other two options match the ordered=true and ordered=false cases today, so we have back compat?  So net/net, while this is a public API change, it is a step forward ... progress not perfection.\n\nI wonder if (separately!) we could explore adding minimal interval semantics to Lucene, like MG4J: http://vigna.di.unimi.it/ftp/papers/EfficientAlgorithmsMinimalIntervalSemantics.pdf ... I don't know much about it, but it sounds compelling and efficient  "
        },
        {
            "id": "comment-15674083",
            "author": "Paul Elschot",
            "date": "2016-11-17T16:06:04+0000",
            "content": "Michael McCandless, the patch is as you stated, and having MatchNear as an enum to choose the matching method is easy to extend.\nI would not mind to have some more opinions on whether the progress is enough to actually add the code.\n\nI know this MG4J paper and it could well be that theorem 11 in there proves that no lazy algorithm is possible for the general case with more than 2 subqueries, but for now I cannot really follow their terminology.  In particular I'd like to know whether or not these efficient algorithms correspond to the current lazy implementations in Lucene. I'm hoping that they do not, because then there might be some room for improvement in Lucene without losing speed.\n\nAs Christoph Goller stated above:\nI want a higher score if the user-query matches for more than one variant\nI don't think the ORDERED_LOOKAHEAD of the patch does  that, because it only matches one variant.\nI hope that there is a non backtracking implementation that can do this, but I'm not sure.\n "
        },
        {
            "id": "comment-15823633",
            "author": "Artem Lukanin",
            "date": "2017-01-16T08:52:21+0000",
            "content": "This issue describes only a partial problem, when 2 SpanTerms are at the same position inside SpanOr. But there is a general problem, when SpanTerms has different positions. For example, if I want to find this text \"aa bb fineness cc ee colority dd\" these queries will not find it, because only the first SpanTerm from 2 is taken into account:\n\n\n  @Test\n  public void testNestedOrQuery3() throws IOException {\n    SpanNearQuery snq = new SpanNearQuery.Builder(FIELD, SpanNearQuery.MatchNear.ORDERED_LOOKAHEAD)\n        .addClause(\n            new SpanNearQuery.Builder(FIELD, SpanNearQuery.MatchNear.ORDERED_LOOKAHEAD)\n                .addClause(new SpanTermQuery(new Term(FIELD, \"aa\")))\n                .addClause(new SpanOrQuery(\n                    new SpanTermQuery(new Term(FIELD, \"bb\")),\n                    new SpanNearQuery.Builder(FIELD, SpanNearQuery.MatchNear.ORDERED_LOOKAHEAD)\n                        .addClause(new SpanTermQuery(new Term(FIELD, \"cc\")))\n                        .addClause(new SpanTermQuery(new Term(FIELD, \"ee\")))\n                        .setSlop(2)\n                        .build()\n                ))\n                .setSlop(2)\n                .build()\n        )\n        .addClause(new SpanTermQuery(new Term(FIELD, \"dd\")))\n        .setSlop(2)\n        .build();\n\n    Spans spans = snq.createWeight(searcher, false).getSpans(searcher.getIndexReader().leaves().get(0), SpanWeight.Postings.POSITIONS);\n    assertEquals(8, spans.advance(8));\n  }\n\n  @Test\n  public void testNestedOrQuery4() throws IOException {\n    SpanNearQuery snq = new SpanNearQuery.Builder(FIELD, SpanNearQuery.MatchNear.ORDERED_LOOKAHEAD)\n        .addClause(new SpanTermQuery(new Term(FIELD, \"aa\")))\n        .addClause(new SpanOrQuery(\n            new SpanTermQuery(new Term(FIELD, \"bb\")),\n            SpanNearQuery.newOrderedNearQuery(FIELD)\n                .addClause(new SpanTermQuery(new Term(FIELD, \"cc\")))\n                .addClause(new SpanTermQuery(new Term(FIELD, \"ee\")))\n                .setSlop(2)\n                .build()\n        ))\n        .addClause(new SpanTermQuery(new Term(FIELD, \"dd\")))\n        .setSlop(2)\n        .build();\n\n    Spans spans = snq.createWeight(searcher, false).getSpans(searcher.getIndexReader().leaves().get(0), SpanWeight.Postings.POSITIONS);\n    assertEquals(8, spans.advance(8));\n  }\n\n\n\nAlso, the patch only works for SpanNear of more than 2 subclauses and the same binary-clauses test does not work either:\n\n\n  @Test\n  public void testNestedOrQuery2() throws IOException {\n    SpanNearQuery snq = new SpanNearQuery.Builder(FIELD, SpanNearQuery.MatchNear.ORDERED_LOOKAHEAD)\n        .addClause(\n            new SpanNearQuery.Builder(FIELD, SpanNearQuery.MatchNear.ORDERED_LOOKAHEAD)\n                .addClause(new SpanTermQuery(new Term(FIELD, \"coordinate\")))\n                .addClause(new SpanOrQuery(\n                    new SpanTermQuery(new Term(FIELD, \"gene\")),\n                    new SpanNearQuery.Builder(FIELD, SpanNearQuery.MatchNear.ORDERED_LOOKAHEAD)\n                        .addClause(new SpanTermQuery(new Term(FIELD, \"gene\")))\n                        .addClause(new SpanTermQuery(new Term(FIELD, \"mapping\")))\n                        .build()\n                ))\n                .build()\n        )\n        .addClause(new SpanTermQuery(new Term(FIELD, \"research\")))\n        .build();\n\n    Spans spans = snq.createWeight(searcher, false).getSpans(searcher.getIndexReader().leaves().get(0), SpanWeight.Postings.POSITIONS);\n    assertEquals(4, spans.advance(4));\n    assertEquals(5, spans.nextDoc());\n  }\n\n "
        },
        {
            "id": "comment-15823865",
            "author": "Artem Lukanin",
            "date": "2017-01-16T12:20:22+0000",
            "content": "Actually testNestedOrQuery4 works if I setSlop(3). I forgot to take into account one more Span, when transferring binary-nested clauses into 3-clauses SpanNearQuery. "
        },
        {
            "id": "comment-15824303",
            "author": "Artem Lukanin",
            "date": "2017-01-16T17:05:30+0000",
            "content": "The patch has a bug. The following sentence is not found, because the look-ahead is too greedy: \"the system of claim 16 further comprising a user location unit adapted to determine user location based on location information received from the user's device\"\n\n\n  @Test\n  public void testNestedOrQueryLookAhead() throws IOException {\n    SpanNearQuery snq = new SpanNearQuery.Builder(FIELD, SpanNearQuery.MatchNear.ORDERED_LOOKAHEAD)\n        .addClause(new SpanOrQuery(\n            new SpanTermQuery(new Term(FIELD, \"user\")),\n            new SpanTermQuery(new Term(FIELD, \"ue\"))\n        ))\n        .addClause(new SpanNearQuery.Builder(FIELD, SpanNearQuery.MatchNear.ORDERED_LOOKAHEAD)\n            .setSlop(3)\n            .addClause(new SpanTermQuery(new Term(FIELD, \"location\")))\n            .addClause(new SpanTermQuery(new Term(FIELD, \"information\")))\n            .build()\n        )\n        .build();\n\n    Spans spans = snq.createWeight(searcher, false).getSpans(searcher.getIndexReader().leaves().get(0), SpanWeight.Postings.POSITIONS);\n    assertEquals(6, spans.advance(0));\n    assertEquals(Spans.NO_MORE_DOCS, spans.nextDoc());\n  }\n\n\n\nThe fix is simple, there should be an additional check inside shrinkToDecreaseSlop():\n\n  /** The subSpans are ordered in the same doc and matchSlop is too big.\n   * Try and decrease the slop by calling nextStartPosition() on all subSpans except the last one in reverse order.\n   * Return true iff an ordered match was found with small enough slop.\n   */\n  private boolean shrinkToDecreaseSlop() throws IOException {\n    int lastStart = subSpans[subSpans.length - 1].startPosition();\n\n    for (int i = subSpans.length - 2; i >= 1; i--) { // intermediate spans for subSpans.length >= 3\n      Spans prevSpans = subSpans[i];\n      int prevStart = prevSpans.startPosition();\n      int prevEnd = prevSpans.endPosition();\n      while (true) { // Advance prevSpans until it is after (lastStart, lastEnd) or the slop increases.\n        if (prevSpans.nextStartPosition() == NO_MORE_POSITIONS) {\n          oneExhaustedInCurrentDoc = true;\n          break; // Check remaining subSpans for final match in current doc\n        } else {\n          int ppEnd = prevSpans.endPosition();\n          if (ppEnd > lastStart) { // no more ordered\n            break; // Check remaining subSpans.\n          } else { // prevSpans still before lastStart\n            int ppStart = prevSpans.startPosition();\n            int slopIncrease = (prevEnd - prevStart) - (ppEnd - ppStart); // span length decrease is slop increase\n            if (slopIncrease > 0) {\n              break; // Check remaining subSpans.\n            } else { // slop did not increase\n                prevStart = ppStart;\n                prevEnd = ppEnd;\n                matchSlop += slopIncrease;\n              }\n            }\n          }\n        }\n      lastStart = prevStart;\n    }\n\n    while (true) { // for subSpans[0] only the end position influences the match slop.\n      int prevEnd = subSpans[0].endPosition();\n      if (subSpans[0].nextStartPosition() == NO_MORE_POSITIONS) {\n        oneExhaustedInCurrentDoc = true;\n        break;\n      }\n      int ppEnd = subSpans[0].endPosition();\n      if (ppEnd > lastStart) { // no more ordered\n        break;\n      }\n      int slopIncrease = prevEnd - ppEnd;\n      if (slopIncrease > 0) {\n        break;\n      }\n      // slop did not increase:\n      matchStart = subSpans[0].startPosition();\n      matchSlop += slopIncrease;\n\n      // FIX STARTS\n      if (matchSlop <= allowedSlop) {\n        break;\n      }\n      // FIX ENDS\n    }\n\n    firstSubSpansAfterMatch = true;\n    boolean match = matchSlop <= allowedSlop;\n    return match; // ordered and allowed slop\n  }\n\n\n\nSorry for not providing a new patch. I'm on a previous version of Lucene. "
        },
        {
            "id": "comment-15892972",
            "author": "Paul Elschot",
            "date": "2017-03-02T20:53:42+0000",
            "content": "One way to view the problem is that when span end positions are used to determine the slop, it becomes impossible to determine an order for moving the subspans to a next position.\n\nSo one direction out of this could be: use NearSpans that determines the slop only by the start positions of the subspans. That leaves only the cases in which the subspans can start (and maybe also end) at the same position.\nTo make sure that all the subspans move forward after a match we could move them all forward until after the current match, and while doing that also count/collect them for scoring/highlighting as long as they are within the match. That should solve the bug reported here, which is about scoring a missed matching occurrence.\n\nThis limits the required slop to using only the starting positions of the subspans. Could this work? "
        },
        {
            "id": "comment-16168576",
            "author": "Tim Allison",
            "date": "2017-09-15T21:49:30+0000",
            "content": "First question:\nIs there any utility in a patch that would throw an exception if it spotted a sibling that shares a term with a niece (or descendant) as in the original problem?  At least let the user know that the returned docs might not be correct?  My little app relies on SpanQueries for retrieval, and this bug is potentially a real problem.\n\nSecond question: would it be of any use if I supplied a PR for @Ignore'd test cases that currently fail?\n\nThird question:\nFor the actual solution, do we have to back off to full dynamic programming (Earley or more modern equivalents)?  Or will Paul Elschot's recommendations above work? "
        },
        {
            "id": "comment-16630529",
            "author": "Michael Gibney",
            "date": "2018-09-27T14:32:43+0000",
            "content": "I have a branch containing a candidate fix for this issue: LUCENE-7398/master\n\nIt includes support for complete graph-based matching, configurable to include:\n\n\tall valid top-level startPosition s\n\tall valid match lengths (in the startPosition - endPosition sense)\n\tall valid match width s (in the slop sense)\n\tall redundant matches (different Term s, same startPosition, endPosition, and width)\n\tall possible valid combinations of subclause positions\n\n\n\nOption 1 is appropriate for top-level matching and document matching (and is complete for that use case); options 2/3 may be used in subclauses to guarantee complete matching of parent Spans; option 4 results in very thorough scoring. Option 5 would be an unusual use case; but I think there are some applications for full combinatoric matching, and the option was well supported by the implementation, so it is included for the sake of completeness.\n\nThe candidate implementation models the match graph as a kind of 2-dimensional queue that supports random-access seek and arbitrary node removal. A more thorough explanation would be unwieldy in a comment, so I wrote three posts, which respectively:\n\n\tProvide some background on the problem associated with LUCENE-7398 (this post is heavily informed by the discussion on this issue)\n\tDescribe the candidate implementation in some detail (also includes information on how to configure/test/evaluate)\n\tAnticipate some possible consequences/applications of new functionality that would be enabled by this (or other equivalent) fix\n\n\n\nSome notes:\n\n\tThe branch contains (and passes) all tests proposed so far in association with this issue (and also quite a few additional tests)\n\tThe candidate implementation is made more complete and performant by the addition of some extra information in the index (e.g., positionLength). This extra information is currently stored using Payload s, though for positionLength at least, there has been some discussion of integrating it more directly in the index (see LUCENE-4312, LUCENE-3843)\n\tSome version of this code has been running in production for several months, and has given no indication of instability, even running every user phrase query (both explicit and pf) as a graph query.\n\tTo facilitate evaluation, the fix is integrated in master, branch_7x, branch_7_5, and branch_7_4.\n\n "
        }
    ]
}