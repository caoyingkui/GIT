{
    "id": "LUCENE-8138",
    "title": "Check that dv producers return the same values with advanceExact",
    "details": {
        "labels": "",
        "priority": "Minor",
        "resolution": "Unresolved",
        "affect_versions": "None",
        "status": "Patch Available",
        "type": "Improvement",
        "components": [],
        "fix_versions": []
    },
    "description": "Follow-up of LUCENE-8117. I'd like to make CheckIndex verify that doc values producers return the same values regardless of whether the iterator was moved with nextDoc/advance or advanceExact.",
    "attachments": {
        "LUCENE-8138.patch": "https://issues.apache.org/jira/secure/attachment/12907653/LUCENE-8138.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16338960",
            "date": "2018-01-25T09:14:41+0000",
            "content": "Here is a patch. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16338963",
            "date": "2018-01-25T09:18:02+0000",
            "content": "+1, patch looks good ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16350326",
            "date": "2018-02-02T13:33:44+0000",
            "content": "This happens to not work well with LUCENE-4198 due to the fact that CheckIndex checks the merge instance, that Lucene70NormsProducer now reuses index inputs at merge time to avoid cloning too much, and that changes in this patch consume two iterators in parallel.\n\nDoes anyone know why CheckIndex uses the merge instance? ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16350338",
            "date": "2018-02-02T13:39:47+0000",
            "content": "merge instance won't lazy-load stuff permanently into memory: under normal conditions the codec may load a bunch of stuff in memory but \"merge instance\" basically instructs it not to store that in any maps.\n\nit prevented huge RAM spikes on merge, especially people that really have more fields than their hardware can support and so on. CheckIndex uses the same logic for the same reasons.\n\nExample:\u00a0each field takes 100MB and there are 100 fields. How much memory should check index require to complete successfully? 100MBish? or 10GBish? That's the difference. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16350347",
            "date": "2018-02-02T13:45:26+0000",
            "content": "Also because checkindex historically did a sequential pass on the data thru all docs (like merging), mergeInstance makes it much faster. For example the stored fields reader decompresses differently for this case. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16350350",
            "date": "2018-02-02T13:46:02+0000",
            "content": "OK, I got it. I'll have to revisit the approach that Lucene70NormsProducer uses to avoid too much cloning. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16350352",
            "date": "2018-02-02T13:49:43+0000",
            "content": "I was hoping we could still see the huge change it made in mike's checkindex benchmark, but i think the data is no longer available. But, something new now looks suspicious about checkindex time:\u00a0LUCENE-8153 ",
            "author": "Robert Muir"
        }
    ]
}