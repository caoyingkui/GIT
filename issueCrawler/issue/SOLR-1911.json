{
    "id": "SOLR-1911",
    "title": "File descriptor leak while indexing, may cause index corruption",
    "details": {
        "affect_versions": "1.5",
        "status": "Resolved",
        "fix_versions": [],
        "components": [
            "update"
        ],
        "type": "Bug",
        "priority": "Critical",
        "labels": "",
        "resolution": "Cannot Reproduce"
    },
    "description": "While adding documents to an already existing index using this build, the number of open file descriptors increases dramatically until the open file per-process limit is reached (1024) , at which point there are error messages in the log to that effect. If the server is restarted the index may be corrupt\n\ncommits are handled by autocommit every 60 seconds or 500 documents (usually the time limit is reached first). \nmergeFactor is 10.\n\nIt looks as though each time a commit takes place, the number of open files  (obtained from \" lsof -p `cat solr.pid` | egrep ' [0-9]+r ' \") increases by 40, There are several open file descriptors associated with each file in the index.\n\nRerunning the same index updates with an older Solr (built from trunk in Feb 2010) doesn't show this problem - the number of open files fluctuates up and down as segments are created and merged, but stays basically constant.",
    "attachments": {
        "openafteropt.txt": "https://issues.apache.org/jira/secure/attachment/12444577/openafteropt.txt",
        "indexlsof.tar.gz": "https://issues.apache.org/jira/secure/attachment/12444710/indexlsof.tar.gz"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Hoss Man",
            "id": "comment-12867756",
            "date": "2010-05-15T00:29:24+0000",
            "content": "Moving log dump out of problem description and into a comment...\n\n\nMay 13, 2010 12:37:04 PM org.apache.solr.update.DirectUpdateHandler2 commit\nINFO: start commit(optimize=false,waitFlush=true,waitSearcher=true,expungeDeletes=false)\nMay 13, 2010 12:37:04 PM org.apache.solr.update.DirectUpdateHandler2$CommitTracker run\nSEVERE: auto commit error...\njava.io.FileNotFoundException: /home/simon/rig2/solr/core1/data/index/_j2.nrm (Too many open files)\n\tat java.io.RandomAccessFile.open(Native Method)\n\tat java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)\n\tat org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput$Descriptor.<init>(SimpleFSDirectory.java:69)\n\tat org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.<init>(SimpleFSDirectory.java:90)\n\tat org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.<init>(NIOFSDirectory.java:80)\n\tat org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:67)\n\tat org.apache.lucene.index.SegmentReader.openNorms(SegmentReader.java:1093)\n\tat org.apache.lucene.index.SegmentReader.get(SegmentReader.java:532)\n\tat org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:634)\n\tat org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:610)\n\tat org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:1012)\n\tat org.apache.lucene.index.IndexWriter.applyDeletes(IndexWriter.java:4563)\n\tat org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:3775)\n\tat org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3623)\n\tat org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3614)\n\tat org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1769)\n\tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1732)\n\tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1696)\n\tat org.apache.solr.update.SolrIndexWriter.close(SolrIndexWriter.java:230)\n\tat org.apache.solr.update.DirectUpdateHandler2.closeWriter(DirectUpdateHandler2.java:181)\n\tat org.apache.solr.update.DirectUpdateHandler2.commit(DirectUpdateHandler2.java:409)\n\tat org.apache.solr.update.DirectUpdateHandler2$CommitTracker.run(DirectUpdateHandler2.java:602)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:138)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:207)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\n\tat java.lang.Thread.run(Thread.java:619)\nMay 13, 2010 12:37:04 PM org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: {} 0 1\nMay 13, 2010 12:37:04 PM org.apache.solr.common.SolrException log\nSEVERE: java.io.IOException: directory '/home/simon/rig2/solr/core1/data/index' exists and is a directory, but cannot be listed: list() returned null\n\tat org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:223)\n\tat org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:234)\n\tat org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:582)\n\tat org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:535)\n\tat org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:316)\n\tat org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:1129)\n\tat org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:999)\n\tat org.apache.solr.update.SolrIndexWriter.<init>(SolrIndexWriter.java:191)\n\tat org.apache.solr.update.UpdateHandler.createMainIndexWriter(UpdateHandler.java:99)\n\tat org.apache.solr.update.DirectUpdateHandler2.openWriter(DirectUpdateHandler2.java:173)\n\tat org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:220)\n\tat org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:61)\n\tat org.apache.solr.handler.XMLLoader.processUpdate(XMLLoader.java:139)\n\tat org.apache.solr.handler.XMLLoader.load(XMLLoader.java:69)\n\tat org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:54)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1321)\n\tat org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:341)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:244)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1089)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:365)\n\tat org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:712)\n\tat org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:405)\n\tat org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:211)\n\tat org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:139)\n\tat org.mortbay.jetty.Server.handle(Server.java:285)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:502)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:835)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:641)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:208)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:378)\n\tat org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:368)\n\tat org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:442)\n\n\n\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12867758",
            "date": "2010-05-15T00:35:28+0000",
            "content": "Simon: can you please provide a few more details:\n\n1) what mechanism are you using to index content?  ie: POSTing XML from a remote client? using the stream.url or stream.file params? Using SolrCell? using DIH? (and if you are using DIH, from what source? DB? HTTP? File? .. and with what transformers?)\n\n2) what files does lsof show are open after each successive commit until the limit is reached?  seeing how the file list grows \u2013 specifically which files are never getting closed \u2013 over time is really the only way to track down what code isn't closing files\n\n(The stack trace you posted shows what it's doing when it runs out of file handles, but there isn't necessarily any correlation between that and what code should be closing files but isn't ) "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12867792",
            "date": "2010-05-15T03:56:19+0000",
            "content": "Going from Feb to May crosses the point when Solr went from Lucene 2.9 to Lucene 3.0\n\nIf there is a fd leak, one place to look would be ref counting in IndexReader and SolrIndexReader that extends FilterIndexReader.  IndexReader is notoriously hard to wrap (subclass and delegate), and any change in implementation can make/expose new bugs. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12867803",
            "date": "2010-05-15T05:57:58+0000",
            "content": "one place to look would be ref counting in IndexReader\n\ntrue, but at this point w/o any idea what files we're dealing with it could be anything (SOLR-1877 comes to mind) "
        },
        {
            "author": "Simon Rosenthal",
            "id": "comment-12867880",
            "date": "2010-05-15T15:21:43+0000",
            "content": "bq: 1) what mechanism are you using to index content? ie: POSTing XML from a remote client? using the stream.url or stream.file params? Using SolrCell? using DIH? (and if you are using DIH, from what source? DB? HTTP? File? .. and with what transformers?)\n\n   posting XML from a local client, not using stream.url or stream.file\n\n2) what files does lsof show are open after each successive commit until the limit is reached? seeing how the file list grows - specifically which files are never getting closed - over time is really the only way to track down what code isn't closing files\n\nwill attach lsof output taken after it reached the limit "
        },
        {
            "author": "Simon Rosenthal",
            "id": "comment-12867881",
            "date": "2010-05-15T15:23:28+0000",
            "content": "lsof output after the error occurred "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12867893",
            "date": "2010-05-15T16:50:55+0000",
            "content": "Hmmm, not sure I see any smoking guns in that lsof output.\n\nAre you using stock solrconfig.xml from the example?\nIf not, can you try the latest trunk, and just with the example server only?  This will help narrow down if it's a problem with some other component such as what hoss pointed out: SOLR-1877  "
        },
        {
            "author": "Simon Rosenthal",
            "id": "comment-12868295",
            "date": "2010-05-17T17:31:00+0000",
            "content": "OK.. I built from latest trunk, used the schema associated with the index and example solrconfig.xml, as you asked.\n\n\n\tStarted with a snapshot of the index taken before this issue reared its head\n\n\n\n\n\tused post.sh to add a file with around 800 documents (different one each time)\n\tdid a commit (no autocommit)\n\tdid an lsof on the process\n\n\n\nrepeated the add/commit/lsof  5 times.\n\nThe attached tarball contains the lsof outputs, and we're still seeing the number of fds incrementing by 38-40 after each commit. I didn't go to the bitter end, but I assume we'd get there...\n\nHere's a clue -?? I looked for file descriptors associated with one .prx file that was present in the original snapshot in each lsof output\n\ngrep -c _r8.prx lsof.*\nlsof.0:1\nlsof.1:2\nlsof.2:3\nlsof.3:4\nlsof.4:5\nlsof.5:6\n\nThe .frq files seem to have the same pattern.\n\nI'm assuming that's not good... "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12868309",
            "date": "2010-05-17T17:46:25+0000",
            "content": "\nHere's a clue -?? I looked for file descriptors associated with one .prx file that was present in the original snapshot in each lsof output\n\ngrep -c _r8.prx lsof.*\nlsof.0:1\nlsof.1:2\nlsof.2:3\nlsof.3:4\nlsof.4:5\nlsof.5:6\n\nOK... this definitely sounds like a leak then.  And the fact that you're using the stock solrconfig.xml should rule out spellchecker, replication, etc. "
        },
        {
            "author": "Simon Rosenthal",
            "id": "comment-12880281",
            "date": "2010-06-18T18:48:06+0000",
            "content": "\nNot able to repeat the problem with a build from branch_3x done earlier today\n\n        Solr Specification Version: 3.0.0.2010.06.18.12.54.02\n\tSolr Implementation Version: 3.1-dev 956045 - simon - 2010-06-18 12:54:02\n\tLucene Specification Version: 3.1-dev\n\tLucene Implementation Version: 3.1-dev 956045 - 2010-06-18 12:54:56\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12880283",
            "date": "2010-06-18T18:54:22+0000",
            "content": "Not able to repeat the problem with a build from branch_3x done earlier today\n\nInteresting... but you see the problem with trunk still? "
        },
        {
            "author": "Simon Rosenthal",
            "id": "comment-12880490",
            "date": "2010-06-19T14:45:47+0000",
            "content": "No - seems to have cleared up with trunk also,.\n\nI'm OK with closing it but am really curious to know what changed between mid May and today to clear up the problem. "
        }
    ]
}