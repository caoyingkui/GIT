{
    "id": "LUCENE-1582",
    "title": "Make TrieRange completely independent from Document/Field with TokenStream of prefix encoded values",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/other"
        ],
        "type": "Improvement",
        "fix_versions": [
            "2.9"
        ],
        "affect_versions": "2.9",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "TrieRange has currently the following problem:\n\n\tTo add a field, that uses a trie encoding, you can manually add each term to the index or use a helper method from TrieUtils. The helper method has the problem, that it uses a fixed field configuration\n\tTrieUtils currently creates per default a helper field containing the lower precision terms to enable sorting (limitation of one term/document for sorting)\n\ttrieCodeLong/Int() creates unnecessarily String[] and char[] arrays that is heavy for GC, if you index lot of numeric values. Also a lot of char[] to String copying is involved.\n\n\n\nThis issue should improve this:\n\n\ttrieCodeLong/Int() returns a TokenStream. During encoding, all char[] arrays are reused by Token API, additional String[] arrays for the encoded result are not created, instead the TokenStream enumerates the trie values.\n\tTrie fields can be added to Documents during indexing using the standard API: new Field(name,TokenStream,...), so no extra util method needed. By using token filters, one could also add payload and so and customize everything.\n\n\n\nThe drawback is: Sorting would not work anymore. To enable sorting, a (sub-)issue can extend the FieldCache to stop iterating the terms, as soon as a lower precision one is enumerated by TermEnum. I will create a \"hack\" patch for TrieUtils-use only, that uses a non-checked Exceptionin the Parser to stop iteration. With LUCENE-831, a more generic API for this type can be used (custom parser/iterator implementation for FieldCache). I will attach the field cache patch (with the temporary solution, until FieldCache is reimplemented) as a separate patch file, or maybe open another issue for it.",
    "attachments": {
        "ASF.LICENSE.NOT.GRANTED--LUCENE-1582.patch": "https://issues.apache.org/jira/secure/attachment/12404532/ASF.LICENSE.NOT.GRANTED--LUCENE-1582.patch",
        "LUCENE-1582.patch": "https://issues.apache.org/jira/secure/attachment/12404672/LUCENE-1582.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-04-02T14:18:41+0000",
            "content": "This sounds like a great improvement! ",
            "author": "Michael McCandless",
            "id": "comment-12695016"
        },
        {
            "date": "2009-04-03T06:33:31+0000",
            "content": "trieCodeLong/Int() returns a TokenStream. During encoding, all char[] arrays are reused by Token API, additional String[] arrays for the encoded result are not created, instead the TokenStream enumerates the trie values.\n\n+1 ",
            "author": "Shalin Shekhar Mangar",
            "id": "comment-12695261"
        },
        {
            "date": "2009-04-03T11:35:58+0000",
            "content": "A first version of the patch:\n\n\tJavaDocs not finished (examples, documentation) yet\n\tNew classes: IntTrieTokenStream, LongTrieTokenStream\n\tRemoved TrieUtils.trieCodeInt/Long()\n\tRemoved TrieUtils.addIndexFields()\n\tRemoved all fields[] arrays, now only one field name is supported everywhere\n\n\n\nTo index a trie-encoded field, just use (preferred way):\n\nFiled f=new Field(name, new LongTrieTokenStream(value, precisionStep));\nf.setOmitNorms(true);\nf.setOmitTermFreqAndPositions(true);\n\n\n(maybe TrieUtils supplies a shortcut helper method that uses these special optimal settings when creating the field, e.g. TrieUtils.newLongTrieField()). This is extensible with TokenFilters, if somebody wants to add payloads and so on.\n\nThis patch also contains the sorting fixes in the core: FieldCache.StopFillCacheException can be thrown from withing the parser. Maybe this should be provides as a separate sub-isse (or top-level issue), because I cannot apply patches to core. Mike, can you do this, when we commit this?\n\nYonik: It would be nice to hear some comments from you, too.\n\nI really like the new way to create trie encoded fields. When this moves to core, the tokenizers can be renamed to IntTokenStream, TrieUtils now only contains the converters to/from doubles and the encoding and range split.\n\nAbout the GC note in the description of this issue: The new API does not use so much array allocations and array copies and reuses the Token. But as it is needed to generate a TokenStream instance for every numeric value, the GC cost is about the same for new and old API. Especially because each TokenStream creates a LinkedHashMap internally for the attributes.\n\nJust a question for the indexer people: Is it possible to add two fields with the same field name to a document, both with a TokenStream? This is needed to add more than one trie encoded value (which worked with the old API). I just want to be sure. ",
            "author": "Uwe Schindler",
            "id": "comment-12695341"
        },
        {
            "date": "2009-04-03T12:21:09+0000",
            "content": "Maybe this should be provides as a separate sub-isse (or top-level issue), because I cannot apply patches to core. Mike, can you do this, when we commit this?\n\nIt's fine to include these changes in this patch \u2013 I can commit them all at once.\n\nBut as it is needed to generate a TokenStream instance for every numeric value, the GC cost is about the same for new and old API. Especially because each TokenStream creates a LinkedHashMap internally for the attributes.\n\nHmm, we should do some perf tests to see how big a deal this turns out to be.  It'd be nice to get some sort of reuse API working if performance is really hurt.  (Eg Analyzers can provide reusableTokenStream, keyed by thread).  You'd presumably have to key on thread & field name.  If you do this then probably a shortcut helper method should be the preferred way.\n\nJust a question for the indexer people: Is it possible to add two fields with the same field name to a document, both with a TokenStream? \n\nEach with a different TokenStream instance, right?  Yes, this should be fine; the tokens are \"logically\" concatenated just like multi-valued String fields. ",
            "author": "Michael McCandless",
            "id": "comment-12695362"
        },
        {
            "date": "2009-04-03T12:31:07+0000",
            "content": "Hmm, we should do some perf tests to see how big a deal this turns out to be. It'd be nice to get some sort of reuse API working if performance is really hurt. (Eg Analyzers can provide reusableTokenStream, keyed by thread). You'd presumably have to key on thread & field name. If you do this then probably a shortcut helper method should be the preferred way.\n\nWe can also leave this to the implementor: If somebody indexes thousands of documents, he could reuse one instance of the TokenStream for each document. As the instance is only read on document addition, he must provide a separate instance for each field, but can reuse it for the next document. This is the same like reusing Field instances during indexing.\n\nI can add a setValue() method to the tokenStream that resets it with the new value. So one could use one instance and always use setValue() to supply a new value for each document. The precisionStep should not be modifiable.\n\n\nJust a question for the indexer people: Is it possible to add two fields with the same field name to a document, both with a TokenStream? \n\nEach with a different TokenStream instance, right? Yes, this should be fine; the tokens are \"logically\" concatenated just like multi-valued String fields.\n\nYes, sure  ",
            "author": "Uwe Schindler",
            "id": "comment-12695364"
        },
        {
            "date": "2009-04-03T14:27:52+0000",
            "content": "I can add a setValue() method to the tokenStream that resets it with the new value.\n\nThat's a good step forward, but it'd likely mean the default is to be slower performance?  In general I prefer (when realistic) to have default ootb experience to be good performance, but in this case it doesn't seem like there's an easy way to have a natural high-performance default.  And eg we don't reuse Document & Field by default, so expecting someone to do a bit of work to reuse Trie's TokenStreams seems OK.\n\nIt's almost like.... Analyzer.reusableTokenStream(...) should \"know\" it's deailing with a Numeric field, and handle the reuse for you, in a future world when Lucene knows that a Field is a NumericField, meant to be indexed using trie.  But we can leave all of that for future optimization; for now, providing setValue is great. ",
            "author": "Michael McCandless",
            "id": "comment-12695400"
        },
        {
            "date": "2009-04-03T20:05:05+0000",
            "content": "Updated patch:\n\n\tsupports a setValue() to reset the TokenStream with a new value for reuse (as discussed before)\n\tcompleted JavaDocs\n\tremove dead code parts\n\tsmall change in RangeBuilder API (unneeded parameters)\n\n\n\nThe difference between reusing fields and tokenstreams and always creating a new one is measureable (I compared in the test case), but not significant. The JavaDocs contain infos, how to reuse.\n\nI have done everything what i planned, now its time to discuss the change. ",
            "author": "Uwe Schindler",
            "id": "comment-12695554"
        },
        {
            "date": "2009-04-05T17:29:47+0000",
            "content": "Some updates in patch, mostly typos, unneeded imports,...\nOne Change: prefixCodedTo...() now accepts CharSequence instead of String (because only this interface's methods are needed for decoding). ",
            "author": "Uwe Schindler",
            "id": "comment-12695880"
        },
        {
            "date": "2009-04-07T10:18:07+0000",
            "content": "New patch. In my opinion, it is now stable.\n\nNew features/changes:\n\n\tAttribute \"ShiftAttribute\" for the new TokenStream API. This makes it possible t write consumers of the TokenStream that maybe index the values to different fields depending on the shift value. This only works with the new API (as the old Token does not have a field for that).\n\tTests for the TokenStreams\n\tMissing initialization of Token in old TokenStream API\n\treverted CharSequence for prefix decoder to String (performance was 5% worse during FieldCache filling)\n\n\n\nI think, it is ready for commit. I did some further performance tests with a index of 10 Mio indexed trie values:\n\n\tThe speed difference between reusing the token streams is marginal, maximum 10% improvement\n\tFilling the FieldCache is really fast now, the use of CharSequence was a bad idea (nicer API-wise but not for performance - the well known Java-Interface problem)\n\n\n\nI did some statistics on this large index: The avg. number of terms for RangeFilters is 450 for 8bit and 70 for 4bit. This is exactly the same I have seen with 10000 docs in the test cases and 500000 docs in our PANGAEA index. This verifies, that the numbero of terms is not related to index size, only related to precision step.\n\nI will do some further speed tests comparing the prefix-encoded FieldCache with the conventional int cache using Integer.parseInt(). I suspect a big improvement, because of the simple encoding.\n\nI will also compare the indexing time with the old API and the new tokenizers.\n\nMike: If you think, the changes in FieldCache are OK, can you commit only the changes to the FieldCache? ",
            "author": "Uwe Schindler",
            "id": "comment-12696462"
        },
        {
            "date": "2009-04-07T10:50:14+0000",
            "content": "OK the changes to FieldCache look OK \u2013 I'll commit shortly.  I'll tone back the javadoc to a Expert/non-back-compat warning.  It doesn't matter much since with LUCENE-831, we should be able to remove it entirely, before releasing 2.9. ",
            "author": "Michael McCandless",
            "id": "comment-12696471"
        },
        {
            "date": "2009-04-07T10:59:40+0000",
            "content": "OK I committed the FieldCache part... thanks Uwe! ",
            "author": "Michael McCandless",
            "id": "comment-12696475"
        },
        {
            "date": "2009-04-07T11:39:27+0000",
            "content": "Thanks, i will then go forward with this.\nFinally: Let's go on with 831...  ",
            "author": "Uwe Schindler",
            "id": "comment-12696485"
        },
        {
            "date": "2009-04-07T11:49:01+0000",
            "content": "Committed Revision: 762710\nI only added term number statistics in the filter tests. ",
            "author": "Uwe Schindler",
            "id": "comment-12696491"
        },
        {
            "date": "2009-04-07T12:16:25+0000",
            "content": "b.q Finally: Let's go on with 831...\n\nHere here! ",
            "author": "Michael McCandless",
            "id": "comment-12696497"
        }
    ]
}