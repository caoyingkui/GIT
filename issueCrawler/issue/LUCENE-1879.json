{
    "id": "LUCENE-1879",
    "title": "Parallel incremental indexing",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "New Feature",
        "fix_versions": [
            "4.5",
            "6.0"
        ],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "A new feature that allows building parallel indexes and keeping them in sync on a docID level, independent of the choice of the MergePolicy/MergeScheduler.\n\nFind details on the wiki page for this feature:\n\nhttp://wiki.apache.org/lucene-java/ParallelIncrementalIndexing \n\nDiscussion on java-dev:\n\nhttp://markmail.org/thread/ql3oxzkob7aqf3jd",
    "attachments": {
        "parallel_incremental_indexing.tar": "https://issues.apache.org/jira/secure/attachment/12420368/parallel_incremental_indexing.tar"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-08-31T08:14:18+0000",
            "content": "I have a prototype version which I implemented in IBM; it contains a version that works on Lucene 2.4.1. I'm not planning on committing as is, because it is implemented on top of Lucene's APIs without any core change and therefore not as efficiently as it could be. The software grant I have lists these files. Shall I attach the tar + md5 here and send the signed software grant to you, Grant?  ",
            "author": "Michael Busch",
            "id": "comment-12749419"
        },
        {
            "date": "2009-09-03T16:27:02+0000",
            "content": "Yes on the soft. grant. ",
            "author": "Grant Ingersoll",
            "id": "comment-12751077"
        },
        {
            "date": "2009-09-23T11:47:18+0000",
            "content": "MD5 (parallel_incremental_indexing.tar) = b9a92850ad83c4de2dd2f64db2dcceab\nmd5 computed on Mac OS 10.5.7\n\nThis tar file contains all files listed in the software grant. It is a prototype that works with Lucene 2.4.x only, not with current trunk.\nIt also has some limitations mentioned before, which are not limitations of the design, but rather because it runs on top of Lucene's APIs (I wanted the code to run with an unmodified Lucene jar).\n\nNext I'll work on a patch that runs with current trunk. ",
            "author": "Michael Busch",
            "id": "comment-12758670"
        },
        {
            "date": "2009-11-04T12:35:07+0000",
            "content": "I wonder if we could change Lucene's index format to make this feature\nsimpler to implement...\n\nIe, you're having to go to great lengths (since this is built\n\"outside\" of Lucene's core) to force multiple separate indexes to\nshare everything but the postings files (merge choices, flush,\ndeletions files, segments files, turning off the stores, etc.).\n\nWhat if we could invert this approach, so that we use only single\nindex/IndexWriter, but we allow \"partitioned postings\", where sets of\nfields are mapped to different postings files in the segment?\n\nWhenever a doc is indexed, postings from the fields are then written\naccording to this partition.  Eg if I map \"body\" to partition 1, and\n\"title\" to partition 2, then I'd have two sets of postings files for\neach segment.\n\nCould something like this work? ",
            "author": "Michael McCandless",
            "id": "comment-12773466"
        },
        {
            "date": "2009-11-04T22:13:30+0000",
            "content": "I realize the current implementation that's attached here is quite\ncomplicated, because it works on top of Lucene's APIs.\n\nHowever, I really like its flexibility. You can right now easily\nrewrite certain parallel indexes without touching others. I use it in\nquite different ways. E.g you can easily load one parallel index into a\nRAMDirectory or SSD and leave the other ones on the conventional disk.\n\nLUCENE-2025 only optimizes a certain use case of the parallel indexing,\nwhere you want to (re)write a parallel index containing only posting\nlists and this will especially improve scenarios like Yonik pointed\nout a while ago on java-dev where you want to update only a few\ndocuments, not e.g. a certain field for all documents.\n\nIn other use cases it is certainly desirable to have a parallel index\nthat contains a store. It really depends on what data you want to\nupdate individually.\n\nThe version of parallel indexing that goes into Lucene's core I\nenvision quite differently from the current patch here. That's why I'd\nlike to refactor the IndexWriter (LUCENE-2026) into SegmentWriter and\nlet's call it IndexManager (the component that controls flushing,\nmerging, etc.). You can then have a ParallelSegmentWriter, which\npartitions the data into parallel segments, and the IndexManager can\nbehave the same way as before.\n\nYou can keep thinking about the whole index as a collection of segments,\njust now it will be a matrix of segments instead of a one-dimensional\nlist.\n\nE.g. the norms could in the future be a parallel segment with a single\ncolumn-stride field that you can update by writing a new generation of\nthe parallel segment.\n\nThings like two-dimensional merge policies will nicely fit into this\nmodel.\n\nDifferent SegmentWriter implementations will allow you to write single\nsegments in different ways, e.g. doc-at-a-time (the default one with\naddDocument()) or term-at-a-time (like addIndexes*() works).\n\nSo I agree we can achieve updating posting lists the way you describe,\nbut it will be limited to posting lists then. If we allow (re)writing\nsegments in both dimensions I think we will create a more flexible\napproach which is independent on what data structures we add to Lucene\n\n\tas long as they are not global to the index but per-segment as most\nof Lucene's structures are today.\n\n\n\nWhat do you think? Of course I don't want to over-complicate all this,\nbut if we can get LUCENE-2026 right, I think we can implement parallel\nindexing in this segment-oriented way nicely. ",
            "author": "Michael Busch",
            "id": "comment-12773663"
        },
        {
            "date": "2009-11-06T10:16:02+0000",
            "content": "This sounds great!  In fact your proposal for a ParallelSegmentWriter\nis just like what I'm picturing \u2013 making the switching \"down low\"\ninstead of \"up high\" (above Lucene).  This'd be more generic than just\nthe postings files, since all index files can be separately written.\n\nIt'd then a low-level question of whether ParallelSegmentWriter stores\nits files in different Directories, or, a single directory with\ndifferent file names (or maybe sub-directories within a directory, or,\nsomething else).  It could even use FileSwitchDirectory, eg to direct\ncertain segment files to an SSD (another way to achieve your example).\n\nThis should also fit well into LUCENE-1458 (flexible indexing) \u2013 one\nof the added test cases there creates a per-field codec wrapper that\nlets you use a different codec per field.  Right now, this means\nseparate file names in the same Directory for that segment, but we\ncould allow the codecs to use different Directories (or, FSD as well)\nif they wanted to.\n\n\nDifferent SegmentWriter implementations will allow you to write single\nsegments in different ways, e.g. doc-at-a-time (the default one with\naddDocument()) or term-at-a-time (like addIndexes*() works).\n\nCan you elaborate on this?  How is addIndexes* term-at-a-time?\n\n\nIf we allow (re)writing segments in both dimensions I think we will\ncreate a more flexible approach which is independent on what data\nstructures we add to Lucene\n\nDimension 1 is the docs, and dimension 2 is the assignment of fields\ninto separate partitions? ",
            "author": "Michael McCandless",
            "id": "comment-12774265"
        },
        {
            "date": "2009-11-06T17:42:56+0000",
            "content": "\nThis sounds great! In fact your proposal for a ParallelSegmentWriter\nis just like what I'm picturing - making the switching \"down low\"\ninstead of \"up high\" (above Lucene). This'd be more generic than just\nthe postings files, since all index files can be separately written. \n\nRight.  The goal should it be to be able to use this for updating Lucene internal things (like norms, column-stride fields), but also giving advanced users APIs, so that they can partition their data into parallel indexes according to their update requirements (which the current \"above Lucene\" approach allows).\n\n\nt'd then a low-level question of whether ParallelSegmentWriter stores\nits files in different Directories, or, a single directory with\ndifferent file names (or maybe sub-directories within a directory, or,\nsomething else). It could even use FileSwitchDirectory, eg to direct\ncertain segment files to an SSD (another way to achieve your example).\n\nExactly! We should also keep the distributed indexing use case in mind here. It could make sense for systems like Katta to not only shard in the document direction.\n\n\nThis should also fit well into LUCENE-1458\n\nSounds great! ",
            "author": "Michael Busch",
            "id": "comment-12774329"
        },
        {
            "date": "2009-11-06T17:56:35+0000",
            "content": "\nCan you elaborate on this? How is addIndexes* term-at-a-time?\n\nLet's say we have an index 1 with two fields a and b and you want to create a new parallel index 2 in which you copy all posting lists of field b. You can achieve this by using addDocument(), if you iterate on all posting lists in 1b in parallel and create for each document in 1 a corresponding document in 2 that contains the terms of the postings lists from 1b that have a posting for the current document. This I called the \"document-at-a-time approach\".\n\nHowever, this is terribly slow (I tried it out), because of all the posting lists you perform I/O on in parallel. It's far more efficient to copy an entire posting list over from 1b to 2, because then you only perform sequential I/O. And if you use 2.addIndexes(IndexReader(1b)), then exactly this happens, because addIndexes(IndexReader) uses the SegmentMerger to add the index. The SegmentMerger iterates the dictionary and consumes the posting lists sequentially. That's why I called this \"term-at-a-time approach\". In my experience this is for a similar use case as the one I described here orders of magnitudes more efficient. My doc-at-a-time algorithm ran ~20 hours, the term-at-a-time one 8 minutes! The resulting indexes were identical. ",
            "author": "Michael Busch",
            "id": "comment-12774338"
        },
        {
            "date": "2009-11-06T17:58:18+0000",
            "content": "\nDimension 1 is the docs, and dimension 2 is the assignment of fields\ninto separate partitions?\n\nYes, dimension 1 is unambiguously the docs. Dimension 2 can be the fields into separate parallel indexes, or also what we call today generations for e.g. the norms files.   ",
            "author": "Michael Busch",
            "id": "comment-12774340"
        },
        {
            "date": "2010-03-04T06:18:26+0000",
            "content": "(Warning, this post is long, and is easier to read in JIRA)\n\nI've investigated the attached code a lot and I'd like to propose a different design and approach to this whole Parallel Index solution. I'll start by describing the limitations of the current design (whether its the approach or the code is debatable):\n\n\tLucene is not built/designed properly to a Master/Slave architecture, where different indexes share important files with others (such as segments_N, segments.gen and .del).\n\t\n\t\tI've realized this when I found that if tests (in this patch) are run with \"-ea\", there are many assert exceptions that are printed from IndexWriter.startCommit. The reason is the Master just updated one of the segments .del generation (and deleted the previous one), but the Slave is not aware of that yet and looks for the wrong .del file. While this does not run on production (e.g. \"-ea\" is usually not activated), it does affect the tests because the assertion stops operations abruptly.\n\t\tThough someone can claim we can fix that, I think it points at a problem in the design, and makes the whole solution fragile.\n\t\n\t\n\tI think it'd be really neat to introduce a ParallelWriter, equivalent to ParallelReader. The latter does not have a Master/Slave notion and so I don't think PW should have.\n\tWhen I inspected the code carefully, I realized there are lots of hoola hoops done in order to make the Master and Slave in sync. Such hoola hoops may be resolved if Lucene's IW API would be more extensible, but still:\n\t\n\t\tThe MergePolicy used is one that for the Slaves never checks the segments for which merges should actually be done. Rather, it relies on the Master policy to set the proper merges. Which is a must in this design because only the master needs to decide when to merge.\n\t\tHowever, and here I think it's because of lack of API on IW, the way the merge is done is that the master first calls mergeInit(merge), then on all slaves .maybeMerge() and then it merges that merge. maybeMerge() on the slaves consume all the merges that were decided to be run by the master, while when that finished, the master didn't finish even one merge ...\n\t\tThat works though because the MergeScheduler used is a Serial one (not SMS but still Serial) and blocking. However that leads to inconsistencies - slaves' segments view is different at one point in time from the master's.\n\t\n\t\n\tThe current approach does not support multi-threaded indexing, but I think that's a limitation that could be solved by exposing some API on IW or DW.\n\tOnly SMS is supported on the slaves.\n\tOptimize, expungeDeletes are unsupported. Though the could and perhaps just not implemented.\n\tThe current approach prevents having an architecture on which some of the parallels reside on different machines, because they share the .del and segments file with the master. It's not the worse limitation in the world, but still a limitation (of having any chance to do it efficiently) I'd like to avoid.\n\tAnd I'm sure there are more disadvantages that I don't remember now.\n\n\n\nI'd like to point out that even if the above limitations can be worked around, I still think the Master and Slave notion is not the best approach. At least, I'd like to propose a different approach:\n\n\tIntroduce a ParallelWriter which serves as a manager/orchestrator on top of other IWs. It is not a pure decorator because it drives everything that happens on the IWs, but it does not contain any actual indexing logic (e.g. add/delete/update documents).\n\t\n\t\tThe IWs PW will manage will be named hereinafter Slices.\n\t\n\t\n\tIW will expose enough API to perform two-phase operations, like the two-phase commit one can achieve today. Example operations (and I don't cover all for the interest of space):\n\t\n\t\taddDocument - first obtain a doc ID, then proceed w/ addDocument on all Slices\n\t\toptimize - already exists\n\t\tmerge - do the merge on all Slices and stamp it after all finished.\n\t\tdeleteDocuments - here we would need to expose some API on IW for DW to get an IndexReader so that IW can still return its readerPool.getReader but PW will return a ParallelSegmentReader or something, to perform the deletes across all Slices.\n\t\tThe idea is that we should do enough on the Slices so that if one fails we can still rollback, and the final 'stamp' process will be very fast and less likely to fail.\n\t\n\t\n\tFor correctness and protectiveness, PW will only accept a Directory and not IW. Reason is:\n\t\n\t\tWe want all sorts of settings like MP, MS, RAM buffer usage to be controlled by PW and not on the Slices. If we allow to pass an IW instance, one could override whatever we set, which is wrong.\n\t\tEven though one could claim that someone 'can shoot himself in the leg freely', I think that we should be resilient enough to protect stupid users from themselves.\n\t\tWe will need to allow to pass in an IW Configuration object, so that we can still account for settings such as Analyzer, MaxFieldLength etc., but discard other settings which PW will control directly\n\t\t\n\t\t\tSuch Configuration was proposed in the past already and will eliminate lots of methods on IW and ctors.\n\t\t\n\t\t\n\t\tOn a side note, ParallelReader accepts IR today, which can lead the problems such as one passes two IRs, one read-only and one not, and then deletes documents by the writable IR, with PR not knowing about it. But it's a different issue, and I'll open a separate one for that.\n\t\n\t\n\tA special MergeScheduler and MergePolicy will be introduced to allow PW to drive merges across the Slices. The idea is to support whatever MS/MP the application wants (SMS, CMS, custom), and ensuring that when MP decides a merge should be performed, that merge is executed by MS across all Slices. Few things:\n\t\n\t\tI think that a special MP is not needed, only MS. But need to validate that. If that's true, then apps could use their own custom MPs freely.\n\t\tI think custom MS may be supported ... all that's required is for the MS to run on PW and whenever it calls its merge(), let PW run the merges across all Slices? But I still need to validate that code.\n\t\tCMS can introduce two-level concurrency. One like today which executes different merges decided by MP concurrently. The other would control the concurrency level those merges are executed on the Slices.\n\t\t\n\t\t\tHmm ... even SMS can benefit from that ...\n\t\t\n\t\t\n\t\n\t\n\n\n\nI realize that accepting only Directory on PW might limit applications who want to pass in their own IW extension, for whatever reason. But other than saying \"if you pass in IW and configure it afterwards, it's on your head\", I don't think there is any other option ... Well maybe except if we expose a package-private API for PW to turn off configuration on an IW after it set it, so successive calls to the underlying IW's setters will throw an exception ... hmm might be doable. I'll look into that. If that will work, we might want to do the same for the ParallelReader as well.\n\nMichael mentioned a scenario above where one would want to rebuild an index Slice. That's still achievable by this design - one should build the IW on the outside and then replace the Directory instance on PW. We'll need to expose such API as well.\n\nBTW, some of the things I've mentioned can be taken care of in different issues, as follow on improvements, such as two-level concurrency, supporting custom MS etc. I've detailed them here just so we all see the bigger picture that's going on in my head.\n\nI think I wrote all (or most) of the high-level details. I'd like to start implementing this soon. In my head it's all chewed and digested, so I feel I can start implementing today. If possible, I'd like to get this out in 3.1. I'll try to break this issue down to as many issues as I can, to make the contributions containable. We should just keep in mind for each such issue the larger picture it solves.\n\nI'd appreciate your comments. ",
            "author": "Shai Erera",
            "id": "comment-12841078"
        },
        {
            "date": "2010-03-11T18:56:10+0000",
            "content": "I like the ParallelWriter (index slices) approach!\n\nIt sounds quite feasible and more \"direct\" in how the PW controls each\nsub writer.  It should be as simple as setting null merge\npolicy/scheduler on the subs would mean they do no merging themselves,\nbut then the PW invokes their .merge methods to explicitly merge at\nthe right times.  Vs the current approach that makes \"faker\" merge\npolicy/scheduler (I think?).\n\nSome of this will require IW to open up some APIs \u2013 eg making docID\nassignment a separate method call.  Likely many of these will just be\nprotected APIs w/in IW. ",
            "author": "Michael McCandless",
            "id": "comment-12844168"
        },
        {
            "date": "2010-03-26T18:09:44+0000",
            "content": "LUCENE-2324 will be helpful to support multi-threaded parallel-indexing.  If we have single-threaded DocumentsWriters, then it should be easy to have a ParallelDocumentsWriter?  ",
            "author": "Michael Busch",
            "id": "comment-12850268"
        },
        {
            "date": "2010-03-26T19:21:03+0000",
            "content": "The way I planned to support multi-threaded indexing is to do a two-phase addDocument. First, allocate a doc ID from DocumentsWriter (synchronized) and then add the Document to each Slice with that doc ID. DocumentsWriter was not suppose to know it is a parallel index ... something like the following.\n\nint docId = obtainDocId();\nfor (IndexWriter slice : slices) {\n  slice.addDocument(docId, Document);\n}\n\n\n\nThat allows ParallelWriter to be really an orchestrator/manager of all slices, while each slice can be an IW on its own.\n\nNow, when you say ParallelDocumentsWriter, I assume you mean that that DocWriter will be aware of the slices? That I think is an interesting idea, which is unrelated to LUCENE-2324. I.e., ParallelWriter will invoke its addDocument code which will get down to ParallelDocumentWriter, which will allocate the doc ID itself and call each slice's DocWriter.addDocument? And then LUCENE-2324 will just improve the performance of that process?\n\nThis might require a bigger change to IW then I had anticipated, but perhaps it's worth it.\n\nWhat do you think? ",
            "author": "Shai Erera",
            "id": "comment-12850313"
        },
        {
            "date": "2010-03-26T19:42:12+0000",
            "content": "First off, I haven't looked at the code here or the comments beyond skimming, but this is something I've had in my head for a long time, but don't have any code.  When I think about the whole update problem, I keep coming back to the notion of Photoshop Layers that essentially mask the underlying part of the photo, w/o damaging it.  The analogy isn't quite the same here, but nevertheless...\n\nThis leads me to wonder if the solution isn't best achieved at the index level and not at the Reader/Writer level.  \n\nSo, thinking out loud here and I'm not sure on the best wording of this:  \nwhen a document first comes in, it is all in one place, just as it is now.  Then, when an update comes in on a particular field, we somehow mark in the index that the document in question is modified and then we add the new change onto the end of the index (just like we currently do when adding new docs, but this time it's just a doc w/ a single field).    Then, when searching, we would, when scoring the affected documents, go to a secondary process that knew where to look up the incremental changes.  As background merging takes place, these \"disjoint\" documents would be merged back together.  We'd maybe even consider a \"high update\" merge scheduler that could more frequently handle these incremental merges.  In a sense, the old field for that document is masked by the new field.  I think, given proper index structure, that we maybe could make that marking of the old field fast (maybe it's a pointer to the new field, maybe it's just a bit indicating to go look in the \"update\" segment)\n\nOn the search side, I think performance would still be maintained b/c even in high update envs. you aren't usually talking about more than a few thousand changes in a minute or two and the background merger would be responsible for keeping the total number of disjoint documents low. ",
            "author": "Grant Ingersoll",
            "id": "comment-12850322"
        },
        {
            "date": "2010-03-26T20:23:55+0000",
            "content": "Hi Grant - I believe what you describe is related to solving the incremental field updates problem, where someone might want to change the value of a specific document's field. But PI is not about that. Rather, PI is about updating a whole slice at once, ie, changing a field's value across all docs, or adding a field to all docs (I believe such question was asked on the user list few days ago). I've listed above several scenarios where PI is useful for, but unfortunately it is unrelated to incremental field updates.\n\nIf I misunderstood you, then please clarify.\n\nRe incremental field updates, I think your direction is interesting, and deserves discussion, but in a separate issue/thread? ",
            "author": "Shai Erera",
            "id": "comment-12850336"
        },
        {
            "date": "2010-03-27T11:16:10+0000",
            "content": "Thanks, Shai, I had indeed misread the intent, and was likely further confused due to the fact that Michael B and I discussed it over tasty Belgian Beer in Oakland.  I'll open a discussion on list for incremental field updates. ",
            "author": "Grant Ingersoll",
            "id": "comment-12850505"
        },
        {
            "date": "2010-04-09T12:59:49+0000",
            "content": "\nI'll start by describing the limitations of the current design (whether its the approach or the code is debatable):\n\nFWIW:  The attached code and approach was never meant to be committed.  I attached it for legal reasons, as it contains the IP that IBM donated to Apache via the software grant.  Apache requires to attach the code that is covered by such a grant.\n\nI wouldn't want the master/slave approach in Lucene core.  You can implement it much nicer inside of Lucene.  The attached code however was developed with the requirement of having to run on top of an unmodified Lucene version.  \n\n\nI've realized this when I found that if tests (in this patch) are run with \"-ea\", there are many assert exceptions that are printed from IndexWriter.startCommit.\n\nThe code runs without exceptions with Lucene 2.4.  It doesn't work with 2.9/3.0, but you'll find an upgraded version that works with 3.0 within IBM, Shai. ",
            "author": "Michael Busch",
            "id": "comment-12855377"
        },
        {
            "date": "2010-04-09T13:06:10+0000",
            "content": "I have found such version ... and it fails too . At least the one I received.\n\nBut never mind that ... as long as we both agree the implementation should change. I didn't mean to say anything bad about what you did .. I know the limitations you had to work with. ",
            "author": "Shai Erera",
            "id": "comment-12855379"
        },
        {
            "date": "2011-06-30T21:00:24+0000",
            "content": "Hi, Michael\n\nIs there any lastest progress on this topic? I am very interested in this! ",
            "author": "hao yan",
            "id": "comment-13058072"
        },
        {
            "date": "2011-08-01T08:45:44+0000",
            "content": "The user mentioned above in comment was me, I guess. Commenting here just to add interesting use case that would be perfectly solved by this issue.  \n\nImagine solr Master - Slave setup, full document contains CONTENT and ID fields, e.g. 200Mio+ collection. On master, we need field ID indexed in order to process delete/update commands. On slave, we do not need lookup on ID and would like to keep our TermsDictionary small, without exploding TermsDictionary with 200Mio+ unique ID terms (ouch, this is a lot compared to 5Mio unique terms in CONTENT, with or without pulsing). \n\nWith this issue,  this could be nativly achieved by modifying solr UpdateHandler not to transfer \"ID-Index\" to slaves at all.\n\nThere are other ways to fix it, but this would be the best.(I am currently investigating an option to transfer full index on update, but to filter-out TermsDictionary on IndexReader level (it remains on disk, but this part never gets accessed on slaves). I do not know yet if this is possible at all in general , e.g. FST based term dictionary is already built (prefix compressed TermDict would be doable) ",
            "author": "Eks Dev",
            "id": "comment-13073462"
        },
        {
            "date": "2013-07-23T18:44:29+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13716963"
        }
    ]
}