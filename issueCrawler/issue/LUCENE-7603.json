{
    "id": "LUCENE-7603",
    "title": "Support Graph Token Streams in QueryBuilder",
    "details": {
        "labels": "",
        "priority": "Major",
        "resolution": "Fixed",
        "affect_versions": "None",
        "status": "Resolved",
        "type": "Improvement",
        "components": [
            "core/queryparser",
            "core/search"
        ],
        "fix_versions": [
            "6.4",
            "7.0"
        ]
    },
    "description": "With LUCENE-6664 we can use multi-term synonyms query time.  A \"graph token stream\" will be created which which is nothing more than using the position length attribute on stacked tokens to indicate how many positions a token should span.  Currently the position length attribute on tokens is ignored during query parsing.  This issue will add support for handling these graph token streams inside the QueryBuilder utility class used by query parsers.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15778672",
            "date": "2016-12-26T17:09:31+0000",
            "content": "GitHub user mattweber opened a pull request:\n\n    https://github.com/apache/lucene-solr/pull/129\n\n    LUCENE-7603: Support Graph Token Streams in QueryBuilder\n\n    Adds support for handling graph token streams inside the\n    QueryBuilder util class used by query parsers.\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/mattweber/lucene-solr LUCENE-7603\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/lucene-solr/pull/129.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #129\n\n\ncommit 568cb43d6af1aeef96cc7b6cabb7237de9058f36\nAuthor: Matt Weber <matt@mattweber.org>\nDate:   2016-12-26T15:50:58Z\n\n    Support Graph Token Streams in QueryBuilder\n\n    Adds support for handling graph token streams inside the\n    QueryBuilder util class used by query parsers.\n\n ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15779150",
            "date": "2016-12-26T23:03:09+0000",
            "content": "Whoa, thanks Matt Weber, I'll have a look, but likely not until I'm back from vacation next year! ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-15782609",
            "date": "2016-12-28T10:36:31+0000",
            "content": "This change looks great; I think it's ready!  The new TestGraphTokenStreamFiniteStrings is just missing the copyright header; I'll fix that before pushing.\n\nThe gist of the change is when query parsing detects that the analyzer produced a graph (any token with PositionLengthAttribute > 1), e.g. because SynonymGraphFilter matched or inserted a multi-token synonym, then it creates a GraphQuery which just a wrapper around sub-queries that traverse each path of the graph.\n\nAt search time, this query is currently rewritten to BooleanQuery with one clause for each path, but that is maybe something we can improve in the future, e.g. if it's a phrase query we could use TermAutomatonQuery ... but we should tackle that separately.\n\nAt long last, this (along with using SynonymGraphFilter at search time) finally fixes the long-standing bugs around multi-token synonyms, e.g. LUCENE-4499, LUCENE-1622, https://lucidworks.com/blog/2014/07/12/solution-for-multi-term-synonyms-in-lucenesolr-using-the-auto-phrasing-tokenfilter ...\n\nThis will also be useful for other tokenizers/token filters as well, e.g. I'm working on having WordDelimiterFilter set position length correctly and Kuromoji (JapaneseTokenizer) already produces graph tokens. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-15783425",
            "date": "2016-12-28T18:20:49+0000",
            "content": "Github user mattweber commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/129\n\n    Rebased against master, added missing ASF header. ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15783452",
            "date": "2016-12-28T18:35:48+0000",
            "content": "GitHub user mattweber opened a pull request:\n\n    https://github.com/apache/lucene-solr/pull/130\n\n    LUCENE-7603: branch_6x Support Graph Token Streams in QueryBuilder\n\n    Adds support for handling graph token streams inside the\n    QueryBuilder util class used by query parsers.  This is a backport to `branch_6x`.\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/mattweber/lucene-solr LUCENE-7603_6x\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/lucene-solr/pull/130.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #130\n\n\ncommit 7ab1638a3da002113e939b316d04ef9e58e38a0b\nAuthor: Matt Weber <matt@mattweber.org>\nDate:   2016-12-26T15:50:58Z\n\n    Support Graph Token Streams in QueryBuilder\n\n    Adds support for handling graph token streams inside the\n    QueryBuilder util class used by query parsers.\n\n ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15783462",
            "date": "2016-12-28T18:40:05+0000",
            "content": "Thanks for reviewing Michael McCandless!  I have added the missing ASF header and moved the test into the proper package.  I have also backported this to 6x and opened a new PR for that.  The only difference in the 6x backport is disabling coord on the rewritten boolean query.  Some of the tests are slightly different as well due to the fact that splitOnWhitespace defaults to true in 6x.\n\nPlease let me know if you need me to change anything! ",
            "author": "Matt Weber"
        },
        {
            "id": "comment-15785544",
            "date": "2016-12-29T15:41:49+0000",
            "content": "Github user dsmiley commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/129#discussion_r94148633\n\n    \u2014 Diff: lucene/core/src/java/org/apache/lucene/util/graph/GraphTokenStreamFiniteStrings.java \u2014\n    @@ -0,0 +1,294 @@\n    +/*\n    + * Licensed to the Apache Software Foundation (ASF) under one or more\n    + * contributor license agreements.  See the NOTICE file distributed with\n    + * this work for additional information regarding copyright ownership.\n    + * The ASF licenses this file to You under the Apache License, Version 2.0\n    + * (the \"License\"); you may not use this file except in compliance with\n    + * the License.  You may obtain a copy of the License at\n    + *\n    + *     http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package org.apache.lucene.util.graph;\n    +\n    +import java.io.IOException;\n    +import java.util.ArrayList;\n    +import java.util.HashMap;\n    +import java.util.List;\n    +import java.util.Map;\n    +\n    +import org.apache.lucene.analysis.TokenStream;\n    +import org.apache.lucene.analysis.tokenattributes.BytesTermAttribute;\n    +import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;\n    +import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;\n    +import org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute;\n    +import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n    +import org.apache.lucene.util.BytesRef;\n    +import org.apache.lucene.util.IntsRef;\n    +import org.apache.lucene.util.automaton.Automaton;\n    +import org.apache.lucene.util.automaton.FiniteStringsIterator;\n    +import org.apache.lucene.util.automaton.Operations;\n    +import org.apache.lucene.util.automaton.Transition;\n    +\n    +import static org.apache.lucene.util.automaton.Operations.DEFAULT_MAX_DETERMINIZED_STATES;\n    +\n    +/**\n    + * Creates a list of \n{@link TokenStream}\n where each stream is the tokens that make up a finite string in graph token stream.  To do this,\n    + * the graph token stream is converted to an \n{@link Automaton}\n and from there we use a \n{@link FiniteStringsIterator}\n to collect the various\n    + * token streams for each finite string.\n    + */\n    +public class GraphTokenStreamFiniteStrings {\n    +  /* TODO:\n    +     Most of this is a combination of code from TermAutomatonQuery and TokenStreamToTermAutomatonQuery. Would be\n    +     good to make this so it could be shared. */\n    +  private final Automaton.Builder builder;\n    +  Automaton det;\n    +  private final Map<BytesRef, Integer> termToID = new HashMap<>();\n    +  private final Map<Integer, BytesRef> idToTerm = new HashMap<>();\n    +  private int anyTermID = -1;\n    +\n    +  public GraphTokenStreamFiniteStrings() \n{\n    +    this.builder = new Automaton.Builder();\n    +  }\n    +\n    +  private static class BytesRefArrayTokenStream extends TokenStream {\n    +    private final BytesTermAttribute termAtt = addAttribute(BytesTermAttribute.class);\n    +    private final BytesRef[] terms;\n    +    private int offset;\n    +\n    +    BytesRefArrayTokenStream(BytesRef[] terms) \n{\n    +      this.terms = terms;\n    +      offset = 0;\n    +    }\n    +\n    +    @Override\n    +    public boolean incrementToken() throws IOException {\n    +      if (offset < terms.length) \n{\n    +        clearAttributes();\n    +        termAtt.setBytesRef(terms[offset]);\n    +        offset = offset + 1;\n    +        return true;\n    +      }\n    +\n    +      return false;\n    +    }\n    +  }\n    +\n    +  /**\n    +   * Gets the list of finite string token streams from the given input graph token stream.\n    +   */\n    +  public List<TokenStream> getTokenStreams(final TokenStream in) throws IOException {\n    +    // build automation\n    +    build(in);\n    +\n    +    List<TokenStream> tokenStreams = new ArrayList<>();\n    +    final FiniteStringsIterator finiteStrings = new FiniteStringsIterator(det);\n    +    for (IntsRef string; (string = finiteStrings.next()) != null; ) {\n    +      final BytesRef[] tokens = new BytesRef[string.length];\n    +      for (int idx = string.offset, len = string.offset + string.length; idx < len; idx++) \n{\n    +        tokens[idx - string.offset] = idToTerm.get(string.ints[idx]);\n    +      }\n    +\n    +      tokenStreams.add(new BytesRefArrayTokenStream(tokens));\n    +    }\n    +\n    +    return tokenStreams;\n    +  }\n    +\n    +  private void build(final TokenStream in) throws IOException {\n    +    if (det != null) \n{\n    +      throw new IllegalStateException(\"Automation already built\");\n    +    }\n    +\n    +    final TermToBytesRefAttribute termBytesAtt = in.addAttribute(TermToBytesRefAttribute.class);\n    +    final PositionIncrementAttribute posIncAtt = in.addAttribute(PositionIncrementAttribute.class);\n    +    final PositionLengthAttribute posLengthAtt = in.addAttribute(PositionLengthAttribute.class);\n    +    final OffsetAttribute offsetAtt = in.addAttribute(OffsetAttribute.class);\n    +\n    +    in.reset();\n    +\n    +    int pos = -1;\n    +    int lastPos = 0;\n    +    int maxOffset = 0;\n    +    int maxPos = -1;\n    +    int state = -1;\n    +    while (in.incrementToken()) {\n    +      int posInc = posIncAtt.getPositionIncrement();\n    +      assert pos > -1 || posInc > 0;\n    +\n    +      if (posInc > 1) {\n    \u2014 End diff \u2013\n\n    This seems like a notable limitation that should be documented in javadocs somewhere.  Can't we support holes without demanding the stream use '*' ?  And might there be a test for this? ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15785941",
            "date": "2016-12-29T19:28:05+0000",
            "content": "Github user mattweber commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/129#discussion_r94171262\n\n    \u2014 Diff: lucene/core/src/java/org/apache/lucene/util/graph/GraphTokenStreamFiniteStrings.java \u2014\n    @@ -0,0 +1,294 @@\n    +/*\n    + * Licensed to the Apache Software Foundation (ASF) under one or more\n    + * contributor license agreements.  See the NOTICE file distributed with\n    + * this work for additional information regarding copyright ownership.\n    + * The ASF licenses this file to You under the Apache License, Version 2.0\n    + * (the \"License\"); you may not use this file except in compliance with\n    + * the License.  You may obtain a copy of the License at\n    + *\n    + *     http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package org.apache.lucene.util.graph;\n    +\n    +import java.io.IOException;\n    +import java.util.ArrayList;\n    +import java.util.HashMap;\n    +import java.util.List;\n    +import java.util.Map;\n    +\n    +import org.apache.lucene.analysis.TokenStream;\n    +import org.apache.lucene.analysis.tokenattributes.BytesTermAttribute;\n    +import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;\n    +import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;\n    +import org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute;\n    +import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n    +import org.apache.lucene.util.BytesRef;\n    +import org.apache.lucene.util.IntsRef;\n    +import org.apache.lucene.util.automaton.Automaton;\n    +import org.apache.lucene.util.automaton.FiniteStringsIterator;\n    +import org.apache.lucene.util.automaton.Operations;\n    +import org.apache.lucene.util.automaton.Transition;\n    +\n    +import static org.apache.lucene.util.automaton.Operations.DEFAULT_MAX_DETERMINIZED_STATES;\n    +\n    +/**\n    + * Creates a list of \n{@link TokenStream}\n where each stream is the tokens that make up a finite string in graph token stream.  To do this,\n    + * the graph token stream is converted to an \n{@link Automaton}\n and from there we use a \n{@link FiniteStringsIterator}\n to collect the various\n    + * token streams for each finite string.\n    + */\n    +public class GraphTokenStreamFiniteStrings {\n    +  /* TODO:\n    +     Most of this is a combination of code from TermAutomatonQuery and TokenStreamToTermAutomatonQuery. Would be\n    +     good to make this so it could be shared. */\n    +  private final Automaton.Builder builder;\n    +  Automaton det;\n    +  private final Map<BytesRef, Integer> termToID = new HashMap<>();\n    +  private final Map<Integer, BytesRef> idToTerm = new HashMap<>();\n    +  private int anyTermID = -1;\n    +\n    +  public GraphTokenStreamFiniteStrings() \n{\n    +    this.builder = new Automaton.Builder();\n    +  }\n    +\n    +  private static class BytesRefArrayTokenStream extends TokenStream {\n    +    private final BytesTermAttribute termAtt = addAttribute(BytesTermAttribute.class);\n    +    private final BytesRef[] terms;\n    +    private int offset;\n    +\n    +    BytesRefArrayTokenStream(BytesRef[] terms) \n{\n    +      this.terms = terms;\n    +      offset = 0;\n    +    }\n    +\n    +    @Override\n    +    public boolean incrementToken() throws IOException {\n    +      if (offset < terms.length) \n{\n    +        clearAttributes();\n    +        termAtt.setBytesRef(terms[offset]);\n    +        offset = offset + 1;\n    +        return true;\n    +      }\n    +\n    +      return false;\n    +    }\n    +  }\n    +\n    +  /**\n    +   * Gets the list of finite string token streams from the given input graph token stream.\n    +   */\n    +  public List<TokenStream> getTokenStreams(final TokenStream in) throws IOException {\n    +    // build automation\n    +    build(in);\n    +\n    +    List<TokenStream> tokenStreams = new ArrayList<>();\n    +    final FiniteStringsIterator finiteStrings = new FiniteStringsIterator(det);\n    +    for (IntsRef string; (string = finiteStrings.next()) != null; ) {\n    +      final BytesRef[] tokens = new BytesRef[string.length];\n    +      for (int idx = string.offset, len = string.offset + string.length; idx < len; idx++) \n{\n    +        tokens[idx - string.offset] = idToTerm.get(string.ints[idx]);\n    +      }\n    +\n    +      tokenStreams.add(new BytesRefArrayTokenStream(tokens));\n    +    }\n    +\n    +    return tokenStreams;\n    +  }\n    +\n    +  private void build(final TokenStream in) throws IOException {\n    +    if (det != null) \n{\n    +      throw new IllegalStateException(\"Automation already built\");\n    +    }\n    +\n    +    final TermToBytesRefAttribute termBytesAtt = in.addAttribute(TermToBytesRefAttribute.class);\n    +    final PositionIncrementAttribute posIncAtt = in.addAttribute(PositionIncrementAttribute.class);\n    +    final PositionLengthAttribute posLengthAtt = in.addAttribute(PositionLengthAttribute.class);\n    +    final OffsetAttribute offsetAtt = in.addAttribute(OffsetAttribute.class);\n    +\n    +    in.reset();\n    +\n    +    int pos = -1;\n    +    int lastPos = 0;\n    +    int maxOffset = 0;\n    +    int maxPos = -1;\n    +    int state = -1;\n    +    while (in.incrementToken()) {\n    +      int posInc = posIncAtt.getPositionIncrement();\n    +      assert pos > -1 || posInc > 0;\n    +\n    +      if (posInc > 1) {\n    \u2014 End diff \u2013\n\n    @dsmiley That was actually pulled out of the existing [TokenStreamToTermAutomatonQuery.java](https://github.com/apache/lucene-solr/blob/master/lucene/sandbox/src/java/org/apache/lucene/search/TokenStreamToTermAutomatonQuery.java#L77).  Let me look into it more. ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15786980",
            "date": "2016-12-30T05:59:34+0000",
            "content": "David Smiley . Thank you for the review!  I was able to come up with a way to preserve position increment gaps.  Can you please take another look?  \n\nMichael McCandless Can you please have another look as well? ",
            "author": "Matt Weber"
        },
        {
            "id": "comment-15787319",
            "date": "2016-12-30T09:37:46+0000",
            "content": "Thanks Matt Weber; I will have a look, probably once back from vacation next year. ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-15787395",
            "date": "2016-12-30T10:19:38+0000",
            "content": "Github user mikemccand commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/129#discussion_r94216469\n\n    \u2014 Diff: lucene/core/src/java/org/apache/lucene/util/graph/GraphTokenStreamFiniteStrings.java \u2014\n    @@ -210,82 +215,20 @@ private void finish() {\n        */\n       private void finish(int maxDeterminizedStates) {\n         Automaton automaton = builder.finish();\n    -\n\n\t// System.out.println(\"before det:\\n\" + automaton.toDot());\n    -\n\tTransition t = new Transition();\n    -\n\t// TODO: should we add \"eps back to initial node\" for all states,\n\t// and det that?  then we don't need to revisit initial node at\n\t// every position?  but automaton could blow up?  And, this makes it\n\t// harder to skip useless positions at search time?\n    -\n\tif (anyTermID != -1) {\n    -\n\t// Make sure there are no leading or trailing ANY:\n\tint count = automaton.initTransition(0, t);\n\tfor (int i = 0; i < count; i++) {\n\tautomaton.getNextTransition(t);\n\tif (anyTermID >= t.min && anyTermID <= t.max) \n{\n    -          throw new IllegalStateException(\"automaton cannot lead with an ANY transition\");\n    -        }\n\t}\n    -\n\tint numStates = automaton.getNumStates();\n\tfor (int i = 0; i < numStates; i++) {\n\tcount = automaton.initTransition(i, t);\n\tfor (int j = 0; j < count; j++) {\n\tautomaton.getNextTransition(t);\n\tif (automaton.isAccept(t.dest) && anyTermID >= t.min && anyTermID <= t.max) \n{\n    -            throw new IllegalStateException(\"automaton cannot end with an ANY transition\");\n    -          }\n\t}\n\t}\n    -\n\tint termCount = termToID.size();\n    -\n\t// We have to carefully translate these transitions so automaton\n\t// realizes they also match all other terms:\n\tAutomaton newAutomaton = new Automaton();\n\tfor (int i = 0; i < numStates; i++) \n{\n    -        newAutomaton.createState();\n    -        newAutomaton.setAccept(i, automaton.isAccept(i));\n    -      }\n    -\n\tfor (int i = 0; i < numStates; i++) {\n\tcount = automaton.initTransition(i, t);\n\tfor (int j = 0; j < count; j++) {\n\tautomaton.getNextTransition(t);\n\tint min, max;\n\tif (t.min <= anyTermID && anyTermID <= t.max) \n{\n    -            // Match any term\n    -            min = 0;\n    -            max = termCount - 1;\n    -          }\n else \n{\n    -            min = t.min;\n    -            max = t.max;\n    -          }\n\tnewAutomaton.addTransition(t.source, t.dest, min, max);\n\t}\n\t}\n\tnewAutomaton.finishState();\n\tautomaton = newAutomaton;\n\t}\n    -\n         det = Operations.removeDeadStates(Operations.determinize(automaton, maxDeterminizedStates));\n       }\n\n\n\n\n\tprivate int getTermID(BytesRef term) {\n    +  private int getTermID(int incr, BytesRef term) {\n         Integer id = termToID.get(term);\n\tif (id == null) {\n    +    if (incr > 1 || id == null) {\n\t\n\t\t\n\t\t\n\t\t\tEnd diff \u2013\n\t\t\n\t\t\n\t\n\t\n\n\n\n    Hmm doesn't this mean that if the same term shows up, but with different `incr`, that it will get different `id` assigned?  But I think that is actually fine, since nowhere here do we depend on / expect that the same term must have the same id. ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15787396",
            "date": "2016-12-30T10:19:38+0000",
            "content": "Github user mikemccand commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/129#discussion_r94215751\n\n    \u2014 Diff: lucene/core/src/java/org/apache/lucene/util/graph/GraphTokenStreamFiniteStrings.java \u2014\n    @@ -114,21 +127,20 @@ private void build(final TokenStream in) throws IOException {\n         in.reset();\n\n         int pos = -1;\n\n\tint lastPos = 0;\n    +    int lastIncr = 1;\n         int maxOffset = 0;\n         int maxPos = -1;\n         int state = -1;\n         while (in.incrementToken()) {\n           int posInc = posIncAtt.getPositionIncrement();\n\tassert pos > -1 || posInc > 0;\n\n\n\n\n\tif (posInc > 1) \n{\n    -        throw new IllegalArgumentException(\"cannot handle holes; to accept any term, use '*' term\");\n    -      }\n    +      // always use inc 1 while building, but save original increment\n    +      int fakePosInc = posInc > 1 ? 1 : posInc;\n\t\n\t\t\n\t\t\n\t\t\tEnd diff \u2013\n\t\t\n\t\t\n\t\n\t\n\n\n\n    Maybe just `Math.min(1, posInc)` instead? ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15787397",
            "date": "2016-12-30T10:19:38+0000",
            "content": "Github user mikemccand commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/129#discussion_r94216511\n\n    \u2014 Diff: lucene/core/src/java/org/apache/lucene/util/graph/GraphTokenStreamFiniteStrings.java \u2014\n    @@ -174,25 +191,13 @@ private void setAccept(int state, boolean accept) {\n       /**\n\n\tAdds a transition to the automaton.\n        */\n\n\n\tprivate void addTransition(int source, int dest, String term) \n{\n    -    addTransition(source, dest, new BytesRef(term));\n    -  }\n    -\n\t/**\n\t* Adds a transition to the automaton.\n\t*/\n\tprivate void addTransition(int source, int dest, BytesRef term) {\n    +  private int addTransition(int source, int dest, int incr, BytesRef term) {\n         if (term == null) {\n\t\n\t\t\n\t\t\n\t\t\tEnd diff \u2013\n\t\t\n\t\t\n\t\n\t\n\n\n\n    This can become an assert? ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15787413",
            "date": "2016-12-30T10:26:58+0000",
            "content": "Github user mikemccand commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/129#discussion_r94217160\n\n    \u2014 Diff: lucene/core/src/java/org/apache/lucene/util/graph/GraphTokenStreamFiniteStrings.java \u2014\n    @@ -0,0 +1,237 @@\n    +/*\n    + * Licensed to the Apache Software Foundation (ASF) under one or more\n    + * contributor license agreements.  See the NOTICE file distributed with\n    + * this work for additional information regarding copyright ownership.\n    + * The ASF licenses this file to You under the Apache License, Version 2.0\n    + * (the \"License\"); you may not use this file except in compliance with\n    + * the License.  You may obtain a copy of the License at\n    + *\n    + *     http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package org.apache.lucene.util.graph;\n    +\n    +import java.io.IOException;\n    +import java.util.ArrayList;\n    +import java.util.HashMap;\n    +import java.util.List;\n    +import java.util.Map;\n    +\n    +import org.apache.lucene.analysis.TokenStream;\n    +import org.apache.lucene.analysis.tokenattributes.BytesTermAttribute;\n    +import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;\n    +import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;\n    +import org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute;\n    +import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n    +import org.apache.lucene.util.BytesRef;\n    +import org.apache.lucene.util.IntsRef;\n    +import org.apache.lucene.util.automaton.Automaton;\n    +import org.apache.lucene.util.automaton.FiniteStringsIterator;\n    +import org.apache.lucene.util.automaton.Operations;\n    +\n    +import static org.apache.lucene.util.automaton.Operations.DEFAULT_MAX_DETERMINIZED_STATES;\n    +\n    +/**\n    + * Creates a list of \n{@link TokenStream}\n where each stream is the tokens that make up a finite string in graph token stream.  To do this,\n    + * the graph token stream is converted to an \n{@link Automaton}\n and from there we use a \n{@link FiniteStringsIterator}\n to collect the various\n    + * token streams for each finite string.\n    + */\n    +public class GraphTokenStreamFiniteStrings {\n    +  /* TODO:\n    +     Most of this is a combination of code from TermAutomatonQuery and TokenStreamToTermAutomatonQuery. Would be\n    +     good to make this so it could be shared. */\n    +  private final Automaton.Builder builder;\n    +  Automaton det;\n    +  private final Map<BytesRef, Integer> termToID = new HashMap<>();\n    +  private final Map<Integer, BytesRef> idToTerm = new HashMap<>();\n    +  private final Map<Integer, Integer> idToInc = new HashMap<>();\n    +\n    +  public GraphTokenStreamFiniteStrings() \n{\n    +    this.builder = new Automaton.Builder();\n    +  }\n    +\n    +  private static class BytesRefArrayTokenStream extends TokenStream {\n    +    private final BytesTermAttribute termAtt = addAttribute(BytesTermAttribute.class);\n    +    private final PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);\n    +\n    +    private final BytesRef[] terms;\n    +    private final int[] increments;\n    +    private int offset;\n    +\n    +    BytesRefArrayTokenStream(BytesRef[] terms, int[] increments) \n{\n    +      this.terms = terms;\n    +      this.increments = increments;\n    +      assert terms.length == increments.length;\n    +      offset = 0;\n    +    }\n    +\n    +    @Override\n    +    public boolean incrementToken() throws IOException {\n    +      if (offset < terms.length) \n{\n    +        clearAttributes();\n    +        termAtt.setBytesRef(terms[offset]);\n    +        posIncAtt.setPositionIncrement(increments[offset]);\n    +        offset = offset + 1;\n    +        return true;\n    +      }\n    +\n    +      return false;\n    +    }\n    +  }\n    +\n    +  /**\n    +   * Gets the list of finite string token streams from the given input graph token stream.\n    +   */\n    +  public List<TokenStream> getTokenStreams(final TokenStream in) throws IOException {\n    \u2014 End diff \u2013\n\n    Could we make this method private, make this class's constructor private, and add a `static` method here, the sole public method on this class, that receives the incoming `TokenStream` and returns the resulting `TokenStream[]`?  Otherwise the API is sort of awkard, since e.g. this method seems like a getter yet it's doing lots of side-effects under the hood ... ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15787421",
            "date": "2016-12-30T10:31:47+0000",
            "content": "Github user mikemccand commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/129#discussion_r94217475\n\n    \u2014 Diff: lucene/core/src/java/org/apache/lucene/util/graph/GraphTokenStreamFiniteStrings.java \u2014\n    @@ -0,0 +1,237 @@\n    +/*\n    + * Licensed to the Apache Software Foundation (ASF) under one or more\n    + * contributor license agreements.  See the NOTICE file distributed with\n    + * this work for additional information regarding copyright ownership.\n    + * The ASF licenses this file to You under the Apache License, Version 2.0\n    + * (the \"License\"); you may not use this file except in compliance with\n    + * the License.  You may obtain a copy of the License at\n    + *\n    + *     http://www.apache.org/licenses/LICENSE-2.0\n    + *\n    + * Unless required by applicable law or agreed to in writing, software\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    + * See the License for the specific language governing permissions and\n    + * limitations under the License.\n    + */\n    +\n    +package org.apache.lucene.util.graph;\n    +\n    +import java.io.IOException;\n    +import java.util.ArrayList;\n    +import java.util.HashMap;\n    +import java.util.List;\n    +import java.util.Map;\n    +\n    +import org.apache.lucene.analysis.TokenStream;\n    +import org.apache.lucene.analysis.tokenattributes.BytesTermAttribute;\n    +import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;\n    +import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;\n    +import org.apache.lucene.analysis.tokenattributes.PositionLengthAttribute;\n    +import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;\n    +import org.apache.lucene.util.BytesRef;\n    +import org.apache.lucene.util.IntsRef;\n    +import org.apache.lucene.util.automaton.Automaton;\n    +import org.apache.lucene.util.automaton.FiniteStringsIterator;\n    +import org.apache.lucene.util.automaton.Operations;\n    +\n    +import static org.apache.lucene.util.automaton.Operations.DEFAULT_MAX_DETERMINIZED_STATES;\n    +\n    +/**\n    + * Creates a list of \n{@link TokenStream}\n where each stream is the tokens that make up a finite string in graph token stream.  To do this,\n    + * the graph token stream is converted to an \n{@link Automaton}\n and from there we use a \n{@link FiniteStringsIterator}\n to collect the various\n    + * token streams for each finite string.\n    + */\n    +public class GraphTokenStreamFiniteStrings {\n    +  /* TODO:\n    +     Most of this is a combination of code from TermAutomatonQuery and TokenStreamToTermAutomatonQuery. Would be\n    +     good to make this so it could be shared. */\n    +  private final Automaton.Builder builder;\n    +  Automaton det;\n    +  private final Map<BytesRef, Integer> termToID = new HashMap<>();\n    +  private final Map<Integer, BytesRef> idToTerm = new HashMap<>();\n    +  private final Map<Integer, Integer> idToInc = new HashMap<>();\n    +\n    +  public GraphTokenStreamFiniteStrings() \n{\n    +    this.builder = new Automaton.Builder();\n    +  }\n    +\n    +  private static class BytesRefArrayTokenStream extends TokenStream {\n    +    private final BytesTermAttribute termAtt = addAttribute(BytesTermAttribute.class);\n    +    private final PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);\n    +\n    +    private final BytesRef[] terms;\n    +    private final int[] increments;\n    +    private int offset;\n    +\n    +    BytesRefArrayTokenStream(BytesRef[] terms, int[] increments) \n{\n    +      this.terms = terms;\n    +      this.increments = increments;\n    +      assert terms.length == increments.length;\n    +      offset = 0;\n    +    }\n    +\n    +    @Override\n    +    public boolean incrementToken() throws IOException {\n    +      if (offset < terms.length) \n{\n    +        clearAttributes();\n    +        termAtt.setBytesRef(terms[offset]);\n    +        posIncAtt.setPositionIncrement(increments[offset]);\n    +        offset = offset + 1;\n    +        return true;\n    +      }\n    +\n    +      return false;\n    +    }\n    +  }\n    +\n    +  /**\n    +   * Gets the list of finite string token streams from the given input graph token stream.\n    +   */\n    +  public List<TokenStream> getTokenStreams(final TokenStream in) throws IOException {\n    +    // build automation\n    +    build(in);\n    +\n    +    List<TokenStream> tokenStreams = new ArrayList<>();\n    +    final FiniteStringsIterator finiteStrings = new FiniteStringsIterator(det);\n    +    for (IntsRef string; (string = finiteStrings.next()) != null; ) {\n    +      final BytesRef[] tokens = new BytesRef[string.length];\n    +      final int[] increments = new int[string.length];\n    +      for (int idx = string.offset, len = string.offset + string.length; idx < len; idx++) {\n    +        int id = string.ints[idx];\n    +        int offset = idx - string.offset;\n    +        tokens[offset] = idToTerm.get(id);\n    +        if (idToInc.containsKey(id)) \n{\n    +          increments[offset] = idToInc.get(id);\n    +        }\n else \n{\n    +          increments[offset] = 1;\n    +        }\n    +      }\n    +\n    +      tokenStreams.add(new BytesRefArrayTokenStream(tokens, increments));\n    +    }\n    +\n    +    return tokenStreams;\n    +  }\n    +\n    +  private void build(final TokenStream in) throws IOException {\n    +    if (det != null) \n{\n    +      throw new IllegalStateException(\"Automation already built\");\n    +    }\n    +\n    +    final TermToBytesRefAttribute termBytesAtt = in.addAttribute(TermToBytesRefAttribute.class);\n    +    final PositionIncrementAttribute posIncAtt = in.addAttribute(PositionIncrementAttribute.class);\n    +    final PositionLengthAttribute posLengthAtt = in.addAttribute(PositionLengthAttribute.class);\n    +    final OffsetAttribute offsetAtt = in.addAttribute(OffsetAttribute.class);\n    +\n    +    in.reset();\n    +\n    +    int pos = -1;\n    +    int lastIncr = 1;\n    +    int maxOffset = 0;\n    +    int maxPos = -1;\n    +    int state = -1;\n    +    while (in.incrementToken()) {\n    +      int posInc = posIncAtt.getPositionIncrement();\n    +\n    +      // always use inc 1 while building, but save original increment\n    +      int fakePosInc = posInc > 1 ? 1 : posInc;\n    +\n    +      assert pos > -1 || fakePosInc > 0;\n    \u2014 End diff \u2013\n\n    Can we upgrade this to a real `if`?  I.e. we need a well-formed `TokenStream` input ... it cannot have `posInc=0` on its first token. ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15787983",
            "date": "2016-12-30T16:50:58+0000",
            "content": "Github user mattweber commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/129\n\n    @mikemccand I addressed you comments.  I also added some more tests and fixed a bug that would yield wrong increment when a term that had previously been seen was found again with an increment of 0.  Tests were added.  I have squashed these changes with the previous commit so it is clear to see the difference between the original PR which did not support position increments and the new one that does. ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15788035",
            "date": "2016-12-30T17:26:53+0000",
            "content": "Github user dsmiley commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/129#discussion_r94243375\n\n    \u2014 Diff: lucene/core/src/java/org/apache/lucene/util/graph/GraphTokenStreamFiniteStrings.java \u2014\n    @@ -210,85 +199,41 @@ private void finish() {\n        */\n       private void finish(int maxDeterminizedStates) {\n         Automaton automaton = builder.finish();\n    -\n    \u2014 End diff \u2013\n\n    So all this code here removed wasn't needed after all?  It's nice to see it all go away (less to maintain / less complexity)  ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15788036",
            "date": "2016-12-30T17:26:53+0000",
            "content": "Github user dsmiley commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/129#discussion_r94243010\n\n    \u2014 Diff: lucene/core/src/java/org/apache/lucene/util/graph/GraphTokenStreamFiniteStrings.java \u2014\n    @@ -210,85 +199,41 @@ private void finish() {\n        */\n       private void finish(int maxDeterminizedStates) {\n         Automaton automaton = builder.finish();\n    -\n\n\t// System.out.println(\"before det:\\n\" + automaton.toDot());\n    -\n\tTransition t = new Transition();\n    -\n\t// TODO: should we add \"eps back to initial node\" for all states,\n\t// and det that?  then we don't need to revisit initial node at\n\t// every position?  but automaton could blow up?  And, this makes it\n\t// harder to skip useless positions at search time?\n    -\n\tif (anyTermID != -1) {\n    -\n\t// Make sure there are no leading or trailing ANY:\n\tint count = automaton.initTransition(0, t);\n\tfor (int i = 0; i < count; i++) {\n\tautomaton.getNextTransition(t);\n\tif (anyTermID >= t.min && anyTermID <= t.max) \n{\n    -          throw new IllegalStateException(\"automaton cannot lead with an ANY transition\");\n    -        }\n\t}\n    -\n\tint numStates = automaton.getNumStates();\n\tfor (int i = 0; i < numStates; i++) {\n\tcount = automaton.initTransition(i, t);\n\tfor (int j = 0; j < count; j++) {\n\tautomaton.getNextTransition(t);\n\tif (automaton.isAccept(t.dest) && anyTermID >= t.min && anyTermID <= t.max) \n{\n    -            throw new IllegalStateException(\"automaton cannot end with an ANY transition\");\n    -          }\n\t}\n\t}\n    -\n\tint termCount = termToID.size();\n    -\n\t// We have to carefully translate these transitions so automaton\n\t// realizes they also match all other terms:\n\tAutomaton newAutomaton = new Automaton();\n\tfor (int i = 0; i < numStates; i++) \n{\n    -        newAutomaton.createState();\n    -        newAutomaton.setAccept(i, automaton.isAccept(i));\n    -      }\n    -\n\tfor (int i = 0; i < numStates; i++) {\n\tcount = automaton.initTransition(i, t);\n\tfor (int j = 0; j < count; j++) {\n\tautomaton.getNextTransition(t);\n\tint min, max;\n\tif (t.min <= anyTermID && anyTermID <= t.max) \n{\n    -            // Match any term\n    -            min = 0;\n    -            max = termCount - 1;\n    -          }\n else \n{\n    -            min = t.min;\n    -            max = t.max;\n    -          }\n\tnewAutomaton.addTransition(t.source, t.dest, min, max);\n\t}\n\t}\n\tnewAutomaton.finishState();\n\tautomaton = newAutomaton;\n\t}\n    -\n         det = Operations.removeDeadStates(Operations.determinize(automaton, maxDeterminizedStates));\n       }\n\n\n\n\n\tprivate int getTermID(BytesRef term) {\n\tInteger id = termToID.get(term);\n\tif (id == null) {\n\tid = termToID.size();\n\tif (term != null) \n{\n    -        term = BytesRef.deepCopyOf(term);\n    -      }\n\ttermToID.put(term, id);\n    +  /**\n    +   * Gets an integer id for a given term.\n    +   *\n    +   * If there is no position gaps for this token then we can reuse the id for the same term if it appeared at another\n    +   * position without a gap.  If we have a position gap generate a new id so we can keep track of the position\n    +   * increment.\n    +   */\n    +  private int getTermID(int incr, int prevIncr, BytesRef term) {\n    +    assert term != null;\n    +    boolean isStackedGap = incr == 0 && prevIncr > 1;\n    +    boolean hasGap = incr > 1;\n    +    term = BytesRef.deepCopyOf(term);\n\t\n\t\t\n\t\t\n\t\t\tEnd diff \u2013\n\t\t\n\t\t\n\t\n\t\n\n\n\n    The deepCopyOf is only needed if you generate a new ID, not for an existing one.  \n\n    BTW... have you seen BytesRefHash?  I think re-using that could minimize the code here to deal with this stuff. ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15788037",
            "date": "2016-12-30T17:26:53+0000",
            "content": "Github user dsmiley commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/129#discussion_r94244009\n\n    \u2014 Diff: lucene/core/src/java/org/apache/lucene/util/graph/GraphTokenStreamFiniteStrings.java \u2014\n    @@ -80,22 +77,41 @@ public boolean incrementToken() throws IOException {\n         }\n       }\n\n    +  private GraphTokenStreamFiniteStrings() \n{\n    +    this.builder = new Automaton.Builder();\n    +  }\n    +\n       /**\n\n\tGets the list of finite string token streams from the given input graph token stream.\n        */\n\n\n\tpublic List<TokenStream> getTokenStreams(final TokenStream in) throws IOException {\n\t// build automation\n    +  public static List<TokenStream> getTokenStreams(final TokenStream in) throws IOException \n{\n    +    GraphTokenStreamFiniteStrings gfs = new GraphTokenStreamFiniteStrings();\n    +    return gfs.process(in);\n    +  }\n    +\n    +  /**\n    +   * Builds automaton and builds the finite string token streams.\n    +   */\n    +  private List<TokenStream> process(final TokenStream in) throws IOException {\n         build(in);\n\n\n\n         List<TokenStream> tokenStreams = new ArrayList<>();\n         final FiniteStringsIterator finiteStrings = new FiniteStringsIterator(det);\n         for (IntsRef string; (string = finiteStrings.next()) != null; ) {\n           final BytesRef[] tokens = new BytesRef[string.length];\n    \u2014 End diff \u2013\n\n    Hmm; rather than materializing an array of tokens and increments, maybe you could simply give the IntsRefString  to BytesRefArrayTokenStream (and make BRATS not static) so that it could do this on the fly?  Not a big deal either way (current or my proposal).  If you do as I suggest then BRATS would no longer be a suitable name; maybe simply FiniteStringTokenStream or CustomTokenStream. ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15788038",
            "date": "2016-12-30T17:26:53+0000",
            "content": "Github user dsmiley commented on a diff in the pull request:\n\n    https://github.com/apache/lucene-solr/pull/129#discussion_r94241922\n\n    \u2014 Diff: lucene/core/src/java/org/apache/lucene/util/graph/GraphTokenStreamFiniteStrings.java \u2014\n    @@ -80,22 +77,41 @@ public boolean incrementToken() throws IOException {\n         }\n       }\n\n    +  private GraphTokenStreamFiniteStrings() {\n    +    this.builder = new Automaton.Builder();\n    \u2014 End diff \u2013\n\n    The other fields are initialized at the declaration; might as well move this here too? ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15788263",
            "date": "2016-12-30T20:14:07+0000",
            "content": "Github user mattweber commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/129\n\n    Thanks @dsmiley!  I have just pushed up code with your suggestions except for using `BytesRefHash` due to the fact we might have the same `BytesRef` but need a different id because we have position gap.\n\n    This has been great, love the feedback! ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15789338",
            "date": "2016-12-31T10:59:57+0000",
            "content": "Github user mikemccand commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/129\n\n    This change looks great to me!  What an awesome improvement, to properly use graph token streams at search time so multi-token synonyms are correct.\n\n    I'll push this in a few days once I'm back home unless someone pushes first (@dsmiley feel free)...\n\n    Thank you @mattweber! ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15789729",
            "date": "2016-12-31T16:41:10+0000",
            "content": "Great thank you both!  I have updated the brach_6x PR with the latest changes as well as rebased+squashed both.  Happy New Year! ",
            "author": "Matt Weber"
        },
        {
            "id": "comment-15794693",
            "date": "2017-01-03T10:15:52+0000",
            "content": "Commit 1bcf9a251d597cdc029295325b287ce5ce661bec in lucene-solr's branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1bcf9a2 ]\n\nLUCENE-7603: add CHANGES entry ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-15794706",
            "date": "2017-01-03T10:22:37+0000",
            "content": "Github user mikemccand commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/129\n\n    I've merged this into Lucene's master (7.0), and I'm working on 6.x (#130) now.  Thanks @mattweber! Can you close this? ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15794725",
            "date": "2017-01-03T10:32:27+0000",
            "content": "Commit 018df31da8b6b5beeb767c90d7ef2a784eca354a in lucene-solr's branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=018df31 ]\n\nLUCENE-7603: add package-info.java for new package ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-15794739",
            "date": "2017-01-03T10:38:57+0000",
            "content": "Commit c980f6a1c2a33a039d09a83cd5b9b95a58fa784f in lucene-solr's branch refs/heads/branch_6x from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c980f6a ]\n\nLUCENE-7603: handle graph token streams in query parsers ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-15794741",
            "date": "2017-01-03T10:39:32+0000",
            "content": "Github user mikemccand commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/130\n\n    I've merged this into Lucene's branch_6x; thank you @mattweber!  Can you please close this now? ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15794743",
            "date": "2017-01-03T10:39:45+0000",
            "content": "Github user asfgit closed the pull request at:\n\n    https://github.com/apache/lucene-solr/pull/130 ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15794745",
            "date": "2017-01-03T10:40:20+0000",
            "content": "Thank you Matt Weber! ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-15795128",
            "date": "2017-01-03T14:02:32+0000",
            "content": "Github user mattweber closed the pull request at:\n\n    https://github.com/apache/lucene-solr/pull/129 ",
            "author": "ASF GitHub Bot"
        },
        {
            "id": "comment-15871563",
            "date": "2017-02-17T09:53:54+0000",
            "content": "Github user EreMaijala commented on the issue:\n\n    https://github.com/apache/lucene-solr/pull/130\n\n    I believe this has caused https://issues.apache.org/jira/browse/LUCENE-7698. ",
            "author": "ASF GitHub Bot"
        }
    ]
}