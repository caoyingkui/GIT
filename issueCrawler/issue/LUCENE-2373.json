{
    "id": "LUCENE-2373",
    "title": "Create a Codec to work with streaming and append-only filesystems",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Since early 2.x times Lucene used a skip/seek/write trick to patch the length of the terms dict into a place near the start of the output data file. This however made it impossible to use Lucene with append-only filesystems such as HDFS.\n\nIn the post-flex trunk the following code in StandardTermsDictWriter initiates this:\n\n    // Count indexed fields up front\n    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT); \n\n    out.writeLong(0);                             // leave space for end index pointer\n\n\nand completes this in close():\n\n      out.seek(CodecUtil.headerLength(CODEC_NAME));\n      out.writeLong(dirStart);\n\n\n\nI propose to change this layout so that this pointer is stored simply at the end of the file. It's always 8 bytes long, and we known the final length of the file from Directory, so it's a single additional seek(length - 8) to read it, which is not much considering the benefits.",
    "attachments": {
        "LUCENE-2373.patch": "https://issues.apache.org/jira/secure/attachment/12448710/LUCENE-2373.patch",
        "LUCENE-2372-2.patch": "https://issues.apache.org/jira/secure/attachment/12448973/LUCENE-2372-2.patch",
        "appending.patch": "https://issues.apache.org/jira/secure/attachment/12448106/appending.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-04-06T23:03:09+0000",
            "content": "Just noticed that the same problem exists in SimpleStandardTermsIndexWriter, and I propose the same solution there. ",
            "author": "Andrzej Bialecki",
            "id": "comment-12854240"
        },
        {
            "date": "2010-04-06T23:30:11+0000",
            "content": "And then IndexOutput.seek() can be deleted. Cool. ",
            "author": "Earwin Burrfoot",
            "id": "comment-12854254"
        },
        {
            "date": "2010-04-07T10:10:12+0000",
            "content": "I would love to make Lucene truly write once (and moreve IndexOutput.seek), but... this approach makes me a little nervous...\n\nIn some environments, relying on the length of the file to be accurate might be risky: it's metadata, that can be subject to different client-side caching than the file's contents.  EG on NFS I've seen issues where the file length was stale yet the file contents were not.\n\nMaybe we could offer a separate codec that takes this approach, for use on filesystems like HDFS that can't seek during write?  We should refactor standard codec so that \"where this long gets stored\" can be easily overridden by a subclass.\n\nOr, alternatively, we could write this \"index of the index\" to a separate file? ",
            "author": "Michael McCandless",
            "id": "comment-12854409"
        },
        {
            "date": "2010-04-12T07:37:15+0000",
            "content": "I'd rather not count on file length as well ... so a put/getTermDictSize method on Codec will allow one to implement it however one wants, if running on HDFS for example? ",
            "author": "Shai Erera",
            "id": "comment-12855877"
        },
        {
            "date": "2010-04-27T03:28:07+0000",
            "content": "Does this make it possible to add a good checksum? \n\nThe Cloud and NRT architectures involve copying lots of segment files around, and disk&RAM&network bandwidth all have error rates. It would be great if the process of making an index file included, on the fly, the creation of a solid checksum that is then baked into the file at the last moment. It should also be in the segments.gen file, but it is more important that the file should have the checksum embedded such that walking the whole file gives a fixed value. ",
            "author": "Lance Norskog",
            "id": "comment-12861214"
        },
        {
            "date": "2010-04-27T07:25:41+0000",
            "content": "Aggregated comments...\n\nMike: I'd hate to add yet another file just for this purpose. Long-term it's perhaps worth it. Short-term for HDFS use case it would be enough to provide a method to write a header and a trailer. Codecs that can seek/overwrite would just use the header, codecs that can't would use both. Codecs that operate on filesystems with unreliable fileLength could write a sync marker before the trailer, and there could be a back-tracking mechanism that starts from the reported fileLength and then tries to find the sync marker (reading back, and/or ahead).\n\nShai: hm, but this would require a separate file that stores the header, right?\n\nLance: yes. The original use case I had in mind was HDFS (Hadoop File System) which already implements on-the-fly checksums. If we go the way that Mike suggested, i.e. implementing a separate codec, then this should be a simple addition. We could also perhaps structure this as a codec wrapper so that this capability can be applied to other codecs too. ",
            "author": "Andrzej Bialecki",
            "id": "comment-12861281"
        },
        {
            "date": "2010-04-27T10:47:06+0000",
            "content": "\nMike: I'd hate to add yet another file just for this purpose. Long-term it's perhaps worth it. Short-term for HDFS use case it would be enough to provide a method to write a header and a trailer. Codecs that can seek/overwrite would just use the header, codecs that can't would use both.\nI think that's a good plan \u2013 abstract the header write/read methods so that another codec can easily subclass to change how/where these are written.  I think Lucene's default (standard) codec should continue to do what it does now?  And then HDFS can take the standard codec, and subclass StandardTermsDictWriter/Reader to put the header at the end.\n\nCodecs that operate on filesystems with unreliable fileLength could write a sync marker before the trailer, and there could be a back-tracking mechanism that starts from the reported fileLength and then tries to find the sync marker (reading back, and/or ahead).\n\nCan't we just use the current standard codec's approach by default?  Back-tracking seems dangerous.  Eg what if .fileLength() is too small on such filesystems?\n\nDoes this make it possible to add a good checksum?\n\nA codec could easily do this, today \u2013 it's orthogonal to using HDFS.  EG Lucene already has a ChecksumIndexOutput/Input, so this should be a simple cutover in standard codec (though we would need to fix up the classes, eg to make \"get me the IndexOutput/Input\" method, so a subclass could override). ",
            "author": "Michael McCandless",
            "id": "comment-12861356"
        },
        {
            "date": "2010-04-27T11:04:43+0000",
            "content": "I think that's a good plan - abstract the header write/read methods so that another codec can easily subclass to change how/where these are written. I think Lucene's default (standard) codec should continue to do what it does now? And then HDFS can take the standard codec, and subclass StandardTermsDictWriter/Reader to put the header at the end.\n\nAssuming we add writeHeader/writeTrailer methods, the standard codec would write the header as it does today using writeHeader(), and in writeTrailer() it would just patch it the same way it does today.\n\nCodecs that operate on filesystems with unreliable fileLength could write a sync marker before the trailer, and there could be a back-tracking mechanism that starts from the reported fileLength and then tries to find the sync marker (reading back, and/or ahead).\n\nCan't we just use the current standard codec's approach by default? Back-tracking seems dangerous. Eg what if .fileLength() is too small on such filesystems?\n\nYes, of course, I was just dreaming up a filesystem that is both append-only and with unreliable fileLength ... not that I know of any off-hand  ",
            "author": "Andrzej Bialecki",
            "id": "comment-12861361"
        },
        {
            "date": "2010-04-29T21:05:37+0000",
            "content": "Lance: yes. The original use case I had in mind was HDFS (Hadoop File System) which already implements on-the-fly checksums. If we go the way that Mike suggested, i.e. implementing a separate codec, then this should be a simple addition. We could also perhaps structure this as a codec wrapper so that this capability can be applied to other codecs too.\n\n+1 for in Lucene itself. Lots of large installations don't use HDFS to move shards around. Also, the HDFS checksum only counts after the file has touched down at the HDFS portal: there are error rates in local RAM, local hard disk, shared file systems and network I/O. Doing the checksum at the origin is more useful. ",
            "author": "Lance Norskog",
            "id": "comment-12862407"
        },
        {
            "date": "2010-04-29T21:09:25+0000",
            "content": "Grid filesystems like larger blocksizes. HDFS uses a default blocksize of 128k right? At this size, is it worth doing a few merges/optimizes to make a segment fit? This pushes the problem of grid filesystems away from low-level indexing. I would want to index locally and push the index through a separate grid FS access manager. ",
            "author": "Lance Norskog",
            "id": "comment-12862409"
        },
        {
            "date": "2010-04-30T07:22:13+0000",
            "content": "HDFS uses 64 or 128 _Mega_Byte blocks. ",
            "author": "Andrzej Bialecki",
            "id": "comment-12862568"
        },
        {
            "date": "2010-05-06T22:32:52+0000",
            "content": "Another reason to create files in a fully sequential mode is that SSD drives do not like random writes - they can get very slow. SSDs function well with sequential writes, sequential reads, and random reads, so if this issues is fixed, they should work well with Lucene. ",
            "author": "Lance Norskog",
            "id": "comment-12864954"
        },
        {
            "date": "2010-05-06T22:36:54+0000",
            "content": ".bq HDFS uses 64 or 128 _Mega_Byte blocks. \nYet another reason to manage memory carefully. \n\nIt should be possible to hit this watermark by using the NoMergePolicy and a RamBuffer size of 64M or 128M:. Hitting the RAMBuffer size causes a segment to flush to a file with little breakage (unused disk space), and it will never be merged again, cutting HDFS overheads. This should give a predictable and consistent segment writing overhead, right? ",
            "author": "Lance Norskog",
            "id": "comment-12864957"
        },
        {
            "date": "2010-05-07T13:40:33+0000",
            "content": "Good point, Lance - though for larger indexes the number of blocks (hence the number of sub-readers in your scenario) would be substantial, maybe too high. Hadoop doesn't do much of local caching of remote blocks, but I implemented a HDFS Directory in Luke that uses ehcache, and it works quite well. ",
            "author": "Andrzej Bialecki",
            "id": "comment-12865152"
        },
        {
            "date": "2010-06-25T23:35:39+0000",
            "content": "This patch contains an implementation of AppendingCodec and necessary refactorings in  CodecProvider and SegmentInfos to support append-only filesystems. There is a unit test that illustrates the use of the codec and verifies that it works with append-only FS.\n\nNote 1: SegmentInfos write/read methods used the seek/rewrite trick to update the checksum, so it was necessary to extend CodecProvider with methods to provide custom implementations of SegmentInfosWriter/Reader (and default implementations thereof).\n\nNote 2: o.a.l.index.codecs.* doesn't have access to many package-level APIs from o.a.l.index.*, so I had to relax the visibility of some methods and fields. Perhaps this may be tightened back in a later revision...\n\nPatch is relative to the latest trunk (rev. 958137). ",
            "author": "Andrzej Bialecki",
            "id": "comment-12882761"
        },
        {
            "date": "2010-06-26T00:27:50+0000",
            "content": "\nNote 2: o.a.l.index.codecs.* doesn't have access to many package-level APIs from o.a.l.index.*, so I had to relax the visibility of some methods and fields. Perhaps this may be tightened back in a later revision...\n\nHi, I wouldn't worry about this. In general Mike had this problem when moving things to the codec package, so we added a javadocs tag for consistent labeling: @lucene.internal\n\nThis expands to the following text: NOTE: This API is for Lucene internal purposes only and might change in incompatible ways in the next release.\n\nExample usage: http://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/IndexFileNames.java\n\nAdditionally we added another tag: @lucene.experimental, which you can use for any new APIs you introduce that might not have the final stable API (most codecs use this already I think).\n\nThis expands to the following text: WARNING: This API is experimental and might change in incompatible ways in the next release.\n\nExample usage:\nhttp://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/src/java/org/apache/lucene/index/codecs/pulsing/PulsingCodec.java ",
            "author": "Robert Muir",
            "id": "comment-12882782"
        },
        {
            "date": "2010-06-26T00:49:39+0000",
            "content": "Yup, I used @lucene.experimental in this patch. ",
            "author": "Andrzej Bialecki",
            "id": "comment-12882791"
        },
        {
            "date": "2010-06-26T01:20:12+0000",
            "content": "Cool, I think @lucene.internal would be good for SegmentInfo etc that must become public. ",
            "author": "Robert Muir",
            "id": "comment-12882796"
        },
        {
            "date": "2010-06-29T22:01:40+0000",
            "content": "I would appreciate a review and a go/no-go from other committers. Especially regarding the part that changes CodecProvider API by adding SegmentInfoWriter/Reader. ",
            "author": "Andrzej Bialecki",
            "id": "comment-12883725"
        },
        {
            "date": "2010-07-01T09:42:43+0000",
            "content": "This looks great Andrzej!  This gives codecs full control over reading/writing of SegmentInfo/s, which now empowers a Codec to store any per-segment info it needs to (eg, hasProx, which is now a hardwired bit in SegmentInfo, is really a codec level detail).  Probably the codec could return a (private to it) subclass of SegmentInfo to hold such extra info...\n\nMaybe we should provide default impls for CodecProvider.getSegmentInfosReader/Writer?  (Ie returning the Default impls)\n\nAlso, should we factor out the \"leave space for index pointer\" (out.writeLong(0)) to the subclass?  (And, the reading of that dirOffset).  Because this is wasted now for the appending codec... ",
            "author": "Michael McCandless",
            "id": "comment-12884232"
        },
        {
            "date": "2010-07-01T14:22:05+0000",
            "content": "Probably the codec could return a (private to it) subclass of SegmentInfo to hold such extra info... \n\nNice idea, I didn't think about this - yes, this should be possible now.\n\nMaybe we should provide default impls for CodecProvider.getSegmentInfosReader/Writer? (Ie returning the Default impls)\n\nDefaultCodecProvider does exactly this. Or do you mean instead of using abstract methods in CodecProvider?\n\nAlso, should we factor out the \"leave space for index pointer\" (out.writeLong(0)) to the subclass? (And, the reading of that dirOffset). Because this is wasted now for the appending codec...\n\nThe reading is already factored out, but the writing ... Well, it's just 8 bytes per segment ... the reason I didn't factor it out is that it would require additional before/after delegation, or a replication of larger sections of code... ",
            "author": "Andrzej Bialecki",
            "id": "comment-12884291"
        },
        {
            "date": "2010-07-04T11:52:58+0000",
            "content": "DefaultCodecProvider does exactly this. Or do you mean instead of using abstract methods in CodecProvider?\n\nRight, I meant move the default impls into CodecProvider, so an app with a custom CodecProvider need not implement the defaults.\n\nThe reading is already factored out, but the writing ... Well, it's just 8 bytes per segment ... the reason I didn't factor it out is that it would require additional before/after delegation, or a replication of larger sections of code...\n\nI hear you, but it looks sort of hackish to factor out one part (seeking to the dir) but not the other part (writing/reading the dirOffset); but I'm fine w/ committing it like this.  Maybe just add a comment in AppendingTermsDictReader.seekDir that dirOffset, which the writer had written into header of file, is ignored? ",
            "author": "Michael McCandless",
            "id": "comment-12885022"
        },
        {
            "date": "2010-07-04T12:05:03+0000",
            "content": "I hear you, but it looks sort of hackish to factor out one part (seeking to the dir) but not the other part (writing/reading the dirOffset); but I'm fine w/ committing it like this. Maybe just add a comment in AppendingTermsDictReader.seekDir that dirOffset, which the writer had written into header of file, is ignored?\n\nI hear you too  I'll try to factor out the whole section, if this becomes too messy then I'll add a comment. Re: CodecProvider default impls - ok. ",
            "author": "Andrzej Bialecki",
            "id": "comment-12885023"
        },
        {
            "date": "2010-07-05T15:00:44+0000",
            "content": "It wasn't too messy after all - here's an updated patch that incorporates your suggestions. ",
            "author": "Andrzej Bialecki",
            "id": "comment-12885255"
        },
        {
            "date": "2010-07-05T16:18:54+0000",
            "content": "Patch looks great!  Thanks Andrzej.\n\nI tweaked a few things \u2013 added some missing copyrights, removed some unnecessary imports, etc.  I also strengthened the test a bit, by having it write 2 segments and then optimize them, which hit an exception because seek was called when building the compound file doc store (cfx) file.  So I fixed test to also disable that compound-file, and added explanation of this in AppendingCodec's jdocs.\n\nWe still need a CHANGES entry, but... should this go into contrib/misc instead of core?  Few people need to use appending codec? ",
            "author": "Michael McCandless",
            "id": "comment-12885265"
        },
        {
            "date": "2010-07-05T17:26:28+0000",
            "content": "contrib/misc is fine with me. I'll update the patch to include contrib/CHANGES.txt and move the content to contrib/misc. ",
            "author": "Andrzej Bialecki",
            "id": "comment-12885275"
        },
        {
            "date": "2010-07-08T12:55:47+0000",
            "content": "Updated patch. I added comments both in top-level CHANGES and in contrib/CHANGES, to account for two new areas of functionality - the customizable SegmentInfosWriter and the appending codec. If there are no objections I'd like to commit it. ",
            "author": "Andrzej Bialecki",
            "id": "comment-12886314"
        },
        {
            "date": "2010-07-09T10:21:17+0000",
            "content": "Looks great Andrzej!  +1 to commit. ",
            "author": "Michael McCandless",
            "id": "comment-12886671"
        },
        {
            "date": "2010-07-09T21:11:46+0000",
            "content": "Committed to trunk in revision 962694. Thank you all for helping and reviewing this issue! ",
            "author": "Andrzej Bialecki",
            "id": "comment-12886883"
        }
    ]
}