{
    "id": "SOLR-9952",
    "title": "S3BackupRepository",
    "details": {
        "components": [
            "Backup/Restore"
        ],
        "type": "New Feature",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "I'd like to have a backup repository implementation allows to snapshot to AWS S3",
    "attachments": {
        "core-site.xml.template": "https://issues.apache.org/jira/secure/attachment/12851634/core-site.xml.template",
        "0001-SOLR-9952-Added-dependencies-for-hadoop-amazon-integ.patch": "https://issues.apache.org/jira/secure/attachment/12851631/0001-SOLR-9952-Added-dependencies-for-hadoop-amazon-integ.patch",
        "Running Solr on S3.pdf": "https://issues.apache.org/jira/secure/attachment/12847389/Running%20Solr%20on%20S3.pdf",
        "0002-SOLR-9952-Added-integration-test-for-checking-backup.patch": "https://issues.apache.org/jira/secure/attachment/12851632/0002-SOLR-9952-Added-integration-test-for-checking-backup.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-01-13T17:22:55+0000",
            "author": "Kevin Risden",
            "content": "Just a thought that backing up to S3 may already be possible with the HdfsBackupRepository. Hdfs supports reading/writing to S3 with S3N and S3A. It may just have to be documented that this is possible. ",
            "id": "comment-15822019"
        },
        {
            "date": "2017-01-13T18:32:12+0000",
            "author": "Trey Cahill",
            "content": "I've done work to run Solr on S3 via HDFS as Kevin Risden described (he also suggested it).  See the attachment \"Running Solr on S3.pdf\".  I tested the backup/restore using the HDFS backup repository (just back up and restore though). ",
            "id": "comment-15822124"
        },
        {
            "date": "2017-01-13T20:30:08+0000",
            "author": "Hrishikesh Gadre",
            "content": "Kevin Risden Trey Cahill Please note that HDFS backup repository implementation reuses the configuration parameters required by the HdfsDirectory implementation. Hence the suggested approach won't work if you are using HdfsDirectory to store the Solr collection data. While we should certainly use the HDFS/S3 connector for S3BackupRepository, we need to have a separate implementation of BackupRepository interface which has different configuration namespace. That will allow everyone (including Solr + HDFS users) to use this functionality. ",
            "id": "comment-15822319"
        },
        {
            "date": "2017-01-13T21:12:57+0000",
            "author": "Kevin Risden",
            "content": "Hmmm that seems to be not a great idea. Does that mean right now its not possible to provide two different HDFS Backup Repositories each pointing to a different cluster? ",
            "id": "comment-15822355"
        },
        {
            "date": "2017-01-13T21:16:47+0000",
            "author": "Hrishikesh Gadre",
            "content": "Yes I was about to say that! How about modifying HdfsBackupRepository to use different configuration namespace ? ",
            "id": "comment-15822357"
        },
        {
            "date": "2017-01-13T21:18:41+0000",
            "author": "Kevin Risden",
            "content": "It would mean that you would need different namespaces per each repository so that two HdfsBackupRepositories don't interfere with each other. ",
            "id": "comment-15822358"
        },
        {
            "date": "2017-01-13T21:20:27+0000",
            "author": "Hrishikesh Gadre",
            "content": "Ok let me take my statement back. The problem is that we are reusing system properties in the solr.xml. If we use absolute params, we should be OK.\n\n\n<backup>\n    <repository name=\"hdfs\" class=\"org.apache.solr.core.backup.repository.HdfsBackupRepository\" default=\"true\">\n      <str name=\"solr.hdfs.home\">${solr.hdfs.home:}</str>\n      <str name=\"solr.hdfs.confdir\">${solr.hdfs.confdir:}</str>\n      <str name=\"solr.hdfs.security.kerberos.enabled\">${solr.hdfs.security.kerberos.enabled:false}</str>\n      <str name=\"solr.hdfs.security.kerberos.keytabfile\">${solr.hdfs.security.kerberos.keytabfile:}</str>\n      <str name=\"solr.hdfs.security.kerberos.principal\">${solr.hdfs.security.kerberos.principal:}</str>\n      <str name=\"solr.hdfs.permissions.umask-mode\">${solr.hdfs.permissions.umask-mode:000}</str>\n    </repository>\n  </backup>\n\n ",
            "id": "comment-15822363"
        },
        {
            "date": "2017-01-13T21:25:19+0000",
            "author": "Kevin Risden",
            "content": "Ah that makes sense! That is good to know. Thanks Hrishikesh Gadre! ",
            "id": "comment-15822367"
        },
        {
            "date": "2017-01-24T08:56:06+0000",
            "author": "Alexey Suprun",
            "content": "Using manual written by Trey Cahill I launch S3 backup. I added patch wich contains dependencies for hadoop integration with AWS and create class S3N. Unfortunately I don't have any time to add test for AWS backup/restore (something like TestHdfsBackupRestoreCore for HDFS). ",
            "id": "comment-15835866"
        },
        {
            "date": "2017-02-08T13:12:00+0000",
            "author": "Alexey Suprun",
            "content": "For test launch you should set properties:\nsolr.s3.bucket.name=s3n://<you bucket here>/ - it is important to end with slash\nsolr.s3.confdir=<path to site-core.xml>. I attached template site-core.xml ",
            "id": "comment-15857966"
        },
        {
            "date": "2017-02-08T16:07:59+0000",
            "author": "Hrishikesh Gadre",
            "content": "Alexey Suprun I reviewed your patch and it looks good. Just one minor suggestion - Instead of testing the core level backup/restore, we should test Solr cloud backup and restore (since it will test end-to-end scenario including core level backup). You may want to take a look at https://github.com/apache/lucene-solr/blob/0e0821fdc17052fa2b53ac7d3dd3038270d5ca64/solr/core/src/test/org/apache/solr/cloud/AbstractCloudBackupRestoreTestCase.java\n\nBTW have you tried running the precommit for your patch? AFAIK for each new dependency you need to add LICENSE, NOTICE and sha files for precommit to succeed. Also we should check the license compatibility for each of these dependencies. ",
            "id": "comment-15858169"
        },
        {
            "date": "2017-03-04T18:35:20+0000",
            "author": "Steve Loughran",
            "content": "\n\tuse s3a:// over s3n://, as s3a is maintained, way better these days, bug reports will be looked at. etc/\n\tso you can drop jets3t as a dependency: s3a depends on the AWS lib, and a version of jackson compatible with it (key troublespot there).\nI don't know enough about your test runner to understand the tests properly, but I don't seen anything to run away from. Do bear in mind that listings can take time to be consistent with your writes to s3; always good for the tests to spin & sleep awaiting their final state, rather than give up immediately\n\n ",
            "id": "comment-15895815"
        },
        {
            "date": "2017-11-17T11:15:17+0000",
            "author": "Constantin Bugneac",
            "content": "Is there a way to use S3 backup without any HDFS (Hadoop) dependencies ?\nI don't want to introduce dependency for HDFS which I'm not using at all. ",
            "id": "comment-16256842"
        },
        {
            "date": "2017-11-17T12:26:20+0000",
            "author": "Varun Thacker",
            "content": "I don't want to introduce dependency for HDFS which I'm not using at all.\n\nCould you be more specific as to where you might have to introduce these dependencies ? Solr already comes with hadoop dependencies and reading the pdf guide you only need to add the hadoop-aws lib additionally .\n\nAlexey Suprun Is there any plan on updating the patch with Hrishikesh's and Steve's suggestion? ",
            "id": "comment-16256904"
        },
        {
            "date": "2017-11-17T14:48:03+0000",
            "author": "Constantin Bugneac",
            "content": "Varun Thacker apologies, I wasn't clear enough. What I meant is to not use HDFS at all as an intermediate layer for this purpose - S3 directly. ",
            "id": "comment-16257062"
        },
        {
            "date": "2017-11-17T16:18:07+0000",
            "author": "Varun Thacker",
            "content": "Varun Thacker apologies, I wasn't clear enough. What I meant is to not use HDFS at all as an intermediate layer for this purpose - S3 directly.\n\nWhat would be the technical advantages of doing so? Given Solr already has all the hadoop deps and has a plugin to write to hdfs ?\n\nAnd if the advantages are worth it ( faster backup speed , easier integration ) then patches are always welcome! I can help you get started and review it ",
            "id": "comment-16257172"
        },
        {
            "date": "2017-11-17T17:46:36+0000",
            "author": "Steve Loughran",
            "content": "Constantin; using s3a:// doesn't mean using HDFS. Only that Solr is using the Hadoop filesystem APIs to make S3 look like the other filesystems it works with, and supports the various work done there on credential management, uploads, failure recovery, etc.\n\nI'd be surprised if you could get faster backups, as generally the bottleneck is the upload bandwidth. If copying from local files, use FileSystem.copyFromLocalFile() and it hands off straight to the aws-sdk bulk uploader ",
            "id": "comment-16257297"
        },
        {
            "date": "2017-11-28T10:19:52+0000",
            "author": "Constantin Bugneac",
            "content": "Steve Loughran Thanks for explanation. Looking at the attached .pdf - am I right that this solution is for running Solr completely on S3 bucket ?\nI would like to combine fast local SSD storage for indexes with possibility to backup/restore from S3 bucket. ",
            "id": "comment-16268508"
        },
        {
            "date": "2018-06-28T09:10:20+0000",
            "author": "Jan H\u00f8ydahl",
            "content": "Is it so that this would also support Google Cloud Storage ootb since it appears to support the s3 protocol? Would be great to add explicit documentation to the Reference Guide on how to backup/restore to AWS S3 and GCS\u00a0as part of this issue. ",
            "id": "comment-16526142"
        },
        {
            "date": "2018-11-01T17:05:42+0000",
            "author": "Michael Joyner",
            "content": "What about \"File gateway presents a file-based interface to Amazon S3, which appears as a network file share\"?\n\n(Preferably glacier storage backed?)\n\n\u00a0 ",
            "id": "comment-16671877"
        },
        {
            "date": "2018-11-02T07:15:12+0000",
            "author": "Mikhail Khludnev",
            "content": "Michael Joyner, storage gateway imposes cost of running ec2. Glacier is rather not typical for this (but it might), quite often it's used as data transfer,  but not literally like backup. ",
            "id": "comment-16672672"
        },
        {
            "date": "2018-11-02T13:52:56+0000",
            "author": "Michael Joyner",
            "content": "Why does it require ec2? (Unless running SolrCloud via Amazon?) ",
            "id": "comment-16673125"
        },
        {
            "date": "2018-11-02T14:49:16+0000",
            "author": "Mikhail Khludnev",
            "content": "Because that's how I understand the Storage Gateway. I might miss something, but for me it's a VM which one run on EC2 (or on-prem, but it's irrelevant for the subj).   ",
            "id": "comment-16673181"
        },
        {
            "date": "2018-11-02T16:03:00+0000",
            "author": "Michael Joyner",
            "content": "Found one article and removed it.\n ",
            "id": "comment-16673331"
        }
    ]
}