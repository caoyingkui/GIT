{
    "id": "LUCENE-6131",
    "title": "optimize SortingMergePolicy",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [
            "5.0",
            "6.0"
        ],
        "priority": "Major",
        "status": "Closed",
        "type": "Bug"
    },
    "description": "This has a number of performance problems today:\n\n\tsuboptimal stored fields merging. This is especially the case with high compression. Today this is 7x-64x times slower than it should be.\n\tram stacking: for any docvalues and norms fields, all instances will be loaded in RAM. for any string docvalues fields, all instances of global ordinals will be built, and none of this released until the whole merge is complete.\n\n\n\nWe can fix these two problems without completely refactoring LeafReader... we won't get a \"bulk byte merge\", checksum computation will still be suboptimal, and its not a general solution to \"merging with filterreaders\" but that stuff can be for another day.",
    "attachments": {
        "LUCENE-6131.patch": "https://issues.apache.org/jira/secure/attachment/12688676/LUCENE-6131.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14255937",
            "author": "Robert Muir",
            "date": "2014-12-22T17:08:15+0000",
            "content": "patch. sorry for the file movements, it makes it huge. The only actual code changes to SortingMergePolicy are in one method, which now looks like this:\n\n\npublic List<LeafReader> getMergeReaders() throws IOException {\n      if (unsortedReaders == null) {\n        unsortedReaders = super.getMergeReaders();\n        // wrap readers, to be optimal for merge;\n        List<LeafReader> wrapped = new ArrayList<>(unsortedReaders.size());\n        for (LeafReader leaf : unsortedReaders) {\n          if (leaf instanceof SegmentReader) {\n            leaf = new MergeReaderWrapper((SegmentReader)leaf);\n          }\n          wrapped.add(leaf);\n        }\n        final LeafReader atomicView;\n        if (wrapped.size() == 1) {\n          atomicView = wrapped.get(0);\n        } else {\n          final CompositeReader multiReader = new MultiReader(wrapped.toArray(new LeafReader[wrapped.size()]));\n          atomicView = new SlowCompositeReaderWrapper(multiReader, true);\n        }\n        docMap = sorter.sort(atomicView);\n        sortedView = SortingLeafReader.wrap(atomicView, docMap);\n      }\n      // a null doc map means that the readers are already sorted\n      return docMap == null ? unsortedReaders : Collections.singletonList(sortedView);\n    }\n\n "
        },
        {
            "id": "comment-14255990",
            "author": "Adrien Grand",
            "date": "2014-12-22T18:12:09+0000",
            "content": "+1 to adding this new boolean merging to SlowCompositeReaderWrapper\n+0 to the MergeReaderWrapper approach. I think the hack is ok until we can figure a way through LUCENE-6065? "
        },
        {
            "id": "comment-14256009",
            "author": "Robert Muir",
            "date": "2014-12-22T18:30:21+0000",
            "content": "Exactly, i dont want to try to rush that issue or make LeafReader apis too complicated or anything.\n\nBut at the same time we should have reasonable performance for SortingMP for 5.0, and not leave the trap where it is terribly slow. "
        },
        {
            "id": "comment-14256241",
            "author": "Robert Muir",
            "date": "2014-12-22T22:02:38+0000",
            "content": "I ran quick benchmarks, indexing 1M docs log data and sorting by timestamp. I used 10k doc segments/logdocMP/serial MS. all fields were indexed and stored, and I enabled DV on timestamp:\n\n\n\n\ncompression\nno sorting\nsort (trunk)\nsort (patch)\n\n\nBEST_SPEED\n37,552ms\n56,095ms\n46,309ms\n\n\nBEST_COMPRESSION\n39,132ms\n206,068ms\n47,395ms\n\n\n\n\n\nSo I think it solves the worst of the worst and we can move forward from here? Another thing that seems not to work is the \"already sorted\" optimization. For this test it should be kicking in? We should look at that in another issue. "
        },
        {
            "id": "comment-14257035",
            "author": "ASF subversion and git services",
            "date": "2014-12-23T14:55:43+0000",
            "content": "Commit 1647587 from Robert Muir in branch 'dev/trunk'\n[ https://svn.apache.org/r1647587 ]\n\nLUCENE-6131: optimize SortingMergePolicy "
        },
        {
            "id": "comment-14257055",
            "author": "ASF subversion and git services",
            "date": "2014-12-23T15:08:21+0000",
            "content": "Commit 1647588 from Robert Muir in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1647588 ]\n\nLUCENE-6131: optimize SortingMergePolicy "
        },
        {
            "id": "comment-14332717",
            "author": "Anshum Gupta",
            "date": "2015-02-23T05:01:31+0000",
            "content": "Bulk close after 5.0 release. "
        }
    ]
}