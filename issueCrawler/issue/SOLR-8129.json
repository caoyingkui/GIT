{
    "id": "SOLR-8129",
    "title": "HdfsChaosMonkeyNothingIsSafeTest failures",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "6.5",
            "7.0"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "New HDFS chaos test in SOLR-8123 hits a number of types of failures, including shard inconsistency.",
    "attachments": {
        "fail.151005_064958": "https://issues.apache.org/jira/secure/attachment/12768337/fail.151005_064958",
        "fail.151005_080319": "https://issues.apache.org/jira/secure/attachment/12765192/fail.151005_080319"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-10-06T15:59:33+0000",
            "author": "Yonik Seeley",
            "content": "Here's the smallest log file that I've been able to generate with a large number of descrepancies:\n\nfail.151005_080319:   > Throwable #1: java.lang.AssertionError: shard2 is not consistent.  Got 2076 from http://127.0.0.1:43897/collection1 (previous client) and got 2103 from http://127.0.0.1:36605/collection1\n\n\n\nThis was without deletes (if I managed to configure that correctly). ",
            "id": "comment-14945248"
        },
        {
            "date": "2015-10-09T04:21:04+0000",
            "author": "Yonik Seeley",
            "content": "This seems to be a case of extreme reordering of updates due to thread scheduling or whatever.\nThe leader is selected by the chaos monkey to be killed, and there are a bunch of updates that are sent to one shard but not another (i.e. they may have been waiting to be sent, but didn't make it before the leader was killed).\nA bunch more updates flood in, and when a leader election is held and peersync is done, everyone thinks they are up-to-date because the most recent updates do match.\n\n\n43897 has fewer docs than 36605: missing: {_version_=1514192462261780480, id=ft1-1715} ... {_version_=1514192462057308160, id=ft1-1700} ... {_version_=1514192462056259585, id=ft1-1698}\n\n// Port 43897 is recovering and begins buffering updates\n  2> 31048 INFO  (RecoveryThread-collection1) [n:127.0.0.1:43897_ c:collection1 s:shard2 r:core_node4 x:collection1] o.a.s.c.RecoveryStrategy Starting recovery process. recoveringAfterStartup=true\n  2> 31049 INFO  (RecoveryThread-collection1) [n:127.0.0.1:43897_ c:collection1 s:shard2 r:core_node4 x:collection1] o.a.s.c.RecoveryStrategy ###### startupVersions=[]\n\n// going \"active\"\n  2> 38552 INFO  (RecoveryThread-collection1) [n:127.0.0.1:43897_ c:collection1 s:shard2 r:core_node4 x:collection1] o.a.s.c.RecoveryStrategy Registering as Active after recovery.\n\n// example of a normal add\n  2> 55692 INFO  (qtp1287419856-309) [n:127.0.0.1:43897_ c:collection1 s:shard2 r:core_node4 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] webapp= path=/update params={update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:49240/collection1/&wt=javabin&version=2} {add=[ft1-48 (1514192447743197184)]} 0 188\n  \n// The leader is going to be killed\n  2> 68281 INFO  (Thread-378) [    ] o.a.s.c.ChaosMonkey monkey: stop shard! 49240\n \n// This is presumably where 43897 see the leader marked as \"down\" in the clusterstate\n  2> 68347 INFO  (zkCallback-25-thread-1-processing-n:127.0.0.1:43897_) [n:127.0.0.1:43897_    ] o.a.s.c.c.ZkStateReader A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/collections/collection1/state.json for collection collection1 has occurred - updating... (live nodes size: 9)\n\n// updates are still being processed by the leader\n  2> 68743 DEBUG (qtp139680011-202) [n:127.0.0.1:49240_ c:collection1 s:shard2 r:core_node1 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE add{,id=ft1-1700} {wt=javabin&version=2}\n\n// Replica 36605 receives and processes the update, but 43897 never does (this may be normal as the leader is being killed)\n  2> 69010 DEBUG (qtp33344819-419) [n:127.0.0.1:36605_ c:collection1 s:shard2 r:core_node7 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE add{,id=ft1-1700} {update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:49240/collection1/&wt=javabin&version=2}\n  2> 69015 INFO  (qtp33344819-419) [n:127.0.0.1:36605_ c:collection1 s:shard2 r:core_node7 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] webapp= path=/update params={update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:49240/collection1/&wt=javabin&version=2} {add=[ft1-1656 (1514192461610614784), ft1-1670 (1514192462015365120), ft1-1679 (1514192462052065280), ft1-1682 (1514192462053113856), ft1-1688 (1514192462055211008), ft1-1695 (1514192462056259584), ft1-1698 (1514192462056259585), ft1-1700 (1514192462057308160), ft1-1702 (1514192462058356736)]} 0 224\n\n// a ton of more update flood in\n  2> 69710 INFO  (qtp1287419856-654) [n:127.0.0.1:43897_ c:collection1 s:shard2 r:core_node4 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] webapp= path=/update params={update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:49240/collection1/&wt=javabin&version=2} {add=[ft1-1516 (1514192460689965056), ft1-1518 (1514192460829425664), ft1-1519 (1514192460867174400), ft1-1539 (1514192460868222976), ft1-1540 (1514192460868222977), ft1-1542 (1514192460868222978), ft1-1546 (1514192460869271552), ft1-1552 (1514192460869271553), ft1-1563 (1514192460871368704), ft1-1564 (1514192460871368705), ... (41 adds)]} 0 992\n  2> 70571 INFO  (qtp1287419856-655) [n:127.0.0.1:43897_ c:collection1 s:shard2 r:core_node4 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] webapp= path=/update params={update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:49240/collection1/&wt=javabin&version=2} {add=[ft1-1790 (1514192462017462272), ft1-1791 (1514192462035288064), ft1-1803 (1514192462036336640), ft1-1810 (1514192462037385216), ft1-1813 (1514192462042628096), ft1-1814 (1514192462043676672), ft1-1815 (1514192462043676673), ft1-1818 (1514192462044725248), ft1-1819 (1514192462044725249), ft1-1820 (1514192462303723520), ... (45 adds)]} 0 856\n 2> 70911 INFO  (qtp1287419856-309) [n:127.0.0.1:43897_ c:collection1 s:shard2 r:core_node4 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] webapp= path=/update params={update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:49240/collection1/&wt=javabin&version=2} {add=[ft1-1998 (1514192463049261056), ft1-1999 (1514192463084912640), ft1-2001 (1514192463085961216), ft1-2003 (1514192463085961217), ft1-2011 (1514192463087009792), ft1-2012 (1514192463088058368), ft1-2016 (1514192463099592704), ft1-2020 (1514192463101689856), ft1-2022 (1514192463102738432), ft1-2023 (1514192463392145408), ... (35 adds)]} 0 325\n  2> 71386 INFO  (qtp1287419856-653) [n:127.0.0.1:43897_ c:collection1 s:shard2 r:core_node4 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] webapp= path=/update params={update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:49240/collection1/&wt=javabin&version=2} {add=[ft1-2203 (1514192463788507136), ft1-2206 (1514192464060088320), ft1-2217 (1514192464074768384), ft1-2219 (1514192464074768385), ft1-2224 (1514192464075816960), ft1-2229 (1514192464075816961), ft1-2231 (1514192464075816962), ft1-2239 (1514192464076865536), ft1-2249 (1514192464077914112), ft1-2268 (1514192464077914113), ... (27 adds)]} 0 471\n  \n\n// the most 100 recent versions match for both replicas\n  2> 76191 DEBUG (zkCallback-25-thread-1-processing-n:127.0.0.1:43897_) [n:127.0.0.1:43897_ c:collection1 s:shard2 r:core_node4 x:collection1] o.a.s.u.PeerSync PeerSync: core=collection1 url=http://127.0.0.1:43897  sorted versions from http://127.0.0.1:36605/collection1/ = [1514192465046798337, 1514192465046798336, ... , 1514192463538946050, 1514192463538946049]\n  2> 76241 DEBUG (qtp33344819-419) [n:127.0.0.1:36605_ c:collection1 s:shard2 r:core_node7 x:collection1] o.a.s.u.PeerSync PeerSync: core=collection1 url=http://127.0.0.1:36605  sorted versions from http://127.0.0.1:43897/collection1/ = [1514192465046798337, 1514192465046798336, ... , 1514192463538946050, 1514192463538946049]\n\n// 43897 becomes the new leader\n  2> 76242 INFO  (zkCallback-25-thread-1-processing-n:127.0.0.1:43897_) [n:127.0.0.1:43897_ c:collection1 s:shard2 r:core_node4 x:collection1] o.a.s.c.ShardLeaderElectionContext I am the new leader: http://127.0.0.1:43897/collection1/ shard2\n\n\n// end of test\n  2> ######shard2 is not consistent.  Got 2076 from http://127.0.0.1:43897/collection1 (previous client) and got 2103 from http://127.0.0.1:36605/collection1\n  2> 129545 INFO  (qtp1287419856-653) [n:127.0.0.1:43897_ c:collection1 s:shard2 r:core_node4 x:collection1] o.a.s.c.S.Request [collection1] webapp= path=/select params={q=*:*&distrib=false&tests=checkShardConsistency/showDiff&fl=id,_version_&sort=id+asc&rows=100000&wt=javabin&version=2} hits=2076 status=0 QTime=114 \n  2> 129656 INFO  (qtp33344819-422) [n:127.0.0.1:36605_ c:collection1 s:shard2 r:core_node7 x:collection1] o.a.s.c.S.Request [collection1] webapp= path=/select params={q=*:*&distrib=false&tests=checkShardConsistency/showDiff&fl=id,_version_&sort=id+asc&rows=100000&wt=javabin&version=2} hits=2103 status=0 QTime=4 \n  2> ######http://127.0.0.1:43897/collection1: SolrDocumentList[sz=2076]=[SolrDocument{id=0-0, _version_=1514192448210862080}, SolrDocument{id=0-100, _version_=1514192469996077056}, SolrDocument{id=0-1000, _version_=1514192493627834368}, SolrDocument{id=0-1006, _version_=1514192493742129152}, SolrDocument{id=0-1008, _version_=1514192493795606528}] , [...] , [SolrDocument{id=ft1-984, _version_=1514192458787848193}, SolrDocument{id=ft1-991, _version_=1514192459077255168}, SolrDocument{id=ft1-992, _version_=1514192459148558336}, SolrDocument{id=ft1-994, _version_=1514192459149606912}, SolrDocument{id=ft1-997, _version_=1514192459150655488}]\n  2> ######http://127.0.0.1:36605/collection1: SolrDocumentList[sz=2103]=[SolrDocument{id=0-0, _version_=1514192448210862080}, SolrDocument{id=0-100, _version_=1514192469996077056}, SolrDocument{id=0-1000, _version_=1514192493627834368}, SolrDocument{id=0-1006, _version_=1514192493742129152}, SolrDocument{id=0-1008, _version_=1514192493795606528}] , [...] , [SolrDocument{id=ft1-984, _version_=1514192458787848193}, SolrDocument{id=ft1-991, _version_=1514192459077255168}, SolrDocument{id=ft1-992, _version_=1514192459148558336}, SolrDocument{id=ft1-994, _version_=1514192459149606912}, SolrDocument{id=ft1-997, _version_=1514192459150655488}]\n  2> ###### sizes=2076,2103\n  2> ###### Only in http://127.0.0.1:36605/collection1: [{_version_=1514192462261780480, id=ft1-1715}, {_version_=1514192462260731904, id=ft1-1711}, {_version_=1514192462263877632, id=ft1-1719}, {_version_=1514192462262829056, id=ft1-1718}, {_version_=1514192462264926208, id=ft1-1720}, {_version_=1514192462015365120, id=ft1-1670}, {_version_=1514192462276460544, id=ft1-1722}, {_version_=1514192462415921152, id=ft1-1844}, {_version_=1514192462413824000, id=ft1-1840}, {_version_=1514192462230323200, id=ft1-1709}, {_version_=1514192462414872576, id=ft1-1841}, {_version_=1514192462412775424, id=ft1-1838}, {_version_=1514192462412775425, id=ft1-1839}, {_version_=1514192462411726848, id=ft1-1837}, {_version_=1514192462230323201, id=ft1-1710}, {_version_=1514192462057308160, id=ft1-1700}, {_version_=1514192462058356737, id=ft1-1703}, {_version_=1514192462058356736, id=ft1-1702}, {_version_=1514192462375026688, id=ft1-1796}, {_version_=1514192462400192512, id=ft1-1798}, {_version_=1514192462399143936, id=ft1-1797}, {_version_=1514192462193623040, id=ft1-1707}, {_version_=1514192462052065280, id=ft1-1679}, {_version_=1514192462055211008, id=ft1-1688}, {_version_=1514192462053113856, id=ft1-1682}, {_version_=1514192462056259584, id=ft1-1695}, {_version_=1514192462056259585, id=ft1-1698}]\n \n ",
            "id": "comment-14949872"
        },
        {
            "date": "2015-10-23T16:48:45+0000",
            "author": "Yonik Seeley",
            "content": "OK, here's another failure I've been analyzing.\nIt comes down to this:\n1) leader is shutdown (CoreContainer.shutdown is called)\n2) a single doc is sent from the leader to one replica successfully, but unsuccessfully to a different replica (rejected task from shutdown executor that the client is using to send)\n3) tons of other updates are still being accepted and sent by the leader\n4) much later, a peersync sees everything as OK since recent versions match up.\n\nOne mystery is why ConcurrentUpdateSolrClient is trying to create a new Runner when there is obviously another runner already running (since it still accepts and sends new updates after that point).\n\nA general way to fix this is to make sure that shutdown happens much more quickly... we should stop reading and processing updates.\n\n\n// good-doc comes into leader 53975\n  2> 43204 DEBUG (qtp1536362844-206) [n:127.0.0.1:53975__%2Fzl c:collection1 s:shard2 r:core_node1 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE add{,id=ft1-476} {wt=javabin&version=2}\n\n// bad-doc comes into leader 53975\n  2> 43204 DEBUG (qtp1536362844-206) [n:127.0.0.1:53975__%2Fzl c:collection1 s:shard2 r:core_node1 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE add{,id=ft1-477} {wt=javabin&version=2}\n\n// good-doc added to shard 57414\n  2> 43273 DEBUG (qtp702407469-354) [n:127.0.0.1:57414__%2Fzl c:collection1 s:shard2 r:core_node5 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE add{,id=ft1-476} {update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:53975/_/zl/collection1/&wt=javabin&version=2}\n  2> 43280 INFO  (qtp702407469-354) [n:127.0.0.1:57414__%2Fzl c:collection1 s:shard2 r:core_node5 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] webapp=/_/zl path=/update params={update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:53975/_/zl/collection1/&wt=javabin&version=2} {add=[ft1-467 (1514187847514456064), ft1-470 (1514187847746191360), ft1-473 (1514187847754579968), ft1-476 (1514187847755628544)]} 0 42\n\n\n// the leader is going to be stopped in the future\n  2> 43345 INFO  (Thread-272) [    ] o.a.s.c.ChaosMonkey monkey: stop shard! 53975\n  2> 43345 INFO  (Thread-272) [    ] o.a.s.c.CoreContainer Shutting down CoreContainer instance=171681388\n\n// overseer gets state:down for leader 53975 \n  2> 43378 INFO  (OverseerStateUpdate-94636738141945860-127.0.0.1:49439__%2Fzl-n_0000000000) [n:127.0.0.1:49439__%2Fzl    ] o.a.s.c.Overseer processMessage: queueSize: 1, message = {\n\n// BUT... 53975 appears to keep processing updates... there are ~736 more updates like the following, continuing another couple of seconds through to time 45555\nfail.151005_064958:  2> 43381 DEBUG (qtp1536362844-204) [n:127.0.0.1:53975__%2Fzl c:collection1 s:shard2 r:core_node1 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE add{,id=ft1-1165} {wt=javabin&version=2}\n\n\n\n// good-doc is added to replica 44323\n  2> 43449 DEBUG (qtp1776514246-272) [n:127.0.0.1:44323__%2Fzl c:collection1 s:shard2 r:core_node3 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE add{,id=ft1-476} {update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:53975/_/zl/collection1/&wt=javabin&version=2}\n  2> 43456 INFO  (qtp1776514246-272) [n:127.0.0.1:44323__%2Fzl c:collection1 s:shard2 r:core_node3 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] webapp=/_/zl path=/update params={update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:53975/_/zl/collection1/&wt=javabin&version=2} {add=[ft1-473 (1514187847754579968), ft1-476 (1514187847755628544)]} 0 108\n\n// more signs of node being stopped\n  2> 43453 WARN  (qtp1536362844-205) [n:127.0.0.1:53975__%2Fzl c:collection1 s:shard2 r:core_node1 x:collection1] o.e.j.s.ServletHandler /_/zl/collection1/update\n  2> org.apache.solr.common.SolrException: Error processing the request. CoreContainer is either not initialized or shutting down.\n\n\n// bad-doc is added to replica 44323\n  2> 43471 DEBUG (qtp1776514246-273) [n:127.0.0.1:44323__%2Fzl c:collection1 s:shard2 r:core_node3 x:collection1] o.a.s.u.p.LogUpdateProcessor PRE_UPDATE add{,id=ft1-477} {update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:53975/_/zl/collection1/&wt=javabin&version=2}\n  2> 43501 INFO  (qtp1776514246-273) [n:127.0.0.1:44323__%2Fzl c:collection1 s:shard2 r:core_node3 x:collection1] o.a.s.u.p.LogUpdateProcessor [collection1] webapp=/_/zl path=/update params={update.distrib=FROMLEADER&distrib.from=http://127.0.0.1:53975/_/zl/collection1/&wt=javabin&version=2} {add=[ft1-477 (1514187847778697216)]} 0 30\n\n// This is the same update thread that has our bad-doc on the leader... it can't send because the update executor has been shut down\n  2> 43472 ERROR (qtp1536362844-206) [n:127.0.0.1:53975__%2Fzl c:collection1 s:shard2 r:core_node1 x:collection1] o.a.s.u.SolrCmdDistributor java.util.concurrent.RejectedExecutionException: Task org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor$1@183e1649 rejected from org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor@7552e28d[Shutting down, pool size = 5, active threads = 5, queued tasks = 0, completed tasks = 62]\n\n// Takes a while for other executors to be shut down\n  2> 45288 ERROR (qtp1536362844-204) [n:127.0.0.1:53975__%2Fzl c:collection1 s:shard2 r:core_node1 x:collection1] o.a.s.u.SolrCmdDistributor java.util.concurrent.RejectedExecutionException: Task org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor$1@4dbd9a35 rejected from org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor@7552e28d[Shutting down, pool size = 2, active threads = 2, queued tasks = 0, completed tasks = 65]\n\n\n// Peer sync... the versions show that 44323 is ahead by 14 updates, but everything else matches up after that point.\n// Our missing update (1514187848796864512) is older than the oldest update of either list.\n  2> 49313 DEBUG (zkCallback-21-thread-2-processing-n:127.0.0.1:44323__%2Fzl) [n:127.0.0.1:44323__%2Fzl c:collection1 s:shard2 r:core_node3 x:collection1] o.a.s.u.PeerSync PeerSync: core=collection1 url=http://127.0.0.1:44323/_/zl  sorted versions from http://127.0.0.1:57414/_/zl/collection1/ = [1514187849851731968, ...\n  2> 49313 INFO  (zkCallback-21-thread-2-processing-n:127.0.0.1:44323__%2Fzl) [n:127.0.0.1:44323__%2Fzl c:collection1 s:shard2 r:core_node3 x:collection1] o.a.s.u.PeerSync PeerSync: core=collection1 url=http://127.0.0.1:44323/_/zl  Our versions are newer. ourLowThreshold=1514187848979316736 otherHigh=1514187849717514240\n\n\n  2> 49567 DEBUG (qtp702407469-351) [n:127.0.0.1:57414__%2Fzl c:collection1 s:shard2 r:core_node5 x:collection1] o.a.s.u.PeerSync PeerSync: core=collection1 url=http://127.0.0.1:57414/_/zl  sorted versions from http://127.0.0.1:44323/_/zl/collection1/ = [1514187849932472320, ...\n\n\n  2> 50327 ERROR (qtp1776514246-274) [n:127.0.0.1:44323__%2Fzl c:collection1 s:shard2 r:core_node3 x:collection1] o.a.s.c.SolrCore org.apache.solr.common.SolrException: No registered leader was found after waiting for 4000ms , collection: collection1 slice: shard2\n\n// the leader is still not all the way shut down... \n  2> 62700 ERROR (qtp1536362844-206) [n:127.0.0.1:53975__%2Fzl c:collection1 s:shard2 r:core_node1 x:collection1] o.a.s.c.s.i.ConcurrentUpdateSolrClient interrupted\n\n\n\n\n\n  2> ######shard2 is not consistent.  Got 2456 from http://127.0.0.1:44323/_/zl/collection1 (previous client) and got 2455 from http://127.0.0.1:57414/_/zl/collection1\n  2> 110640 INFO  (qtp1776514246-276) [n:127.0.0.1:44323__%2Fzl c:collection1 s:shard2 r:core_node3 x:collection1] o.a.s.c.S.Request [collection1] webapp=/_/zl path=/select params={q=*:*&distrib=false&tests=checkShardConsistency/showDiff&fl=id,_version_&sort=id+asc&rows=100000&wt=javabin&version=2} hits=2456 status=0 QTime=91 \n  2> 110762 INFO  (qtp702407469-354) [n:127.0.0.1:57414__%2Fzl c:collection1 s:shard2 r:core_node5 x:collection1] o.a.s.c.S.Request [collection1] webapp=/_/zl path=/select params={q=*:*&distrib=false&tests=checkShardConsistency/showDiff&fl=id,_version_&sort=id+asc&rows=100000&wt=javabin&version=2} hits=2455 status=0 QTime=9 \n  2> ######http://127.0.0.1:44323/_/zl/collection1: SolrDocumentList[sz=2456]=[SolrDocument{id=0-100, _version_=1514187858469978112}, SolrDocument{id=0-1000, _version_=1514187879861977089}, SolrDocument{id=0-1005, _version_=1514187879928037376}, SolrDocument{id=0-1009, _version_=1514187879977320448}, SolrDocument{id=0-1011, _version_=1514187880001437696}] , [...] , [SolrDocument{id=ft1-92, _version_=1514187844608851968}, SolrDocument{id=ft1-95, _version_=1514187844616192000}, SolrDocument{id=ft1-96, _version_=1514187844623532032}, SolrDocument{id=ft1-985, _version_=1514187847262797824}, SolrDocument{id=ft1-99, _version_=1514187844641357824}]\n  2> ######http://127.0.0.1:57414/_/zl/collection1: SolrDocumentList[sz=2455]=[SolrDocument{id=0-100, _version_=1514187858469978112}, SolrDocument{id=0-1000, _version_=1514187879861977089}, SolrDocument{id=0-1005, _version_=1514187879928037376}, SolrDocument{id=0-1009, _version_=1514187879977320448}, SolrDocument{id=0-1011, _version_=1514187880001437696}] , [...] , [SolrDocument{id=ft1-92, _version_=1514187844608851968}, SolrDocument{id=ft1-95, _version_=1514187844616192000}, SolrDocument{id=ft1-96, _version_=1514187844623532032}, SolrDocument{id=ft1-985, _version_=1514187847262797824}, SolrDocument{id=ft1-99, _version_=1514187844641357824}]\n  2> ###### sizes=2456,2455\n  2> ###### Only in http://127.0.0.1:44323/_/zl/collection1: [{_version_=1514187847778697216, id=ft1-477}]\n\n ",
            "id": "comment-14971347"
        },
        {
            "date": "2015-10-23T17:41:00+0000",
            "author": "Yonik Seeley",
            "content": "\nOne mystery is why ConcurrentUpdateSolrClient is trying to create a new Runner when there is obviously another runner already running (since it still accepts and sends new updates after that point).\n\nMark pointed me at this comment in ConcurrentUpdateSolrClient:\n          // see if queue is half full and we can add more runners\n          // special case: if only using a threadCount of 1 and the queue\n          // is filling up, allow 1 add'l runner to help process the queue\n\nTimothy Potter It looks like you added that comment... but it's not clear to me how the code implements that special case.  Thoughts? ",
            "id": "comment-14971436"
        },
        {
            "date": "2015-10-23T18:34:47+0000",
            "author": "Timothy Potter",
            "content": "heh - yeah, that's my bad ... I think I had that in there at one point when I was dealing with the perf improvements but it didn't actually help or at least I had other changes that masked it's benefit. I'll revisit that idea ... for now, the comment is misleading and should be removed. Sorry 'bout that! ",
            "id": "comment-14971564"
        },
        {
            "date": "2015-10-23T19:38:03+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks for the details, Yonik.\n\nA general way to fix this is to make sure that shutdown happens much more quickly... we should stop reading and processing updates.\n\nmaybe HttpSolrCall can return an error immediately if container has been shutdown? ",
            "id": "comment-14971697"
        },
        {
            "date": "2015-10-23T21:38:24+0000",
            "author": "Yonik Seeley",
            "content": "Thanks for clarifying Tim, it had me wondering what I was missing.\n\nmaybe HttpSolrCall can return an error immediately if container has been shutdown?\n\nYeah, I think there are a lot of places we could check if the container has been shut down... not sure yet what is the best place.  I think new calls would be rejected already... it's really existing streaming calls that are the issue I think.\n\nI'm still first trying to find out why one update was rejected and then updates that happened after that were still processed though.  This particular failure would not have happened without that piece as well. ",
            "id": "comment-14971917"
        },
        {
            "date": "2015-10-23T22:57:13+0000",
            "author": "Yonik Seeley",
            "content": "I think the first thing I'll try to track down is if it's the same ConcurrentUpdateSolrClient instance or not.  If it is, that points to a race condition somewhere in that class. ",
            "id": "comment-14972038"
        },
        {
            "date": "2015-10-24T14:53:50+0000",
            "author": "Yonik Seeley",
            "content": "I'm still first trying to find out why one update was rejected and then updates that happened after that were still processed though.\n\nOK, figured it out... it was my incorrect mental model assuming that there was a single ConcurrentUpdateSolrClient between a leader and a replica.\nI made that leap looking at StreamingSolrClients.getSolrClient called from SolrCmdDistributor.  But there is a different SolrCmdDistributor created per update request to Solr.\n\nSo there is no longer any mystery around the reject followed by many other updates.  Different client objects (and once they have runners going, they can keep going processing updates). ",
            "id": "comment-14972633"
        },
        {
            "date": "2015-10-24T15:31:00+0000",
            "author": "Yonik Seeley",
            "content": "maybe HttpSolrCall can return an error immediately if container has been shutdown?\n\nI think somewhere in the dispatch filter path may already reject new requests when things are shutting down.  The issue in this last fail was existing update requests that were streaming via ConcurrentUpdateSolrClient.\n\nMark is investigating using a more aggressive shutdown (i.e. interrupt) of the update executor (it's just used to talk to other nodes).\nWe could also check the core container status anywhere in the update path as well and throw an error that would stop processing the update streams.  Thoughts? ",
            "id": "comment-14972652"
        },
        {
            "date": "2015-10-24T15:44:11+0000",
            "author": "Mark Miller",
            "content": "I've just finished beasting HdfsChaosMonkeyNothingIsSafeTest for 100 runs. The only change I made was to interrupt the update executor on shutdown. Mostly this was just a test to see if this helped and what else might still pop up.\n\nIn 100 runs, I saw 4 or 5 fails, but no replica inconsistency.\n\n\n\tI saw control off with cloud, by 1 and by more than 1. (There is a known test issue here, but we don't know it covers all cases)\n\tI saw a hang in CUSC#blockUntilFinished - I think I already have an open JIRA issue for something like that.\n\tBefore this 100 runs I saw a strange fail where the connection pool for the test client is shutdown while we still try and use - I removed the try, finally that wraps the test prints the zklayout on failure - I saw that printout but could not work out the real issue - just hanging threads because we didn't actually wait for indexing threads to stop. Since removing that, have not seen this, but could just be lucky. It seemed like somehow the true fail reason was being swallowed somehow.\n\n\n\nI'll keep it running for now, while we work out the best way to cut things off faster. ",
            "id": "comment-14972658"
        },
        {
            "date": "2015-10-24T16:13:35+0000",
            "author": "Yonik Seeley",
            "content": "For now, I'm going to try and run with this:\n\n\nIndex: solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java\n===================================================================\n--- solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java\t(revision 1710343)\n+++ solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java\t(working copy)\n@@ -1443,6 +1443,10 @@\n   }\n \n   private void zkCheck() {\n+    if (req.getCore().getCoreDescriptor().getCoreContainer().isShutDown()) {\n+      throw new SolrException(ErrorCode.SERVICE_UNAVAILABLE, \"CoreContainer is shutting down.\");\n+    }\n+\n     if ((updateCommand.getFlags() & (UpdateCommand.REPLAY | UpdateCommand.PEER_SYNC)) != 0) {\n       // for log reply or peer sync, we don't need to be connected to ZK\n       return;\n\n ",
            "id": "comment-14972668"
        },
        {
            "date": "2015-10-24T16:46:10+0000",
            "author": "Yonik Seeley",
            "content": "I opened SOLR-8203 to handle these \"let's shutdown faster\" changes. ",
            "id": "comment-14972695"
        },
        {
            "date": "2015-10-24T17:16:31+0000",
            "author": "Mark Miller",
            "content": "I saw a strange fail where the connection pool for the test client is shutdown while we still try and use\n\nOkay, this looks like it is the result of: Throwable #1: java.lang.AssertionError: The Monkey ran for over 30 seconds and no jetties were stopped - this is worth investigating!\n\nThis causes the test to bail before stopping the indexing threads. ",
            "id": "comment-14972741"
        },
        {
            "date": "2015-10-24T17:33:15+0000",
            "author": "Mark Miller",
            "content": "Seems the above happens because we only have one Solr instance and nothing to kill. The logic to make sure we have at least 2 is is not correct and needs to be fixed.\n\n\n      // we make sure that there's at least one shard with more than one replica\n      // so that the ChaosMonkey has something to kill\n      numShards = sliceCount + random().nextInt(TEST_NIGHTLY ? 12 : 2) + 1;\n\n\n\nshould be \n\n\n      // we make sure that there's at least one shard with more than one replica\n      // so that the ChaosMonkey has something to kill\n      numShards = sliceCount + random().nextInt(TEST_NIGHTLY ? 12 : 2) + 2;\n\n\n\n\n\tThat's actually probably not the right fix it looks - but something like this.\n\n ",
            "id": "comment-14972759"
        },
        {
            "date": "2015-10-24T17:53:17+0000",
            "author": "Mark Miller",
            "content": "Actually, it appears to just be really bad luck for the monkey. The fail is reproducible, but it's actually with numSlices = 2 and numShards = 3.\n\nI was seeing:\n\n\n  [junit4]   2> 55703 INFO  (Thread-219) [    ] o.a.s.c.ChaosMonkey monkey: only one active node in shard - monkey cannot kill :(\n   [junit4]   2> 57863 INFO  (Thread-219) [    ] o.a.s.c.ChaosMonkey monkey: only one active node in shard - monkey cannot kill :(\n   [junit4]   2> 63113 INFO  (Thread-219) [    ] o.a.s.c.ChaosMonkey monkey: only one active node in shard - monkey cannot kill :(\n   [junit4]   2> 69451 INFO  (Thread-219) [    ] o.a.s.c.ChaosMonkey monkey: only one active node in shard - monkey cannot kill :(\n   [junit4]   2> 78330 INFO  (Thread-219) [    ] o.a.s.c.ChaosMonkey monkey: only one active node in shard - monkey cannot kill :(\n\n\n\nBut apparently it just keeps randomly picking the shard with only a single replica in it. With the same numShards and slices and a different seed, the test passes. ",
            "id": "comment-14972773"
        },
        {
            "date": "2015-10-24T18:22:24+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1710371 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1710371 ]\n\nSOLR-8129: Raise no kill by monkey warning from 30 to 45. ",
            "id": "comment-14972790"
        },
        {
            "date": "2015-10-24T18:33:59+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1710375 from Mark Miller in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1710375 ]\n\nSOLR-8129: Raise no kill by monkey warning from 30 to 45. ",
            "id": "comment-14972802"
        },
        {
            "date": "2015-10-25T02:55:01+0000",
            "author": "Mark Miller",
            "content": "Man, manyy hundreds of runs and I still have not seen a replica out of sync issue.\n\nHave not seen the above issue since the above commit either. ",
            "id": "comment-14972967"
        },
        {
            "date": "2015-10-25T12:46:12+0000",
            "author": "Mark Miller",
            "content": "I ran another 500 runs overnight. No new fails. Still saw one fail where the monkey did not kill anything even after the new 45 second limit. Should probably try 60 next. ",
            "id": "comment-14973223"
        },
        {
            "date": "2016-01-21T18:39:23+0000",
            "author": "Yonik Seeley",
            "content": "Here's a visualization of a recent fail:\n\nNode A starts off as the leader, gets a bunch of updates that it ever sends to node B before it is killed.\nNode B becomes the leader.\nNode A comes up, does a PeerSync and the lists pretty much overlap in time (looking at low threshold / high threshold only), so node A asks node B for the docs it's missing (and ends up with a lot more docs than node B).\n\nThe list below is ordered from oldest to newest:\n\n1523440456046739456    B \n1523440456047788032    B \n1523440456049885184    B \n1523440456050933760    B \n1523440456051982336    B \n1523440456053030912    B \n1523440456053030913    B \n1523440456053030914    B \n1523440456054079488    B \n1523440456055128064    B \n1523440456059322368    B \n1523440456314126336    B \n1523440456314126337    B \n1523440456315174912    B \n1523440456316223488    B \n1523440456318320640    B \n1523440456342437888    B \n1523440456343486464    B \n1523440456343486465    B \n1523440456344535040    B \n1523440456362360832    B \n1523440456363409408  A \n1523440456372846592  A B \n1523440456375992320  A B \n1523440456375992321  A B \n1523440456379138048  A \n1523440456381235200  A \n1523440456382283776  A B \n1523440456392769536  A \n1523440456401158144  A \n1523440456403255296  A B \n1523440456437858304  A \n1523440456463024128  A \n1523440456472461312  A \n1523440456480849920  A \n1523440456531181568  A \n1523440456543764480  A \n1523440456544813056  A \n1523440456544813057  A \n1523440456545861632  A \n1523440456550055936  A B \n1523440456552153088  A B \n1523440456552153089  A \n1523440456559493120  A B \n1523440456561590272  A B \n1523440456561590273  A B \n1523440456562638848  A B \n1523440456563687424  A B \n1523440456565784576  A \n1523440456609824768  A \n1523440456610873344  A \n1523440456610873345  A \n1523440456611921920  A \n1523440456669593600  A \n1523440456669593601  A \n1523440456669593602  A \n1523440456670642176  A \n1523440456671690752  A B \n1523440456672739328  A \n1523440456673787904  A \n1523440456674836480  A \n1523440456675885056  A \n1523440456686370816  A \n1523440456690565120  A \n1523440456702099456  A B \n1523440456726216704  A B \n1523440456772354048  A B \n1523440456785985536  A B \n1523440456826880000  A B \n1523440456857288704  A B \n1523440456858337280  A B \n1523440456921251840  A \n1523440456921251841  A \n1523440456922300416  A \n1523440456926494720  A B \n1523440456926494721  A B \n1523440456927543296  A \n1523440456927543297  A \n1523440456929640448  A B \n1523440456929640449  A \n1523440456934883328  A \n1523440456944320512  A \n1523440456950611968  A \n1523440456975777792  A \n1523440456975777793  A \n1523440456975777794  A \n1523440456976826368  A \n1523440456976826369  A \n1523440456976826370  A \n1523440456999895040  A \n1523440457004089344  A \n1523440457008283648  A \n1523440457009332224  A \n1523440457009332225  A \n1523440457010380800  A \n1523440457056518144  A B \n1523440457064906752  A B \n1523440457065955328  A B \n1523440457067003904  A B \n1523440457070149632  A B \n1523440457071198208  A B \n1523440457071198209  A B \n1523440457074343936  A B \n1523440457077489664  A B \n1523440457078538240  A B \n1523440457079586816  A B \n1523440457080635392  A B \n1523440457116286976  A \n1523440457116286977  A \n1523440457117335552  A \n1523440457138307072  A \n1523440457149841408  A \n1523440457170812928  A \n1523440457172910080  A \n1523440457173958656  A \n1523440457173958657  A \n1523440457175007232  A \n1523440457175007233  A \n1523440457180250112  A \n1523440457181298688  A \n1523440457181298689  A \n1523440460638453760    B \n1523440460641599488    B \n1523440460641599489    B \n1523440460653133824    B \n1523440460708708352    B \n1523440460881723392    B \n1523440460915277824    B \n1523440461056835584    B \n1523440461057884160    B \n1523440461145964544    B \n1523440461206781952    B \n1523440461227753472    B \n1523440461237190656    B \n1523440461259210752    B \n1523440461272842240    B \n1523440461370359808    B \n1523440461379796992    B \n1523440461486751744    B \n1523440461550714880    B \n1523440461615726592    B \n1523440461659766784    B \n1523440461713244160    B \n1523440461754138624    B \n1523440461787693056    B \n1523440461817053184    B \n1523440461862141952    B \n1523440461881016320    B \n1523440461917716480    B \n1523440461939736576    B \n1523440461953368064    B \n1523440461987971072    B \n1523440462001602560    B \n1523440462224949248    B \n1523440462292058112    B \n1523440462313029632    B \n1523440462325612544    B \n1523440462379089920    B \n1523440462421032960    B \n1523440462461927424    B \n1523440462486044672    B \n1523440462501773312    B \n1523440462545813504    B \n1523440474431422464    B\n\n\n\nMassive reorders, which PeerSync was not designed for.\nPossible remedies:\n\n\tgreatly lower the probability of these big reorders\n\twhere there is overlap in versions, make PeerSync check that it is \"dense\" (both shards have all docs in the overlap)\n\t\n\t\tthis seems extremely strict and could cause peersync to fail due to a missing doc right at the end of an overlap... which end matters a lot.\n\t\n\t\n\texpand PeerSync to cover complete index\n\t\n\t\tuse hashes over all versions in the index\n\t\n\t\n\n ",
            "id": "comment-15111065"
        },
        {
            "date": "2016-01-21T18:42:26+0000",
            "author": "Mark Miller",
            "content": "expand PeerSync to cover complete index\n\nI think this would be great if we could do it reasonably.\n\ngreatly lower the probability of these big reorders\n\nMain reason I don't like this is that it just seems fragile over time, and will constrict things more. ",
            "id": "comment-15111068"
        },
        {
            "date": "2016-01-21T19:53:19+0000",
            "author": "Gregory Chanan",
            "content": "+1 for Mark's reasoning. ",
            "id": "comment-15111192"
        },
        {
            "date": "2016-01-21T20:45:21+0000",
            "author": "Yonik Seeley",
            "content": "Yep, but reducing the probability of big reorders will still be important, because if we do detect a big reorder (with hashes over the entire index), the only current general remedy we have is to fail peersync and copy the whole index. ",
            "id": "comment-15111261"
        },
        {
            "date": "2016-01-21T22:29:25+0000",
            "author": "Mark Miller",
            "content": "Yeah, but I'd prefer full replication over holes. Better that reducing reorders is an optimization than a bug. ",
            "id": "comment-15111508"
        },
        {
            "date": "2016-01-22T02:26:24+0000",
            "author": "Yonik Seeley",
            "content": "Yeah, but I'd prefer full replication over holes. Better that reducing reorders is an optimization than a bug.\n\nSure, we're saying the same thing.  I was just pointing out that the reorders can still be very painful (and thus important) even if we improve/fix everything else. ",
            "id": "comment-15111805"
        },
        {
            "date": "2016-01-22T02:58:22+0000",
            "author": "Mark Miller",
            "content": "we're saying the same thing. \n\nI just get the feeling you have been trying to avoid doing this full hash, but it seems so much more bulletproof than these quick window checks. I'd rather recovery be a lot slower and get it right and worry about speeding that up later. \n\nthe only current general remedy we have is to fail peersync and copy the whole index.\n\nThese kind of reorders don't seem that common? And we have many other reasons you will probably have to replicate the whole index. 100 updates in peer sync means it's best simply for static indexes or low usage indexes. Sure, I'd rather not replicate a full index, but I think it's pretty common to have to do already.\n\n ",
            "id": "comment-15111833"
        },
        {
            "date": "2016-01-22T17:50:50+0000",
            "author": "Yonik Seeley",
            "content": "Opened SOLR-8586 for hashing all versions.\n\nThese kind of reorders don't seem that common?\n\nSeems way too common on HDFS (on my linux box at least).  \n\nEven with SOLR-8586, big enough reorders can still cause other issues, such as going beyond our delete window.  That would cause shards to get out-of-sync even when all the shards are up and live (and hence we wouldn't catch it until the next time we did a version-hash for some reason). ",
            "id": "comment-15112775"
        },
        {
            "date": "2016-01-27T18:09:00+0000",
            "author": "J\u00e9r\u00e9mie MONSINJON",
            "content": "I\u2019m stressing a SolR cluster with Gatling for perf tests. 3 shards, 3 nodes per shard (1 leader, 2 replicas). \nAbout 300 updates per second using CloudSolrServer (about 100 per shard) and 40 qps.\nUsing round robin for select queries on each node.\nAfter 2 minutes playing the scenario, it seems that the leader of a shard can\u2019t send an update to a replica.\n\nREPLICA : 51\n\n2016-01-27 15:18:00.252 WARN  (qtp1057941451-128982) [   ] o.e.j.h.HttpParser badMessage: java.lang.IllegalStateException: too much data after closed for HttpChannelOverHttp@428e80f\nUnknown macro: {r=1098,c=false,a=IDLE,uri=-} \n\nLEADER : 33\n\n2016-01-27 15:18:00.245 ERROR (updateExecutor-2-thread-3557-processing-http:////solrNode051:8983//solr//offers_full_shard3_replica1 x:offers_full_shard3_replica3 r:core_node10 n:solrNode033:8983_solr s:shard3 c:offers_full) [c:offers_full s:shard3 r:core_node10 x:offers_full_shard3_replica3] o.a.s.u.StreamingSolrClients error\norg.apache.http.NoHttpResponseException: solrNode051:8983 failed to respond\n2016-01-27 15:18:00.267 INFO  (qtp1057941451-45551) [c:offers_full s:shard3 r:core_node10 x:offers_full_shard3_replica3] o.a.s.c.ZkController Put replica core=offers_full_shard3_replica1 coreNodeName=core_node3 on solrNode051:8983_solr into leader-initiated recovery.\n\nREPLICA : 51\n\n2016-01-27 15:18:00.276 INFO  (qtp1057941451-129018) [   ] o.a.s.h.a.CoreAdminHandler It has been requested that we recover: core=offers_full_shard3_replica1\n2016-01-27 15:18:00.277 INFO  (Thread-117036) [c:offers_full s:shard3 r:core_node3 x:offers_full_shard3_replica1] o.a.s.u.DefaultSolrCoreState Running recovery - first canceling any ongoing recovery\n\n2016-01-27 15:18:08.296 INFO  (updateExecutor-2-thread-4594-processing-n:solrNode051:8983_solr x:offers_full_shard3_replica1 s:shard3 c:offers_full r:core_node3) [c:offers_full s:shard3 r:core_node3 x:offers_full_shard3_replica1] o.a.s.c.RecoveryStrategy Attempting to PeerSync from http://solrNode033:8983/solr/offers_full_shard3_replica3/ - recoveringAfterStartup=false\n\n2016-01-27 15:18:08.296 ERROR (updateExecutor-2-thread-4594-processing-n:solrNode051:8983_solr x:offers_full_shard3_replica1 s:shard3 c:offers_full r:core_node3) [c:offers_full s:shard3 r:core_node3 x:offers_full_shard3_replica1] o.a.s.u.PeerSync PeerSync: core=offers_full_shard3_replica1 url=http://solrNode051:8983/solr ERROR, update log not in ACTIVE or REPLAY state. FSUpdateLog{state=BUFFERING, tlog=tlog{file=/var/lib/solr5/data/offers_full_shard3_replica1/data/tlog/tlog.0000000000000004550 refcount=1}}\n2016-01-27 15:18:08.297 WARN  (updateExecutor-2-thread-4594-processing-n:solrNode051:8983_solr x:offers_full_shard3_replica1 s:shard3 c:offers_full r:core_node3) [c:offers_full s:shard3 r:core_node3 x:offers_full_shard3_replica1] o.a.s.u.PeerSync PeerSync: core=offers_full_shard3_replica1 url=http://solrNode051:8983/solr too many updates received since start - startingUpdates no longer overlaps with our currentUpdates\n\n2016-01-27 15:18:15.160 WARN  (updateExecutor-2-thread-4594-processing-n:solrNode051:8983_solr x:offers_full_shard3_replica1 s:shard3 c:offers_full r:core_node3) [c:offers_full s:shard3 r:core_node3 x:offers_full_shard3_replica1] o.a.s.h.IndexFetcher File _d2r_1wk.liv did not match. expected checksum is 3620017595 and actual is checksum 442433238. expected length is 38405 and actual length is 38405\n2016-01-27 15:18:15.160 INFO  (updateExecutor-2-thread-4594-processing-n:solrNode051:8983_solr x:offers_full_shard3_replica1 s:shard3 c:offers_full r:core_node3) [c:offers_full s:shard3 r:core_node3 x:offers_full_shard3_replica1] o.a.s.h.IndexFetcher Starting download (fullCopy=true) to NRTCachingDirectory(MMapDirectory@/var/lib/solr5/data/offers_full_shard3_replica1/data/index.20160127151815143 lockFactory=org.apache.lucene.store.NativeFSLockFactory@721552d5; maxCacheMB=48.0 maxMergeSizeMB=4.0)\n\nREPLICA : 27\n\n2016-01-27 15:20:54.009 INFO  (zkCallback-3-thread-198-processing-n:solrNode027:8983_solr) [   ] o.a.s.c.c.ZkStateReader A live node change: WatchedEvent state:Sync\nConnected type:NodeChildrenChanged path:/live_nodes, has occurred - updating... (live nodes size: 11)\n2016-01-27 15:20:54.016 INFO  (zkCallback-3-thread-198-processing-n:solrNode027:8983_solr) [c:offers_full s:shard3 r:core_node7 x:offers_full_shard3_replica2] o.a.s.c.ShardLeaderElectionContext Running the leader process for shard shard3\n\nREPLICA : 51\n\n2016-01-27 15:20:54.011 INFO  (zkCallback-3-thread-110-processing-n:solrNode051:8983_solr) [c:offers_full s:shard3 r:core_node3 x:offers_full_shard3_replica1] o.a.s.u.DefaultSolrCoreState Running recovery - first canceling any ongoing recovery\t\n\nREPLICA : 27\n\nBECAME THE LEADER\n\n2016-01-27 15:20:56.525 INFO  (zkCallback-3-thread-198-processing-n:solrNode027:8983_solr) [c:offers_full s:shard3 r:core_node7 x:offers_full_shard3_replica2] o.a.s.c.SyncStrategy Sync Success - now sync replicas to me\n\n2016-01-27 15:21:08.793 ERROR (qtp1057941451-43187) [c:offers_full s:shard3 r:core_node7 x:offers_full_shard3_replica2] o.a.s.u.p.DistributedUpdateProcessor Request says it is coming from leader, but we are the leader: update.distrib=FROMLEADER&distrib.from=http://solrNode033:8983/solr/offers_full_shard3_replica3/&wt=javabin&version=2\t\n\n(OLD) LEADER : 33\n\n2016-01-27 15:21:08.801 ERROR (qtp1057941451-45871) [c:offers_full s:shard3 r:core_node10 x:offers_full_shard3_replica3] o.a.s.u.p.DistributedUpdateProcessor On core_node10, replica http://solrNode027:8983/solr/offers_full_shard3_replica2/ now thinks it is the leader! Failing the request to let the client retry! org.apache.solr.common.SolrException:Service Unavailable\n\n2016-01-27 15:21:09.712 INFO  (zkCallback-3-thread-36-processing-n:solrNode033:8983_solr-EventThread) [   ] o.a.s.c.c.ConnectionManager Our previous ZooKeeper session was expired. Attempting to reconnect to recover relationship with ZooKeeper...\n\n2016-01-27 15:21:09.747 INFO  (zkCallback-3-thread-36-processing-n:solrNode033:8983_solr-EventThread) [c:offers_full s:shard3 r:core_node10 x:offers_full_shard3_replica3] o.a.s.c.ZkController publishing state=down\n2016-01-27 15:21:09.760 INFO  (zkCallback-3-thread-36-processing-n:solrNode033:8983_solr-EventThread) [c:offers_full s:shard3 r:core_node10 x:offers_full_shard3_replica3] o.a.s.c.ZkController Replica core_node10 NOT in leader-initiated recovery, need to wait for leader to see down state.\n\n(NEW) LEADER : 27\n\n2016-01-27 15:21:10.137 INFO  (zkCallback-3-thread-199-processing-n:solrNode027:8983_solr) [   ] o.a.s.c.c.ZkStateReader A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/collections/offers_full/state.json for collection offers_full has occurred - updating... (live nodes size: 10)\n2016-01-27 15:21:10.139 INFO  (zkCallback-3-thread-199-processing-n:solrNode027:8983_solr) [   ] o.a.s.c.c.ZkStateReader Updating data for offers_full from 1344 to 1345\n2016-01-27 15:21:10.215 INFO  (qtp1057941451-43418) [   ] o.a.s.h.a.CoreAdminHandler Going to wait for coreNodeName: core_node10, state: down, checkLive: null, onlyIfLeader: null, onlyIfLeaderActive: null\n\n(OLD) LEADER : 33\n\n2016-01-27 15:21:10.220 INFO  (zkCallback-3-thread-36-processing-n:solrNode033:8983_solr-EventThread) [c:offers_full s:shard3 r:core_node10 x:offers_full_shard3_replica3] o.a.s.c.ElectionContext cancelElection did not find election node to remove /overseer_elect/election/238989758703863355-solrNode033:8983_solr-n_0000000158\n2016-01-27 15:21:10.221 INFO  (zkCallback-3-thread-36-processing-n:solrNode033:8983_solr-EventThread) [c:offers_full s:shard3 r:core_node10 x:offers_full_shard3_replica3] o.a.s.c.LeaderElector Joined leadership election with path: /overseer_elect/election/238989758703863364-solrNode033:8983_solr-n_0000000175\n2016-01-27 15:21:10.238 INFO  (zkCallback-3-thread-36-processing-n:solrNode033:8983_solr-EventThread) [c:offers_full s:shard3 r:core_node10 x:offers_full_shard3_replica3] o.a.s.c.ZkController Register node as live in ZooKeeper:/live_nodes/solrNode033:8983_solr\n2016-01-27 15:21:10.290 INFO  (updateExecutor-2-thread-3542-processing-n:solrNode033:8983_solr x:offers_full_shard3_replica3 s:shard3 c:offers_full r:core_node10) [c:offers_full s:shard3 r:core_node10 x:offers_full_shard3_replica3] o.a.s.c.RecoveryStrategy Sending prep recovery command to http://solrNode027:8983/solr; WaitForState: action=PREPRECOVERY&core=offers_full_shard3_replica2&nodeName=solrNode033:8983_solr&coreNodeName=core_node10&state=recovering&checkLive=true&onlyIfLeader=true&onlyIfLeaderActive=true\t\n\n(NEW) LEADER : 27\n\n2016-01-27 15:21:10.329 INFO  (qtp1057941451-43519) [   ] o.a.s.s.HttpSolrCall [admin] webapp=null path=/admin/cores params=\nUnknown macro: {nodeName=solrNode033} \n status=0 QTime=2\t\n\n(OLD) LEADER : 33\n\nProcess Recovering\n\n2016-01-27 15:21:52.007 INFO  (zkCallback-3-thread-211-processing-n:solrNode033:8983_solr) [   ] o.a.s.c.c.ZkStateReader A live node change: WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/live_nodes, has occurred - updating... (live nodes size: 11)\n2016-01-27 15:21:52.008 INFO  (zkCallback-3-thread-210-processing-n:solrNode033:8983_solr) [c:offers_full s:shard3 r:core_node10 x:offers_full_shard3_replica3] o.a.s.c.ActionThrottle The last leader attempt started 22366010ms ago.\n2016-01-27 15:21:52.008 INFO  (zkCallback-3-thread-210-processing-n:solrNode033:8983_solr) [c:offers_full s:shard3 r:core_node10 x:offers_full_shard3_replica3] o.a.s.c.ShardLeaderElectionContext Running the leader process for shard shard3\n2016-01-27 15:21:52.007 INFO  (zkCallback-3-thread-211-processing-n:solrNode033:8983_solr) [   ] o.a.s.c.c.ZkStateReader A live node change: WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/live_nodes, has occurred - updating... (live nodes size: 11)\n2016-01-27 15:21:52.011 INFO  (zkCallback-3-thread-210-processing-n:solrNode033:8983_solr) [c:offers_full s:shard3 r:core_node10 x:offers_full_shard3_replica3] o.a.s.c.ShardLeaderElectionContext Checking if I should try and be the leader.\n2016-01-27 15:21:52.011 INFO  (zkCallback-3-thread-210-processing-n:solrNode033:8983_solr) [c:offers_full s:shard3 r:core_node10 x:offers_full_shard3_replica3] o.a.s.c.ShardLeaderElectionContext There may be a better leader candidate than us - going back into recovery\t\n\n(NEW) LEADER : 27\n\n2016-01-27 15:21:53.211 INFO  (zkCallback-3-thread-73-processing-n:solrNode027:8983_solr-EventThread) [   ] o.a.s.c.c.ConnectionManager Watcher org.apache.solr.common.cloud.ConnectionManager@478ca953 name:ZooKeeperConnection Watcher:solrNode013:2181,solrNode014:2181,solrNode015:2181 got event WatchedEvent state:Disconnected type:None path:null path:null type:None\n2016-01-27 15:21:53.211 INFO  (zkCallback-3-thread-73-processing-n:solrNode027:8983_solr-EventThread) [   ] o.a.s.c.c.ConnectionManager zkClient has disconnected\t\n\nAll this time, the replica 51 can't catch up and stay in recovering\tstate.\nThe old leader 33 stays in full recovering state because 27 is the new leader. During several minutes, the node 33 tries to download all the segments but fail on every checksum (\"expected length is 137136 and actual length is 2096582\" for example)\nBecause it cannot keep its zookeeper session, the 27 stays down because there are no leader for the shard...\n\nThis behaviour happens every time I send too many QPS... Under 40 qps, we do not have any problems...\n\nDoes it look similar to these ChaosMonkey failures ? ",
            "id": "comment-15119905"
        },
        {
            "date": "2016-02-01T10:44:05+0000",
            "author": "Stephan Lagraulet",
            "content": "I'm trying to gather all issues related to SolrCloud that affects Solr 5.4. Can you affect SolrCloud component to this issue ? ",
            "id": "comment-15126091"
        },
        {
            "date": "2017-02-20T01:41:05+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 72f71275b28635cc4383fa12923245aaba69d592 in lucene-solr's branch refs/heads/master from markrmiller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=72f7127 ]\n\nSOLR-8129: Harden HdfsChaosMonkeyNothingIsSafeTest some ",
            "id": "comment-15873937"
        },
        {
            "date": "2017-02-22T15:20:23+0000",
            "author": "Mark Miller",
            "content": "I'm going to close this and open a new issue for more recent failures. ",
            "id": "comment-15878458"
        }
    ]
}