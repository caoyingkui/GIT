{
    "id": "LUCENE-2662",
    "title": "BytesHash",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "affect_versions": "4.0-ALPHA",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "This issue will have the BytesHash separated out from LUCENE-2186",
    "attachments": {
        "LUCENE-2662.patch": "https://issues.apache.org/jira/secure/attachment/12455238/LUCENE-2662.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-09-22T06:24:32+0000",
            "content": "We need unit tests and a base implementation as BytesHash is abstract... ",
            "author": "Jason Rutherglen",
            "id": "comment-12913415"
        },
        {
            "date": "2010-09-22T14:54:03+0000",
            "content": "The current hash implementation needs to be separated out of TermsHashPerField.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12913589"
        },
        {
            "date": "2010-09-22T15:05:30+0000",
            "content": "Jason: I am confused... there is no hash impl in TermsHashPerField.\n\nthe hashing, and term encoding and other things, is the responsibility of the analysis chain (TermToBytesRefAttribute):\n\n    // Get the text & hash of this term.\n    int code = termAtt.toBytesRef(utf8);\n\n\n\nthis way, implementations can 'hash-as-they-go' like we do when encoding unicode char[] -> byte[],\nor they can simply return BytesRef.hashCode() if they don't have an optimized implementation. ",
            "author": "Robert Muir",
            "id": "comment-12913599"
        },
        {
            "date": "2010-09-22T15:33:07+0000",
            "content": "The THPF is hashing tokens for use in the indexing RAM buffer and the creation of postings, ie, the lookup of term byte[]s to term ids.  The hash component is currently interwoven into THPF.  \n\nHere's some of the variables being used in THPF.\n\n\nprivate int postingsHashSize = 4;\nprivate int postingsHashHalfSize = postingsHashSize/2;\nprivate int postingsHashMask = postingsHashSize-1;\nprivate int[] postingsHash;\n\n\n\nAlso there's the methods rehashPostings, shrinkHash, postingEquals, and add(int textStart) has the lookup.  \n\nWe'll probably also need to separate out the quick sort implementation in THPF, I'll add that to this issue. ",
            "author": "Jason Rutherglen",
            "id": "comment-12913622"
        },
        {
            "date": "2010-09-22T15:52:15+0000",
            "content": "Jason: what I am saying is if i look at the method in your patch:\n\npublic T add(BytesRef bytes)\n\nthe first thing it does is compute the hash, but this is already computed in the analysis chain.\n\nwhy not have\n\npublic T add(BytesRef bytes, int hashCode)\n\n\n\nand also:\n\npublic T add(BytesRef bytes) {\n  return add(bytes, bytes.hashCode());\n}\n\n\n\nthen we can avoid computing this twice, and keep the optimization in UnicodeUtil ",
            "author": "Robert Muir",
            "id": "comment-12913628"
        },
        {
            "date": "2010-09-22T16:00:24+0000",
            "content": "Ah, ok, I didn't write this code, I extracted it from LUCENE-2186, and nice, you reviewed it can be improved.  I'll make changes to it shortly, hopefully. ",
            "author": "Jason Rutherglen",
            "id": "comment-12913632"
        },
        {
            "date": "2010-09-22T16:05:45+0000",
            "content": "jason, can you please hold off with this since I have newer / different versions of this class already with tests etc. I understand that you need that class but creating all these issues and rushing ahead is rather counter productive.\n\n@Robert: this class is standalone in this patch and doesn't know about the analysis chain. But thanks for the comments I will incorporate them.\n\nsimon ",
            "author": "Simon Willnauer",
            "id": "comment-12913636"
        },
        {
            "date": "2010-09-22T16:10:43+0000",
            "content": "Simon, when do you think you'll be posting? ",
            "author": "Jason Rutherglen",
            "id": "comment-12913638"
        },
        {
            "date": "2010-09-22T16:18:00+0000",
            "content": "Simon, when do you think you'll be posting?\n\nmaybe within the next week I have a busy schedule but does this patch keep you from doing any work? You shouldn't just pull out stuff from 1 month old patches especially as you don't even give me time to reply on the orig. discussion. \n\nAny rush on this? ",
            "author": "Simon Willnauer",
            "id": "comment-12913642"
        },
        {
            "date": "2010-09-22T16:29:26+0000",
            "content": "It'd be nice to get deletes working, ie, LUCENE-2655 and move forward in a way that's useful long term.  What changes have you made? ",
            "author": "Jason Rutherglen",
            "id": "comment-12913651"
        },
        {
            "date": "2010-09-24T12:42:04+0000",
            "content": "This patch contains a slightly different version of BytesHash (renamed it to BytesRefHash but that is to be discussed - while writing this I actually think BytesHash is the better name).  BytesRefHash is now final and does not create Entry objects anymore. Internally it maintains two integer arrays one acting as the hash buckets and the other one contain the bytes-start offset in the ByteBlockPool. Each added entry is assigned to an increasing ordinal since this is what Entry is used in almost all use-cases (in CSF though). For TermsHashPerField this is also \"native\" since is uses the same kind of referencing system.\n\nThese changes keep this class as efficient as possible, keeping GC costs low and allows JIT to do better optimizations. IMO this class is super performance critical and since we recently refactored indexing towards parallel arrays adding another \"object\" array might not be the way to go anyway.\n\nI also incorporated robers comments - thanks for the review anyway. I guess that is the first step towards factoring it out of TermsHashPerField, the next question is are we gonna do that in a different issue and get this committed first?\n\ncomments / review welcome!!\n\nOne more thing, I did not move ByteBlockPool to o.a.l.utils but I thing it belongs there, thoughts? ",
            "author": "Simon Willnauer",
            "id": "comment-12914443"
        },
        {
            "date": "2010-09-24T12:48:33+0000",
            "content": "I guess that is the first step towards factoring it out of TermsHashPerField, the next question is are we gonna do that in a different issue and get this committed first?\n\nI think it would be better if this class were used in the patch... i wouldn't commit it by itself unused. Its difficult for people to review its behavior, since its just a standalone unused thing (for instance, the hashCode thing i brought up) ",
            "author": "Robert Muir",
            "id": "comment-12914452"
        },
        {
            "date": "2010-09-24T14:25:16+0000",
            "content": "> BytesRefHash is now final and does not create Entry objects anymore\n\nThat's good.\n\n> move ByteBlockPool to o.a.l.utils\n\nSure why not.\n\n> factoring it out of TermsHashPerField, the next question is are we gonna do that in a different issue and get this committed first?\n\nWe need to factor it out of THPF otherwise this patch isn't really useful for committing.  Also, it'll get tested through the entirety of the unit tests, ie, it'll get put through the laundry.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12914478"
        },
        {
            "date": "2010-09-24T14:40:00+0000",
            "content": "We need to factor it out of THPF otherwise this patch isn't really useful for committing. Also, it'll get tested through the entirety of the unit tests, ie, it'll get put through the laundry.\n\nYeah, lets see this as the first baby step towards it. I will move ByteBockPool to o.a.l.utils and start cutting THPF over to it. We need to do benchmarking in any case just to make sure JIT doesn't play nasty tricks with us again.\n\nsimon ",
            "author": "Simon Willnauer",
            "id": "comment-12914486"
        },
        {
            "date": "2010-09-24T16:28:43+0000",
            "content": "make sure JIT doesn't play nasty tricks with us again.\n\nWhat would we do if this happens? ",
            "author": "Jason Rutherglen",
            "id": "comment-12914521"
        },
        {
            "date": "2010-09-24T19:52:40+0000",
            "content": "Patch looks good Simon \u2013 some ideas:\n\n\n\n\n\tIn the class jdocs, I think state that this is basically a\n    Map<BytesRef,int>?\n\n\n\n\n\tMaybe we also move ByteBlockPool --> oal.util?\n\n\n\n\n\tMaybe move out the ByteBlockAllocator to its own class (in util)?\n    RecyclingByteBlockAllocator?\n\n\n\n\n\tCan we have DocumentsWriter share the ByteBlockAllocator?  (Right\n    now it's dup'd code since DW also implements this).\n\n\n\n\n\tMaybe rename ords -> keys?  And hash -> values?  (The key isn't\n    really an \"ord\" (I think?) because it increases by more than 1\n    each time... it's more like an address since it references an\n    address in the byte-pool space).\n\n\n\n\n\tWe should advertise the limits in the jdocs \u2013 limited to <= 2GB\n    total byte storage, each key must be <= BLOCK SIZE-2 in length.\n\n\n\n\n\tCan we have sortedEntries() not allocate a new iterator object?\n    Ie, just return the sorted bytesStart int[]?  (This is what's done\n    today, and, for term vectors on small docs, this method is pretty\n    hot).  And the javadocs for this should be stronger \u2013 it's not\n    that the behaviour is undefined after, it's that you must .clear()\n    after you're done consume the sorted entries.\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12914621"
        },
        {
            "date": "2010-09-24T19:54:13+0000",
            "content": "\nmake sure JIT doesn't play nasty tricks with us again.\n\nWhat would we do if this happens?\n\nCry?\n\nOr... install Harmony and see if it has the same problem and if so submit a patch to them to fix it  ",
            "author": "Michael McCandless",
            "id": "comment-12914623"
        },
        {
            "date": "2010-09-24T20:07:17+0000",
            "content": "In the class jdocs, I think state that this is basically a Map<BytesRef,int>?\nyeah that simplifies it - will do.\n\nMaybe we also move ByteBlockPool --> oal.util?\nyeah I did that already - that makes totally sense\n\nMaybe move out the ByteBlockAllocator to its own class (in util)? RecyclingByteBlockAllocator?\n+1 yeah I like that - I also think we should allow to pass the blockpool to the byteshash instead of the allocator. From what I can tell now I think this is necessary for the refactoring anyway since we share pools with secondary TermsHash instances in the termvector case.\n\n\nMaybe rename ords -> keys? And hash -> values? (The key isn't\nreally an \"ord\" (I think?) because it increases by more than 1\neach time... it's more like an address since it references an\naddress in the byte-pool space).\nyeah that depends how you see it - the array index really is the ord though. but I like those names. I will change.\n\n\nWe should advertise the limits in the jdocs - limited to <= 2GB\ntotal byte storage, each key must be <= BLOCK SIZE-2 in length.\nI think I have done the latter already but I will add the other too.\n\n\nCan we have sortedEntries() not allocate a new iterator object?\nIe, just return the sorted bytesStart int[]? (This is what's done\ntoday, and, for term vectors on small docs, this method is pretty\nhot). And the javadocs for this should be stronger - it's not\nthat the behaviour is undefined after, it's that you must .clear()\nafter you're done consume the sorted entries.\nAh I see - good point. I think what you refer to is   public int[] sort(Comparator<BytesRef> comp) - the iterator one is just more convenient one. I will change though.\n\nthanks mike! ",
            "author": "Simon Willnauer",
            "id": "comment-12914627"
        },
        {
            "date": "2010-09-25T18:48:35+0000",
            "content": "Attaching my current state for feedback and iteration.\n\n\n\tfactored out ByteBlockAllocator from DocumentsWriter\n\tmoved ByteBlockPool to o.a.l.util\n\tadded RecyclingByteBlockAllocator which can be used with or without synchronization. IMO the DummyConcurrentLock will be optimized away so that his might be super low cost. - feedback for that would more than welcome.\n\taddressed all the comments from mike - thanks again\n\tadded more tests\n\tcut over constants from DocumentsWriter to ByteBlockPool\n\n\n\nTermsHashPerField is next.... feedback welcome.\n\nsimon ",
            "author": "Simon Willnauer",
            "id": "comment-12914859"
        },
        {
            "date": "2010-09-25T22:43:23+0000",
            "content": "An API change to BBP that would be useful is instead of passing in the \"size in bytes\" to newSlice, it'd be more useful if the level and/or the size were passed in.  In fact, throughout the codebase, a level, specifically the first, is all that is passed into the newSlice method.  The utility of this change is, I'm recording the level of the last slice for the skip list in LUCENE-2312. ",
            "author": "Jason Rutherglen",
            "id": "comment-12914888"
        },
        {
            "date": "2010-09-26T23:50:46+0000",
            "content": "Simon, the patch looks like it's ready for the next stage, ie, TermsHashPerField deparchment.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12915079"
        },
        {
            "date": "2010-09-27T06:03:57+0000",
            "content": "We are almost there. I factored out ByteRefHash out of TermsHashPerField just having two \"nocommit\" parts left in the code I need to find a solution for. \n\n\n\tthere needs to be a way to communicate the byte usage up to DocumentsWriter which I haven't explored yet\n\ttextStarts in ParallelPostingsArray needs to be replaced since it is already maintained in ByteRefHash. I will need to look closer into that but suggestions are welcome. One way to do it would be to attach a reference to BRH instead of the textStart - but that is a naive suggestion since I haven't looked into that in more detail.\n\n\n\nAll tests are passing so far and TermsHashPerField looks somewhat cleaner. I will work on fixing those nocommits and run some indexing perf test against the patch. \n ",
            "author": "Simon Willnauer",
            "id": "comment-12915135"
        },
        {
            "date": "2010-09-28T10:31:11+0000",
            "content": "\nMaybe rename ords -> keys? And hash -> values? (The key isn't really an \"ord\" (I think?) because it increases by more than 1 each time... it's more like an address since it references an address in the byte-pool space).\n\nyeah that depends how you see it - the array index really is the ord though. but I like those names. I will change.\n\nDuh, I agree \u2013 the new names are confusing!!  Sorry.  I was\nconfused... you are right that what's now called \"keys\" are in fact\nreally ords!  They are always incr'd by one, on adding a new one.\n\nHow about renaming key back to ord?  And then maybe rename values to\nbytesStart?  And in their decls add comments saying they are indexed\nby hash code?  And maybe rename addByOffset -> addByBytesStart?\n\n\n\n\tOn the nocommit in ByteBlockPool \u2013 I think that's fine?  It's an\n    internal class....\n\n\n\n\n\tThe nocommit in BytesRefHash seems wrong?  (Ie, compact is used\n    internally)... though maybe we make it private if it's not used\n    externally?\n\n\n\n\n\tOn the \"nocommit factor this out!\" in THPF.java... I agree, the\n    postingsArray.textStarts should go away right?  Ie, it's a\n    [wasteful] copy of what the BytesRefHash is already storing?\n\n\n\n\n\tCan we impl BytesRefHash.bytesUsed as an AtomicLong (hmm maybe\n    AtomicInt \u2013 none of these classes can address > 2GB)?  Then the\n    pool would add in blockSize every time it binds a new block.  That\n    method (DW.bytesUsed) is called alot \u2013 at least once on every\n    addDoc.\n\n\n\n\n\tI'm confused again \u2013 when do we use RecyclingByteBlockAllocator\n    from a single thread...?  Ie, why did the sync need to be\n    conditional for this class, again....?  It seems like we always\n    need it sync'd (both the main pool & per-doc pool need this)?  If\n    so we can simplify and make these methods sync'd?\n\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12915700"
        },
        {
            "date": "2010-09-28T11:52:03+0000",
            "content": "\nHow about renaming key back to ord? And then maybe rename values to\nbytesStart? And in their decls add comments saying they are indexed\nby hash code? And maybe rename addByOffset -> addByBytesStart?\nI don't like addByBytesStart I would like to keep offset since it really is an offset into the pool. addByPoolOffset?\nThe names ord and bytesStart are a good compromise  lets shoot for that.\n\n\n\nOn the nocommit in ByteBlockPool - I think that's fine? It's an\ninternal class....\nyou refer to this: // nocommit - public arrays are not nice! ?\nyeah that more of an style thing but if somebody changes them its their fault for being stupid I guess.\n\n\nThe nocommit in BytesRefHash seems wrong? (Ie, compact is used\ninternally)... though maybe we make it private if it's not used\nexternally?\n\nAh yeah thats bogus - its from a previous iteration which was wrong as well, I will remove.\n\n\nOn the \"nocommit factor this out!\" in THPF.java... I agree, the\npostingsArray.textStarts should go away right? Ie, it's a\n[wasteful] copy of what the BytesRefHash is already storing?\nYeah that is the reason for that nocommit. Yet, I though about this a little and I have two options for this.\n\n\twe could factor out a super class from ParallelPostingArray which only has the textStart int array, the grow and copy method and let ParallelPostingArray subclass it.\nBytesRefHash would accept this class, don't have a good name for it but lets call it TextStartArray for now, and use it internally. It would call grow() once needed inside BytesRefHash and all the other code would be unchanged since PPA is a subclass. \n\tthe other way would be to bind the ByteRefHash to the postings array which seems odd to me though.\n\n\n\nMore ideas?\n\n\nCan we impl BytesRefHash.bytesUsed as an AtomicLong (hmm maybe\nAtomicInt - none of these classes can address > 2GB)? Then the\npool would add in blockSize every time it binds a new block. That\nmethod (DW.bytesUsed) is called alot - at least once on every\naddDoc.\n\nI did exactly that in the not yet uploaded patch. But I figured that it would maybe make more sense to use that AtomicInt in the allocator as well as in THPF or is that what you mean?\n\n\nI'm confused again - when do we use RecyclingByteBlockAllocator\nfrom a single thread...? Ie, why did the sync need to be\nconditional for this class, again....? It seems like we always\nneed it sync'd (both the main pool & per-doc pool need this)? If\nso we can simplify and make these methods sync'd?\n\nman, I am sorry - I  thought I will use this in LUCENE-2186 in a single threaded env but if so I should change it there if needed. I was one step ahead though.\nI will change and maybe have a second one if needed. Agree?\n\nsimon\n\n\n\n\n\n ",
            "author": "Simon Willnauer",
            "id": "comment-12915713"
        },
        {
            "date": "2010-09-28T12:33:36+0000",
            "content": "\nI don't like addByBytesStart I would like to keep offset since it really is an offset into the pool. addByPoolOffset?\nThe names ord and bytesStart are a good compromise  lets shoot for that.\n\nOK!\n\nwe could factor out a super class from ParallelPostingArray which only has the textStart int array, the grow and copy method and let ParallelPostingArray subclass it.\n\nThis seems good?  So, this would be the \"store\" that BRH manages... and by subclassing it you can have other parallel arrays storing anything, indexed by ord.\n\nI did exactly that in the not yet uploaded patch. But I figured that it would maybe make more sense to use that AtomicInt in the allocator as well as in THPF or is that what you mean?\n\nI think we should use it everywhere to track bytes used \n\nman, I am sorry - I thought I will use this in LUCENE-2186 in a single threaded env but if so I should change it there if needed. I was one step ahead though.\nI will change and maybe have a second one if needed. Agree?\n\nAhh that's right I forgot the whole driver for this refactoring heh   Yeah I think leave it sync'd for now and we can test if this hurts perf in LUCENE-2186?  \"Supposedly\" uncontended locks are low-cost (but I'm not sure...). ",
            "author": "Michael McCandless",
            "id": "comment-12915723"
        },
        {
            "date": "2010-09-30T03:05:45+0000",
            "content": "we could factor out a super class from ParallelPostingArray which only has the textStart int array, the grow and copy method and let ParallelPostingArray subclass it. \n\nThis option, makes the most sense.  ParallelByteStartsArray?\n\n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12916355"
        },
        {
            "date": "2010-09-30T16:28:44+0000",
            "content": "Next iteration - seems to be very close!\n\nI have applied the following changes:\n\n\n\tintroduces a AtomicLong to track bytesUsed in DocumetnsWriter, TermsHashPerField, ByteRefHash and RecyclingByteBlockAllocator\n\tFactored out  a BytesStartArray class from BytesRefHash that manages the int[] holding the bytesStart offsets. TermsHashPerField subclasses and manages the ParallelPostingsArray through it.\n\tremove remaining no-commits\n\tmade RecyclingbyteBlockAllocator synced by default (we use synchronized methods for it now)\n\n\n\nI run a quick Wikipedia 100k docs benchmark against trunk vs. LUCENE-2662 and the results are promising.\n\n\n\nversion\nrec/sec\nelapsed sec\navgUsedMem\n\n\nLUCENE-2662\n717.30\n139.41\n536,682,592\n\n\ntrunk\n 682.66\n146.49\n546,065,344\n\n\n\n\n\nI will run the 10M benchmark once I get back to this. ",
            "author": "Simon Willnauer",
            "id": "comment-12916542"
        },
        {
            "date": "2010-10-01T03:41:54+0000",
            "content": "Simon, looks good.\n\nAre we using:\n\npublic int add(BytesRef bytes, int code)\n\n\n\nyet? ",
            "author": "Jason Rutherglen",
            "id": "comment-12916767"
        },
        {
            "date": "2010-10-01T06:26:03+0000",
            "content": "Are we using:...\n\nyeah, look at TermsHashPerFields add() method\n\n       termID = bytesHash.add(termBytesRef, termAtt.toBytesRef(termBytesRef));\n\n\n\nsimon ",
            "author": "Simon Willnauer",
            "id": "comment-12916799"
        },
        {
            "date": "2010-10-01T12:28:50+0000",
            "content": "I indexed 10M 1KB wikipedia docs, single threaded, and also see things a bit faster w/ the patch (10,353 docs/sec vs 10,182 docs/sec).  Nice to have a refactor improve performance for a change, heh.\n\nThe avgUsedMem was quite a bit higher (1.5GB vs 1.0GB), but, I'm not sure this stat is trustworthy.... I'll re-run w/ infoStream enabled to see if anything looks suspicious (eg, we are somehow not tracking bytes used correctly).\n\nStill, the resulting indices had identical structure (ie we seem to flush at exactly the same points), so I think bytes used is properly tracked. ",
            "author": "Michael McCandless",
            "id": "comment-12916872"
        },
        {
            "date": "2010-10-01T12:33:24+0000",
            "content": "Still, the resulting indices had identical structure (ie we seem to flush at exactly the same points), so I think bytes used is properly tracked.\n\nSorry, scratch that \u2013 I was inadvertently flushing by doc count, not by RAM usage.  I'm re-running w/ flush-by-RAM to verify we flush at exactly the same points as trunk. ",
            "author": "Michael McCandless",
            "id": "comment-12916873"
        },
        {
            "date": "2010-10-01T12:46:34+0000",
            "content": "In RecyclingByteBlockAllocator.recycleByteBlocks, if we cannot recycle all of the blocks (ie because it exceeds maxBufferedBlocks), we are failing to null out the entries in the incoming array?\n\nAlso maybe rename pos -> freeCount?  (pos is a little too generic?) ",
            "author": "Michael McCandless",
            "id": "comment-12916875"
        },
        {
            "date": "2010-10-01T13:07:44+0000",
            "content": "Simon, thank you for renaming the 'utf8' variables here.  ",
            "author": "Robert Muir",
            "id": "comment-12916882"
        },
        {
            "date": "2010-10-01T13:21:53+0000",
            "content": "Simon, thank you for renaming the 'utf8' variables here.\nYW \n\nIn RecyclingByteBlockAllocator.recycleByteBlocks, if we cannot recycle all of the blocks (ie because it exceeds maxBufferedBlocks), we are failing to null out the entries in the incoming array?\nAhh you are right - I will fix. \n\nAlso maybe rename pos -> freeCount? (pos is a little too generic?)\nI mean its internal though but I see your point.\n\nthanks for reviewing it closely. \n\n\nThe avgUsedMem was quite a bit higher (1.5GB vs 1.0GB), but, I'm not sure this stat is trustworthy.... I'll re-run w/ infoStream enabled to see if anything looks suspicious (eg, we are somehow not tracking bytes used correctly).\n\nhmm I will dig once I get back to my workstation.\n\nsimon ",
            "author": "Simon Willnauer",
            "id": "comment-12916885"
        },
        {
            "date": "2010-10-01T15:27:41+0000",
            "content": "OK my 2nd indexing test (10M wikipedia docs, flush @ 256 MB ram used) finished and trunk/patch are essentially the same throughput, and, all flushes happened at identical points.  So I think we are good to go...\n\nNice work Simon! ",
            "author": "Michael McCandless",
            "id": "comment-12916913"
        },
        {
            "date": "2010-10-01T17:35:40+0000",
            "content": "I also ran a test w/ 5 threads \u2013 they are close (22,402 docs/sec for patch, 22,868 docs/sec for trunk), and this time avgUsedMem is closer (811 MB for trunk, 965 MB for patch).\n\nI don't think the avgUsedMem is that meaningful \u2013 it takes the max of Runtime.totalMemory() - Runtime.freeMemory() (which includes garbage I think), after each completed task, and then averages across all tasks.  In my case I think it's averaging 1 measure per thread, so it's really sort of measuring how much garbage there happened to be at the time. ",
            "author": "Michael McCandless",
            "id": "comment-12916965"
        },
        {
            "date": "2010-10-01T18:18:17+0000",
            "content": "I instrumented trunk & the patch to see how many times we do new byte[bufferSize] while building 5M index, and they both alloc the same number of byte[] from the BBA.  So I don't think we have a memory issue... ",
            "author": "Michael McCandless",
            "id": "comment-12916988"
        },
        {
            "date": "2010-10-02T12:35:59+0000",
            "content": "This patch fixes nulling out the recycled but not reused byte blocks in RecyclingByteBlockAllocator.\n\nI thing we are ready to go I will commit to trunk soon. I don't think we need a CHANGES.TXT here - at least I can not find any section this refactoring would fit to. \n\nsimon ",
            "author": "Simon Willnauer",
            "id": "comment-12917186"
        },
        {
            "date": "2010-10-02T12:45:25+0000",
            "content": "Committed to trunk in rev. 1003790\n\n@Jason: do you need that merged into Realtime-Branch or is buschmi going to do that? Otherwise I can help too\n\nI will keep it open until this is merged into Realtime Branch ",
            "author": "Simon Willnauer",
            "id": "comment-12917188"
        },
        {
            "date": "2010-10-03T16:04:21+0000",
            "content": "Simon, I'm going to get deletes working, tests passing using maps in the RT branch, then we can integrate.  This'll probably be best. ",
            "author": "Jason Rutherglen",
            "id": "comment-12917354"
        },
        {
            "date": "2010-10-03T18:10:12+0000",
            "content": "Simon, I'm going to get deletes working, tests passing using maps in the RT branch, then we can integrate. This'll probably be best.\nJason, I suggest you create a separate issue something like \"Integrate BytesRefHash in Realtime Branch\" and I will take care of it. I think this issue had a clear target to factor out the hash table out of TermsHashPerField and we should close it. lets use a new one to track the integration.\n\nThoughts?\n\nSimon ",
            "author": "Simon Willnauer",
            "id": "comment-12917372"
        },
        {
            "date": "2010-10-03T22:10:26+0000",
            "content": "Lets commit this to trunk.  We need to merge in all of trunk to the RT branch, or vice versa at some point anyways.  This patch could be a part of that bulk merge-in, or we can simply do it now. ",
            "author": "Jason Rutherglen",
            "id": "comment-12917416"
        },
        {
            "date": "2010-10-04T09:51:06+0000",
            "content": "This was already committed to trunk... ",
            "author": "Michael McCandless",
            "id": "comment-12917537"
        },
        {
            "date": "2010-10-25T07:56:45+0000",
            "content": "Why is this issue still open, if the patch was already committed to trunk? ",
            "author": "Mathias Walter",
            "id": "comment-12924484"
        },
        {
            "date": "2010-10-25T08:02:42+0000",
            "content": "Why is this issue still open, if the patch was already committed to trunk?\n\nsee my comment above: \n\nI will keep it open until this is merged into Realtime Branch ",
            "author": "Simon Willnauer",
            "id": "comment-12924488"
        },
        {
            "date": "2010-11-25T10:42:34+0000",
            "content": "I will keep it open until this is merged into Realtime Branch\nI think we should really close this since RT branch is not very active right now....\n ",
            "author": "Simon Willnauer",
            "id": "comment-12935716"
        },
        {
            "date": "2010-11-25T12:13:40+0000",
            "content": "I think we should really close this since RT branch is not very active right now....\n\nSorry about that.  I need to merge trunk into RT, then I'll get this change too.  It's a big merge though with tons of conflicts... ",
            "author": "Michael Busch",
            "id": "comment-12935735"
        },
        {
            "date": "2010-11-25T12:24:11+0000",
            "content": "Sorry about that. I need to merge trunk into RT, then I'll get this change too. It's a big merge though with tons of conflicts...\nHA! good to see you here!  have fun with the merge ",
            "author": "Simon Willnauer",
            "id": "comment-12935739"
        },
        {
            "date": "2010-11-25T12:44:52+0000",
            "content": "HA! good to see you here!  have fun with the merge\n\nHe is working hard, it's 4:45 am in California  ",
            "author": "Uwe Schindler",
            "id": "comment-12935744"
        },
        {
            "date": "2010-11-25T12:51:39+0000",
            "content": "He is working hard, it's 4:45 am in California \ntrue but he is in germany  ",
            "author": "Simon Willnauer",
            "id": "comment-12935746"
        },
        {
            "date": "2010-11-25T13:02:31+0000",
            "content": "Yeah sitting in Stuttgart, going to hit the Weihnachtsmarkt soon - let's see how the merge goes after several glasses of Gluehwein  ",
            "author": "Michael Busch",
            "id": "comment-12935747"
        }
    ]
}