{
    "id": "LUCENE-4556",
    "title": "FuzzyTermsEnum creates tons of objects",
    "details": {
        "components": [
            "core/search",
            "modules/spellchecker"
        ],
        "fix_versions": [
            "4.10",
            "6.0"
        ],
        "affect_versions": "4.0",
        "priority": "Critical",
        "labels": "",
        "type": "Improvement",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "I ran into this problem in production using the DirectSpellchecker. The number of objects created by the spellchecker shoot through the roof very very quickly. We ran about 130 queries and ended up with > 2M transitions / states. We spend 50% of the time in GC just because of transitions. Other parts of the system behave just fine here.\n\nI talked quickly to robert and gave a POC a shot providing a LevenshteinAutomaton#toRunAutomaton(prefix, n) method to optimize this case and build a array based strucuture converted into UTF-8 directly instead of going through the object based APIs. This involved quite a bit of changes but they are all package private at this point. I have a patch that still has a fair set of nocommits but its shows that its possible and IMO worth the trouble to make this really useable in production. All tests pass with the patch - its a start....",
    "attachments": {
        "LUCENE-4556.patch": "https://issues.apache.org/jira/secure/attachment/12553317/LUCENE-4556.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-11-13T14:52:58+0000",
            "content": "here is a patch ...scary\u2122 ",
            "author": "Simon Willnauer",
            "id": "comment-13496235"
        },
        {
            "date": "2012-11-13T16:43:23+0000",
            "content": "I hit this test failure:\n\nant test  -Dtestcase=TestSlowFuzzyQuery2 -Dtests.method=testFromTestData -Dtests.seed=6019B3869272BDF0 -Dtests.locale=el_CY -Dtests.timezone=Asia/Anadyr -Dtests.file.encoding=ANSI_X3.4-1968\n\n ",
            "author": "Michael McCandless",
            "id": "comment-13496315"
        },
        {
            "date": "2012-11-13T19:13:48+0000",
            "content": "What spooks me about this patch is this code (LevenshteinAutomaton) is already REALLY hairy ... and this change would add yet more hair to it (when really we need to be doing the reverse, so the code becomes approachable to new eyeballs).\n\nAlso: are we sure the objects created here are really such a heavy GC load...?\n\nI ran a quick test, respelling (using DirectSpellChecker() w/ its defaults) a set of 500 5-character terms against the full Wikipedia English (33.M docs) index, using concurrent mark/sweep collector w/ 2 GB heap and I couldn't see any difference in the net throughput on a 24 core box ... both got ~780 respells/sec.\n\nSimon can you describe what use case you're seeing where GC is cutting throughput by 50%? ",
            "author": "Michael McCandless",
            "id": "comment-13496439"
        },
        {
            "date": "2012-11-19T00:32:16+0000",
            "content": "I'm attaching a possible alternate way to reduce objects ... it's\nonly just a start ...\n\nI created a new LightAutomaton class (I'm not wed to that name!) which\nplaces a severe \"append only\" restriction on how you are allowed to\nbuild up the FSA: you must add all transitions for a given state\nbefore adding another state's transitions.\n\nIt operates with only \"int state\", and stores all transitions in a\nprivate int[].\n\nThis is a big restriction, but I think a number of our FSA ops would\nwork fine with this.  I'm pretty sure building the LevA, and doing the\nUTF32->UTF8 conversion would work fine append-only...\n\nIn the patch, I added Automaton.toLightAutomaton to convert from\n\"heavy\" to LightAutomaton, and then fixed CompiledAutomaton (and its\nconsumers) to use that.  Tests pass.\n\nI think it shouldn't be too hard to cut over the Lev building to this\ntoo ... but wanted to get feedback first.\n\nSimon, it'd be great if you could try this patch on your benchmark\nsince I can't reproduce the too-heavy GC in my benchmark ... I'm\nparticularly curious whether the 50% time spent in GC you see is due\nto 1) creating too many objects vs 2) holding onto those objects for\ntoo long (in CompiledAutomaton, while the query runs...).  So this\npatch would test whether it's case 2). ",
            "author": "Michael McCandless",
            "id": "comment-13499975"
        },
        {
            "date": "2013-07-23T18:44:33+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13716986"
        },
        {
            "date": "2014-04-16T12:54:49+0000",
            "content": "Move issue to Lucene 4.9. ",
            "author": "Uwe Schindler",
            "id": "comment-13970890"
        },
        {
            "date": "2014-05-28T10:57:58+0000",
            "content": "I'm having GC trouble and I'm using the DirectCandidateGenerator.  Its obviously kind of hard to tell how much the automata is contributing in production but when I try it locally just generating the automata for two or three terms takes about 200KB of memory.  Napkin math (200KB * 250queries/second) says this makes about 50MB of garbage per second per index.  Obviously it gets worse if you run this in a sharded context where each shard does the generating.  Well, not really worse, but the large up front cost and memory consumption of this process is relatively static based on shard size so this becomes a reason to use larger shards. \n\nI should propose that in addition to Simon's patches another other option is to try to implement something like the stack based automaton simulation that the Schulz Mihov paper (the one that proposed the Lev automaton) describes in section 6.  Its not useful for stuff like intersecting the enums but if you are willing to forgo that you could probably get away with much less memory consumption. ",
            "author": "Nik Everett",
            "id": "comment-14011014"
        },
        {
            "date": "2014-06-20T21:40:25+0000",
            "content": "This should now be fixed by LUCENE-5752. ",
            "author": "Michael McCandless",
            "id": "comment-14039405"
        }
    ]
}