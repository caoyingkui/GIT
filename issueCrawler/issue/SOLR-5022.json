{
    "id": "SOLR-5022",
    "title": "PermGen exhausted test failures on Jenkins.",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "5.3"
        ],
        "components": [
            "Tests"
        ],
        "type": "Test",
        "priority": "Critical",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "",
    "attachments": {
        "SOLR-5022.patch": "https://issues.apache.org/jira/secure/attachment/12591432/SOLR-5022.patch",
        "intern-count-win.txt": "https://issues.apache.org/jira/secure/attachment/12591446/intern-count-win.txt",
        "SOLR-5022-permgen.patch": "https://issues.apache.org/jira/secure/attachment/12591652/SOLR-5022-permgen.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-13702876",
            "date": "2013-07-09T04:19:26+0000",
            "content": "Comment from dev list:\n\n\nLooks like we currently don't set the max perm gen for tests, so you get the default - I think we want to change that regardless - we don't want it to vary IMO - it should work like Xmx.\n\nI think we should just set it to 128 mb, and these tests should have plenty of room to run. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13703084",
            "date": "2013-07-09T09:21:13+0000",
            "content": "-1 to increasing permgen.\n\nsolr ran fine without it before, I want to know \"why something wants\" more permgen: and for what? classes, interned strings, what exactly. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13703087",
            "date": "2013-07-09T09:28:20+0000",
            "content": "This isnt the heap where you give things \"plenty of room\".\n\nThis is a memory leak that should be fixed. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13703100",
            "date": "2013-07-09T09:52:04+0000",
            "content": "The problem with raising permgen is:\n\n\tIt's Hotspot specific only, so does not work with other JVMs\n\tIts no longer available in Java 8\n\n\n\nI would really prefer to maybe tune the tests and maybe not create so many nodes in the cloud tests. It looks like the bug happens more often with higher test multiplier (-Dtests.multiplier=3), so maybe we can really tune that.\nIf we want to raise permgen, we have to do it in a similar way like we do enable the heap dumps - with lots of <condition/> tasks in ANT...  "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13703102",
            "date": "2013-07-09T09:56:31+0000",
            "content": "One thing in addition:\nWe currently have a assumeFalse() in the hadoop tests that check for windows and freebsd. But the latter, freebsd is bogus, as only the configuration of Jenkins FreeBSD is wrong, not FreeBSD in general (the blackhole must be enabled).\n\nI would prefer to add a property to ANT \"tests.disable.hadoop\" that defaults to \"true\" (on Windows) and \"false\" elsewhere. In the tests we can make an assume on the existence of this property. Or alternatively put all hadoop tests in a test group that can be disabled (I would prefer the latter, maybe Dawid Weiss can help).\n\nOn FreeBSD jenkins we would set this property to \"true\", on other jenkins we can autodetect it (windows, or other). And if one does not want to run Hadoop tests at all, he can disable.\n "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13703104",
            "date": "2013-07-09T10:02:15+0000",
            "content": "Or alternatively put all hadoop tests in a test group that can be disabled\n\nThis shouldn't be a problem \u2013 create a new test group (an annotation marked with a meta-annotation, see existing code of BadApple for example), enable or disable the test group by default, override via ANT.\n\nThe group would be disabled/enabled via ant's condition and a value passed via system property, much like it is the case with badapple and nightly. There is no way to evaluate a test group's execution status at runtime; an alternative here is to use a before-suite-rule and an assumption in there. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13703245",
            "date": "2013-07-09T13:17:26+0000",
            "content": "Maybe I know, why the permgen issues do not happen for all of us! The reason is:\n\n\n\tSomething seems to eat permgen by interning strings! Those interned strings are never freed until the JVM dies.\n\tIf you run with many CPUs, the test runner runs tests in multiple parallel JVMs, so every JVMs runs less tests.\n\n\n\n...the Jenkins server on MacOSX runs with one JVM only (because the virtual box has only 2 virtual CPUs). So all tests have to share the permgen. Windows always passes because no hadoop used. And linux fails more seldom (2 parallel JVMs). On FreeBSD we also don't run hadoop tests.\n\nWe have to find out: Something seems to eat all permgen, not by loading classes, but by interning strings. And that\u2019s the issue here. My idea would be: I will run forbidden-apis on all JAR files of Solr that were added and will forbid String#intern() signature. This should show us very fast, who interns strings and we can open bug reports or hot-patch those jar files. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13703263",
            "date": "2013-07-09T13:37:17+0000",
            "content": "solr ran fine without it before,\n\nIt runs fine now as well - requiring more perm gen in tests is not a Solr bug - sorry. Simply saying words don't make things true \n\nFor running the clover target, we set the perm size to 192m - quick, fix that bug! Oh wait, thats a stupid thing to say... "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13703264",
            "date": "2013-07-09T13:39:05+0000",
            "content": "It looks like the bug happens more often with higher test multiplier (-Dtests.multiplier=3), so maybe we can really tune that.\n\nYes, we could make our tests shittier rather than give them the required resources to run, but thats a pretty silly trade. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13703265",
            "date": "2013-07-09T13:39:50+0000",
            "content": "Its no longer available in Java 8\n\nAnd do you see the problem on Java 8 runs? "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13703273",
            "date": "2013-07-09T13:48:42+0000",
            "content": "Here is a patch not for the permgen issue, but make Jenkins more flexible. A sysprop -Dtests.disableHdfs=true is now supported. It is by default true on Windows.\n\nThe good thing, if you have cygwin, you can enable them now \n\nI will commit this as a first step to make the Hdfs stuff more flexible. The ASF Jenkins server gets this sysprop hardcoded into the jenkins config (like tests.jettyConnector).  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13703274",
            "date": "2013-07-09T13:48:42+0000",
            "content": "the Jenkins server on MacOSX runs with one JVM only (because the virtual box has only 2 virtual CPUs). So all tests have to share the permgen. Windows always passes because no hadoop used. And linux fails more seldom (2 parallel JVMs). \n\nYes, this would match what I have seen in the wild - on the machines that have fewer cores, I was more likely to see perm gen issues with certain agressive tests. With my 6 core machines where I run with 8 jvms, I have never even remotely seen an issue. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13703275",
            "date": "2013-07-09T13:49:48+0000",
            "content": "And do you see the problem on Java 8 runs?\n\nNo, also not on jRockit or IBM J9. But MacOSX only has Java 6 and Java 7 at the moment, so it's not 100% for sure. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13703278",
            "date": "2013-07-09T13:51:14+0000",
            "content": "For running the clover target, we set the perm size to 192m - quick, fix that bug! Oh wait, thats a stupid thing to say...\n\nClover only works on Oracle JDKs... "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13703282",
            "date": "2013-07-09T13:57:25+0000",
            "content": "I wouldn't want to argue whether increasing permgen is a good fix or not, but it's an interesting debugging problem on its own. I've just ran Solr tests with an aspect that intercepts intern() calls. I'll post the results here once the tests complete. Let's see what we can get.  "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13703294",
            "date": "2013-07-09T14:06:29+0000",
            "content": "Thanks Dawid, so I don't need to setup forbidden-apis for that! That was my first idea how to find the places that call intern(). "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13703295",
            "date": "2013-07-09T14:06:53+0000",
            "content": "Patch looks good Uwe - +1 on that approach.\n\nincreasing permgen is a good fix or not,\n\nI would call it a workaround more than a fix - longer term it would be nice to see the root cause addressed - but considering it would seem to involve code in another project, you have to work from a short term and 'possible' long term perspective. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13703297",
            "date": "2013-07-09T14:08:49+0000",
            "content": "Commit 1501278 from Uwe Schindler\n[ https://svn.apache.org/r1501278 ]\n\nSOLR-5022: Make it possible to disable HDFS tests on ANT command line (so ASF Jenkins can use it). Windows is disabled by default, too. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13703305",
            "date": "2013-07-09T14:09:49+0000",
            "content": "Commit 1501279 from Uwe Schindler\n[ https://svn.apache.org/r1501279 ]\n\nMerged revision(s) 1501278 from lucene/dev/trunk:\nSOLR-5022: Make it possible to disable HDFS tests on ANT command line (so ASF Jenkins can use it). Windows is disabled by default, too. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13703306",
            "date": "2013-07-09T14:10:47+0000",
            "content": "Commit 1501281 from Uwe Schindler\n[ https://svn.apache.org/r1501281 ]\n\nMerged revision(s) 1501278 from lucene/dev/trunk:\nSOLR-5022: Make it possible to disable HDFS tests on ANT command line (so ASF Jenkins can use it). Windows is disabled by default, too. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13703365",
            "date": "2013-07-09T15:15:14+0000",
            "content": "This is a count/uniq of a full run from a Windows box. I forgot it won't run Hadoop tests in this mode \u2013 will retry on a Mac this evening (preemptive interrupt from kids). "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13703835",
            "date": "2013-07-09T21:36:08+0000",
            "content": "Eh... those Solr tests run foreeeeever (90 minutes using a single JVM). I ran the code on 4x branch and I honestly don't see anything being interned in Hadoop. It might be interning something indirectly via Java system classes (which are not aspect-woven) but I doubt it. \n\nThe full execution log is here:\nhttp://www.cs.put.poznan.pl/dweiss/tmp/full.log.gz\n\nand the interning stats are here (first column is the # of calls, then the origin class and the interned string):\nhttp://www.cs.put.poznan.pl/dweiss/tmp/log.stats\n\nA few libraries intern strings heavily:\n\norg.apache.xmlbeans.*\norg.apache.velocity.*\n\n\n\nbut a lot of calls comes from Lucene itself:\n\norg.apache.lucene.codecs.lucene3x.TermBuffer\norg.apache.lucene.codecs.lucene3x.Lucene3xFields$PreTermsEnum\n\n\n\nWhen you look at the stats file overall though, ALL the interned strings shouldn't take more than 1.4M (that's the size of unique strings and additional boilerplate).\n\nSo no luck yet. Unless you can explain what's happening based on that output.\n\nThe next step for me is to log permgen use before/ after each test and see where \nmemory is consumed and how. I'll do it tomorrow, perhaps on a different machine (it really takes ages to run those tests). "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13703838",
            "date": "2013-07-09T21:39:27+0000",
            "content": "One more thing \u2013 Uwe, do you remember the seed/ command line to reproduce that permgen error (on a mac?). "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13703849",
            "date": "2013-07-09T21:50:05+0000",
            "content": "Dawid:\n\nLucene branch_4x, run http://jenkins.thetaphi.de/job/Lucene-Solr-4.x-MacOSX/618/consoleFull, heapdumps: http://jenkins.thetaphi.de/job/Lucene-Solr-4.x-MacOSX/618/artifact/heapdumps/ (I set this build to be sticky, so you can download heapdumps)\n\n[Lucene-Solr-4.x-MacOSX] $ /bin/sh -xe /var/folders/qg/h2dfw5s161s51l2bn79mrb7r0000gn/T/hudson1681176734157627309.sh\n+ echo Using JDK: 64bit/jdk1.6.0 -XX:+UseCompressedOops -XX:+UseParallelGC\nUsing JDK: 64bit/jdk1.6.0 -XX:+UseCompressedOops -XX:+UseParallelGC\n+ /Users/jenkins/tools/java/64bit/jdk1.6.0/bin/java -XX:+UseCompressedOops -XX:+UseParallelGC -version\njava version \"1.6.0_51\"\nJava(TM) SE Runtime Environment (build 1.6.0_51-b11-457-11M4509)\nJava HotSpot(TM) 64-Bit Server VM (build 20.51-b01-457, mixed mode)\n[Lucene-Solr-4.x-MacOSX] $ /Users/jenkins/jenkins-slave/tools/hudson.tasks.Ant_AntInstallation/ANT_1.8.2/bin/ant \"-Dargs=-XX:+UseCompressedOops -XX:+UseParallelGC\" -Dtests.jvms=1 jenkins-hourly\n\n\n\nMaster-Seed was: 143E6CCF7E42064B "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13703850",
            "date": "2013-07-09T21:52:42+0000",
            "content": "Lucene 3 interned field names, so the 3.x codec does this, too:\n\n\norg.apache.lucene.codecs.lucene3x.TermBuffer\norg.apache.lucene.codecs.lucene3x.Lucene3xFields$PreTermsEnum\n\nThis should be fine.\n\n\nWhen you look at the stats file overall though, ALL the interned strings shouldn't take more than 1.4M (that's the size of unique strings and additional boilerplate).\n\nToo bad, so i have no idea anymore. No crazy classloaders going amok. No interned strings, what else can fill permgen? "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13703915",
            "date": "2013-07-09T22:41:49+0000",
            "content": "Commit 1501595 from Uwe Schindler\n[ https://svn.apache.org/r1501595 ]\n\nSOLR-5022: Pass-through disableHdfs to Maven Surefire "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13703925",
            "date": "2013-07-09T22:43:56+0000",
            "content": "Commit 1501596 from Uwe Schindler\n[ https://svn.apache.org/r1501596 ]\n\nMerged revision(s) 1501595 from lucene/dev/trunk:\nSOLR-5022: Pass-through disableHdfs to Maven Surefire "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13703931",
            "date": "2013-07-09T22:44:56+0000",
            "content": "Commit 1501597 from Uwe Schindler\n[ https://svn.apache.org/r1501597 ]\n\nMerged revision(s) 1501595 from lucene/dev/trunk:\nSOLR-5022: Pass-through disableHdfs to Maven Surefire "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13704285",
            "date": "2013-07-10T07:42:56+0000",
            "content": "Ok, thanks Uwe. I'll keep digging. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13704322",
            "date": "2013-07-10T08:51:19+0000",
            "content": "Commit 1501678 from Uwe Schindler\n[ https://svn.apache.org/r1501678 ]\n\nSOLR-5022: Make the Maven build also automatically populate the tests.disableHdfs property by a build profile. Otherwise the maven build fails by default on Windows. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13704507",
            "date": "2013-07-10T12:36:48+0000",
            "content": "It's been both fun and a learning experience debugging this. I have good news and bad news:\n\n\n\tthe good news is: it's not a memory leak,\n\tthe bad news is:  it's not a memory leak \n\n\n\nthe debugging process\n\nClearly permgen is one of the most wicked JVM features - it's damn hard to figure out what its\ncontent really is (I didn't find a way to dump it from within a running process without invoking\nthe debugging interface, which in turn starts its own threads, etc.).\n\nThe way I approached the problem (which may be useful for future reference) is as follows:\n\n\n\tI wrote a short aspect that injects itself before any String.intern is called:\n\n    pointcut targetMethod(): call(String java.lang.String.intern());\n\n    before() : targetMethod()\n    {\n        final JoinPoint jp = thisJoinPoint;\n        System.out.println(\"String#intern() from: \" \n            + jp.getSourceLocation().getWithinType() + \" => \"\n            + jp.getTarget());\n    }\n\n\n\n\n\n\n\tthen I added a Before and After hook (executed before/after each test) that dumped memory pools:\n\n        System.out.println(\"Memdump#from: \" \n            + this.getClass().getName() + \" => \");\n\n        for (MemoryPoolMXBean bean : ManagementFactory.getMemoryPoolMXBeans()) {\n            MemoryUsage usage = bean.getUsage();\n            System.out.println(\n                String.format(Locale.ENGLISH,\n                    \"%20s - I:%7.1f U:%7.1f M:%7.1f\",\n                    bean.getName(),\n                    usage.getInit() / (1024 * 1024.0d),\n                    usage.getUsed() / (1024 * 1024.0d),\n                    usage.getMax()  / (1024 * 1024.0d)));\n        }\n\n\n\n\n\n\n\tthen I ran solr test in one JVM, with the following parameters:\n\nant -Dtests.seed=143E6CCF7E42064B \n    -Dtests.leaveTemporary=true \n    -Dtests.jvms=1 \n    -Dargs=\"-javaagent:aspectjweaver.jar -XX:+UseCompressedOops -XX:+UseParallelGC -XX:+TraceClassLoading\"\n    test-core\n\n\nI had to modify common-build.xml to include aspectj classpath entries (and the aspect itself) because \nI couldn't get it to work by passing -cp via the args parameter (didn't look too deeply since it's a hack).\n\n\n\n\n\tI again modified common-build.xml and added:\n\nsysouts=\"true\" jvmoutputaction=\"pipe,ignore\"\n\n\nto junit4:junit4 task's attributes so that all output is emitted to temporary files under a build folder.\n\n\n\nthe results\n\nFrom the dumped output streams we have the following weave info indicating which methods run String.intern:\n\n$ grep \"String.intern(\" junit4-J0-20130710_122632_726.syserr\n\nin Type 'com.ctc.wstx.util.SymbolTable'\nin Type 'com.ctc.wstx.util.SymbolTable'\nin Type 'com.ctc.wstx.util.InternCache'\nin Type 'org.apache.solr.response.JSONWriter'\nin Type 'org.apache.lucene.codecs.lucene3x.TermBuffer'\nin Type 'org.apache.lucene.codecs.lucene3x.Lucene3xFields$PreTermsEnum'\nin Type 'org.apache.solr.common.luke.FieldFlag'\nin Type 'org.apache.solr.search.DocSetPerf'\nin Type 'org.joda.time.tz.ZoneInfoProvider'\nin Type 'org.joda.time.tz.DateTimeZoneBuilder$PrecalculatedZone'\nin Type 'org.joda.time.tz.DateTimeZoneBuilder$Recurrence'\nin Type 'org.joda.time.chrono.GJLocaleSymbols'\nin Type 'org.apache.solr.request.TestWriterPerf'\n\n\n\nThese indeed intern a lot of strings but they're typically the same so they don't amount to the growth of permgen.\nThis in turn is very steady over the runtime of the test JVM:\n\n$ egrep -o -e \"PS Perm Gen[^%]+\" junit4-J0-20130710_122632_726.sysout\n\nPS Perm Gen - I:   20.8 U:   15.9 M:   82.0\nPS Perm Gen - I:   20.8 U:   16.1 M:   82.0\nPS Perm Gen - I:   20.8 U:   34.6 M:   82.0\nPS Perm Gen - I:   20.8 U:   37.7 M:   82.0\nPS Perm Gen - I:   20.8 U:   37.7 M:   82.0\nPS Perm Gen - I:   20.8 U:   37.9 M:   82.0\nPS Perm Gen - I:   20.8 U:   37.9 M:   82.0\nPS Perm Gen - I:   20.8 U:   38.0 M:   82.0\n...\nPS Perm Gen - I:   20.8 U:   77.3 M:   82.0\nPS Perm Gen - I:   20.8 U:   77.4 M:   82.0\nPS Perm Gen - I:   20.8 U:   77.4 M:   82.0\nPS Perm Gen - I:   20.8 U:   77.4 M:   82.0\nPS Perm Gen - I:   20.8 U:   77.4 M:   82.0\n\n\n\nI stands for \"initial\", U for \"used\", M for \"maximum\". So you can see that the permgen is nearly-exhausted in this run \n(it didn't OOM though). Out of curiosity I checked for class loading markers \u2013 classes are loaded throughout the whole run,\nbecause each test loads different fragments of the code. So even at the end of the run you get things like:\n\nMemdump#from: org.apache.solr.update.processor.ParsingFieldUpdateProcessorsTest => \n          Code Cache - I:    2.4 U:   27.0 M:   48.0\n       PS Eden Space - I:   62.9 U:   68.7 M:  167.9\n   PS Survivor Space - I:   10.4 U:    0.8 M:    0.8\n          PS Old Gen - I:  167.5 U:   97.8 M:  341.4\n         PS Perm Gen - I:   20.8 U:   72.7 M:   82.0\n[Loaded org.joda.time.ReadWritableInstant from file:/C:/Work/lucene-solr-svn/branch_4x/solr/core/lib/joda-time-2.2.jar]\n[Loaded org.joda.time.ReadWritableDateTime from file:/C:/Work/lucene-solr-svn/branch_4x/solr/core/lib/joda-time-2.2.jar]\n[Loaded org.joda.time.MutableDateTime from file:/C:/Work/lucene-solr-svn/branch_4x/solr/core/lib/joda-time-2.2.jar]\n[Loaded org.joda.time.field.AbstractReadableInstantFieldProperty from file:/C:/Work/lucene-solr-svn/branch_4x/solr/core/lib/joda-time-2.2.jar]\n[Loaded org.joda.time.MutableDateTime$Property from file:/C:/Work/lucene-solr-svn/branch_4x/solr/core/lib/joda-time-2.2.jar]\n[Loaded org.joda.time.chrono.GJLocaleSymbols from file:/C:/Work/lucene-solr-svn/branch_4x/solr/core/lib/joda-time-2.2.jar]\nMemdump#from: org.apache.solr.update.processor.ParsingFieldUpdateProcessorsTest => \n\n\n\nIt seems like the problem leading to the permgen is just the huge number of classes being loaded under a single class loader (and these\nclasses cannot be unloaded because they're either cross-referenced or something else is holding on to them).\n\nverifying the class-number hipothesis\n\nIt was interesting to answer the question: how much permgen space would it take to load all these classes without running tests? I wrote\na small utility that parses the output log with class loading information:\n\n...\n[Loaded org.apache.lucene.index.DocTermOrds from file:/C:/Work/lucene-solr-svn/branch_4x/lucene/build/core/classes/java/]\n[Loaded org.apache.lucene.search.FieldCacheImpl$DocTermOrdsCache from file:/C:/Work/lucene-solr-svn/branch_4x/lucene/build/core/classes/java/]\n[Loaded org.apache.lucene.search.FieldCacheImpl$DocsWithFieldCache from file:/C:/Work/lucene-solr-svn/branch_4x/lucene/build/core/classes/java/]\n[Loaded org.apache.lucene.search.FieldCache$2 from file:/C:/Work/lucene-solr-svn/branch_4x/lucene/build/core/classes/java/] \n...\n\n\nand turns it into a custom URLClassLoader with the URLs that appear in those entries. Then the tool attempts to load all the referenced classes (and run initializers) \nbut does not do anything else. It also dumps the permgen state every 100 classes. The results are as follows:\n\n# 10240 classes from 61 sources.\n    0 -           Code Cache - I:    2.4 U:    0.6 M:   48.0\n    0 -        PS Eden Space - I:   62.9 U:   54.1 M: 1319.1\n    0 -    PS Survivor Space - I:   10.4 U:    2.6 M:   10.4\n    0 -           PS Old Gen - I:  167.5 U:    0.0 M: 2680.0\n    0 -          PS Perm Gen - I:   20.8 U:    4.1 M:   82.0\n...\n 1400 -           Code Cache - I:    2.4 U:    0.7 M:   48.0\n 1400 -        PS Eden Space - I:   62.9 U:   18.8 M: 1319.1\n 1400 -    PS Survivor Space - I:   10.4 U:    3.5 M:   10.4\n 1400 -           PS Old Gen - I:  167.5 U:    0.0 M: 2680.0\n 1400 -          PS Perm Gen - I:   20.8 U:   12.0 M:   82.0\n...\n 6200 -           Code Cache - I:    2.4 U:    1.3 M:   48.0\n 6200 -        PS Eden Space - I:   62.9 U:   33.3 M: 1319.1\n 6200 -    PS Survivor Space - I:   10.4 U:   10.4 M:   10.4\n 6200 -           PS Old Gen - I:  167.5 U:   10.7 M: 2680.0\n 6200 -          PS Perm Gen - I:   20.8 U:   45.6 M:   82.0\n...\n10239 -           Code Cache - I:    2.4 U:    1.5 M:   48.0\n10239 -        PS Eden Space - I:   62.9 U:    4.8 M: 1319.1\n10239 -    PS Survivor Space - I:   10.4 U:   10.4 M:   10.4\n10239 -           PS Old Gen - I:  167.5 U:   21.7 M: 2680.0\n10239 -          PS Perm Gen - I:   20.8 U:   71.5 M:   82.0\n\n\n\nwhich, if you forgot already, very nicely matches the result acquired from the real test run (classes plus\ninterned strings):\n\nMemdump#from: org.apache.solr.util.FileUtilsTest => \n          Code Cache - I:    2.4 U:   24.3 M:   48.0\n       PS Eden Space - I:   62.9 U:   39.6 M:  166.5\n   PS Survivor Space - I:   10.4 U:    1.1 M:    2.1\n          PS Old Gen - I:  167.5 U:  173.9 M:  341.4\n         PS Perm Gen - I:   20.8 U:   77.4 M:   82.0\n\n\n\nI repeated the above results with JDK 1.7 (64 bit) and the required permgen space is smaller:\n\n10239 -           Code Cache - I:    2.4 U:    1.3 M:   48.0\n10239 -        PS Eden Space - I:   62.9 U:   97.5 M: 1319.1\n10239 -    PS Survivor Space - I:   10.4 U:   10.4 M:   10.4\n10239 -           PS Old Gen - I:  167.5 U:   27.5 M: 2680.0\n10239 -          PS Perm Gen - I:   20.8 U:   59.9 M:   82.0\n\n\n\nwhich may be a hint why we're seing the problem only on 1.6 \u2013 we're running very close to the limit and 1.6\nis less space-conservative.\n\nI also ran it with jrockit (for fun):\n\n10239 -              Nursery - I:   -0.0 U:   13.0 M: 2918.4\n10239 -            Old Space - I:   64.0 U:   62.0 M: 3072.0\n10239 -         Class Memory - I:    0.5 U:   68.7 M:   -0.0\n10239 -    ClassBlock Memory - I:    0.5 U:    4.0 M:   -0.0\n\n\n\nand with J9:\n\n10239 -        class storage - I:    0.0 U:   41.3 M:   -0.0\n10239 -       JIT code cache - I:    0.0 U:    8.0 M:   -0.0\n10239 -       JIT data cache - I:    0.0 U:    0.3 M:   -0.0\n10239 - miscellaneous non-heap storage - I:    0.0 U:    0.0 M:   -0.0\n10239 -            Java heap - I:    4.0 U:   38.3 M:  512.0\n\n\n\nconclusions\n\nSo it's the number of classes that is the core of the problem. The workarounds in the order of difficulty:\n\n\tincrease max permgen for hotspot (other JVMs should be able to do it dynamically),\n\tsplit solr core tests into multiple ant sub-calls so that they don't run in a single JVM,\n\tchange the runner to support running tests in isolation (for example max-N tests per JVM, then relaunch)\n\tprobably a lot more options here, depending on your current creativity levels \n\n "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13704515",
            "date": "2013-07-10T12:45:34+0000",
            "content": "Full logs are at: http://goo.gl/gzlwk "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13704516",
            "date": "2013-07-10T12:48:26+0000",
            "content": "Thanks Dawid,\n\ngood and bad news. I agree, the best would be to split the tests, but for now the only chance is to raise permgen to 128 MB on Hotspot VMs.\n\nI will prepare a patch that rauses permgen for Solr tests only, have to think how to combine that in a good way also with the clover special case. I will also check if java 8 still alows the permgen parameter or not. J9 and JRockit are fine. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13704519",
            "date": "2013-07-10T12:59:23+0000",
            "content": "Dawid++ "
        },
        {
            "author": "Jack Krupansky",
            "id": "comment-13704534",
            "date": "2013-07-10T13:10:14+0000",
            "content": "Is this still looking like a test-only issue, or might users who use Solr intensively with lots of their own add-on plugins hit this same wall, such that things are fine for them with 4.3, but then 4.4 just stops working? Or, do they have an easy workaround by setting PermGen size? "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13704535",
            "date": "2013-07-10T13:11:59+0000",
            "content": "I was thinking about the same, would the additional classes make users fail after upgrading, although they don't use Hadoop? "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13704537",
            "date": "2013-07-10T13:15:14+0000",
            "content": "The easiest way to check would be to expose/inspect permgen stats. For just the number of classes you can run the VM with:\n\njava ... -XX:+TraceClassLoading | grep \"[Loading\" | wc -l\n\nwhich will give you the number of classes loaded by the VM once it exits. The memory pools can also be checked from within the VM (as in the example above) \u2013 we could actually add an assertion to LuceneTestCase that would monitor the use of permgen and throw an assertion if, say, 90% of the maximum permgen space is used. This would prevent permgen-related process-hung-forever issues, at least it'd be an attempt to detect them early. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13704546",
            "date": "2013-07-10T13:28:17+0000",
            "content": "Thanks Dawid. I owe you some beer for the effort \n\nhit this same wall\n\nIt's not really a wall - perm gen is configurable for a reason (mainly because oracle impl around perm gen sucks - probably why configuring it is going away like with other jvms). 64 or 92 is just an arbitrary size - I don't consider it a big deal if we have to set it to 128.  \n\nBut no, this is not all of a sudden going to start biting you in a non test env because we hit some limit - been running Solr like this with no perm gen bump for months in many different situations and envs. We are adding a lot of classes and what not by running hdfs to test against - which we don't do in production - hdfs runs separately. "
        },
        {
            "author": "David Smiley",
            "id": "comment-13704550",
            "date": "2013-07-10T13:33:01+0000",
            "content": "Thorough investigation Dawid!  Wow. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13704576",
            "date": "2013-07-10T13:46:33+0000",
            "content": "\nWe are adding a lot of classes and what not by running hdfs to test against - which we don't do in production - hdfs runs separately.\n\nHere is a breakdown of the number of classes in all the .jars of the current war. So I'm confused if it isn't needed in production, can we remove from the war?\n\n\nrmuir@beast:~/workspace/lucene-trunk/solr/dist/WEB-INF/lib$ (for file in *.jar; do printf \"$file\\t\" && (unzip -l $file | grep class | wc -l); done) | sort -nrk 2 -\nhadoop-hdfs-2.0.5-alpha.jar\t1731\nguava-14.0.1.jar\t1594\nhadoop-common-2.0.5-alpha.jar\t1392\nlucene-core-5.0-SNAPSHOT.jar\t1336\nsolr-core-5.0-SNAPSHOT.jar\t1252\nlucene-analyzers-common-5.0-SNAPSHOT.jar\t450\nzookeeper-3.4.5.jar\t437\norg.restlet-2.1.1.jar\t398\nhttpclient-4.2.3.jar\t323\nwstx-asl-3.2.7.jar\t251\nlucene-queryparser-5.0-SNAPSHOT.jar\t247\nsolr-solrj-5.0-SNAPSHOT.jar\t235\njoda-time-2.2.jar\t229\nprotobuf-java-2.4.0a.jar\t204\nhttpcore-4.2.2.jar\t190\nlucene-codecs-5.0-SNAPSHOT.jar\t169\ncommons-configuration-1.6.jar\t165\nlucene-queries-5.0-SNAPSHOT.jar\t150\ncommons-lang-2.6.jar\t133\ncommons-io-2.1.jar\t104\ncommons-codec-1.7.jar\t85\nlucene-suggest-5.0-SNAPSHOT.jar\t77\nlucene-highlighter-5.0-SNAPSHOT.jar\t75\nlucene-grouping-5.0-SNAPSHOT.jar\t62\nlucene-spatial-5.0-SNAPSHOT.jar\t59\nlucene-misc-5.0-SNAPSHOT.jar\t55\nlucene-analyzers-kuromoji-5.0-SNAPSHOT.jar\t49\nconcurrentlinkedhashmap-lru-1.2.jar\t44\ncommons-fileupload-1.2.1.jar\t43\nspatial4j-0.3.jar\t41\nhadoop-auth-2.0.5-alpha.jar\t26\ncommons-cli-1.2.jar\t22\nhadoop-annotations-2.0.5-alpha.jar\t17\nhttpmime-4.2.3.jar\t15\nlucene-memory-5.0-SNAPSHOT.jar\t14\nnoggit-0.5.jar\t11\norg.restlet.ext.servlet-2.1.1.jar\t8\nlucene-analyzers-phonetic-5.0-SNAPSHOT.jar\t6\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13704585",
            "date": "2013-07-10T13:56:48+0000",
            "content": "can we remove from the war?\n\nNo - those are the classes with the client code that we use to talk to hdfs.\n\nThe tests add a variety of other dependencies and test jars and actually start up hdfs.\n\nSolr and the webapp simply talk to hdfs. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13704587",
            "date": "2013-07-10T13:59:39+0000",
            "content": "Here is a patch that at least fixes the problem for now. Some notes:\n\n\tJRockit, IBM J9 and Java 8 actually ignore the permgen Java option, so it will not hurt. They just print a warning that it is unused\n\tI moved the -Dargs last, because the JVM command line parsing puts the last one overide previous ones. So you can override with -Dargs\n\n\n\nThe other options should be investigated later, the current patch should make the problems go away now. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13704596",
            "date": "2013-07-10T14:06:26+0000",
            "content": "In general last night I had an idea: We could start the HDFS cluset in a separate JVM for tests that need it! "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13704597",
            "date": "2013-07-10T14:09:34+0000",
            "content": "\nThe tests add a variety of other dependencies and test jars and actually start up hdfs.\n\nBut these account for ~2000 classes whereas the ones in solr.war account for ~3000 classes.\n\nI mean this explains why we see the issue, from a test environment this hadoop stuff nearly doubled the number of classes. But the \"client code\" stuff is heavy too (its still 3000 additional classes). \n\nReally if hadoop integration+tests were in a contrib module, we probably wouldnt even see the problem, or we could contain it, because its tests would run isolated in their own jvm(s). Maybe we should do that?\n\n\nrmuir@beast:~/workspace/lucene-trunk/solr/test-framework/lib$ (for file in *.jar; do printf \"$file\\t\" && (unzip -l $file | grep class | wc -l); done) | sort -nrk 2 -\nant-1.8.2.jar\t1090\njunit4-ant-2.0.10.jar\t1038\nhadoop-common-2.0.5-alpha-tests.jar\t675\nhadoop-hdfs-2.0.5-alpha-tests.jar\t640\ncommons-collections-3.2.1.jar\t458\njersey-core-1.16.jar\t351\njunit-4.10.jar\t252\njetty-6.1.26.jar\t237\nrandomizedtesting-runner-2.0.10.jar\t142\njetty-util-6.1.26.jar\t105\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13704599",
            "date": "2013-07-10T14:11:45+0000",
            "content": "I mean this explains why we see the issue, from a test environment this hadoop stuff nearly doubled the number of classes. But the \"client code\" stuff is heavy too (its still 3000 additional classes).\n\nThat doesn't really matter - what matter is what classes are loaded.\n\nMaybe we should do that?\n\nI discussed in the issue - I don't think it should be a contrib - especially for the reason of a ton of unloaded class files sitting in a jar... "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13704600",
            "date": "2013-07-10T14:15:17+0000",
            "content": "\nThat doesn't really matter - what matter is what classes are loaded.\n\nIt matters to me: some developer accidentally leaves in a debugging statement in some core solr class that references a hadoop class, yet tests pass and everything because we've increased MaxPermSize for all tests and nobody knows, then we release and suddenly all users servers are failing in production.\n\nThats why i'm against just increasing MaxPermSize for all of solr and saying \"well its only for tests and doesnt impact real users\". \n\nBecause nothing will test thats actually the case. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13704605",
            "date": "2013-07-10T14:20:08+0000",
            "content": "Already, perm gen size can vary by jvm, OS, version, etc. This is not a clean battle. And your argument is super general.\n\nIf you want a real test for monitoring perm gen usage, make one, but it's silly to count on variable ceilings in the wild to be high enough to avoid that kind of 'bug' "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13704611",
            "date": "2013-07-10T14:23:53+0000",
            "content": "My argument is that if this stuff is really optional and the classes are intended not to be loaded unless you use it, that we should just separate it out as a module (doesnt have to be under contrib/) and enforce this with the compiler.\n\nLets be honest: whatever special jvm flags are used in solr tests, i'm going to recommend people use the same ones in production. just like recommending to use jetty over tomcat. because thats what its tested with, so i know it should work. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13704613",
            "date": "2013-07-10T14:28:56+0000",
            "content": "My argument is that if this stuff is really optional and the classes are intended not to be loaded unless you use it,\n\nI didn't say it was optional and classes are not intended to be loaded unless you use it - I said that the tests start up hdfs and the client code does not. The idea that someone will 'accidentally' imports a class that does the equivalent class loading of starting up hdfs is absurd (and impossible given the other classes needed to do it are only available in tests). "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13704616",
            "date": "2013-07-10T14:33:47+0000",
            "content": "I dont care what starts up what, I'm talking about raw number of classes:\n\n\t~ 3000 hadoop classes in the .war (linked to solr-core)\n\t~ 2000 hadoop classes from test-framework (used only by tests).\n\n\n\nSo the argument this is a test issue is absurd, given that there are more hadoop classes in non-test code actually. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13704619",
            "date": "2013-07-10T14:36:01+0000",
            "content": "If you won't accept that the raw number of classes is not the issue, there is not much else to talk about with you.\n\nSo the argument this is a test issue is absurd, given that there are more hadoop classes in non-test code actually.\n\nThat's not true, you just don't understand. Running HDFS loads a lot of those classes. Using the client API's load a lot less of those classes. It's pretty simple... "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13704620",
            "date": "2013-07-10T14:38:57+0000",
            "content": "Its not that i dont understand, its that there is nothing to prove that to me: if solr-core tests are only passing with -XXsomearg, then thats the only condition i really know that solr-core works with. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13704643",
            "date": "2013-07-10T15:14:52+0000",
            "content": "Commit 1501789 from Uwe Schindler\n[ https://svn.apache.org/r1501789 ]\n\nMerged revision(s) 1501678 from lucene/dev/trunk:\nSOLR-5022: Make the Maven build also automatically populate the tests.disableHdfs property by a build profile. Otherwise the maven build fails by default on Windows. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13704644",
            "date": "2013-07-10T15:16:02+0000",
            "content": "Commit 1501790 from Uwe Schindler\n[ https://svn.apache.org/r1501790 ]\n\nMerged revision(s) 1501678 from lucene/dev/trunk:\nSOLR-5022: Make the Maven build also automatically populate the tests.disableHdfs property by a build profile. Otherwise the maven build fails by default on Windows. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13704649",
            "date": "2013-07-10T15:24:36+0000",
            "content": "That's not true, you just don't understand. Running HDFS loads a lot of those classes. Using the client API's load a lot less of those classes. It's pretty simple...\n\nCan we put the client classes into a smaller JAR? Is there none available in Maven? Then we would add the student-first-year MiniDFSCluster into a separate JAR and run it only with tests. This would not solve the permgen problem, but would make the Solr WAR smaller. It would also ensure, that no core class accidently starts hadoop (in extra slow mode, just joking).\n\nIdeally, can we start a separate HDFS cluster (empty) in a parallel JVM next to all tests and use that one remotely (as it would be in reality - the cluster would also not run in the same JVM). Means, ant starts an empty HDFS cluster and all tests use it as data store? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13704656",
            "date": "2013-07-10T15:35:23+0000",
            "content": "Can we put the client classes into a smaller JAR?\n\nSimple in theory, hard in practice I think - especially on an ongoing basis.\n\nThen we would add the student-first-year MiniDFSCluster into a separate JAR and run it only with tests. \n\nIt already is in a separate jar - a test jar that is not part of Solr.\n\nbut would make the Solr WAR smaller. \n\nNot that noticeably - these jars are already pretty small from a size perspective.\n\nIt would also ensure, that no core class accidently starts hadoop\n\nI don't think that's a valid concern - you cannot do this easily at all - first, without the dfsminitcluster code from the test jars, good luck to you. Second, without the other test dependencies, hdfs won't start. So this is like saying, someone in the future could write a virus into Solr - we better not run code. The only way you could load all those classes is if you were hell bent on doing it, and even then it would not be easy. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13704658",
            "date": "2013-07-10T15:40:41+0000",
            "content": "OK, Mark. The hadoop-Jars are already small, OK.\n\nMy second idea was to run the MiniDFSCluster (which also needs the Jetty 1.6, right?) as a separate Java Process started before the tests and shut down after the tests. This would also emulate a more-real-world scenario, because in production you would never ever run the storage cluster in the same JVM...\n\nHow about this? Is it worth a try? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13704668",
            "date": "2013-07-10T15:46:24+0000",
            "content": "My second idea was to run the MiniDFSCluster (which also needs the Jetty 1.6, right?) as a separate Java Process started before the tests and shut down after the tests.\n\nYeah, I have not responded yet because I think it's both interesting and scary.\n\nI think it could work, but it does make some things more difficult. You could no longer run tests in your IDE that work against hdfs easily right? Not without starting up a separate hdfs and pointing the test to it.\n\nAnd you end up with other issues to debug that are harder because you have more moving pieces - and logging output is now split....\n\nI'm not totally against it, but I think it has it's own issues.\n\nThis would also emulate a more-real-world scenario, because in production you would never ever run the storage cluster in the same JVM...\n\nYes, we have larger integration tests at cloudera that tests against a real hdfs setup (outside of the unit tests). The mini cluster is what the hadoop tests count on though, so it's a pretty solid way to test when it comes to hadoop. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13704676",
            "date": "2013-07-10T15:56:05+0000",
            "content": "We are here somehow in a deadlock:\n\n\tI have to stop Jenkins from failing all the time. As this seems to not happen soon (Robert does not like the patch raising permgen - same on my side). So I will pass -Dtests.disableHdfs=true to the Policeman Linux and MacOSX jobs - sorry! The other Jenkins servers don't run Hadoop\n\tI would (like Robert) prefer to move the Hadoop Directory and all its dependencies to a separate module. I have no idea why this is so hard, but from my current experience there are approx 10 test classes in a separate package. Just svn mv all this stuff (oas.cloud.hdfs.**) to a new module and we are done? WHERE IS THE PROBLEM IN DOING THIS?\n\tI would not like to have Hadoop by default installed as most users won't use it. If you want a Hadoop-enabled Solr, install the contrib into your instance's lib folder. This is theonly way to solve this issue.\n\tIf you want to reuse a test from core, but let it just run on HDFS, import core's tests into the hadoop module, too and subclass the core tests, giving it a separate configuration and start up the MiniDFSCluster.\n\n\n\nI just repeat: I have no problem with Hadoop at all! It's fine to store an index in Hdfs (maybe), although I would prefer to store the index on local disks with MMAP! But this is purely optional so should be in a separate module! "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13704686",
            "date": "2013-07-10T15:59:48+0000",
            "content": "NOTE: I (temporary) disabled the Hdfs tests on Policeman Jenkins. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13705548",
            "date": "2013-07-11T07:12:51+0000",
            "content": "My second idea was to run the MiniDFSCluster (which also needs the Jetty 1.6, right?) as a separate Java Process started before the tests and shut down after the tests.\n\nThis is when TestNG's BeforeSuite and AfterSuite annotations would be so handy. These are executed before and after all tests so they act like a setup for \"all tests\", regardless of their number. Very handy for setting up a one-time costly things like a web server or other things. JUnit doesn't have this functionality as far as I know. Perhaps it could be patched at the randomizedtesting's runner level if there's interest (as a non-standard JUnit extension). "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13705565",
            "date": "2013-07-11T07:43:57+0000",
            "content": "Updated patch, only applying permgen to Solr Core. This would be a quick hack, although I don't like it (for the reasons explained above).\n\nOnce we moved HDFS to a separate contrib or module, the same logic could be applied there. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13705567",
            "date": "2013-07-11T07:45:50+0000",
            "content": "This is when TestNG's BeforeSuite and AfterSuite annotations would be so handy. These are executed before and after all tests so they act like a setup for \"all tests\", regardless of their number. Very handy for setting up a one-time costly things like a web server or other things. JUnit doesn't have this functionality as far as I know. Perhaps it could be patched at the randomizedtesting's runner level if there's interest (as a non-standard JUnit extension).\n\nCould this be done as a separate attribute to the junit4 task with an java interface suplying beforeSuite and afterSuite? "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13705572",
            "date": "2013-07-11T07:54:14+0000",
            "content": "Yeah... not that I think of it it'd be tricky. You'd want to be able to run isolated classes from IDEs like Eclipse so it'd have to be built-in in the RandomizedRunner, not the ant task.\n\nI think it'd have to be done using method annotations and a common superclass (LuceneTestCase or its equivalent subclass that handles Solr tests). This would ensure you could still run your tests from Eclipse or other IDEs (because the class hierarchy being run would still hold those annotations). Whether or not everything would fall into place is hard to tell (IDEs vary, some trickery would be needed to detect before-after-all-tests moment from within the runner itself).\n "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13717324",
            "date": "2013-07-23T18:47:52+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13971200",
            "date": "2014-04-16T12:57:26+0000",
            "content": "Move issue to Solr 4.9. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14204147",
            "date": "2014-11-09T22:22:39+0000",
            "content": "Commit 1637754 from Uwe Schindler in branch 'dev/trunk'\n[ https://svn.apache.org/r1637754 ]\n\nSOLR-5022: Reenable Hdfs tests on Java 8+, because they have no permgen. Windows is still disabled, because Hadoop needs Cygwin. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14204148",
            "date": "2014-11-09T22:23:26+0000",
            "content": "Commit 1637755 from Uwe Schindler in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1637755 ]\n\nMerged revision(s) 1637754 from lucene/dev/trunk:\nSOLR-5022: Reenable Hdfs tests on Java 8+, because they have no permgen. Windows is still disabled, because Hadoop needs Cygwin. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-14204164",
            "date": "2014-11-09T22:41:58+0000",
            "content": "As Java 7's days are counted, I close this as \"Won't fix\". There is no need to add crazy permgen options anymore. People that want to tests Hdfs, should use Java 8+ or explicitely enable it using -Dtests.disableHdfs=false.\n\nOn Jenkins I enabled Hdfs tests, if Java is on version 8+ "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-14332715",
            "date": "2015-02-23T05:01:30+0000",
            "content": "Bulk close after 5.0 release. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-14640518",
            "date": "2015-07-24T14:44:17+0000",
            "content": "I want to reopen this. Recently because there were more and more libraries added to Solr's core, the tests are hanging (kill -9 required) more often in branch_5x. This is no longer an issue in trunk.\n\nI would propose to apply the provided patch, but only set permgen for Java 7 (Oracle variant) "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-14640522",
            "date": "2015-07-24T14:51:39+0000",
            "content": "See: http://mail-archives.apache.org/mod_mbox/lucene-dev/201507.mbox/%3C05bb01d0c5e3%2442ed9960%24c8c8cc20%24%40thetaphi.de%3E "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-14644455",
            "date": "2015-07-28T14:37:38+0000",
            "content": "This build failed on Windows: http://jenkins.thetaphi.de/job/Lucene-Solr-5.x-Windows/4951/\n\nThis generally happens when the following is true:\n\n\tJava 7 on branch_5x\n\tSome Solr test fails (and does not clean up after itsself completely)\n\tAnother expensive test starts afterwards in the same JVM. This one gets a hidden permgen error and then hangs.\n\n\n\nI would like to raise permgen in the 5.x branch, if the JRE version is exactly JDK 7 (Oracle/OpenJDK) as a temporary workaround. We should do this really before 5.3, as the smoke tester also sometimes hangs! "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-14649005",
            "date": "2015-07-31T10:07:09+0000",
            "content": "Patch for branch_5x.\n\nI will commit this now. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14649009",
            "date": "2015-07-31T10:09:30+0000",
            "content": "Commit 1693559 from Uwe Schindler in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1693559 ]\n\nSOLR-5022: On Java 7 raise permgen for running tests "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14649018",
            "date": "2015-07-31T10:16:09+0000",
            "content": "Commit 1693560 from Uwe Schindler in branch 'dev/trunk'\n[ https://svn.apache.org/r1693560 ]\n\nSOLR-5022: Merged revision(s) 1693559 from lucene/dev/branches/branch_5x: cleanup outdated Java 7 stuff "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14649816",
            "date": "2015-07-31T20:46:11+0000",
            "content": "Commit 1693649 from Uwe Schindler in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1693649 ]\n\nSOLR-5022: Limit permgen settings to Hotspot/OpenJDK "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14713225",
            "date": "2015-08-26T13:06:08+0000",
            "content": "Bulk close for 5.3.0 release "
        }
    ]
}