{
    "id": "LUCENE-6682",
    "title": "StandardTokenizer performance bug: buffer is unnecessarily copied when maxTokenLength doesn't change",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [
            "5.3",
            "6.0"
        ],
        "priority": "Major",
        "status": "Closed",
        "type": "Bug"
    },
    "description": "From Piotr Idzikowski on java-user mailing list http://markmail.org/message/af26kr7fermt2tfh:\n\n\nI am developing own analyzer based on StandardAnalyzer.\nI realized that tokenizer.setMaxTokenLength is called many times.\n\n\nprotected TokenStreamComponents createComponents(final String fieldName,\nfinal Reader reader) {\n    final StandardTokenizer src = new StandardTokenizer(getVersion(),\nreader);\n    src.setMaxTokenLength(maxTokenLength);\n    TokenStream tok = new StandardFilter(getVersion(), src);\n    tok = new LowerCaseFilter(getVersion(), tok);\n    tok = new StopFilter(getVersion(), tok, stopwords);\n    return new TokenStreamComponents(src, tok) {\n      @Override\n      protected void setReader(final Reader reader) throws IOException {\n        src.setMaxTokenLength(StandardAnalyzer.this.maxTokenLength);\n        super.setReader(reader);\n      }\n    };\n  }\n\n\n\nDoes it make sense if length stays the same? I see it finally calls this\none( in StandardTokenizerImpl ):\n\n\npublic final void setBufferSize(int numChars) {\n     ZZ_BUFFERSIZE = numChars;\n     char[] newZzBuffer = new char[ZZ_BUFFERSIZE];\n     System.arraycopy(zzBuffer, 0, newZzBuffer, 0,\nMath.min(zzBuffer.length, ZZ_BUFFERSIZE));\n     zzBuffer = newZzBuffer;\n   }\n\n\n\nSo it just copies old array content into the new one.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14631414",
            "author": "Steve Rowe",
            "date": "2015-07-17T14:39:14+0000",
            "content": "In setMaxTokenLength() we should call setBufferSize() only if the length has changed.\n\nAlso, setMaxTokenLength() maxes out the buffer size at 1M chars (i.e. UTF-16 code units), but getMaxTokenLength() will lie and report the requested length, even though it exceeded 1M chars, and the buffer length, maxed out at 1M chars, is what really controls max token length.  Not cool.  We should instead throw an exception when the requested maxTokenLength exceeds 1M.\n\nHere's a patch that fixes both:\n\n\nIndex: lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java\n===================================================================\n--- lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java\t(revision 1691570)\n+++ lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java\t(working copy)\n@@ -88,6 +88,8 @@\n     \"<HANGUL>\"\n   };\n   \n+  public static final int MAX_TOKEN_LENGTH_LIMIT = 1024 * 1024;\n+  \n   private int skippedPositions;\n \n   private int maxTokenLength = StandardAnalyzer.DEFAULT_MAX_TOKEN_LENGTH;\n@@ -97,9 +99,13 @@\n   public void setMaxTokenLength(int length) {\n     if (length < 1) {\n       throw new IllegalArgumentException(\"maxTokenLength must be greater than zero\");\n+    } else if (length > MAX_TOKEN_LENGTH_LIMIT) {\n+      throw new IllegalArgumentException(\"maxTokenLength may not exceed \" + MAX_TOKEN_LENGTH_LIMIT);\n     }\n-    this.maxTokenLength = length;\n-    scanner.setBufferSize(Math.min(length, 1024 * 1024)); // limit buffer size to 1M chars\n+    if (length != maxTokenLength) {\n+      maxTokenLength = length;\n+      scanner.setBufferSize(length);\n+    }\n   }\n \n   /** @see #setMaxTokenLength */\n\n "
        },
        {
            "id": "comment-14631428",
            "author": "Robert Muir",
            "date": "2015-07-17T14:52:55+0000",
            "content": "+1 "
        },
        {
            "id": "comment-14631521",
            "author": "ASF subversion and git services",
            "date": "2015-07-17T16:16:37+0000",
            "content": "Commit 1691604 from Steve Rowe in branch 'dev/trunk'\n[ https://svn.apache.org/r1691604 ]\n\nLUCENE-6682: StandardTokenizer performance bug: scanner buffer is unnecessarily copied when maxTokenLength doesn't change.  Also stop silently maxing out buffer size (and effectively also max token length) at 1M chars, but instead throw an exception from setMaxTokenLength() when the given length is greater than 1M chars. "
        },
        {
            "id": "comment-14631528",
            "author": "ASF subversion and git services",
            "date": "2015-07-17T16:20:27+0000",
            "content": "Commit 1691605 from Steve Rowe in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1691605 ]\n\nLUCENE-6682: StandardTokenizer performance bug: scanner buffer is unnecessarily copied when maxTokenLength doesn't change.  Also stop silently maxing out buffer size (and effectively also max token length) at 1M chars, but instead throw an exception from setMaxTokenLength() when the given length is greater than 1M chars. (merged trunk r1691604) "
        },
        {
            "id": "comment-14631531",
            "author": "Steve Rowe",
            "date": "2015-07-17T16:21:30+0000",
            "content": "Committed to trunk and branch_5x.\n\nThanks for reporting, Piotr! "
        },
        {
            "id": "comment-14713134",
            "author": "Shalin Shekhar Mangar",
            "date": "2015-08-26T13:05:52+0000",
            "content": "Bulk close for 5.3.0 release "
        }
    ]
}