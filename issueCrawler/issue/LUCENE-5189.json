{
    "id": "LUCENE-5189",
    "title": "Numeric DocValues Updates",
    "details": {
        "components": [
            "core/index"
        ],
        "fix_versions": [
            "4.6",
            "6.0"
        ],
        "affect_versions": "None",
        "priority": "Major",
        "labels": "",
        "type": "New Feature",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "In LUCENE-4258 we started to work on incremental field updates, however the amount of changes are immense and hard to follow/consume. The reason is that we targeted postings, stored fields, DV etc., all from the get go.\n\nI'd like to start afresh here, with numeric-dv-field updates only. There are a couple of reasons to that:\n\n\n\tNumericDV fields should be easier to update, if e.g. we write all the values of all the documents in a segment for the updated field (similar to how livedocs work, and previously norms).\n\n\n\n\n\tIt's a fairly contained issue, attempting to handle just one data type to update, yet requires many changes to core code which will also be useful for updating other data types.\n\n\n\n\n\tIt has value in and on itself, and we don't need to allow updating all the data types in Lucene at once ... we can do that gradually.\n\n\n\nI have some working patch already which I'll upload next, explaining the changes.",
    "attachments": {
        "LUCENE-5189-segdv.patch": "https://issues.apache.org/jira/secure/attachment/12610100/LUCENE-5189-segdv.patch",
        "LUCENE-5189-no-lost-updates.patch": "https://issues.apache.org/jira/secure/attachment/12608138/LUCENE-5189-no-lost-updates.patch",
        "LUCENE-5189.patch": "https://issues.apache.org/jira/secure/attachment/12600398/LUCENE-5189.patch",
        "LUCENE-5189-4x.patch": "https://issues.apache.org/jira/secure/attachment/12605865/LUCENE-5189-4x.patch",
        "LUCENE-5189-updates-order.patch": "https://issues.apache.org/jira/secure/attachment/12606523/LUCENE-5189-updates-order.patch",
        "LUCENE-5189_process_events.patch": "https://issues.apache.org/jira/secure/attachment/12606069/LUCENE-5189_process_events.patch",
        "LUCENE-5189-renames.patch": "https://issues.apache.org/jira/secure/attachment/12611563/LUCENE-5189-renames.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2013-08-28T13:49:27+0000",
            "content": "Patch adds numeric-dv field updates capabilities:\n\n\n\tIndexWriter.updateNumericDocValue(term, field, value) updates the value of 'field' of all documents associated with 'term' to the new 'value'\n\n\n\n\n\tWhen you update the value of field 'f' of few documents, a new pair of .dvd/.dvm files are created, with the values of all documents for that field.\n\t\n\t\tThat way you can end up with e.g. _0_Lucene45_0_1.dvd and *.dvm for field 'f' and the _0.cfs for other fields which were not updated.\n\t\tSegmentInfoPerCommit tracks for each field in which 'gen' it's recorded, and SegmentCoreReaders uses that map to read the values of the field from the respective gen.\n\t\n\t\n\n\n\n\n\tTestNumericDocValuesUpdates contains a dozen or so testcases which cover different angles, from simple updates, to unsetting values, merging segments, deletes etc. During development I ran into many interesting scenarios .\n\n\n\n\n\tReaderAndLiveDocs.writeLiveDocs applies in addition to the deletes, the field updates too. BufferedDeletes tracks the updates, similar to how it tracks deletes.\n\n\n\n\n\tSegmentCoreReaders no longer has a single DVConsumer it uses, but rather per field it uses the appropriate DVConsumer (depends on the 'gen').\n\t\n\t\tI put a nocommit to remove DVConsumers from SegCoreReaders into a RefCount'd object in SegmentReader so that we can keep SegCoreReaders manage the 'readers' that are shared between all SegReaders, and also make sure to reuse same DVConsumer by multiple SegReaders. I'll do that later.\n\t\n\t\n\n\n\n\n\tSegment merging is supported in that when a segment with updates is merged, the correct values are written to the merged segment and the resulting segment has no 'gen' .dvd.\n\n\n\n\n\tI put a nocommit in DVFormat.fieldsConsumer/Producer by adding another variant which takes fieldInfosGen. The default impl throws UnsupportedOpException, while Lucene45 implements it.\n\t\n\t\tI want to have only one variant of that method, thereby breaking the API. This is important IMO cause we need to ensure that whatever custom DVFormats out there pay attention to the new fieldInfosGen parameter, or otherwise they might overwrite previously created files.\n\t\tThere is also a nocommit touching that with a suggestion to forbid createOutput call in TrackingDir if the file is already referenced by an IndexCommit.\n\t\tIt is important that we break something here so that users/apps pay attention to the new feature \u2013 suggestions are welcome!\n\t\n\t\n\n\n\nFew remarks:\n\n\n\tFor now, only updating by a single term is supported (simplicity).\n\tYou cannot add a new field through field update, only update existing fields. This is a 'schema' change, and there are other way to do it, e.g. through addIndexes and FilterAtomicReader. Attempting to support it means that we need to created gen'd FieldInfosFormat also, which complicates matters.\n\tI dropped some nocommits about renaming classes/methods. I didn't want to do it yet, cause it creates an unnecessarily bloated patch. Feel free to comment, we can take care of the renames later.\n\tI will probably create a branch for that feature cause there are some things that need to be take care of (add some tests, finish Codecs support etc.)\n\tAlso, I haven't yet benchmarked the effect of field updates on indexing/search ... I will get to it at some point, but if someone wants to help, I promise not to say no .\n\n\n\nI may have forgot to describe some changes, feel free to ask for clarification! ",
            "author": "Shai Erera",
            "id": "comment-13752402"
        },
        {
            "date": "2013-08-28T14:00:21+0000",
            "content": "I forgot to mention \u2013 this work started from a simple patch Rob sent me few weeks, so thanks Rob! And also, thanks Mike for helping me get around Lucene core code! At times I felt like I'm literally hammering through the code to get the thing working . ",
            "author": "Shai Erera",
            "id": "comment-13752409"
        },
        {
            "date": "2013-08-28T14:07:39+0000",
            "content": "\nThat way you can end up with e.g. _0_Lucene45_0_1.dvd and *.dvm for field 'f'\n...\nI put a nocommit in DVFormat.fieldsConsumer/Producer by adding another variant which takes fieldInfosGen.\n...\nI want to have only one variant of that method, thereby breaking the API. This is important IMO cause we need to ensure that whatever custom DVFormats out there pay attention to the new fieldInfosGen parameter, or otherwise they might overwrite previously created files.\n\nMaybe we shouldnt pass this parameter to the codec at all. Instead IndexWriter can just put this into the segment suffix and the codec can be blissfully unaware? \n\n\nSegmentCoreReaders no longer has a single DVConsumer it uses, but rather per field it uses the appropriate DVConsumer (depends on the 'gen').\n\n    I put a nocommit to remove DVConsumers from SegCoreReaders into a RefCount'd object in SegmentReader so that we can keep SegCoreReaders manage the 'readers' that are shared between all SegReaders, and also make sure to reuse same DVConsumer by multiple SegReaders. I'll do that later.\n\nI hope we can do this in a cleaner way than 3.x did it for setNorm, that was really crazy \n\n\nI put a nocommit in DVFormat.fieldsConsumer/Producer by adding another variant which takes fieldInfosGen.\n\nCan we refer to this consistently as docValuesGen or something else (I see the patch already does this in some places, but other places its called fieldInfosGen). I dont think this should ever be referred to as fieldInfosGen because, its not a generation for the fieldinfos data, and that would be horribly scary! ",
            "author": "Robert Muir",
            "id": "comment-13752419"
        },
        {
            "date": "2013-08-28T14:20:20+0000",
            "content": "Can we refer to this consistently as docValuesGen\n\nYes, I think that makes sense. At some point I supported this by gen'ing FieldInfos hence the name, but things have changed since. I'll rename.\n\nMaybe we shouldnt pass this parameter to the codec at all. Instead IndexWriter can just put this into the segment suffix and the codec can be blissfully unaware? \n\nMaybe for writing it can work, but the producer needs to know from which Directory to read the file, e.g. if it's CFS, the gen'd files are written outside. I have this code in Lucene45DVProducer:\n\n\n    final Directory dir;\n    if (fieldInfosGen != -1) {\n      dir = state.segmentInfo.dir; // gen'd files are written outside CFS, so use SegInfo directory\n    } else {\n      dir = state.directory;\n    }\n\n\n\nI think that if we want to mask that away from the Codec entirely, we should somehow tell the Codec the segmentSuffix and the Directory from which to read the file. Would another Directory parameter be confusing (since we also have it in SegReadState)?\n\nI hope we can do this in a cleaner way than 3.x did it for setNorm, that was really crazy\n\nWell ... I don't really know how setNorm worked in 3.x, so I'll do what I think and you tell me if it's crazy or not?  ",
            "author": "Shai Erera",
            "id": "comment-13752424"
        },
        {
            "date": "2013-08-28T14:32:49+0000",
            "content": "\nMaybe for writing it can work, but the producer needs to know from which Directory to read the file, e.g. if it's CFS, the gen'd files are written outside. \n\nWait... we shouldnt design around this bug though (and it is an api bug). this problem you point out is definitely existing bogusness: I think we should fix this instead so a codec gets a single \"directory\" and doesnt need to know or care what its impl is, and whether its got TrackingDirectoryWrapper or CFS or whatever around it. ",
            "author": "Robert Muir",
            "id": "comment-13752436"
        },
        {
            "date": "2013-08-28T14:33:27+0000",
            "content": "by the way if we want to fix the state.directory vs state.segmentInfo.dir i would love to help with this, its bothered me forever. ",
            "author": "Robert Muir",
            "id": "comment-13752438"
        },
        {
            "date": "2013-08-28T14:38:56+0000",
            "content": "Rename fieldInfosGen to docValuesGen. I think I got all places fixed. ",
            "author": "Shai Erera",
            "id": "comment-13752444"
        },
        {
            "date": "2013-08-28T14:49:06+0000",
            "content": "The problem is that there are two Directories and the logic of where the file is read from depends if it's gen'd or not (so far it has been only livedocs). Maybe what we can do is have CFS revert to directory.openInput if file does not exist? We can do that in a separate issue? If we fix that, then I think we might really be able to \"hide\" the gen from the Codec cleanly. Actually, if the fix is that simple (CFD.openInput reverting to dir.openInput), I can do it as part of this issue, it's small enough? ",
            "author": "Shai Erera",
            "id": "comment-13752456"
        },
        {
            "date": "2013-08-28T14:56:14+0000",
            "content": "I think its true we can tackle this in a separate issue: but I think i'd rather have SR/SCR just pass the correct directory always in the segmentreadstate/segmentwritestate to the different codec components (e.g. segmentreadstate.dir is always the 'correct' directory the codec component should use, and even when CFS is enabled, livedocsformat always gets the inner one and so on).\n\nIts ok if we want to have the 'inner dir' accessible in SegmentInfo for SR/SCR to do this: like we could make it package private and everything would then just work?\n\nThis would greatly reduce the possibility of mistakes. I think having CFS fall back on its inner directory on FileNotFoundException is less desirable. ",
            "author": "Robert Muir",
            "id": "comment-13752462"
        },
        {
            "date": "2013-08-28T18:05:01+0000",
            "content": "Patch addresses Rob's idea:\n\n\n\tReaderAndLiveDocs and SegCoreReaders set segmentSuffix to docValuesGen and also set SegReadState.directory accordingly (CFS or si.info.dir if dvGen != -1).\n\n\n\n\n\tAll the changes to DVFormat were removed (including the 45Producer/Consumer). I had to fix a bug in PerFieldDVF which ignore state.segmentSuffix (and also resolved a TODO on the way, since it now respects it).\n\n\n\n\n\tRemoved the nocommit in ReaderAndLiveDocs regarding letting TrackingDirWrapper forbid createOutput on a file which is referenced by a commit, since now Codecs are not aware of dvGen at all. As long as they don't ignore segmentSuffix (which they better, otherwise they're broken), they can be upgraded safely to support DVUpdate. We can still do that though under a separate issue, as another safety mechanism.\n\n\n\nI wanted to get rid of the nocommit in TestNumericDocValuesUpdates which sets the default Codec to Lucene45 since now presumably all Codecs should support dv-update. But when the test runs with Lucene40 (I haven't tried other codecs, it's the first one that failed), I hit an exception as if trying to write to the same CFS file. Looking at Lucene40DVF.fieldsProducer, I see that it defaults to CFS extension and also Lucene40DVWriter uses hard-coded segmentSuffix=\"dv\" and ignore state.segmentSuffix. I guess that the actual Codec that was used is Lucene40RWDocValuesFormat, otherwise fieldsProducer should have hit an exception. I didn't know our tests pick \"old\" codecs at random too . How can I avoid picking the \"old\" Codecs (40, 42)? I still want to test other codecs, such as Asserting, maybe MemoryDVF (if it's chosen at random). ",
            "author": "Shai Erera",
            "id": "comment-13752642"
        },
        {
            "date": "2013-08-28T22:51:54+0000",
            "content": "You can add @SuppressCodecs({\"Lucene40\", \"SomethingElse\", ...}) annotation to the top of your test for this. ",
            "author": "Robert Muir",
            "id": "comment-13752977"
        },
        {
            "date": "2013-08-29T05:17:37+0000",
            "content": "Thanks Rob. I forgot about SuppressCodecs . I guess I was confused by why Lucene40 was picked in the first place, as I thought we don't test writing indexes with old Codecs? ",
            "author": "Shai Erera",
            "id": "comment-13753312"
        },
        {
            "date": "2013-08-29T07:06:14+0000",
            "content": "In the case of old codecs: what we do is pretty tricky for testing:\n\n\n\twe make them read-only officially for the user (so that new segments are written in the latest format, but old segments can still be read).\n\tthis has the additional caveat they are not \"purely\" read-only, because actually we allow liveDocs updates (deletes) against the old formats. so they are \"mostly read-only\".\n\ttests have read-write versions (like in branch4x: PreFlexRWCodec). These allow in tests for us to override the read-only-ness, and write like the old formats did and read them in transparently in tests.\n\tOf course they cannot support the newest features with this \"impersonator\" testing we do, but in general we get a lot more test coverage than if we relied solely upon TestBackwardsCompatibility.\n\n ",
            "author": "Robert Muir",
            "id": "comment-13753391"
        },
        {
            "date": "2013-08-29T09:37:41+0000",
            "content": "Patch adds some nocommits and tests that expose some problems:\n\nProblem 1\nIf you run the test with -Dtests.method=testSegmentMerges -Dtests.seed=7651E2AEEBC55BDF, you'll hit an exception:\n\n\nNOTE: reproduce with: ant test  -Dtestcase=TestNumericDocValuesUpdates -Dtests.method=testSegmentMerges -Dtests.seed=7651E2AEEBC55BDF -Dtests.locale=en_AU -Dtests.timezone=Etc/GMT+11 -Dtests.file.encoding=UTF-8\nAug 29, 2013 11:57:35 AM com.carrotsearch.randomizedtesting.ThreadLeakControl checkThreadLeaks\nWARNING: Will linger awaiting termination of 1 leaked thread(s).\nAug 29, 2013 11:57:35 AM com.carrotsearch.randomizedtesting.RandomizedRunner$QueueUncaughtExceptionsHandler uncaughtException\nWARNING: Uncaught exception in thread: Thread[Lucene Merge Thread #0,6,TGRP-TestNumericDocValuesUpdates]\norg.apache.lucene.index.MergePolicy$MergeException: java.lang.AssertionError: formatName=Lucene45 prevValue=Memory\n\tat __randomizedtesting.SeedInfo.seed([7651E2AEEBC55BDF]:0)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:545)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:518)\nCaused by: java.lang.AssertionError: formatName=Lucene45 prevValue=Memory\n\tat org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat$FieldsWriter.getInstance(PerFieldDocValuesFormat.java:133)\n\tat org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat$FieldsWriter.addNumericField(PerFieldDocValuesFormat.java:105)\n\tat org.apache.lucene.index.ReadersAndLiveDocs.writeLiveDocs(ReadersAndLiveDocs.java:389)\n\tat org.apache.lucene.index.ReadersAndLiveDocs.getReader(ReadersAndLiveDocs.java:178)\n\tat org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3732)\n\tat org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3401)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)\n\tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)\n\n\n\nWhat happens is the test uses RandomCodec and picks MemoryDVF for writing that field. Later, when ReaderAndLiveDocs applies updates to that field, it uses SI.codec, which is not RandomCodec anymore, but Lucene45Codec (or in this case Facet45Codec - based on Codec.forName(\"Lucene45\")), and its DVF returns for that field Lucene45DVF, because Lucene45Codec always returns that. The way it works during search is that PerFieldDVF.FieldsReader does not rely on the Codec at all, but rather looks up an attribute in FieldInfo which tells it the DVFormat.name and then it calls DVF.forName. But for writing, it relies on the Codec.\n\nI am not sure how to resolve this. I don't think ReaderAndLiveDocs is doing anything wrong \u2013 per-field is not exposed on Codec API, therefore it shouldn't assume it should do any per-field stuff. But on the other hand, Lucene45Codec instances return per-field DVF based on what the instance says, and don't look at the FieldInfo attributes, as PerFieldDVF.FieldsReader does. Any ideas?\n\nProblem 2\nRobert thought of this usecase: if you have a sparse DocValue field 'f', such that say in segment 1 only doc1 has a value, but in segment 2 none of the documents have values, you cannot really update documents in segment 2, because the FieldInfos for that segment won't list the field as having DocValues at all. For now, I catch that case in ReaderAndLiveDocs and throw an exception. The workaround is to make sure you always have values for a field in a segment, by e.g. always setting some default value. But this is ugly and exposes internal stuff (e.g. segments) to users. Also, it's bad because e.g. if segments 1+2 are merged, you suddenly can update documents that were in segment2 before.\n\nA way to solve it is to gen FieldInfos as well. That will allow us to additionally support adding new fields through field updates, though that's optional and we can still choose to forbid it. If we gen FieldInfos though, the changes I've done to SegmentInfos (recording per-field dvGen) need to be reverted. So it's important that we come to a resolution about this in this issue. This is somewhat of a corner case (sparse fields), but I don't like the fact that users can trip on exceptions that depend whether or not the segment was merged...\n\nProblem 3\nFieldInfos.Builder neglect to update globalFieldNumbers.docValuesType map, if it updates a FieldInfo's DocValueType. It's an easy fix, and I added a test to numeric updates. If someone has an idea how to reproduce this outside of numeric updates scope, I'll be happy handle this in a separate issue. The problem is that if you add two fields to a document with same name, one as a posting and one as DV, globalFieldNumbers does not record that field name in its docValuesType map. This map however is currently used by IW.updateNumericDVField and by an assert in FieldInfos.Builder, therefore I wasn's able to reproduce it outside of numeric updates scope. ",
            "author": "Shai Erera",
            "id": "comment-13753465"
        },
        {
            "date": "2013-08-29T10:01:33+0000",
            "content": "Regarding problem 1, I don't know if it's a valid solution, but maybe if we recorded a per-field-format map for each SegInfo, Lucene45Codec could initialize its dvFormat accordingly? This is not generic though .. it's like we need to have a Codec.serialize() method which dumps stuff to SegInfo (or returns a BytesRef/String from which it can later initialize itself). We'd then not need the attributes on FieldInfo. We have to somehow employ the same logic as we do in PerFieldDVF.FieldsReader, in PerFieldDVF.FieldsWriter for updating existing segments. Whatever solution we'll do here, will help us when we come to implement field updates for postings. ",
            "author": "Shai Erera",
            "id": "comment-13753476"
        },
        {
            "date": "2013-08-29T10:09:57+0000",
            "content": "BTW, this may generally not be a bad idea, to let the Codec serialize some stuff which is later given to it in Codec.init(BytesRef). E.g. if a Codec is initialized with some parameters that are also important during search (e.g FacetsCodec can be initialized with FacetIndexingParams, which get lost during search because the Codec is initialized with default ctor), this could be a way for it to serialize/deserialize itself. The name will be used for the newInstance(), the rest to initialize the Codec. ",
            "author": "Shai Erera",
            "id": "comment-13753482"
        },
        {
            "date": "2013-08-29T10:54:36+0000",
            "content": "Regarding problem 3, Mike helped me construct a simple test which reproduces the bug - I opened LUCENE-5192 to fix. ",
            "author": "Shai Erera",
            "id": "comment-13753500"
        },
        {
            "date": "2013-08-29T12:27:42+0000",
            "content": "Regarding problem 1, I hardwired the test to use Lucene45Codec for now so that I'm not blocked. I thought about Codec.serlize/attributes and now I realize it's not a good idea since those attributes must be recorded per-segment, yet the Codec is single-instance for all segments. We can however record these in SegmentInfo.attributes(). The documentation suggests this is where the Codec should record stuff per-segment. Would it work if PerFieldDVF recorded the per-field-format in SegWriteStage.si.attributes() and read them later, instead of FieldInfo.attributes? ",
            "author": "Shai Erera",
            "id": "comment-13753557"
        },
        {
            "date": "2013-08-29T13:05:46+0000",
            "content": "Hi,\nI had an idea yesterday when thinking about this. Currently (like for deletes) we can update DocValues based on an ID term (by docid is not easily possible with IndexWriter). As the ID term can be anything, you could also use some (group) key that updates lots of documents (like you can delete all documents with a specific term). The current code updates the given field for all those documents to a fixed value. My two ideas are:\n\n\n\twe could also support update by query (means like for deletes you provide a query that selects the documents to update)\n\twe could make \"modifications\" possible: Instead of giving a value that is set for all selected documents, we could provide a \"callback\" interface that is used to modify the current docvalue (numeric or String) of the document to update and returns a changed value. This would be a one-method interface, so it could be used as closure in Java 8, like writer.updateDocValues(term, value -> value+1); (in Java 6/7 this would be writer.updateDocValues(term, new NumericDocValuesUpdater() { public long update(long value) { return value+1; }});). Servers like Solr or ElasticSearch could implement this interface/closure using e.g. javascript, so one could execute a docvalues update and pass a javascript function applied to every value. We just have to think about concurency: What happens if 2 threads are updating the same value at the same time - maybe this is already handled by the BufferedDeletesQueue!?\n\n\n\nI just wanted to write this down in this issue, so we could think about allowing to implement this. Of course the current patch is more important to get the whole game running! The updateable by term/query is just one thing which is often requested by users. The typical example is a webapp where you can vote for a document. In that case one would execute the closure value -> value+1. If we implement this so low level, the whole concurreny should be easier than how it is currently impelemented e.g. in Solr or ES. ",
            "author": "Uwe Schindler",
            "id": "comment-13753586"
        },
        {
            "date": "2013-08-29T13:14:17+0000",
            "content": "I definitely want to add update by query, but in a separate issue. And the callback idea is interesting. This callback would need to also get the docid I guess (it's missing in your API example)? ",
            "author": "Shai Erera",
            "id": "comment-13753600"
        },
        {
            "date": "2013-08-29T13:36:58+0000",
            "content": "This callback would need to also get the docid I guess (it's missing in your API example)?\n\nOf course we could add this. Java 8 would also support this cool syntax, something like: writer.updateDocValues(term, (docid, value) -> value+1);\n\nThe Java 8 example here was just syntactic sugar: For all this its only important that it is an interface with only one method that gets as many parameters as needed and returns one value. We automatically get the cool java 8 syntax for users, if we design the callback interface to these guidelines. One common example  is the \"Comparator\" interface in Java. Every Comparator<T> can be written in this cool syntax  ",
            "author": "Uwe Schindler",
            "id": "comment-13753622"
        },
        {
            "date": "2013-09-02T19:51:29+0000",
            "content": "Patch adds testStressMultiThreading and testUpdateOldSegments to ensure updating segments written with older format than \"Lucene45\" is not supported. ",
            "author": "Shai Erera",
            "id": "comment-13756232"
        },
        {
            "date": "2013-09-03T10:32:06+0000",
            "content": "Patch looks great!  I review some of the remaining nocommits:\n\n\n// nocommit no one calls this method, why do we have it? and if we need it, do we need one for docValuesGen too?\npublic void setDelGen(long delGen) {\n\nNuke it!  We only use .advanceNextWriteDelGen (and the patch adds this\nfor DVs too).\n\n\n// nocommit no one calls this, remove?\nvoid clearDelGen() {\n\nNuke it!\n\nclass ReadersAndLiveDocs { // nocommit (RENAME) to ReaderAndUpdates?\n\n+1 for ReaderAndUpdates\n\n\n    // nocommit why do we do that, vs relying on TrackingDir.getCreatedFiles(),\n    // like we do for updates?\n\nThat's a good question ... I'm not sure.  We in fact already use\nTrackingDirWrapper (in ReadersAndLiveDocs.writeLiveDocs)... so we\ncould in theory record those files in SIPC and remove\nLiveDocsFormat.files().  Maybe make this a TODO though?\n\n\n// nocommit: review!\nfinal static int BYTES_PER_NUMERIC_UPDATE = BYTES_PER_DEL_TERM + 2*RamUsageEstimator.NUM_BYTES_OBJECT_REF + RamUsageEstimator.NUM_BYTES_INT + RamUsageEstimator.NUM_BYTES_LONG;\n\nI think it makes sense to start from BYTES_PER_DEL_TERM, but then\ninstead of mapping to value Integer we map to value\nMap<String,NumericUpdate> whose per-Term RAM usage is something like:\n\n\n  PTR (for LinkedHashMap, since it must link each entry to the next?)\n\n  Map\n    HEADER\n    PTR (to array)\n    3 INT\n    1 FLOAT\n\n  for each occupied Entry<String,NumericUpdate>\n    PTR (from Map's entries array) * 2 (overhead for load factor)\n    HEADER\n    PTR * 2 (key, value)\n\n    String key\n      HEADER\n      INT\n      PTR (to char[])\n      \n      ARRAY_HEADER + 2 * length-of-string (field)\n\n    NumericUpdate value\n      HEADER\n      PTR (to Term; ram already accounted for)\n      PTR (to String; ram already accounted for)\n      PTR (to Long value) + HEADER + 8 (long)    \n      INT\n\n\n\nThe thing is, this is so hairy ... that I think maybe we should\ninstead use RamUsageEstimator to \"calibrate\" this?  Ie, make a\nstandalone test that keeps adding Term + fields into this structure\nand measures the RAM with RUE?  Do this on 32 bit and on 64 bit JVM,\nand then conditionalize the constants.  You'll still need to add in\nbytes according to field/term lengths...\n\n+public class SegmentInfoPerCommit { // nocommit (RENAME) to SegmentCommit?\n\nNot sure about that rename ... since this class is just the \"metadata\"\nabout a commit, not an \"actual\" commit.  Maybe SegmentCommitInfo? ",
            "author": "Michael McCandless",
            "id": "comment-13756494"
        },
        {
            "date": "2013-09-03T10:49:08+0000",
            "content": "Patch adds per-field support. I currently do that by adding a boolean 'isFieldUpdate' to SegWriteState which is set to true only by ReaderAndLiveDocs. PerFieldDVF then peeks into that boolean and if it's true, it reads the format name from FieldInfo.attributes() instead of relying on Codec.getPerFieldDVF(). If we'll eventually gen FieldInfos, there won't be a need for this boolean as PerFieldDVF will get that from FI.dvGen.\n\nSo far all Codecs work. I had to remove an assert from SimpleText which tested that all fields read from the file are in the state.fieldInfos. But it doesn't use that information, only an assert. And SegCoreReader now passes to each DVProducer only the fields it needs to read.\n\nAdded some tests too. ",
            "author": "Shai Erera",
            "id": "comment-13756501"
        },
        {
            "date": "2013-09-03T14:20:39+0000",
            "content": "Thanks for the review Mike. I nuked the two unused methodsand I like SegmentCommitInfo, so changed the nocommit text.\n\nI changed the nocomit in SIPC to a TODO. Don't think we need to tackle it in this issue.\n\nI'm working on improving the RAM accounting. I want to add to NumericUpdate.sizeInBytes() and count it per-update that is actually added. Then, because it's a Map<Term,Map<String,NumericUpdate>> and the Term and String are both in NumericUdpate already (and will be accounted for in its calculation, only their PTR needs to be taken into account. Also, the sizeInBytes should grow by new entry to outer map only when one is actually added, same for inner map. Therefore I don't think we can have a single constant here, but instead maybe two: for every Entry<Term,Map> added to the outer map and every Entry<String,NumericUpdate> added to the inner map. I think, because I need to compute the shallow sizes only (since Term and String are accounted for in NumericUpdate), it's a single constant for Entry<Object,Object>? ",
            "author": "Shai Erera",
            "id": "comment-13756650"
        },
        {
            "date": "2013-09-03T14:52:48+0000",
            "content": "Patch improves RAM accounting in BufferedDeletes and FrozenBD. I added NumericUpdate.sizeInBytes() so most of the logic is done there. BD adds two constants - for adding to outer Map (includes inner map OBJ_HEADER) and adding an actual update to inner map (excludes map's OBJ_HEADER). Only the pointers are taken into account. ",
            "author": "Shai Erera",
            "id": "comment-13756673"
        },
        {
            "date": "2013-09-03T14:58:00+0000",
            "content": "\nPatch adds per-field support. I currently do that by adding a boolean 'isFieldUpdate' to SegWriteState which is set to true only by ReaderAndLiveDocs. PerFieldDVF then peeks into that boolean and if it's true, it reads the format name from FieldInfo.attributes() instead of relying on Codec.getPerFieldDVF(). If we'll eventually gen FieldInfos, there won't be a need for this boolean as PerFieldDVF will get that from FI.dvGen.\n\nWe can't move forward really with this boolean: it only attacks the symptom (puts a HACK in per-field) without fixing the disease (the codec API).\n\nIn general if a codec needs to write to and read from FieldInfos/SegmentInfos.attributes, it doesnt work here: this api needs to be fixed. ",
            "author": "Robert Muir",
            "id": "comment-13756678"
        },
        {
            "date": "2013-09-03T18:21:32+0000",
            "content": "I don't understand the problem that you raise. Until then, I think that SWS.isFieldUpdate is fine. It works, it's simple, and most importantly, it allows me to move forward. Let's discuss how to improve it even further, but I don't think this is a blocker. We can always improve that later on. ",
            "author": "Shai Erera",
            "id": "comment-13756863"
        },
        {
            "date": "2013-09-03T18:33:40+0000",
            "content": "It really doesn't work: its definitely a blocker for me!\n\nThis leaves the general api (FieldInfo.attributes and SegmentInfo.attributes) broken for codecs, and only hacks a specific implementation that uses them.\n\nWith or without the current boolean, if a numeric docvalues impl puts something in FieldInfo.attributes during an update, it will go into a black hole, because FieldInfos is write-once per-segment (and not per-commit). Same goes with SegmentInfo.attributes. ",
            "author": "Robert Muir",
            "id": "comment-13756884"
        },
        {
            "date": "2013-09-03T18:44:25+0000",
            "content": "By the way: the \"general\" issue is that for updates, its unfortunately not enough to concern ourselves with data, we have to worry about metadata too:\n\nI see at least 4 problems (and i have not thought about it completely):\n\n\tFieldInfo.attributes: these \"writes\" by the NumericDocValues impl will be completely discarded during update, because its per-segment, not per-commit.\n\tSegmentInfo.attributes: same as the above\n\tField doesnt exist in FieldInfo at all: (because the segment the update applies to happens to have no values for the field)\n\tField exists in FieldInfo, but is incomplete: (because the segment the update applies to, had say a stored-only or stored+indexed value for the field, but no dv one).\n\n\n\nPerFieldDVF is just one implementation that happens to use #1. Fixing it is fixing the symptom, thats why I say we really need to instead fix the disease, or things will get very ugly.\n\nThe only reasons you dont see more problems with #1 and #2, is that currently its not used very much (only by PerField and back-compat). If we had more codecs exercising the APIs, you would be seeing these problems already.\n\nA perfectly good solution would be to remove these APIs completely for public use (which would solve #1 and #2). PerField(PF/DVF) could write its own .per file instead. Back compat cruft could then use these now-internal-only-APIs (and it wont matter since they dont support updates), or we could implement their hacks in another way.\n\nBut this still leaves issues like #3 and #4.\n\nAdding a boolean 'isFieldUpdate' doesn't really solve anything, and it totally breaks the whole concept of the codec being unaware of updates.\n\nIt is the wrong direction. ",
            "author": "Robert Muir",
            "id": "comment-13756896"
        },
        {
            "date": "2013-09-03T19:05:07+0000",
            "content": "OK, so now I get your point. The problem is that we pass to Codec FI.attributes with say an attribute 'foo=bar'. The Codec, unaware that this is an update, looks at the given numericFields and decides to encode them using method \"bar2\", so it encodes into the attributes 'foo=bar2', but those attributes get lost because they're not rewritten to FIS. Do I understand correctly?\n\nOf course, we could say that since the Codec has to peek into SWS.isFieldUpdate, thereby making it updates-aware, it should not encode stuff in a different format, but SWS.isFieldUpdate is not enough to enforce that.\n\nI don't think that gen'ing FIS solves the problem of obtaining the right DVF in the first place. Sure, after we do that, the Codec can put whatever attributes that it wants, they will be recorded in the new FIS.gen.\n\nBut maybe we can solve these two problems by gen'ing FIS:\n\n\n\tAdd FieldInfo.dvGen. The Codec will receive the FieldInfos with their dvGen bumped up.\n\tCodec can choose to look at FI.dvGen and pull the right DVF e.g. like PerField does.\n\t\n\t\tOr it can choose to completely ignore it, and always write udpates using the new format.\n\t\n\t\n\tCodec is free to record whatever attributes it wants on this FI. Since we gen FIS, they will be recorded and used by the reader.\n\n\n\nWhat do you think? ",
            "author": "Shai Erera",
            "id": "comment-13756931"
        },
        {
            "date": "2013-09-04T11:35:05+0000",
            "content": "We could simply document this as a limitation, today?  Ie, that if it's an update, the DVFormat cannot use the attributes APIs.  This would let us proceed (progress not perfection) and then later, we address it.  Ie, I think the added boolean is a fair compromise.\n\nOr, we can pursue gen'ing FIS on this patch, but this is going to add a lot of trickiness/complexity; I think it'd be better to explore it separately. ",
            "author": "Michael McCandless",
            "id": "comment-13757674"
        },
        {
            "date": "2013-09-04T11:44:50+0000",
            "content": "I think it's important to solve FIS.gen, either on this issue or a separate one, but before 4.5 is out. Because now SegmentInfos records per-field dvGen and if we gen FIS, this will be recorded by a new Lucene45FieldInfosFormat, and SIS will need to record fieldInfosGen. I actually don't mind to do it in this issue. It's work that's needed and affects NDV-updates (e.g. sparse fields which now hit a too late cryptic exception).\n\nBut I also don't mind moving forward with SWS.isFieldUpdate and remove it in a follow on issue ... as long as it's done before 4.5. ",
            "author": "Shai Erera",
            "id": "comment-13757688"
        },
        {
            "date": "2013-09-04T14:21:30+0000",
            "content": "\nBut I also don't mind moving forward with SWS.isFieldUpdate and remove it in a follow on issue ... as long as it's done before 4.5.\n\nI don't think that will be an issue at all.\n\nif we want to iterate and leave the codec APIs broken, I won't object: but simple rule.\n\nTrunk only.\n\nWe can't do this kind of stuff on the stable branch at all: Things that get backported there need to be \"ready to ship\". ",
            "author": "Robert Muir",
            "id": "comment-13757802"
        },
        {
            "date": "2013-09-04T15:23:23+0000",
            "content": "Just so I understand, if we gen FieldInfos, does that solve the brokenness of the Codec APIs (in addition to the other things that it solves)? If not, in what way are they broken, and is this break a new thing that NDV updates cause/expose, or it's a break that exists in general? Can you list the breaks here (because I think that FIS.gen solves all the points you raised above). ",
            "author": "Shai Erera",
            "id": "comment-13757864"
        },
        {
            "date": "2013-09-04T15:33:52+0000",
            "content": "\nThis would let us proceed (progress not perfection) and then later, we address it. Ie, I think the added boolean is a fair compromise.\n\nIts not a fair compromise at all.\n\nTo me, as a search engine library, this is not progress. its going backwards.\nYes: I'm looking at it solely from an API perspective.\nYes: others look at things from only features/performance perspective and do not seem to care about APIs.\n\nBut as a library, the API is all that matters.\n\nSo I just want to make it clear: saying \"progress not perfection\" is not a good excuse for leaving broken APIs about the codebase and shoving in features as fast as possible: its not progress to me so I simply do not see it that way. \n\nFrankly I am tired of hearing this phrase being used in this way, and when I see it in the future, it will encourage me to take a closer inspection of APIs and do pickier reviews. ",
            "author": "Robert Muir",
            "id": "comment-13757875"
        },
        {
            "date": "2013-09-04T15:43:03+0000",
            "content": "\nJust so I understand, if we gen FieldInfos, does that solve the brokenness of the Codec APIs (in addition to the other things that it solves)? If not, in what way are they broken, and is this break a new thing that NDV updates cause/expose, or it's a break that exists in general? Can you list the breaks here (because I think that FIS.gen solves all the points you raised above).\n\nIt does not solve problem #2 (SegmentInfos.attributes). This API should removed, deprecated, made internal-only, or something like that. Another option is to move this stuff into the commit, but that might be overkill: today this stuff is only used as a backwards-compatibility crutch (i think) to read 3.x indexes: so it can possibly be just removed in trunk right now.\n\nGen'ing FieldInfos brings about its own set of questions as far as when/how/if any new fieldinfo information is merged and when/how its visible to the codec API. its very scary but I don't see any alternative at the moment. ",
            "author": "Robert Muir",
            "id": "comment-13757885"
        },
        {
            "date": "2013-09-04T16:25:31+0000",
            "content": "It does not solve problem #2 (SegmentInfos.attributes)\n\nCorrect. So this API is broken today for LiveDocsFormat (since it's the only updateable thing), but field updates only broaden the broken-ness into other formats (now only DVF, but in the future others too). Correct?\n\nI think that moving this API into the commit is not an overkill. I remember Mike and I once discussed if we can use that API to save per-segment facets \"schema details\". I don't remember how this ended, but maybe we shouldn't remove it? Alternatively, we could gen SIFormat too ... that may be an overkill though. Recording per-segment StringStringMap in SIS seems simple enough.\n\nRegarding FIS.gen, I honestly thought to keep it simple by writing all FIS entirely in each gen and not complicate the code by writing parts of an FI in different gens and merging them by SR. This is what I plan to do in this issue. ",
            "author": "Shai Erera",
            "id": "comment-13757921"
        },
        {
            "date": "2013-09-04T19:21:58+0000",
            "content": "Frankly I am tired of hearing this phrase being used in this way\n\nActually, I think this is a fair use of \"progress not perfection\".\nEither that or I don't understand what you're calling \"broken APIs\" in\nthe current patch.\n\nAs I understand it, what's \"broken\" here is that you cannot set the\nattributes in SegmentInfo nor FieldInfo from your DocValuesFormat\nwriter when it's an update being written: the changes won't be saved.\n\nSo, I proposed that we document this as a limitation of the SI/FI\nattributes API: when writing updates, any changes will be lost.  For\n\"normal\" segment flushes, they work correctly. It'd be a documented\nlimitation, and we can later fix it.\n\nI think this situation is very similar to LUCENE-5197, which I would\nalso call \"progress not perfection\": we are adding a new API\n(SegmentReader.ramBytesUsed), with an initial implementation that we\nthink might be improved by later cutting over to RamUsageEstimator.\nBut I think we should commit the initial approach (it's useful, it\nshould work well) and later improve the implementation. ",
            "author": "Michael McCandless",
            "id": "comment-13758233"
        },
        {
            "date": "2013-09-04T19:34:26+0000",
            "content": "One option, to solve the \"some segments might be missing the field entirely so you cannot update those\" would be to have the FieldInfos accumulate across segments, i.e. a more global FieldInfos, maybe written to a separate global file (not per segment).\n\nThis way, if any doc in any segment has added the field, then the global FieldInfos would contain it.\n\nNot saying this is an appealing option (there are tons of tradeoffs), but I think it would address that limitation. ",
            "author": "Michael McCandless",
            "id": "comment-13758251"
        },
        {
            "date": "2013-09-04T19:41:35+0000",
            "content": "Actually, that would also solve the other problems as well?\n\nIe, the global FieldInfos would be gen'd: on commit we'd write a new FIS file, which all segments in that commit point would use.\n\nAny attribute changes to a FieldInfo would be saved, even on update; new fields could be created via update; any segments that have no documents with the field won't be an issue. ",
            "author": "Michael McCandless",
            "id": "comment-13758257"
        },
        {
            "date": "2013-09-04T20:46:15+0000",
            "content": "I think global FIS is an interesting idea, but per-segment FIS.gen is a lower hanging fruit. I did it once and it was quite straightforward (maybe someone will have reservations on how I did it though):\n\n\n\tSIS tracks fieldInfosGen (in this patch, rename all dvGen in SIS to fisGen)\n\tFI tracks dvGen\n\tA new FIS45Format reads/writes each FI's dvGen\n\tReaderAndLiveDocs writes a new FIS gen, containing the entire FIS, so SR only reads the latest gen to load FIS\n\n\n\nI think we should explore global FIS separately, because it brings its own issues, e.g. do we keep FISFormat or nuke it? Who invokes it (probably SIS)? It's also quite orthogonal to that issue, or at least, we can proceed with it and improve FIS gen'ing later with global FIS.\n\nAs for SI.attributes(), I think we can move them under SIS. We should open an issue to do that. ",
            "author": "Shai Erera",
            "id": "comment-13758311"
        },
        {
            "date": "2013-09-04T20:53:47+0000",
            "content": "The problem that Mike highlights \"some segments might be missing the field entirely so you cannot update those\", is pretty bad though.  Things work differently (i.e. your update may fail) depending on exactly how segment flushes and merges are done. ",
            "author": "Yonik Seeley",
            "id": "comment-13758325"
        },
        {
            "date": "2013-09-04T20:55:51+0000",
            "content": "Correct, that's a problem that Rob identified few days ago and it can be solved if we gen FieldInfos, because ReaderAndLiveDocs will detect that case and add a new FieldInfo, as well as create a new gen for this segment's FIS.I have two tests in TestNumericDVUpdates which currently test that this is not supported \u2013 once we gen FIS, we'll change them to assert it is supported. ",
            "author": "Shai Erera",
            "id": "comment-13758328"
        },
        {
            "date": "2013-09-11T11:07:45+0000",
            "content": "I've been busy hunting down concurrency bugs and making field updates work with index sorting (thought it will be an easy nocommit to handle... boy, I was wrong!). Patch adds field updates to TestSortingMP as well as improves TestNumericDVUpdates.testSegmentMerges to stress that more.\n\nThere are some nocommit (RENAME) which I'd love to get some feedback on. Also there are two standing out nocommits around FieldInfos.gen and an optimization to SegmentCoreReaders/SegmentReader around reusing DVProducers of update gens. I think these two can be handled in separate issues, just to get this issue focused.\n\nI'd like to commit this to trunk, so that I can move on to the other issues. Therefore I'd appreciate some review on the patch. ",
            "author": "Shai Erera",
            "id": "comment-13764209"
        },
        {
            "date": "2013-09-14T20:10:02+0000",
            "content": "Fixed NRT support \u2013 DVProducers moved from SegmentCoreReaders to SegmentReader, where they are being wrapped by RefCount which keeps track of ref counts (new class I added). When an SR is shared, SR inc-refs the DVProducers that it uses (according to SIPC.getDVGen(field)) and dec-ref on doClose().\n\nAll Lucene tests pass, including the new numeric DV updates ones. A review would be appreciated though.\n\nThe remaining nocommits are for renames and FieldInfos.gen. I think I'll leave the renames as TODOs, to handle prior to merging to 4x (after this one bakes in trunk), that way avoiding potential messy merges. I can handle FieldInfos.gen either as part of this issue, or a separate one. Preferences? ",
            "author": "Shai Erera",
            "id": "comment-13767563"
        },
        {
            "date": "2013-09-14T20:53:37+0000",
            "content": "New patch looks great!  The ref counting is very clean.  Maybe add a comment that gen is allowed to be -1, just means the field has no DV updates yet, in SegmentReader when building up the map?  And then call doClose() in a finally clause if !success so on exception we don't leak open files. ",
            "author": "Michael McCandless",
            "id": "comment-13767578"
        },
        {
            "date": "2013-09-14T21:16:54+0000",
            "content": "Thanks Mike. Fixed the two ctors to call doClose if (!success), as well as added a comment about gen=-1.\n\nI also fixed the two tests which ensure we don't allow updating a field in a segment where it doesn't exist (testUpdateSegmentWithPostingButNoDocValues() and testUpdateSegmentWithNoDocValues()) to set NoMergePolicy, otherwise the segments could merge and the update becomes legit. This exposes the weirdness of the exception \u2013 you may not hit it if segments are merged. Once we gen FIS, these tests will change though, to ensure these cases ARE allowed . ",
            "author": "Shai Erera",
            "id": "comment-13767584"
        },
        {
            "date": "2013-09-15T07:23:54+0000",
            "content": "Added some javadocs, converted all nocommits to TODOs. I think it's ready for trunk. I'd like to handle FIS.gen next. ",
            "author": "Shai Erera",
            "id": "comment-13767716"
        },
        {
            "date": "2013-09-15T13:59:18+0000",
            "content": "+1 to go to trunk. thanks Shai. ",
            "author": "Robert Muir",
            "id": "comment-13767799"
        },
        {
            "date": "2013-09-15T17:11:53+0000",
            "content": "Commit 1523461 from Shai Erera in branch 'dev/trunk'\n[ https://svn.apache.org/r1523461 ]\n\nLUCENE-5189: add NumericDocValues updates ",
            "author": "ASF subversion and git services",
            "id": "comment-13767852"
        },
        {
            "date": "2013-09-15T17:14:05+0000",
            "content": "Thanks, committed to trunk, revision 1523461. After we resolve all corner issues, and let Jenkins sleep on it for a while, I'll port to 4x. ",
            "author": "Shai Erera",
            "id": "comment-13767853"
        },
        {
            "date": "2013-09-16T01:56:47+0000",
            "content": "Commit 1523525 from Robert Muir in branch 'dev/trunk'\n[ https://svn.apache.org/r1523525 ]\n\nLUCENE-5189: add testcase ",
            "author": "ASF subversion and git services",
            "id": "comment-13767999"
        },
        {
            "date": "2013-09-16T10:05:54+0000",
            "content": "I only briefly looked at the changed in DW, DWPT, IW & BDS and I have 2 questions:\n\n\n\tSegmentWriteState flushState; in DWPT is unused - can we remove it? (I generally want this class to have only final members as well if possible)\n\tIn DW the `updateNumericDocValue` method is synchronized - I don't think it needs to. The other two deletes methods don't need to be synced either - maybe we can open another issue to remove the synchronization? It won't be possible to just drop it but it won't be much work.\n\n\n\nI really like the way how this is implemented piggybacking on the delete queue to get a total ordering \nnice one! \n\n\n ",
            "author": "Simon Willnauer",
            "id": "comment-13768204"
        },
        {
            "date": "2013-09-16T12:19:11+0000",
            "content": "SegmentWriteState flushState; in DWPT is unused\n\n+1 to remove it. Indeed it's unused, but because it's package-private, eclipse doesn't complain about it.\n\nIn DW the `updateNumericDocValue` method is synchronized\n\nI followed the other two delete methods. I'm fine with opening a separate issue to remove the synchronization, especially if it's not trivial.\n\nI really like the way how this is implemented piggybacking on the delete queue to get a total ordering\n\nThanks, it was very helpful to have deletes already covered like that. I only had to follow their breadcrumbs . ",
            "author": "Shai Erera",
            "id": "comment-13768271"
        },
        {
            "date": "2013-09-16T12:20:31+0000",
            "content": "Jenkins reported this failure, which I'm unable to reproduce with and without the seed (master and child), with iters.\n\n\n1 tests failed.\nREGRESSION:  org.apache.lucene.index.TestNumericDocValuesUpdates.testManyReopensAndFields\n\nError Message:\ninvalid value for doc=351, field=f1 expected:<15> but was:<14>\n\nStack Trace:\njava.lang.AssertionError: invalid value for doc=351, field=f1 expected:<15> but was:<14>\n        at __randomizedtesting.SeedInfo.seed([5E1E0079E35D52E:331D82281FC0B632]:0)\n        at org.junit.Assert.fail(Assert.java:93)\n        at org.junit.Assert.failNotEquals(Assert.java:647)\n        at org.junit.Assert.assertEquals(Assert.java:128)\n        at org.junit.Assert.assertEquals(Assert.java:472)\n        at org.apache.lucene.index.TestNumericDocValuesUpdates.testManyReopensAndFields(TestNumericDocValuesUpdates.java:757)\n        ...\n\nBuild Log:\n[...truncated 776 lines...]\n   [junit4] Suite: org.apache.lucene.index.TestNumericDocValuesUpdates\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestNumericDocValuesUpdates -Dtests.method=testManyReopensAndFields -Dtests.seed=5E1E0079E35D52E -Dtests.multiplier=3 -Dtests.slow=true -Dtests.locale=tr -Dtests.timezone=Etc/GMT-6 -Dtests.file.encoding=US-ASCII\n   [junit4] FAILURE 1.40s J0 | TestNumericDocValuesUpdates.testManyReopensAndFields <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: invalid value for doc=351, field=f1 expected:<15> but was:<14>\n   [junit4]    >        at __randomizedtesting.SeedInfo.seed([5E1E0079E35D52E:331D82281FC0B632]:0)\n   [junit4]    >        at org.apache.lucene.index.TestNumericDocValuesUpdates.testManyReopensAndFields(TestNumericDocValuesUpdates.java:757)\n   [junit4]    >        at java.lang.Thread.run(Thread.java:724)\n   [junit4]   2> NOTE: test params are: codec=Asserting, sim=RandomSimilarityProvider(queryNorm=false,coord=no): {}, locale=tr, timezone=Etc/GMT-6\n   [junit4]   2> NOTE: Linux 3.2.0-53-generic amd64/Oracle Corporation 1.8.0-ea (64-bit)/cpus=8,threads=1,free=66621176,total=210272256\n   [junit4]   2> NOTE: All tests run in this JVM: [TestSegmentReader, TestStressNRT, TestSort, TestShardSearching, TestEliasFanoSequence, TestBytesRefHash, TestPhrasePrefixQuery, TestLucene45DocValuesFormat, TestFastCompressionMode, TestEliasFanoDocIdSet, TestSearchForDuplicates, TestFixedBitSet, TestIsCurrent, TestFilteredSearch, TestFieldCacheSanityChecker, TestSegmentTermEnum, TestDeletionPolicy, TestSimpleExplanations, TestRegexpRandom, TestIndexCommit, TestCloseableThreadLocal, TestNumericRangeQuery32, TestTwoPhaseCommitTool, TestIndexWriterOnDiskFull, TestPhraseQuery, TestSearchAfter, TestParallelReaderEmptyIndex, TestMaxTermFrequency, TestFlushByRamOrCountsPolicy, TestSimilarity, TestNumericRangeQuery64, TestByteSlices, TestSameScoresWithThreads, TestDocValuesWithThreads, TestMockAnalyzer, TestArrayUtil, TestPostingsOffsets, TestCompressingTermVectorsFormat, TestSentinelIntSet, TestCustomNorms, TestExternalCodecs, TestNumericDocValuesUpdates]\n   [junit4] Completed on J0 in 83.46s, 24 tests, 1 failure <<< FAILURES!\n\n ",
            "author": "Shai Erera",
            "id": "comment-13768274"
        },
        {
            "date": "2013-09-20T04:39:54+0000",
            "content": "Commit 1524900 from Shai Erera in branch 'dev/trunk'\n[ https://svn.apache.org/r1524900 ]\n\nLUCENE-5189: fixed concurrency bug ",
            "author": "ASF subversion and git services",
            "id": "comment-13772623"
        },
        {
            "date": "2013-09-20T04:55:25+0000",
            "content": "Commit 1524901 from Shai Erera in branch 'dev/trunk'\n[ https://svn.apache.org/r1524901 ]\n\nLUCENE-5189: remove leftover ",
            "author": "ASF subversion and git services",
            "id": "comment-13772628"
        },
        {
            "date": "2013-09-28T03:13:36+0000",
            "content": "Commit 1527147 from Shai Erera in branch 'dev/trunk'\n[ https://svn.apache.org/r1527147 ]\n\nLUCENE-5189: disable merges in testChangeCodec ",
            "author": "ASF subversion and git services",
            "id": "comment-13780670"
        },
        {
            "date": "2013-09-30T08:16:15+0000",
            "content": "Backported all the changes related to this issue to 4x. All tests pass, I think it's ready. ",
            "author": "Shai Erera",
            "id": "comment-13781670"
        },
        {
            "date": "2013-09-30T09:52:28+0000",
            "content": "I don't think we should backport this to 4.x yet. I'd want this to bake in further and maybe optimize the memory useage of the internal datastructures as well before this goes into the stable branch. This is a significant change and we should be careful with backporting things like this. can we wait with this a bit more? ",
            "author": "Simon Willnauer",
            "id": "comment-13781720"
        },
        {
            "date": "2013-09-30T10:30:41+0000",
            "content": "I don't understand - why can't it live in 4x and be tested by Jenkins as well as users? DocValues went through two releases before they were overhauled, I don't see how this issue is different. It baked in trunk for 2+ weeks, I think it can bake in the 4x branch as well (I believe we have enough time until 4.6 will be released?). Optimizations, which are important, can come next as improvements. For instance, we don't know yet if the data structures used (Maps!) are inefficient, because nobody uses this feature yet - maybe we'll spend a lot of time optimizing while in practice it won't make a big difference? I think that as long as this feature does not affect anyone who doesn't use numeric DV updates, it's safe to backport. ",
            "author": "Shai Erera",
            "id": "comment-13781731"
        },
        {
            "date": "2013-09-30T11:28:56+0000",
            "content": "I think backporting now is OK: we are at the start of the release cycle for 4.6 (so Jenkins will have lots of time to bake the changes for 4.6), the change has baked for ~ 2 weeks in trunk now, perf graphs look fine ( http://people.apache.org/~mikemccand/lucenebench/ ).\n\nI agree we can optimize in-RAM data structures, but I don't think that needs to hold up backporting.\n\nI suppose we could also wait some more (the test did uncover a big issue recently), but I don't think we need to... ",
            "author": "Michael McCandless",
            "id": "comment-13781746"
        },
        {
            "date": "2013-09-30T13:18:44+0000",
            "content": "I'm only confused by the strategy of the backport: I see things like SegmentWriteState.isFieldUpdate here that were fixed in other issues.\n\nIs this intentional? If the argument is to get user testing, I think to the user this would actually be \"unbaking\" if such a term exists...?\n\nCan you svn merge the revs to LUCENE-5215 and any other issues (e.g. bugfixes) so we have one coherent backport? ",
            "author": "Robert Muir",
            "id": "comment-13781788"
        },
        {
            "date": "2013-09-30T13:29:40+0000",
            "content": "I wrote (somewhere, don't remember where) that I'd like to backport issue-by-issue. So here I backport the changes done in this issue, after that I'll backport 5215, 5216 and 5246. That way the commits to 4x will be associated with the proper issues. Do you see a problem w/ that strategy? I definitely intend to backport all the changes in one go, only do that in multiple commits, one per issue. ",
            "author": "Shai Erera",
            "id": "comment-13781797"
        },
        {
            "date": "2013-09-30T13:34:01+0000",
            "content": "I don't think we should do that: then tests maybe fail, users hit too many open files, hit exceptions when segments dont contain their field, etc, etc in 4.x and we say \"sorry, 4.x is only temporarily broken: maybe you should try trunk for a more stable user experience?\" ",
            "author": "Robert Muir",
            "id": "comment-13781799"
        },
        {
            "date": "2013-09-30T13:43:13+0000",
            "content": "I suppose we could also wait some more (the test did uncover a big issue recently), but I don't think we need to...\n\nI think we should apply a general rule here that we (at least I believe so) in Lucene land had for a long time. ->> \"Only port stuff to the stable branch unless you'd be happy to release it tomorrow.\" I looked at the change and honestly this looks pretty scary to me. I'd not be happy with this being released tomorrow. We can happily aim for this being in 4.6 but baking in should happen in trunk. This is a huge change and I am thankful for the work that has been done on it but I think we should not rush this into the stable branch. This should IMO bake in more in trunk and have several rounds of optimization to it in terms of datastructures before we go and release it.\n ",
            "author": "Simon Willnauer",
            "id": "comment-13781810"
        },
        {
            "date": "2013-09-30T14:34:46+0000",
            "content": "I don't mind if we backport the whole thing in one commit. Just thought it will be cleaner to backport each issue's commits. I doubt anyone would \"hit\" an issue within the couple of hours it will take. But I'll do this in one backport.\n\nOnly port stuff to the stable branch unless you'd be happy to release it tomorrow\n\nI agree, though what if we decide to release 5.0 in one month? Do we revert the whole feature? I just think that it's software, and software always improves. Even if we optimize the way updates are kept (the problem is in ReaderAndLiveDocs), it can always be improved tomorrow even more. That's why the feature is marked @lucene.experimental \u2013 it may not be the most optimized thing, but it works and more importantly - it doesn't affect users that don't use it (\"do no harm\").\n\nI will look into improving the way updates are kept in RALD (Map<String,Map<Integer,Long>>), though honestly, we have no data points as to whether it's efficient or not, or whether the new structure is more efficient. What I think we can do is keep the updates in conceptually an int[] and long[] pair arrays (maybe one of those **Buffer we have for better compression). I'll start w/ that. ",
            "author": "Shai Erera",
            "id": "comment-13781868"
        },
        {
            "date": "2013-09-30T15:07:31+0000",
            "content": "I opened LUCENE-5248 to handle the data structure (irrespective of whether we choose to backport first or not). ",
            "author": "Shai Erera",
            "id": "comment-13781914"
        },
        {
            "date": "2013-09-30T16:32:14+0000",
            "content": "I'd not be happy with this being released tomorrow.\n\nCan you give more details here?  I.e., do you consider the optimizations necessary for release?  Or are there other things? ",
            "author": "Michael McCandless",
            "id": "comment-13781991"
        },
        {
            "date": "2013-10-01T09:35:08+0000",
            "content": "I found a problem with this on how updates are processed. The updateNumericDocValues method on DW doesn't correctly handle if flushControl.doApplyAllDeletes() returns true and we might miss flushing deletes at all. It also doesn't process IW events at all which this patch fixes.\n\nWhat worries me more here is that we don't have a test failing on this. ",
            "author": "Simon Willnauer",
            "id": "comment-13782762"
        },
        {
            "date": "2013-10-01T10:08:52+0000",
            "content": "Thanks Simon, good catch. I think there's no test for it because updates are not lost, they are just not applied based on RAM buffer settings. So e.g. if we reopened a reader, committed or kicked off a merge, they will be applied, right? Nevertheless, we should fix it as I'm seeing this affects my attempts to work on LUCENE-5248, where the updates are only applied when IW.close is called (since I don't reopen, merge or commit). ",
            "author": "Shai Erera",
            "id": "comment-13782782"
        },
        {
            "date": "2013-10-01T10:31:48+0000",
            "content": "here is an updated patch that contains a test for this as well - will commit this later ",
            "author": "Simon Willnauer",
            "id": "comment-13782795"
        },
        {
            "date": "2013-10-01T10:51:56+0000",
            "content": "Looks good. Thanks Simon. ",
            "author": "Shai Erera",
            "id": "comment-13782802"
        },
        {
            "date": "2013-10-01T12:17:38+0000",
            "content": "+1 for Simon's patch; good catch.  Sort of sneaky that\n\"doApplyAllDeletes\" as a side effect clears that bit.  Maybe it should\nbe named \"getAndClearDoApplyAllDeletes\" to make this behavior clear?\n\nSo, basically, if you only call IW.updateNumericDocValue, then we were\nfailing to flush once RAM usage was over the limit? ",
            "author": "Michael McCandless",
            "id": "comment-13782863"
        },
        {
            "date": "2013-10-01T12:33:09+0000",
            "content": "So, basically, if you only call IW.updateNumericDocValue, then we were failing to flush once RAM usage was over the limit?\n\nyes\n\nMaybe it should be named \"getAndClearDoApplyAllDeletes\" to make this behavior clear?\n\nI think we should change it to \"getAndResetApplyDeletes()\" I agree. Will do that in a separate commit and open a dedicated issue for it. ",
            "author": "Simon Willnauer",
            "id": "comment-13782875"
        },
        {
            "date": "2013-10-01T12:37:59+0000",
            "content": "+1 for getAndResetApplyDeletes ... I don't think it needs a separate issue ... thanks! ",
            "author": "Michael McCandless",
            "id": "comment-13782882"
        },
        {
            "date": "2013-10-01T12:59:57+0000",
            "content": "Commit 1528076 from Simon Willnauer in branch 'dev/trunk'\n[ https://svn.apache.org/r1528076 ]\n\nLUCENE-5189: Check if deletes need to be applied when docvalues are updated and process IW event if needed ",
            "author": "ASF subversion and git services",
            "id": "comment-13782905"
        },
        {
            "date": "2013-10-01T13:02:36+0000",
            "content": "Commit 1528077 from Simon Willnauer in branch 'dev/trunk'\n[ https://svn.apache.org/r1528077 ]\n\nLUCENE-5189: s/doApplyAllDeletes/getAndResetApplyAllDeletes/ ",
            "author": "ASF subversion and git services",
            "id": "comment-13782906"
        },
        {
            "date": "2013-10-01T13:25:11+0000",
            "content": "Commit 1528081 from Simon Willnauer in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1528081 ]\n\nLUCENE-5189: s/doApplyAllDeletes/getAndResetApplyAllDeletes/ ",
            "author": "ASF subversion and git services",
            "id": "comment-13782923"
        },
        {
            "date": "2013-10-01T13:25:34+0000",
            "content": "backported the renaming to 4x as well. ",
            "author": "Simon Willnauer",
            "id": "comment-13782924"
        },
        {
            "date": "2013-10-03T04:17:53+0000",
            "content": "While debugging LUCENE-5248, I've hit a bug when same terms update same doc multiple times. E.g. if the updates are sent key'd by the following term sequences: t1, t2, t1 \u2013 the updated value was that of 't2' and not 't1'. This is caused because LinkedHashMap traverses in insertion-order and when we encounter the second reference of 't1', we should remove and re-add it to the map. But the fix isn't that simple because BufDeletes currently holds a Map<Term,Map<String,NumericUpdate>> (for each Term, all the fields that it affects). We cannot remove and re-add a Term entry from the outer map, because we will move all the affected fields to the end of the iteration, which is wrong.\n\nI changed the map to be Map<String,LinkedHashMap<Term,NumericUpdate>> (per-field, all terms that update it, ordered) and wrote a simple testcase which reproduces this. TestNumericDVUpdates passed for few hundred iterations. ",
            "author": "Shai Erera",
            "id": "comment-13784809"
        },
        {
            "date": "2013-10-03T09:16:27+0000",
            "content": "As I continued to work on LUCENE-5248, I realized that the code in ReaderAndLiveDocs does not work well for documents without field (i.e. the segment has the field, but some docs don't, and they aren't updated). It falsely read their value from the existing ndv (reading 0L), which means that before the update, docsWithField returned false and after the updated it returned true. I consider this a bug as well, and incorporated a fix and a test in this patch. ",
            "author": "Shai Erera",
            "id": "comment-13784939"
        },
        {
            "date": "2013-10-03T13:02:30+0000",
            "content": "Commit 1528837 from Shai Erera in branch 'dev/trunk'\n[ https://svn.apache.org/r1528837 ]\n\nLUCENE-5189: fix updates-order and docsWithField bugs ",
            "author": "ASF subversion and git services",
            "id": "comment-13785059"
        },
        {
            "date": "2013-10-06T11:50:42+0000",
            "content": "Committed a fix to SegmentReader (r1529611) to clean unused memory (un-referenced DVPs) as well as don't use an anonymous RefCount class since it resulted in a memory leak, where those anonymous classes held a reference to their SR, therefore we always referenced unused DVPs. ",
            "author": "Shai Erera",
            "id": "comment-13787576"
        },
        {
            "date": "2013-10-06T14:56:19+0000",
            "content": "If someone isn't doing numeric updates, and just using (e.g. memory docvalues), are we really re-loading docvalues for each segmentreader now versus holding it in the segment core? ",
            "author": "Robert Muir",
            "id": "comment-13787630"
        },
        {
            "date": "2013-10-06T15:19:48+0000",
            "content": "To get NRT working for docvalues again, I feel like the logic should be something like this in SegmentReader:\n\nfooDV(field X) {\n  final Producer producer;\n  if (gen != core.gen) \n{\n     producer = this.producer;\n  }\n  else \n{\n     producer = core.producer;\n  }\n  ...\n}\n\nthis way, SR only opens up docvalues for ones that have been updated since the SCR was open, but otherwise uses the shared producer as before, and NRT works with docvalues again. ",
            "author": "Robert Muir",
            "id": "comment-13787642"
        },
        {
            "date": "2013-10-06T15:31:56+0000",
            "content": "Or maybe its a simple oversight that that RefCount thing is in SR versus SCR?\n\nI dont understand why a SR would need refcounting, since its unmodifiable? ",
            "author": "Robert Muir",
            "id": "comment-13787648"
        },
        {
            "date": "2013-10-06T15:34:48+0000",
            "content": "Hi, to me the whole code in SegmentReader looks like too complicated and contains things that should not be in SegmentReader: SegmenReaders are unmodifiabe and final. Why does SegmentReader need to use refcounting? the Refcounting should be in SegmentCoreReaders, SegmentReaders should only have final fields and unmodifiable data structures, please no refcounts!.\n\nIf a DirectoryReader is reopened you get a new instance of SegmentReader with same corecache key, why is it then having modifiable stuff?\n\nMaybe it is just moving stuff to SegmentCoreReaders, but I did so much hard work to keep this class simple when we removed modiications from IndexReaders in 4.x, thanks to Robert for the help! But now its as complicated or more complicated than before (see number of code lines in Lucene 3.6!). ",
            "author": "Uwe Schindler",
            "id": "comment-13787650"
        },
        {
            "date": "2013-10-06T17:06:00+0000",
            "content": "Let me explain: SCR is still used for all the shared objects between all SRs, objects that are closed when the last of the SRs using this 'core' is closed. With gen'd DVPs, they no longer belong in SCR, because there isn't a single DVP that all SRs share. For example, suppose that you start with an index without updates, then the DV fields gen is -1. So a DVP for field 'f' will be created. Next you update a field and reopen, the gen for 'f' is incremented to 1 and a new DVP is needed. The DVP for gen=-1 is no longer needed.\n\nRob, if we did what you propose - store the initial DVPs in SCR and the updated ones in SRs (as they are updated and reopened), Soon the DVPs in SCR will not be used by any SR, and just hang there (possibly consuming expensive RAM, e.g. MemoryDVF) until the last SR is closed.\n\nRather, DVPs are shared between SRs. RefCount is a simple object which keeps ref-counting for an object. When an SR is opened anew (e.g. initializes a new SCR), the DVPs it initializes all have RefCount(1). When an SR is opened by sharing another SR (e.g. NRT, DirReader.openIfChanged), it lists all the fields with DV. If the other SR has a DVP for their dvGen, it reuses it and inc-ref it, otherwise it opens a new DVP with RC(1).\n\nSR itself doesn't need any ref-counting, it's the DVPs that need them. Putting them in SCR I think only complicates things (or at least doesn't simplify). For example, currently when SR.doClose is called, it dec-ref all DVPs that it uses. And DocValuesRefCount.release() closes the DVP when its ref-count reaches 0. If we moved all the DVPs to SCR, then SR.doClose would need to go an dec-ref all DVPs that it uses in SCR .. but how does it know which DVP it uses if all DVPs just sit there in SCR - even ones that it doesn't use? I think that that they are in SR actually simplifies and keeps the code clear.\n\nRob, if you don't use DV updates, all DV fields have dvGen=-1 and they are shared between all SRs. ",
            "author": "Shai Erera",
            "id": "comment-13787683"
        },
        {
            "date": "2013-10-06T17:16:17+0000",
            "content": "because there isn't a single DVP that all SRs share\n\nlet me clarify that \u2013 if you don't use DV updates, you can say that the DVPs for gen=-1 are shared between all the readers. But with updates, gen=-1 may soon be unused by any reader. I prefer for all DVPs management to be in one place than to split it between SCR and SR.\n\nNevertheless, if you think this can be simplified somehow, I'm open to suggestions. I don't want to move them to SCR just for the sake of saying they are in SCR. The code which decides if to reuse a certain DVP or create a new one will still be in SR, and so it makes sense to me that SR manages them. ",
            "author": "Shai Erera",
            "id": "comment-13787688"
        },
        {
            "date": "2013-10-06T17:26:55+0000",
            "content": "Thanks shai... I think for me, this is the main one:\n\n\nSR itself doesn't need any ref-counting, it's the DVPs that need them. Putting them in SCR I think only complicates things (or at least doesn't simplify).\n\nIf SR itself doesnt need ref-counting, perhaps we can pull this out of SR then? (rote-refactor into DV-thingy or something). ",
            "author": "Robert Muir",
            "id": "comment-13787691"
        },
        {
            "date": "2013-10-06T18:52:19+0000",
            "content": "If SR itself doesnt need ref-counting, perhaps we can pull this out of SR then? (rote-refactor into DV-thingy or something).\n\nYou mean something like SegmentDocValues? It's doable I guess. SR would need to keep track of the DV.gens it uses though, so that in SR.doClose it can call segDV.decRef(gens) so that the latter can decRef all the DVPs that are used for these gens. If it also removes a gen from the map when it's no longer referenced by any SR, we don't need to take care of clearing the genDVP map when all SRs were closed (otherwise I think we'll need to refCount SegDV too, like SCR).\n\nI'll give it a shot. ",
            "author": "Shai Erera",
            "id": "comment-13787730"
        },
        {
            "date": "2013-10-12T04:18:54+0000",
            "content": "Commit 1531496 from Shai Erera in branch 'dev/trunk'\n[ https://svn.apache.org/r1531496 ]\n\nLUCENE-5189: test unsetting a document's value while the segment is merging ",
            "author": "ASF subversion and git services",
            "id": "comment-13793242"
        },
        {
            "date": "2013-10-12T05:28:12+0000",
            "content": "Add field updates to TestIndexWriterDelete.testNoLostDeletesOrUpdates. I had to change to test to catch IOException and ignore if it's FakeIOE, or ioe.getCause() is a FakeIOE. The reason is that if the exception happens during merge (in mergeMiddle), IW registers the exception in mergeExceptions and later throws it as a wrapped IOE. This caused the test to falsely fail. ",
            "author": "Shai Erera",
            "id": "comment-13793274"
        },
        {
            "date": "2013-10-12T23:00:36+0000",
            "content": "The test case changes look good!\n\nThis confused me:\n\n\n+            } else if (!fieldUpdate || random().nextBoolean()) { // sometimes do both deletes and updates\n\n\n\nShouldn't that just be its own if (not else if)?  Otherwise I think the comment is wrong ...\n\nAlso I think this:\n\n\n+          if (liveDocs != null && liveDocs.get(i)) {\n\n\n\nshould be:\n\n\n+          if (liveDocs == null || livedocs.get(i)) {\n\n\n\n?\n\nThat's a great catch on forceMerge carrying the wrapped IOExc forward.  Why is jenkins not hitting this already...? ",
            "author": "Michael McCandless",
            "id": "comment-13793521"
        },
        {
            "date": "2013-10-13T06:18:18+0000",
            "content": "Thanks for the comments Mike, I fixed the code.\n\nThat's a great catch on forceMerge carrying the wrapped IOExc forward. Why is jenkins not hitting this already...?\n\nI think because liveDocs aren't written during forceMerge, i.e. they are carried in RAM? ",
            "author": "Shai Erera",
            "id": "comment-13793579"
        },
        {
            "date": "2013-10-13T06:37:31+0000",
            "content": "Commit 1531620 from Shai Erera in branch 'dev/trunk'\n[ https://svn.apache.org/r1531620 ]\n\nLUCENE-5189: add field updates to TestIndexWriterDelete.testNoLostDeletesOrUpdatesOnIOException ",
            "author": "ASF subversion and git services",
            "id": "comment-13793580"
        },
        {
            "date": "2013-10-24T14:28:41+0000",
            "content": "I think it's ready to backport to 4x. ",
            "author": "Shai Erera",
            "id": "comment-13804246"
        },
        {
            "date": "2013-10-24T15:35:28+0000",
            "content": "I reviewed the issue and noticed I forgot to handle the refactoring Robert and Uwe asked for (SegmentDocValues). Here's a patch that handles it - quite straightforward. ",
            "author": "Shai Erera",
            "id": "comment-13804328"
        },
        {
            "date": "2013-10-24T20:26:45+0000",
            "content": "Commit 1535526 from Shai Erera in branch 'dev/trunk'\n[ https://svn.apache.org/r1535526 ]\n\nLUCENE-5189: factor out SegmentDocValues from SegmentReader ",
            "author": "ASF subversion and git services",
            "id": "comment-13804640"
        },
        {
            "date": "2013-10-25T07:17:06+0000",
            "content": "Patch covers the work from all the issues, ported to 4x (created w/ --show-copies-as-adds). I think it's ready (tests pass several times).\n\nIf there are no objections, I will add a CHANGES entry and commit it. ",
            "author": "Shai Erera",
            "id": "comment-13805115"
        },
        {
            "date": "2013-10-25T20:20:06+0000",
            "content": "+1 to backport. ",
            "author": "Michael McCandless",
            "id": "comment-13805658"
        },
        {
            "date": "2013-11-01T05:41:15+0000",
            "content": "Commit 1537829 from Shai Erera in branch 'dev/trunk'\n[ https://svn.apache.org/r1537829 ]\n\nLUCENE-5189: add CHANGES ",
            "author": "ASF subversion and git services",
            "id": "comment-13811057"
        },
        {
            "date": "2013-11-01T06:04:08+0000",
            "content": "Commit 1537832 from Shai Erera in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1537832 ]\n\nLUCENE-5189: add NumericDocValues field updates ",
            "author": "ASF subversion and git services",
            "id": "comment-13811069"
        },
        {
            "date": "2013-11-01T06:05:33+0000",
            "content": "Backported to 4x. I'll handle the \"TODO (DVU_RENAME)\" tasks (rote renaming of internal API). ",
            "author": "Shai Erera",
            "id": "comment-13811070"
        },
        {
            "date": "2013-11-01T07:03:36+0000",
            "content": "Handle \"TODO (DVU_RENAME)\" comments. It's really a rote renaming + I renamed some members that referred to these objects. ",
            "author": "Shai Erera",
            "id": "comment-13811096"
        },
        {
            "date": "2013-11-02T04:39:04+0000",
            "content": "Commit 1538143 from Shai Erera in branch 'dev/trunk'\n[ https://svn.apache.org/r1538143 ]\n\nLUCENE-5189: rename internal API following NumericDocValues updates ",
            "author": "ASF subversion and git services",
            "id": "comment-13811872"
        },
        {
            "date": "2013-11-02T06:18:54+0000",
            "content": "Commit 1538146 from Shai Erera in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1538146 ]\n\nLUCENE-5189: rename internal API following NumericDocValues updates ",
            "author": "ASF subversion and git services",
            "id": "comment-13811890"
        },
        {
            "date": "2013-11-02T06:21:54+0000",
            "content": "Finished backporting the changes to 4x. Thanks all for your valuable comments and help to get this feature in! ",
            "author": "Shai Erera",
            "id": "comment-13811892"
        },
        {
            "date": "2013-11-02T19:32:22+0000",
            "content": "Commit 1538258 from Shai Erera in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1538258 ]\n\nLUCENE-5189: add @Deprecated annotation to SegmentInfo.attributes ",
            "author": "ASF subversion and git services",
            "id": "comment-13812124"
        },
        {
            "date": "2013-11-20T21:23:47+0000",
            "content": "Shai Erera let me ask a trivial question.\n\n\tam I right that it writes to file when I call IW.updateNumericDocValue() ?\n\tand obviously, if it does so, I suppose it will be more efficient to somehow bulk series of updates, and flush it afterwards?\n\tor I'm wrong and there is some implicit flush semantics?\n\n\n\nthanks!  ",
            "author": "Mikhail Khludnev",
            "id": "comment-13828140"
        },
        {
            "date": "2013-11-20T21:31:38+0000",
            "content": "Mikhail Khludnev the updates are buffered in memory \"just like deletes\" and the batched when deletes are flushed to disk / are applied. correct me if I am wrong shai.  ",
            "author": "Simon Willnauer",
            "id": "comment-13828149"
        },
        {
            "date": "2013-11-21T06:14:46+0000",
            "content": "You're right Simon. The updates are buffered in their raw form in memory until a flush is needed (e.g. commit(), or NRT-open). At that point they are resolved and written to the Directory. This is where it differs from deletes - while deletes are small enough to keep the resolved form in-memory, updates aren't - a single update can affect millions of documents, each takes a long (updated value) ... perhaps future work could be to distinguish between small and large updates, and keep the small updates still in memory. But I believe that will affect a lot more code, e.g. SegReader will now need to be aware of in-memory NDV and on-disk and do a kind of merge between them when an NDV is requested for such field ... it's not going to be pretty-looking code I imagine. ",
            "author": "Shai Erera",
            "id": "comment-13828524"
        },
        {
            "date": "2014-03-15T18:25:02+0000",
            "content": "Just want to leave one caveat for memories. When you call \n\nIW.updateNumericDocValue(Term, String, Long)\n\n make sure that the term is deeply cloned before. Otherwise, if you modify term or bytes, then the modified version will be applied. That's might be a problem. ",
            "author": "Mikhail Khludnev",
            "id": "comment-13936265"
        },
        {
            "date": "2014-03-15T19:32:38+0000",
            "content": "I checked the code and it looks the same with e.g. deleteDocuments(Term) - the Term isn't cloned internally. So your comment pertains to other IW methods. ",
            "author": "Shai Erera",
            "id": "comment-13936279"
        },
        {
            "date": "2014-04-02T21:35:28+0000",
            "content": "One more note for adopters. if you start with experimenting with a test, make sure you suppress older codecs (check how a certain DV update test does it), otherwise you can spend some time to digging in really odd exception.  ",
            "author": "Mikhail Khludnev",
            "id": "comment-13958205"
        },
        {
            "date": "2017-04-04T02:58:35+0000",
            "content": "Erick Erickson was this a case of accidental keyboard presses causing JIRA to assign it to you accidentally?  This happened to me once and it was very embarrassing so I sympathize and doubt you intended to do this! ",
            "author": "David Smiley",
            "id": "comment-15954500"
        },
        {
            "date": "2017-04-04T06:22:03+0000",
            "content": "Yep, total inadvertent reassignment, sent it back to Shai... ",
            "author": "Erick Erickson",
            "id": "comment-15954610"
        }
    ]
}