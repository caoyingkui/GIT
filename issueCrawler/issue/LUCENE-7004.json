{
    "id": "LUCENE-7004",
    "title": "Duplicate tokens using WordDelimiterFilter for a specific configuration",
    "details": {
        "resolution": "Implemented",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [],
        "priority": "Minor",
        "status": "Resolved",
        "type": "Bug"
    },
    "description": "When using both the options PRESERVE_ORIGINAL|SPLIT_ON_CASE_CHANGE|CONCATENATE_ALL using the WordDelimiterFilter, we have duplicate tokens on strings contaning only case changes.\n\nWhen using the SPLIT_ON_CASE_CHANGE option, \"abcDef\" is split into \"abc\", \"Def\".\n\nWhen having PRESERVE_ORIGINAL, we keep \"abcDef\".\n\nHowever, when one uses CONCATENATE_ALL (or CATENATE_WORDS ?), it also adds another token built from the concatenation of the splited words, giving \"abcDef\" again.\n\nI'm not 100% certain that token filters should not produce duplicate tokens (same word, same start and end positions). Can someone confirm this is a bug ?",
    "attachments": {
        "FIX-LUCENE-7004.PATCH": "https://issues.apache.org/jira/secure/attachment/12785379/FIX-LUCENE-7004.PATCH",
        "wdf-analysis.png": "https://issues.apache.org/jira/secure/attachment/12785038/wdf-analysis.png",
        "TEST-LUCENE-7004.PATCH": "https://issues.apache.org/jira/secure/attachment/12785380/TEST-LUCENE-7004.PATCH"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15120268",
            "author": "Shawn Heisey",
            "date": "2016-01-27T22:13:03+0000",
            "content": "Assuming I understand this correctly, that the duplicate terms are at the same term position, this is something we should probably fix.  The term position is a different piece of information than the start/end position, which refers to character position in the unprocessed string supplied to analysis.\n\nTL;DR details:\n\nIn terms of matching, this behavior isn't a bug.  It won't cause incorrect matches or prevent correct matches, as long as the duplicate terms are at the same term position.\n\nThere are however potential ramifications for relevance.\n\nRemoveDuplicatesTokenFilterFactory exists, but from what I understand, this filter will only take effect when the duplicate tokens are next to each other in the stream ... frequently when WDF creates duplicates, they are NOT consecutive tokens.\n\nThis issue is not filed in the correct project \u2013 the class in question is a Lucene class.  I am waiting for confirmation on that before I move it. "
        },
        {
            "id": "comment-15122373",
            "author": "Jean-Baptiste Lespiau",
            "date": "2016-01-28T21:54:49+0000",
            "content": "Indeed, there is no incorrect match.\n\nYou can move this issue to its correct place. I didn't know there was a different place for Lucene.\n\nThe resulting token for \"abcDef\" are :\n\"abcDef\", \"abc\", \"abcDef\", \"Def\"\n\nThe 2 \"abcDef\" are the same, except for the \"positionIncrement\", since the first one degins a new word, and the second one does not.\n\ntermAtt=abcDef, startOffset=0, endOffset=6, typeAtt=word,\npositionIncrement=1, positionLength=1, keywordAtt=null]\n\ntermAtt=abcDef, startOffset=0, endOffset=6, typeAtt=word, \npositionIncrement=0, positionLength=1, keywordAtt=null\n\nWhat do you mean by the same term position ? There is no term attribute for me in a token. "
        },
        {
            "id": "comment-15122466",
            "author": "Shawn Heisey",
            "date": "2016-01-28T22:43:44+0000",
            "content": "I might be using incorrect terminology.  If so, I apologize.  See the attached screenshot showing Solr's analysis of WDF.  The \"position\" value shown at the very bottom of the screenshot is the information I was talking about.  The first three terms all are at position 1, and the \"Def\" term is at position 2. "
        },
        {
            "id": "comment-15122541",
            "author": "Erik Hatcher",
            "date": "2016-01-28T23:41:22+0000",
            "content": "The RemoveDuplicatesTokenFilter would remove one of those \"abcDef\"'s I believe.   "
        },
        {
            "id": "comment-15122672",
            "author": "Shawn Heisey",
            "date": "2016-01-29T01:06:00+0000",
            "content": "When I read the description of the RemoveDuplicates filter on the Solr wiki, it seems to say that duplicates will only be removed when they are consecutive tokens in the stream.  If that's indeed how it works, it would not remove duplicates in this case, because the duplicates are separated by another token. "
        },
        {
            "id": "comment-15123929",
            "author": "Jean-Baptiste Lespiau",
            "date": "2016-01-29T18:28:12+0000",
            "content": "I have looked at the source code and tested it, the RemoveDuplicates filter *will* filter the duplicate terms.\n\nIndeed, it removes duplicates in the set of tokens sharing the same position.\n\nHowever, it will do so by comparing every token with other tokens at the same position. For performance issues, I will still correct the WDTK, so that is it not necessary to call the RemoveDuplicates afterwards (it does only one iteration, and not 2). "
        },
        {
            "id": "comment-15125171",
            "author": "Jean-Baptiste Lespiau",
            "date": "2016-01-31T01:03:14+0000",
            "content": "Here are the patches:\nTEST-LUCENE-7004.PATCH: Add more tests, to prevent regression. I was not confident enough with the previous tests around the functionnality I wanted to modify.\nFIX-LUCENE-7004.PATCH: Fix the generation of the duplicate tokens, and change the few tests accordingly.\n\nIt also uses a new class for debuging introduced by LUCENE-7003.\n\nFeel free to comment.\n\nI don't know the patch format you require (just differences, of also the commits information). Just tell me if I need to change something.\n\n\nI suspect the \"!shouldGenerateParts(concatenation.type)\" in the condition of the flushConcatenation function in WDTK to be semantically incorrect, but it seems not to be producing any errors because it's dead code (i.e. always false). Removing it does not modify the tests passing. Well, that's another matter.\n\nPS: Thanks for the one who moved it into LUCENE. "
        },
        {
            "id": "comment-15137367",
            "author": "Jean-Baptiste Lespiau",
            "date": "2016-02-08T18:02:09+0000",
            "content": "I don't know the process for a patch to be committed to the code base. I imagine that it needs to be reviewed, and I am well aware that reviewers should have a lot of work.\n\nDo I have to do something ? I'm just following this through  "
        },
        {
            "id": "comment-15258977",
            "author": "Jean-Baptiste Lespiau",
            "date": "2016-04-26T21:37:48+0000",
            "content": "Change the status to implemented, hoping someone will review it.\nBe careful that LUCENE-7003 is a dependency. "
        }
    ]
}