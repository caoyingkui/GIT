{
    "id": "LUCENE-8231",
    "title": "Nori, a Korean analyzer based on mecab-ko-dic",
    "details": {
        "labels": "",
        "priority": "Major",
        "resolution": "Fixed",
        "affect_versions": "None",
        "status": "Closed",
        "type": "New Feature",
        "components": [
            "modules/analysis"
        ],
        "fix_versions": [
            "7.4",
            "master (8.0)"
        ]
    },
    "description": "There is a dictionary similar to IPADIC but for Korean called mecab-ko-dic:\nIt is available under an Apache license here:\nhttps://bitbucket.org/eunjeon/mecab-ko-dic\n\nThis dictionary was built with MeCab, it defines a format for the features adapted for the Korean language.\nSince the Kuromoji tokenizer uses the same format for the morphological analysis (left cost + right cost + word cost) I tried to adapt the module to handle Korean with the mecab-ko-dic. I've started with a POC that copies the Kuromoji module and adapts it for the mecab-ko-dic.\nI used the same classes to build and read the dictionary but I had to make some modifications to handle the differences with the IPADIC and Japanese. \nThe resulting binary dictionary takes 28MB on disk, it's bigger than the IPADIC but mainly because the source is bigger and there are a lot of\ncompound and inflect terms that define a group of terms and the segmentation that can be applied. \nI attached the patch that contains this new Korean module called godori nori. It is an adaptation of the Kuromoji module so currently\nthe two modules don't share any code. I wanted to validate the approach first and check the relevancy of the results. I don't speak Korean so I used the relevancy\ntests that was added for another Korean tokenizer (https://issues.apache.org/jira/browse/LUCENE-4956) and tested the output against mecab-ko which is the official fork of mecab to use the mecab-ko-dic.\nI had to simplify the JapaneseTokenizer, my version removes the nBest output and the decomposition of too long tokens. I also\nmodified the handling of whitespaces since they are important in Korean. Whitespaces that appear before a term are attached to that term and this\ninformation is used to compute a penalty based on the Part of Speech of the token. The penalty cost is a feature added to mecab-ko to handle \nmorphemes that should not appear after a morpheme and is described in the mecab-ko page:\nhttps://bitbucket.org/eunjeon/mecab-ko\nIgnoring whitespaces is also more inlined with the official MeCab library which attach the whitespaces to the term that follows.\nI also added a decompounder filter that expand the compounds and inflects defined in the dictionary and a part of speech filter similar to the Japanese\nthat removes the morpheme that are not useful for relevance (suffix, prefix, interjection, ...). These filters don't play well with the tokenizer if it can \noutput multiple paths (nBest output for instance) so for simplicity I removed this ability and the Korean tokenizer only outputs the best path.\nI compared the result with mecab-ko to confirm that the analyzer is working and ran the relevancy test that is defined in HantecRel.java included\nin the patch (written by Robert for another Korean analyzer). Here are the results:\n\n\n\n\nAnalyzer\nIndex Time\nIndex Size\nMAP(CLASSIC)\nMAP(BM25)\nMAP(GL2)\n\n\nStandard\n35s\n131MB\n.007\n.1044\n.1053\n\n\nCJK\n36s\n164MB\n.1418\n.1924\n.1916\n\n\nKorean\n212s\n90MB\n.1628\n.2094\n.2078\n\n\n\n\n\nI find the results very promising so I plan to continue to work on this project. I started to extract the part of the code that could be shared with the\nKuromoji module but I wanted to share the status and this POC first to confirm that this approach is viable. The advantages of using the same model than\nthe Japanese analyzer are multiple: we don't have a Korean analyzer at the moment , the resulting dictionary is small compared to other libraries that\nuse the mecab-ko-dic (the FST takes only 5.4MB) and the Tokenizer prunes the lattice on the fly to select the best path efficiently.\nThe dictionary can be built directly from the godori module with the following command:\nant regenerate (you need to create the resource directory (mkdir lucene/analysis/godori/src/resources/org/apache/lucene/analysis/ko/dict) first since the dictionary is not included in the patch).\nI've also added some minimal tests in the module to play with the analysis.",
    "attachments": {
        "LUCENE-8231-remap-hangul.patch": "https://issues.apache.org/jira/secure/attachment/12916846/LUCENE-8231-remap-hangul.patch",
        "LUCENE-8231.patch": "https://issues.apache.org/jira/secure/attachment/12916687/LUCENE-8231.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16418277",
            "date": "2018-03-29T00:20:05+0000",
            "content": "I really like the approach here, thanks Jim! \n\nIn the kuromoji case there are is a lot of japanese-specific compression that Uwe and I did, if you are worried about size/ram we can try to shrink this korean data in ways that make sense for it. That can really be a followup/polish/nice-to-have: how big is the built JAR now? something semi-reasonable?\n\nI'll try to at least build your proof of concept and poke around. Funny to see HantecRel.java in the patch, it looked somehow familiar  ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16418283",
            "date": "2018-03-29T00:32:50+0000",
            "content": "I was able to build and run tests with your instructions.\n\nAnswering my own question, here is the built size:\n\nrw-rw-r- 1 rmuir rmuir   9063332 Mar 28 20:27 lucene-analyzers-godori-8.0.0-SNAPSHOT.jar\n\nHere are uncompressed sizes and gzip ratios:\n\nrw-rw-r- 1 rmuir rmuir    65564 Mar 28 20:26 CharacterDefinition.dat\nrw-rw-r- 1 rmuir rmuir 11178837 Mar 28 20:26 ConnectionCosts.dat\nrw-rw-r- 1 rmuir rmuir 11408895 Mar 28 20:26 TokenInfoDictionary$buffer.dat\nrw-rw-r- 1 rmuir rmuir  5640925 Mar 28 20:26 TokenInfoDictionary$fst.dat\nrw-rw-r- 1 rmuir rmuir   811783 Mar 28 20:26 TokenInfoDictionary$targetMap.dat\nrw-rw-r- 1 rmuir rmuir      129 Mar 28 20:26 UnknownDictionary$buffer.dat\nrw-rw-r- 1 rmuir rmuir       36 Mar 28 20:26 UnknownDictionary$targetMap.dat\n\n  adding: CharacterDefinition.dat (deflated 99%)\n  adding: ConnectionCosts.dat (deflated 89%)\n  adding: TokenInfoDictionary$buffer.dat (deflated 71%)\n  adding: TokenInfoDictionary$fst.dat (deflated 25%)\n  adding: TokenInfoDictionary$targetMap.dat (deflated 67%)\n  adding: UnknownDictionary$buffer.dat (deflated 44%)\n  adding: UnknownDictionary$targetMap.dat (deflated 25%)\n\nI'll take a peek and try to get familiar with the data, just for kicks. But I think sizes are reasonable already, thanks again for bringing this all together. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16418304",
            "date": "2018-03-29T01:16:05+0000",
            "content": "Should there be a ReadingFormFilter similar to the kuromoji case? e.g. this would yield hanja -> hangul if someone wants that. I remember seeing some of this in that hantec corpus, and I think the other analyzer did this conversion. so you could try it out for kicks. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16418690",
            "date": "2018-03-29T10:05:08+0000",
            "content": "Thanks for looking Robert !\n\n\n\nShould there be a ReadingFormFilter similar to the kuromoji case?\n\n\nI attached a new patch that adds this filter, the readings were already in the binary dictionary so this does not change the size of the jar.\u00a0\n\n\n\nIn the kuromoji case there are is a lot of japanese-specific compression that Uwe and I did, if you are worried about size/ram we can try to shrink this korean data in ways that make sense for it. That can really be a followup/polish/nice-to-have: how big is the built JAR now? something semi-reasonable?\n\n\n\u00a0\n\nI think the size is reasonable especially if you compare with other libraries that use the mecab-ko-dic .\n\nThough there are still some rooms for improvement. I did not add the semantic class of the token but we could do the same than for the Japanese dic where the pos are added in a separate file. The semantic class + POS is unique per leftId so this could also save 1 byte in the binary dictionary (we use 1 byte per POS per term in the main dictionary).\n\nThe expression that contains the decompounds can also be compressed. For compound nouns I serialize the segmentations with the term but we could just use offset from the surface form. It doesn't work for Inflects which can add tokens or use a different form. To be honest I don't know how we can derive the offsets for the decompound of Inflects, I don't think there is an easy way to do that but I could be completely wrong.\n\n\u00a0\n\nIn the patch I attached the user dictionary is broken, I copied the one from Kuromoji but we should probably change it to accept simple nouns (NNG or NNP) where there is no segmentation and use the PREANALYSIS type to add custom segmentations (or COMPOUND for nouns only).\n\n\u00a0\n\nI talked to my Korean colleagues and they told me that godori has a negative meaning in Korea. It is linked with illegal gambling and it also has an ancient meaning of \"killed by king's order\" which is bad . This is what happens when you pick a name without knowing the culture so apologize for this. I changed the name to \"nori\" which is the proposal they made, it means joy/play. This is a very generic name, in Japanese it means\u00a0seaweeds and is used to wrap sushis and onigiri which I find nice because it 's also a reference to the Japanese analyzer which was used as a root for this.\n\n\u00a0\n\n\u00a0 ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16418762",
            "date": "2018-03-29T10:59:01+0000",
            "content": "\nThe expression that contains the decompounds can also be compressed. For compound nouns I serialize the segmentations with the term but we could just use offset from the surface form. It doesn't work for Inflects which can add tokens or use a different form. To be honest I don't know how we can derive the offsets for the decompound of Inflects, I don't think there is an easy way to do that but I could be completely wrong.\n\nI would really do this, I think it will help with the common case. I think you should just add a way to \"escape\" and encode this \"literally\" just like today for the inflected verb/adjective forms as a corner case. They are like 5% of the total dictionary, don't bother optimizing them. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16418785",
            "date": "2018-03-29T11:20:33+0000",
            "content": "How about sharing code between the 2 modules? I had no time to closely look into this.\n\nIt's long time ago, but I remember that Robert and I did a lot to shrink the files. Yes there was a lot \"compression\" involved, but mostly it was thinking of better data structures. We should maybe review this here, too. I may have some time over easter vacation, so we can maybe have a \"hackaton\". ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16418793",
            "date": "2018-03-29T11:29:27+0000",
            "content": "I think its ok if they don't share code initially. We can try to maybe factor out a \"kuromoji engine\" or something into a analyzers-common class, but seems like a lot of work.\n\nYes for the compression it just means doing a lot of experimentation. For example in this case I am not sure about the whole FST representation. Root arc caching of all syllables is heavy there, thats a big range, and very different than a small alphabet. What is the performance of this analyzer if this caching is disabled?\n\nAnd it hints at another possibility: maybe syllables should not be encoded into the FST? You could try just encoding the decomposed jamo form instead. Then in general you are working with a small alphabet in the FST: it would make the traversal cost 3x, but would allow efficient root arc caching and prevent huge slow binary searches of many syllables. Basically a larger but \"easier\" FST. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16418821",
            "date": "2018-03-29T11:41:15+0000",
            "content": "Just in case its unclear in the above, i'm saying input of \u1112\u1161\u11ab could go thru the FST as:\u1112 + \u1161 + \u11ab. But you can then swap jamo range with latin-1 range so that its just 3 bytes instead of 6 with a UTF-8 encoding. Don't waste time with the hanja as its presumably rare, which matches UTF-8 expectations. This way you'd be encoding actual \"characters\" instead of syllables and the FST is bigger but has a nicer structure. I feel like i did this on the other korean analyzer, but its been a long time and I can't remember. there may even be code there in its branch. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16418833",
            "date": "2018-03-29T11:54:10+0000",
            "content": "Root arc caching of all syllables is heavy there, thats a big range, \n\nI think that root cache is already restricted to a max. of 0x80 elements.\n\nhttps://github.com/apache/lucene-solr/blob/ca22f17662c9b79ada1f90fb200f76d9a58c0e75/lucene/core/src/java/org/apache/lucene/util/fst/FST.java#L384-L390 ",
            "author": "Dawid Weiss"
        },
        {
            "id": "comment-16418840",
            "date": "2018-03-29T11:56:45+0000",
            "content": "and looking more, you'd need full byte range to do that. So a BYTE1 FST with raw bytes (no UTF-8) but its doable. The root cache would just be 256 entries. Maybe you just have a separate BYTE2 FST for the other stuff such as hanja forms. but i think overall the performance may be faster. The decomposition/recomposition is not so bad from what i remember, its some simple math on the unicode codepoint numbers. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16418845",
            "date": "2018-03-29T11:58:48+0000",
            "content": "\nI think that root cache is already restricted to a max. of 0x80 elements.\n\nhttps://github.com/apache/lucene-solr/blob/ca22f17662c9b79ada1f90fb200f76d9a58c0e75/lucene/core/src/java/org/apache/lucene/util/fst/FST.java#L384-L390\n\nDawid, the code here has its \"own\" cache, but its huge and caching whole precomposed syllables.\n\n\n    for (int i = 0; i < rootCache.length; i++) {\n      if (fst.findTargetArc(0xAC00 + i, firstArc, arc, fstReader) != null) {\n        rootCache[i] = new FST.Arc<Long>().copyFrom(arc);\n      }\n    }\n\n\n\nKuromoji does a similar trick but its just cache the kana which are alphabets. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16418863",
            "date": "2018-03-29T12:03:41+0000",
            "content": "Ah, sorry. I though it's using FST directly. When I experimented with large alphabets (full terms) on the first arc I ended up using an associative array from a symbol to its target's offset \u2013 this was faster than binary lookup on the array-expanded root node (but we made huge amounts of those lookups alone, not part of the full indexing pipeline \u2013 the difference may be negligible when embedded in a larger algorithm). ",
            "author": "Dawid Weiss"
        },
        {
            "id": "comment-16418884",
            "date": "2018-03-29T12:19:20+0000",
            "content": "yeah its just the general case that if you only have 256 possible values for a node because its a \"character\", we have more options to make it fast. Today because this FST represents syllables instead, it means the FST is gonna have less nodes, but huge numbers of transitions from each one, so I expect lookups are pretty slow due to that. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16418888",
            "date": "2018-03-29T12:25:55+0000",
            "content": "\n\nand looking more, you'd need full byte range to do that. So a BYTE1 FST with raw bytes (no UTF-8) but its doable. The root cache would just be 256 entries. Maybe you just have a separate BYTE2 FST for the other stuff such as hanja forms. but i think overall the performance may be faster. The decomposition/recomposition is not so bad from what i remember, its some simple math on the unicode codepoint numbers.\n\n\n\u00a0\n\nThat's a good idea. I'll give it a try. FYI I reindexed the HantecRel corpus without the root arc caching and it took 300s to build instead of the 200s so caching helps but I agree that caching the full syllabe range is too much. ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16418952",
            "date": "2018-03-29T13:00:25+0000",
            "content": "Yeah i think thats a good sign that the huge binary searches in the FST are slow? So its promising at least as far as a different representation being faster. There is some limit in FST somewhere to use \"array arcs\" but I don't remember what it is. Hopefully you'd generally get that optimization and we'd be looking at 3 O(1) lookups instead of 1 O(log n) lookup for each syllable. Unfortunately we won't know if its the right tradeoff unless we experiment and find it out. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16418972",
            "date": "2018-03-29T13:11:21+0000",
            "content": "See code from that other analyzer for doing these kind of lookups: https://svn.apache.org/viewvc/lucene/dev/branches/lucene4956/lucene/analysis/arirang/src/java/org/apache/lucene/analysis/ko/dic/HangulDictionary.java?r1=1534141&r2=1534364\n\nThe whole diff for that change is here: https://svn.apache.org/viewvc?view=revision&revision=1534364 ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16419121",
            "date": "2018-03-29T14:54:34+0000",
            "content": "I tried this approach and generated a new FST with the remap chars. The size of the FST after conversion is 4MB + 1MB for the separated Hanja FST which is roughly the same size as the FST with the hangul syllab and the Hanja together (5.4MB). I also ran the HantecRel indexation and it tooks approximatively 235s to build (I tried multiple times and the times were pretty consistent) with root caching for the 255 first arcs. That's surprising because it's slower than the FST with hangul syllab and root caching (200s) so I wonder if this feature is worth the complexity ? I checked the size of the root caching for the 11,171 syllabs for Hangul and it takes approximatively 250k so that's not bad considering that this version is faster.\n\n\u00a0\n\nI'll try the compression for compounds now. ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16419140",
            "date": "2018-03-29T15:00:46+0000",
            "content": "well according to my commit years ago it was \"smaller and much faster\". But I don't remember exactly what the numbers were, only that it was worth the trouble. Maybe something wasn't quite right? I remember it being a big difference for lookup performance. Do you have a patch for your experiment somewhere? i wouldn't mind taking a look to see if it was something silly. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16419178",
            "date": "2018-03-29T15:20:06+0000",
            "content": "Sure I attached a new patch (LUCENE-8231-remap-hangul.patch) that applies the remap at build and analyze time. I skipped all entries that are not hangul or latin-1 chars to make it easier to test. I must have missed something so thanks for testing !\n\n\u00a0 ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16420404",
            "date": "2018-03-30T11:28:37+0000",
            "content": "Thanks for uploading the patch! I will dig into this some, hopefully it doesn't turn out to be a failed experiment.  ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16420467",
            "date": "2018-03-30T12:59:38+0000",
            "content": "I attached a new patch that adds a better compression for the compounds (saves 1MB on the total size) and fixes the handling of the user dictionary. It is now possible to add common nouns or compounds in the user dictionary. I also added more tests to make sure that the dictionary contains valid data. In terms of feature I think it's ready, now I'll focus on cleanup and adding more tests.\n\nUwe Schindler, thanks for\u00a0volunteering ! I think it would be nice to share some code with the Kuromoji but I agree with Robert, this is a lot of work and I didn't want to change the format and the processing of the Kuromoji too early. We can still reevaluate the feasibility of merging some code when we have a better idea of the final format for this analyzer. ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16421320",
            "date": "2018-03-31T13:01:33+0000",
            "content": "Hi Jim, I dug into this a bit more to explain your results, just some notes from what regenerate prints:\n\n\n\tsyllable FST: 171397 nodes, 826926 arcs, 5644960 bytes\n\tcharacter FST: 464305 nodes, 944356 arcs, 4230896 bytes\n\n\n\nI think the syllable fst is \"unhealthy\" in shape but all the strangeness is basically at the root, and the cache takes care of that. otherwise it goes sparse pretty quickly due to how the dictionary is organized. you don't need a cache at all for the character one.\n\ni think the difference may be that this dictionary is structured differently: it contains full compounds as entries and each compound lists its decomposition. \nThis is very different from LUCENE-4956 decompounding process, which didn't have full compounds as entries, making the dictionary 10x smaller.  ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16421333",
            "date": "2018-03-31T13:51:46+0000",
            "content": "I looked at the recent patch, one thing we need to warn about is that this thing currently loads > 30MB of stuff into RAM. \n\n20MB of that is the connection costs, which seems a bit excessive on the heap. Maybe this should be done with a direct shortbuffer similar to BinaryDictionary? ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16421353",
            "date": "2018-03-31T15:04:07+0000",
            "content": "There is still quite a bit of redundancy in the compound storage. Can we optimize the case where all the parts have the same POS (and that's also the POS of the compound itself)? we could steal an unused sign bit or something like that to indicate this, and it would save writing all those POS bytes for ~ 300k items. This seems to be the 90% case, with the notable exception of places (which seem to often be proper + general noun). ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16421364",
            "date": "2018-03-31T15:41:21+0000",
            "content": "Another thing to look at is if we really need two bytes for type + tag. i have trouble following this from the CSV file, but how many unique combinations are there really?  ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16421999",
            "date": "2018-04-02T07:32:33+0000",
            "content": "Hi Robert, thanks for your testings and suggestions !\n\nI pushed another patch that applies your suggestions. The connection costs is now loaded in a direct byte buffer on init (the matrix is still compressed on disk). I also changed the format of the binary dictionary to use 2 bits of the left id to encode the type (compound, inflect, morpheme or prenanalysis), added a POS dict that maps the left id and the part of speech tag and introduced a new flag to indicate entries with a single POS. The new size on disk is 25MB with 10MB on the heap at start since we don't load the connection costs in the heap anymore.\u00a0 ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16422791",
            "date": "2018-04-02T17:12:38+0000",
            "content": "I attached a new patch with lots of cleanups and fixes. I ran HantecRel again, here are the results:\n\n\n\n\nAnalyzer\nIndex Time\nIndex Size\nMAP(CLASSIC)\nMAP(BM25)\nMAP(GL2)\n\n\nKorean\n178s\n90MB\n.1638\n.2101\n.2081\n\n\n\n\n\nI am not sure why it got faster, could be the new compressed format, I'll dig. ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16423345",
            "date": "2018-04-03T01:25:01+0000",
            "content": "Hi Jim, the latest changes look great. Thanks for optimizing it more!\n\nDo you think there is an easy way to preserve the original compound with the decompoundfilter? Purely based on memory, I think kuromoji may do this by default, and maybe even our german decompounders too. From what I remember from relevance experiments in other languages, it seems like a pretty practical investment, especially since lucene's terms dict is good. \n\nMainly the concern i have is to have some handling for errors in the decompounding process. For this analyzer the representation of the model with respect to compounds is a little awkward, and I worry about OOV cases. So at least preserving can assist with that a bit. Might be an easy win. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16425218",
            "date": "2018-04-04T09:05:57+0000",
            "content": "Hi Robert,\nI pushed another iteration that moves the decompound process and the POS filtering in the tokenizer. I think it's simpler to perform the decompound and the filtering directly in the tokenizer, this also allows to keep the compound token (I added a decompound mode option that disallow decompound (none), discard the decompound (discard) or perform the decompound and keep the original token (mixed)). By default the compound token is discarded but it can be kept using the mixed mode. \nI also changed the normalization option when building the dictionary, instead of adding the normalized form and the original form the builder now replaces the original form with the normalized one. By default the normalization is not activated but it can be useful for other Korean dictionaries that uses a decomposed form for hanguls like the Handic for instance:\nhttps://ja.osdn.net/projects/handic/\nI added more tests and javadocs, I think it's getting closer  ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16432078",
            "date": "2018-04-10T11:12:17+0000",
            "content": "I attached a new patch that fixes an issue with offsets of the compound nouns. Currently the patch outputs a single path and can also keep the original compound as well as the decompounds. I think we can add the N best paths in a follow up. ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16434869",
            "date": "2018-04-12T03:05:03+0000",
            "content": "We may want to tweak the attributes reflection, or look at this POS enum some. It makes things a bit hard for debugging any analysis issues, e.g.\n\n\nterm=\ud55c\uad6d,bytes=[ed 95 9c ea b5 ad],startOffset=0,endOffset=2,positionIncrement=1,positionLength=1,type=word,termFrequency=1,posType=MORPHEME,leftPOS=NNP,rightPOS=NNP,morphemes=null,reading=null\nterm=\ub300\ub2e8,bytes=[eb 8c 80 eb 8b a8],startOffset=4,endOffset=6,positionIncrement=1,positionLength=1,type=word,termFrequency=1,posType=MORPHEME,leftPOS=XR,rightPOS=XR,morphemes=null,reading=null\nterm=\ub098\ub77c,bytes=[eb 82 98 eb 9d bc],startOffset=8,endOffset=10,positionIncrement=1,positionLength=1,type=word,termFrequency=1,posType=MORPHEME,leftPOS=NNG,rightPOS=NNG,morphemes=null,reading=null\nterm=\uc774,bytes=[ec 9d b4],startOffset=10,endOffset=13,positionIncrement=1,positionLength=1,type=word,termFrequency=1,posType=MORPHEME,leftPOS=VCP,rightPOS=VCP,morphemes=null,reading=null\n\n\n\nCan we make it so the user doesn't have to decode the tag abbreviations, at least when using the toString (reflector stuff)? If they have to look this up from comments in the source code it will be more difficult. For example kuromoji:\n\n\nterm=\u591a\u304f,bytes=[e5 a4 9a e3 81 8f],startOffset=0,endOffset=2,positionIncrement=1,positionLength=1,type=word,termFrequency=1,baseForm=null,partOfSpeech=\u540d\u8a5e-\u526f\u8a5e\u53ef\u80fd,partOfSpeech (en)=noun-adverbial,reading=\u30aa\u30aa\u30af,reading (en)=ooku,pronunciation=\u30aa\u30fc\u30af,pronunciation (en)=oku,inflectionType=null,inflectionType (en)=null,inflectionForm=null,inflectionForm (en)=null,keyword=false\nterm=\u5b66\u751f,bytes=[e5 ad a6 e7 94 9f],startOffset=3,endOffset=5,positionIncrement=2,positionLength=1,type=word,termFrequency=1,baseForm=null,partOfSpeech=\u540d\u8a5e-\u4e00\u822c,partOfSpeech (en)=noun-common,reading=\u30ac\u30af\u30bb\u30a4,reading (en)=gakusei,pronunciation=\u30ac\u30af\u30bb\u30a4,pronunciation (en)=gakusei,inflectionType=null,inflectionType (en)=null,inflectionForm=null,inflectionForm (en)=null,keyword=false\nterm=\u8a66\u9a13,bytes=[e8 a9 a6 e9 a8 93],startOffset=6,endOffset=8,positionIncrement=2,positionLength=1,type=word,termFrequency=1,baseForm=null,partOfSpeech=\u540d\u8a5e-\u30b5\u5909\u63a5\u7d9a,partOfSpeech (en)=noun-verbal,reading=\u30b7\u30b1\u30f3,reading (en)=shiken,pronunciation=\u30b7\u30b1\u30f3,pronunciation (en)=shiken,inflectionType=null,inflectionType (en)=null,inflectionForm=null,inflectionForm (en)=null,keyword=false\n\n ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16434875",
            "date": "2018-04-12T03:13:27+0000",
            "content": "An easy win related to this is to make enum values have real javadocs comments instead of source code ones.\n\ne.g. today it looks like:\n\npublic enum Tag {\n    // Infix\n    E(100),\n    // Interjection\n    IC(110),\n\n\n\nBut if we make \"Infix\" and \"Interjection\" real javadocs comments then the user has a way to see the whole POS tagset in the generated documentation. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16435420",
            "date": "2018-04-12T11:58:25+0000",
            "content": "Thanks Robert.\nI attached a new patch that changes the enum to attach a description to each tag and reflected the description in javadocs comments.\nThe toString reflection now returns the description of the POS tag:\n\nKoreanTokenizer@22f9baea term=\ud3c9\ucc3d,bytes=[ed 8f 89 ec b0 bd],startOffset=0,endOffset=2,positionIncrement=1,positionLength=1,type=word,termFrequency=1,posType=MORPHEME,leftPOS=NNP(Proper Noun),rightPOS=NNP(Proper Noun),morphemes=null,reading=null\n\n\n... and the compounds are correctly rendered:\n\nKoreanTokenizer@292528fd term=\uac00\ub77d\uc9c0\ub098\ubb3c,bytes=[ea b0 80 eb 9d bd ec a7 80 eb 82 98 eb ac bc],startOffset=0,endOffset=5,positionIncrement=1,positionLength=1,type=word,termFrequency=1,posType=COMPOUND,leftPOS=NNG(General Noun),rightPOS=NNG(General Noun),morphemes=\uac00\ub77d\uc9c0/NNG(General Noun)+\ub098\ubb3c/NNG(General Noun),reading=null\n\n\n\nI also change the format for the Preanalysis token, they are now compressed using the same technic than for Compounds which gives another 2MB improvement over the last patch.\n ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16435496",
            "date": "2018-04-12T12:48:42+0000",
            "content": "very nice. may want to look at generated javadocs (last patch i looked at had some precommit issues we may need to fix first) and take a stab at the overview.html? It is the first thing someone sees when they click the module's doc on the website, e.g.: https://lucene.apache.org/core/7_3_0/analyzers-icu/index.html\n\nalso i noticed DEFAULT_STOP_TAGS is in the TokenizerFactory, that makes the tokenizer very difficult to use from the lucene API. Can we push such defaults into the Tokenizer instead so that it is easier to instantiate etc?  ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16435655",
            "date": "2018-04-12T14:19:06+0000",
            "content": "I think I've seen this root arc caching technique in at least a couple other places.  Perhaps the FST ought to natively do this?  Or perhaps with an optional wrapper to add configurable toggles (i.e. ranges to cache) like I see here in TokenInfoFST? ",
            "author": "David Smiley"
        },
        {
            "id": "comment-16435685",
            "date": "2018-04-12T14:32:53+0000",
            "content": "i don't think it should. it is very specific to what kuromoji/nori are doing. i think the caching FST does by default is already good. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16435755",
            "date": "2018-04-12T15:27:55+0000",
            "content": "I attached a new patch that passes precommit checks. The javadocs looks fine, all part of speech tags have a description attached. \nThe DEFAULT_STOP_TAGS is now in the Tokenizer and is used by default when no tags are specified. ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16435770",
            "date": "2018-04-12T15:30:54+0000",
            "content": "I still don't see a KoreanTokenizer ctor that uses these defaults? I only see ones with 4 or 5 required parameters. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16435802",
            "date": "2018-04-12T15:43:56+0000",
            "content": "Right, I changed the Analyzer but not the Tokenizer. I attached a new patch that adds two more ctor that use the default parameters. ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16435888",
            "date": "2018-04-12T16:35:57+0000",
            "content": "If the UserDictionary is optional can we just have a no-arg ctor? I think building a custom dictionary is pretty expert. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16435922",
            "date": "2018-04-12T16:55:10+0000",
            "content": "Sure, I added two more ctr in the last patch, one with no-arg and one with only the AttributeFactory. ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16435951",
            "date": "2018-04-12T17:08:49+0000",
            "content": "Do you think we should remove KoreanTokenizer(AttributeFactory, UserDictionary) and KoreanTokenizer(UserDictionary)? \n\nThis way we just have \"defaults\" and \"everything\" ctors.\n\nIt would also remove some compile-time ambiguity, KoreanTokenizer(AttributeFactory) vs KoreanTokenizer(UserDictionary), which is not good if null values are allowed. And I don't see passing a custom UserDictionary as being any less expert than changing the stopwords lists or preserving original compounds. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16435964",
            "date": "2018-04-12T17:11:42+0000",
            "content": "And i havent looked into why the tokenizer takes stoptags.\n\nIt may help to separate concerns better if this is moved to a tokenfilter like how kuromoji does it: https://github.com/apache/lucene-solr/blob/master/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapanesePartOfSpeechStopFilter.java\n ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16436001",
            "date": "2018-04-12T17:29:09+0000",
            "content": "I agree this will also simplify the understanding of these ctoes. I'll remove them and keep only the defaults and everything ctors. \nRegarding why the KoreanTokenizer takes stoptags, it is done to simplify the removal of tokens when we keep compounds since we need to set the position length of the compound token without the tokens that should be removed. Otherwise the stop tags filter should handle position length when it removes a token and I find it simpler to do it directly in the Tokenizer especially if we add the support for keeping the N best paths in a follow up. ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16436006",
            "date": "2018-04-12T17:31:51+0000",
            "content": "Shouldn't FilteringTokenFilter be enough? It just requires you to return true/false. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16436019",
            "date": "2018-04-12T17:39:05+0000",
            "content": "No because FilteringTokenFilter doesn't handle positionLength so if it removes a token from a compound it needs to change the posLength of the original compound. I tried to write something to handle this case in the filtering token filter but it's not trivial and requires a lot of code so I choose the simple path of removing the tokens directly in the tokenizer.  ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16436030",
            "date": "2018-04-12T17:45:57+0000",
            "content": "I don't understand why it needs to change posLength, I think this is some other issue. Why is this analyzer any different than the japanese one for this case? ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16436042",
            "date": "2018-04-12T17:54:01+0000",
            "content": "I think that the Japanese analyzer has the same issue and it's even worst since it can outputs multiple paths. If we have a compound \"AB\" that is decomposed into \"A\" and \"B\", if we remove \"B\" we need to change the posLength of \"AB\" to be 1 (instead of 2). ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16436063",
            "date": "2018-04-12T18:02:10+0000",
            "content": "I didn't really see consensus on this issue though (there was some discussion about it on LUCENE-4065, and it seemed FilteringTokenFilter may be doing the right thing) definitely think its a concern unrelated to korean and we shouldn't put stopword filtering into our tokenizers yet until its understood and discussed. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16436066",
            "date": "2018-04-12T18:05:23+0000",
            "content": "Ok I'll restore the KoreanPartOfSpeechStopFilter then and we can discuss LUCENE-4065 separately. ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16436086",
            "date": "2018-04-12T18:16:20+0000",
            "content": "We may want to make a new issue and link. LUCENE-4065 was originally about a totally separate idea (not posLength related), i think it just has a bad too-general title. so its kinda confusing to follow at the moment. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16436123",
            "date": "2018-04-12T18:42:46+0000",
            "content": "I attached a new patch that restores the KoreanPartOfSpeechFilter and changes the ctors for the KoreanTokenizer.\n\nI also opened https://issues.apache.org/jira/browse/LUCENE-8250 for the stop filter issue with position length. ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16436664",
            "date": "2018-04-13T01:45:24+0000",
            "content": "+1 to commit the latest patch. Thanks for all the work here. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16437064",
            "date": "2018-04-13T09:27:07+0000",
            "content": "Commit e851b89cbeb1f55edc0f2c1276e2ae812eca2643 in lucene-solr's branch refs/heads/master from Jim Ferenczi\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=e851b89 ]\n\nLUCENE-8231: Add a new analysis module (nori) similar to Kuromoji but to handle Korean\n\nThis change adds a korean analyzer in a new analysis module named nori. It is similar\nto Kuromoji but uses the mecab-ko-dic dictionary to perform morphological analysis of Korean\ntext. ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-16437065",
            "date": "2018-04-13T09:27:38+0000",
            "content": "Thanks a lot Robert ! Any objections to backport to 7x ? ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16437141",
            "date": "2018-04-13T10:26:35+0000",
            "content": "+1 to backport ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16437162",
            "date": "2018-04-13T10:40:41+0000",
            "content": "+1 to backport ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16437241",
            "date": "2018-04-13T12:17:58+0000",
            "content": "Commit 8651fbaa8bc65fdfec397ad4e7ebe2b5acf78d7b in lucene-solr's branch refs/heads/branch_7x from Jim Ferenczi\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=8651fba ]\n\nLUCENE-8231: Add a new analysis module (nori) similar to Kuromoji but to handle Korean\n\nThis change adds a korean analyzer in a new analysis module named nori. It is similar\nto Kuromoji but uses the mecab-ko-dic dictionary to perform morphological analysis of Korean\ntext. ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-16437242",
            "date": "2018-04-13T12:21:12+0000",
            "content": "Commit 0544486b3912c88cadf0a0307074c0bf255d5415 in lucene-solr's branch refs/heads/master from Jim Ferenczi\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0544486 ]\n\nLUCENE-8231: update CHANGES.txt after backport to 7x ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-16437244",
            "date": "2018-04-13T12:22:20+0000",
            "content": "Thanks Robert and Uwe. ",
            "author": "Jim Ferenczi"
        },
        {
            "id": "comment-16462495",
            "date": "2018-05-03T14:12:02+0000",
            "content": "Commit 9b261087abcd7ef350e49d4fa4e72e075a135799 in lucene-solr's branch refs/heads/master from Jim Ferenczi\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=9b26108 ]\n\nLUCENE-8231: Add missing part of speech filter in the SPI META-INF file ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-16462497",
            "date": "2018-05-03T14:13:13+0000",
            "content": "Commit 1ed95c097b82ee5f175e93f3fe62572abe064da6 in lucene-solr's branch refs/heads/branch_7x from Jim Ferenczi\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1ed95c0 ]\n\nLUCENE-8231: Add missing part of speech filter in the SPI META-INF file ",
            "author": "ASF subversion and git services"
        }
    ]
}