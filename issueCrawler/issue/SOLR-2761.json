{
    "id": "SOLR-2761",
    "title": "FSTLookup should use long-tail like discretization instead of proportional (linear)",
    "details": {
        "affect_versions": "3.4",
        "status": "Closed",
        "fix_versions": [
            "3.5",
            "3.6",
            "4.0-ALPHA"
        ],
        "components": [
            "spellchecker"
        ],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Duplicate"
    },
    "description": "The Suggester's FSTLookup implementation discretizes the term frequencies into a configurable number of buckets (configurable as \"weightBuckets\") in order to deal with FST limitations. The mapping of a source frequency into a bucket is a proportional (i.e. linear) mapping from the minimum and maximum value. I don't think this makes sense at all given the well-known long-tail like distribution of term frequencies. As a result of this problem, I've found it necessary to increase weightBuckets substantially, like >100, to get quality suggestions.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Michael McCandless",
            "id": "comment-13104608",
            "date": "2011-09-14T16:06:49+0000",
            "content": "What limitation of FSTs is causing us to discretize the term frequencies? "
        },
        {
            "author": "David Smiley",
            "id": "comment-13104641",
            "date": "2011-09-14T16:36:02+0000",
            "content": "FSTLookup is well documented, thanks to Dawid.  Here is a link to the Javadocs for your convenience, Mike: https://builds.apache.org/job/Lucene-3.x/javadoc/all/org/apache/lucene/search/suggest/fst/FSTLookup.html?is-external=true "
        },
        {
            "author": "David Smiley",
            "id": "comment-13104653",
            "date": "2011-09-14T16:47:13+0000",
            "content": "It should be noted there are code comments Dawid left on doing another approach:\n\n    // Distribute weights into at most N buckets. This is a form of discretization to\n    // limit the number of possible weights so that they can be efficiently encoded in the\n    // automaton.\n    //\n    // It is assumed the distribution of weights is _linear_ so proportional division \n    // of [min, max] range will be enough here. Other approaches could be to sort \n    // weights and divide into proportional ranges.\n\n "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13104710",
            "date": "2011-09-14T17:42:02+0000",
            "content": "Let me know if anything is not clear, Mike. "
        },
        {
            "author": "Michael McCandless",
            "id": "comment-13104761",
            "date": "2011-09-14T18:35:13+0000",
            "content": "Ooooh, the javadocs and comments are awesome! \u2013 thanks Dawid and\nDavid.\n\nI was just wondering what specifically is the limitation on our FST\nimpl and whether it's something we could improve.  It sounds like the\nlimitation is just how we quantize the incoming weights...\n\nDavid, when you use > 100 buckets did you see bad performance for\nlow-weight lookups?\n\nMaybe, in addition to the up-front quantization, we could also store\na more exact weight for each term (eg as the output).  Then on\nretrieve we could re-sort the candidates by that exact weight.  But\nthis will make the FST larger... "
        },
        {
            "author": "David Smiley",
            "id": "comment-13104766",
            "date": "2011-09-14T18:39:43+0000",
            "content": "David, when you use > 100 buckets did you see bad performance for low-weight lookups?\n\nI didn't try in any serious way. I was simply writing about this feature when I observed the suggestions were poor compared to other Lookup impls and other ways of doing term completion. Then I started digging into why and what could be done about it. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13104845",
            "date": "2011-09-14T20:11:35+0000",
            "content": "I guess a lot depends on the use case. In my case quantization was not a problem (the scores were \"rough\" and query independent anyway, so they did fall into corresponding buckets). \"poor\" performance would then have to be backed by what the requirement really is \u2013 if one needs sorting by exact scores then the method used to speed up FSTLookup simply isn't a good fit. Still, compared to fetching everything and resorting this is a hell of a lot faster, so many folks (including me) may find it helpful.\n\nIt all depends, in other words.\n\nAs for using more buckets \u2013 sure, you can do this. In fact, you can combine both approaches and use quantization to prefetch a buffer of matches, then collect outputs, sort and if this fills your desired number of results then there is no need to search any further because all other buckets will have lower scores (exact). "
        },
        {
            "author": "David Smiley",
            "id": "comment-13104864",
            "date": "2011-09-14T20:52:48+0000",
            "content": "I recommend that we follow through with the alternative suggested in the source code comment: sort by weight and divide evenly.  That will handle the actual distribution in the data no matter what the curve looks like. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13105214",
            "date": "2011-09-15T08:39:54+0000",
            "content": "I'll work on it. You may be the first serious user of that code  Most people simply use Solr queries to do suggestions it seems (because it provides more flexibility in terms of dynamic scoring). "
        },
        {
            "author": "David Smiley",
            "id": "comment-13105400",
            "date": "2011-09-15T14:49:37+0000",
            "content": "LOL; I am not using it \"seriously\". I'm merely kicking the tires to see how well it works so I can write about it in the 2nd edition of my book.  When you say \"Most people use Solr queries to do suggestions it seems\", do you mean search query logs? That requires sufficient query volume, and it's more work to set up, for sure, than query term completion/suggest. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13105404",
            "date": "2011-09-15T14:53:33+0000",
            "content": "Don't laugh, I'm serious  I once wanted to collect some feedback about who's using Lookups and how they're using it and it seems people just load queries to Solr and do regular prefix searches over an index (possibly mutating the scoring with a function of geoip proximity or other factors).  "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13151159",
            "date": "2011-11-16T12:16:29+0000",
            "content": "Refactoring of FSTLookup API. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13151160",
            "date": "2011-11-16T12:20:44+0000",
            "content": "Brainstorming discussions with Robert and Simon who had real use cases. The outcome is that discretization into buckets will be problematic to \"get right\" in the general case. The distribution of weight functions may require custom tweaks and tunings that should best be done before weights are added to the FSTLookup. An explicit API of the form add(term, int bucket) will be added, with an adapter over TermFreqIterator to do min/max (value range) or long-tail (sorted input) bucketing. These adapters will be more costly as they may require additional passes over the data or re-sorting of the input data. The add(term, int bucket) will be cheap(er) with only a single sort required. "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13152798",
            "date": "2011-11-18T11:28:38+0000",
            "content": "Incorporated into SOLR-2888 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13157847",
            "date": "2011-11-27T12:36:04+0000",
            "content": "Bulk close after 3.5 is released "
        },
        {
            "author": "Dawid Weiss",
            "id": "comment-13190783",
            "date": "2012-01-22T21:14:28+0000",
            "content": "Linking arbitrary score fst-based autocompletion algorithm. "
        }
    ]
}