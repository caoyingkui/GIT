{
    "id": "LUCENE-5928",
    "title": "WildcardQuery may has memory leak",
    "details": {
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Not A Problem",
        "components": [
            "core/search"
        ],
        "affect_versions": "4.9",
        "status": "Resolved",
        "fix_versions": []
    },
    "description": "data 800G, records 15*10000*10000.\none search thread.\ncontent:???\ncontent:*\ncontent:*1\ncontent:*2\ncontent:*3\n\njvm heap=96G, but the jvm memusage over 130g?\nrun more wildcard, use memory more....\n\nDoes luence search/index use a lot of DirectMemory or Native Memory?\nI use -XX:MaxDirectMemorySize=4g, it does nothing better.\n\n\nThanks.",
    "attachments": {
        "idxb1-top-sorted-mem.png": "https://issues.apache.org/jira/secure/attachment/12669900/idxb1-top-sorted-mem.png",
        "top_java.jpg": "https://issues.apache.org/jira/secure/attachment/12667569/top_java.jpg"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14126800",
            "author": "Littlestar",
            "content": "when I changed to NIO, It works OK.\nDoes MMAP use a lot of NativeMemory? ",
            "date": "2014-09-09T09:48:13+0000"
        },
        {
            "id": "comment-14126867",
            "author": "Uwe Schindler",
            "content": "Hi,\nthis is not an issue of WildcardQuery. This is also not related to used heap space. What you see differences in is in most cases a common misunderstanding about those 2 terms:\n\n\n\tVirtual Memory (VIRT): This is allocated address space, it is not allocated memory. On 64 bit platforms this is for free and is not limited by physical memory (it is not even related to each other). If you use mmap, VIRT is something like RES + up to 2 times the size of all open indexes. Internally the whole index is seen like a swap file to the OS kernel.\n\tResident Memory (RES): This is size of heap space + size of direct memory. This is allocated memory, but may reside on swap, too. Please keep in mind, that some operating systems also count memory, which was mmapped from file system cache to the process there, because this is resident. You can see this looking at SHR (share), which is memory shared with other processes (in that case the kernel). For Lucene this RES memory is also not a problem, becaus ethe file system cache is managed by the kernel and freed on request (SHR/RES goes down then).\n\n\n\nBy executing a Wildcard like \":\" you just access the whole term dictionary and all positings lists, so they are accessed on disk and therefore loaded into file system cache. When using MMap, the space in file system cache is also shown in VIRT of the process, because the linux/windows kernel maps the file system memory into the address space. But its does not \"waste\" memory.\n\nFor more information, see: http://blog.thetaphi.de/2012/07/use-lucenes-mmapdirectory-on-64bit.html ",
            "date": "2014-09-09T10:52:04+0000"
        },
        {
            "id": "comment-14126913",
            "author": "Uwe Schindler",
            "content": "You may also look at: http://stackoverflow.com/questions/561245/virtual-memory-usage-from-java-under-linux-too-much-memory-used ",
            "date": "2014-09-09T11:52:49+0000"
        },
        {
            "id": "comment-14127909",
            "author": "Littlestar",
            "content": "hi, \n   when using default MMapDirectory,  jvm heap=96G,  the java process RES over 130g, not VIRT(VIRT >=900G).\n\nsee attachement, thanks.\n ",
            "date": "2014-09-10T01:42:49+0000"
        },
        {
            "id": "comment-14139919",
            "author": "Shawn Heisey",
            "content": "I have seen this exact memory reporting issue on my Solr installs.  Java appears to misreport memory usage.  See how your SHR value is 37GB?  If you subtract that from your RES value, I think that's a lot closer to how much memory it's actually using.\n\nFor what follows, reference the screenshot that I have attached.\n\n\n\nSolr on this machine has a max heap of 6144M.  The machine has 64GB of RAM.  Let's add up some numbers, each of which I would ordinarily consider to be absolute truth:\n\n16GB: RES size of the Solr process.\n9.8GB: The amount of memory listed as free.\n44GB: The amount of memory used by the OS disk cache.\n1.1GB: next largest java process.\n0.3GB: next largest java process.\n\nThese numbers add up to more than 70GB ... but the machine only has 64GB total.  But if you subtract the 11GB value in the SHR column, then it all fits, and the resulting number is also less than the max heap of 6144M. ",
            "date": "2014-09-19T03:17:27+0000"
        },
        {
            "id": "comment-14140089",
            "author": "Uwe Schindler",
            "content": "Hi,\n\nas I mentioned before, SHR is shared memory - used by multiple processes. In the case of mmap, it is shared memory used by the kernel (FS cache) and at the same time by the process (mmapped files currently in fs cache), so you cannot count it 2 times. In fact, if you use mmap, the \"real\" resident memory by a process is RES - SHR. The remaining SHR is just resident (because allocated) because the kernel has actually done this allocation and frees it on request (there must be something that allocates RAM for the disk cache...).\n\nIt is a little bit confusing, but correct  There is nothing wrong with Solr. ",
            "date": "2014-09-19T06:40:55+0000"
        }
    ]
}