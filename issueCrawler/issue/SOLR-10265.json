{
    "id": "SOLR-10265",
    "title": "Overseer can become the bottleneck in very large clusters",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "Let's say we have a large cluster. Some numbers:\n\n\tTo ingest the data at the volume we want to I need roughly a 600 shard collection.\n\tIndex into the collection for 1 hour and then create a new collection\n\tFor a 30 days retention window with these numbers we would end up wth  ~400k cores in the cluster\n\tJust a rolling restart of this cluster can take hours because the overseer queue gets backed up. If a few nodes looses connectivity to ZooKeeper then also we can end up with lots of messages in the Overseer queue\n\n\n\nWith some tests here are the two high level problems we have identified:\n\n1> How fast can the overseer process operations:\n\nThe rate at which the overseer processes events is too slow at this scale. \n\nI ran OverseerTest#testPerformance which creates 10 collections ( 1 shard 1 replica ) and generates 20k state change events. The test took 119 seconds to run on my machine which means ~170 events a second. Let's say a server can process 5x of my machine so 1k events a second. \n\nTotal events generated by a 400k replica cluster = 400k * 4 ( state changes till replica become active ) = 1.6M / 1k events a second will be 1600 minutes.\n\nSecond observation was that the rate at which the overseer can process events slows down when the number of items in the queue gets larger\n\nI ran the same OverseerTest#testPerformance but changed the number of events generated to 2000 instead. The test took only 5 seconds to run. So it was a lot faster than the test run which generated 20k events\n\n\n2> State changes overwhelming ZK:\n\nFor every state change Solr is writing out a big state.json to zookeeper. This can lead to the zookeeper transaction logs going out of control even with auto purging etc set . \n\nI haven't debugged why the transaction logs ran into terabytes without taking into snapshots but this was my assumption based on the other problems we observed",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2017-03-11T16:28:05+0000",
            "content": "Trying to think a little OOB here:\n\n> Is there any way to split one or more collections across multiple ZK ensembles? My instant response is yuck, that'd be major surgery and perhaps practically impossible but I thought I'd toss it out there.\n\n> Can we reduce the number of messages the Overseer has to process, especially when nodes are coming up?\n\n> Does all that stuff have to be written to the tlog so often (I'd guess yes, just askin').\n\n> Does all this stuff actually have to flow through ZK?\n\n> Could we \"somehow\" roll up/batch changes? I.e. go through the queue, assemble all of the changes that affect a particular znode and write one state change (or do we already)? \n\nAnd I don't know that code at all so much of this may be nonsense or already done. Think of it as brainstorming ",
            "author": "Erick Erickson",
            "id": "comment-15906241"
        },
        {
            "date": "2017-03-11T16:42:30+0000",
            "content": "Side comment, need its own issue if we decide to pursue.\n\nOnly about 60000 entries can fit in the overseer queue before retrieving the queue exceeds the default jute.maxbuffer and ZOOKEEPER-1162 becomes a problem.\n\nSome calculation revealed that if we use a nested node structure where we create a \"directory\" node in the overseer queue and then put events inside those directories, limiting the number of directories to 32768 and the number of entries in each directory to 32768, we can handle a billion entries without exceeding the 1MB maxbuffer.  Figuring out how to handle rollover in the directory names would be necessary.\n\nOf course it would likely take weeks or months to actually process a billion entries, but it does need to be able to handle more than 60000 without breaking down. ",
            "author": "Shawn Heisey",
            "id": "comment-15906245"
        },
        {
            "date": "2017-03-12T00:55:58+0000",
            "content": "I haven't debugged why the transaction logs ran into terabytes without taking into snapshots but this was my assumption based on the other problems we observed\n\nThe current design is suboptimal. Just to change one small bit of information, we write ~300bytes per replica. for 600 replicas we write ~180kb of data (add the fact that 600 nodes have to read that data and parse for every state change operation). Every write is appended to the ZK transaction log. We must split the state.json to scale any further. \nIf we write out the state separately to a format as follows\n\n{\n\"replica1\": 1\n\"replica2\":0\n}\n\n\n\nwe can bring down the data size to around 10 bytes per replica. This means 600, replicas will have only 6K data per update.  ",
            "author": "Noble Paul",
            "id": "comment-15906381"
        },
        {
            "date": "2017-03-18T05:31:30+0000",
            "content": "We should seriously reconsider the idea of using ZK queue for the overseer messages. \n\nThere is no harm in maintaining an in memory queue at the overseer and posting messages directly to overseer. There is a risk of Overseer dying, or getting moved in between. This can be alleviated by making the client smarter. \n\n\n\tclient posts a message to Overseer\n\toverseer puts the message into an internal queue returns an id for the message\n\tOverseer processes the message, one after other\n\tclient polls overseer to check the status . if successfully processed, return the call\n\tif the overseer fails in between. got to step #1\n\n ",
            "author": "Noble Paul",
            "id": "comment-15931070"
        },
        {
            "date": "2017-03-18T19:14:26+0000",
            "content": "Noble Paul Interesting. You're essentially replacing the overseer zknodes with an in-memory queue, right?\n\nI do wonder about sequencing. The order of operations could get switched, although I'm not sure it matters if there's some sort of optimistic concurrency provisions (or maybe even if not). Imagine request1 and request2 in that order, the overseer dies. Depending on the timing of the retry logic, the clients could well submit them in reverse order....\n\nAnother thought I had was how often we actually need to write the state out. Imagine 100 updates for the same state.json file. Is there any good reason those updates couldn't all be written at once? The algorithm would look something like\n\nget N messages from the queue\nApply all the changes\nwrite out all affected nodes to ZK.\n\nHmmm, in that case, though, any watchers expecting an inermediate transition state wouldn't see it. Imagine node goes straight from \"down\" to \"active\". That is all the intermediate state changes are collapsed into a single write. Would that matter? In the situation we're talking about here this would certainly cut down a lot of chatter.\n\nMusings on Saturday morning.\n ",
            "author": "Erick Erickson",
            "id": "comment-15931336"
        },
        {
            "date": "2017-03-18T22:45:35+0000",
            "content": "Depending on the timing of the retry logic, the clients could well submit them in reverse order.\n\nThe client should take care of the ordering. if message with id n is to be retried, then all messages with id >n must be retried too\n\nAnother thought I had was how often we actually need to write the state out. Imagine 100 updates for the same state.json file. Is there any good reason those updates couldn't all be written at once? \n\nYeah, that is an easy optimization. I was working on a patch for the same\n ",
            "author": "Noble Paul",
            "id": "comment-15931437"
        },
        {
            "date": "2017-03-19T00:26:07+0000",
            "content": "bq: The client should take care of the ordering. \n\nThe case I was wondering about was two different clients. Or doesn't it matter as long as each client preserves the ordering of all requests that originate on that client only?\n ",
            "author": "Erick Erickson",
            "id": "comment-15931500"
        },
        {
            "date": "2017-03-19T00:27:51+0000",
            "content": "Order of events between two clients don't matter ",
            "author": "Noble Paul",
            "id": "comment-15931501"
        },
        {
            "date": "2017-03-19T02:08:41+0000",
            "content": "We should seriously reconsider the idea of using ZK queue for the overseer messages. \n\nSerious question, not being confrontational:  What are the characteristics of zookeeper which make it bad choice for the overseer queue?  My suspicion is that it's problematic because of the number of messages in a typical queue, not because of technology issues.  If a way can be found to accomplish the overseer job with a very small number of messages instead of a very large number, would ZK be suitable?\n\nCan't we create more queue entry types that can accomplish with a single entry what currently can take hundreds or thousands of entries (depending on the number of collections)?  I can envision types that indicate \"node X just came up\" and \"node X just went down\" ... the overseer should be able to use the cluster information in ZK to decide what state changes are required, rather than processing an individual message for each state change.  Other types that I haven't thought of would probably be needed.  Another type might specify state changes that affect multiple collections or shard replicas with a single entry.\n\nWhen a collection is created, is there REALLY a need to send state changes for EVERY collection in the cloud?  That seems completely unnecessary to me.  I have not attempted other actions like creating a new replica, but I would not be surprised to learn that they also send many unnecessary state changes.\n\nIf ZK isn't used for the overseer queue, then some other method must be found to make sure the queue is not lost when nodes go down.  An in-memory queue would need to be somehow replicated to at least one other node in the cluster.  Doing that well probably means finding another dependency where somebody else has worked out the bugs. ",
            "author": "Shawn Heisey",
            "id": "comment-15931532"
        },
        {
            "date": "2017-03-19T03:14:11+0000",
            "content": "bq: My suspicion is that it's problematic because of the number of messages in a typical queue\n\nWell, that's certainly true, in this case the overseer work queue for at least two reasons: 1> the current method of dealing with the queue doesn't batch and 2> the state changes get written to the logs which get huge. Part of the bits we're exploring are being smarter about that too.\n\nbq: I can envision types that indicate \"node X just came up\" and \"node X just went down\"\n\n\"Just went down\" is being considered, but \"just came up\" is a different problem. Consider a single JVM hosting 400 replicas from 100 different collections each of which has 4 shards. \"Node just came up\" could (I think) move them all from \"down\" to \"recovering\" in one go, but all the leader election stuff still needs to be hashed out. Also consider if a leader and follower are on the same node. How do they coordinate the leader coming up and informing the follower of that fact without sending individual messages?\n\nbq: When a collection is created, is there REALLY a need to send state changes for EVERY collection in the cloud? \n\nIIUC this just shouldn't be happening. If it is we need to fix it. I've certainly seen some allegations that this is so.\n\nbq: If ZK isn't used for the overseer queue, ....\n\nWell, that's part of the proposal. The in-memory Overseer queue is OK (according to Noble's latest comment) if each client retries submitting the pending operations that aren't ack'd. So no alternate queueing mechanism is necessary.\n\nWhat's really happening here IMO is the evolution of SorlCloud. So far we've gotten away with inefficiencies because we haven't really had huge clusters to deal with, now we're getting them. So we need to squeeze what efficiencies we can out of the current architecture while considering if we really need ZK as the intermediary. Of course cutting down the number of messages would help, the fear is that there's been a great deal of work put into hardening all the wonky conditions so I do worry a bit about wholesale restructuring.... Ya gotta bite the bullet sometime though. ",
            "author": "Erick Erickson",
            "id": "comment-15931544"
        },
        {
            "date": "2017-03-19T04:20:50+0000",
            "content": "The main reason a zk queue was used instead of in memory was that there was a grand idea that eventually you could create the collection by simply putting the command on the queue and call it successful, even if things crashed. The Overseer would have the create captured and work towards its parameters over time. \n\nThat is still ideal vs the current system. It's never been done though, and could be accomplished without the zk queue largely. ",
            "author": "Mark Miller",
            "id": "comment-15931560"
        },
        {
            "date": "2017-03-19T15:43:28+0000",
            "content": "Hmmm, could we take a step in that direction now? Off the top of my head on Sunday morning before coffee has really kicked in, so maybe nonsense.\n\nAnyway, it seems like we have two sets of operations here, what about splitting them out? The state change messages go into memory and the Collections-level calls like \"create collection\", \"addreplica\" and the like continue to use the Overseer queue. Seems like that would allow us to then build robustness into the collection-level ops \"sometime\" and have them nicely isolated. ",
            "author": "Erick Erickson",
            "id": "comment-15931754"
        },
        {
            "date": "2017-03-19T23:48:28+0000",
            "content": "Yeah, the overseer messages and collection action messages are already handled separately in two different queues. If we can  manage the  overseer messages without sending to zk it's a huuuge win because that's the bulk of the operations  ",
            "author": "Noble Paul",
            "id": "comment-15932032"
        },
        {
            "date": "2017-04-10T21:49:12+0000",
            "content": "There are perhaps other wins we could get in the short term as well. For example, if we could tell a cluster stop and start from a simple Overseer failover, we could avoid the common waste where we fire tons of state updates due to shutdown or some other reason and then when we start the cluster backup, we first have to run through all those useless state updates in the queue.  ",
            "author": "Mark Miller",
            "id": "comment-15963527"
        },
        {
            "date": "2017-07-29T04:03:35+0000",
            "content": "I haven't participated in these overseer related conversations yet so pardon my display of ignorance.  What if the collection's state.json were further split such that each replica had it's own znode file.  Then, when a replica comes up and down, it could simply write directly to ZK; no overseer needed.  \n\nIf for some reason a per-replica znode is unworkable, would a per-shard znode possibly work?  If so, then the shard leader could take care to manage the state of it's replicas.  Then replicas would contact their shard leader (not the Overseer) to notify a change in state.  And the leader would do state changes directly to ZooKeeper. ",
            "author": "David Smiley",
            "id": "comment-16106015"
        },
        {
            "date": "2017-07-29T06:01:30+0000",
            "content": "If for some reason a per-replica znode is unworkable, would a per-shard znode possibly work?\n\nper replica znode is useless. per-shard is the finest we can get.\n\nHowever, the problem is not how big the state.json is. The real problem is the frequency of updates because of replica state changes. That is why I suggested that we should split out the state from the rest of the data (which rarely changes).  ",
            "author": "Noble Paul",
            "id": "comment-16106038"
        },
        {
            "date": "2017-07-29T16:29:49+0000",
            "content": "Hmm, it seems to me that another problem with splitting any finer than collection is watchers.  Currently IIUC a watcher is set on the collection's state.json for every collection that is hosted on a particular Solr instance. Would a watcher have to be set for every shard of every collection hosted on a particular Solr if we split it up more granularly?\n\nOr has this all changed and I'm behind the times?\n\nOr, more radically, is ZooKeeper the right place to keep this info? Not that I have any idea what to do to replace it, just throwin' it out there. When SolrCloud started we were talking about making 5 billion doc collections manageable. The current model has gotten us this far which is great, I mean we're talking 100B document (and larger) collections currently being supported out there. Are there other models that would better suit the next 100x expansion? ",
            "author": "Erick Erickson",
            "id": "comment-16106167"
        },
        {
            "date": "2017-07-31T01:56:57+0000",
            "content": "Would a watcher have to be set for every shard of every collection hosted on a particular Solr if we split it up more granularly?\n\nyes. I don't think it is a huge problem\n\nLet's do a back-of-the-envelope calculation\n\nlet's split our 100B doc index to 1000 shards (100m docs/shard) and 5 replicas . that is just 5000 cores. This leads to having 5000 watchers if no node has multiple replicas of same shard. IIRC zookeeper really has no issues with 5000 watchers ",
            "author": "Noble Paul",
            "id": "comment-16106729"
        },
        {
            "date": "2017-10-06T09:05:15+0000",
            "content": "Maybe the problem here is Overseer is processing all messages in a single thread ( with a lot of IO blocking for every time we peek, poll messages and write new clusterstate )? So it will be wasted in case of using a powerful machine just for Overseer.\nThe idea here is each collection has its own states.json, so messages for different collections can be processed and updated in parallel. It can be tricky to implement but If we want a cluster with 400k cores, we can not just use a single thread to process 1m6 messages. ",
            "author": "Cao Manh Dat",
            "id": "comment-16194319"
        },
        {
            "date": "2017-10-06T18:04:32+0000",
            "content": "Guava has something really cool that I think we could use for a multi-threaded overseer, and it is available in the 14.0.1 version that Solr currently includes:\n\nhttps://google.github.io/guava/releases/14.0/api/docs/com/google/common/util/concurrent/Striped.html\n\nIf we use the name of the collection as the stripe input, then we will have individual locks for each collection, so we can guarantee order of operations.  If there are any Overseer operations that are cluster-wide and not applicable to one collection, we probably need to come up with a special name for those.\n\nNB: It's possible that different collection names might hash to the same stripe and therefore block each other, but that would hopefully be a rare occurrence.\n\nProbably what should happen is that each thread would grab an operation from the queue, and attempt to acquire the lock for the collection mentioned in that operation.  If a ton of operations happen that all apply to one collection, then it would still act like the current single-thread implementation.  One thing that I do not know is whether the Java \"Lock\" implementation (which is what is obtained from the Striped class) guarantees what order the threads will be granted the lock.  The hope is of course that locks will be granted in the order they are requested when several threads are all attempting to use the same lock. ",
            "author": "Shawn Heisey",
            "id": "comment-16194970"
        },
        {
            "date": "2017-10-10T18:46:23+0000",
            "content": "No idea how this got assigned to me.  I probably was trying to type in another window while Jira had focus. ",
            "author": "Shawn Heisey",
            "id": "comment-16199169"
        },
        {
            "date": "2017-10-10T19:27:06+0000",
            "content": "Dat has been working on it recently, so I'll reassign to him. ",
            "author": "Cassandra Targett",
            "id": "comment-16199235"
        },
        {
            "date": "2017-10-11T03:45:51+0000",
            "content": "I linked two child issues for this issue which is SOLR-11443 and SOLR-11465. Estimated that if we finish both of them, Overseer can process messages about 15x to 20x times faster. So with 1.6M messages, we can process all of them in 100 minutes ( or less ). Which I think is enough for this ticket. ",
            "author": "Cao Manh Dat",
            "id": "comment-16199766"
        }
    ]
}