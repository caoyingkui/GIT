{
    "id": "LUCENE-1749",
    "title": "FieldCache introspection API",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/search"
        ],
        "type": "Improvement",
        "fix_versions": [
            "2.9"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "FieldCache should expose an Expert level API for runtime introspection of the FieldCache to provide info about what is in the FieldCache at any given moment.  We should also provide utility methods for sanity checking that the FieldCache doesn't contain anything \"odd\"...\n\n\tentries for the same reader/field with different types/parsers\n\tentries for the same field/type/parser in a reader and it's subreader(s)\n\tetc...",
    "attachments": {
        "LUCENE-1749-hossfork.patch": "https://issues.apache.org/jira/secure/attachment/12415096/LUCENE-1749-hossfork.patch",
        "fieldcache-introspection.patch": "https://issues.apache.org/jira/secure/attachment/12413729/fieldcache-introspection.patch",
        "LUCENE-1749.patch": "https://issues.apache.org/jira/secure/attachment/12413764/LUCENE-1749.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-07-16T19:37:22+0000",
            "content": "\nThe motivation for this issue is all of the changes coming in 2.9 in how Lucene internally uses the FieldCache API \u2013 the biggest change being per Segment sorting, but there may be others not immediately obvious.\n\nWhile these changes are backwards compatible from an API and functionality perspective, they could have some pretty serious performance impacts for existing apps that also use the FieldCache directly and after upgrading the apps suddenly seem slower to start (because of redundant FieldCache initialization) and require 2X as much RAM as they did before.  This could lead people people to assume Lucene has suddenly became a major memory hog.  SOLR-1111 and SOLR-1247 are some quick examples of the types of problems that apps could encounter.\n\nCurrently the only way for a User to even notice the problem is to do memory profiling, and the FieldCache data structure isn't the easiest to understand.  It would be a lot nicer to have some methods for doing this inspection programaticly, so users could write automated tests for incorrect/redundent usage. ",
            "author": "Hoss Man",
            "id": "comment-12732110"
        },
        {
            "date": "2009-07-16T19:41:02+0000",
            "content": "Here's the start of a patch to provide this functionality \u2013 it just provides a new method/datastructure for inspecting the cache; the sanity checking utility methods should be straightforward assuming people think this is a good idea.\n\nThe new method itself is fairly simple, but quite a bit of refactoring to how the caches are managed was necessary to make it possible to implement the method sanely.  These changes to the FieldCache internals seem like they are generally a good idea from a maintenance standpoint even if people don't like the new method. ",
            "author": "Hoss Man",
            "id": "comment-12732116"
        },
        {
            "date": "2009-07-16T19:43:19+0000",
            "content": "Technically this isn't a bug, so i probably shouldn't add it to the 2.9 blocker list, but i really think it would be a good idea to have something like this in the 2.9 release.\n\nAt the very least: i'd like to put it on the list until/unless there is consensus that it's not needed. ",
            "author": "Hoss Man",
            "id": "comment-12732117"
        },
        {
            "date": "2009-07-16T20:01:24+0000",
            "content": "nice - would be great if it could estimate ram usage as well. ",
            "author": "Mark Miller",
            "id": "comment-12732123"
        },
        {
            "date": "2009-07-16T21:01:20+0000",
            "content": "+1 \u2013 this'd be great to get into 2.9. ",
            "author": "Michael McCandless",
            "id": "comment-12732157"
        },
        {
            "date": "2009-07-16T21:12:28+0000",
            "content": "Looks good as a start, one question about a comment:\n\nWhat do you mean with:\n\n\t:TODO: is the \"int\" sort type still needed? ... doesn't seem to be used anywhere, code just tests \"custom\" for SortComparator vs Parser.\n\n\n\nI do not understand, do you want to remove the IntCache? What is different with it in comparison with the other ones?\n\nUwe ",
            "author": "Uwe Schindler",
            "id": "comment-12732166"
        },
        {
            "date": "2009-07-16T21:43:26+0000",
            "content": ":TODO: is the \"int\" sort type still needed? ... doesn't seem to be used anywhere, code just tests \"custom\" for SortComparator vs Parser.\n\nsorry ... badly placed quotes ... that was in referent to Entry.type. \n\nUntil i changed getStrings, getStringIndex, and getAuto to construct Entry objects as part of my refactoring the \"type\" attribute (and the constructor that takes a type argument) didnt' seem to be used anywhere (as far as i could tell)\n\nMy guess: maybe some previous changes refactored logic that switched on type up into the SortFields?, so the FieldCache no longer needs to care about it? ",
            "author": "Hoss Man",
            "id": "comment-12732190"
        },
        {
            "date": "2009-07-17T01:15:12+0000",
            "content": "Here is a start towards guessing the fieldcache ram usage.\n\nIt probably works fairly well, though it will be limited by stack space on a very heavily nested object graph.\n\nI've added the size guess for getValue in the introspection output.\n\nIts a start anyway. ",
            "author": "Mark Miller",
            "id": "comment-12732288"
        },
        {
            "date": "2009-07-17T01:32:08+0000",
            "content": "We prob would want to provide an alternate toString that includes the ram guess and the default that skips it - i havn't tested performance, but it might take a while to check a gigantic string array.\n\nAlso, JavaImpl should probably actually be JavaMemoryModel or MemoryModel. ",
            "author": "Mark Miller",
            "id": "comment-12732297"
        },
        {
            "date": "2009-07-22T01:38:00+0000",
            "content": "More progress building on Mark's patch.\n\nadded some sanity checking that reader/fieldname combos aren't reused in odd ways \u2013 i made it ignore cases where different parsers ultimately resolve to identical cache objects (ie null vs DEFAULT_LONG_PARSER) and it ignores any CreationPlaceholder objects (not sure about that one)\n\nsome tests were modified to make their pathological behavior more \"sane\" and hooks were addded so that future tests can bypass the sanity testing in the tearDown method if they really need to.\n\nStill need sanity testing of the Reader/subreader variety.  also lots of docs and code cleanup. \n\nBTW: i was focused on test-core ... still waiting on test-contrib to finish running, so i'm not yet sure if i broke anything there. ",
            "author": "Hoss Man",
            "id": "comment-12733945"
        },
        {
            "date": "2009-07-22T02:32:07+0000",
            "content": "note to self: of the contribs, TestRemoteSort had two failed tests (not horribly surprising) and PatternAnalyzerTest generated an Error (?!?!) ...\n\n\njava.lang.IllegalStateException: termText\n    at org.apache.lucene.index.memory.PatternAnalyzerTest.assertEquals(PatternAnalyzerTest.java:213)\n    at org.apache.lucene.index.memory.PatternAnalyzerTest.run(PatternAnalyzerTest.java:148)\n    at org.apache.lucene.index.memory.PatternAnalyzerTest.testMany(PatternAnalyzerTest.java:87)\n\n ",
            "author": "Hoss Man",
            "id": "comment-12733955"
        },
        {
            "date": "2009-07-23T01:52:08+0000",
            "content": "minor checkpoint: improved assert messages, and massaged TestRemoteSort so that it appearers more sane.\n\nproblem with PatternAnalyzerTest was unrelated (see LUCENE-1756) ",
            "author": "Hoss Man",
            "id": "comment-12734423"
        },
        {
            "date": "2009-07-23T02:40:43+0000",
            "content": "Hmmm... somehow i overlooked the fact that even after i \"fixed\" TestRemoteSort in the last patch, it's still failing. Here's the assertion failure...\n\njunit.framework.AssertionFailedError: testRemoteCustomSort Comparator: multi FieldCaches for same reader/fieldname with diff types\n   at org.apache.lucene.util.LuceneTestCase.assertSaneFieldCaches(LuceneTestCase.java:110)\n   at org.apache.lucene.search.TestRemoteSort.testRemoteCustomSort(TestRemoteSort.java:261)\n   at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:265)\n\n\n\n...and here's the debugging dump of the FieldCache...\n\n\n*** BEGIN testRemoteCustomSort Comparator: FieldCache Losers ***\n'org.apache.lucene.index.DirectoryReader@1108727'=>'custom',interface java.lang.Comparable,9,org.apache.lucene.search.SampleComparable$2@651e95,null=>[Ljava.lang.Comparable;#22056753 size guess:2 KB\n'org.apache.lucene.index.DirectoryReader@1108727'=>'custom',interface java.lang.Comparable,9,org.apache.lucene.search.SampleComparable$2@5b78cf,null=>[Ljava.lang.Comparable;#32045680 size guess:2 KB\n*** END testRemoteCustomSort Comparator: FieldCache Losers ***\n*** BEGIN org.apache.lucene.search.TestRemoteSort.testRemoteCustomSort: FieldCache Losers ***\n'org.apache.lucene.index.DirectoryReader@1108727'=>'custom',interface java.lang.Comparable,9,org.apache.lucene.search.SampleComparable$2@651e95,null=>[Ljava.lang.Comparable;#22056753 size guess:2 KB\n'org.apache.lucene.index.DirectoryReader@1108727'=>'custom',interface java.lang.Comparable,9,org.apache.lucene.search.SampleComparable$2@5b78cf,null=>[Ljava.lang.Comparable;#32045680 size guess:2 KB\n*** END org.apache.lucene.search.TestRemoteSort.testRemoteCustomSort: FieldCache Losers ***\n\n\n\nWhat really confuses me about this is that the same SampleComparable instance is being used with two different queries \u2013 once with reverse=true and once with reverse=false, yet two different SampleComparable instances are showing up in cache keys \u2013 the probably only happens when SampleComparable is used to get a SortComparator \u2013 not when it uses a ComparatorSource earlier in the test. \n\nIs this a real bug in remote sorting? ",
            "author": "Hoss Man",
            "id": "comment-12734433"
        },
        {
            "date": "2009-07-24T22:29:47+0000",
            "content": "Is this a real bug in remote sorting\n\nI think so.\n\nSortField is constructed on the client side and passed as a param to the remote Searchable. It seems you get a new factory every time this happens, not the client factory on the SortField (of course, because its constructed on the other side of the wire).\n\nSo every search call adds a new factory to the mix. If you just make the call once, the test will pass.\n\nIts a nice find. ",
            "author": "Mark Miller",
            "id": "comment-12735190"
        },
        {
            "date": "2009-07-24T22:53:18+0000",
            "content": "Here is a patch that just updates the ram usage estimator code. ",
            "author": "Mark Miller",
            "id": "comment-12735198"
        },
        {
            "date": "2009-07-25T04:24:24+0000",
            "content": "You know what would be absolute icing on the cake here would be some way during the introspection by some code looking for large sort fields that perhaps can be discarded/unloaded as needed (programmatically).\n\nWhat I'm thinking here is a use case we've come into where we have had to sort by subject.  Well the unique # subjects gets pretty large, and while we still need to support the use case, it'd be nice to be able to periodically 'toss' sort fields like this so they don't hog memory permanently while the IndexReader is still in memory.  (sorting by subject is used, just not often so a good candidate for tossing)\n\nBecause we have multiple large IndexReaders open concurrently, it'd be nice to be able to scan periodically and kick out any unneeded ones.\n\nIt's nice to be able to inspect and print out these, but even better if one can make changes based on what one finds.\n ",
            "author": "Paul Smith",
            "id": "comment-12735242"
        },
        {
            "date": "2009-07-28T21:21:47+0000",
            "content": "Mark: i have a little time to work on this today ... do you have any updates that youv'e been working on locally (i noticed some patch add/retract from you in hte history)\n\nPaul: over in LUCENE-831 there was a lot of discussion and work done towards making the entire FieldCAche internals pluggable so you could customize the cache behavior all sorts of ways ... i feel out of the loop on that issue, but my understanding is that it was pushed back to 3.1 at the earliest because it wasn't clear how the APIs should be setup given the work being done with reopen and with moving FieldCache usage down to the subreaders.\n\nfor now my goal with this issue (LUCENE-1749) is purely to provide an experimental  (ie: no back compat expectations) API for app developers to use to sanity check that the changes in 2.9 havne't blown their RAM usage sky high. ",
            "author": "Hoss Man",
            "id": "comment-12736304"
        },
        {
            "date": "2009-07-28T21:26:19+0000",
            "content": "I do - I removed that last patch because I just realized that it was missing everything but one class.\n\nGo ahead though - I'll merge with what you have. ",
            "author": "Mark Miller",
            "id": "comment-12736309"
        },
        {
            "date": "2009-07-28T21:32:06+0000",
            "content": "uh ... ok .. what kind of updates do you have locally?  (no point in merging later if i'm just going to write stuff you've already written) ",
            "author": "Hoss Man",
            "id": "comment-12736316"
        },
        {
            "date": "2009-07-28T21:35:01+0000",
            "content": "No worries, the updates are to the ram estimator and other minor things (eg if something fails the sanity check, the error output comes out twice because of the double check in teardown ) - nothing feature wise at the moment. I'll see what you add first. ",
            "author": "Mark Miller",
            "id": "comment-12736320"
        },
        {
            "date": "2009-07-29T00:57:40+0000",
            "content": "checkpoint: really hack implementation of checkFieldCacheSubReaderSanity that tells you when a FieldCache contains entries on the same field in a both wrapper/inner readers ... but it doesn't tell you which entries (and unlike checkFieldCacheTypeSanity it's no obvious just looking at the toString())\n\nThis patch causes an error in TestStressSort and LOTS of errors in TestCustomScoreQuery, TestFieldScoreQuery, and TestOrdValues.\n\nI'd like to think these errors are just from tests doing abnormal things, and in the case of TesStressSort that may be true (it looks like it has some hinky reuse of readers in a MultiReader) but in the other test cases where it's a little easier to see at a glance what's going on, there's really nothing odd here \u2013 simple use of a single IndexSearcher to execute a CustomScoreQuery, ValueSourceQuery, etc... these are each causing multiple FieldCache instances to pop up for a single field (one keyed on the DirectoryReader and another keyed on a CompoundFileReader$CSIndexInput)\n\ni'm try to work on refactoring the sanity checking methods so they are easier to use (and write some tests for them to prove the work as expected on both sane/insane) but it would be helpful if someone who understands more about how FieldCaches should look (pushed into the subreaders) could tyr out this patch and let me know if these failures look legit. ",
            "author": "Hoss Man",
            "id": "comment-12736399"
        },
        {
            "date": "2009-07-29T15:21:13+0000",
            "content": "checkpoint: refactored the sanity checking code into a utility class and wrote tests specifically for it to prove it finds insane stuff.\n\nTODO:\n\n\tclean up the api, make it less clunky (and not static)\n\t\n\t\treturn structured data showing exactly which combinations in FieldCache are insane\n\t\n\t\n\tjavadocs\n\tfigure out why previously mentioned tests are breaking (need help with this one ... don't know enough about the code these tests excercise)\n\n ",
            "author": "Hoss Man",
            "id": "comment-12736660"
        },
        {
            "date": "2009-07-29T18:13:45+0000",
            "content": "figure out why previously mentioned tests are breaking (need help with this one ... don't know enough about the code these tests excercise\n\nEh - its yucky. There are parts where the tests are passing the top level reader (say to a collector) when it should be using the sub readers. I fixed one \nBut then there is more - looked at a couple more difficult ones that also pass the top level reader for the test.\n\nAnd then there is explain - IndexSearcher passes the top level reader to the weight explain, and valuesourcequery will get a fieldcache based on that reader. I guess that one is a bug.\n\nAnd there are prob a few other similar type things... ",
            "author": "Mark Miller",
            "id": "comment-12736732"
        },
        {
            "date": "2009-07-29T18:54:12+0000",
            "content": "And then there is explain - IndexSearcher passes the top level reader to the weight explain, and valuesourcequery will get a fieldcache based on that reader. I guess that one is a bug.\n\nI don't even know what to do about this one. All I can think is that you pump out an explain for each sub reader - but thats pretty unhelpful.\n\nPerhaps the best we can do is javadoc the extra requirements that may be needed when you use explain? ",
            "author": "Mark Miller",
            "id": "comment-12736750"
        },
        {
            "date": "2009-07-29T23:17:16+0000",
            "content": "Updates:\n\n\n\tmerged in updated ram usage estimator code\n\tupdated most failing tests to work without creating top level FieldCaches\n\tremoved offending calls to explain - I left nocommit comments here - depending on what we decide, we could turn off the subreader check for these\n\tTurned off the subreader check for stress sort test - it sorts in back compat mode and compares to the new mode - so it loads both on purpose.\n\tI don't remember if I touched anything else.\n\n\n\ntests pass now ",
            "author": "Mark Miller",
            "id": "comment-12736903"
        },
        {
            "date": "2009-07-30T18:26:01+0000",
            "content": "Mark: thanks for looking into the tests.\n\nIf the CustomScoreQuery class(es) push the FieldCache sage into the subReaders during scoring, then shouldn't the explain methods do the same thing?  it definitely seems like a bug if getting score explanation from a query causes your memory footprint to double.\n\nLast night i thought over what a more useful API for hte sanity checker would like like ... \nMy power is getting turned off for a few hours this afternoon so i'll work on it them and should have a much cleaner looking patch to post this evening.\n\n(BTW: random thought that occurred to me last night: wouldn't the simplest way to implement the RamEstimator just be to use vanilla java serialization to a custom OutputStream that just counted the bytes and sent them to /dev/null) ? ",
            "author": "Hoss Man",
            "id": "comment-12737217"
        },
        {
            "date": "2009-07-30T19:35:06+0000",
            "content": "(BTW: random thought that occurred to me last night: wouldn't the simplest way to implement the RamEstimator just be to use vanilla java serialization to a custom OutputStream that just counted the bytes and sent them to /dev/null) ?\n\nThat's one way to go. Its got its own little issues though - some bookkeeping stuff is not serialized, and extra info about class, fields is serialzied. Transient fields (niche issue for sure) would also not be serialized. Its def another way to get an estimate. I chose a different route after considering both (googled the topic for a bit and looked at some examples before choosing). I'd be open to another route, but I thought this method was fairly fast, accurate, and generic. ",
            "author": "Mark Miller",
            "id": "comment-12737247"
        },
        {
            "date": "2009-07-30T19:48:51+0000",
            "content": "If the CustomScoreQuery class(es) push the FieldCache sage into the subReaders during scoring, then shouldn't the explain methods do the same thing? it definitely seems like a bug if getting score explanation from a query causes your memory footprint to double.\n\nIt should do the same thing - but thats sticky. If you push explain to the sub readers, you will get why it scored as it did for each subreader - not one top level explain. I won't deny its kind of bug - but I'm not sure at the moment what the best way to address it is. I'll look into the possibility of pushing the fieldcache access to the subreaders while leaving everything else at the top reader - I have no thoughts about the feasibility of that at the moment though. I guess it might be doable. ",
            "author": "Mark Miller",
            "id": "comment-12737254"
        },
        {
            "date": "2009-07-30T20:57:23+0000",
            "content": "Here is a rough draft for an explain fix.\n\nExplain for custom and valuesource now drop to per segment to retrieve fieldcache values. Resolving this issue will also resolve LUCENE-1771. ",
            "author": "Mark Miller",
            "id": "comment-12737284"
        },
        {
            "date": "2009-07-30T21:18:45+0000",
            "content": "This issue was a fantastic idea by the way! ",
            "author": "Mark Miller",
            "id": "comment-12737294"
        },
        {
            "date": "2009-07-31T05:43:06+0000",
            "content": "This is a complete overhaul of the internals of FieldCacheSanityChecker, and it's API so that it's a lot cleaner and easier to use programaticly.  \n\nThis also makes it easier for tests to run an analsis, and then ignore the types of errors they \"expect\" without ignoring whole cache entries (so a test that expects to have subreader problems can ignore those, even if one of those cache entires also fails a sanity check with another entry for a different reason (ie: different parser on same reader)\n\nAnd in the long run: this should make it easier for us to add new types of sanity checks (which i'm guessing we'll think of when the internals get overhauled) without changing the API too much.\n\nNOTE: This is based on Miller's attachment #12414960 (29/Jul/09) ... i haven't merged in or looked at any of the changes he made after that.  i suspect the only overlap (if any) is how CacheEntry uses the Ram Estimation code ... i switched to having estimateSize(RamUsageEstimator) cache the value and then toString uses it if it's there ... the FieldCacheSanityChecker takes care of calling it if the client code asks for it.\n\nMark: viv's got all weekend off, so i'm probably not going to have time to look at this again for 4 or 5 days, if you want to take a stab at merging the patches thta would be seriously awesome.\n ",
            "author": "Hoss Man",
            "id": "comment-12737423"
        },
        {
            "date": "2009-07-31T05:51:53+0000",
            "content": "Quick responses to some other comments...\n\nI chose a different route after considering both\n\ni trust you to make the right call, just thought i'd point it out in case you hadn't though of it.\n\nIf you push explain to the sub readers, you will get why it scored as it did for each subreader - not one top level explain\n\nI don't really follow you on this (i need to take a look at your proposed fix) .. i'm not suggesting we push the whole explain down to the subreader, just that when the explain method wants to get hte FieldCache value for a doc, it should fetch the FieldCache for the SegmentReader the doc is in \u2013 so it gets the exact same value (and same FieldCache entry) as the scoring code did when it scores the document.  (or maybe i'm completley missunderstanding how these classes were reimplimented to use segment based field caches)\n\nThis issue was a fantastic idea by the way!\n\nyeah ... i was pretty out of the loop on all the \"push sorting down into the segment\" discussion, but when i noticed yonik pointing out all the ways solr's fieldcache usage was going to explode if we didn't change it it occured to me that this would probably be a big problem for anyone doing non-trivial stuff with Lucene, so it would be nice to have a way to toruble shoot it (i also had very little faith in Lucene-Java's test coverage since we only have unit tests that verify \"correct\" behavior when we make changes \u2013 but nothing sanity checks how that behavior happened (at least: not untill now) ",
            "author": "Hoss Man",
            "id": "comment-12737424"
        },
        {
            "date": "2009-07-31T12:30:29+0000",
            "content": "I don't really follow you on this (i need to take a look at your proposed fix) .. i'm not suggesting we push the whole explain down to the subreader, just that when the explain method wants to get hte FieldCache value for a doc, it should fetch the FieldCache for the SegmentReader the doc is in - so it gets the exact same value (and same FieldCache entry) as the scoring code did when it scores the document. (or maybe i'm completley missunderstanding how these classes were reimplimented to use segment based field caches)\n\nThe way the per segment stuff went, we don't push down to the sub readers for the fieldcache per say - we just search each sub reader separately - so per reader fieldcache is just kind of a side effect. Then the top level reader is still used for things like stats and explain. \n\nI switched the explain for the offending stuff (custom/valuesource) to use a DocValues class that does push down to each subreader for the fieldcache though (while everything else still uses the top reader) - its in the scorer, so I added a switch to push down to subreaders for fieldcache access or not - only explain pushes  down, while regular scoring doesn't (regular scoring will be working per sub reader anyway, because they are searched one at a time). \n\nI can merge up the patches. ",
            "author": "Mark Miller",
            "id": "comment-12737509"
        },
        {
            "date": "2009-07-31T12:48:38+0000",
            "content": "the explain method wants to get hte FieldCache value for a doc, it should fetch the FieldCache for the SegmentReader the doc is in\n\nOne more note to try and be a bit more clear:\n\nFirst, I wasn't sure how easy this was to do because I don't know explain code or the function package very well at all (eg I've never used it). And the explain method itself did not grab values from the field cache, it loaded up a scorer that did so. So I just wasn't sure how doable this fix was. Thats why I was saying pushing down explain to the subreader wasn't great, but I wasn't sure what else you could do. The fix turned out to be fairly easy though - the scorer for valuesource just needed two modes - one for normal scoring and one for explain (that breaks up the requests for a fieldcache val per sub reader) - the explain method would work for both ways, but no reason to try and break down per reader when its going to score per reader anyway, so I have both. Standard scorer for valuesource works as it did, and explain trips a setting to break out subreaders and distrib fieldcache requests. And then the custom query needed a tweak to work right (flip that setting) with its underlying valuesource queries.\n ",
            "author": "Mark Miller",
            "id": "comment-12737517"
        },
        {
            "date": "2009-07-31T18:11:50+0000",
            "content": "I believe that ConstantScoreQuery will need it's explain() fixed too? ",
            "author": "Yonik Seeley",
            "id": "comment-12737626"
        },
        {
            "date": "2009-08-01T00:40:51+0000",
            "content": "In the case that it is a caching filter? I hadn't actually looked to see if there are any other FieldCache ones either - just what tripped the tests.\n\nI guess it could be dealt with the same way? A DocIdSet that distributes to sub readers ... ",
            "author": "Mark Miller",
            "id": "comment-12737792"
        },
        {
            "date": "2009-08-01T03:47:49+0000",
            "content": "\n\tmerged patches (and little tweaks to explain fix code)\n\tadded a MultiDocIdSet for handling constantscorequery explain - first draft - needs some thought. I just banged it out. Its got a couple simple tests.\n\n ",
            "author": "Mark Miller",
            "id": "comment-12737825"
        },
        {
            "date": "2009-08-01T04:32:55+0000",
            "content": "Already finding some corner cases with the multi docidset stuff - I'll keep working along those lines a bit, then maybe look at some of the code you have been working on and post another patch later this weekend. ",
            "author": "Mark Miller",
            "id": "comment-12737832"
        },
        {
            "date": "2009-08-01T05:03:50+0000",
            "content": "In the insanity check, when you drop into the sequential subreaders - I think its got to be recursive - you might have a multi at the top with other subs, or any combo thereof. I can add to next patch. ",
            "author": "Mark Miller",
            "id": "comment-12737834"
        },
        {
            "date": "2009-08-01T12:08:19+0000",
            "content": "This was an excellent idea, and it's great that it uncovered some\ndangerous and very unexpected places where we are passing top-level\nreader to the FieldCache (eg that explain() could suddenly populate\nthe FieldCache w/ top-level values is quite shocking!).\n\nReaderUtil.subSearcher is doing the same thing as\nDirectoryReader.readerIndex.\n\nI love the RAMUsageEstimator... we have other places that estimate RAM\n(eg IndexWriter does so for added & deleted docs) that we should\neventually cutover to this new API.\n\nI particularly love the new class named Insanity:\n\n\n  public static Insanity[] checkSanity(FieldCache cache)\n\n\n\nMultiDocIdSet/Iterator makes me a bit nervous, because it's further\n\"propogating\" a non-segment-based iterator deeper into Lucene than I\nthink we want to.  It's similar to eg using\nDirectoryReader.MultiTermDocs (what Lucene used to do), instead of\nstepping through the segments yourself.\n\nAlso, shouldn't explain most closely match what was done during\nsearching (ie, run \"per segment\")?  So simply pushing explain down to\nthe sub-reader that has the doc seems appropriate?  Ie we want it to\nshare as much of the code path as possible with how searching was in\nfact done?\n\nEG for ConstantScoreQuery.explain, it seems like we should 1) locate\nthe sub-reader that this doc falls in, and 2) get a scorer against\nthat reader, then 3) build up the explanation from that?  Likewise for\nCustomScoreQuery? \n\nIn fact.... maybe we should simply fix IndexSearcher.explain to do\nthis for all queries?  Ie, get the top-level weight, locate sub-reader\nthat has the doc, un-base the doc, and then invoke QueryWeight.explain\nwith that sub-reader and un-based doc?  Then we don't have to do\nanything special for each query.  I think QueryWeight.scorer()\nshouldn't be expected to handle a \"top level reader\" being passed in.\nIe, higher up in Lucene we should do that switch, so that we don't\nhave to do it (this \"valuesFromSubReaders\" arg) for every scorer.\n\nHmm: why do we even have explain at both the QueryWeight and Scorer\n\"levels\"?  It seems like we should pick one level and do it there,\nconsistently.  Most queries seem to only implement the QueryWeight one\nand often simply throw UOE in the Scorer's explain, but eg PhraseQuery\nimplements in both places.\n\n(BTW: I'll be offline for approx the next 36 hours or so!) ",
            "author": "Michael McCandless",
            "id": "comment-12737871"
        },
        {
            "date": "2009-08-01T12:40:32+0000",
            "content": "In fact.... maybe we should simply fix IndexSearcher.explain to do this for all queries?\n\nThat was my first thought... but it would probably break more than it helped right now (by exposing more limitations) - for example idf in TermWeight.explain() ",
            "author": "Yonik Seeley",
            "id": "comment-12737877"
        },
        {
            "date": "2009-08-01T14:33:57+0000",
            "content": "bq . Ie we want it to share as much of the code path as possible with how searching was in fact done\n\nWell of course  I was a bit hazy on explain, so for some reason I had it in my head that you would have to combine the explanations from multiple subreaders - but of course its a doc at a time, so the doc will only come from one subreader, and the sim/weight will be top level. So easy peasy fix. That boolean valuesFromSubReaders def had some code smell - just didn't have an alternative at the moment - fix then improve !\n\nI'll leave the 'explain at multiple levels' for another issue - I havn't even started thinking about this issue yet - I prefer to code  Which is kind of an oxymoron.\n\n\ni don't have the code in front of me, but i thought i was adding the sub\nreaders to the list it's iterating over, so it will eventually recurse all\nthe way to the bottom.\n\nAh right, sorry about the false alarm. One of the few times I've seen .size() in a for loop where its actually needed  ",
            "author": "Mark Miller",
            "id": "comment-12737889"
        },
        {
            "date": "2009-08-01T14:35:25+0000",
            "content": "changes to just go per reader for each doc - and a couple other unrelated tiny tweaks. ",
            "author": "Mark Miller",
            "id": "comment-12737890"
        },
        {
            "date": "2009-08-01T21:31:03+0000",
            "content": "Patch cleanup - more suitable for browsing. ",
            "author": "Mark Miller",
            "id": "comment-12737953"
        },
        {
            "date": "2009-08-02T03:27:23+0000",
            "content": "I was a bit hazy on explain, so for some reason I had it in my head that you would have to combine the explanations from multiple subreaders\n\nbut it would probably break more than it helped right now (by exposing more limitations) - for example idf in TermWeight.explain()\n\nTo be a little more clear - this was originally why I went the direction I did - I assumed the reader was being used for stats that needed to come from the top level reader. Gut reaction seeing it go into scorer. I hadn't really checked that, at least for these queries, that wasn't the case - they just use it for the filter/fieldcache. ",
            "author": "Mark Miller",
            "id": "comment-12737984"
        },
        {
            "date": "2009-08-02T17:40:40+0000",
            "content": "That was my first thought... but it would probably break more than it helped right now (by exposing more limitations) - for example idf in TermWeight.explain()\n\nUgh, you're right.\n\nI think It shouldn't be doing that?  Ie, a Weight instance should\n\"capture\" all stats needed from the top-level searcher, on creation,\nand then when we ask for a scorer or an explain (or other future\nthings that take an IndexReader) we should always pass in a single\nsegment reader.  This way we don't have to duplicate the \"go find the\nright sub-reader\" in many places.\n\nIt's interesting that we didn't (I think?) have a similar problem w/\nscorer when we switched to passing it the sub-reader.\n\nI'll leave the 'explain at multiple levels' for another issue\n\nIt looks like it's up to each query, which level does what.\nIndexSearcher's explain always calls Weight.explain, but then some\nQuery impls (eg BooleanQuery) do everything in Weight.explain, while\nothers (eg TermQuery, PhraseQuery) do some work in Weight.explain and\nsome in the scorer.\n\nI guess this makes sense: \"atomic\" Queries (TermQuery, PhraseQuery)\nwill need to fire up a scorer since there's real work to be done to\nsee the specifics of how that doc was matched.  Whereas BooleanQuery simply\n\"glues\" together other queries so it doesn't need to forward to its\n[many] scorers.\n\nSo the only odd thing is why explain is part of Scorer base\nclass... seems like the method could/should live \"privately\" to only\nthose queries that need it.\n\nBut I agree let's leave this be for now... ",
            "author": "Michael McCandless",
            "id": "comment-12738100"
        },
        {
            "date": "2009-08-02T22:27:22+0000",
            "content": "It's interesting that we didn't (I think?) have a similar problem w/ scorer when we switched to passing it the sub-reader.\n\nRight - that code was well tested and exercised via MultiSearcher in the past (all idf values had to come from Weight to avoid getting idfs per sub-searcher).\nOne thing that's missing for explain() is that there is no way to get \"df\" as opposed to \"idf\" from the Weight.\n\nSo the only odd thing is why explain is part of Scorer base class\nRight.. it doesn't belong there.  Perhaps deprecate and remove from the Scorer base in 3.0? (since one can't reliably call it now anyway). ",
            "author": "Yonik Seeley",
            "id": "comment-12738140"
        },
        {
            "date": "2009-08-04T00:14:56+0000",
            "content": "I've got one more draft here with the smallest of tweaks - javadoc spelling errors, and one perhaps one or two other tiny things - stuff I just would toss out rather than merge - but are you doing anything here right now Hoss? I think not at the moment, so if thats the case I'll put up one more patch before you grab the conch back. Otherwise I'll hold off on anything till you put something up. ",
            "author": "Mark Miller",
            "id": "comment-12738698"
        },
        {
            "date": "2009-08-04T00:57:20+0000",
            "content": "\n\n: I've got one more draft here with the smallest of tweaks - javadoc \n: spelling errors, and one perhaps one or two other tiny things - stuff I \n: just would toss out rather than merge - but are you doing anything here \n: right now Hoss? I think not at the moment, so if thats the case I'll put \n: up one more patch before you grab the conch back. Otherwise I'll hold \n: off on anything till you put something up.\n\n\nyou have the conch ... i haven't worked on anything related to this issue \nsince my last patch.\n\ni'll try to look at it again tomorow.\n\n\n\n-Hoss\n ",
            "author": "Chris Hostetter (Unused)",
            "id": "comment-12738721"
        },
        {
            "date": "2009-08-04T18:26:59+0000",
            "content": "Right - that code was well tested and exercised via MultiSearcher in the past (all idf values had to come from Weight to avoid getting idfs per sub-searcher).\n\nAhh right.\n\nOne thing that's missing for explain() is that there is no way to get \"df\" as opposed to \"idf\" from the Weight.\n\nBut this only affects the \"atomic\" queries, right?  So eg TermWeight\ncould simply hold onto this value and then use it during explain.\nHmm... though TermQuery's ctor doesn't get the df directly, because it\ncalls similarity.idf(term, searcher).  I don't really like making a\nseparate additional call to docFreq.\n\nHow about, for queries that need to go and look up docFreq, their\nQueryWeight impls simply hold onto the [top-level] IndexSearcher that\nhad been passed to their ctor, and then do the docFreq call against\nthat, if explain is invoked?\n\nRight.. it doesn't belong there. Perhaps deprecate and remove from the Scorer base in 3.0? (since one can't reliably call it now anyway).\n\n+1\n\nHoss/Mark do you want to fold it in to the patch, here?  Or I can open\na new issue? ",
            "author": "Michael McCandless",
            "id": "comment-12739080"
        },
        {
            "date": "2009-08-04T18:46:52+0000",
            "content": "\nHow about, for queries that need to go and look up docFreq, their QueryWeight impls simply hold onto the [top-level] IndexSearcher that\nhad been passed to their ctor, and then do the docFreq call against\nthat, if explain is invoked?\n\nAsking the searcher for the docFreq is the right thing to do... but people who rely on Weight being serializable might be in for a nasty surprise.\nOf course... one might wonder if we should bother supporting serializable in Lucene longer term at all - anyone dealing with distributed systems has found it to have too many shortcomings anyway. ",
            "author": "Yonik Seeley",
            "id": "comment-12739089"
        },
        {
            "date": "2009-08-04T18:57:50+0000",
            "content": "General Comments on mark's latest patch...\n\n\tthe changes that i understand all seem good ... some of the details in reader/searcher/query internals elude me but it sounds Yonik & McCandless have their eyes on them so i trust the three of you have it covered.\n\twe still need to fill in some empty/sparse javadocs, but that can be done after an initial commit.\n\tis it a bug that AverageGuessMemoryModel.getSize() will NPE on a non primitive class ... or should/will the docs for that API say it only works on primitives?\n\n\n\nBig Questions I Still Have....\n\n\tdoes anyone have any reservations about the new APIs introduced?\n\tFieldCache.CreationPlaceholder (promoted from FieldCacheImpl)\n\tFieldCache.CacheEntry\n\tFieldCache.getCacheEntries()\n\tFieldCache.purgeAllCaches()\n\tFieldCacheSanityChecker\n\tRamUsageEstimator\n\tdoes anyone have any reservations about the refactoring done in FieldCacheImpl to make this new API possible? (ie: did i break the thread safety in a way i'm not noticing?)\n\tis the FieldCacheImpl.Entry.type (the \"SortField\" int type) still needed by FieldCacheImpl.Entry? ... nothing seems to use it so it would be nice to eliminate it and simplify the CacheEntry API as well.  (i suspect it got refactored into obsolescence when the Sorting got moved into the subreaders)\n\tThe sanity checking ignores CreationPlaceholder \u2013 largely because of the way the numeric caches first try one parser, and then if they get an NFE try a different parser \u2013 but this leaves the CreationPlaceholder in the cache.  It's not a big object, so i assume it was implemented this way on purpose and the sanity checker is doing the correct thing by ignoring it, but i wanted to make sure people are aware/ok with this behavior.\n\n\n\nLastly: This patch feels unnecessarily large at this point.  Several of the bugs/improvements we've uncovered here don't seem like belong in this patch, and should be tracked in separate Jira issues, which can be committed independently and enumerated in CHANGES.txt....\n\n\tnew ReaderUtil and the usage in DirectoryReader, MultiReader, MultiSearcher & IndexSearcher\n\texplain subreader bug fixes in ConstantScoreQuery, QueryWeight, ValueSourceQuery, CustomScoreQuery, etc...\n...i think this issue (and this patch) should be reduced to just the new sanity checkig API, and tests that have been changed to be more sane (where the underlying code was already fine)\n\n\n\nMark: would you mind splitting up the latest patch you have (you mentioned some additional minor tweaks) and opening new issues for these peripheral changes and then attaching back what's left for this patch.  Then I'll take the conch back and work on the missing javadocs.\n\n(I'll happily commit once i get at least one thumbs up from someone on the \"Big Questions\" above ... we can always tweak the javadocs further in subsequent commits)\n ",
            "author": "Hoss Man",
            "id": "comment-12739092"
        },
        {
            "date": "2009-08-04T19:00:02+0000",
            "content": "Hoss/Mark do you want to fold it in to the patch, here? Or I can open a new issue?\n\nas i alluded to above, i'm in favor of individual issues for each \"bug\" uncovered by this issue so they can be tracked separately. ",
            "author": "Hoss Man",
            "id": "comment-12739093"
        },
        {
            "date": "2009-08-04T19:03:41+0000",
            "content": "Mark: would you mind splitting up the latest patch you have (you mentioned some additional minor tweaks) and opening new issues for these peripheral changes and then attaching back what's left for this patch. \n\nI've already got separate issues and patches up were it makes sense (not the last one Mike mentions) - I wanted to keep them here too though until the insanity tests were complete - the tests that the fixes are somewhat correct are in this patch, and I don't like to manage layers of patches. if we don't plan on doing anymore with the insanity tests here though, I'll spin them out of this patch now.\n\nI'll put up one more version and then you can have it back. ",
            "author": "Mark Miller",
            "id": "comment-12739097"
        },
        {
            "date": "2009-08-04T19:05:38+0000",
            "content": "Asking the searcher for the docFreq is the right thing to do... but people who rely on Weight being serializable might be in for a nasty surprise.\n\nIf we decide not to ask weight to hang onto it's searcher, then the other way to do it right is to change explain() to accept a Searcher as well as a IndexReader. ",
            "author": "Yonik Seeley",
            "id": "comment-12739098"
        },
        {
            "date": "2009-08-04T19:15:25+0000",
            "content": "is it a bug that AverageGuessMemoryModel.getSize() will NPE on a non primitive class ... or should/will the docs for that API say it only works on primitives?\n\nIts only meant to work with primitives. I'll change the name to getPrimitiveSize - on my last pass through, I'll also review the javadoc for the classes I added. ",
            "author": "Mark Miller",
            "id": "comment-12739108"
        },
        {
            "date": "2009-08-04T19:54:33+0000",
            "content": "\nAsking the searcher for the docFreq is the right thing to do... but people who rely on Weight being serializable might be in for a nasty surprise.\n\nArgh, right.\n\nIf we decide not to ask weight to hang onto it's searcher, then the other way to do it right is to change explain() to accept a Searcher as well as a IndexReader.\n\n+1\n\nOf course... one might wonder if we should bother supporting serializable in Lucene longer term at all - anyone dealing with distributed systems has found it to have too many shortcomings anyway.\n\nYeah this was never really \"settled\" in LUCENE-1473.  Lucene currently\nsupports live serialization, but not cross-version\nserialization... and we have moved RemoteSearchable to contrib and\nremoved RMI from Searchable.\n\nDoes Solr ever rely on Lucene's \"implements Serializable\"? ",
            "author": "Michael McCandless",
            "id": "comment-12739120"
        },
        {
            "date": "2009-08-04T19:56:04+0000",
            "content": "\nHoss/Mark do you want to fold it in to the patch, here? Or I can open a new issue?\n\nas i alluded to above, i'm in favor of individual issues for each \"bug\" uncovered by this issue so they can be tracked separately.\n\nOK I'll open a new issue for this one (deprecate Scorer.explain). ",
            "author": "Michael McCandless",
            "id": "comment-12739122"
        },
        {
            "date": "2009-08-04T21:01:10+0000",
            "content": "Does Solr ever rely on Lucene's \"implements Serializable\"?\n\nNope - itra-node communications use the same mechanism as clients... a generic data structures (Map,List,Document,etc) that has custom serialization to XML,JSON,Python,Ruby or Binary (and binary is now used by default). ",
            "author": "Yonik Seeley",
            "id": "comment-12739163"
        },
        {
            "date": "2009-08-05T20:54:21+0000",
            "content": "patch is coming soon - I've merged to trunk and pulled the separate issues - just want to look over some a bit later. Would have had this sooner, but Eclipse decided to start crashing every 5 minutes this morning because firefox brought in a new xulrunner and ... ugg - at least its not Windows ... coming though. ",
            "author": "Mark Miller",
            "id": "comment-12739743"
        },
        {
            "date": "2009-08-06T13:01:08+0000",
            "content": "I still havn't looked at this in the detail that I want to, but time is my enemy at the moment, so take it back and at least you can finish up what you have planned.\n\nHopefully its all in good working order after all the extracting and to trunking - let me know if you see anything off and I'll spin another.  ",
            "author": "Mark Miller",
            "id": "comment-12740039"
        },
        {
            "date": "2009-08-06T17:16:27+0000",
            "content": "P.S. I'm not sure we want to go with the way I have changed the tests here.\n\nI switched things to go per subreader rather than use the overall reader - this is how things happen in IndexSearcher now. But we lose the top level reader test. We might want to do it both ways, and when doing it by top reader, ignore the triggered insanity check? ",
            "author": "Mark Miller",
            "id": "comment-12740155"
        },
        {
            "date": "2009-08-06T21:33:00+0000",
            "content": "Mark: I'll start working on improving the docs (and other things from my previous todo list)\n\nP.S. I'm not sure we want to go with the way I have changed the tests here.\n\nAre you talking about TestOrdValues and TestFieldScoreQuery ?\n\nif we expect OrdValues and FieldScoreQuery to use subReader based field caches, then the test seems to be doing the correct thing (in your patch) .. inspecting the fieldcaches per subreader.  Is there a code path where we expect those methods to get called on a MultiReader?\n\n(Actually: that seems like a wroth while improvement to make to these classes: a MultiDocValues impl that all of the getValues(IndexReader) methods use when passed a MultiReader ... it uses getSequentialSubReaders to construct DocValue instances for each so you don't get FieldCache expolsions if code inadvertenly passes the wrong reader to getValues.  What do you think? ... new issue?) ",
            "author": "Hoss Man",
            "id": "comment-12740256"
        },
        {
            "date": "2009-08-06T21:45:40+0000",
            "content": "Hmmmm...\n\nactually mark, testing our your latest patch against hte trunk i'm seeing (FieldCache sanity) failures from TestCustomScoreQuery, TestFieldScoreQuery, TestOrdValues, and TestSort ... have you seen these?  did some other recent change on the trunk trigger these? ",
            "author": "Hoss Man",
            "id": "comment-12740265"
        },
        {
            "date": "2009-08-06T22:04:56+0000",
            "content": "I think that TestCustomScoreQuery, TestFieldScoreQuery, and TestOrdValues all fail because the fix for them is now in another issue.\n\nTestSort I didn't notice. It looks like its considering String[] and StringIndex the same for the two multi and parallel sort tests - merged to trunk, so perhaps something has gone awry there? I've looked over the patch and I don't see any obvious mistake - I don't know that I have time to dig more now, but since you are more familiar with that code anyway, perhaps you can tell me why its now considering them the same anyway? Otherwise I will look more before too long. ",
            "author": "Mark Miller",
            "id": "comment-12740272"
        },
        {
            "date": "2009-08-06T22:06:50+0000",
            "content": "Here is the output - it appears to think String[] and StringIndex are both string:\n\nVALUEMISMATCH: Multiple distinct value objects for org.apache.lucene.index.CompoundFileReader$CSIndexInput@56d73c7a+string\n\t'org.apache.lucene.index.CompoundFileReader$CSIndexInput@56d73c7a'=>'string',class org.apache.lucene.search.FieldCache$StringIndex,3,null,null=>org.apache.lucene.search.FieldCache$StringIndex#279807577 (size =~ 152 bytes)\n\t'org.apache.lucene.index.CompoundFileReader$CSIndexInput@56d73c7a'=>'string',class java.lang.String,11,null,null=>[Ljava.lang.String;#647057258 (size =~ 108 bytes) ",
            "author": "Mark Miller",
            "id": "comment-12740275"
        },
        {
            "date": "2009-08-06T22:12:17+0000",
            "content": "(Actually: that seems like a wroth while improvement to make to these classes: a MultiDocValues impl that all of the getValues(IndexReader) methods use when passed a MultiReader ... it uses getSequentialSubReaders to construct DocValue instances for each so you don't get FieldCache expolsions if code inadvertenly passes the wrong reader to getValues. What do you think? ... new issue?)\n\nVery interesting idea - def a new issue I think. Not sure its worth it if you can't protect general fieldcache access as well though ... ",
            "author": "Mark Miller",
            "id": "comment-12740278"
        },
        {
            "date": "2009-08-06T23:04:27+0000",
            "content": "checkpoint: no functional change from mark's previous patch, just improved all the javadocs, including explanation of SanityCheckers purpose and experimental/expert warnings where appropriate. ",
            "author": "Hoss Man",
            "id": "comment-12740305"
        },
        {
            "date": "2009-08-06T23:24:01+0000",
            "content": "Okay, sorry - I messed up when merging with trunk.\n\nIn TestSort you had moved the local sorting to the bottom in the multi sort test - I kept that, but I also kept them higher up. So thats the fail - they just have to be removed.\n\nLine 953-957 it looks - sorry bout that - just didn't notice it fail with the other 3. ",
            "author": "Mark Miller",
            "id": "comment-12740308"
        },
        {
            "date": "2009-08-06T23:32:47+0000",
            "content": "I think that TestCustomScoreQuery, TestFieldScoreQuery, and TestOrdValues all fail because the fix for them is now in another issue.\n\nah ... are you talking about LUCENE-1771 ? (the jira dependency sems backwards in that case)\n\nIn TestSort you had moved the local sorting to the bottom in the multi sort test - I kept that, but I also kept them higher up. So thats the fail - they just have to be removed.\n\nyeah .. i just caught that and was starting to reply ... the interestingthing is that the CacheEntry.toString() doesn't show the Local.US was used when getting the Strings[] FieldCache. .. i'm currently trying to figure out why (because that could confuse people as well) ",
            "author": "Hoss Man",
            "id": "comment-12740311"
        },
        {
            "date": "2009-08-06T23:36:11+0000",
            "content": "Right - I set that up when the code was in this issue - reversed now! ",
            "author": "Mark Miller",
            "id": "comment-12740314"
        },
        {
            "date": "2009-08-06T23:46:50+0000",
            "content": "\nthe interestingthing is that the CacheEntry.toString() doesn't show the Local.US was used when getting the Strings[] FieldCache\n\nI'm an idiot ... the Locale isn't used like a FieldCache Parser ... the same String[] is used regardless of the Localed, so it's never part of the CacheKey.  the output is correct.\n\nrevised patch fixes TestSort as mark pointed out, and updates some javadocs where i missleading suggested different Locales might trigger InsanityType.VALUEMISMATCH ",
            "author": "Hoss Man",
            "id": "comment-12740317"
        },
        {
            "date": "2009-08-07T01:05:35+0000",
            "content": "confirmed that patch in LUCENE-1771 fixes the remaining broken tests in this issue. ",
            "author": "Hoss Man",
            "id": "comment-12740341"
        },
        {
            "date": "2009-08-07T16:36:14+0000",
            "content": "Maybe we should simply print a warning, eg to System.err, on detecting that 2X RAM usage has occurred, pointing people to the sanity checker?  We could eg do it once only so we don't spam the stderr logs... ",
            "author": "Michael McCandless",
            "id": "comment-12740624"
        },
        {
            "date": "2009-08-10T18:37:46+0000",
            "content": "Maybe we should simply print a warning, eg to System.err, on detecting that 2X RAM usage has occurred, pointing people to the sanity checker? We could eg do it once only so we don't spam the stderr logs\n\nI'm not really comfortable dumping anything to System.err without user requesting it ... but this is a really interesting idea.  (I suppose we could add an infoStream type idea to FieldCache to expose this)\n\nFieldCacheImpl.Cache.get could use the FieldCacheSanityChecker to inspect itself immediately after calling createValue, and could even test if any of the Insanity instances returned are related to the current call (by comparing the CacheEntry with the Entry it's using) ... it could even log a useful stack trace since the sanity check would be happening in the same call stack as at least one of the CacheEntries in the Insanity object.\n\nI've opened LUCENE-1798 to track implmenting somehting like this once the FieldCacheSanityChecker gets committed. ",
            "author": "Hoss Man",
            "id": "comment-12741479"
        },
        {
            "date": "2009-08-11T06:16:52+0000",
            "content": "slightly revised patch based on java-dev@lucene discussion...\n\nthe sortFieldTYpe and Locale portions of Cache.Entry are never used by FieldCache \u2013 just a deprecated class that abuses the Entry api out of lazyiness... so the CacheEntry debugging abstraction shouldn't expose them (but i left in code to manifest them in the toString() if they are atypical just in case).  Also added some deprecation notices so we remember to remove them once they are no longer needed.\n ",
            "author": "Hoss Man",
            "id": "comment-12741707"
        },
        {
            "date": "2009-08-12T17:46:32+0000",
            "content": "updated patch to trunk (QueryWeight->Weight) and tweaked some FieldCacheImpl methods to use the non-deprecated Entry constructors (forgot that part before)\n\nI'll commit as soon as my test run is finished. ",
            "author": "Hoss Man",
            "id": "comment-12742476"
        },
        {
            "date": "2009-08-12T19:29:30+0000",
            "content": "one last updated: the Locale.US asserts in TestRemoteSort had the same problem as TestSort, they were suppose to be moved, but instead they were just copied (not sure how i missed that before) ",
            "author": "Hoss Man",
            "id": "comment-12742535"
        },
        {
            "date": "2009-08-12T19:32:13+0000",
            "content": "Committed revision 803676. ",
            "author": "Hoss Man",
            "id": "comment-12742537"
        }
    ]
}