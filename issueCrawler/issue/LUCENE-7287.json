{
    "id": "LUCENE-7287",
    "title": "New lemma-tizer plugin for ukrainian language.",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [
            "modules/analysis"
        ],
        "labels": "",
        "fix_versions": [
            "6.2",
            "7.0"
        ],
        "priority": "Minor",
        "status": "Closed",
        "type": "New Feature"
    },
    "description": "Hi all,\n\nI wonder whether you are interested in supporting a plugin which provides a mapping between ukrainian word forms and their lemmas. Some tests and docs go out-of-the-box =) .\n\nhttps://github.com/mrgambal/elasticsearch-ukrainian-lemmatizer\n\nIt's really simple but still works and generates some value for its users.\n\nMore: https://github.com/elastic/elasticsearch/issues/18303",
    "attachments": {
        "Screen Shot 2016-06-23 at 8.41.28 PM.png": "https://issues.apache.org/jira/secure/attachment/12812884/Screen%20Shot%202016-06-23%20at%208.41.28%20PM.png",
        "Screen Shot 2016-06-23 at 8.23.01 PM.png": "https://issues.apache.org/jira/secure/attachment/12812882/Screen%20Shot%202016-06-23%20at%208.23.01%20PM.png",
        "LUCENE-7287.patch": "https://issues.apache.org/jira/secure/attachment/12811703/LUCENE-7287.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15293384",
            "author": "Michael McCandless",
            "date": "2016-05-20T13:41:23+0000",
            "content": "Thanks Dmytro Hambal, this sounds nice!  The license is MIT license which is compatible with ASL (good!).\n\nI looked very briefly and it looks like there is a large (~94 MB) data file that is loaded into heap ... where did this data/dictionary come from?  And, once loaded into heap, how much heap does it consume?  Seems like it could be very high (it's loaded as a HashMap<String,String> I think?).  E.g. our hunspell token filter works hard to use a compact in-heap representation, and does also support Ukrainian I believe. "
        },
        {
            "id": "comment-15298470",
            "author": "Andriy Rysin",
            "date": "2016-05-24T16:43:30+0000",
            "content": "Quick check via jvisualvm shows ~400MB used by the dictionary map. The dictionary originally is coming from https://github.com/arysin/dict_uk, that project developed from Ukrainian hunspell dictionary (which was very compact: on average each hunspell flag was producing 12 words) but diverged a bit and now the system of affixes in dict_uk is not compatible with that in hunspell.\nI have on my TODO to add a convertor to produce hunspell dictionary from dict_uk sources. If that helps here (I'm not familiar with hunspell token filter in lucene) I could put it a bit higher in my priority. "
        },
        {
            "id": "comment-15298505",
            "author": "Dmytro Hambal",
            "date": "2016-05-24T16:59:19+0000",
            "content": "Michael McCandless speaking of this data file, we had an idea to keep it in DAWG format which should have taken like ~8 MB.\nBut at the moment we didn't find correct realisation which could handle both map-like interface and an ability to load the data from files. More details here: https://github.com/mrgambal/elasticsearch-ukrainian-lemmatizer/issues/1 "
        },
        {
            "id": "comment-15298624",
            "author": "Michael McCandless",
            "date": "2016-05-24T18:00:02+0000",
            "content": "The dictionary originally is coming from https://github.com/arysin/dict_uk\n\nAlas that project is distributed under the GPL license, which we cannot use here.  Do you have an alternative dictionary source that has a more reasonable license? "
        },
        {
            "id": "comment-15298680",
            "author": "Andriy Rysin",
            "date": "2016-05-24T18:32:08+0000",
            "content": "There's no alternative open dictionary for Ukrainian with acceptable quality (I know since I've been working on it for last 10 years ).\nBut I can relicense the https://github.com/arysin/dict_uk or the derivatives under MIT if it helps. "
        },
        {
            "id": "comment-15299149",
            "author": "Michael McCandless",
            "date": "2016-05-24T23:34:33+0000",
            "content": "There's no alternative open dictionary for Ukrainian with acceptable quality (I know since I've been working on it for last 10 years ).\n\nOK thanks Andriy Rysin ... it looks like all Ukrainian dictionaries I can find, lead back to you!\n\nRelicensing your data files (and maybe also the hunspell dictionaries) to ASL2 or MIT or BSD would be wonderful, if you are able/allowed to!\n\nI think we need to understand how your approach differs from the Hunspell tokenizer Lucene already provides.\n\nSee https://lucene.apache.org/core/6_0_0/analyzers-common/org/apache/lucene/analysis/hunspell/HunspellStemFilter.html for some details, and e.g. http://github.com/elastic/hunspell for how it's integrated into ES.  This is all quite new to me and I don't have an appreciation for what the differences are, in tokenization accuracy, heap used, tokens per second processing, etc.  I know Robert Muir spent quite a bit of time trying to keep heap usage low and tokenization performance high on the original hunspell issues. "
        },
        {
            "id": "comment-15299381",
            "author": "Ahmet Arslan",
            "date": "2016-05-25T03:10:34+0000",
            "content": "This looks like a wrapper for string to string mapping. No need to roll a custom Lucene code for this: Just replace comma with tab in the mapping_sorted.csv file and use good old StemmerOverrideFilter, which has the fast lookup that does not require termAtt.toString() conversion. "
        },
        {
            "id": "comment-15304000",
            "author": "Andriy Rysin",
            "date": "2016-05-27T12:33:30+0000",
            "content": "So do we need to build hunspell dictionary (this may take me some time, probably a week or two) or using StemmerOverrideFilter with existing dictionary as suggested by Ahmet is good enough?\nBTW older Ukrainian hunspell used in http://github.com/elastic/hunspell is not very suitable as it's \"too compact\" - it often combines multiple lemmas together (most frequently direct and reverse verbs, adjectives and adverbs etc). "
        },
        {
            "id": "comment-15304009",
            "author": "Andriy Rysin",
            "date": "2016-05-27T12:39:52+0000",
            "content": "BTW how does hunspell stemming works for \"exceptions\"? There are bunch of words in Ukrainian whose inflections is hard to put in hunspell affix rules. "
        },
        {
            "id": "comment-15306770",
            "author": "Dmitry Chaplinsky",
            "date": "2016-05-30T15:41:58+0000",
            "content": "I really want this project to happen.\n\nAhmet Arslan, Michael McCandless, is there anything I can do to help? "
        },
        {
            "id": "comment-15306947",
            "author": "Andriy Rysin",
            "date": "2016-05-30T21:00:52+0000",
            "content": "From my point of view we can use dict_uk as a source for lucene (and we can provide acceptable license). The question is whether we need hunspell data with affixes that are based on lemmas (a bit more work) or we can get away with flat file as suggested by Ahmet Arslan (this we can do pretty quickly). "
        },
        {
            "id": "comment-15317807",
            "author": "Andriy Rysin",
            "date": "2016-06-07T04:16:30+0000",
            "content": "I just realized that Lucene includes morfologik analyzer (https://github.com/apache/lucene-solr/blob/master/lucene/analysis/morfologik/src/java/org/apache/lucene/analysis/morfologik/MorfologikAnalyzer.java). We already use the Ukrainian dictionary in morfologik format for LanguageTool (https://github.com/languagetool-org/languagetool/blob/master/languagetool-language-modules/uk/src/main/resources/org/languagetool/resource/uk/ukrainian.dict).\nIt's about 1.6MB in file and should be quite fast and memory efficient. "
        },
        {
            "id": "comment-15318587",
            "author": "Michael McCandless",
            "date": "2016-06-07T14:38:56+0000",
            "content": "That sounds like a great solution Andriy Rysin!  Would that give the same functionality as your original plugin sources?  Users can do this today, just by using Morfologik with a custom (your) dictionary? "
        },
        {
            "id": "comment-15323800",
            "author": "Andriy Rysin",
            "date": "2016-06-10T03:21:39+0000",
            "content": "Ok, I've imported lucene-sorl and the Ukrainian analyzer project from Dmytro Hambal into Eclipse and looked through the code.\nUnfortunately we can't use the whole morfologik package as is - it's very specific for Polish. We could still probably use part of morfologik for compact dictionary representation. The whole Ukrainian dictionary in this format with POS tags is ~1.6MB compared to 98M in csv and we could probably make it smaller if we strip the tags.\nThere are several things I'd like to note:\n1) this dictionary is for inflections (not related words) so this stemming will be producing lemmas not quite root words (this is probably ok and in some cases even better?)\n2) as this is dictionary-based stemming it won't stem unknown words (but dictionary contains ~200K lemmas so it should give good output)\n3) as Ukrainian has high level of inflection (nouns produce up to 7 forms, direct verbs up to 20, reverse verbs up to 30 forms) with many rules and exceptions developing quality rule-base stemming will not be trivial\n4) I was planning to work on Ukrainian analyzer in a separate project but if it's better for the review process I can fork lucene-solr and work inside the fork\n5) I am thinking to create org.apache.lucene.analysis.uk classes based on Dmytro Hambal's work and the csv file we have and once it's working try more compact representation\n\nThe question: once we have it working shall we include the dictionary in the lucene project or make it an external dependency (like with morfologik-polish.jar)? First is simpler but second will allow easy updates for the dictionary (which I can see being actively developed for another year or two) and also will keep the binary blob out of the project. I am leaning towards second but open for discussion.\n "
        },
        {
            "id": "comment-15324651",
            "author": "Michael McCandless",
            "date": "2016-06-10T15:39:05+0000",
            "content": "Thanks for the detailed analysis Andriy Rysin!  On where the dictionary lives, I think option 2 is good?\n\nOn #4, whichever is best for you! "
        },
        {
            "id": "comment-15326066",
            "author": "Andriy Rysin",
            "date": "2016-06-11T22:29:15+0000",
            "content": "Ok, guys, I've created little project with Ukrainian analyzer for lucene using MorfologikAnalyzer: https://github.com/arysin/lucene_uk\nThe test (https://github.com/arysin/lucene_uk/blob/master/src/test/java/org/apache/lucene/analysis/uk/TestUkrainianAnalyzer.java) runs successfully inside lucene but I can't run it in my project (getting NPE at RunListenerPrintReproduceInfo.java:131).\nI can run simple standalone test app though with no problem: https://github.com/arysin/lucene_uk/blob/master/src/test/java/org/lucene_uk/test/LuceneTest.java\nFor simplicity for now I just included Ukrainian binary morfologik dictionary in the project itself. The only currently published artifact with Ukrainian dictionary is http://mvnrepository.com/artifact/org.languagetool/language-uk but it requires languagetool-core and dragging it into lucene probably does not make sense. If the PoC is good enough I can take a shot at creating separate artifact with just a dictionary (this may take some time) or we can just live with the blob in lucene.\n\nI would appreciate if you can take a look and let me know how it looks. If it's acceptable I would need to work on including some of the goodies from Dmytro's project: handling different apostrophes and ignoring accent character. "
        },
        {
            "id": "comment-15326624",
            "author": "Andriy Rysin",
            "date": "2016-06-12T20:39:16+0000",
            "content": "I've added a token filter for unicode apostrophes and stress symbol. "
        },
        {
            "id": "comment-15326628",
            "author": "Ahmet Arslan",
            "date": "2016-06-12T20:58:46+0000",
            "content": "May be MappingCharFilter could be used instead of a token filter? "
        },
        {
            "id": "comment-15326753",
            "author": "Andriy Rysin",
            "date": "2016-06-13T02:44:20+0000",
            "content": "Thanks for the hint, I've changed the code to use MappingCharFilter.\nIt's slightly slower but architecturally more correct. "
        },
        {
            "id": "comment-15333888",
            "author": "Andriy Rysin",
            "date": "2016-06-16T14:25:32+0000",
            "content": "Michael McCandless, Ahmet Arslan does this implementation look good enough for inclusion? Is there anything else needs to be done? Thanks. "
        },
        {
            "id": "comment-15336280",
            "author": "Michael McCandless",
            "date": "2016-06-17T15:11:34+0000",
            "content": "Andriy Rysin I think this looks nice, thank you!  I think we should place it in its own sub-module under Lucene's analysis module?  Maybe just analysis/ukrainian? "
        },
        {
            "id": "comment-15337982",
            "author": "Andriy Rysin",
            "date": "2016-06-18T15:47:50+0000",
            "content": "I guess it does not fit under analysis/common as it depends on Morfologik so analysis/ukrainian is probably a good place. Or we could put it under analisys/morfologik (as a .uk subpackage) - it's your call. If we do that will the stopwords go with the stemmer or should they live under common/ (as they are not morfologik-specific and may be used for other Ukrainian implementations)?\nI am also thinking if we could build generic stemmer for Ukrainian based on the affix rules we have in dict_uk project (they are hunspell-like but fully based on regular expressions which makes them way more compact). "
        },
        {
            "id": "comment-15338468",
            "author": "Michael McCandless",
            "date": "2016-06-19T10:29:36+0000",
            "content": "Or we could put it under analisys/morfologik (as a .uk subpackage) - it's your call.\n\nI like this idea!\n\nIf we do that will the stopwords go with the stemmer or should they live under common/ (as they are not morfologik-specific and may be used for other Ukrainian implementations)?\n\nI think we should keep the stop-words in the same location?  I think users seeking Ukrainian tokenization should still be able to find them, under analysis/morfologik?\n\nI am also thinking if we could build generic stemmer for Ukrainian based on the affix rules we have in dict_uk project (they are hunspell-like but fully based on regular expressions which makes them way more compact).\n\nThat sounds compelling!  This would be a \"light\" stemmer, vs what we are adding for this issue (dictionary based)?  We should open a separate issue for that I think...\n\nOK, I'll work on folding your project into Lucene, under analysis/mofologik in a uk sub-package.  Thank you for all the hard work here! "
        },
        {
            "id": "comment-15338482",
            "author": "Michael McCandless",
            "date": "2016-06-19T11:09:27+0000",
            "content": "OK here's a patch, just a rote copy of the files from Andriy Rysin's project, and fixing up a few things ant precommit was unhappy about, and some small code styling fixes.  Tests pass, I think it's ready!\n\nThank you Andriy Rysin! "
        },
        {
            "id": "comment-15340996",
            "author": "Andriy Rysin",
            "date": "2016-06-21T03:34:58+0000",
            "content": "Looks cool, thanks a lot Michael!\n\nI wonder if we should add little javadoc for this analyzer that it's dictionary based so if we add a light-stemming analyzer users can easily tell the difference.\nAlso since I created a project I've updated the dictionary once (https://github.com/arysin/lucene_uk/commit/7cc8bea59c402e9b9729afd63d0a53cb34045e750) not sure if you're using the latest update.\n\nI'll open another issue for the \"light\" stemmer for Ukrainian. "
        },
        {
            "id": "comment-15341398",
            "author": "Michael McCandless",
            "date": "2016-06-21T08:57:11+0000",
            "content": "Thanks Andriy Rysin, I'll tweak the javadocs for UkrainianMorfologikAnalyzer stating that it's dictionary based and push shortly.  It looks like I have the latest dictionary.  Thank you! "
        },
        {
            "id": "comment-15341432",
            "author": "ASF subversion and git services",
            "date": "2016-06-21T09:23:56+0000",
            "content": "Commit 6ef174f52737b37e8b0625208ccc7cc64c3bd5b0 in lucene-solr's branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6ef174f ]\n\nLUCENE-7287: add UkrainianMorfologikAnalyzer, a dictionary-based analyzer for the Ukrainian language "
        },
        {
            "id": "comment-15341444",
            "author": "ASF subversion and git services",
            "date": "2016-06-21T09:33:18+0000",
            "content": "Commit 4a71e03a32fb5739b15ca4b0f893d50392caeb71 in lucene-solr's branch refs/heads/branch_6x from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=4a71e03 ]\n\nLUCENE-7287: add UkrainianMorfologikAnalyzer, a dictionary-based analyzer for the Ukrainian language "
        },
        {
            "id": "comment-15341653",
            "author": "Uwe Schindler",
            "date": "2016-06-21T12:08:12+0000",
            "content": "Michael McCandless: Can you remove the absolute path here?\n\n\nreturn Dictionary.read(UkrainianMorfologikAnalyzer.class.getResource(\"/org/apache/lucene/analysis/uk/ukrainian.dict\"));\n\n\n\nThe file is in same package, so just the filename should be fine to resolve the URL. "
        },
        {
            "id": "comment-15341848",
            "author": "Michael McCandless",
            "date": "2016-06-21T14:16:41+0000",
            "content": "Uwe Schindler oh yeah I'll fix that! "
        },
        {
            "id": "comment-15341896",
            "author": "ASF subversion and git services",
            "date": "2016-06-21T14:43:48+0000",
            "content": "Commit ceb6e21f84414b42f6b1b3866fc5b62e7ab474c0 in lucene-solr's branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ceb6e21 ]\n\nLUCENE-7287: don't use full paths to resources "
        },
        {
            "id": "comment-15341899",
            "author": "ASF subversion and git services",
            "date": "2016-06-21T14:44:32+0000",
            "content": "Commit 21eb654e408727b56a78c1c6a00541efe6eda31e in lucene-solr's branch refs/heads/branch_6x from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=21eb654 ]\n\nLUCENE-7287: don't use full paths to resources "
        },
        {
            "id": "comment-15342944",
            "author": "Ahmet Arslan",
            "date": "2016-06-21T22:41:32+0000",
            "content": "Can we use this analyzer in solr?\n\n\n <filter class=\"solr.MorfologikFilterFactory\" dictionary-resource=\"uk\"/>\n\n "
        },
        {
            "id": "comment-15343258",
            "author": "Andriy Rysin",
            "date": "2016-06-22T03:06:22+0000",
            "content": "I don't know much about solr, but I think MorfologikFilterFactory uses dictionary= parameter instead of dictionary-resource=\nhttps://lucene.apache.org/core/6_1_0/analyzers-morfologik/org/apache/lucene/analysis/morfologik/MorfologikFilterFactory.html\n\nAlso would that mean that we don't get the stop words filter and apostrophe/stress character normalization? "
        },
        {
            "id": "comment-15343877",
            "author": "Ahmet Arslan",
            "date": "2016-06-22T08:02:31+0000",
            "content": "So, Solr field type counterpart of this analyzer would be something like:\n\n    \n    <!-- Ukrainian -->\n    <fieldType name=\"text_uk\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer> \n        <charFilter class=\"solr.MappingCharFilterFactory\" mapping=\"lang/mappings_uk.txt\"/>\n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" words=\"org/apache/lucene/analysis/uk/stopwords.txt\" />\n        <filter class=\"solr.MorfologikFilterFactory\" dictionary=\"org/apache/lucene/analysis/uk/ukrainian.dict\"/>\n      </analyzer>\n    </fieldType>\n    \n\n\nIt would be nice to add an entry for Ukranian to https://cwiki.apache.org/confluence/display/solr/Language+Analysis "
        },
        {
            "id": "comment-15344252",
            "author": "Andriy Rysin",
            "date": "2016-06-22T12:45:21+0000",
            "content": "Thanks Ahmet, that looks good! Would you add/push those changes or shall I work on this? "
        },
        {
            "id": "comment-15344290",
            "author": "Ahmet Arslan",
            "date": "2016-06-22T13:19:44+0000",
            "content": "I think you, as the author of Ukrainian. Thanks! "
        },
        {
            "id": "comment-15344396",
            "author": "Andriy Rysin",
            "date": "2016-06-22T14:33:35+0000",
            "content": "I've logged in into cwiki but I don't seem to have rights to edit the page. "
        },
        {
            "id": "comment-15344403",
            "author": "Ahmet Arslan",
            "date": "2016-06-22T14:40:14+0000",
            "content": "only committers have rights to edit confluence wiki. Contributors include the proposed change/addition as a message at the end of the page. "
        },
        {
            "id": "comment-15344484",
            "author": "Cassandra Targett",
            "date": "2016-06-22T15:20:49+0000",
            "content": "If you do that (make a comment on the page with some text), I'll make sure it gets into the Solr Ref Guide. Just so you know, since this is for 6.2, I won't be able to add the content until after the current Ref Guide for 6.1 is released (vote going on now).\n\nedit: removed some of my earlier comment, I got this confused with SOLR-7739. "
        },
        {
            "id": "comment-15346413",
            "author": "Andriy Rysin",
            "date": "2016-06-23T13:18:38+0000",
            "content": "Sure, I can add a comment, but I guess I need to test the solution first and as I am not familiar with solr so it may take me few days. Unless Ahmet Arslan already verified this solution then we can just post it. "
        },
        {
            "id": "comment-15346816",
            "author": "Ahmet Arslan",
            "date": "2016-06-23T17:27:03+0000",
            "content": "Hi,\n\nI was able to run the analyzer successfully. Without mapping chart filter. Because character mappings are hardcoded into code.\nI am attaching an analysis screen shot. However, it looks like we need a remove duplicates token filter at the end.\nIt looks like Morfologik filter injects multiple tokens at the same position "
        },
        {
            "id": "comment-15346837",
            "author": "Ahmet Arslan",
            "date": "2016-06-23T17:37:18+0000",
            "content": " \n  <!-- Ukrainian -->\n    <fieldType name=\"text_uk\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer> \n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" words=\"org/apache/lucene/analysis/uk/stopwords.txt\" />\n        <filter class=\"solr.MorfologikFilterFactory\" dictionary=\"org/apache/lucene/analysis/uk/ukrainian.dict\" />\n      </analyzer>\n    </fieldType>\n\n "
        },
        {
            "id": "comment-15346844",
            "author": "Ahmet Arslan",
            "date": "2016-06-23T17:42:53+0000",
            "content": "Here is the screen shot of analysis admin page, with RemoveDuplicatesTokenFilter added.\n\n   <!-- Ukrainian -->\n    <fieldType name=\"text_uk\" class=\"solr.TextField\" positionIncrementGap=\"100\">\n      <analyzer> \n        <tokenizer class=\"solr.StandardTokenizerFactory\"/>\n        <filter class=\"solr.LowerCaseFilterFactory\"/>\n        <filter class=\"solr.StopFilterFactory\" words=\"org/apache/lucene/analysis/uk/stopwords.txt\" />\n        <filter class=\"solr.MorfologikFilterFactory\" dictionary=\"org/apache/lucene/analysis/uk/ukrainian.dict\" />\n        <filter class=\"solr.RemoveDuplicatesTokenFilterFactory\"/>\n      </analyzer>\n    </fieldType>\n\n "
        },
        {
            "id": "comment-15346857",
            "author": "Ahmet Arslan",
            "date": "2016-06-23T17:48:44+0000",
            "content": "Please see screenshots in the attachments section at the begging of the page and let me know what you think. "
        },
        {
            "id": "comment-15346862",
            "author": "Andriy Rysin",
            "date": "2016-06-23T17:53:53+0000",
            "content": "Thanks Ahmet!\nShall I create mappings_uk.txt so we can use it in solr?\nAs for the multiple tokens, MorfologikFilter produces lemmas so (how I understand) it may have multiple tokens in the output for single token in the input. "
        },
        {
            "id": "comment-15346875",
            "author": "Ahmet Arslan",
            "date": "2016-06-23T18:04:47+0000",
            "content": "Hi, \nmultiple tokens OK, but multiple identical tokens look weird, no?\nHave you checked the screenshot that includes RemoveDuplicatesTokenFilterFactory (RDTF)?\n\nShall I create mappings_uk.txt so we can use it in solr?\n\nLets ask Michael. \nEither separate file or we can just recommend to use mapping char filter the recommended mappings.\nMay be we can place the uk_mappings.txt file under https://github.com/apache/lucene-solr/tree/master/solr/server/solr/configsets/sample_techproducts_configs/conf/lang "
        },
        {
            "id": "comment-15346878",
            "author": "Andriy Rysin",
            "date": "2016-06-23T18:05:17+0000",
            "content": "Hmm, that does not look right. Yes we can either use RemoveDuplicatesTokenFilterFactory (we'll have to add that to the UkrainianMorfologikAnalyzer too) or I need to rebuild the dictionary to remove the duplicates (probably preferred way).\nThe problem is that currently the dictionary is the POS dictionary so there may be duplicate lemma records as long as the POS tags are different.\nI am thinking to file new jira issue for that and will provide a pull request, does that make sense? "
        },
        {
            "id": "comment-15346949",
            "author": "Ahmet Arslan",
            "date": "2016-06-23T18:43:35+0000",
            "content": "This is a new feature that is never released, new ticket may not be needed. "
        },
        {
            "id": "comment-15347153",
            "author": "Andriy Rysin",
            "date": "2016-06-23T20:49:36+0000",
            "content": "Ok, then I'll prepare the changes as part of this ticket.\n\nI've looked deeper into the morfologik dictionaries we have in LanguageTool and the Polish one has token+lemma normalized (with POS tags concatenated for each unique token+lemma), other dictionaries including Ukrainian have separate records thus token+lemma is not unique. I've sent an email to the morfologik guys and once I get an explanation I'll update the dictionary appropriately so we don't have have duplicates. "
        },
        {
            "id": "comment-15348585",
            "author": "Andriy Rysin",
            "date": "2016-06-24T19:31:24+0000",
            "content": "I've created the dictionary that collapses token+lemma in one record (like Polish dictionary does) and added tests to make sure we don't generate duplicate lemmas.\nI'll do a bit more testing and will create a pull request. "
        },
        {
            "id": "comment-15348895",
            "author": "ASF GitHub Bot",
            "date": "2016-06-24T23:41:54+0000",
            "content": "GitHub user arysin opened a pull request:\n\n    https://github.com/apache/lucene-solr/pull/45\n\n    LUCENE-7287: normalize Ukrainian morfologik dictionary to have unique\u2026\n\n    \u2026 token+lemma pair\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/arysin/lucene-solr master\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/lucene-solr/pull/45.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #45\n\n\ncommit 45d1a80899ceb1afb467433529fe66d29e1a1d2b\nAuthor: Andriy Rysin <arysin@gmail.com>\nDate:   2016-06-24T23:41:07Z\n\n    LUCENE-7287: normalize Ukrainian morfologik dictionary to have unique token+lemma pair\n\n "
        },
        {
            "id": "comment-15348927",
            "author": "Andriy Rysin",
            "date": "2016-06-25T00:10:11+0000",
            "content": "Ok, I was able to run solr with Ukrainian analyzer and I can confirm it generates unique lemmas.\nI've created a pull request https://github.com/apache/lucene-solr/pull/45\n\nI've also added mapping_uk.txt so we can use mapping filter in solr, once it's merged we can add this line:\n        <charFilter class=\"solr.MappingCharFilterFactory\" mapping=\"org/apache/lucene/analysis/uk/mapping_uk.txt\"/>\n\nWe could potentially change UkrainianMorfologikAnalyzer to use MappingCharFilterFactory to read from the same file (so we don't have the mapping both in the code and the file) but not sure how appropriate using of factories in lucene is.\n\nMany thanks to Ahmet who helped with solr integration and found duplicate tokens! "
        },
        {
            "id": "comment-15358188",
            "author": "Andriy Rysin",
            "date": "2016-07-01T01:10:31+0000",
            "content": "Hey Michael McCandless, can we please merge the pull request above, that should wrap up dictionary-based analyzer for Ukrainian. Thanks! "
        },
        {
            "id": "comment-15360116",
            "author": "Michael McCandless",
            "date": "2016-07-02T10:37:54+0000",
            "content": "Andriy Rysin thank you!  I'll merge this likely early next week ... "
        },
        {
            "id": "comment-15364093",
            "author": "ASF subversion and git services",
            "date": "2016-07-06T10:17:50+0000",
            "content": "Commit bc502bd9c91669cec72f40fd6fc13b6a68e90c52 in lucene-solr's branch refs/heads/master from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=bc502bd ]\n\nLUCENE-7287: normalize Ukrainian morfologik dictionary to have unique token+lemma pairs "
        },
        {
            "id": "comment-15364096",
            "author": "ASF subversion and git services",
            "date": "2016-07-06T10:20:10+0000",
            "content": "Commit 6c730ab74f2ac8a865d2d514344db18572f059da in lucene-solr's branch refs/heads/branch_6x from Mike McCandless\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=6c730ab ]\n\nLUCENE-7287: normalize Ukrainian morfologik dictionary to have unique token+lemma pairs "
        },
        {
            "id": "comment-15364098",
            "author": "Michael McCandless",
            "date": "2016-07-06T10:21:41+0000",
            "content": "Andriy Rysin, I pushed the normalization changes above, thank you! "
        },
        {
            "id": "comment-15365063",
            "author": "Andriy Rysin",
            "date": "2016-07-06T20:55:01+0000",
            "content": "Thanks Michael, much appreciated! "
        },
        {
            "id": "comment-15438949",
            "author": "Michael McCandless",
            "date": "2016-08-26T13:58:40+0000",
            "content": "Bulk close resolved issues after 6.2.0 release. "
        },
        {
            "id": "comment-15628837",
            "author": "Andriy Rysin",
            "date": "2016-11-02T12:41:48+0000",
            "content": "Cassandra looks like 6.2 is out could you please add Ukrainian section to https://cwiki.apache.org/confluence/display/solr/Language+Analysis ? "
        },
        {
            "id": "comment-15630171",
            "author": "Cassandra Targett",
            "date": "2016-11-02T19:33:49+0000",
            "content": "I missed it last go-around. I don't know if I will have time to add it for 6.3, but I added it to the TODO list (https://cwiki.apache.org/confluence/display/solr/Internal+-+TODO+List) so we'll at least know it needs to get done. "
        },
        {
            "id": "comment-16196975",
            "author": "ASF GitHub Bot",
            "date": "2017-10-09T13:40:46+0000",
            "content": "Github user arysin closed the pull request at:\n\n    https://github.com/apache/lucene-solr/pull/45 "
        }
    ]
}