{
    "id": "LUCENE-1035",
    "title": "Optional Buffer Pool to Improve Search Performance",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/store"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Not A Problem",
        "status": "Closed"
    },
    "description": "Index in RAMDirectory provides better performance over that in FSDirectory.\nBut many indexes cannot fit in memory or applications cannot afford to\nspend that much memory on index. On the other hand, because of locality,\na reasonably sized buffer pool may provide good improvement over FSDirectory.\n\nThis issue aims at providing such an optional buffer pool layer. In cases\nwhere it fits, i.e. a reasonable hit ratio can be achieved, it should provide\na good improvement over FSDirectory.",
    "attachments": {
        "LUCENE-1035.contrib.patch": "https://issues.apache.org/jira/secure/attachment/12377642/LUCENE-1035.contrib.patch",
        "LUCENE-1035.patch": "https://issues.apache.org/jira/secure/attachment/12368441/LUCENE-1035.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2007-10-26T01:33:49+0000",
            "content": "Coding Changes\n--------------\nNew classes are localized to the store package and so as most of the changes.\n\n\tTwo new interfaces: BareInput and BufferPool.\n\tBareInput takes a subset of IndexInput's methods such as readBytes\n    (IndexInput now implements BareInput).\n\tBufferPoolLRU is a simple implementation of BufferPool interface.\n    It uses a doubly linked list for the LRU algorithm.\n\tBufferPooledIndexInput is a subclass of BufferedIndexInput. It takes\n    a BareInput and a BufferPool. For BufferedIndexInput's readInternal,\n    it will read from the BufferPool, and BufferPool will read from its\n    cache if it's a hit and read from BareInput if it's a miss.\n\tA FSDirectory object can optionally be created with a BufferPool with\n    its size specified by a buffer size and number of buffers. BufferPool\n    is shared among IndexInput of read-only files in the directory.\n\n\n\nUnit tests\n\n\tTestBufferPoolLRU.java is added.\n\tMinor changes were made to _TestHelper.java and TestCompoundFile.java\n    because they made specific assumptions of the type of IndexInput returns\n    by FSDirectory.openInput.\n\tAll unit tests pass when I switch to always use a BufferPool.\n\n\n\n\nPerformance Results\n-------------------\nI ran some experiments using the enwiki dataset. The experiments were run on\na dual 2.0Ghz Intel Xeon server running Linux. The dataset has about 3.5M\ndocuments and the index built from it is more than 3G. The only store field\nis a unique docid which is retrieved for each query result. All queries are\ntwo-term AND queries generated from the dictionary. The first set of queries\nreturns between 1 to 1000 results with an average of 40. The second set\nreturns between 1 to 3000 with an average of 560. All tests were run warm.\n\n1 Query set with average 40 results\n  Buffer Pool Size    Hit Ratio    Queries per second\n      0                 N/A            230\n      16M               55%            250\n      32M               63%            282\n      64M               73%            345\n      128M              85%            476\n      256M              95%            672\n      512M              98%            685\n\n2 Query set with average 560 results\n  Buffer Pool Size    Hit Ratio    Queries per second\n      0                 N/A             27\n      16M               56%             29\n      32M               70%             37\n      64M               89%             55\n      128M              97%             67\n      256M              98%             71\n      512M              99%             72\n\nOf course if the tests are run cold, or if the queried portion of the index\nis significantly larger than the file system cache, or there are a lot of\npre-processing of the queries and/or post-processing of the results, the\nspeedup will be less. But where it applies, i.e. a reasonable hit ratio can\nbe achieved, it should provide a good improvement. ",
            "author": "Ning Li",
            "id": "comment-12537799"
        },
        {
            "date": "2007-10-26T02:02:24+0000",
            "content": "I don't think this is any better than the NIOFileCache directory I had already submitted.\n\nIt not really approved because the community felt that it did not offer much over the standard OS file system cache.\n\nMy tests showed it was better, but I think this would fall into the same problem. ",
            "author": "robert engels",
            "id": "comment-12537805"
        },
        {
            "date": "2007-10-26T04:24:40+0000",
            "content": "Were the tests run using the same set of queries they were warmed for?  If so, an interesting benchmark might be to, e.g., start with 200 queries, then warm things with the first 100 and use the second for the benchmark.  Ideally you'd start with a log of real queries, but those are hard to obtain.  Over ten years ago I released a 1M query log from Excite, which I still see people reference in papers, so it must be out there somewhere.  It would be better than nothing for these kinds of benchmarks.  Or perhaps we can obtain a copy of the more-recent AOL query log?  Otherwise you've only demonstrated an improvement when queries are frequently repeated.  There are better ways to optimize for that, e.g., by caching hit lists, no? ",
            "author": "Doug Cutting",
            "id": "comment-12537827"
        },
        {
            "date": "2007-10-26T14:27:40+0000",
            "content": "> I don't think this is any better than the NIOFileCache directory I had already submitted.\n\nAre you referring to LUCENE-414? I just read it and yes, it's similar to the MemoryLRUCache part. Hopefully this is more general, not just for NioFile.\n\n> It not really approved because the community felt that it did not offer much over the standard OS file system cache.\n\nWell, it shows it has its value in cases where you can achieve a reasonable hit ratio, right? This is optional. An application can test with its query log first to see the hit ratio and then decide whether to use a buffer pool and if so, how large. ",
            "author": "Ning Li",
            "id": "comment-12537972"
        },
        {
            "date": "2007-10-26T14:43:11+0000",
            "content": "A couple of random thoughts\n\n\tprevious tests showed that vint decoding was often bottleneck, but these tests seem to indicate otherwise (that the bottleneck is the system call to move data from OS  FS cache to userspace?)... perhaps this is due to the fact that all queries are \"AND\" and match a max of 1000 docs?  The recent addition of multi-level skipping perhaps removes the vint decoding bottleneck for these types of queries that match few documents.\n\tmost lucene usecases store much more than just the document id... that would really affect locality.\n\tIt seems like a simple LRU cache could really be blown out of the water by certain types of queries (retrieve a lot of stored fields, or do an expanding term query) that would force out all previously cached hotspots.  Most OS level caching has protection against this (multi-level LRU or whatever).  But of our user-level LRU cache fails, we've also messed up the OS level cache since we've been hiding page hits from it.\n\tI'd like to see single term queries, \"OR\" queries, and queries across multiple fields (also a common usecase) that match more documents tested also.\n\n\n\n ",
            "author": "Yonik Seeley",
            "id": "comment-12537977"
        },
        {
            "date": "2007-10-26T14:50:53+0000",
            "content": "> Were the tests run using the same set of queries they were warmed for?\n\nYes, the same set of queries were used. The warm-up and the real run are two separate runs, which means the file system cache is warmed, but not the buffer pool.\n\nYes, it'd much better if a real query log could be obtained. I'll take a look at the AOL query log. I used to have an intranet query log which has a lot of term locality. That's why I think this could provide a good improvement.\n\n> There are better ways to optimize for that, e.g., by caching hit lists, no?\n\nThat's useful and that's for exact query match. If there are a lot of shared query term but not exact query match, caching hit list won't help. This is sort of caching posting list but simpler. ",
            "author": "Ning Li",
            "id": "comment-12537978"
        },
        {
            "date": "2007-10-26T14:52:38+0000",
            "content": "Also, in addition to some kind of protection against the LRU cache being busted by a single query, perhaps the ability to not cover parts of the index (like stored fields) would also help. ",
            "author": "Yonik Seeley",
            "id": "comment-12537980"
        },
        {
            "date": "2007-10-26T15:50:31+0000",
            "content": "> most lucene usecases store much more than just the document id... that would really affect locality.\n\nIn the experiments, I was simulating the (Google) paradigm where you retrieve just the docids and go to document servers for other things. If store almost always negatively affects locality, we can make the buffer pool sit only on data/files which we expect good locality (say posting lists), but not others.\n\n> It seems like a simple LRU cache could really be blown out of the water by certain types of queries (retrieve a lot of stored fields, or do an expanding term query) that would force out all previously cached hotspots. Most OS level caching has protection against this (multi-level LRU or whatever). But of our user-level LRU cache fails, we've also messed up the OS level cache since we've been hiding page hits from it.\n\nThat's a good point. We can improve the algorithm but hopefully still keep it simple and general. This buffer pool is not a fit-all solution. But hopefully it will benefit a number of use cases. That's why I say \"optional\". \n\n> I'd like to see single term queries, \"OR\" queries, and queries across multiple fields (also a common usecase) that match more documents tested also.\n\nI'll change to \"OR\" queries and see what happens. The dataset is enwiki with four fields: docid, date (optional), title and body. Most terms are from title and body. ",
            "author": "Ning Li",
            "id": "comment-12537995"
        },
        {
            "date": "2007-10-26T15:56:47+0000",
            "content": "Actually, phrase queries would be really interesting too since they hit the term positions. ",
            "author": "Yonik Seeley",
            "id": "comment-12537998"
        },
        {
            "date": "2007-10-26T19:30:32+0000",
            "content": "\ndid you compare it against MMAP? I ",
            "author": "Eks Dev",
            "id": "comment-12538065"
        },
        {
            "date": "2007-10-26T21:52:55+0000",
            "content": "> I'll change to \"OR\" queries and see what happens.\n\n  Query set with average 590K results, retrieving docids for the first 5K\n  Buffer Pool Size    Hit Ratio    Queries per second\n     0                 N/A             1.9\n     16M               53%             1.9\n     32M               68%             2.0\n     64M               90%             2.3\n     128M/256M/512M              99%             2.3\n\nAs Yonik pointed out, in the previous \"AND\" tests, the bottleneck is the system call to move data from file system cache to userspace. Here in the \"OR\" tests, much fewer such calls are made therefore the speedup is less significant. Wish I could get a real query workload for this dataset.\n\n> Actually, phrase queries would be really interesting too since they hit the term positions.\n\nPhrase queries are rare and term distribution is highly skewed according to the following study on the Excite query log:\nSpink, Amanda and Xu, Jack L. (2000)   \"Selected results from a large study of Web searching: the Excite study\".  Information Research, 6(1) Available at: http://InformationR.net/ir/6-1/paper90.html\n\n\"4. Phase Searching: Phrases (terms enclosed by quotation marks) were seldom, while only 1 in 16 queries contained a phrase - but correctly used.\n5. Search Terms: Distribution: Jansen, et al., (2000) report the distribution of the frequency of use of terms in queries as highly skewed.\"\n\nI didn't find a good on on the AOL query log. In any case, this buffer pool is not intended for general purpose. I mentioned RAMDirectory earlier. This is more like an alternative to RAMDirectory (that's why it's per directory): you want persistent storage for the index, yet it's not too big that you want RAMDirectory search performance. In addition, the entire index doesn't have to fit into memory, as long as the most queried part does. Hopefully, this benefits a subset of Lucene use cases.\n\n> did you compare it against MMAP? I\n\nThe index I experimented on didn't fit in memory... ",
            "author": "Ning Li",
            "id": "comment-12538112"
        },
        {
            "date": "2007-10-26T22:00:47+0000",
            "content": ">> 4. Phase Searching: Phrases (terms enclosed by quotation marks) were seldom, while only 1 in 16 queries contained a phrase\n\nquoted phrases in raw user input may be rare, but that does't mean PhraseQueries are as rare ... apps may artificially create a sloppy PhraseQuery containing all of the individual words in the users raw query string to help identify matches where the input words all appear close together (i may be bias in assuming this is common, since it's something i do a lot of personally) ",
            "author": "Hoss Man",
            "id": "comment-12538118"
        },
        {
            "date": "2007-10-26T22:45:34+0000",
            "content": "> Query set with average 590K results, retrieving docids for the first 5K\n\nThat seems like quite a few docs to retrieve--any particular reason why?  (It would be good to know if the speedup is occuring in the query phase or doc retrieval).  This would also explain why VInt decoding is not the bottleneck (it wouldn't be much-used for retrieving stored fields).\n\nI echo Hoss' comment--proximity searching is important even if it isn't used much directly by users. ",
            "author": "Mike Klaas",
            "id": "comment-12538126"
        },
        {
            "date": "2007-10-26T23:03:39+0000",
            "content": "> That seems like quite a few docs to retrieve--any particular reason why?\n\nI was guessing most applications won't want all 590K results, no? Lucene is used in so many different ways. No represent-all use case.\n\n> I echo Hoss' comment--proximity searching is important even if it isn't used much directly by users.\n\nHmm, I agree with you and Hoss, especially in applications where proximity is used to rank results of OR queries. ",
            "author": "Ning Li",
            "id": "comment-12538129"
        },
        {
            "date": "2007-10-29T20:22:20+0000",
            "content": "Ning, I didn't mean to sound negative about this.  Your benchmarks do show that in some situations this can provide significant speedup.  The question is whether such situations are common enough to warrant adding this to the core.  A way around that might be to layer it on top of FSDirectory and add it to contrib. ",
            "author": "Doug Cutting",
            "id": "comment-12538576"
        },
        {
            "date": "2007-10-29T20:34:49+0000",
            "content": "Again, see my previous code in issue 414.  That it only works NioFile is not really a limitation, it can easily work with any underlying \"file\". This is just an implementation detail.\n\nThis code is already implemented as a layer on top of FS directory, so the caller can decide to use an original FS directory or a caching one.\n\nWe actually have a multiplexing directory that (depending on file type and size), either opens the file purely in memory, uses a cached file, or lets the OS do the caching. Works really well.\n ",
            "author": "robert engels",
            "id": "comment-12538582"
        },
        {
            "date": "2007-10-29T23:41:14+0000",
            "content": "> The question is whether such situations are common enough to warrant adding this to the core.\n\nAgree.\n\n> A way around that might be to layer it on top of FSDirectory and add it to contrib.\n\nI'd be happy to do that. I'll also include the following in the javadoc which hopefully is a fair assessment:\n\n\"When will a buffer pool help:\n\n\tWhen an index is significantly larger than the file system cache, the hit ratio of a buffer pool is probably low which means insignificant performance improvement.\n\tWhen an index is about the size of the file system cache or smaller, a buffer pool with good enough hit ratio will help if the IO system calls are the bottleneck. An example is if you have many \"AND\" queries which causes a lot large skips.\"\n\n ",
            "author": "Ning Li",
            "id": "comment-12538638"
        },
        {
            "date": "2008-02-17T07:51:40+0000",
            "content": "It looks like this was never fully done.  I wonder if this should be closed, esp. since Ning might be working on slightly different problems now. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12569652"
        },
        {
            "date": "2008-03-03T22:01:22+0000",
            "content": "Robert,\nyou said: \n....We actually have a multiplexing directory that (depending on file type and size), either opens the file purely in memory, uses a cached file, or lets the OS do the caching. Works really well...\n\nDid you create a patch somewhere, or is this your internal work?\n\nI have a case where this could come in very handy, I plan to use MMAP for postings & co... but FSDirectory for stored fields as they could easily blow the size ... With possibility to to select on file type/size  makes MMAP use case much much closer to many users... one Directory implementation that allows users to select strategy is indeed perfect, LRU, FSDirectora, MMAP, RAM or whatnot  ",
            "author": "Eks Dev",
            "id": "comment-12574759"
        },
        {
            "date": "2008-03-03T23:12:05+0000",
            "content": "> It looks like this was never fully done. I wonder if this should be closed, esp. since Ning might be working on slightly different problems now.\n\nSorry for the delay. I'll spend some time later this week or early next week to update and make it a contrib patch. ",
            "author": "Ning Li",
            "id": "comment-12574782"
        },
        {
            "date": "2008-03-11T21:50:34+0000",
            "content": "Re-do as a contrib package. Creating BufferPooledDirectory with your customized file name filter for readers allows you to decide which files you want to use the caching layer for.\n\nThe package includes some tests. I also modified and tested the core tests with the caching layer in a private setting and all tests passed. ",
            "author": "Ning Li",
            "id": "comment-12577616"
        },
        {
            "date": "2011-01-27T10:47:45+0000",
            "content": "It seems there was never consensus to carry on with this issue, mostly because the gains are not too clear (e.g., if you have AND queries, this directory may not be such a big win, and I think AND is used a lot more today by default?) or if your index is bigger than the FS cache, which I think it also something that's becoming more and more relevant these days \u2013 we see lot more biggish indexes than small ones.\n\nSince there was no activity in the past 2.5 years, I'm closing it. If at some point someone will think that this is worth reopening, please do. ",
            "author": "Shai Erera",
            "id": "comment-12987489"
        }
    ]
}