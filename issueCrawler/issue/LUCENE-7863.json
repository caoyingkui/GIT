{
    "id": "LUCENE-7863",
    "title": "Don't repeat postings (and perhaps positions) on ReverseWF, EdgeNGram, etc",
    "details": {
        "labels": "",
        "priority": "Major",
        "resolution": "Unresolved",
        "affect_versions": "None",
        "status": "Open",
        "type": "Improvement",
        "components": [
            "core/index"
        ],
        "fix_versions": []
    },
    "description": "Context\n*suffix and *infix* searches on large indexes. \n\nProblem\nObviously applying ReversedWildcardFilter doubles an index size, and I'm shuddering to think about EdgeNGrams...\n\nProposal \nDRY postings",
    "attachments": {
        "LUCENE-7863.patch": "https://issues.apache.org/jira/secure/attachment/12882794/LUCENE-7863.patch",
        "benchmark-1m.out": "https://issues.apache.org/jira/secure/attachment/12887845/benchmark-1m.out",
        "LUCENE-7863.hazard": "https://issues.apache.org/jira/secure/attachment/12870854/LUCENE-7863.hazard",
        "bench-byte-array2.out": "https://issues.apache.org/jira/secure/attachment/12888506/bench-byte-array2.out",
        "byterefshash-bench.txt": "https://issues.apache.org/jira/secure/attachment/12896135/byterefshash-bench.txt",
        "bench-byte-array-long.out": "https://issues.apache.org/jira/secure/attachment/12888729/bench-byte-array-long.out"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16033650",
            "date": "2017-06-01T20:44:01+0000",
            "content": "Let's index six one-word docs:\n\n\n\nfoo\n\n\nfoo\n\n\nfoo\n\n\nbar\n\n\nbar\n\n\nbar\n\n\n\n\n\nInverted index with ReversedWildcardFilter\n\n\n\n\nterm\nposting offset (relative)\n\n\n1oof\n0\n\n\n1rab\n3\n\n\nbar\n3\n\n\nfoo\n3\n\n\n\n\n\n\n\n\nPostings (absolute values)\n\n\n0,1,2\n\n\n3,4,5\n\n\n3,4,5\n\n\n0,1,2\n\n\n\n\n\nHere you see that postings (and positions) are duplicated for every derived term.\n\nProposal: DRY\n\n\n\n\nterm\nposting offset (relative)\n\n\n1oof\n0\n\n\n1rab\n3\n\n\nbar\n-3\n\n\nfoo\n-3\n\n\n\n\n\n\n\n\nPostings (absolute values)\n\n\n0,1,2\n\n\n3,4,5\n\n\n\n\n\nNote\nIt seems like it's really challenging to implement, giving that codecs doesn't allow such tweaking, I had to change o.a.l.i classes. This code introduces the relation between terms see FreqProxTermsEnum.getTwinTerm() and so one (it's one of the ugliest pieces). It also requires to change the term block format: posting offsets are written in ZLong (instead of Vlong), since they need to be negative. I'm afraid it ruins a lot of tests since I was interested in the only one TestReversedWildcardFilterFactory. It passes. I also experiment with 5M enwiki and it seems roughly works: RWF blows index from 13G to 28G and this code keeps it at 17G and runs *leading queries fast.\nIt aims only RWF where the derived term is 1-1 to the origin one. This patch for branch_6x.\n\nDisclaimer\nCurrent patch is mad and dirty (trickedFields = Arrays.asList(\"one\", \"body_txt_en\"), and plenty of sysout ), I've just scratched the idea. \n\nTODO\n\n\tHow to carry relation between origin and derived NGramm terms (1 - Many)?\n\tHow to adjust the current o.a.l.i to bring reduplicated postings to the codec?\n\n\n\nThe next idea\nFor *infix* searches it needs to derive the following terms (for three bar docs and three baz docs):\n\n\n\nterm\nposition offset\n\n\nar_bar\n0\n\n\naz_baz\n3\n\n\nbar\n-3\n\n\nbaz\n3\n\n\nr_bar\n-3\n\n\nz_baz\n3\n\n\n\n\nHere we should write both postings only once. And on *a* query find both posting with a prefix query a*. \n\n\n ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16033673",
            "date": "2017-06-01T20:53:36+0000",
            "content": "LUCENE-7863.hazard without indentation-only changes.  ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16054262",
            "date": "2017-06-19T16:15:03+0000",
            "content": "Instead of hacking the index/codec, have you thought of using another field that only has the reversed terms? ",
            "author": "David Smiley"
        },
        {
            "id": "comment-16054679",
            "date": "2017-06-19T20:22:02+0000",
            "content": "David Smiley, thanks for breaking silence. \nIt gives six docs\n\n\n\nname\nname_rev\n\n\nfoo\noof\n\n\nfoo\noof\n\n\nfoo\noof\n\n\nbar\nrab\n\n\nbar\nrab\n\n\nbar\nrab\n\n\n\n\n\nthe term dictionary is\n\n\n\n\nfield/term\nposting offset (relative)\n\n\nname\n\u00a0\n\n\nbar\n0\n\n\nfoo\n3\n\n\nname_rev\n\u00a0\n\n\noof\n3\n\n\nrab\n3\n\n\n\n\n\n\n\n\nPostings (absolute values)\n\n\n3,4,5\n\n\n0,1,2\n\n\n0,1,2\n\n\n3,4,5\n\n\n\n\n\nThus, we still see 12 postings, that's duplication, which I want to avoid. Or you propose to have an auxiliary docs like foo->oof?   ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16054688",
            "date": "2017-06-19T20:32:41+0000",
            "content": "oh right I was thinking of freq, positions, offsets, payloads.  It's true my suggestion repeats the postings (doc ID list) but at least not the rest.  I don't see how auxiliary docs would help.  Maybe something is possibly by wrapping another codec. ",
            "author": "David Smiley"
        },
        {
            "id": "comment-16054717",
            "date": "2017-06-19T20:52:02+0000",
            "content": "It's true my suggestion repeats the postings (doc ID list) but at least not the rest.\nI suppose if it repeats postings, it also repeats positions, offsets, payloads.  Doesn't it?   ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16054727",
            "date": "2017-06-19T20:56:52+0000",
            "content": "I suppose if it repeats postings, it also repeats positions, offsets, payloads. Doesn't it?\n\nNope!  Configure it for IndexOptions.DOCS only \u2013 no rest of that stuff.  Then you could write a TermsEnum wrapper that delegates to the original field that has all the rest of that stuff.  For inspiration, see TermVectorFilteredTermsEnum in the unified highlighter.  It's not the same but the idea is that you blend 2 TermsEnum. ",
            "author": "David Smiley"
        },
        {
            "id": "comment-16054803",
            "date": "2017-06-19T21:59:24+0000",
            "content": "Got it. Nice decision! \n\nSo, instead of searching for name:*ar, it flips query to name_rev:ra*, then for every doc (if we need phrase logic or highlighting): it seeks original term's postings to the same doc, and read positions and offsets.  \n\nThinking about EdgeNGramms (searching for name:*a*), derivative field should go like this: ar_bar, r_bar to be able to switch to original term's posting. So, I still think that even with this approach (second DOCS_ONLY field) blowing postings by these derivative terms still might not be affordable.   \n\nAnd coming back to your question:\nhave you thought of using another field that only has the reversed terms?\nNo. I haven't thought about it. It's a great idea! Thanks for contributing it.   ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16134551",
            "date": "2017-08-20T19:57:39+0000",
            "content": "WIP LUCENE-7863.patch\nIt introduces a codec with two posting formats:\n\n\thijacking PF which stores posting offsets for original terms\n\tinjecting PF which reverses terms and supplies offset to the original terms postings (here is the only file format is changed - it's written with Zlong since these offset deltas are negative)\nIt has to break into any private and final members that blow up the patch.\n\n ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16134899",
            "date": "2017-08-21T08:53:48+0000",
            "content": "infix proof case  ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16143394",
            "date": "2017-08-28T05:51:02+0000",
            "content": "LUCENE-7863.patch\nMove test bencmark to resolve dependency. Started to work on benchmark, WIP    ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16166496",
            "date": "2017-09-14T15:45:42+0000",
            "content": "one severe fix LUCENE-7863.patch, benchmark is in progress. ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16168062",
            "date": "2017-09-15T15:53:17+0000",
            "content": "LUCENE-7863.patch benchmark fixes. \n ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16168601",
            "date": "2017-09-15T22:09:17+0000",
            "content": "LUCENE-7863.patch has significant fixes for codec registration.\n\n\tit looks like the large enough term dictionary hit some code path in IntersectingTermsEnum which is broken due to introduced index format changes.\n\tit's reproduced with derivative-terms-only.alg\n\njava.io.EOFException: seek past EOF: MMapIndexInput(path=\"...lucene-solr/lucene/benchmark/deriv/index/_0_Lucene50HijackInjector_0.doc\")\n\tat org.apache.lucene.store.ByteBufferIndexInput$SingleBufferImpl.seek(ByteBufferIndexInput.java:366)\n\tat org.apache.lucene.codecs.lucene50.Lucene50PostingsReader$BlockDocsEnum.reset(Lucene50PostingsReader.java:306)\n\tat org.apache.lucene.codecs.lucene50.Lucene50PostingsReader.postings(Lucene50PostingsReader.java:210)\n\tat org.apache.lucene.codecs.blocktree.SegmentTermsEnum.postings(SegmentTermsEnum.java:1006)\n\tat org.apache.lucene.search.MultiTermQueryConstantScoreWrapper$1.rewrite(MultiTermQueryConstantScoreWrapper.java:166)\n\n\n\toverall, the idea to just change Vlong to Zlong through overriding turns out not really good, it leads to many changes removes incapsulation and final that means there is no any sense in them.\n\n ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16169037",
            "date": "2017-09-16T20:05:34+0000",
            "content": "ok. LUCENE-7863.patch should pass the benchmark. \nThe thing is, there should be single posting format since different formats write into the different suffixes.\nThe idea to write negative offsets (Zlong) conditionally turns out to be a dead end. Hold on reviews until I change format to write Zlong for all fields metadata.   ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16169179",
            "date": "2017-09-17T03:23:20+0000",
            "content": "Mikhail Khludnev nice chatting with you about this at Lucene/Solr Revolution.  I forgot to bring up LUCENE-7639 \u2013 but I see you are aware as it's linked here already.  Any thoughts on the relative merits of these approaches? ",
            "author": "David Smiley"
        },
        {
            "id": "comment-16169239",
            "date": "2017-09-17T09:07:20+0000",
            "content": "LUCENE-7863.patch is much friendly for review.   ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16169242",
            "date": "2017-09-17T09:18:44+0000",
            "content": "David Smiley, yep. It was great to catch up! \nI don't believe that suffix arrays and/or particular implemantation outperforms FST, at least significantly. Although, it's a subject for benchmarking). Derivative terms might be applicable for many other usages line deep taxonomies and so one, therefore, it's worth to discuss this little format change.   ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16169620",
            "date": "2017-09-18T05:37:59+0000",
            "content": "Merge thread fails to read terms  \n\n     ... \n     [java]  263.21 sec --> main added     80000 docs\n     [java]  264.29 sec --> main added     85000 docs\n     [java]  313.78 sec --> main added     90000 docs\n     [java]  315.81 sec --> main added     95000 docs\n     [java] #################### \n\n     [java] Caused by: org.apache.lucene.index.CorruptIndexException: invalid docCount: 5312 maxDoc: 5259 (resource=MMapIndexInput(path=\".../lucene-solr/lucene/benchmark/deriv/index/_1_Lucene50Hijack_0.tim\"))\n     [java]     at org.apache.lucene.codecs.blocktree.BlockTreeTermsReader.<init>(BlockTreeTermsReader.java:193)\n     [java]     at org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat.fieldsProducer(Lucene50PostingsFormat.java:445)\n     [java]     at org.apache.lucene.codecs.derivativeterms.TermsDerivingPostingsFormat.fieldsProducer(TermsDerivingPostingsFormat.java:137)\n     [java]     at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsReader.<init>(PerFieldPostingsFormat.java:292)\n     [java]     at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat.fieldsProducer(PerFieldPostingsFormat.java:372)\n     [java]     at org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:112)\n     [java]     at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:78)\n     [java]     at org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:208)\n     [java]     at org.apache.lucene.index.ReadersAndUpdates.getReaderForMerge(ReadersAndUpdates.java:836)\n     [java]     at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4360)\n     [java]     at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:4030)\n     [java]     at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:624)\n     [java]     at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:661)\n\n ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16169724",
            "date": "2017-09-18T08:03:53+0000",
            "content": "LUCENE-7863.patch properly instantiates offset buffer.  ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16171237",
            "date": "2017-09-19T07:34:26+0000",
            "content": "I've run benchmark for 1M wiki docs benchmark-1m.out. Turns out, a memory consumption for derivative terms (or for the current impl at least) is terrific. So, I couldn't run even 4M benchmark on 16G laptop. Therefore, using ByteRefsHash is absolutely necessary (current code is pretty dumb). Also, I've realized that terms are derived for every merge, but I have no idea how to avoid it. \n\nHere is the comparison on 1M wiki with url terms excluded.  \n\n\n\n\nround\nindexing, mins\nsearch req/sec\nram total, GB \nindex size, GB\n\n\n EdgeNGramm \n25\n81.04\n1.9\n6.3\n\n\nderived edges\n18\n35.31\n10.2\n2.0\n\n\n\n\n\nSo, far search results don't match side by side, but I'm not sure whether they are expected to match in benchmark. A good random test is necessary (fwiw, existing test actually tests nothing).   ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16175143",
            "date": "2017-09-21T17:24:00+0000",
            "content": "LUCENE-7863.patch replaces TreeMap<String> to BytesRefArray see ByteArrayDerivativeWriter.java. Here are results for 5M docs\n\n\n\nround\nindexing, mins\nsearch req/sec\nram total, GB \nindex size, GB\n\n\n EdgeNGramm \n85\n27.82\n2.3\n23\n\n\nderived edges\n51\n7.22\n5.5\n9.1\n\n\n\n\nWe have index size and even index time gain that costs some ram as it's expected. \nEdgeNGramm cache can be made a little bit more compact. The trick is to append something to edgegramm to make it unique. \nThe interesting thing is the 3 times slower search time, I suppose that posting offsets obtained during term expansion could be sorted before reading postings.   ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16176486",
            "date": "2017-09-22T14:24:01+0000",
            "content": "Here is solid benchmark log bench-byte-array2.out with running both rounds one by one: edgeNgram then derivativeTerms.\ncomposing same result table again:\n\n\n\nround\nindexing, sec\nsearch req/sec\nram total, GB \nindex size, GB\n\n\n EdgeNGramm \n5,890.05\n61.55\n2,7\n23\n\n\nderived edges\n6,981.87\n26.51\n11.5\n8.4\n\n\n\n\n\nIt's somewhat different. Derived terms indexing is slower, probably because of really small RAM buffer, which I set earlier. Search time is 3 times slower.  ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16176616",
            "date": "2017-09-22T15:56:46+0000",
            "content": "running longer time search only benchmark makes the difference not so significant: 107 vs 63 reqs/s edges vs derivatives.\n\n     [java] ------------> Report sum by Prefix (search) and Round (2 about 2 out of 8)\n     [java] Operation   round work dir  src                        runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem directory size, Mb\n     [java] search_5000     0edge EnwikiEmptyEdgeContentSource        1         5000       107.87       46.35    67,118,120    141,033,472          23,784.00\n     [java] search_5000     1deriv EnwikiEmptyEdgeContentSource        1         5000        63.49       78.75    95,709,256    119,537,664           8,415.00\n\n\n\nComparing sampling snapshot shows that derivative terms are slower for readXxxBlock. ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16176629",
            "date": "2017-09-22T16:03:33+0000",
            "content": "Tried to sort postings' positions before merging {{MultiTermQueryConstantScoreWrapper.createWeight(...).new ConstantScoreWeight() \n{...}\n.rewrite(LeafReaderContext)}}. Alas, it doesn't help to read derivative terms' postings faster. ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16177997",
            "date": "2017-09-23T22:18:47+0000",
            "content": "bench-byte-array-long.out here is the long test log evaluated larger ram buffer for derivative terms. Here is the summary. \n\n\tderivative terms are indexed 25% slower than edgeNgramms (see below)\n\tthey significantly reduces index size. For a usual case, the gain would be bigger, since here we have multi language docs that make postings shorter\n\tderivative terms roughly double ram consumption for indexing (see below)\n\tsearching for derivative terms is 30..60%% slower since it's required to gather randomly distributed postings.\n\n\n\nIndexing can be optimized with using BytesRefHash for collecting multivalue mapping:\n\nEdgeNGramm -> {postingOffset}\n\n\n. \nIt also allows appending EdgeNGramms with the least number of bytes to make unique entries from them. Now, it wastefully appends every EdgeNGramm with 5 bytes. ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16239667",
            "date": "2017-11-05T18:15:34+0000",
            "content": "LUCENE-7863.patch with more efficient ngram mapping with BytesRefHash. byterefshash-bench.txt benchmark results on i5 and ssd. \nSummary\n\n------------> Report sum by Prefix (index) and Round (2 about 2 out of 18)\nOperation   round work dir  src                          flush  cdc                  runCnt   recsPerRun    rec/s    elapsedSec     avgUsedMem  avgTotalMem         directory size, Mb\nindex           0 edge      EnwikiEdgeContentSource      000.00 Lucene70Codec        1           5000001    619.77    8,067.48   1,397,385,984  1,512,570,880          23,783.00\nindex           1 deriv     EnwikiEmptyEdgeContentSource 50.00  DeriveBodyRevCodec   1           5000001    571.40    8,750.42   2,095,467,008  6,383,730,688           7,687.00\n\n ------------> Report sum by Prefix (search) and Round (2 about 2 out of 18)\nOperation   round work dir  src                          flush cdc                  runCnt   recsPerRun   rec/s  elapsedSec    avgUsedMem    avgTotalMem directory size, Mb\nsearch_50       0 edge      EnwikiEdgeContentSource     000.00 Lucene70Codec        1           50        17.63        2.84 1,291,492,352  1,510,998,016          23,783.00\nsearch_50       1 deriv     EnwikiEmptyEdgeContentSource 50.00 DeriveBodyRevCodec   1           50         5.97        8.38 2,205,875,712  6,383,730,688           7,687.00\n\n\n\n\n\tindexing deriving terms is 8% slower than edge-ngramms\n\theap consumption for indexing is 4 times greater (1.5 G vs 6.4)\n\tindex size more than 3 times smaller. I expect bigger gain on regular indices.\n\tsearch throughput is 3 lower with derivative terms. But it's only few cold searches. There is a reason why searching wildcards on deriving terms is slower - it's random reads; however at some point absence of repeating postings should pays back and let it search faster eg when index isn't fully mapped.\n\n ",
            "author": "Mikhail Khludnev"
        },
        {
            "id": "comment-16239673",
            "date": "2017-11-05T18:29:28+0000",
            "content": "Question \nWhat I can use to write ints sequentially, and then reading them back in the same sequence, without Arrays resizes? I've found IntBlockPool.SliceWriter and use in the last patch but I'm not sure whether it's right.\n\nOverall, I need to map (BytesRef, BlockTermState) tuple to EdgeNgram, BlockTermState[]. Now this code ByteArrayDerivativeWriter seems like mad juggling between Arrrays, IntBlockPool, and BytesRefHash I hardly control it already. At least it consumes heap moderately and looks roughly correct. Any ideas and suggestions are really welcome and absolutely appreciated! Thanks. ",
            "author": "Mikhail Khludnev"
        }
    ]
}