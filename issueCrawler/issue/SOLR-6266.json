{
    "id": "SOLR-6266",
    "title": "Couchbase plug-in for Solr",
    "details": {
        "affect_versions": "None",
        "status": "Open",
        "fix_versions": [],
        "components": [],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "It would be great if users could connect Couchbase and Solr so that updates to Couchbase can automatically flow to Solr. Couchbase provides some very nice API's which allow applications to mimic the behavior of a Couchbase server so that it can receive updates via Couchbase's normal cross data center replication (XDCR).\n\nOne possible design for this is to create a CouchbaseLoader that extends ContentStreamLoader. This new loader would embed the couchbase api's that listen for incoming updates from couchbase, then marshal the couchbase updates into the normal Solr update process. \n\nInstead of marshaling couchbase updates into the normal Solr update process, we could also embed a SolrJ client to relay the request through the http interfaces. This may be necessary if we have to handle mapping couchbase \"buckets\" to Solr collections on the Solr side.",
    "attachments": {
        "solr-couchbase-plugin-0.0.5.1-SNAPSHOT.tar.gz": "https://issues.apache.org/jira/secure/attachment/12675499/solr-couchbase-plugin-0.0.5.1-SNAPSHOT.tar.gz",
        "solr-couchbase-plugin-0.0.3-SNAPSHOT.tar.gz": "https://issues.apache.org/jira/secure/attachment/12671473/solr-couchbase-plugin-0.0.3-SNAPSHOT.tar.gz",
        "solr-couchbase-plugin-0.0.5-SNAPSHOT.tar.gz": "https://issues.apache.org/jira/secure/attachment/12672771/solr-couchbase-plugin-0.0.5-SNAPSHOT.tar.gz",
        "solr-couchbase-plugin.tar.gz": "https://issues.apache.org/jira/secure/attachment/12669194/solr-couchbase-plugin.tar.gz"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Karol Abramczyk",
            "id": "comment-14136148",
            "date": "2014-09-16T20:34:41+0000",
            "content": "I have been working for couple of days on Solr-Couchbase Plugin which is based on Elasticsearch Couchbase Plugin.  It's main features are as follows:\n\n\tDesigned as a RequestHandler which mimics the behaviour of Couchbase server\n\tBuilt for Solrcloud cluster configuration - under development. Currently supports locking to have only one RequestHandler running in Solr Cluster. Recovery from network failures and cluster reconfiguration not supported yet.\n\tReal time indexing\n\tSupport for different data types\n\tSupport for nested documents\n\tCurrently no support for multiple collections - all documents are indexed to one collection.\n\n\n\nI attach source code of this plugin. "
        },
        {
            "author": "Karol Abramczyk",
            "id": "comment-14136176",
            "date": "2014-09-16T20:48:24+0000",
            "content": "Solr Couchbase plugin source code "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14136357",
            "date": "2014-09-16T22:18:08+0000",
            "content": "Karol,\n\nThanks for contributing your work! I should have time this week to review the patch and provide some feedback.\n\nJoel "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14140907",
            "date": "2014-09-19T17:19:24+0000",
            "content": "I reviewed Karol's contribution today, it looks great. Let's use this as our base implementation.\n\nIt looks like Karol has worked out a lot of details of how to embed the Couchbase API's and handle documents. This is excellent.\n\nI think we need to take a step back and do some planning around two areas before iterating on what's here.\n\n1) SolrCloud architecture. Some questions to think about:\n\nHow does the plugin work in the context of a single collection?  Should it run in all replicas or just leaders?\n\nHow does the plugin work in the context of multiple collections sharing the same Solr nodes? Should there be a different CAPIServer running for each collection? Or should there be a CAPIServer per Solr node?\n\n2) Error handling. We'll need to understand the different failure scenarios and have strategies for handling them. And we'll need to fully understand how the Couchbases API's account for failure scenarios.\n\nI'll need to catch up on the Couchbase API's before I can weigh-in on these issue. I should have time to review the API's next week. In the meantime if anyone has any thoughts fire away. "
        },
        {
            "author": "Karol Abramczyk",
            "id": "comment-14141185",
            "date": "2014-09-19T19:58:25+0000",
            "content": "Joel Bernstein In the meantime I finished my basic implementation of CAPIServer failover. Solr plugin runs only one CAPIServer on the leader of shard1 and replicas put a watch on it to start a new CAPIServer when the first one goes down. I will update the source and remove unnecessary dependencies. "
        },
        {
            "author": "Karol Abramczyk",
            "id": "comment-14141187",
            "date": "2014-09-19T19:59:00+0000",
            "content": "Updated plugin source "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14141222",
            "date": "2014-09-19T20:19:58+0000",
            "content": "Karol,\n\nCan you explain your thinking with the SolrCloud design? Why only run the CAPIServer on the shard leader, why not run it on all replicas?\n\nIt seems like it would be a simpler design to run it on all replicas.  "
        },
        {
            "author": "Karol Abramczyk",
            "id": "comment-14143073",
            "date": "2014-09-22T10:20:47+0000",
            "content": "Joel,\n\nMy plan was to collect Couchbase data with a single Solr shard leader (to start with) and let Solr replicate the data to the other replicas. I think that running CAPIServer on all replicas is pointless, as at the end they will send all received documents to the shard leader to index it. And this would cause increased network load because of the communication between Couchase servers and all the CAPIServers on replicas and also between the Solr replicas and Solr shards. I also wasn't sure if running CAPIServer on multiple replicas would result in indexing one document multiple times with different IDs. However, it could be useful to run CAPIServer on all shard leaders, but it would require the shard leader to calculate if the received document should be indexed in it.  "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14143257",
            "date": "2014-09-22T14:44:28+0000",
            "content": "From my understanding the CAPIServer is listening on an ip/port. Couchbase can be configured to replicate a bucket to a specific host and port.  So running the CAPIServer on each replica just means that there will be many CAPIServers listening. The actual replication session will be between Couchbase and a single CAPIServer. So in a single replication session documents will flow to one CAPIServer and that CAPIServer will move the documents into the distributed indexing flow.\n\nFrom this scenario running a CAPIServer on all replicas really has no downside. \n\nBut running the CAPIServer from just the leader has a couple of major downsides:\n\n1) Leaders and replicas will change. Couchbase is pointing directly to an ip:port. If all of sudden that node is no longer the leader then replication has stopped. If the CAPIServer is running on all replicas then this is not an issue. \n\n2) If we run the CAPIServer only on leaders then we need to manage bringing up and down CAPIServers, This adds unneeded complexity to the implementation and tests. \n\nWe don't have to worry about duplicate indexing on shards by running CAPIServers on the replicas. If we inject the documents properly into the SolrCloud indexing flow, then SolrCloud will ensure that documents get to the right place.\n\nWhat we do have to consider very carefully is whether we need a CAPIServer running per Collection or per Solr node, because this effects the entire design.\n\nMy thinking is that we should have a single CAPIServer per Solr node to service all collections. I'm assuming that the CAPIServer has thread overhead that we don't want for each collection. \n\nBut if we decide to go this route, then we will need to route documents to correct collections based on the bucket name. We'll need to also figure out where to place the CAPIServer so there is only one per node. \n\n\n\n "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-14144590",
            "date": "2014-09-23T09:19:03+0000",
            "content": "Hi Joel, Karol and I work together, so I thought I'd chime in.\n\nWe'll need to also figure out where to place the CAPIServer so there is only one per node.\nI think there is no such place for \"global\" components in Solr yet, the only special component that is global being the CoreAdminHandler. It would be a nice feature, but it's outside the scope of this issue.\n\nSo, if you can't have what you like you have to like what you have  This means that for now the only option is to run an instance of CAPIServer per collection.\n\nFrom my understanding the CAPIServer is listening on an ip/port. Couchbase can be configured to replicate a bucket to a specific host and port.\n\nKarol is working now on using the Couchbase REST API to configure Couchbase automatically to send docs to a particular instance of CAPIServer that is active. This will eliminate the need for manual configuration on the Couchbase end, and will allow to re-target the replication to any other instance that becomes active, should the current instance of CAPIServer disappear.\n\nRegarding running of CAPIServers on all replicas: with the auto-configuration mechanism as described above it's not needed, it's enough to activate a single instance per collection, using e.g. always the first shard's leader. If this node goes down, another leader will be elected and the CAPIServer instance will activate there and register itself with Couchbase.\n\nCouchbase always sends all changes for a bucket to a replica, so if you had in mind an optimization where each shard would get only its own documents then it wouldn't work - CAPIServer-s would get all documents anyway and they would have to discard (N-1)/N docs - so this would only create heavier load on Couchbase and Solr.\n\nIf we ran multiple active CAPIServer-s on replicas it wouldn't work right either - copies of the same documents would be received multiple times, and while they would be correctly re-routed to the right shards, each shard would receive multiple copies and the ordering would be non-deterministic - not so important for adds but crucial for a mix of adds / deletes. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14144805",
            "date": "2014-09-23T13:49:02+0000",
            "content": "So, it appears that I am misunderstanding something about how Couchbase XDCR works. What it seems like you're saying, is that Couchbase will replicate each document to each available CAPIserver at a XDCR endpoint. \n\nI had assumed that this would not be the case. I had assumed that Couchbase would auto-discover the nodes for a specific replication end point. But, that it would only send each document to one node for the cross-datacenter replication. The receiving node would then be responsible for the intra-cluster replication. \n\n\n\n\n\n\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14144858",
            "date": "2014-09-23T14:54:18+0000",
            "content": "It makes sense to have a per-cluster/per-collection/per shard components in Solr. \n\nThe component should be available at all nodes and SolrCloud should ensure that it runs in only one node (depending on scope) at a given time.\n\nFor this usecase\n\n\n\tI need a per collection component\n\tSolrCloud should ensure that one and only one instance of this component runs per collection\n\tThe component lifecycle should be managed by SolrCloud. i.e callbacks for component init() when it is started up in a node. If possible , give a unload() callback if the system decided to switch the node\n\n\n "
        },
        {
            "author": "Andrzej Bialecki",
            "id": "comment-14144863",
            "date": "2014-09-23T14:56:24+0000",
            "content": "I'm no Couchbase expert by any means, but my reading of the docs indicates that you can only define a destination cluster as a single host:port to receive all documents from a bucket, and Couchbase then sends all documents to a selected host:port and it's the task of the target cluster to handle distribution across the target cluster. In a sense it's similar to how Solr's distrib indexing works.\n\nSo if we had multiple active CAPIServer-s and each were registered as an XCDR destination then each of them would receive all documents. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14144944",
            "date": "2014-09-23T15:59:23+0000",
            "content": "Andrzej, you may be right, but let's confirm. It seems like the right design for XCDR would be to only replicate each document once across datacenters. Then let the receiving node handle the intra-cluster replication. I'll try to get confirmation on which approach is used. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14144967",
            "date": "2014-09-23T16:16:53+0000",
            "content": "Andrzej, just reread your last comment more closely. It seems that you're thinking that the CAPIServer in the remote datacenter will automatically forward each document to all available CAPIServers. I was thinking that the CAPIServer would just pass the documents to the local bulkDocs implementation and be done with it. But I could be mistaken here. I'll review some code and see if I can confirm. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14144992",
            "date": "2014-09-23T16:39:23+0000",
            "content": "Just did a quick review of:\n\n https://github.com/couchbaselabs/couchbase-capi-server/blob/master/src/main/java/com/couchbase/capi/servlet/CAPIServlet.java\n\nIt appears the documents are not forwarded to other CAPIServers automatically. They are just processed locally, so it is the responsibility of the bulkDocs implementation to handle intra-cluster replication/routing. \n\nSo this appears to be the basic flow:\n\n1) Couchbase sets up the initial replication and discovers the available CAPIServers through the couchbase behavior api.\n2) Couchbase replicates each document once across datacenter to one of the available CAPIServers. (Still needs to be confirmed).\n3) The local CAPIServer only forwards the docs to the bulkDocs implementation\n\nIf this scenario is correct, then we control the intra-cluster replication ourselves, and we can run a CAPIServer on each replica without any issues.\n "
        },
        {
            "author": "Karol Abramczyk",
            "id": "comment-14147850",
            "date": "2014-09-25T15:24:10+0000",
            "content": "Joel,\n\nI confirmed today, that Couchbase replicates each document to only one of the running CAPIServers. My test configuration was Solr 1 and 2 running CAPIServers 1 and 2. Couchbase test bucket had 2 documents A and B. Only CAPIServer 1 was configured for replication with Couchbase, but in CouchbaseBehaviour.getNodesServingPool() method it was placing info about itself and CAPIServer 2 as well. When replication finished, CAPIServer 1 indexed only document A, and CAPISErver 2 only document B. So you were right about the Couchbase replication. But it also seems that only one CAPIServer configured with Couchbase is sufficient as long as it knows about the other CAPIServers in this cluster. We could register operating CAPIServers in ZooKeeper, to have this info available for every node. I didn't check what happens if more CAPIServers is configured with Couchabse XDCR. "
        },
        {
            "author": "Varun",
            "id": "comment-14148183",
            "date": "2014-09-25T19:25:43+0000",
            "content": "Andrzej,\n In your setup have you tried partial update ? Lets say you have 10 docs in couchbase, and change one of them, does solr again get all 10 again, or just the changed one from couchbase?\nSeems in your bulkDocs method you are always returning empty result list. Shouldn't solr index and return latest revision number of documents, to let couchbase know what revision solr already have ? "
        },
        {
            "author": "Karol Abramczyk",
            "id": "comment-14148329",
            "date": "2014-09-25T21:19:13+0000",
            "content": "Varun,\nI found out today I missed this results in bulkDocs and in revsDiff. I already fixed it, so now in bulkDocs Solr gets only documents that are not in the index or the revision is different. "
        },
        {
            "author": "Varun",
            "id": "comment-14148654",
            "date": "2014-09-26T02:52:17+0000",
            "content": "Thanks Karol and Andrzej. Please upload the latest patch so that we can improve it further. We are trying to setup an instance and try out various scenarios. "
        },
        {
            "author": "Joel Bernstein",
            "id": "comment-14149105",
            "date": "2014-09-26T12:44:29+0000",
            "content": "Karol,\n\nThanks for confirming the couchbase behavior. Let's see if we can reach consensus on the SolrCloud design. Here is what I propose:\n\n1) I agree that having a CAPIServer per-collection makes the most sense for the reasons that Andrzej laid out. \n\n2) Based on your findings, let's go with the simplest plan of having CAPIServers listening on all replicas. This will also be the most robust scenario and allow Couchbase to replicate in multiple threads to multiple nodes simultaneously.\n\n3) In CouchbaseBehaviour.getNodesServingPool()  lets auto-discover the running CAPIServers from the existing Zookeeper state information. To keep things simple we could return all active replicas in the collection. Or we could ping each active replica on the CAPIServer port to make sure the CAPIServer is running.\n\nLet's also not keep any extra book-keeping in Zookeeper unless we absolutely have to for this ticket.\n\nJoel\n\n\n. "
        },
        {
            "author": "Karol Abramczyk",
            "id": "comment-14149583",
            "date": "2014-09-26T16:01:37+0000",
            "content": "Joel, Varun,\n\nI fixed the code today to properly handle revsDiff and bulkDocs requests. It still runs only one CAPIServer per collection. I'm trying to implement automatic XDCR configuration via XDCR REST, but for unknown reasons I cannot create a remote cluster reference there. Usually I get response \"Cluster uuid does not match the requested.\". But I can get all remote clusters though.  "
        },
        {
            "author": "Karol Abramczyk",
            "id": "comment-14156376",
            "date": "2014-10-02T11:10:58+0000",
            "content": "commons-io-2.4 is required by couchbase-capi-server project used in this plugin "
        },
        {
            "author": "Karol Abramczyk",
            "id": "comment-14157988",
            "date": "2014-10-03T13:54:22+0000",
            "content": "I fixed couple of critical errors in this plugin, like setting numVBuckets parameter for running this plugin on Macs, synchronization speed improvements, using ant fixcrlf instead of executing dos2unix to patch solr configuration files and few more. I attach latest 0.0.5 version snapshot. "
        },
        {
            "author": "Kwan-I Lee",
            "id": "comment-14173588",
            "date": "2014-10-16T09:50:14+0000",
            "content": "Karol, Andrzej,\n\nI'm interested in how this plugin handles different replication failure scenarios. Here is one of my tests:\n1. Add some documents in Couchbase.\n2. Activate this plugin, creating remote cluster and replication in Couchbase. - Data successfully pushed to Solr through XDCR. The Couchbase documents are now visible in Solr. \n3. Stop Solr instance. Add a document, doc1, in Couchbase.\n4. Restart Solr instance and activate plugin.\n\nWith Elasticsearch-Couchbase plugin, doc1 will be pushed to Elasticsearch node once the machine is back. However with this plugin, the replication of doc1 will fail and never go to Solr instance. \n\nI spent some time debugging and tracing both Elasticsearch and Solr plugin code. For Elasticsearch one, couchbase.capi.servlet.ClusterMapServlet.doGet() will eventually get correct pool from req.getPathInfo(). However for Solr one, req.getPathInfo() keeps getting null value for pool no matter how many times Couchbase sends doc1 update request to Solr plugin. \nI'm testing it on Mac, so not sure if it happens in other systems. "
        },
        {
            "author": "Karol Abramczyk",
            "id": "comment-14175174",
            "date": "2014-10-17T15:47:55+0000",
            "content": "Kwan-I Lee,\n\nHandling replication failure hasn't been implemented yet in this plugin. I think that most important improvents now are:\n\n\tautomatic XDCR configuration in Couchbase Server at plugin startup\n\ttests for current functionality\n\n\n\nWith both this improvents I had difficulties so far and couldn't succeed. However, when this is done, all following work should be much easier.\n\nIn the meantime I fixed some important bugs, so I attach the latest sourcecode.\n "
        },
        {
            "author": "Kwan-I Lee",
            "id": "comment-14175485",
            "date": "2014-10-17T20:08:54+0000",
            "content": "Karol,\n\nThanks for the reply. \nI also tried automatic XDCR configuration in Couchbase server by using the APIs they provide. The problem I encountered is they don't have API to switch XDCR protocol (at least I didn't find it on their website.) By default the protocol is set to version 2 upon creation, while we need version 1 to make the plugin work. I'll try to talk with Couchbase people to see if there's any way to make it. \n\nKwan "
        },
        {
            "author": "Karol Abramczyk",
            "id": "comment-14178553",
            "date": "2014-10-21T15:55:12+0000",
            "content": "I made this plugin's repository public. It's here: https://github.com/LucidWorks/solr-couchbase-plugin. I think it would be much more convenient to use it with Pull Requests to provide new features. "
        }
    ]
}