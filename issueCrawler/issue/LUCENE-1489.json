{
    "id": "LUCENE-1489",
    "title": "highlighter problem with n-gram tokens",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/highlighter"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Won't Fix",
        "status": "Resolved"
    },
    "description": "I have a problem when using n-gram and highlighter. I thought it had been solved in LUCENE-627...\n\nActually, I found this problem when I was using CJKTokenizer on Solr, though, here is lucene program to reproduce it using NGramTokenizer(min=2,max=2) instead of CJKTokenizer:\n\n\npublic class TestNGramHighlighter {\n\n  public static void main(String[] args) throws Exception {\n    Analyzer analyzer = new NGramAnalyzer();\n    final String TEXT = \"Lucene can make index. Then Lucene can search.\";\n    final String QUERY = \"can\";\n    QueryParser parser = new QueryParser(\"f\",analyzer);\n    Query query = parser.parse(QUERY);\n    QueryScorer scorer = new QueryScorer(query,\"f\");\n    Highlighter h = new Highlighter( scorer );\n    System.out.println( h.getBestFragment(analyzer, \"f\", TEXT) );\n  }\n\n  static class NGramAnalyzer extends Analyzer {\n    public TokenStream tokenStream(String field, Reader input) {\n      return new NGramTokenizer(input,2,2);\n    }\n  }\n}\n\n\n\nexpected output is:\nLucene <B>can</B> make index. Then Lucene <B>can</B> search.\n\nbut the actual output is:\nLucene <B>can make index. Then Lucene can</B> search.",
    "attachments": {
        "LUCENE-1489.patch": "https://issues.apache.org/jira/secure/attachment/12427643/LUCENE-1489.patch",
        "lucene1489.patch": "https://issues.apache.org/jira/secure/attachment/12421091/lucene1489.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-01-26T22:26:19+0000",
            "content": "As I mentioned on the Solr list, I've discovered similar problems when highlighting with the ShingleFilter. (ShingleFilter does n-gram processing on Tokens, whereas NGramAnalyzer does n-gram processing on characters.) Here's a variation on Koji's demo program that exhibits some problems with ShingleFilter, as well as offering a slightly more textured example of how things work with NGramAnalyzer:\n\n\nimport org.apache.lucene.analysis.Analyzer;\nimport org.apache.lucene.analysis.TokenStream;\nimport org.apache.lucene.queryParser.QueryParser;\nimport org.apache.lucene.search.Query;\nimport org.apache.lucene.search.highlight.QueryScorer;\nimport org.apache.lucene.search.highlight.NullFragmenter;\nimport org.apache.lucene.search.highlight.Highlighter;\nimport org.apache.lucene.analysis.ngram.NGramTokenizer;\nimport org.apache.lucene.analysis.shingle.ShingleFilter;\nimport org.apache.lucene.analysis.WhitespaceTokenizer;\nimport java.io.Reader;\n\npublic class Main {\n\n    public static void main(String[] args) throws Exception {\n        testAnalyzer(new BigramShingleAnalyzer(true), \"Bigram shingle analyzer (bigrams and unigrams)\");\n        testAnalyzer(new NGramAnalyzer(), \"Bigram (non-shingle) analyzer (bigrams only)\");\n    }\n\n    static void testAnalyzer(Analyzer analyzer, String analyzerDescription) throws Exception {\n        System.out.println(\"Testing analyzer \" + analyzerDescription + \"...\");\n        System.out.println(\"---------------------------------\");\n        test(analyzer, \"Lucene can index and can search\", \"Lucene\");\n        test(analyzer, \"Lucene can make an index\", \"can\");\n        test(analyzer, \"Lucene can index and can search\", \"can\");\n        test(analyzer, \"Lucene can index can search and can highlight\", \"can\");\n        test(analyzer, \"Lucene can index can search and can highlight\", \"+index +search\");\n        System.out.println();\n    }\n\n    static void test(Analyzer analyzer, String text, String queryStr) throws Exception {\n        QueryParser parser = new QueryParser(\"f\", analyzer);\n        Query query = parser.parse(queryStr);\n        QueryScorer scorer = new QueryScorer(query, \"f\");\n        Highlighter h = new Highlighter(scorer);\n        h.setTextFragmenter(new NullFragmenter()); // We're not testing fragmenter here.\n        System.out.println(h.getBestFragment(analyzer, \"f\", text) + \" [query='\" + queryStr + \"']\");\n    }\n\n    static class NGramAnalyzer extends Analyzer {\n        public TokenStream tokenStream(String field, Reader input) {\n            return new NGramTokenizer(input, 2, 2);\n        }\n    }\n\n    static class BigramShingleAnalyzer extends Analyzer {\n        boolean outputUnigrams;\n\n        public BigramShingleAnalyzer(boolean outputUnigrams) {\n            this.outputUnigrams = outputUnigrams;\n        }\n\n        public TokenStream tokenStream(String field, Reader input) {\n            ShingleFilter sf = new ShingleFilter(new WhitespaceTokenizer(input));\n            sf.setOutputUnigrams(outputUnigrams);\n            return sf;\n        }\n    }\n}\n\n\n\nHere's the current output, with commentary:\n\nTesting analyzer Bigram shingle analyzer (bigrams and unigrams)...\n---------------------------------\n// works ok:\n<B>Lucene</B> can index and can search [query='Lucene']\n// works ok:\nLucene <B>can</B> make an index [query='can']\n// same as Koji's example:\nLucene <B>can index and can</B> search [query='can']\n// if there are three matches, they all get bundled into a single highlight:\nLucene <B>can index can search and can</B> highlight [query='can']\n// it doesn't have to be the same search term that matches:\nLucene can <B>index can search</B> and can highlight [query='+index +search']\n\nTesting analyzer Bigram (non-shingle) analyzer (bigrams only)...\n---------------------------------\n// works ok:\n<B>Lucene</B> can index and can search [query='Lucene']\n// is 'an' being treated as a match for 'can'(?):\nLucene <B>can make an</B> index [query='can']\n// same as Koji's example:\nLucene <B>can index and can</B> search [query='can']\n// if there are three matches, they all get bundled into a single highlight:\nLucene <B>can index can search and can</B> highlight [query='can']\n// not sure what' happening here:\nLucene can <B>index can search and</B> can highlight [query='+index +search']\n\n\n\nI'm interested what others think, but for me it makes sense to classify both of these as the same issue. From a high-level perspective, the problem in each case seems to be that Highlighter.getBestTextFragments(TokenStream tokenStream, String text, boolean mergeContiguousFragments, int maxNumFragments) makes use of a TokenGroup abstraction that doesn't really work for the n-gram or the bigram shingle case:\n\nA TokenGroup is supposed to represent \"one, or several overlapping tokens, along with the score(s) and the scope of the original text\". (I assume TokenGroup was introduced to deal with synonym filter expansions.) Tokens are determined to overlap or not basically by seeing whether tokenB.startOffset() >= tokenA.endOffset(). (It's slightly more complex than this, but that's approximately what the test in TokenGroup.isDistinct() amounts to.) With the two analyzers under discussion, that criterion basically means that each token \"overlaps\" with the next.\n\nIn Koji's bigram case, consider how \"dogs\" would get tokenized:\n\n\"do\" (startOffset=0, endOffset=2)\n\"og\" (startOffset=1, endOffset=3)\n\"gs\" (startOffset=2, endOffset=4)\n\nOr in my shingle case, consider how \"I love Lucene\" would get tokenized:\n\n\"I\" (startOffset=0, endOffset=1)\n\"I love\" (startOffset=0, endOffset=6)\n\"love\" (startOffset=2, endOffset=6)\n\"love Lucene\" (startOffset=2, endOffset=13)\n\"Lucene\" (startOffset=7, endOffset=13)\n\nIn both cases, you never have a token whose startOffset is >= the preceding token's endOffset. So all these tokens are part of the same TokenGroup. That should mean these tokens all \"overlap\", but that would make for a rather mysterious notion of \"overlapping\". ",
            "author": "Chris Harris",
            "id": "comment-12667469"
        },
        {
            "date": "2009-01-27T11:56:21+0000",
            "content": "It looks to me like this could be fixed in the \"Formatter\" classes when marking up the output string.\n\nCurrently classes such as SimpleHTMLFormatter in their \"highlightTerm\" method put a tag around the whole section of text, if it contains a hit, i.e.\n\nSimpleHTMLFormatter.java\n\tpublic String highlightTerm(String originalText, TokenGroup tokenGroup)\n\t{\n\t\tStringBuffer returnBuffer;\n\t\tif(tokenGroup.getTotalScore()>0)\n\t\t{\n\t\t\treturnBuffer=new StringBuffer();\n\t\t\treturnBuffer.append(preTag);\n\t\t\treturnBuffer.append(originalText);\n\t\t\treturnBuffer.append(postTag);\n\t\t\treturn returnBuffer.toString();\n\t\t}\n\t\treturn originalText;\n\t}\n\n\n\nThe TokenGroup object passed to this method contains all of the tokens and their scores so it should be possible to use this information to deconstruct the originalText parameter and inject markup according to which tokens in the group had a match rather than putting a tag around the whole block.  Some complexity may lie in handling token streams that produce tokens that \"rewind\" to earlier offsets.\nSimpleHtmlFormatter suddenly seems less simple!\n\nTokenStreams that produce entirely overlapping streams of tokens will automatically be broken into multiple TokenGroups because TokenGroup has a maximum number of linked Tokens it will ever hold in a single group.\n\nI haven't got the time to fix this right now but if someone has a burning need to leap in, the above seems like what may be required.\n\nCheers\nMark\n\n\n\n ",
            "author": "Mark Harwood",
            "id": "comment-12667654"
        },
        {
            "date": "2009-10-02T00:53:52+0000",
            "content": "Here's a patch to Highlighter.java that fixes the examples.  The basic idea is to throw away (or ignore) overlapping tokens when they don't have a score, so that a token group doesn't get expanded beyond a sequence of tokens that should be highlighted. ",
            "author": "David Bowen",
            "id": "comment-12761435"
        },
        {
            "date": "2009-10-02T01:01:32+0000",
            "content": "Mark, I tried the approach you suggested of using the Formatter interface.  I found it didn't work because the Formatter did not have a way to map the tokens in the token group into the text.  This could be fixed by providing a public accessor function for TokenGroup's matchStartOffset field.  However, it seems convoluted to go to the trouble of constructing a TokenGroup only to have every Formatter have to take it all apart again to find the places within it that need highlighting.  It seems to me that the purpose of a TokenGroup is to identify (up to) one span of characters that needs to be highlighted. ",
            "author": "David Bowen",
            "id": "comment-12761439"
        },
        {
            "date": "2009-10-02T01:05:11+0000",
            "content": "By the way. here is the output from Chris's test program with this patch:\n\nTesting analyzer Bigram shingle analyzer (bigrams and unigrams)...\n---------------------------------\n<B>Lucene</B> can index and can search [query='Lucene']\nLucene <B>can</B> make an index [query='can']\nLucene <B>can</B> index and <B>can</B> search [query='can']\nLucene <B>can</B> index <B>can</B> search and <B>can</B> highlight [query='can']\nLucene can <B>index</B> can <B>search</B> and can highlight [query='+index +search']\n\nTesting analyzer Bigram (non-shingle) analyzer (bigrams only)...\n---------------------------------\n<B>Lucene</B> can index and can search [query='Lucene']\nLucene <B>can</B> make an index [query='can']\nLucene <B>can</B> index and <B>can</B> search [query='can']\nLucene <B>can</B> index <B>can</B> search and <B>can</B> highlight [query='can']\nLucene can <B>index</B> can <B>search</B> and can highlight [query='+index +search']\n\n\n ",
            "author": "David Bowen",
            "id": "comment-12761441"
        },
        {
            "date": "2009-12-10T22:20:03+0000",
            "content": "Updated patch to work with tokenizer API changes. ",
            "author": "David Bowen",
            "id": "comment-12788970"
        },
        {
            "date": "2010-01-07T04:07:49+0000",
            "content": "I tried this patch. After applying, following testcase fail:\n\n\n    [junit] Testcase: testOverlapAnalyzer2(org.apache.lucene.search.highlight.HighlighterTest):\tFAILED\n    [junit] null expected:<<B>Hi[-]Speed</B>10 foo> but was:<<B>Hi[</B>-<B>]Speed</B>10 foo>\n    [junit] junit.framework.ComparisonFailure: null expected:<<B>Hi[-]Speed</B>10 foo> but was:<<B>Hi[</B>-<B>]Speed</B>10 foo>\n    [junit] \tat org.apache.lucene.search.highlight.HighlighterTest$30.run(HighlighterTest.java:1558)\n    [junit] \tat org.apache.lucene.search.highlight.SynonymTokenizer$TestHighlightRunner.start(HighlighterTest.java:1947)\n    [junit] \tat org.apache.lucene.search.highlight.HighlighterTest.testOverlapAnalyzer2(HighlighterTest.java:1594)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:212)\n    [junit] \n    [junit] \n    [junit] Test org.apache.lucene.search.highlight.HighlighterTest FAILED\n\n ",
            "author": "Jens Muecke",
            "id": "comment-12797501"
        },
        {
            "date": "2012-05-23T23:34:19+0000",
            "content": "Is this still a problem? ",
            "author": "Lance Norskog",
            "id": "comment-13282040"
        },
        {
            "date": "2012-05-24T01:12:09+0000",
            "content": "This problem was my motivation for creating FastVectorHighlighter. Once FVH was committed, I'd never tried the combination of CJKTokenizer + Highlighter. So, I don't know the latest situation.\n\nI'd like to close this as won't fix. ",
            "author": "Koji Sekiguchi",
            "id": "comment-13282094"
        }
    ]
}