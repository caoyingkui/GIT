{
    "id": "LUCENE-648",
    "title": "Allow changing of ZIP compression level for compressed fields",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "1.9,                                            2.0.0,                                            2.1",
        "resolution": "Won't Fix",
        "status": "Closed"
    },
    "description": "In response to this thread:\n\n      http://www.gossamer-threads.com/lists/lucene/java-user/38810\n\nI think we should allow changing the compression level used in the call to java.util.zip.Deflator in FieldsWriter.java.  Right now it's hardwired to \"best\":\n\n      compressor.setLevel(Deflater.BEST_COMPRESSION);\n\nUnfortunately, this can apparently cause the zip library to take a very long time (10 minutes for 4.5 MB in the above thread) and so people may want to change this setting.\n\nOne approach would be to read the default from a Java system property, but, it seems recently (pre 2.0 I think) there was an effort to not rely on Java System properties (many were removed).\n\nA second approach would be to add static methods (and static class attr) to globally set the compression level?\n\nA third method would be in document.Field class, eg a setCompressLevel/getCompressLevel?  But then every time a document is created with this field you'd have to call setCompressLevel since Lucene doesn't have a global Field schema (like Solr).\n\nAny other ideas / prefererences for either of these methods?",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2006-08-11T01:29:02+0000",
            "content": "Just curious, have you tried other values in here to see what kind of difference it makes before we go looking for a solution?  Could you maybe put together a little benchmark that tries out the various levels and report back?\n\nIt could be possible to add another addDocument method from the IndexWriter, so you could change it per document, we could make it part of the constructor to IndexWriter or we could do it as mentioned above.  I am not sure what is the best way just yet.\n\nI think this also may fall under the notion of the Flexible Indexing thread that we have been talking about (someday it will get implemented). ",
            "author": "Grant Ingersoll",
            "id": "comment-12427385"
        },
        {
            "date": "2006-08-11T02:37:55+0000",
            "content": "Good question!  I will try to get the original document if possible and also run some simple tests to see the variance of CPU time consumed vs % compressed. ",
            "author": "Michael McCandless",
            "id": "comment-12427394"
        },
        {
            "date": "2006-08-11T05:17:58+0000",
            "content": "I think the compression level is only one part of the performance problem. Another drawback of the current implementation is how compressed fields are being merged: the FieldsReader uncompresses the fields, the SegmentMerger concatenates them and the FieldsWriter compresses the data again. The uncompress/compress steps are completely unnecessary and result in a large overhead. Before a document is written to the disk, the data of its fields is even being compressed twice. Firstly, when the DocumentWriter writes the single-document segment to the RAMDirectory, secondly, when the SegmentMerger merges the segments inside the RAMDirectory to write the merged segment to the disk.\n\nPlease checkout Jira Issue 629 (http://issues.apache.org/jira/browse/LUCENE-629), where I recently posted a patch that fixes this problem and increases the indexing speed significantly. I also included some performance test results which quantify the improvement. Mike, it would be great if you could also try out the patched version for your tests with the compression level. ",
            "author": "Michael Busch",
            "id": "comment-12427421"
        },
        {
            "date": "2006-08-11T06:30:44+0000",
            "content": "I you find that compression level has a meaningful impact (which it may not as suggested), one approach for a low-impact fix would be to allow the end user to specify their own Inflater/Deflater when creating the IndexWriter. If not specified, then behaviour remains as is.  If the user specifies a different compression level when retrieving the document, that's their bad luck. ",
            "author": "Jason Polites",
            "id": "comment-12427449"
        },
        {
            "date": "2006-08-11T21:24:04+0000",
            "content": "\nOK I ran some basic benchmarks to test the effect on indexing of\nvarying the ZIP compression level from 0-9.\n\nLucene currently hardwires compression level at 9 (= BEST).\n\nI found a decent text corpus here:\n\n     http://people.csail.mit.edu/koehn/publications/europarl\n\nI ran all tests on the \"Portuguese-English\" data set, which is total\nof 327.5 MB of plain text across 976 files.\n\nI just ran the demo IndexFiles, modified to add the file contents as\nonly a compressed stored field (ie not indexed).  Note that this\n\"amplifies\" the cost of compression because in a real setting there\nwould also be a number of indexed fields.\n\nI didn't change any of the default merge factor settings.  I'm running\non Ubuntu Linux 6.06, single CPU (2.4 ghz Pentium 4) desktop machine with\nindex stored on an internal ATA hard drive.\n\nI first tested indexing time with and without the patch from\nLUCENE-629 here:\n\n      old version: 648.7 sec\n\n  patched version: 145.5 sec\n\nWe clearly need to get that patch committed & released!  Compressed\nfields are far more costly than they ought to be, and people are now\nusing this (as of 1.9 release).\n\nSo, then I ran all subsequent tests with the above patch applied.  All\nnumbers are avg. of 3 runs:\n\n  Level  Index time (sec)  Index size (MB)\n\n   None              65.3            322.3          \n      0              92.3            322.3\n      1              80.8            128.8\n      2              80.6            122.2\n      3              81.3            115.8\n      4              89.8            111.3\n      5             104.0            106.2\n      6             121.8            103.6\n      7             131.7            103.1\n      8             144.8            102.9\n      9             145.5            102.9\n\nQuick conclusions:\n\n\n\tThere is indeed a substantial variance when you change the compression\n    level.\n\n\n\n\n\tThe \"sweet spot\" above seems to be around 4 or 5 \u2013 should we\n    change the default from 9?\n\n\n\n\n\tI would still say we should make it possible for Lucene users to\n    change the compression level?\n\n ",
            "author": "Michael McCandless",
            "id": "comment-12427630"
        },
        {
            "date": "2006-08-13T06:13:04+0000",
            "content": "I agree.  I like the idea of externalizing this, too, as suggested by Robert on the mailing list. ",
            "author": "Otis Gospodnetic",
            "id": "comment-12427735"
        },
        {
            "date": "2006-09-15T20:12:24+0000",
            "content": "Won't fix, as I think it is agreed that compression should be handled outside of Lucene and then stored as a binary value ",
            "author": "Grant Ingersoll",
            "id": "comment-12435104"
        }
    ]
}