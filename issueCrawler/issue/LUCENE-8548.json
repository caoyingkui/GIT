{
    "id": "LUCENE-8548",
    "title": "Reevaluate scripts boundary break in Nori's tokenizer",
    "details": {
        "components": [],
        "status": "Resolved",
        "resolution": "Fixed",
        "fix_versions": [
            "master (8.0)",
            "7.7"
        ],
        "affect_versions": "None",
        "labels": "",
        "priority": "Minor",
        "type": "Improvement"
    },
    "description": "This was first reported in https://issues.apache.org/jira/browse/LUCENE-8526:\n\nTokens are split on different character POS types (which seem to not quite line up with Unicode character blocks), which leads to weird results for non-CJK tokens:\n\n\u03b5\u1f30\u03bc\u03af is tokenized as three tokens: \u03b5/SL(Foreign language) + \u1f30/SY(Other symbol) + \u03bc\u03af/SL(Foreign language)\nka\u0320k\u031at\u0361\u0255\u0348a\u0320k\u031a is tokenized as ka/SL(Foreign language) + \u0320/SY(Other symbol) + k/SL(Foreign language) + \u031a/SY(Other symbol) + t/SL(Foreign language) + \u0361\u0255\u0348/SY(Other symbol) + a/SL(Foreign language) + \u0320/SY(Other symbol) + k/SL(Foreign language) + \u031a/SY(Other symbol)\n\u0411\u0430\u0300\u043b\u0442\u0438\u0447\u043a\u043e\u0304 is tokenized as \u0431\u0430/SL(Foreign language) + \u0300/SY(Other symbol) + \u043b\u0442\u0438\u0447\u043a\u043e/SL(Foreign language) + \u0304/SY(Other symbol)\ndon't is tokenized as don + t; same for don't (with a curly apostrophe).\n\u05d0\u05d5\u05b9\u05d2\u05f3\u05d5\u05bc is tokenized as \u05d0\u05d5\u05b9\u05d2/SY(Other symbol) + \u05d5\u05bc/SY(Other symbol)\n\u041coscow (with a Cyrillic \u041c and the rest in Latin) is tokenized as \u043c + oscow\nWhile it is still possible to find these words using Nori, there are many more chances for false positives when the tokens are split up like this. In particular, individual numbers and combining diacritics are indexed separately (e.g., in the Cyrillic example above), which can lead to a performance hit on large corpora like Wiktionary or Wikipedia.\n\nWork around: use a character filter to get rid of combining diacritics before Nori processes the text. This doesn't solve the Greek, Hebrew, or English cases, though.\n\nSuggested fix: Characters in related Unicode blocks\u2014like \"Greek\" and \"Greek Extended\", or \"Latin\" and \"IPA Extensions\"\u2014should not trigger token splits. Combining diacritics should not trigger token splits. Non-CJK text should be tokenized on spaces and punctuation, not by character type shifts. Apostrophe-like characters should not trigger token splits (though I could see someone disagreeing on this one).",
    "attachments": {
        "testCyrillicWord.dot.png": "https://issues.apache.org/jira/secure/attachment/12949056/testCyrillicWord.dot.png",
        "screenshot-1.png": "https://issues.apache.org/jira/secure/attachment/12949313/screenshot-1.png",
        "LUCENE-8548.patch": "https://issues.apache.org/jira/secure/attachment/12949740/LUCENE-8548.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16665014",
            "author": "Robert Muir",
            "content": "As far as the suggested fix, why reinvent the wheel? In unicode each character gets assigned a script integer value. But there are special values such as \"Common\" and \"Inherited\", etc.\n\nSee https://unicode.org/reports/tr24/ or icutokenizer code https://github.com/apache/lucene-solr/blob/master/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ScriptIterator.java#L141\n\n\u00a0 ",
            "date": "2018-10-26T10:52:02+0000"
        },
        {
            "id": "comment-16665235",
            "author": "Jim Ferenczi",
            "content": "Thanks for the pointer\u00a0Robert Muir, I'll work on a patch. ",
            "date": "2018-10-26T14:17:31+0000"
        },
        {
            "id": "comment-16688242",
            "author": "Christophe Bismuth",
            "content": "Hi Jim Ferenczi, have you started to work on patch or maybe I could help? I'm not an Unicode guru but I can read the docs and learn. Feel free to let me know. ",
            "date": "2018-11-15T15:46:16+0000"
        },
        {
            "id": "comment-16689780",
            "author": "Jim Ferenczi",
            "content": "I didn't start to work on it so feel free to start something if you are interested\u00a0Christophe Bismuth.\u00a0 ",
            "date": "2018-11-16T18:15:17+0000"
        },
        {
            "id": "comment-16690004",
            "author": "Christophe Bismuth",
            "content": "Yes, I'm interested in this issue \u00a0I'll start to work on it and let you know. ",
            "date": "2018-11-16T20:59:54+0000"
        },
        {
            "id": "comment-16693478",
            "author": "Christophe Bismuth",
            "content": "I'll use the test failure below as a starting point.\n\n\n  // LUCENE-8548 - file TestKoreanAnalyzer.java\n  public void testCyrillicWord() throws IOException {\n    final Analyzer analyzer = new KoreanAnalyzer(TestKoreanTokenizer.readDict(),\n        KoreanTokenizer.DEFAULT_DECOMPOUND, KoreanPartOfSpeechStopFilter.DEFAULT_STOP_TAGS, false);\n    assertAnalyzesTo(analyzer, \"\u043coscow\",\n        new String[]{\"\u043coscow\"},\n        new int[]{0},\n        new int[]{6},\n        new int[]{1}\n    );\n  }\n\n ",
            "date": "2018-11-20T16:51:28+0000"
        },
        {
            "id": "comment-16694827",
            "author": "Christophe Bismuth",
            "content": "I'm hacking around in the KoreanTokenizer class, but I'll need some mentoring to keep going on.\n\nHere is what I've done so far:\n\n\tImplement a Cyrillic test failure (see previous comment)\n\tLocate the KoreanAnalyzer and KoreanTokenizer classes\n\tLocate the ICUTokenizer and its CompositeBreakIterator class attribute (following UAX #29: Unicode Text Segmentation)\n\tTry to make Ant nori module depend on icu module to try to reuse some ICUTokenizer logic parts (but I failed to tweak Ant scripts)\n\tEnable verbose output (see output below)\n\tEnable Graphiz ouput (see attached picture)\n\tDebug step by step the org.apache.lucene.analysis.ko.KoreanTokenizer#parse method\n\tAdd a breakpoint in the DictionaryToken constructor to try to understand how and when tokens are built (I also played with outputUnknownUnigrams parameter)\n\n\n\nI would need some code or documentation pointers when you have time.\n\n\n\nTokenizer verbose output:\n\n\nPARSE\n\n  extend @ pos=0 char=\u043c hex=43c\n    1 arcs in\n    UNKNOWN word len=1 1 wordIDs\n      fromIDX=0: cost=138 (prevCost=0 wordCost=795 bgCost=138 spacePenalty=0) leftID=1793 leftPOS=SL)\n        **\n      + cost=933 wordID=36 leftID=1793 leastIDX=0 toPos=1 toPos.idx=0\n\n  backtrace: endPos=1 pos=1; 1 characters; last=0 cost=933\n    add token=DictionaryToken(\"\u043c\" pos=0 length=1 posLen=1 type=UNKNOWN wordId=0 leftID=1798)\n  freeBefore pos=1\nTEST-TestKoreanAnalyzer.testCyrillicWord-seed#[9AA9487A32EFEB]:    incToken: return token=DictionaryToken(\"\u043c\" pos=0 length=1 posLen=1 type=UNKNOWN wordId=0 leftID=1798)\n\nPARSE\n\n  extend @ pos=1 char=o hex=6f\n    1 arcs in\n    UNKNOWN word len=6 1 wordIDs\n      fromIDX=0: cost=-1030 (prevCost=0 wordCost=795 bgCost=-1030 spacePenalty=0) leftID=1793 leftPOS=SL)\n        **\n      + cost=-235 wordID=30 leftID=1793 leastIDX=0 toPos=7 toPos.idx=0\n    no arcs in; skip pos=2\n    no arcs in; skip pos=3\n    no arcs in; skip pos=4\n    no arcs in; skip pos=5\n    no arcs in; skip pos=6\n  end: 1 nodes\n\n  backtrace: endPos=7 pos=7; 6 characters; last=1 cost=-235\n    add token=DictionaryToken(\"w\" pos=6 length=1 posLen=1 type=UNKNOWN wordId=0 leftID=1798)\n    add token=DictionaryToken(\"o\" pos=5 length=1 posLen=1 type=UNKNOWN wordId=0 leftID=1798)\n    add token=DictionaryToken(\"c\" pos=4 length=1 posLen=1 type=UNKNOWN wordId=0 leftID=1798)\n    add token=DictionaryToken(\"s\" pos=3 length=1 posLen=1 type=UNKNOWN wordId=0 leftID=1798)\n    add token=DictionaryToken(\"s\" pos=2 length=1 posLen=1 type=UNKNOWN wordId=0 leftID=1798)\n    add token=DictionaryToken(\"o\" pos=1 length=1 posLen=1 type=UNKNOWN wordId=0 leftID=1798)\n  freeBefore pos=7\n\n ",
            "date": "2018-11-21T15:11:25+0000"
        },
        {
            "id": "comment-16696038",
            "author": "Robert Muir",
            "content": "\nTry to make Ant nori module depend on icu module to try to reuse some ICUTokenizer logic parts (but I failed to tweak Ant scripts)\n\nLets avoid this: this logic in the ICUTokenizer should stay private. Otherwise its api (javadocs, etc) will be confusing, it should not expose any of its guts stuff as protected or public.\n\nThe high level is, if you want to break on script boundaries, unicode specifies a way to do it. you don't have to reinvent the wheel. You can get the property data from text files https://www.unicode.org/Public/11.0.0/ucd/Scripts.txt, or you can depend on icu jar for access to the properties. The link to ICUTokenizer just gives an example of how you might do it, but it should not be exposed to this nori.\n\u00a0\n\n\u00a0 ",
            "date": "2018-11-22T15:46:17+0000"
        },
        {
            "id": "comment-16696052",
            "author": "Christophe Bismuth",
            "content": "Great! Thank you Robert Muir, I'll dig into this  ",
            "date": "2018-11-22T15:59:00+0000"
        },
        {
            "id": "comment-16696072",
            "author": "Jim Ferenczi",
            "content": "Yes we should not depend on the icu module. You need to implement something specific anyway because the tokenizer in the Nori module uses a rolling buffer to read the input so it has its own logic to inspect the underlying characters.\n\n\n\nAdd a breakpoint in the\u00a0DictionaryToken\u00a0constructor to try to understand how and when tokens are built (I also played with\u00a0outputUnknownUnigrams\u00a0parameter)\n\n\n\u00a0\n\nCurrently Nori breaks on character class. The character classes are defined in the MeCab model\u00a0https://bitbucket.org/eunjeon/mecab-ko-dic/src/df15a487444d88565ea18f8250330276497cc9b9/seed/char.def?at=master&fileviewer=file-view-default\u00a0and we access them through the CharacterDefinition class. You can see the logic to group unknown words here:\n\nhttps://github.com/apache/lucene-solr/blob/master/lucene/analysis/nori/src/java/org/apache/lucene/analysis/ko/KoreanTokenizer.java#L729\n\nSo instead of splitting on character class, you need to extend the logic to break on script boundaries. If the new block contains multiple character classes (that are compatible with each other) you still need to choose one character id to extract the costs associated to that block:\n\nhttps://github.com/apache/lucene-solr/blob/master/lucene/analysis/nori/src/java/org/apache/lucene/analysis/ko/KoreanTokenizer.java#L745\n\nIn such case picking the first character id in the block should be enough.\n\n\u00a0\n\n\u00a0 ",
            "date": "2018-11-22T16:09:53+0000"
        },
        {
            "id": "comment-16696085",
            "author": "Robert Muir",
            "content": "Yeah, if you want to just change that logic as jim suggests, I would look into https://docs.oracle.com/javase/7/docs/api/java/lang/Character.UnicodeScript.html#of(int) which is build into the JDK. ",
            "date": "2018-11-22T16:20:34+0000"
        },
        {
            "id": "comment-16696093",
            "author": "Christophe Bismuth",
            "content": "That is really nice, thank you Jim Ferenczi and Robert Muir, I should be able to start patch, thanks again! ",
            "date": "2018-11-22T16:27:41+0000"
        },
        {
            "id": "comment-16697336",
            "author": "Christophe Bismuth",
            "content": "I've made some progress and opened PR #505 to share them with you. Feel free to stop me as I don't want to make you loose your time.\n\nHere is what has been done so far:\n\n\n\tBreak on script boundaries with built-in JDK API,\n\tTrack character classes in a growing byte array,\n\tI feel a tiny bit lost when it comes to extract costs: should I call unkDictionary.lookupWordIds(characterId, wordIdRef) for each tracked character class?\n\t\u043coscow word is correctly parsed in the Graphviz output below ...\n\t... but test failed on this line and I still have to understand why.\n\n\n\n ",
            "date": "2018-11-23T16:33:53+0000"
        },
        {
            "id": "comment-16700881",
            "author": "Jim Ferenczi",
            "content": "Thanks Christophe Bismuth. I played with your patch and I have a better idea of what can be achieved now. Currently we rely on the unicode block defined in the MeCab model to split unknown words. This work fine for most of the cases but there are some edge cases that are not handled correctly:\n\n\tCombining diacritics are splitted from their associated character.\n\tSome extra latin tokens are not recognized correctly.\n\n\n\nHowever there are some things that we need to preserve. The splits on punctuation is required, we don't want to mix punctuations and letters so the \"don't\" case should still be splitted as \"don\" and \"t\".\n\nWe also don't want to add custom rules to merge scripts together so the latin+cyrillic case (\u041coscow) should also be splitted into two tokens, that's what the ICUTokenizer does so I don't think we should do differently here. I should have stated this before so I apologize for that but the goal here is to improve the detection of script boundaries using the unicode specification.\n\nI attached a patch that I built on top of your pr. It implements the logic to handle Inherited and Common characters (they should be merged with the surrounding text if they are not punctuations chars). This means that the following cases are not splitted anymore:\n\n*\u00a0\u03b5\u1f30\u03bc\u03af\n\n*\u00a0ka\u0320k\u031at\u0361\u0255\u0348a\u0320k\u031a\n\n*\u00a0\u0411\u0430\u0300\u043b\u0442\u0438\u0447\u043a\u043e\u0304\n\nThe other examples in the description have the same output than before.\u00a0\n\n\u00a0 ",
            "date": "2018-11-27T19:07:04+0000"
        },
        {
            "id": "comment-16701958",
            "author": "Christophe Bismuth",
            "content": "Thanks a lot for sharing this Jim Ferenczi, and no worries at all as the first iteration was an interesting journey! I think taking time to read about Viterbi would help me some more, let's add it to my pretty own todo list \n\nI diffed your patch with master and debugged new tests step-by-step, and I think I understand the big picture. Among others, I totally missed the if (isCommonOrInherited(scriptCode) && isCommonOrInherited(sc) == false) condition which is essential.\n\nI still have one more question, could you please explain what information is contained in the wordIdRef variable and what the unkDictionary.lookupWordIds(characterId, wordIdRef) statement does? The debugger tells me wordIdRef.length is always equal to 36 or 42 and even though 42 is a really great number, I'm a tiny lost in there ... ",
            "date": "2018-11-28T14:39:21+0000"
        },
        {
            "id": "comment-16706907",
            "author": "Jim Ferenczi",
            "content": "\n\nI still have one more question, could you please explain what information is contained in the\u00a0wordIdRef\u00a0variable and what the\u00a0unkDictionary.lookupWordIds(characterId, wordIdRef)\u00a0statement does? The debugger tells me\u00a0wordIdRef.length\u00a0is always equal to 36 or 42 and even though 42 is a really great number, I'm a tiny lost in there ...\n\n\n\u00a0\n\nThe wordIdRef is an indirection that maps an id to an array of word ids. These word ids are the candidates for the current word so for known words it is a pointer to the part of speech + cost + ... for each variant of the word (noun, verb, ...) and for character ids this is a pointer to the cost and part of speech for words written with this id (mapped with the unicode scripts). I don't know how you found 36 or 42 but unknown words should always have a single wordIdRef. ",
            "date": "2018-12-03T09:50:44+0000"
        },
        {
            "id": "comment-16706925",
            "author": "ASF subversion and git services",
            "content": "Commit 643ffc6f9fb3f7368d48975d750f75f8a66783e2 in lucene-solr's branch refs/heads/master from Christophe Bismuth\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=643ffc6 ]\n\nLUCENE-8548: The KoreanTokenizer no longer splits unknown words on combining diacritics and\ndetects script boundaries more accurately with Character#UnicodeScript#of.\n\nSigned-off-by: Jim Ferenczi <jimczi@apache.org> ",
            "date": "2018-12-03T10:05:25+0000"
        },
        {
            "id": "comment-16706938",
            "author": "ASF subversion and git services",
            "content": "Commit 01f9adbb3dd4971a1d0eee74a030fcd7948c35b3 in lucene-solr's branch refs/heads/branch_7x from Christophe Bismuth\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=01f9adb ]\n\nLUCENE-8548: The KoreanTokenizer no longer splits unknown words on combining diacritics and\ndetects script boundaries more accurately with Character#UnicodeScript#of.\n\nSigned-off-by: Jim Ferenczi <jimczi@apache.org> ",
            "date": "2018-12-03T10:15:04+0000"
        },
        {
            "id": "comment-16706939",
            "author": "Jim Ferenczi",
            "content": "Thanks Christophe Bismuth and Robert Muir ! ",
            "date": "2018-12-03T10:15:08+0000"
        },
        {
            "id": "comment-16707497",
            "author": "Christophe Bismuth",
            "content": "That's great, thanks Jim Ferenczi for all the details  ",
            "date": "2018-12-03T16:39:12+0000"
        }
    ]
}