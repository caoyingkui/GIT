{
    "id": "SOLR-3993",
    "title": "SolrCloud leader election on single node stucks the initialization",
    "details": {
        "affect_versions": "4.0",
        "status": "Closed",
        "fix_versions": [
            "4.1",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "setup:\n1 node, 4 cores, 2 shards.\n15 documents indexed.\n\nproblem:\ninit stage times out.\n\nprobable cause:\nAccording to the init flow, cores are initialized one by one synchronously.\nActually, the main thread waits ShardLeaderElectionContext.waitForReplicasToComeUp until retry threshold, while replica cores are not yet initialized, in other words there is no chance other replicas go up in the meanwhile.\nstack trace:\nThread [main] (Suspended)\n        owns: HashMap<K,V>  (id=3876)\n        owns: StandardContext  (id=3877)\n        owns: HashMap<K,V>  (id=3878)\n        owns: StandardHost  (id=3879)\n        owns: StandardEngine  (id=3880)\n        owns: Service[]  (id=3881)\n        Thread.sleep(long) line: not available [native method]\n        ShardLeaderElectionContext.waitForReplicasToComeUp(boolean, String) line: 298\n        ShardLeaderElectionContext.runLeaderProcess(boolean) line: 143\n        LeaderElector.runIamLeaderProcess(ElectionContext, boolean) line: 152\n        LeaderElector.checkIfIamLeader(int, ElectionContext, boolean) line: 96\n        LeaderElector.joinElection(ElectionContext) line: 262\n        ZkController.joinElection(CoreDescriptor, boolean) line: 733\n        ZkController.register(String, CoreDescriptor, boolean, boolean) line: 566\n        ZkController.register(String, CoreDescriptor) line: 532\n        CoreContainer.registerInZk(SolrCore) line: 709\n        CoreContainer.register(String, SolrCore, boolean) line: 693\n        CoreContainer.load(String, InputSource) line: 535\n        CoreContainer.load(String, File) line: 356\n        CoreContainer$Initializer.initialize() line: 308\n        SolrDispatchFilter.init(FilterConfig) line: 107\n        ApplicationFilterConfig.getFilter() line: 295\n        ApplicationFilterConfig.setFilterDef(FilterDef) line: 422\n        ApplicationFilterConfig.<init>(Context, FilterDef) line: 115\n        StandardContext.filterStart() line: 4072\n        StandardContext.start() line: 4726\n        StandardHost(ContainerBase).addChildInternal(Container) line: 799\n        StandardHost(ContainerBase).addChild(Container) line: 779\n        StandardHost.addChild(Container) line: 601\n        HostConfig.deployDescriptor(String, File, String) line: 675\n        HostConfig.deployDescriptors(File, String[]) line: 601\n        HostConfig.deployApps() line: 502\n        HostConfig.start() line: 1317\n        HostConfig.lifecycleEvent(LifecycleEvent) line: 324\n        LifecycleSupport.fireLifecycleEvent(String, Object) line: 142\n        StandardHost(ContainerBase).start() line: 1065\n        StandardHost.start() line: 840\n        StandardEngine(ContainerBase).start() line: 1057\n        StandardEngine.start() line: 463\n        StandardService.start() line: 525\n        StandardServer.start() line: 754\n        Catalina.start() line: 595\n        NativeMethodAccessorImpl.invoke0(Method, Object, Object[]) line: not available [native method]\n        NativeMethodAccessorImpl.invoke(Object, Object[]) line: not available\n        DelegatingMethodAccessorImpl.invoke(Object, Object[]) line: not available\n        Method.invoke(Object, Object...) line: not available\n        Bootstrap.start() line: 289\n        Bootstrap.main(String[]) line: 414\n\n\nAfter a while, the session times out and following exception appears:\nOct 25, 2012 1:16:56 PM org.apache.solr.cloud.ShardLeaderElectionContext waitForReplicasToComeUp\nINFO: Waiting until we see more replicas up: total=2 found=0 timeoutin=-95\nOct 25, 2012 1:16:56 PM org.apache.solr.cloud.ShardLeaderElectionContext waitForReplicasToComeUp\nINFO: Was waiting for replicas to come up, but they are taking too long - assuming they won't come back till later\nOct 25, 2012 1:16:56 PM org.apache.solr.common.SolrException log\nSEVERE: Errir checking for the number of election participants:org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /collections/collection1/leader_elect/shard2/election\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:118)\n        at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)\n        at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1249)\n        at org.apache.solr.common.cloud.SolrZkClient$6.execute(SolrZkClient.java:227)\n        at org.apache.solr.common.cloud.SolrZkClient$6.execute(SolrZkClient.java:224)\n        at org.apache.solr.common.cloud.ZkCmdExecutor.retryOperation(ZkCmdExecutor.java:63)\n        at org.apache.solr.common.cloud.SolrZkClient.getChildren(SolrZkClient.java:224)\n        at org.apache.solr.cloud.ShardLeaderElectionContext.waitForReplicasToComeUp(ElectionContext.java:276)\n        at org.apache.solr.cloud.ShardLeaderElectionContext.runLeaderProcess(ElectionContext.java:143)\n        at org.apache.solr.cloud.LeaderElector.runIamLeaderProcess(LeaderElector.java:152)\n        at org.apache.solr.cloud.LeaderElector.checkIfIamLeader(LeaderElector.java:96)\n        at org.apache.solr.cloud.LeaderElector.joinElection(LeaderElector.java:262)\n        at org.apache.solr.cloud.ZkController.joinElection(ZkController.java:733)\n        at org.apache.solr.cloud.ZkController.register(ZkController.java:566)\n        at org.apache.solr.cloud.ZkController.register(ZkController.java:532)\n        at org.apache.solr.core.CoreContainer.registerInZk(CoreContainer.java:709)\n        at org.apache.solr.core.CoreContainer.register(CoreContainer.java:693)\n        at org.apache.solr.core.CoreContainer.load(CoreContainer.java:535)\n        at org.apache.solr.core.CoreContainer.load(CoreContainer.java:356)\n        at org.apache.solr.core.CoreContainer$Initializer.initialize(CoreContainer.java:308)\n        at org.apache.solr.servlet.SolrDispatchFilter.init(SolrDispatchFilter.java:107)\n        at org.apache.catalina.core.ApplicationFilterConfig.getFilter(ApplicationFilterConfig.java:295)\n        at org.apache.catalina.core.ApplicationFilterConfig.setFilterDef(ApplicationFilterConfig.java:422)\n        at org.apache.catalina.core.ApplicationFilterConfig.<init>(ApplicationFilterConfig.java:115)\n        at org.apache.catalina.core.StandardContext.filterStart(StandardContext.java:4072)\n        at org.apache.catalina.core.StandardContext.start(StandardContext.java:4726)\n        at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:799)\n        at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:779)\n        at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:601)\n        at org.apache.catalina.startup.HostConfig.deployDescriptor(HostConfig.java:675)\n        at org.apache.catalina.startup.HostConfig.deployDescriptors(HostConfig.java:601)\n        at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:502)\n        at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1317)\n        at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:324)\n        at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:142)\n        at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1065)\n        at org.apache.catalina.core.StandardHost.start(StandardHost.java:840)\n        at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1057)\n        at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:463)\n        at org.apache.catalina.core.StandardService.start(StandardService.java:525)\n        at org.apache.catalina.core.StandardServer.start(StandardServer.java:754)\n        at org.apache.catalina.startup.Catalina.start(Catalina.java:595)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n        at java.lang.reflect.Method.invoke(Unknown Source)\n        at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:289)\n        at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:414)\n\nFollowed by:\nOct 25, 2012 1:17:27 PM org.apache.solr.cloud.RecoveryStrategy doRecovery\nSEVERE: Recovery failed - trying again... core=collection1\nOct 25, 2012 1:18:32 PM org.apache.solr.common.SolrException log\nSEVERE: Error while trying to recover. core=collection1\nOct 25, 2012 1:18:32 PM org.apache.solr.common.SolrException log\nSEVERE: Error while trying to recover. core=collection1:org.apache.solr.common.SolrException: No registered leader was found, collection:collection1 slice:shard1\n        at org.apache.solr.common.cloud.ZkStateReader.getLeaderProps(ZkStateReader.java:413)\n        at org.apache.solr.common.cloud.ZkStateReader.getLeaderProps(ZkStateReader.java:399)\n        at org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:318)\n        at org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:220)",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Markus Jelsma",
            "id": "comment-13488596",
            "date": "2012-11-01T10:36:35+0000",
            "content": "We're seeing this too using a current trunk on a 10 node test cluster. We can trigger this state if we restart all servlet containers sequentially or roughly at the same time, most nodes are stuck in this state too while others sometimes change to a different state throwing all kinds of exceptions (see list). We can only get out of this state by restarting some servlet containers again. It does not happen at all with Zookeeper's data directory wiped clean. The cluster then starts very nicely. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13490016",
            "date": "2012-11-03T13:17:38+0000",
            "content": "Hey guys - I've been out of power for the week and am headed to ApacheCon for next week, but I'll try and look into this soon. 4.1 is coming soon and I'd like all these issues fixed in it. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13494935",
            "date": "2012-11-11T18:53:35+0000",
            "content": "I think a good way to solve this might be SOLR-4063: CoreContainer should be able to load multiple SolrCore's in parallel rather than just serially.\n\nThen just require a value of at least 2 when running in SolrCloud mode. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13495057",
            "date": "2012-11-12T00:42:14+0000",
            "content": "I've written a test for that issue that fails before SOLR-4063 and passes after it.\n\nI'll commit it shortly. "
        },
        {
            "author": "Commit Tag Bot",
            "id": "comment-13495342",
            "date": "2012-11-12T15:35:23+0000",
            "content": "[trunk commit] Mark Robert Miller\nhttp://svn.apache.org/viewvc?view=revision&revision=1408313\n\nSOLR-3993: If multiple SolrCore's for a shard coexist on a node, on cluster restart, leader election would stall until timeout, waiting to see all of  the replicas come up.\n "
        },
        {
            "author": "Commit Tag Bot",
            "id": "comment-13495346",
            "date": "2012-11-12T15:39:28+0000",
            "content": "[branch_4x commit] Mark Robert Miller\nhttp://svn.apache.org/viewvc?view=revision&revision=1408323\n\nSOLR-3993: If multiple SolrCore's for a shard coexist on a node, on cluster restart, leader election would stall until timeout, waiting to see all of  the replicas come up.\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13495348",
            "date": "2012-11-12T15:39:52+0000",
            "content": "Thanks Alexey! "
        },
        {
            "author": "Alexey Kudinov",
            "id": "comment-13497874",
            "date": "2012-11-15T09:34:49+0000",
            "content": "Thanks Mark!\nI will test it as soon as possible "
        },
        {
            "author": "Werner Maier",
            "id": "comment-13498840",
            "date": "2012-11-16T14:49:24+0000",
            "content": "Thanks Mark, works better. \ntested nightly 2012-11-16  on a 1-shard-3-node-cluster (hardware, each core on different server, jetty container).\n\nI still can reproduce a problem:\n1) simulate a power outage on the cluster (kill -9 on all servers), leaving the zookeeper ensemble running.\n2) start only ONE core that has NOT been the former leader.\n\nResult: loop in Running recovery...\n\n3) restart that core (clean shutdown with kill instead of kill -9):\n\nResult: all will be fine.\n(the node connects to zookeeper, registeres as leader, runs all changes, cleans the zookeeper queue, and finally \nwaits for other replicas to come up. After that timeout it declares itself as leader and all is fine).\n\nSecond Problem (might be the at least similar):\n1) setup: 1 shard, 3 codes, zookeeper ensemble on three servers. node 1 is leader. \n2) thistime killall -9 java (shuts down zookeeper ensemble and solr cores - simulated power outage on all three servers)\n3) start solr core on server 2 and 3 (which has NOT been leader). tries to connect to zookeeper, but can't)\n4) start zookeeper on server 2 and 3 (that still simulates hardware failure of server #1)\n\nsometiomes both a core loops in recovery. \nsometimes a core keeps stuck in \"shutting down\". \"INFO: Client->ZooKeeper status change trigger but we are already closed\".\n\nrestarting the cores helps everytime. \n\nkind regards.\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13499543",
            "date": "2012-11-17T22:48:31+0000",
            "content": "Bummer - hard to make a test that simulates kill -9. I imagine the key issue is that the nodes going down are having the leader ephemerals timeout - perhaps when I get some time I can work on simulating that. "
        },
        {
            "author": "Po Rui",
            "id": "comment-13502629",
            "date": "2012-11-22T07:49:34+0000",
            "content": "TO Werner :\n\n\n\n2) start only ONE core that has NOT been the former leader.\n\u2026\u2026\n\n\nResult: loop in Running recovery...\n\n\n\n\n\nthis only one core will be the leader finally. this will take a long time cause the waitForReplicasComeup() will end for timeout. this core will cancelRecovery() after he is the leader. it wouldn't run in loop cause cancelRecovery() will stop recovery thread. But before this core become the leader the recovery thread try to connect to old leader and do recovery. recovery thread will throw a lot of exceptions along this process duo to the dead leader. it's fine since the recovery thread will die too \n\n\n\n\nSecond Problem (might be the at least similar):\nlive zookeeper is a precondition of solr cluster and also the zookeeper service  must ready before start a solr instance. you'd better use zookeeper service separately\n\n\n\n "
        },
        {
            "author": "Werner Maier",
            "id": "comment-13502672",
            "date": "2012-11-22T09:28:10+0000",
            "content": "To Po: (thanks for your comment)\n\n1) [...]this only one core will be the leader finally. this will take a long time cause the waitForReplicasComeup() [...]\n\nI don't see a WaitForReplicasComeUp in the logs in that case. Maybe I missinterpreted the lots of exceptions in the log.\nAfter power fail or kill -9 it shows (for me) a recovery loop. Maybe this loop will end eventually (I stopped watching \nafter some 10..15min). I'll try that again if I have some minutes. \n\n2) Zookeeper:\nI know. But I'm a sysadmin dealing with real hardware (and failures) - not a programmer that just uses failure-free hardware and proposes preconditions \nI have seen a ROW of 19\"-cabinets going down - even though each of them had TWO redundant Power Lines and the compunting center had a N+2 UPS. So I'm just trying all possible scenarios that I can imagine - and one of theese is a power outage for zookeeper and solr at the same time. Some things can be controlled with startup order (first zookeeper then solr) on single machines, but if multiple machines are involed, this gets difficult.\nIf such a problem can easily circumvented by the solar instance reconnecting to the zookeeper, then the solr instance should just do that.\n\nSo I tried these things and wrote the bug report - as maybe the developers (that do a very good job!) might just not have considered these cases.  "
        },
        {
            "author": "Po Rui",
            "id": "comment-13503023",
            "date": "2012-11-23T05:14:45+0000",
            "content": "To Werner:\n\n1)are you sure your solr is 4.0 rather than 4.0alpha or 4.0beta?  generally, the situation you described lead a lot of waitForReplicasComeup log produced. if no exception it will stop 3 mins latter. those logs due to zookeeper keep those old server's info in clustersate.\n\n2) above situation both refer to zookeeper leader reelection.all solr service can't work before this election procedure over. the solr node may die if the election time over the solr connect-zookeeper retry time. so the problem is hard to deal with since the zook leader reelection procedure is uncontrollable.      "
        },
        {
            "author": "Werner Maier",
            "id": "comment-13503179",
            "date": "2012-11-23T12:32:31+0000",
            "content": "1) Yes I'm sure. as I wrote: \"tested nightly 2012-11-16\" after Mark's fix for the leader election. Sorry, I wrote \"nightly\" instead of \"nightly build\" that would have been more spefific.\nThe nodes were running:\nsolr-impl 4.1-2012-11-16_01-01-43 1410134 - hudson - 2012-11-16 01:10:49\nlucene-impl 4.1-2012-11-16_01-01-43 1410134 - hudson - 2012-11-16 01:08:46\n(and zookeeper 3.4.4)\n\n2) I understand. Then it might be a solution for us just to test the zookeeper connections before starting the solr nodes. this could be done using 4-letter-commands like 'ruok' + 'stat' in the startup script for solr, if I'm not wrong.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13508064",
            "date": "2012-12-01T21:41:16+0000",
            "content": "So I just tested out this scenario, and it seemed to work as expected for me. I killed all the replicas in a shard and then started one that had not been the leader before - it waited to see the other replicas come back - it does this for safety reasons. Once the timeout went by (I think it's 5 minutes), or I started the other replicas, everything went green.\n\nYou can lower the timeout or turn it off if you want. Just set the leaderVoteWait attribute on 'cores' in solr.xml. "
        },
        {
            "author": "Commit Tag Bot",
            "id": "comment-13610562",
            "date": "2013-03-22T16:17:46+0000",
            "content": "[branch_4x commit] Mark Robert Miller\nhttp://svn.apache.org/viewvc?view=revision&revision=1408323\n\nSOLR-3993: If multiple SolrCore's for a shard coexist on a node, on cluster restart, leader election would stall until timeout, waiting to see all of  the replicas come up. "
        }
    ]
}