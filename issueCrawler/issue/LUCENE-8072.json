{
    "id": "LUCENE-8072",
    "title": "Improve accuracy of similarity scores",
    "details": {
        "labels": "",
        "priority": "Minor",
        "resolution": "Unresolved",
        "affect_versions": "None",
        "status": "Open",
        "type": "Task",
        "components": [],
        "fix_versions": []
    },
    "description": "I noticed two things we could do to improve the accuracy of our scores:\n\n\tuse Math.log1p instead of Math.log(1+x), especially when x is expected to be small\n\tuse doubles for intermediate values that are used to compute norms in BM25Similarity",
    "attachments": {
        "LUCENE-8072.patch": "https://issues.apache.org/jira/secure/attachment/12900228/LUCENE-8072.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16274471",
            "date": "2017-12-01T14:43:58+0000",
            "content": "Here is a patch that applies these ideas to various similarities. ",
            "author": "Adrien Grand"
        },
        {
            "id": "comment-16274486",
            "date": "2017-12-01T14:57:32+0000",
            "content": "There is a cost to log1p, I'm not sure we should do this per-hit. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16274490",
            "date": "2017-12-01T15:01:31+0000",
            "content": "Also i dont see the benefit to relevance. I am fine with taking any perf hit, if it really helps, but I think we need to do this carefully on a case-by-case basis, not blindly across the board.\n\nFor example in the BM25 case it does not help to do this in the IDF, such tiny idfs are the stopword case so additional precision does not matter. It also does not help \"behavior\" since Math.log is already required to be semi-monotonic. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16274493",
            "date": "2017-12-01T15:05:08+0000",
            "content": "As far as changes to double precision, we should be careful here too. Really the test improvements for LUCENE-8015 needs to be applied before we make any alterations for formulas because the current tests are too inefficient.\n\nSimilarity has to deal with crazy values for a variety of reasons in lucene and our first challenge is to get all of our scoring behaving properly with monotonicity we need for optimizations. Extra precision in various places may or may not help that, anyway lets avoid playing whack-a-mole  ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16276581",
            "date": "2017-12-04T10:12:56+0000",
            "content": "OK, those looked like easy wins to me but I get your point about better tests. ",
            "author": "Adrien Grand"
        }
    ]
}