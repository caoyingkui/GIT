{
    "id": "SOLR-6095",
    "title": "SolrCloud cluster can end up without an overseer with overseer roles",
    "details": {
        "affect_versions": "4.8",
        "status": "Resolved",
        "fix_versions": [
            "4.10",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "We have a large cluster running on ec2 which occasionally ends up without an overseer after a rolling restart. We always restart our overseer nodes at the very last otherwise we end up with a large number of shards that can't recover properly.\n\nThis cluster is running a custom branch forked from 4.8 and has SOLR-5473, SOLR-5495 and SOLR-5468 applied. We have a large number of small collections (120 collections each with approx 5M docs) on 16 Solr nodes. We are also using the overseer roles feature to designate two specified nodes as overseers. However, I think the problem that we're seeing is not specific to the overseer roles feature.\n\nAs soon as the overseer was shutdown, we saw the following on the node which was next in line to become the overseer:\n\n2014-05-20 09:55:39,261 [main-EventThread] INFO  solr.cloud.ElectionContext  - I am going to be the leader ec2-xxxxxxxxxx.compute-1.amazonaws.com:8987_solr\n2014-05-20 09:55:39,265 [main-EventThread] WARN  solr.cloud.LeaderElector  - \norg.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /overseer_elect/leader\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:119)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)\n\tat org.apache.solr.common.cloud.SolrZkClient$10.execute(SolrZkClient.java:432)\n\tat org.apache.solr.common.cloud.ZkCmdExecutor.retryOperation(ZkCmdExecutor.java:73)\n\tat org.apache.solr.common.cloud.SolrZkClient.makePath(SolrZkClient.java:429)\n\tat org.apache.solr.common.cloud.SolrZkClient.makePath(SolrZkClient.java:386)\n\tat org.apache.solr.common.cloud.SolrZkClient.makePath(SolrZkClient.java:373)\n\tat org.apache.solr.cloud.OverseerElectionContext.runLeaderProcess(ElectionContext.java:551)\n\tat org.apache.solr.cloud.LeaderElector.runIamLeaderProcess(LeaderElector.java:142)\n\tat org.apache.solr.cloud.LeaderElector.checkIfIamLeader(LeaderElector.java:110)\n\tat org.apache.solr.cloud.LeaderElector.access$200(LeaderElector.java:55)\n\tat org.apache.solr.cloud.LeaderElector$ElectionWatcher.process(LeaderElector.java:303)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)\n\tat org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\n\n\n\nWhen the overseer leader node is gracefully shutdown, we get the following in the logs:\n\n2014-05-20 09:55:39,254 [Thread-63] ERROR solr.cloud.Overseer  - Exception in Overseer main queue loop\norg.apache.solr.common.SolrException: Could not load collection from ZK:sm12\n\tat org.apache.solr.common.cloud.ZkStateReader.getExternCollectionFresh(ZkStateReader.java:778)\n\tat org.apache.solr.common.cloud.ZkStateReader.updateClusterState(ZkStateReader.java:553)\n\tat org.apache.solr.common.cloud.ZkStateReader.updateClusterState(ZkStateReader.java:246)\n\tat org.apache.solr.cloud.Overseer$ClusterStateUpdater.run(Overseer.java:237)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Object.wait(Object.java:503)\n\tat org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1342)\n\tat org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:1040)\n\tat org.apache.solr.common.cloud.SolrZkClient$4.execute(SolrZkClient.java:226)\n\tat org.apache.solr.common.cloud.SolrZkClient$4.execute(SolrZkClient.java:223)\n\tat org.apache.solr.common.cloud.ZkCmdExecutor.retryOperation(ZkCmdExecutor.java:73)\n\tat org.apache.solr.common.cloud.SolrZkClient.exists(SolrZkClient.java:223)\n\tat org.apache.solr.common.cloud.ZkStateReader.getExternCollectionFresh(ZkStateReader.java:767)\n\t... 4 more\n2014-05-20 09:55:39,254 [Thread-63] INFO  solr.cloud.Overseer  - Overseer Loop exiting : ec2-xxxxxxxxxx.compute-1.amazonaws.com:8986_solr\n2014-05-20 09:55:39,256 [main-EventThread] WARN  common.cloud.ZkStateReader  - ZooKeeper watch triggered, but Solr cannot talk to ZK\n2014-05-20 09:55:39,259 [ShutdownMonitor] INFO  server.handler.ContextHandler  - stopped o.e.j.w.WebAppContext{/solr,file:/vol0/cloud86/solr-webapp/webapp/},/vol0/cloud86/webapps/solr.war\n\n\n\nNotice how the overseer kept on running almost till the last point i.e. until the jetty context stopped. On some runs, we got the following on the overseer leader node on graceful shutdown:\n\n2014-05-19 21:33:43,657 [Thread-75] ERROR solr.cloud.Overseer  - Exception in Overseer main queue loop\norg.apache.solr.common.SolrException: Could not load collection from ZK:sm71\n\tat org.apache.solr.common.cloud.ZkStateReader.getExternCollectionFresh(ZkStateReader.java:778)\n\tat org.apache.solr.common.cloud.ZkStateReader.updateClusterState(ZkStateReader.java:553)\n\tat org.apache.solr.common.cloud.ZkStateReader.updateClusterState(ZkStateReader.java:246)\n\tat org.apache.solr.cloud.Overseer$ClusterStateUpdater.run(Overseer.java:237)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.InterruptedException\n\tat java.lang.Object.wait(Native Method)\n\tat java.lang.Object.wait(Object.java:503)\n\tat org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1342)\n\tat org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1153)\n\tat org.apache.solr.common.cloud.SolrZkClient$7.execute(SolrZkClient.java:277)\n\tat org.apache.solr.common.cloud.SolrZkClient$7.execute(SolrZkClient.java:274)\n\tat org.apache.solr.common.cloud.ZkCmdExecutor.retryOperation(ZkCmdExecutor.java:73)\n\tat org.apache.solr.common.cloud.SolrZkClient.getData(SolrZkClient.java:274)\n\tat org.apache.solr.common.cloud.ZkStateReader.getExternCollectionFresh(ZkStateReader.java:769)\n\t... 4 more\n2014-05-19 21:33:43,662 [main-EventThread] WARN  common.cloud.ZkStateReader  - ZooKeeper watch triggered, but Solr cannot talk to ZK\n2014-05-19 21:33:43,663 [Thread-75] INFO  solr.cloud.Overseer  - Overseer Loop exiting : ec2-xxxxxxxxxxxx.compute-1.amazonaws.com:8987_solr\n2014-05-19 21:33:43,664 [OverseerExitThread] ERROR solr.cloud.Overseer  - could not read the data\norg.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /overseer_elect/leader\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:127)\n\tat org.apache.zookeeper.KeeperException.create(KeeperException.java:51)\n\tat org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)\n\tat org.apache.solr.common.cloud.SolrZkClient$7.execute(SolrZkClient.java:277)\n\tat org.apache.solr.common.cloud.SolrZkClient$7.execute(SolrZkClient.java:274)\n\tat org.apache.solr.common.cloud.ZkCmdExecutor.retryOperation(ZkCmdExecutor.java:73)\n\tat org.apache.solr.common.cloud.SolrZkClient.getData(SolrZkClient.java:274)\n\tat org.apache.solr.cloud.Overseer$ClusterStateUpdater.checkIfIamStillLeader(Overseer.java:329)\n\tat org.apache.solr.cloud.Overseer$ClusterStateUpdater.access$300(Overseer.java:85)\n\tat org.apache.solr.cloud.Overseer$ClusterStateUpdater$1.run(Overseer.java:293)\n2014-05-19 21:33:43,665 [ShutdownMonitor] INFO  server.handler.ContextHandler  - stopped o.e.j.w.WebAppContext{/solr,file:/vol0/cloud87/solr-webapp/webapp/},/vol0/cloud87/webapps/solr.war\n\n\nAgain the overseer was clinging on till the last moment and by the time it exited the ZK session expired and it couldn't delete the /overseer_elect/leader node. The exception on the next-in-line node was the same i.e. NodeExists for /overseer_elect/leader.\n\nIn both cases, we are left with no overseers after restart. I can easily reproduce this problem by just restarting overseer leader nodes repeatedly.",
    "attachments": {
        "SOLR-6095.patch": "https://issues.apache.org/jira/secure/attachment/12648856/SOLR-6095.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14003046",
            "date": "2014-05-20T10:21:02+0000",
            "content": "I also opened SOLR-6091 but that didn't help. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14003585",
            "date": "2014-05-20T16:26:54+0000",
            "content": "The problem that I could find is in LeaderElector.checkIfIamLeader where we have the following code:\n\nif (seq <= intSeqs.get(0)) {\n      // first we delete the node advertising the old leader in case the ephem is still there\n      try {\n        zkClient.delete(context.leaderPath, -1, true);\n      } catch(Exception e) {\n        // fine\n      }\n\n      runIamLeaderProcess(context, replacement);\n    }\n\n\n\nIf for whatever reason, the zkClient.delete was unsuccessful, we just ignore and go ahead to runIamLeaderProcess(...) which leads to OverseerElectionContext.runLeaderProcess(...) where it tries to create the /overseer_elect/leader node:\n\nzkClient.makePath(leaderPath, ZkStateReader.toJSON(myProps),\n        CreateMode.EPHEMERAL, true);\n\n\nThis is where things go wrong. Because the /overseer_elect/leader node already existed, the zkClient.makePath fails and the node decides to give up because it think that there is already a leader. It never tries to rejoin election ever. Then once the ephemeral /overseer_elect/leader node goes away (after the previous overseer leader exits), the cluster is left with no leader.\n\nShouldn't the node next in line to become a leader try again or rejoin the election instead of giving up? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14012802",
            "date": "2014-05-29T20:02:22+0000",
            "content": "We always restart our overseer nodes at the very last otherwise we end up with a large number of shards that can't recover properly.\n\nDo you know if there is a JIRA issue for that? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14013306",
            "date": "2014-05-30T05:13:07+0000",
            "content": "No, I don't think there's a jira for it. The reason that we could find was that if for some reason the rolling restart sequence matches with the overseer election sequence then the overseer keep shifting with each bounce and are unable to process events. This is kinda okay in small clusters but in large clusters, by the time the rolling restarts complete, some nodes reach \"recovery_failed\" state and won't try to come back up again.\n\nOnce we changed our restart sequence to restart the overseer node at the very last, we did not encounter this problem any more. "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14014137",
            "date": "2014-05-30T19:30:50+0000",
            "content": "Not sure I understand. You bring down first wave, overseers move to second wave. When you bring back first wave, they use the overseer in the second wave to recover and become active. Then you start with the second wave. Why would this be a problem? "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14014535",
            "date": "2014-05-31T06:07:52+0000",
            "content": "Except we don't do our rolling restarts like that. Our restart scripts iterates through hosts looked up using EC2 APIs (and it almost always returns the node names in the same order) and restarts them one by one and after each restart, waits for 60 seconds, verifies that node is up and continues with the next host.\n\nSince the script originally created the nodes too in the same order, the election nodes are also approximately in the same order. This causes each host restart to displace the overseer to the next host in line which is again displaced and so on. "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14014780",
            "date": "2014-05-31T19:49:30+0000",
            "content": "That would explain it, our start script blocks until all cores are active, hence we don't have this issue.. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14021135",
            "date": "2014-06-08T07:24:10+0000",
            "content": "Here's a testcase which fails with these same issue. Finally after discarding many iterations I managed to create this test. This test sets up 16 shards (2x8) and adds overseer roles to three nodes and then in a loop restarts just the overseers over and over again. It fails usually after the a couple of restarts. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14032386",
            "date": "2014-06-16T12:32:11+0000",
            "content": "I have tweaked the roles feature a bit as follows\n\nThe new approach \n\nIf the current order is (everyone who is below is listening to the one right above)\n\n\n\tnodeA-0 <leader>\n\tnodeB-1\n\tnodeC-2\n\tnodeD-3\n\tnodeE-4\n\n\n\nAnd addrole asks nodeD to become overseer\n\n\nAccording to the new approach , a command is sent to nodeD to rejoin election at head, so the new Q becomes \n\n\n\tnodeA-0 <leader>\n\tnodeB-1  nodeD-1\n\tnodeC-2\n\tnodeE-4\n\n\n\nNow, both nodeB and nodeD are waiting on nodeA to become the leader\n\nThe next step is to send a rejoin (not at head) command to nodeB . So the new order automatically is as follows where nodeD is the next node in line to become the leader. \n\n\n\tnodeA-0 <leader>\n\tnodeD-1\n\tnodeC-2\n\tnodeE-4\n\tnodeB-5\n\n\n\n\nThe next step is to send a quit command to nodeA (current leader) . So the new order becomes\n\n\n\n\tnodeD-1 <leader>\n\tnodeC-2\n\tnodeE-4\n\tnodeB-5\n\tnodeA-6\n\n\n\nSo we have promoted nodeD to leader with just 3 operations . The advantage is that , irrespective of the no:of nodes in the queue , the no:of operations is still the same 3 , So it does not matter if it is a big cluster or small. The good thing is there will never be a loss of overseer , even if the designate does not become the leader (because of errors happening in the prioritizeOverseerNodes)\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14032387",
            "date": "2014-06-16T12:33:35+0000",
            "content": "This has the new stress test and the new approach. I plan to commit this soon "
        },
        {
            "author": "Jessica Cheng Mallet",
            "id": "comment-14032616",
            "date": "2014-06-16T16:34:51+0000",
            "content": "What if before step 2 nodeA dies. Is there a possibility that we'd end up with two Overseers (nodeB and nodeD)? What's done to prevent this from happening? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14032619",
            "date": "2014-06-16T16:43:41+0000",
            "content": "Is there a possibility that we'd end up with two Overseers (nodeB and nodeD)? \n\nNo , only one can succeed. If nodeD succeeds it is great, \nIf it does not,  \n\n\tnodeB will become Overseer\n\tnodeD will rejoin at the back because the , \"leader\" node already exists (created by nodeB)\n\tand nodeB will go through all the same steps as explained above\n\n "
        },
        {
            "author": "Jessica Cheng Mallet",
            "id": "comment-14032659",
            "date": "2014-06-16T17:24:15+0000",
            "content": "\nnodeD will rejoin at the back because the , \"leader\" node already exists (created by nodeB)\nWhen does this happen? The classic zk leader election recipe would not have checked for the leader node. In LeaderElector.checkIfIAmLeader, the node with the smallest seqId deletes the leader node without looking at it before writing itself down as the leader. If the first node that wrote itself down as the leader already passed the amILeader() check in the Overseer loop before the second node overwrites it, it is then possible that the first node will be active for at least one loop iteration while the second node becomes the new leader. \n\nOne of the fundamental assumption of the zk leader election recipe (http://zookeeper.apache.org/doc/trunk/recipes.html#sc_leaderElection), which I believe solr follows, is that seqId is handed out by zookeeper and is therefore unique. This change will violate that assumption, so my question is--what's built around the code in this change that makes it ok to violate that assumption?\n\n\n\nand nodeB will go through all the same steps as explained above\nEven if say what you describe above worked, here nodeB gets re-prioritized down and then nodeC becomes the leader, we still don't have the right results. What happens then? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14032760",
            "date": "2014-06-16T18:42:55+0000",
            "content": "The classic recipe is tweaked a bit so that the churn of large no:of nodes are avoided in prioritizing another a node .In that case,  the node id is created by the client instead of asking zk to create one. The checkIfIamLeader is modified in this new patch to take care of 2 nodes with same sequence Id.  "
        },
        {
            "author": "Jessica Cheng Mallet",
            "id": "comment-14032807",
            "date": "2014-06-16T19:15:13+0000",
            "content": "I see that your new patch is trying to fix the \"seq <= intSeqs.get(0)\" case in LeaderElector, but the fix doesn't quite work. Note that the delete statement is meant to delete the old leader's node in case it hasn't expired yet, which is a possible scenario. If the old leader's node indeed hasn't expired, both nodeB and nodeD will fail your new statement. "
        },
        {
            "author": "Jessica Cheng Mallet",
            "id": "comment-14032814",
            "date": "2014-06-16T19:26:07+0000",
            "content": "Sorry that wasn't true. You were comparing election path, not the leader node. However, this still possibly doesn't work because sortSeqs extracts just the sequence number (n_0000000001) out of the entire node string and sorts based on that, so in fact the sort order of nodeB and nodeD might not be deterministic in different JVM (calls to zk.getChildren does not guarantee return list ordering, so even though the Collections.sort is stable, the original list is not), which makes this new if statement also non-deterministic. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14032836",
            "date": "2014-06-16T19:44:54+0000",
            "content": "I haven't really gone into the implementation of the Arrays.sort() . But as long as the getChildren returns the nodes in the same order , the Arrays.sort() would give the same order right? (sort can be non-deterministic when the input is random right? )Because ZK does not sort based on the sequence number\n\nBut again , this solution does not give 100% guarantee that nodeD would become the leader ,if the last step quit command is not executed . So, there is a very small possibility that the overseer is not a designate, but there will always be a leader. Only if the leader quits , because of an explicit rejoin coreadmin command, or if the node dies "
        },
        {
            "author": "Jessica Cheng Mallet",
            "id": "comment-14032868",
            "date": "2014-06-16T20:11:11+0000",
            "content": "The problem is I don't think getChildren is guaranteed to return nodes in the same order. Its javadoc states\n\n\nThe list of children returned is not sorted and no guarantee is provided as to its natural or lexical order.\n\nIf somehow getChildren doesn't return nodes in the same order (unless we can verify otherwise, and add this as a regression test against each zk upgrade since the API doesn't guarantee it), the sort can possibly get different ordering of nodeB and nodeD so that they both believe they're the top item in their own invocation, and we're back to the temporary two-Overseer case (for one loop iteration). "
        },
        {
            "author": "Jessica Cheng Mallet",
            "id": "comment-14032923",
            "date": "2014-06-16T20:48:17+0000",
            "content": "Just checked zookeeper's code. Its children is held by HashSet in DataNode, which means that if you hit different instances of zookeeper in the ensemble, you may get different results ordering back. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14033424",
            "date": "2014-06-17T04:31:17+0000",
            "content": "The sort is now going to be deterministic "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-14033451",
            "date": "2014-06-17T05:32:29+0000",
            "content": "RollingRestartTest.regularRestartTest() is commented out. If it\u2019s not required, you might want to remove it (or uncomment it and let it run). "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14033734",
            "date": "2014-06-17T12:34:08+0000",
            "content": "RollingRestartTest.regularRestartTest() is commented out. If it\u2019s not required, you might want to remove it (or uncomment it and let it run).\n\nYes, it is not required in its current form. We can remove it. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14035515",
            "date": "2014-06-18T09:39:38+0000",
            "content": "Commit 1603382 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1603382 ]\n\nSOLR-6095 SolrCloud cluster can end up without an overseer with overseer roles "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14035520",
            "date": "2014-06-18T09:42:19+0000",
            "content": "Commit 1603383 from Noble Paul in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1603383 ]\n\nSOLR-6095 SolrCloud cluster can end up without an overseer with overseer roles "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14035785",
            "date": "2014-06-18T14:46:39+0000",
            "content": "Commit 1603467 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1603467 ]\n\nSOLR-6095 Uncaught Exception causing test failures "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14035787",
            "date": "2014-06-18T14:48:12+0000",
            "content": "Commit 1603468 from Noble Paul in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1603468 ]\n\nSOLR-6095 Uncaught Exception causing test failures "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14040728",
            "date": "2014-06-23T13:13:50+0000",
            "content": "Commit 1604791 from Noble Paul in branch 'dev/trunk'\n[ https://svn.apache.org/r1604791 ]\n\nSOLR-6095 wait for http responses "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14040730",
            "date": "2014-06-23T13:15:17+0000",
            "content": "Commit 1604792 from Noble Paul in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1604792 ]\n\nSOLR-6095 wait for http responses "
        }
    ]
}