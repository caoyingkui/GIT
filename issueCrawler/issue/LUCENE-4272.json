{
    "id": "LUCENE-4272",
    "title": "another idea for updatable fields",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "New Feature",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "I've been reviewing the ideas for updatable fields and have an alternative\nproposal that I think would address my biggest concern:\n\n\n\tnot slowing down searching\n\n\n\nWhen I look at what Solr and Elasticsearch do here, by basically reindexing from stored fields, I think they solve a lot of the problem: users don't have to \"rebuild\" their document from scratch just to update one tiny piece.\n\nBut I think we can do this more efficiently: by avoiding reindexing of the unaffected fields.\n\nThe basic idea is that we would require term vectors for this approach (as the already store a serialized indexed version of the doc), and so we could just take the other pieces from the existing vectors for the doc.\n\nI dont think we should discard the idea because vectors are slow/big today, this seems like something we could fix.\n\nPersonally I like the idea of not slowing down search performance to solve the problem, I think we should really start from that angle and work towards making the indexing side more efficient, not vice-versa.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2012-07-30T19:51:42+0000",
            "content": "This is an interesting idea!  And it makes sense to factor this down from ElasticSearch/Solr.\n\nSo we have the codec approach (LUCENE-3837), the stacked-segments approach (LUCENE-4258), and this new approach (copy over already-inverted fields).\n\nWe could quite efficiently add the already-inverted doc (term vectors) to the in-memory postings.  And then there'd be zero impact to search performance, and no (well, small) index format changes.\n\nThe only downside is the use case of replacing tiny fields on otherwise massive docs: in this case the other approaches would be faster at indexing (but still slower at searching).  I agree not slowing down search is a big plus for this approach.\n\nWe'd also need to open up the TV APIs so we can get TVs for a doc in the current segment, for the case where app adds a doc and later (before flush), replaces some fields.  And we need to pool readers in IW so the updates can on-demand resolve the Term to docIDs.  Hmm and we'd need to be able to do so for the in-memory segment (I think we should not support replaceFields by Query for starters). ",
            "author": "Michael McCandless",
            "id": "comment-13425159"
        },
        {
            "date": "2012-07-30T20:14:46+0000",
            "content": "Well I think there are a few other advantages: \n\ncomplexity, e.g. not having to stack segments keeps the number of \"dimensions\" the same. \nThe general structure of the index would be unchanged as well.\n\nto IndexSearcher/Similarity/etc everything would appear just as if someone had deleted and re-added\ncompletely like today: this means we dont have to change our search APIs to have maxDoc(field) or anything\nelse: scoring works just fine.\n\nit seems possible we could support tryXXX incremental updates by docid via just like LUCENE-4203 too, though\nthats just an optimization.\n\nas far as tiny fields on otherwise massive docs, i think we can break this down into 3 layers:\n\n\tdocument 'build' <-- retrieving from your SQL database / sending over the wire / etc\n\tfield 'analyze' <-- actually doing the text analysis etc on the doc\n\tfield 'indexing' <-- consuming the already-analyzed pieces thru the indexer chain/codec flush/etc\n\n\n\nToday people 'pay' for 1, 2, and 3. If they use the solr/es approach they only pay 2 and 3 I think?\nWith this approach its just 3. I think for the vast majority of apps it will be fast enough, as I\nam totally convinced 1 and 2 are the biggest burden on people. I think these are totally possible\nto fix without hurting search performance. I cant imagine many real world apps where its 3, not\n1 and 2, that are their bottleneck AND they are willing to trade off significant search performance for that. ",
            "author": "Robert Muir",
            "id": "comment-13425182"
        },
        {
            "date": "2012-07-30T20:38:42+0000",
            "content": "\nWe'd also need to open up the TV APIs so we can get TVs for a doc in the current segment, for the case where app adds a doc and later (before flush), replaces some fields.\n\nRealistically I'd like to support that anyway for the norms case so that codecs can index term impacts (LUCENE-4198),\nas this is going to involve length normalization in addition to TF. But currently the postings writer has no way\nto \"see\" this.\n\nSo it would be nice if we could do solve that too, then we wouldnt need norms/dvs in the vectors (they are already per-doc).\nThis would make for a faster way of updating docvalues fields: for that specific case I think more can be done\nbut it would be an improvement and fit well. ",
            "author": "Robert Muir",
            "id": "comment-13425208"
        },
        {
            "date": "2012-12-20T11:18:16+0000",
            "content": "That's an interesting idea Robert. I agree that (1) is sometimes more expensive than re-indexing and I'll admit that in the cases I've seen, fetching docs from the DB was a huge bottleneck, because the DB was used for many other application transactions, while search was not the majority of transactions. Also, (2) is not so cheap either. So I agree your approach would keep the users with (3) only.\n\nThere is a downside to this approach, that it requires the app to store everything in the index too (in addition to the DB). Even if it's just term vectors, that's still extra storage. I know that for large applications, the index stores the minimal set of fields that are required to build the search results. For really large apps, the content isn't even there, but rather the search snippets are computed on a different cluster.\nJust want to point that out. It may not be a big deal to small applications ... but then reindexing documents when you have a small application isn't a big deal either ...\n\nI also think that your approach may not work well for apps with relatively high frequency of tiny updates? I mean, today they need to re-index the entire document, doing steps 1-3 and with your approach they'll need to do just #3. But in the approach on LUCENE-4258, the cost of indexing an update is proportional to the size of the update? We still don't know the impact on the search side, but we know for sure that if updates are frequently merged down to the segment (a'la expunge deletes), there is no effect on search?\n\nPerhaps what we should do on LUCENE-4258 is run a benchmark on an index w/ low, mid and high number of updates and measure the impact on search. ",
            "author": "Shai Erera",
            "id": "comment-13536961"
        },
        {
            "date": "2012-12-20T12:04:28+0000",
            "content": "\nPerhaps what we should do on LUCENE-4258 is run a benchmark on an index w/ low, mid and high number of updates and measure the impact on search.\n\nYes. Especially the impact on mean average precision. ",
            "author": "Robert Muir",
            "id": "comment-13536983"
        },
        {
            "date": "2012-12-20T21:28:38+0000",
            "content": "Especially the impact on mean average precision.\n\nI'll focus on performance first because I think that we should give a good solution for DOCS_ONLY type of fields.\n\nAlso, constructing a test which can reliably check the effect on MAP is not trivial. Maybe if e.g. I replace the entire content field, or some part of it.\n\nBut, to measure MAP I'd need to use the TREC (GOV, GOV2) collection, for which I have judgements. But then I believe I'm the only one that can run the test? Unless anyone else has access to that collection? Do you know of any other open collection with judgements that I can use?\n\nNot saying that it's not important to measure, but to me that comes second in the list, at least for the first step of field updates. ",
            "author": "Shai Erera",
            "id": "comment-13537395"
        },
        {
            "date": "2012-12-20T21:41:37+0000",
            "content": "\nI'll focus on performance first because I think that we should give a good solution for DOCS_ONLY type of fields.\n\nI dont know about this. \n\nTo me its not a case of \"progress not perfection\". I don't see the design for LUCENE-4258 scaling beyond DOCS_ONLY + OMIT_NORMS fields. ",
            "author": "Robert Muir",
            "id": "comment-13537402"
        },
        {
            "date": "2012-12-20T21:47:47+0000",
            "content": "That remains to be seen. Storing entire documents (term vectors or not) is not going to scale either I think. Merging will just merge this data over and over .. unless you put it in another index or something. Sivan and I tried that (before 4258) in a project, it didn't perform so well. For every tiny update fetch the content from a stored field (yes we did #2 and #3, not just #3) simply didn't perform.\n\nI think we're coming from different worlds. We may need to develop two different solutions for field updates, each is better for some scenarios.\n\nOr hopefully, the approach on 4258 would prove performing enough, so we stick w/ just one approach. ",
            "author": "Shai Erera",
            "id": "comment-13537408"
        },
        {
            "date": "2012-12-20T22:42:22+0000",
            "content": "+1 on term vector approach\n\nI would like to see the following added to IndexableField:\n/** Expert. index inverted terms for field */\npublic Terms invertedTerms();\n\nthis would allow partial updates via term vectors without having to flatten back into TokenStream first\n\nThis would also facilitate things like the following:\n\n\tindex document into memory index\n\trun \"alert\" queries/per-doc analysis against memory index\n\tget \"terms\" from memory index for all fields and index into on disk index using IndexableField.invertedTerms()\n\tdouble tokenization/analysis/inversion is now avoided\n\n\n\n\n\n\n\n\n ",
            "author": "Tim Smith",
            "id": "comment-13537449"
        },
        {
            "date": "2012-12-21T00:51:44+0000",
            "content": "edit: just to make it clear we dont need to change the index format if we wnt to implement this: its \"just code\".\n\nnorms for unaffected fields can be reused as-is. for the affected fields when digesting the Terms, we could just process them as normal. ",
            "author": "Robert Muir",
            "id": "comment-13537572"
        }
    ]
}