{
    "id": "SOLR-5656",
    "title": "Add autoAddReplicas feature for shared file systems.",
    "details": {
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [
            "4.10",
            "6.0"
        ],
        "components": [],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "When using HDFS, the Overseer should have the ability to reassign the cores from failed nodes to running nodes.\n\nGiven that the index and transaction logs are in hdfs, it's simple for surviving hardware to take over serving cores for failed hardware.\n\nThere are some tricky issues around having the Overseer handle this for you, but seems a simple first pass is not too difficult.\n\nThis will add another alternative to replicating both with hdfs and solr.\n\nIt shouldn't be specific to hdfs, and would be an option for any shared file system Solr supports.\n\nhttps://reviews.apache.org/r/23371/",
    "attachments": {
        "SOLR-5656.patch": "https://issues.apache.org/jira/secure/attachment/12653826/SOLR-5656.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-13993837",
            "date": "2014-05-09T19:07:22+0000",
            "content": "I've got a pretty nicely working patch for auto replica failover ready for this issue. It will need some small extensions to work without a shared filesystem, but it provides all of the ground work that we will want for that. I'll post my current progress in a couple days. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13994516",
            "date": "2014-05-11T10:56:30+0000",
            "content": "It will need some small extensions to work without a shared filesystem, but it provides all of the ground work that we will want for that. I'll post my current progress in a couple days.\n\nThat's awesome, looking forward to it! "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14051445",
            "date": "2014-07-03T13:48:35+0000",
            "content": "Sorry, took me a while to get this patch up.\n\nHere is a first patch for feed back. It's a git patch against trunk from a couple days ago.\n\nI'll add a new patch that's converted to svn trunk shortly.\n\nI'll also comment shortly with more details on the patch. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14051680",
            "date": "2014-07-03T16:57:42+0000",
            "content": "Here is an svn patch against trunk. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14053804",
            "date": "2014-07-07T16:12:12+0000",
            "content": "The approach is fairly simple.\n\nThe Overseer class gets a new thread that periodically evaluates live nodes and cluster state and fires off SolrCore create commands to add replicas when there are not enough replicas up to meet a collections replicationFactor.\n\nThe feature is enabled per collection by an additional boolean create collections API param called autoAddReplicas.\n\nThis feature only works with the Collections API.\n\nIn this initial implementation, replicas are not removed if you end up with too many for some reason, and replicas are not rebalanced when nodes come back to life. You must manually move replicas after restoring a node to rebalance.\n\nThere are three settings exposed:\n\nautoReplicaFailoverWorkLoopDelay: How often the Overseer inspects the clusterstate and possibly takes action.\n\nautoReplicaFailoverWaitAfterExpiration: Once a replica no longer looks live, it won't be replaced until at least this much time has passed after noticing that.\n\nautoReplicaFailoverBadNodeExpiration: Once a replica is marked as looking like it needs to be replaced, if it still looks bad on a future cycle, it will be replaced. Once a node is marked as looking bad, after this much time it will be unmarked.\n\nAdditional automated testing needs to be added as initially I have focused on manual testing. To aid in that, I have improved the cloud-dev scripts to make this type of feature much easier to test. I have once more patch to put up that expands on that a bit by starting another Solr node that can run ZooKeeper external to the cluster and that can be used to view the Solr Admin Cloud tab without actually participating in the cluster. Just makes monitoring while testing easier and takes away needing to run zk yourself and internally on one of the cluster nodes.\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14053968",
            "date": "2014-07-07T18:23:46+0000",
            "content": "Here is another patch with a couple additions:\n\n\n\tA couple fixes that 'ant precommit' brought up.\n\n\n\n\n\tFinished the improvements to the /cloud-scripts I was working on it.\n\n "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-14055639",
            "date": "2014-07-08T22:58:41+0000",
            "content": "An aside: we should probably set up review board for this project.  No need to make it necessary, but it might make it easier to review and thus improve the number of reviews.\n\nHdfsChaosMonkeySafeLeaderTest.java:\n\nLine 28: import org.junit.Ignore;\n\n\ndoesn't seem to be needed\n\nClusterState.java:\n\nLine 218: public String getShardIdByCoreNodeName(String collectionName, String coreNodeName) { \n\n\n\nThis is a general comment, but there are a bunch of strings like coreNodeName, replica names, baseUrls whose relations I'm not sure about (I usually end up logging them and doing some ad-hoc comparisons).  Perhaps turning these into types with proper conversions would make things clearer?  Not necessary for this patch, just wondering your thoughts.\n\nClusterStateUtil.java:\n\nLine 40: *          how to wait before giving up\n\nhow long to wait (this appears multiple times)\n\n\nLine 113: for (Replica replica : replicas) {\n\n\ncan we reduce the amount of duplicate code in these functions?\n\n\nLine 200: int timeOutInSeconds) {\n\n\nSeconds seem limiting, why not milliseconds?\n\n\nLine 242: isAddAddReplicas(...)\n\n\nmove ZkStateReader to first param to match other functions here\n\n\nLine 247: boolean autoAddRepliacs = docCollection.getAutoAddReplicas();\n\n\ncan just return autoAddReplicas here (also Repliacs is mispelled)\n\nZkStateReader.java:\n\n\nLine 369: for (LiveNodesListener listener : liveNodesListeners) {\n\n\nDo we need this?  No one seems to be using it.  It's a little hard to figure out the model as well, i.e. it's not called when you explicitly call updateClusterState.  I think the real issue is ZkStateReader is too complicated, but I haven't thought about how to address that.\n\n\nLine 284: controlJetty = createJetty(controlJettyDir, useJettyDataDir ? getDataDir(testDir\n\n\nWhy can't we call createControlJetty?\n\nMockZkStateReader.java\nThere was some comment that this is only necessary temporarily.  Is that already addressed?  Is there a JIRA "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14055679",
            "date": "2014-07-08T23:50:51+0000",
            "content": "An aside: we should probably set up review board for this project. No need to make it necessary, but it might make it easier to review and thus improve the number of reviews.\n\nI filed an issue a while back: https://issues.apache.org/jira/browse/INFRA-7630\n\n there are a bunch of strings like coreNodeName, replica names, baseUrls whose relations I'm not sure about (I usually end up logging them and doing some ad-hoc comparisons). Perhaps turning these into types with proper conversions would make things clearer? Not necessary for this patch, just wondering your thoughts.\n\nSounds interesting to me - I agree, let's do it another issue though.\n\ncan we reduce the amount of duplicate code in these functions?\n\nI'll look at it - I made an attempt at one point to converge some code and some small annoyance had me just revert it then.\n\nSeconds seem limiting, why not milliseconds?\n\nI think given the speed at which state updates propagate through the system, seconds is all the granularity that is needed, but I don't mind making it milliseconds.\n\nDo we need this? \n\nNo, the listeners were for a failed path. I had thought I had taken them out in git, but probably used SmartGit to edit it in the index and not the working tree and lost the changes. Happens sometimes when I decide to edit with SmartGit.\n\nI think the real issue is ZkStateReader is too complicated\n\nYeah, it kind of became a catch all class for anything above SolrZkClient and below ZkController. I'd just file a JIRA issue to at least start collecting ideas on some refactoring. ZkController could probably also use some editing.\n\nWhy can't we call createControlJetty?\n\nI'll have to look. At one point, because it needed to be different, but I don't remember offhand if that is still the case.\n\nThere was some comment that this is only necessary temporarily. Is that already addressed?\n\nThat's https://issues.apache.org/jira/browse/SOLR-5473. I merged this up to that the first time it was committed. Once that was temporarily reverted and I updated to trunk, I commented that bit out. I'll leave it in until that issue is resolved without putting a ZkStateReader in ClusterState. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14055680",
            "date": "2014-07-08T23:51:08+0000",
            "content": "I'll put up another patch shortly. "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-14055684",
            "date": "2014-07-09T00:03:36+0000",
            "content": "DocCollection.java\n\nLine 154: if (maxShardsPerNode == null) {\n\n\nWhy is this being checked?  https://cwiki.apache.org/confluence/display/solr/Collections+API says replication factor is required, but not maxShards?\n\nSharedFSAutoReplicaFailoverTest.java \n\nLine 133: assertTrue(\"Timeout waiting for all live and active\", ClusterStateUtil.waitForAllActiveAndLive(cloudClient.getZkStateReader(), collection1, 120));\n\n\nHow come you only check collection1 throughout this test?\n\n\nLine 135: assertSliceAndReplicaCount(collection1);\n\n\nWhat about specifically targeting the node with the overseer\n\n\nLine 181: assertEquals(2, slices.size());\n\n\nLots of magic numbers here\n\nSharedFSAutoReplicaFailoverUtilsTest.java\n\nLine 204: * c = collection, s = slice, r = replica, r\\d = node\\d, -\\d = state (1=active,2=recovering,3=down,4=recovery_failed), * = bad replica \n\n\nI can't figure anything out from this description.  Maybe examples would help?  I doubt you are actually saving much space compared to some simple builder. "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-14055706",
            "date": "2014-07-09T00:28:44+0000",
            "content": "autoReplicaFailoverBadNodeExpiration: This name is a bit confusing \u2013 from just  name I can't figure out if this is the time until a node that has been marked bad is retried or until we stop trying once we detect a node is bad.  Maybe something like autoReplicaFailoverBadNodeTimeUntilRetry?\n\nOverseer.java:\n\nLine 356: System.err.println(\"Process msg \" + message);\n\n\nYou meant to leave this in?\n\n\n660:  //if (!checkCollectionKeyExistence(message)) return clusterState;\n\n\nwhy is this commented out?\n\nOverseerAuthReplicaFailoverThread.java:\n\nLine 82: private static Integer lastClusterStateVersion;\n\n\nshould be volatile if static?  Why static?\n\n\nLine 293:  static String getBestCreateUrl(ZkStateReader zkStateReader, DownReplica badReplica) {\n\n\nThe API is a bit confusing, b/c this is the only function that takes a ZkStateReader \u2013 I think this is just b/c you want to test this function.  Can the test just create one of these objects but not start it to simplify the API?\n\nConfigSolr.java:\n\nLine 295: SOLR_AUTOREPLICAFAILOVER, \n\nIs this meant to be here?  There's no accessor?  I think it's only controlled based on what's in the request?\n\nConfigSolrXml.java:\n\nLine 120: propMap.put(CfgProp.SOLR_AUTOREPLICAFAILOVER, doSub(\"solr/solrcloud/bool[@name='genericCoreNodeNames']\"));\n\n\nThis looks wrong \u2013  \u2013 should be \n\nsolr/solrcloud/bool[@name='autoReplicaFailover']\n\n\nConfigSolrXmlOld.java:\n\nLine 168: config.getVal(\"solr/cores/@autoReplicaFailoverWaitAfterExperation\", false));\n\n\nExpiration\n\nlog4j.properties:\n\nLine 29: log4j.logger.org.apache.solr.common.cloud.ClusterStateUtil=DEBUG\n\n\nwe want this on for every test? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14055721",
            "date": "2014-07-09T01:05:23+0000",
            "content": "I doubt you are actually saving much space compared to some simple builder.\n\nIt's not a space saving technique - I find it much faster to construct a clusterstate by typing a string of letters (especially a large one) vs dealing with any java code. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14055741",
            "date": "2014-07-09T01:43:40+0000",
            "content": "This patch addresses most comments.\n\nOther responses coming tomorrow. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14055743",
            "date": "2014-07-09T01:45:43+0000",
            "content": "Line 295: SOLR_AUTOREPLICAFAILOVER, \nIs this meant to be here? \n\nNo, from an earlier path. Removed.\n\nlog4j.properties:\n\nNo, I'll remove it before committing, but find it convenient to keep with the patch until then. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14056501",
            "date": "2014-07-09T17:41:15+0000",
            "content": "ReviewBoard has been setup and this seems like a good issue to trial it with. I created a review request: https://reviews.apache.org/r/23371/\n\nI also sent an email to the dev list so that we can have discussion around ReviewBoard and the Lucene project. "
        },
        {
            "author": "David Smiley",
            "id": "comment-14056572",
            "date": "2014-07-09T18:34:25+0000",
            "content": "It seems this may be the case but I just want to confirm it: will this issue obviate the pointless replication (duplication) of data on a shared file system between replicas? "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-14056749",
            "date": "2014-07-09T20:57:21+0000",
            "content": "ReviewBoard has been setup and this seems like a good issue to trial it with. I created a review request: https://reviews.apache.org/r/23371/\n\nGreat!\n\nI doubt you are actually saving much space compared to some simple builder.\n\nSorry, I meant screen space . "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14056778",
            "date": "2014-07-09T21:31:03+0000",
            "content": "The format is driven by my laziness. If I want to test how a specific clusterstate will choose to fail over, I want to make it super simple to setup such a clusterstate so that we can test a large variety of them with minimal effort. It is a work in progress overall though - hacked together while traveling to California a month or two ago.\n\nSo to setup a clusterstate.\n\ncsr*r2sr3csr2\n\nThat creates 2 collections. Collection1 has 2 shards. In it's first shard, it's first replica is on node 1 and down. It's second replica is on node 2. In it's second shard, a replica is on node 3. Collection2 has a single replica on it's first shard.\n\nI left the printout when you run tests in SharedFSAutoReplicaFailoverUtilsTest so it's easy to check the full printout for the clusterstate created. "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-14056867",
            "date": "2014-07-09T22:38:53+0000",
            "content": "So, I should assume if there is no node number it's on node 1?  And that * attaches to the previous shard?  I still don't really know what \"-\" does. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14056906",
            "date": "2014-07-09T23:37:25+0000",
            "content": "So, I should assume if there is no node number it's on node 1?\n\nCurrently it defaults to 1. I was going to make it explicit, but not a lot of error checking yet anyway, so left it for further improvement later. I figure this will be reused in a few other places that have to choose nodes given a clusterstate.\n\nAnd that * attaches to the previous shard?\n\nThe * marks a replica as the one being replaced. The current replacement algorithm looks at each replica - when it finds one, it looks for the best place to replace it given a clusterstate.\n\nI still don't really know what \"-\" does.\n\nIt just overrides a specific state for a replica in clusterstate.json - so rather than ACTIVE, you could mark them as RECOVERING or DOWN. "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-14056934",
            "date": "2014-07-10T00:12:00+0000",
            "content": "I figure this will be reused in a few other places that have to choose nodes given a clusterstate.\n\nI definitely think this sort of thing is useful, I just find it difficult to parse currently\n\nThe * marks a replica as the one being replaced. The current replacement algorithm looks at each replica - when it finds one, it looks for the best place to replace it given a clusterstate.\n\nWhat I mean here is, for example, csr1-2*r2, does the * bind to (r1-2) or (r2).\n\nCurrently it defaults to 1. I was going to make it explicit, but not a lot of error checking yet anyway, so left it for further improvement later. I figure this will be reused in a few other places that have to choose nodes given a clusterstate.\n\nI think making it explicit is a good idea.  If being explicit isn't required, is csr-2 legal?\n\nIt just overrides a specific state for a replica in clusterstate.json - so rather than ACTIVE, you could mark them as RECOVERING or DOWN.\n\nI wonder if it would be clearer if these states were non-intersecting with the node specification.  So like, A=active, D=down, F=failed, R=recovering or if you are worried about case, could make recovering C or M (for moving, there isn't really a difference from a replica being moved vs recovering logically I think)?  Then you wouldn't need the minus either.  A default of \"A\" there seems reasonable too.  What do you think? "
        },
        {
            "author": "Gregory Chanan",
            "id": "comment-14057012",
            "date": "2014-07-10T01:43:17+0000",
            "content": "I put up a new review on reviewboard. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14057955",
            "date": "2014-07-10T21:07:59+0000",
            "content": "It seems this may be the case but I just want to confirm it: will this issue obviate the pointless replication (duplication) of data on a shared file system between replicas?\n\nThis is just another option. It works both with or without replicas for a shard. There are trade offs in failover transparency, time, and query throughput depending on what you choose.\n\nAnother option I'm about to start pursuing is SOLR-6237 An option to have only leaders write and replicas read when using a shared file system with SolrCloud.\n\nI don't yet fully know what trade offs may come up in that. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14081519",
            "date": "2014-07-31T21:36:22+0000",
            "content": "What I mean here is, for example, csr1-2*r2, does the * bind to (r1-2) or (r2).\n\nIt binds to r1-2. It goes csr and everything binds to the right.\n\nI've got a patch that integrates some of this discussion. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14082353",
            "date": "2014-08-01T15:36:47+0000",
            "content": "If being explicit isn't required, is csr-2 legal?\n\nYeah, it's legal. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14092686",
            "date": "2014-08-11T11:29:57+0000",
            "content": "Didn't ping the issue itself, but I put up a new patch about a week and a half ago: https://reviews.apache.org/r/23371/ "
        },
        {
            "author": "Scott Lindner",
            "id": "comment-14093272",
            "date": "2014-08-11T20:48:00+0000",
            "content": "Does this depend on SOLR-6237? Your comments above seem to imply that it's not, but conceptually can't you end up with multiple replicas pointing to the same location on disk (admittedly I haven't dug into your code)?  If so, what impact does this have with multiple solr instances (i.e. multiple replicas of a given shard) pointing to the same location on disk where each could be accepting write changes? \n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14093309",
            "date": "2014-08-11T21:21:31+0000",
            "content": "Does this depend on SOLR-6237? \n\nNo, that is a separate feature.\n\nbut conceptually can't you end up with multiple replicas pointing to the same location on disk \n\nThere are efforts to prevent his, I don't think it's likely, but we will improve this over time. There are settings that can be tuned around timing as well. I have done a fair amount of manual testing already, but the unit tests will be expanded over time.\n\nIf so, what impact does this have with multiple solr instances (i.e. multiple replicas of a given shard) pointing to the same location on disk where each could be accepting write changes?\n\nYou should be running with the hdfs lock impl so that one of the SolrCores would fail to start.\n\nI plan on committing this soon so that I don't have to keep it up to date with trunk. We can still iteratate if their are further comments. The feature is optional per collection and defaults to off. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14096912",
            "date": "2014-08-14T12:33:15+0000",
            "content": "Commit 1617919 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1617919 ]\n\nSOLR-5656: Add autoAddReplicas feature for shared file systems. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14100805",
            "date": "2014-08-18T16:26:58+0000",
            "content": "Commit 1618655 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1618655 ]\n\nSOLR-5656: Add autoAddReplicas feature for shared file systems. "
        },
        {
            "author": "David Smiley",
            "id": "comment-14168298",
            "date": "2014-10-11T18:58:20+0000",
            "content": "How is it that Solr figures out which HDFS data nodes have the same replicated data and thus would make prime candidates to add a shard replica?  I looked some of the patch but it wasn't apparent to me how it determined this. "
        }
    ]
}