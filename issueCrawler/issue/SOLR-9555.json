{
    "id": "SOLR-9555",
    "title": "Leader incorrectly publishes state for replica when it puts replica into LIR.",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "7.3",
            "master (8.0)"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "See https://jenkins.thetaphi.de/job/Lucene-Solr-master-Linux/17888/consoleFull for an example",
    "attachments": {
        "SOLR-9555-WIP-3.patch": "https://issues.apache.org/jira/secure/attachment/12863903/SOLR-9555-WIP-3.patch",
        "SOLR-9555.patch": "https://issues.apache.org/jira/secure/attachment/12864368/SOLR-9555.patch",
        "SOLR-9555-WIP-2.patch": "https://issues.apache.org/jira/secure/attachment/12863689/SOLR-9555-WIP-2.patch",
        "lir-9555-problem.png": "https://issues.apache.org/jira/secure/attachment/12878186/lir-9555-problem.png",
        "SOLR-9555-WIP.patch": "https://issues.apache.org/jira/secure/attachment/12853386/SOLR-9555-WIP.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-09-24T19:25:14+0000",
            "author": "Alan Woodward",
            "content": "The race looks something like this:\n\n\tA node goes down, and then restarts\n\tThe leader tries to send a document to the starting node, and gets a 503 'not ready yet'\n\tThe node publishes its state as RECOVERING\n\tThe leader's LIR thread publishes the recovering node's state as DOWN\n\tThe node sends a PREPRECOVERY request to the leader\n\tThe leader waits for the node's state to be RECOVERING, but as it's just been set as DOWN by the LIR thread, everything hangs\n\n\n\nI think the fix is for the LIR thread to only set the node's state as DOWN if it's current state is ACTIVE, using ZK versions to check that the leader's state is up-to-date, but I'd like to get comments from people who know this bit of the code better - Shalin Shekhar Mangar Timothy Potter does this look right to you? ",
            "id": "comment-15519503"
        },
        {
            "date": "2016-10-07T15:55:21+0000",
            "author": "Alan Woodward",
            "content": "Another instance of this:\nhttps://jenkins.thetaphi.de/job/Lucene-Solr-master-Windows/6164/ ",
            "id": "comment-15555452"
        },
        {
            "date": "2017-01-17T17:20:53+0000",
            "author": "Alan Woodward",
            "content": "This is causing occasional issues in PeerSyncReplicationTest as well - here's another example: https://jenkins.thetaphi.de/job/Lucene-Solr-master-MacOSX/3785/\n\nPing Shalin Shekhar Mangar ",
            "id": "comment-15826417"
        },
        {
            "date": "2017-01-17T23:12:44+0000",
            "author": "Pushkar Raste",
            "content": "I assume this would happen in the prod deployment (outside of the test itself) as well. \n\nDoes the PeerSyncReplicationTest.waitTillNodesActive method looks good or method itself has some bug?  ",
            "id": "comment-15827013"
        },
        {
            "date": "2017-01-18T03:41:18+0000",
            "author": "Mike Drob",
            "content": "I've seen this same thing happen on a prod cluster. However, in my case, the problem corrected itself after 180 seconds when the leader's wait timed out and the whole process started again. Agree that the fix is to only set DOWN if current state is ACTIVE. I can try to put together a patch later this week if nobody else is already working on it. ",
            "id": "comment-15827382"
        },
        {
            "date": "2017-01-18T08:53:28+0000",
            "author": "Cao Manh Dat",
            "content": "This issue seems related to SOLR-9945. \nAs I discussed with Shalin Shekhar Mangar, \n\nIn SOLR-9945, the reason that leader can not send the update to replica because the underlying socket is closed on replica side, so the leader will put replica into LIR state.\nI don't think we should fix the problem by \"only set DOWN if the current state is ACTIVE\" , because we did not send the update to the replica, so the replica won't have that update, we should put the replica into LIR, it's a right thing to do. I think we should fix the problem by \"close\" the underlying socket based on ZK events. ",
            "id": "comment-15827656"
        },
        {
            "date": "2017-01-18T16:22:33+0000",
            "author": "Alan Woodward",
            "content": "Mike, Pushkar: Yes, this will happen in production, but will correct itself after the 180 seconds.  That's long enough to timeout the test though, and I think it's still worth addressing.  Patches are welcome \n\nDat: We won't be losing any updates, because the replica is already in recovery; the problem is that we have two different places writing state into zookeeper, and they're racing with each other. ",
            "id": "comment-15828351"
        },
        {
            "date": "2017-01-18T16:41:04+0000",
            "author": "Pushkar Raste",
            "content": "Can't we just timeout in the test to let's say 200 ",
            "id": "comment-15828380"
        },
        {
            "date": "2017-01-18T23:15:45+0000",
            "author": "Cao Manh Dat",
            "content": "Alan Woodward: In basically the reason for a leader put a replica into LIR state is \"it failed to send an update to the replica\". FYI: \"leader won't send an update to the replica if the state of replica is down\". So there are two possible case here\n\n\tbefore sending update, the state of replica is active, then the replica go down right after, so the update will be failed to send to the replica.\n\tbefore sending update, the state of replica is recovery, but the underlying socket to replica is closed because the replica have just restarted, so the update will be failed to send to the replica.\n\n\n\nIn the second case, although the replica already in recovery state, putting the replica into LIR is a right thing to do. Because the buffering log doesn't contain the failed update.  ",
            "id": "comment-15828949"
        },
        {
            "date": "2017-01-19T13:46:49+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "I agree with Dat that \"only set DOWN if current state is ACTIVE\" is the wrong solution because it has the potential to lose updates. Recovery process is guaranteed to bring a replica up to date only if all updates after a hard commit make it to the replica's buffered log. Whether the state of the replica is already RECOVERING or not makes no difference. If the leader is not able to send an update then that guarantee cannot be made and therefore it is safest to restart the recovery process. ",
            "id": "comment-15829955"
        },
        {
            "date": "2017-01-19T14:44:42+0000",
            "author": "Alan Woodward",
            "content": "Cao Manh DatShalin Shekhar Mangar You guys know this code better than me, so I'll defer to you   I'm not sure how to resolve the race, though - how do we know in PrerecoveryOp that a node has been put into LIR, and so we should stop waiting for it? ",
            "id": "comment-15830047"
        },
        {
            "date": "2017-01-19T14:58:13+0000",
            "author": "Mark Miller",
            "content": "I haven't examined this closely, but it was a mistake to implement LIR where the leader can publish state for a replica. We have needed to fix that for some time. A replica should only ever publish its own state.  ",
            "id": "comment-15830065"
        },
        {
            "date": "2017-01-19T15:08:45+0000",
            "author": "Mark Miller",
            "content": "Comment was short because I was on my phone. Anyway, it's always a been a bug the leader even tries to do this.\n\nThe way this should work is that replicas should set a watch on ZK LIR nodes so they are notified when put into LIR - so that if they can speak to ZK but are partitioned from the leader, they set themselves to not ACTIVE. In the SolrCloud design, no replica or leader should ever publish state for another replica or leader. Either the leader can talk to replica and puts it into recovery, or it cannot and so communicates to it through ZK.  ",
            "id": "comment-15830078"
        },
        {
            "date": "2017-01-19T15:09:11+0000",
            "author": "Mark Miller",
            "content": "\"only set DOWN if current state is ACTIVE\"\n\nAnd yeah, this is also generally an anti pattern in the SolrCloud design. ",
            "id": "comment-15830080"
        },
        {
            "date": "2017-01-19T15:18:07+0000",
            "author": "Mark Miller",
            "content": "Whether the state of the replica is already RECOVERING or not makes no difference. If the leader is not able to send an update then that guarantee cannot be made and therefore it is safest to restart the recovery process.\n\nAnd this is correct too. Everytime an update fails from leader to replica we have to ensure a new recovery starts. This is why recoveries used to be able to stack up and had to unwind, though we now at least short circuit stack ups. ",
            "id": "comment-15830086"
        },
        {
            "date": "2017-01-19T17:22:19+0000",
            "author": "Mark Miller",
            "content": "A replica should only ever publish its own state.\n\nTo be more precise, in normal mode of operation, this is true. You can imagine a further improvement in line with another JIRA we have that enables a replica to try and publish itself as DOWN if sees local resource issues. You can imagine a case where the replica might maintain it's ZK connection but not be operational. In this case a leader might recognize that and take hard core action like publishing DOWN for that replica. It should only do this as an emergency action though, after it's clear the replica still appears up but is useless or detrimental. In this common path for LIR, we should only be trying to communicate directly and through ZooKeeper and do no false publishing. ",
            "id": "comment-15830288"
        },
        {
            "date": "2017-01-24T23:51:19+0000",
            "author": "Mike Drob",
            "content": "I've been thinking about this a bunch and have some observations.\n\nWe already have special nodes for LIR. It looks like these are currently only checked during leader election (or more generally, core load). If we instead have replicas watch for this at all times, then we wouldn't need to forcefully publish a down state, we could publish a down LIR state. Does this model move the race instead of eliminating it though? Alan's sequence would look like:\n\n\tA node goes down, and then restarts\n\tThe leader tries to send a document to the starting node, and gets a 503 'not ready yet'\n\tThe node publishes its state as RECOVERING\n\tThe leader's LIR thread publishes the recovery node's LIR state as DOWN\n\tThe node sends a PREPRECOVERY request to the leader\n\tThe leader waits for the node's state to be RECOVERING, which it already is, and can proceed.\n\tAt some point (possibly already happened) node sees new LIR state and abandons current recovery and starts a new one.\n\n\n\nIn the case where we get an error during recovery, the recovering replica would know to restart recovery process, so that works too.\n\nWe would also need to keep the Active state in the LIR path instead of deleting it so that there is a node we that replicas can set a watcher on.\n\nThe potential downside here is that we end up keeping two copies of the state, but I think it's ok? One is what the replica thinks it is, and one is what the leader thinks it is. I'll keep thinking about this more, but I wonder if there's a way to condense all these operations down to one znode safely. ",
            "id": "comment-15836870"
        },
        {
            "date": "2017-01-25T00:08:56+0000",
            "author": "Mark Miller",
            "content": "The potential downside here is that we end up keeping two copies of the state, but I think it's ok? One is what the replica thinks it is, and one is what the leader thinks it is\n\nI'm not sure if we really have to end up doing that. We already are communicating in ZK in a way that the replica can see itself it needs to enter recovery. I think we just need a strategy for the replica to have watches that alert it the leader has put it in LIR. The first time it notices this, it can be sure to go into recovery - it may even be better to only do that and remove the code that has the leader ask the replica to go into recovery on each failure. I think we can do that by pre creating some base LIR nodes and then watch it's children or something along those lines. Each LIR node in ZK could have a unique version so that we could tell if a new LIR request came in while responding to one (so start recovery over).\n\nWe should assume the replica can simply be alerted to mark itself as down when it sees it's been marked on LIR. If its too messed up to do that, it should lose it's zk connection, and if not, I think that's a corner case that needs an alternate solution. ",
            "id": "comment-15836899"
        },
        {
            "date": "2017-02-17T23:46:54+0000",
            "author": "Mike Drob",
            "content": "Here's a work in progress patch. The rough outline of the changes is:\n\n\tWhen publishing an active state, also publish an Active into the LIR znode and put a watch on that. If the leader overwrites this as down, start recovery.\n\t\n\t\tI tried to have a check here to ensure that nodes can only publish themselves as active, but I got messed up on the logic. Not sure if it's necessary for correctness, but felt like a good safegaurd.\n\t\n\t\n\tLeader no longer needs to send a request recovery command directly to the replica. The ZK watch should handle this.\n\tLeader no longer publishes the node's state. The node will update this itself when it starts the recovery process.\n\t\n\t\tThis means that there is a period of time after the leader has encountered the first error and before the node puts itself into recovery that the leader may try to send additional updates and get additional errors. Might need a flag to mark the node as dead locally or something like that.\n\t\n\t\n\n\n\n\nI've got about 5 test failures here, and I put an @Ignore on the TestLeaderInitiatedRecoveryThread class because the whole internals of that are changing. I think I somehow broke leader election with this change set, so any help would be appreciated. ",
            "id": "comment-15872751"
        },
        {
            "date": "2017-02-28T17:27:46+0000",
            "author": "Mark Miller",
            "content": "This is a great start Mike, I'll take a look. One thing I have been thinking about is perhaps queuing up LIR publishes to ZK for a second or 2 rather than hitting it for every update. You can index thousands of documents per second and they can fail by the thousands, so it would be nice to have a little throttle on ZK communication.\n\nMight need a flag to mark the node as dead locally or something like that.\n\nI don't know that it's critical, because this would always be a problem to a lesser degree anyway, but this is probably a good idea. I don't know how tricky it ends up being, but seems like we could locally mark the state as down until we notice it's state change in the clusterstate. ",
            "id": "comment-15888501"
        },
        {
            "date": "2017-04-11T17:09:15+0000",
            "author": "Cao Manh Dat",
            "content": "I'm looking at the patch and It seems that ForceLeaderTest is failed with the patch. The reason for that problem is when we close SocketProxy for none leader node, these nodes can still talk to the leader node ( I think this is big problem for our jetty proxy stuff, right? ), so if they being putted into lir down, they still can run recovery successfully and become active.  ",
            "id": "comment-15964649"
        },
        {
            "date": "2017-04-14T15:04:58+0000",
            "author": "Mike Drob",
            "content": "Cao Manh Dat - yea, ForceLeaderTest was one of the ones that was failing for me. A node still able to recover when it shouldn't would make sense, but I don't really understand how the proxy stuff works enough to debug it or know how to fix it. Is the right path to rip the proxy stuff out and kill nodes some other way? ",
            "id": "comment-15969118"
        },
        {
            "date": "2017-04-14T15:16:46+0000",
            "author": "Mark Miller",
            "content": "I think this is big problem for our jetty proxy stuff, right?\n\nI guess it depends on the test. Timothy Potter], any thoughts on this comment?\n\nI'm not sure we want to rip the proxy stuff out just yet, but we do want to look into Cao Manh Dat's comment and perhaps see if we can improve things if we need to. ",
            "id": "comment-15969127"
        },
        {
            "date": "2017-04-14T15:23:06+0000",
            "author": "Mark Miller",
            "content": "My first thought would be to guess that ForceLeaderTest is just using the proxy stuff incorrectly. Turning off the proxy for a jetty cuts off communication to that jetty (to the port that jetty is listening on), not also 'from' that jetty was my understanding. ",
            "id": "comment-15969139"
        },
        {
            "date": "2017-04-14T15:44:15+0000",
            "author": "Mike Drob",
            "content": "I guess I was unclear. I meant to rip the proxy stuff out from this test. I'm sure it's fine (and necessary) elsewhere. ",
            "id": "comment-15969172"
        },
        {
            "date": "2017-04-14T16:03:29+0000",
            "author": "Mark Miller",
            "content": "I suppose that depends on if it's the same as removing force leader test coverage. We want to maintain it if we can - whether that means using the proxy, changing we use it, or changing the test.  ",
            "id": "comment-15969203"
        },
        {
            "date": "2017-04-14T16:16:34+0000",
            "author": "Mark Miller",
            "content": "Maybe we can just close the leader proxy before we index the docs. ",
            "id": "comment-15969221"
        },
        {
            "date": "2017-04-14T17:54:52+0000",
            "author": "Mark Miller",
            "content": "more precisely, if the issue is as stated, it would seem the problem is that we stop the replicas from being able to receive requests, then we index 2 docs, then we kill the leader and close it's proxy. That is where there seems to be room for a replica to recover from the leader. If we do getProxyForReplica(leader).close() when we close the replica proxies, that should prevent any replica from being able to recover from it even before we kill the leader jetty. Doesn't seem like it would have any negative consequences either on first look. It's not like the test expects a replica to still be able to talk to the leader during this point, right?\n\n\n    // ok, now introduce a network partition between the leader and both replicas\n    log.info(\"Closing proxies for the non-leader replicas...\");\n    for (SocketProxy proxy : nonLeaderProxies)\n      proxy.close();\n\n    // indexing during a partition\n    log.info(\"Sending a doc during the network partition...\");\n    sendDoc(2);\n\n    // Wait a little\n    Thread.sleep(2000);\n\n    // Kill the leader\n    log.info(\"Killing leader for shard1 of \" + collectionName + \" on node \" + leader.getNodeName() + \"\");\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    getProxyForReplica(leader).close();\n    leaderJetty.stop();\n\n ",
            "id": "comment-15969334"
        },
        {
            "date": "2017-04-14T17:56:12+0000",
            "author": "Mark Miller",
            "content": "Instead we would do:\n\n\n    // ok, now introduce a network partition between the leader and both replicas\n    log.info(\"Closing proxies for the non-leader replicas...\");\n    for (SocketProxy proxy : nonLeaderProxies)\n      proxy.close();\n\n    JettySolrRunner leaderJetty = getJettyOnPort(getReplicaPort(leader));\n    getProxyForReplica(leader).close();\n\n    // indexing during a partition\n    log.info(\"Sending a doc during the network partition...\");\n    sendDoc(2);\n\n    // Wait a little\n    Thread.sleep(2000);\n\n    // Kill the leader\n    log.info(\"Killing leader for shard1 of \" + collectionName + \" on node \" + leader.getNodeName() + \"\");\n    leaderJetty.stop();\n\n ",
            "id": "comment-15969337"
        },
        {
            "date": "2017-04-15T05:22:34+0000",
            "author": "Mark Miller",
            "content": "We would probably also have to be sure and index directly to the leader if that is not the current behavior. ",
            "id": "comment-15969818"
        },
        {
            "date": "2017-04-15T05:26:40+0000",
            "author": "Mark Miller",
            "content": "And I suppose to hit the leader we would have to hit the actual leader jetty port. I assume that is still possible? Have not looked into it. ",
            "id": "comment-15969819"
        },
        {
            "date": "2017-04-17T21:59:03+0000",
            "author": "Mike Drob",
            "content": "Continuing to work on this. I think I've actually made progress on the design part of it, but I feel like I'm backsliding on the implementation.\n\nSummary of changes:\n\n\tRemoves LIRThread\n\tMoves LIR down notification entirely to ZK\n\tEach core sets a watch on LIR node when publishing self as active to be aware of LIR state changes.\n\tEach leader sets a watch on LIR when publishing follower into LIR to know when it has started to recover and can start sending updates again.\n\n ",
            "id": "comment-15971711"
        },
        {
            "date": "2017-04-18T22:48:31+0000",
            "author": "Mike Drob",
            "content": "Looking at the ForceLeaderTest code snippet that you posted, Mark Miller, I don't think we can reorder to kill the leader first. We close the followers' connections, and then need to send a doc so that it can fail to distribute and that is what triggers the LIR. So I get the impression that closing a proxy only prevents inbound connections, not outbound connections, which is why the followers are able to recover gracefully. Interestingly, removing the 2 second sleep allows the test to pass for me, and I can't see how it invalidates the premises under test here.\n\nIntermittently, ForceLeaderTest::testReplicasInLIRNoLeader will still fail for me, and I haven't figured that one out yet since I haven't caught it with a reproducible seed yet.\n\nI've had a few more other test failures in the total suite, but I think this patch is getting close to finished. ",
            "id": "comment-15973650"
        },
        {
            "date": "2017-04-19T14:40:34+0000",
            "author": "Mark Miller",
            "content": "We close the followers' connections, and then need to send a doc so that it can fail to distribute and that is what triggers the LIR.\n\nThis comment applies to that: And I suppose to hit the leader we would have to hit the actual leader jetty port (there should be the proxy port, and the legit jetty port). I assume that is still possible? Have not looked into it. ",
            "id": "comment-15974803"
        },
        {
            "date": "2017-04-19T14:43:51+0000",
            "author": "Mark Miller",
            "content": "So I get the impression that closing a proxy only prevents inbound connections, not outbound connections, which is why the followers are able to recover gracefully. \n\nYes, the idea of the proxy is simple afaik. It pipes from one port to another, and so lets you cut off the connection or play other weird games if you want. The is the port jetty listens out, there is no interaction with outbound connections. That is why the ForceLeaderTest is not written correctly. I think you can write it correctly as I describe above though with a few simple changes.\n\nInterestingly, removing the 2 second sleep allows the test to pass for me, and I can't see how it invalidates the premises under test here.\n\nI don't know that it invalidates anything, it just is a little flakey and failure prone to count on something like this, especially with all the env and gc events and parallel tests we run. We want rock solid. ",
            "id": "comment-15974808"
        },
        {
            "date": "2017-04-19T14:48:29+0000",
            "author": "Mark Miller",
            "content": "If everything else is in order though, I can try and tackle that test before commit. I'm pretty sure we can bypass the proxy to send updates to the leader. There are a lot of things we might want to test like this. ",
            "id": "comment-15974813"
        },
        {
            "date": "2017-04-20T21:42:50+0000",
            "author": "Mike Drob",
            "content": "This one is pretty close, give it a look and let me know what y'all think. Depends on SOLR-10525.\n\ncc: Cao Manh Dat, Shalin Shekhar Mangar, Mark Miller ",
            "id": "comment-15977597"
        },
        {
            "date": "2017-04-22T09:46:25+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Thanks Mike, I'll review it on Monday. ",
            "id": "comment-15979840"
        },
        {
            "date": "2017-04-24T05:27:58+0000",
            "author": "Mark Miller",
            "content": "I commented a bit more about this in SOLR-10525, but the gist is, I think we should take this opportunity to stop trying to do LIR per update. We don't want update volume to be in control of how much chatter this generates any longer. What we really need is to simply trigger something that says, let the replica know it needs to enter recovery, and send that to ZK at some throttled rate that has a chatter / delay tradeoff that we control. Even if we allowed this once or twice per second or something, thats drastically better than the 1000's per second you can hit now. ",
            "id": "comment-15980740"
        },
        {
            "date": "2017-04-24T05:32:18+0000",
            "author": "Mark Miller",
            "content": "We can open a new issue on that realistically if we don't want to tackle it right away though.\n\nI'll spend some time with the latest patch tomorrow. ",
            "id": "comment-15980742"
        },
        {
            "date": "2017-04-24T14:57:31+0000",
            "author": "Mike Drob",
            "content": "I think we should take this opportunity to stop trying to do LIR per update\nMy patch kind of does this.\n\nBefore, we would locally mark a follower as in LIR after a failed update. Then when the LIR thread successfully sent a recovery request, we would unmark it, after which we start sending more updates (to buffer) and if those fail more requests. This could lead to a lot of recovery requests stacking up, hence the need to discard them at the follower.\n\nWith my patch, we still locally mark a replica as in LIR. However, we won't unmark it until we see that it has ack'd the LIR update in ZK and updated itself to 'recovering'. I don't know how fast this process can be, maybe it can still be 1000s per second. I imagine that going through ZK and having to wait long enough to actually start a recovery necessarily makes it slower.\n\nIf it's still bad, we can add a configurable 100ms sleep to the start of any recovery. In actual use, it shouldn't affect things too much since the rest of the recovery will be much longer, but it would be enough to throttle things. ",
            "id": "comment-15981298"
        },
        {
            "date": "2017-04-24T16:28:21+0000",
            "author": "Mike Drob",
            "content": "Looking at my patch again, I can't tell if there is a really subtle bug going on with the ZK watches or not. When setting the LIR state to down, we will add a watch that will notify us when the LIR state changes next and if it's no longer down (i.e. recovering) then we can un-blacklist the follower for updates. However, if it's still down when we get that request (or down again, I guess?) then we won't get any notifications the next time it actually changes state to recovering. Except whatever set the second down state should have also set a new watch, so I think we're still fine. Would appreciate somebody taking a close look at this though.\n\nAlso, it's possible that we might need to call replicasInLeaderInitiatedRecovery.remove(znodePath); when we get a prep recovery request instead? ",
            "id": "comment-15981460"
        },
        {
            "date": "2017-04-24T18:22:37+0000",
            "author": "Mark Miller",
            "content": "haven't caught up on your comments yet, but FYI, I've been seeing consistent fails during ant test on the force leader test. Either threads leaking due to some resource leak and recently the following NPE:\n\n\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=ForceLeaderTest -Dtests.method=testReplicasInLIRNoLeader -Dtests.seed=AE8388F349DFBDA7 -Dtests.slow=true -Dtests.locale=ms -Dtests.timezone=AET -Dtests.asserts=true -Dtests.file.encoding=ISO-8859-1\n   [junit4] ERROR   21.2s J1 | ForceLeaderTest.testReplicasInLIRNoLeader <<<\n   [junit4]    > Throwable #1: java.lang.NullPointerException\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([AE8388F349DFBDA7:4814BC33705D44C6]:0)\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:1182)\n   [junit4]    > \tat org.apache.solr.client.solrj.impl.CloudSolrClient.request(CloudSolrClient.java:1073)\n   [junit4]    > \tat org.apache.solr.cloud.AbstractFullDistribZkTestBase.sendDocsWithRetry(AbstractFullDistribZkTestBase.java:749)\n   [junit4]    > \tat org.apache.solr.cloud.ForceLeaderTest.putNonLeadersIntoLIR(ForceLeaderTest.java:359)\n   [junit4]    > \tat org.apache.solr.cloud.ForceLeaderTest.testReplicasInLIRNoLeader(ForceLeaderTest.java:97)\n   [junit4]    > \tat org.apache.solr.BaseDistributedSearchTestCase$ShardsRepeatRule$ShardsFixedStatement.callStatement(BaseDistributedSearchTestCase.java:985)\n   [junit4]    > \tat org.apache.solr.BaseDistributedSearchTestCase$ShardsRepeatRule$ShardsStatement.evaluate(BaseDistributedSearchTestCase.java:960)\n\n ",
            "id": "comment-15981615"
        },
        {
            "date": "2017-04-24T18:38:30+0000",
            "author": "Mark Miller",
            "content": "If it's still bad,\n\nAh, ok, if you throttle with the watch implementation, that is an improvement. I don't think we want the per failed update call to ZK either though. We never really should have had per update communication like this, to ZK or the replica. It looked like we still did that, but I'm still going through the patch. Have been focusing more on testing initially. ",
            "id": "comment-15981644"
        },
        {
            "date": "2017-04-25T17:15:04+0000",
            "author": "Mike Drob",
            "content": "Here's a patch that fixes the resource leak. I'm still getting a different kind of failure now, that doesn't reproduce for me. It shouldn't be happening at all since the previous two asserts check for each of the docs individually, so I don't understand how they could be missing on the next query. Going to need help on figuring out what's wrong here.\n\nMaybe related to SOLR-10562, since there is a difference between using RTG or not after a commit.\n\n\n  2> NOTE: reproduce with: ant test  -Dtestcase=ForceLeaderTest -Dtests.method=testReplicasInLIRNoLeader -Dtests.seed=B6BDBFC48B148FB6 -Dtests.slow=true -Dtests.locale=en -Dtests.timezone=Europe/Belgrade -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n[11:25:21.677] FAILURE 45.6s | ForceLeaderTest.testReplicasInLIRNoLeader <<<\n   > Throwable #1: java.lang.AssertionError: Expected only 2 documents in the index expected:<2> but was:<0>\n   >    at __randomizedtesting.SeedInfo.seed([B6BDBFC48B148FB6:502A8B04B29676D7]:0)\n   >    at org.junit.Assert.fail(Assert.java:93)\n   >    at org.junit.Assert.failNotEquals(Assert.java:647)\n   >    at org.junit.Assert.assertEquals(Assert.java:128)\n   >    at org.junit.Assert.assertEquals(Assert.java:472)\n   >    at org.apache.solr.cloud.ForceLeaderTest.testReplicasInLIRNoLeader(ForceLeaderTest.java:152)\n\n\n\nEdit: When I say \"doesn't reproduce\" I mean that rerunning with the same seed doesn't cause it to always fail, but running ant test will get a failure maybe 20% of the time. ",
            "id": "comment-15983273"
        },
        {
            "date": "2017-04-26T19:55:26+0000",
            "author": "Mike Drob",
            "content": "Noticed that on that check if it hits the follower it gets 2 docs, and if it hits the leader it gets 0. This seems pretty consistent. Adding a 10s sleep like suggested in SOLR-10562 doesn't fix the problem. Based on this, it feels like it should fail 50% of the time, but I'm definitely not seeing that.\n\nThe good news is that that is the only failure I'm seeing in solr/core tests now, so I'm hoping it's a test issue and not a code issue for me. ",
            "id": "comment-15985465"
        },
        {
            "date": "2017-04-26T22:07:56+0000",
            "author": "Mike Drob",
            "content": "Cleans up patch to pass ant precommit ",
            "id": "comment-15985645"
        },
        {
            "date": "2017-04-27T14:52:39+0000",
            "author": "Mark Miller",
            "content": "one fail I've seen:   \n\n\n [junit4] FAILURE 49.9s J1 | ForceLeaderTest.testReplicasInLIRNoLeader <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError: Expected only 2 documents in the index expected:<2> but was:<0>\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([3976EDCE80FD24FA:DFE1D90EB97FDD9B]:0)\n   [junit4]    > \tat org.apache.solr.cloud.ForceLeaderTest.testReplicasInLIRNoLeader(ForceLeaderTest.java:150)\n   [junit4]    > \tat org.apache.solr.BaseDistributedSearchTestCase$ShardsRepeatRule$ShardsFixedStatement.callStatement(BaseDistributedSearchTestCase.java:985)\n\n ",
            "id": "comment-15986760"
        },
        {
            "date": "2017-04-27T15:48:55+0000",
            "author": "Mark Miller",
            "content": "I didn't have the other patch in. Got one success after putting that in, I'll beast that test after a few more runs. ",
            "id": "comment-15986849"
        },
        {
            "date": "2017-04-27T16:01:42+0000",
            "author": "Mark Miller",
            "content": "I've seen ChaosMonkeySafeLeaderTest hang once in a previous patch and with the latest. I have to see how common that is without this patch now. Some more test beasting to do on that. ",
            "id": "comment-15986889"
        },
        {
            "date": "2017-04-27T18:00:09+0000",
            "author": "Mark Miller",
            "content": "Okay, even with SOLR-10525, I did hit that same fail. I'll try and debug it. ",
            "id": "comment-15987167"
        },
        {
            "date": "2017-07-20T14:46:08+0000",
            "author": "Mano Kovacs",
            "content": "Attaching explanation diagram of the first problem. ",
            "id": "comment-16094781"
        },
        {
            "date": "2017-08-23T11:53:11+0000",
            "author": "Cao Manh Dat",
            "content": "Great work Mike Drob, but there are one thing that I concern about your direction. \nFirstly, There are one requirement for LIR implementation is : the replica that miss the update, should not allowed to serve query requests.\n\n\tIn current implementation, Leader will publish the replica as DOWN, so all clients will know that and stop query that replica. ( low latency )\n\tIn your implementation, Leader only publish LIR state of that replica as DOWN, so all clients will keep query that replica, until the replica acknowledge the change in LIR node then publish it self as DOWN ( high latency  )\n\n\n\nSecondly, Now we have 2 nodes update state of a replica which is considered as bad practice. With your implementation, we have 2 nodes update lir state of a replica, will we have race condition bug in the future? ",
            "id": "comment-16138255"
        },
        {
            "date": "2017-10-01T22:04:41+0000",
            "author": "Erick Erickson",
            "content": "Must have hit the \"assign to me\" link by accident. ",
            "id": "comment-16187552"
        },
        {
            "date": "2017-12-08T03:12:06+0000",
            "author": "Cao Manh Dat",
            "content": "Mike Drob Mark Miller  please take a look at my recent patch for SOLR-11702. I think that is the safer/better way to solve this problem. ",
            "id": "comment-16282965"
        },
        {
            "date": "2018-01-29T09:00:40+0000",
            "author": "Cao Manh Dat",
            "content": "Fixed by SOLR-11702 ",
            "id": "comment-16343098"
        }
    ]
}