{
    "id": "LUCENE-2090",
    "title": "convert automaton to char[] based processing and TermRef / TermsEnum api",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/search"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "The automaton processing is currently done with String, mostly because TermEnum is based on String.\nit is easy to change the processing to work with char[], since behind the scenes this is used anyway.\n\nin general I think we should make sure char[] based processing is exposed in the automaton pkg anyway, for things like pattern-based tokenizers and such.",
    "attachments": {
        "LUCENE-2090_TermRef_flex2.patch": "https://issues.apache.org/jira/secure/attachment/12426095/LUCENE-2090_TermRef_flex2.patch",
        "LUCENE-2090_TermRef_flex.patch": "https://issues.apache.org/jira/secure/attachment/12426074/LUCENE-2090_TermRef_flex.patch",
        "LUCENE-2090_TermRef_flex3.patch": "https://issues.apache.org/jira/secure/attachment/12426099/LUCENE-2090_TermRef_flex3.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-11-22T21:32:05+0000",
            "content": "Spinoff from LUCENE-1606. ",
            "author": "Michael McCandless",
            "id": "comment-12781222"
        },
        {
            "date": "2009-11-22T22:01:09+0000",
            "content": "Michael, here is one idea that isn't too crazy.\n\nseparately i think we should make it convenient for a MTQ to get a char[], this should not change.\n\nhowever, lets consider this:\n\n  /**\n   * Returns true if the given string is accepted by this automaton.\n   */\n  public boolean run(String s) {\n    int p = initial;\n    int l = s.length();\n    for (int i = 0; i < l; i++) {\n      p = step(p, s.charAt(i));\n      if (p == -1) return false;\n    }\n    return accept[p];\n  }\n\n\n\nchecking a string, is really just stepping thru one char at a time.\nwould 'incremental, one char at a time' conversion actually help, or do you think it would just be slower?\n\nconceptually, this isn't that much different than using a Reader with java i/o, at a much smaller scale.\ni am not familiar with decoding performance, but I thought I would mention this, just in the case there is a way to do it clean. ",
            "author": "Robert Muir",
            "id": "comment-12781233"
        },
        {
            "date": "2009-11-22T23:29:15+0000",
            "content": "I changed only the accept(final TermRef term) method from Mike's flex patch of this enum to use char[], instead of string.\nI did not modify the \"smart\" part, its more complex, but will probably help the ????NNN case.\n\nthe results change significantly for the *N case (i used my old benchmark, just because it was already setup in my eclipse)\n\n\n\nPattern\nIter\nAvgHits\nAvgMS (String)\nAvgMS (char[])\n\n\nN?N?N?N\n10\n1000.0\n36.2\n34.9\n\n\n?NNNNNN\n10\n10.0\n4.9\n5.1\n\n\n??NNNNN\n10\n100.0\n8.0\n11.5\n\n\n???NNNN\n10\n1000.0\n35.4\n34.0\n\n\n????NNN\n10\n10000.0\n250.9\n230.9\n\n\nNN??NNN\n10\n100.0\n9.1\n5.0\n\n\nNN?N*\n10\n10000.0\n8.3\n7.5\n\n\n?NN*\n10\n100000.0\n63.5\n28.7\n\n\n*N\n10\n1000000.0\n3027.8\n1922.7\n\n\nNNNNN??\n10\n100.0\n3.7\n3.7\n\n\n\n ",
            "author": "Robert Muir",
            "id": "comment-12781249"
        },
        {
            "date": "2009-11-23T09:30:20+0000",
            "content": "would 'incremental, one char at a time' conversion actually help, or do you think it would just be slower?\n\nI like this idea!  It's worth exploring a Reader-like interface from UnicodeUtil?  Is this a hotspot in automaton's processing?  Ie, could we save much conversion by only doing it on demand? ",
            "author": "Michael McCandless",
            "id": "comment-12781344"
        },
        {
            "date": "2009-11-23T11:15:23+0000",
            "content": "Michael, I think i would have to profile things to determine this?\nI guess it would be a close one, because strings in term dictionary are pretty short.\njust an idea, i think moving all the code to char[] first would be the best for starters. ",
            "author": "Robert Muir",
            "id": "comment-12781365"
        },
        {
            "date": "2009-11-23T23:07:20+0000",
            "content": "Hi Mike, I think an easier win is to perhaps add endsWith() byte[] comparison in TermRef.\n(for now, I can use regular endsWith(), or run the machine backwards, or something like that).\n\nI can use this in \"dumb mode\", i.e. *N, where I know the first part of the machine is a loop.\nfor whatever reason dumb mode checks \"constant prefix\" right now, which is useless, it will always be 0 in dumb mode.\ninstead I should build \"constant suffix\" in dumb mode. this would be much more useful for a quick comparison. ",
            "author": "Robert Muir",
            "id": "comment-12781694"
        },
        {
            "date": "2009-11-24T00:17:27+0000",
            "content": "That sounds compelling \u2013 you'd still do the full scan, but testing each term is much faster? ",
            "author": "Michael McCandless",
            "id": "comment-12781710"
        },
        {
            "date": "2009-11-24T00:46:25+0000",
            "content": "right, we could use constant suffix to stay with bytes. \nfor example *N in this test, well 90% of the charset conversion of TermRefs disappears, because they can be eliminated by comparing bytes. ",
            "author": "Robert Muir",
            "id": "comment-12781718"
        },
        {
            "date": "2009-11-24T18:12:47+0000",
            "content": "Mike, I implemented this common suffix, but only for dumb mode, it does not help smart mode.\nso i got rid of common prefix entirely, as its useless, and just replaced it.\nI also take measures to ensure the suffix is well-formed UTF-8 \n\non my *N trunk tests its now 5700/5800ms on average versus 6000ms, just using String.endsWith() before checking the DFA.\nits a consistent gain, so I think for really crappy worst-case wildcards and regular expressions, \nwe have a lot to gain by doing this with bytes, before converting to char[] and running against the DFA.\n\nI guess since TermRef exposes all the bytes, I could implement endsWith myself in AutomatonTermsEnum in the future, \nbut it seems like it would be a nice complement to startsWith() ? ",
            "author": "Robert Muir",
            "id": "comment-12782065"
        },
        {
            "date": "2009-11-24T18:36:08+0000",
            "content": "I guess now you have me starting to think about byte[] contains()\nBecause really the real worst case, which I bet a lot of users do, are not things like *foobar but instead *foobar* !\nin UTF-8 you can do such things safely, I would have to sucker out the \"longest common constant sequence\" out of a DFA.\nThis might be more generally applicable.\n\ncommonSuffix is easy... at least it makes progress for now, even slightly later in trunk. \n\nthis could be a later improvement. ",
            "author": "Robert Muir",
            "id": "comment-12782075"
        },
        {
            "date": "2009-11-25T05:57:11+0000",
            "content": "Attached is a patch to TermRef to implement endsWith()\n\nthis is a huge win on flex, even though constant suffix gain is very minor on trunk, because it avoids unicode conversion (char[]) for the worst cases that must do lots of comparisons.\n\n*N\t1705.7ms avg -> 1195.4ms avg\n*NNNNNN\t1844.9ms avg -> 1192.3ms avg\n\nit doesn't really matter if the suffix is short, if there is a way in FilteredTermsEnum.accept() for a multitermquery to accept/reject a term without unicode conversion, it helps a lot.\n\nin my opinion, this is the cleanest way to improve these cases, other crazy ideas i have tossed around out here like the iterative \"reader-like\" conversion or even TermRef substring matching will probably not gain much more over this, will be a lot more complex, and only apply to automatonquery.\n\nMike, if you get a chance to review, this, I'll commit it to flex branch (the tests pass). ",
            "author": "Robert Muir",
            "id": "comment-12782314"
        },
        {
            "date": "2009-11-25T10:23:32+0000",
            "content": "Patch looks good, except, I think I wouldn't factor startsWith/endsWith to share any code, to save the \"+ pos\" inside startsWith's loop?\n\n\n*N\t1705.7ms avg -> 1195.4ms avg\n*NNNNNN\t1844.9ms avg -> 1192.3ms avg\n\nWhoa \u2013 those are great results! ",
            "author": "Michael McCandless",
            "id": "comment-12782353"
        },
        {
            "date": "2009-11-25T12:13:40+0000",
            "content": "Patch looks good, except, I think I wouldn't factor startsWith/endsWith to share any code, to save the \"+ pos\" inside startsWith's loop? \n\nforgive my ignorance, but shouldnt the JRE hoist this constant additive to the array index out anyway?\nI checked, this is how harmony, etc implement startsWith/endsWith even for String...\n(I will change it, just curious) ",
            "author": "Robert Muir",
            "id": "comment-12782384"
        },
        {
            "date": "2009-11-25T12:37:30+0000",
            "content": "alternative patch for if you do not trust your compiler \nI think the do the same thing though... ",
            "author": "Robert Muir",
            "id": "comment-12782395"
        },
        {
            "date": "2009-11-25T12:57:01+0000",
            "content": "shouldnt the JRE hoist this constant additive to the array index out anyway?\n\nMaybe?\n\nalternative patch for if you do not trust your compiler\n\nThanks  ",
            "author": "Michael McCandless",
            "id": "comment-12782401"
        },
        {
            "date": "2009-11-25T13:04:55+0000",
            "content": "BTW, we've discussed someday having a codec whose terms dict (or maybe just terms index) is represented as an FST, at which point AutomatonTermsEnum would be an intersection + walk of two FSTs.  Because suffix's are also shared in the FST, you could more easily (more efficiently) handle *XXX cases as well (it'd just be symmetic with the XXX* cases). ",
            "author": "Michael McCandless",
            "id": "comment-12782409"
        },
        {
            "date": "2009-11-25T13:06:49+0000",
            "content": "Maybe also make TermRef final in the patch? ",
            "author": "Michael McCandless",
            "id": "comment-12782412"
        },
        {
            "date": "2009-11-25T13:11:48+0000",
            "content": "BTW, we've discussed someday having a codec whose terms dict (or maybe just terms index) is represented as an FST\n\nthis would open up more opportunities.\n\nMaybe also make TermRef final in the patch?\n\nok ",
            "author": "Robert Muir",
            "id": "comment-12782414"
        },
        {
            "date": "2009-11-25T13:37:16+0000",
            "content": "Mike, here TermRef is final also. This doesn't remove any flexibility does it?\nif the term dictionary is encoded in a different way (i.e. BOCU-1), will TermRef still be UTF-8 byte[] ? ",
            "author": "Robert Muir",
            "id": "comment-12782431"
        },
        {
            "date": "2009-11-25T16:06:03+0000",
            "content": "Mike, here TermRef is final also. This doesn't remove any flexibility does it?\n\nI'd actually rather lock it down for now, and then only open up flexibility when/if we get there... patch looks good! ",
            "author": "Michael McCandless",
            "id": "comment-12782476"
        },
        {
            "date": "2009-11-25T16:11:38+0000",
            "content": "I'd actually rather lock it down for now, and then only open up flexibility when/if we get there... patch looks good!\n\nOk, I will commit it.\n\nJust as a side note, maybe i can add a comment if you need it... the existing startsWith(), and now the new endsWith() are correct against byte[] for any Unicode encoding form.\nHowever, some other encodings (including alternate encodings someone might flex to), do not have the properties of non-overlap, etc.\n\nif someone was to implement a codec to store the index in one of those other encodings, they would have to write significantly more complex code that is aware of character boundaries, depending upon the properties of said encoding.\noh yeah, and their sort order would be different, too... (I suppose we should also fix compareTerm here for UTF-16 ordering at some point?) ",
            "author": "Robert Muir",
            "id": "comment-12782479"
        },
        {
            "date": "2009-11-25T16:14:16+0000",
            "content": "I suppose we should also fix compareTerm here for UTF-16 ordering at some point?\n\nYes... I'm [slowly] working towards that. ",
            "author": "Michael McCandless",
            "id": "comment-12782482"
        },
        {
            "date": "2009-11-25T16:43:58+0000",
            "content": "Yes... I'm [slowly] working towards that.\n\nGlad it is you working on it instead of me. If I wrote it, it would be very slow.\n\nCommitted revision 884190 for TermRef ",
            "author": "Robert Muir",
            "id": "comment-12782495"
        },
        {
            "date": "2009-11-25T16:56:56+0000",
            "content": "Mike, by the way, looking at this code, I don't see a way to expose the UnicodeUtil / char[] functionality in a clean way via TermRef/FilteredTermsEnum.\n\nOnce I see that most of the other enums survive with TermRef alone, and don't need it, and its handy to have multiple TermRefs around in the same enum,\nit doesn't make sense I guess.\n\nAlso I guess people in general aren't writing MultiTermQueries every day, so I think this is ok?\nThe rest of this issue should only involve automaton code itself... ",
            "author": "Robert Muir",
            "id": "comment-12782503"
        },
        {
            "date": "2009-11-25T19:16:15+0000",
            "content": "OK I'll first focus on making sure DW flushes in UTF-16 sort order... ",
            "author": "Michael McCandless",
            "id": "comment-12782565"
        },
        {
            "date": "2009-12-07T22:29:51+0000",
            "content": "Mike, I converted this to char[] api (see LUCENE-1606 for the patch).\n\nIn order for this to work, I needed to expose UnicodeUtil.nextUTF16ValidString(UTF16Result). \nThe code is not duplicated, the String based method is just a wrapper for this, take a look if you get a chance.\n\nthe other thing I forgot, I think TermRef.copy(UTF8Result) would be handy... is there anywhere you could use this too?\n\nedit: i meant utf-8 result, sorry ",
            "author": "Robert Muir",
            "id": "comment-12787175"
        },
        {
            "date": "2009-12-08T10:13:39+0000",
            "content": "Mike, I converted this to char[] api\n\nNice!\n\nthe other thing I forgot, I think TermRef.copy(UTF8Result) would be handy... is there anywhere you could use this too?\n\nThat sounds reasonable \u2013 maybe just add it?  Or... we could also deprecate UTF8Result, entirely, replacing it w/ TermRef...?  Hmmm. ",
            "author": "Michael McCandless",
            "id": "comment-12787389"
        },
        {
            "date": "2009-12-10T01:43:42+0000",
            "content": "i am marking this one resolved, the goals have been met (char[]/byte[] based processing and TermRef/TermsEnum api) ",
            "author": "Robert Muir",
            "id": "comment-12788444"
        }
    ]
}