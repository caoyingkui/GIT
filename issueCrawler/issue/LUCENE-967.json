{
    "id": "LUCENE-967",
    "title": "Add \"tokenize documents only\" task to contrib/benchmark",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/benchmark"
        ],
        "type": "Improvement",
        "fix_versions": [
            "2.3"
        ],
        "affect_versions": "2.3",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "I've been looking at performance improvements to tokenization by\nre-using Tokens, and to help benchmark my changes I've added a new\ntask called ReadTokens that just steps through all fields in a\ndocument, gets a TokenStream, and reads all the tokens out of it.\n\nEG this alg just reads all Tokens for all docs in Reuters collection:\n\n  doc.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersDocMaker\n  doc.maker.forever=false\n  {ReadTokens > : *",
    "attachments": {
        "LUCENE-967.patch": "https://issues.apache.org/jira/secure/attachment/12362636/LUCENE-967.patch",
        "LUCENE-967.take3.patch": "https://issues.apache.org/jira/secure/attachment/12362968/LUCENE-967.take3.patch",
        "LUCENE-967.take2.patch": "https://issues.apache.org/jira/secure/attachment/12362728/LUCENE-967.take2.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2007-07-26T18:09:47+0000",
            "content": "Attached patch that adds ReadTokensTask.java.  I also added change to print net elapsed time of the algorithm. ",
            "author": "Michael McCandless",
            "id": "comment-12515814"
        },
        {
            "date": "2007-07-29T00:59:38+0000",
            "content": "New rev of this patch.  Only real change is to reduce overhead\n(slightly) of benchmark framework by pre-building array of PerfTasks\ninstead of creating new iterator for each document. ",
            "author": "Michael McCandless",
            "id": "comment-12516195"
        },
        {
            "date": "2007-07-31T17:22:16+0000",
            "content": "I plan to commit this soon... ",
            "author": "Michael McCandless",
            "id": "comment-12516747"
        },
        {
            "date": "2007-07-31T21:29:52+0000",
            "content": "I'm reviewing it now... ",
            "author": "Doron Cohen",
            "id": "comment-12516806"
        },
        {
            "date": "2007-07-31T22:07:44+0000",
            "content": "Applies cleanly and all test pass (running from contrib/benchmark.)\n\nI like the efficiency changes.\n\nA few suggestions:\n\n  1) in ReadTokensTask change doLogic() to return the number of tokens \n       processed in that specific call to doLogic() (differs from tokensCount \n       which aggregates all calls).\n\n  2) in TestPerfTaskLogic the comment in testReadTokens seems \n      copy/pasted from testLineDocFile and should be changed. \n\n\n\tAlso (I am not sure if it is worth your time, but) to really test it, you\n      could open a reader against the created index and verify the number \n      of docs, and also the index sum-of-DF comparing to the total tokens \n      counts numbers in ReadTokensTask. \n\n ",
            "author": "Doron Cohen",
            "id": "comment-12516809"
        },
        {
            "date": "2007-08-01T00:32:43+0000",
            "content": "Also, I think the addition of printing of elapsed time is redundant, \nbecause you get it anyhow as the elapsed time reported for the \noutermost task sequence. \n\nFor instance, if you add to tokenize.alg this line:\n     RepSumByName\nYou get this output:\n     Operation   round   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem\n     Seq_Exhaust     0        1        21578        638.2       33.81    15,694,368     20,447,232\n     Net elapsed time: 33.809 sec\nSo the total elapsed time is actually printed twice now - do we need this? ",
            "author": "Doron Cohen",
            "id": "comment-12516837"
        },
        {
            "date": "2007-08-01T12:15:43+0000",
            "content": "> Also, I think the addition of printing of elapsed time is redundant, \n> because you get it anyhow as the elapsed time reported for the \n> outermost task sequence. \n\nDuh, right   I will remove that.\n\n>  1) in ReadTokensTask change doLogic() to return the number of tokens \n>       processed in that specific call to doLogic() (differs from tokensCount \n>       which aggregates all calls).\n\nAhh good idea!\n\n>  2) in TestPerfTaskLogic the comment in testReadTokens seems \n>      copy/pasted from testLineDocFile and should be changed. \n\nWoops, will fix.\n\n>      - Also (I am not sure if it is worth your time, but) to really test it, you \n>      could open a reader against the created index and verify the number \n>      of docs, and also the index sum-of-DF comparing to the total tokens \n>      counts numbers in ReadTokensTask. \n\nOK I added this too.  Will submit new patch shortly. ",
            "author": "Michael McCandless",
            "id": "comment-12516945"
        },
        {
            "date": "2007-08-01T18:05:34+0000",
            "content": "Thanks for fixing this Michael, looks perfect to me now. ",
            "author": "Doron Cohen",
            "id": "comment-12517031"
        },
        {
            "date": "2007-08-01T18:14:58+0000",
            "content": "Thank you for reviewing!  I will commit shortly. ",
            "author": "Michael McCandless",
            "id": "comment-12517035"
        }
    ]
}