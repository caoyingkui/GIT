{
    "id": "SOLR-534",
    "title": "Return all query results with parameter rows=-1",
    "details": {
        "affect_versions": "1.3",
        "status": "Closed",
        "fix_versions": [],
        "components": [
            "search"
        ],
        "type": "New Feature",
        "priority": "Minor",
        "labels": "",
        "resolution": "Resolved"
    },
    "description": "The searcher should return all results matching a query when the parameter rows=-1 is given.\n\nI know that it is a bad idea to do this in general, but as it explicitly requires a special parameter, people using this feature will be aware of what they are doing. The main use case for this feature is probably debugging, but in some cases one might actually need to retrieve all results because they e.g. are to be merged with results from different sources.",
    "attachments": {
        "solr-all-results.patch": "https://issues.apache.org/jira/secure/attachment/12379363/solr-all-results.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Lars Kotthoff",
            "id": "comment-12585434",
            "date": "2008-04-04T07:35:17+0000",
            "content": "Patch adding the feature. If rows is negative, all results are returned. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12585675",
            "date": "2008-04-04T18:11:20+0000",
            "content": "+0\n\nrows=$REALLY_BIG_NUMBER works just as well, makes people just as aware of what they are doing, and helps protected people who think they are aware of what they are doing but really don't (ie: \"I know this query will never return more then a thousand things, so I'll use rows=-1 to get them all at once\" ... if you know it will never contain more then a thousand, use rows=1000, assert numFound<1000, and eliminate the risk of crashing Solr, hozing your net, or crashing your client. "
        },
        {
            "author": "Lisa Carter",
            "id": "comment-12796982",
            "date": "2010-01-06T04:31:02+0000",
            "content": "I would argue that REALLY_BIG_NUMBER is actually significantly MORE dangerous than a crash. \n\nHere's why: A crash at least lets the programmer know something went wrong. Missing data is a silent failure. \n\n1) If the result set is too large for the client, it will run out of memory and generate an exception. The programmer will immediately know they did something wrong.\n\n2) If the result set is too large for the network (unlikely) this will disconnect and fail. The programmer will immediately know they did something wrong.\n\n3) If the result set is too large for solr, solr should not crash but rather return a page with the standard error handler \"result set too large\"/\"out of memory\". The programmer will immediately know they did something wrong. Solr sure as heck better be checking this already--you never know when you'll run into bizarre low memory conditions;allocations should ALWAYS be checked for.\n\nBut if you use the REALLY_BIG_NUMBER approach, the same bad programmer who never thought he would get back more than a 1000 records will never check whether the result set contains more than 1000 records either. If the programmer was expecting the complete result set and the database now contains 1002 records instead of 999, they will not know there is a problem... the last records in the set are simply truncated. The programmer who wrote the code may not be the person maintaining the application, quite common in production environments. The maintenance person may not know for weeks or months that a problem even exists! \n\nThe -1 approach ensures immediate, loud failure.\n\nThe REALLY_BIG_NUMBER ensures only silent failure.\n\nWhile it's impossible to idiot-proof everything, loud failure is always preferable to silent failure. Barking loudly saves the poor soul who maintains the idiot's code a lot of heartache. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12832333",
            "date": "2010-02-11T01:01:24+0000",
            "content": "But if you use the REALLY_BIG_NUMBER approach, the same bad programmer who never thought he would get back more than a 1000 records will never check whether the result set contains more than 1000 records either.\n\nIf we're going to assume the programmer doesn't check the actual number found, then why assume that the programmer pays attention to anything in the response at all? \n\nIf you think it's likley that programmers will write code that only looks at the docList to iterates over all the docs in a response and doesn't notice that the numFound at the top of the docList is higher then the number asked for. then why do you assume that same programmer would be smart enough to check if an error message is returned when they ask for \"all\" rows and Solr can't provide them?\n\nBottom line: we can't protect programmers from all possible forms of stupidity stupidity, but we can make them be explicit about exactly what they want \u2013 if they want 100, they ask for 100;  if they want 10000 they ask for 10000, if they want \"all\" they have to specify how big they think \"all\" is.\n\nSolr sure as heck better be checking this already--you never know when you'll run into bizarre low memory conditions;allocations should ALWAYS be checked for.\n\nThis isn't as easy as it may sound in Java ... the APIS available to test for the amount of memory available are limited, and even if hte JVM has the resources to allocate a 10,000,000 item PiorityQuery when computing the results, that doesn't mean doing so won't eat up all the available RAM causing some later (extremely tiny) allocation to trigger an OOM \u2014 but If you've got a suggestion to help prevent OOM in situations like this, by all means patches welcome.  "
        },
        {
            "author": "Walter Underwood",
            "id": "comment-12832351",
            "date": "2010-02-11T01:44:48+0000",
            "content": "-1\n\nThis adds a denial of service vulnerability to Solr. One query can use lots of CPU or memory, or even crash the server.\n\nThis could also take out an entire distributed system.\n\nIf this is added, we MUST add a config option to disable it.\n\nLet's take this back to the mailing list and find out why they believe all results are needed.There must be a better way to solve this. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12832357",
            "date": "2010-02-11T01:59:12+0000",
            "content": "Something like SOLR-1687 would definitely be needed if this was added. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13835718",
            "date": "2013-11-30T13:25:13+0000",
            "content": "2013 Old JIRA cleanup "
        },
        {
            "author": "Alexandre Rafalovitch",
            "id": "comment-16533206",
            "date": "2018-07-05T02:45:29+0000",
            "content": "We now have cursor-paging as well as export handler. This is probably sufficient for extra-large dataset exports and we no longer need to discuss this proposal.\n\nSafe to close? "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-16533817",
            "date": "2018-07-05T15:36:29+0000",
            "content": "+1 to close "
        },
        {
            "author": "Alexandre Rafalovitch",
            "id": "comment-16533918",
            "date": "2018-07-05T17:19:39+0000",
            "content": "The proposal superseded by other already implemented features. "
        }
    ]
}