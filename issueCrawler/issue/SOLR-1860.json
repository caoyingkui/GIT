{
    "id": "SOLR-1860",
    "title": "improve stopwords list handling",
    "details": {
        "affect_versions": "3.1",
        "status": "Closed",
        "fix_versions": [
            "3.6",
            "4.0-ALPHA"
        ],
        "components": [
            "Schema and Analysis"
        ],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Currently Solr makes it easy to use english stopwords for StopFilter or CommonGramsFilter.\nRecently in lucene, we added stopwords lists (mostly, but not all from snowball) to all the language analyzers.\n\nSo it would be nice if a user can easily specify that they want to use a french stopword list, and use it for StopFilter or CommonGrams.\n\nThe ones from snowball, are however formatted in a different manner than the others (although in Lucene we have parsers to deal with this).\nAdditionally, we abstract this from Lucene users by adding a static getDefaultStopSet to all analyzers.\n\nThere are two approaches, the first one I think I prefer the most, but I'm not sure it matters as long as we have good examples (maybe a foreign language example schema?)\n\n1. The user would specify something like:\n\n <filter class=\"solr.StopFilterFactory\" fromAnalyzer=\"org.apache.lucene.analysis.FrenchAnalyzer\" .../>\n This would just grab the CharArraySet from the FrenchAnalyzer's getDefaultStopSet method, who cares where it comes from or how its loaded.\n\n2. We add support for snowball-formatted stopwords lists, and the user could something like:\n\n<filter class=\"solr.StopFilterFactory\" words=\"org/apache/lucene/analysis/snowball/french_stop.txt\" format=\"snowball\" ... />\nThe disadvantage to this is they have to know where the list is, what format its in, etc. For example: snowball doesn't provide Romanian or Turkish\nstopword lists to go along with their stemmers, so we had to add our own.\n\nLet me know what you guys think, and I will create a patch.",
    "attachments": {
        "SOLR-1860.patch": "https://issues.apache.org/jira/secure/attachment/12450470/SOLR-1860.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Robert Muir",
            "id": "comment-12852978",
            "date": "2010-04-02T22:32:37+0000",
            "content": "A third idea from Hoss Man:\n\nWe should make it easy to edit these lists like english.\nSo an idea is to create an intl/ folder or similar under the example with stopwords_fr.txt, stopwords_de.txt\nAdditionally we could have a schema-intl.xml with example types 'text_fr', 'text_de', etc setup for various languages.\nI like this idea best. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12853135",
            "date": "2010-04-03T14:03:16+0000",
            "content": "How many languages are we talking? \n\n I like the idea of an export - it's transparent and neatly handles back compat concerns.\nTo avoid clutter, putting them all in a separate directory seems like a good idea:\n/conf/stopwords/stopwords_en.txt\n/conf/stopwords/stopwords_fr.txt\n\nOr will there be other per-language files?  If so, maybe\n/conf/lang/stopwords_en.txt\n/conf/lang/protected_en.txt\n/conf/lang/synonyms_en.txt\n\nAs far as file format: I think we sould also support the snowball stopword format.\n\nNot sure at this point if it makes more sense trying to put a text_fr, etc, in the normal schema.xml or in a separate schema_intl.xml.   Partly depends on the number of text_<lang> types and resource usage I guess... need to consider things like core load time, etc.\nWe may want to think about lazy-loaded analyzers (but that could be another ball of wax since misconfigurations don't immediately fail). "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12853679",
            "date": "2010-04-06T01:34:23+0000",
            "content": "I like the idea of an export - it's transparent and neatly handles back compat concerns.\n\nthat's the same conclusion robert and i came to on IRC ... being able to load directly sounds less redundent, but as soon as a user wants to customize (and let's face it: stop words can easily be domain specific) qe need a way of exporting that's convenient even for novice users who don't know anything about jars and wars.\n\nNot sure at this point if it makes more sense trying to put a text_fr, etc, in the normal schema.xml or in a separate schema_intl.xml.\n\nThe idea robert pitched on IRC was to create a new example solr-instance directory with a barebones solrconfig.xml file, and a schema.xml file that only demonstrated fields using various tricks for various lanagues.  All the language specific stopword files would then live in this new instancedir.  The idea being that people interested in non-english fields, could find a \"recommended\" fieldtype declaration in this schema.xml file, and cut/paste it to their schema.xml (probably copied from the main example)\n\nThe key here being that we don't want an entire clone of the example (all the numeric fields, and multiple request handler declarations,etc...)  this will just show the syntax for declaring all the various langages that we can provide suggestions for.\n\nAs far as file format: I think we sould also support the snowball stopword format.\n\nAgreed, but it's a trivially minor chicken/egg choice.  Either we can setup a simple export and conversion to the format Solr currently supports now, and if/when someon updates StopFilterFactory to support the new format, then we can stop converting when we export; or we can modify StopFilter to support both formats first, and then setup the simple export w/o worrying about conversion.   \n\nFrankly: If Robert's planning on doing the work either way, I'm happy to let him decide which approach makes the most sense. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12853684",
            "date": "2010-04-06T01:39:37+0000",
            "content": "Either we can setup a simple export and conversion to the format Solr currently supports now, and if/when someon updates StopFilterFactory to support the new format, then we can stop converting when we export\n\nWell, this isn't that big of a deal either way. \n\nIn Lucene we have a helper class called WordListLoader that supports loading this format from an InputStream.\n\nOne idea to consider: we could try merging some of what SolrResourceLoader does with this WordListLoader, then its all tested and in one place. \nit appears there might be some duplication of effort here... e.g. how long till a lucene user complains about UTF-8 bom markers in their stoplists \n\nWe can still use ant to keep the files in sync automatically from the lucene copies. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12892154",
            "date": "2010-07-25T23:50:24+0000",
            "content": "I'd still like to fix all this duplication between wordlistloader etc, but for now i will add the snowball stop support and introduce examples that use the embedded stopwords in the jar files.\n\nAnd as discussed on SOLR-2015, if we are gonna lay down traps for other languages like autogenerating phrase queries, then these should be in the main schema.xml, not tucked away. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12892300",
            "date": "2010-07-26T13:02:02+0000",
            "content": "here is a first step, 2 of the analyzers (Brazilian, Czech) use embedded stopword sets.\nI think this was an oversight, this moves these to .txt files like the rest "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12899812",
            "date": "2010-08-18T10:11:59+0000",
            "content": "committed this as rev 986612 (and 3x rev 986615).  "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12900969",
            "date": "2010-08-21T03:46:28+0000",
            "content": "This is a nice piece of work. One thing I've learned is that configurations should be as flat and transparent as possible. Pushing all of these word lists out of the classes and into files is a great improvement.  The Greek Analyzer, for example, is (was) nothing but a default list of stopwords.\n\nBut, having the stopwords as text files runs smack into character encoding wackiness (why, yes, I do use windows). Can the file format or importer at least support the XML or URL notations for Unicode characters? Maybe a list of words that include prot\u0274 ge for protege? "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12900975",
            "date": "2010-08-21T04:47:12+0000",
            "content": "\nThe Greek Analyzer, for example, is (was) nothing but a default list of stopwords.\n\nThis is no longer true. there is a stemmer, too.\n\n\nBut, having the stopwords as text files runs smack into character encoding wackiness (why, yes, I do use windows).\n\nWhat wackiness? The files are all unicode UTF-8, which windows too supports.\n\n\nCan the file format or importer at least support the XML or URL notations for Unicode characters?\n\nOnly if we escape with ALL english strings in all files too. But I prefer things to be readable. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-12901103",
            "date": "2010-08-21T21:52:44+0000",
            "content": "What wackiness? The files are all unicode UTF-8, which windows too supports.\n\n'Supports' does not mean 'you can get it done without a pounding headache'. UTF-8 is not the default and you cannot make it the default. I'm guessing some linux editors don't understand the funky binary starting bytes that mark a UTF-8 file. Having UTF-8 characters in the Java source blows up also.   An XML file format would go a long way to useability. \n\n.  "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12901104",
            "date": "2010-08-21T22:02:57+0000",
            "content": "If it's documented to be UTF-8, its clear what you have to provide (in Solr). If you use Lucene directly, the stopword file parser does not care about encodings at all, it simply takes a java.io..Reader. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12901105",
            "date": "2010-08-21T22:07:57+0000",
            "content": "Lance, I don't know what your OS problems are, but the whole reason it exists is so things like these files can be viewable/editable in their own languages and not encoded in hex.\n\nSo, I don't plan on making life cryptic for people that use languages other than english because you are scared of UTF-8 or don't know how to configure your computer. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13200790",
            "date": "2012-02-05T15:24:02+0000",
            "content": "Now that Simon cleaned up wordlistloader, this is easy.\n\nAttached is a patch to support the snowball format (format=\"snowball\") in StopFilterFactory and the common-grams factories.\n\nAlong with something like the ant task in SOLR-3097, we should be able to move forwards with having some default configurations for other languages out-of-box.\n "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13200836",
            "date": "2012-02-05T19:55:44+0000",
            "content": "I committed this.\n\nIll open up a new issue (related to SOLR-3097),\nto provide setups for other languages. "
        }
    ]
}