{
    "id": "LUCENE-1574",
    "title": "PooledSegmentReader, pools SegmentReader underlying byte arrays",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/other"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "affect_versions": "2.4.1",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "PooledSegmentReader pools the underlying byte arrays of deleted docs and norms for realtime search.  It is designed for use with IndexReader.clone which can create many copies of byte arrays, which are of the same length for a given segment.  When pooled they can be reused which could save on memory.  \n\nDo we want to benchmark the memory usage comparison of PooledSegmentReader vs GC?  Many times GC is enough for these smaller objects.",
    "attachments": {
        "LUCENE-1574.patch": "https://issues.apache.org/jira/secure/attachment/12469203/LUCENE-1574.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-04-02T20:23:44+0000",
            "content": "Presumably it wouldn't save on memory (the pool would presumably sometimes be holding onto spares, for future reuse), but could save on time, right?\n\nOr, maybe instead we could spend our effort making a simple transactional data structure for holding deletes/norms (I think there's already an issue on this \u2013 maybe it's LUCENE-1526). ",
            "author": "Michael McCandless",
            "id": "comment-12695115"
        },
        {
            "date": "2009-04-02T20:52:00+0000",
            "content": "True the pool would hold onto spares, but they would expire.\nIt's mostly useful for the large on disk segments as those byte\narrays (for BitVectors) are large, and because there's more docs\nin them would get hit with deletes more often, and so they'd be\nreused fairly often. \n\nI'm not knowledgeable enough to say whether the transactional\ndata structure will be fast enough. We had been using\nhttp://fastutil.dsi.unimi.it/docs/it/unimi/dsi/fastutil/ints/IntR\nBTreeSet.html in Zoie for deleted docs and it's way slow. Binary\nsearch of an int array is faster, albeit not fast enough. The\nmulti dimensional array thing isn't fast enough (for searching)\nas we implemented this in Bobo. It's implemented in Bobo because\nwe have a multi value field cache (which is quite large because\nfor each doc we're storing potentially 64 or more values in an\ninplace bitset) and a single massive array kills the GC. In some\ncases this is faster than a single large array because of the\nway Java (or the OS?) transfers memory around through the CPU\ncache.  ",
            "author": "Jason Rutherglen",
            "id": "comment-12695130"
        },
        {
            "date": "2009-06-10T22:18:38+0000",
            "content": "Moving out. ",
            "author": "Michael McCandless",
            "id": "comment-12718232"
        },
        {
            "date": "2009-08-01T20:37:36+0000",
            "content": "Re: Zoie and deleted docs:\nThat is no longer true, Zoie is using a bloom filter over a intHash set from fastutil for exactly the perf reason Jason pointed. ",
            "author": "John Wang",
            "id": "comment-12737950"
        },
        {
            "date": "2009-11-11T06:03:42+0000",
            "content": "Yonik,\n\nDo you recommend using the method in SimpleStringInterner for lockless pooling? ",
            "author": "Jason Rutherglen",
            "id": "comment-12776299"
        },
        {
            "date": "2009-11-11T06:50:11+0000",
            "content": "I suppose as we're on Java 1.5, ConcurrentLinkedQueue can be used. ",
            "author": "Jason Rutherglen",
            "id": "comment-12776321"
        },
        {
            "date": "2009-11-11T19:02:10+0000",
            "content": "A likely optimization for this patch is we'll only pool if the doc count is above a threshold, 100,000 seems like a good number.  Also pooling will be optional.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12776576"
        },
        {
            "date": "2011-01-21T23:24:09+0000",
            "content": "I'm going to revive this, and if it works fine for trunk, then we can use the basic system for RT eg, LUCENE-2312.  I think the only open question is how we'll shrink the pool, most likely there'd be an expiration on the pooled objects.  With RT, the parallel arrays will grow, so the pool will need to be size based, eg, when the arrays are grown, all of the previous arrays may be forcefully evicted, or they may simply expire.\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12984996"
        },
        {
            "date": "2011-01-22T11:16:09+0000",
            "content": "I'm working on an initial patch for this...\n\nI think the only open question is how we'll shrink the pool, most likely there'd be an expiration on the pooled objects.\n\nI think we can simply have a \"max pooled free bit vectors\"... or we may want to expire by time/staleness as well.\n\nWith RT, the parallel arrays will grow, so the pool will need to be size based, eg, when the arrays are grown, all of the previous arrays may be forcefully evicted, or they may simply expire.\n\nTrue... but, like the other per-doc arrays, the BV can be overallocated (ArrayUtil.oversize) to accommodate further added docs. ",
            "author": "Michael McCandless",
            "id": "comment-12985091"
        },
        {
            "date": "2011-01-22T16:18:28+0000",
            "content": "ThreadPoolExecutor can act as a guide, it's main parameters are corePoolSize, maximumPoolSize, keepAliveTime.  \n\nIn regards to using System.arraycopy for the RT parallel arrays, if they grow to become too large, then SC could become a predominant cost.  However if the default thread states is 8, which'd mean that many DWPTs, the arrays would probably never grow to be too large for their SC to become noticeably expensive, hopefully. ",
            "author": "Jason Rutherglen",
            "id": "comment-12985150"
        },
        {
            "date": "2011-01-23T20:51:58+0000",
            "content": "We want to record the deletes between getReader calls however there's no way to know ahead of time if a term or query is going to hit many documents or not, meaning we can't always save del docids, because we'd have too many ints in RAM.  I'm curious how we plan on handling this case? ",
            "author": "Jason Rutherglen",
            "id": "comment-12985408"
        },
        {
            "date": "2011-01-24T18:28:39+0000",
            "content": "I'm curious how we plan on handling this case?\n\nI think we should keep the replay log smallish, or, expire it aggressively with age.  I suspect this opto is only going to be \"worth it\" for very frequent reopens... but I'm not sure yet. ",
            "author": "Michael McCandless",
            "id": "comment-12985877"
        },
        {
            "date": "2011-01-24T20:37:07+0000",
            "content": "Attached rough patch.\n\nAt least one test fails....\n\nAnd, I haven't yet seen that this is in fact worthwhile.  The rough benchmark I have (which hits other issues so the results aren't conclusive yet) doesn't show much difference w/ this patch.  I think this patch may only be worthwhile at insane reopen rates, which I think in practice is rarely a legitimate use case (even though many apps start off thinking it is). ",
            "author": "Michael McCandless",
            "id": "comment-12985971"
        },
        {
            "date": "2011-01-24T20:51:06+0000",
            "content": "What size segments is the benchmark deleting against?  Maybe we're underestimating the speed of arraycopy, eg, it's really a hardware operation that could be optimized? ",
            "author": "Jason Rutherglen",
            "id": "comment-12985975"
        },
        {
            "date": "2011-01-25T19:28:29+0000",
            "content": "I've been testing on a 25M doc index (all of en Wikipedia, at least as of March 2010).\n\nYes, I think likely alloc of big BitVector, System.arraycopy, destroying it, may be a fairly low cost compared to lucene resolving the deleted term, indexing the doc, flushing the tiny segment, etc. ",
            "author": "Michael McCandless",
            "id": "comment-12986630"
        },
        {
            "date": "2011-01-25T19:33:54+0000",
            "content": "I think likely alloc of big BitVector, System.arraycopy, destroying it, may be a fairly low cost compared to lucene resolving the deleted term, indexing the doc, flushing the tiny segment, etc.\n\nRight, and if we pool the byte[]s we'd take the cost of instantiating and GC'ing out of the picture in the high NRT throughput case.  It's counter intuitive and will require testing. ",
            "author": "Jason Rutherglen",
            "id": "comment-12986637"
        },
        {
            "date": "2013-07-23T18:44:25+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 ",
            "author": "Steve Rowe",
            "id": "comment-13716945"
        },
        {
            "date": "2014-04-16T12:54:41+0000",
            "content": "Move issue to Lucene 4.9. ",
            "author": "Uwe Schindler",
            "id": "comment-13970843"
        }
    ]
}