{
    "id": "LUCENE-4063",
    "title": "FrenchLightStemmer performs abusive compression of (arbitrary) repeated characters in long tokens",
    "details": {
        "affect_versions": "3.4,                                            4.0-ALPHA",
        "status": "Closed",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "FrenchLightStemmer performs aggressive deletions on repeated character sequences, even on numbers.\nThis might be unexpected during full text search.",
    "attachments": {
        "SOLR-3463.patch": "https://issues.apache.org/jira/secure/attachment/12527647/SOLR-3463.patch",
        "LUCENE-4063.patch": "https://issues.apache.org/jira/secure/attachment/12527689/LUCENE-4063.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Tanguy Moal",
            "id": "comment-13276832",
            "date": "2012-05-16T16:00:23+0000",
            "content": "This patch implements the solution suggested by Robert Muir on the ML.\n\nPatch for lucene/solr trunk, generated from root directory. "
        },
        {
            "author": "Tanguy Moal",
            "id": "comment-13276848",
            "date": "2012-05-16T16:08:54+0000",
            "content": "Updated patch to cover corner case (code also performs additional deletion of last character if it equals last character minus 1.\n\nAlso added very minimal unit test (which exhibited the uncovered corner case) "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13276856",
            "date": "2012-05-16T16:26:55+0000",
            "content": "+1\n\nTanguy, can you add a couple more tests?  You should demonstrate that the deletion of repeated characters still works (with letter chars).  Also, since there are two repetition removal operations in the code, a test specific to each would be useful. "
        },
        {
            "author": "Tanguy Moal",
            "id": "comment-13276867",
            "date": "2012-05-16T16:40:13+0000",
            "content": "Added some tests related to this issue. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13276875",
            "date": "2012-05-16T16:46:15+0000",
            "content": "Added some tests related to this issue.\n\nThe new patch looks identical to the old patch - I don't see any new tests? "
        },
        {
            "author": "Tanguy Moal",
            "id": "comment-13276977",
            "date": "2012-05-16T18:28:32+0000",
            "content": "I didn't drink anything yet but maybe it's time to begin  "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13277051",
            "date": "2012-05-16T19:44:41+0000",
            "content": "Tanguy, since this is entirely a Lucene change, I've moved the issue's project from Solr to Lucene. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13277061",
            "date": "2012-05-16T19:59:22+0000",
            "content": "Patch with a couple more tests, and a CHANGES.txt entry.\n\nCommitting to trunk shortly. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13277066",
            "date": "2012-05-16T20:06:00+0000",
            "content": "Committed to trunk.  Thanks Tanguy!\n\nI'm not sure if this should be committed on the 3.6 branch, since that branch is bug-fix only, and this issue is marked as an improvement.  Thoughts? "
        },
        {
            "author": "Tanguy Moal",
            "id": "comment-13277635",
            "date": "2012-05-17T07:19:16+0000",
            "content": "I'd be glad to see this on 3.x x >=4 since that's the version I used to spot the issue, may be should I have marked this issue as a bug rather than improvement ? \n\nI have a custom filterfactory marking numbers as keywords anyway as I needed a quick fix.\nSo from my point of view it doesn't really matter... I could just drop that filter from my analysis if the patch finds its way to 3x.\n\nThank you very much for your quick responses about this issue. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13277728",
            "date": "2012-05-17T11:15:18+0000",
            "content": "As far as this being a bug, the original code implements the algorithm it claims to implement, and undoubling anything was its heuristic: see http://members.unine.ch/jacques.savoy/clef/frenchStemmerPlus.txt "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13278941",
            "date": "2012-05-18T16:46:39+0000",
            "content": "I agree with Robert, this is a design change, not a bug fix, so I won't backport to the 3.6 branch. "
        },
        {
            "author": "Tanguy Moal",
            "id": "comment-13287408",
            "date": "2012-06-01T13:54:43+0000",
            "content": "I agree with both of you, it sounds like a design change.\n\nI think Jacques Savoy's algorithm was intended to be used on words. Not on numbers, or mixes of both (like in 22h00).\n\nWhich is true for any stemmer, I think. That's why on the mailing I also suggested we could have each stemmer share a common interface that would filter non-stemmable literals out of the way. That could prevent the same issue to rise from a different stemming implementation.\n\nI'm just saying this as I think about it. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13287447",
            "date": "2012-06-01T14:36:36+0000",
            "content": "\nThat's why on the mailing I also suggested we could have each stemmer share a common interface that would filter non-stemmable literals out of the way\n\nWe actually have this in place, but its too limited. Its called KeywordAttribute. When this is set, the stemmer will not touch the word.\n\nCurrently the only way to set this out of box is to use KeywordMarkerFilter which takes a Set of protected words.\n\nBut to make your idea more flexible: I could imagine a couple more filters:\n\n\tone that marks as Keyword based on a set of types. In this case you would just add NUM to that set, and no stemmers would touch any numbers. Of course\n  for french this is solved already, but imagine if you are using the URLEmail tokenizer: I think a set like \n{ URL, EMAIL }\n would be very useful,\n  otherwise stemmers will probably muck with them.\n\tone that marks as Keyword based on a regular expression. This could be good for fine-tuning stemmers for a lot of general purpose needs: e.g. on the\n  mailing list before someone was unhappy about how russian stemmers would treat russian place names and they had a certain set of suffixes they didnt\n  want stemmed.\n\n\n\nAnyway, I would really like to see these filters, I think they would be pretty simple to implement as well.  "
        }
    ]
}