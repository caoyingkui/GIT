{
    "id": "LUCENE-7639",
    "title": "Use Suffix Arrays for fast search with leading asterisks",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "If query term starts with asterisks FST checks all words in the dictionary so request processing speed falls down. This problem can be solved with Suffix Array approach. Luckily, Suffix Array can be constructed after Lucene start from existing index. Unfortunately, Suffix Arrays requires a lot of RAM so we can use it only when special flag is set:\n\n-Dsolr.suffixArray.enable=true\n\nIt is possible to  speed up Suffix Array initialization using several threads, so we can control number of threads with \n\n-Dsolr.suffixArray.initialization_treads_count=5\n\nThis system property can be omitted, the default value is 5.  \n\nAttached patch is the suggested implementation for SuffixArray support, it works for all terms starting with asterisks with at least 3 consequent non-wildcard characters. This patch do not change search results and  affects only performance issues.\n\nUpdate\nsuffix-arra-2.patch is an improved version of the first patch, system properties for it are following::\n\nlucene.suffixArray.enable - true, if you want to enable Suffix Array support. Default value - false.\nlucene.suffixArray.initializationThreadsCount - number of threads for Suffix Array initialization, if you set 0 - no additional threads used. Default value - 5.",
    "attachments": {
        "suffix-array.patch": "https://issues.apache.org/jira/secure/attachment/12847797/suffix-array.patch",
        "suffix-array-2.patch": "https://issues.apache.org/jira/secure/attachment/12861846/suffix-array-2.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-01-17T13:10:29+0000",
            "content": "Wouldn't it be better to create an fst for all reversed forms of all terms and lookup the leading wildcard's matches by its concrete suffix instead (you'd need to reverse it too)? This wouldn't work for (single) infix wildcards, but these could then be implemented as an intersection between the two (terms matching the leading/trailing set).\n\nAlso, there are better (as in: linear) ways of constructing suffix arrays compared to sorting. We used to implement a few of them, if you wanted to reuse the code (it should be optimized for strings/ byte sequences though) [1].\n\n[1] https://github.com/carrotsearch/jsuffixarrays ",
            "author": "Dawid Weiss",
            "id": "comment-15826021"
        },
        {
            "date": "2017-01-17T14:15:50+0000",
            "content": "Of cause, index for reversed terms is popular solution for this problem, but it doesn't supports cases when we have asterisks at both ends and it requires much bigger index. Imagine situation when you have 1 read-only storage for index that is used by 10 servers. You can enable suffix array support only at 1 server with bigger RAM size and route all requests with asterisks to it, additional cost for such solution will be pretty small.\n\nFast construction of suffix array is the most challenging part of  this approach and I spent a lot of time trying various algorithms. Unfortunately, linear-time algorithms are too complex, usually its initialization takes longer than 3-way radix quicksort full run. Also performance of suffix array creation is not very important, because current approach transparently uses FST for the segments where suffix array construction hasn't been completed yet. And using multiple threads for suffix arrays construction allows to reduce initialization time to acceptable values. Also 3-way radix quicksort is perfect from RAM usage point of view. ",
            "author": "Yakov Sirotkin",
            "id": "comment-15826108"
        },
        {
            "date": "2017-01-17T14:22:02+0000",
            "content": "[...] and it requires much bigger index\n\nWhy is this the case? The inverted FST should be smaller than the suffix array and intersecting both should be a viable option to get *abc* wildcard matches? ",
            "author": "Dawid Weiss",
            "id": "comment-15826115"
        },
        {
            "date": "2017-01-17T14:43:50+0000",
            "content": "Inverted FST requires additional space on disk and additional RAM, and Suffix Arrays requires only a lot of (much bigger that inverted FST) additional RAM, so there are different pro et contra for these approaches. And I suppose that it is not possible to process *abc* terms fast with inverted FST. ",
            "author": "Yakov Sirotkin",
            "id": "comment-15826150"
        },
        {
            "date": "2017-01-17T14:54:18+0000",
            "content": "Sure, I didn't mean to somehow diminish suffix arrays \u2013 they're super nice! I'd still be curious whether the thing can be done on a finite state automaton alone. ",
            "author": "Dawid Weiss",
            "id": "comment-15826166"
        },
        {
            "date": "2017-01-17T15:15:07+0000",
            "content": "I'd still be curious whether the thing can be done on a finite state automaton alone.\n\nThat was my starting point: I tried to trace FST and figure out what is wrong with leading asterisks and how it can be improved. But I suppose that FST is so perfect that any attempt to improve performance for leading asterisks will decrease performance for the rest of requests. And leading asterisk is the FST's Achilles' heel, it iterates over all words in the index and says: \"Oops, this is wrong word, let's try the next one!\" ",
            "author": "Yakov Sirotkin",
            "id": "comment-15826191"
        },
        {
            "date": "2017-01-18T09:36:01+0000",
            "content": "Nothing is wrong with the FST, really. A suffix array is functionally equivalent to a suffix tree, which you could build and encode as an FST. Then any infix matching would be done similarly to suffix array-based lookups. These are all interchangeable concepts. The way we use FSTs in Lucene is just one particular application to solve one particular problem (essentially implement an efficient partial prefix trie), hence my remark on the possibility of using two automata to make an intersection of wildcards possible.\n\nAs for your patch, I really don't know how much of a problem (in typical use) prefix and infix wildcards are. While leading wildcards are heavily used for many things (suggestions, etc.) I can't quite remember when I needed a prefix or infix wildcard. \n\nIn other words \u2013 don't know whether there'll be much interest in maintaining this code if it went into Lucene, especially considering the implementation details you mentioned (amount of memory it requires at runtime, separate construction process, etc.). Let's see what others think though. ",
            "author": "Dawid Weiss",
            "id": "comment-15827721"
        },
        {
            "date": "2017-01-18T11:19:41+0000",
            "content": "Hi,\nI am fine with adding those optimizations to the index file format (one could save the suffix array as part of the BlockTreeTermsdict?). As this is not needed by all users, the right way to do this would be to use another codec that adds this to the terms dict.\n\nWhat is impossible in Lucene (which is a library, not only used by Solr):\n\n\tDon't spawn thread pools or threads anywhere inside Lucene. Those stuff has to be done by the calling code (e.g., passing an Executor instance, see IndexSearcher). Of course this does not work with static initializers and it can also not passed to codecs. We have a forbiddenapis rule that forbids that. In addition, you are using a thread pool without proper thread names.\n\tLucene nowhere reads system properties, especially not those starting with \"solr.\". The right way to configure this is by customizing your codec you pass to IndexWriter/IndexReader.\n\tWe also do not want to do staff like this when IndexReader initializes or the first query executes. For that reason \"FieldCache\" was removed, which was invented as a workaround in Lucene initially (uncontrollable memory usage, delays on searches,...), and it took long time to get rid of it! This patch looks identical to this - it is a \"cache\" which is generated on the fly. The message here is: If something like a suffix tree is useful for searching, then it must be persisted to disk during indexing (see above proposal to have a BlockTreeTerms variant as separate codec).\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-15827886"
        },
        {
            "date": "2017-02-03T11:42:48+0000",
            "content": "It would be nice to have a simple option to make heavy (infix, prefix) wildcard queries fast.\n\nI think a custom codec, and likely a custom WildcardQuery impl that \"sees\" it is working with the custom codec and taps into the suffix array, is a good way to implement this.  It should maybe be a straightforward conversion of the current patch into a custom codec, i.e. your codec's postings implementation would wrap the default codec and hold onto the WildcardHelper instance.\n\nSeparately, I am curious how Dawid Weiss's idea (also index the reversed form of the field, then do two prefix searches and intersect the resulting terms) compares in performance (index time, index size, search heap, query cost) to the suffix array.\n\nThe patch falls back to Java's Pattern for checking each term in the more complex cases, but couldn't you just use the CompiledAutomaton's run method to check instead?\n\nI ran Lucene's tests w/ this patch, but first 1) hard-wiring the property check to true, and 2) making the init synchronous (not using the thread pool), and Lucene's WildcardQuery tests hit some failures, e.g.:\n\n\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestTerms -Dtests.method=testTermMinMaxRandom -Dtests.seed=11FDF2AE5B77A883 -Dtests.locale=es-GT -Dtests.timezone=CET -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] FAILURE 0.04s J0 | TestTerms.testTermMinMaxRandom <<<\n   [junit4]    > Throwable #1: java.lang.AssertionError\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([11FDF2AE5B77A883:5D8E7DDA37EB73E6]:0)\n   [junit4]    > \tat org.apache.lucene.util.UnicodeUtil.UTF8toUTF16(UnicodeUtil.java:593)\n   [junit4]    > \tat org.apache.lucene.util.BytesRef.utf8ToString(BytesRef.java:152)\n   [junit4]    > \tat org.apache.lucene.codecs.blocktree.WildcardHelper.<init>(WildcardHelper.java:106)\n   [junit4]    > \tat org.apache.lucene.codecs.blocktree.FieldReader.<init>(FieldReader.java:106)\n   [junit4]    > \tat org.apache.lucene.codecs.blocktree.BlockTreeTermsReader.<init>(BlockTreeTermsReader.java:234)\n   [junit4]    > \tat org.apache.lucene.codecs.lucene50.Lucene50PostingsFormat.fieldsProducer(Lucene50PostingsFormat.java:445)\n   [junit4]    > \tat org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:109)\n   [junit4]    > \tat org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:74)\n   [junit4]    > \tat org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:143)\n   [junit4]    > \tat org.apache.lucene.index.ReadersAndUpdates.getReadOnlyClone(ReadersAndUpdates.java:195)\n   [junit4]    > \tat org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:103)\n   [junit4]    > \tat org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:473)\n   [junit4]    > \tat org.apache.lucene.index.RandomIndexWriter.getReader(RandomIndexWriter.java:376)\n   [junit4]    > \tat org.apache.lucene.index.RandomIndexWriter.getReader(RandomIndexWriter.java:313)\n   [junit4]    > \tat org.apache.lucene.index.TestTerms.testTermMinMaxRandom(TestTerms.java:76)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n\n\n\nBut those failures a possibly harmless, because those tests are sending non-UTF8 data into Lucene, whereas this change (the property) would only be enabled on fields that are UTF8.\n\nIt also hit a stack overflow w/ a long term:\n\n\n   [junit4] Suite: org.apache.lucene.index.TestIndexWriter\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestIndexWriter -Dtests.method=testWickedLongTerm -Dtests.seed=524558B2F180613F -Dtests.locale=ru-RU -Dtests.timezone=Atlantic/Faroe -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] ERROR   2.47s J1 | TestIndexWriter.testWickedLongTerm <<<\n   [junit4]    > Throwable #1: java.lang.StackOverflowError\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([524558B2F180613F:120594CA57A7BADF]:0)\n   [junit4]    > \tat org.apache.lucene.codecs.blocktree.SuffixArrayBytes.sort(SuffixArrayBytes.java:82)\n   [junit4]    > \tat org.apache.lucene.codecs.blocktree.SuffixArrayBytes.sort(SuffixArrayBytes.java:84)\n   [junit4]    > \tat org.apache.lucene.codecs.blocktree.SuffixArrayBytes.sort(SuffixArrayBytes.java:84)\n   [junit4]    > \tat org.apache.lucene.codecs.blocktree.SuffixArrayBytes.sort(SuffixArrayBytes.java:84)\n\n\n\nI guess your radix sort implementation is consuming one java stack frame per character in the term.  Maybe in practice this is OK too. ",
            "author": "Michael McCandless",
            "id": "comment-15851384"
        },
        {
            "date": "2017-02-03T23:46:16+0000",
            "content": "The inverted FST should be smaller than the suffix array and intersecting both should be a viable option to get *abc* wildcard matches?\n\nHmm Dawid Weiss can you explain more how the infix case (*abc*) could be handled by the forward and reversed FSTs? ",
            "author": "Michael McCandless",
            "id": "comment-15852345"
        },
        {
            "date": "2017-02-04T21:13:32+0000",
            "content": "On vacation, so short. Now that I thought about it for longer I think my reasoning here was wrong \u2013 the reversed fst would work for prefix wildcards, but for infixes you'd still need a full suffix tree (or an automaton created for all suffixes and leading to term ID):\n\nA suffix array is functionally equivalent to a suffix tree, which you could build and encode as an FST. Then any infix matching would be done similarly to suffix array-based lookups. ",
            "author": "Dawid Weiss",
            "id": "comment-15852935"
        },
        {
            "date": "2017-02-04T21:13:58+0000",
            "content": "On vacation, so short. Now that I thought about it for longer I think my reasoning here was wrong \u2013 the reversed fst would work for prefix wildcards, but for infixes you'd still need a full suffix tree (or an automaton created for all suffixes and leading to term ID):\n\nA suffix array is functionally equivalent to a suffix tree, which you could build and encode as an FST. Then any infix matching would be done similarly to suffix array-based lookups. ",
            "author": "Dawid Weiss",
            "id": "comment-15852937"
        },
        {
            "date": "2017-04-04T07:30:18+0000",
            "content": "System properties for  suffix-arra-2.patch:\n\nlucene.suffixArray.enable - true, if you want to enable Suffix Array support. Default value - false.\n\nlucene.suffixArray.initializationThreadsCount - number of threads for Suffix Array initialization, if you set 0 - no additional threads used. Default value - 5. ",
            "author": "Yakov Sirotkin",
            "id": "comment-15954707"
        },
        {
            "date": "2017-04-04T07:38:21+0000",
            "content": "Many thanks to all for feedback, here is the list of changes in suffix-array-2.patch:\n\n1. Suffix Array construction implemented without recursion, it fixes major bug discovered by TestIndexWriter.testWickedLongTerm test.\n2. Sort wordIds instead of words  - words are already sorted in index. \n3. SegmentTermsEnum used inside ListTermsEnum.\n4. Entire Suffix Array construction moved to special thread to avoid startup delays.\n5. Properties renamed to lucene.suffixArray.enable and lucene.suffixArray.initializationThreadsCount.\n6. If lucene.suffixArray.initializationThreadsCount set to 0, initialization is synchronous, additional ExecutorService is not created.  \n7. CompiledAutomaton used instead of Java's Pattern.\n8. Additional flag lucene.suffixArray.optimizeForUTF with default value true was added. If it is set to false, we assume that index can contain any bytes, not necessary representing UTF characters. In this case code starts to pass some tests, but for real application it increase memory consumption twice and reduce performance.  ",
            "author": "Yakov Sirotkin",
            "id": "comment-15954722"
        },
        {
            "date": "2017-04-04T07:41:10+0000",
            "content": "Maybe I have explanation why search with leading asterisk is not easy. Let's assume that you have traditional address book on paper and your are looking for someone with compound surname Zeta-Jones. If you forget the second part you can search by Zeta without any problems.\nBut if you forget the first part, you need to check the whole address book looking for Jones, in fact, index is useless in such case. ",
            "author": "Yakov Sirotkin",
            "id": "comment-15954727"
        }
    ]
}