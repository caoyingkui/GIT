{
    "id": "LUCENE-5015",
    "title": "Unexpected performance difference between SamplingAccumulator and StandardFacetAccumulator",
    "details": {
        "components": [
            "modules/facet"
        ],
        "fix_versions": [
            "4.4",
            "6.0"
        ],
        "affect_versions": "4.3",
        "priority": "Minor",
        "labels": "",
        "type": "Bug",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "I have an unexpected performance difference between the SamplingAccumulator and the StandardFacetAccumulator. \n\nThe case is an index with about 5M documents and each document containing about 10 fields. I created a facet on each of those fields. When searching to retrieve facet-counts (using 1 CountFacetRequest), the SamplingAccumulator is about twice as fast as the StandardFacetAccumulator. This is expected and a nice speed-up. \n\nHowever, when I use more CountFacetRequests to retrieve facet-counts for more than one field, the speeds of the SampingAccumulator decreases, to the point where the StandardFacetAccumulator is faster. \n\n\n \nFacetRequests  Sampling    Standard\n 1               391 ms     1100 ms\n 2               531 ms     1095 ms \n 3               948 ms     1108 ms\n 4              1400 ms     1110 ms\n 5              1901 ms     1102 ms\n\n \n\nIs this behaviour normal? I did not expect it, as the SamplingAccumulator needs to do less work? \n\nSome code to show what I do:\n\n\n\tsearcher.search( facetsQuery, facetsCollector );\n\tfinal List<FacetResult> collectedFacets = facetsCollector.getFacetResults();\n\n\n\n\n\nfinal FacetSearchParams facetSearchParams = new FacetSearchParams( facetRequests );\n\nFacetsCollector facetsCollector;\n\nif ( isSampled )\n{\n\tfacetsCollector =\n\t\tFacetsCollector.create( new SamplingAccumulator( new RandomSampler(), facetSearchParams, searcher.getIndexReader(), taxo ) );\n}\nelse\n{\n\tfacetsCollector = FacetsCollector.create( FacetsAccumulator.create( facetSearchParams, searcher.getIndexReader(), taxo ) );",
    "attachments": {
        "LUCENE-5015.patch": "https://issues.apache.org/jira/secure/attachment/12584544/LUCENE-5015.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2013-05-23T08:18:21+0000",
            "content": "Hello Rob,\n\nIndeed that looks unexpected.\nThe immediate suspect is the \"fixing\" part of the sampling, where after sampled top-cK are computed for each facet request, each of the candidates for top-K gets a real count computation, rather than a count over the sampled set of results.\n\nHow many results are in the result set? All the documents? ",
            "author": "Gilad Barkai",
            "id": "comment-13664976"
        },
        {
            "date": "2013-05-23T08:22:45+0000",
            "content": "I use a MatchAddDocsQuery(), so I retrieve all the 5 million documents as hits. ",
            "author": "Rob Audenaerde",
            "id": "comment-13664980"
        },
        {
            "date": "2013-05-23T09:09:41+0000",
            "content": "Sampling, with its defaults, has its toll. \n\nIn its defaults, Sampling aims to produce the exact top-K results for each request, as if a StandardFacetAccumulator would have been used. Meaning it aims at producing the same top-K with the same counts.\n\nThe process begins with sampling the result set and computers the top-cK candidates for each of the M facet requests, producing amortized results. That part is faster than StandardFacetAccumulator because less documents' facets information gets processed.\n\nThe next part is the \"fixing\", using a SampleFixer retrieved from a Sampler, in which \"fixed\" counts are produced which correlate better with the original document result set, rather than the sampled one. The default (and currently only implementation) for such fixer is TakmiSampleFixer which produced exact counts for each of the cK candidates for each of the M facet requests. The counts are not computed against the facet information of each document, but rather matching the skiplist of the drill-down term, of each such candidate category with the bitset of the (actual) document results. The amount of matches is the count. \nThis is equivalent to total-hit collector with a drilldown query for the candidate category over original query. \nThere's tipping point in which not sampling is faster than sampling and fixing using c x K x M skiplists matches against the bitset representing the document results. c defaults to 2 (see overSampleFactor in SamplingParams); \n\nOver-sampling (a.k.a c) is important for exact counts, as it is conceivable that the accuracy of a sampled top-k is not 100%, but according to some measures we once ran it is very likely that the true top-K results are within the sampled 2K results. Fixing those 2K with their actual counts and re-sorting them accordingly yields much more accurate top-K. \n\n\nE.g Requesting 5 count requests for top-10 with overSampleFactor of 2, results in 100 skiplist matching against the document results bitset.\n\n\nIf amortized results suffice, a different SampleFixer could be coded - which E.g amortize the true count from the sampling ration. E.g if category C got count of 3, and the sample was of 1,000 results out of a 1,000,000 than the \"AmortizedSampleFixer\" would fix the count of C to be 3,000.\nSuch fixing is very fast, and the overSampleFactor should be set to 1.0.\n\nEdit:\nI now see that it is not that easy to code a different SampleFixer, nor get it the information needed for the amortized result fixing as suggested above. \nI'll try to open the API some and make it more convenient. ",
            "author": "Gilad Barkai",
            "id": "comment-13665018"
        },
        {
            "date": "2013-05-23T10:01:55+0000",
            "content": "Hi Gilad,\n\nThanks for the swift and very useful reply, it has given me good insight in the sampling process. \n\nAs I see it, I could benefit from a more straight-forward SampleFixer; like the behaviour of the AmortizedSampleFixer you described. It would be great to have one, or be able to code one up, so +1 for the API improvements which are needed for this.  ",
            "author": "Rob Audenaerde",
            "id": "comment-13665047"
        },
        {
            "date": "2013-05-23T18:36:33+0000",
            "content": "Added a parameter to SamplingParams named fixToExact which defaults to false. \nI think it is probable that one who uses sampling may not be interested in exact results.\n\nIn the proposed approach, the Sampler would create either the old, slow, and accurate TakmiSampleFixer if SamplingParams.shouldFixToExact() is true. Otherwise the much (much!} faster AmortizedSampleFixer would be used, when it only take under account the sampling ratio, assuming the sampled set represent the whole set with 100% accuracy.\n\nWith these changes, the code above should already use the amortized fixer, as the default is now it.\nIf the old fixer is to be used - for comparison - the code could look as follows:\n\n\nfinal FacetSearchParams facetSearchParams = new FacetSearchParams( facetRequests );\n\nFacetsCollector facetsCollector;\n\nif ( isSampled )\n{\n\t// Create SamplingParams which denotes fixing to exact\n\tSamplingParams samplingParams = new SamplingParams();\n\tsamplingParams.setFixToExact(true);\n\n\t// Use the custom sampling params while creating the RandomSampler\n\tfacetsCollector =\n\t\tFacetsCollector.create( new SamplingAccumulator( new RandomSampler(samplingParams, new Random(someSeed)), facetSearchParams, searcher.getIndexReader(), taxo ) );\n}\nelse\n{\n\tfacetsCollector = FacetsCollector.create( FacetsAccumulator.create( facetSearchParams, searcher.getIndexReader(), taxo ) );\n}\n\n\n\nThe sampling tests still use the \"exact\" fixer, as it is not easy asserting against amortized results. I'm still looking into creating a complete faceted search flow test with the amortized-fixer. ",
            "author": "Gilad Barkai",
            "id": "comment-13665468"
        },
        {
            "date": "2013-05-23T18:39:39+0000",
            "content": "Older patch was against trunk/lucene/facet. This one is rooted with trunk.  ",
            "author": "Gilad Barkai",
            "id": "comment-13665469"
        },
        {
            "date": "2013-05-24T05:27:37+0000",
            "content": "Gilad this looks good! I have few comments:\n\n\n\tAmortizedSampleFixer's jdocs need a <p> tag instead of the empty line. Otherwise I think this does not render as expected.\n\t\n\t\tSame in TakmiSamplerFixer\n\t\n\t\n\tSampleFixer has a TODO next to the new param\n\tAmortizedSamplerFixerTest and SamplerTest should extend FacetTestCase so that it doesn't use Lucene3x codec accidentally (which doesn't support DocValues and hence facets)\n\n\n\nIn general, what do you think if SamplingParams take a SampleFixer instead of fixToExact?\n\n\tWe could default to Amortized, while the current sampling tests will set Takmi\n\tIt will allow someone who doesn't care about the value at all to not fix it. I.e., if I just want to show 5% in the UI, I don't really need Amortized right?\n\tIt will allow to experiment with other SampleFixers implementations, e.g. maybe Takmi can be made more efficient or something.\n\n\n\nCurrently SampleFixer is public though there's really no point to override it since you cannot pass it anywhere? Therefore I think that taking a fixer is better. ",
            "author": "Shai Erera",
            "id": "comment-13666007"
        },
        {
            "date": "2013-05-24T05:37:15+0000",
            "content": "Shai, thanks for the comments.\nFirst three points are taken care of in the new patch.\nAs for SamplingParams taking a SampleFixer, it's a good idea, and I've been there, but it makes it harder on the e.g. SamplingAccumulator to figure out whether to oversample - and trim - for this SampleFixer. It would than move this logic to the SampleFixer. \nThat's not bad, but it changes the API a bit more, also the name SampleFixer does not match the functionality any more (perhaps it should oversample and trim itself?) ",
            "author": "Gilad Barkai",
            "id": "comment-13666009"
        },
        {
            "date": "2013-05-24T08:03:55+0000",
            "content": "Well, as long as we keep SampleFixer hidden, users will not be able to solve sampling issues on their own. So the API has to be open on that end too. Maybe SamplingAccumulator can have a protected shouldOverSample with a default impl that handles the two known fixers and otherwise returns false? Then the user who provides his own fixer, can provide his accumulator too. ",
            "author": "Shai Erera",
            "id": "comment-13666091"
        },
        {
            "date": "2013-05-24T10:50:33+0000",
            "content": "Looking at SamplingParams, isn't overSampleFactor enough to decide whether to over sample or not? It can default to not oversample, with the default Amortized fixer or some other default (2.0?) if fixer is Takmi. Then user can change it, and if he passes a fixer which requires over sampling, he should set that too. ",
            "author": "Shai Erera",
            "id": "comment-13666190"
        },
        {
            "date": "2013-05-26T07:40:55+0000",
            "content": "True, looking at overSampleFactor is enough, but it's not obvious that TakmiFixer should be used with overSampleFactor > 1, to better the chances of the result top-k being accurate.\nI'll add some documentation w.r.t this issue, I hope it will do.\n\nNew patch defaults to NoopSampleFixer which does not touch the results at all - if the need is only for a top-k and their counts does not matter, this is the least expensive one. \nAlso if instead of counts, a percentage sould be displayed (as how much of the results match this category), the sampled valued out of the sample size would yield the same result as the amortized fixed results out of the actual result set size. That might render the amortized fixer moot..\n\nNew patch account of SampleFixer being set in SamplingParams ",
            "author": "Gilad Barkai",
            "id": "comment-13667221"
        },
        {
            "date": "2013-05-26T07:52:04+0000",
            "content": "Thanks Gilad. Now that we have SampleFixer on SamplingParams, I wonder why we need Noop and Amortized? Could we just make the default fixer null and not oversample + fix if it's null? And Amortized ... well as you said, it looks kind of redundant now... I don't think it's rocket science for an app to do value/ratio on its own, yet it's one more class that we need to maintain going forward? ",
            "author": "Shai Erera",
            "id": "comment-13667223"
        },
        {
            "date": "2013-05-26T09:15:32+0000",
            "content": "Shai, I think you're right, a null SampleFixer makes more sense. \n\nWhile working on a test which validates that a flow works with the null fixer, I found it it did not. The reason is Complements. By default the complements kicks in when enough results are found. I think this may hold the key to the performance differences as well.\n\nRod, could you please try the following code and report the results?\n\n\n    SamplingAccumulator accumulator = new SamplingAccumulator( new RandomSampler(),  facetSearchParams, searcher.getIndexReader, taxo);\n\n    // Make sure no complements are in action\n    accumulator.setComplementThreshold(StandardFacetsAccumulator.DISABLE_COMPLEMENT);\n    \n    facetsCollector = FacetsCollector.create(accumulator);\n\n\n\n\nFor the mean time, made the changes to the patch, and added the test for null fixer. ",
            "author": "Gilad Barkai",
            "id": "comment-13667243"
        },
        {
            "date": "2013-05-28T07:08:21+0000",
            "content": "Hi all, thanks for all the progress. \n\nI will try to build a Lucene with the latests patch and give it a go.. \n\n(do I take the 4.3 release version? or is there a 4.3 development branch where the patch has to be applied?) ",
            "author": "Rob Audenaerde",
            "id": "comment-13668134"
        },
        {
            "date": "2013-05-28T07:22:26+0000",
            "content": "I took the revisionnumber that was in the patchfile and checked that out.\n\n svn checkout http://svn.apache.org/repos/asf/lucene/dev/trunk@1486401 .\n\nAfter installing Ivy I'm now building Lucene myself for the first time ",
            "author": "Rob Audenaerde",
            "id": "comment-13668143"
        },
        {
            "date": "2013-05-28T07:28:38+0000",
            "content": "Rob, you don't need to build Lucene to try what Gilad suggested, just modify your search code to disable complements. The problem is that if complements indeed kick in, and from the setup your describe it seems that they do because you search with MADQ, then sampling isn't done at all, yet the accumulator still corrects the counts.\n\nAfter you try it, we can tell if the performance overhead is indeed because of complements or that the counts are corrected. In either case, I think it will be good to open up the SampleFixer. ",
            "author": "Shai Erera",
            "id": "comment-13668151"
        },
        {
            "date": "2013-05-28T12:11:14+0000",
            "content": "Hi Shai,\n\nI will check tomorrow. Just to be sure, this is what I will do:\n\n\n\tLucene 4.3 release\n\tFacetsAccumulator with and without complements\n\t\n\t\tWith the 'default' TakmiSampleFixer\n\t\tWith a NOOP empty Sampler implementation that I will return by overriding the 'getSampleFixer' method in the Sampler that I will provide.\n\t\n\t\n\tMADQ with 1..5 selected facets\n\tsome other query that will return about 50% of the documents, also with 1..5 facets\n\n\n\nI currently have a nice 15M document set, I will use that as a basis.  ",
            "author": "Rob Audenaerde",
            "id": "comment-13668267"
        },
        {
            "date": "2013-05-28T12:16:19+0000",
            "content": "yes that sounds good. If you don't want to apply the patch so you can use the Noop fixer, that's fine too. I think the main goal is to see whether the complements that kicked in were in the way. ",
            "author": "Shai Erera",
            "id": "comment-13668270"
        },
        {
            "date": "2013-05-28T12:47:42+0000",
            "content": "Patch adds CHANGES entry as well as makes SampleFixer and TakmiSampleFixer public. I think this is ready but let's wait for Rob's input. ",
            "author": "Shai Erera",
            "id": "comment-13668281"
        },
        {
            "date": "2013-05-29T08:01:27+0000",
            "content": "Time in ms.\n\n\n \n        MADQ\t\t\t\t75% hits\t\t\t\n        Complements\tDISABLE Com.\tComplements\tDISABLE complements\t\n#facets Takmi \tNoop\tTakmi\tNoop\tTakmi\tNoop\tTakmi\tNoop\n1         999\t433\t1024\t393\t1239\t541\t969\t432\n2        2292\t388\t2877\t512\t2379\t609\t2489\t457\n3        2501\t219\t3228\t413\t2477\t569\t2590\t434\n4        3589\t224\t5052\t392\t3372\t562\t4093\t503\n5        4764\t247\t6863\t493\t4356\t577\t5103\t533\n\n \n\n\n \t\t\t\t\t\nSamplingParams sampleParams = new SamplingParams();\t\t\t\t\t\t\t\t\t\nsampleParams.setMaxSampleSize( 5000 );\t\t\t\t\t\t\t\t\t\nsampleParams.setMinSampleSize( 5000 );\t\t\t\t\t\t\t\t\t\nsampleParams.setSamplingThreshold( 75000 );\t\t\t\t\t\t\t\t\t\nsampleParams.setOversampleFactor( 1.0d );\t\t\t\t\t\t\t\t\t\n\n  ",
            "author": "Rob Audenaerde",
            "id": "comment-13669082"
        },
        {
            "date": "2013-05-29T08:14:29+0000",
            "content": "Thanks Rob. This shows that complements don't affect the performance much, and Takmi is the main issue. This is good!\nAnd also, Noop is stable with the number of growing facet requests, which is expected because it doesn't do any more work, while Takmi gets worse as more requests are added.\nActually, you use overSampleFactor=1, which is a bit optimistic for Takmi. Usually we use 2. That would show an even worse running time.\n\nW.r.t running the test, do you loop through the number of requests, or start a new JVM for each testcase? Do you do \"warmup\" runs to exclude their results before the actual measure? This won't change the fact that Takmi is slower than Noop, just perhaps explain why Noop w/ 5 requests is faster than 1 (which makes no sense, I take it it's an OS cache, or no warmup run).\n\nAnyway, I think this proves we need to make the default fixer null (which is equivalent to noop). I'll go ahead and commit the changes. ",
            "author": "Shai Erera",
            "id": "comment-13669085"
        },
        {
            "date": "2013-05-29T08:29:19+0000",
            "content": "Committed to trunk and 4x. Thanks Rob for reporting and taking the time to test this and Gilad for the fix! ",
            "author": "Shai Erera",
            "id": "comment-13669090"
        },
        {
            "date": "2013-05-29T11:36:22+0000",
            "content": "Thank you too \n\nSome more test-details:\n\nEach 'column' of 5 facets is done in a new JVM. Each individual cell is 4 searches, the first is disregarded, the three left are averaged. \n\nFor the SampingParams, I reduced the numbers from the defaults to speed up testing. ",
            "author": "Rob Audenaerde",
            "id": "comment-13669164"
        },
        {
            "date": "2013-05-29T13:20:42+0000",
            "content": "Thanks. I usually take the minimum, not average, since technically it's the fastest we could get. Also, discarding only one run is not always enough, since it may take the OS cache more time to warm up. But anyway, the numbers are clear. Thanks for doing this Rob! ",
            "author": "Shai Erera",
            "id": "comment-13669226"
        },
        {
            "date": "2013-07-23T18:37:10+0000",
            "content": "Bulk close resolved 4.4 issues ",
            "author": "Steve Rowe",
            "id": "comment-13716769"
        }
    ]
}