{
    "id": "SOLR-798",
    "title": "FileListEntityProcessor can't handle directories containing lots of files",
    "details": {
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [],
        "components": [
            "contrib - DataImportHandler"
        ],
        "type": "Bug",
        "priority": "Minor",
        "labels": "",
        "resolution": "Won't Fix"
    },
    "description": "The FileListEntityProcessor currently tries to process all documents in a single directory at once, and stores the results into a hashmap.  On directories containing a large number of documents, this quickly causes OutOfMemory errors.\n\nUnfortunately, the typical fix to this is to hack FileFilter to do the work for you and always return false from the accept method.  It may be possible to hook up some type of Producer/Consumer multithreaded FileFilter approach whereby the FileFilter blocks until the nextRow() mechanism requests another row, thereby avoiding the need to cache everything in the map.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Fergus McMenemie",
            "id": "comment-12669932",
            "date": "2009-02-03T11:52:41+0000",
            "content": "Despite the above. In my experience, using solutions provided by messrs Verity and Autonomy, letting the search engine walk a directory tree with millions of documents always lets you down. It could take days to recover from some situations. You have to manage the collection of files yourself and while doing so build bulk insert/delete files (bif files) which are passed to the search engine to control indexing. So it perhaps a blessing in disguise to see that Solr wont even let me walk large directory trees.\n\nI have a vague intention to write a DIH enhancement to implement reading BIF files containing a list of add/delete instructions. I only my java was better!\n\nHowever for the record, how large a directory tree were you able to walk? I am currently walking about 40,000 documents. But this is only while messing about trying to get a feel for Solr, this strategy could not be used in production.\n "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13604363",
            "date": "2013-03-16T18:55:57+0000",
            "content": "SPRING_CLEANING_2013 we can reopen if necessary.  "
        }
    ]
}