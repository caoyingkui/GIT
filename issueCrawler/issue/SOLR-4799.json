{
    "id": "SOLR-4799",
    "title": "DIH: join=\"zipper\" aka merge join for nested EntityProcessors",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "5.0",
            "6.0"
        ],
        "components": [
            "contrib - DataImportHandler"
        ],
        "type": "New Feature",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "DIH is mostly considered as a playground tool, and real usages end up with SolrJ. I want to contribute few improvements target DIH performance.\n\nThis one provides performant approach for joining SQL Entities with miserable memory at contrast to http://wiki.apache.org/solr/DataImportHandler#CachedSqlEntityProcessor  \n\nThe idea is:\n\n\n\tparent table is explicitly ordered by it\u2019s PK in SQL\n\tchildren table is explicitly ordered by parent_id FK in SQL\n\tchildren entity processor joins ordered resultsets by \u2018zipper\u2019 algorithm.\n\n\n\nexample usage for what's committed:\n\n<dataConfig>\n\t<document>\n\t\t<entity name=\"parent\" processor=\"SqlEntityProcessor\" query=\"SELECT * FROM PARENT ORDER BY id\">\t\t\n\t\t\t<entity name=\"child_1\" processor=\"SqlEntityProcessor\"\n\t\t\t\twhere=\"parent_id=parent.id\" query=\"SELECT * FROM CHILD_1 ORDER BY parent_id\" join=\"zipper\" >\n\t\t\t</entity>\t\t\t\n\t\t</entity>\n\t</document>\n</dataConfig>\n\n \nmind about:\n\n\tordering both sides\n\tspecifying join=\"zipper\" at children entity\n\tnote that it works with any entity processors",
    "attachments": {
        "SOLR-4799.patch": "https://issues.apache.org/jira/secure/attachment/12591823/SOLR-4799.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "James Dyer",
            "id": "comment-13651923",
            "date": "2013-05-08T14:17:20+0000",
            "content": "Mikhail,\n\nLet me clarify that DIH is not mostly considered a \"playground tool\".  It performs very well and has a rich feature-set.  We use this in production to import millions of documents each day, with each document consisting of fields from 50+ data sources.  For simpler imports, it is a quick and easy way to get your data into Solr and run imports.  Many many installations use this in production and it works well in many cases.\n\nThat said, the codebase has suffered from years of neglect.  Over time people have been more willing to add features rather than refactor.  A lot of the code needs to be simpified, re-worked, less-important features removed, etc.  The tests need further improvement as well.\n\nYour idea has great merit.  I think this would be an awesome feature to have in DIH.  I've wished for it before.  But I personally tend to shy away from committing big features to DIH because the code is not stable enough in my opinion.  I even have features in JIRA that I've developed and use in Production but feel uneasy about committing until more refactoring and test improvement work is done. "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13652066",
            "date": "2013-05-08T17:13:26+0000",
            "content": "James, I think it is time to do some of the tasks that you have outlined and I'm willing to do the work.\n\nMikhail, I'm happy to review and commit such a patch. I think it is a very nice improvement. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13652088",
            "date": "2013-05-08T17:26:22+0000",
            "content": "Ok. James, I've got your point. Let me collect code and tests for submitting.  "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13656291",
            "date": "2013-05-13T19:54:16+0000",
            "content": "I want do review the functionality, here is the proposed config\n\n\n<dataConfig>\n\t<document>\n\t\t<entity name=\"parent\" processor=\"SqlEntityProcessor\" query=\"SELECT * FROM PARENT ORDER BY id\">\t\t\n\t\t\t<entity name=\"child_1\" processor=\"OrderedChildrenEntityProcessor\"\n\t\t\t\twhere=\"parent_id=parent.id\" query=\"SELECT * FROM CHILD_1 ORDER BY parent_id\" >\n\t\t\t</entity>\t\t\t\n\t\t</entity>\n\t</document>\n</dataConfig>\n\n\n\nDo you like it?\n\nParent and child SQLs can have different order that kills zipper. OrderedChildrenEntityProcessor can enforce ASC order for the PK and FK keys (and throw exception in case of violation), but it also might detect order itself that complicates the code a little. What do you expect for the first code contribution? "
        },
        {
            "author": "James Dyer",
            "id": "comment-13656323",
            "date": "2013-05-13T20:17:47+0000",
            "content": "It would be even more awesome if it didn't assume the entities were extending SqlEntityProcessor.  I mean, make zipperjoin an option for any entity processor as opposed to its own new variant on SqlE.P. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13705099",
            "date": "2013-07-10T21:34:50+0000",
            "content": "Attaching the first drop. \n\nI don't say I share your idea James Dyer about adding zipper ability across all processor, anyway let's check how it would be.\n\nImplementation itself is not a big deal 'cause it's based on guava, it's enabled by join='zipper' . Note: it doesn't support case of People *-> Country, but only classic People -*> Sports. though oneliner  covers that. \n\nI extracted DIHSupport constructor, which parses attrs into Relation class. I introduced Zipper as EP internal strategy like DIHCacheSupport. It seems all these stuff should be extracted as few proper strategies at future.\n\nderby test covers only sports, not countries. They can be also covered, but not both. Joining both sides by zipper will make test super puzzling. So, it needs to be addressed later. \n\nThe most thing which I worry about is the test data. From what I see, we have only vanilla data: for every people we have few or single sports. Zipper caveats are orphaned sports and sportless peoples. if there is a bug in zipper it can mess following entities. btw, giving my experience obtained in DIH vs Threads battle, I can say it menaces to caching implementations also. Ideally, I'd like to pause this one, improve derby test for orphaned children and childless parents and continue with zipper afterwards. \n\nPlease let me know what you think!   "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13728987",
            "date": "2013-08-04T20:44:11+0000",
            "content": "James Dyer any chance you have a look? "
        },
        {
            "author": "James Dyer",
            "id": "comment-13738424",
            "date": "2013-08-13T16:19:36+0000",
            "content": "Mikhail, This seems like a great feature, but I haven't looked at it.  As I said, I do not feel it wise to add features that won't neatly plug-in the current DIH infrastructure until we improve the code.  Really, I would love to chop out features (Debug mode, delta updates, streaming from a POST request, etc), and make it work independently from Solr before we build more into it.  But I've been busy with other things and haven't had much time.  \n\nBy the way, have you any experience with Apache Flume?  In your opinion, could it become DIH's successor?  A Solr Sink was added earlier in the year that will index disparate data.  I haven't looked much at it, but my first impression is that it is a big, complicated tool whereas DIH is smaller and simpler and a the 2 would have different use-cases.  Also, not so sure it has any support yet for RDBMS. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13742992",
            "date": "2013-08-17T18:51:43+0000",
            "content": "James,\nI don't really understand. I wanted to add a tiny plugin into DIH, but \nI mean, make zipperjoin an option for any entity processor as opposed to its own new variant on SqlE.P.\nand after I went this way after heavy doubts\nAs I said, I do not feel it wise to add features that won't neatly plug-in the current DIH infrastructure until we improve the code. \nAnyway, I absolutely share your concerns - DIH is a great idea, but it's worth to revamp an engine. I have no experience with Flume, but I consider it as some kind of transport. I want to look at Pentaho Kettle (kind of old school ETL tool), Cloudera Morphlines. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13743363",
            "date": "2013-08-18T20:32:52+0000",
            "content": "look http://pedroalves-bi.blogspot.ru/2011/07/elasticsearch-kettle-and-ctools.html\nthere is elasticsearch sink for Kettle.  "
        },
        {
            "author": "Alexandre Rafalovitch",
            "id": "comment-14019763",
            "date": "2014-06-06T11:20:04+0000",
            "content": "Morphline is now part of Apache Solr distribution. That probably points the direction in which this will go.\n\nAt the same time, in nearly a year, no further improvements on DIH were done as far as I know. So, perhaps this addition should be committed even if it is not ideal. "
        },
        {
            "author": "James Dyer",
            "id": "comment-14019885",
            "date": "2014-06-06T14:42:00+0000",
            "content": "At the same time, in nearly a year, no further improvements on DIH were done as far as I know. So, perhaps this addition should be committed even if it is not ideal.\n\nI would say the exact opposite.  There are not very many people maintaining DIH code, and those of us that do are lazy about it.  Therefore, let's not stuff more big features in and make more code to maintain when there are no maintainers.  I have code here in JIRA that I've used in production for years that I've been unwilling to commit just for this very reason.\n\nI do see Flume as a great DIH replacement, but from the documentation I don't see it having very great RDBMS support?  I think a lot of DIH users are using it to import data from an RDBMS into Solr. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14020207",
            "date": "2014-06-06T18:45:54+0000",
            "content": "Despite of many things described above, I agree with James Dyer. Until we see real demand for this feature, there is no need to pitch it. This \"plugin\" is really easy to check as a separate drop-in, but you see how many users tried to check it ... no one.\n\nI understand what the morphlines is, after all. It's a pretty cool lightweight transformation pipeline. but:\n\n\tit doesn't have jdbc input so far (i don't think it's hard to implement it)\n\ta pipeline implies a single chain of transformation, I don't see how to naturally join two streams of records.\n\n\n\nRegarding Flume I'm still concerned about its' minimum footprint. \n\nfwiw here is the Kettle's approach to do the subj http://wiki.pentaho.com/display/EAI/Merge+Join \n "
        },
        {
            "author": "Shawn Heisey",
            "id": "comment-14020260",
            "date": "2014-06-06T19:15:30+0000",
            "content": "I think a lot of DIH users are using it to import data from an RDBMS into Solr.\n\nThis is exactly what I use it for.  Based on mailing list and IRC traffic, I think that most people who use DIH are using it for database import.  DIH works, and it's a lot more efficient than any single-threaded program that I could write.  I don't believe that it is a \"playground tool.\"\n\nAlthough DIH used to handle all our indexing, we currently only use it for full index rebuilds.  A SolrJ app handles the once-a-minute maintenance.  I have plans to build an internal multi-threaded SolrJ tool to handle full rebuilds, but that effort still has not made it through the design phase.  Because DIH works so well, we don't have a strong need to replace it. "
        },
        {
            "author": "Alexandre Rafalovitch",
            "id": "comment-14021105",
            "date": "2014-06-08T05:08:42+0000",
            "content": "Re: \"nobody tried this plugin\" - finding anything on JIRA is next to impossible. I would not really take that as indication of need or lack of such. \n\nRe: pipeline, morphline has nested pipeline including things like unzip a file, find every entry inside, run tika over it and then pass it through XML extraction. Seems a fairly close match to DIH?\n\nRegarding JDBC support, etc. How about plugging Morphline as a DIH entity processor that works with any DataSource (it's only 3 different usage patterns)? I would be happy to have a go at it, but only if it has a chance of actually incorporating it into DIH. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14028206",
            "date": "2014-06-11T18:49:26+0000",
            "content": "There are a plenty of sibling point discussed here, let me keep one more. I checked one thing with Kettle ETL (Pentaho). the main problem with Kettle is Eclipse based IDE UI. Giving the DIH replatforming, we expect some Web UI for DSL editing. I found sibling project CDA, which is looking pretty much like this. Here is the summary: \n\n\tthe project itself seems modular enough (CBF), hence we can slice some pieces for using in DIH2.0\n\tCDA is just a data access - whatever to JSON via HTTP GET\n\tthus, it lacks of final indexing steps (via POST or xxxSolrServer);\n\talso, it lacks of long lasting command framework (it's a trivial thread with interruption and status flags; not a much deal, but nothing for free there)\n\tit shows pretty cute usage of ETL primitives (and I still think that Kettle guts are much powerful than Morflines'): it uses xml DSL to configure Kettle steps and run data export as ETL process.\n\n "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14048155",
            "date": "2014-06-30T20:57:36+0000",
            "content": "just a small off-topic update. \nI found old Kettle plugin for Solr export https://code.google.com/p/kettle-solr-plugin/\nrefreshed it for Kettle 5.0 and put first results to https://github.com/m-khl/kettle-solr-plugin\n\nIt's just a proof, it has some bugs, and lacks of the desired functionality eg streaming/cloud. I'm just dropping off for a while, and wish you get my status if somebody interested it \"true\" ETL.  "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14222042",
            "date": "2014-11-22T16:49:49+0000",
            "content": "Note regarding the patch. It introduces join=\"zipper\" attribute for any entity processor, which is used as child, for sure both entities should be ordered by the same ID: \n\n<dataConfig>\n\t<document>\n\t\t<entity name=\"parent\" processor=\"SqlEntityProcessor\" query=\"SELECT * FROM PARENT ORDER BY id\">\t\t\n\t\t\t<entity name=\"child_1\" processor=\"SqlEntityProcessor\"\n\t\t\t\twhere=\"parent_id=parent.id\" query=\"SELECT * FROM CHILD_1 ORDER BY parent_id\" join=\"zipper\" >\n\t\t\t</entity>\t\t\t\n\t\t</entity>\n\t</document>\n</dataConfig>\n\n \nLet me know if you wish to see it like separate class OrderedChildrenEntityProcessor see above. cc Noble Paul "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14223190",
            "date": "2014-11-24T17:44:30+0000",
            "content": "does this have any external dependencies ? "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14223412",
            "date": "2014-11-24T19:51:27+0000",
            "content": "nope. for sure. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14226174",
            "date": "2014-11-26T13:30:45+0000",
            "content": "hi , can yoou post a patch updated to the trunk  please. If I'm not wrong the code kicks in when the entity attribute \"join\" is present . So it is a low risk feature anyway "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14228081",
            "date": "2014-11-28T05:37:39+0000",
            "content": "hi , can yoou post a patch updated to the trunk please.\nI'm trying. I'm stuck on commons-codec;1.10 dependency so far. It takes a few days.\nthe code kicks in when the entity attribute \"join\" is present . So it is a low risk feature anyway\nabsolutely. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14228542",
            "date": "2014-11-28T22:46:06+0000",
            "content": "updated the patch. checked the tests. \nI had to add super.firstInit() into TikaEntityProcessor.firstInit(). it becomes mandatory. \n\nWDYT? "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14228945",
            "date": "2014-11-29T21:19:17+0000",
            "content": "improved test coverage in TestSqlEntityProcessor* :\n\n\tadded countries into zipper join\n\tadded more entropy into data (orphans, and childfree)\n\timproved few asserts, made them randomized and mandatory\n\n\n\nJames Dyer improved tests are worth to consider, beside of this jira  "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14229230",
            "date": "2014-11-30T20:13:41+0000",
            "content": "improved patch again. now Zipper strictly checks that both sides (primary and foreign keys) are in ascending order. Negative tests are included. \n\nNoble Paul it's %100 ready.   "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14229635",
            "date": "2014-12-01T10:17:46+0000",
            "content": "I can't really understand why an optional component object should be explicitly be initialized all the time. That object should not even be created at all\n\nEntityProcessorBase.java\n  protected void firstInit(Context context) {\n    entityName = context.getEntityAttribute(\"name\");\n    String s = context.getEntityAttribute(ON_ERROR);\n    if (s != null) onError = s;\n    \n    zipper = new Zipper(context);\n    \n    if(!zipper.isActive()){\n      initCache(context);\n    }\n    isFirstInit = false;\n  }\n\n\n\nI would say , please construct the rowIterator using Zipper instead of making it a part of a core class such as EntityProcessorBase "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14229817",
            "date": "2014-12-01T14:09:10+0000",
            "content": "yep. agree. Zipper is made optional and baked by factory method. \n\nI either didn't get your second point or I'd like to address this refactoring separately. It seems like DIHCacheSupport, Zipper, and straightforward case with N queries (one per every parent row) should be covered under separate abstraction of rowIterator, which will be reset per every parent row. nevertheless it's more than moderate long story. Isn't it?  "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14229854",
            "date": "2014-12-01T14:42:56+0000",
            "content": "second one is optional , but desirable. The first one should be a prerequisite "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14230118",
            "date": "2014-12-01T17:52:41+0000",
            "content": "Noble Paul did you catch the recent patch? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14231599",
            "date": "2014-12-02T15:19:09+0000",
            "content": "Isn't only relevant for SqlEntityProcessor ? can zipper be initialized at SqlEntityProcessor instead of EntityProcessorBase ? "
        },
        {
            "author": "James Dyer",
            "id": "comment-14231640",
            "date": "2014-12-02T15:35:13+0000",
            "content": "Let me mention that with SOLR-2943, you can read back cached data in key order, which would allow you to zipper-join anything that can be previously cached.  While this is not a committed feature, it demonstrates that you can have entities other than SQL with the keys in the correct order for joining.  So if possible, I wouldn't make this just for SQL. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14231681",
            "date": "2014-12-02T16:05:12+0000",
            "content": "If that is the case please change the title/description to match that "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14231942",
            "date": "2014-12-02T19:12:20+0000",
            "content": "Noble Paul do like this summary?  "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14232937",
            "date": "2014-12-03T12:09:32+0000",
            "content": "comitted r1643097\n\nStrangely, svn commits are not posting messages to JIRA "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14235720",
            "date": "2014-12-05T16:53:42+0000",
            "content": "Commit 1643351 from Adrien Grand in branch 'dev/trunk'\n[ https://svn.apache.org/r1643351 ]\n\nSOLR-4799: Fix javadocs generation. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-14235726",
            "date": "2014-12-05T16:57:24+0000",
            "content": "Commit 1643352 from Adrien Grand in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1643352 ]\n\nSOLR-4799: Fix javadocs generation. "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-14332875",
            "date": "2015-02-23T05:02:22+0000",
            "content": "Bulk close after 5.0 release. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14616567",
            "date": "2015-07-07T11:50:14+0000",
            "content": "linking a blog post http://blog.griddynamics.com/2015/07/how-to-import-structured-data-into-solr.html "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-14654239",
            "date": "2015-08-04T19:44:49+0000",
            "content": "Cassandra Targett fwiw, I updated ref guide https://cwiki.apache.org/confluence/pages/diffpagesbyversion.action?pageId=32604271&selectedPageVersions=44&selectedPageVersions=45 "
        }
    ]
}