{
    "id": "SOLR-2382",
    "title": "DIH Cache Improvements",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "3.6",
            "4.0-ALPHA"
        ],
        "components": [
            "contrib - DataImportHandler"
        ],
        "type": "New Feature",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Functionality:\n 1. Provide a pluggable caching framework for DIH so that users can choose a cache implementation that best suits their data and application.\n\n 2. Provide a means to temporarily cache a child Entity's data without needing to create a special cached implementation of the Entity Processor (such as CachedSqlEntityProcessor).\n\n 3. Provide a means to write the final (root entity) DIH output to a cache rather than to Solr.  Then provide a way for a subsequent DIH call to use the cache as an Entity input.  Also provide the ability to do delta updates on such persistent caches.\n\n 4. Provide the ability to partition data across multiple caches that can then be fed back into DIH and indexed either to varying Solr Shards, or to the same Core in parallel.\n\n\nUse Cases:\n 1. We needed a flexible & scalable way to temporarily cache child-entity data prior to joining to parent entities.\n\n\tUsing SqlEntityProcessor with Child Entities can cause an \"n+1 select\" problem.\n\tCachedSqlEntityProcessor only supports an in-memory HashMap as a Caching mechanism and does not scale.\n\tThere is no way to cache non-SQL inputs (ex: flat files, xml, etc).\n\n\n\n 2. We needed the ability to gather data from long-running entities by a process that runs separate from our main indexing process.\n\n 3. We wanted the ability to do a delta import of only the entities that changed.\n\n\tLucene/Solr requires entire documents to be re-indexed, even if only a few fields changed.\n\tOur data comes from 50+ complex sql queries and/or flat files.\n\tWe do not want to incur overhead re-gathering all of this data if only 1 entity's data changed.\n\tPersistent DIH caches solve this problem.\n\n\n\n 4. We want the ability to index several documents in parallel (using 1.4.1, which did not have the \"threads\" parameter).\n\n 5. In the future, we may need to use Shards, creating a need to easily partition our source data into Shards.\n\n\nImplementation Details:\n 1. De-couple EntityProcessorBase from caching.  \n\n\tCreated a new interface, DIHCache & two implementations:\n\tSortedMapBackedCache - An in-memory cache, used as default with CachedSqlEntityProcessor (now deprecated).\n\tBerkleyBackedCache - A disk-backed cache, dependent on bdb-je, tested with je-4.1.6.jar\n\tNOTE: the existing Lucene Contrib \"db\" project uses je-3.3.93.jar.  I believe this may be incompatible due to Generic Usage.\n\tNOTE: I did not modify the ant script to automatically get this jar, so to use or evaluate this patch, download bdb-je from http://www.oracle.com/technetwork/database/berkeleydb/downloads/index.html\n\n\n\n 2. Allow Entity Processors to take a \"cacheImpl\" parameter to cause the entity data to be cached (see EntityProcessorBase & DIHCacheProperties).\n\n 3. Partially De-couple SolrWriter from DocBuilder\n\n\tCreated a new interface DIHWriter, & two implementations:\n\tSolrWriter (refactored)\n\tDIHCacheWriter (allows DIH to write ultimately to a Cache).\n\n\n\n 4. Create a new Entity Processor, DIHCacheProcessor, which reads a persistent Cache as DIH Entity Input.\n\n 5. Support a \"partition\" parameter with both DIHCacheWriter and DIHCacheProcessor to allow for easy partitioning of source entity data.\n\n 6. Change the semantics of entity.destroy()\n\n\tPreviously, it was being called on each iteration of DocBuilder.buildDocument().\n\tNow it is does one-time cleanup tasks (like closing or deleting a disk-backed cache) once the entity processor is completed.\n\tThe only out-of-the-box entity processor that previously implemented destroy() was LineEntitiyProcessor, so this is not a very invasive change.\n\n\n\n\nGeneral Notes:\nWe are near completion in converting our search functionality from a legacy search engine to Solr.  However, I found that DIH did not support caching to the level of our prior product's data import utility.  In order to get our data into Solr, I created these caching enhancements.  Because I believe this has broad application, and because we would like this feature to be supported by the Community, I have front-ported this, enhanced, to Trunk.  I have also added unit tests and verified that all existing test cases pass.  I believe this patch maintains backwards-compatibility and would be a welcome addition to a future version of Solr.",
    "attachments": {
        "TestThreaded.java.patch": "https://issues.apache.org/jira/secure/attachment/12505259/TestThreaded.java.patch",
        "SOLR-2382-solrwriter.patch": "https://issues.apache.org/jira/secure/attachment/12486233/SOLR-2382-solrwriter.patch",
        "SOLR-2382-properties.patch": "https://issues.apache.org/jira/secure/attachment/12486219/SOLR-2382-properties.patch",
        "SOLR-2382-solrwriter-verbose-fix.patch": "https://issues.apache.org/jira/secure/attachment/12488221/SOLR-2382-solrwriter-verbose-fix.patch",
        "TestCachedSqlEntityProcessor.java-wrong-pk-detected-due-to-lack-of-where-support.patch": "https://issues.apache.org/jira/secure/attachment/12505514/TestCachedSqlEntityProcessor.java-wrong-pk-detected-due-to-lack-of-where-support.patch",
        "SOLR-2382_3x.patch": "https://issues.apache.org/jira/secure/attachment/12519286/SOLR-2382_3x.patch",
        "SOLR-2382-dihwriter_standalone.patch": "https://issues.apache.org/jira/secure/attachment/12505376/SOLR-2382-dihwriter_standalone.patch",
        "SOLR-2382-entities.patch": "https://issues.apache.org/jira/secure/attachment/12486352/SOLR-2382-entities.patch",
        "TestCachedSqlEntityProcessor.java-fix-where-clause-by-adding-cachePk-and-lookup.patch": "https://issues.apache.org/jira/secure/attachment/12505384/TestCachedSqlEntityProcessor.java-fix-where-clause-by-adding-cachePk-and-lookup.patch",
        "SOLR-2382.patch": "https://issues.apache.org/jira/secure/attachment/12471980/SOLR-2382.patch",
        "TestCachedSqlEntityProcessor.java-break-where-clause.patch": "https://issues.apache.org/jira/secure/attachment/12505383/TestCachedSqlEntityProcessor.java-break-where-clause.patch",
        "SOLR-2382-dihwriter.patch": "https://issues.apache.org/jira/secure/attachment/12486468/SOLR-2382-dihwriter.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Lance Norskog",
            "id": "comment-13000070",
            "date": "2011-02-28T00:29:26+0000",
            "content": "Bravo!\n\nIt's great to see someone else learning the DIH code.\n\n\n\n "
        },
        {
            "author": "James Dyer",
            "id": "comment-13000488",
            "date": "2011-02-28T19:48:36+0000",
            "content": "Updated patch with better delta update capabilities for DIHCacheWriter.  Also cleaned a couple things up.  All tests pass. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13007674",
            "date": "2011-03-16T20:09:54+0000",
            "content": "Updated patch with 2 fixes for things I missed when porting this from 1.4.1 to Trunk.  Also added 1 more unit test.\n\nI think this is ready for someone else to evaluate if anyone has the time & desire.  I do believe this would be a nice addition to the DIH product. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13011266",
            "date": "2011-03-25T16:30:59+0000",
            "content": "Fix for two bugs in BerkleyBackedCache:\n\n\n\tIf the passed-in fieldNames & fieldTypes have leading or trailing spaces, opening the cache would fail.\n\tIf the cache was set up for Delta updates, then closed & re-opened, adding documents would cause an NPE.\n\n "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13011700",
            "date": "2011-03-26T23:37:07+0000",
            "content": "Have you tested this under threading? "
        },
        {
            "author": "James Dyer",
            "id": "comment-13011730",
            "date": "2011-03-27T05:14:23+0000",
            "content": "There is a multi-threaded unit test in \"TestDihCacheWriterAndProcessor.java\".  However, I have not used the \"threads\" param in a real-world setting. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13017560",
            "date": "2011-04-08T17:28:22+0000",
            "content": "This version fixes a bug involving the DIHCacheProcessor in the case of a many-to-[one|many] join between the parent entity and a child entity.  If the child entity used a DIHCacheProcessor and the same child joined to consecutive parents, only the first parent would join to the child. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13017567",
            "date": "2011-04-08T17:45:12+0000",
            "content": "In light of the recent discussion about the Spatial Contrib, I do wonder if seeking to get this committed is a non-starter because of its dependency on bdb-je.  I thought this wouldn't be an issue because we have an existing Lucene contrib (db) with this same dependency, but then I noticed that some of the committers regret the existence of the db contrib for this reason (and others).\n\nIn any case, even if the \"BerkleyBackedCache\" part of this patch could not be committed, having this framework in place so that developers can write their own persistent cache impls would be a major improvement in my opinion.  (I had originally started with a Lucene-backed cache but switch to bdb-je because I couldn't figure out how to achieve acceptable performance for \"get\"s from the cache). "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13049064",
            "date": "2011-06-14T08:48:02+0000",
            "content": "James , Does it make sense to split this issue into two separate issues\n\n1) The changes required in DIH to support cache (DIHCache interface etc)\n2) Actual cache implementations "
        },
        {
            "author": "James Dyer",
            "id": "comment-13052004",
            "date": "2011-06-20T15:10:40+0000",
            "content": "Noble,\n\nI appreciate your interest in this issue!  I could easily move BerkleyBackedCache to its one issue.  This would remove any difficulty in dealing with the Sleepycat License.  We would still want to maintain the SortedMapBackedCache, however.  Otherwise we would lose all caching ability (it would break CachedSqlEntityProcessor).\n\nIn any case, if your goal is to break this issue into more managable chunks just offloading BerkleyBackedCache might not be enough.  I had considered breaking this up into possibly 3 parts because I realize this is a huge patch.  But the functionality is all designed to work together and it would have been more work for me, etc.\n\nLet me know what you want me to do.  I would love to see this integrated with a GA release someday.  I think this would have broad application and a lot of real-world use cases.  (& we depend on it here...) "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13052394",
            "date": "2011-06-21T07:21:49+0000",
            "content": "At least the BDB based cache will have to go to a different issue. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13052853",
            "date": "2011-06-21T21:30:14+0000",
            "content": "Here's a new patch:\n\n\n\tApplies cleanly to latest trunk.\n\tBetter de-coupling of SolrWriter and the PropertiesWriter\n\tBerkleyBackedCache removed (will open a separate issue for this).\n\tUnit test enhancements to make it easy to unit test new cache impl's as they get added.\n\n\n\nNote that because SortedMapBackedCache does not support persistence, most of the features are untestable (one test was removed...The others just skip for now).  Three possible solutions:\n\n\n\tcreate a persistence option for SortedMapBackedCache (maybe just use java Serialization).\n\tcreate a new cache impl that doesn't depend on a non-compatible licensed product (maybe Lucene-backed, although I tried this long ago and didn't get the peformance I needed).\n\tFigure out an acceptable way to include BerkleyBackedCache (we did it for the Lucene db module, so why not this?)\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13053090",
            "date": "2011-06-22T07:15:31+0000",
            "content": "The patch does not apply on trunk "
        },
        {
            "author": "James Dyer",
            "id": "comment-13053313",
            "date": "2011-06-22T15:35:06+0000",
            "content": "Noble,\n\nI just updated to the latest and re-applied this patch and it worked for me.  If you can give me specifics I'll try to dig more to see what might be going wrong.  Also, in case you're not on the very latest, there were some very recent commits from about a week ago that broke the previous versions of this patch (r1135954 & r1136789). This newest patch will only work on code from after those commits. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13053414",
            "date": "2011-06-22T19:31:50+0000",
            "content": "Just found a little bug in SortedMapBackedCache.  This patch version includes a fix for it. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13053417",
            "date": "2011-06-22T19:36:12+0000",
            "content": "Sorry...that last patch included some unrelated code.  This one is correct. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13053800",
            "date": "2011-06-23T11:33:21+0000",
            "content": "The patch applies well. \n\nSuggestions\n\nThe SolrWriter/DIHPropertiesWriter abstarction can be a separate patch and I can commit it right away . It may also have the changes for passing the handler name .\n\nThe DIHCache should take the Context as a param and the EntityProcessor does not need to make a copy of the attributes "
        },
        {
            "author": "James Dyer",
            "id": "comment-13054104",
            "date": "2011-06-23T21:17:29+0000",
            "content": "\nThe DIHCache should take the Context as a param and the EntityProcessor does not need to make a copy of the attributes\n\nI started down this road this afternoon hoping to have you another patch version to look at today.  But it turns out to be more complicated than I first anticipated.  Any ideas how to get around these difficulties?\n\n\n\tDIHCacheProcessor enforces \"readOnly=false\" and \"deletePriorData=true\".  It also modifies the cacheName if the user specifies partitions.  Seeing that Context-Entity-Attributes are immutable, should I pass these as Entity-Scope-Session-Attributes?\n\n\n\n\n\tcacheInit() in EntityProcessorBase specifically passes only the parameters that apply to the current situation.  This way, if a user applies something non-applicable they are safely ignored rather than getting undefined behavior.  Just forwarding the context on doesn't give this flexibility.  Do you think its ok to just forward on the context anyway?\n\n\n\n\n\tDocBuilder instantiates DIHCacheWriter which in turn gets the user-specified Cache Implementation and instantiates that.  At this point in time, there doesn't seem to be a Context to pass. So, rather than do this in the constructor, is there a safer place down the road where I should be instantiating the DIHCacheWriter?\n\n\n\nI realize that its more lines of code to always copy these properties into a property map to send to the cache, but I was looking at the cache at being a layer down in the stack and maybe it shouldn't have the whole context sent to it.  What do you think? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13054263",
            "date": "2011-06-24T06:40:39+0000",
            "content": "cacheInit() in EntityProcessorBase specifically passes only the parameters that apply to the current situation\n\nit doen't matter . It can use any params which are relevant to it. Anyway you can't define what params are required for a future DIHCache impl. Look at a Transformer implementation it can read anything it wants. The cache should be initialized like that only \n\nWhy should the DocBuilder be even aware of DIHCache , Should it not be kept local to the EntityProcessor? "
        },
        {
            "author": "James Dyer",
            "id": "comment-13054569",
            "date": "2011-06-24T17:30:46+0000",
            "content": "Here is a version that passes parameters via the Context object, rather than by building maps. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13054575",
            "date": "2011-06-24T17:43:24+0000",
            "content": "\nWhy should the DocBuilder be even aware of DIHCache , Should it not be kept local to the EntityProcessor?\n\nYou're right that when the cache is owned by an EntityProcessor, DocBuilder has no knowledge of it.  But there is another way these caches can be used, described in the \"functionality\" section of this issue's description:\n\n\n3. Provide a means to write the final (root entity) DIH output to a cache rather than to Solr ... Also provide the ability to do delta updates on such persistent caches.\n\nIn this case, DocBuilder is outputting not to SolrWriter, but to DIHCacheWriter.  It is arguable the DIHCacheWriter should not be instantiated in DocBuilder in this instance, as I currently have it.  Perhaps its should happen up the stack in DataImporter, etc.  But in any case, whenvever DIHCacheWriter gets instantiated, it needs to know which CacheImpl to create and also pass on any parameters that CacheImpl needs. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13063372",
            "date": "2011-07-11T14:47:26+0000",
            "content": "Noble,\n\nAre you still able to work with me on this issue?  Is there anything else you are waiting for from me?  The patch I submitted on June 24 passes parameters via the Context object as you requested.  Also, I previously separated \"BerkleyBackedCache\" out into a separate issue to (SOLR-2613) so we won't run into licensing issues here.  Let me know what else you think we need to do.  Thanks. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13063727",
            "date": "2011-07-12T05:32:21+0000",
            "content": "my apologies for the delay.\n\nThe problem w/ the patch is the size/scope. You may not need to open up other issues but stuff like abstracting DIHWriter,DIHpropertiesWriter etc can be given as a separate patch in the same issue and I can commit them straight away. Though the issue is aboout cache improvements , it goes far beyond that scope. committing it in as a whole is difficult. \n\n\n "
        },
        {
            "author": "James Dyer",
            "id": "comment-13064114",
            "date": "2011-07-12T20:16:28+0000",
            "content": "This is the properties file writer abstraction taken from the June 24 version of the entire patch.\n\nThis removes property file operations from SolrWriter, paving the way for multiple \"DIHWriter\" implementations (such as \"DIHCacheWriter\"). "
        },
        {
            "author": "James Dyer",
            "id": "comment-13064146",
            "date": "2011-07-12T21:27:21+0000",
            "content": "The previous patch left out a couple of things.  here's a fixed version... "
        },
        {
            "author": "James Dyer",
            "id": "comment-13064165",
            "date": "2011-07-12T21:47:04+0000",
            "content": "This is the \"DIHWriter\" abstraction taken from the June 24 version of the entire patch.  This patch depends on \"SOLR-2382-properties.patch\".\n\nSolrWriter now implements the new DIHWriter interface.  Also, logging operations are removed from SolrWriter.  This opens the possibility of creating plugable DIHWriters so that DIH can write to something other than a Solr index.  This in turn opens up the ability to create a Writer that can write to a Persistent Cache for later use. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13064410",
            "date": "2011-07-13T07:46:19+0000",
            "content": "The patch does not apply. the codebase has changed in the trunk. But it still does not apply.  "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13064503",
            "date": "2011-07-13T11:58:27+0000",
            "content": "The patch does not apply. the codebase has changed in the trunk. But it still does not apply. \n\ntrunk/dev-tools/scripts/SOLR-2452.patch.hack.pl should be able to adjust patches like this one. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13064504",
            "date": "2011-07-13T12:01:16+0000",
            "content": "I've uploaded the patch I modified w/ the abovementioned script "
        },
        {
            "author": "James Dyer",
            "id": "comment-13064577",
            "date": "2011-07-13T14:18:37+0000",
            "content": "Steve,\n\nThanks for the tip, but I'm a bit confused because yesterday I imported a fresh check-out of trunk into my company's svn server so that I could start the process of creating all of these separate patches.  I should have had the solr-2452 changes already, shouldn't have I?\n\nFor instance, the 1st line of the \"solrwriter\" patch has:\n\nIndex: solr/contrib/dataimporthandler-extras/src/test/org/apache/solr/handler/dataimport/TestMailEntityProcessor.java\n\npre-2452, this path would have started with \"solr/contrib/dataimporthandler/src/extras/test/java\", right?\n\nIn any case if anyone still has difficulty in applying these, let me know specifically what you the problem is and I'll try to make it right.  Thanks. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13064606",
            "date": "2011-07-13T14:39:55+0000",
            "content": "James,\n\nAs you say, patches against recent checkouts don't need to be adjusted (specifically, checkouts from after SOLR-2452 was committed to trunk in r1144761 on 9 July at 23:01 UTC).\n\n\nFor instance, the 1st line of the \"solrwriter\" patch has:\n\nIndex: solr/contrib/dataimporthandler-extras/src/test/org/apache/solr/handler/dataimport/TestMailEntityProcessor.java\n\npre-2452, this path would have started with \"solr/contrib/dataimporthandler/src/extras/test/java\", right?\n\nRight. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13064785",
            "date": "2011-07-13T19:18:32+0000",
            "content": "This is the Entity Processor Caching portion taken from the June 24 version of the entire patch.  This patch depends both on the \"properties\" patch and the \"solrwriter\" patch.\n\nThis gives users the ability to specify a \"cacheImpl\" parameter to any Entity Processor, adding caching functionality.  CachedSqlEntityProcessor is gutted and deprecated (with no loss of functionality).  Instead, users can now use SqlEntityProcessor and specify a \"cacheImpl\".  Also, the caching functionality has been removed from EntityProcessorBase and placed into SortedMapBackedCache, which is the only Cache implementation available here (as BerkleyBackedCache is removed to SOLR-2613).\n\nTwo new unit tests are included in this patch, one for SortedMapBackedCache and another demonstrating the use of an ephemeral cache to do joins. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13065381",
            "date": "2011-07-14T16:49:35+0000",
            "content": "This is the \"DIHCacheWriter\" and \"DIHCacheProcessor\" portion taken from the June 24 version of the entire patch. This patch depends both on the \"properties\" patch and the \"solrwriter\" patch.\n\nDIHWriter is a drop-in replacement for SolrWriter, allowing a DIH run to write its output to a DIHCache rather than to Solr.  Users can specify which CacheImpl to use.  If the Cache supports persistence, delta updates can be performed on cached data.\n\nDIHCacheProcessor is an EntityProcessor that takes a DIHCache as its input.\n\nUnit tests are included.  However, most tests skip because SortedMapBackedCache does not support persistence.  To get the tests to run, (for now), we need to also apply SOLR-2613 (BerkleyBackedCache). "
        },
        {
            "author": "James Dyer",
            "id": "comment-13065386",
            "date": "2011-07-14T16:58:58+0000",
            "content": "Noble,\n\nI now have this original issue split up into 5 separate pieces.  The first 4 are included on this jira issue.  BerkleyBackedCache is spun off to SOLR-2613.  These patches all apply to trunk post-solr-2452, as Steve Rowe noted.  These patches are based on the June 24 version of the big patch, including the change to use \"Context\" to pass parameters.\n\nHere is a summary.  The patches need to be applied in this order.\n\n1. SOLR-2382-properties.patch - Remove property file operations from SolrWriter.\n\n2. SOLR-2382-solrwriter.patch - Allow multiple DIHWriters that inherit a common interface.  Remove logging operations from SolrWriter.\n\n3. SOLR-2382-entities.patch - Allow any EntityProcessor to specify a \"cacheImpl\" and cache its data.  Deprecate CachedSQLEntityProcessor.  Introduce SortedMapBackedCache.\n\n4. SOLR-2382-dihwriter.patch - Introduce the DIHCacheWriter and DIHCacheProcessor.\n\n5. SOLR-2613 - Introduce BerkleyBackedCache, a fast cache Impl that supports persistence.\n\nLet me know whatever else you need me to do on this.  I appreciate your time in working with me. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13065919",
            "date": "2011-07-15T13:33:05+0000",
            "content": "Good James. The patches look fine.I shall review it and commit or suggest mofifications "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13068230",
            "date": "2011-07-20T08:50:26+0000",
            "content": "@James .\nCommitted the first one. take a look and let me know if you want something to be changed "
        },
        {
            "author": "James Dyer",
            "id": "comment-13068496",
            "date": "2011-07-20T17:16:53+0000",
            "content": "Looks good.  I agree that DIHPropertiesWriter.isWritable() is preferable over exposing the File entirely. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13068498",
            "date": "2011-07-20T17:19:32+0000",
            "content": "DIHPropertiesWriter doesn't need to import java.io.File anymore... "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13068921",
            "date": "2011-07-21T11:38:16+0000",
            "content": "@James can you please update the next patch (SOLR-2382-solrwriter.patch) to trunk? "
        },
        {
            "author": "James Dyer",
            "id": "comment-13069138",
            "date": "2011-07-21T19:05:53+0000",
            "content": "Here is the \"solrwriter\" patch, sync'ed to the latest trunk, which now has the first patch (\"properties\") committed... "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13069414",
            "date": "2011-07-22T06:48:53+0000",
            "content": "SOLR-2382-solrwriter.patch is committed. see if SOLR-2382-entities.patch needs an update  "
        },
        {
            "author": "James Dyer",
            "id": "comment-13069786",
            "date": "2011-07-22T21:25:11+0000",
            "content": "Here is a freshly-sync'ed version of the \"entities\" patch. "
        },
        {
            "author": "Alexey Serba",
            "id": "comment-13071614",
            "date": "2011-07-27T09:18:39+0000",
            "content": "Hmm, after last Solr upgrade my application stopped working. Previously DataImportHandler response contained \"verbose-output\" part in debug/verbose mode, but not anymore. I see there'r some major refactorings going on related to debugLogger / etc. Any clues why \"verbose-ouput\" part is missing? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13071616",
            "date": "2011-07-27T09:21:27+0000",
            "content": "Hi,I shall take a look "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13071624",
            "date": "2011-07-27T09:41:26+0000",
            "content": "This feature was just for debugging. Do you mean that your app was using the debug+verbose for your app? "
        },
        {
            "author": "Alexey Serba",
            "id": "comment-13071652",
            "date": "2011-07-27T11:05:05+0000",
            "content": "Yes, I'm using this feature to test if DIH configuration is valid and to present full stack trace to user if there's any problem with SQL query, etc. I checked out source codes and it seems there's still condition to display \"verbose-output\", but it doesn't working. Something is off after latest refactorings. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13071970",
            "date": "2011-07-27T20:15:40+0000",
            "content": "Alexey,  thanks for finding this problem and letting us know!\n\nThe problem is that SolrWriter contains a package-private null reference to \"DebugLogger\".  This variable is always null and never gets initalized.  DataImportHandler.handleRequestBody() null-checks for it before writing out \"verbose-output\".  Because it is always null, \"verbose-output\" never can get written. \n\nIn the original refactoring, getDebugLogger() was part of the DIHWriter interface, and calling it on SolrWriter would initalize the \"DebugLogger\" variable.  The final version removed getDebugLogger() from the interface, which seems right to me as logging isn't a core function of writing DIH data.  Unfortunately, now DocBuilder holds the real reference to \"DebugLogger\" and this isn't exposed to DataImportHandler.handleRequestBody(). \n\nIt seem to me that DocBuilder probably needs to be adding the verbose output to the final response, moving this out of DataImportHandler.handleRequestBody().  Noble, what do you think?  Let me know if you have time to fix this or if you'd like me to create a new patch. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13072173",
            "date": "2011-07-28T04:36:27+0000",
            "content": "@james it would be helpful if u can give a patch "
        },
        {
            "author": "James Dyer",
            "id": "comment-13072875",
            "date": "2011-07-29T15:50:23+0000",
            "content": "This patch fixes verbose debugging output and can be applied to the current Trunk. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13072879",
            "date": "2011-07-29T16:02:12+0000",
            "content": "Here is a slight tweak on the \"entities\" patch.  A convenience method in CachePropertyUtil to get properties as Objects rather than as Strings is added.  (this fixes a bug I found in BerkleyBackedCache that was introduced recently by changes we've made...) "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13073058",
            "date": "2011-07-29T22:42:20+0000",
            "content": "Is SOLR-2382-entities.patch/July 29, the latest full patch?  "
        },
        {
            "author": "James Dyer",
            "id": "comment-13073225",
            "date": "2011-07-30T18:14:08+0000",
            "content": "Lance,\n\nI apologize that its getting pretty confusing.  If you've got the current trunk, apply:\n\nfirst: SOLR-2382-solrwriter-verbose-fix.patch (7/29) (fixes bug reported by Alexey)\nsecond:  SOLR-2382-entities.patch (7/29)\nthird: SOLR-2382-dihwriter.patch (7/14)\nfourth: SOLR-2613 (BerkleyBackedCache, if desired)\n\n\n\n\n "
        },
        {
            "author": "Alexey Serba",
            "id": "comment-13073437",
            "date": "2011-08-01T04:59:00+0000",
            "content": "James, I tested your patch (SOLR-2382-solrwriter-verbose-fix.patch) and verified that verbose output is working ok now. Noble, could you please review and commit this patch? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13073440",
            "date": "2011-08-01T05:35:09+0000",
            "content": "Sure\nhttps://issues.apache.org/jira/browse/SOLR-2382?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13073437#comment-13073437]\nverified that verbose output is working ok now. Noble, could you please\nreview and commit this patch?\nSOLR-2382-entities.patch, SOLR-2382-entities.patch,\nSOLR-2382-properties.patch, SOLR-2382-properties.patch,\nSOLR-2382-solrwriter-verbose-fix.patch, SOLR-2382-solrwriter.patch,\nSOLR-2382-solrwriter.patch, SOLR-2382-solrwriter.patch, SOLR-2382.patch,\nSOLR-2382.patch, SOLR-2382.patch, SOLR-2382.patch, SOLR-2382.patch,\nSOLR-2382.patch, SOLR-2382.patch, SOLR-2382.patch\na cache implementation that best suits their data and application.\nneeding to create a special cached implementation of the Entity Processor\n(such as CachedSqlEntityProcessor).\nrather than to Solr. Then provide a way for a subsequent DIH call to use the\ncache as an Entity input. Also provide the ability to do delta updates on\nsuch persistent caches.\nthen be fed back into DIH and indexed either to varying Solr Shards, or to\nthe same Core in parallel.\ndata prior to joining to parent entities.\nproblem.\nCaching mechanism and does not scale.\nprocess that runs separate from our main indexing process.\nchanged.\nfew fields changed.\n1 entity's data changed.\n1.4.1, which did not have the \"threads\" parameter).\npartition our source data into Shards.\nCachedSqlEntityProcessor (now deprecated).\nwith je-4.1.6.jar\nbelieve this may be incompatible due to Generic Usage.\nto use or evaluate this patch, download bdb-je from\nhttp://www.oracle.com/technetwork/database/berkeleydb/downloads/index.html\nentity data to be cached (see EntityProcessorBase & DIHCacheProperties).\npersistent Cache as DIH Entity Input.\nDIHCacheProcessor to allow for easy partitioning of source entity data.\nDocBuilder.buildDocument().\ndisk-backed cache) once the entity processor is completed.\ndestroy() was LineEntitiyProcessor, so this is not a very invasive change.\nlegacy search engine to Solr. However, I found that DIH did not support\ncaching to the level of our prior product's data import utility. In order to\nget our data into Solr, I created these caching enhancements. Because I\nbelieve this has broad application, and because we would like this feature\nto be supported by the Community, I have front-ported this, enhanced, to\nTrunk. I have also added unit tests and verified that all existing test\ncases pass. I believe this patch maintains backwards-compatibility and would\nbe a welcome addition to a future version of Solr. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13078354",
            "date": "2011-08-02T18:38:43+0000",
            "content": "Hello-\n\nAre there any benchmark results with this patch? Given, say two tables with a million elements in a pairwise join, how well does this caching system work?\n "
        },
        {
            "author": "James Dyer",
            "id": "comment-13078384",
            "date": "2011-08-02T19:26:33+0000",
            "content": "Lance,\n\nI do not have any scientific benchmarks, but I can tell you how we use BerkleyBackedCache and how it performs for us.  \n\nIn our main app, we fully re-index all our data every night (13+ million records).  Its basically a 2-step process.  First we run ~50 DIH handlers, each of which builds a cache from databases, flat files, etc.  The caches partition the data 8-ways.  Then a \"master\" DIH script does all the joining, runs transformers on the data, etc.  We have all 8 invocations of this same \"master\" DIH config running simultaneously indexing to the same Solr core, so each DIH invocation is processing 1.6 million records directly out of caches, doing all the 1-many joins, running transformer code, indexing, etc.  This takes 1-1/2 hours, so maybe 250-300 solr records get added per second.  We're using fast local disks configured with raid-0 on an 8-core 64gb server.  This app is running solr 1.4, using the original version of this patch, prior to my front-porting it to trunk.  No doubt some of the time is spent contending for the Lucene index as all 8 DIH invocations are indexing at the same time.\n\nWe also have another app that uses Solr4.0 with the patch I originally posted back in February, sharing hardware with the main app.  This one has about 10 entities and uses a simple 1-dih-handler configuration.  The parent entity drives directly off the database while all the child entities use SqlEntityProcessor with BerkleyBackedCache.  There are only 25,000 fairly narrow records and we can re-index everything in about 10 minutes.  This includes database time, indexing, running transformers, etc in addition to the cache overhead.\n\nThe inspiration for this was that we were converting off of Endeca and we were relying on Endeca's \"Forge\" program to join & denormalize all of the data.  Forge has a very fast disk-backed caching mechanism and I needed to match that performance with DIH.  I'm pretty sure what we have here surpasses Forge.  And we also get a big bonus in that it lets you persist caches and use them as a subsequent input.  With Forge, we had to output the data into huge delimited text files and then use that as input for the next step...\n\nHope this information gives you some idea if this will work for your use case. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13079720",
            "date": "2011-08-05T01:13:33+0000",
            "content": "Thank you for this explanation!\n\nHere is what seems to be a different use case: we need to find each record in the side table only once. Those foreign keys are in the same order as the primary table. Would we write another kind of DIHCache that loads N foreign keys at a time, and walks through those in order?\n\nThanks.\n\n "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13079748",
            "date": "2011-08-05T02:18:09+0000",
            "content": "\nI apologize that its getting pretty confusing. If you've got the current trunk, apply:\n\nfirst: SOLR-2382-solrwriter-verbose-fix.patch (7/29) (fixes bug reported by Alexey)\nsecond: SOLR-2382-entities.patch (7/29)\nthird: SOLR-2382-dihwriter.patch (7/14)\nfourth: SOLR-2613 (BerkleyBackedCache, if desired)\n\n\n\n\n\tThe first patch has been committed on the trunk: SOLR-2382-solrwriter-verbose-fix.patch without notice. At least, applying this patch to today's trunk (August 4) claims that the patch is a no-op.\n\tThe second patch is required, but has a mistake in it: SOLR-2382-entities.patch\n\t\n\t\tThe section for CachePropertyUtil.java has an error:\n\n--- solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/CachePropertyUtil.java   (revision 0)^M\n+++ solr/contrib/dataimporthandler/src/java/org/apache/solr/handler/dataimport/CachePropertyUtil.java   (revision 0)^M\n@@ -0,0 +1,18 @@^M\n\n\nThe 1,18 should be 1,31 (or 32). This problem caused the file to only have the first 18 lines.\n\t\n\t\n\tHave not tried the third patch yet, or the BDB implementation.\n\n\n\nThe unit tests run. And thank you for adding full tests. "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13079761",
            "date": "2011-08-05T02:38:55+0000",
            "content": "One request: the DIH multi-threaded feature \"threads\" is commonly used with SQL data sources. Please add multi-threaded variants of the unit tests for this feature, and for the BDB plug-in. If possible, \"thrash-testing\" would be very important for catching regressions.\n\nThanks. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13080015",
            "date": "2011-08-05T15:26:32+0000",
            "content": "Updating the patches to the current Trunk.  To use these, apply \"entities\" first, then \"dihwriter\". "
        },
        {
            "author": "James Dyer",
            "id": "comment-13080031",
            "date": "2011-08-05T15:46:17+0000",
            "content": "\nwe need to find each record in the side table only once. Those foreign keys are in the same order as the primary table. Would we write another kind of DIHCache that loads N foreign keys at a time, and walks through those in order?\n\nI don't fully understand what you mean.  You may be interested in DIHCacheProperties.CACHE_NO_DUPLICATE_KEYS ?  This parameter is supported by BerkleyBackedCache and would allow the kinds of 1-to-1 join I think you are describing.  From there, you can use this just like CachedSqlEntityProcessor works today, and join on whatever field the child entity is keyed on (assuming it is in the parent).  The only difference is you can specify a different \"cacheImpl\" and therefore you have options beyond the in-memory-hashmap cache we've got today.\n\n\nPlease add multi-threaded variants of the unit tests for this feature, and for the BDB plug-in. If possible, \"thrash-testing\" would be very important for catching regressions\n\nDid you see TestDihCacheWriterAndProcessor.testMultiThreaded()?  There is also a BDB variant of the test included with SOLR-2613.  I know this doesn't exactly \"thrash-test\" anything but its a good start and on-par with the other multi-threaded DIH tests.  Would you be willing to help with such tests and any subsequent debugging? "
        },
        {
            "author": "Pulkit Singhal",
            "id": "comment-13125207",
            "date": "2011-10-11T17:24:18+0000",
            "content": "@jdyer - Awesome work!\n@noble.paul - Seems like the SOLR-2382-entities.patch file is ready to be committed to trunk. What do you think about it? "
        },
        {
            "author": "James Dyer",
            "id": "comment-13125230",
            "date": "2011-10-11T17:57:39+0000",
            "content": "Noble,\n\nI've been meaning to ask you if there is anything I can do to help move this along.  But then I got very busy and needed to wait until I could genuinely offer assistance!  Things are slowing a bit for me now but I realize you no doubt are busy too.  \n\nIn any case, do you have any lingering concerns you'd like to see addressed?  Like I mentioned before, we wouldn't be in production with Solr without this functionality.  I would imagine many other users would like this also.  "
        },
        {
            "author": "Pulkit Singhal",
            "id": "comment-13125376",
            "date": "2011-10-11T20:48:51+0000",
            "content": "James,\n\nI'm trying to figure out how the data-config.xml would be written to support this case:\n\nWe needed a flexible & scalable way to temporarily cache child-entity data prior to joining to parent entities.\n\n\n1) Can you please provide a sample?\n\n2) Also my guess is that testing this would feature would require the application of both entities.patch and dihwriter.patch ... Correct? I am guessing so as I think that the dihwriter.patch has the code changes for data to be written-to and consumed-from a cache. In my use-case there is one huge text fixed-position child-entity file which has:\n\nparentDataID_1 someData1\nparentDataID_1 someData2\nparentDataID_2 someData3\nparentDataID_2 someData4\n\n\nSo I just want to make sure I'm getting the right set of patches and understanding the application of the use case properly. Please advice. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13125555",
            "date": "2011-10-12T02:58:19+0000",
            "content": "Sorry for the delay. i shall take it soon.  "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13125693",
            "date": "2011-10-12T09:51:02+0000",
            "content": "The DIHCache interface should not have a the following methods\n\n/**\n\t * <p>\n\t *  Get the next document in the cache or NULL if the last record has been reached. \n\t *  Use this method to efficiently iterate through the entire cache.\n\t * </p>\n\t * @return\n\t */\n\tpublic Map<String, Object> getNext() ;\n\t\n\t/**\n\t * <p>Reset the cache's internal iterator so that a subsequent call to getNext() will return the first cached row</p>\n\t */\n\tpublic void resetNext() ;\n\n\n\ninstead we should have a methods\n\nIterator<Map<String,Object>> getIterator()\n\nIterator<Map<String,Object>> getIterator(String key)\n\n\n\n\n\nThis means all the states maintained for iteration will go away\n\nIf we can do that the following fields will be redundant in SortedMapBackedCache\n\n\tprivate Iterator<Map.Entry<Object, List<Map<String, Object>>>> theMapIter = null;\n\tprivate Object currentKey = null;\n\tprivate List<Map<String, Object>> currentKeyResult = null;\n\tprivate Iterator<Map<String, Object>> currentKeyResultIter = null;\n\n\n "
        },
        {
            "author": "James Dyer",
            "id": "comment-13126751",
            "date": "2011-10-13T17:46:07+0000",
            "content": "Pulkit,\n\nI take it you have a root entity (possibly from a SQL database) and a child entity from a flat text file, and you need to join the two data sources.  There are two ways to do this using this caching.  In either case you'll need both patches (\"entities\" & \"dihwriter\").  Also, if an in-memory cache is not adequate, you also need to get BerkleyBackedCache from SOLR-2613 (required by the second 2-handler approach).  \n\nThe simple way uses a temporary (or ephemeral) cache.  To do this, create a single DIH Request Handler and add your cached child entity to data-config.xml.  DIH will load and cache \"child_entity\" every time you do an import.  When the import is finished, the cache is deleted.  This allows you to do joins on flat files whereas without caching it would not be possible.  The downside is if the flat file changes infrequently, or if you're doing delta updates on your index, it would be inefficient to load and cache a large flat file every time.  Here's a sample data-config.xml:\n\n\n<dataConfig>\n <dataSource name=\"SQL\" ... />\n <dataSource name=\"URL\" baseUrl=\"path_to_flat_file\" type=\"URLDataSource\" />\n <document name=\"my_doc\">\n   <entity name=\"root_entity\" rootEntity=\"true\" dataSource=\"SQL\" pk=\"root_id\" query=\"select root_id, more_data, etc...\" >\n    <entity\n     name=\"child_entity\"\n     processor=\"LineEntityProcessor\"\n     dataSource=\"URL\"\n     transformer=\"BreakIntoFieldsTransformerIWrote\"\n     cacheImpl=\"BerkleyBackedCache\"\n     cacheBaseDir=\"temp_location_for_cache\"\n     cachePk=\"root_id\"\n     cacheLookup=\"root_entity.root_id\"\n     fieldNames=\"root_id,  flatfiledata1, etc\"\n     fieldTypes=\"BIGDECIMAL, STRING, etc\"\n    />\n   </entity>\n </document>\n</dataConfig>\n\n\n\nThe second approach is to create a second DIH Request Handler in your solrconfig.xml for the child entity.  This request handler has its own data-config.xml (named dih-flatfile.xml).  You would run this second request handler to build a persistent cache for the flat file, prior to running the main DIH request handler.  Here's an example of this second DIH Request Handler configured in sorlconfig.xml:\n\n\n<requestHandler name=\"/dih-flatfile\" class=\"org.apache.solr.handler.dataimport.DataImportHandler\">\n <lst name=\"defaults\">\n  <str name=\"config\">dih-flatfile.xml</str>\n  <str name=\"cacheDeletePriorData\">true</str>\n  <str name=\"fieldNames\">root_id,  flatfiledata1, etc</str>\n  <str name=\"fieldTypes\">BIGDECIMAL, STRING, etc</str>\n  <str name=\"writerImpl\">org.apache.solr.handler.dataimport.DIHCacheWriter</str>\n  <str name=\"cacheImpl\">BerkleyBackedCache</str>\n  <str name=\"cacheBaseDir\">location_of_persistent_caches</str>\n  <str name=\"cacheName\">flatfile_cache_name</str>\n  <str name=\"cachePk\">root_id</str>\n </lst>\n</requestHandler>\n\n\n\nAnd here is what \"dih-flatfile.xml\" would look like:\n\n\n<dataConfig>\n <dataSource name=\"URL\" baseUrl=\"path_to_flat_file\" type=\"URLDataSource\" />\n <document name=\"my_doc_child\">\n   <entity name=\"child_entity\" processor=\"LineEntityProcessor\" dataSource=\"URL\" transformer=\"BreakIntoFieldsTransformerIWrote\" />\n </document>\n</dataConfig>\n\n\n\nYour main \"dataconfig-xml\" would look like this:\n\n\n<dataConfig>\n <dataSource name=\"SQL\" ... />\n <dataSource name=\"URL\" baseUrl=\"path_to_flat_file\" type=\"URLDataSource\" />\n <document name=\"my_doc\">\n   <entity name=\"root_entity\" rootEntity=\"true\" dataSource=\"SQL\" pk=\"root_id\" query=\"select root_id, more_data, etc...\" >\n    <entity\n     name=\"child_entity\"\n     processor=\"org.apache.solr.handler.dataimport.DIHCacheProcessor\"\n     cacheImpl=\"BerkleyBackedCache\"\n     cachePk=\"root_id\"\n     cacheLookup=\"root_entity.root_id\"\n     cacheBaseDir=\"location_of_persistent_caches\"\n     cacheName=\"flatfile_cache_name\"\n    />\n   </entity>\n </document>\n</dataConfig>\n\n\n\nThis second approach offers more flexibility (you can load the persistent cache off-hours, re-use it, do delta updates on it, etc) but it is significantly more complex.  The worst part is to create a scheduler that will run the child entity's DIH request handler, wait until it finishes, then run the main DIH Request handler.  But this is moot if you only need to load the child once or once in a great while.\n\nShould this all get committed, I will eventually create something on the wiki.  In the mean time, I hope you find all of this helpful.  For more examples, see the xml files these patches add to the \"solr/contrib/dataimporthandler/src/test-files/dih/solr/conf\" folder, and also the new unit tests that use these. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13129200",
            "date": "2011-10-17T21:07:41+0000",
            "content": "Noble,\n\nHere is a version of the \"entities\" patch using \".iterator()\" methods as you suggest.  Let me know if this is what you had in mind and also if there is anything else you'd like to address. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13129493",
            "date": "2011-10-18T05:14:07+0000",
            "content": "A few more points\n\nLet us avoid methods which are not used \n\nexample\nSolrWriter.setDeltaKeys();\n\nNow that we have a concept of DIHCache, move all cache related logic from EntityprocessorBase to another class. Probably a baseDIHCache. \n\nit is not implemented and I am not even clear why it is there\n\nLet us put in the minimum amount of changes\n\nremove the DIHCacheProperties class and inline the constants. That is the way it done everywhere else\n\nI don't understand the need for DocBuilder.resetEntity() According to me the DataCOnfig state must not be changed between runs. \n "
        },
        {
            "author": "James Dyer",
            "id": "comment-13129778",
            "date": "2011-10-18T14:59:29+0000",
            "content": "Noble,\n\nThanks for the comments.  Let me see if I can answer some of your questions and perhaps you can give further guidance as to how we can address your concerns.  In the mean time, I can try to get a patch together that incorporates as much as possible of what you suggest.\n\n\nSolrWriter.setDeltaKeys();\nit is not implemented and I am not even clear why it is there\n\nThis implements the new DIHWriter interface, but I see I failed to put the proper annotations in, hence the confusion.  This method is required by a DIHWriter that supports both delta updates and duplicate keys (ex. the DIHCacheWriter, in the next patch).  SolrWriter does not implement this because Solr does not support duplicate keys.  That is, in the case of a delta update to a Solr Index, a repeat key is definitely an Update, and cannot be an Add.  Caches that support duplicate keys, however, need to know up-front whether or not a duplicate key is an Add or an Update.  In the next patch, I will put all the proper annotations in place.  Will this satisfy your concern?\n\n\nNow that we have a concept of DIHCache, move all cache related logic from EntityprocessorBase to another class. Probably a baseDIHCache.\nI realized back when this was first developed that this would be a good future refactoring but this is a pretty big project already and I was trying to minimize the changes.  But I can do this in the next patch version if you'd like it done now.  Sound good? \n\n\nremove the DIHCacheProperties class and inline the constants. That is the way it done everywhere else\nIt made more sense back when I was developing this to have the constants centralized because many of them are being used by more than one class.  But for consistency I can inline them somewhere for the next patch version.  Agree?\n\n\nI don't understand the need for DocBuilder.resetEntity() According to me the DataCOnfig state must not be changed between runs. \nAll this does is recursively set the \"initalized\" flag on an entity and its children back to \"false\".  (the \"initalized\" flag ensures that \"destroy\" is only called on an entity once..See \"Implementation Details\" #6 in my original description for this issue).  I think I added \"resetEntity\" as a safety measure because I don't know enough about DIH to be guaranteed that these entity objects never get used again.  If you're pretty sure its impossible the same entity objects would be used again, we can remove \"resetEntity\".  In the mean time, let me see if all the unit tests pass for everything with this removed.  If you're sure, and if all unit tests pass without it, then I'd agree we should remove it.  Sound like a plan on this one? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13131486",
            "date": "2011-10-20T09:38:20+0000",
            "content": "\nI realized back when this was first developed that this would be a good future refactoring but this is a pretty big project already and I was trying to minimize the changes \n\n\n\nIt may be worth a try. Because, this issue is about having pluggable caching . If the caching logic lives in the core code the issue is not really complete w/o it\n\n\neither implement the SolrWriter.setDeltaKeys() or remove it. Either way I'm fine\n\nThe entities are reused . But it has always been like that. Why do you need that initialized flag? What is initialized? \n\n "
        },
        {
            "author": "James Dyer",
            "id": "comment-13131674",
            "date": "2011-10-20T14:54:33+0000",
            "content": "\nThe entities are reused . But it has always been like that. Why do you need that initialized flag? What is initialized? \n\nPerhaps \"intialized\" is the wrong name for this flag, but let me explain how its used.  In \"Implementation Details\" #6 in this issue's description, I mentioned the need to change the semantics for \"entity.destroy()\".  Previous to this patch, for child entities, both \"entity.destroy()\" & \"entity.init\" get called once per parent row.  So throughout the course of a DIH import, child entities constantly get their \"init\" and \"destroy\" methods called over and over again.  But what if we have \"init\" and \"destroy\" operations that are meant to be executed only once?  \"init\" copes with this by setting a \"firstInit\" flag on each entity and having any init steps that get called only once controlled by this flag.  \n\nBut there was no such coping mechanism built into \"destroy\".  There was never a need because in actuality only one of our prepacked entities implements \"destroy()\". But entities that use persistent caching require that there be a way to clean up any unneeded caches at the end.  Because \"destroy()\" was largely unused, I decided to change its semantics to handle this end-of-lifecycle cleanup operation.  (The one entity that already implements \"destroy\" is LineEntitiyProcessor, but prior to this patch we cannot use LineEntityProcessor as a child entity and do joins, so the semantic change here doesn't matter.)  \n\nThus the \"entityWrapper.initalized\" flag gets set (DocBuilder lines 637-640) the first time a particular entity is encountered.  The flag ensures that the entity gets added to the \"Destroy-List\" only once.  When any entity is done being used (its parent is finished), the appropriate \"Destroy-List\" is looped through, the children are destroyed, and their initialized flags get set back to \"false\". (DocBuilder lines 617-621).  \"resetEntity()\" sets the flag back, existing in its own method so that it may be done recursively.\n\nI apologize for this very long explanation, but I hope this is helpful.  Obviously I've made design decisions here that you may (or perhaps not) differ on.  Basically I need to have an \"entity.destroy()\" that is guaranteed to get called only once, at the time the entity is done executing.  If you would like this done differently, let me know what you have in mind and I can try and change it.  \n\nDo you now understand why I am using an \"initalized\" flag?  Is this ok as-is, or if not, how would you like the design changed? "
        },
        {
            "author": "James Dyer",
            "id": "comment-13132942",
            "date": "2011-10-21T19:03:13+0000",
            "content": "Noble,\n\nI have updated the \"entities\" patch to address the changes we've discussed this week.  The only thing I didn't address here was your question about \"resetEntity()\", which I hope was adequately explained yesterday.\n\nIs there anything else that needs to be done with \"entities\" before it can be committed? "
        },
        {
            "author": "James Dyer",
            "id": "comment-13132944",
            "date": "2011-10-21T19:04:02+0000",
            "content": "Here is a version of the \"dihwriter\" patch compatible with today's \"entity\" patch update. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13132945",
            "date": "2011-10-21T19:04:48+0000",
            "content": "re-attaching \"entities\" with \"grant license\" button selected. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13136933",
            "date": "2011-10-27T09:56:21+0000",
            "content": "With some clean up. \nI think there is a very big omission. The EntityProcessorBase.transformers field is not used in the latest patch. How do does transformation work? "
        },
        {
            "author": "James Dyer",
            "id": "comment-13137184",
            "date": "2011-10-27T14:43:53+0000",
            "content": "\nThe EntityProcessorBase.transformers field is not used in the latest patch. How do does transformation work?\nThis hasn't been in use since SOLR-1120 was applied back in 2009 (r766608).  Since SOLR-1120, Transformers are applied in class EntityProcessorWrapper.  We could remove this variable from the class now as it does nothing and is not used by this class or any prepackaged subclasses.  Agree?\n\n\nWith some clean up. \nWhat other kinds of things do you have in mind? "
        },
        {
            "author": "James Dyer",
            "id": "comment-13147072",
            "date": "2011-11-09T14:51:50+0000",
            "content": "Noble,\n\nIs there anything else you need from me to help move this forward? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13147492",
            "date": "2011-11-10T05:08:10+0000",
            "content": "I shall commit the latest patch shortly. I'm seeing some exceptions (unrelated may be). Will investigate a bit firther "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13149540",
            "date": "2011-11-14T10:30:15+0000",
            "content": "committed svn version 121659. Thanks James "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13149711",
            "date": "2011-11-14T16:12:00+0000",
            "content": "Hi Noble,\n\nIn DIHCache.java, you used the javadoc tag @solr.experimental, but there is no support in the build system for this tag, so it causes javadoc warnings, which fail the build, e.g.: https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/11327/consoleText (scroll down to the bottom to see the warning):\n\n\n[javadoc] [...]/DIHCache.java:14: warning - @solr.experimental is an unknown tag.\n\n\n\nWould you mind if I switch @solr.experimental to @lucene.experimental? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13149731",
            "date": "2011-11-14T16:41:38+0000",
            "content": "please go ahead "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13149739",
            "date": "2011-11-14T16:49:05+0000",
            "content": "please go ahead\n\nDone: r1201784 "
        },
        {
            "author": "James Dyer",
            "id": "comment-13149916",
            "date": "2011-11-14T21:03:53+0000",
            "content": "Here is an updated version of the \"dihwriter\" patch, which is the last part of this ticket.  This is in-sync with Noble's commit of the \"entities\" patch.\n\nThis includes:\n1. DIHCacheWriter - lets DIH write to a DIHCache instead of to Solr for future processing.  This also supports Delta Updates on caches.\n2. DIHCacheProcessor - lets DIH read from a cache that was previously  written by DIHCacheWriter.\n3. MockDIHCache - a bare-bones cache implementation that supports persistence and delta updates.  This allows us to run all of the unit tests without also needing to apply SOLR-2613 (BerkleyBackedCache).  This also provides a full reference implementation for those who would like to write their own Caches.\n\nNoble, will you be able to work through this patch also with me? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13154123",
            "date": "2011-11-21T11:38:58+0000",
            "content": "Is this required ? Could you please let me know the use cases for this? "
        },
        {
            "author": "James Dyer",
            "id": "comment-13154257",
            "date": "2011-11-21T15:43:30+0000",
            "content": "Noble,\n\nI can't speak for every use case, but these were necessary for one of our applications.  The whole idea is it lets you load your caches in advance of indexing (DIHCacheWriter), then read back your caches at a later time when you're ready to index (DIHCacheProcessor).\n\n\n\tThis is especially helpful if you have a lot of different data sources that each contribute a few data elements in each Solr record.  (we have at least 40 data sources.)\n\n\n\n\n\tIf you have slow data sources, you can run multiple DIH scripts at the same time and build your caches simultaneously (My app builds 12 DIH Caches at a time as we have some slow legacy databases to content with).\n\n\n\n\n\tIf you have a some data sources that change infrequently and other that are changing all the time, you can build caches for the infrequently-changing data sources, making it unnecessary to re-acquire this data every time you do a delta update (this is actually a very common case.  Imagine having Solr loaded with Product metadata.  Most of the data would seldom change but things like prices, availability flags, stock numbers, etc, might change all the time.)\n\n\n\n\n\tThe fact that you can do delta imports on caches allows users to optimize the indexing process further.  If you have multiple child-entity caches with data that mostly stays the same, but each has churn on a small percentage of the data, being able to just go in and delta update the cache lets you only re-acquire what changed.  Otherwise, you have to take every record that had a change in even 1 data source and re-acquire all of the data sources for every record.\n\n\n\n\n\tThese last two points relate to the fact that Lucene cannot do an \"update\" but only a \"replace\".  Being able to store your system-of-record data in caches alleviates the need to re-acquire all of your data sources every time you need to do an \"update\" on a few fields.\n\n\n\n\n\tSome systems do not have a separate system-of-record as the data being indexed to Solr is ephemeral or changes frequently.  Having the data in caches gives you the freedom to delta update the information or easily re-index all data at system upgrades, etc.  I could see for some users these caches factoring into their disaster recovery strategy.\n\n\n\n\n\tThere is also a feature to partition the data into multiple caches, which would make it easier to subsequently index the data to separate shards.  We use this feature to index the data in parallel to the same core (we're using Solr 1.4, which did not have a \"threads\" parameter), but this would apply to using multiple shards also.\n\n\n\nIs this convincing enough to go ahead and work towards commit? "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13157979",
            "date": "2011-11-27T19:45:38+0000",
            "content": "Hello,\n\nI want to contribute test for Parent/Child usecase with CachedSqlentityProcessor in single- and multi-thread modes: \nTestThreaded.java.patch on r1144761\nPls, let me know, how do you feel about it?\n\nActually, I've explored this case at 3.4 some \ntime ago, but decided to wait a little until this re-factoring made a progress.\n\n\n\tthe first issue is testCachedSingleThread_FullImport() failure. It's caused by\nDocBuilder.java line 473\n   } finally {\n        entityProcessor.destroy();\n      }\n\n\nthis code, which cleanups the cache, makes sense, but for parent entities only, and causes a failure for the child entities enumeration, when run() is called from line :510. It shouldn't be a big deal to fix. \n\n\n\n\n\tthen, some minor moaning: looks like where=\"xid=x.id\" is not supported by new code, which relies on cachePk=\"xid\" and cacheLookup=\"x.id\". for me it's a matter of opinion, backward compatibility and documentation.\n\n\n\n\n\tthe most interesting problem is failure of testCachedMultiThread_FullImport(). At 3.4 it's caused by concurrent access of child entities iteration state. Now it looks like DIHCacheSupport.dataSourceRowCache is accessed by multiple threads from ThreadedEntityProcessorWrapper. I have some ideas, but want to know your opinion.\n\n\n\nGuys, I can handle some of these, let me know how I can help.\n\n\u2013\nMikhail "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13158258",
            "date": "2011-11-28T07:16:32+0000",
            "content": "@James \n\n\tMy question is, Is it relevant to a large no:of people to have this last patch ?\n\tCan you achieve your objective by just adding an implementation? I mean will you need to patch DIH to meet your needs?\n\n\n "
        },
        {
            "author": "James Dyer",
            "id": "comment-13158512",
            "date": "2011-11-28T15:31:34+0000",
            "content": "Mikhail,\n\nThank you for testing this and providing information and a patch.  I have a few questions.\n\n\nthis code, which cleanups the cache, makes sense, but for parent entities only, and causes a failure for the child entities enumeration\nCurrently none of the existing unit tests fail with this change and I'm not sure exactly how to reproduce the problem you're describing.  Could you create a failing unit test for this to clarify what you're experiencing?  \n\n\nlooks like where=\"xid=x.id\" is not supported by new code, which relies on cachePk=\"xid\" and cacheLookup=\"x.id\"\nDo you mean this breaks CachedSqlEntityProcessor?  Looking at TestCachedSQLEntityProcessor, it seems like the case you describe is adequately tested and this test still passes.  Possibly you mean something different?  Once again, a failing unit test would be helpful in knowing how to reproduce the specific problem you've found.\n\n\nthe most interesting problem is failure of testCachedMultiThread_FullImport(). At 3.4 it's caused by concurrent access...\nThis sounds like a valid issue, and just looking it seems like you've got a good unit test for it.  But if its happening in 3.4 then its not related to SOLR-2382, which is in Trunk/4.0 only.  Would you mind opening a new \"bug\" issue for this? "
        },
        {
            "author": "James Dyer",
            "id": "comment-13158606",
            "date": "2011-11-28T18:04:57+0000",
            "content": "Here is an updated \"dihwriter\" patch, fixing a test bug. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13158623",
            "date": "2011-11-28T18:46:55+0000",
            "content": "James,\n\nPls find my replies below\n\nCurrently none of the existing unit tests fail with this change and I'm not sure exactly how to reproduce the problem you're describing. Could you create a failing unit test for this to clarify what you're experiencing?\nI've done it yesterday, I attached TestThreaded.java.patch on trunk r1144761.\n Can you see it in attachments?\n After you applied it both of new tests fail. The simplest one testCachedSingleThread_FullImport() can be fixed by disabling cache destroying by commenting DocBuilder.java line:473 - // entityProcessor.destroy();\nI believe it should be properly covered by the separate issue.\n\n\n Once again, a failing unit test would be helpful in knowing how to reproduce the specific problem you've found.\ntestCachedSingleThread_FullImport() fails again if you remove  cachePk=\"xid\" cacheLookup=\"x.id\" from test config constant line 109. I briefly look into TestCachedSQLEntityProcessor it seems to me I've got where it is not accurate enough, let me attach my findings soon. \n\n\nBut if its happening in 3.4 then its not related to SOLR-2382, which is in Trunk/4.0 only\nThe problem the same as it is at 3.x. My test TestThreaded.java.patch has testCached_Multi_ Thread_FullImport() which covers particularly this race condition. You can see it after make testCached_Single_Thread_FullImport() green somehow (see issue no.1). I can't spawn it as new issue because of the blocker no.1\n "
        },
        {
            "author": "James Dyer",
            "id": "comment-13158666",
            "date": "2011-11-28T19:26:15+0000",
            "content": "\nCan you achieve your objective by just adding an implementation? I mean will you need to patch DIH to meet your needs?\nThis patch, \"SOLR-2382-dihwriter_standalone.patch\" separates the parameter names so that it is entirely stand-alone.  So yes, for my purposes, we can put this code in a separate .jar and it will not need any patches in DIH.\n\n\nMy question is, Is it relevant to a large no:of people to have this last patch ?\nAnyone who is indexing a lot of data that needs to join from multiple sources would probably consider using this, provided it gets well-documented.  In our case, we have different data sources across our enterprise that all contribute a few fields to each Solr document, so having this added flexibility was vital for us.\n\nIn any case, if you do not want to work towards committing this last patch now I think it would be wise to close SOLR-2382 and I can open a new case just for this last patch.  That'll make it easier for someone who wants the functionality in the future to find it in JIRA, or if some other committer wants to pick it up someday.  Let me know what you plan to do. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13158689",
            "date": "2011-11-28T19:50:56+0000",
            "content": "James,\n\npls find my proof for absence of where=\"xid=x.id\" support. TestCachedSqlEntityProcessor.java-break-where-clause.patch it looks puzzling - I'm  sorry for that. The test was green due to relying on keys order in the map. Wrapping by sorted map breaks that order and lead to peaking up wrong primarykey column. pls find explanation below.\n\nfrom my pov the most cruel thing is lines:27-28 it pick ups just first key from the map as primary key, when it wasn't properly detected from attributes. so this condition hides a problem, until just face it and address.\n\nleft part of where clause isn't used here at lines 45-48 and \"where=\"\"\" is ignored again at lines 185-190\n\nyou can see that the second attach TestCachedSqlEntityProcessor.java-fix-where-clause-by-adding-cachePk-and-lookup.patch fixes the test by adding cachePk and lookup into attributes.\n\nMy proposals are:\n\n\tfix it. it's not a big deal to came where attr back\n\tbut why the new attributes cachePk and cacheLoop are better than old where attribute ? in according to reply I vote for\n\t\n\t\tdecommission where=\"\" or for\n\t\trolling new cahePk/Lookup attributes back\n\t\n\t\n\tcan't we add more randomization into AbstractDataImportHandlerTestCase.createMap(Object...) to find more similar hidden issues. I propose to choose concrete map behaviour randomly: hash, sorted, sorted-reverse. WDYT?\n\tthe names withWhereClause() and withKeyAndLookup() should be swapped. their content contradicts to the names\n\n  public void withWhereClause() {\n...\n        \"query\", q, DIHCacheSupport.CACHE_PRIMARY_KEY,\"id\", DIHCacheSupport.CACHE_FOREIGN_KEY ,\"\n...\n  public void withKeyAndLookup() {\n...\n    Map<String, String> entityAttrs = createMap(\"query\", q, \"where\", \"id=x.id\",\n...\n\n  \n\n "
        },
        {
            "author": "James Dyer",
            "id": "comment-13158797",
            "date": "2011-11-28T21:30:21+0000",
            "content": "Mikhail,\n\nI looked at \"TestCachedSqlEntityProcessor.java-break-where-clause.patch\".  It looks like a bug is being introduced into the test because we have a Map<Object> with some Strings and some Integers in it.  You modified the test case to store these in a TreeMap.  But when the TreeMap's comparator tries to get these in order, it cannot cast the String to Integer and/or visa-versa.  I don't mean to say the bug you describe doesn't exist, but that this new test case doesn't seem to properly test for it.  Could you improve this test case?\n "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13159418",
            "date": "2011-11-29T18:13:22+0000",
            "content": "James,\n\nYour observation contradicts to the stack trace:\n\norg.apache.solr.handler.dataimport.DataImportHandlerException: java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer\n    at org.apache.solr.handler.dataimport.DataImportHandlerException.wrapAndThrow(DataImportHandlerException.java:64)\n    at org.apache.solr.handler.dataimport.EntityProcessorWrapper.nextRow(EntityProcessorWrapper.java:240)\n    at org.apache.solr.handler.dataimport.TestCachedSqlEntityProcessor.doWhereTest(TestCachedSqlEntityProcessor.java:227)\n    at org.apache.solr.handler.dataimport.TestCachedSqlEntityProcessor.withKeyAndLookup(TestCachedSqlEntityProcessor.java:211)\n.....\nCaused by: java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Integer\n    at java.lang.Integer.compareTo(Integer.java:37)\n    at java.util.TreeMap.getEntry(TreeMap.java:328)\n    at java.util.TreeMap.get(TreeMap.java:255)\n    at org.apache.solr.handler.dataimport.SortedMapBackedCache.iterator(SortedMapBackedCache.java:106)\n    at org.apache.solr.handler.dataimport.DIHCacheSupport.getIdCacheData(DIHCacheSupport.java:160)\n    at org.apache.solr.handler.dataimport.DIHCacheSupport.getCacheData(DIHCacheSupport.java:127)\n    at org.apache.solr.handler.dataimport.EntityProcessorBase.getNext(EntityProcessorBase.java:131)\n    at org.apache.solr.handler.dataimport.SqlEntityProcessor.nextRow(SqlEntityProcessor.java:75)\n    at org.apache.solr.handler.dataimport.EntityProcessorWrapper.nextRow(EntityProcessorWrapper.java:237)\n    ... 31 more\n\n\n\nUsing a SortedMap is absolutely valid here. It has Strings as column names and String and Integer as a values. The problem is caused by the detecting primary key, which I described above. \"desc\" column was recognized as Pk, and parent fk is looked up in it. That causes ClassCastException.\n\nLet me reattach the patch with LinkedHashMap instead of sorted, I need just have \"desc\" column first to reproduce the issue. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13159431",
            "date": "2011-11-29T18:33:31+0000",
            "content": "TestCachedSqlEntityProcessor.java-wrong-pk-detected-due-to-lack-of-where-support.patch breaks withKeyAndLookup() by reordering entries in row map by LinkedHashMap() (pls have a look to createLinkedMap())\n\nStacktrace is pretty the same as at the comment above. you can check by debugger the failed instance of SortedMapBackedCache. it has \"desc\" as primaryKeyName and messed up theMap:\n\n {another another three=[{id=3, desc=another another three}], another three=[{id=3, desc=another three}], another two=[{id=2, desc=another two}], one=[{desc=one, id=1}], three=[{id=3, desc=three}], two=[{id=2, desc=two}]}\n\n\nRegards "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13160683",
            "date": "2011-12-01T06:38:12+0000",
            "content": "I spawned subtask SOLR-2933 "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13160688",
            "date": "2011-12-01T06:47:52+0000",
            "content": "@James \nYes create a new issue for all the further functionalities and let's close this one "
        },
        {
            "author": "James Dyer",
            "id": "comment-13161726",
            "date": "2011-12-02T17:11:26+0000",
            "content": "Noble,\n\nI have attached a patch with a corrected unit test & fix on SOLR-2933, to fix one of the problems Mikhail described.  Indeed the \"where\" parameter was broken by our last commit and TestCachedSqlEntityProcessor would mask the failure and pass anyway.  Would you mind looking at my patch and committing it?  Thanks. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13161831",
            "date": "2011-12-02T20:19:20+0000",
            "content": "Spun off the remaining piece to SOLR-2943, so closing this one.\n\nNoble,  Thank you for all your help with this. "
        },
        {
            "author": "Mikhail Khludnev",
            "id": "comment-13162467",
            "date": "2011-12-04T20:23:18+0000",
            "content": "James, Noble,\n\nPls have a look to the new one: SOLR-2947 "
        },
        {
            "author": "James Dyer",
            "id": "comment-13234800",
            "date": "2012-03-21T18:21:39+0000",
            "content": "Re-Open to backport for 3.x   This is being done so that the SOLR-3011 thread fixes can be included in 3.x.  (Plan is to give 3.x users a stable-enough \"threads\" implementation in 3.x, then remove \"threads\" from trunk.) "
        },
        {
            "author": "James Dyer",
            "id": "comment-13234864",
            "date": "2012-03-21T19:02:05+0000",
            "content": "Patch for 3.x includes everything already committed to Trunk as well as various bug fixes (also in Trunk already).  (Patch is here for reference only ; changes were actually moved using svn merge)\n\nI will commit to 3.x shortly. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13235691",
            "date": "2012-03-22T16:33:09+0000",
            "content": "commit to 3.x: r1303792 (& r1303822 - license headers) "
        }
    ]
}