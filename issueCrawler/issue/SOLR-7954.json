{
    "id": "SOLR-7954",
    "title": "ArrayIndexOutOfBoundsException from distributed HLL serialization logic when using using stats.field={!cardinality=1.0} in a distributed query",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.4",
            "6.0"
        ],
        "affect_versions": "5.2.1",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "User reports indicate that using stats.field={!cardinality=1.0}foo on a field that has extremely high cardinality on a single shard (example: 150K unique values) can lead to \"ArrayIndexOutOfBoundsException: 3\" on the shard during serialization of the HLL values.\n\nusing \"cardinality=0.9\" (or lower) doesn't produce the same symptoms, suggesting the problem is specific to large log2m and regwidth values.",
    "attachments": {
        "SOLR-7954.patch": "https://issues.apache.org/jira/secure/attachment/12752105/SOLR-7954.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-08-21T06:09:53+0000",
            "author": "Modassar Ather",
            "content": "The schema used is as follows.\n\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<schema name=\"collection\" version=\"1.5\">\n\n     <types>\n        <fieldType name=\"string\" class=\"solr.StrField\" sortMissingLast=\"true\" stored=\"false\" omitNorms=\"true\"/>\n        <fieldType name=\"string_dv\" class=\"solr.StrField\" sortMissingLast=\"true\" stored=\"false\" indexed=\"false\" docValues=\"true\"/>\n        <fieldType name=\"long\" class=\"solr.TrieLongField\" precisionStep=\"0\" omitNorms=\"true\" positionIncrementGap=\"0\" stored=\"false\"/>\n    </types>\n\n    <fields>\n        <field name=\"field1\"      type=\"string\"         stored=\"true\"      />\n        <field name=\"field\"       type=\"string_dv\"      multiValued=\"true\" />\n        <field name=\"version\"   type=\"long\"           stored=\"true\"      />\n        <field name=\"colid\"       type=\"string\"         stored=\"true\"      />\n    </fields>\n    <uniqueKey>colid</uniqueKey>\n</schema> ",
            "id": "comment-14706280"
        },
        {
            "date": "2015-08-21T10:34:19+0000",
            "author": "Modassar Ather",
            "content": "Following test method can be used to add data using which the exception can be reproduced. Please do the necessary changes.\nChange <ZKHOST>:<ZKPORT> to point to zkhost available and <COLECTION> to the available collection.\n\npublic void index() throws SolrServerException, IOException {\n    CloudSolrClient s = new CloudSolrClient(\"<ZKHOST>:<ZKPORT>\");\n        int count = 0;\n        s.setDefaultCollection(\"<COLECTION>\");\n        List<SolrInputDocument> documents = new ArrayList<>();\n        for (int i = 1; i <= 1000000; i++) {\n            SolrInputDocument doc = new SolrInputDocument();\n            doc.addField(\"field1\", i);\n            doc.addField(\"colid\", \"val!\"+i+\"!-\"+\"ref\"+i);\n            doc.addField(\"field\", \"DATA\"+(12345+i));\n            documents.add(doc);\n            if((documents.size() % 10000) == 0){\n                count = count + 10000;\n            \ts.add(documents);\n            \tSystem.out.println(System.currentTimeMillis() + \" - Indexed document # \" + NumberFormat.getInstance().format(count));\n            \tdocuments = new ArrayList<>();\n            }\n\t\t}\n\t\t\n                System.out.println(\"Comitting.....................................\");\n\t\ts.commit(true, true);\n\t\tSystem.out.println(\"Optimizing.....................................\");\n                s.optimize(true, true, 1);\n\t\ts.close();\n\t\tSystem.out.println(\"Done.....................................\");\n\t}\n\n ",
            "id": "comment-14706525"
        },
        {
            "date": "2015-08-24T07:41:08+0000",
            "author": "Modassar Ather",
            "content": "I tested following schema with the same data in field and field2. Both reproduced the problem. Then I tried to find if it is value in cardinality which is causing the issue. I tried with 100000 to 120000 document and both the field returned cardinality but after increasing it to around 150000 it caused the exception.\n\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<schema name=\"collection\" version=\"1.5\">\n\n<types>\n<fieldType name=\"string\" class=\"solr.StrField\" sortMissingLast=\"true\" stored=\"false\" omitNorms=\"true\"/>\n<fieldType name=\"string_dv\" class=\"solr.StrField\" sortMissingLast=\"true\" stored=\"false\" indexed=\"false\" docValues=\"true\"/>\n<fieldType name=\"long\" class=\"solr.TrieLongField\" precisionStep=\"0\" omitNorms=\"true\" positionIncrementGap=\"0\" stored=\"false\"/>\n</types>\n\n<fields>\n<field name=\"field\"         type=\"string_dv\"   multiValued=\"true\" />\n<field name=\"field1\"       type=\"string\"         stored=\"true\" />\n<field name=\"field2\"       type=\"string\"         multiValued=\"true\" />\n<field name=\"version\"    type=\"long\"           stored=\"true\" />\n<field name=\"colid\"        type=\"string\"          stored=\"true\" />\n</fields>\n<uniqueKey>colid</uniqueKey>\n</schema>\n\n\n\nFollowing is the method to index data.\n\npublic void index() throws SolrServerException, IOException {\n    CloudSolrClient s = new CloudSolrClient(\"<ZKHOST>:<ZKPORT>\");\n        int count = 0;\n        s.setDefaultCollection(\"<COLECTION>\");\n        List<SolrInputDocument> documents = new ArrayList<>();\n        for (int i = 1; i <= 1000000; i++) {\n            SolrInputDocument doc = new SolrInputDocument();\n            doc.addField(\"field1\", i);\n            doc.addField(\"colid\", \"val!\"+i+\"!-\"+\"ref\"+i);\n            doc.addField(\"field\", \"DATA\"+(12345+i));\n            doc.addField(\"field2\", \"DATA\"+(12345+i));\n            documents.add(doc);\n            if((documents.size() % 10000) == 0){\n                count = count + 10000;\n            \ts.add(documents);\n            \tSystem.out.println(System.currentTimeMillis() + \" - Indexed document # \" + NumberFormat.getInstance().format(count));\n            \tdocuments = new ArrayList<>();\n            }\n\t\t}\n\t\t\n                System.out.println(\"Comitting.....................................\");\n\t\ts.commit(true, true);\n\t\tSystem.out.println(\"Optimizing.....................................\");\n                s.optimize(true, true, 1);\n\t\ts.close();\n\t\tSystem.out.println(\"Done.....................................\");\n\t}\n\n ",
            "id": "comment-14708909"
        },
        {
            "date": "2015-08-24T23:33:09+0000",
            "author": "Hoss Man",
            "content": "I tested following schema with the same data in field and field2. Both reproduced the problem. \n\nOk good \u2013 that means the problem is not actually dependent on docValues or not \u2013 which was the most confusing and suprising part of your initial bug report.\n\nThen I tried to find if it is value in cardinality which is causing the issue. I tried with 100000 to 120000 document and both the field returned cardinality but after increasing it to around 150000 it caused the exception.\n\nok, so somewhere arround 150K docs is the sweetspot.\n\n\n\nReviewing the code you posted, i noticed a few things:\n\n1) every doc gets a unique value in the field you are computing stats on\n2) your query matches all docs\n3) because of how your uniqueKey is defined using composite routing keys (\"!\") every doc will wind up in the same shard.\n\nthe combination of all of these means that ulitmately what's causing problems is:\n\n\tbuilding an HLL data struc using the max possible log2m & regwidth opts (that's what cardinality=1.0 does)\n\tadding ~150K unique(ish) hash values to the HLL\n\tserializing the HLL to bytes (which is what happens in a distributed query to coordinate)\n\n\n\nbased on that, i was able to create a unit test that demonstrates the same underlying ArrayIndexOutOfBoundsException which i'll attach shortly \u2013 still haven't dug in enough to udnerstand hte cause.\n\n(NOTE: since Solr 5.2.1, we've forked the HLL and imported it directly into the org.apache.solr.util.hll package, but the basic structure/functionality of the various classes is still the same) ",
            "id": "comment-14710272"
        },
        {
            "date": "2015-08-24T23:35:18+0000",
            "author": "Hoss Man",
            "content": "patch demonstrating underlying problem \u2013 note that becuase this builds up some pretty large datastructures and byte arrays, you need to give it an increased heap to run...\n\n\n$ ant test -Dtestcase=BigHllSerializationTest -Dtests.heapsize=2g\n...\n   [junit4] ERROR   0.27s | BigHllSerializationTest.testSerialization <<<\n   [junit4]    > Throwable #1: java.lang.ArrayIndexOutOfBoundsException: 3\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([26FE9A72EDF24F3:56B788EF098D196F]:0)\n   [junit4]    > \tat org.apache.solr.util.hll.BigEndianAscendingWordSerializer.writeWord(BigEndianAscendingWordSerializer.java:151)\n   [junit4]    > \tat org.apache.solr.util.hll.BitVector.getRegisterContents(BitVector.java:244)\n   [junit4]    > \tat org.apache.solr.util.hll.HLL.toBytes(HLL.java:908)\n   [junit4]    > \tat org.apache.solr.util.hll.HLL.toBytes(HLL.java:859)\n   [junit4]    > \tat org.apache.solr.util.hll.BigHllSerializationTest.testSerialization(BigHllSerializationTest.java:41)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n\n ",
            "id": "comment-14710273"
        },
        {
            "date": "2015-08-24T23:39:53+0000",
            "author": "Hoss Man",
            "content": "cleaned up summary & description now that we have a better idea what the root symptoms/cause are. ",
            "id": "comment-14710280"
        },
        {
            "date": "2015-08-25T05:13:22+0000",
            "author": "Modassar Ather",
            "content": "To add to the summary and description.\n\nI changed the \n\ndoc.addField(\"colid\", \"val!\"+i+\"!-\"+\"ref\"+i);\n\n to \n\ndoc.addField(\"colid\", \"val\"+i+\"!-\"+\"ref\"+i);\n\nThe documents got distributed to all the nodes. I indexed 1 million documents and was able to reproduce the issue. All the shards had around 200000 documents each.\nLater I indexed 400000 documents on which I could not reproduce it. All the shards had around 100000 documents each.\nThere are 4 shards with no replica on my test environment. ",
            "id": "comment-14710599"
        },
        {
            "date": "2015-08-25T18:49:59+0000",
            "author": "Hoss Man",
            "content": "Tracked down the root problem to some integer overflow - the HLL code was multiplying 2 large integers (w/o casting them individual to longs first) then assigning the (already overflowed) value to a long.\n\nattached patch includes fix, but i want to work on randomizing the test some more \u2013 make sure there aren't similar bugs in other code paths. ",
            "id": "comment-14711770"
        },
        {
            "date": "2015-08-25T18:55:14+0000",
            "author": "Hoss Man",
            "content": "Later I indexed 400000 documents on which I could not reproduce it. All the shards had around 100000 documents each.\nThere are 4 shards with no replica on my test environment.\n\nModassar: as i tried to explain in my earlier comments, the number of shards / documents doesn't really affect the issue \u2013 the root problem has to do with the number of unique values in a single shard which are added to the underlying HyperLogLog data structure and then serialized.  Doing more testing where you tweak the routing or doc counts may find differnet bugs, but for this specific bug the core problem is reviewing the HLL serialization code related to the various precision options (which are set based on the \"cardinality\" local param) and the number of unique (hashed) values in each HLL. ",
            "id": "comment-14711783"
        },
        {
            "date": "2015-08-26T00:49:45+0000",
            "author": "Hoss Man",
            "content": "Reviewing the code & tests more in depth, i realized a few things...\n\n\n\tactaully having a large # of unique values isn't needed to trigger this in the low level HLL code \u2013 you just need to be using the FULL representation with large enough values of log2m and regwidth (which is why at the Solr API level you have to use cardinality=1.0 AND have a lot of unique values \u2013 we defualt to usingthe sparse representation and only promote to the full representation once a lot of values are added.\n\tthe original HLL code's HLLSerializationTest actaully had a test that would have caught this bug, but it was hamstrung with this lovely comment...\n\n// NOTE: log2m<=16 was chosen as the max log2m parameter so that the test\n//       completes in a reasonable amount of time. Not much is gained by\n//       testing larger values - there are no more known serialization\n//       related edge cases that appear as log2m gets even larger.\n// NOTE: This test completed successfully with log2m<=MAXIMUM_LOG2M_PARAM\n//       on 2014-01-30.\n\n\n\n\n\nAwesome.\n\nI refactored HLLSerializationTest a bit so we still have the same Nightly test coverage as before, but also some new Monster tests for exercising some random permutations of options for large sized HLLs (with only a few values) as well as some random permutations of HLLs (of various sizes) with lots of values in them.  (so my previous BigHllSerializationTest is no longer needed)\n\n\n\nI think this is ready to commit & backport, i'll move forward tomorrow unless there are any concerns. ",
            "id": "comment-14712268"
        },
        {
            "date": "2015-08-26T16:33:40+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1697969 from hossman@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1697969 ]\n\nSOLR-7954: Fixed an integer overflow bug in the HyperLogLog code used by the 'cardinality' option of stats.field to prevent ArrayIndexOutOfBoundsException in a distributed search when a large precision is selected and a large number of values exist in each shard ",
            "id": "comment-14714473"
        },
        {
            "date": "2015-08-26T17:24:08+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1697977 from hossman@apache.org in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1697977 ]\n\nSOLR-7954: Fixed an integer overflow bug in the HyperLogLog code used by the 'cardinality' option of stats.field to prevent ArrayIndexOutOfBoundsException in a distributed search when a large precision is selected and a large number of values exist in each shard (merge r1697969) ",
            "id": "comment-14715188"
        },
        {
            "date": "2015-08-27T20:31:03+0000",
            "author": "Hoss Man",
            "content": "Thanks for reporting this Modassar! ",
            "id": "comment-14717476"
        },
        {
            "date": "2015-08-27T20:36:55+0000",
            "author": "Hoss Man",
            "content": "Upstream bug report: https://github.com/aggregateknowledge/java-hll/issues/17 ",
            "id": "comment-14717486"
        },
        {
            "date": "2016-01-07T09:24:00+0000",
            "author": "Modassar Ather",
            "content": "\nq=fl1:net*&facet.field=fl&facet.limit=50&stats=true&stats.field={!cardinality=1.0}fl\n  \nAbove query is returning cardinality around 15 million. It is taking around 4 minutes. Similar response time is seen with different queries which yields high cardinality. Kindly note that the cardinality=1.0 is the desired goal.\nHere in the above example the fl1 is a text field whereas fl is a docValue enabled non-stroed, non-indexed field.\nKindly let me know if such response time is expected or I am missing something about this feature in my query. ",
            "id": "comment-15087105"
        }
    ]
}