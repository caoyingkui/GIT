{
    "id": "SOLR-9055",
    "title": "Make collection backup/restore extensible",
    "details": {
        "components": [],
        "type": "Task",
        "labels": "",
        "fix_versions": [
            "6.4",
            "7.0"
        ],
        "affect_versions": "None",
        "status": "Resolved",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "SOLR-5750 implemented backup/restore API for Solr. This JIRA is to track the code cleanup/refactoring. Specifically following improvements should be made,\n\n\n\tAdd Solr/Lucene version to check the compatibility between the backup version and the version of Solr on which it is being restored.\n\tAdd a backup implementation version to check the compatibility between the \"restore\" implementation and backup format.\n\tIntroduce a Strategy interface to define how the Solr index data is backed up (e.g. using file copy approach).\n\tIntroduce a Repository interface to define the file-system used to store the backup data. (currently works only with local file system but can be extended). This should be enhanced to introduce support for \"registering\" repositories (e.g. HDFS, S3 etc.)",
    "attachments": {
        "SOLR-9055.patch": "https://issues.apache.org/jira/secure/attachment/12801747/SOLR-9055.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-05-02T17:15:47+0000",
            "author": "Hrishikesh Gadre",
            "content": "The repository interface defined as part of this patch could be used while defining APIs in SOLR-7374 ",
            "id": "comment-15267004"
        },
        {
            "date": "2016-05-02T18:30:50+0000",
            "author": "David Smiley",
            "content": "Thanks for contributing,  Hrishikesh Gadre.  I suggest renaming this to: Make collection backup/restore extensible\n\nDoes this API enable the possibility of a hard-link based copy (applicable for both backup & restore).  It doesn't seem so but I'm unsure?\n\nI have a general question about HDFS; I have no real experience with it: I wonder if Java's NIO file abstractions could be used so we don't have to have separate code?  If so it would be wonderful \u2013 simpler and less code to maintain.  See https://github.com/damiencarol/jsr203-hadoop   What do you think?\n\nNitpick: In your editor, if it has this feature (IntelliJ does), configure it to strip trailing whitespace only on Modified Lines.  IntelliJ: Editor/General/Other \"Strip trailing spaces on Save\".\n\nBefore committing to this API, it would be good to have it implement something useful (HDFS or whatever), otherwise we very well may miss problems with the API \u2013 we probably will.  I'm not saying this issue needs to implement HDFS, but at least the proposed patch might have an implementation specific part in some separate files that wouldn't be committed with this issue.  I suppose this isn't strictly required but it would help. ",
            "id": "comment-15267215"
        },
        {
            "date": "2016-05-02T21:06:22+0000",
            "author": "Hrishikesh Gadre",
            "content": ">>I have a general question about HDFS; I have no real experience with it: I wonder if Java's NIO file abstractions could be used so we don't have to have separate code? If so it would be wonderful \u2013 simpler and less code to maintain. See https://github.com/damiencarol/jsr203-hadoop What do you think?\n\nAlthough integrating HDFS and Java NIO API sounds interesting, I would prefer if it is directly provided by HDFS client library as against a third party library which may/may not be supported in future. Also since Solr provides a HDFS backed Directory implementation, it probably make sense to reuse it.\n\nHowever if we want to keep things simple, we can choose to not provide separate APIs to configure \"repositories\". Instead we can just pick the same file-system used to store the indexed data. That means in case of local file-system, the backup will be stored on shared file-system using SimpleFSDirectory implementation AND for HDFS we will use HdfsDirectory impl. Make sense? ",
            "id": "comment-15267507"
        },
        {
            "date": "2016-05-02T21:58:41+0000",
            "author": "Hrishikesh Gadre",
            "content": ">>Nitpick: In your editor, if it has this feature (IntelliJ does), configure it to strip trailing whitespace only on Modified Lines. IntelliJ: Editor/General/Other \"Strip trailing spaces on Save\".\n\nSorry about that. Let me resubmit the patch without this noise.\n\n>>Does this API enable the possibility of a hard-link based copy (applicable for both backup & restore). It doesn't seem so but I'm unsure?\n\nThe current \"IndexBackupStrategy\" API works at the Overseer level and not at the \"core\" level. Since \"hard-link\" based copy needs to be done at the \"core\" level, it doesn't handle this use-case.\n\n>>Before committing to this API, it would be good to have it implement something useful (HDFS or whatever), otherwise we very well may miss problems with the API \u2013 we probably will. I'm not saying this issue needs to implement HDFS, but at least the proposed patch might have an implementation specific part in some separate files that wouldn't be committed with this issue. I suppose this isn't strictly required but it would help.\n\nMy primary motivation was just to make the code modular (instead of having one gigantic method incorporating all logic). But I agree that delaying the interface definition would probably be better. So I can remove the \"IndexBackupStrategy\" interface and have BackupManager use \"CopyFilesStrategy\" by default. Would that be sufficient? ",
            "id": "comment-15267585"
        },
        {
            "date": "2016-05-03T00:32:49+0000",
            "author": "Hrishikesh Gadre",
            "content": ">>However if we want to keep things simple, we can choose to not provide separate APIs to configure \"repositories\". Instead we can just pick the same file-system used to store the indexed data. That means in case of local file-system, the backup will be stored on shared file-system using SimpleFSDirectory implementation AND for HDFS we will use HdfsDirectory impl. Make sense?\n\nI think the main problem here is identifying type of file-system used for a given collection at the Overseer (The solr core on the other hand already has a Directory factory reference. So we can instantiate appropriate directory in the snapshooter).  ",
            "id": "comment-15267808"
        },
        {
            "date": "2016-05-03T17:12:03+0000",
            "author": "Mark Miller",
            "content": "Let me resubmit the patch without this noise.\n\nCan you attach a cleaned up patch so it's easier to review? ",
            "id": "comment-15269112"
        },
        {
            "date": "2016-05-03T17:20:39+0000",
            "author": "David Smiley",
            "content": "(p.s. use bq. to quote)\n\n(me) I have a general question about HDFS; I have no real experience with it: I wonder if Java's NIO file abstractions could be used so we don't have to have separate code? If so it would be wonderful \u2013 simpler and less code to maintain. See https://github.com/damiencarol/jsr203-hadoop What do you think?\n\n(Gadre) Although integrating HDFS and Java NIO API sounds interesting, I would prefer if it is directly provided by HDFS client library as against a third party library which may/may not be supported in future. Also since Solr provides a HDFS backed Directory implementation, it probably make sense to reuse it.\n\nAny thoughts on this one Mark Miller or Gregory Chanan perhaps?\n\nHowever if we want to keep things simple, we can choose to not provide separate APIs to configure \"repositories\". Instead we can just pick the same file-system used to store the indexed data. That means in case of local file-system, the backup will be stored on shared file-system using SimpleFSDirectory implementation AND for HDFS we will use HdfsDirectory impl. Make sense?\n\nI understand what you mean, but it seems a shame, and loses the extensibility we want.  I think what this comes down to is, should we re-use the Lucene Directory API for moving data in/out of the backup location, or should we use something else. \n\nI think the main problem here is identifying type of file-system used for a given collection at the Overseer (The solr core on the other hand already has a Directory factory reference. So we can instantiate appropriate directory in the snapshooter).\n\nIt was suggested early in SOLR-5750 that the location param should have a protocol/impl scheme URL prefix (assume file:// if not specified).  That may help the Overseer?  Or if you mean it needs to know the directory impl of the live indexes well I imagine it could look this up in the same way that it is done from Solr's admin screen (which shows the impl factory).\n\n\nI doubt I'll have time to help much more here... I'm a bit behind on my work load.\n ",
            "id": "comment-15269135"
        },
        {
            "date": "2016-05-03T18:03:04+0000",
            "author": "Hrishikesh Gadre",
            "content": "It was suggested early in SOLR-5750 that the location param should have a protocol/impl scheme URL prefix (assume file:// if not specified). That may help the Overseer?\n\nI thought about that and the BackupRepositoryFactory implementation (in my patch) is using the the \"scheme\" of the URI to instantiate the correct repository instance. The problem is that the repository implementation may require additional parameters (e.g. S3 credentials, kerberos settings for HDFS etc.) which will also need to be propagated from client to the overseer AND from overseer to the individual cores. We will also need to come up with a mechanism to specify these \"extra\" parameters. Instead of providing such complicated interface to the users, I am thinking to provide a \"registry\" of repositories configured across the cluster. The users will need to configure the registry once and just refer to it by name. In this case Solr will already have all the information necessary to communicate with the repository (which can be a file-system or an object store).\n\nI think what this comes down to is, should we re-use the Lucene Directory API for moving data in/out of the backup location, or should we use something else.\n\nYes I think it is possible to use Lucene Directory implementation without requiring a different \"Repository\" interface. Currently we don't have Directory implementation available for S3 though. Should we do that?\n ",
            "id": "comment-15269224"
        },
        {
            "date": "2016-05-03T18:27:57+0000",
            "author": "Hrishikesh Gadre",
            "content": "Mark Miller Please take a look at the updated patch. ",
            "id": "comment-15269301"
        },
        {
            "date": "2016-05-03T21:40:18+0000",
            "author": "Gregory Chanan",
            "content": "Any thoughts on this one Mark Miller or Gregory Chanan perhaps?\n\nI agree with Hrishikesh's take here.  In addition to the third party library issue, HDFS does not in general support all the local file system APIs you'd expect.  I don't know if there are issues with nio specifically, but two examples in the past have been truncate and directory sync. ",
            "id": "comment-15269647"
        },
        {
            "date": "2016-05-04T13:42:15+0000",
            "author": "Mark Miller",
            "content": "I think Uwe has brought up a similar idea for the current HDFS integration. I think it's certainly worth exploring at some time, but for this issue, I'm also +1 on using our current 'known' approach to interacting with HDFS. ",
            "id": "comment-15270644"
        },
        {
            "date": "2016-05-04T19:49:30+0000",
            "author": "Hrishikesh Gadre",
            "content": "Yes I think it is possible to use Lucene Directory implementation without requiring a different \"Repository\" interface. Currently we don't have Directory implementation available for S3 though. Should we do that?\n\nOK. I will update the patch to use Directory interface (and remove Repository interface). But still I would like to understand how should we proceed with integration with different file-systems? It occurs to me that the \"DirectoryFactory\" configuration in solrconfig.xml can be exposed at a higher level so that it would be useful for both both index management and backup/restore. e.g. consider how HDFS configuration is done today,\nhttps://github.com/cloudera/lucene-solr/blob/25d722e35238cca776abbe3a621e0c5b733e762d/cloudera/solrconfig.xml#L119\n\nIf this is exposed via a separate \"Repository\" API, then solrconfig.xml can also refer to it via user-configurable \"name\". (Please note that some care needs to be taken to allow \"block-cache\" to be configured selectively as backup/restore solution does not need it). This way users can register multiple repositories (e.g. local file-system, HDFS etc.) and choose one of index management and other for backup/restore without duplicate configuration (e.g. one in solrconfig.xml and other as part of \"Repository\" API).\n\nIt seems like a major change though it is a \"correct\" solution. So any feedback on this would be great. ",
            "id": "comment-15271319"
        },
        {
            "date": "2016-05-05T14:23:37+0000",
            "author": "Mark Miller",
            "content": "I don't know that a Repository is a bad idea. A Directory is a very specific object that is made for accessing files for reading and writing Lucene indexes - I don't know that we want to tie that up with simply needing to be able to copy off files to different places. A Repository impl might use Directory impls to do it's work, but I don't know that I buy that Directory is a good replacement for Repository. Many possible Repository locations may not even make reasonable Directory implementations. ",
            "id": "comment-15272434"
        },
        {
            "date": "2016-05-05T16:26:00+0000",
            "author": "Hrishikesh Gadre",
            "content": "Yes that's why I thought to define a separate interface (although there is some redundancy wrt Directory interface). So I am thinking to define a new section i n the solr.xml to configure the backup directories. e.g.\n\n<backup-locations>\n   <backup-location name=\"hdfs\" type=\"solr.HdfsRepository\">\n       <base_location>/solr-backups</base_location>\n       ... (Other params)\n    </backup-location>\n    ...\n</backup-locations>\n\nDuring the backup/restore operation, user can specify \"name\" of the location. In case this parameter is absent, we will use the local file-system implementation for backwards compatibility.\n\nThoughts?\n ",
            "id": "comment-15272576"
        },
        {
            "date": "2016-05-06T15:08:02+0000",
            "author": "Mark Miller",
            "content": "+1, that makes sense to me. ",
            "id": "comment-15274173"
        },
        {
            "date": "2016-05-07T00:38:25+0000",
            "author": "Hrishikesh Gadre",
            "content": "Mark Miller\n\nI successfully refactored the \"core backup\" logic to use the Repository interface. But it looks like during \"restore\" operation we need to use Lucene API to compute the file checksum. This API unfortunately requires the Lucene Directory/IndexInput implementation.\nhttps://github.com/apache/lucene-solr/blob/a5586d29b23f7d032e6d8f0cf8758e56b09e0208/solr/core/src/java/org/apache/solr/handler/RestoreCore.java#L83\n\nIs there any other way to compute the checksum? Since without such support, we will need to implement Directory interface for each type of file-system we want to integrate with (and it makes Repository interface redundant). \n\nAlso I didn't quite understand the logic in the following code-block,\nhttps://github.com/apache/lucene-solr/blob/a5586d29b23f7d032e6d8f0cf8758e56b09e0208/solr/core/src/java/org/apache/solr/handler/RestoreCore.java#L88-L95\n\nWhy would we want to use the files in the \"local\" directory? In case of collection restoration, there will be no files (since we create a new core). I am not sure if I understand the actual problem here...\n\n\n\n ",
            "id": "comment-15274981"
        },
        {
            "date": "2016-05-07T15:29:16+0000",
            "author": "Varun Thacker",
            "content": "Also I didn't quite understand the logic in the following code-block,\n\nYou can restore to an existing core. So for example:\n\n\tCore=CoreX gets backed up\n\tSome docs gets added to CoreX\n\tWe want to restore CoreX to an earlier point.\n\n\n\nIn this scenario there can be lots of segment files which are already present for CoreX . So instead of copying the entire restore folder, this code block tries to optimize and prefers local copy so that the copy is faster theoretically. ",
            "id": "comment-15275275"
        },
        {
            "date": "2016-05-07T16:58:07+0000",
            "author": "Varun Thacker",
            "content": "Should we mark our current backup/restore API as experimental as this patch might need to change the API? It would be safer in-case this doesn't make it to whenever the 6.1 release ( which we don't know when either though ) ",
            "id": "comment-15275305"
        },
        {
            "date": "2016-05-07T17:23:36+0000",
            "author": "Mark Miller",
            "content": "But it looks like during \"restore\" operation we need to use Lucene API to compute the file checksum.\n\nI think it's reading it from the index files, not calculating it? But yeah, I'm sure you could recalculate it, though some files may not even have it.\n\nIt doesn't seem like a full Directory implementation is required - just the ability to read the checksums for a file in the repo (read a header and see if it matches a checksum). And of course an impl could just do the same checksum computation over itself. It should just be used to be sure we don't treat a file with the same name and size as another file the same when the data is actually different. This can happen fairly easily with different Lucene indexes.\n ",
            "id": "comment-15275315"
        },
        {
            "date": "2016-05-07T17:41:21+0000",
            "author": "Uwe Schindler",
            "content": "Yes' you can read it from any index file that has a codec header. The CodecUtil class has methods for it: https://lucene.apache.org/core/6_0_0/core/org/apache/lucene/codecs/CodecUtil.html#retrieveChecksum-org.apache.lucene.store.IndexInput-\n\nTo validate and recalculate, you can also use a method, but this may take long...: https://lucene.apache.org/core/6_0_0/core/org/apache/lucene/codecs/CodecUtil.html#checksumEntireFile-org.apache.lucene.store.IndexInput- ",
            "id": "comment-15275320"
        },
        {
            "date": "2016-05-07T17:44:37+0000",
            "author": "Uwe Schindler",
            "content": "In both cases you don't need a full Directory impl. A IndexInput impl for read access to file is enough. ",
            "id": "comment-15275322"
        },
        {
            "date": "2016-05-09T17:24:15+0000",
            "author": "Hrishikesh Gadre",
            "content": "Uwe Schindler Mark Miller\n\nThanks for the comments. Yes we just need IndexInput and not the entire directory implementation. Let me work on this. ",
            "id": "comment-15276661"
        },
        {
            "date": "2016-05-09T17:35:26+0000",
            "author": "Mark Miller",
            "content": "Yes we just need IndexInput and not the entire directory implementation. \n\nOr even just something that can read the start of files and do the same hash calc, or does the hash calc while copying...but yeah, probably by an IndexInput usually.\n\nI have not had a chance to see why we do this though. I know we do it when replicating because you are copying a lucene index into an existing index. But when backing up, you don't expect any files to already exist do you? Can't you you just ensure the backup is only done to new / empty locations? I have to look at some more context still though, I've only looked at the couple lines pointed at github above. ",
            "id": "comment-15276689"
        },
        {
            "date": "2016-05-09T19:37:04+0000",
            "author": "Hrishikesh Gadre",
            "content": "Varun Thacker Thanks for the comments!\n\nShould we mark our current backup/restore API as experimental as this patch might need to change the API? It would be safer in-case this doesn't make it to whenever the 6.1 release ( which we don't know when either though )\n\nI think it make sense.\n\nBTW I have filed SOLR-9091 to capture various problems with the \"restore\" operation.\n\n\n ",
            "id": "comment-15276881"
        },
        {
            "date": "2016-05-12T04:37:54+0000",
            "author": "Hrishikesh Gadre",
            "content": "Mark Miller I have posted partial patch in SOLR-7374. The primary reason for this split is to keep the patch short and easy to review. I will post the remaining changes as part of this issue soon.\n\nPlease take a look and let me have your feedback. ",
            "id": "comment-15281201"
        },
        {
            "date": "2016-05-12T22:00:44+0000",
            "author": "Mark Miller",
            "content": "Cool, I'll take a look over the next day or two.  ",
            "id": "comment-15282132"
        },
        {
            "date": "2016-05-23T23:35:59+0000",
            "author": "Hrishikesh Gadre",
            "content": "Mark Miller Please find the updated patch. This builds on the patch submitted as part of SOLR-7374. \n\nThis patch implements following,\n\n\tAdded Solr/Lucene version to check the compatibility between the backup version and the version of Solr on which it is being restored.\n\tAdded a backup implementation version to check the compatibility between the \"restore\" implementation and backup format.\n\tIntroduced a Strategy interface to define how the Solr index data is backed up (e.g. using file copy approach)\n\tSolr cloud backup/restore implementation is file-system agnostic.\n\tUnit test added to verify integration with HDFS ( + some unit test refactoring ).\n\n ",
            "id": "comment-15297350"
        },
        {
            "date": "2016-05-24T00:37:09+0000",
            "author": "Mark Miller",
            "content": "Sorry! I got sucked deep into vacation. Tomorrow I spend time on this. ",
            "id": "comment-15297436"
        },
        {
            "date": "2016-05-24T16:06:15+0000",
            "author": "Mark Miller",
            "content": "Hmm...going to try again, but having some trouble compiling after applying the patch. ",
            "id": "comment-15298404"
        },
        {
            "date": "2016-05-24T16:18:39+0000",
            "author": "Mark Miller",
            "content": "Oh wait, sorry, missed the \"builds on\" part. Let me go again. ",
            "id": "comment-15298440"
        },
        {
            "date": "2016-06-23T23:47:48+0000",
            "author": "Hrishikesh Gadre",
            "content": "Mark Miller Varun Thacker Based on our discussion in SOLR-7374, I created SOLR-9242 to track changes required to support collection level backup/restore for other file systems. Once those changes are committed, I will submit another patch here. It would include following,\n\n\n\tAdd Solr/Lucene version to check the compatibility between the backup version and the version of Solr on which it is being restored.\n\tAdd a backup implementation version to check the compatibility between the \"restore\" implementation and backup format.\n\tIntroduce a Strategy interface to define how the Solr index data is backed up (e.g. using file copy approach).\n\n ",
            "id": "comment-15347447"
        },
        {
            "date": "2016-08-10T22:07:06+0000",
            "author": "ASF GitHub Bot",
            "content": "GitHub user hgadre opened a pull request:\n\n    https://github.com/apache/lucene-solr/pull/67\n\n    SOLR-9055 Make collection backup/restore extensible\n\n\n\tIntroduced a Strategy interface to define how the Solr index data is backed up.\n\tTwo concrete implementations of this strategy interface defined.\n\tOne using core Admin API (BACKUPCORE)\n\tOther skipping the backup of index data altogether. This is useful when\n        the index data is copied via an external mechanism in combination with named\n        snapshots (Please refer to SOLR-9038 for details)\n\tIn future we can add additional implementations of this interface (e.g. based on HDFS snapshots etc.)\n\tAdded a backup property to record the Solr version. This helps to check the compatibility\n      of backup with respect to the current version during the restore operation. This\n      compatibility check is not added since its unclear what the Solr level compatibility guidelines\n      are. But at-least having version information as part of the backup would be very useful.\n\n\n\nYou can merge this pull request into a Git repository by running:\n\n    $ git pull https://github.com/hgadre/lucene-solr SOLR-9055_fix\n\nAlternatively you can review and apply these changes as the patch at:\n\n    https://github.com/apache/lucene-solr/pull/67.patch\n\nTo close this pull request, make a commit to your master/trunk branch\nwith (at least) the following in the commit message:\n\n    This closes #67\n\n\ncommit ee07e54a36989637c39b110f1cba19c8af14a0fb\nAuthor: Hrishikesh Gadre <hgadre@cloudera.com>\nDate:   2016-08-10T21:41:12Z\n\n    SOLR-9055 Make collection backup/restore extensible\n\n\n\tIntroduced a Strategy interface to define how the Solr index data is backed up.\n\tTwo concrete implementations of this strategy interface defined.\n\tOne using core Admin API (BACKUPCORE)\n\tOther skipping the backup of index data altogether. This is useful when\n        the index data is copied via an external mechanism in combination with named\n        snapshots (Please refer to SOLR-9038 for details)\n\tIn future we can add additional implementations of this interface (e.g. based on HDFS snapshots etc.)\n\tAdded a backup property to record the Solr version. This helps to check the compatibility\n      of backup with respect to the current version during the restore operation. This\n      compatibility check is not added since its unclear what the Solr level compatibility guidelines\n      are. But at-least having version information as part of the backup would be very useful.\n\n\n\n ",
            "id": "comment-15416107"
        },
        {
            "date": "2016-08-10T22:10:37+0000",
            "author": "Hrishikesh Gadre",
            "content": "Varun Thacker Mark Miller Please take a look at this pull request. I think we should check the compatibility of the backup index version during restore operation. But I am not quite sure about the compatibility guidelines for Solr (which includes Lucene index format + Solr config files + other collection level meta-data). ",
            "id": "comment-15416116"
        },
        {
            "date": "2016-11-04T14:50:03+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1381dd9287a23c950eaaa3c258249a5ebc812f35 in lucene-solr's branch refs/heads/master from markrmiller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1381dd9 ]\n\nSOLR-9055: Make collection backup/restore extensible.\n\n\n\tIntroduced a parameter for the Backup operation to specify index backup strategy.\n\tIntroduced two strategies for backing up index data.\n\tOne using core Admin API (BACKUPCORE)\n\tOther skipping the backup of index data altogether. This is useful when\n    the index data is copied via an external mechanism in combination with named\n    snapshots (Please refer to SOLR-9038 for details)\n\tIn future we can add additional implementations of this interface (e.g. based on HDFS snapshots etc.)\n\tAdded a backup property to record the Solr version. This helps to check the compatibility\n  of backup with respect to the current version during the restore operation. This\n  compatibility check is not added since its unclear what the Solr level compatibility guidelines\n  are. But at-least having version information as part of the backup would be very useful.\n\n ",
            "id": "comment-15636548"
        },
        {
            "date": "2016-11-05T01:47:48+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 03cac8c7b5cb03a0940b1810bcece58466744f26 in lucene-solr's branch refs/heads/branch_6x from markrmiller\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=03cac8c ]\n\nSOLR-9055: Make collection backup/restore extensible.\n\n\n\tIntroduced a parameter for the Backup operation to specify index backup strategy.\n\tIntroduced two strategies for backing up index data.\n\tOne using core Admin API (BACKUPCORE)\n\tOther skipping the backup of index data altogether. This is useful when\n    the index data is copied via an external mechanism in combination with named\n    snapshots (Please refer to SOLR-9038 for details)\n\tIn future we can add additional implementations of this interface (e.g. based on HDFS snapshots etc.)\n\tAdded a backup property to record the Solr version. This helps to check the compatibility\n  of backup with respect to the current version during the restore operation. This\n  compatibility check is not added since its unclear what the Solr level compatibility guidelines\n  are. But at-least having version information as part of the backup would be very useful.\n\n\n\n\n\tConflicts:\n\tsolr/CHANGES.txt\n\n ",
            "id": "comment-15638321"
        },
        {
            "date": "2016-11-05T04:24:49+0000",
            "author": "Mark Miller",
            "content": "Thanks! If you want to discuss enhancing that version check, let's spin it off into a new issue. ",
            "id": "comment-15638572"
        }
    ]
}