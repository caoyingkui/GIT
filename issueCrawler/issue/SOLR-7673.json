{
    "id": "SOLR-7673",
    "title": "Race condition in shard splitting can cause operation to hang indefinitely or sub-shards to never become active",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.3",
            "6.0"
        ],
        "affect_versions": "4.10.4,                                            5.2",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "I found this while digging into the failure on https://builds.apache.org/job/Lucene-Solr-Tests-trunk-Java8/69/\n\n\n   [junit4]   2> 390032 INFO  (OverseerStateUpdate-93987739315601411-127.0.0.1:57926_-n_0000000000) [n:127.0.0.1:57926_    ] o.a.s.c.o.ReplicaMutator Update state numShards=1 message={\n   [junit4]   2>   \"core\":\"testasynccollectioncreation_shard1_0_replica2\",\n   [junit4]   2>   \"core_node_name\":\"core_node5\",\n   [junit4]   2>   \"roles\":null,\n   [junit4]   2>   \"base_url\":\"http://127.0.0.1:38702\",\n   [junit4]   2>   \"node_name\":\"127.0.0.1:38702_\",\n   [junit4]   2>   \"numShards\":\"1\",\n   [junit4]   2>   \"state\":\"active\",\n   [junit4]   2>   \"shard\":\"shard1_0\",\n   [junit4]   2>   \"collection\":\"testasynccollectioncreation\",\n   [junit4]   2>   \"operation\":\"state\"}\n   [junit4]   2> 390033 INFO  (OverseerStateUpdate-93987739315601411-127.0.0.1:57926_-n_0000000000) [n:127.0.0.1:57926_    ] o.a.s.c.o.ZkStateWriter going to update_collection /collections/testasynccollectioncreation/state.json version: 18\n   [junit4]   2> 390033 INFO  (RecoveryThread-testasynccollectioncreation_shard1_0_replica2) [n:127.0.0.1:38702_ c:testasynccollectioncreation s:shard1_0 r:core_node5 x:testasynccollectioncreation_shard1_0_replica2] o.a.s.u.UpdateLog Took 2 ms to seed version buckets with highest version 1503803851028824064\n   [junit4]   2> 390033 INFO  (zkCallback-167-thread-1-processing-n:127.0.0.1:57926_) [n:127.0.0.1:57926_    ] o.a.s.c.c.ZkStateReader A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/collections/testasynccollectioncreation/state.json for collection testasynccollectioncreation has occurred - updating... (live nodes size: 2)\n   [junit4]   2> 390033 INFO  (RecoveryThread-testasynccollectioncreation_shard1_0_replica2) [n:127.0.0.1:38702_ c:testasynccollectioncreation s:shard1_0 r:core_node5 x:testasynccollectioncreation_shard1_0_replica2] o.a.s.c.RecoveryStrategy Finished recovery process.\n   [junit4]   2> 390033 INFO  (zkCallback-173-thread-1-processing-n:127.0.0.1:38702_) [n:127.0.0.1:38702_    ] o.a.s.c.c.ZkStateReader A cluster state change: WatchedEvent state:SyncConnected type:NodeDataChanged path:/collections/testasynccollectioncreation/state.json for collection testasynccollectioncreation has occurred - updating... (live nodes size: 2)\n   [junit4]   2> 390034 INFO  (zkCallback-167-thread-1-processing-n:127.0.0.1:57926_) [n:127.0.0.1:57926_    ] o.a.s.c.c.ZkStateReader Updating data for testasynccollectioncreation to ver 19 \n   [junit4]   2> 390034 INFO  (zkCallback-173-thread-1-processing-n:127.0.0.1:38702_) [n:127.0.0.1:38702_    ] o.a.s.c.c.ZkStateReader Updating data for testasynccollectioncreation to ver 19 \n   [junit4]   2> 390298 INFO  (qtp1859109869-1664) [n:127.0.0.1:38702_    ] o.a.s.h.a.CollectionsHandler Invoked Collection Action :requeststatus with paramsrequestid=1004&action=REQUESTSTATUS&wt=javabin&version=2 \n   [junit4]   2> 390299 INFO  (qtp1859109869-1664) [n:127.0.0.1:38702_    ] o.a.s.s.SolrDispatchFilter [admin] webapp=null path=/admin/collections params={requestid=1004&action=REQUESTSTATUS&wt=javabin&version=2} status=0 QTime=1 \n   [junit4]   2> 390348 INFO  (qtp1859109869-1665) [n:127.0.0.1:38702_    ] o.a.s.h.a.CoreAdminHandler Checking request status for : 10042599422118342586\n   [junit4]   2> 390348 INFO  (qtp1859109869-1665) [n:127.0.0.1:38702_    ] o.a.s.s.SolrDispatchFilter [admin] webapp=null path=/admin/cores params={qt=/admin/cores&requestid=10042599422118342586&action=REQUESTSTATUS&wt=javabin&version=2} status=0 QTime=1 \n   [junit4]   2> 390348 INFO  (OverseerThreadFactory-891-thread-4-processing-n:127.0.0.1:57926_) [n:127.0.0.1:57926_    ] o.a.s.c.OverseerCollectionProcessor Asking sub shard leader to wait for: testasynccollectioncreation_shard1_0_replica2 to be alive on: 127.0.0.1:38702_\n   [junit4]   2> 390349 INFO  (OverseerThreadFactory-891-thread-4-processing-n:127.0.0.1:57926_) [n:127.0.0.1:57926_    ] o.a.s.c.OverseerCollectionProcessor Creating replica shard testasynccollectioncreation_shard1_1_replica2 as part of slice shard1_1 of collection testasynccollectioncreation on 127.0.0.1:38702_\n   [junit4]   2> 390349 INFO  (OverseerThreadFactory-891-thread-4-processing-n:127.0.0.1:57926_) [n:127.0.0.1:57926_    ] o.a.s.c.c.ZkStateReader Load collection config from:/collections/testasynccollectioncreation\n   [junit4]   2> 390350 INFO  (OverseerThreadFactory-891-thread-4-processing-n:127.0.0.1:57926_) [n:127.0.0.1:57926_    ] o.a.s.c.c.ZkStateReader path=/collections/testasynccollectioncreation configName=conf1 specified config exists in ZooKeeper\n   [junit4]   2> 390352 INFO  (qtp381614561-1631) [n:127.0.0.1:57926_    ] o.a.s.s.SolrDispatchFilter [admin] webapp=null path=/admin/cores params={nodeName=127.0.0.1:38702_&core=testasynccollectioncreation_shard1_0_replica1&async=10042599424132346542&qt=/admin/cores&coreNodeName=core_node5&action=PREPRECOVERY&checkLive=true&state=recovering&onlyIfLeader=true&wt=javabin&version=2} status=0 QTime=2 \n\n\n\nThe following sequence of events lead to deadlock:\n\n\ttestasynccollectioncreation_shard1_0_replica2 (core_node5) becomes active\n\tOCP asks sub-shard leader testasynccollectioncreation_shard1_0_replica1  to wait until replica2 is in recovery\n\n\n\nAt this point, the test just keeps pinging for status until timeout and fails.",
    "attachments": {
        "SOLR-7673.patch": "https://issues.apache.org/jira/secure/attachment/12742734/SOLR-7673.patch",
        "Lucene-Solr-Tests-trunk-Java8-69.log": "https://issues.apache.org/jira/secure/attachment/12739334/Lucene-Solr-Tests-trunk-Java8-69.log"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-06-12T21:19:49+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Attaching full log from the jenkins failure. ",
            "id": "comment-14584102"
        },
        {
            "date": "2015-06-12T21:46:56+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "I think there is another race condition. For example, we update the shard state to recovery after adding a replica but if in the meantime the replica becomes active, the slice won't be set to \"active\" anytime soon. ",
            "id": "comment-14584143"
        },
        {
            "date": "2015-06-30T07:40:53+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Here's a patch which fixes both the race conditions.\n\nThis patch creates the replicas in cluster state first (by submitting an addreplica command to the overseer queue), then it calls updateshardstate to set the sub-slices to 'recovery' state and then it invokes addreplica action to actually create the replica cores.\n\nA new flag called \"skipCreateReplicaInClusterState\" is added to the addreplica action so that OCP doesn't try to create the replica in cluster state again. This is a hack to avoid duplicating most of the logic of addreplica inside splitshard and I've put in comments to this effect. I did not want to perform potentially dangerous refactoring as part of this bug fix but once this is committed, we can open a separate issue just to refactor OCP.\n\nI've been beasting this test quite a bit and haven't been able to reproduce the failures. ",
            "id": "comment-14607886"
        },
        {
            "date": "2015-06-30T08:27:52+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1688396 from shalin@apache.org in branch 'dev/trunk'\n[ https://svn.apache.org/r1688396 ]\n\nSOLR-7673: Race condition in shard splitting can cause operation to hang indefinitely or sub-shards to never become active ",
            "id": "comment-14607939"
        },
        {
            "date": "2015-06-30T08:51:08+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1688404 from shalin@apache.org in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1688404 ]\n\nSOLR-7673: Race condition in shard splitting can cause operation to hang indefinitely or sub-shards to never become active ",
            "id": "comment-14607967"
        },
        {
            "date": "2015-08-26T13:06:29+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Bulk close for 5.3.0 release ",
            "id": "comment-14713340"
        }
    ]
}