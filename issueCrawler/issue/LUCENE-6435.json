{
    "id": "LUCENE-6435",
    "title": "java.util.ConcurrentModificationException: Removal from the cache failed error in SimpleNaiveBayesClassifier",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "5.1",
        "components": [
            "modules/classification"
        ],
        "labels": "",
        "fix_versions": [
            "6.0"
        ],
        "priority": "Major",
        "status": "Resolved",
        "type": "Bug"
    },
    "description": "While using SimpleNaiveBayesClassifier\u00a0on a very large index (all Italian Wikipedia articles) I see the following code triggering a ConcurrentModificationException when evicting the Query from the LRUCache.\n\nBooleanQuery booleanQuery = new BooleanQuery();\n    BooleanQuery subQuery = new BooleanQuery();\n    for (String textFieldName : textFieldNames) {\n      subQuery.add(new BooleanClause(new TermQuery(new Term(textFieldName, word)), BooleanClause.Occur.SHOULD));\n    }\n    booleanQuery.add(new BooleanClause(subQuery, BooleanClause.Occur.MUST));\n    booleanQuery.add(new BooleanClause(new TermQuery(new Term(classFieldName, c)), BooleanClause.Occur.MUST));\n    //...\n    TotalHitCountCollector totalHitCountCollector = new TotalHitCountCollector();\n    indexSearcher.search(booleanQuery, totalHitCountCollector);\n    return totalHitCountCollector.getTotalHits();\n\n\n\nthis is the complete stacktrace:\n\njava.util.ConcurrentModificationException: Removal from the cache failed! This is probably due to a query which has been modified after having been put into  the cache or a badly implemented clone(). Query class: [class org.apache.lucene.search.BooleanQuery], query: [#text:panoram #cat:1356]\n\tat __randomizedtesting.SeedInfo.seed([B6513DEC3681FEF5:138235BE33532634]:0)\n\tat org.apache.lucene.search.LRUQueryCache.evictIfNecessary(LRUQueryCache.java:285)\n\tat org.apache.lucene.search.LRUQueryCache.putIfAbsent(LRUQueryCache.java:268)\n\tat org.apache.lucene.search.LRUQueryCache$CachingWrapperWeight.scorer(LRUQueryCache.java:569)\n\tat org.apache.lucene.search.ConstantScoreWeight.scorer(ConstantScoreWeight.java:82)\n\tat org.apache.lucene.search.Weight.bulkScorer(Weight.java:137)\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:560)\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:367)\n\tat org.apache.lucene.classification.SimpleNaiveBayesClassifier.getWordFreqForClass(SimpleNaiveBayesClassifier.java:288)\n\tat org.apache.lucene.classification.SimpleNaiveBayesClassifier.calculateLogLikelihood(SimpleNaiveBayesClassifier.java:248)\n\tat org.apache.lucene.classification.SimpleNaiveBayesClassifier.assignClassNormalizedList(SimpleNaiveBayesClassifier.java:169)\n\tat org.apache.lucene.classification.SimpleNaiveBayesClassifier.assignClass(SimpleNaiveBayesClassifier.java:125)\n\tat org.apache.lucene.classification.WikipediaTest.testItalianWikipedia(WikipediaTest.java:126)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1627)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:836)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:872)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:886)\n\tat org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)\n\tat org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)\n\tat org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)\n\tat org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)\n\tat org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\tat com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)\n\tat com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)\n\tat com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:845)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:747)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:781)\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:792)\n\tat org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\tat org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)\n\tat com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)\n\tat com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\tat org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)\n\tat org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)\n\tat org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)\n\tat org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\n\tat com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\n\nThe strange thing is that the above doesn't happen if I change the last lines of the above piece of code to not use the TotalHitCountsCollector:\n\nreturn indexSearcher.search(booleanQuery, 1).totalHits;",
    "attachments": {
        "LUCENE-6435.patch": "https://issues.apache.org/jira/secure/attachment/12764236/LUCENE-6435.patch",
        "patch.rtf": "https://issues.apache.org/jira/secure/attachment/12756190/patch.rtf"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14499938",
            "author": "Adrien Grand",
            "date": "2015-04-17T14:50:32+0000",
            "content": "Hi Tommaso, the reason why it does not fail with indexSearcher.search(booleanQuery, 1).totalHits is that in that case you are computing scores so caching does not kick in. The above exception means that somehow you executed a query against an indexsearcher, then modified it (eg. by adding clauses). What happens under the hood is that after the first search operation, the query is used as a cache key but then the changes changed the hashcode which made the eviction from the query cache impossible. (This is one of the motivations to make queries immutable.) "
        },
        {
            "id": "comment-14718530",
            "author": "Tommaso Teofili",
            "date": "2015-08-28T13:07:23+0000",
            "content": "thanks Adrien Grand for the explanation, however in this specific case I can't see why the Exception is raised.\nThe query is created, executed, the hit counts are fetched all within the same method and the query object is never reused again.\nTo me it seems it should work. "
        },
        {
            "id": "comment-14727119",
            "author": "Adrien Grand",
            "date": "2015-09-02T10:07:02+0000",
            "content": "Do you have a test case that reproduces the issue that I could look into? "
        },
        {
            "id": "comment-14728521",
            "author": "Tommaso Teofili",
            "date": "2015-09-03T05:20:16+0000",
            "content": "enable SNCTest#testPerformance  (comment @Ignore) and you should see that happening (it performs classification evaluation on 100 randomly generated indexed docs). "
        },
        {
            "id": "comment-14747081",
            "author": "Chang KaiShin",
            "date": "2015-09-16T07:28:08+0000",
            "content": "The hashcode of class TermQuery doesn't remain consistent that cause the iterator.remove() failing.\nI'll post the patch later time "
        },
        {
            "id": "comment-14747126",
            "author": "Chang KaiShin",
            "date": "2015-09-16T08:23:25+0000",
            "content": "By running the JUnit test in debugging mode , I get the ConcurrentModificationException - Removal from the cache failed! This is probably due to a query which has been modified after having been put into the cache or a badly implemented clone(). As the description suggests, I started looking into the LRUQueryCache class that contains the most recently used queries and find out that that class TermQuery hashCode()  function doesn't remain the same value. So that the HashMap(mostRecentlyUsedQueries) is unable to locate the index to remove it from the cache. However , I'm not diving into the question why TermQuery hashCode()  function failed to remain the save value.   "
        },
        {
            "id": "comment-14747291",
            "author": "Tommaso Teofili",
            "date": "2015-09-16T10:53:36+0000",
            "content": "with the patch provided by Chang KaiShin the SNCTest#testPerformance test passes for me, by the way I'd be curious to understand why/how the patch solves the problem.  "
        },
        {
            "id": "comment-14935064",
            "author": "Tommaso Teofili",
            "date": "2015-09-29T12:09:58+0000",
            "content": "Adrien Grand what do you think of the attached patch to TermQuery ? Could this be affecting also other types of queries ? "
        },
        {
            "id": "comment-14935074",
            "author": "Adrien Grand",
            "date": "2015-09-29T12:15:09+0000",
            "content": "Woops sorry I had lost track of this issue. The patch seems to imply that there is something that changes the boost after the query has been passed to IndexSearcher. I'll have a deeper look. "
        },
        {
            "id": "comment-14935095",
            "author": "Adrien Grand",
            "date": "2015-09-29T12:36:45+0000",
            "content": "OK I found the problem: the classifier builds term queries that are built out of a Term that comes from a TermsEnum that reuses bytes. Here is a proposed fix. "
        },
        {
            "id": "comment-14935108",
            "author": "Tommaso Teofili",
            "date": "2015-09-29T12:46:50+0000",
            "content": "now I see Adrien, thanks a lot, the patch looks good and makes the test pass.\nMore generally should such code pattern be avoided / discouraged or should we mitigate that on the query caching side ? "
        },
        {
            "id": "comment-14935113",
            "author": "ASF subversion and git services",
            "date": "2015-09-29T12:49:42+0000",
            "content": "Commit 1705850 from Tommaso Teofili in branch 'dev/trunk'\n[ https://svn.apache.org/r1705850 ]\n\nLUCENE-6435 - applied Adrien Grand's patch to avoid CME on SNBC "
        },
        {
            "id": "comment-14935120",
            "author": "Adrien Grand",
            "date": "2015-09-29T12:54:01+0000",
            "content": "This is a good question. I opened LUCENE-6821 which is another way to fix this issue (which would also remove the trap). "
        },
        {
            "id": "comment-14939428",
            "author": "Tommaso Teofili",
            "date": "2015-10-01T07:23:57+0000",
            "content": "thanks Adrien for following up LUCENE-6821, I'll mark this as resolved as the classification specific issue has been fixed. "
        },
        {
            "id": "comment-14940825",
            "author": "Chang KaiShin",
            "date": "2015-10-02T06:47:55+0000",
            "content": "Good to hear the problem solved! "
        }
    ]
}