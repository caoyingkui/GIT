{
    "id": "LUCENE-5269",
    "title": "TestRandomChains failure",
    "details": {
        "components": [],
        "fix_versions": [
            "4.5.1",
            "4.6",
            "6.0"
        ],
        "affect_versions": "None",
        "priority": "Major",
        "labels": "",
        "type": "Bug",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "One of EdgeNGramTokenizer, ShingleFilter, NGramTokenFilter is buggy, or possibly only the combination of them conspiring together.",
    "attachments": {
        "LUCENE-5269.patch": "https://issues.apache.org/jira/secure/attachment/12607683/LUCENE-5269.patch",
        "LUCENE-5269_test.patch": "https://issues.apache.org/jira/secure/attachment/12607667/LUCENE-5269_test.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2013-10-09T22:07:12+0000",
            "content": "Here's a test. For whatever reason the exact text in jenkins wouldnt reproduce with checkAnalysisConsistency with the exact configuration.\n\nHowever the random seed reproduces in jenkins easily. I suspect maybe there is something not reset and the linedocs file is triggering it???\n\nIf i blast random data at the configuration it fails the same way.\n\nI then removed various harmless filters and so on until I was left with these three and it was still failing... ",
            "author": "Robert Muir",
            "id": "comment-13790911"
        },
        {
            "date": "2013-10-09T22:35:35+0000",
            "content": "extremely noisy version of the same test ",
            "author": "Robert Muir",
            "id": "comment-13790931"
        },
        {
            "date": "2013-10-09T22:40:56+0000",
            "content": "with SopFilter 2.0 ",
            "author": "Robert Muir",
            "id": "comment-13790938"
        },
        {
            "date": "2013-10-09T22:42:41+0000",
            "content": "Now i see stuff like this:\n\nEdgeNGramTokenizer.reset()\nShingleFilter.reset()\nNGramTokenFilter.reset()\nEdgeNGramTokenizer->term=\u295e ,bytes=[e2 a5 9e 20],positionIncrement=1,positionLength=1,startOffset=0,endOffset=2,type=word,clearCalled=true\nEdgeNGramTokenizer->term=\u295e \ud802\udd0b,bytes=[e2 a5 9e 20 f0 90 a4 8b],positionIncrement=1,positionLength=1,startOffset=0,endOffset=4,type=word,clearCalled=true\nEdgeNGramTokenizer->term=\u295e \ud802\udd0b\ud802\udd1f,bytes=[e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f],positionIncrement=1,positionLength=1,startOffset=0,endOffset=6,type=word,clearCalled=true\nEdgeNGramTokenizer->term=\u295e \ud802\udd0b\ud802\udd1f ,bytes=[e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20],positionIncrement=1,positionLength=1,startOffset=0,endOffset=7,type=word,clearCalled=true\nEdgeNGramTokenizer->term=\u295e \ud802\udd0b\ud802\udd1f x,bytes=[e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78],positionIncrement=1,positionLength=1,startOffset=0,endOffset=8,type=word,clearCalled=true\nEdgeNGramTokenizer->term=\u295e \ud802\udd0b\ud802\udd1f xq,bytes=[e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71],positionIncrement=1,positionLength=1,startOffset=0,endOffset=9,type=word,clearCalled=true\nEdgeNGramTokenizer->term=\u295e \ud802\udd0b\ud802\udd1f xqx,bytes=[e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71 78],positionIncrement=1,positionLength=1,startOffset=0,endOffset=10,type=word,clearCalled=true\nEdgeNGramTokenizer->term=\u295e \ud802\udd0b\ud802\udd1f xqxp,bytes=[e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71 78 70],positionIncrement=1,positionLength=1,startOffset=0,endOffset=11,type=word,clearCalled=true\nEdgeNGramTokenizer->term=\u295e \ud802\udd0b\ud802\udd1f xqxp ,bytes=[e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71 78 70 20],positionIncrement=1,positionLength=1,startOffset=0,endOffset=12,type=word,clearCalled=true\nEdgeNGramTokenizer->term=\u295e \ud802\udd0b\ud802\udd1f xqxp \u0016,bytes=[e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71 78 70 20 16],positionIncrement=1,positionLength=1,startOffset=0,endOffset=13,type=word,clearCalled=true\nEdgeNGramTokenizer.end()\nShingleFilter->term=\u295e ,bytes=[e2 a5 9e 20],positionIncrement=1,positionLength=1,startOffset=0,endOffset=2,type=word,clearCalled=true\nShingleFilter->term=\u295e  \u295e \ud802\udd0b,bytes=[e2 a5 9e 20 20 e2 a5 9e 20 f0 90 a4 8b],positionIncrement=0,positionLength=2,startOffset=0,endOffset=4,type=shingle,clearCalled=true\nShingleFilter->term=\u295e  \u295e \ud802\udd0b \u295e \ud802\udd0b\ud802\udd1f,bytes=[e2 a5 9e 20 20 e2 a5 9e 20 f0 90 a4 8b 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f],positionIncrement=0,positionLength=3,startOffset=0,endOffset=6,type=shingle,clearCalled=true\nShingleFilter->term=\u295e  \u295e \ud802\udd0b \u295e \ud802\udd0b\ud802\udd1f \u295e \ud802\udd0b\ud802\udd1f ,bytes=[e2 a5 9e 20 20 e2 a5 9e 20 f0 90 a4 8b 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20],positionIncrement=0,positionLength=4,startOffset=0,endOffset=7,type=shingle,clearCalled=true\nShingleFilter->term=\u295e  \u295e \ud802\udd0b \u295e \ud802\udd0b\ud802\udd1f \u295e \ud802\udd0b\ud802\udd1f  \u295e \ud802\udd0b\ud802\udd1f x,bytes=[e2 a5 9e 20 20 e2 a5 9e 20 f0 90 a4 8b 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78],positionIncrement=0,positionLength=5,startOffset=0,endOffset=8,type=shingle,clearCalled=true\nShingleFilter->term=\u295e  \u295e \ud802\udd0b \u295e \ud802\udd0b\ud802\udd1f \u295e \ud802\udd0b\ud802\udd1f  \u295e \ud802\udd0b\ud802\udd1f x \u295e \ud802\udd0b\ud802\udd1f xq,bytes=[e2 a5 9e 20 20 e2 a5 9e 20 f0 90 a4 8b 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71],positionIncrement=0,positionLength=6,startOffset=0,endOffset=9,type=shingle,clearCalled=true\nShingleFilter->term=\u295e  \u295e \ud802\udd0b \u295e \ud802\udd0b\ud802\udd1f \u295e \ud802\udd0b\ud802\udd1f  \u295e \ud802\udd0b\ud802\udd1f x \u295e \ud802\udd0b\ud802\udd1f xq \u295e \ud802\udd0b\ud802\udd1f xqx,bytes=[e2 a5 9e 20 20 e2 a5 9e 20 f0 90 a4 8b 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71 78],positionIncrement=0,positionLength=7,startOffset=0,endOffset=10,type=shingle,clearCalled=true\nShingleFilter->term=\u295e  \u295e \ud802\udd0b \u295e \ud802\udd0b\ud802\udd1f \u295e \ud802\udd0b\ud802\udd1f  \u295e \ud802\udd0b\ud802\udd1f x \u295e \ud802\udd0b\ud802\udd1f xq \u295e \ud802\udd0b\ud802\udd1f xqx \u295e \ud802\udd0b\ud802\udd1f xqxp,bytes=[e2 a5 9e 20 20 e2 a5 9e 20 f0 90 a4 8b 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71 78 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71 78 70],positionIncrement=0,positionLength=8,startOffset=0,endOffset=11,type=shingle,clearCalled=true\nShingleFilter->term=\u295e  \u295e \ud802\udd0b \u295e \ud802\udd0b\ud802\udd1f \u295e \ud802\udd0b\ud802\udd1f  \u295e \ud802\udd0b\ud802\udd1f x \u295e \ud802\udd0b\ud802\udd1f xq \u295e \ud802\udd0b\ud802\udd1f xqx \u295e \ud802\udd0b\ud802\udd1f xqxp \u295e \ud802\udd0b\ud802\udd1f xqxp ,bytes=[e2 a5 9e 20 20 e2 a5 9e 20 f0 90 a4 8b 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71 78 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71 78 70 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71 78 70 20],positionIncrement=0,positionLength=9,startOffset=0,endOffset=12,type=shingle,clearCalled=true\nNGramTokenFilter->term=\u295e  \u295e \ud802\udd0b \u295e \ud802\udd0b\ud802\udd1f \u295e \ud802\udd0b\ud802\udd1f  \u295e \ud802\udd0b\ud802\udd1f x \u295e \ud802\udd0b\ud802\udd1f xq \u295e \ud802\udd0b\ud802\udd1f xqx \u295e \ud802\udd0b\ud802\udd1f xqxp \u295e \ud802\udd0b,bytes=[e2 a5 9e 20 20 e2 a5 9e 20 f0 90 a4 8b 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71 78 20 e2 a5 9e 20 f0 90 a4 8b f0 90 a4 9f 20 78 71 78 70 20 e2 a5 9e 20 f0 90 a4 8b],positionIncrement=0,positionLength=9,startOffset=0,endOffset=12,type=word,clearCalled=true\nTEST FAIL: useCharFilter=false text='\\u295e \\u1090b\\u1091f xqxp \\u0016'\n\njava.lang.AssertionError: first posIncrement must be >= 1\n\tat __randomizedtesting.SeedInfo.seed([6CC8BD35A010E1FF:714032FA1B8FBB60]:0)\n\n ",
            "author": "Robert Muir",
            "id": "comment-13790940"
        },
        {
            "date": "2013-10-09T23:13:06+0000",
            "content": "Mike spotted the bug. Here is a hack patch.\n\nI will add optimization, tests, and factory. ",
            "author": "Robert Muir",
            "id": "comment-13790978"
        },
        {
            "date": "2013-10-09T23:14:07+0000",
            "content": "with 'svn add' ",
            "author": "Robert Muir",
            "id": "comment-13790980"
        },
        {
            "date": "2013-10-10T00:39:01+0000",
            "content": "Cleaned up patch.\n\nI also tried to enhance ngrams tests in general (these filters had offsets checks disabled, always hardcoded certain parameters, etc).\n\n@jpountz was this intentional? Can you review if you get a chance? ",
            "author": "Robert Muir",
            "id": "comment-13791052"
        },
        {
            "date": "2013-10-10T16:51:22+0000",
            "content": "Good catch. This was definitely not intentional, thanks for fixing those tests!\n\nPatch looks good to me! ",
            "author": "Adrien Grand",
            "id": "comment-13791661"
        },
        {
            "date": "2013-10-11T03:53:44+0000",
            "content": "Commit 1531186 from Robert Muir in branch 'dev/trunk'\n[ https://svn.apache.org/r1531186 ]\n\nLUCENE-5269: Fix NGramTokenFilter length filtering ",
            "author": "ASF subversion and git services",
            "id": "comment-13792311"
        },
        {
            "date": "2013-10-11T04:30:39+0000",
            "content": "The test needs some improvement... after backporting i ran tests about 30 times, and I hit this one:\n\nant test  -Dtestcase=TestBugInSomething -Dtests.method=testUnicodeShinglesAndNgrams -Dtests.seed=1BFA8BADE39EDF70 -Dtests.slow=true -Dtests.locale=th_TH_TH_#u-nu-thai -Dtests.timezone=Europe/Copenhagen -Dtests.file.encoding=US-ASCII\n\n\n   [junit4] Suite: org.apache.lucene.analysis.core.TestBugInSomething\n   [junit4]   2> TEST FAIL: useCharFilter=true text='ike to thank the rap'\n   [junit4]   2> ?.?. ??, ???? ?:??:?? ?????????? com.carrotsearch.randomizedtesting.RandomizedRunner$QueueUncaughtExceptionsHandler uncaughtException\n   [junit4]   2> WARNING: Uncaught exception in thread: Thread[Thread-2,5,TGRP-TestBugInSomething]\n   [junit4]   2> java.lang.OutOfMemoryError: GC overhead limit exceeded\n   [junit4]   2> \tat __randomizedtesting.SeedInfo.seed([1BFA8BADE39EDF70]:0)\n   [junit4]   2> \tat org.apache.lucene.analysis.tokenattributes.CharTermAttributeImpl.toString(CharTermAttributeImpl.java:269)\n   [junit4]   2> \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:696)\n   [junit4]   2> \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:605)\n   [junit4]   2> \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.access$000(BaseTokenStreamTestCase.java:57)\n   [junit4]   2> \tat org.apache.lucene.analysis.BaseTokenStreamTestCase$AnalysisThread.run(BaseTokenStreamTestCase.java:476)\n   [junit4]   2> \n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestBugInSomething -Dtests.method=testUnicodeShinglesAndNgrams -Dtests.seed=1BFA8BADE39EDF70 -Dtests.slow=true -Dtests.locale=th_TH_TH_#u-nu-thai -Dtests.timezone=Europe/Copenhagen -Dtests.file.encoding=US-ASCII\n   [junit4] ERROR   30.6s | TestBugInSomething.testUnicodeShinglesAndNgrams <<<\n   [junit4]    > Throwable #1: java.lang.RuntimeException: some thread(s) failed\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:526)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:428)\n   [junit4]    > \tat org.apache.lucene.analysis.core.TestBugInSomething.testUnicodeShinglesAndNgrams(TestBugInSomething.java:255)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:724)Throwable #2: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=12, name=Thread-2, state=RUNNABLE, group=TGRP-TestBugInSomething]\n   [junit4]    > Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([1BFA8BADE39EDF70]:0)\n   [junit4]    > \tat org.apache.lucene.analysis.tokenattributes.CharTermAttributeImpl.toString(CharTermAttributeImpl.java:269)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:696)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:605)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.access$000(BaseTokenStreamTestCase.java:57)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase$AnalysisThread.run(BaseTokenStreamTestCase.java:476)\n   [junit4]   2> NOTE: test params are: codec=DummyCompressingStoredFields(storedFieldsFormat=CompressingStoredFieldsFormat(compressionMode=DUMMY, chunkSize=313), termVectorsFormat=CompressingTermVectorsFormat(compressionMode=DUMMY, chunkSize=313)), sim=RandomSimilarityProvider(queryNorm=true,coord=crazy): {}, locale=th_TH_TH_#u-nu-thai, timezone=Europe/Copenhagen\n   [junit4]   2> NOTE: Linux 3.5.0-27-generic amd64/Oracle Corporation 1.7.0_25 (64-bit)/cpus=8,threads=1,free=155107808,total=477233152\n   [junit4]   2> NOTE: All tests run in this JVM: [TestBugInSomething]\n   [junit4] Completed in 30.92s, 1 test, 1 error <<< FAILURES!\n   [junit4] \n   [junit4] \n   [junit4] Tests with failures:\n   [junit4]   - org.apache.lucene.analysis.core.TestBugInSomething.testUnicodeShinglesAndNgrams\n\n\n\nI will see if i can make a less-ridiculous version of the test that still fails with the bug. ",
            "author": "Robert Muir",
            "id": "comment-13792325"
        },
        {
            "date": "2013-10-11T04:38:59+0000",
            "content": "Commit 1531193 from Robert Muir in branch 'dev/trunk'\n[ https://svn.apache.org/r1531193 ]\n\nLUCENE-5269: make test use less RAM ",
            "author": "ASF subversion and git services",
            "id": "comment-13792330"
        },
        {
            "date": "2013-10-11T04:55:37+0000",
            "content": "Commit 1531195 from Robert Muir in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1531195 ]\n\nLUCENE-5269: Fix NGramTokenFilter length filtering ",
            "author": "ASF subversion and git services",
            "id": "comment-13792334"
        },
        {
            "date": "2013-10-11T06:02:40+0000",
            "content": "Commit 1531202 from Robert Muir in branch 'dev/branches/lucene_solr_4_5'\n[ https://svn.apache.org/r1531202 ]\n\nLUCENE-5269: Fix NGramTokenFilter length filtering ",
            "author": "ASF subversion and git services",
            "id": "comment-13792375"
        },
        {
            "date": "2013-10-11T06:42:22+0000",
            "content": "This is so crazy! Why did we never hit this combination before?\n\nThanks for fixing, although I see the CodePointLengthFilter not really as a bug fix, it is more a new feature! Maybe explicitely add this as \"new feature\" to changes.txt? ",
            "author": "Uwe Schindler",
            "id": "comment-13792402"
        },
        {
            "date": "2013-10-11T07:15:05+0000",
            "content": "I didnt want new features mixed with bugfixes really \n\nBut in my opinion this was the simplest way to solve the problem: to just add a filter like this and for it to use that instead of LengthFilter.\n\nI think it would be wierd to see \"new features\" in a 4.5.1? ",
            "author": "Robert Muir",
            "id": "comment-13792424"
        },
        {
            "date": "2013-10-11T07:27:43+0000",
            "content": "\nThis is so crazy! Why did we never hit this combination before?\n\nThis combination is especially good at finding the bug, here's why:\n\nTokenizer tokenizer = new EdgeNGramTokenizer(TEST_VERSION_CURRENT, reader, 2, 94);\nTokenStream stream = new ShingleFilter(tokenizer, 5);\nstream = new NGramTokenFilter(TEST_VERSION_CURRENT, stream, 55, 83);\n\n\n\nThe edge-ngram has min=2 max=94, its basically brute forcing every token size.\nthen the shingles makes tons of tokens with positionIncrement=0.\nso it makes it easy for the (previously buggy ngramtokenfilter with wrong length filter) to misclassify tokens with its logic expecting codepoints, emit an initial token with posinc=0:\n\n\nif ((curPos + curGramSize) <= curCodePointCount) {\n...\n          posIncAtt.setPositionIncrement(curPosInc);\n\n ",
            "author": "Robert Muir",
            "id": "comment-13792429"
        },
        {
            "date": "2013-10-11T08:30:41+0000",
            "content": "I didnt want new features mixed with bugfixes really \n\nI agree! But now we have the \"new feature\", so I just asked to add this as a separate entry in CHANGES.txt under \"New features\", just the new filter nothing more. ",
            "author": "Uwe Schindler",
            "id": "comment-13792461"
        },
        {
            "date": "2013-10-11T17:30:26+0000",
            "content": "Commit 1531368 from Robert Muir in branch 'dev/trunk'\n[ https://svn.apache.org/r1531368 ]\n\nLUCENE-5269: satisfy the policeman ",
            "author": "ASF subversion and git services",
            "id": "comment-13792845"
        },
        {
            "date": "2013-10-11T17:30:49+0000",
            "content": "Commit 1531369 from Robert Muir in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1531369 ]\n\nLUCENE-5269: satisfy the policeman ",
            "author": "ASF subversion and git services",
            "id": "comment-13792846"
        }
    ]
}