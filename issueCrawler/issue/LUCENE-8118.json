{
    "id": "LUCENE-8118",
    "title": "ArrayIndexOutOfBoundsException in TermsHashPerField.writeByte during indexing",
    "details": {
        "labels": "",
        "priority": "Major",
        "resolution": "Unresolved",
        "affect_versions": "7.2",
        "status": "Open",
        "type": "Bug",
        "components": [
            "core/index"
        ],
        "fix_versions": []
    },
    "description": "Indexing a large collection of about 20 million paragraph-sized documents results in an ArrayIndexOutOfBoundsException in org.apache.lucene.index.TermsHashPerField.writeByte  (full stack trace below). \n\n\nThe bug is possibly related to issues described in here  and SOLR-10936 \u2013 but I am not using SOLR, I am directly using Lucene Core.\n\nThe issue can be reproduced using code from  GitHub trec-car-tools-example \n\n\n\tcompile with `mvn compile assembly:single`\n\trun with `java -cp ./target/treccar-tools-example-0.1-jar-with-dependencies.jar edu.unh.cs.TrecCarBuildLuceneIndex paragraphs paragraphCorpus.cbor indexDir`\n\n\n\nWhere paragraphCorpus.cbor is contained in this archive\n\n\n\nException in thread \"main\" java.lang.ArrayIndexOutOfBoundsException: -65536                                                                           at org.apache.lucene.index.TermsHashPerField.writeByte(TermsHashPerField.java:198)                                                                                                                             at org.apache.lucene.index.TermsHashPerField.writeVInt(TermsHashPerField.java:224)                                                                                                                             at org.apache.lucene.index.FreqProxTermsWriterPerField.addTerm(FreqProxTermsWriterPerField.java:159)                                                                                                           at org.apache.lucene.index.TermsHashPerField.add(TermsHashPerField.java:185)                                                                                                                                   at org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:786)                                                                                                                 at org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:430)                                                                                                                    at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:392)                                                                                                                 at org.apache.lucene.index.DocumentsWriterPerThread.updateDocuments(DocumentsWriterPerThread.java:281)                                                                                                         at org.apache.lucene.index.DocumentsWriter.updateDocuments(DocumentsWriter.java:451)                                                                                                                           at org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1532)                                                                                                                                  at org.apache.lucene.index.IndexWriter.addDocuments(IndexWriter.java:1508)\n        at edu.unh.cs.TrecCarBuildLuceneIndex.main(TrecCarBuildLuceneIndex.java:55)",
    "attachments": {
        "LUCENE-8118_test.patch": "https://issues.apache.org/jira/secure/attachment/12904819/LUCENE-8118_test.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16312971",
            "date": "2018-01-05T11:15:45+0000",
            "content": "Looking at your code it seems that there is only one commit at the end, and your collection is big. What if you try to commit every, let's say, 50k docs?   ",
            "author": "Diego Ceccarelli"
        },
        {
            "id": "comment-16313256",
            "date": "2018-01-05T15:02:54+0000",
            "content": "Yes, that works - Thanks, Diego!\n\nI think I could have been helped with an Exception message that indicates \"Buffer full, call index.commit!\"\n\n ",
            "author": "Laura Dietz"
        },
        {
            "id": "comment-16313271",
            "date": "2018-01-05T15:07:11+0000",
            "content": "Issuing unnecessary commits is just masking the issue: you shouldn't see this exception. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16313284",
            "date": "2018-01-05T15:18:13+0000",
            "content": "I agree, that was just a workaround for Laura Dietz   ",
            "author": "Diego Ceccarelli"
        },
        {
            "id": "comment-16313292",
            "date": "2018-01-05T15:20:37+0000",
            "content": "Robert, that would be even better!\n\nIt is difficult to guess what the right interval of issuing a commits is. I understand that some hand tuning might be necessary to get the highest performance for given resource constraints. If the issue is a buffer that is filling up, it would be helpful to have some form of an emergency auto-commit. ",
            "author": "Laura Dietz"
        },
        {
            "id": "comment-16313299",
            "date": "2018-01-05T15:24:50+0000",
            "content": "It is nothing like that, it is simply a bug. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16313318",
            "date": "2018-01-05T15:33:25+0000",
            "content": "Well, I understand the bug, but not sure what the fix is.\n\nIndexing code implements Iterable etc to pull in the docs, and makes one single call to addDocuments().\n\nThis is supposed to be an \"atomic add\" of multiple documents at once which gives certain guarantees: needed for nested documents and features like that so they document IDs will be aligned in a particular way.\n\nIn your case, its too much data, IndexWriter isn't going to be able to do 200M docs in one operation like this. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16313322",
            "date": "2018-01-05T15:35:55+0000",
            "content": "whatever we decide to do, we can be sure that AIOOBE is not the right answer  ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16313323",
            "date": "2018-01-05T15:36:26+0000",
            "content": "I think my mistake was to abuse addDocuments(iterator).\n\nI switched to addDocument(doc) with a commit every so often (see master branch) ",
            "author": "Laura Dietz"
        },
        {
            "id": "comment-16313337",
            "date": "2018-01-05T15:47:12+0000",
            "content": "yeah, but we still need to fix the case where someone passes too many documents for addDocuments to succeed: it needs to be better than AIOOBE. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16313391",
            "date": "2018-01-05T16:31:32+0000",
            "content": "Here's a really bad test, but it works (takes about 2 minutes). \n\nlucene/core$ ant test -Dtestcase=TestIndexWriter -Dtestmethod=testAddDocumentsMassive -Dtests.heapsize=4G\n\n\n  [junit4] <JUnit4> says \u0645\u0631\u062d\u0628\u0627! Master seed: 1655BF16A8843A6A\n   [junit4] Executing 1 suite with 1 JVM.\n   [junit4] \n   [junit4] Started J0 PID(22813@localhost).\n   [junit4] Suite: org.apache.lucene.index.TestIndexWriter\n   [junit4] HEARTBEAT J0 PID(22813@localhost): 2018-01-05T11:27:48, stalled for 71.2s at: TestIndexWriter.testAddDocumentsMassive\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestIndexWriter -Dtests.method=testAddDocumentsMassive -Dtests.seed=1655BF16A8843A6A -Dtests.locale=fr-FR -Dtests.timezone=Asia/Oral -Dtests.asserts=true -Dtests.file.encoding=UTF-8\n   [junit4] ERROR    121s | TestIndexWriter.testAddDocumentsMassive <<<\n   [junit4]    > Throwable #1: java.lang.ArrayIndexOutOfBoundsException: -65536\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([1655BF16A8843A6A:2B0B86082D338FEA]:0)\n   [junit4]    > \tat org.apache.lucene.index.TermsHashPerField.writeByte(TermsHashPerField.java:198)\n   [junit4]    > \tat org.apache.lucene.index.TermsHashPerField.writeVInt(TermsHashPerField.java:224)\n   [junit4]    > \tat org.apache.lucene.index.FreqProxTermsWriterPerField.writeProx(FreqProxTermsWriterPerField.java:80)\n   [junit4]    > \tat org.apache.lucene.index.FreqProxTermsWriterPerField.addTerm(FreqProxTermsWriterPerField.java:171)\n   [junit4]    > \tat org.apache.lucene.index.TermsHashPerField.add(TermsHashPerField.java:185)\n   [junit4]    > \tat org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:786)\n   [junit4]    > \tat org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:430)\n   [junit4]    > \tat org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:392)\n   [junit4]    > \tat org.apache.lucene.index.DocumentsWriterPerThread.updateDocuments(DocumentsWriterPerThread.java:281)\n   [junit4]    > \tat org.apache.lucene.index.DocumentsWriter.updateDocuments(DocumentsWriter.java:452)\n   [junit4]    > \tat org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1530)\n   [junit4]    > \tat org.apache.lucene.index.IndexWriter.addDocuments(IndexWriter.java:1506)\n   [junit4]    > \tat org.apache.lucene.index.TestIndexWriter.testAddDocumentsMassive(TestIndexWriter.java:2994)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]   2> NOTE: leaving temporary files on disk at: /home/rmuir/workspace/lucene-solr/lucene/build/core/test/J0/temp/lucene.index.TestIndexWriter_1655BF16A8843A6A-001\n   [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70), sim=Asserting(org.apache.lucene.search.similarities.AssertingSimilarity@2a213e8b), locale=fr-FR, timezone=Asia/Oral\n   [junit4]   2> NOTE: Linux 4.4.0-104-generic amd64/Oracle Corporation 1.8.0_45 (64-bit)/cpus=8,threads=1,free=543468648,total=2733637632\n   [junit4]   2> NOTE: All tests run in this JVM: [TestIndexWriter]\n   [junit4] Completed [1/1 (1!)] in 121.55s, 1 test, 1 error <<< FAILURES!\n   [junit4] \n   [junit4] \n   [junit4] Tests with failures [seed: 1655BF16A8843A6A]:\n   [junit4]   - org.apache.lucene.index.TestIndexWriter.testAddDocumentsMassive\n   [junit4] \n   [junit4] \n   [junit4] JVM J0:     0.38 ..   122.56 =   122.18s\n   [junit4] Execution time total: 2 minutes 2 seconds\n   [junit4] Tests summary: 1 suite, 1 test, 1 error\n\nBUILD FAILED\n/home/rmuir/workspace/lucene-solr/lucene/common-build.xml:1512: The following error occurred while executing this line:\n/home/rmuir/workspace/lucene-solr/lucene/common-build.xml:1038: There were test failures: 1 suite, 1 test, 1 error [seed: 1655BF16A8843A6A]\n\nTotal time: 2 minutes 5 seconds\n\n ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16313744",
            "date": "2018-01-05T19:43:39+0000",
            "content": "the test had to work hard to hit AIOOBE instead of OOM. \n\nI think most users that do something like this will hit OOM which is just as confusing and bad. it may technically be a different problem but due to the names of the methods and the apis, i think its easy someone will hit it too. Seems like add/updateDocuments need some sanity checks... ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16314514",
            "date": "2018-01-06T11:28:20+0000",
            "content": "Note that committing only once at the end is entirely normal and often exactly the right choice.\n\nIt's hard to know how to fix this \u2013 we could add a best effort check that if the RAM usage of that one in-memory segment (DWPT) exceeds the hard limit (IWC.setRAMPerThreadHardLimitMB) we throw a better exception? ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-16314545",
            "date": "2018-01-06T12:44:28+0000",
            "content": "Well, I think a simple limit can work. For this API, e.g a simple counter, throw exc if the iterator has over 100k docs. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16314548",
            "date": "2018-01-06T13:12:14+0000",
            "content": "Well, I think a simple limit can work. For this API, e.g a simple counter, throw exc if the iterator has over 100k docs.\n\n+1 ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-16314698",
            "date": "2018-01-06T15:12:04+0000",
            "content": "OOMs are complicated in general because once you hit one, there's a very real risk that you won't be able to recover anyway (even constructing a new exception message typically requires memory allocation and this just goes on and on in a vicious cycle). I remember thinking about it a lot in the early days of randomizedrunner, but without any constructive conclusions. I tried preallocating stuff in advance (not possible in all cases) and workarounds like keeping a memory buffer that is made reclaimable on OOM (so that there's some memory available before we hit the next one)... these are hacks more than solutions and they don't always work anyway (as in when you have background heap-competing threads...).\n\nI like Java, but it starts to show its wrinkles. \n ",
            "author": "Dawid Weiss"
        },
        {
            "id": "comment-16314853",
            "date": "2018-01-06T19:23:31+0000",
            "content": "Dawid, Michael, my computer has plenty of RAM, which is why I never see an OOM exception and always get the AIOOBE.  ",
            "author": "Laura Dietz"
        },
        {
            "id": "comment-16314869",
            "date": "2018-01-06T19:42:01+0000",
            "content": "Dawid it is not complicated in this case. It is trivial to fix.\n\nAgain to explain:\n\n\n\tWith addDocument you don't hit OOM and you dont need a huge heap. just keep indexing documents and lucene will flush to disk appropriately.\n\tWith addDocumentS it will try to add anything you pass all atomically as one \"transaction\".\n\n\n\nThere are a couple problems here. First is the method's name (addDocuments is not the plural form of addDocument, its something totally different alltogether. It needs to be addDocumentsAtomic or addDocumentsBlock or something else, anything else. Its also missing bounds checks which is why you see the AIOOBE, those need to be added. ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16314939",
            "date": "2018-01-06T22:01:03+0000",
            "content": "+1 ",
            "author": "Laura Dietz"
        }
    ]
}