{
    "id": "SOLR-1657",
    "title": "convert the rest of solr to use the new tokenstream API",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "3.1",
            "4.0-ALPHA"
        ],
        "components": [],
        "type": "Task",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "org.apache.solr.analysis:\nBufferedTokenStream\n > -CommonGramsFilter\n > -CommonGramsQueryFilter\n > -RemoveDuplicatesTokenFilter\nCapitalizationFilterFactory\nHyphenatedWordsFilter\nLengthFilter (deprecated, remove)\nSynonymFilter\nSynonymFilterFactory\nWordDelimiterFilter\n\norg.apache.solr.handler:\nAnalysisRequestHandler\nAnalysisRequestHandlerBase\n\norg.apache.solr.handler.component:\nQueryElevationComponent\nSpellCheckComponent\n\norg.apache.solr.highlight:\nDefaultSolrHighlighter\n\norg.apache.solr.spelling:\nSpellingQueryConverter",
    "attachments": {
        "SOLR-1657_synonyms_ugly_slightly_less_slow.patch": "https://issues.apache.org/jira/secure/attachment/12438753/SOLR-1657_synonyms_ugly_slightly_less_slow.patch",
        "SOLR-1657_synonyms_ugly_slow.patch": "https://issues.apache.org/jira/secure/attachment/12438751/SOLR-1657_synonyms_ugly_slow.patch",
        "SOLR-1657.patch": "https://issues.apache.org/jira/secure/attachment/12428834/SOLR-1657.patch",
        "SOLR-1657_part2.patch": "https://issues.apache.org/jira/secure/attachment/12438681/SOLR-1657_part2.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-12790860",
            "date": "2009-12-15T18:17:04+0000",
            "content": "+ test classes that use old stuff for testing  "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12790865",
            "date": "2009-12-15T18:23:44+0000",
            "content": "+ test classes that use old stuff for testing \n\nin fact I am working on the tests first. In my opinion what we should do is take the functionality of lucene's BaseTokenStreamTestCase and add it to BaseTokenTestCase.\nPerhaps this old stuff can be implemented on top of assertTokenStreamContents, assertAnalyzesTo, etc, or we can change the tests.\n\nEither way, my reasoning is that this logic is very careful about ensuring that tokenstreams behave properly, it inserts garbage data to make sure that attributes are cleared correctly, etc, etc. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12792998",
            "date": "2009-12-20T16:00:19+0000",
            "content": "i would help working on this, but before SOLR-1674 should be committed. Otherwise I would get stuck with multiple patches. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12793002",
            "date": "2009-12-20T16:55:02+0000",
            "content": "Hi Uwe, I was thinking about some ideas on how we could do this, here is what I think in general:\n\n\n\tCommonGrams[Query]Filter and RemoveDuplicatesFilter should be implemented with new api directly, I think they can be a lot more efficient.\n\tBufferedTokenStream should be deprecated, it cannot properly support the attributes api, only those in Token.\n\tNon-final tokenstreams should be made final.\n\n\n\nSo according to the above there are some breaks in api compatibility, but it is not possible to move to the new api without breaks. This is because you cannot properly support the attributes api, yet at the same time extend BufferedTokenStream (see CommonGrams for an example). on top of this these are non-final! So it is impossible in my opinion. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12794087",
            "date": "2009-12-23T16:02:49+0000",
            "content": "converts CommonGramsFilter, CommonGramsQueryFilter, and RemoveDuplicatesFilter to the new tokenstream API.\n\nwith this patch, no solr tokenstreams extend BufferedTokenStream, so now it can be removed, deprecated, converted with a warning it doesn't reuse (it never did), or whatever. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12794122",
            "date": "2009-12-23T17:42:25+0000",
            "content": "this one includes a converted HyphenatedWordsFilter too. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12797161",
            "date": "2010-01-06T16:20:56+0000",
            "content": "Hello, I am working on WordDelimiterFilter and I have a question: how do we want custom attributes to work here?\n\nThis affects performance of the filter under the new tokenstream API, as it will determine when/if we have to save/restore state.\n\nHere are two alternatives:\n\nAlternative #1 (most performant): custom attributes from the original term will only apply to words with no delimiters, or in the case of words with delimiters, only the 'original' token output with the 'preserveOriginal' option. This is easiest to understand in my opinion, and would perform the best. Its arguable that if you split a term into 10 subwords, applying these attributes to all 10 subwords may no longer make sense \n\nAlternative #2: (least performant): custom attributes from the original term will only apply to non-injected terms: this means if a word is split into 10 tokens, all 10 subword tokens, but not their concatenations, also have the attributes derived from the original term. If preserveOriginal is on, then it has the attributes also.\n\nAlternative #3: ??? your ideas?\n\nIn my opinion, we should shoot for maximum performance, as I view this as somewhat like a tokenizer, and custom attributes in general would be applied after WDF, because trying to apply them before WDF and expecting them to make sense afterwards will be confusing. but it does not matter really. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12798188",
            "date": "2010-01-08T21:34:03+0000",
            "content": "What about preserving the attributes for just the first token?  That makes a lot of sense in many cases (say when WDF is just removing punctuation).\nSo if preserveOriginal==true, the first token would always be the original.  This should also be the most performant since it's just a modification to the first token (offset and termText)? "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12798199",
            "date": "2010-01-08T22:12:42+0000",
            "content": "Yonik, I agree, this is almost what the current patch does (take a look if you want, SOLR-1710).\n\nThere is one difference i must change, the 'when WDF is just removing punctuation' case. Current patch does not preserve attributes for this case (you must use preserveOriginal=true)\n\nBut the odd thing about this will be, when 'WDF is just removing punctuation' && preserveOriginal == true, obviously the attributes will only apply to the original... does this make sense? \n\nI will make the change to the SOLR-1710 patch. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12798252",
            "date": "2010-01-08T23:37:43+0000",
            "content": "But the odd thing about this will be, when 'WDF is just removing punctuation' && preserveOriginal == true, obviously the attributes will only apply to the original... does this make sense? \n\nNot sure... I guess it depends on the attribute and what it does.\n\n "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12798255",
            "date": "2010-01-08T23:44:18+0000",
            "content": "Not sure... I guess it depends on the attribute and what it does.\n\nme neither! well there are 2 patches now on SOLR-1710, so if we don't want this we can just use the first one.\ni thought about this one a lot and came to the conclusion that if you really care about your custom attributes making sense, you will use preserveOriginal, but i think both versions work well with that line of reasoning. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12798259",
            "date": "2010-01-09T00:07:23+0000",
            "content": "striking thru WDF since i think its at least close. "
        },
        {
            "author": "Chris Male",
            "id": "comment-12798547",
            "date": "2010-01-10T21:48:43+0000",
            "content": "Attaching a small update to Robert's patch.  Includes some small code readability improvements, much along the same lines that I've done for the WordDelimiterFilter in SOLR-1710. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12829969",
            "date": "2010-02-05T05:59:57+0000",
            "content": "Chris's patch, except also \"implement\" BufferedTokenStream. its marked deprecated, its api cannot support custom attributes (so the 6 are simply copied into tokens and back), and its unused in solr with this patch. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12844863",
            "date": "2010-03-13T11:59:07+0000",
            "content": "This may be interesting for SynonymTokenFilter: LUCENE-2314 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12844889",
            "date": "2010-03-13T14:45:04+0000",
            "content": "Here's a separate patch (_part2.patch) for all the remaining tokenstreams.\n\nThe only one remaining now is SynonymFilter.\n\nFor several areas in this patch, I didn't properly change any APIs to fully\nsupport the new Attributes-based API, I just got them off deprecated methods,\nstill working with Token, and left TODOs.\n\nI figure it would be better to hash this out later on separate issues, where\nwe modify those APIs to really take advantage of an Attributes-based API. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12845073",
            "date": "2010-03-14T13:04:50+0000",
            "content": "A very very ugly, very slow, but simple and conservative conversion of SynonymFilter to the new TokenStream API. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12845078",
            "date": "2010-03-14T13:37:48+0000",
            "content": "attached is a less slow version of the above.\nit preserves the fast path from the previous code. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-12850604",
            "date": "2010-03-27T23:22:45+0000",
            "content": "This was resolved in revision 922957. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12872777",
            "date": "2010-05-28T02:50:24+0000",
            "content": "\nCorrecting Fix Version based on CHANGES.txt, see this thread for more details...\n\nhttp://mail-archives.apache.org/mod_mbox/lucene-dev/201005.mbox/%3Calpine.DEB.1.10.1005251052040.24672@radix.cryptio.net%3E "
        },
        {
            "author": "Grant Ingersoll",
            "id": "comment-13013113",
            "date": "2011-03-30T15:45:34+0000",
            "content": "Bulk close for 3.1.0 release "
        }
    ]
}