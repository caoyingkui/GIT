{
    "id": "LUCENE-836",
    "title": "Benchmarks Enhancements (precision/recall, TREC, Wikipedia)",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/other"
        ],
        "type": "New Feature",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "Would be great if the benchmark contrib had a way of providing precision/recall benchmark information ala TREC.  I don't know what the copyright issues are for the TREC queries/data (I think the queries are available, but not sure about the data), so not sure if the is even feasible, but I could imagine we could at least incorporate support for it for those who have access to the data.  It has been a long time since I have participated in TREC, so perhaps someone more familiar w/ the latest can fill in the blanks here.\n\nAnother option is to ask for volunteers to create queries and make judgments for the Reuters data, but that is a bit more complex and probably not necessary.  Even so, an Apache licensed set of benchmarks may be useful for the community as a whole.  Hmmm.... \n\nWikipedia might be another option instead of Reuters to setup as a download for benchmarking, as it is quite large and I believe the licensing terms are quite amenable.  Having a larger collection would be good for stressing Lucene more and would give many users a demonstration of how Lucene handles large collections.\n\nAt any rate, this kind of information could be useful for people looking at different indexing schemes, formats, payloads and different query strategies.",
    "attachments": {
        "lucene-836.benchmark.quality.patch": "https://issues.apache.org/jira/secure/attachment/12360847/lucene-836.benchmark.quality.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2007-03-20T10:57:15+0000",
            "content": "Regarding data and user queries, I have a 150 000 document corpus with 4 000 000 queries that I might be able to convince the owners to release. It is great data, but a bit politically incorrect (torrents). \n\nThere is some simple Wikipedia harvesting in LUCENE-826, and I'm in the middle of rewriting it to a more general Wikipedia library for text mining purposes. Perhaps you have some ideas you want to put in there? I plan something like this:\n\npublic class WikipediaCorpus {  \n  Map<String, String> wikipediaDomainPrefixByLanguageISO\n  Map<URL, WikipediaArticle> harvestedArticle\n\n  public WikipediaArticle getArticle(String languageISO, String title) \n{\n    ..\n  }\n}\n\npublic class WikipediaArticle {\n  WikipediaArticle(URL url) {    ..  }\n\n  String languageISO;\n  String title;\n  String[] contentParagraphs\n\n  Date[] modified; \n\n  Map<String, String> articleInOtherLanguagesByLanguageISO\n\n}\n ",
            "author": "Karl Wettin",
            "id": "comment-12482367"
        },
        {
            "date": "2007-06-29T21:36:59+0000",
            "content": "lucene-836.benchmark.quality.patch adds a new package \"quality\" under o.a.l.benchmark. \n\nThis is also followup to some of http://www.mail-archive.com/java-dev@lucene.apache.org/msg10851.html\n\nPatch is based on trunk folder. \nFastest way to test it: \"ant test\" from contrib/benchmark dir.\nTo see more output in this run, try \"ant test -Dtests.verbose=true\".\n\nThis is early code, not ready to commit - wanted to show it sooner for feedback, especially the API. \n\nFor a quick view of the API see benchmark.quality at http://people.apache.org/~doronc/api (note that not much javadocs yet - I would wait with that for API closure.)\n\nCode in this patch is:\n\n\textendable.\n\tcan run a quality benchmark.\n\treport quality results, comparing to given judgements (optional).\n\tcreate a submission log (optional).\n\tformat of submission log can be modified, by extending a logger class.\n\tformat of inputs - queries, judgments - can be modified, by extending\n    default readers, or by providing pre-read ones.\n\n\n\nThere is a general \"Judge\" interface - answering if a given doc name is valid for a given \"QualityQuery\". And one implementation of it, based on Trec's QRels. The alternative of TRels, for instance, would mean another implementation of the \"Judge\" interface. (I would love a better name for it, btw...)\n\nA new TestQualityRun tests this package on the Reuters collection - so that test source is a good place to start, to see how to run a quality test. ",
            "author": "Doron Cohen",
            "id": "comment-12509208"
        },
        {
            "date": "2007-07-24T01:20:09+0000",
            "content": "Updated patch is cleaner and almost ready to commit: interfaces are more clean now, and most javadocs is in place. Package javadocs still missing.  ",
            "author": "Doron Cohen",
            "id": "comment-12514822"
        },
        {
            "date": "2007-07-27T05:20:17+0000",
            "content": "A ready to commit patch for search quality benchmarking. \n\nJavadocs can be reviewed in http://people.apache.org/~doronc/api/ - see the benchmark.quality package for a code sample to run the quality benchmark with your input index, queries, judgments, etc.\n\nI would like to commit this in a day or two, to make it easier to proceed with LUCENE-965 and the other search quality ideas - comments (especially on the API) are most welcome... ",
            "author": "Doron Cohen",
            "id": "comment-12515958"
        },
        {
            "date": "2007-07-27T10:58:51+0000",
            "content": "+1\n\nApplies clean and I like the API, but I think you should have a Jury object too...  \n\nI can't actually run it w/o TREC but the tests pass.  I think I might have TREC Arabic lying around somewhere, maybe I will give a run w/ that some day, but don't wait on me to apply this. ",
            "author": "Grant Ingersoll",
            "id": "comment-12516004"
        },
        {
            "date": "2007-07-27T19:49:57+0000",
            "content": "Thanks for the review Grant!\n\nNote that you can see the output by the default Logger and default SubmissionReport by running the TestQualityRun Junit with -Dtests.verbose=true. The submission report and the log both go to stdio so they will be intermixed, but you'll at least get to see what gets printed. ",
            "author": "Doron Cohen",
            "id": "comment-12516084"
        },
        {
            "date": "2007-07-28T23:29:17+0000",
            "content": "Committed.  ",
            "author": "Doron Cohen",
            "id": "comment-12516190"
        }
    ]
}