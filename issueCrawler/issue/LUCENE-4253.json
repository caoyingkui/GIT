{
    "id": "LUCENE-4253",
    "title": "ThaiAnalyzer fail to tokenize word.",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/analysis"
        ],
        "type": "Bug",
        "fix_versions": [],
        "affect_versions": "Realtime Branch",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "Method \nprotected TokenStreamComponents createComponents(String,Reader)\n\nreturn a component that unable to tokenize Thai word.\nThe current return statement is:\nreturn new TokenStreamComponents(source, new StopFilter(matchVersion,        result, stopwords));\n\nMy experiment is change the return statement to:\nreturn new TokenStreamComponents(source, result);\n\nIt give me a correct result.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2012-07-25T10:28:37+0000",
            "content": "One note: \"Java 1.7.0-b147\"\n\nDon't use that old Java version, it is broken with Lucene and creates corrupt indexes!!! Upgrade to at least (as absolute minimum) to Java 7u1, see http://blog.thetaphi.de/2011/07/real-story-behind-java-7-ga-bugs.html ",
            "author": "Uwe Schindler",
            "id": "comment-13422143"
        },
        {
            "date": "2012-07-25T13:44:39+0000",
            "content": "I am confused what the problem is here, but it seems like you don't want to remove stopwords.\n\nIf you dont want to remove stopwords just create a ThaiAnalyzer, passing CharArraySet.EMPTY_SET \nas the stopwords parameter.  ",
            "author": "Robert Muir",
            "id": "comment-13422266"
        },
        {
            "date": "2012-07-26T09:02:45+0000",
            "content": "Hi Robert,\n\nBased on your suggestion, i found the actual problem.\nThe problem is \"stopwords.txt\" in package \"org.apache.lucene.analysis.th\" contain a lot of words that is stop words for a specific type of usage. The only type of usage is already stated inside the file.\nAnd based on the javadoc, since Lucene 3.6, these words are being used by default.\n\nIn my opinion, these set of words shall not be used by default. ",
            "author": "Nattapong Sirilappanich",
            "id": "comment-13422970"
        },
        {
            "date": "2012-07-26T13:41:02+0000",
            "content": "Hi Nattapong: do you want to help us clean it up?\n\nI don't like how long the list is.\n\nPerhaps we can use a smaller list (e.g. the list in http://researchrepository.murdoch.edu.au/7764/2/02Whole.pdf)? ",
            "author": "Robert Muir",
            "id": "comment-13423063"
        },
        {
            "date": "2012-07-27T03:24:07+0000",
            "content": "Hi Reobrt,\n\nStop words will only be useful when it is able to deal with correct tokenization.\nThe problem, as stated in the thesis, is the tokenization process can never give a 100% correct result by any todate technology.\n\nI'd give it a try for the approach in the thesis but it'd be risky if it doesn't deliver what it promised in thesis.\nMy preference now is to use no stop word at all to avoid potential problems.\n\nAn example problem is a word \"\u0e04\u0e07\u0e2d\u0e22\u0e39\u0e48\" (Two syllables Thai word mean persisting and surviving).\nIt will be segmented into \"\u0e04\u0e07\" (mean may, might and probably in English) and \"\u0e2d\u0e22\u0e39\u0e48\" (mean stay, live and reside in English). By using the existing stop word, there is no way to find this word. By using the new stop words in the thesis, the term \"\u0e04\u0e07\" is the only way to find the word which is not going to make sense. How come the word which mean \"might\" return a result with the word meaning \"survive\" ? ",
            "author": "Nattapong Sirilappanich",
            "id": "comment-13423651"
        },
        {
            "date": "2012-07-27T03:40:07+0000",
            "content": "Right but having less than 100% segmentation isnt unique to thai (it happens in many other languages too).\n\nIts always a tradeoff: if those measurements are correct and 30% of typical thai text is stopwords,\nthen its a pretty significant performance (and often relevance) degradation to keep all stopwords.\n\nIn general these list are useful, someone can also choose to use them with commongrams filter for maybe \nan even better tradeoff. Thats why I think its good to keep them (of course as short and minimal as possible).\n\nIf someone doesnt mind the downsides, you can always pass CharArraySet.EMPTY_SET parameter as I mentioned before.\n ",
            "author": "Robert Muir",
            "id": "comment-13423655"
        },
        {
            "date": "2012-07-27T05:31:20+0000",
            "content": "I see your point.\nHowever, it is harder than it look.\nCorrect me if i'm wrong.\n\nAs stated in the thesis itself:\nThis makes retrieval and proper recognition of the documents which contain the phrase \"SOME THAI PHRASE\" almost impossible.\n\nIt is because Thai text may construct a word from many stop words it that list. Without better tokenzier, such word will disappear from index.\n\nI don't have a chance to view the thesis that research over those stop words. In my own opinion, the only set of words that shall not cause a truncated is a set of conjunction words. ",
            "author": "Nattapong Sirilappanich",
            "id": "comment-13423684"
        },
        {
            "date": "2012-07-27T11:38:35+0000",
            "content": "\nThis makes retrieval and proper recognition of the documents which contain the phrase \"SOME THAI PHRASE\" almost impossible.\n\nRight, i just mean the problem is general. 'a' is a english stopword but this can screw up \nthings like 'L.A.' (Los Angeles) and other \"terms\", because of how they are tokenized. This is just a common tradeoff.\n\nIts just that with the current Thai tokenizer and the overly aggressive list, its much more pronounced.\n\n\nI don't have a chance to view the thesis that research over those stop words. In my own opinion, the only set of words that shall not cause a truncated is a set of conjunction words.\n\nYes, this paper seems difficult to get a hold of. \n\nBut I think its definitely a good idea should try to reduce the current list to not be so large. It should be\nless aggressive.\n ",
            "author": "Robert Muir",
            "id": "comment-13423816"
        },
        {
            "date": "2012-07-30T08:19:57+0000",
            "content": "But I think its definitely a good idea should try to reduce the current list to not be so large. It should be\nless aggressive.\nI think trying with something that was researched is a good idea.\nPlease proceed with the new list.\nThanks for spending time discuss a Thai specific problem here. ",
            "author": "Nattapong Sirilappanich",
            "id": "comment-13424743"
        }
    ]
}