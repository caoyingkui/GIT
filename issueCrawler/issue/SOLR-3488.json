{
    "id": "SOLR-3488",
    "title": "Create a Collections API for SolrCloud",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.0",
            "6.0"
        ],
        "components": [
            "SolrCloud"
        ],
        "type": "New Feature",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "",
    "attachments": {
        "SOLR-3488.patch": "https://issues.apache.org/jira/secure/attachment/12529737/SOLR-3488.patch",
        "SOLR-3488_2.patch": "https://issues.apache.org/jira/secure/attachment/12530520/SOLR-3488_2.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-13283382",
            "date": "2012-05-25T13:31:09+0000",
            "content": "I'll post an initial patch just for create soon. It's just a start though. I've added a bunch of comments for TODOs or things to consider for the future. I'd like to start simple just to get 'something' in though.\n\nSo initially, you can create a new collection and pass an existing collection name to determine which shards it's created on. Would also be nice to be able to explicitly pass the shard urls to use, as well as simply offer X shards, Y replicas. In that case, perhaps the leader overseer could handle ensuring that. You might also want to be able to simply say, create it on all known shards.\n\nFurther things to look at:\n\n\n\tother commands, like remove/delete.\n\twhat to do when some create calls fail? should we instead add a create node to a queue in zookeeper? Make the overseer responsible for checking for any jobs there, completing them (if needed) and then removing the job from the queue? Other ideas.\n\n "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13283478",
            "date": "2012-05-25T14:32:29+0000",
            "content": "should we instead add a create node to a queue in zookeeper? Make the overseer responsible for checking for any jobs there, completing them (if needed) and then removing the job from the queue? \n\nI like this idea, i would also refactor current zkcontroller->overseer communication to use this same technique. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13283510",
            "date": "2012-05-25T14:55:19+0000",
            "content": "should we instead add a create node to a queue in zookeeper? \n\nYeah, a work queue in ZK makes perfect sense.  Perhaps serialize the params to a JSON map/object per line?\nedit: or perhaps it makes more sense for each operation to be a separate file (which is what I think you wrote anyway)\n\nPossible parameters:\n\n\tname of the collection\n\tthe config for the collection\n\tnumber of shards in the new collection\n\tdefault replication factor\n\n\n\nOperations:\n\n\tadd a collection\n\tremove a collection\n\tdifferent options here... leave cores up, bring cores down, completely remove cores (and data)\n\tchange collection properties (replication factor, config)\n\texpand collection (split shards)\n\tadd/remove a collection alias\n\n\n\nShard operations:\n\n\tadd a shard (more for custom sharding)\n\tremove a shard\n\tchange shard properties (replication factor)\n\tsplit a shard\n\tadd/remove a shard alias\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13283561",
            "date": "2012-05-25T16:02:01+0000",
            "content": "I'm going on vacation for a week, so here is my early work on just getting something basic going. It does not involved any overseer stuff yet.\n\nSomeone feel free to take it - commit it and iterate, or iterate in patch form - whatever makes sense. I'll help when I get back if there is more to do, and if no one makes any progress, I'll continue on it when I get back.\n\nCurrently, I've copied the core admin handler pattern and made a collections handler. There is one simple test and currently the only way to choose which nodes the collection is put on is to give an existing template collection.\n\nThe test asserts nothing at the moment - all very early work. But I imagine we will be changing direction a fair amount, so that's good I think.\n "
        },
        {
            "author": "Tommaso Teofili",
            "id": "comment-13287225",
            "date": "2012-06-01T07:48:07+0000",
            "content": "slight improvements to Mark's patch.\nRegarding the template based creation I think it should use a different parameter name for the collection template (e.g. \"template\") and use the \"collection\" parameter for the new collection name.\nApart from that I think it may be useful to clearly define different creation strategies (I've created an interface for that), the right one is chosen on the basis of the passed HTTP parameters. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13292418",
            "date": "2012-06-09T19:32:41+0000",
            "content": "Thanks Tommaso!\n\nRegarding the template based creation I think it should use a different parameter name for the collection template (e.g. \"template\") and use the \"collection\" parameter for the new collection name.\n\nI'm actually hoping that perhaps that stuff is temporary, and I just did it to have something that works now. I think though, that we should really change how things work - so that you just pass the number of shards and the number of replicas, and the overseer just ensures the collection is on the right number of nodes. Then we don't have to have this 'template' collection to figure out what nodes to create on - or explicitly pass the nodes.\n\nSami has a distributed work queue for the overseer setup now, and I'm working on integrating this with that. "
        },
        {
            "author": "Tommaso Teofili",
            "id": "comment-13292697",
            "date": "2012-06-11T08:58:33+0000",
            "content": "I think though, that we should really change how things work - so that you just pass the number of shards and the number of replicas, and the overseer just ensures the collection is on the right number of nodes. Then we don't have to have this 'template' collection to figure out what nodes to create on - or explicitly pass the nodes.\n\nsure, +1.\n\nSami has a distributed work queue for the overseer setup now, and I'm working on integrating this with that.\n\nthat looks great. By the way, I think it would be good if that could be also (optionally) used for indexing in SolrCloud. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13295689",
            "date": "2012-06-15T14:35:44+0000",
            "content": "I've got my first patch ready - still some things to address, but it currently does queue based collection creation.\n\nOne thing I recently realized when I put some last minute pieces together is that I cannot share the same Overseer queue that already exists - it will cause a deadlock as we wait for states to be registered. Processing the queue with multiple threads still seemed scary if you were to create a lot of collections at once - so it seems just safer to use a different queue.\n\nI'm still somewhat unsure about handing failures though - for the moment I'm simply adding the job back onto the queue - this gets complicated quickly though. Especially if you add in delete collection and it can fail. If you start putting commands back on the queue you could have weird create/delete command retry reordering?\n\nI also have not switched to requiring or respecting a replication factor - I was thinking perhaps specifying nothing or -1 would give you what you have now? An infinite rep factor? And we would enforce a lower rep factor if requested? For now I still require that you pass a collection template and new nodes are created on the nodes that host the collection template.\n\nI'm not sure how replication factor would be enforced though? The Oveerseer just periodically prunes and adds given what it sees and what the rep factor is? Is that how failures should be handled? Don't readd to the queue, just let the periodic job attempt to fix things later? \n\nWhat if someone starts a new node with a new replicas pre configured in solr.xml? The Overseer periodic job would simply remove them shortly thereafter if the rep factor was not high enough?\n\nOne issue with pruning at the moment is that unloading a core will not remove it's data dir. We probably want to fix that for collection removal.\n\nIf we go too far down this path, it seems rebalancing starts to become important as well.\n\nI've got some other thoughts and ideas to get down, but that is a start so I can gather some feedback.\n\nI have not yet integrated Tomasso's work, but will if we don't end up changing things much from now. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13295876",
            "date": "2012-06-15T19:31:40+0000",
            "content": "One thing I recently realized when I put some last minute pieces together is that I cannot share the same Overseer queue that already exists - it will cause a deadlock as we wait for states to be registered. Processing the queue with multiple threads still seemed scary if you were to create a lot of collections at once - so it seems just safer to use a different queue.\n\nI'm sure you guys have thought about the command queue more than me at this point... but some brainstorming off the top of my head:\n\n\tThe type of request could implicitly be be synchronous (must complete before moving to the next request) or asynchronous\n\teven for an asynchronous command, the executor for certain types of commands could be limited to a single thread to avoid complexity (not sure if that helps your deadlock situation or not)\n\n\n\nTelling when something is done:\n\n\tcould have a flag that requests the command be put on a completed queue, along with any completion info.\n\tto prevent unbounded growth of a completed queue, it could have a limited size (it could be pretty useful to see a recent history of operations)\n\n\n\nThe work queue thing could be a public interface (i.e. external management systems could directly make use of it), in which case we'd want to document it well eventually. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13397211",
            "date": "2012-06-20T01:45:40+0000",
            "content": "updated patch - some refactoring and started adding remove collection code - though currently we do not remove all collection info from zk even when you unload every shard - something we should probably start doing? "
        },
        {
            "author": "Tommaso Teofili",
            "id": "comment-13397299",
            "date": "2012-06-20T06:40:07+0000",
            "content": "currently we do not remove all collection info from zk even when you unload every shard - something we should probably start doing?\n+1\n\nI have not yet integrated Tomasso's work, but will if we don't end up changing things much from now.\nit should be easy to merge but I think that it'd be also good to start committing your patch and improve things on SVN from now on to ease code review (no patch merging) and concurrent works. "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13397362",
            "date": "2012-06-20T08:47:16+0000",
            "content": "Mark, nice work.\n\nI'm still somewhat unsure about handing failures though...\n\nIMO Fail fast: at minimum an error should be reported back (the completed queue Yonik mentions?). It seems that in the latest patch even in case of failure the job is removed from queue.\n\nI also have not switched to requiring or respecting a replication factor - I was thinking perhaps specifying nothing or -1 would give you what you have now? An infinite rep factor? And we would enforce a lower rep factor if requested?\n\nSounds good to me.\n\nI'm not sure how replication factor would be enforced though? The Oveerseer just periodically prunes and adds given what it sees and what the rep factor is? Is that how failures should be handled? Don't readd to the queue, just let the periodic job attempt to fix things later?\n\nI would first implement the simplest? case first where if not enough nodes are available to meet #shards and/or #replication factor: report error to user and do not try to create the collection. Or did you mean at runtime after the collection has been created?\n\nI have one question about the patch specifically in the OverseerCollectionProcessor where you create the collection: why do you need the collection param? \nIn context of creating N * R cluster: why don't you just go though live nodes to find available nodes and perhaps then based on some \"strategy\" class create specific shards (with shardids) to specific nodes? The rest of the overseer would have to respect that same strategy (instead of the dummy AssignShard that is now used) so that things would not break when new nodes are attached to the collection. Perhaps this \"strategy\" could also handle things like time based sharding and whatnot?\n\nit should be easy to merge but I think that it'd be also good to start committing your patch and improve things on SVN from now on to ease code review (no patch merging) and concurrent works.\n\n+1 for committing this as is, there are some minor weak spots in the current patch like checking the input for the collections api requests (unexisitng params cause OverseerCollectionProcessor to die with NPE), reporting back input errors etc. put lets just put this in and open more jira issues to cover the improvement tasks and bugs?\n\nOne more thing: I am seeing BasicDistributedZkTest failing (not just sporadically), nut sure if it is related, with the following error:\n\n\n [junit4] ERROR   0.00s J1 | BasicDistributedZkTest (suite)\n   [junit4]    > Throwable #1: java.lang.AssertionError: ERROR: SolrIndexSearcher opens=496 closes=494\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([F1C0A91EB78BAB39]:0)\n   [junit4]    > \tat org.junit.Assert.fail(Assert.java:93)\n   [junit4]    > \tat org.apache.solr.SolrTestCaseJ4.endTrackingSearchers(SolrTestCaseJ4.java:190)\n\n\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13397510",
            "date": "2012-06-20T13:51:16+0000",
            "content": "It seems that in the latest patch even in case of failure the job is removed from queue.\n\nRight - I was putting it back on the queue, but once I added deletes, I removed that because I was worried about reorderings. I figure we may need a different strategy in general. I'll expand on that in a new comment.\n\n\nreport error to user and do not try to create the collection\n\nYeah, that is one option - then we have to remove the collection on the other nodes though. For instance, what happens if one of the create cores calls fails due to an intermittent connection error. Do we fail then? We would need to clean up first. Then what if one of those nodes fails before we could remove it. And then comes back with that core later. I agree that simple might be the best bet to start, but in failure scenarios it gets a little muddy quickly. Which may be fine to start as you suggest.\n\nI have one question about the patch specifically in the OverseerCollectionProcessor where you create the collection: why do you need the collection param? \n\nMostly just simplicity to start - getting the nodes based on a template collection was easy. Tommaso did some work on extracting a strategy class, but I have not yet integrated it. Certainly we need more options at a minimum, or perhaps just a different strategy. Simplest might be a way to go, but it also might be a back compat problem if we choose to do something else. I'll try and elaborate in a new comment a bit later today.\n\nand improve things on SVN from now\n\nOkay, that sounds fine to me. I'll try and polish the patch a smidgen and commit it as a start soon. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13397695",
            "date": "2012-06-20T17:57:09+0000",
            "content": "Perhaps its a little too ambitious, but the reason I brought up the idea of the overseer handling collection management every n seconds is:\n\nLets say you have 4 nodes with 2 collections on them. You want each collection to use as many nodes as are available. Now you want to add a new node. To get it to participate in the existing collections, you have to configure them, or create new compatible cores over http on the new node. Wouldn't it be nice if the Overseer just saw the new node, that the collections had repFactor=MAX_INT and created the cores for you?\n\nAlso, consider failure scenarios:\n\nIf you remove a collection, what happens when a node that was down comes back and had that a piece of that collection? Your collection will be back as a single node. An Overseer process could prune this off shortly after.\n\nSo numShards/repFactor + Overseeer smarts seems simple and good to me. But sometimes you may want to be precise in picking shards/repliacs. Perhaps simply doing some kind of 'rack awareness' type feature down the road is the best way to control this though. You could create connections and weight costs using token markers for each node or something.\n\nSo I think maybe we would need a new zk node where solr instances register rather than cores? then we know what is available to place replicas on - even if that Solr instance has no cores?\n\nThen the Overseer would have a process that ran every n (1 min?) and looked at each collection and its repFactor and numShards, and add or prune given the current state.\n\nThis would also account for failures on collection creation or deletion. If a node was down and missed the operation, when it came back, within N seconds, the Overseer would add or prune with the restored node.\n\nIt handles a lot of failures scenarios (with some lag) and makes the interface to the user a lot simpler. Adding nodes can eventually mean just starting up a node new rather than requiring any config. It's also easy to deal with changing the replication factor. Just update it in zk, and when the Overseer process runs next, it will add and prune to match the latest value (given the number of nodes available).\n\n "
        },
        {
            "author": "Tommaso Teofili",
            "id": "comment-13398232",
            "date": "2012-06-21T06:56:50+0000",
            "content": "Lets say you have 4 nodes with 2 collections on them. You want each collection to use as many nodes as are available. Now you want to add a new node. To get it to participate in the existing collections, you have to configure them, or create new compatible cores over http on the new node. Wouldn't it be nice if the Overseer just saw the new node, that the collections had repFactor=MAX_INT and created the cores for you?\n\nsure, that'd be nice indeed. Maybe this should be configurable (a param like greedy=boolean)\n\nIf you remove a collection, what happens when a node that was down comes back and had that a piece of that collection? Your collection will be back as a single node. An Overseer process could prune this off shortly after.\n\nso basically, if I understood correctly, is that the overseer has the capability of doing periodic checks without an explicit action / request from a client which can help on cleaning states / check for failures / etc.\n\nSo I think maybe we would need a new zk node where solr instances register rather than cores? then we know what is available to place replicas on - even if that Solr instance has no cores?\n\nwouldn't be possible to store the instances information in the same zk node? Because otherwise we could've to also check the two nodes are in consistent states (I don't know Zookeeper very much so I may be wrong here)\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13398942",
            "date": "2012-06-21T22:28:54+0000",
            "content": "so basically, if I understood correctly, is that the overseer has the capability of doing periodic checks without an explicit action / request from a client which can help on cleaning states / check for failures / etc.\n\nYeah - basically, either every n seconds, or when the overseer sees a new node come or go, it looks at each collection, checks its replication factor, and either adds or removes nodes to match it given the nodes that are currently up. So with some lag, whatever you set for the replication will eventually be matched no matter the failures or random state of the cluster when the collection is created or its replication factor changed. "
        },
        {
            "author": "Sami Siren",
            "id": "comment-13399117",
            "date": "2012-06-22T05:04:28+0000",
            "content": "Yeah - basically, either every n seconds, or when the overseer sees a new node come or go, it looks at each collection, checks its replication factor, and either adds or removes nodes to match it given the nodes that are currently up. So with some lag, whatever you set for the replication will eventually be matched no matter the failures or random state of the cluster when the collection is created or its replication factor changed.\n\nThat sounds like a good goal. I think we need to have special handling for situation where whole cluster/collection is bounced or some planned maintenance is to be done.\n\nHdfs has this feature called safe mode that is enabled on startup (and can be turned on at any point) and while in that mode replication of blocks is prohibited. When certain percentage of blocks are availabe it moves away from this mode. Something like that might work in solr context also - meaning no shard reorg would happen until certain percentage of the shards are available or solr is specifically told to leave this mode.\n\nhttp://hadoop.apache.org/hdfs/docs/current/hdfs_user_guide.html#Safemode "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13399290",
            "date": "2012-06-22T12:43:43+0000",
            "content": "To get something incrementally committable I'm changing from using a collection template to a simple numReplicas. I have hit an annoying stall where it is difficult to get all of the node host urls. The live_nodes list is translated from url to path safe. It's not reversible if _ is in the original url. You can put the url in data at each node, but then you have to slowly read each node rather than a simple getChildren call. You can also try and find every node by running through the whole json cluster state file - but that wouldn't give you any nodes that had no cores on it at the moment (say after a collection delete). "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13399518",
            "date": "2012-06-22T19:00:13+0000",
            "content": "Re the above: I'm tempted to add another data node that just has the list of nodes? I think it would be good to have an efficient way to get that list. It's pain with clusterstate.json and that loses nodes with no cores on it now.\n\nSomething I just remembered I have to look into: the default location of the data dir for on the fly cores that are created is probably not great.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13403155",
            "date": "2012-06-28T15:00:14+0000",
            "content": "I'm about ready to commit a basic first draft so it is easier for others to contribute.\n\nOne of the main limitations is that it picks servers randomly to create a new collection - we probably want to change that before 4 so that it chooses servers based on the number of cores they already have or something.\n\nWhether we allow different creation strategies that let you specify which servers to choose in alternate ways still seems up for debate - but that is also easy to add in if we choose to go that route. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13403223",
            "date": "2012-06-28T17:02:39+0000",
            "content": "Before I commit I have to track down an occasional fail where an index directory fails to be made. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13403248",
            "date": "2012-06-28T17:23:22+0000",
            "content": "Looks like the issue is the data dir for created collections in tests - because each jetty shares the same solr home, dynamically created cores from the collections api share data dirs (bad). Other tests specify appropriate data dirs, but the collections api does not provide an easy way to specify different data dirs for each collection on each node that it will create...\n\nThinking... "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13405053",
            "date": "2012-07-02T13:09:42+0000",
            "content": "So I refactored some of the zk distrib tests to use a new solrhome copy for each jetty rather than simply a different data directory. Trying to address this with the data directory seemed like a non starter across multiple jetty instances. With a unique Solr home though, creating new cores dynamically with the default data dir will no longer step on each other.\n\nDoing some last minute testing, but I'll commit what I have to trunk shortly. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13412102",
            "date": "2012-07-11T22:26:01+0000",
            "content": "bulk fixing the version info for 4.0-ALPHA and 4.0 all affected issues have \"hoss20120711-bulk-40-change\" in comment "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13412872",
            "date": "2012-07-12T15:20:22+0000",
            "content": "I'm going to add a collection RELOAD command, and beef up the tests a little. Still more to do after that. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13412874",
            "date": "2012-07-12T15:24:05+0000",
            "content": "Is it intended for a collection RELOAD action to reload all collection cores immediately? That implies downtime i assume? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13412888",
            "date": "2012-07-12T15:52:47+0000",
            "content": "Is it intended for a collection RELOAD action to reload all collection cores immediately? \n\nYes, at least initially. Essentially a convenience method for reloading your cores to pick up changed config or settings. There may be other ways we allow that to happen more automatically eventually, but at a minimum we need the ability to trigger a collection wide reload. There are things to consider for a truly massive cluster - do you really want every node trying to read the new configs form zk at the same time? That's in the future if I end up working on it. We'd have to see how many servers it takes before you end up with a problem, if it is indeed a problem at all.\n\n\nThat implies downtime i assume?\n\nI'm not sure why? Core reloads don't involve any down time. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13412927",
            "date": "2012-07-12T16:51:02+0000",
            "content": "Thanks for claryfing, it makes sense. About the downtime on core reload, a load balancer pinging Solr's admin/ping handler will definately mark the node as down; the ping request will time out for up to a few seconds or even longer in case of many firstSearcher events.\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13412937",
            "date": "2012-07-12T16:57:52+0000",
            "content": "the ping request will time out for up to a few seconds or even longer in case of many firstSearcher events.\n\nHmmm, perhaps we could add an option for the new core to have a registered searcher before we swap it in and close the old core? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13412957",
            "date": "2012-07-12T17:16:02+0000",
            "content": "Yeah, this sounds like something we have to fix to me. There should not be a gap in serving requests on core reload. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13413080",
            "date": "2012-07-12T19:02:42+0000",
            "content": "There should not be a gap in serving requests on core reload.\n\nJust to clarify: it's more a practical gap than a real gap... it should be impossible for a query to not be serviced - it's just that a cold core could take longer to service the query than desired.  But it should be pretty easy to allow waiting for that searcher in the new core. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13413086",
            "date": "2012-07-12T19:06:41+0000",
            "content": "Ah, did not catch it was just a timeout issue. Was wondering what the problem could be.\n\nYeah, not as bad I thought then. An option would be nice. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13413340",
            "date": "2012-07-12T23:54:43+0000",
            "content": "Commit improved tests and reload command in a moment.\n\nAnother thing we will need before too long is a way to get a response I think? Right now, the client can't learn of the success or failure of the command. It's just int he Overseers logs.\n\nTo get notified, I suppose the call would have to block and then get a result from the overseer.\n\nI suppose that could be done by something like: create a new emphemeral node for each job - client watches the node - when overseer is done, it sets the result as data on the node - client gets a watch notify and reads the result? Then how to clean up? Not sure about the idea overall, brainstorming ... don't see a simple way to have the over seer do the work in an async fashion and have the client easily get the results of that. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13429855",
            "date": "2012-08-07T03:43:37+0000",
            "content": "rmuir20120906-bulk-40-change "
        },
        {
            "author": "Lance Norskog",
            "id": "comment-13442757",
            "date": "2012-08-27T22:04:00+0000",
            "content": "Yeah, a work queue in ZK makes perfect sense. \n\nhttp://zookeeper-user.578899.n2.nabble.com/Announcing-KeptCollections-distributed-Java-Collections-for-ZooKeeper-td5816709.html\n\nhttps://github.com/anthonyu/KeptCollections\n\nDistributed Java Collections implementations. Apache licensed. Years of use. "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13455686",
            "date": "2012-09-14T09:27:17+0000",
            "content": "I've been playing with the collections API lately.\n\nFirst, I am more in favor of the approach where all config and config changes are done against ZK. I do not like the idea of having to start a solr node in order to define a new collection or change various configs. All initial config as well as config changes should be possible to check in to source control and roll out to a cluster as files without starting and stopping live nodes (perhaps except ZK itself).\n\nThen regarding the current collections API. I don't know if it is by design or a bug:\n\n\tStart a master, boostrapping collection1 from local config\n\tCreate another collection specifying number of replicas using the collection API: http://localhost:8983/solr/admin/collections?action=CREATE&name=demo&numShards=1&numReplicas=5\n\tAdd more Solr nodes to the cluster\n\tThe original collection \"collection1\" starts using the extra nodes as replicas, but the new \"demo\" collection does not\n\n "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13456166",
            "date": "2012-09-14T21:24:21+0000",
            "content": "I don't know if it is by design or a bug:\n\nFor the moment, it's by design. \"collection1\" would only show up on the new nodes because you have it pre configured in solr.xml. Unless you take that out, it will be on any node you ever start. That's just the result of how we currently ship Solr and the user not making any changes.\n\nWe do plan on making it so that your replication setting for a collection is adjusted over time - so if you only have 2 replicas now, but create the collection with a replication factor of 5, when you add 3 nodes, we'd like the overseer to recognize this and create the new replicas. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13456172",
            "date": "2012-09-14T21:39:42+0000",
            "content": "but create the collection with a replication factor of 5,\n\nWhich reminds me... is \"replicationFactor\" a better name than \"numReplicas\" that we have now?  If so we should change at the same time we make the changes for the slice properties.  It seems like replicationFactor better expresses that it's a target and not the actual number of replicas.  This will also make things less confusing when we allow the replication factor to be overridden on a per-shard basis.  Imagine seeing \"numReplicas=3\" as a shard property but then seeing only one replica listed under that shard! "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13456174",
            "date": "2012-09-14T21:41:43+0000",
            "content": "is \"replicationFactor\" a better name than \"numReplicas\" \n\n+1 "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13456473",
            "date": "2012-09-15T19:04:02+0000",
            "content": "SOLR-3845 : Rename numReplicas to replicationFactor in Collections API. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13456475",
            "date": "2012-09-15T19:06:17+0000",
            "content": "\nFirst, I am more in favor of the approach where all config and config changes are done against ZK. I do not like the idea of having to start a solr node in order to define a new collection or change various configs. All initial config as well as config changes should be possible to check in to source control and roll out to a cluster as files without starting and stopping live nodes (perhaps except ZK itself).\n\nI'm not sure I follow that...why would you need to start a solr node to deal with collections? The collection manager is designed the same way as core admin - you should not need a core... "
        },
        {
            "author": "Jan H\u00f8ydahl",
            "id": "comment-13456498",
            "date": "2012-09-15T22:11:16+0000",
            "content": "I'm not sure I follow that...why would you need to start a solr node to deal with collections? The collection manager is designed the same way as core admin - you should not need a core...\n\nSo how to create a new collection offline? Push an updated solr.xml to ZK? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13456508",
            "date": "2012-09-15T23:15:55+0000",
            "content": "Yes, if you want to predefine a collection you do it in solr.xml, the same way that collection1 is done. Otherwise, you can start Solr with no collections and create them with the API. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13461149",
            "date": "2012-09-22T13:10:35+0000",
            "content": "Further work should be new issues. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13654118",
            "date": "2013-05-10T10:34:05+0000",
            "content": "Closed after release. "
        }
    ]
}