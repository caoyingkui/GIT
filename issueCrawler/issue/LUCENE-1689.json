{
    "id": "LUCENE-1689",
    "title": "supplementary character handling",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "for Java 5. Java 5 is based on unicode 4, which means variable-width encoding.\n\nsupplementary character support should be fixed for code that works with char/char[]\n\nFor example:\nStandardAnalyzer, SimpleAnalyzer, StopAnalyzer, etc should at least be changed so they don't actually remove suppl characters, or modified to look for surrogates and behave correctly.\nLowercaseFilter should be modified to lowercase suppl. characters correctly.\nCharTokenizer should either be deprecated or changed so that isTokenChar() and normalize() use int.\n\nin all of these cases code should remain optimized for the BMP case, and suppl characters should be the exception, but still work.",
    "attachments": {
        "testCurrentBehavior.txt": "https://issues.apache.org/jira/secure/attachment/12410615/testCurrentBehavior.txt",
        "LUCENE-1689.patch": "https://issues.apache.org/jira/secure/attachment/12415190/LUCENE-1689.patch",
        "LUCENE-1689_lowercase_example.txt": "https://issues.apache.org/jira/secure/attachment/12410505/LUCENE-1689_lowercase_example.txt"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-06-12T18:48:11+0000",
            "content": "an example of how LowercaseFilter might be fixed. \nI only changed the new-api method for demonstration purposes. ",
            "author": "Robert Muir",
            "id": "comment-12718937"
        },
        {
            "date": "2009-06-12T18:54:11+0000",
            "content": "my example has some \"issues\" (i.e. it depends upon the knowledge that no surrogate pairs lowercase to BMP codepoints).\nit also doesn't correctly handle invalid unicode data (unpaired surrogates).\n\nSuch complexity can be all added to the slower-path supplementary case. the intent is just to show it wouldnt be a terribly invasive change.\nThanks! ",
            "author": "Robert Muir",
            "id": "comment-12718938"
        },
        {
            "date": "2009-06-12T19:07:15+0000",
            "content": "icu uses the following idiom to check if a char ch is a surrogate. might be the fastest way to ensure the performance impact is minimal:\n\n(ch & 0xFFFFF800) == 0xD800\n ",
            "author": "Robert Muir",
            "id": "comment-12718942"
        },
        {
            "date": "2009-06-13T09:45:19+0000",
            "content": "Robert, could you flesh this patch out to a committable point?  Ie, handle unpaired surrogates, add test case that first shows that LowercaseFilter incorrectly breaks up surrogates?  Thanks!\n\nit depends upon the knowledge that no surrogate pairs lowercase to BMP codepoints\n\nIs it invalid to make this assumption?  Ie, does the unicode standard not guarantee it? ",
            "author": "Michael McCandless",
            "id": "comment-12719104"
        },
        {
            "date": "2009-06-13T14:19:42+0000",
            "content": "Michael: LowerCaseFilter doesn't incorrectly break surrogates, it just won't lowercase supplementary codepoints.\n\nBut I can get it in shape, sure. ",
            "author": "Robert Muir",
            "id": "comment-12719123"
        },
        {
            "date": "2009-06-13T14:23:02+0000",
            "content": "The scary thing is that this happens already if you run lucene on a 1.5 VM even without introducing 1.5 code. \nI think we need to act on this issue asap and release it together with 3.0. -> ful support for unicode 4.0 in lucene 3.0 \nI also thought about the implementation a little bit. The need to detect chars > BMP and operate on those might be spread out across lucene (quite a couple of analyzers and filters etc). Performance could truely suffer from this if it is done \"wrong\" or even more than once. It might be considerable to make the detection pluggable with an initial filter that only checks where surrogates are present in a token and sets an indicator to the token represenation so that subsequent TokenStreams can operate on it without rechecking. This would also preserve performance for those who do not need chars > BMP (which could be quite a large amout of people). Those could then simply not supply such a initial filter.\n\nJust a couple of random thoughts. ",
            "author": "Simon Willnauer",
            "id": "comment-12719125"
        },
        {
            "date": "2009-06-13T14:55:01+0000",
            "content": "Simon, I want to address your comment on performance.\nI think that surrogate detection is cheap when done right and I don't think there's a ton of places that need changes.\nBut I don't think any indicator is really appropriate, for example my TokenFilter might want to convert one chinese character in the BMP to another one outside of the BMP. It is all unicode.\n\nBut there is more than just analysis involved here, for example I have not tested WildcardQuery: ? operator.\nI'm not trying to go berzerko and be 'ultra-correct', but basic things like that should work.\nFor situations where its not worth it, i.e. FuzzyQuery's scoring, we should just doc that the calculation is based on 'code units', and leave it alone. ",
            "author": "Robert Muir",
            "id": "comment-12719127"
        },
        {
            "date": "2009-06-13T15:16:58+0000",
            "content": "I am curious how you plan on approaching backwards compat?\n\n#1 The patch is needed for correct java 1.5 behavior, its a 1.5 migration issue.\n#2 The patch won't work on java 1.4 because the jdk does not supply the needed functionality\n#3 Its my understanding you like to deprecate things (say CharTokenizer) and give people a transition time. How will this work?\n\ni intend to supply patch that fixes the problems, but I wanted to bring up those issues... ",
            "author": "Robert Muir",
            "id": "comment-12719139"
        },
        {
            "date": "2009-06-13T15:40:57+0000",
            "content": "Simon, a response to your other comment 'I think we need to act on this issue asap and release it together with 3.0. -> full support for unicode 4.0 in lucene 3.0'\n\nFull unicode support would be great!\n\nBut this involves a whole lot more, I'm trying to create a contrib under LUCENE-1488 for \"full unicode support\". \nA lot of what is necessary isn't available in the Java 1.5 JDK, such as unicode normalization.\n\nMaybe we can settle for 'full support for UCS (Unicode Character Set)' as a start! ",
            "author": "Robert Muir",
            "id": "comment-12719146"
        },
        {
            "date": "2009-06-13T17:01:07+0000",
            "content": "i forgot to answer your question Michael: \n\nit depends upon the knowledge that no surrogate pairs lowercase to BMP codepoints\nIs it invalid to make this assumption? Ie, does the unicode standard not guarantee it?\n\nI do not think it guarantees this for all future unicode versions. In my opinion, we should exploit things like this if I can show a test case that proves its true for all codepoint in the current version of unicode \nAnd it should be documented that this could possibly change in some future version.\nIn this example, its a nice simplification because it guarantees the length (in code units) will not change!\n\nI think for a next step on this issue I will create and upload a test case showing the issues and detailing some possible solutions.\nFor some of them, maybe a javadoc update is the most appropriate, but for others, maybe an API change is the right way to go.\nThen we can figure out what should be done. ",
            "author": "Robert Muir",
            "id": "comment-12719158"
        },
        {
            "date": "2009-06-13T18:11:12+0000",
            "content": "I am curious how you plan on approaching backwards compat? \nI don't see a real back compat issue... I can't imagine anyone relying on the fact that >BMP chars wouldn't be lowercased.  To rely on that would also be relying on undocumented behavior.\n\nIt seems like this (handling code points beyond the BMP) is really a collection of issues that could be committed independently when ready?\n\nMoving to 3.1 since it requires Java 1.5 for much of it.\nIf there's something desirable to slip in for 2.9 that doesn't depend on Java5, we can still do that.\n ",
            "author": "Yonik Seeley",
            "id": "comment-12719167"
        },
        {
            "date": "2009-06-13T19:06:55+0000",
            "content": "Yonik, I think the problem is where method signatures must change, such as CharTokenizer, required to fix LetterTokenizer and friends.\n\nThese are probably the worst offenders, as a lot of these tokenizers just remove >BMP chars, which is really nasty behavior for CJK. \n\nI agree its a collection of issues but maybe there can be an overall strategy?\n ",
            "author": "Robert Muir",
            "id": "comment-12719174"
        },
        {
            "date": "2009-06-13T20:09:02+0000",
            "content": "onik, I think the problem is where method signatures must change, such as CharTokenizer, required to fix LetterTokenizer and friends. \n\nAh, OK.  Actually it just occurred to me that this would also require reindexing, otherwise queries that hit documents in the past would mysteriously start missing them (for text outside the BMP). ",
            "author": "Yonik Seeley",
            "id": "comment-12719179"
        },
        {
            "date": "2009-06-15T01:12:44+0000",
            "content": "this is just a patch with testcases showing the existing behavior.\n\nperhaps these should be fixed:\nSimple/Standard/StopAnalyzer/etc: deletes all supp. characters completely.\nLowerCaseFilter: doesn't lowercase supp. characters correctly.\nWildcardQuery: ? operator does not work correctly.\n\nperhaps these just need some javadocs:\nFuzzyQuery: scoring is strange because its based upon surrogates, leave alone and javadoc it.\nLengthFilter: length is calculated based on utf-16 code units, leave alone and javadoc it.\n\n\n... and theres always the option to not change any code, but just javadoc all the behavior as a \"fix\", providing stuff in contrib or elsewhere that works correctly.\nlet me know what you think. ",
            "author": "Robert Muir",
            "id": "comment-12719363"
        },
        {
            "date": "2009-07-31T14:27:52+0000",
            "content": "Robert, are you still working on this issue? I have seen a couple of questions about supplementary character support on the mailinig-lists though - I will see how much time I have during the next weeks to get this going.\n\nsimon ",
            "author": "Simon Willnauer",
            "id": "comment-12737535"
        },
        {
            "date": "2009-07-31T16:02:32+0000",
            "content": "Simon, not yet. I was trying to solicit some opinions on what the best approach should be. I realize this is a very special case.\n ",
            "author": "Robert Muir",
            "id": "comment-12737576"
        },
        {
            "date": "2009-07-31T19:43:58+0000",
            "content": "one tricky part of this is CharTokenizer. a few things subclass this guy.\n\n\n\tSometimes it is ok (WhitespaceTokenizer, because all whitespace in unicode <= 0xFFFF)\n\tSometimes it is not (LetterTokenizer)\n\n\n\nIt depends how its being used. So I think instead of just\n\nabstract boolean isTokenChar(char)\nchar normalize(char)\n\n\n\nwe can add additional int-based methods, where the default implementation is to defer to the char-based ones.\n\nThis would preserve backwards compatibility. ",
            "author": "Robert Muir",
            "id": "comment-12737679"
        },
        {
            "date": "2009-07-31T21:58:15+0000",
            "content": "That sounds right.\n\nWe would also deprecate the char based methods?  And note that in 3.0 we'll remove them, and make the int-based methods abstract?\n\nAnd, then fix CharTokenizer to properly handle the extended chars?\n\nWe may need to use reflection to see if the current subclass \"wants\" chars or ints, and fallback to the old (current) char processing if it's a subclass that doesn't define the int-based methods. ",
            "author": "Michael McCandless",
            "id": "comment-12737732"
        },
        {
            "date": "2009-07-31T22:14:28+0000",
            "content": "michael yes thats what I had in mind.\n\nlike you mentioned, the non-final CharTokenizer-based things (Whitespace, Letter) are a problem.\n\nThe easiest way (it looks) is to put the reflection inside the non-final CharTokenizers: Whitespace and Letter.\nI want them to work correctly if you have not subclassed them, but if you have, it should call your char based method (which wont work for supp characters, but it didnt before)!\n\nthoughts? ",
            "author": "Robert Muir",
            "id": "comment-12737740"
        },
        {
            "date": "2009-07-31T22:21:42+0000",
            "content": "just a dump to show where I am trying to go.\nI havent done anything to Whitespace/Letter yet, but you can see the changes to CharTokenizer. ",
            "author": "Robert Muir",
            "id": "comment-12737745"
        },
        {
            "date": "2009-07-31T22:24:39+0000",
            "content": "by the way i plan to replace any scary bit operations with UnicodeUtil functions. JDK doesnt have methods to yank the lead or trail surrogate out of an int, except in sun.text.normalizer of course  ",
            "author": "Robert Muir",
            "id": "comment-12737747"
        },
        {
            "date": "2009-08-03T12:19:18+0000",
            "content": "I think instead of the way I prototyped in the first patch, it might be better to have the original chartokenizer incrementToken definition still available in the code.\nthis is some temporary code duplication but would perform better for the backwards compat case, and the backwards compatibility would be more clear to me at least. ",
            "author": "Robert Muir",
            "id": "comment-12738312"
        },
        {
            "date": "2009-08-08T10:01:02+0000",
            "content": "\nI think instead of the way I prototyped in the first patch, it might be better to have the original chartokenizer incrementToken definition still available in the code.\nthis is some temporary code duplication but would perform better for the backwards compat case, and the backwards compatibility would be more clear to me at least.\n\nI suppose we could simply make an entirely new class, which properly handles surrogates, and deprecate CharTokenizer in favor of it?  Likewise we'd have to make new classes for the current subclasses of CharTokenizer (Whitespace,LetterTokenizer).  That would simplify being back compatible. ",
            "author": "Michael McCandless",
            "id": "comment-12740866"
        },
        {
            "date": "2009-08-08T13:27:49+0000",
            "content": "Michael, I do think that would be the simplest, but most invasive (we would have to make new duplicate classes for Arabic & Russian even!) \ni think i will try to create a patch for my 2nd method, and if it cannot be made simple, we can always do the new class/deprecation route? ",
            "author": "Robert Muir",
            "id": "comment-12740896"
        },
        {
            "date": "2009-08-09T04:12:07+0000",
            "content": "patch with a different technique for CharTokenizer and friends. I like this much more.\n\nfor non-final subclasses that are moved to the new int api, I had to expose a protected boolean switch so that any old subclasses of them work as expected: see Arabic/Russian/Letter/Whitespace for examples.\n ",
            "author": "Robert Muir",
            "id": "comment-12741020"
        },
        {
            "date": "2009-08-09T12:42:24+0000",
            "content": "patch with some more work in contrib for this issue. ",
            "author": "Robert Muir",
            "id": "comment-12741073"
        },
        {
            "date": "2009-08-09T14:14:37+0000",
            "content": "In your latest patch you have this method getDeclaringClass() using the while loop. To get the class declaring a method, it is simplier to call \n\nthis.getClass().getMethod(...).getDeclaringClass()\n\nSee TokenStream.java for example where the same is done for the new TokenStream API. ",
            "author": "Uwe Schindler",
            "id": "comment-12741083"
        },
        {
            "date": "2009-08-09T14:30:15+0000",
            "content": "Uwe, unfortunately Class.getMethod only works for public methods!\n\nSo I must use Class.getDeclaredMethod in this case, because the methods in question are protected. The problem with Class.getDeclaredMethod is that it does not check superclasses like Class.getMethod does, thus the loop. ",
            "author": "Robert Muir",
            "id": "comment-12741085"
        },
        {
            "date": "2009-08-09T14:43:52+0000",
            "content": "OK, I did not know this. I read again the Class.getMethod() javadocs and it states that it only returns public methods. In the case of TokenStream bw compatibility, only public methods are affected, so I used the \"simple and faster\" way.\n\nI was already wondering why you changed this call, because the rest of the reflection code seemed to be copied from TokenStream e.g. the name of the parameter type arrays...   ",
            "author": "Uwe Schindler",
            "id": "comment-12741088"
        },
        {
            "date": "2009-08-09T14:55:25+0000",
            "content": "Uwe, and I thank you for the good example! I tried to apply your same logic but was frustrated when it wasn't working only to discover this annoyance with java reflection. ",
            "author": "Robert Muir",
            "id": "comment-12741089"
        },
        {
            "date": "2009-08-09T15:09:12+0000",
            "content": "For me it is strange, that getMethod() does not work for protected methods, because they can be inherited. That it does not work for private methods is clear, because they are only known to the class itsself (you cannot even override a private methods). But the JDK defines it in that way and we must life with it \n\nI would stop the iteration when getSuperclass() returns null and then return null (which cannot happen, as the method is defined at least in CharTokenizer, you wrote this in the comment).\n\n\nprivate Class getDeclaringClass(String name, Class[] params) {\n  for (Class clazz = this.getClass(); clazz != null; clazz = clazz.getSuperclass()) {\n    try {\n      clazz.getDeclaredMethod(name, params);\n      return clazz;\n    } catch (NoSuchMethodException e) {}\n  }\n  return null; /* impossible */\n}\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-12741091"
        },
        {
            "date": "2009-08-09T15:37:35+0000",
            "content": "Uwe, thanks. I will modify the loop in that way, it is much easier to read. ",
            "author": "Robert Muir",
            "id": "comment-12741097"
        },
        {
            "date": "2009-08-11T11:46:37+0000",
            "content": "this is just a reminder that I think if we go this route, we will need caching similar to what Uwe added for TokenStream back compat so that construction performance is not bad. ",
            "author": "Robert Muir",
            "id": "comment-12741815"
        },
        {
            "date": "2009-11-16T15:44:00+0000",
            "content": "a couple people have asked me about this issue lately, I would prefer to spin off smaller issues rather than create large patches that become out of date.\n\nalso I think Simon is interested in working on some of this, so more jira spam but i think easier to make progress. ",
            "author": "Robert Muir",
            "id": "comment-12778378"
        },
        {
            "date": "2009-11-16T18:03:02+0000",
            "content": "Yonik, or anyone else, please let me know your thoughts on the following:\n\nI don't see a real back compat issue... I can't imagine anyone relying on the fact that >BMP chars wouldn't be lowercased. To rely on that would also be relying on undocumented behavior.\nAh, OK. Actually it just occurred to me that this would also require reindexing, otherwise queries that hit documents in the past would mysteriously start missing them (for text outside the BMP).\n\nwhat should be our approach here wrt index back compat?\nFor the issues mentioned here, I cant possibly see >BMP working currently for anyone, but you are right it will change results.\n\nI don't want to break index back compat, just wanted to mention that introducing Unicode 4 support, still with API back compat, with no performance degradation, is going to be somewhat challenging already.\nIf we want to somehow support the \"broken\" analysis components for index back compat, then we have to also have a broken implementation available on top of the correct impl (probably using Version to handle this).\nIn my opinion, this would introduce a lot of complexity, I will help do it though, if we must. ",
            "author": "Robert Muir",
            "id": "comment-12778421"
        },
        {
            "date": "2009-11-16T18:10:06+0000",
            "content": "btw, its worth mentioning that this whole concept of index back compat wrt Unicode is already completely broken in Lucene.\n\nif someone upgrades to lucene 3.0 and uses java 1.5, Character.* operations already return different results than on java 1.4, so they are already broken from their previous index. ",
            "author": "Robert Muir",
            "id": "comment-12778427"
        },
        {
            "date": "2009-11-16T20:42:54+0000",
            "content": "If there is nothing we can do here, then we just have to do the best we can -\n\nsuch as a prominent notice alerting that if you transition JVM's between building and searching the index and you are using or doing X, things will break.\n\nWe should put this in a spot that is always pretty visible - perhaps even a new readme file titlted something like IndexBackwardCompatibility or something, to which we can add other tips and gotchyas as they come up. Or MaintainingIndicesAcrossVersions, or FancyWhateverGetsYourAttentionAboutUpgradingStuff. Or a permanent entry/sticky entry at the top of Changes. ",
            "author": "Mark Miller",
            "id": "comment-12778516"
        },
        {
            "date": "2009-11-16T20:46:45+0000",
            "content": "If there is nothing we can do here, then we just have to do the best we can -\n\nOh there is definitely a choice. and as I said, I am willing to help either way we go.\nI am just warning there will be a lot more complexity if we want to use say, Version to support both broken implementation and fixed implementation.\nlots of tests, and hopefully no bugs \n\nUwe already commented he would like it controlled with Version, just saying suppl. characters are notoriously tricky as it is! ",
            "author": "Robert Muir",
            "id": "comment-12778524"
        },
        {
            "date": "2009-11-16T20:51:21+0000",
            "content": "I'm speaking in regards to:\n\n\nbtw, its worth mentioning that this whole concept of index back compat wrt Unicode is already completely broken in Lucene.\nif someone upgrades to lucene 3.0 and uses java 1.5, Character.* operations already return different results than on java 1.4, so they are already broken from their previous index.\n\nWe can fix that too? If so, I think we should. The tokenstream backcompat was a bitch as well - but we had to slog through it. ",
            "author": "Mark Miller",
            "id": "comment-12778526"
        },
        {
            "date": "2009-11-16T20:57:02+0000",
            "content": "We can fix that too? If so, I think we should. The tokenstream backcompat was a bitch as well - but we had to slog through it.\n\nMark honestly, I do not yet know how this one could be reasonably done.\nThe problem is the behavior is dependent a lot upon the users JVM, how in the hell do we know which JVM version was used to create some given index???? ",
            "author": "Robert Muir",
            "id": "comment-12778527"
        },
        {
            "date": "2009-11-16T20:58:48+0000",
            "content": "I don't know if this is the right place to point this out, but: JFlex-generated scanners (e.g. StandardAnalyzer) do not properly handle supplementary characters.\n\nUnfortunately, it looks like the as-yet-unreleased JFlex 1.5 will not support supplementary characters either, so this will be a gap in Lucene's Unicode handling for a while. ",
            "author": "Steve Rowe",
            "id": "comment-12778528"
        },
        {
            "date": "2009-11-16T21:01:13+0000",
            "content": "Mark honestly, I do not yet know how this one could be reasonably done.\n\nThen thats what I am saying we should be documenting.\n\nhow in the hell do we know which JVM version was used to create some given index????\n\nIf something like this made sense long term, we can start putting it in as index metadata - not helpful now, but could end up being so. ",
            "author": "Mark Miller",
            "id": "comment-12778531"
        },
        {
            "date": "2009-11-16T21:01:18+0000",
            "content": "Steven, no its definitely the right place to point it out! I know this is true, even with 1.5 \n\nOne reason I wanted to split this issue up was to try to make 'improvements', maybe we do not fix everything.\nthere are also other options for StandardTokenizer/Jflex\nFor instance, we could not break between any surrogate pairs and classify them as CJK (index individual character) for the time being.\nWhile technically incorrect, it would handle the common cases, i.e. ideographs from Big5-HKSCS, etc.\n\nbut right now the topic is i guess on unicode and index back compat in general... trying to figure out what is the reasonable approach to handling this (supporting the old broken behavior/indexes created with them) ",
            "author": "Robert Muir",
            "id": "comment-12778532"
        },
        {
            "date": "2009-11-16T21:03:32+0000",
            "content": "Then thats what I am saying we should be documenting.\n\n+1 I think it should be in the release notes that if you upgrade Java version to either 1.5 or 1.7, you should reindex because Unicode versions change.\nBecause of this, character properties change and some tokenizers, etc will emit different results.\n\nThis is really only somewhat related to this issue, but thats what my email thread on java-dev was all about. ",
            "author": "Robert Muir",
            "id": "comment-12778536"
        },
        {
            "date": "2012-09-19T19:20:25+0000",
            "content": "Robert, is there anything left to do here?  I think this issue can be resolved as fixed. ",
            "author": "Steve Rowe",
            "id": "comment-13458997"
        },
        {
            "date": "2013-01-12T23:06:52+0000",
            "content": "Resolving.  Any remaining problems can be opened as separate issues. ",
            "author": "Steve Rowe",
            "id": "comment-13552093"
        }
    ]
}