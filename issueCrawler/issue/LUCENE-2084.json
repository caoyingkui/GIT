{
    "id": "LUCENE-2084",
    "title": "remove Byte/CharBuffer wrapping for collation key generation",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/other"
        ],
        "type": "Improvement",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "We can remove the overhead of ByteBuffer and CharBuffer wrapping in CollationKeyFilter and ICUCollationKeyFilter.\n\nthis patch moves the logic in IndexableBinaryStringTools into char[],int,int and byte[],int,int based methods, with the previous Byte/CharBuffer methods delegating to these.\nPreviously, the Byte/CharBuffer methods required a backing array anyway.",
    "attachments": {
        "LUCENE-2084.patch": "https://issues.apache.org/jira/secure/attachment/12425508/LUCENE-2084.patch",
        "collation.benchmark.tar.bz2": "https://issues.apache.org/jira/secure/attachment/12428974/collation.benchmark.tar.bz2",
        "TopTFWikipediaWords.tar.bz2": "https://issues.apache.org/jira/secure/attachment/12428973/TopTFWikipediaWords.tar.bz2"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-12-26T06:06:49+0000",
            "content": "synched to current trunk, after the LUCENE-2124 move ",
            "author": "Steve Rowe",
            "id": "comment-12794586"
        },
        {
            "date": "2009-12-26T06:25:26+0000",
            "content": "Atached collation.benchmark.tar.bz2, which contains stuff to run an analysis-only contrib benchmark for the (ICU)CollationKeyAnalyzers over 4 languages: English, French, German, and Ukrainian.\n\nIncluded are:\n\n\n\tFor each language, a line-doc containing the most frequent 100K words from a corresponding Wikipedia dump from November 2009;\n\tFor each language, Java code for a no-argument analyzer callable from a benchmark alg, that specializes (ICU)CollationKeyAnalyzer and uses PerFieldAnalyzerWrapper to only run it over the line-doc body field\n\tA script to compile and jarify the above analyzers\n\tA benchmark alg running 5 iterations of 10 repetitions of analysis only over the line-doc for each language\n\tA script to find the minimum elapsed time for each combination, and output the results as a JIRA table\n\tA script to run the previous two scripts once for each of three JDK versions\n\tA script to compare the output of the above script before and after applying the attached patch removing Char/ByteBuffer wrapping, and output the result as a JIRA table\n\n ",
            "author": "Steve Rowe",
            "id": "comment-12794587"
        },
        {
            "date": "2009-12-26T06:47:03+0000",
            "content": "To run the benchmark:\n\n\n\tUnpack collation.benchmark.tar.bz2 in a full Lucene-java tree into the contrib/benchmark/ directory.  All contents will be put under a new directory named collation/.\n\tCompile and jarify the localized (ICU)CollationKeyAnalyzer code: from the collation/ directory, run the script build.test.analyzer.jar.sh.\n\tFrom an unpatched java/trunk/, build Lucene's jars: ant clean package.\n\tFrom the contrib/benchmark/ directory, run the collation benchmark: collation/run-benchmark.sh > unpatched.collation.bm.table.txt\n\tApply the attached patch to the Lucene-java tree\n\tFrom java/trunk/, build Lucene's jars: ant clean package.\n\tFrom the contrib/benchmark/} directory, run the collation benchmark: {{collation/run-benchmark.sh > patched.collation.bm.table.txt\n\tProduce the comparison table:  collation/compare.collation.benchmark.tables.pl unpatched.collation.bm.table.txt patched.collation.bm.table.txt > collation.diff.bm.table.txt\n\n\n ",
            "author": "Steve Rowe",
            "id": "comment-12794588"
        },
        {
            "date": "2009-12-26T06:54:32+0000",
            "content": "Steven this looks cool. trying to coerce my little netbook into running this thing now but I took a glance already.\n\nDo you think after this issue is resolved (whether it helps or doesn't help/won't fix either way) that we should open a separate issue to work on committing the benchmark so we have collation benchmarks for the future? Seems like it would be good to have in the future. ",
            "author": "Robert Muir",
            "id": "comment-12794589"
        },
        {
            "date": "2009-12-26T07:05:51+0000",
            "content": "Here are the unpatched results I got - these look quite similar to the results I posted from a custom (non-contrib-benchmark) benchmark in the description of LUCENE-1719 :\n\n\n\n\nSun JVM\nLanguage\njava.text\nICU4J\nKeywordAnalyzer\nICU4J Improvement\n\n\n1.5.0_15 (32-bit)\nEnglish\n9.00s\n4.89s\n2.90s\n207%\n\n\n1.5.0_15 (32-bit)\nFrench\n10.64s\n5.12s\n2.95s\n254%\n\n\n1.5.0_15 (32-bit)\nGerman\n10.19s\n5.19s\n2.97s\n225%\n\n\n1.5.0_15 (32-bit)\nUkrainian\n13.66s\n7.20s\n2.96s\n152%\n\n\nSun JVM\nLanguage\njava.text\nICU4J\nKeywordAnalyzer\nICU4J Improvement\n\n\n1.5.0_15 (64-bit)\nEnglish\n5.97s\n2.55s\n1.50s\n326%\n\n\n1.5.0_15 (64-bit)\nFrench\n6.86s\n2.74s\n1.56s\n349%\n\n\n1.5.0_15 (64-bit)\nGerman\n6.85s\n2.76s\n1.59s\n350%\n\n\n1.5.0_15 (64-bit)\nUkrainian\n9.56s\n4.01s\n1.56s\n227%\n\n\nSun JVM\nLanguage\njava.text\nICU4J\nKeywordAnalyzer\nICU4J Improvement\n\n\n1.6.0_13 (64-bit)\nEnglish\n3.04s\n2.06s\n1.07s\n99%\n\n\n1.6.0_13 (64-bit)\nFrench\n3.58s\n2.04s\n1.14s\n171%\n\n\n1.6.0_13 (64-bit)\nGerman\n3.35s\n2.22s\n1.14s\n105%\n\n\n1.6.0_13 (64-bit)\nUkrainian\n4.48s\n2.94s\n1.21s\n89%\n\n\n\n\n\nHere are the results after applying the synced-to-trunk patch:\n\n\n\n\nSun JVM\nLanguage\njava.text\nICU4J\nKeywordAnalyzer\nICU4J Improvement\n\n\n1.5.0_15 (32-bit)\nEnglish\n8.73s\n4.61s\n2.90s\n241%\n\n\n1.5.0_15 (32-bit)\nFrench\n10.38s\n4.87s\n2.94s\n285%\n\n\n1.5.0_15 (32-bit)\nGerman\n9.95s\n4.94s\n2.97s\n254%\n\n\n1.5.0_15 (32-bit)\nUkrainian\n13.37s\n6.91s\n2.90s\n161%\n\n\nSun JVM\nLanguage\njava.text\nICU4J\nKeywordAnalyzer\nICU4J Improvement\n\n\n1.5.0_15 (64-bit)\nEnglish\n5.78s\n2.65s\n1.57s\n290%\n\n\n1.5.0_15 (64-bit)\nFrench\n6.74s\n2.74s\n1.64s\n364%\n\n\n1.5.0_15 (64-bit)\nGerman\n6.69s\n2.86s\n1.66s\n319%\n\n\n1.5.0_15 (64-bit)\nUkrainian\n9.40s\n4.18s\n1.62s\n204%\n\n\nSun JVM\nLanguage\njava.text\nICU4J\nKeywordAnalyzer\nICU4J Improvement\n\n\n1.6.0_13 (64-bit)\nEnglish\n3.06s\n1.82s\n1.09s\n170%\n\n\n1.6.0_13 (64-bit)\nFrench\n3.36s\n1.88s\n1.16s\n206%\n\n\n1.6.0_13 (64-bit)\nGerman\n3.40s\n1.95s\n1.14s\n179%\n\n\n1.6.0_13 (64-bit)\nUkrainian\n4.33s\n2.65s\n1.21s\n117%\n\n\n\n\n\nAnd here is a comparison of the two:\n\n\n\n\nSun JVM\nLanguage\njava.text improvement\nICU4J improvement\n\n\n1.5.0_15 (32-bit)\nEnglish\n5.1%\n16.8%\n\n\n1.5.0_15 (32-bit)\nFrench\n3.8%\n12.9%\n\n\n1.5.0_15 (32-bit)\nGerman\n3.9%\n13.1%\n\n\n1.5.0_15 (32-bit)\nUkrainian\n2.6%\n6.2%\n\n\nSun JVM\nLanguage\njava.text improvement\nICU4J improvement\n\n\n1.5.0_15 (64-bit)\nEnglish\n6.6%\n-2.2%\n\n\n1.5.0_15 (64-bit)\nFrench\n4.4%\n7.7%\n\n\n1.5.0_15 (64-bit)\nGerman\n5.0%\n-2.0%\n\n\n1.5.0_15 (64-bit)\nUkrainian\n3.3%\n-3.7%\n\n\nSun JVM\nLanguage\njava.text improvement\nICU4J improvement\n\n\n1.6.0_13 (64-bit)\nEnglish\n0.5%\n36.1%\n\n\n1.6.0_13 (64-bit)\nFrench\n11.4%\n25.5%\n\n\n1.6.0_13 (64-bit)\nGerman\n-1.7%\n33.8%\n\n\n1.6.0_13 (64-bit)\nUkrainian\n5.3%\n20.6%\n\n\n\n\n\nIt's not unequivocal, but there is a definite overall improvement in the patched version; I'd say these results justify applying the patch.\n\nI won't post them here, (mostly  because I didn't save them  ) but I've run the same benchmark (with some variation in the number of iterations) and noticed that while there are always a couple of places where the unpatched version appears to do slightly better, the place at which this occurs is not consistent, and the cases where the patched version improves throughput always dominate. ",
            "author": "Steve Rowe",
            "id": "comment-12794590"
        },
        {
            "date": "2009-12-26T07:19:31+0000",
            "content": "TopTFWikipediaWords.tar.bz2 contains a Maven2 project to parse unpacked Wikipedia dump files, create a Lucene index from the tokens produced by the contrib WikipediaTokenizer, iterate over the indexed tokens' term docs, accumulating term frequencies, store the results in a bounded priority queue, then output contrib benchmark LineDoc format, with the title field containing the collection term frequency, the date containing the date the file was generated, and the body containing the term text.\n\nThis code knows how to handle English, German, French, and Ukrainian, but could be extended for other languages.\n\nI used this project to generate the line-docs for the 4 languages' 100k most frequent terms, in the collation benchmark archive attachment on this issue. ",
            "author": "Steve Rowe",
            "id": "comment-12794591"
        },
        {
            "date": "2009-12-26T07:27:05+0000",
            "content": "Fixed up version of collation.benchmark.tar.bz2 that removes printing of progress from the collation/run-benchmark.sh script - otherwise the same as the previous version. ",
            "author": "Steve Rowe",
            "id": "comment-12794592"
        },
        {
            "date": "2009-12-26T07:49:38+0000",
            "content": "Do you think after this issue is resolved (whether it helps or doesn't help/won't fix either way) that we should open a separate issue to work on committing the benchmark so we have collation benchmarks for the future? Seems like it would be good to have in the future.\n\nYeah, I don't know quite how to do that - the custom code wrapping ICU/CollationKeyAnalyzer is necessary because the contrib benchmark alg format can only handle zero-argument analyzer constructors (or those that take Version arguments).  I think it would be useful to have a general-purpose alg syntax to handle this case without requiring custom code, and also, more generally, to allow for the construction of aribitrary analysis pipelines without requiring custom code (a la Solr schema).  The alg parsing code is a bit dense though - I think it could be converted to a JFlex-generated grammar to simplify this kind of syntax extension.\n\nCan you think of an alternate way to package this benchmark that fits with current practice? ",
            "author": "Steve Rowe",
            "id": "comment-12794593"
        },
        {
            "date": "2009-12-26T15:52:41+0000",
            "content": "\nI think it would be useful to have a general-purpose alg syntax to handle this case without requiring custom code, and also, more generally, to allow for the construction of aribitrary analysis pipelines without requiring custom code (a la Solr schema). The alg parsing code is a bit dense though - I think it could be converted to a JFlex-generated grammar to simplify this kind of syntax extension.\n\nthat would be a great improvement to the benchmark package!\n\nalternatively in the short term, i wonder if we can we somehow abuse a system property for the locale, setting it with the alg file to change the locale? ",
            "author": "Robert Muir",
            "id": "comment-12794626"
        },
        {
            "date": "2009-12-26T17:24:45+0000",
            "content": "In my opinion in the committed patch and also before there is an error in handling the ByteBuffer/CharBuffer arrayOffset() value. It calculated the length using limit()-arrayOffset() which is wrong. The length available in the buffer is simple remaining() - this is how all other encoders/decoders in the JDK work (see src). array() and arrayOffset() simply give the offset of the wrapped array, but do not say anything about the length. The starting position to decode must be also customized and is arrayOffset()+position() - The code fails, if you wrap an array with offset!=0 and some length or if you pass an not 0-positioned buffer. \n\nFor correct usage of NIO Buffers see my latest improvements in the payload.IdentityEncoder (issue LUCENE-2157). Any code getting a buffer for encoding/decoding should assume, that it should read from the buffer starting at postion(), reading remaining() items. ",
            "author": "Uwe Schindler",
            "id": "comment-12794636"
        },
        {
            "date": "2009-12-26T17:47:03+0000",
            "content": "Uwe, another thing is that currently the indexablebinarystringcode requires that the buffer have a backing array, which is optional.\n\none idea is that if we implement the code with byte[]/char[] as this patch does, perhaps we can make the buffer code (which is just helper methods around this) to remove this restriction\n\nafter all, now we have a benchmark so we can measure this and make sure it does not hurt performance for the charbuffer/bytebuffer case,  although with this patch these buffer methods are not used by any collation filters. ",
            "author": "Robert Muir",
            "id": "comment-12794637"
        },
        {
            "date": "2009-12-27T05:30:59+0000",
            "content": "Steve, I ran the benchmark, worked like a charm. here are my numbers.\nsome differences are: very slow computer, newer jvm versions, etc.\n\nUnpatched:\n\n\n\n\nSun JVM\nLanguage\njava.text\nICU4J\nKeywordAnalyzer\nICU4J Improvement\n\n\n1.5.0_22 (32-bit)\nEnglish\n31.61s\n21.34s\n10.66s\n96%\n\n\n1.5.0_22 (32-bit)\nFrench\n34.11s\n22.59s\n11.12s\n100%\n\n\n1.5.0_22 (32-bit)\nGerman\n34.59s\n22.69s\n10.84s\n100%\n\n\n1.5.0_22 (32-bit)\nUkrainian\n50.72s\n30.19s\n11.09s\n107%\n\n\nSun JVM\nLanguage\njava.text\nICU4J\nKeywordAnalyzer\nICU4J Improvement\n\n\n1.6.0_16 (32-bit)\nEnglish\n26.42s\n19.98s\n10.11s\n65%\n\n\n1.6.0_16 (32-bit)\nFrench\n28.48s\n20.14s\n10.30s\n85%\n\n\n1.6.0_16 (32-bit)\nGerman\n28.41s\n21.38s\n9.78s\n61%\n\n\n1.6.0_16 (32-bit)\nUkrainian\n40.55s\n28.56s\n9.70s\n64%\n\n\n\n\n\nPatched:\n\n\n\n\nSun JVM\nLanguage\njava.text\nICU4J\nKeywordAnalyzer\nICU4J Improvement\n\n\n1.5.0_22 (32-bit)\nEnglish\n29.59s\n19.33s\n10.56s\n117%\n\n\n1.5.0_22 (32-bit)\nFrench\n32.38s\n21.02s\n10.88s\n112%\n\n\n1.5.0_22 (32-bit)\nGerman\n32.92s\n20.58s\n10.66s\n124%\n\n\n1.5.0_22 (32-bit)\nUkrainian\n49.59s\n27.05s\n10.77s\n138%\n\n\nSun JVM\nLanguage\njava.text\nICU4J\nKeywordAnalyzer\nICU4J Improvement\n\n\n1.6.0_16 (32-bit)\nEnglish\n24.91s\n18.28s\n9.72s\n77%\n\n\n1.6.0_16 (32-bit)\nFrench\n26.30s\n19.12s\n10.25s\n81%\n\n\n1.6.0_16 (32-bit)\nGerman\n26.70s\n19.23s\n10.48s\n85%\n\n\n1.6.0_16 (32-bit)\nUkrainian\n38.48s\n25.66s\n9.80s\n81%\n\n\n\n\n\nComparison:\n\n\n\n\nSun JVM\nLanguage\njava.text improvement\nICU4J improvement\n\n\n1.5.0_22 (32-bit)\nEnglish\n10.5%\n22.2%\n\n\n1.5.0_22 (32-bit)\nFrench\n7.4%\n13.6%\n\n\n1.5.0_22 (32-bit)\nGerman\n7.1%\n19.9%\n\n\n1.5.0_22 (32-bit)\nUkrainian\n2.5%\n17.8%\n\n\nSun JVM\nLanguage\njava.text improvement\nICU4J improvement\n\n\n1.6.0_16 (32-bit)\nEnglish\n7.8%\n15.8%\n\n\n1.6.0_16 (32-bit)\nFrench\n13.7%\n11.4%\n\n\n1.6.0_16 (32-bit)\nGerman\n15.3%\n33.0%\n\n\n1.6.0_16 (32-bit)\nUkrainian\n8.0%\n19.4%\n\n\n\n ",
            "author": "Robert Muir",
            "id": "comment-12794662"
        },
        {
            "date": "2009-12-27T06:50:06+0000",
            "content": "In my opinion in the committed patch and also before there is an error in handling the ByteBuffer/CharBuffer arrayOffset() value. It calculated the length using limit()-arrayOffset() which is wrong\n\nUwe, I think you might be wrong here. \nIn this class, the javadocs state the buffer must have a backing array.\nTherefore, limit - offset is a correct way to calculate length of the entire buffer, which is what this class has always used.\n\nThe length available in the buffer is simple remaining() - this is how all other encoders/decoders in the JDK work (see src).\n\nThis is not correct, this is the remaining relative length, as remaining() is based upon position: Returns the number of elements between the current position and the limit.\nUnlike typical jdk code, this code always ignored relative position and instead encodes/decodes entire buffers.\n\nWe should not change this under this issue... it is unrelated. I do not think we should change the semantics of this encode/decode to take any relative position into account. ",
            "author": "Robert Muir",
            "id": "comment-12794663"
        },
        {
            "date": "2009-12-27T09:35:05+0000",
            "content": "updated patch:\n\n\tdeprecate the now unused CharBuffer/ByteBuffer methods.\n\tadd tests to test the char[]/byte[] based methods (keep the old NIO ones, but deprecate them)\n\tchange getEncodedLength to take byte[],int,int consistent with getDecodedLength. even though it only needs to know the number of bytes, in the future if we want to modify this encoding, we will not need to break the api.\n\tjavadocs fixes\n\n ",
            "author": "Robert Muir",
            "id": "comment-12794672"
        },
        {
            "date": "2009-12-27T09:57:15+0000",
            "content": "Therefore, limit - offset is a correct way to calculate length of the entire buffer, which is what this class has always used.\n\nThis is exactly wrong: The lengthz of the \"valid\" area inside the buffer is always limit()-position() [see javadocs of wrap()]. If arrayOffset()!=0, position() may still be 0 (because the arrayoffset is only used when wrapping byte arrays. The arrayOffset() defines the postion in the underlying array that defines the buffer's position()==0 (see javadocs).\n\nThis error is mostly no problem for dynamical allocated array buffers (as most users use), because arrayOffset() is always 0 (I have never seen a buffer with offset != 0)\n\nA problem also occurs if you wrap an array with wrap([], offset, length). the offset there is not the arrayOffset(), it is the position()! And length is after wrapping remaining(). The capacity() is the array length. Therefore it is really a bug. You deprecated it, that's good, but it should be fixed, it does not work correct.\n\nFor verifying, read the source code of ByteBuffer and HeapByteBuffer from src.zip in your jdk distrib and of course the javadocs.\n\nSo just to repeat, the correct code would be:\n\n\tstart postion in array=arrayOffset()+position()\n\tlength=remaining()\n\n\n\nThis is how the IdentityEncode for Payloads works. So deprecate and fix it.\nIf we really want to work with buffers, the correct way to implement this would be that the whole class implements CharsetEncoder. ",
            "author": "Uwe Schindler",
            "id": "comment-12794675"
        },
        {
            "date": "2009-12-27T12:36:41+0000",
            "content": "I found one place where the arrayOffset() bug occurs:\n\n\tCreate an ByteBuffer by wrapping another byte[] with offset and length.\n\tuse ByteBuffer.slice() to create an independednt buffer which position()==0 the current position of the prev ByteBuffer (which is the wrapped byte[]). This buffer will have an arrayOffset() of the original offset (which is position=0 in this buffer). The limit/cpacity of this buffer is the above length, which is remaining() on the original buffer.\n\n\n\nThe length formula is the definitely wrong, if would sometimes undeflow and give negative numbers, e.g. if the position() of the sliced buffer if e.g. 5 (so arrayoffset==5 in the slice) and the remaining bytes are e.g. 3 => length in the code before/after tha patch is -2.\n\nI will provide a testcase later that shows both bugs (position() and arrayOffset() misuse). ",
            "author": "Uwe Schindler",
            "id": "comment-12794680"
        },
        {
            "date": "2009-12-27T16:08:47+0000",
            "content": "Uwe, can we please do any nio bugfixes under another issue? \n\nIts not related to this one. This issue is about not using NIO since it only makes collation slower. ",
            "author": "Robert Muir",
            "id": "comment-12794687"
        },
        {
            "date": "2009-12-27T16:27:35+0000",
            "content": "in this patch, preserve the javadoc at the beginning of this class which describes in detail the previous behavior of this class: exactly how the length is calculated from arrayOffset() and limit()\n\nthe only difference is i change the text from\nThis class's operations are defined over CharBuffers and ByteBuffers ...\nto\nSome methods in this class are defined over CharBuffers and ByteBuffers ..\n\nimho, this existing nio behavior is documented and now deprecated and should not be changed, and definitely not changed in this issue. ",
            "author": "Robert Muir",
            "id": "comment-12794692"
        },
        {
            "date": "2009-12-27T16:59:31+0000",
            "content": "Hi, this is a simple, straightforward performance improvement between 10% and 20% on average it looks, with no backwards problems.\n\nWhat Uwe describes as a problem is not introduced here, but exists already and is documented as such in the class javadocs, and preserved for backwards compatibility.\n\nSo, I think such backwards-breakage should be under a separate issue.\n\nI will commit this one in a few days if no one objects.  ",
            "author": "Robert Muir",
            "id": "comment-12794701"
        },
        {
            "date": "2009-12-27T17:05:23+0000",
            "content": "+1\n\nAnd please add a note that the Char/ByteBuffer methods are completely broken. I have two test here that broke with negativeArraySize and so on. The person who wrote the orginbal CharBuffer code misunderstood the indeed complicated logic behind ByteBuffers.\n\nSo the only requirement from my side is to write in the deprecated message or the docs on top:\n\ninstead:\n\n * Note that this class calls array() and arrayOffset()\n * on the CharBuffers and ByteBuffers it uses, so only wrapped arrays may be\n * used.  This class interprets the arrayOffset() and limit() values returned by\n * its input buffers as beginning and end+1 positions on the wrapped array,\n * respectively; similarly, on the output buffer, arrayOffset() is the first\n * position written to, and limit() is set to one past the final output array\n * position.\n\n\n\nplease:\n\n * The methods operating on CharBuffers and ByteBuffers are\n * deprecated in favour of methods that directly operate on byte[] and char[]\n * arrays. Note that this class calls array() and arrayOffset()\n * on the CharBuffers and ByteBuffers it uses, so only wrapped arrays may be\n * used. Furthermore, the deprecated methods are only working\n * correctly, if you simply use Buffer.wrap on an array without any offsets != 0\n * and not work on buffers that are returned by the slice() method.\n * Buffers wrapped with an offset completely ignore the offset and transform\n * limit() bytes. Buffer slices are completely broken and calculate\n * the length of the buffer sometimes as negative.\n\n\n\nMaybe in better english. ",
            "author": "Uwe Schindler",
            "id": "comment-12794702"
        },
        {
            "date": "2009-12-27T17:14:35+0000",
            "content": "Thanks Uwe, I will incorporate some of this wording into the javadocs and upload a new patch.\n\nlater, if we want we can discuss changing how the nio works, perhaps it is best to break the back compat, but i am just trying to avoid doing this within the same issue as a performance improvement that is trying to preserve all back compat  ",
            "author": "Robert Muir",
            "id": "comment-12794703"
        },
        {
            "date": "2009-12-27T17:28:36+0000",
            "content": "OK.\n\nI do not want to discuss further, the correct way for both directions according to NIO would be:\n\n\nfinal int inputOffset = input.arrayOffset() + input.position();\nfinal int inputLength = input.remaining();\nfinal int outputOffset = output.arrayOffset() + output.position();\nfinal int outputLength = getEncodedLength(input.array(), inputOffset, inputLength);\noutput.limit(outputLength + output.position());\n//bogus: output.position(0);\n\n\n\nThe same the other way round (only with getEncodedLength()).\n\nIf we want to fix this, put this into the new issue. But as the methods are deprecated, with the big warning we do not need to fix it. ",
            "author": "Uwe Schindler",
            "id": "comment-12794706"
        },
        {
            "date": "2009-12-27T21:59:23+0000",
            "content": "updated patch with uwe's javadoc suggestions ",
            "author": "Robert Muir",
            "id": "comment-12794746"
        },
        {
            "date": "2009-12-28T03:40:21+0000",
            "content": "Hi Robert, I took a look at the patch and found a couple of minor issues:\n\n\n\tThe newly deprecated methods should get @Deprecated annotations (in addition to the @deprecated javadoc tags)\n\tIntelliJ tells me that the \"final\" modifier on some of the public static methods is not cool - AFAICT, although static implies final, it may be useful to leave this anyway, since unlike the static modifier, the final modifier disallows hiding of the method by sublasses?  I dunno.  (Checking Lucene source, there are many \"static final\" methods, so maybe I should tell IntelliJ it's not a problem.)\n\tUnlike getEncodedLength(byte[],int,int), getDecodedLength(char[],int,int) doesn't protect against overflow in the int multiplication by casting to long.\n\n ",
            "author": "Steve Rowe",
            "id": "comment-12794762"
        },
        {
            "date": "2009-12-28T04:32:47+0000",
            "content": "Steve, thanks for pointing these out.\n\n#3 concerns me somewhat, this is an existing problem in trunk (i guess only for enormous terms, though). Should we consider backporting a fix?\n\nI will upload a new patch, takes a couple tries for one this large on my internet connection... ",
            "author": "Robert Muir",
            "id": "comment-12794763"
        },
        {
            "date": "2009-12-28T09:05:16+0000",
            "content": "updated patch, with Steven's additions. ",
            "author": "Robert Muir",
            "id": "comment-12794781"
        },
        {
            "date": "2009-12-28T09:09:52+0000",
            "content": "for the record, i think in the future we should decide on some consistency with @Deprecated annotation in lucene. \n\nPersonally, I think it is useless  ",
            "author": "Robert Muir",
            "id": "comment-12794783"
        },
        {
            "date": "2009-12-28T09:23:14+0000",
            "content": "It is somehow useless, because you always have to add a @deprecated also to javadocs with an explanation. If we want to add it, wen can use eclipse to do it automatically with the cleanup function (like I added @Override). ",
            "author": "Uwe Schindler",
            "id": "comment-12794786"
        },
        {
            "date": "2009-12-28T09:26:29+0000",
            "content": "Uwe, I think if we are going to add it here, then we should add it everywhere separately in a later issue. \n\notherwise, we should not add it here, and remove it anywhere it is found.\n\nits 100% definitely useless if we do not consistently use it for all deprecations. ",
            "author": "Robert Muir",
            "id": "comment-12794787"
        },
        {
            "date": "2009-12-28T09:29:36+0000",
            "content": "It is now used also in new commits (not only this issue). And flex uses it very often. ",
            "author": "Uwe Schindler",
            "id": "comment-12794788"
        },
        {
            "date": "2009-12-30T15:47:24+0000",
            "content": "\n3. Unlike getEncodedLength(byte[],int,int), getDecodedLength(char[],int,int) doesn't protect against overflow in the int multiplication by casting to long.\n\n#3 concerns me somewhat, this is an existing problem in trunk (i guess only for enormous terms, though). Should we consider backporting a fix?\n\nThe current form of this calculation will correctly handle original binary content of lengths up to 136MB.  IMHO the likelihood of encoding terms this enormous with IndexableBinaryStringTools is so miniscule that it's not worth the effort to backport. ",
            "author": "Steve Rowe",
            "id": "comment-12795337"
        },
        {
            "date": "2010-01-03T09:23:19+0000",
            "content": "Committed revision 895341.\n\nThanks Steven and Uwe ",
            "author": "Robert Muir",
            "id": "comment-12795961"
        }
    ]
}