{
    "id": "SOLR-1949",
    "title": "overwrite document fails if Solr index is not optimized",
    "details": {
        "affect_versions": "1.4",
        "status": "Resolved",
        "fix_versions": [],
        "components": [
            "update"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Not A Problem"
    },
    "description": "Scenario:\n\n\tSolr 1.4 with multicore\n\tWe have a set of 5.000 source documents that we want to index.\n\tWe send these set to Solr by SolrJ API and they are added correctly. We have field ID as string and uniqueKey, so the update operation overwite documents with the same ID. The result is 4500 unique documents in Solr. Also all documents have an index field that contains the source repository of each document, we need it because we want to index another sources.\n\tAfter add operation, we send optimization.\n\n\n\nAll works fine at this point.  Solr have 4.500 documents at Solr core (and 4.500 max documents too).\n\nNow these 5.000 sources documents are updated by users, and a set of them are deleted (supose, 1000). So, now we want to update our Solr index with these change (unfortunately our repository doesn't support an incremental approach), the operations are:\n\n\n\tAt index Solr, delete documents by query  (by the field that contains document source repository). We use deleteByQuery and commit SolrJ operations.\n\tAt this point Solr core have 0 documents (but 4.500 max documents, important!!!)\n\tNow we add to Solr the new version of source documents  (4000). Remember that documents don't have unique identifiers, supose that unique items are 3000. So when add operation finish (after commit sended) Solr index must have 3.000 unique items.\n\n\n\nBut the result isn't 3.000 unique items, we obtains a random results: 3000, 2980, 2976, etc. It's a serious problem because we lost documents.\n\nWe have a workaround. At these operations just after delete operation, we send an optimization to Solr (maxDocuments are updated). After this, we send new documents. By this way the result is always fine.\n\nIn our tests, we can see that this issue is only when the new documents overwrites documents that existed in solr.\n\nThanks!!",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Lance Norskog",
            "id": "comment-12878448",
            "date": "2010-06-13T23:24:48+0000",
            "content": "Does commit with expungeDeletes=true solve this? "
        },
        {
            "author": "Miguel B.",
            "id": "comment-12879385",
            "date": "2010-06-16T17:08:11+0000",
            "content": "We are running a lot of test and now we can't get the same error. At this point I can say that the issue don't exists, so we don't use expungeDeletes=true. We don't know what really happened, it would be resolved because we removed all data directory before run new tests. \n\nWe continue with tests.\n\nMy apologies.\n "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12884849",
            "date": "2010-07-02T23:03:52+0000",
            "content": "1) In the future, please don't use jira to ask questions about odd behavior you are seeing \u2013  that is what the solr-user mailing list is for.  as a general rule you should only open a \"Bug\" in Jira issue if you have already asked a question on the solr-user mailing list and have verified that there isn't a mistake/missunderstanding in your config.\n\n2) your initial report is inconsistent and makes no sense...\n\nWe have field ID as string and uniqueKey, so the update operation overwite documents with the same ID. \n...\nRemember that documents don't have unique identifiers\n\n...if you do follow up on solr-user, please be more explicit, and clarify exactly what you are doing (showing us your schema.xml, and some sample documents is pretty much a neccdessity to make sense of problems like this that don't produce an error message) "
        }
    ]
}