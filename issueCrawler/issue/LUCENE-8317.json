{
    "id": "LUCENE-8317",
    "title": "TestStressNRT fails with missing document",
    "details": {
        "components": [],
        "status": "Closed",
        "resolution": "Fixed",
        "fix_versions": [
            "7.4",
            "master (8.0)"
        ],
        "affect_versions": "None",
        "labels": "",
        "priority": "Major",
        "type": "Bug"
    },
    "description": "11:39:01    [junit4] Suite: org.apache.lucene.index.TestStressNRT\n11:39:01    [junit4]   1> READER1: FAILED: unexpected exception\n11:39:01    [junit4]   1> java.lang.AssertionError: No documents or tombstones found for id 49, expected at least 66 reader=StandardDirectoryReader(segments_g:325:nrt _2i(7.4.0):c114/106:delGen=1 _2h(7.4.0):c76/75:delGen=1 _2j(7.4.0):c32/28:delGen=1 _2k(7.4.0):c1 _2l(7.4.0):C38/23:delGen=1 _2n(7.4.0):C6/4:delGen=1 _2m(7.4.0):C23/16:delGen=1)\n11:39:01    [junit4]   1> \tat org.junit.Assert.fail(Assert.java:93)\n11:39:01    [junit4]   1> \tat org.apache.lucene.index.TestStressNRT$2.run(TestStressNRT.java:353)\n11:39:01    [junit4]   2> May 16, 2018 7:39:01 PM com.carrotsearch.randomizedtesting.RandomizedRunner$QueueUncaughtExceptionsHandler uncaughtException\n11:39:01    [junit4]   2> WARNING: Uncaught exception in thread: Thread[READER1,5,TGRP-TestStressNRT]\n11:39:01    [junit4]   2> java.lang.RuntimeException: java.lang.AssertionError: No documents or tombstones found for id 49, expected at least 66 reader=StandardDirectoryReader(segments_g:325:nrt _2i(7.4.0):c114/106:delGen=1 _2h(7.4.0):c76/75:delGen=1 _2j(7.4.0):c32/28:delGen=1 _2k(7.4.0):c1 _2l(7.4.0):C38/23:delGen=1 _2n(7.4.0):C6/4:delGen=1 _2m(7.4.0):C23/16:delGen=1)\n11:39:01    [junit4]   2> \tat __randomizedtesting.SeedInfo.seed([B7A02DE785EE2387]:0)\n11:39:01    [junit4]   2> \tat org.apache.lucene.index.TestStressNRT$2.run(TestStressNRT.java:382)\n11:39:01    [junit4]   2> Caused by: java.lang.AssertionError: No documents or tombstones found for id 49, expected at least 66 reader=StandardDirectoryReader(segments_g:325:nrt _2i(7.4.0):c114/106:delGen=1 _2h(7.4.0):c76/75:delGen=1 _2j(7.4.0):c32/28:delGen=1 _2k(7.4.0):c1 _2l(7.4.0):C38/23:delGen=1 _2n(7.4.0):C6/4:delGen=1 _2m(7.4.0):C23/16:delGen=1)\n11:39:01    [junit4]   2> \tat org.junit.Assert.fail(Assert.java:93)\n11:39:01    [junit4]   2> \tat org.apache.lucene.index.TestStressNRT$2.run(TestStressNRT.java:353)\n11:39:01    [junit4]   2> \n11:39:01    [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestStressNRT -Dtests.method=test -Dtests.seed=B7A02DE785EE2387 -Dtests.slow=true -Dtests.badapples=true -Dtests.locale=en-AU -Dtests.timezone=Asia/Taipei -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n11:39:01    [junit4] ERROR   0.61s J1 | TestStressNRT.test <<<\n11:39:01    [junit4]    > Throwable #1: java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still 7 open files: {_2h.cfs=1, _2n.fdt=1, _2k.cfs=1, _2l.fdt=1, _2j.cfs=1, _2i.cfs=1, _2m.fdt=1}\n11:39:01    [junit4]    > \tat org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:841)\n11:39:01    [junit4]    > \tat org.apache.lucene.index.TestStressNRT.test(TestStressNRT.java:403)\n11:39:01    [junit4]    > \tat java.lang.Thread.run(Thread.java:748)\n11:39:01    [junit4]    > Caused by: java.lang.RuntimeException: unclosed IndexInput: _2l.fdt\n11:39:01    [junit4]    > \tat org.apache.lucene.store.MockDirectoryWrapper.addFileHandle(MockDirectoryWrapper.java:732)\n11:39:01    [junit4]    > \tat org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:776)\n11:39:01    [junit4]    > \tat org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.<init>(CompressingStoredFieldsReader.java:150)\n11:39:01    [junit4]    > \tat org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat.fieldsReader(CompressingStoredFieldsFormat.java:121)\n11:39:01    [junit4]    > \tat org.apache.lucene.codecs.lucene50.Lucene50StoredFieldsFormat.fieldsReader(Lucene50StoredFieldsFormat.java:173)\n11:39:01    [junit4]    > \tat org.apache.lucene.codecs.asserting.AssertingStoredFieldsFormat.fieldsReader(AssertingStoredFieldsFormat.java:43)\n11:39:01    [junit4]    > \tat org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:126)\n11:39:01    [junit4]    > \tat org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:78)\n11:39:01    [junit4]    > \tat org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:193)\n11:39:01    [junit4]    > \tat org.apache.lucene.index.BufferedUpdatesStream$SegmentState.<init>(BufferedUpdatesStream.java:265)\n11:39:01    [junit4]    > \tat org.apache.lucene.index.FrozenBufferedUpdates.openSegmentStates(FrozenBufferedUpdates.java:369)\n11:39:01    [junit4]    > \tat org.apache.lucene.index.FrozenBufferedUpdates.apply(FrozenBufferedUpdates.java:266)\n11:39:01    [junit4]    > \tat org.apache.lucene.index.IndexWriter.lambda$publishFrozenUpdates$3(IndexWriter.java:2579)\n11:39:01    [junit4]    > \tat org.apache.lucene.index.IndexWriter.processEvents(IndexWriter.java:5051)\n11:39:01    [junit4]    > \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1587)\n11:39:01    [junit4]    > \tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1576)\n11:39:01    [junit4]    > \tat org.apache.lucene.index.RandomIndexWriter.updateDocument(RandomIndexWriter.java:270)\n11:39:01    [junit4]    > \tat org.apache.lucene.index.TestStressNRT$1.run(TestStressNRT.java:274)Throwable #2: com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=472, name=READER1, state=RUNNABLE, group=TGRP-TestStressNRT]\n11:39:01    [junit4]    > Caused by: java.lang.RuntimeException: java.lang.AssertionError: No documents or tombstones found for id 49, expected at least 66 reader=StandardDirectoryReader(segments_g:325:nrt _2i(7.4.0):c114/106:delGen=1 _2h(7.4.0):c76/75:delGen=1 _2j(7.4.0):c32/28:delGen=1 _2k(7.4.0):c1 _2l(7.4.0):C38/23:delGen=1 _2n(7.4.0):C6/4:delGen=1 _2m(7.4.0):C23/16:delGen=1)\n11:39:01    [junit4]    > \tat __randomizedtesting.SeedInfo.seed([B7A02DE785EE2387]:0)\n11:39:01    [junit4]    > \tat org.apache.lucene.index.TestStressNRT$2.run(TestStressNRT.java:382)\n11:39:01    [junit4]    > Caused by: java.lang.AssertionError: No documents or tombstones found for id 49, expected at least 66 reader=StandardDirectoryReader(segments_g:325:nrt _2i(7.4.0):c114/106:delGen=1 _2h(7.4.0):c76/75:delGen=1 _2j(7.4.0):c32/28:delGen=1 _2k(7.4.0):c1 _2l(7.4.0):C38/23:delGen=1 _2n(7.4.0):C6/4:delGen=1 _2m(7.4.0):C23/16:delGen=1)\n11:39:01    [junit4]    > \tat org.apache.lucene.index.TestStressNRT$2.run(TestStressNRT.java:353)\n11:39:01    [junit4]   2> NOTE: test params are: codec=Asserting(Lucene70): {id=PostingsFormat(name=Memory)}, docValues:{}, maxPointsInLeafNode=188, maxMBSortInHeap=7.356975177594725, sim=RandomSimilarity(queryNorm=false): {id=DFR I(ne)L3(800.0)}, locale=en-AU, timezone=Asia/Taipei\n11:39:01    [junit4]   2> NOTE: Linux 4.4.0-1055-aws amd64/Oracle Corporation 1.8.0_171 (64-bit)/cpus=4,threads=1,free=105189712,total=269484032\n11:39:01    [junit4]   2> NOTE: All tests run in this JVM: [TestDirectory, TestWeakIdentityMap, TestDuelingCodecs, TestIndexWriterDelete, TestLucene70SegmentInfoFormat, TestLucene50TermVectorsFormat, TestSoftDeletesDirectoryReaderWrapper, TestTopFieldCollectorEarlyTermination, TestSpanOrQuery, TestVirtualMethod, TestComplexExplanations, TestMatchesIterator, Test2BPostings, Test2BDocs, TestAddIndexes, TestFieldType, TestTransactionRollback, TestNot, TestEarlyTerminatingSortingCollector, TestDirectMonotonic, TestSpanContainQuery, TestNoMergeScheduler, TestPositionIncrement, TestIndexWriterUnicode, TestDocInverterPerFieldErrorInfo, TestGraphTokenizers, TestAtomicUpdate, TestSortRandom, TestRecyclingByteBlockAllocator, TestBlockPostingsFormat3, TestSpanExplanations, TestAutomatonQueryUnicode, TestFilterSpans, TestPriorityQueue, TestNGramPhraseQuery, TestLongRange, TestSameScoresWithThreads, TestSingleInstanceLockFactory, TestPrefixInBooleanQuery, TestMutablePointsReaderUtils, TestDoc, TestIndexSearcher, TestOmitPositions, TestSizeBoundedForceMerge, TestCodecUtil, Test2BPagedBytes, TestPhraseQuery, TestSpanFirstQuery, TestTransactions, TestRegexpRandom2, TestDemo, TestMultiTermConstantScore, TestTermVectors, TestLucene50LiveDocsFormat, TestFilterIterator, TestMaxPosition, TestIndexTooManyDocs, TestRollback, TestPendingSoftDeletes, TestSpanExplanationsOfNonMatches, TestMathUtil, TestStressDeletes, TestStressNRT]",
    "attachments": {
        "LUCENE-8317.patch": "https://issues.apache.org/jira/secure/attachment/12923715/LUCENE-8317.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16477611",
            "author": "Simon Willnauer",
            "content": "this failed on our CI today and I think it's a real issue but pretty long standing. I have a test and a fix for it that shows the problem. I will attach in a bit. ",
            "date": "2018-05-16T15:56:20+0000"
        },
        {
            "id": "comment-16477617",
            "author": "Simon Willnauer",
            "content": "here is a patch. Michael McCandless you might be interested in this ",
            "date": "2018-05-16T15:58:49+0000"
        },
        {
            "id": "comment-16478194",
            "author": "Michael McCandless",
            "content": "\u00a0\n\nfully --> full\n\nI'm a little confused: what goes wrong if we allow flushControl.getAndResetApplyAllDeletes when it's a full flush?\u00a0 Are we really exposing a \"future delete\" in the flush? ",
            "date": "2018-05-16T22:16:34+0000"
        },
        {
            "id": "comment-16478528",
            "author": "Simon Willnauer",
            "content": "> I'm a little confused: what goes wrong if we allow flushControl.getAndResetApplyAllDeletes when it's a full flush?  Are we really exposing a \"future delete\" in the flush?\n\nyes we are exposing a future delete. We also exposing the future delete of the update which is worse I guess. If you remove the IW changes I made the test fails with 0 docs every time. ",
            "date": "2018-05-17T05:22:25+0000"
        },
        {
            "id": "comment-16478759",
            "author": "Michael McCandless",
            "content": "OK, phew, awesome find!\u00a0 +1 ",
            "date": "2018-05-17T08:50:44+0000"
        },
        {
            "id": "comment-16478790",
            "author": "Simon Willnauer",
            "content": "added changes entry. I think it's ready I will push soon once the test run finishes ",
            "date": "2018-05-17T09:13:49+0000"
        },
        {
            "id": "comment-16478833",
            "author": "Simon Willnauer",
            "content": "Michael McCandless I found one more issue while writing an evil test to cause a ton of updates of the same doc. Good news is that without my fix it fails very quickly. But there is a possible NPE in FlushByRamOrCountsPolicy.java that I fixed, can you take one more look? ",
            "date": "2018-05-17T09:59:00+0000"
        },
        {
            "id": "comment-16478955",
            "author": "Michael McCandless",
            "content": "+1 phew. ",
            "date": "2018-05-17T12:05:03+0000"
        },
        {
            "id": "comment-16478963",
            "author": "ASF subversion and git services",
            "content": "Commit 88f8718f1bfe0e5aeddc6f960cc74513a89c0610 in lucene-solr's branch refs/heads/master from Simon Willnauer\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=88f8718 ]\n\nLUCENE-8317: Prevent concurrent deletes from being applied during full flush\n\nFuture deletes could potentially be exposed to flushes/commits/refreshes if the\namount of RAM used by deletes is greater than half of the IW RAM buffer. ",
            "date": "2018-05-17T12:10:13+0000"
        },
        {
            "id": "comment-16478965",
            "author": "ASF subversion and git services",
            "content": "Commit 922fd26859cd1e288c8e9ed0d1f22bf75306de90 in lucene-solr's branch refs/heads/branch_7x from Simon Willnauer\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=922fd26 ]\n\nLUCENE-8317: Prevent concurrent deletes from being applied during full flush\n\nFuture deletes could potentially be exposed to flushes/commits/refreshes if the\namount of RAM used by deletes is greater than half of the IW RAM buffer. ",
            "date": "2018-05-17T12:11:56+0000"
        }
    ]
}