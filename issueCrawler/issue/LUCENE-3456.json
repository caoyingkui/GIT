{
    "id": "LUCENE-3456",
    "title": "Analysis Consumers should end and close any TokenStreams",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Sub-task",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "While converting consumers over to using reusableTokenStream, I notice many don't call end() or close() on TokenStreams.\n\nEven if they are test TSs only created once, we should follow the defined usage pattern.",
    "attachments": {
        "LUCENE-3456.patch": "https://issues.apache.org/jira/secure/attachment/12496389/LUCENE-3456.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2011-09-25T09:21:13+0000",
            "content": "I have some idea how to assert this is tests (now that Analyzer.(reuseable)tokenStream is final everywhere):\nWe can wrap the TokenStream reaturned by those two methods in Analyzer using a AssertingTokenFilter that simply tracks in its methods the correct usage of reset() [throw AssertionFailed if missing], increment(), end(), close() [this last one is hard to track].\n\nThe idea is:\nIf assertions are enabled, the tokenstream-returning methods in Analyzer.java should check the desiredAssertionStatus of this' class and wrap the TokenStream using that TokenFilter described before. If a consumer then forgets to call any of these methods oir does this in wrong order, an AssertionError is thrown.\n\nI could hack something together (yes, it is a hack). ",
            "author": "Uwe Schindler",
            "id": "comment-13114187"
        },
        {
            "date": "2011-09-25T09:41:31+0000",
            "content": "How would you track the end and close invocations? (or lack thereof)? ",
            "author": "Chris Male",
            "id": "comment-13114192"
        },
        {
            "date": "2011-09-25T10:50:18+0000",
            "content": "\nI could hack something together (yes, it is a hack).\n\nWe already do this in MockTokenizer?\n\nThe problem is that many disable the assertions (MockTokenizer.setEnableChecks)\n\nSo, I dont think we need a new tokenstream to test this... ",
            "author": "Robert Muir",
            "id": "comment-13114211"
        },
        {
            "date": "2011-09-25T11:10:24+0000",
            "content": "Yes it's down low, my idea was to do it top-level. But you are right, it's not really needed. And we cannot 100% handle end/close unless we reuse.\n\nThe problem my approach would solve is that Solr would be asserted all the time, not only if it uses Mock*, which is rarely does? ",
            "author": "Uwe Schindler",
            "id": "comment-13114214"
        },
        {
            "date": "2011-09-25T11:14:21+0000",
            "content": "\nThe problem my approach would solve is that Solr would be asserted all the time, not only if it uses Mock*, which is rarely does?\n\nBut perhaps a better solution to this problem would be to use MockTokenizer etc in Solr test configs (there are already enough tests testing the example config) ",
            "author": "Robert Muir",
            "id": "comment-13114217"
        },
        {
            "date": "2011-09-25T11:26:15+0000",
            "content": "Sorting this out for the current codebase isn't a huge challenge.  I work from incrementToken() / reuseable/TokenStream() calls, and see whats being called in their vicinity.  But going forward, I agree using Mock* in Solr more makes sense. ",
            "author": "Chris Male",
            "id": "comment-13114221"
        },
        {
            "date": "2011-09-25T11:41:28+0000",
            "content": "here is a patch cutting over solr/core... \n\nthis looks to have uncovered some bugs, i didnt fix any of them.\n\nadditionally i think we should do the same thing for solrj, contrib/*, etc. ",
            "author": "Robert Muir",
            "id": "comment-13114222"
        },
        {
            "date": "2011-09-25T18:25:24+0000",
            "content": "I'm iterating on this patch a bit, so we don't do duplicate work.\n\nit probably won't find all the problems, but it will fix some. ",
            "author": "Robert Muir",
            "id": "comment-13114311"
        },
        {
            "date": "2011-09-25T18:53:03+0000",
            "content": "attached is a patch cutting over all uses of whitespace->mocktokenizer in solr test configs, and fixing the errors this found.\n\ni'd like to commit shortly, we can improve the coverage more by looking at other configs and cutting over things like keywordtokenizer ",
            "author": "Robert Muir",
            "id": "comment-13114320"
        },
        {
            "date": "2011-09-26T02:31:27+0000",
            "content": "Where are we at with this Robert? I'm about to complete converting over to reusableTokenStream() in Solr.  Should I work on that or wait for you to finish here? ",
            "author": "Chris Male",
            "id": "comment-13114436"
        },
        {
            "date": "2011-09-26T02:35:32+0000",
            "content": "i have a new patch i made on the airplane, but it found more bugs... ill fix them tonight and commit! ",
            "author": "Robert Muir",
            "id": "comment-13114437"
        },
        {
            "date": "2011-09-26T02:41:26+0000",
            "content": "Alright I'll stick to Lucene and modules for a bit. ",
            "author": "Chris Male",
            "id": "comment-13114438"
        },
        {
            "date": "2011-09-26T03:24:23+0000",
            "content": "ok, i committed my stuff, just test bugs (ill backport though)... go ahead! ",
            "author": "Robert Muir",
            "id": "comment-13114444"
        },
        {
            "date": "2013-01-30T00:30:22+0000",
            "content": "Robert, Chris, Uwe: Can this be resolved now? ",
            "author": "Steve Rowe",
            "id": "comment-13566009"
        }
    ]
}