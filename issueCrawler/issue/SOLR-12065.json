{
    "id": "SOLR-12065",
    "title": "Restore replica always in buffering state",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "Backup/Restore"
        ],
        "type": "Bug",
        "fix_versions": [
            "7.3.1",
            "7.4",
            "master (8.0)"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Steps to reproduce:\n\n\u00a0\n\n\thttp://localhost:8983/solr/admin/collections?action=CREATE&name=test_backup&numShards=1&nrtReplicas=1\n\n\n\n\n\tcurl http://127.0.0.1:8983/solr/test_backup/update?commit=true -H 'Content-type:application/json' -d '\n [ {\"id\" : \"1\"}\n]'\u00a0\n\n\n\n\n\thttp://localhost:8983/solr/admin/collections?action=BACKUP&name=test_backup&collection=test_backup&location=/Users/varunthacker/backups\n\n\n\n\n\thttp://localhost:8983/solr/admin/collections?action=RESTORE&name=test_backup&location=/Users/varunthacker/backups&collection=test_restore\n\n\n\n\n\tcurl http://127.0.0.1:8983/solr/test_restore/update?commit=true -H 'Content-type:application/json' -d '\n [\n{\"id\" : \"2\"}\n]'\n\n\n\n\n\tSnippet when you try adding a document\n\n\n\n\nINFO - 2018-03-07 22:48:11.555; [c:test_restore s:shard1 r:core_node22 x:test_restore_shard1_replica_n21] org.apache.solr.update.processor.DistributedUpdateProcessor; Ignoring commit while not ACTIVE - state: BUFFERING replay: false\nINFO - 2018-03-07 22:48:11.556; [c:test_restore s:shard1 r:core_node22 x:test_restore_shard1_replica_n21] org.apache.solr.update.processor.LogUpdateProcessorFactory$LogUpdateProcessor; [test_restore_shard1_replica_n21] webapp=/solr path=/update params={commit=true}{add=[2 (1594320896973078528)],commit=} 0 4\n\n\n\tIf you see \"TLOG.state\" from http://localhost:8983/solr/admin/metrics it's always 1 (BUFFERING)",
    "attachments": {
        "12605UTLogs.txt.zip": "https://issues.apache.org/jira/secure/attachment/12916659/12605UTLogs.txt.zip",
        "logs_and_metrics.zip": "https://issues.apache.org/jira/secure/attachment/12916660/logs_and_metrics.zip",
        "restore_snippet.log": "https://issues.apache.org/jira/secure/attachment/12913466/restore_snippet.log",
        "12065.patch": "https://issues.apache.org/jira/secure/attachment/12916661/12065.patch",
        "SOLR-12065.patch": "https://issues.apache.org/jira/secure/attachment/12917680/SOLR-12065.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2018-03-07T22:57:03+0000",
            "content": "I've attached a log file from the local run:\n\n\u00a0\n\nWhat's interesting is that Line 83 says that the collection is restored successfully ( while the shard state is in \"construction\" ) while only in Line 101 that the shard state becomes active\n\n\nThis looks like a race condition as it should first become active and then complete the task.\u00a0\n\nhttps://github.com/apache/lucene-solr/blob/master/solr/core/src/java/org/apache/solr/cloud/api/collections/RestoreCmd.java#L280-L292\u00a0tries to mark it as active , refresh the cluster-state and then proceed which seems broken as it's not really refreshed yet and hence is seeing a stale state ",
            "author": "Varun Thacker",
            "id": "comment-16390397"
        },
        {
            "date": "2018-03-08T07:44:48+0000",
            "content": "I spent some time looking into this but my initial assessment is we need to replay the tlogs which will then mark it as active\n\n\u00a0\n\nrequestMap = new HashMap<>();\nfor (Slice slice : restoreCollection.getSlices()) {\nReplica oneReplica = slice.getReplicas().iterator().next();\n\nlog.info(\"Applying buffered updates on : \" + oneReplica.getName());\n\nModifiableSolrParams params = new ModifiableSolrParams();\nparams.set(CoreAdminParams.ACTION, CoreAdminParams.CoreAdminAction.REQUESTAPPLYUPDATES.toString());\nparams.set(CoreAdminParams.NAME, oneReplica.getCoreName());\n\n//ocmh.sliceCmd(clusterState, params, null, slice, shardHandler, asyncId, requestMap);\n\nocmh.sendShardRequest(oneReplica.getNodeName(), params, shardHandler, asyncId, requestMap);\n}\n\nSomething like this around Line 284 to RestoreCmd.java\n\nI'll try spending some time tomorrow and working on this\u00a0 ",
            "author": "Varun Thacker",
            "id": "comment-16390870"
        },
        {
            "date": "2018-03-17T15:31:43+0000",
            "content": "Amrit Sarkar, not that [^SOLR-12071.patch] jira. ",
            "author": "Mikhail Khludnev",
            "id": "comment-16403486"
        },
        {
            "date": "2018-03-17T16:12:45+0000",
            "content": "oh damn. Mikhail Khludnev, thanks for pointing this out. ",
            "author": "Amrit Sarkar",
            "id": "comment-16403520"
        },
        {
            "date": "2018-03-28T17:59:00+0000",
            "content": "Changed flow in RestoreCmd.java and improved the backup and restore test case:\n\n\tPost restoration index new docs to the restored collection\n\tVerify that the new documents are searchable and the original doc count + newly indexed doc count matches are correct\n\n\n\n\u00a0\n\nAdditional steps performed to test and verify:12605UTLogs.txt.ziplogs_and_metrics.zip\n\n1. http://localhost:8983/solr/admin/collections?action=CREATE&name=test_backup&numShards=2&nrtReplicas=2&replicationFactor=2&maxShardsPerNode=2\n\n2. curl http://127.0.0.1:8983/solr/test_backup/update?commit=true -H 'Content-type:application/json' -d '[ {\"id\" : \"1\"}]' \n\n3. http://localhost:8983/solr/admin/collections?action=BACKUP&name=test_backup&collection=test_backup&location=/Users/Rohit/Desktop/backup\n\n4. http://localhost:8983/solr/admin/collections?action=RESTORE&name=test_backup&location=/Users/Rohit/Desktop/backup&collection=test_restore\n\n5.\u00a0curl http://127.0.0.1:8983/solr/test_restore/update?commit=true -H 'Content-type:application/json' -d '[ {\"id\" : \"2\"}]'\n\n\u00a0\n\na) Verify doc count on all replicas for each shard.\n\n\"response\":{\"numFound\":2,\"start\":0,\"maxScore\":1.0,\"docs\":[ \n{ \"id\":\"2\"}\n, { \"id\":\"1\"}]\n\nb) Verify from Solr logs (attached)\n\nc) Verify from tlog.state in the admin/metrics api response for node 8983 and 7574 (attached)\n\n\u00a0\n\nAttaching the patch and logs to the JIRA. ",
            "author": "Rohit",
            "id": "comment-16417843"
        },
        {
            "date": "2018-03-29T20:57:51+0000",
            "content": "The patch seems great. Thanks, Rohit. Varun Thacker, would you mind to skim through? If you approve, I'll proceed with commit.    ",
            "author": "Mikhail Khludnev",
            "id": "comment-16419749"
        },
        {
            "date": "2018-03-30T18:03:38+0000",
            "content": "Hi Rohit,\n\nThanks for the patch. This looks great!\n\nSome feedback from the patch:\n\n1. In AbstractCloudBackupRestoreTestCase we added a new method indexNewDocsToCollection . Can't we reuse indexDocs ? We can make changes to that method so that it's generally more reusable for your test\nAlso in general we don't need to do Random random = new Random(docsSeed); . We can do Random r = random(); because the test setup already has a static method to get random.\n\n2. Maybe {[getDocCountInCollection}} can be something like this since we are counting the number of docs in a collection , so hitting any underlying node will be fine \n\nprivate long getDocCountInCollection(String collectionName) throws SolrServerException, IOException {\n return cluster.getSolrClient().query(collectionName, new SolrQuery(\"*:*\")).getResults().getNumFound();\n}\n\n\n\n3. RestoreCmd has a unused import. This will make ant precommit fail\n\n4. Should both of the log.info lines added to RestoreCmd be debug level? Also can we use parameterized logging ?\n\n5. After the REQUESTAPPLYUPDATES command is sent , shouldn't we validate the response? So something like this\n\n ocmh.processResponses(new NamedList(), shardHandler, true, \"REQUESTAPPLYUPDATES calls did not succeed\", asyncId, requestMap);\n\n ",
            "author": "Varun Thacker",
            "id": "comment-16420785"
        },
        {
            "date": "2018-03-30T18:05:46+0000",
            "content": "Lastly, if you upload the patch as\u00a0SOLR-12065.patch , you can then hit the submit patch button and the automated QA system will be able to validate your patch as well. That's the general convention we follow for patch naming formats. ",
            "author": "Varun Thacker",
            "id": "comment-16420787"
        },
        {
            "date": "2018-04-05T11:38:24+0000",
            "content": "1. In AbstractCloudBackupRestoreTestCase we added a new method indexNewDocsToCollection . Can't we reuse indexDocs ? We can make changes to that method so that it's generally more reusable for your test\n\n\tDone\n\n\n\n2. Maybe {[getDocCountInCollection}} can be something like this since we are counting the number of docs in a collection , so hitting any underlying node will be fine\n\n\tReused the existing\u00a0getShardToDocCountMap function\n\n\n\n3. RestoreCmd has a unused import. This will make\u00a0ant precommit\u00a0fail\n\n\tRemoved the un-used import and verified with ant precommit\u00a0\n\t[exec] Crawl/parse...\n [exec] \n [exec] Verify...\n [echo] Checking for malformed docs...\n\n\n\nprecommit:\n\nBUILD SUCCESSFUL\n Total time: 16 minutes 30 seconds\n\n4. Should both of the log.info lines added to RestoreCmd be debug level? Also can we use parameterized logging ?\n\n\tChanged at one place and used parameterised logging\n\n\n\n5. After the REQUESTAPPLYUPDATES command is sent , shouldn't we validate the response? So something like this\n\n ocmh.processResponses(new NamedList(), shardHandler, true, \"REQUESTAPPLYUPDATES calls did not succeed\", asyncId, requestMap);\n\n\n\tDone, added it to the RestoreCmd source.\n\n\n\nVerified the build on Master and state in tlog.state for test_restore collection using the admin/metrics API on two node Solr cluster\n\nSEARCHER.searcher.searcherName\":\"Searcher@70ed14d[test_restore_shard1_replica_n47] main\",\n {{ \"SEARCHER.searcher.warmupTime\":0,}}\n ......\n {{ \"TLOG.replay.remaining.bytes\":0,}}\n {{ \"TLOG.replay.remaining.logs\":0,}}\n {{ \"TLOG.state\":3,}}\"SEARCHER.searcher.registeredAt\":\"2018-04-05T11:24:20.667Z\",\n {{ \"SEARCHER.searcher.searcherName\":\"Searcher@475f6bda[test_restore_shard2_replica_n45] main\",}}\n {{ \"TLOG.replay.remaining.bytes\":104,}}\n {{ \"TLOG.replay.remaining.logs\":1,}}\n {{ \"TLOG.state\":3,}}\n\n\u00a0\n\nVarun Thacker Requesting you to have a look and let me know your feedback ",
            "author": "Rohit",
            "id": "comment-16426778"
        },
        {
            "date": "2018-04-12T00:29:03+0000",
            "content": "\nfor (Slice shard : restoreCollection.getSlices()) {\n  ocmh.waitForNewShard(restoreCollectionName, shard.getName());\n}\n\nI don't think we need to explicitly do this in RestoreCmd. We call createcollection command which internally wait's for the shard to become active. So we are good here.\n\nI removed this logging line\n\nint numberOfActiveShards = restoreCollection.getActiveSlices().size();\nlog.info(\"Number of activeShards: \"  + numberOfActiveShards);\n\nAlso in the test case I cleaned up how we are counting docs before and after indexing\n\nWith the latest patch , the HDFS backup test fails quite regularly on my machine.\n\nhere's what's happening:\n\n\twe add docs after the restore is complete\n\tcall commit\u00a0\n\tquery to assert doc count. Now if the query hits a non-leader replica and open searcher hasn't\u00a0been executed\u00a0on\u00a0the replica then gives the old count and the test fails\n\n\n\n\u00a0 ",
            "author": "Varun Thacker",
            "id": "comment-16434763"
        },
        {
            "date": "2018-04-12T02:17:27+0000",
            "content": "With the latest patch , the HDFS backup test fails quite regularly on my machine.\nLooking more closely at the logs, there were 3 replicas for the shard . And the replica that got queried was a pull replica. I think I'll revert to querying the leaders explicitly and counting the total ",
            "author": "Varun Thacker",
            "id": "comment-16434833"
        },
        {
            "date": "2018-04-12T02:47:35+0000",
            "content": "Uploaded a new patch with a CHANGES entry . Running tests and precommit on this patch before committing it ",
            "author": "Varun Thacker",
            "id": "comment-16434857"
        },
        {
            "date": "2018-04-12T15:20:52+0000",
            "content": "Commit 7a57ca8c0d10ceb23cad6fe9bc3538314ce6b6ce in lucene-solr's branch refs/heads/master from Varun Saxena\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7a57ca8c0 ]\n\nSOLR-12065: A successful restore collection should mark the shard state as active and not buffering ",
            "author": "ASF subversion and git services",
            "id": "comment-16435747"
        },
        {
            "date": "2018-04-12T17:16:19+0000",
            "content": "Commit b25ae6779c9ad875eabd8b3e2a72b7108e740a33 in lucene-solr's branch refs/heads/branch_7x from Varun Saxena\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b25ae67 ]\n\nSOLR-12065: A successful restore collection should mark the shard state as active and not buffering\n\n(cherry picked from commit 7a57ca8) ",
            "author": "ASF subversion and git services",
            "id": "comment-16435972"
        },
        {
            "date": "2018-04-12T17:16:42+0000",
            "content": "Thanks Rohit! ",
            "author": "Varun Thacker",
            "id": "comment-16435973"
        },
        {
            "date": "2018-04-25T05:39:23+0000",
            "content": "Commit 8894db1a727dff5a52444f9b4a5838995a8f7513 in lucene-solr's branch refs/heads/branch_7_3 from Varun Saxena\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=8894db1 ]\n\nSOLR-12065: A successful restore collection should mark the shard state as active and not buffering ",
            "author": "ASF subversion and git services",
            "id": "comment-16451681"
        }
    ]
}