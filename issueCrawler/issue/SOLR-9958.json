{
    "id": "SOLR-9958",
    "title": "The FileSystem used by HdfsBackupRepository gets closed before the backup completes.",
    "details": {
        "components": [
            "Hadoop Integration"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "6.2.1",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Critical"
    },
    "description": "My shards get backed up correctly, but then it fails when backing up the state from ZK. From the logs, it looks like the underlying FS gets closed before the config stuff is written:\n\nDEBUG - 2017-01-11 22:39:12.889; [   ] com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase; GHFS.close:=> \nINFO  - 2017-01-11 22:39:12.889; [   ] org.apache.solr.handler.SnapShooter; Done creating backup snapshot: shard1 at gs://master-sector-142100.appspot.com/backups2/tim5\nINFO  - 2017-01-11 22:39:12.889; [   ] org.apache.solr.servlet.HttpSolrCall; [admin] webapp=null path=/admin/cores params={core=gettingstarted_shard1_replica1&qt=/admin/cores&name=shard1&action=BACKUPCORE&location=gs://master-sector-142100.appspot.com/backups2/tim5&wt=javabin&version=2} status=0 QTime=24954\nINFO  - 2017-01-11 22:39:12.890; [   ] org.apache.solr.cloud.BackupCmd; Starting to backup ZK data for backupName=tim5\nINFO  - 2017-01-11 22:39:12.890; [   ] org.apache.solr.common.cloud.ZkStateReader; Load collection config from: [/collections/gettingstarted]\nINFO  - 2017-01-11 22:39:12.891; [   ] org.apache.solr.common.cloud.ZkStateReader; path=[/collections/gettingstarted] [configName]=[gettingstarted] specified config exists in ZooKeeper\nERROR - 2017-01-11 22:39:12.892; [   ] org.apache.solr.common.SolrException; Collection: gettingstarted operation: backup failed:java.io.IOException: GoogleHadoopFileSystem has been closed or not initialized.\n    at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.checkOpen(GoogleHadoopFileSystemBase.java:1927)\n    at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.mkdirs(GoogleHadoopFileSystemBase.java:1367)\n    at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1877)\n    at org.apache.solr.core.backup.repository.HdfsBackupRepository.createDirectory(HdfsBackupRepository.java:153)\n    at org.apache.solr.core.backup.BackupManager.downloadConfigDir(BackupManager.java:186)\n    at org.apache.solr.cloud.BackupCmd.call(BackupCmd.java:111)\n    at org.apache.solr.cloud.OverseerCollectionMessageHandler.processMessage(OverseerCollectionMessageHandler.java:222)\n    at org.apache.solr.cloud.OverseerTaskProcessor$Runner.run(OverseerTaskProcessor.java:463)\n    at org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor.lambda$execute$0(ExecutorUtil.java:229)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)",
    "attachments": {
        "SOLR-9958.patch": "https://issues.apache.org/jira/secure/attachment/12847128/SOLR-9958.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-01-12T01:10:13+0000",
            "author": "Timothy Potter",
            "content": "I changed the title because I don't see the HdfsBackupRepository's close method being called before that error in the description occurs, so it looks like something else closed FileSystem from out under the repo! FileSystem.close closes it for all that were retrieved using FileSystem.get. Not sure if this patch (for 6.2.1) is the correct approach, but it fixes the problem and allows the backup to complete correctly. Basically, does FileSystem.newInstance instead of FileSystem.get. This may just be a bug in the underlying Google cloud storage impl, but I think we should try to work around it if possible and this seems like a reasonable approach to me. However, I haven't had my head in HDFS code in a long while, so may be missing something ... ",
            "id": "comment-15819763"
        }
    ]
}