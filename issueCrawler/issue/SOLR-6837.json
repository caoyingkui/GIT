{
    "id": "SOLR-6837",
    "title": "Inconsistent replicas when update is succesful against leader partitioned from all replicas",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "4.10.2",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "Refer to the following question on solr-user:\nhttps://www.marshut.net/kttiuz/inconsistent-doc-value-across-two-nodes-very-simple-test-what-s-the-expected-behavior.html\n\n\nConfig\nSolr 4.7.2 / Jetty. \nSoldCloud on two nodes, and  3 ZK, all running in localhost. \nsingle collection: single shard with two replicas.\n\nReproducing:\nstart node1 9.148.58.114:8983\nstart node2 9.148.58.114:8984\nCluster state: node1 leader. node2 active.\n\nindex value 'A' (id=\"change me\").\nquery and expect 'A' -> success\n\nStop node2\nCluster state: node1 leader. node2 gone.\nquery and expect 'A' -> success\n\nUpdate document value from 'A'->'B'\nquery and expect 'B' -> success\n\nStop node1\nthen\nStart node2.\nCluster state: node1 gone. node2 down.\n\n    104510 [coreZkRegister-1-thread-1] INFO  org.apache.solr.cloud.ShardLeaderElectionContext Waiting until we see more replicas up for shard shard1: total=2 found=1 timeoutin=5.27665925E14ms\n\nwait 3m.\n\n    184679 [coreZkRegister-1-thread-1] INFO  org.apache.solr.cloud.ShardLeaderElectionContext  I am the new leader: http://9.148.58.114:8984/solr/quick-results-collection_shard1_replica2/ shard1    \nCluster state: node1 gone. node2 leader.\n\nquery and expect 'A' (old value) -> success\n\nstart node1\nCluster state: node1 actove. node2 leader.\n\nInconsistency: \n    Querying node1 always returns 'B'. http://localhost:8983/solr/quick-results-collection_shard1_replica1/select?q=*%3A*&wt=json&indent=true\n    Querying node1 always returns 'A'. http://localhost:8984/solr/quick-results-collection_shard1_replica2/select?q=*%3A*&wt=json&indent=true\n\nIn such a case, the final steady state of the system has inconsistent replicas.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2014-12-11T20:14:04+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "My reply to the email:\n\n\nA write in Solr, by default, is only guaranteed to exist in 1 place i.e. the leader and the safety valves that we have to preserve these writes are:\n\n1. The leaderVoteWait time for which leader election is suspended until enough live replicas are available\n2. The two-way peer-sync between leader candidate and other replicas\n\nThe other safety valve is on the client side with the \"min_rf\" parameter introduced by SOLR-5468 in Solr 4.9. If you set this param to 2 while making the request then Solr will return the number of replicas to which it could successfully send the update. Then depending on the response you can make a decision to retry the update at a later time assuming it is idempotent. This kinda puts the onus ensuring consistency on the client side which is not ideal but better than nothing. See SOLR-5468 for more discussion on this topic.\n\nIn your particular example, none of these safeties are invoked because you start node2 while node1 was down and node2 goes ahead with leader election after the wait period. Also since node1 was down during leader election, peer sync doesn't happen and then node2 becomes the leader.\n\nWhen node1 comes back online and joins as a replica, it recovers from the leader using peer-sync (which returns the newest 100 updates) and finds that there's nothing newer on the leader. However, there are no checks to make sure that the replica doesn't have a newer update itself which is why you end up with the inconsistent replica. If there were a lot of updates on node2 (more than 100) while node1 was down, in which case peer-sync isn't applicable, then it'd would have done a replication recovery and this inconsistency would have been resolved.\n\nSo yeah we have a valid consistency bug such that we have inconsistent replicas in a steady state. I wonder if the right way is to bump min_rf to a higher value or peer-sync both ways during replica recovery. I'll need to think more on this. ",
            "id": "comment-14243069"
        }
    ]
}