{
    "id": "SOLR-11",
    "title": "DeDupTokenFilter{Factory}",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "1.1.0"
        ],
        "components": [
            "search"
        ],
        "type": "Wish",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "I recently noticed a situation in which my Query analyzer was producing the same Token more then once, resulting in it getting two equally boosted clauses in the resulting query.  In my specific case, i was using the same synonym file for multiple fields (some stemmed some not) and two synonyms for a word stemmed to the same root, which ment that particular word was worth twice as as any of the other variations of the synonym \u2013 but I can imagine other situations where this might come up, both at index time and at query time, particularlay when using SynonymFilter in combination with the WordDelimiter filter.\n\nIt occured to me that a DeDupFilter would be handy.  In it's simplest form it would drop any Token it gets where the startOffset, endOffset,termText,and type are all identical to the previous token and the positionIncriment is 0.  A more robust implimentation might support init options indicating that only certain combinations of those things should be used to determine equality (ie: just termText, just termText and positionIncriment=0, etc...) but in this case, an option might also be neccessary to determine with of the Tokens should be propogated (the first of the last)",
    "attachments": {
        "solr.analysis.RemoveDuplicateTokensFilter.java": "https://issues.apache.org/jira/secure/attachment/12326463/solr.analysis.RemoveDuplicateTokensFilter.java",
        "solr.analysys.RemomveDuplicateTokensFilter.linkedhashmap.java": "https://issues.apache.org/jira/secure/attachment/12326473/solr.analysys.RemomveDuplicateTokensFilter.linkedhashmap.java",
        "SOLR-11-BufferedTokenStream-RemoveDuplicatesTokenFilter.patch": "https://issues.apache.org/jira/secure/attachment/12336171/SOLR-11-BufferedTokenStream-RemoveDuplicatesTokenFilter.patch",
        "ArrayQueue.java": "https://issues.apache.org/jira/secure/attachment/12336388/ArrayQueue.java"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Richard \"Trey\" Hyde",
            "id": "comment-12378681",
            "date": "2006-05-10T01:19:38+0000",
            "content": "This one is working well for me.   It also fixes a number of the problems that came up after my SOLR-14 patch. "
        },
        {
            "author": "Richard \"Trey\" Hyde",
            "id": "comment-12378683",
            "date": "2006-05-10T01:20:45+0000",
            "content": "Sorry for no factory class.  My code repo is way too divergent for a clean diff at the moment but the factory is fairly obvious. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12378702",
            "date": "2006-05-10T02:22:54+0000",
            "content": "It looks like this implementation is O(n^2), right?\nIt also looks like it removes all duplicates for a whole field, not limiting itself to those tokens with a positionIncrement=0.  Is this the indended behavior? "
        },
        {
            "author": "Richard \"Trey\" Hyde",
            "id": "comment-12378708",
            "date": "2006-05-10T02:31:53+0000",
            "content": "re algorith,: Yes, it would not be advisable for large fields.   \n I'm not all that familiar with the subtleties of position but for at least some of my data, I am geting duplicates in other posistions. "
        },
        {
            "author": "Richard \"Trey\" Hyde",
            "id": "comment-12378728",
            "date": "2006-05-10T03:08:55+0000",
            "content": "Alternate implementation based on LinkedHashMap instead of a LinkedList queue.   Should work better for larger data sets.   Still doesn't pay attention to position but I don't know enough to know if that is important. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12378758",
            "date": "2006-05-10T04:33:39+0000",
            "content": "DeDuping purely on tem text without any regard for position seems a litle extreme for most cases ... it will seriously throw off term frequencies and phrase queries.\n\nI'm sure it has benefits, but i'd prefer a solution where it's that behavior is an \"option\" and the defualt is to only rmeove duplicate tokens at the same position. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12378759",
            "date": "2006-05-10T04:46:17+0000",
            "content": "Another issue is memory use for very large fields (the current impl doesn't stream).\n\nIt seems like a lot of token filter implementations need some kind of buffering that could be refactored into a TokenQueue, or perhaps a base-class BufferedTokenFilter. "
        },
        {
            "author": "Richard \"Trey\" Hyde",
            "id": "comment-12378780",
            "date": "2006-05-10T06:34:48+0000",
            "content": "YMMV, I had to put this together rather quickly this morning  to dedupe tokens in different positions.  But again, that's a result of my imperfect SOLR-14 patch.  I just thought I'd get a little discussion started on the topic.  I'd like to see some solution integrated.\n\nRe BufferedTokenFilter.   That would be great.  It's much easier to deal with context and symantics when you have forward and reverse state... especially for non-Java-literate people like myself. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12418518",
            "date": "2006-06-30T02:18:08+0000",
            "content": "Back in this email...\n\nhttp://www.nabble.com/BufferingTokenStream-and-RemoveDups-p4320716.html\n\n...yonik posted an off the cuff solution to this probem that also included a nice reusable \"BufferedTokenStream\"\n\nI've cleaned it up, added some tests, and fixed the bugs the tests surfaced (mainly the infinite loops and the fact that the dup detection ignored every token with a non zero position gap thta was followed by a 0 position gap)\n\nI'll commit this in the next few days unless anyone has any objections/comments regarding ways to imporve it.  the RemoveDuplicatesTokenFilter.process method is much less elegant then Yonik's orriginal version, but that's the only way i could get it to work.  I'd welcome any suggestions on regaining the elegance without breaking the tests.\n "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12419337",
            "date": "2006-07-06T04:44:47+0000",
            "content": "I looked it over quick, looks fine to me!\n\nA few weeks ago I did some more premature optimization, writing a circular queue (power-of-two based) that's about twice as fast as a LinkedList for our typical usage.  I was intending it for use in BufferedTokenFilter if insertion/removal of tokens in the middle of the buffers was unneeded (or rare, as it could be implemented).\n\nAnyway, I'm attaching it here for lack of a better place.  I support committing the current BufferedTokenFilter as-is, since I doubt the LinkedList implementation will be any kind of bottleneck. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12419408",
            "date": "2006-07-06T12:40:00+0000",
            "content": "patch commited as is, plus some additional test cases and example schema usages. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12589357",
            "date": "2008-04-15T23:56:44+0000",
            "content": "This bug was modified as part of a bulk update using the criteria...\n\n\n\tMarked (\"Resolved\" or \"Closed\") and \"Fixed\"\n\tHad no \"Fix Version\" versions\n\tWas listed in the CHANGES.txt for 1.1\n\n\n\nThe Fix Version for all 38 issues found was set to 1.1, email notification\nwas suppressed to prevent excessive email.\n\nFor a list of all the issues modified, search jira comments for this\n(hopefully) unique string: 20080415hossman3 "
        }
    ]
}