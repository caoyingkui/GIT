{
    "id": "SOLR-6491",
    "title": "Umbrella JIRA for managing the leader assignments",
    "details": {
        "affect_versions": "5.0,                                            6.0",
        "status": "Closed",
        "fix_versions": [
            "5.0",
            "6.0"
        ],
        "components": [],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Leaders can currently get out of balance due to the sequence of how nodes are brought up in a cluster. For very good reasons shard leadership cannot be permanently assigned.\n\nHowever, it seems reasonable that a sys admin could optionally specify that a particular node be the preferred leader for a particular collection/shard. During leader election, preference would be given to any node so marked when electing any leader.\n\nSo the proposal here is to add another role for preferredLeader to the collections API, something like\nADDROLE?role=preferredLeader&collection=collection_name&shard=shardId\n\nSecond, it would be good to have a new collections API call like ELECTPREFERREDLEADERS?collection=collection_name\n(I really hate that name so far, but you see the idea). That command would (asynchronously?) make an attempt to transfer leadership for each shard in a collection to the leader labeled as the preferred leader by the new ADDROLE role.\n\nI'm going to start working on this, any suggestions welcome!\n\nThis will subsume several other JIRAs, I'll link them momentarily.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Erick Erickson",
            "id": "comment-14126133",
            "date": "2014-09-08T21:27:06+0000",
            "content": "I think the functionality of all three of these JIRAs will be provided by this one. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-14127074",
            "date": "2014-09-09T15:10:12+0000",
            "content": "Right, Noble's comment on the dev list that it would be nice to explain why this is desirable is well taken. Heck, I know what I'm thinking, don't others? ....\n\nSee the referenced JIRAs. But what I've seen \"in the wild\" is that depending on how a cluster comes up, all the leaders can wind up on a single (or small number) of machines. Since updates do put  some extra load on the leader, this can create an odd load distribution.\n\nThere's no really good external method to rebalance leaders without bouncing nodes and hoping that leaders come up \"in the right place\". The idea here is to allow the sys admin to establish a \"model leader distribution\" via the \"preferredLeader\" attribute, and then be able to trigger something like \"rebalance leaders for collection X\" to bring the actuality close to the model. The preferredLeader role would also tend to bring the actual leader nodes for particular collections into congruence with the model over time I'd guess, b/c any time leader election takes place for a shard, the preferred leader would probably be elected as leader (if it's up).\n\nNothing about this is set in stone. By that I mean the preferredLeader role is a \"hint\", not an absolute requirement. Really a \"try me first\" operation not \"require that I be the leader\" rule.\n\nI'm a bit nervous about the \"rebalance leaders for collection X\" command, I'm not quite sure yet how/whether one needs to throttle this. I mean if a cluster has 150 shards, having them all re-elect leaders at the same time while under heavy indexing load seems ....fraught. I don' think this is insurmountable however, I'll see some more about this as I get deeper into the code. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14134953",
            "date": "2014-09-16T04:52:51+0000",
            "content": "SOLR-5476 , SOLR-5893 does the same for overseer "
        },
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-14134954",
            "date": "2014-09-16T04:53:18+0000",
            "content": "Since updates do put some extra load on the leader, this can create an odd load distribution.\n\nWhat's the extra load? Every replica writes to the transaction log and the index. The leader has an extra thread or two to write updates to replicas but that is it. I don't see why that should be expensive or create undue load. We've had bugs in the past such as SOLR-6136 but if there are others then we should find those and fix them before we go through with this feature. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-14134975",
            "date": "2014-09-16T05:16:39+0000",
            "content": "Noble Paul right, but AFAIK, the Overseer stuff isn't particularly sensitive to collections and shards. Is there a notion of assigning an overseer role for a particular node but only for a particular collection/shard combination? All you've got is the role and the node, I don't see any way to say \"add a role for this collection and this shard.\n\nOf course if there is a way I've just wasted a bunch of time.\n\nShalin Shekhar Mangar I'm trusting some folks who are reporting the edge case, hoping they'll chime in with \"real world\". But even if not, \"an extra thread or two\" times 100 shards can mount up. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14134978",
            "date": "2014-09-16T05:19:00+0000",
            "content": "It was just an FYI . Just in case it helps you\n\nBTW , to add to Shalin's point . I'm not yet convinced that the performance hit is significant enough  . Do we have any real users reporting this as a problem? "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14135171",
            "date": "2014-09-16T09:06:58+0000",
            "content": "These are the concerns with the leadership mechanism as it stands currently, with no balancing (which would result in leaders all ganging up on one set of machines). I am talking based on experience with a NRT system with a fairly high rate of indexing, very low commit interval, and hundreds of shards (50+ on each machine).\n\n\n\tThe biggest performance issue is not during indexing normally but when some replicas are recovering. In such a case, the machines with leaders have to service around 50+ IO intensive recovery operations, indexing can really take a hit during this time (we have seen indexing latency increase by a few times).\n\t\n\t\tSOLR-6485 somewhat improves this situation, but is a compromise really, it increases the time taken for recovery when you could really spread the IO load on different machines, doesn't help prevent \"spikiness\" (you hit IO hard for a few 100ms, and then stay quiet for a few 100ms more), and is risky in a cloud environment because recovery can be spontaneous (say, a ZK disconnect) \u2013 in such a case, the system is already vulnerable due to unplanned, reduced capacity and this prolongs that situation.\n\t\n\t\n\tOverseer is hit harder when a machine with leaders dies, or goes down, or if there's a ZK expiry on a Solr instance with all cores being leaders. You have a lot more elections happening at the same time, and despite various improvements done to Overseer recently, it's finally bound as well by how fast ZK can respond. This in turn impacts the amount of time replicas find themselves without noticing a leader and hence ingestion slows down considerably.\n\t\n\t\tA lesser case of this is when an instance encounters a ZK expiry, you need to re-elect each one of the cores in it if all the leaders gang up in one place.\n\t\n\t\n\tIf the machine containing the leaders dies, then there's a ephemeral node timeout which would affect indexing in general even before elections kick in. This is a lot worse (affects a lot more documents) if leadership is concentrated on a machine.\n\tEven if instances on a 'leader' machine are orderly shutting down, there's a time delay between the instance shutting down and the instance losing it's leadership because of the servlet model we are currently tied to (the container first refuses connections, then gets the servlet to deal with it). Having leaders in one place leads to more documents being affected by this. I agree this however could potentially be solved by other mechanisms, for example, by having a different handler which forces cores to let go of leadership, which is called by a script prior to shutdown, or ideally, by getting rid of the servlet model as the long term plan is..\n\n "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14135178",
            "date": "2014-09-16T09:13:21+0000",
            "content": "From what I see , we just need to ensure that leaders are chosen in different hosts if possible. It can be done automatically by the system. We can avoid unnecessary work by the users "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14135264",
            "date": "2014-09-16T10:45:42+0000",
            "content": "I think that's part of the plan, see SOLR-6513 "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14135269",
            "date": "2014-09-16T10:49:20+0000",
            "content": "I don't think they are the same. I mean , it should need no external configuration. The system should do it implicitly.\n\nIf we go by my suggestion, nobody is a preferred leader, any node which has least no:of leaders at that given point in time becomes the preferred leader "
        },
        {
            "author": "Mark Miller",
            "id": "comment-14135601",
            "date": "2014-09-16T15:34:11+0000",
            "content": "All you have to do is run a half real cluster for half a second to know the leader is a hot spot...\n\nThat it has to be dealt with is not a doubt. How it's done...that is where my doubts pop up... "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-14135786",
            "date": "2014-09-16T17:37:13+0000",
            "content": "Right, there's an ecosystem here, several related bits to bring it all together. At least here's my current vision, I'm completely open to suggestions...\n\n1> assigning preferred leader roles.\n1a> manually via the ADDREPLICAROLE (SOLR-6512)\n1b> automatically via SOLR-6513. NOTE: this will just update the clusterstate with the role assignments for a single collection, it won't trigger any leader re-election.\n\n2> giving preference during leader election to those assigned roles. Not absolutely enforcing that assignment, but putting the preferred leader at the head of the list whenever leader election is happening for some other reason. Mechanism TBD. I don't think this has a lot of impact on the system, but you never know. Probably needs a new JIRA, this one is turning into an umbrella JIRA\n\n3> when the system does get out of whack, a collections API to \"try to make all the leaders the preferred leader now\". The mechanism here is very much TBD, the last thing we need to do is flood the system with a zillion leader elections so it'll have to be throttled somehow. (SOLR-6517)\n\nIt may be that <3> becomes an infrequent event. With <2> in place, the system will tend towards the leadership topology that's set up, but I'm pretty sure there'll still be occasions when it'll be required.\n\nAnd consider the pathological situation we face now. Hypothetically all the leaders can be on a single node. In fact situations approaching this have been observed \"in the field\". If that node goes down, all leaders are elected at once.\n\nAnyway, as I said this isn't cast in stone. It does seem that one approach would be a background process (possibly in the Overseer code?) whose job is to try to keep things in balance by issuing \"re-elect leader\" commands whenever the actual leader isn't the preferred leader and the node that is the preferred leader is live. \n\nOr maybe a way to delay execution of overseer tasks for N seconds, the idea here would be that <3> above would find all the leaders that were improperly assigned and issue all the \"relectMeAsLeaderIfPossible\" commands at once but with a time in the future to run, so you'd get Overseer commands like \"relectMeAsLeaderIfPossible in 5 seconds\" for some node, \"relectMeAsLeaderIfPossible in 10 seconds\" for the next, etc. to keep from flooding the system with election requests.\n\nBut I'm into code that has lots of complications, any guidance quite welcome. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14135851",
            "date": "2014-09-16T18:06:35+0000",
            "content": "I'm still not convinced, why we need t mark some node is preferred or not. \n\nA node may be preferred for  sometime and after sometime it can change . The state of the cluster can change so quickly that these \"preferred\" tags have no meaning. \nHowever it would help to have a command to just say switch leader for shardX from node A to nodeB\n\nWhat you really need is a more intelligent leader election mechanism. It can be like this\n\nAfter a new leader election happens overseer does a quick check on all the nodes where the shard has replicas, and find out if there is a more suitable node for that shard in the cluster. If yes, force that node to become the shard leader. This behavior can be enabled at a collection level flag (or even by a cluster property),if required.  I would say this behavior must be default.  "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-14136004",
            "date": "2014-09-16T19:14:08+0000",
            "content": "bq: A node may be preferred for sometime and after sometime it can change\nTrue. That's where the SOLR-6513 comes in. Remember also that this is a preference, not an absolute map.\n\nbq: After a new leader election happens overseer does a quick check on all the nodes where the shard has replicas\nOne situation we're talking about here is one in which the pathology has already happened. For illustration, consider the following:\n\nI have 10 shards and 10 nodes. I have 100 replicas, i.e. each node has one replica for every shard.\nAll the nodes are shut down.\nI start 1 node. All 10 leaders are elected on that node.\nNow I bring up my other 9 machines.\n\nNo leader election goes on here as the other 9 nodes come up. So \"After a new leader election happens overseer does a quick check on all the nodes where the shard has replicas, and find out if there is a more suitable node for that shard in the cluster\" doesn't apply since there's no leader election going on. I suppose we could just fire a \"balance leaders\" command, but that kind of pre-supposes a symmetric cluster.\n\nI'm reluctant to try to do everything automatically. Consider an asymmetric cluster, both in terms of hardware and collection characteristics. You have machines of varying power hosting (perhaps many) collections with different usage patterns. I don't see how to replace the operations folks' knowledge of all this with an auto-assignment process; the \"and find out if there is a more suitable node\" above. Suitable by what criteria? To do this really well, we'd need to have a language to define capabilities for both collections and nodes, and a heuristic to factor all that in auto-leader election.\n\nIf we do take a stab at figuring this all out automatically, it seems that the preferredLeader mechanism could be the way this is realized; auto-assign the roles, then trigger the re-election process. With room for the ops people to tweak it as they see fit.\n\nNow all that said, maybe I'm over-thinking the problem. It might be \"good enough\" to just not bother with the preferredLeader role and have a \"rebalance leaders now\" command. And trust the operations people to distribute their collections appropriately. By \"appropriately\", I'm thinking of assigning the collections across the cluster such that the leaders will be acceptably distributed if we just balance them within a collection.\n\nBTW, I think the idea of more intelligently electing leaders is a good one, and this in no way means we shouldn't do that too. I'm concerned more intelligence there is only part of the total solution though. "
        },
        {
            "author": "Steven Bower",
            "id": "comment-14136680",
            "date": "2014-09-17T02:33:53+0000",
            "content": "I think the preferredLeader flag is a good approach.. It gives explicit control to people who require it.. When building extremely high performance/large scale systems being able to know exactly where things are occurring is important... I don't see this as in conflict with a more automated approach in fact I think they could play nicely together (ie use explicit choice first and if that node isn't available fall back to balanced distribution algo.\n\nAlso the cause of this ganging up of the leaders on a single node is due to rolling restarts of a solr instances/servers for maintenance/upgrades/etc... Because election algo in ZK creates an array of numbered ephemeral nodes and choses the lowest, when you sequentially bounce instances in the same order you eventually sort the servers in the array used in the leader election for all shards... when you have lots of shards you end up with all the leaders on a single (or very few servers)... This effect occurs regardless of whether multiple shards are in a single instance, in multiple instances on a server or multi-tenant configs with multiple unrelated solr clusters running on the same server (when you bounce all instances on a server)... "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14136816",
            "date": "2014-09-17T05:52:18+0000",
            "content": "Now all that said, maybe I'm over-thinking the problem. It might be \"good enough\" to just not bother with the preferredLeader role and have a \"rebalance leaders now\" command\n\nI have gone through the use case and I see a \"rebalanceLeaders\" command would be a useful command to attack the scenario of  a rolling restart\n\nMy concern with the preferredLeader tag for a replica is that, over time replicas are reassigned in the cluster and the tag no more makes sense . I would rather have a \"switchLeader\" operation which is a one time operation and you say the replica name to which the leader is to be switched. So, there is no permanent tag at all .  "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14136826",
            "date": "2014-09-17T06:03:10+0000",
            "content": "The problem with the switch leader operation is that the external mechanism now has no idea surrounding when to actually run the command. If you were unlucky enough, you could run a switch leader, a minute later a ZK blip happens causing an election, and your command has effectively lost value.\n\nI get your concern with the topology changing causing the manual overrides to not make sense, but that's true of any manual override setting. To draw a parallel, lets say we set an overseer role on a machine and then end up adding a lot of collections to the same machine \u2013 now the machine is taxed already due to too many collections and that node being the choice for the overseer is a bad one.\n\nMy point is that all these manual expert operations are only invalidated by other external operations which aren't implicitly initiated by SolrCloud. So it's the prerogative of the expert to ensure that the external operations they do are in sync with any expert override commands they issue. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14137068",
            "date": "2014-09-17T11:20:36+0000",
            "content": "I agree that the fine grained control may be useful to some users . But in a very large cluster keeping tag of who is the leader is very annoying.\n\nI would say the \"rebalanceLeadersNow\" should take care of most of the usecases and that should be tackled first and do the other later. The good thing about this command is I can just do it after a beer without screwing up the cluster badly  "
        },
        {
            "author": "Ramkumar Aiyengar",
            "id": "comment-14138576",
            "date": "2014-09-18T06:29:11+0000",
            "content": "+1 to that, the automatic rebalance should certainly be preferred, the manual reordering should only be for cases where that doesn't suffice. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-14138928",
            "date": "2014-09-18T13:38:05+0000",
            "content": "bq: The good thing about this command is I can just do it after a beer without screwing up the cluster badly\n\nAhhh, yes. The \"drunken monkey proof\" API. I worked with a guy once who had a theory that if you couldn't understand your code after 3 beers it was too complicated and you should simplify it, although on the next day. Ever since I've tried to get places I work to institute the Friday Afternoon Beer Code Review but failed. I really bet that would be a way to get more code reviews!\n\nWhat we're talking about here should do that however. There's the auto assign ticket to distribute the preferred roles evenly and the \"make it so\" ticket to actually change leadership. SOLR-6513 and SOLR-6517.\n\nWe could also extend the leader election process to automatically do this, there's nothing precluding that here.\n\nSo it's feeling like we can carry this idea forward, probably later today I'll post the assign-replica-property code for review and start working on 6513 and we'll go from there? "
        },
        {
            "author": "Noble Paul",
            "id": "comment-14138967",
            "date": "2014-09-18T13:54:35+0000",
            "content": "So it's feeling like we can carry this idea forward,\n\nyeah I would like it in the following order\n\n\trebalanceLeaders . params (collection, shard(optional))\n\tswitchLeader . params (collection, shard, targetReplicaName)\n\tI'm not a fan of the preferredLeader thing . This information is persisted and the ops guy will have no clue where he assigned the leader. Or worse, another ops guy would take over and he will be completely clueless . I'm already the culprit of the overseer role feature. But, my defense would be that, it is only one node for the entire cluster and it can't be too bad\n\n "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-14175243",
            "date": "2014-10-17T16:57:55+0000",
            "content": "SOLR-6517 is the final piece here. See the Reference Guide for\n\nADDREPLICAPROP\nBALANCESLICEUNIQUE\nREBALANCELEADERS "
        },
        {
            "author": "Anshum Gupta",
            "id": "comment-14332870",
            "date": "2015-02-23T05:02:21+0000",
            "content": "Bulk close after 5.0 release. "
        }
    ]
}