{
    "id": "SOLR-4956",
    "title": "make maxBufferedAddsPerServer configurable",
    "details": {
        "affect_versions": "4.3,                                            6.0",
        "status": "Resolved",
        "fix_versions": [],
        "components": [],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "Anecdotal user's list evidence indicates that in high-throughput situations, the default of 10 docs/batch for inter-shard batching can generate significant CPU load. See the thread titled \"Sharding and Replication\" on June 19th, but the gist is below.\n\nI haven't poked around, but it's a little surprising on the surface that Asif is seeing this kind of difference. So I'm wondering if this change indicates some other underlying issue. Regardless, this seems like it would be good to investigate.\n\nHere's the gist of Asif's experience from the thread:\n\nIts a completely practical problem - we are exploring Solr to build a real\ntime analytics/data solution for a system handling about 1000 qps. We have\nvarious metrics that are stored as different collections on the cloud,\nwhich means very high amount of writes. The cloud also needs to support\nabout 300-400 qps.\n\nWe initially tested with a single Solr node on a 16 core / 24 GB box  for a\nsingle metric. We saw that writes were not a issue at all - Solr was\nhandling it extremely well. We were also able to achieve about 200 qps from\na single node.\n\nWhen we set up the cloud ( a ensemble on 6 boxes), we saw very high CPU\nusage on the replicas. Up to 10 cores were getting used for writes on the\nreplicas. Hence my concern with respect to batch updates for the replicas.\n\nBTW, I altered the maxBufferedAddsPerServer to 1000 - and now CPU usage is\nvery similar to single node installation.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Erick Erickson",
            "id": "comment-13691532",
            "date": "2013-06-23T17:40:43+0000",
            "content": "Actually, I wonder whether just upping the default is worthwhile, maybe to 100 or something. Is there a downside to that? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13691535",
            "date": "2013-06-23T17:49:15+0000",
            "content": "Yes, it's specifically low and not configurable for a reason - I'm struggling to remember the details - it's been a long time since Yonik Seeley brought up the issue - the size of this buffer has ramifications for recovery. "
        },
        {
            "author": "Illu Y Ying",
            "id": "comment-13714455",
            "date": "2013-07-20T15:34:52+0000",
            "content": "Do u plan to make maxBufferedAddsPerServer configurable? "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-13714493",
            "date": "2013-07-20T18:05:56+0000",
            "content": "Btw. we are seeing the same thing when using SolrCloud with a very high number of batched updates. It appears that after we send a big batch of docs to one of the shards, it sends them around to other shards in very small batches, which seems inefficient.  Maybe Yonik Seeley will remember why the maxBufferedAddsPerServer is set to just 10 and is not configurable? "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13714507",
            "date": "2013-07-20T18:29:41+0000",
            "content": "I have no plans to work on this in the foreseeable future, way too much going on for me. Shouldn't be a hard thing to actually do, and I'd like to get a reading from Mark and/or Yonik on why this was set to a low number in the first place before anyone tackles it.\n\nThat said, setting it low in anticipation of a recovery problem (which is, one hopes, rare) .vs. paying a penalty the rest of the time is certainly a point to consider. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13714535",
            "date": "2013-07-20T20:32:21+0000",
            "content": "I think it relates to peer sync only going back 100 - faint memory - I'd have to really sit down and think to work it out - hoping yonik will chime in instead  "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13715210",
            "date": "2013-07-22T13:49:55+0000",
            "content": "Yes, if we're buffering per-thread/request, then the larger the buffer, the larger the practical chances that big update reorderings can happen on replicas.  If the reordering is big enough, a recovering node using peer sync could think it was pretty much up to date.  It's not just peer sync either - other things like deletes, and deleteByQuery have limited lookback to handle reorders.\n\nBuffering isn't ideal for a lot of reasons\n\n\tupdate reorderings on the order of nIndexingThreads * bufferSize\n\tincreases the intermediate difference between replicas and leaders (wrt queries)\n\tmemory size - we don't know how big documents are... they could be huge\n\n\n\nIt would be nice to move in the other direction and do less buffering. "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13736291",
            "date": "2013-08-11T14:46:32+0000",
            "content": "So it sounds like we have competing needs here. On the one hand\nwe have several anecdotal statements that upping the buffer size\nhad significant impact on throughput.\n\nOn the other, just upping the buffer size has potential for Bad\nOutcomes.\n\nSo it seems we have three options here:\n1> make it configurable with a warning that if you change it it\n   may lead to Bad Stuff.\n2> Leave it as-is and forget about it.\n3> Do the harder thing and see if we can figure out why changing\n   the batch size makes such a difference and fix the underlying\n   cause (if there is one).\n\nI'm totally unfamiliar with the code, but the 20,000 ft. smell is\nthat there's something about the intra-node routing code that's\nvery inefficient and making the buffers bigger is masking that. On\nthe surface, just sending the packets around doesn't seem like it\nshould spike the CPU that much... But like I said, I haven't looked\nat the code at all. "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-13736298",
            "date": "2013-08-11T15:02:41+0000",
            "content": "Buffering can also slow indexing speeds...\nSay you up the buffer to 100 docs and then you send in a batch of 50.  All 50 docs will be indexed locally and only then will all 50 be sent to the replica (where we have to wait for all 50 docs to be indexed again). "
        },
        {
            "author": "Erick Erickson",
            "id": "comment-13736301",
            "date": "2013-08-11T15:12:45+0000",
            "content": "Hmmm, sounds like we need more details. I wonder if the\nsituation where buffering up more docs is helping are also\nsituations in which there is only a leader?\n\nI guess the thing that's puzzling me is the high CPU\nrates that are reported being related to internal buffering\nsizes. "
        },
        {
            "author": "Greg Pendlebury",
            "id": "comment-13739058",
            "date": "2013-08-14T00:15:00+0000",
            "content": "\"So it seems we have three options here:\n1> make it configurable with a warning that if you change it it may lead to Bad Stuff.\"\n\nI'd support this solely from the perspective of testing its impact. Rebuilding code to change a hardcoded integer is a tad annoying if you are just diagnosing what impact things could have. We batch ingest several thousand documents at a time into a 96 JVM cluster (32 shards * 3 replicas). I'd love to see if we could lower CPU load by altering this setting... even if it is only a diagnostic step that is at odds with long term goals related to batching at all. "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-13741556",
            "date": "2013-08-15T22:07:05+0000",
            "content": "+1 for what Greg Pendlebury wrote.  Let me be the master of my own destiny. "
        },
        {
            "author": "asif",
            "id": "comment-13757468",
            "date": "2013-09-04T05:09:00+0000",
            "content": "Just to put things in perspective - I ran few more tests on our setup. \n\nWe buffer up to 1000 documents before we post to cloud every 5-10 seconds - On average I notice 2-4 times higher cpu on replica's when maxBufferedAddsPerServer is set to 10. \n\nThe fact that 1000 documents are sent as 100 different requests of 10 documents each might explain the higher load on the replica. \n\nWhen we alter it to about 1000 - cpu usage is more or less in line.  "
        },
        {
            "author": "Otis Gospodnetic",
            "id": "comment-13764662",
            "date": "2013-09-11T19:29:11+0000",
            "content": "Will be fixed by SOLR-5232. "
        }
    ]
}