{
    "id": "LUCENE-4372",
    "title": "CachingCollector.create(boolean, boolean, double) is trappy",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Task",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Unresolved",
        "status": "Open"
    },
    "description": "Followup to LUCENE-3102.\n\nShai proposed a method that just caches all scores so they can be replayed:\n\nDo you think we can modify this Collector to not necessarily wrap another Collector? We have such Collector which stores (in-memory) all matching doc IDs + scores (if required). Those are later fed into several processes that operate on them (e.g. fetch more info from the index etc.). I am thinking, we can make CachingCollector optionally wrap another Collector and then someone can reuse it by setting RAM limit to unlimited (we should have a constant for that) in order to simply collect all matching docs + scores.\n\nBut Mike had concerns about the RAM usage:\n\nI'd actually rather not have the constant \u2013 ie, I don't want to make\nit easy to be unlimited? It seems too dangerous... I'd rather your\ncode has to spell out 10*1024 so you realize you're saying 10 GB (for\nexample).\n\nMy concern here is what happens when you dont specify enough, I think those hits are just silently dropped (which is worse than using lots of RAM).",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2012-09-10T15:11:56+0000",
            "content": "CachingCollector has this in its javadocs:\n\n\n * Caches all docs, and optionally also scores, coming from\n * a search, and is then able to replay them to another\n * collector.  You specify the max RAM this class may use.\n * Once the collection is done, call {@link #isCached}. If\n * this returns true, you can use {@link #replay(Collector)}\n * against a new collector.  If it returns false, this means\n * too much RAM was required and you must instead re-run the\n * original search.\n\n\n\nNotice the last sentence about isCached returning false.\n\nShould we just fix the static create() method's documentation (even though it points to the class's javadocs)?\n\nI don't see any alternative \u2013 if the user specified a too low RAM limit, what can you do besides discarding the docs and documenting that behavior? I'd hate to see exceptions thrown... ",
            "author": "Shai Erera",
            "id": "comment-13452055"
        },
        {
            "date": "2012-09-10T15:15:23+0000",
            "content": "Thanks Shai, i missed this exception in replay() !\n\nWe can cancel the issue: I don't have any concerns if you are gonna get an exception. ",
            "author": "Robert Muir",
            "id": "comment-13452057"
        },
        {
            "date": "2012-09-10T15:17:38+0000",
            "content": "btw: what confuses me is what happens in the \"run-out-of-ram\" case,\nwhy do we even bother collecting the overflow'd docs here (other.collect)?\n\n\n      if (curDocs == null) {\n        // Cache was too large\n        cachedScorer.score = scorer.score();\n        cachedScorer.doc = doc;\n        other.collect(doc);\n        return;\n      }\n\n\n ",
            "author": "Robert Muir",
            "id": "comment-13452060"
        },
        {
            "date": "2012-09-10T15:29:15+0000",
            "content": "Thanks Shai, i missed this exception in replay() !\n\nHmm, I didn't even noticed it. I was referring plainly to the class javadocs. That exception is even better ! \n(In my response, I was against throwing an exception during collect(), but during replay() it's fair)\n\n\nbtw: what confuses me is what happens in the \"run-out-of-ram\" case,\nwhy do we even bother collecting the overflow'd docs here (other.collect)?\n\nYou mean in the flow where the user called CachingCollector.create(boolean, boolean, double), right? Because this class can just wrap another Collector (see other create()), in which case you'd want to delegate the calls to (you don't know what the Collector is).\n\nBut in the case where the user just wants to have the docs (+scores) cached, I agree it's useless to continue to delegate. The Collector just as well might throw an exception. To optimize it, we'll need to add an 'if' check, or create another specialized CachingCollector impl ... or continue to call the empty Collector impl, hoping the right thing will happen . ",
            "author": "Shai Erera",
            "id": "comment-13452075"
        },
        {
            "date": "2012-09-10T15:30:55+0000",
            "content": "btw: what confuses me is what happens in the \"run-out-of-ram\" case, why do we even bother collecting the overflow'd docs here (other.collect)?\n\nThis was created for grouping.  Originally, I always ran the query twice - once to get the top groups, then again to get the top docs per group.  Caching scores was an optimization added later, but you definitely need to still call the delegate, even if you run out of cache space. ",
            "author": "Yonik Seeley",
            "id": "comment-13452077"
        },
        {
            "date": "2012-09-10T15:31:35+0000",
            "content": "\nBut in the case where the user just wants to have the docs (+scores) cached, I agree it's useless to continue to delegate. The Collector just as well might throw an exception. To optimize it, we'll need to add an 'if' check, or create another specialized CachingCollector impl ... or continue to call the empty Collector impl, hoping the right thing will happen\n\nDo we? I think i'm just proposing to modify the existing 'if' check (if curDocs == null) to do nothing at all or to throw an exception earlier  ",
            "author": "Robert Muir",
            "id": "comment-13452078"
        },
        {
            "date": "2012-09-10T15:40:17+0000",
            "content": "Do we? I think i'm just proposing to modify the existing 'if' check (if curDocs == null) to do nothing at all or to throw an exception earlier\n\nI don't think so. That 'if' only indicates whether the RAM limit has been reached, and we don't cache anymore. But the collector doesn't know if the wrapped Collector needs to receive the documents that are still to be collected or not.\n\nCachingCollector is an optimization. If you can spare 10 MB of cache, and you happen to execute queries that answer #documents that can be cached within that limit, then as in the case Yonik describes, you can execute the query once, collect the docs (by a real Collector) and cache them (CachingCollector) in order to replay to another Collector. If there wasn't enough RAM, you cannot use replay() (you'll hit IllegalStateEx).\n\nThe 'if' I was talking about adding is inside \"if (cacheDocs == null)\" to follow by an \"if (collector == null)\" and pass a null Collector from the create() that is not interested in wrapping a Collector. ",
            "author": "Shai Erera",
            "id": "comment-13452087"
        },
        {
            "date": "2012-09-10T15:53:09+0000",
            "content": "\nCachingCollector is an optimization. If you can spare 10 MB of cache, and you happen to execute queries that answer #documents that can be cached within that limit, then as in the case Yonik describes, you can execute the query once, collect the docs (by a real Collector) and cache them (CachingCollector) in order to replay to another Collector. If there wasn't enough RAM, you cannot use replay() (you'll hit IllegalStateEx).\n\nRight but then when we run out of cache, shouldnt we dumping the existing cache to the underlying collector first? What am I missing?\n\nit seems to me we just throw away the cache completely and then start collecting, thats why it doesnt make a lot of sense to me:\n\n\n          if (nextLength <= 0) {\n            // Too many docs to collect -- clear cache\n            curDocs = null;\n            cachedSegs.clear();\n            cachedDocs.clear();\n            other.collect(doc);\n            return;\n          }\n\n\n ",
            "author": "Robert Muir",
            "id": "comment-13452095"
        },
        {
            "date": "2012-09-10T16:01:49+0000",
            "content": "I see whats confusing me (it wasnt obvious until poking around this thing enough), there are 2 collectors, the one you wrap is just a passthrough always.\nthe one you replay to is separate.\n\nok it makes sense now. ",
            "author": "Robert Muir",
            "id": "comment-13452103"
        },
        {
            "date": "2012-09-10T16:02:08+0000",
            "content": "it seems to me we just throw away the cache completely and then start collecting,\n\nNo, collection (delegation) is always done.  The cache is a transparent pass-through that may cache scores if there aren't too many. ",
            "author": "Yonik Seeley",
            "id": "comment-13452104"
        }
    ]
}