{
    "id": "LUCENE-5897",
    "title": "performance bug (\"adversary\") in StandardTokenizer",
    "details": {
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed",
        "components": [],
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.9.1",
            "4.10",
            "6.0"
        ]
    },
    "description": "There seem to be some conditions (I don't know how rare or what conditions) that cause StandardTokenizer to essentially hang on input: I havent looked hard yet, but as its essentially a DFA I think something wierd might be going on.\n\nAn easy way to reproduce is with 1MB of underscores, it will just hang forever.\n\n  public void testWorthyAdversary() throws Exception {\n    char buffer[] = new char[1024 * 1024];\n    Arrays.fill(buffer, '_');\n    int tokenCount = 0;\n    Tokenizer ts = new StandardTokenizer();\n    ts.setReader(new StringReader(new String(buffer)));\n    ts.reset();\n    while (ts.incrementToken()) {\n      tokenCount++;\n    }\n    ts.end();\n    ts.close();\n    assertEquals(0, tokenCount);\n  }",
    "attachments": {
        "LUCENE-5897.patch": "https://issues.apache.org/jira/secure/attachment/12663628/LUCENE-5897.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14104202",
            "author": "Robert Muir",
            "content": "it seems stuck here:\n\nTRACE 301319:\n        org.apache.lucene.analysis.standard.StandardTokenizerImpl.getNextToken(StandardTokenizerImpl.java:756)\n        org.apache.lucene.analysis.standard.StandardTokenizer.incrementToken(StandardTokenizer.java:150)\n        org.apache.lucene.analysis.core.TestStandardAnalyzer.testWorthyAdversary(TestStandardAnalyzer.java:286)\n\n\n\nThis is in generated code: so I don't yet know if its something about our grammar or something in jflex itself? ",
            "date": "2014-08-20T17:46:57+0000"
        },
        {
            "id": "comment-14104272",
            "author": "Robert Muir",
            "content": "I spent some time trying to debug it, didn't get very far.\n\nI know at least you can substitute any Extend_Num_Let for the underscore and it hangs. I don't yet know if other word break categories would have a similar issue, maybe even in other contexts. ",
            "date": "2014-08-20T18:17:58+0000"
        },
        {
            "id": "comment-14104287",
            "author": "Steve Rowe",
            "content": "I'm looking into it, I think it's a bug in JFlex.  zzRefill(), which pulls data into and if necessary expands the buffer over which tokenization occurs, is being called repeatedly even though EOF has been reached.\n\nI'm going to see if this reproduces in Lucene 4.9 - I suspect I introduced the bug in JFlex 1.6.  If so, the thing to do is likely revert the JFlex 1.5->1.6 changes (LUCENE-5770), since that hasn't been released yet. ",
            "date": "2014-08-20T18:26:27+0000"
        },
        {
            "id": "comment-14104299",
            "author": "Steve Rowe",
            "content": "I'm going to see if this reproduces in Lucene 4.9 - I suspect I introduced the bug in JFlex 1.6. If so, the thing to do is likely revert the JFlex 1.5->1.6 changes (LUCENE-5770), since that hasn't been released yet.\n\nUnfortunately, the hanging behavior occurs on 4.9 too, so reverting LUCENE-5770 won't help. ",
            "date": "2014-08-20T18:35:24+0000"
        },
        {
            "id": "comment-14104300",
            "author": "Robert Muir",
            "content": "I think it might be older? I tried 4.7 branch really quick, and it hung too. ",
            "date": "2014-08-20T18:35:30+0000"
        },
        {
            "id": "comment-14104352",
            "author": "Steve Rowe",
            "content": "This looks exactly like LUCENE-5400: very slow tokenization when a rule has to search to end of stream for a condition that doesn't occur. ",
            "date": "2014-08-20T19:01:51+0000"
        },
        {
            "id": "comment-14104379",
            "author": "Robert Muir",
            "content": "OK, that makes sense. Can we do something crazy during generation of jflex (e.g. inject a little code) to bound this to at least maxTokenLength() ? ",
            "date": "2014-08-20T19:15:57+0000"
        },
        {
            "id": "comment-14104519",
            "author": "Steve Rowe",
            "content": "I'll try to figure out a way to limit the search, as you say, to to maxTokenLength().  I worry about two things though, both of which are currently handled (though badly in these adversary cases):\n\n\n\tin a rule with alternates, one of which is satisfied below the limit, the satisfied alternate should produce a match when a partially matching alternate exceeds the limit and is aborted.\n\tWhen rule A matches partially, exceeds the limit, and is aborted, and rule B matches a prefix that is under the limit, rule B should produce a match.\n\n ",
            "date": "2014-08-20T20:35:06+0000"
        },
        {
            "id": "comment-14104529",
            "author": "Robert Muir",
            "content": "I agree its not ideal. \n\ncan it be based on the way the rules are encoded in our grammar?\n\nI know for example, if i substitute latest unicode breakiterator instead, it doesn't have the problem, but I know that has a different (typically slower) representation. But the rules (IIRC) use a chaining mechanism which is hard to think about. ",
            "date": "2014-08-20T20:39:40+0000"
        },
        {
            "id": "comment-14104804",
            "author": "Steve Rowe",
            "content": "can it be based on the way the rules are encoded in our grammar?\n\nI don't know how to do that - as I mentioned on LUCENE-5400, adding large repeat counts to sub-regexes made JFlex OOM at generation time.  Were you thinking of something other than repeat counts?\n\nI'm thinking it should be possible to abuse JFlex's buffer handling to just never grow the buffer beyond the initial size, but still allow the contents to be shifted to enable (maximally) buffer-length matches.  This would have a nice secondary effect of reducing max memory usage.  If I can make it work, I'll add a generation option for this to JFlex. ",
            "date": "2014-08-20T23:44:40+0000"
        },
        {
            "id": "comment-14104932",
            "author": "Robert Muir",
            "content": "Well, I guess one concern is the 'adversary' case but I'm a little concerned the behavior might impact ordinary performance: so I'm just stretching a bit and trying to figure out how com.icu.ibm.text.BreakIterator (which impls the same algo) doesnt' get hung in such an adversary case.\n\nI looked at http://icu-project.org/docs/papers/text_boundary_analysis_in_java/\n\nespecially: \"If the current state is an accepting state, the break position is after that character. Otherwise, the break position is after the last character that caused a transition to an accepting state. (In other words, we keep track of the break position, updating it to after the current position every time we enter an accepting state. This is called \"marking\" the position.)\"\n\nSo more generally, can we optimize the general case to also remove what appears to be a backtracking algo? I know jflex is more general than what ICU offers, so its like comparing apples and oranges, but i can't help but wonder... ",
            "date": "2014-08-21T01:44:34+0000"
        },
        {
            "id": "comment-14105104",
            "author": "Steve Rowe",
            "content": "So more generally, can we optimize the general case to also remove what appears to be a backtracking algo? I know jflex is more general than what ICU offers, so its like comparing apples and oranges, but i can't help but wonder...\n\nSorry, I don't know enough about how the automaton is constructed and run to know if this is possible. ",
            "date": "2014-08-21T06:50:49+0000"
        },
        {
            "id": "comment-14105181",
            "author": "Steve Rowe",
            "content": "I removed the buffer expansion logic in StandardTokenizerImpl.zzRefill(), and the tokenizer still functions - as I had hoped, partial match searches are limited to the buffer size:\n\n\n@@ -509,16 +509,6 @@\n       zzStartRead = 0;\n     }\n \n-    /* is the buffer big enough? */\n-    if (zzCurrentPos >= zzBuffer.length - zzFinalHighSurrogate) {\n-      /* if not: blow it up */\n-      char newBuffer[] = new char[zzBuffer.length*2];\n-      System.arraycopy(zzBuffer, 0, newBuffer, 0, zzBuffer.length);\n-      zzBuffer = newBuffer;\n-      zzEndRead += zzFinalHighSurrogate;\n-      zzFinalHighSurrogate = 0;\n-    }\n-\n     /* fill the buffer with new input */\n     int requested = zzBuffer.length - zzEndRead;           \n     int totalRead = 0;\n\n\n\nand ran Robert's testWorthyAdversary() with the input length ranging from 100k to 3.2M chars, and varying the buffer size from 4k chars (the default) to 255, compared to the current implementation, where unlimited buffer expansion is allowed (NBE = no buffer expansion; times are in seconds; Oracle Java 1.7.0_55; OS X 10.9.4):\n\n\n\n\nInput chars\ncurrent impl.\n4k buff, NBE\n2k buff, NBE\n1k buff, NBE\n255 buff, NBE\n\n\n100k\n29s\n3s\n1s\n<1s\n<1s\n\n\n200k\n136s\n5s\n3s\n1s\n<1s\n\n\n400k\n547s\n11s\n5s\n3s\n1s\n\n\n800k\n2,272s\n22s\n11s\n5s\n1s\n\n\n1,600k\n9,000s (est.)\n43s\n23s\n11s\n3s\n\n\n3,200k\n40,000s (est.)\n91s\n43s\n22s\n6s\n\n\n\n\n\nI didn't actually run the test against the current implementation with 1.6M and 3.2M input chars - the numbers above with (est.) after them are estimates - but for the ones I did measure, doubling the input length roughly quadruples the run time.\n\nBy contrast, when the buffer length is limited, doubling the input length only doubles the run time.\n\nWhen the buffer length is limited, doubling the buffer length doubles the run time.\n\nBased on this, I'd like to introduce a new max buffer size setter to StandardTokenizer, which defaults to the initial buffer size.  That way, by default buffer expansion is disabled, but can be re-enabled by setting a max buffer size larger than the initial buffer size.\n\nI ran luceneutil's TestAnalyzerPerf, just testing StandardAnalyzer using enwiki-20130102-lines.txt, with unpatched trunk against trunk patched to disable buffer expansion, and with a buffer size of 255 (the default max token size), 5 runs each:\n\n\n\n\n\u00a0\nMillion tokens/sec, trunk\nMillion tokens/sec, patched\n\n\nrun 1\n7.162\n7.020\n\n\nrun 2\n7.079\n7.245\n\n\nrun 3\n7.381\n7.200\n\n\nrun 4\n7.352\n7.192\n\n\nrun 5\n7.160\n7.169\n\n\nmean\n7.227\n7.166\n\n\nstddev\n0.1323\n0.08589\n\n\n\n\n\nThese are pretty noisy, but comparing the best throughput numbers, the patched version has 1.8% lower throughput. \n\nBased on the above, I'd also like to:\n\n\n\tset the initial buffer size at the max token length\n\twhen basing the initial buffer size on the max token length, don't go above 1M or 2M chars, to guard against people specifying Integer.MAX_VALUE for the max token length\n\n\nand from above: \n\n\n\tadd a max buffer size setter to StandardTokenizer, which defaults to the initial buffer size.\n\n ",
            "date": "2014-08-21T08:23:50+0000"
        },
        {
            "id": "comment-14105729",
            "author": "Robert Muir",
            "content": "do we need a separate max buffer size parameter? can it just be an impl detail based on max token length? ",
            "date": "2014-08-21T18:26:29+0000"
        },
        {
            "id": "comment-14105745",
            "author": "Steve Rowe",
            "content": "do we need a separate max buffer size parameter? can it just be an impl detail based on max token length?\n\nIt depends on whether we think anybody will want the (apparently minor) benefit of having a larger buffer, regardless of max token length ",
            "date": "2014-08-21T18:36:48+0000"
        },
        {
            "id": "comment-14105763",
            "author": "Steve Rowe",
            "content": "Oh, and one other side effect that people might want: when buffer size is larger than max token length, too-large tokens are not emitted, and no attempt is made to find smaller matching prefixes.\n\nThese two seem like very minor benefits for a small audience, so I'm fine going without a separate max buffer size parameter. ",
            "date": "2014-08-21T18:47:05+0000"
        },
        {
            "id": "comment-14106651",
            "author": "Steve Rowe",
            "content": "Trunk patch, fixes both this issue and LUCENE-5400:\n\n\n\tmodifies jflex generation to disable scanner buffer expansion\n\twhen StandardTokenizerInterface.setMaxTokenLength() is called, the scanner's buffer size is also modified, but is limited to max 1M chars\n\tadded randomized tests for StandardTokenizer and UAX29URLEmailTokenizer.\n\tI tried to find problematic text sequences for the other JFlex grammars (HTMLStripCharFilter, ClassicTokenizer, and WikipediaTokenizer), but nothing I tried worked, so I left these as-is.\n\n\n\nAll analysis-common tests pass, as does precommit (after locally patching some javadoc problems unrelated to this issue).  I'll commit to trunk and branch_4x after I've run the whole test suite.\n\nI'd like to include this fix in 4.10. ",
            "date": "2014-08-22T09:09:59+0000"
        },
        {
            "id": "comment-14106684",
            "author": "ASF subversion and git services",
            "content": "Commit 1619730 from Use account \"steve_rowe\" instead in branch 'dev/trunk'\n[ https://svn.apache.org/r1619730 ]\n\nLUCENE-5897, LUCENE-5400: JFlex-based tokenizers StandardTokenizer and UAX29URLEmailTokenizer tokenize extremely slowly over long sequences of text partially matching certain grammar rules.  The scanner default buffer size was reduced, and scanner buffer growth was disabled, resulting in much, much faster tokenization for these text sequences. ",
            "date": "2014-08-22T10:19:10+0000"
        },
        {
            "id": "comment-14106758",
            "author": "ASF subversion and git services",
            "content": "Commit 1619773 from Use account \"steve_rowe\" instead in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1619773 ]\n\nLUCENE-5897, LUCENE-5400: JFlex-based tokenizers StandardTokenizer and UAX29URLEmailTokenizer tokenize extremely slowly over long sequences of text partially matching certain grammar rules.  The scanner default buffer size was reduced, and scanner buffer growth was disabled, resulting in much, much faster tokenization for these text sequences. (merged trunk r1619730) ",
            "date": "2014-08-22T12:11:52+0000"
        },
        {
            "id": "comment-14106923",
            "author": "Ryan Ernst",
            "content": "Thanks for the hard work Steve! I will merge this over into the 4.10 branch. ",
            "date": "2014-08-22T15:02:03+0000"
        },
        {
            "id": "comment-14106966",
            "author": "ASF subversion and git services",
            "content": "Commit 1619836 from Ryan Ernst in branch 'dev/branches/lucene_solr_4_10'\n[ https://svn.apache.org/r1619836 ]\n\nLUCENE-5897, LUCENE-5400: JFlex-based tokenizers StandardTokenizer and UAX29URLEmailTokenizer tokenize extremely slowly over long sequences of text partially matching certain grammar rules. The scanner default buffer size was reduced, and scanner buffer growth was disabled, resulting in much, much faster tokenization for these text sequences. (merged branch_4x r1619773) ",
            "date": "2014-08-22T15:28:35+0000"
        },
        {
            "id": "comment-14106976",
            "author": "ASF subversion and git services",
            "content": "Commit 1619840 from Ryan Ernst in branch 'dev/trunk'\n[ https://svn.apache.org/r1619840 ]\n\nLUCENE-5672,LUCENE-5897,LUCENE-5400: move changes entry to 4.10.0 ",
            "date": "2014-08-22T15:32:15+0000"
        },
        {
            "id": "comment-14106979",
            "author": "ASF subversion and git services",
            "content": "Commit 1619841 from Ryan Ernst in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1619841 ]\n\nLUCENE-5672,LUCENE-5897,LUCENE-5400: move changes entry to 4.10.0 ",
            "date": "2014-08-22T15:33:02+0000"
        },
        {
            "id": "comment-14106984",
            "author": "ASF subversion and git services",
            "content": "Commit 1619842 from Ryan Ernst in branch 'dev/branches/lucene_solr_4_10'\n[ https://svn.apache.org/r1619842 ]\n\nLUCENE-5672,LUCENE-5897,LUCENE-5400: move changes entry to 4.10.0 ",
            "date": "2014-08-22T15:33:54+0000"
        },
        {
            "id": "comment-14107650",
            "author": "Steve Rowe",
            "content": "Thanks for including in 4.10 and doing the backport work, Ryan Ernst. ",
            "date": "2014-08-22T22:58:50+0000"
        },
        {
            "id": "comment-14136398",
            "author": "Michael McCandless",
            "content": "Reopen to backport to 4.9.1... ",
            "date": "2014-09-16T22:39:35+0000"
        },
        {
            "id": "comment-14136403",
            "author": "Michael McCandless",
            "content": "Hmm ... merge conflicts on backport ... Use account \"steve_rowe\" instead maybe you can try to backport?  The conflicts seem to be only in the autogen'd sources so maybe I just need to backport and regen? ",
            "date": "2014-09-16T22:42:25+0000"
        },
        {
            "id": "comment-14136417",
            "author": "Steve Rowe",
            "content": "Sure, I'll do the backport. ",
            "date": "2014-09-16T22:49:52+0000"
        },
        {
            "id": "comment-14136439",
            "author": "Steve Rowe",
            "content": "LUCENE-5770 (JFlex 1.6 upgrade) happened in 4.10, so backporting to 4.9 will require some changes. ",
            "date": "2014-09-16T23:01:37+0000"
        },
        {
            "id": "comment-14136734",
            "author": "ASF subversion and git services",
            "content": "Commit 1625458 from Use account \"steve_rowe\" instead in branch 'dev/branches/lucene_solr_4_9'\n[ https://svn.apache.org/r1625458 ]\n\nLUCENE-5897, LUCENE-5400: JFlex-based tokenizers StandardTokenizer and UAX29URLEmailTokenizer tokenize extremely slowly over long sequences of text partially matching certain grammar rules.  The scanner default buffer size was reduced, and scanner buffer growth was disabled, resulting in much, much faster tokenization for these text sequences. (merged branch_4x r1619773) ",
            "date": "2014-09-17T03:58:23+0000"
        },
        {
            "id": "comment-14137225",
            "author": "ASF subversion and git services",
            "content": "Commit 1625586 from Use account \"steve_rowe\" instead in branch 'dev/branches/lucene_solr_4_9'\n[ https://svn.apache.org/r1625586 ]\n\nLUCENE-5897, LUCENE-5400: change JFlex-generated source munging so that zzRefill() doesn't call Reader.read(buffer,start,len) with len=0 ",
            "date": "2014-09-17T13:45:09+0000"
        },
        {
            "id": "comment-14151045",
            "author": "Michael McCandless",
            "content": "Bulk close for Lucene/Solr 4.9.1 release ",
            "date": "2014-09-28T09:05:49+0000"
        }
    ]
}