{
    "id": "SOLR-7136",
    "title": "Add an AutoPhrasing TokenFilter",
    "details": {
        "components": [],
        "type": "New Feature",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "None",
        "status": "Open",
        "resolution": "Unresolved",
        "priority": "Major"
    },
    "description": "Adds an 'autophrasing' token filter which is designed to enable noun phrases that represent a single entity to be tokenized in a singular fashion. Adds support for ManagedResources and Query parser auto-phrasing support given LUCENE-2605.\n\nThe rationale for this Token Filter and its use in solving the long standing multi-term synonym problem in Lucene Solr has been documented online. \n\nhttp://lucidworks.com/blog/automatic-phrase-tokenization-improving-lucene-search-precision-by-more-precise-linguistic-analysis/\n\nhttps://lucidworks.com/blog/solution-for-multi-term-synonyms-in-lucenesolr-using-the-auto-phrasing-tokenfilter/",
    "attachments": {
        "AutoPhaseFiniteStateDiagram.pdf": "https://issues.apache.org/jira/secure/attachment/12773318/AutoPhaseFiniteStateDiagram.pdf",
        "SOLR-7136.patch": "https://issues.apache.org/jira/secure/attachment/12700048/SOLR-7136.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-02-21T19:21:07+0000",
            "author": "Ted Sullivan",
            "content": "Initial Patch.  Need to add tests for AutoPhrasingQParserPlugin ",
            "id": "comment-14330398"
        },
        {
            "date": "2015-02-23T20:26:20+0000",
            "author": "Ted Sullivan",
            "content": "A little shameless marketing for this ticket if I may:  A recent comment on a Hacker News discussion thread:\n\n\"At my company, we've been beating our heads to the wall[0] in getting multi-term synonyms to work correctly in SOLR. e.g.\n   fruit extractor => fruit juicer, citrus juicer \"\n\nAnd a reply from elchief:\n\n\"The Solr guys don't give a flying F about this issue though\"\n\nHopefully, this is not the case \n ",
            "id": "comment-14333759"
        },
        {
            "date": "2015-02-23T21:10:39+0000",
            "author": "Ahmet Arslan",
            "content": "To solve synonym problem, This token filters needs its special query parser, right?\nRegarding multi word synonyms, what do you think about other solutions, SOLR-4381 and SOLR-5379? ",
            "id": "comment-14333823"
        },
        {
            "date": "2015-02-23T21:26:59+0000",
            "author": "Ted Sullivan",
            "content": "Yes Ahmet - that is correct, this patch includes a QParserPlugin as a workaround for LUCENE-2605 also mentioned in SOLR-5379. (AutophrasingQParserPlugin) The Query Parser solution published by Nolan Lawson and submitted as SOLR-4381 is a good solution too.  In fact, the \"Solr guys don't give a flying F\" comment on HN was in response to the fact that SOLR-4381 which was filed over 2 years ago is still not committed. \n\nNote however that the AutoPhrasing parser first solves a problem of tokenizing phrases that represent single entities as single tokens - making the Lucene docID lookup cleaner.  Solutions like SOLR-5379 solve this indirectly and may have different edge cases because not all phrases are meant to represent single entities. For example, generalized phrase processing paradigms like mm or ps may not deal as precisely with phrases that include a multi-term entity with something else like \"New York City restaurants\". Since it is part of an analysis pipeline, the AutophrasingTokenFilter can be used in conjunction with the SynonymTokenFilter to solve the multi-term synonym problem but that is an architectural solution. In other words this TokenFilter was not written to solve the multi-term synonym problem - that is a side benefit of what it does, given the nature of Lucene analysis chains.\n\n It has other benefits as well just by forcing exact-match semantics on phrases that should be treated as semantic or linguistic entities. It does have the downside of requiring autophrase lists, but so then does synonym processing. ",
            "id": "comment-14333833"
        },
        {
            "date": "2015-02-25T12:50:05+0000",
            "author": "Ted Sullivan",
            "content": "Added Test for AutoPhrasingQParserPlugin.  All tests pass. ",
            "id": "comment-14336449"
        },
        {
            "date": "2015-02-26T02:30:53+0000",
            "author": "Otis Gospodnetic",
            "content": "Ted Sullivan do you know / have you tested and compared this with SOLR-5379 to the point where you can say that the functionality provided in this issue is a superset of SOLR-5379?  Or is that not the case?  Or maybe you didn't test and compare enough to be able to say?  Thanks. ",
            "id": "comment-14337734"
        },
        {
            "date": "2015-02-26T04:58:35+0000",
            "author": "Ted Sullivan",
            "content": "Otis Gospodnetic I am looking at this now - I have not tested/compared these solutions yet. I will definitely do that. The strategy of SOLR-5379 is a clever one - it forces tokenization of phrases that are in synonyms.txt by detecting internal whitespace and then either forces PhraseQuery logic or automatic quoting when building the Lucene Query (using TypeAttributes). In that sense, the two ideas are similar.\n\nThe autophrasing token filter solves a slightly different problem than does SOLR-5379 in that it does not require that a term be listed as a synonym of something else to get the correct semantic tokenization. Simply reducing false positives due to partial hits on a phrase can be a large improvement in precision and not everything has an obvious synonym. (That said, can you have a single term in synonyms.txt that does not map to anything else? I haven't tried that - maybe because it would need SOLR-5379 to work.) Therefore, it has value even if SOLR-5379 is also committed  (or patched). Another difference is that one of the SOLR-5379  patches uses PhraseQuery whereas the solution of combining autophrasing with synonym mapping does not. How much of a performance difference this might entail I can't say - probably not a great deal unless we are talking about very large queries. The auto quoting parser patches work in a similar fashion to the AutoPhrasingQParserPlugin as a workaround to LUCENE-2605. \n\n I think that the query parser solution in SOLR-5379 is better as it solves the problem in a general way. To get non-synonymous phrases into this may require some tweaking to get the TypeAttribute to match up. I wouldn't use - typeAttribute.type().equals(\"SYNONYM\") maybe typeAttribuyte.type() should be \"PHRASE\". So if both are committed, we should remove the AutoPhrasingQParserPlugin from this patch because both solve the exact same problem.\n\nThe autophrasing multi-term synonym solution does have the disadvantage of requiring coupling between the autophrases.txt and synonyms.txt, which the other solution does not. But that said, the other solution does not deal with multi-word terms that do not have synonyms (I suppose that you could create a dummy synonym but that would be difficult to maintain).\n\nTo answer your question about a 'superset' - yes if you consider that the solutions for multi-term synonym mapping would be equivalent. All in all, I would say that both solutions are useful and would add useful functionality to the available Solr toolset. Dealing with multi-word terms is a problem that many Solr deployments have and it is one that remains unresolved.\n ",
            "id": "comment-14337873"
        },
        {
            "date": "2015-02-26T05:17:04+0000",
            "author": "Ted Sullivan",
            "content": "Another thing that the autophrasing parser does is correctly deal with overlapping phrases such as \"income tax refund\" where \"income tax\" and \"tax refund\" are auto-phrases. Not sure how SOLR-5379 would handle that use case - I'll investigate. ",
            "id": "comment-14337896"
        },
        {
            "date": "2015-05-18T20:34:34+0000",
            "author": "Ted Sullivan",
            "content": "Fixed a bug where overlapping autophrases were emitting an extra token (erroneously) if the match ended before the overlapping token finished. e.g. if the autophrases are \"foo bar\" and \"bar baz\" and the test phrase is \"foo bar\", without the fix, it will emit \"foo_bar\" and \"bar\", now it just emits \"foo_bar\" ",
            "id": "comment-14549170"
        },
        {
            "date": "2015-05-28T07:38:03+0000",
            "author": "Markus Jelsma",
            "content": "Hi Ted - this is another interesting approach to the typical problem. I was thinking about the repercussions your token filter has on IDF values. Surely, phrases will get a inflated score because they become much rarer than their constituent terms, which seems like a good thing. I do have a problem with the query parser, it won't work for multi language environments, and it doesn't interact with edismax, which is presumably the de facto parser for free text input.\n\nAlso, your latest patch uses a StringBuffer in the token filter, i believe you should rely on StringBuilder instead, you don't need thread-safety at that point. Another thing is the usage of String.replaceAll(String, String) in the parser, isn't that going to eat cycles we should spare? ",
            "id": "comment-14562427"
        },
        {
            "date": "2015-06-08T14:41:29+0000",
            "author": "Ted Sullivan",
            "content": "Good points Marcus - yes, StringBuffer is obsolete - my bad - old coding habits die hard as it were  As to the IDF issue, that also would need to be looked at - thanks for pointing that out - and I also agree on your comment about String.replaceAll - but since this is working on queries which are typically very small compared to documents, I didn't think it would hurt that much but this is probably erroneous thinking when considering load. This QParser as discussed below probably needs an overhaul at this point.\n\non Edismax - you can set the defType in the AutophrasingQParser plugin to edismax (it defaults to the lucene parser) but that said, I have noticed some issues with it - It messes up on simple things too and it really needs to be rethought somewhat.  One change that I will post soon is to enable it to use different Tokenizer implementations - the initial patch uses WhitespaceTokenizer which is hard coded. I think that it should use StandardTokenizer as a default and then allow other impls to be switched in via configuration. ",
            "id": "comment-14577267"
        },
        {
            "date": "2015-07-03T04:50:22+0000",
            "author": "Bill Bell",
            "content": "+1\n\nLet' get it committed ",
            "id": "comment-14612829"
        },
        {
            "date": "2015-08-13T16:26:48+0000",
            "author": "Alexander Radzievskiy",
            "content": "Hi Ted\nCould you please explain what exactly will not work when using  AutophrasingQParser plugin with Edismax. \nOr is it something that has been solved in the last version of the plugin?\nThanks a lot in advance. ",
            "id": "comment-14695500"
        },
        {
            "date": "2015-11-19T20:13:48+0000",
            "author": "Koorosh Vakhshoori",
            "content": "Here I am uploading a new implementation of AutoPhrasing in coordination with Ted. This version adds a few features on top of the previous code. Here they are:\n\n\tThe phrase detection algorithm is refactored as a finite-state machine. This FSM takes a term as input for each transition. I am including the FSM diagram here.\n\tThe new code correctly keeps track of the start and end offsets in all cases.\n\tNow the code records the PostionLength attribute, since it would be handy for highlighter. That is once the highlighter is fixed, SOLR-3390.\n\tThere is a new argument \u2018emitAmbiguousPhrases\u2019. When it is set to false, it would only emit auto-phrase that matches the longest sequence of terms. For example, if we have \u2018New York City\u2019 and \u2018New York\u2019 in the autophrases.txt file and the text is \u2018New York City is a great place to live\u2019, in this case only \u2018New York City\u2019 is emitted. Well, my use case required it and I am sure others may want it too.\n\tRather than applying AutoPhrasing at index time, now you can detect it at query time by setting \u2018quotePhrase\u2019 to true. This is a major enhancement, no need to do anything special at index time, now the queryParser would just double quote the detected phrase and run the search as a phrase query. Another advantage is you can update the autophrases.txt file on the fly, no need to re-index.\n\tUpdated the queryParser so it would not touch any term in quoted string, since it would be interfering with user\u2019s intend. For example, in query \u2018we are going to \u201cNew York airport\u201d\u2019 the phrase \u201cnew York airport\u201d is untouched.\nSide note, as far as comparing SOLR-4381 patch and this one, in my opinion they are complementary not competing. I did some experimentation by chaining AutoPhrasing and Query-time Synonym as a queryParser. They work well together, where one detected the phrases and the other one expanded the query to its synonyms. However, one issue I found was around acronyms in synonym list. For example, DC stands for \u2018Direct Current\u2019. If the index text has DC in it, searching for \u2018Current\u2019 would not match DC, since the indexed document has not expanded the term to \u2018Direct Current\u2019.\n\n ",
            "id": "comment-15014272"
        },
        {
            "date": "2015-11-20T16:07:06+0000",
            "author": "Ted Sullivan",
            "content": "Thanks for this submission Koorosh Vakhshoori! I think that this really helps to scale the autophrasing solution. Also the improvement in dealing with PositionLength is a big plus, as are the improvements in the query parser. Great work, thanks.\n\nI have seen some reports on the github version of my code about memory leaks. Have you looked at that? I will take your patch and try to do some A/B comparisons on this to see if the new FSM implementation (hopefully) removes that problem too. But in general, have you done any performance/scaling tests on your version of the autophrasing filter? Obviously, this goes along with the production-readiness that your new implementation makes possible. Thanks again for submitting this patch.\n\nAs to complementarity with SOLR-4381 - I would agree - nice to hear that the two solutions play nicely with each other  IMO this is an important problem that needs a committed solution. If we give Solr users more than one way to \"skin the cat\" - the better the chance that they will find a solution for their own problem set.  \n\nAs to the acronym 'DC' - this is also ambiguous because it also stands for \"District of Columbia\" - certainly domain context will clear this up some but not if you have a global search problem like Google or Bing. I'll look into this problem too. ",
            "id": "comment-15018207"
        },
        {
            "date": "2015-11-20T17:46:55+0000",
            "author": "Koorosh Vakhshoori",
            "content": "As far as the memory leaks issue, I looked at my version and identified couple of areas that it could cause problems: 1) in AutoPhrasingQParserPluging I updated the code so all resources associated with AutoPhrasingTokenFilter instance is released by calling end() and close(), 2) when it came to phraseSets, I make sure it is populated once, in the earlier version every time the filter was instantiated the constructor would repopulated it. However, in some cases you want that, I have a version of constructor that force-populate the phraseSets! I call it in the ManagedAutophraseFilterFactory class.\n\nAs far as performance/scaling, no I have not done any formal evaluation. All I can tell, we have it running in our QA and people who have tested it are satisfied with the speed. However, our speed requires are in seconds and not milliseconds. I love to hear the result of your A/B testing.\n\nOn the acronym topic, you hit the nail on the head. This falls under personalized or context search. In our use case, our content is collections of different corpus, i.e. carpi. This means different users depending on their specialty want to see different results for the same query. This is a tough nut to crack. Glad to hear you would be looking into this issue. ",
            "id": "comment-15018409"
        }
    ]
}