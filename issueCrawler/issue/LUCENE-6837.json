{
    "id": "LUCENE-6837",
    "title": "Add N-best output capability to JapaneseTokenizer",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "5.3",
        "components": [
            "modules/analysis"
        ],
        "labels": "",
        "fix_versions": [
            "6.0"
        ],
        "priority": "Minor",
        "status": "Resolved",
        "type": "Improvement"
    },
    "description": "Japanese morphological analyzers often generate mis-segmented tokens. N-best output reduces the impact of mis-segmentation on search result. N-best output is more meaningful than character N-gram, and it increases hit count too.\n\nIf you use N-best output, you can get decompounded tokens (ex: \"\u30b7\u30cb\u30a2\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30a8\u30f3\u30b8\u30cb\u30a2\" => \n{\"\u30b7\u30cb\u30a2\", \"\u30b7\u30cb\u30a2\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30a8\u30f3\u30b8\u30cb\u30a2\", \"\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\", \"\u30a8\u30f3\u30b8\u30cb\u30a2\"}\n) and overwrapped tokens (ex: \"\u6570\u5b66\u90e8\u9577\u8c37\u5ddd\" => \n{\"\u6570\u5b66\", \"\u90e8\", \"\u90e8\u9577\", \"\u9577\u8c37\u5ddd\", \"\u8c37\u5ddd\"}\n), depending on the dictionary and N-best parameter settings.",
    "attachments": {
        "LUCENE-6837 for 5.4.zip": "https://issues.apache.org/jira/secure/attachment/12781696/LUCENE-6837%20for%205.4.zip",
        "LUCENE-6837.patch": "https://issues.apache.org/jira/secure/attachment/12766268/LUCENE-6837.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14954421",
            "author": "KONNO, Hiroharu",
            "date": "2015-10-13T05:32:30+0000",
            "content": "LUCENE-6837.patch "
        },
        {
            "id": "comment-14954483",
            "author": "Koji Sekiguchi",
            "date": "2015-10-13T06:32:13+0000",
            "content": "We have our own morphological analyzer with n-best output.\n\nIf nobody take this, I'll assign to me.  "
        },
        {
            "id": "comment-14954487",
            "author": "Christian Moen",
            "date": "2015-10-13T06:35:56+0000",
            "content": "Thanks.  I've had a very quick look at the code and have some comments and questions.  I'm happy to take care of this, Koji. "
        },
        {
            "id": "comment-14954729",
            "author": "Michael McCandless",
            "date": "2015-10-13T09:54:23+0000",
            "content": "Wow, adding nbest to the best-first viterbi search is not easy! "
        },
        {
            "id": "comment-14978156",
            "author": "Christian Moen",
            "date": "2015-10-28T10:25:57+0000",
            "content": "Thanks a lot for this, Konno-san.  Very nice work!  I like the idea to calculate the n-best cost using examples.\n\nSince search mode and also extended mode solves a similar problem, I'm wondering if it makes sense to introduce n-best as a separate mode in itself.  In your experience in developing the feature, do you think it makes a lot of sense to use it with search and extended mode?\n\nI think I'm in favour of supporting it for all the modes, even though it perhaps makes the most sense for normal mode.  The reason for this is to make sure that the entire API for JapaneseTokenizer is functional for all the tokenizer modes.\n\nI'll add a few tests and I'd like to commit this soon. "
        },
        {
            "id": "comment-14979908",
            "author": "KONNO, Hiroharu",
            "date": "2015-10-29T06:20:34+0000",
            "content": "Thank you for your good evaluation.\n\nBecause the difference between N-best output and search-mode output is quite big, so I agree to your opinion to support the N-best for all modes. "
        },
        {
            "id": "comment-14995604",
            "author": "Christian Moen",
            "date": "2015-11-08T11:22:20+0000",
            "content": "I've attached a new patch with some minor changes:\n\n\n\tMade the System.out.printf being subject to VERBOSE being true\n\tIntroduced RuntimeException to deal with the initialization error cases\n\tRenamed the new parameters to nBestCost and nBestExamples\n\tAdded additional javadoc here and there to document the new functionality\n\n\n\nI'm planning on running some stability tests with the new tokenizer parameters next. "
        },
        {
            "id": "comment-15010673",
            "author": "Christian Moen",
            "date": "2015-11-18T10:16:51+0000",
            "content": "Tokenizing Japanese Wikipedia seems fine with nBestCost set, but it seems like random-blasting doesn't pass.\n\nKonno-san, I'm wondering if I can ask you the trouble of looking into why the testRandomHugeStrings fails with the latest patch?\n\nThe test basically does random-blasting with nBestCost set to 2000.  I think it's a good idea that we fix this before we commit.  I believe it's easily reproducible, but I used\n\n\nant test  -Dtestcase=TestJapaneseTokenizer -Dtests.method=testRandomHugeStrings -Dtests.seed=99EB179B92E66345 -Dtests.slow=true -Dtests.locale=sr_CS -Dtests.timezone=PNT -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n\n\n\nin my environment. "
        },
        {
            "id": "comment-15015323",
            "author": "KONNO, Hiroharu",
            "date": "2015-11-20T06:53:10+0000",
            "content": "Hi Christian,\n\nI found my mistake, and I update patch file. (only +1 line)\nI thought that the result of Set<Integer>.toArray() is sorted, but it is not.\nI added explicit sort code and it seems fine.\nPlease try it.\nthanks. "
        },
        {
            "id": "comment-15029762",
            "author": "Christian Moen",
            "date": "2015-11-27T10:53:06+0000",
            "content": "Thanks a lot, Konno-san.  Things look good.  My apologies that I couldn't look into this earlier.\n\nI've attached a new patch where I've included your fix and also renamed some methods.  I think it's getting ready... "
        },
        {
            "id": "comment-15037353",
            "author": "ASF subversion and git services",
            "date": "2015-12-03T06:27:45+0000",
            "content": "Commit 1717713 from Christian Moen in branch 'dev/trunk'\n[ https://svn.apache.org/r1717713 ]\n\nAdd n-best output to JapaneseTokenizer (LUCENE-6837) "
        },
        {
            "id": "comment-15042258",
            "author": "Michael McCandless",
            "date": "2015-12-04T21:27:45+0000",
            "content": "Christian Moen are you planning to backport this for 5.5? "
        },
        {
            "id": "comment-15093057",
            "author": "Ippei UKAI",
            "date": "2016-01-12T01:13:14+0000",
            "content": " I wanted to give this feature a try, and would like to share what I did so you can try it too.\n\nAttached is a slightly modified version of JapaneseTokenizer from r1717713 compiled for use with Lucene/Solr 5.4. Basically, modified classes are in a separate package so it does not conflict with existing.\n\nsolrconfig.xml:\n\n<lib dir=\"${solr.solr.home}/lib/LUCENE-6837\" />\n\n\n(if you place the files under $SOLR_HOME/lib/LUCENE-6837)\n\nschema.xml:\n\n        <tokenizer class=\"org.apache.lucene.analysis.ja6837.JapaneseTokenizerFactory\"\n           mode=\"NORMAL\"\n           discardPunctuation=\"true\"\n           nBestExamples=\"/\u30b7\u30cb\u30a2\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30a8\u30f3\u30b8\u30cb\u30a2-\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2/\u6570\u5b66\u90e8\u9577\u8c37\u5ddd-\u9577\u8c37\u5ddd/\"\n        />\n\n "
        },
        {
            "id": "comment-15093096",
            "author": "Christian Moen",
            "date": "2016-01-12T01:37:32+0000",
            "content": "Hello Mike,\n\nYes, I'd like to backport this to 5.5. "
        },
        {
            "id": "comment-15865799",
            "author": "Jan H\u00f8ydahl",
            "date": "2017-02-14T13:59:28+0000",
            "content": "This could be resolved with fix version 6.0, right? "
        }
    ]
}