{
    "id": "LUCENE-7866",
    "title": "Add TokenFilter to add custom term frequency (like DelimitedPayloadTokenFilter)",
    "details": {
        "labels": "",
        "priority": "Major",
        "resolution": "Fixed",
        "affect_versions": "None",
        "status": "Resolved",
        "type": "Improvement",
        "components": [
            "modules/analysis"
        ],
        "fix_versions": [
            "7.0"
        ]
    },
    "description": "This is a followup of LUCENE-7854. This will add a simple TokenFilter like DelimitedPayloadTokenFilter that can be used to index a custom term frequency: \"token|5\" will be index token \"token\" with a term freq of 5. The effect is the same as adding the token 5 times by a \"repeat token filter\".",
    "attachments": {
        "LUCENE-7866.patch": "https://issues.apache.org/jira/secure/attachment/12871701/LUCENE-7866.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16039768",
            "date": "2017-06-06T22:22:17+0000",
            "content": "Here is a quick patch.\n\nIn the payload package we also have a separate TokenFilter that adds a payload depending on the token type. We could have the same here. This allows to boost substantives or verbs, if the analysis chain assigns those token types.\n\nWhat do you think? We could then also move the whole thing to a separate package. ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16039776",
            "date": "2017-06-06T22:26:18+0000",
            "content": "Can we please document this only works when indexing terms & frequencies (but positions omitted) and only at index time?\n ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16039779",
            "date": "2017-06-06T22:27:45+0000",
            "content": "Simplify code by using subSequence; don't modify TF if no delimiter was found. ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16039781",
            "date": "2017-06-06T22:28:13+0000",
            "content": "Robert: OK! Will post new patch tomorrow! ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16039787",
            "date": "2017-06-06T22:33:50+0000",
            "content": "Maybe also avoid the toString by using the ArrayUtil method: https://github.com/apache/lucene-solr/blob/master/lucene/core/src/java/org/apache/lucene/util/ArrayUtil.java#L50-L52 ",
            "author": "Robert Muir"
        },
        {
            "id": "comment-16039793",
            "date": "2017-06-06T22:39:03+0000",
            "content": "New patch with documentation for TokenFilter and Factory (solr example) mentioning the requirement to not store positions or offsets. ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16039795",
            "date": "2017-06-06T22:39:59+0000",
            "content": "Maybe also avoid the toString by using the ArrayUtil method\n\nAh cool forgot about that. You are faster than I am creating patches  ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16039812",
            "date": "2017-06-06T22:43:36+0000",
            "content": "new patch with ArrayUtil. Thanks Robert! ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16044710",
            "date": "2017-06-09T17:21:00+0000",
            "content": "Michael McCandless: Does this look OK for you? What do you think about a TokenFilter that maps \"TypeAttribute\" to TF values? For payloads it is there, but I am not sure if this is useful. ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16045001",
            "date": "2017-06-09T20:44:25+0000",
            "content": "Patch looks great, thanks Uwe Schindler!\n\nI don't have any real opinion on the TypeAttribute approach as well; maybe see if users request this? ",
            "author": "Michael McCandless"
        },
        {
            "id": "comment-16045087",
            "date": "2017-06-09T21:53:31+0000",
            "content": "Thanks Mike, I committed that. \u2013 Uwe ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16045092",
            "date": "2017-06-09T21:56:38+0000",
            "content": "Commit b3b8344a720f8f7df34523126a46fb95fd1661e7 in lucene-solr's branch refs/heads/master from Uwe Schindler\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=b3b8344 ]\n\nLUCENE-7866: Add a new DelimitedTermFrequencyTokenFilter that allows to mark tokens with a custom term frequency (2nd commit to fix issue number) ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-16045214",
            "date": "2017-06-09T23:29:27+0000",
            "content": "My Jenkins found a reproducing TestFactories failure:\n\n\n   [junit4] Suite: org.apache.lucene.analysis.core.TestFactories\n   [junit4]   2> TEST FAIL: useCharFilter=true text='-q)|.f nvexq '\n   [junit4]   2> NOTE: reproduce with: ant test  -Dtestcase=TestFactories -Dtests.method=test -Dtests.seed=CF319AD344836FD4 -Dtests.slow=true -Dtests.locale=it -Dtests.timezone=America/Pangnirtung -Dtests.asserts=true -Dtests.file.encoding=US-ASCII\n   [junit4] ERROR   27.1s J0 | TestFactories.test <<<\n   [junit4]    > Throwable #1: java.lang.NumberFormatException: Unable to parse\n   [junit4]    > \tat __randomizedtesting.SeedInfo.seed([CF319AD344836FD4:4765A509EA7F022C]:0)\n   [junit4]    > \tat org.apache.lucene.util.ArrayUtil.parse(ArrayUtil.java:94)\n   [junit4]    > \tat org.apache.lucene.util.ArrayUtil.parseInt(ArrayUtil.java:83)\n   [junit4]    > \tat org.apache.lucene.util.ArrayUtil.parseInt(ArrayUtil.java:51)\n   [junit4]    > \tat org.apache.lucene.analysis.miscellaneous.DelimitedTermFrequencyTokenFilter.incrementToken(DelimitedTermFrequencyTokenFilter.java:67)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkAnalysisConsistency(BaseTokenStreamTestCase.java:731)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:642)\n   [junit4]    > \tat org.apache.lucene.analysis.BaseTokenStreamTestCase.checkRandomData(BaseTokenStreamTestCase.java:540)\n   [junit4]    > \tat org.apache.lucene.analysis.core.TestFactories.doTestTokenFilter(TestFactories.java:105)\n   [junit4]    > \tat org.apache.lucene.analysis.core.TestFactories.test(TestFactories.java:58)\n   [junit4]    > \tat java.lang.Thread.run(Thread.java:745)\n   [junit4]   2> NOTE: test params are: codec=CheapBastard, sim=RandomSimilarity(queryNorm=true): {}, locale=it, timezone=America/Pangnirtung\n   [junit4]   2> NOTE: Linux 4.1.0-custom2-amd64 amd64/Oracle Corporation 1.8.0_77 (64-bit)/cpus=16,threads=1,free=400823216,total=514850816\n   [junit4]   2> NOTE: All tests run in this JVM: [TestApostropheFilter, TestZeroAffix, TestDutchAnalyzer, TestMorphData, TestCodepointCountFilter, TestKeywordTokenizer, TestFactories]\n   [junit4] Completed [126/281 (1!)] on J0 in 27.14s, 1 test, 1 error <<< FAILURES!\n\n ",
            "author": "Steve Rowe"
        },
        {
            "id": "comment-16045448",
            "date": "2017-06-10T07:56:03+0000",
            "content": "Hi,\nI think this filter must be excluded. I will check how this is done with the DelimitedPayloadTokenFilter... ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16045456",
            "date": "2017-06-10T08:31:17+0000",
            "content": "Commit 1b9f060e25f09445e6f60956793d049dfca7a774 in lucene-solr's branch refs/heads/master from Uwe Schindler\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1b9f060 ]\n\nLUCENE-7866: Exclude DelimitedTermFrequencyTokenFilter from random data tests in random chains and factory tests ",
            "author": "ASF subversion and git services"
        },
        {
            "id": "comment-16045457",
            "date": "2017-06-10T08:31:24+0000",
            "content": "OK, I figured out:\n\nThe DelimitedPayloadTokenFilter does never fail in TestFactories or TestRandomChains, because it is impossible to \"randomly\" get a PayloadEncoder for ints (as you need to pass a special value to the factory). By default it just encodes the text behind the payload as a string payload. Because of that it never fails.\n\nBut In our case the constructor just has an char (the delimiter) and the part behind the delimiter MUST be an integer. And that breaks with all types of random data. ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16045458",
            "date": "2017-06-10T08:31:54+0000",
            "content": "I fixed it in master by excluding the filter from TestRandomChains and TestFactories. ",
            "author": "Uwe Schindler"
        },
        {
            "id": "comment-16045940",
            "date": "2017-06-11T12:29:52+0000",
            "content": "Commit d7808ebc6029a51c6f80e6700ca53f7c9a6523f0 in lucene-solr's branch refs/heads/master from Uwe Schindler\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d7808eb ]\n\nLUCENE-7866: Apply changes also to cloned test (TODO: fix this!) ",
            "author": "ASF subversion and git services"
        }
    ]
}