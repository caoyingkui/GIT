{
    "id": "SOLR-8367",
    "title": "The new LIR 'all replicas participate' failsafe code needs to be improved.",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.5",
            "6.0"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "For one, it currently only kicks in the first attempted leader. If it's another replica that is stuck in LIR, it won't help.\n\nSecond, when we attempt to be leader, knowing we might fail due to LIR, we should not put other replicas into recovery if they fail to sync with us - not until we know we will actually be leader.",
    "attachments": {
        "SOLR-8367.patch": "https://issues.apache.org/jira/secure/attachment/12776337/SOLR-8367.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-12-08T15:21:01+0000",
            "author": "Mark Miller",
            "content": "Patch with initial logic change.\n\nWe need to see if all the replicas are involved even if we are not the first replica in line for the leader election.\n\nWe also wait to request recoveries until we actually see we will be the leader. ",
            "id": "comment-15046960"
        },
        {
            "date": "2015-12-08T17:48:30+0000",
            "author": "Mark Miller",
            "content": "Working on a test, just hitting some issues. I think someone broke it so that when you restart a Jetty instance in a test, it's not getting the same data dir or something. ",
            "id": "comment-15047142"
        },
        {
            "date": "2015-12-08T18:31:03+0000",
            "author": "Mark Miller",
            "content": "Or it's my head is dopey and I need to be hard coding a filesystem based directory.\n\nJust surprised that when you don't, tlogs don't survive restart either. Perhaps because of the old index dir cleanup code. ",
            "id": "comment-15047208"
        },
        {
            "date": "2015-12-08T22:42:06+0000",
            "author": "Mark Miller",
            "content": "Okay, here is basically what we need with additional testing. Still need to give it another once over. ",
            "id": "comment-15047612"
        },
        {
            "date": "2015-12-09T00:09:25+0000",
            "author": "Mike Drob",
            "content": "SyncStrategy, the try-catch at line ~218 can go away.\nDo we need to check isClosed again during requestRecoveries? It's possible that it's false when the recoveries are being set up, but changes to true later by the time we actually make the RPC.\nAn optimization might be to break out of the whole loop when closed, since it looks like not much will be happening anyway.\n\nIn ZkController, can we log which retry we are on (in both places)? That will make parsing logs easier when this failure happens.\n\nYou have a couple System.out.println in SolrCore that could probably be log.debug or even trace.\n\nIn the test, you could store (HttpSolrClient) clients.get(0)).getBaseURL() as a local variable instead of loading it twice.\n\nIf you're fixing shard count to 2 in the test, do we still want to createCollection(testCollectionName, 1, 3, 1); for three shards?\n\nArchitecture question \u2013 What happens when you send an update FROMLEADER to the one that happens to be the leader? Also, why are we using decreasing versions? ",
            "id": "comment-15047731"
        },
        {
            "date": "2015-12-09T14:52:33+0000",
            "author": "Mark Miller",
            "content": "in SolrCore that could probably be log.debug or even trace.\n\nSystem.out is not allowed in SolrCore, I just have not cleaned up yet.\n\nIf you're fixing shard count to 2 in the test, do we still want to createCollection(testCollectionName, 1, 3, 1); for three shards?\n\nYes, create api uses 3 shards (there is a control jetty).\n\nWhat happens when you send an update FROMLEADER to the one that happens to be the leader?\n\nSee the conflict exception we check for.\n\nAlso, why are we using decreasing versions?\n\nBecause it doesn't matter for this test. ",
            "id": "comment-15048768"
        },
        {
            "date": "2015-12-09T15:00:13+0000",
            "author": "Mark Miller",
            "content": "I've clean up the patch and beasted the test. Looking good. ",
            "id": "comment-15048776"
        },
        {
            "date": "2015-12-09T15:03:49+0000",
            "author": "Mark Miller",
            "content": "An optimization might be to break out of the whole loop when closed, since it looks like not much will be happening anyway.\n\nSince they are all fired off async, I don't know that it is really worth it. All the isClosed stuff is really just best effort to bail early, but not really critical it's at every point. ",
            "id": "comment-15048782"
        },
        {
            "date": "2015-12-09T18:32:13+0000",
            "author": "Mike Drob",
            "content": "Since they are all fired off async, I don't know that it is really worth it. All the isClosed stuff is really just best effort to bail early, but not really critical it's at every point.\nI see what you're saying about it being async, so it was still possible for a close to sneak in before this patch as well. If we're closed, but still request a replica to recover, then I see that it has it's own checks for shutting down and closed as well so things will be fine there.\n\nUnrelated: while trying to trace the execution path here, I noticed that CoreAdminHandler::handleRequestRecoveryAction creates a thread and starts it without either giving it a name or submitting it to an executor. Should I file a separate JIRA for that? Looks like that thread was added by you in SOLR-4254, maybe that was before we had the executors everywhere. ",
            "id": "comment-15049146"
        },
        {
            "date": "2015-12-09T18:37:23+0000",
            "author": "Mark Miller",
            "content": "Should I file a separate JIRA for that? \n\nYeah, probably a good idea.\n\nmaybe that was before we had the executors everywhere.\n\nYeah, most of the first pass cloud code was not using executors - recovery stuff, syncstrategy stuff, etc. Most of it has been converted over time. ",
            "id": "comment-15049155"
        },
        {
            "date": "2015-12-09T23:30:36+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1718987 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1718987 ]\n\nSOLR-8367: Fix the LeaderInitiatedRecovery 'all replicas participate' fail-safe. ",
            "id": "comment-15049625"
        },
        {
            "date": "2015-12-10T03:02:45+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1719005 from Mark Miller in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1719005 ]\n\nSOLR-8367: Fix the LeaderInitiatedRecovery 'all replicas participate' fail-safe. ",
            "id": "comment-15049928"
        },
        {
            "date": "2015-12-10T16:22:28+0000",
            "author": "Mark Miller",
            "content": "Thanks for the review Mike! Let me know if anything else comes up. ",
            "id": "comment-15051178"
        }
    ]
}