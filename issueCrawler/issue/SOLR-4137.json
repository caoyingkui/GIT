{
    "id": "SOLR-4137",
    "title": "FastVectorHighlighter: StringIndexOutOfBoundsException in BaseFragmentsBuilder",
    "details": {
        "affect_versions": "3.6.1,                                            4.0,                                            4.1,                                            4.2,                                            4.2.1",
        "status": "Closed",
        "fix_versions": [
            "4.3"
        ],
        "components": [
            "highlighter"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "under some circumstances the BaseFragmentsBuilder genereates a StringIndexOutOfBoundsException inside the makeFragment method.\nThe starting offset is higher than the end offset.\n\nI did a small patch checking the offsets and posted it over there at Stackoverflow: http://stackoverflow.com/questions/12456448/solr-highlight-bug-with-usefastvectorhighlighter\n\nThe code in 4.0 seems to be the same as in 3.6.1\n\nExample how to reproduce the behaviour:\nThere is a word called \"www.DAKgesundAktivBonus.de\" inside the index. If you search for \"dak bonus\" some offset calculations went wrong.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Markus Jelsma",
            "id": "comment-13508737",
            "date": "2012-12-03T13:52:53+0000",
            "content": "Can you attach your patch to this issue? "
        },
        {
            "author": "Marcel",
            "id": "comment-13508741",
            "date": "2012-12-03T13:57:53+0000",
            "content": "Sure!\n\nI've edited the BaseFragmentsBuilder method makeFragments on line 166 and following:\n\nBaseFragmentsBuilder.java\n \nint startOffset = to.getStartOffset() - modifiedStartOffset[0] < 0 ? 0 : to.getStartOffset() - modifiedStartOffset[0];\nint endOffset = to.getEndOffset() - modifiedStartOffset[0] > src.length()-1 ? src.length()-1 : to.getEndOffset() - modifiedStartOffset[0];\nif (srcIndex < startOffset) {\n    fragment\n    .append(encoder.encodeText(src.substring(srcIndex, startOffset)))\n    .append( getPreTag( preTags, subInfo.getSeqnum() ) )\n    .append( encoder.encodeText( src.substring(startOffset, endOffset ) ) )\n    .append( getPostTag( postTags, subInfo.getSeqnum() ) );\n}\nsrcIndex = endOffset; \n\n \n\nYou can download the patched JAR here. Did not use the maven classifier - instead just a version modifier.\n\nDid some logging before - seems like the main problem is the srcIndex being bigger than the startOffset.  "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13508754",
            "date": "2012-12-03T14:21:34+0000",
            "content": "Hi, can you provide information to reproduce what makes these bogus offsets?\n\nIt doesn't have to be a test: maybe its enough to post your analysis chain for the field.\n\nThis is a bug in an analyzer: I don't think we should hide it in the highlighter. "
        },
        {
            "author": "Marcel",
            "id": "comment-13508764",
            "date": "2012-12-03T14:34:05+0000",
            "content": "@Robert - sure. Posted an example in the bug description. Here is my analyzer chain:\n\n\nIndex Analyzer\norg.apache.solr.analysis.WhitespaceTokenizerFactory {luceneMatchVersion=LUCENE_36}\norg.apache.solr.analysis.HyphenatedWordsFilterFactory {luceneMatchVersion=LUCENE_36}\norg.apache.solr.analysis.WordDelimiterFilterFactory {preserveOriginal=1, splitOnCaseChange=1, generateNumberParts=1, catenateWords=1, types=wdftypes.txt, luceneMatchVersion=LUCENE_36, generateWordParts=1, catenateAll=0, catenateNumbers=1}\norg.apache.solr.analysis.LowerCaseFilterFactory {luceneMatchVersion=LUCENE_36}\norg.apache.solr.analysis.StopFilterFactory {words=lang/stopwords_de.txt, ignoreCase=true, enablePositionIncrements=true, luceneMatchVersion=LUCENE_36}\norg.apache.solr.analysis.DictionaryCompoundWordTokenFilterFactory {maxSubwordSize=15, onlyLongestMatch=true, minSubwordSize=4, minWordSize=5, dictionary=spellings.txt, luceneMatchVersion=LUCENE_36}\norg.apache.solr.analysis.GermanNormalizationFilterFactory {luceneMatchVersion=LUCENE_36}\norg.apache.solr.analysis.GermanStemFilterFactory {luceneMatchVersion=LUCENE_36}\norg.apache.solr.analysis.SnowballPorterFilterFactory {language=German2, luceneMatchVersion=LUCENE_36}\norg.apache.solr.analysis.RemoveDuplicatesTokenFilterFactory {luceneMatchVersion=LUCENE_36}\n\n "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13508769",
            "date": "2012-12-03T14:41:50+0000",
            "content": "Thanks Marcel. Its useful to know (but I'm sorry you are having to deal with it), that these failures\nare not just theoretical but happening in real life.\n\nWe have a test that finds these bugs and documents a list of broken analyzers (http://svn.apache.org/repos/asf/lucene/dev/trunk/lucene/analysis/common/src/test/org/apache/lucene/analysis/core/TestRandomChains.java) \n\nYou can see e.g. WordDelimiterFilter, HyphenatedWordsFilter, etc on these lists and also in your chain.\nDue to the specific error, I think its WordDelimiterFilter in this case.\n\nits time to start fixing these buggy analyzers! Thanks for reporting this. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13508793",
            "date": "2012-12-03T15:18:12+0000",
            "content": "We've seen this issue only for some documents in a large corpus due to HyphenatorCompound filter. "
        },
        {
            "author": "Marcel",
            "id": "comment-13508883",
            "date": "2012-12-03T17:35:52+0000",
            "content": "Cool! So it might be fixed when updating to the 4.x version next year \nUntil then I have to work with my fixed JAR - this will do it.\n\nGood luck fixing it! I'm glad helping you... "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13509777",
            "date": "2012-12-04T14:50:52+0000",
            "content": "I have to add that we're also seeing this rather frequently with analyzers that use the WDF. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13537001",
            "date": "2012-12-20T12:47:54+0000",
            "content": "Robert, do you happen to have opened issues for the issues with the filter themselves? I could not find any in Lucene nor in Solr's jira. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13537004",
            "date": "2012-12-20T12:57:58+0000",
            "content": "there are various ones.\n\nThe master list however, is the shitlists in TestRandomChains.java\n\ni think this should reference the appropriate jira issues too, where they exist. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13537007",
            "date": "2012-12-20T13:02:13+0000",
            "content": "I will take a look at this list later and try to add comments for each one.\nwe can also make a big jira issue that also includes those comments for more visibility.\n\ntrust me, i want this crap fixed. but its really hard.\n\na lot of these analyzers in these lists are broken by design, its not like we can just go and fix them. "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13537041",
            "date": "2012-12-20T14:28:14+0000",
            "content": "Markus, please see LUCENE-4641. This is the current list I referred to. "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13537113",
            "date": "2012-12-20T16:12:34+0000",
            "content": "Thanks!! "
        },
        {
            "author": "Simon Willnauer",
            "id": "comment-13624355",
            "date": "2013-04-06T06:01:07+0000",
            "content": "Markus, can you take a look if LUCENE-4899 fixes this issue for you?\n\nsimon "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13626319",
            "date": "2013-04-09T07:07:43+0000",
            "content": "Simon, we'll try to reproduce the problem without LUCENE-4899 and report if we can and whether the patch works.. "
        },
        {
            "author": "Simon Willnauer",
            "id": "comment-13626322",
            "date": "2013-04-09T07:13:45+0000",
            "content": "thanks markus, this would be awesome! "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13627768",
            "date": "2013-04-10T13:13:47+0000",
            "content": "Simon. We've reproduced the issue by running a large set of queries with our custom decompounder enabled and got plenty SIOOBE's. After patching LUCENE-4899 we could no longer reproduce the SIOOB so this is very promising!\n\nAlthough i don't like to assume things, this might also fixes a rare SIOOB when using the WordDelimiterFilter but we have no material with which to safely reproduce. But since both filters produce faulty positions (LUCENE-4641) i have good hopes!\n\nThanks Simon! "
        },
        {
            "author": "Simon Willnauer",
            "id": "comment-13627785",
            "date": "2013-04-10T13:32:34+0000",
            "content": "awesome so I guess we can close this issue? "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13627789",
            "date": "2013-04-10T13:37:08+0000",
            "content": "Yes please.\nCheers! "
        },
        {
            "author": "Marcel",
            "id": "comment-13627792",
            "date": "2013-04-10T13:40:59+0000",
            "content": "Cool to see the positive process on this!  "
        },
        {
            "author": "Simon Willnauer",
            "id": "comment-13627798",
            "date": "2013-04-10T13:50:54+0000",
            "content": "fixed with LUCENE-4899 "
        },
        {
            "author": "Markus Jelsma",
            "id": "comment-13644451",
            "date": "2013-04-29T12:39:08+0000",
            "content": "Hi Simon, FYI, it seems part of the bug is still present. We're testing whether or not we can get rid of the stopfilter, now we're seeing several queries fail with the following stacktrace if there's a stopword present in the query.\n\n\njava.lang.StringIndexOutOfBoundsException: String index out of range: 228\n\tat java.lang.String.substring(String.java:1934)\n\tat org.apache.lucene.search.vectorhighlight.BaseFragmentsBuilder.makeFragment(BaseFragmentsBuilder.java:178)\n\tat org.apache.lucene.search.vectorhighlight.BaseFragmentsBuilder.createFragments(BaseFragmentsBuilder.java:144)\n\tat org.apache.lucene.search.vectorhighlight.FastVectorHighlighter.getBestFragments(FastVectorHighlighter.java:186)\n\tat org.apache.solr.highlight.DefaultSolrHighlighter.doHighlightingByFastVectorHighlighter(DefaultSolrHighlighter.java:564)\n\tat org.apache.solr.highlight.DefaultSolrHighlighter.doHighlighting(DefaultSolrHighlighter.java:391)\n\tat org.apache.solr.handler.component.HighlightComponent.process(HighlightComponent.java:139)\n\tat org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:216)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1811)\n\tat org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:660)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:345)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:141)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1307)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:453)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:560)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1072)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:382)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1006)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:365)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:485)\n\tat org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:926)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:988)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:635)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\tat org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)\n\tat org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:662)\n\n "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13653982",
            "date": "2013-05-10T10:33:38+0000",
            "content": "Closed after release. "
        }
    ]
}