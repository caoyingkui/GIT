{
    "id": "LUCENE-5917",
    "title": "complex Query cause luene outMemory",
    "details": {
        "type": "Bug",
        "priority": "Minor",
        "labels": "",
        "resolution": "Unresolved",
        "components": [
            "core/search"
        ],
        "affect_versions": "4.9",
        "status": "Open",
        "fix_versions": []
    },
    "description": "RangeQuery, prefixQuery and WildcardQuery use FixedBitSet when TERM_COUNT >= 350 or DOC_COUNT_PERCENT >=0.1.\nIt use a lots of memory when maxDoc very large.\n\nMultiTermQueryWrapperFilter<Q extends MultiTermQuery> extends Filter\n\na little threads run with query \"a* OR b* OR c*.....OR z*\u201c will cause luene outMemory, but there is no ways to prevent it.\n\nanother thing, some complex query also use a lot of memory too..\n\n\nI think query implements Accountable(#ramSizeInBytes, #ramBytesUsed), users can throw a exception better than OutOfMemory.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14117852",
            "author": "Littlestar",
            "content": "\nMultiTermQueryWrapperFilter.java\n...........\n@Override\n  public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {\n    final AtomicReader reader = context.reader();\n    final Fields fields = reader.fields();\n    if (fields == null) {\n      // reader has no fields\n      return null;\n    }\n\n    final Terms terms = fields.terms(query.field);\n    if (terms == null) {\n      // field does not exist\n      return null;\n    }\n\n    final TermsEnum termsEnum = query.getTermsEnum(terms);\n    assert termsEnum != null;\n    if (termsEnum.next() != null) {\n      // fill into a FixedBitSet\n      final FixedBitSet bitSet = new FixedBitSet(context.reader().maxDoc()); //==================here......\n      DocsEnum docsEnum = null;\n      do {\n        // System.out.println(\"  iter termCount=\" + termCount + \" term=\" +\n        // enumerator.term().toBytesString());\n        docsEnum = termsEnum.docs(acceptDocs, docsEnum, DocsEnum.FLAG_NONE);\n        int docid;\n        while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {\n          bitSet.set(docid);\n        }\n      } while (termsEnum.next() != null);\n      // System.out.println(\"  done termCount=\" + termCount);\n\n      return bitSet;\n    } else {\n      return null;\n    }\n  }\n\n ",
            "date": "2014-09-02T03:09:45+0000"
        },
        {
            "id": "comment-14118179",
            "author": "Michael McCandless",
            "content": "Unfortunately such adversarial queries do exist in Lucene ... it's not clear how best to address this.\n\nAdding Accountable to Query is a little tricky because it depends on the searcher (Accountable doesn't take searcher today) and for these MTQs ... they have to do quite a bit of work in rewrite to figure out which strategy they will use in order to figure out how much RAM that strategy will take.\n\nAlso, with LUCENE-5879, we may be able to reduce the times that we write to the bitset for prefix queries. ",
            "date": "2014-09-02T13:17:35+0000"
        }
    ]
}