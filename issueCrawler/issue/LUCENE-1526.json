{
    "id": "LUCENE-1526",
    "title": "For near real-time search, use paged copy-on-write BitVector impl",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "core/index"
        ],
        "type": "Improvement",
        "fix_versions": [],
        "affect_versions": "2.4",
        "resolution": "Won't Fix",
        "status": "Closed"
    },
    "description": "SegmentReader currently uses a BitVector to represent deleted docs.\nWhen performing rapid clone (see LUCENE-1314) and delete operations,\nperforming a copy on write of the BitVector can become costly because\nthe entire underlying byte array must be created and copied. A way to\nmake this clone delete process faster is to implement tombstones, a\nterm coined by Marvin Humphrey. Tombstones represent new deletions\nplus the incremental deletions from previously reopened readers in\nthe current reader. \n\nThe proposed implementation of tombstones is to accumulate deletions\ninto an int array represented as a DocIdSet. With LUCENE-1476,\nSegmentTermDocs iterates over deleted docs using a DocIdSet rather\nthan accessing the BitVector by calling get. This allows a BitVector\nand a set of tombstones to by ANDed together as the current reader's\ndelete docs. \n\nA tombstone merge policy needs to be defined to determine when to\nmerge tombstone DocIdSets into a new deleted docs BitVector as too\nmany tombstones would eventually be detrimental to performance. A\nprobable implementation will merge tombstones based on the number of\ntombstones and the total number of documents in the tombstones. The\nmerge policy may be set in the clone/reopen methods or on the\nIndexReader.",
    "attachments": {
        "LUCENE-1526.patch": "https://issues.apache.org/jira/secure/attachment/12425180/LUCENE-1526.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2009-01-21T20:18:02+0000",
            "content": "> A tombstone merge policy needs to be defined to determine when to\n> merge tombstone DocIdSets into a new deleted docs BitVector as too\n> many tombstones would eventually be detrimental to performance. \n\nFor Lucene, I think the SegmentReader should lazily create an internal\nstructure to hold the deleted doc IDs on the first search.  It would either be\nan integer array (if there are few deletions) or a BitVector (if there are\nmany), and it would be created by recording the output of a priority queue\nmerging multiple tombstone streams.\n\nSubsequent calls would not require the priority queue, but would use an\niterator wrapper around the shared int array / BitVector.\n\nFor Lucy/KS, if we are to stick with the \"cheap IndexReader\" model, we'll want\nto keep using the priority queue.  Thus, it will be important to keep the\ndeletion rate for the whole index down, in order to minimize priority queue\niteration costs.  One possibility is to have the default merge policy\nautomatically consolidate any segment as soon as its deletion rate climbs over\n10%.  That's pretty aggressive, but it keeps the search-time deletions\niteration costs reasonably low, and it's in the spirit of \"few writes, many\nreads\".\n\n> A probable implementation will merge tombstones based on the number of\n> tombstones and the total number of documents in the tombstones. The merge\n> policy may be set in the clone/reopen methods or on the IndexReader. \n\nWould it make sense to realize a new integer array / BitVector on the first\nsearch after any new tombstone rows are added which affect the segment in\nquestion? ",
            "author": "Marvin Humphrey",
            "id": "comment-12665955"
        },
        {
            "date": "2009-01-21T21:28:29+0000",
            "content": "\n\nFor Lucene, I think the SegmentReader should lazily create an internal\nstructure to hold the deleted doc IDs on the first search.\n\nThis is basically doing the copy-on-write, which for realtime search\nwe're wanting to avoid.  But as long as this is a sparse structure\n(sorted list of deleted docIDs, assuming not many deletes accumulate\nin RAM) it should be OK.\n\nI also think for Lucene we could leave the index format unchanged\n(which means commit() is still more costly than it need be, but I'm\nnot sure that's too serious), and use tombstones/list-of-sorted-docIDs\nrepresentation only in RAM.\n\nFor realtime search, I think we can accept some slowdown of search\nperformance in exchange for very low latency turnaround when\nadding/deleting docs.\n\nBut I think these decisions (the approach we take here) is very much\ndependent on what we learn from the performance tests from\nLUCENE-1476. ",
            "author": "Michael McCandless",
            "id": "comment-12665969"
        },
        {
            "date": "2009-01-21T21:32:07+0000",
            "content": "Also, an in-RAM binary tree representation can be nicely transactional (there\nwas a recent thread about java-dev about this), so that on clone we would\nmake a very low cost clone of the deleted docs which we could then change\nw/o affecting the original.  Ie the 'copy on write' is still done, but it's not all\ndone up front \u2013 each insert would copy only the nodes it needs, so the cost\nis amortized. ",
            "author": "Michael McCandless",
            "id": "comment-12665971"
        },
        {
            "date": "2009-01-23T17:45:44+0000",
            "content": "\nIn addition to deletions, we also eventually need an\nincremental-copy-on-write data structure for norms.  Full\ncopy-on-write for norms (which LUCENE-1314 will enable) is even more\ncostly than full copy-on-write for deletions since each norm is 1 byte\nvs 1 bit for deletions.\n\nThough, it seems less common the people are setting norms, so it's\nprobably lower priority.\n\nBut I can still see \"real-time norm changing\" as being quite useful,\neg if you want to bias normal relevance sorting to mixin some measure\nof popularity (eg recording how often each document is clicked), it'd\nbe good to have real-time norm updating for that. ",
            "author": "Michael McCandless",
            "id": "comment-12666626"
        },
        {
            "date": "2009-01-23T18:34:41+0000",
            "content": "> we also eventually need an incremental-copy-on-write data structure for norms.\n\nI'd rather do realtime  incremental column stride fields in general with norms being a specific case.\n\n> if you want to bias normal relevance sorting to mixin some measure\n\nThis seem to fit in with allowing alternative search algorithms to TF/IDF.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12666643"
        },
        {
            "date": "2009-01-23T19:18:36+0000",
            "content": "\nFor realtime search, I think we can accept some slowdown of\nsearch performance in exchange for very low latency turnaround when\nadding/deleting docs. \n\nI'm still creating performance tests, however the system is simply\nusing skipto, to obtain the next deleted doc, the next deleted doc is\ncached in segmenttermdocs. So there shouldn't be a slowdown? ",
            "author": "Jason Rutherglen",
            "id": "comment-12666659"
        },
        {
            "date": "2009-01-23T19:31:28+0000",
            "content": "I'd rather do realtime incremental column stride fields in general with norms being a specific case.\n\nThat would be even better! ",
            "author": "Michael McCandless",
            "id": "comment-12666664"
        },
        {
            "date": "2009-01-23T19:58:20+0000",
            "content": "\nI'm still creating performance tests, however the system is simply\nusing skipto, to obtain the next deleted doc, the next deleted doc is\ncached in segmenttermdocs. So there shouldn't be a slowdown?\n\nI was actually thinking more generally that this is the natural\ntradeoff one makes with realtime search.  EG flushing a new segment\nevery document or two will necessarily give worse indexing throughput\nthan bulk indexing w/ large RAM buffer, but gives much faster\nturnaround on searching.\n\nBut it sounds like you're talking specifically about performance of\nswitching to iterator access to deleted docs.  I think the larger\nnumber of [harder-for-cpu-to-predict] if statements may be the cause\nof the slowdown once %tg deletes gets high enough?  Also, if the\nunderlying skipTo is a linear scan over a non-sparse representation\nthen that's further cost. ",
            "author": "Michael McCandless",
            "id": "comment-12666676"
        },
        {
            "date": "2009-03-18T18:21:46+0000",
            "content": "I don't think we should block 2.9 for this. ",
            "author": "Michael McCandless",
            "id": "comment-12683119"
        },
        {
            "date": "2009-06-12T18:24:02+0000",
            "content": "This issue has somewhat changed course from relying on DocIdSet\nto having a BitVector implementation that is backed by a\nbyte[][] instead of a byte[]. This will allow modification of\nparts of the array without allocating the entire array for\nclone/copy-on-write. This saves on byte[] allocation for large\nsegments. \n\nWe'll also need to benchmark vs. BitVector byte[].  ",
            "author": "Jason Rutherglen",
            "id": "comment-12718932"
        },
        {
            "date": "2009-09-25T23:55:44+0000",
            "content": "This moves us on our way more efficient memory usage in heavy\nnear realtime search apps, because we don't have to reallocate\nan entire byte[] equals to the maxDoc of the segment(s) that\nhave new deletes.\n\n\n\tA 2 dimensional byte array is used where each actual array is\n1024 in length. \n\n\n\n\n\tA refs boolean array keeps track of the which arrays need to\nbe copied when a bit is set (copy on ref). \n\n\n\n\n\tThe code can be benchmarked against the existing BV\nimplementation for any possible speed slowdown due to the extra\narray lookup (probably minimal to nothing)\n\n\n\n\n\tNeed to implement the dgaps encoding\n\n\n\n\n\tCode isn't committable\n\n\n\n\n\tStill need to run all tests, however TestMultiBitVector passes\n\n ",
            "author": "Jason Rutherglen",
            "id": "comment-12759819"
        },
        {
            "date": "2009-11-07T14:19:23+0000",
            "content": "This looks like good progress!  So it's a copy-on-write, by fixed page\nsize (8 kbits, by default), BitVector.\n\nOne danger of MultiBitVector is if one does a deletion against an\nalready cloned instance, right?  Because each instance only tracks\nrefs back to the instance it was cloned from, not forward refs of\nother instances that have cloned it?  So a clone of myself would\nincorrectly see changes that I make.\n\nThat said, Lucene's internal use shouldn't ever do that \u2013 when we\nclone the deleted docs, the previous instance should never be touched\nagain (the \"write lock\" moves to the new clone, just like when\nreopening a reader that's holding the write lock).  Can you add\nassertions into MultiBitVector to verify this?  And explain in\njavadocs that once you clone it, it's frozen.\n\nAlso, I don't think we should force SegmentReader to always use MBV,\nunless we're sure the perf hit is negligible?  Can we somehow\nconditionalize that?\n\nWhat remains here?  Ie what tests fail & why?  (Or, why isn't it\ncommittable?).  If you can get it to a committable state, I can run\nsome perf tests...\n\nIn LUCENE-1458, the new flex API uses a simple interface (called\n\"Bits\") to represent docs that should be skipped, and when you ask for\nthe DocsEnum, you pass in your \"Bits skipDocs\".  This will be\nimportant for LUCENE-1536, but also important for this issue because\nit'll make swapping in different Bits impls easy. ",
            "author": "Michael McCandless",
            "id": "comment-12774602"
        },
        {
            "date": "2009-11-07T14:24:08+0000",
            "content": "Changing summary to match evolution of this issue... ",
            "author": "Michael McCandless",
            "id": "comment-12774603"
        },
        {
            "date": "2009-11-07T14:31:02+0000",
            "content": "Another approach we might take here is to track new deletions using a\nsorted tree.  New deletions insert in O(log(N)) time, and then we can\nefficiently expose a DocIdSetIterator from that.\n\nThen, every search would have this (if present) folded in as a \"not\"\nclause/filter.\n\nOnly near real-time readers would have this.\n\nAnd then, somehow, periodically that tree would have to be merged in\nto the real deleted docs if it ever got to big, or, it was time to\ncommit.\n\nThis is a biggish change because suddenly IndexSearcher must be aware\nthat a given SegmentReader is carrying near-real-time deletions, and \nbuild a BooleanQuery each time.  Whereas MultiBitVector nicely keeps\nall this under-the-hood. ",
            "author": "Michael McCandless",
            "id": "comment-12774605"
        },
        {
            "date": "2009-11-07T15:01:46+0000",
            "content": "Another approach we might take here is to track new deletions using a\nsorted tree. New deletions insert in O(log(N)) time, and then we can\nefficiently expose a DocIdSetIterator from that. \n\nWe did this in Zoie for a while, and it turned out to be a bottleneck - not as much of a bottleneck as continually cloning a bitvector (that was even worse), but still not good.  We currently use a bloomfilter on top of an openintset, which performs pretty fantastically: constant-time adds and even-faster constant-time contains() checks, with small size (necessary for the new Reader per query scenario since this requires lots of deep-cloning of this structure).\n\nJust a note from our experience over in zoie-land.  It also helped to not produce a docIdset iterator using these bits, but instead override TermDocs to be returned on the disk reader, and keep track of it directly there. ",
            "author": "Jake Mannix",
            "id": "comment-12774628"
        },
        {
            "date": "2009-11-07T15:41:51+0000",
            "content": "We did this in Zoie for a while, and it turned out to be a bottleneck - not as much of a bottleneck as continually cloning a bitvector (that was even worse), but still not good. We currently use a bloomfilter on top of an openintset, which performs pretty fantastically: constant-time adds and even-faster constant-time contains() checks, with small size (necessary for the new Reader per query scenario since this requires lots of deep-cloning of this structure).\n\nGood, real-world feedback \u2013 thanks!  This sounds like a compelling\napproach.\n\nSo the SegmentReader still had its full BitVector, but your OpenIntSet\n(what exactly is that?) + the bloom filter is then also checked when\nyou enum the TermDocs?  It's impressive this is fast enough... do you\nexpect this approach to be faster than the paged \"copy on write\" bit\nvector approach?\n\nIt also helped to not produce a docIdset iterator using these bits, but instead override TermDocs to be returned on the disk reader, and keep track of it directly there.\n\nThe flex API should make this possible, without overriding TermDocs\n(just expose the Bits interface). ",
            "author": "Michael McCandless",
            "id": "comment-12774631"
        },
        {
            "date": "2009-11-07T20:17:52+0000",
            "content": "The issue of not using a BitSet/BitVector is not simply performance, but also memory cost.\nIn the case where deletes are sparse (expectedly so) and the index is large, BitSet/BitVector is not a good representation of a DocSet.\nFor deleted checks, it is usually of this pattern:\n\nIterate thru my docs, for each doc i\n   isDeletedCheck(doc i)\n\nSo the cost is not really iterating the deleted set per say, it is the check.\n\nWe use bloomfilter to filter out negatives (mostly the case) and back it up with the underlying docset (normally an instance of openhashset) for positives.\n\nCode is at:\n\nhttp://code.google.com/p/zoie/source/browse/trunk/java/proj/zoie/api/impl/util/IntSetAccelerator.java\n\nFeel free to play with it and run some perf numbers on your data. ",
            "author": "John Wang",
            "id": "comment-12774662"
        },
        {
            "date": "2009-11-07T21:02:19+0000",
            "content": "The issue of not using a BitSet/BitVector is not simply performance, but also memory cost.\n\nBut don't you cutover all future searches to the newest NRT reader\nright away?  Ie, those large bit vectors are all transient, so net mem\ncost at any given time should be well contained?\n\nOr... do you keep many readers (against different commit points)\naround for a longish time (longer than just allowing for the in-flight\nsearches to complete)?\n\nIn the case where deletes are sparse (expectedly so) and the index is large, BitSet/BitVector is not a good representation of a DocSet.\n\nI agree... it's done so \"contains\" is fast.  But we had looked into\n\"being sparse\" (and using an iterator to \"and not\" the deleted docs,\nat the TermDocs level) and the performance was quite a bit worse...\n\n\n\nFor deleted checks, it is usually of this pattern:\nIterate thru my docs, for each doc i\nisDeletedCheck(doc i)\n\nSo the cost is not really iterating the deleted set per say, it is the check.\n\nRight.\n\nWe use bloomfilter to filter out negatives (mostly the case) and back it up with the underlying docset (normally an instance of openhashset) for positives.\n\nThis makes sense, and it's a nice solution.\n\nBut how do you make this transactional?  Do you just make a full clone\nof IntSetAccelerator (& the underling int set) on every reopen?\n\nAlso, what policy/approach do you use to periodically fold the deletes\nback into the SegmentReader?  As you accumulate more and more deletes\nin the IntSet, cloning becomes more costly.\n\n\nCode is at:\n\nhttp://code.google.com/p/zoie/source/browse/trunk/java/proj/zoie/api/impl/util/IntSetAccelerator.java\n\nFeel free to play with it and run some perf numbers on your data.\n\nThanks! ",
            "author": "Michael McCandless",
            "id": "comment-12774667"
        },
        {
            "date": "2009-11-07T21:23:48+0000",
            "content": "\nWe do not hold the deleted set for a long period of time. I agree the memory cost is not a \"killer\" but it is tremendously wasteful, e.g. 10 M doc index, you have say 2 docs deleted, 0 and 9999999, you are representing it with 5M of memory (where you could have reprsented it with 2 ints, 8 bytes). Sure it is an extremely case, if you look at the avg number of deleted docs vs index size, it is usually sparse. hence we avoided this approach.\n\nWe looked at trade-offs between this vs. our approach, for us, it was a worth-while trade-off. \n\nWe do not accumulate deletes. Deletes/updates are tracked per batch, and special TermDocs are returned to skip over the deleted/mod set for the given reader.\n\nWe simply call delete on the diskreader for each batch and internal readers are refreshed with deletes loaded in background. ",
            "author": "John Wang",
            "id": "comment-12774671"
        },
        {
            "date": "2009-11-07T23:53:06+0000",
            "content": "We do not hold the deleted set for a long period of time. I agree the memory cost is not a \"killer\" but it is tremendously wasteful, e.g. 10 M doc index, you have say 2 docs deleted, 0 and 9999999, you are representing it with 5M of memory (where you could have reprsented it with 2 ints, 8 bytes). Sure it is an extremely case, if you look at the avg number of deleted docs vs index size, it is usually sparse. hence we avoided this approach.\n\nActually a 10 M doc index would be 1.25 MB BitVector right?\n\nBut, I agree it's wasteful of space when deletes are so\nsparse... though it is fast.\n\nSo are you using this, only, as your deleted docs?  Ie you don't store\nthe deletions with Lucene?  I'm getting confused if this is only for\nthe NRT case, or, in general.\n\nHave you compared performance of this vs straight lookup in BitVector?\n\nWe do not accumulate deletes. Deletes/updates are tracked per batch, and special TermDocs are returned to skip over the deleted/mod set for the given reader.\n\nOK, I think I'm catching up here... so you only open a new reader at\nthe batch boundary right?  Ie, a batch update (all its adds & deletes)\nis atomic from the readers standpoint?\n\nWe simply call delete on the diskreader for each batch and internal readers are refreshed with deletes loaded in background.\n\nOK so a batch is quickly reopened, using bloom filter + int set for\nfast \"contains\" check for the deletions that occurred during that\nbatch (and, custom TermDocs that does the \"and not deleted\").  This\ngets you your fast turnaround and decent search performance.\n\nIn the BG the deletes are applied to Lucene \"for real\".\n\nThis is similar to Marvin's original tombstone deletions, in that the\ndeletions against old segments are stored with the new segment, rather\nthan aggressively pushed to the old segment's bit vectors. ",
            "author": "Michael McCandless",
            "id": "comment-12774688"
        },
        {
            "date": "2009-11-08T00:00:26+0000",
            "content": "Alas, I'm confused again.  If your reader searching a given batch is\nholding deletions against past segments, you can you make a TermDocs\nthat filters them?  (The TermDocs only enumerates the current batch's\ndocs).\n\nOr: do you make the IntSetAccelerator for each past segments that\nreceived deletions in the current batch? ",
            "author": "Michael McCandless",
            "id": "comment-12774690"
        },
        {
            "date": "2009-11-08T00:00:41+0000",
            "content": "Alas, I'm confused again.  If your reader searching a given batch is\nholding deletions against past segments, you can you make a TermDocs\nthat filters them?  (The TermDocs only enumerates the current batch's\ndocs).\n\nOr: do you make the IntSetAccelerator for each past segments that\nreceived deletions in the current batch? ",
            "author": "Michael McCandless",
            "id": "comment-12774691"
        },
        {
            "date": "2009-11-08T00:19:52+0000",
            "content": "But, I agree it's wasteful of space when deletes are so\nsparse... though it is fast.\n\nIt's fast for random access, but it's really slow if you need to make a lot of these (either during heavy indexing if copy-on-write, or during heavy query load if copy-on-reopen).\n\nSo are you using this, only, as your deleted docs? Ie you don't store\nthe deletions with Lucene? I'm getting confused if this is only for\nthe NRT case, or, in general.\n\nThese are only to augment the deleted docs of the disk reader - the disk reader isn't reopened at all except infrequently - once a batch (a big enough RAMDirectory is filled, or enough time goes by, depending on configuration) is ready to be flushed to disk, diskReader.addIndexes is called and when the diskReader is reopened, the deletes live in the normal diskReader's delete set.   Before this time is ready, when there is a batch in ram that hasn't been flushed, the IntSetAccelerator is applied to the not-reopened diskReader.  It's a copy-on-read ThreadLocal.  \n\nSo I'm not sure if that described it correctly: only the deletes which should have been applied to the diskReader are treated separately - those are basically batched: for T amount of time or D amount of docs (configurable) whichever comes first, they are applied to the diskReader, which knows about Lucene's regular deletions and now these new ones as well.   Once the memory is flushed to disk, the in-memory delSet is emptied, and applied to the diskReader using regular apis before reopening.\n\nOK, I think I'm catching up here... so you only open a new reader at\nthe batch boundary right? Ie, a batch update (all its adds & deletes)\nis atomic from the readers standpoint?\n\nYes - disk reader, you mean, right?  This is only reopened at batch boundary.\n\nOK so a batch is quickly reopened, using bloom filter + int set for\nfast \"contains\" check for the deletions that occurred during that\nbatch (and, custom TermDocs that does the \"and not deleted\"). This\ngets you your fast turnaround and decent search performance.\n\nThe reopening isn't that quick, but it's in the background, or are you talking about the RAMDirectory?  Yeah, that is reopened per query (if necessary - if there are no changes, of course no reopen), but it is kept very small (10k docs or less, for example).  It's actually pretty fantastic performance - check out the zoie perf pages: http://code.google.com/p/zoie/wiki/Performance_Comparisons_for_ZoieLucene24ZoieLucene29LuceneNRT \n ",
            "author": "Jake Mannix",
            "id": "comment-12774693"
        },
        {
            "date": "2009-11-08T08:46:05+0000",
            "content": "Michael:\n\n    I think I confused you by not giving you enough background information.\n\n    IntSetAccelerator holds UIDs, not docids. For each segment, the termdocs iterates its docid, we map to its corresponding uid and then check in the set.\n\n    Hopefully this clears it up.\n\n-John ",
            "author": "John Wang",
            "id": "comment-12774743"
        },
        {
            "date": "2009-11-09T13:18:25+0000",
            "content": "\nBut, I agree it's wasteful of space when deletes are so sparse... though it is fast.\n\nIt's fast for random access, but it's really slow if you need to make a lot of these (either during heavy indexing if copy-on-write, or during heavy query load if copy-on-reopen).\n\nBut how many msec does this clone add in practice?\n\nNote that it's only done if there is a new deletion against that\nsegment.\n\nI do agree it's silly wasteful, but searching should then be faster\nthan using AccelerateIntSet or MultiBitSet.  It's a tradeoff of the\nturnaround time for search perf.\n\nI'd love to see how the worst-case queries (matching millions of hits)\nperform with each of these three options.\n\nHowever, I suspect it's not the clone time that's very costly... I bet\nit's the fact that Lucene has to resolve the deletions to docIDs, in\nthe foreground of reopen, that dominates.  And probably also that\nLucene doesn't yet use RAMDir (but LUCENE-1313 is working towards\nfixing that).\n\nIe, Zoie is \"late binding\" (filters out UIDs as it encounters them\nduring searching), while Lucene is \"early binding\" (immediately\nresolves UIDs -> docIDs during reopen).  And because Zoie does the\n\"resolve deletions to docIDs\" in the BG, it's not on any query's\nexecution path.\n\nHow does Zoie resolve UID -> docID, now?  I remember a thread about\nthis a while back...\n\nActually one simple fix we could make to Lucene is to resolve\ndeletions in the foreground, when the deleteDocuments is called.\nThis'd mean it's the thread that does the updateDocument that pays the\nprice, rather than a future reopen.  Net/net it's a zero sum game\n(just distributed the cost from the reopen to the indexing), but it'd\nmean the reopen time is minimized, which is clearly the direction we\nwant to go in.  I'll open a new issue.\n\n\nSo are you using this, only, as your deleted docs? Ie you don't store the deletions with Lucene? I'm getting confused if this is only for the NRT case, or, in general.\n\nThese are only to augment the deleted docs of the disk reader - the disk reader isn't reopened at all except infrequently - once a batch (a big enough RAMDirectory is filled, or enough time goes by, depending on configuration) is ready to be flushed to disk, diskReader.addIndexes is called and when the diskReader is reopened, the deletes live in the normal diskReader's delete set. Before this time is ready, when there is a batch in ram that hasn't been flushed, the IntSetAccelerator is applied to the not-reopened diskReader.\n\nI think I now understand it (plus John's comment that these are Zoie's\nUIDs not Lucene's docIDs, helped)...\n\nWhen a doc needs to be updated, you index it immediately into the\nRAMDir, and reopen the RAMDir's IndexReader.  You add it's UID to the\nAcceleratedIntSet, and all searches \"and NOT\"'d against that set.  You\ndon't tell Lucene to delete the old doc, yet.\n\nPeriodically, in the BG, you use addIndexes to push the RAMDir to\ndisk, and, on a perhaps separate schedule, you resolve the deleted\nUIDs to docIDs and flush them to disk.\n\nOne question: does Zoie preserve Lucene's \"point in time\" searching?\nIs a new deletion immediately visible to all past reopened readers?  I\nthink for Lucene we need to preserve this, so we need a data structure\nthat can be \"efficiently\" transactional.  I guess we could consider\nallowing an NRT to optionally violate this, in which case we wouldn't\nneed to do any cloning of the deleted docs.\n\nIt's a copy-on-read ThreadLocal.\n\nHmm \u2013 why does each thread make a full clone of the\nAcceleratedBitSet?  Just for thread safety against additions to the\nset?  Or is this somehow preserving \"point in time\"?  And it fully\nre-clones whenever new updates have been committed to the RAMDir?\n\nIt's actually pretty fantastic performance - check out the zoie perf pages: http://code.google.com/p/zoie/wiki/Performance_Comparisons_for_ZoieLucene24ZoieLucene29LuceneNRT\n\nThese are great results!  If I'm reading them right, it looks like\ngenerally you get faster query throughput, and roughly equal indexing\nthroughput, on upgrading from 2.4 to 2.9?\n\nZoie also gets much better performance than raw Lucene NRT, but this\ntest focuses on reopen performance, I think?  Ie, a query reopens the\nreader if any new docs were indexed?  If you change that to, say,\nreopen once per N seconds, I wonder how the results would compare.\n\nOne optimization you could make with Zoie is, if a real-time deletion\n(from the AcceleratedIntSet) is in fact hit, it could mark the\ncorresponding docID, to make subsequent searches a bit faster (and\nsave the bg CPU when flushing the deletes to Lucene). ",
            "author": "Michael McCandless",
            "id": "comment-12774958"
        },
        {
            "date": "2009-11-09T19:04:04+0000",
            "content": " check out the zoie perf pages:\nhttp://code.google.com/p/zoie/wiki/Performance_Comparisons_for_Zo\nieLucene24ZoieLucene29LuceneNRT \n\nWhat's missing as it pertains to this jira issue is the raw\nquery speed (meaning the time a query takes to execute, not QPS,\nover various percentages of deleted docs), without concurrent\nindexing. \n\nIf query speed goes down, users need to know by how much.  ",
            "author": "Jason Rutherglen",
            "id": "comment-12775076"
        },
        {
            "date": "2009-11-09T19:13:06+0000",
            "content": "I'll try to get those numbers for you, they should be in our logs, and if not, it's easy enough to put them.    My guess it that if zoie is doing 7x the QPS, the latency is significantly less than the NRT latency, not more, but I could be wrong.\n\nNote that without concurrent indexing, the query speed will be the same, as all docs will be flushed to disk and the isDeleted check reduces to exactly the raw lucene case. ",
            "author": "Jake Mannix",
            "id": "comment-12775082"
        },
        {
            "date": "2009-11-09T19:47:16+0000",
            "content": "\nWhat's missing as it pertains to this jira issue is the raw\nquery speed (meaning the time a query takes to execute, not QPS,\nover various percentages of deleted docs), without concurrent\nindexing.\n\nI'd love to see this too.  Ie, this would measure the query performance when deletes are checked against AcceleratedIntSet, and wouldn't measure the reopen cost (which Lucene NRT will be slower at) at all.  Specifically I'd love to see the worst case queries.  It's those queries that drive the cutover to a shard'd architecture.\n\nIe, we need to separately measure query performance vs reopen performance. ",
            "author": "Michael McCandless",
            "id": "comment-12775095"
        },
        {
            "date": "2009-11-09T20:25:50+0000",
            "content": "OK I opened LUCENE-2047, to resolve deletedDoc(s) to their docID(s) in the foreground. ",
            "author": "Michael McCandless",
            "id": "comment-12775114"
        },
        {
            "date": "2009-11-09T21:18:34+0000",
            "content": "But how many msec does this clone add in practice?  Note that it's only done if there is a new deletion against that\nsegment.  I do agree it's silly wasteful, but searching should then be faster\nthan using AccelerateIntSet or MultiBitSet. It's a tradeoff of the\nturnaround time for search perf.\n\nI actually don't know for sure if this is the majority of the time, as I haven't actually run both the AcceleratedIntSet or 2.9 NRT through a profiler, but if you're indexing at high speed (which is what is done in our load/perf tests), you're going to be cloning these things hundreds of times per second (look at the indexing throughput we're forcing the system to go through), and even if it's fast, that's costly.\n\nI'd love to see how the worst-case queries (matching millions of hits)\nperform with each of these three options.\n\nIt's pretty easy to change the index and query files in our test to do that, that's a good idea.  You can feel free to check out our load testing framework too - it will let you monkey with various parameters, monitor the whole thing via JMX, and so forth, both for the full zoie-based stuff, and where the zoie api is wrapped purely around Lucene 2.9 NRT.   The instructions for how to set it up are on the zoie wiki.\n\nWhen a doc needs to be updated, you index it immediately into the\nRAMDir, and reopen the RAMDir's IndexReader. You add it's UID to the\nAcceleratedIntSet, and all searches \"and NOT\"'d against that set. You\ndon't tell Lucene to delete the old doc, yet.\n\nYep, basically.  The IntSetAccellerator (of UIDs) is set on the (long lived) IndexReader for the disk index - this is why it's done as a ThreadLocal - everybody is sharing that IndexReader, but different threads have different point-in-time views of how much of it has been deleted.\n\nThese are great results! If I'm reading them right, it looks like\ngenerally you get faster query throughput, and roughly equal indexing\nthroughput, on upgrading from 2.4 to 2.9?\n\nThat's about right.  Of course, the comparison between zoie with either 2.4 or 2.9 against lucene 2.9 NRT is an important one to look at: zoie is pushing about 7-9x better throughput for both queries and indexing than NRT.\n\nI'm sure the performance numbers would change if we allowed not realtimeness, yes, that's one of the many dimensions to consider in this (along with percentage of indexing events which are deletes, how many of those are from really old segments vs. newer ones, how big the queries are, etc...).\n\nOne optimization you could make with Zoie is, if a real-time deletion\n(from the AcceleratedIntSet) is in fact hit, it could mark the\ncorresponding docID, to make subsequent searches a bit faster (and\nsave the bg CPU when flushing the deletes to Lucene).\n\nThat sound interesting - how would that work?  We don't really touch the disk indexReader, other than to set this modSet on it in the ThreadLocal, where would this mark live? ",
            "author": "Jake Mannix",
            "id": "comment-12775151"
        },
        {
            "date": "2009-11-10T06:39:54+0000",
            "content": "I'd love to see how the worst-case queries (matching millions of hits) perform with each of these three options.\n\nI wrote a small program on my laptop, 100 docs in the set, iterates thru 5M numbers and calls contains().\nI see 44 ms with BitVector and 64ms with IntAccelerator backed by IntOpenHashSet (from fastUtil)\n\nThis is however an extreme case, so test 2, I chose 5000 docs from the set, e.g. mod 1000 to be a candidate for check. And both sets performed equally, around 45ms.\n\nSo with the memory cost, and the allocations and clones of the BitVector, I think for us at least, using the IntSetAccelerator works well.\n\nwhy does each thread make a full clone of the AcceleratedBitSet?\n\nThese are for updates, e.g. you updated doc x, it is updated to the ramdir, but it is already on the disk dir. So at query time, you need this set for dup removal.\n\nI'd love to see this too.\n\nSome more details on the test we ran:\n\nNRT - indexing only\n***********************************************************\nSUMMARY:\n***********************************************************\nTOTAL TRANSACTIONS: 622201\nTOTAL EXECUTIONS: 622201\nTOTAL SUCCESSFUL EXECUTIONS: 622201\nTOTAL FAILED EXECUTIONS: 0\nTOTAL RUNTIME IN MINS: 30.07\nINTERVAL FOR AVERAGE TIME CAPTURE IN MINS: 1\n***********************************************************\n\nzoie - indexing only\nSUMMARY:\n***********************************************************\nTOTAL TRANSACTIONS: 6265384\nTOTAL EXECUTIONS: 6265384\nTOTAL SUCCESSFUL EXECUTIONS: 6265384\nTOTAL FAILED EXECUTIONS: 0\nTOTAL RUNTIME IN MINS: 30.07\nINTERVAL FOR AVERAGE TIME CAPTURE IN MINS: 1\n***********************************************************\n\nzoie - update\nSUMMARY:\n***********************************************************\nTOTAL TRANSACTIONS: 1923592\nTOTAL EXECUTIONS: 1923592\nTOTAL SUCCESSFUL EXECUTIONS: 1923592\nTOTAL FAILED EXECUTIONS: 0\nTOTAL RUNTIME IN MINS: 30.07\nINTERVAL FOR AVERAGE TIME CAPTURE IN MINS: 1\n***********************************************************\n\nnrt - update\n\nSUMMARY:\n***********************************************************\nTOTAL TRANSACTIONS: 399893\nTOTAL EXECUTIONS: 399893\nTOTAL SUCCESSFUL EXECUTIONS: 399893\nTOTAL FAILED EXECUTIONS: 0\nTOTAL RUNTIME IN MINS: 30.07\nINTERVAL FOR AVERAGE TIME CAPTURE IN MINS: 1\n***********************************************************\n\nLatencies:\n\nZoie - insert test:  linear growth from 1 ms to 5 ms as index grows in the duration of the test from 0 docs to 660k docs.\nZoie - update test: averaged at 9ms, as index with continuous update and stayed in 1M docs\nNRT - insert test: fluctuated between 17 ms to 50 ms as index grows in the duration of the test from 0 docs to 220 docs.\nNRT - update test: big peak when query started, latency spiked up to 550ms and then dropped and stayed steadily at 50ms, with continuous updates to stay in 1M docs.\n\nSome observation at the NRT update test, I am seeing some delete issues, e.g. realtime deletes does not seem to reflect, and indexing speed sharply dropped.\n\nIt's quite possible that I am not using NRT the most optimal way in my setup. Feel free to run the tests yourself. I'd happy to help with the setup.\nOne thing with Zoie is that it is a full stream indexing system with a pluggable realtime engine, so you can actually use zoie for perf testing for NRT.\n\nOne thing about the test to stress, we are testing realtime updates, so buffered indexing events up and flush once it a while is not realtime, and katta has already achieved good results with batch indexing with just minutes of delay, without making any internal changes to lucene. ",
            "author": "John Wang",
            "id": "comment-12775319"
        },
        {
            "date": "2009-11-10T10:45:02+0000",
            "content": "Thanks for running these tests John.\n\nThe micro-benchmark of BitVector vs IntAccelerator is nice, but, we\nneed to see it in the real-world context of running actual worst case\nqueries.\n\nZoie aims for super fast reopon time, at the expense of slower query\ntime since it must double-check the deletions.\n\nLucene NRT makes the opposite tradeoff.\n\nThe tests so far make it clear that Zoie's reopen time is much faster\nthan Lucene's, but they don't yet measure (as far as I can see) what\ncost the double-check for deletions is adding to Zoie for the\nworst-case queries.\n\nSo if you really need to reopen 100s of times per second, and can\naccept that your worst case queries will run slower (we're still not\nsure just how much slower), the Zoie approach is best.\n\nIf you want full speed query performance, and can instead reopen once\nper second or once every 2 seconds, Lucene's approach will be better\n(though we still have important fixes to make \u2013 LUCENE-2047,\nLUCENE-1313).\n\nCan you describe the setup of the \"indexing only \"test?  Are you doing\nany reopening at all? ",
            "author": "Michael McCandless",
            "id": "comment-12775371"
        },
        {
            "date": "2009-11-10T10:48:09+0000",
            "content": "\nOne optimization you could make with Zoie is, if a real-time deletion (from the AcceleratedIntSet) is in fact hit, it could mark the corresponding docID, to make subsequent searches a bit faster (and save the bg CPU when flushing the deletes to Lucene).\n\nThat sound interesting - how would that work? We don't really touch the disk indexReader, other than to set this modSet on it in the ThreadLocal, where would this mark live?\n\nWhenever a query happens to \"discover\" a pending deletion, you could\nrecord somewhat the UID -> docID mapping, and then when it's time to\nflush the deletes you don't need to re-resolve the UIDs that the query\nhad resolved for you.  Likely in practice this would be a tiny\nspeedup, though, so the added complexity is probably not worth it.\nEspecially since this resolution is done in the BG for Zoie... ",
            "author": "Michael McCandless",
            "id": "comment-12775373"
        },
        {
            "date": "2009-11-10T16:29:02+0000",
            "content": "we need to see it in the real-world context of running actual worst case queries.\n\nIsn't checking every document in the corpus for deletes the worse case? e.g. first test?\n\nat the expense of slower query time\n\nAccording to the test, Zoie's query time is faster.\n\nit must double-check the deletions.\n\nTrue, this double-check is only done for a candidate for a hit from the underlying query. Normally result set is much smaller than the corpus, the overhead is not large. The overhead is 1 array lookup + a delset look up vs. 1 bitvector lookup. \n\nCan you describe the setup of the \"indexing only \"test?\n\nstarting off with an empty index and keep on adding documents, at the same time, for each search request, return a reader for the current state of the indexing. Our test assumes 10 concurrent threads making search calls. ",
            "author": "John Wang",
            "id": "comment-12775930"
        },
        {
            "date": "2009-11-10T20:50:03+0000",
            "content": "\nwe need to see it in the real-world context of running actual worst case queries.\n\nIsn't checking every document in the corpus for deletes the worse case? e.g. first test?\n\nZoie must do the IntSet check plus the BitVector check (done by\nLucene), right?\n\nIe comparing IntSet lookup vs BitVector lookup isn't the comparison\nyou want to do.  You should compare the IntSet lookup (Zoie's added\ncost) to 0.\n\nSo, for a query that hits 5M docs, Zoie will take 64 msec longer than\nLucene, due to the extra check.  What I'd like to know is what\npctg. slowdown that works out to be, eg for a simple TermQuery that\nhits those 5M results \u2013 that's Zoie's worst case search slowdown.\n\n\nat the expense of slower query time\n\nAccording to the test, Zoie's query time is faster.\n\nThe tests so far are really testing Zoie's reopen time vs Lucene's.\n\nTo test the query time, you should set up Zoie w/ some pending\ndeletions, then turn off all indexing, then run the query test.\n\nThat would give us both extreme datapoints \u2013 how much slower Lucene\nis at reopening (which the current tests show), and how much slower\nZoie is during searching.\n\n\nit must double-check the deletions.\n\nTrue, this double-check is only done for a candidate for a hit from the underlying query.\n\nRight, Zoie's search slowdown is in proportion to the size of the\nresult set.  So eg for hard queries that produce very few results, the\nimpact will be negligible.  For simple queries that produce lots of\nresults, the relative cost is highest (but we don't yet know what it\nactually is in practice).\n\nIt could still be neglible, eg since the \"if\" rarely triggers, the CPU\nshould be able to predict it just fine.\n\nNet/net, Zoie has faster reopen time than Lucene, but then pays a\nhigher price (double check for deletion) for every result of every\nsearch.  Users need to know what that price really is, in order to\nmake informed decision about which approach is best for their\nsituation.\n\nNormally result set is much smaller than the corpus, the overhead is not large.\n\nWell, this is generally app dependent, and it's the net/net worst case\nqueries that apps need to worry about.  Lucene can't [yet] take\navantage of concurrency within a single query, so you're forced to\nshard (= big step up in deployment complexity) once your worst case\nqueries get too slow.\n\n\nCan you describe the setup of the \"indexing only \"test?\n\nstarting off with an empty index and keep on adding documents, at the same time, for each search request, return a reader for the current state of the indexing. Our test assumes 10 concurrent threads making search calls.\n\nOh I see: that test is just adding documents, vs the 2nd test which is\ndoing updateDocument.  Got it.\n\nSo, that's interesting... because, with no deletions, thus no\nresolving of Term -> docID, and no cloning of the BitVector, Lucene's\nreopen is still quite a bit slower.\n\nWhat differences remain at this point?  Just the fact that the RAM dir\nis being used to flush the new [tiny] segments?  Hmm what about the\nmerge policy? ",
            "author": "Michael McCandless",
            "id": "comment-12776074"
        },
        {
            "date": "2009-11-10T21:28:49+0000",
            "content": "Zoie will take 64 msec longer than Lucene, due to the extra check.\n\nThat is not true. If you look at the report closely, it is 20ms difference, 64ms is the total size. (after I turned on -server, the diff is about 10ms). This is running on my laptop, hardly a production server.\n\nThis is also assuming the entire corpus is returned, where we should really take an average of the result set from the query log.\n\nHowever, to save this \"overhead\", using BitVector is wasting a lot of memory, which is expensive to clone, new and gc. In a running system, much of that cost is hard to measure. This is simply a question of trade-offs.\n\nAgain, I would suggest to run the tests yourself, afterall, it is open source  and make decisions for yourself, this way, we can get a better understanding from concrete numbers and scenarios.\n\nBTW, is there a performance benchmark/setup for lucene NRT?\n\nThe tests so far are really testing Zoie's reopen time vs Lucene's\n\nThat is not true either. This test is simply testing searching with indexing turned on. Not specific to re-open. I don't think the statement that the performance difference is solely due to reopen is substantiated. I am seeing the following with NRT:\n\ne.g. \n1) file handle leak - Our prod-quality machine fell over after 1 hr of running using NRT due to file handle leaking.\n2) cpu and memory starvation - monitoring cpu and memory usage, the machine seems very starved, and I think that leads to performance differences more than the extra array look.\n3) I am seeing also correctness issues as well, e.g. deletes don't get applied correctly. I am not sure about the unit test coverage for NRT to comment specifically.\n\nAgain, this can all be specific to my usage of NRT or the test setup. That is why I urge you guys to run our tests yourself and correct us if you see areas we are missing to make a fair comparison.\n\n ",
            "author": "John Wang",
            "id": "comment-12776085"
        },
        {
            "date": "2009-11-11T03:15:06+0000",
            "content": "The BitVector memory consumption is 125,000 bytes/122K for 1 million documents.  I'd be surprised if copying a byte array has a noticeable performance impact.   ",
            "author": "Jason Rutherglen",
            "id": "comment-12776254"
        },
        {
            "date": "2009-11-11T04:48:32+0000",
            "content": "But Jason, if you're indexing 300 documents a second (possibly all of which are delete+re-add), and querying at a thousand queries a second, how many of these BitVectors are you going to end up making? ",
            "author": "Jake Mannix",
            "id": "comment-12776277"
        },
        {
            "date": "2009-11-11T04:59:53+0000",
            "content": "wrote a little pgm on my mac pro (8 core 16GM mem beefy machine.\n100 threads mimic real load, each thread loops 100 times doing just a new on a BitVector for an index of numDocs (configurable) and take the average number.\n\n5M docs, 16 - 18 ms overhead\n10M docs, 35 - 40 ms overhead.\n\nThat is not insignificant. ",
            "author": "John Wang",
            "id": "comment-12776282"
        },
        {
            "date": "2009-11-11T05:48:18+0000",
            "content": "Zoie must do the IntSet check plus the BitVector check (done by\nLucene), right?\n\nYes, so how does Lucene NRT deal with new deletes?  The disk-backed IndexReader still does its internal check for deletions, right?  I haven't played with the latest patches on LUCENE-1313, so I'm not sure what has changed, but if you're leaving the disk index alone (to preserve point-in-time status of the index without writing to disk all the time), you've got your in-memory BitVector of newly uncommitted deletes, and then the SegmentReaders from the disk have their own internal deletedDocs BitVector.  Are these two OR'ed with each other somewhere?  What is done in NRT to minimize the time of checking both of these without modifying the read-only SegmentReader?  In the current 2.9.0 code, the segment is reloaded completely on getReader() if there are new add/deletes, right?\n\nIe comparing IntSet lookup vs BitVector lookup isn't the comparison\nyou want to do. You should compare the IntSet lookup (Zoie's added\ncost) to 0.\n\nIf you've got that technique for resolving new deletes against the disk-based ones while maintaining point-in-time nature and can completely amortize the reopen cost so that it doesn't affect performance, then yeah, that would be the comparison.  I'm not sure I understand how the NRT implementation is doing this currently - I tried to step through the debugger while running the TestIndexWriterReader test, but I'm still not quite sure what is going on during the reopen.\n\nSo, for a query that hits 5M docs, Zoie will take 64 msec longer than\nLucene, due to the extra check. What I'd like to know is what\npctg. slowdown that works out to be, eg for a simple TermQuery that\nhits those 5M results - that's Zoie's worst case search slowdown.\n\nYes, this is a good check to see, for while it is still a micro-benchmark, really, since it would be done in isolation, while no other production tasks are going on, like rapid indexing and the consequent flushes to disk and reader reopening is going on, but it would be useful to see.\n\nWhat would be even better, however, would be to have a running system whereby there is continual updating of the index, and many concurrent requests are coming in which hit all 5M documents, and measure the mean latency for zoie in this case, in both comparison to NRT, and in comparison to lucene when you don't reopen the index (ie. you do things the pre-lucene2.9 way, where the CPU is still being consumed by indexing, but the reader is out of date until the next time it's scheduled by the application to reopen).  This would measure the effective latency and throughtput costs of zoie and NRT vs non-NRT lucene.  I'm not really sure it's terribly helpful to see \"what is zoie's latency when you're not indexing at all\" - why on earth would you use either NRT or zoie if you're not doing lots of indexing?  ",
            "author": "Jake Mannix",
            "id": "comment-12776294"
        },
        {
            "date": "2009-11-11T05:51:43+0000",
            "content": "300 documents a second\n\nWhoa, pretty insane volume. \n\nhow many of these BitVectors are you going to end up making? \n\nA handful by pooling the BitVector fixed size bytes arrays (see\nLUCENE-1574). I'm not sure if the synchronization on the pool\nwill matter. If it does, we can use ConcurrentHashMap like\nSolr's LRUCache. Granted, JVMs are supposed to be able to handle\nrapid allocation efficiently, however I can't see the overhead\nof pooling being too significant. If it is, there's always the\ndefault of allocating new BVs. \n\nI really need a solution that will absolutely not affect query\nperformance from what is today. Personally, pooling is the\nsafest route for me to use in production as then, there are no\nworries about slowing down queries with alternative deleted docs\nmechanisms. And the memory allocation is kept within scope. The\noverhead is System.arraycopy, which no doubt will be\ninsignificant for my use case. \n\nhttp://java.sun.com/performance/reference/whitepapers/6_performance.html#2.1.5\nhttp://www.javapractices.com/topic/TopicAction.do?Id=3\n\nI suppose if one has fairly simple queries and is willing to\nsacrifice query performance for update rate, then other deleted\ndocs mechanisms may be a desired solution. I need to implement a more\nconservative approach. ",
            "author": "Jason Rutherglen",
            "id": "comment-12776296"
        },
        {
            "date": "2009-11-11T07:06:38+0000",
            "content": "Whoa, pretty insane volume. \n\nAiming for maxing out indexing speed and query throughput at the same time is what we're testing here, and this is a reasonable extreme limit to aim for when stress-testing real-time search.\n\nA handful by pooling the BitVector fixed size bytes arrays (see LUCENE-1574).\n\nPooling, you say?  But what if updates come in too fast to reuse your pool?  If you're indexing at the speeds I'm describing, won't you run out of BitVectors in the pool?\n\nI really need a solution that will absolutely not affect query performance from what is today\n\n\"You\" really need this?  Why is the core case for real-time search a scenario where taking a hit of a huge reduction in throughput worth a possible gain in query latency?  If the cost was 20% query latency drop in exchange for 7x throughput cost when doing heavy indexing, is that worth it?  What about 10% latency cost vs 2x throughput loss?  These questions aren't easily answered by saying real-time search with Lucene needs  to absolutely not affect query performance from what it is today.  These kinds of absolute statements should be backed up by comparisons with real performance and load testing.\n\nThere are many axes of performance to optimize for: \n\n\tquery latency\n\tquery throughput\n\tindexing throughput\n\tindex freshness (how fast before documents are visible)\n\n\n\nSaying that one of these is absolutely of more importance than the others without real metrics showing which ones are affected in which ways by different implementation choices is doing a disservice to the community, and is not by any means \"conservative\". ",
            "author": "Jake Mannix",
            "id": "comment-12776329"
        },
        {
            "date": "2009-11-11T10:56:59+0000",
            "content": "\n1) file handle leak - Our prod-quality machine fell over after 1 hr of running using NRT due to file handle leaking.\n2) cpu and memory starvation - monitoring cpu and memory usage, the machine seems very starved, and I think that leads to performance differences more than the extra array look.\n3) I am seeing also correctness issues as well, e.g. deletes don't get applied correctly. I am not sure about the unit test coverage for NRT to comment specifically.\n\nThese sound serious \u2013 if you can provide any details, that'd help.\nI'll do some stress testing too.  Thanks for testing and reporting \n\nYes, so how does Lucene NRT deal with new deletes?\n\nLucene NRT makes a clone of the BitVector for every reader that has\nnew deletions.  Once this is done, searching is \"normal\" \u2013 it's as if\nthe reader were a disk reader.  There's no extra checking of deleted\ndocs (unlike Zoie), no OR'ing of 2 BitVectors, etc.\n\nYes, this makes Lucene's reopen more costly.  But, then there's no\ndouble checking for deletions.  That's the tradeoff, and this is why\nthe 64 msec is added to Zoie's search time.  Zoie's searches are\nslower.\n\nThe fact that Zoie on the pure indexing case (ie no deletions) was 10X\nfaster than Lucene is very weird \u2013 that means something else is up,\nbesides how deletions are carried in RAM.  It's entirely possible it's\nthe fact that Lucene doesn't flush the tiny segments to a RAMDir\n(which LUCENE-1313 addresses).  Or, maybe there's another difference\nin that test (eg, MergePolicy?).  Jake or John, if you could shed some\nlight on any other specific differences in that test, that would help.\n\nThis is simply a question of trade-offs.\n\nPrecisely: Zoie has faster reopen time, but slower search time.  But\nwe haven't yet measured how much slower Zoie's searches are.\n\nActually I thought of a simple way to run the \"search only\" (not\nreopen) test \u2013 I'll just augment TopScoreDocCollector to optionally\ncheck the IntSetAccelerator, and measure the cost in practice, for\ndifferent numbers of docs added to the IntSet.\n\nBTW, is there a performance benchmark/setup for lucene NRT?\n\nIn progress \u2013 see LUCENE-2050.\n\nAiming for maxing out indexing speed and query throughput at the same time is what we're testing here, and this is a reasonable extreme limit to aim for when stress-testing real-time search.\n\nBut your test is missing a dimension: frequency of reopen.  If you\nreopen once per second, how do Zoie/Lucene compare?  Twice per second?\nOnce every 5 seconds?  Etc.\n\nIt sounds like LinkedIn has a hard requirement that the reopen must\nhappen hundreds of times per second, which is perfectly fine.  That's\nwhat LinkedIn needs.  But other apps have different requirements, and\nso to make an informed decision they need to see the full picture. ",
            "author": "Michael McCandless",
            "id": "comment-12776415"
        },
        {
            "date": "2009-11-11T14:26:14+0000",
            "content": "if you're indexing 300 documents a second (possibly all of which are delete+re-add), and querying at a thousand queries a second, how many of these BitVectors are you going to end up making?\n\nHopefully not much more than a few per second?\n\nWe should be careful what we measure to ensure that we're targeting the right use cases.  Seems like almost all apps should be well served by second reopen resolution on average (with the ability to immediately reopen on demand).  The only thing that would seem to need lower latency is when an automated client does an add, and then immediately does a query and needs to see it.  In that case, that client could specify that they need an immediate reopen (either during the add or the query).\n\nRequirements calling for zero latency updates (all index changes are always visible) are often in error (i.e. it's usually not a true requirement). ",
            "author": "Yonik Seeley",
            "id": "comment-12776464"
        },
        {
            "date": "2009-11-11T18:47:24+0000",
            "content": "These sound serious - if you can provide any details, that'd help.  I'll do some stress testing too. Thanks for testing and reporting \n\nOut of these, the specific issue of incorrectness of applied deletes is easiest to see - we saw it by indexing up to a million docs, then keep adding docs but only after doing a delete on the UID where UID instead of increasing, is looped around mod 1million.  Calling numDocs (not maxDoc) on the reader with Zoie always returns 1M after looping around, but with NRT, it starts slowly growing above 1M.\n\nThe CPU and memory is undoubtedly due to the constant reopening of these readers, and yes, could be aleiviated by not doing this - we're just comparing to the zoie case, where we do reopen (the RAMDir) on every request (and copy the delSet) if there have been modifications since the last update.\n\nLucene NRT makes a clone of the BitVector for every reader that has new deletions. Once this is done, searching is \"normal\" - it's as if the reader were a disk reader. There's no extra checking of deleted docs (unlike Zoie), no OR'ing of 2 BitVectors, etc.\n\nOk, so if this is copy-on-write, it's done every time there is a new delete for that segment?  If the disk index is optimized that means it would happen on every update, a clone of the full numDocs sized BitVector?  I'm still a little unsure of how this happens. \n\n\n\tsomebody calls getReader() - they've got all the SegmentReaders for the disk segments, and each of them have BitVectors for deletions.\n\tIW.update() gets called - the BitVector for the segment which now has a deletion is cloned, and set on a new pooled SegmentReader as its deletedSet\n\tmaybe IW.update() gets called a bunch more - do these modify the pooled but as-yet-unused SegmentReader?  New readers in the pool?  What?\n\tanother call to getReader() comes in, and gets an IndexReader wrapping the pooled SegmentReaders.\n\n\n\nYes, this makes Lucene's reopen more costly. But, then there's no double checking for deletions. That's the tradeoff, and this is why the 64 msec is added to Zoie's search time. Zoie's searches are slower.\n\nSo we re-ran some of our tests last night, commenting out our deleted check to measure it's cost in the most extreme case possible: a dead easy query (in that it's only one term), but one which yes, hits the entire index (doing a MatchAllDocs query is actually special-cased in our code, and is perfectly fast, so not a good worst case to check), and as the index grows up above a million documents, zoie could shave somewhere from 22-28% of its time off by not doing the extra check.  \n\nWe haven't re-run the test to see what happens as the index grows to 5M or 10M yet, but I can probably run that later today.\n\nThe fact that Zoie on the pure indexing case (ie no deletions) was 10X faster than Lucene is very weird - that means something else is up,\nbesides how deletions are carried in RAM. It's entirely possible it's the fact that Lucene doesn't flush the tiny segments to a RAMDir (which LUCENE-1313 addresses).\n\nYeah, if you call getReader() a bunch of times per second, each one does a flush(true,true,true), right?  Without having LUCENE-1313, this kills the indexing performance if querying is going on.  If no getReader() is being called at all, Zoie is about 10% slower than pure Lucene IndexWriter.add() (that's the cost of doing it in two steps - index into two RAMDirs [so they are hot-swappable] and then writing segments to disk with addIndexesNoOptimize() periodically).\n\nI don't think there's any difference in the MergePolicy - I think they're both using the BalancedMergePolicy (since that's the one which is optimized for the realtime case).\n\nActually I thought of a simple way to run the \"search only\" (not reopen) test - I'll just augment TopScoreDocCollector to optionally check the IntSetAccelerator, and measure the cost in practice, for different numbers of docs added to the IntSet.\n\nDue to the bloomfilter living on top of the hashSet, at least at the scales we're dealing with, we didn't see any change in cost due to the number of deletions (zoie by default keeps no more than 10k modifications in memory before flushing to disk, so the biggest the delSet is going to be is that, and we don't see the more-than-constant scaling yet at that size).\n\nBut your test is missing a dimension: frequency of reopen. If you reopen once per second, how do Zoie/Lucene compare? Twice per second? Once every 5 seconds? Etc.\n\nYep, this is true.  It's a little more invasive to put this into Zoie, because the reopen time is so fast that there's no pooling, so it would need to be kinda hacked in, or tacked on to the outside.  Not rocket science, but not just the change of a parameter.\n\nLinkedIn doesn't have any hard requirements of having to reopen hundreds of times per second, we're just stressing the system, to see what's going on.  As you can see, nobody's filing a bug here that Lucene NRT is \"broken\" because it can't handle zero-latency updates.  What we did try to make sure was in the system was determinism: not knowing whether an update will be seen because there is some background process doing addIndexes from another thread which hasn't completed, or not knowing how fresh the pooled reader is, that kind of thing.  \n\nThis kind of determinism can certainly be gotten with NRT, by locking down the IndexWriter wrapped up in another class to keep it from being monkeyed with by other threads, and then tuning exactly how often the reader is reopened, and then dictate to clients that the freshness is exactly at or better than this freshness timeout, sure.  This kind of user-friendliness is one of Zoie's main points - it provides an indexing system which manages all this, and certainly for some clients, we should add in the ability to pool the readers for less real-timeness, that's a good idea.\n\nOf course, if your reopen() time is pretty heavy (lots of FieldCache data / other custom faceting data needs to be loaded for a bunch of fields), then at least for us, even not needing zero-latency updates means that the more realistically 5-10% degredation in query performance for normal queries is negligable, and we get deterministic zero-latency updates as a consequence.\n\nThis whole discussion reminded me that there's another realtime update case, which neither Zoie nor NRT is properly optimized for: the absolutely zero deletes case with very fast indexing load and the desire for minimal latency of updates (imagine that you're indexing twitter - no changes, just adds), and you want to be able to provide a totally stream-oriented view on things as they're being added (matching some query, naturally) with sub-second turnaround.  A subclass of SegmentReader which is constructed which doesn't even have a deletedSet could be instantiated, and the deleted check could be removed entirely, speeding things up even further. ",
            "author": "Jake Mannix",
            "id": "comment-12776568"
        },
        {
            "date": "2009-11-11T23:00:38+0000",
            "content": "\nThese sound serious - if you can provide any details, that'd help. I'll do some stress testing too. Thanks for testing and reporting\n\nOut of these, the specific issue of incorrectness of applied deletes is easiest to see - we saw it by indexing up to a million docs, then keep adding docs but only after doing a delete on the UID where UID instead of increasing, is looped around mod 1million. Calling numDocs (not maxDoc) on the reader with Zoie always returns 1M after looping around, but with NRT, it starts slowly growing above 1M.\n\nSo far I've had no luck repro'ing this.  I have a 5M doc wikipedia\nindex.  Then I created an alg with 2 indexing threads (each replacing\ndocs at 100 docs/sec), and reopening ~ 60 times per second.  Another\nthread then verifies that the docCount is always 5M.  It's run fine\nfor quite a while now...\n\nHmm maybe I need to try the balanced merge policy?  That would be\nspooky if it caused the issue... ",
            "author": "Michael McCandless",
            "id": "comment-12776718"
        },
        {
            "date": "2009-11-11T23:36:51+0000",
            "content": "Correction: We are NOT using BalancedMergePolicy for NRT test, only default is used. BalancedMergePolicy is only used inside zoie. ",
            "author": "John Wang",
            "id": "comment-12776739"
        },
        {
            "date": "2009-11-12T03:46:47+0000",
            "content": "As far as the implementation of this patch goes... I'm thinking\nwe can increase the page size, and do a simulated setNextPage\ntype of deal in SegmentTermDocs. We'll start at the first page,\nif we hit an ArrayIndexOutOfBoundsException, we figure out what\npage they're trying to access and return true|false for the bit.\nWe can continue this process of accessing iteratively on a page,\nuntil we hit the AIOOBE, then figure out again. I think this is\na good approach because Java arrays already perform the access\nbounds check, hitting an exception hopefully shouldn't be too\ncostly if it only happens a handful of times per posting\niteration, and then we're avoiding the small but extra array\naccess lookup for eac bit. We'll be executing the same access pattern as\ntoday (i.e. random access on the byte array with about the same\noverhead, with the AIOOBE occurring when a page is completed).\nWe can benchmark and see the performance difference.\n\nI think in general, we'll simply benchmark the options we've\ndiscussed 1) NRT 2) 1313 3) 1313 + pooling 4) 1313 + 1526 5)\n1313 + 1526 w/iterative sequential page access. ",
            "author": "Jason Rutherglen",
            "id": "comment-12776823"
        },
        {
            "date": "2009-11-12T13:07:33+0000",
            "content": "Correction: We are NOT using BalancedMergePolicy for NRT test, only default is used. BalancedMergePolicy is only used inside zoie.\n\nOK.\n\nI've also tried doing separate delete then add in the indexer threads, and still I don't see any deletions getting lost... I can't repro this correctness issue. ",
            "author": "Michael McCandless",
            "id": "comment-12776984"
        },
        {
            "date": "2009-11-12T13:13:23+0000",
            "content": "2) cpu and memory starvation - monitoring cpu and memory usage, the machine seems very starved, and I think that leads to performance differences more than the extra array look.\n\nCPU starvation is fully expected (this is a redline test).\n\nMemory starvation is interesting, because the bit vectors should all\nbe transient, and should \"die young\" from the GC's standpoint.  Plus\nthese are all 1/8th the number of docs in RAM usage, and it's only\nthose segments that have deletions whose bit vector is cloned.  Are\nyou starting from an optimized index?\n\nOh, here's one idea: how many searches does your test allow to be\nin-flight at once?  (Or: how large a thread pool are you using on the\nserver?).  Since you effectively reopen per search, each search will\nhave dup'd the deleted docs.  If you allow many searches in flight,\nthat could explain it. ",
            "author": "Michael McCandless",
            "id": "comment-12776985"
        },
        {
            "date": "2009-11-12T13:24:37+0000",
            "content": "\nif you're indexing 300 documents a second (possibly all of which are delete+re-add), and querying at a thousand queries a second, how many of these BitVectors are you going to end up making?\n\nHopefully not much more than a few per second?\n\nWe should be careful what we measure to ensure that we're targeting the right use cases.\n\nRequirements calling for zero latency updates (all index changes are always visible) are often in error (i.e. it's usually not a true requirement).\n\nRight, I think testing reopening 100s of times per second isn't all\nthat useful (most apps don't really need to do this).\n\nI think seeing results broken out according to reopen frequency is\nmore helpful.\n\n\nSeems like almost all apps should be well served by second reopen resolution on average (with the ability to immediately reopen on demand). The only thing that would seem to need lower latency is when an automated client does an add, and then immediately does a query and needs to see it. In that case, that client could specify that they need an immediate reopen (either during the add or the query).\n\nTo prevent against accidental or intentional denial-of-service for\nclients that do the add + immediate query, one could also sync such\nclients up to the reopen frequency.\n\nThis would also provide for the clean semantics (like GData) of \"once\nthe 'update document' request returns, it's in the index\", which I\nagree is a very convenient API semantics.\n\nIe, if your reopen rate is 4x per second (once every 250 msec), then\nyou could hold all add requests coming in until the reopen completes,\nthen return those requests.\n\nSo the API can still build the well defined semantics on top of\nLucene, even if the reopen is rate limited under the hood. ",
            "author": "Michael McCandless",
            "id": "comment-12776990"
        },
        {
            "date": "2009-11-12T13:33:47+0000",
            "content": "\nLucene NRT makes a clone of the BitVector for every reader that has new deletions. Once this is done, searching is \"normal\" - it's as if the reader were a disk reader. There's no extra checking of deleted docs (unlike Zoie), no OR'ing of 2 BitVectors, etc.\n\nOk, so if this is copy-on-write, it's done every time there is a new delete for that segment? If the disk index is optimized that means it would happen on every update, a clone of the full numDocs sized BitVector? I'm still a little unsure of how this happens.\n\nRight.  Actually is the index optimized in your tests?  My current\ncorrectness testing (for the \"lost deletes\") isn't optimized... I'll\ntry optimizing it.\n\n\n\n\tsomebody calls getReader() - they've got all the SegmentReaders for the disk segments, and each of them have BitVectors for deletions.\n\tIW.update() gets called - the BitVector for the segment which now has a deletion is cloned, and set on a new pooled SegmentReader as its deletedSet\n\n\n\nActually, the IW.updateDocument call merely buffers the Term to be\ndeleted.  It does not resolve that term to the corresponding docID\nuntil the getReader (same as reopen) is called again.  But it would be\nbetter if Lucene did the resolution in the FG (during the\nupdateDocument) call; this is what LUCENE-2047 will fix.  This\nbackgrounds the resolution, ie, reopen is no longer resolving all\ndeletes in the FG.\n\nBut, yes, the clone happens on the first delete to arrive against a\nSegmentReader after it had been cloned in the NRT reader.\n\n* maybe IW.update() gets called a bunch more - do these modify the pooled but as-yet-unused SegmentReader? New readers in the pool? What?\n\nJust more buffering right now, but after LUCENE-2047, it will mark\nfurther bits in the already cloned vector.  Ie, the clone happens only\nafter getReader has returned a reader using that SegmentReader.\n\n* another call to getReader() comes in, and gets an IndexReader wrapping the pooled SegmentReaders.\n\nEach SegmentReader is cloned, and referenced by the reader returned by\ngetReader.  And then the next delete to arrive to thse segments will\nforce the bit vector to clone. ",
            "author": "Michael McCandless",
            "id": "comment-12776992"
        },
        {
            "date": "2009-11-12T13:40:46+0000",
            "content": "\n\nSo we re-ran some of our tests last night, commenting out our deleted check to measure it's cost in the most extreme case possible: a dead easy query (in that it's only one term), but one which yes, hits the entire index (doing a MatchAllDocs query is actually special-cased in our code, and is perfectly fast, so not a good worst case to check), and as the index grows up above a million documents, zoie could shave somewhere from 22-28% of its time off by not doing the extra check.\n\nOK, thanks for running that test...\n\nSo in the worst case (dead easy query, matching many many docs) Zoie's\nsearch slowdown is 22-28%.  It's presumably quite a bit less\n(approaching zero) for hard queries that match few docs.  So the\nsearch slowdown is app dependent.\n\nI think it'd be possible (though, complex!) to do a hybrid approach.\nMeaning you use Zoie to get the insanely fast reopen, but, to avoid\nthe search slowdown, in the background you convert the buffered UIDs\nto the docID bit vector, such that once all conversion is done, you\nstop checking the int set.\n\nI guess you'd have to throttle the conversion so that in the unnatural\n(100s per sec) reopen test, with many queries in flight at once, you\ndon't exhaust the heap. ",
            "author": "Michael McCandless",
            "id": "comment-12776994"
        },
        {
            "date": "2009-11-12T13:47:13+0000",
            "content": "\nThe fact that Zoie on the pure indexing case (ie no deletions) was 10X faster than Lucene is very weird - that means something else is up, besides how deletions are carried in RAM. It's entirely possible it's the fact that Lucene doesn't flush the tiny segments to a RAMDir (which LUCENE-1313 addresses).\n\nYeah, if you call getReader() a bunch of times per second, each one does a flush(true,true,true), right? Without having LUCENE-1313, this kills the indexing performance if querying is going on. If no getReader() is being called at all, Zoie is about 10% slower than pure Lucene IndexWriter.add() (that's the cost of doing it in two steps - index into two RAMDirs [so they are hot-swappable] and then writing segments to disk with addIndexesNoOptimize() periodically).\n\nIt'll be great if LUCENE-1313 nets us a 10X improvement in indexing\nrate.  With the improvements to benchmark (LUCENE-2050), I'm hoping\nthis'll be easy to confirm...\n\nAhh I see, so with very rare reopens, Zoie's indexing rate is also\nslower than Lucene's (because of the double buffering).\n\nSo the big picture tradeoff here is Zoie has wicked fast reopen times,\ncompared to Lucene, but has slightly slower (10%) indexing rate, and\nslower searches (22-28% in the worst case), as the tradeoff.\n\nIt seems like we need to find the \"break even\" point.  Ie, if you\nnever reopen, then on fixed hardware, Lucene is faster at indexing and\nsearching than Zoie.  If you reopen at an insane rate (100s per sec),\nZoie is much faster than Lucene on both indexing and searching.  But\nwhat if you reopen 2x, 1x per second?  Once every 2 seconds, etc.  At\nsome point the crossover will happen. ",
            "author": "Michael McCandless",
            "id": "comment-12776995"
        },
        {
            "date": "2009-11-12T15:47:44+0000",
            "content": "Due to the bloomfilter living on top of the hashSet, at least at the scales we're dealing with, we didn't see any change in cost due to the number of deletions (zoie by default keeps no more than 10k modifications in memory before flushing to disk, so the biggest the delSet is going to be is that, and we don't see the more-than-constant scaling yet at that size).\n\nBlooom filters are nice \n\n\nBut your test is missing a dimension: frequency of reopen. If you reopen once per second, how do Zoie/Lucene compare? Twice per second? Once every 5 seconds? Etc.\n\nYep, this is true. It's a little more invasive to put this into Zoie, because the reopen time is so fast that there's no pooling, so it would need to be kinda hacked in, or tacked on to the outside. Not rocket science, but not just the change of a parameter.\n\nOK.  It's clear Zoie's design is optimized for insanely fast reopen.\n\nLUCENE-2050 should make it easy to test this for pure Lucene NRT.\n\nLinkedIn doesn't have any hard requirements of having to reopen hundreds of times per second, we're just stressing the system, to see what's going on.\n\nRedline tests are very important, to understand how the system will\nbehave at extremes.\n\nBut I think it'd be useful to controll which dimension to redline.\n\nEG what I'd love to see is, as a function of reopen rate, the \"curve\"\nof QPS vs docs per sec.  Ie, if you reopen 1X per second, that\nconsumes some of your machine's resources.  What's left can be spent\nindexing or searching or both, so, it's a curve/line.  So we should\nset up fixed rate indexing, and then redline the QPS to see what's\npossible, and do this for multiple indexing rates, and for multiple\nreopen rates.\n\nThen this all becomes a capacity question for apps.\n\nAs you can see, nobody's filing a bug here that Lucene NRT is \"broken\" because it can't handle zero-latency updates.\n\nRight, Zoie is making determined tradeoffs.  I would expect that most\napps are fine with controlled reopen frequency, ie, they would choose\nto not lose indexing and searching performance if it means they can\n\"only\" reopen, eg, 2X per second.\n\n(Of course we will need to test, with LUCENE-2050, at what reopen\nfrequency you really eat into your indexing/searching performance,\ngiven fixed hardware).\n\n\nWhat we did try to make sure was in the system was determinism: not knowing whether an update will be seen because there is some background process doing addIndexes from another thread which hasn't completed, or not knowing how fresh the pooled reader is, that kind of thing.\n\nThis kind of determinism can certainly be gotten with NRT, by locking down the IndexWriter wrapped up in another class to keep it from being monkeyed with by other threads, and then tuning exactly how often the reader is reopened, and then dictate to clients that the freshness is exactly at or better than this freshness timeout, sure. This kind of user-friendliness is one of Zoie's main points - it provides an indexing system which manages all this, and certainly for some clients, we should add in the ability to pool the readers for less real-timeness, that's a good idea.\n\nI agree \u2013 having such well defined API semantics (\"once updateDoc\nreturns, searches can see it\") is wonderful.  But I think they can be\ncleanly built on top of Lucene NRT as it is today, with a\npre-determined (reopen) latency.\n\n\nOf course, if your reopen() time is pretty heavy (lots of FieldCache data / other custom faceting data needs to be loaded for a bunch of fields), then at least for us, even not needing zero-latency updates means that the more realistically 5-10% degredation in query performance for normal queries is negligable, and we get deterministic zero-latency updates as a consequence.\n\nI think the \"large merge just finished\" case is the most costly for\nsuch apps (which the \"merged segment warmer\" on IW should take care\nof)?  (Because otherwise the segments are tiny, assuming everything is\ncutover to \"per segment\").\n\n\nThis whole discussion reminded me that there's another realtime update case, which neither Zoie nor NRT is properly optimized for: the absolutely zero deletes case with very fast indexing load and the desire for minimal latency of updates (imagine that you're indexing twitter - no changes, just adds), and you want to be able to provide a totally stream-oriented view on things as they're being added (matching some query, naturally) with sub-second turnaround. A subclass of SegmentReader which is constructed which doesn't even have a deletedSet could be instantiated, and the deleted check could be removed entirely, speeding things up even further.\n\nI think Lucene could handle this well, if we made an IndexReader impl\nthat directly searches DocumentWriter's RAM buffer.  But that's\nsomewhat challenging  ",
            "author": "Michael McCandless",
            "id": "comment-12777025"
        },
        {
            "date": "2009-11-12T17:20:00+0000",
            "content": "OK. It's clear Zoie's design is optimized for insanely fast reopen.\n\nThat, and maxing out QPS and indexing rate while keeping query latency degredation to a minimum.  From trying to turn off the extra deleted check, the latency overhead on a 5M doc index is a difference of queries taking 12-13ms with the extra check turned on, and 10ms without it, and you only really start to notice on the extreme edges (the queries hitting all 5million docs by way of an actual query (not MatchAllDocs)), when your performance goes from maybe 100ms to 140-150ms.  \n\nEG what I'd love to see is, as a function of reopen rate, the \"curve\" of QPS vs docs per sec. Ie, if you reopen 1X per second, that consumes some of your machine's resources. What's left can be spent indexing or searching or both, so, it's a curve/line. So we should set up fixed rate indexing, and then redline the QPS to see what's possible, and do this for multiple indexing rates, and for multiple reopen rates.\n\nYes, that curve would be a very useful benchmark.  Now that I think of it, it wouldn't be too hard to just sneak some reader caching into the ZoieSystem with a tunable parameter for how long you hang onto it, so that we could see how much that can help.  One of the nice things that we can do in Zoie by using this kind of index-latency backoff, is that because we have an in-memory two-way mapping of zoie-specific UID to docId, if we actually have time (in the background, since we're caching these readers now) to zip through and update the real delete BitVectors on the segments, and lose the extra check at query time, only using that if you have the index-latency time set below some threshold (determined by how long it takes the system to do this resolution - mapping docId to UID is an array lookup, the reverse is a little slower).\n\nRight, Zoie is making determined tradeoffs. I would expect that most apps are fine with controlled reopen frequency, ie, they would choose to not lose indexing and searching performance if it means they can \"only\" reopen, eg, 2X per second.\n\nIn theory Zoie is making tradeoffs - in practice, at least against what is on trunk, Zoie's just going way faster in both indexing and querying in the redline perf test.  I agree that in principle, once LUCENE-1313 and other improvements and bugs have been worked out of NRT, that query performance should be faster, and if zoie's default BalancedMergePolicy (nee ZoieMergePolicy) is in use for NRT, the indexing performance should be faster too - it's just not quite there yet at this point.\n\nI agree - having such well defined API semantics (\"once updateDoc returns, searches can see it\") is wonderful. But I think they can be cleanly built on top of Lucene NRT as it is today, with a pre-determined (reopen) latency.\n\nOf course!  These api semantics are already built up on top of plain-old Lucene - even without NRT, so I can't imagine how NRT would remove this ability! \n\nI think the \"large merge just finished\" case is the most costly for such apps (which the \"merged segment warmer\" on IW should take care of)? (Because otherwise the segments are tiny, assuming everything is cutover to \"per segment\").\n\nDefinitely.  One thing that Zoie benefited from, from an API standpoint, which might be nice in Lucene, now that 1.5 is in place, is that the IndexReaderWarmer could replace the raw SegmentReader with a warmed user-specified subclass of SegmentReader:\n\n\npublic abstract class IndexReaderWarmer<R extends IndexReader> {\n  public abstract T warm(IndexReader rawReader);\n}\n\n\n\nWhich could replace the reader in the readerPool with the possibly-user-overridden subclass of SegmentReader (now that SegmentReader is as public as IndexReader itself is) which has now been warmed.  For users who like to decorate their readers to keep additional state, instead of use them as keys into WeakHashMaps kept separate, this could be extremely useful (I know that the people I talked to at Apple's iTunes store do this, as well as in bobo, and zoie, to name a few examples off the top of my head).\n\nI think Lucene could handle this well, if we made an IndexReader impl that directly searches DocumentWriter's RAM buffer. But that's somewhat challenging\n\nJason mentioned this approach in his talk at ApacheCon, but I'm not at all convinced it's necessary - if a single box can handle indexing a couple hundred smallish documents a second (into a RAMDirectory), and could be sped up by using multiple IndexWriters (writing into multiple RAMDirecotries in parallel, if you were willing to give up some CPU cores to indexing), and you can search against them without having to do any deduplification / bloomfilter check against the disk, then I'd be surprised if searching the pre-indexed RAM buffer would really be much of a speedup in comparison to just doing it the simple way.  But I could be wrong, as I'm not sure how much faster such a search could be. ",
            "author": "Jake Mannix",
            "id": "comment-12777068"
        },
        {
            "date": "2009-11-14T11:05:14+0000",
            "content": "One of the nice things that we can do in Zoie by using this kind of index-latency backoff, is that because we have an in-memory two-way mapping of zoie-specific UID to docId, if we actually have time (in the background, since we're caching these readers now) to zip through and update the real delete BitVectors on the segments, and lose the extra check at query time, only using that if you have the index-latency time set below some threshold (determined by how long it takes the system to do this resolution - mapping docId to UID is an array lookup, the reverse is a little slower).\n\nRight \u2013 I think such a hybrid approach would have the best tradeoffs\nof all.  You'd get insanely fast reopen, and then searching would only\ntake the performance hit until the BG resolution of deleted UID ->\nLucene docID completed.  Similar to the JRE's BG hotspot compiler.\n\n\nRight, Zoie is making determined tradeoffs. I would expect that most apps are fine with controlled reopen frequency, ie, they would choose to not lose indexing and searching performance if it means they can \"only\" reopen, eg, 2X per second.\n\nIn theory Zoie is making tradeoffs - in practice, at least against what is on trunk, Zoie's just going way faster in both indexing and querying in the redline perf test. I agree that in principle, once LUCENE-1313 and other improvements and bugs have been worked out of NRT, that query performance should be faster, and if zoie's default BalancedMergePolicy (nee ZoieMergePolicy) is in use for NRT, the indexing performance should be faster too - it's just not quite there yet at this point.\n\nWell.. unfortunately, we can't conclude much from the current test,\nbesides that Zoie's reopen time is much faster than Lucene's (until/if\nwe add the \"reopen frequency\" as a dimension, and see those results).\n\nAlso the test is rather synthetic, in that most apps don't really need\nto reopen 100s of times per second.  We really should try to test more\nrealistic cases.\n\nOne question: where is CPU utilization when you run the Lucene test?\nPresumably, if you block an incoming query until the reopen completes,\nand because only one reopen can happen at once, it seems like CPU must\nnot be saturated?\n\nBut, I agree, there are alot of moving parts here still \u2013 Zoie has\nfar faster add-only throughput than Lucene (could simply be due to\nlack of LUCENE-1313), Lucene may have correctness issue (still can't\nrepro), Lucene has some pending optimizations (LUCENE-2047), etc.\n\nIn LUCENE-2061 I'm working on a standard benchmark we can use to test\nimprovements to Lucene's NRT; it'll let us assess potential\nimprovements and spot weird problems.\n\n\nOne thing that Zoie benefited from, from an API standpoint, which might be nice in Lucene, now that 1.5 is in place, is that the IndexReaderWarmer could replace the raw SegmentReader with a warmed user-specified subclass of SegmentReader:\n\n\n \npublic abstract class IndexReaderWarmer<R extends IndexReader> {\n  public abstract T warm(IndexReader rawReader);\n}\n\n \nWhich could replace the reader in the readerPool with the possibly-user-overridden subclass of SegmentReader (now that SegmentReader is as public as IndexReader itself is) which has now been warmed. For users who like to decorate their readers to keep additional state, instead of use them as keys into WeakHashMaps kept separate, this could be extremely useful (I know that the people I talked to at Apple's iTunes store do this, as well as in bobo, and zoie, to name a few examples off the top of my head).\n\nThis is a good idea, and it's been suggested several times now,\nincluding eg notification when segment merging starts/commits, but I\nthink we should take it up in the larger context of how to centralize\nreader pooling?  This pool is just the pool used by IndexWriter, when\nits in NRT mode; I think IndexReader.open should somehow share the\nsame infrastructure.  And maybe LUCENE-2026 (refactoring IW) is the\nvehicle for \"centralizing\" this?  Can you go carry over this\nsuggestion there?\n\n\nI think Lucene could handle this well, if we made an IndexReader impl that directly searches DocumentWriter's RAM buffer. But that's somewhat challenging\n\nJason mentioned this approach in his talk at ApacheCon, but I'm not at all convinced it's necessary - if a single box can handle indexing a couple hundred smallish documents a second (into a RAMDirectory), and could be sped up by using multiple IndexWriters (writing into multiple RAMDirecotries in parallel, if you were willing to give up some CPU cores to indexing), and you can search against them without having to do any deduplification / bloomfilter check against the disk, then I'd be surprised if searching the pre-indexed RAM buffer would really be much of a speedup in comparison to just doing it the simple way. But I could be wrong, as I'm not sure how much faster such a search could be.\n\nRight, we should clearly only take such a big step if performance\nshows it's justified.  From the initial results I just posted in\nLUCENE-2061, it looks like Lucene does in fact handle the add-only\ncase very well (ie degredation to QPS is fairly contained), even into\nan FSDir.  I need to restest with LUCENE-1313. ",
            "author": "Michael McCandless",
            "id": "comment-12777881"
        },
        {
            "date": "2009-11-14T11:17:14+0000",
            "content": "We should test the performance tradeoffs incurred by switching to\ntransactional data structure (like the proposed paged bit vector),\nbut... my inclination at this point would be it's not a good tradeoff\nfor Lucene NRT to make.\n\nIe, it'd be making the same tradeoff Zoie now makes \u2013 faster reopen\ntime for slower searching, which I don't think makes sense for most\napps. ",
            "author": "Michael McCandless",
            "id": "comment-12777890"
        },
        {
            "date": "2009-11-17T03:04:52+0000",
            "content": "Here's a working version of this. The page size is statically\nconfigurable by adjusting CONST in PagedBitVector. I set it on\nthe high side because the next thing is to inline the page and\ndoc checking into SegmentTermDocs for benchmarking.\n\nThe test is fairly randomized, though I think there's more that\ncan be added.\n\nThe pages are saved one by one, either as dgaps or bytes, which\nmeans the .del file format has changed. We can probably read the\nold format, write the new format if this is deployed.  ",
            "author": "Jason Rutherglen",
            "id": "comment-12778699"
        },
        {
            "date": "2009-11-17T06:40:43+0000",
            "content": "Inlined into SegmentTermDocs. If there's an issue with the del\ndocs null check we could go extreme and instantiate specialized\ninstances of SegTD that doesn't perform the check. I'm not sure\nwhat would slow this down, but profiling will lets us know\nwhat's up.\n\nTestIndexReaderReopen and TestIndexWriterReader passes so I\nfigure we're ready for benchmarking. ",
            "author": "Jason Rutherglen",
            "id": "comment-12778743"
        },
        {
            "date": "2009-12-05T13:31:20+0000",
            "content": "Jake, have you guys had a chance to re-run your tests across varying\nreopen rates?  Are you still hitting OOM / file handle leaks with\nstraight Lucene NRT?  I've been unable to reproduce these issues in\nmy stress testing.... so I'd like to hone in on what's different in our\ntesting. ",
            "author": "Michael McCandless",
            "id": "comment-12786404"
        },
        {
            "date": "2009-12-05T14:03:17+0000",
            "content": "Yes, we still see the issue. The performance/stress test after 20+ min of run, latency spiked from 5ms to 550ms and file handle leakage was severe enough that the test crashed. This is the code:\n\nhttp://code.google.com/p/zoie/source/browse/branches/BR_DELETE_OPT/java/proj/zoie/impl/indexing/luceneNRT/ThrottledLuceneNRTDataConsumer.java\n\nOur logging indicates there is at most 3 index readers instances at open state. Yet the file handle count is very high. ",
            "author": "John Wang",
            "id": "comment-12786408"
        },
        {
            "date": "2009-12-06T09:58:22+0000",
            "content": "Yes, we still see the issue\n\nOK I'll open a separate issue to try to get to the bottom of this... ",
            "author": "Michael McCandless",
            "id": "comment-12786590"
        },
        {
            "date": "2009-12-06T10:01:29+0000",
            "content": "OK I spunoff LUCENE-2120. ",
            "author": "Michael McCandless",
            "id": "comment-12786591"
        },
        {
            "date": "2009-12-06T10:03:18+0000",
            "content": "John, what about memory exhaustion?  Are you still hitting that as well? ",
            "author": "Michael McCandless",
            "id": "comment-12786592"
        },
        {
            "date": "2011-01-24T21:16:51+0000",
            "content": "Won't be working on these and they're old ",
            "author": "Jason Rutherglen",
            "id": "comment-12986029"
        }
    ]
}