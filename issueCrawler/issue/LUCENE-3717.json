{
    "id": "LUCENE-3717",
    "title": "Add fake charfilter to BaseTokenStreamTestCase to find offsets bugs",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Task",
        "fix_versions": [
            "3.6",
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "Recently lots of issues have been fixed about broken offsets, but it would be nice to improve the\ntest coverage and test that they work across the board (especially with charfilters).\n\nin BaseTokenStreamTestCase.checkRandomData, we can sometimes pass the analyzer a reader wrapped\nin a \"MockCharFilter\" (the one in the patch sometimes doubles characters). If the analyzer does\nnot call correctOffsets or does incorrect \"offset math\" (LUCENE-3642, etc) then eventually\nthis will create offsets and the test will fail.\n\nOther than tests bugs, this found 2 real bugs: ICUTokenizer did not call correctOffset() in its end(),\nand ThaiWordFilter did incorrect offset math.",
    "attachments": {
        "LUCENE-3717_more.patch": "https://issues.apache.org/jira/secure/attachment/12511473/LUCENE-3717_more.patch",
        "LUCENE-3717_ngram.patch": "https://issues.apache.org/jira/secure/attachment/12511656/LUCENE-3717_ngram.patch",
        "LUCENE-3717.patch": "https://issues.apache.org/jira/secure/attachment/12511456/LUCENE-3717.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2012-01-23T00:26:21+0000",
            "content": "I committed this. I will go thru the analyzers and try to make sure they are all using checkRandomData (i think most are), just to see if we have any other bugs sitting out there.\n\nIt would be nice to have these offsets all under control for the next release. ",
            "author": "Robert Muir",
            "id": "comment-13190835"
        },
        {
            "date": "2012-01-23T03:52:51+0000",
            "content": "i started adding checkRandomData to more analyzers, and found 5 bugs already:\n\n\tbroken offsets in TrimFilter, WordDelimiterFilter along the same lines here\n\tHyphenatedWordsFilter was broken worse: if the text ends with a hyphen the last token had end offset of 0 always (because it read uninitialized attributes)\n\tPatternAnalyzer completely broken with charfilters.\n\tWikipediaTokenizer broken in many ways, in general the tokenizer keeps a ton of state variables, but never resets this state.\n\n\n\npatch fixes these but I'm sure adding more tests to the remaining filters will find more bugs. ",
            "author": "Robert Muir",
            "id": "comment-13190860"
        },
        {
            "date": "2012-01-23T04:14:35+0000",
            "content": "reopening since we have more work to do / more bugs.\n\nI'll look at committing/backporting the current patch as a start but i think we should check every tokenizer/filter/etc and just clean this up. ",
            "author": "Robert Muir",
            "id": "comment-13190863"
        },
        {
            "date": "2012-01-23T21:54:20+0000",
            "content": "second patch is committed+backported.\n\njust remains to add the random test to all remaining tokenstreams... ",
            "author": "Robert Muir",
            "id": "comment-13191515"
        },
        {
            "date": "2012-01-24T09:48:36+0000",
            "content": "more bugs in the n-gram tokenizers. they:\n\n\twere wrongly computing end() from the trimmed length\n\tnot calling correctOffset\n\tnot checking return value of Reader.read causing bugs in some situations (e.g. empty stringreader)\n\n ",
            "author": "Robert Muir",
            "id": "comment-13192042"
        }
    ]
}