{
    "id": "LUCENE-1350",
    "title": "Filters which are \"consumers\" should not reset the payload or flags and should better reuse the token",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "modules/analysis",
            "modules/other"
        ],
        "type": "Bug",
        "fix_versions": [
            "2.4"
        ],
        "affect_versions": "None",
        "resolution": "Duplicate",
        "status": "Closed"
    },
    "description": "Passing tokens with payloads through SnowballFilter results in tokens with no payloads.\nA workaround for this is to apply stemming first and only then run whatever logic creates the payload, but this is not always convenient.\n\nOther \"consumer\" filters have similar problem.\n\nThese filters can - and should - reuse the token, by implementing next(Token), effectively also fixing the unwanted resetting.",
    "attachments": {
        "LUCENE-1350.patch": "https://issues.apache.org/jira/secure/attachment/12387545/LUCENE-1350.patch",
        "LUCENE-1350-test.patch": "https://issues.apache.org/jira/secure/attachment/12387937/LUCENE-1350-test.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2008-08-05T08:40:50+0000",
            "content": "Patch fix this by using Token.clone().\nI'll search other filters that might reset payloads - but if you are aware of any that would be useful to know. ",
            "author": "Doron Cohen",
            "id": "comment-12619813"
        },
        {
            "date": "2008-08-05T15:17:19+0000",
            "content": "When we go to the reuse pattern across all of Lucene, the problem will be nearly everywhere.\n\nThe pattern for Token after deprecations is removed is:\npublic Token next(Token token) {\n...\ntoken.clear(); // This clears Payload\ntoken.setTermBuffer(newBuffer);\n...\n}\nIn https://issues.apache.org/jira/browse/LUCENE-1333, I've changed snowballs next(Token token) to be this pattern.\n\nUsing clone is probably not the best.\nThe following pattern works:\npublic Token next(Token token) {\n...\nPayload payload = token.getPayload();\ntoken.clear(); // This clears Payload\ntoken.setTermBuffer(newBuffer);\ntoken.setPayload(payload);\n...\n}\n\nIf payload is to be preserved in the face of the reuse pattern, perhaps clear() should not clear Payload. Since Payload is experimental and marked as subject to change, I don't think that this break of backward compatibility should be an issue. If it is, I think there is a better pattern for Token.\n\nThe filter order issue concerning payload also pertains to the flags field, which is also marked experimental, and I also think it pertains to type.\n\nThe most typical pattern of Token reuse is:\ntoken.clear(); // reset everything except startOffset, endOffset and type to their defaults.\ntoken.setStartOffset(newStartOffset);\ntoken.setEndOffset(newEndOffset);\ntoken.setType(Token.DEFAULT_TYPE);\ntoken.setTermBuffer(newTerm); // or some variation of this.\n\nThis is rather tedious and I think clear is a bit to agressive with setting payload and flags to their default. I think it would be good to add to Token the following and deprecate clear():\npublic void reuse(char[] buffer, int offset, int length, int startOffset, int endOffset, String type)\n{\n  setTermBuffer(buffer, offset, length);\n  this.positionIncrement = 1;\n  this.startOffset = startOffset;\n  this.endOffset = endOffset;\n  this.type = type;\n}\n\npublic void reuse(String buffer, int offset, int length, int startOffset, int endOffset, String type)\n{\n  setTermBuffer(buffer, offset, length);\n  this.positionIncrement = 1;\n  this.startOffset = startOffset;\n  this.endOffset = endOffset;\n  this.type = type;\n}\n\npublic void reuse(String buffer, int startOffset, int endOffset, String type)\n{\n  setTermBuffer(buffer);\n  this.positionIncrement = 1;\n  this.startOffset = startOffset;\n  this.endOffset = endOffset;\n  this.type = type;\n}\n\npublic void reuse(char[] buffer, int offset, int length, int startOffset, int endOffset)\n{\n  setTermBuffer(buffer, offset, length);\n  this.positionIncrement = 1;\n  this.startOffset = startOffset;\n  this.endOffset = endOffset;\n}\n\npublic void reuse(String buffer, int offset, int length, int startOffset, int endOffset)\n{\n  setTermBuffer(buffer, offset, length);\n  this.positionIncrement = 1;\n  this.startOffset = startOffset;\n  this.endOffset = endOffset;\n}\n\npublic void reuse(String buffer, int startOffset, int endOffset)\n{\n  setTermBuffer(buffer);\n  this.positionIncrement = 1;\n  this.startOffset = startOffset;\n  this.endOffset = endOffset;\n} ",
            "author": "DM Smith",
            "id": "comment-12619925"
        },
        {
            "date": "2008-08-05T16:15:37+0000",
            "content": "These are interesting points to consider. \n\nLet's look at the two cases reuse and non-reuse.\n\nThe current SnowballFilter implements the non-reuse method next(). It actually clones,  just without using clone(). So I still think that fixing it to use clone is acceptable. Hope you agree with this.\n\n(Btw, I checked LUCENE-1142 (\"Updated Snowball package\") - there too the non-reuse version is used.)\n\nFor the reuse case, i.e. next(Token), there is a distinction between consumers and producers (in TokenStream API) - only consumers invoke clear(), and then set everything. Filters are consumers, so clear() is not in place. \n\n(As a side comment, I think an explicit call like setEndOffset() is somewhat clearer than a method with 3 int args.) ",
            "author": "Doron Cohen",
            "id": "comment-12619942"
        },
        {
            "date": "2008-08-05T17:14:42+0000",
            "content": "The non-reuse interface is deprecated. LUCENE-1333 deals with cleaning that up and applying reuse in all of Lucene. To date, it was partially applied to core. This results in sub-optimal performance with Filter chains that use both reuse and non-reuse inputs and filters.\n\nSo LUCENE-1333 updates SnowballFilter to use next(Token).\n\nThe documentation in TokenStream documents that only producers invoke clear().\n\nTo me, it is not clearcut what a producer or a consumer actually is. Obviously, input streams are producers. Some filters, generate multiple tokens as a replacement for the current one (e.g. NGram, stemming,...). To me, these are producers.\n\nIf the rule of thumb is that Filters are consumers, merely changing their token's term, then there are lot's of places that need to be changed. I noticed that SnowballFilter's methodology was fairly common:\nToken token = input.next();\n...\nString newTerm = ....;\n...\nreturn new Token(newTerm, token.startOffset(), token.endOffset(), token.type());\n\nIn migrating this to the reuse pattern, I saw new Token(...) as a producer pattern and to maintain the equivalent behavior clear() needed to be called:\npublic Token next(Token token)\n{\ntoken = input.next(token);\n...\nString newTerm = ....;\n...\ntoken.clear(); // do most of the initialization that new Token does\ntoken.setTermBuffer(newTerm); // new method introduced in LUCENE-1333\nreturn token;\n}\n\nI don't know why the following pattern was not originally used (some filters do this) or why you didn't migrate to this:\nToken token = input.next();\n...\nString newTerm = ....;\n...\ntoken.setTermText(newTerm);\nreturn token;\n\nThis would be faster than cloning and would preserve all fields.\n ",
            "author": "DM Smith",
            "id": "comment-12619961"
        },
        {
            "date": "2008-08-05T18:17:35+0000",
            "content": "Survey of other potential problem areas (I did not look at test cases):\no.a.l.index.memory.SynonymTokenFilter (generated synonyms do not propagate payload or flags)\no.a.l.analysis.de.GermanStemFilter (creates a replacement token and does not propagate payload or flags)\no.a.l.analysis.ngram.EdgeNGramTokenFilter (generated ngrams do not propagate payload or flags; positionIncrement is inappropriate)\no.a.l.analysis.ngram.NGramTokenFilter (generated ngrams do not propagate payload or flags; positionIncrement is inappropriate)\no.a.l.analysis.br.BrazilianStemFilter (creates a replacement token and does not propagate payload or flags)\no.a.l.analysis.fr.ElisionFilter (creates a replacement token and does not propagate payload or flags)\no.a.l.analysis.fr.FrenchStemFilter (creates a replacement token and does not propagate payload or flags)\no.a.l.analysis.shingle.ShingleFilter (generated shingles do not propagate payload or flags)\no.a.l.analysis.ru.RussianLowerCaseFilter (creates a replacement token and does not propagate payload or flags)\no.a.l.analysis.ru.RussianStemFilter (creates a replacement token and does not propagate payload or flags)\no.a.l.analysis.el.GreekLowerCaseFilter (creates a replacement token and does not propagate payload or flags)\no.a.l.analysis.compound.CompoundWordTokenFilterBase  (generated parts do not propagate payload or flags)\no.a.l.analysis.th.ThaiWordFilter (creates a replacement for non-Thai words; generated parts do not propagate payload or flags)\no.a.l.analysis.nl.DutchStemFilter (creates a replacement token and does not propagate payload or flags) ",
            "author": "DM Smith",
            "id": "comment-12619982"
        },
        {
            "date": "2008-08-06T20:06:20+0000",
            "content": "\nThe non-reuse interface is deprecated. LUCENE-1333 deals with cleaning that up and applying reuse in all of Lucene. To date, it was partially applied to core. This results in sub-optimal performance with Filter chains that use both reuse and non-reuse inputs and filters.\n\nNon-reuse TokenStream API is not deprecated in the trunk. I guess you mean it will be deprecated by LUCENE-1333.\n\n\nTo me, it is not clearcut what a producer or a consumer actually is. Obviously, input streams are producers. Some filters, generate multiple tokens as a replacement for the current one (e.g. NGram, stemming,...). To me, these are producers.\n\nRight, such filters function as producers.  Javadocs should say something weaker, like \"most filters are consumers\" or \"filters are usually consumers\". \n\n\nI don't know why the following pattern was not originally used (some filters do this) or why you didn't migrate to this:\nToken token = input.next();\n...\nString newTerm = ....;\n...\ntoken.setTermText(newTerm);\nreturn token;\n\nThis would be faster than cloning and would preserve all fields.\n\nGood point, thanks. \n\nSo I wonder what's next with this issue. The complete LUCENE-1333 is dated for 2.4. So it seems in place to fix filters behavior now, to preserve payload (and flags, thanks for pointing this out), following the above (reuse) code pattern. Makes sense? ",
            "author": "Doron Cohen",
            "id": "comment-12620408"
        },
        {
            "date": "2008-08-06T21:41:03+0000",
            "content": "\nNon-reuse TokenStream API is not deprecated in the trunk. I guess you mean it will be deprecated by LUCENE-1333.\n\nMy argument for actually deprecating it in LUCENE-1333 was that it was implicitly deprecated already.\n\n\nSo I wonder what's next with this issue. The complete LUCENE-1333 is dated for 2.4. So it seems in place to fix filters behavior now, to preserve payload (and flags, thanks for pointing this out), following the above (reuse) code pattern. Makes sense?\n\nYes. This issue is a bug fix and if there is a bug release, this should go with it.\n\nI think that you should expand this issue to fix all the other TokenFilters that have the same problem. And to also propagate the flags as well. As you found out a fix is trivial. If you want, I'd be happy to work up a patch for it.\n\nRegarding LUCENE-1333:\nBTW, it can go before 2.4. I just set it to 2.4 since that was the next non-bug release.\n\nIt will be easy for me to adjust it after this goes in.\n\nI'll change the JavaDoc to be clearer as to when clear() should be called, what is a producer and what is a consumer, as part of it since that is part of it's scope. ",
            "author": "DM Smith",
            "id": "comment-12620435"
        },
        {
            "date": "2008-08-07T06:57:40+0000",
            "content": "\nI think that you should expand this issue to fix all the other TokenFilters that have the same problem. And to also propagate the flags as well. \n\nAgree. I will rename the issue accordingly.\n\n\nAs you found out a fix is trivial. If you want, I'd be happy to work up a patch for it.\n\nThis would be great, thanks!  (would it fit in a single patch file, please? ) ",
            "author": "Doron Cohen",
            "id": "comment-12620521"
        },
        {
            "date": "2008-08-08T16:50:54+0000",
            "content": "\nIt seems like there are three different things, here:\n\n\n\tMany filters (eg SnowballFilter) incorrectly erase the Payload,\n    token Type and token flags, because they are basically doing\n    their own Token cloning.  This is pre-existing (before re-use API\n    was created).\n\tSeparately, these filters do not use the re-use API, which we are\n    wanting to migrate to anyway.\n\tAdding new \"reuse\" methods on Token which are like clear() except\n    they also take args to replace the termBuffer, start/end offset,\n    etc, and they do not clear the payload/flags to their defaults.\n\n\n\nSince in LUCENE-1333 we are aggressively moving all Lucene core &\ncontrib TokenStream & TokenFilters to use the re-use API (formally\ndeprecating the original non-reuse API), we may as well fix 1 & 2 at\nonce.\n\nI think the reuse API proposal is reasonable: it mirrors the current\nconstructors on Token.  But, since we are migrating to reuse api, you\nneed the analog (of all these constructors) without making a new\nToken.\n\nBut maybe change the name from \"reuse\" to maybe \"update\", \"set\",\n\"reset\", \"reinit\", or \"change\"?  But: I think this method should still\nreset payload, position incr, etc, to defaults?  Ie calling this\nmethod should get you the same result as creating a new Token(...)\npassing in the termBuffer, start/end offset, etc, I think?\n\nShould we just absorb this issue into LUCENE-1333?  DM, of your list\nabove (of filters that lose payload), are there any that are not fixed\nin LUCENE-1333?  I'm confused on the overlap and it's hard to work\nwith all the patches.  Actually if in LUCENE-1333 you could\nconsolidate down to a single patch (big toplevel \"svn diff\"), that'd\nbe great  ",
            "author": "Michael McCandless",
            "id": "comment-12620970"
        },
        {
            "date": "2008-08-08T20:18:09+0000",
            "content": "Mike, thanks for clearing things...\n\nYou're right - this is fixed by LUCENE-1333. \nIf LUCENE-1333 gets committed soon there's no point in \ndoing this here, just making more work for DM in reworking 1333.\nThe only motivation to do this is if there will be another\nfix release 2.3.3.3, in which case it would make sense to\nfix this issue, but not the deprecation of the non-reuse \nAPI done by 1333. Or do you agree with DM that since payloads\nand flags are marked experimental they can remain broken \n(in regard of this issue) until 2.4? (not perfect, but I can \nlive with it).\n\nFor the reuse methods names, I like reinit()... ",
            "author": "Doron Cohen",
            "id": "comment-12621041"
        },
        {
            "date": "2008-08-08T20:44:24+0000",
            "content": "\nShould we just absorb this issue into LUCENE-1333? DM, of your list\nabove (of filters that lose payload), are there any that are not fixed\nin LUCENE-1333? I'm confused on the overlap and it's hard to work\nwith all the patches. Actually if in LUCENE-1333 you could\nconsolidate down to a single patch (big toplevel \"svn diff\"), that'd\nbe great\n\nLUCENE-1333 will have to include all of this. I have already created a patch for LUCENE-1350 and LUCENE-1333, which satisfies this requirement. If LUCENE-1350 goes first, then the patch for LUCENE-1333 will need to be re-built. If LUCENE-1333 goes first then this one can be closed.\n\nI really don't care which is done first. If both are going to be in the next release, then I think just do LUCENE-1333. But if for some reason, we are going to do a release before 2.4 and only LUCENE-1350 is going in it, then that's fine with me.\n\nAs to the effort I have already done the work. And I was happy to do it  ",
            "author": "DM Smith",
            "id": "comment-12621051"
        },
        {
            "date": "2008-08-09T19:52:34+0000",
            "content": "DM Thanks for taking care of this large change!\nBy Mike's comments on LUCENE-1333 seems LUCENE-1333 will \nbe committed and this one will be canceled so I feel kinda bad for \nthe time you put in the last patch here.  ",
            "author": "Doron Cohen",
            "id": "comment-12621175"
        },
        {
            "date": "2008-08-11T09:24:35+0000",
            "content": "This issue is fixed by LUCENE-1333, but wait with closing it until LUCENE-1333 is committed.  ",
            "author": "Doron Cohen",
            "id": "comment-12621387"
        },
        {
            "date": "2008-08-11T09:26:08+0000",
            "content": "Attaching the test which fails now but is fixed with LUCENE-1333.  ",
            "author": "Doron Cohen",
            "id": "comment-12621388"
        },
        {
            "date": "2008-09-03T21:42:01+0000",
            "content": "Isn't this one now a dup of LUCENE-1333? ",
            "author": "Michael McCandless",
            "id": "comment-12628153"
        },
        {
            "date": "2008-09-03T21:47:31+0000",
            "content": "Yes it is a dup, thanks Mike for taking care of this (I planned to do this yesterday but didn't make it) ",
            "author": "Doron Cohen",
            "id": "comment-12628158"
        }
    ]
}