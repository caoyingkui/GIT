{
    "id": "LUCENE-6100",
    "title": "Further tuning of Lucene50Codec(BEST_COMPRESSION)",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [],
        "labels": "",
        "fix_versions": [
            "5.0",
            "6.0"
        ],
        "priority": "Major",
        "status": "Closed",
        "type": "Improvement"
    },
    "description": "Currently this codec has two options: BEST_SPEED and BEST_COMPRESSION. But in the case of highly compressible data, the ratio for BEST_COMPRESSION is not much over BEST_SPEED, because they share the same underlying format which is not optimized for this here.\n\nblock size is currently 24576 (32kb sliding window size minus 8kb \"grace\" to avoid going over it). And we compress this in a stateless manner, each block is its own stream and they dont share preset dictionary or anything. So we have a lot of waste in many cases, since zlib has to reboot itself, then we generally throw away 1/4 of the window and start over.\n\nI ran some experiments with highly compressible logs data:\n\n\n\nmethod\ntime indexing(ms)\ntime merging(ms)\nfdt\nfdx\n\n\nBEST_SPEED\n101,729\n15,638\n372,845,282\n406,964\n\n\nBEST_COMPRESSION\n114,364\n23,474\n269,387,347\n275.909\n\n\npatch (60KB)\n105,533\n18,914\n237,284,342\n117,639\n\n\n\n\n\nThe other experiments I ran were:\n\n\n\nmethod\ntime indexing(ms)\ntime merging(ms)\nfdt\nfdx\n\n\ncrappy preset\n130,854\n38,095\n234,603,971\n274,500\n\n\n64KB\n107,256\n21,570\n236,004,297\n111,135\n\n\ncrappy preset+64KB\n121,503\n30,030\n222,422,924\n110,751\n\n\n\n\n\nFor 'crappy preset' I just use arbitrary first 32KB bytes of original data as a preset dictionary for every block. This is effective, but slow because of some unnecessary overhead involved (like computing adler32 over and over of the preset dict for each block). However, this overhead is reduced with larger block sizes, and still offers benefits, so maybe in the future we can do it (especially e.g. if its per-chunk and we can bulk merge chunks without recompressing, etc).\n\nFor 64KB, we measure removing the \"grace\" completely so it spills to another block each time. The proposed smaller \"grace\" amount still offers cpu savings, so I think we should keep it. But its not terrible if you go over.",
    "attachments": {
        "LUCENE-6100.patch": "https://issues.apache.org/jira/secure/attachment/12685619/LUCENE-6100.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14238509",
            "author": "Ryan Ernst",
            "date": "2014-12-08T21:49:04+0000",
            "content": "+1 "
        },
        {
            "id": "comment-14240440",
            "author": "ASF subversion and git services",
            "date": "2014-12-10T00:59:51+0000",
            "content": "Commit 1644272 from Robert Muir in branch 'dev/trunk'\n[ https://svn.apache.org/r1644272 ]\n\nLUCENE-6100: further tuning of BEST_COMPRESSION "
        },
        {
            "id": "comment-14240483",
            "author": "ASF subversion and git services",
            "date": "2014-12-10T01:36:38+0000",
            "content": "Commit 1644300 from Robert Muir in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1644300 ]\n\nLUCENE-6100: further tuning of BEST_COMPRESSION "
        },
        {
            "id": "comment-14241859",
            "author": "Adrien Grand",
            "date": "2014-12-10T22:28:38+0000",
            "content": "I'm coming a bit late to the party but +1! "
        },
        {
            "id": "comment-14332981",
            "author": "Anshum Gupta",
            "date": "2015-02-23T05:02:56+0000",
            "content": "Bulk close after 5.0 release. "
        }
    ]
}