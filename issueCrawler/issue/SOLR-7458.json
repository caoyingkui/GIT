{
    "id": "SOLR-7458",
    "title": "Expose HDFS Block Locality Metrics",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [
            "5.3",
            "6.0"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "We should publish block locality metrics when using HDFS.",
    "attachments": {
        "Data Locality.png": "https://issues.apache.org/jira/secure/attachment/12738072/Data%20Locality.png",
        "SOLR-7458.part-2.patch": "https://issues.apache.org/jira/secure/attachment/12739634/SOLR-7458.part-2.patch",
        "SOLR-7458.patch": "https://issues.apache.org/jira/secure/attachment/12727782/SOLR-7458.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-04-23T23:41:53+0000",
            "author": "Mike Drob",
            "content": "I'm working on a patch for this, but it will require the changes in SOLR-7457 first. ",
            "id": "comment-14510078"
        },
        {
            "date": "2015-04-24T01:25:08+0000",
            "author": "Mike Drob",
            "content": "Patch against trunk. ",
            "id": "comment-14510215"
        },
        {
            "date": "2015-04-27T12:44:57+0000",
            "author": "Mark Miller",
            "content": "Looks like directories is never set to non null. ",
            "id": "comment-14514066"
        },
        {
            "date": "2015-04-27T13:15:50+0000",
            "author": "Mark Miller",
            "content": "What do we know about the expense of this? For a monitoring tool that might be pounding JMX on a dozen large collections every 30-60 seconds, I'm a little worried about iterating through all the files and making server calls for each one. ",
            "id": "comment-14514101"
        },
        {
            "date": "2015-04-27T13:51:03+0000",
            "author": "Mark Miller",
            "content": "Adding a bit of history: We used to get the size of a directory in hdfs by running through the files and adding up the size for each. On a large cluster with many cores, this was a huge performance issue. We had to switch to a single call that was more like a df command.\n\nThat makes me think this impl will have the same type of problems. We may have to look into doing cached responses or something. ",
            "id": "comment-14514151"
        },
        {
            "date": "2015-04-27T14:53:11+0000",
            "author": "Mike Drob",
            "content": "Looks like directories is never set to non null.\nOops, thanks. I had to redo this file when splitting the patch from other work and looks like I missed this. Will fix in the next version of the patch.\n\nThat makes me think this impl will have the same type of problems. We may have to look into doing cached responses or something.\nI know that HBase has done something similar, I'll check with some folks over there to see if they have battle tested experience. ",
            "id": "comment-14514236"
        },
        {
            "date": "2015-04-29T21:18:28+0000",
            "author": "Mike Drob",
            "content": "Here's a version that caches the results for each file. We still have to do a directory listing each time we get a JMX request, but that seems not too bad.\n\nAn alternative approach would be to update the cache only when we open a file (i.e. new HdfsIndexInput()), but that had more intricate object life-cycle implications so I decided to avoid it unless somebody else thought it might be worthwhile. ",
            "id": "comment-14520272"
        },
        {
            "date": "2015-05-01T11:50:04+0000",
            "author": "Mark Miller",
            "content": "This looks like a good approach.\n\nlocalhost = System.getProperty(\"solr.host\", \"localhost\");\n\nThis looks off to me. For one, I'm wary of using 'localhost' vs 127.0.0.1. Are we sure that is the right approach?\n\nAlso, 'solr.host' is not any kind of official system property. There is a host property in solr.xml.\n\nprivate final Map<HdfsDirectory,Map<FileStatus,BlockLocation[]>> cache\n\nThis cache needs to be thread safe. ",
            "id": "comment-14523102"
        },
        {
            "date": "2015-05-01T13:29:22+0000",
            "author": "Mike Drob",
            "content": "This looks off to me. For one, I'm wary of using 'localhost' vs 127.0.0.1. Are we sure that is the right approach?\nNot at all sure. I'm open to suggestions.\n\nI saw the property in solr.xml, but could not figure out how to access it. A pointer here would be helpful.\n\nThis cache needs to be thread safe.\nI guess there could be a JMX call happening while a new shard is being created? Do I need to plan for multiple concurrent JMX calls? ",
            "id": "comment-14523190"
        },
        {
            "date": "2015-05-01T14:50:10+0000",
            "author": "Mark Miller",
            "content": "Not at all sure. I'm open to suggestions.\n\nI'm not yet sure myself. My main concern is that whatever this host property gets assigned ends up matching what hdfs returns as the host. This seems tricky to ensure offhand.\n\nI saw the property in solr.xml, but could not figure out how to access it. A pointer here would be helpful.\n\nIt's SolrXmlConfig or something along those lines, accessible at the CoreContainer level. I've been noodling how you might make this CoreContainer level scope rather than static scope so you have access to that host config. Again, looks a bit tricky.\n\nI guess there could be a JMX call happening while a new shard is being created? Do I need to plan for multiple concurrent JMX calls?\n\nRight, SolrCores can be created at any time and will use new directories. I'm  not so worried about staleness, but gets on a hashmap with concurrent get/updates can also hang or other bad stuff. Prob best to just use a ConcurrentHashMap. ",
            "id": "comment-14523270"
        },
        {
            "date": "2015-05-04T17:54:36+0000",
            "author": "Mike Drob",
            "content": "Did some digging with HDFS folks and it looks like BlockLocation::host is generally a hostname (and not an ip, with caveat that your cluster is configured reasonably). The general solution to determining a hostname for a machine is very difficult, since any given server could have multiple interfaces with multiple names for each alias, etc. We probably just have to rely on some well known one that we can get, and not spend too much effort worrying about if \"localhost\" is good enough.\n\nWell look at SolrXmlConfig. Will add in ConcurrentHashMap. ",
            "id": "comment-14526933"
        },
        {
            "date": "2015-05-05T14:10:01+0000",
            "author": "Mike Drob",
            "content": "So I might be able to do something like\n\n\n    String solrHome = SolrResourceLoader.locateSolrHome();\n    ConfigSolr cfg = ConfigSolr.fromSolrHome(new SolrResourceLoader(solrHome), solrHome);\n    String host = cfg.getHost();\n\n\n\nBut that feels incredibly fragile. Will continue to look for alternative solutions. ",
            "id": "comment-14528482"
        },
        {
            "date": "2015-05-05T15:45:35+0000",
            "author": "Mark Miller",
            "content": "What if we added DirectoryFactory to awareCompatibility in SolrResourceLoader and make HdfsDirectoryFactory implement SolrCoreAware.\n\nThat get's us access to the SolrCore from inform which will fire before init. From the SolrCore we can access the CoreContainer and the solr.xml config that get us the configured host name.\n\nThe general solution to determining a hostname for a machine is very difficult, since any given server could have multiple interfaces with multiple names for each alias, etc.\n\nRight. That is my concern.\n\nThat's why we use the following process for Solr:\n\n\n\tIf no host is specified, guess which to use.\n\tAllow a specific host override to be specified.\n\n\n\nIf we don't happen to match up, won't we report the wrong results about what is local? ",
            "id": "comment-14528688"
        },
        {
            "date": "2015-05-05T16:47:34+0000",
            "author": "Mike Drob",
            "content": "What if we added DirectoryFactory to awareCompatibility in SolrResourceLoader and make HdfsDirectoryFactory implement SolrCoreAware.\n\nYes, I think this will work. New patch incoming shortly. ",
            "id": "comment-14528774"
        },
        {
            "date": "2015-05-06T02:24:44+0000",
            "author": "Mike Drob",
            "content": "Haven't had a chance to throw this up onto a real install and check the output, but wanted to make the patch available for review. ",
            "id": "comment-14529779"
        },
        {
            "date": "2015-05-20T21:54:07+0000",
            "author": "Mike Drob",
            "content": "An update on my status here: It seems to work on startup, but there is an issue when refreshing the data. Will continue looking into it. This is probably a good indicator that I needed more logging, anyway... ",
            "id": "comment-14553197"
        },
        {
            "date": "2015-06-01T13:40:28+0000",
            "author": "Mark Miller",
            "content": "Any progress on this issue Mike? ",
            "id": "comment-14567297"
        },
        {
            "date": "2015-06-01T16:51:58+0000",
            "author": "Mike Drob",
            "content": "Not anything significant. I'll upload my latest patch that includes some log statements. ",
            "id": "comment-14567565"
        },
        {
            "date": "2015-06-05T22:02:49+0000",
            "author": "Mike Drob",
            "content": "So... It turns out that my problem where I thought it was not updating was completely unrelated.\n\nAttached is a screenshot for a 4.10.3 based patch showing what it looks like there. Will be the same in trunk, naturally, this is the one I happened to have available.\n\nI've got a new patch that also adds some testing, so I think we are getting close to wrapped up here. ",
            "id": "comment-14575316"
        },
        {
            "date": "2015-06-10T15:08:40+0000",
            "author": "Mark Miller",
            "content": "Should we call this attribute hdfs-locality instead of just locality? ",
            "id": "comment-14580617"
        },
        {
            "date": "2015-06-10T15:24:56+0000",
            "author": "Mike Drob",
            "content": "Should we call this attribute hdfs-locality instead of just locality?\nThat makes sense.\n\nWorth noting that if you are not using HDFS, then these metrics won't show up at all, so I think there is less potential for confusion. ",
            "id": "comment-14580635"
        },
        {
            "date": "2015-06-10T16:51:58+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1684711 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1684711 ]\n\nSOLR-7458: Expose HDFS Block Locality Metrics via JMX ",
            "id": "comment-14580784"
        },
        {
            "date": "2015-06-10T17:37:25+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1684720 from Mark Miller in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1684720 ]\n\nSOLR-7458: Expose HDFS Block Locality Metrics via JMX ",
            "id": "comment-14580859"
        },
        {
            "date": "2015-06-11T13:17:46+0000",
            "author": "Mark Miller",
            "content": "Thanks Mike! ",
            "id": "comment-14581915"
        },
        {
            "date": "2015-06-13T22:17:04+0000",
            "author": "Mark Miller",
            "content": "Hmm...I've noticed this jenkins fail twice now:\n\n\nBuild: https://builds.apache.org/job/Lucene-Solr-Tests-trunk-Java8/72/\n\n1 tests failed.\nREGRESSION:  org.apache.solr.core.HdfsDirectoryFactoryTest.testLocalityReporter\n\nError Message:\nexpected:<0> but was:<1>\n\nStack Trace:\njava.lang.AssertionError: expected:<0> but was:<1>\n        at __randomizedtesting.SeedInfo.seed([FA6B1344CA602824:B4769CBC9B5059C9]:0)\n        at org.junit.Assert.fail(Assert.java:93)\n        at org.junit.Assert.failNotEquals(Assert.java:647)\n        at org.junit.Assert.assertEquals(Assert.java:128)\n        at org.junit.Assert.assertEquals(Assert.java:147)\n        at org.apache.solr.core.HdfsDirectoryFactoryTest.testLocalityReporter(HdfsDirectoryFactoryTest.java:217)\n\n ",
            "id": "comment-14584837"
        },
        {
            "date": "2015-06-15T15:15:26+0000",
            "author": "Mike Drob",
            "content": "Strange. I tried running the suggested ant command and was unable to reproduce locally. The command output on Jenkins isn't terribly useful either. I can submit a patch to add error messages to the asserts if we think that will help long term maintainability.\n\nThe failure looks like the test runner on Jenkins is able to count the written blocks as local despite no hostname having been set. Maybe something in the environment affects how MiniDfsCluster operates. We could set an explicit bogus hostname to ensure that the data is \"not local.\" I'm not sure how the test will interact with 127.0.0.1 (the next assert) on Jenkins though. ",
            "id": "comment-14586175"
        },
        {
            "date": "2015-06-15T15:44:42+0000",
            "author": "Mark Miller",
            "content": "I can submit a patch to add error messages to the asserts if we think that will help long term maintainability.\n\n+1 ",
            "id": "comment-14586221"
        },
        {
            "date": "2015-06-15T15:53:06+0000",
            "author": "Mike Drob",
            "content": "New JIRA or just attach to this one? ",
            "id": "comment-14586232"
        },
        {
            "date": "2015-06-15T16:00:59+0000",
            "author": "Mark Miller",
            "content": "This one is fine since we are just improving the test messages. ",
            "id": "comment-14586242"
        },
        {
            "date": "2015-06-15T16:10:14+0000",
            "author": "Mike Drob",
            "content": "Addendum patch against trunk. ",
            "id": "comment-14586250"
        },
        {
            "date": "2015-06-16T14:33:12+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1685847 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1685847 ]\n\nSOLR-7458: Improve test assert messages. ",
            "id": "comment-14588108"
        },
        {
            "date": "2015-06-16T14:34:28+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1685848 from Mark Miller in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1685848 ]\n\nSOLR-7458: Improve test assert messages. ",
            "id": "comment-14588110"
        },
        {
            "date": "2015-08-26T13:06:23+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Bulk close for 5.3.0 release ",
            "id": "comment-14713306"
        }
    ]
}