{
    "id": "LUCENE-461",
    "title": "StandardTokenizer splitting all of Korean words into separate characters",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Bug",
        "fix_versions": [
            "1.9"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "StandardTokenizer splits all those Korean words inth separate character tokens. For example, \"?????\" is one Korean word that means \"Hello\", but StandardAnalyzer separates it into five tokens of \"?\", \"?\", \"?\", \"?\", \"?\".",
    "attachments": {
        "StandardTokenizer_KoreanWord.patch": "https://issues.apache.org/jira/secure/attachment/12320534/StandardTokenizer_KoreanWord.patch",
        "TestStandardAnalyzer_KoreanWord.patch": "https://issues.apache.org/jira/secure/attachment/12320535/TestStandardAnalyzer_KoreanWord.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2005-11-08T15:59:01+0000",
            "content": "Here are patches to preserve one Korean word not to be separated into each characters. The TestStandardAnalyzer test case attached has passed with StandardTokenizer with patch applied. ",
            "author": "Cheolgoo Kang",
            "id": "comment-12357014"
        },
        {
            "date": "2005-11-12T17:36:24+0000",
            "content": "These patches have been applied, thanks! \n\nThere is one thing to note, and that is a change in the token type emitted from \"<CJK>\" to \"<CJ>\".  It is possible that folks have written code to rely on that, but this token type is currently brittle as it is based on the JavaCC grammar definition and I view this as an acceptable break in full backwards compatibility because it is unlikely that anyone is using that token type. ",
            "author": "Erik Hatcher",
            "id": "comment-12357482"
        }
    ]
}