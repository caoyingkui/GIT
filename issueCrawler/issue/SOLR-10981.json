{
    "id": "SOLR-10981",
    "title": "Allow update to load gzip files",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "SolrJ"
        ],
        "type": "Improvement",
        "fix_versions": [
            "7.6",
            "master (8.0)"
        ],
        "affect_versions": "6.6",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "We currently import large CSV files. We store them in gzip files as they compress at around 80%.\n\nTo import them we must gunzip them and then import them. After that we no longer need the decompressed files.\n\nThis patch allows directly opening either URL, or local files that are gzipped.\n\nFor URLs, to determine if the file is gzipped, it will check the content encoding==\"gzip\" or if the file ends in \".gz\"\n\nFor files, if the file ends in \".gz\" then it will assume the file is gzipped.\n\nI have tested the patch with 4.10.4, 6.6.0, 7.0.1 and master from git.",
    "attachments": {
        "SOLR-10981.patch": "https://issues.apache.org/jira/secure/attachment/12875156/SOLR-10981.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-06-29T23:56:28+0000",
            "content": "correctly named patch generated from the command line rather than ij. ",
            "author": "Andrew Lundgren",
            "id": "comment-16069216"
        },
        {
            "date": "2017-06-30T04:29:57+0000",
            "content": "Can the gzipped file be streamed in (using zcat)? ",
            "author": "Ishan Chattopadhyaya",
            "id": "comment-16069457"
        },
        {
            "date": "2017-06-30T05:53:43+0000",
            "content": "Not in this case. They are stored off box in s3.  They are loaded via a URL. ",
            "author": "Andrew Lundgren",
            "id": "comment-16069542"
        },
        {
            "date": "2017-06-30T09:03:40+0000",
            "content": "This is a duplicate of SOLR-7925, although with different solution proposals. Have not looked at the patches in detail.\n\nHow would this work if you want to post an XML, JSON or PDF? Today our handlers rely on content-type header to select the right update handler. If we want generic support for Content-Type: application/gzip then how to know what content type is inside the uncompressed stream? ",
            "author": "Jan H\u00f8ydahl",
            "id": "comment-16069747"
        },
        {
            "date": "2017-06-30T15:21:24+0000",
            "content": "As this doesn't use the content-type header, it uses the content-encoding header, it does not interfere with the existing content-type header usage. \n\nIn the patched ContentStreamBase, line 84 the content type is taken from the connection.  As this is not changed by the gzip contentEncoding header on line 89; code using the content stream is unaffected.  If the contentEncoding is not set, then the code will also detect if the file ends with \".gz\".  This could be dropped, though it seemed a reasonable usage.\n\nIn the patched ContentStreamBase, lines 117, 121 the content type of a FileStream is determined by the first character found in the stream.  As the stream is already opened and the gunzip stream applied over the input stream, the code that determines the content type is unaffected.  The FileStream will work with any existing format that is gzipped as it determines the content type based on the first character of the decompressed stream.  (Attached is a new patch that causes this method to use the getStream method on 117 rather than open the file itself applying the gzip layer)\n\nI agree that using generic Content-Type: application/gzip would lead to confusion.  To me, the gzip layer is the encoding of the content, not the type itself.  By using the encoding type you are able to handle the gzip at a lower layer, and keep all of your content-type support untouched.\n\nSee: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Encoding\nSee: https://www.iana.org/assignments/media-types/media-types.xhtml\n\nThe current handling of a FileStream and a file:// URL are inconsistent, as the FileStream tries to guess the content type based on the first character.  The file:// URL uses mime-types to determine the content.   They seemingly should be consistent, though I did not try to make them consistent, as the FileStream implementation point's out, it's implementation is buggy. ",
            "author": "Andrew Lundgren",
            "id": "comment-16070259"
        },
        {
            "date": "2017-06-30T15:31:05+0000",
            "content": "Added tests around checking content type. Fixed bug in checking content type of FileStream for gzipped streams.  Added comments in test of FileStream vs file:// URL Stream. ",
            "author": "Andrew Lundgren",
            "id": "comment-16070279"
        },
        {
            "date": "2017-06-30T18:19:48+0000",
            "content": "Updated code to handle .gzip as well .gz and mixed case.  \nUpdated FileStream to use suffixes to determine ContentType before looking inside file.\nAdded .cvs extension support to FileStream ContentType\nModified FileStream ContentType so that when it does look inside the file it handles whitespaces and end of line.\n\nFileStream and file:// URLStream now behave consistently when application/gzip, application/octet-stream, content/unknown are returned. ",
            "author": "Andrew Lundgren",
            "id": "comment-16070510"
        },
        {
            "date": "2017-06-30T19:06:22+0000",
            "content": "Sorry for confusing content-type and content-encoding. What you propose with the encoding sounds like the correct thing to do.\nHave not looked closely on the whole patch, but what would it take to be able to POST a gzipped csv file using cURL?\n\ncurl -XPOST -H 'Content-Type: text/csv' -H 'Content-Encoding: gzip' http://solr.server:8983/solr/coll/update --data-binary @myfile.csv.gz\n\n ",
            "author": "Jan H\u00f8ydahl",
            "id": "comment-16070591"
        },
        {
            "date": "2017-07-10T10:39:15+0000",
            "content": "I tagged this as fix for 7.1. Would you be able to also add a CHANGES.txt line to your patch (for v7.1), and attempt to update the solr-refguide where appropriate? Then I guess we could add this and if more work is needed to accept gzipped content as ordinary POST request, we do that in followup issues. ",
            "author": "Jan H\u00f8ydahl",
            "id": "comment-16080141"
        },
        {
            "date": "2017-07-10T14:34:28+0000",
            "content": "Sorry for not getting back on this sooner.  I spent quite a bit of time digging into what the post would take.\n\nIn doing so I ended up refactoring and consolidating a fair amount of code around the loaders.  More so than I think probably belongs in a small patch that I was hoping to get back on the 6.X branch.\n\nI ended up working a fair amount on the StringStream loader trying to get it to work with a compressed stream.  That ended up as a dead end. (The String goes into seemed to go double byte mode and corrupting the binary contents of the gzip format.)  If I followed it correctly the code that handles the post uses that class.  That code will need to get smarter so that it can use the header to determine which input stream it needs to use.\n\nI will continue working on this, but I think it best that it be on another issue, as the amount of changed code is much larger including the refactoring.\n\nWhat would it take to get the patch to handle the gzip URL/File on the 6.X branch? ",
            "author": "Andrew Lundgren",
            "id": "comment-16080415"
        },
        {
            "date": "2017-07-10T16:53:52+0000",
            "content": "Yes, let's spin off the POST into another issue.\n\nThere are no further 6.x feature releases planned, and 7.0 is already in feature freeze. So the earliest you can hope for is 7.1.\nIf we get the feature into 7.1 in not too long time, we can always backport to branch_6x in case of a potential 6.7 release.\nYou can of course also build your own custom Solr off from 6.6 branch with this patch applied. ",
            "author": "Jan H\u00f8ydahl",
            "id": "comment-16080631"
        },
        {
            "date": "2017-12-19T19:01:53+0000",
            "content": "Patch that includes Changes.txt updates and solr-ref-guide.\n\nThis patch should work for the 7 series as well as master.\n\nJan H\u00f8ydahl Sorry for the delay.  Sometimes RL gets busy. ",
            "author": "Andrew Lundgren",
            "id": "comment-16297263"
        },
        {
            "date": "2018-05-10T14:33:49+0000",
            "content": "Are there any problems with this that is causing it to keep getting bumped to the \"next\" release? ",
            "author": "Andrew Lundgren",
            "id": "comment-16470463"
        },
        {
            "date": "2018-05-10T16:03:50+0000",
            "content": "Just curious but wouldn't Jetty be able to handle this for us in a generic way?:\u00a0https://www.eclipse.org/jetty/documentation/9.4.x/gzip-filter.html ",
            "author": "David Smiley",
            "id": "comment-16470608"
        },
        {
            "date": "2018-05-10T16:06:39+0000",
            "content": "Sorry nevermind. \u00a0After looking at the linked issue I see that gzip Jetty filter only does responses. ",
            "author": "David Smiley",
            "id": "comment-16470611"
        },
        {
            "date": "2018-05-12T01:01:43+0000",
            "content": "The client side (requests) has to be handled by the client of course - we have an option to use gzip with our clients.\n\nI guess the difference here is that we are not gzipping the file, or whatever we send to Jetty, it's already gzipped. ",
            "author": "Mark Miller",
            "id": "comment-16472848"
        },
        {
            "date": "2018-07-19T17:39:48+0000",
            "content": "\n\n\n  -1 overall \n\n\n\n\n\n\n\n\n\n Vote \n Subsystem \n Runtime \n Comment \n\n\n -1 \n  patch  \n   0m  5s \n  SOLR-10981 does not apply to master. Rebase required? Wrong Branch? See https://wiki.apache.org/solr/HowToContribute#Creating_the_patch_file for help.  \n\n\n\n\n\n\n\n\n\n Subsystem \n Report/Notes \n\n\n JIRA Issue \n SOLR-10981 \n\n\n JIRA Patch URL \n https://issues.apache.org/jira/secure/attachment/12902908/SOLR-10981.patch \n\n\n Console output \n https://builds.apache.org/job/PreCommit-SOLR-Build/146/console \n\n\n Powered by \n Apache Yetus 0.7.0   http://yetus.apache.org \n\n\n\n\n\n\nThis message was automatically generated.\n ",
            "author": "Lucene/Solr QA",
            "id": "comment-16549613"
        },
        {
            "date": "2018-08-30T09:39:12+0000",
            "content": "This feature is exclusively for use with stream.url and stream.file, correct? If so, I think that should be clearly stated both in CHANGES entry and in refguide docs.\n\nIn your patch you define constants for various content types static final String APPLICATION_XML = \"application/xml\"; but you could easily reference e.g. org.apache.http.ContentType.APPLICATION_XML.getMimeType() instead. ",
            "author": "Jan H\u00f8ydahl",
            "id": "comment-16597245"
        },
        {
            "date": "2018-09-25T06:26:05+0000",
            "content": "\n\n\n  -1 overall \n\n\n\n\n\n\n\n\n\n Vote \n Subsystem \n Runtime \n Comment \n\n\n\u00a0\n\u00a0\n\u00a0\n  Prechecks  \n\n\n +1 \n  test4tests  \n   0m  0s \n  The patch appears to include 1 new or modified test files.  \n\n\n\u00a0\n\u00a0\n\u00a0\n  master Compile Tests  \n\n\n +1 \n  compile  \n   2m 24s \n  master passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Patch Compile Tests  \n\n\n +1 \n  compile  \n   1m 54s \n  the patch passed  \n\n\n +1 \n  javac  \n   1m 54s \n  the patch passed  \n\n\n +1 \n  Release audit (RAT)  \n   1m 58s \n  the patch passed  \n\n\n -1 \n  Check forbidden APIs  \n   1m 54s \n  Check forbidden APIs check-forbidden-apis failed  \n\n\n -1 \n  Validate source patterns  \n   1m 54s \n  Check forbidden APIs check-forbidden-apis failed  \n\n\n -1 \n  Validate ref guide  \n   1m 54s \n  Check forbidden APIs check-forbidden-apis failed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Other Tests  \n\n\n +1 \n  unit  \n   7m 15s \n  solrj in the patch passed.  \n\n\n  \n   \n  15m  1s \n   \n\n\n\n\n\n\n\n\n\n Subsystem \n Report/Notes \n\n\n JIRA Issue \n SOLR-10981 \n\n\n JIRA Patch URL \n https://issues.apache.org/jira/secure/attachment/12937709/SOLR-10981.patch \n\n\n Optional Tests \n  compile  javac  unit  ratsources  checkforbiddenapis  validatesourcepatterns  validaterefguide  \n\n\n uname \n Linux lucene2-us-west.apache.org 4.4.0-112-generic #135-Ubuntu SMP Fri Jan 19 11:48:36 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux \n\n\n Build tool \n ant \n\n\n Personality \n /home/jenkins/jenkins-slave/workspace/PreCommit-SOLR-Build/sourcedir/dev-tools/test-patch/lucene-solr-yetus-personality.sh \n\n\n git revision \n master / 9bc4b8d \n\n\n ant \n version: Apache Ant(TM) version 1.9.6 compiled on July 20 2018 \n\n\n Default Java \n 1.8.0_172 \n\n\n Check forbidden APIs \n https://builds.apache.org/job/PreCommit-SOLR-Build/187/artifact/out/patch-check-forbidden-apis-solr.txt \n\n\n Validate source patterns \n https://builds.apache.org/job/PreCommit-SOLR-Build/187/artifact/out/patch-check-forbidden-apis-solr.txt \n\n\n Validate ref guide \n https://builds.apache.org/job/PreCommit-SOLR-Build/187/artifact/out/patch-check-forbidden-apis-solr.txt \n\n\n  Test Results \n https://builds.apache.org/job/PreCommit-SOLR-Build/187/testReport/ \n\n\n modules \n C: solr solr/solrj solr/solr-ref-guide U: solr \n\n\n Console output \n https://builds.apache.org/job/PreCommit-SOLR-Build/187/console \n\n\n Powered by \n Apache Yetus 0.7.0   http://yetus.apache.org \n\n\n\n\n\n\nThis message was automatically generated.\n ",
            "author": "Lucene/Solr QA",
            "id": "comment-16626839"
        },
        {
            "date": "2018-09-28T23:37:08+0000",
            "content": "I updated the patch to use:\u00a0org.apache.http.entity.ContentType.APPLICATION* where applicable.\n\nI updated toLowerCase() to use the Locale.ROOT based on the forbidden api results.\n\nI updated the\u00a0change and usage\u00a0to include: \"This feature is exclusively for use with\u00a0stream.url\u00a0and\u00a0stream.file\"\n\nReady for another review! ",
            "author": "Andrew Lundgren",
            "id": "comment-16632649"
        },
        {
            "date": "2018-09-29T09:13:25+0000",
            "content": "\n\n\n  +1 overall \n\n\n\n\n\n\n\n\n\n Vote \n Subsystem \n Runtime \n Comment \n\n\n\u00a0\n\u00a0\n\u00a0\n  Prechecks  \n\n\n +1 \n  test4tests  \n   0m  0s \n  The patch appears to include 1 new or modified test files.  \n\n\n\u00a0\n\u00a0\n\u00a0\n  master Compile Tests  \n\n\n +1 \n  compile  \n   1m  3s \n  master passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Patch Compile Tests  \n\n\n +1 \n  compile  \n   1m  0s \n  the patch passed  \n\n\n +1 \n  javac  \n   1m  0s \n  the patch passed  \n\n\n +1 \n  Release audit (RAT)  \n   1m  4s \n  the patch passed  \n\n\n +1 \n  Check forbidden APIs  \n   1m  0s \n  the patch passed  \n\n\n +1 \n  Validate source patterns  \n   1m  0s \n  the patch passed  \n\n\n +1 \n  Validate ref guide  \n   1m  0s \n  the patch passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Other Tests  \n\n\n +1 \n  unit  \n   3m 33s \n  solrj in the patch passed.  \n\n\n  \n   \n   8m 53s \n   \n\n\n\n\n\n\n\n\n\n Subsystem \n Report/Notes \n\n\n JIRA Issue \n SOLR-10981 \n\n\n JIRA Patch URL \n https://issues.apache.org/jira/secure/attachment/12941784/SOLR-10981.patch \n\n\n Optional Tests \n  compile  javac  unit  ratsources  checkforbiddenapis  validatesourcepatterns  validaterefguide  \n\n\n uname \n Linux lucene1-us-west 4.4.0-130-generic #156~14.04.1-Ubuntu SMP Thu Jun 14 13:51:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux \n\n\n Build tool \n ant \n\n\n Personality \n /home/jenkins/jenkins-slave/workspace/PreCommit-SOLR-Build/sourcedir/dev-tools/test-patch/lucene-solr-yetus-personality.sh \n\n\n git revision \n master / 964cc88 \n\n\n ant \n version: Apache Ant(TM) version 1.9.3 compiled on July 24 2018 \n\n\n Default Java \n 1.8.0_172 \n\n\n  Test Results \n https://builds.apache.org/job/PreCommit-SOLR-Build/193/testReport/ \n\n\n modules \n C: solr solr/solrj solr/solr-ref-guide U: solr \n\n\n Console output \n https://builds.apache.org/job/PreCommit-SOLR-Build/193/console \n\n\n Powered by \n Apache Yetus 0.7.0   http://yetus.apache.org \n\n\n\n\n\n\nThis message was automatically generated.\n ",
            "author": "Lucene/Solr QA",
            "id": "comment-16632898"
        },
        {
            "date": "2018-10-10T15:55:30+0000",
            "content": "Any other thoughts or suggestions on this?\u00a0 Anything blocking acceptance now? ",
            "author": "Andrew Lundgren",
            "id": "comment-16645177"
        },
        {
            "date": "2018-10-15T20:45:03+0000",
            "content": "This is looking very good Andrew! \u00a0Thanks for your patience. \u00a0I applied the patch locally and ran tests and poked around. I only made slight edits and did some reformatting. I like that the test was revamped to use try-with-resources.\n\nI'm inclined to alter the ref-guide change. I don't think it's worthy of a header. I edited the ref guide notes as follows:\n\nThe source of the data can be compressed using gzip, and Solr will generally detect this.\nThe detection is based on either the presence of a `Content-Encoding: gzip` HTTP header or the file ending with .gz or .gzip.\nGzip doesn't apply to `stream.body`.\n\nWDYT? \u00a0CHANGES.txt will credit you first, then Jan and I. ",
            "author": "David Smiley",
            "id": "comment-16650765"
        },
        {
            "date": "2018-10-17T23:58:26+0000",
            "content": "Sounds great! ",
            "author": "Andrew Lundgren",
            "id": "comment-16654407"
        },
        {
            "date": "2018-10-18T23:53:38+0000",
            "content": "Commit 1a8188d92b8148f2d937bd038f48f103526fcbcc in lucene-solr's branch refs/heads/master from Andrew Lundgren\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1a8188d ]\n\nSOLR-10981: Support for stream.url or stream.file pointing to gzipped data ",
            "author": "ASF subversion and git services",
            "id": "comment-16656049"
        },
        {
            "date": "2018-10-19T00:01:36+0000",
            "content": "Commit 2f61f96bfae9d97e3536305e49865433e28737c2 in lucene-solr's branch refs/heads/branch_7x from Andrew Lundgren\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=2f61f96 ]\n\nSOLR-10981: Support for stream.url or stream.file pointing to gzipped data\n\n(cherry picked from commit 1a8188d92b8148f2d937bd038f48f103526fcbcc) ",
            "author": "ASF subversion and git services",
            "id": "comment-16656061"
        },
        {
            "date": "2018-10-22T08:07:07+0000",
            "content": "Commit 1a8188d92b8148f2d937bd038f48f103526fcbcc in lucene-solr's branch refs/heads/jira/http2 from Andrew Lundgren\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1a8188d ]\n\nSOLR-10981: Support for stream.url or stream.file pointing to gzipped data ",
            "author": "ASF subversion and git services",
            "id": "comment-16658731"
        }
    ]
}