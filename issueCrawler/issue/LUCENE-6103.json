{
    "id": "LUCENE-6103",
    "title": "StandardTokenizer doesn't tokenize word:word",
    "details": {
        "resolution": "Not A Problem",
        "affect_versions": "4.9",
        "components": [
            "modules/analysis"
        ],
        "labels": "",
        "fix_versions": [],
        "priority": "Major",
        "status": "Resolved",
        "type": "Bug"
    },
    "description": "StandardTokenizer (and by result most default analyzers) will not tokenize word:word and will preserve it as one token. This can be easily seen using Elasticsearch's analyze API:\n\nlocalhost:9200/_analyze?tokenizer=standard&text=word%20word:word\n\nIf this is the intended behavior, then why? I can't really see the logic behind it.\n\nIf not, I'll be happy to join in the effort of fixing this.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14239763",
            "author": "Steve Rowe",
            "date": "2014-12-09T18:08:47+0000",
            "content": "StandardTokenizer implements the word boundary rules in Unicode UAX#29.\n\nThe ASCII colon (and other colonicalish forms) is included in the set of characters matched by the WordBreak:MidLetter property value, which appears in rules WB6 and WB7 - these rules forbid word breaks between the colon and surrounding letters.\n\nTo get what you want, you could customize the JFlex grammar used to generate StandardTokenizer by removing colons from the MidLetter definition used.\n\nAnother alternative is ICUTokenizer, which allows runtime per-orthographic-script specification of word-break rules - check out the factory javadocs: http://lucene.apache.org/core/4_9_0/analyzers-icu/org/apache/lucene/analysis/icu/segmentation/ICUTokenizerFactory.html \n\n\n\n "
        },
        {
            "id": "comment-14239784",
            "author": "Itamar Syn-Hershko",
            "date": "2014-12-09T18:19:38+0000",
            "content": "Yes, I figured it will be down to some Unicode rules. Can you give a rationale for this, mainly out of curiosity?\n\nNot a Unicode expert, but I'd assume just like you wouldn't want English words to not-break on Hebrew Punctuation Gershayim (e.g. Test\"Word is actually 2 tokens and \u05de\u05e0\u05db\"\u05dc\u05d9\u05dd is one), maybe this rule is meant for specific scenarios and not for the general use case?\n\nOn another note, any type of Gershayim should be preserved within Hebrew words, not only U+05F4. This is mainly because keyboards and editors used produce the standard \" character in most cases. I had a chat with Robert a while back where he said that's the case, I'm just making sure you didn't follow the specs to the letter in that regard... "
        },
        {
            "id": "comment-14239928",
            "author": "Steve Rowe",
            "date": "2014-12-09T19:42:02+0000",
            "content": "Yes, I figured it will be down to some Unicode rules. Can you give a rationale for this, mainly out of curiosity?\n\nThe comment in the MidLetter list says it's for Swedish.  If you look at the revision history at the bottom of the page, the colon was temporarily removed from MidLetter in between Unicode versions 6.2 and 6.3, but then put back before 6.3 was released (I guess this should be read from the bottom upward):\n\n\n\n\tRestored colon and equivalents (removed in previous draft).\n\tRemoved colon from MidLetter, so that it is no longer contained within words. Handling of colon for word boundary determination in Swedish would be done by tailoring, instead \u2013 for example by a Swedish localization definition in CLDR.\n\n\n\nI guess the Swedish contingent among Unicoders is strong?\n\n\nNot a Unicode expert, but I'd assume just like you wouldn't want English words to not-break on Hebrew Punctuation Gershayim (e.g. Test\"Word is actually 2 tokens and \u05de\u05e0\u05db\"\u05dc\u05d9\u05dd is one), maybe this rule is meant for specific scenarios and not for the general use case?\n\nStandardTokenizer is not intended to be English-centric - instead it should do something reasonable with any text.\n\n\nOn another note, any type of Gershayim should be preserved within Hebrew words, not only U+05F4. This is mainly because keyboards and editors used produce the standard \" character in most cases. I had a chat with Robert a while back where he said that's the case, I'm just making sure you didn't follow the specs to the letter in that regard...\n\nI did follow the specs to the letter, and it does the right thing:\n\nRules WB7b and WB7c forbid breaks around the ASCII double quote character, but only when surrounded by Hebrew letters. "
        },
        {
            "id": "comment-14240090",
            "author": "Itamar Syn-Hershko",
            "date": "2014-12-09T21:26:30+0000",
            "content": "Good stuff, thanks Steve. I'm going to see if the rest of the UAX is good for us, and if so see if I can comply with the 6.2.5 version of the specs.\n\nIt's a good thing StandardTokenizer is no longer English centric, but I cannot imagine what use the colon has especially since in most cases it is not \"something reasonable\"  "
        },
        {
            "id": "comment-14240133",
            "author": "Itamar Syn-Hershko",
            "date": "2014-12-09T22:00:14+0000",
            "content": "Ok so I did some homework. In swedish, \"connect\" is a way to shortcut writings of words. So \"C:a\" is infact \"cirka\" which means \"approximately\". I guess it can be thought of as English acronyms, only apparently its way less commonly used in Swedish (my source says \"very very seldomly used; old style and not used in modern Swedish at all\").\n\nNot only it is hardly being used, apparently it's only legal in 3 letter combinations (c:a but not c:ka).\n\nAnd also, the affects it has are quite severe at the moment - 2 words with a colon in between that didn't have space will be outputted as one token even though its 100% its not applicable to Swedish, since each words has > 2 characters.\n\nI'm not aiming at changing the Unicode standards, that's way beyond my limited powers, but:\n\n1. Given the above, does it really make sense to use this tokenizer in all language-specific analyzers as well? e.g. https://github.com/apache/lucene-solr/blob/lucene_solr_4_9_1/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java#L105\n\nI'd think for language specific analyzers we'd want tokenizers aiming for this language with limited support for others. So, in this case, colon will always be considered a tokenizing char.\n\n2. Can we change the jflex definition to at least limit the effects of this, e.g. only support colon as MidLetter if the overall token length == 3, so c:a is a valid token and word:word is not? "
        },
        {
            "id": "comment-14240314",
            "author": "Steve Rowe",
            "date": "2014-12-09T23:45:54+0000",
            "content": "Cool info about Swedish.\n\n0. The beauty of implementing a standard is that once you've done that, making tweaks to suit particular constituencies isn't necessary.  StandardTokenizer implements UAX#29 word break rules.  Done.\n\n1. If you'd like to create tailored tokenizers for each individual language, please go ahead.\n\n2. See #0.\n\nOne other technique you may find useful: put a char filter to change problematic chars in front of your tokenizer, e.g. PatternReplaceCharFilter, with the pattern something like (\\p{L}):(\\p{L}), and the replacement $1 $2. "
        },
        {
            "id": "comment-14240392",
            "author": "Itamar Syn-Hershko",
            "date": "2014-12-10T00:28:59+0000",
            "content": "0. You mean it implements UAX#29 version 6.3 \n\n1. I'll likely be sending a PR for #1 sometime soon. Would you recommend using UAX#29 minus specific non-English tweaks, or fall back to ClassicStandardTokenizer which is English specific, or something else?\n\n2. Here's the thing: the standard is wrong, or buggy. Ask any Swedish and they will tell you, and any non-Swedish corpus wouldn't care. And basically this is a bug in every Lucene based system today because of the word:word scenario; its a bit of an edge case but I bet I can find multiple occurrences in every big enough system. What can we do about that?\n\nWe already solved this using char filters, converting colons to a comma. It feels a bit hacky though, and again - this is a flaw in Lucene's analysis even though it conforms to a Unicode standard. "
        },
        {
            "id": "comment-14241213",
            "author": "Steve Rowe",
            "date": "2014-12-10T15:13:28+0000",
            "content": "0. In Lucene 4.7 through 4.10, yes, it implements the revision of UAX#29 associated with Unicode 6.3.  I thought there was a JIRA to upgrade Lucene to Unicode 7.0, but I can't find it ATM.  JFlex 1.6 and ICU 54.1 support Unicode 7.0.\n\n1. I recommend a language-specific tailoring of UAX#29.  There are tailoring notes in the standard you'll want to look at.\n\n2. Unfortunately, I think the correct approach here is lobbying to change the standard. "
        },
        {
            "id": "comment-14241214",
            "author": "Itamar Syn-Hershko",
            "date": "2014-12-10T15:16:07+0000",
            "content": "Maybe out of scope of this ticket, but how do we go about #2? will be happy to take this discussion offline as well "
        },
        {
            "id": "comment-14241236",
            "author": "Steve Rowe",
            "date": "2014-12-10T15:25:32+0000",
            "content": "Maybe out of scope of this ticket, but how do we go about #2? will be happy to take this discussion offline as well\n\nYeah, I'm not sure where the discussion should go, here's fine for me.\n\nPrior to releasing new Unicode versions, PRIs (Public Review Issues) are created for proposed changes to individual standards: http://www.unicode.org/review/ - people can then submit comments, which are then considered for incorporation into the final standard.  I don't see one there for UAX#29, but there have been for previous releases.\n\nI think Robert Muir is an individual member of the Unicode consortium - maybe he'll have some ideas on how to proceed? "
        },
        {
            "id": "comment-14241253",
            "author": "Steve Rowe",
            "date": "2014-12-10T15:38:11+0000",
            "content": "Looks like the intent is to funnel all public input into future Unicode versions through this contact form: http://www.unicode.org/reporting.html - you could start there. "
        },
        {
            "id": "comment-14241306",
            "author": "Itamar Syn-Hershko",
            "date": "2014-12-10T16:12:31+0000",
            "content": "Sent them a request. I'll buy Robert beers if that could help pushing this forward! "
        },
        {
            "id": "comment-14241321",
            "author": "Robert Muir",
            "date": "2014-12-10T16:19:43+0000",
            "content": "I really like beers, but i think I can only give some suggestions:\n\nMaybe it would be good to figure out the exact 'diff' you recommend to the data files / specifications, and also any actual data to support why word breaks are better. Try to think of the general task of word breaks and why the change would be better and keep searching out of it, etc. "
        }
    ]
}