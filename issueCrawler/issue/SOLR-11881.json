{
    "id": "SOLR-11881",
    "title": "Retry update requests from leaders to replicas",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Bug",
        "fix_versions": [
            "7.5",
            "master (8.0)"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "We can see that a connection reset is causing LIR.\n\nIf a leader -> replica update get's a connection like this the leader will initiate LIR\n\n2018-01-08 17:39:16.980 ERROR (qtp1010030701-468988) [c:collection s:shardX r:core_node56 collection_shardX_replicaY] o.a.s.u.p.DistributedUpdateProcessor Setting up to try to start recovery on replica https://host08.domain:8985/solr/collection_shardX_replicaY/\njava.net.SocketException: Connection reset\n        at java.net.SocketInputStream.read(SocketInputStream.java:210)\n        at java.net.SocketInputStream.read(SocketInputStream.java:141)\n        at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)\n        at sun.security.ssl.InputRecord.read(InputRecord.java:503)\n        at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973)\n        at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375)\n        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403)\n        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387)\n        at org.apache.http.conn.ssl.SSLSocketFactory.connectSocket(SSLSocketFactory.java:543)\n        at org.apache.http.conn.ssl.SSLSocketFactory.connectSocket(SSLSocketFactory.java:409)\n        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:177)\n        at org.apache.http.impl.conn.ManagedClientConnectionImpl.open(ManagedClientConnectionImpl.java:304)\n        at org.apache.http.impl.client.DefaultRequestDirector.tryConnect(DefaultRequestDirector.java:611)\n        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:446)\n        at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:882)\n        at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n        at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)\n        at org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrClient$Runner.sendUpdateStream(ConcurrentUpdateSolrClient.java:312)\n        at org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrClient$Runner.run(ConcurrentUpdateSolrClient.java:185)\n        at org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor.lambda$execute$0(ExecutorUtil.java:229)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\n\nFrom https://issues.apache.org/jira/browse/SOLR-6931 Mark says \"On a heavy working SolrCloud cluster, even a rare response like this from a replica can cause a recovery and heavy cluster disruption\" .\n\nLooking at SOLR-6931 we added a http retry handler but we only retry on GET requests. Updates are POST requests ConcurrentUpdateSolrClient#sendUpdateStream\n\nUpdate requests between the leader and replica should be retry-able since they have been versioned.",
    "attachments": {
        "SOLR-11881.patch": "https://issues.apache.org/jira/secure/attachment/12907028/SOLR-11881.patch",
        "SOLR-11881-SolrCmdDistributor.patch": "https://issues.apache.org/jira/secure/attachment/12921495/SOLR-11881-SolrCmdDistributor.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2018-01-22T08:45:45+0000",
            "content": "Does this also fix SOLR-9826? ",
            "author": "Ere Maijala",
            "id": "comment-16334040"
        },
        {
            "date": "2018-01-26T02:32:38+0000",
            "content": "Hi Ere,\n\n\u00a0\n\nNo these are different issues. We should fix both but this one's separate. ",
            "author": "Varun Thacker",
            "id": "comment-16340481"
        },
        {
            "date": "2018-04-25T19:13:09+0000",
            "content": "We ran into another such scenario where a SocketTimeoutException caused LIR.\n\nThe replica had this in it's logs\n\ndate time WARN [qtp768306356-580185] ? (:) - \njava.nio.channels.ReadPendingException: null\nat org.eclipse.jetty.io.FillInterest.register(FillInterest.java:58) ~[jetty-io-9.4.8.v20171121.jar:9.4.8.v20171121]\nat org.eclipse.jetty.io.AbstractEndPoint.fillInterested(AbstractEndPoint.java:353) ~[jetty-io-9.4.8.v20171121.jar:9.4.8.v20171121]\nat org.eclipse.jetty.io.AbstractConnection.fillInterested(AbstractConnection.java:134) ~[jetty-io-9.4.8.v20171121.jar:9.4.8.v20171121]\nat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267) ~[jetty-server-9.4.8.v20171121.jar:9.4.8.v20171121]\nat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:279) ~[jetty-io-9.4.8.v20171121.jar:9.4.8.v20171121]\nat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102) ~[jetty-io-9.4.8.v20171121.jar:9.4.8.v20171121]\nat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:289) ~[jetty-io-9.4.8.v20171121.jar:9.4.8.v20171121]\nat org.eclipse.jetty.io.ssl.SslConnection$3.succeeded(SslConnection.java:149) ~[jetty-io-9.4.8.v20171121.jar:9.4.8.v20171121]\nat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102) ~[jetty-io-9.4.8.v20171121.jar:9.4.8.v20171121]\nat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:124) ~[jetty-io-9.4.8.v20171121.jar:9.4.8.v20171121]\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:247) ~[jetty-util-9.4.8.v20171121.jar:9.4.8.v20171121]\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:140) ~[jetty-util-9.4.8.v20171121.jar:9.4.8.v20171121]\nat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131) ~[jetty-util-9.4.8.v20171121.jar:9.4.8.v20171121]\nat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:382) ~[jetty-util-9.4.8.v20171121.jar:9.4.8.v20171121]\nat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:708) ~[jetty-util-9.4.8.v20171121.jar:9.4.8.v20171121]\nat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:626) ~[jetty-util-9.4.8.v20171121.jar:9.4.8.v20171121]\nat java.lang.Thread.run(Thread.java:748) [?:1.8.0-zing_17.11.0.0]\n\ndate time WARN [qtp768306356-580185] ? (:) - Read pending for org.eclipse.jetty.server.HttpConnection$BlockingReadCallback@2e98df28 prevented AC.ReadCB@424271f8{HttpConnection@424271f8[p=HttpParser{s=START,0 of -1},g=HttpGenerator@424273ae{s=START}]=>HttpChannelOverHttp@4242713d{r=141,c=false,a=IDLE,uri=null}<-DecryptedEndPoint@4242708d{/host:52824<->/host:port,OPEN,fill=FI,flush=-,to=1/86400}->HttpConnection@424271f8[p=HttpParser{s=START,0 of -1},g=HttpGenerator@\n\nAnd the leader waited exactly the socket timeout period after this error and threw a socket-timeout-exception . At that point the leader put the replica into recovery\u00a0 ",
            "author": "Varun Thacker",
            "id": "comment-16452907"
        },
        {
            "date": "2018-04-27T16:17:35+0000",
            "content": "+1 ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16456691"
        },
        {
            "date": "2018-04-27T16:32:22+0000",
            "content": "Update requests between the leader and replica should be retry-able since they have been versioned.\n\nYes, nice, this was a big miss. It's the user client that can't easily auto retry. ",
            "author": "Mark Miller",
            "id": "comment-16456702"
        },
        {
            "date": "2018-04-27T16:56:08+0000",
            "content": "We should also consider turning retry on for the read side as well. It's only done on IOException and you might not have another replica to retry to.\n\nFor the updates side I'm wondering if we should not just turn it on in general. We explicitly disable admin request from retry, is there anything that uses the update client where a retry would be a problem? ",
            "author": "Mark Miller",
            "id": "comment-16456742"
        },
        {
            "date": "2018-04-27T16:57:00+0000",
            "content": "is there anything that uses the update client where a retry would be a problem?\n\nHmm, perhaps when a replica forwards an update to the leader. ",
            "author": "Mark Miller",
            "id": "comment-16456743"
        },
        {
            "date": "2018-04-27T17:58:21+0000",
            "content": "It's been a while since I've thought about this, mind is begining to churn again.\n\nDoes this even help? We stream updates from leader to the replica, and streaming cannot be retried, you'd have to buffer the stream or something. It gets a can't retry exception. ",
            "author": "Mark Miller",
            "id": "comment-16456842"
        },
        {
            "date": "2018-04-27T18:03:00+0000",
            "content": "Maybe we need to implement our own retry in distrib update handler. ",
            "author": "Mark Miller",
            "id": "comment-16456848"
        },
        {
            "date": "2018-04-27T21:14:22+0000",
            "content": "Hi Mark,\n\nConcurrentUpdateSolrClient#sendUpdateStream is the relevant code sending the update from leader->replica right?\u00a0\n\nI don't know this piece of code very\u00a0closely but do we only stream for xml ?\u00a0\u00a0 ",
            "author": "Varun Thacker",
            "id": "comment-16457076"
        },
        {
            "date": "2018-04-27T21:51:39+0000",
            "content": "\u00a0ConcurrentUpdateSolrClient#sendUpdateStream\n\nSounds right.\n\nIt should be using JavaBin and we only stream. It's the only way to do efficient high volume indexing. The only case you can really get away with no doing it is if you know the request is single document per request. That's how things used to work (even if you batch or streamed to the leader, it was split up into document per request), but it only works well with low load. ",
            "author": "Mark Miller",
            "id": "comment-16457124"
        },
        {
            "date": "2018-04-28T01:08:07+0000",
            "content": "If I\u2019m reading the code correctly, this RetryHandler is called in two cases, after an exception trying to establish the connection, and after an exception executing the request. Retrying in the first case should be fine, in the second, not so easy.\nThe way we do streaming is by keeping the connection and having an EntityTemplate that reads updates from a queue and writes each one and flushes. If I understand correctly, if we want to retry the request instead of throwing an error we need to retry the update too, by putting it back in the queue. Even then, I\u2019m not sure we can know that updates before the failure were consumed (even if we call flush for each one) ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16457273"
        },
        {
            "date": "2018-04-28T04:06:25+0000",
            "content": "I think we may be able to use the RetryNode stuff that update forwarding to the leader does or something similar. I\u2019d have to refresh, but I believe if we send an update, even streaming, we get a response for that update. If it\u2019s success we are good. I don\u2019t think we want to retry with the ConcurrentUpdateClient, but instead like forwards do with SolrCmdDistributor. ",
            "author": "Mark Miller",
            "id": "comment-16457345"
        },
        {
            "date": "2018-04-30T05:17:55+0000",
            "content": "SOLR-12290 may help with the cause of the connection reset. Hard to say though, SSL has different connection semantics I believe and we have not looked into how fully reusable they are. ",
            "author": "Mark Miller",
            "id": "comment-16458316"
        },
        {
            "date": "2018-04-30T05:20:42+0000",
            "content": "I\u2019m not sure we can know that updates before the failure were consumed (even if we call flush for each one)\n\nYeah, so we are sending individual requests on a single connection - each request is good if you get a good response and requests are fully serial per connection. So the information is there, it's just how hard is it to use as we need given the current code. ",
            "author": "Mark Miller",
            "id": "comment-16458319"
        },
        {
            "date": "2018-04-30T05:27:47+0000",
            "content": "Varun Thacker, look at the exception in the summary, you can almost be sure this is SOLR-12290. It's trying to just start the connection and on the first read finds out it's closed. This is the normal signature for when a server connection gets improperly closed. ",
            "author": "Mark Miller",
            "id": "comment-16458322"
        },
        {
            "date": "2018-04-30T05:58:32+0000",
            "content": "For the second exception here is something similiar: https://github.com/eclipse/jetty.project/issues/1047\n\nThats fixed in jetty-9.3.17.v20170317 it looks though. ",
            "author": "Mark Miller",
            "id": "comment-16458334"
        },
        {
            "date": "2018-04-30T06:04:52+0000",
            "content": "Another possibility (beyond some different bug) for the second one is that it's reusing connections and there is a case we don't fully clear the connection streams. Normally, Jetty will just not reuse that connection if that happens, perhaps SSL with this new async stuff can hit this if that happens though. Just a guess and reminder that I should review our code that ensures everyone is fully reading streams. ",
            "author": "Mark Miller",
            "id": "comment-16458338"
        },
        {
            "date": "2018-04-30T06:09:59+0000",
            "content": ">\u00a0For the second exception here is something similiar:\u00a0https://github.com/eclipse/jetty.project/issues/1047\n\nYeah\u00a0that issue\u00a0seems to have been fixed but here's the mailing list thread that the jetty folks pointed me to a bug with ssl/async :\u00a0https://dev.eclipse.org/mhonarc/lists/jetty-dev/msg03165.html\n\u00a0 ",
            "author": "Varun Thacker",
            "id": "comment-16458341"
        },
        {
            "date": "2018-05-01T23:17:58+0000",
            "content": "Attached a rough patch that handles the retry in SolrCmdDistributor:\n\n\tI only added retry to requests from leader to it's replicas.\n\tDidn't add any tests yet, I've been running the ChaosMonkey to see how the retries behave\n\tI change the retry exception from only ConnectException to SocketException or NoHttpResponseException\n\tI plan to reduce the number of retries for this case (25 sounds like a lot, I was thinking of 5 or 10 max, but I'm open to suggestions)\n\n\n\nVarun Thacker, Mark Miller let me know what you think ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16460289"
        },
        {
            "date": "2018-05-02T17:32:38+0000",
            "content": "Cool, looks like the right approach.\n\nConnectException\n\nWhat's the logic for removing the retry on that?\n\nI plan to reduce the number of retries\n\nI think something like 3 is good, I think that is what we use at the HttpClient level. ",
            "author": "Mark Miller",
            "id": "comment-16461353"
        },
        {
            "date": "2018-05-02T18:20:46+0000",
            "content": "What's the logic for removing the retry on that?\nNot removing, ConnectException is a SocketException so it should be retried. Things like \"broken pipe\" are SocketExceptions and I think it should be fine to retry too. One thing though, I noticed that in SolrCmdDistributorTest there is a test case to explicitly validate that we don't retry on SocketException. I'm not sure if this was done with SocketException intentionally (because there is something I'm missing about this error case) or if this is just an example of exception was was not retried on. \nI think something like 3 is good\nThat was my original plan too. But then I was looking at the ChaosMonkey logs and the amount of success after retries increased a lot in retries 5 to 10. I know this is just a synthetic situation but it's the best I have now. I'm thinking also in terms of time spent in retries, we wait 500 ms between retries, and 2.5 secs doesn't sound too bad if the consequence is saving Solr from a recovery. The impact on the other hand is slower updates in cases of single replicas being slow/faulty. Maybe this should be made configurable? ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16461455"
        },
        {
            "date": "2018-05-02T18:48:07+0000",
            "content": "But then I was looking at the ChaosMonkey logs and the amount of success after retries increased a lot in retries 5 to 10.\n\nYeah, okay, it's probably waiting for failover. I guess that is fine. That is probably how it went so high to begin with - allowing the forward to leader requests to wait for a new leader.\n\nI'm not sure if this was done with SocketException intentionally\n\nYes it was, because it can happen mid request and we don't know if the request failed or succeeded. Given we are counting on versions for retry though, this actually shouldnt matter, so that should be fine. ",
            "author": "Mark Miller",
            "id": "comment-16461487"
        },
        {
            "date": "2018-05-02T19:00:58+0000",
            "content": "Yes it was, because it can happen mid request \nAh! Good point. So we probably still don't to retry on those for the forwards, but we are OK with retrying on the FROMLEADER requests... ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16461499"
        },
        {
            "date": "2018-05-02T21:15:24+0000",
            "content": "Yeah, good catch - we are not versioned yet on forward. ",
            "author": "Mark Miller",
            "id": "comment-16461629"
        },
        {
            "date": "2018-05-02T22:26:45+0000",
            "content": "Yeah, now I remember why forward has a retry and why it was so high - same issue, to survive chaos monkey tests, even when you run them longer and if you run them over and over. So at least for the forwarding, I wouldn't lower it much without good confidence with beasting chaos monkey tests running a good amount of time (default test run times are somewhat low).\n\nBasically, update forwarding to the leader allows the cloud client to fall back to sending to non leaders and get held up rather than having those updates fail and forcing the user to resolve it. Perhaps the client should just block updates itself for a while waiting to see a leader - but then it has to have kind of special logic - right now even a php client could take advantage of this by just falling back to sending updates to non leaders while failover happens.\n\nI have no problem with updates from leader to replica retrying less.\n ",
            "author": "Mark Miller",
            "id": "comment-16461686"
        },
        {
            "date": "2018-05-04T19:50:07+0000",
            "content": "Uploaded a new patch to https://reviews.apache.org/r/66967/ ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16464325"
        },
        {
            "date": "2018-05-04T23:33:37+0000",
            "content": "So recently I've been seeing this problem in this form:\n\n\n\tThe replica get's a ReadPendingException from Jetty\u00a0\n\ndate time WARN [qtp768306356-580185] ? (:) - \njava.nio.channels.ReadPendingException: null\nat org.eclipse.jetty.io.FillInterest.register(FillInterest.java:58) ~[jetty-io-9.4.8.v20171121.jar:9.4.8.v20171121]\nat org.eclipse.jetty.io.AbstractEndPoint.fillInterested(AbstractEndPoint.java:353) ~[jetty-io-9.4.8.v20171121.jar:9.4.8.v20171121]\n\n\n\n\tThe leader keeps waiting till socket timeout and then get's a socket timeout exception and put's the replica into recovery\n\n\n\nSo I took\u00a0Tom\u00e1s latest patch and added SocketTimeoutException to the isRetriableException check.\u00a0\n\nQ: What all exceptions should we retry on? Currently in the patch we have\u00a0SocketException /\u00a0NoHttpResponseException\n\n\u00a0\n\nOnce I added SocketTimeoutException as a\u00a0retriable exception , I then set the socket timeout to 100ms and sent updates to manually test if Solr's retrying correctly . To my surprise I was never able to hit a socket timeout exception . After some debugging here's why\n\nIn ConcurrentUpdateSolrClient we do this\n\norg.apache.http.client.config.RequestConfig.Builder requestConfigBuilder = HttpClientUtil.createDefaultRequestConfigBuilder();\nif (soTimeout != null) {\n  requestConfigBuilder.setSocketTimeout(soTimeout);\n}\nif (connectionTimeout != null) {\n  requestConfigBuilder.setConnectTimeout(connectionTimeout);\n}\n\nmethod.setConfig(requestConfigBuilder.build());\n\nSo\u00a0createDefaultRequestConfigBuilder doesn't respect the timeout set in solr.xml and uses a default of 60 seconds.\n\nI debugged the code and if we simply remove these lines then the http-client will use the default requestConfig which Solr creates with the settings specified from the solr.xml file.\u00a0\n\nMark : Do you\u00a0remember the motivation for overriding the defaults from update shard handlers httpclient and explicitly specifying a\u00a0RequestConfig in CUSC? Happy to track this in a separate Jira\u00a0 ",
            "author": "Varun Thacker",
            "id": "comment-16464502"
        },
        {
            "date": "2018-05-05T00:33:10+0000",
            "content": "I believe my current patch breaks the \"minRf\" behavior. I'll take a look and add a test for that ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16464533"
        },
        {
            "date": "2018-05-05T00:47:03+0000",
            "content": "Varun Thacker, when we update the httpclient we could no longer set things we did after construction - but our client api let you change these settings on the fly (I see those methods are deprecated now - good). So this was to not break that. I think if timeouts are null, we need to try and pull them from the httpclient? ",
            "author": "Mark Miller",
            "id": "comment-16464539"
        },
        {
            "date": "2018-05-05T01:35:25+0000",
            "content": "\u00a0I think if timeouts are null, we need to try and pull them from the httpclient?\nI followed the code and if we don't set the timeout on the HttpPost request by setting a request config , it will use the default request config. In our case we set the default request config while creating the httpclient in HttpClientUtil#setupBuilder so it will use the values defined in the solr.xml file . I'll file a separate Jira right now\n\nRequestConfig requestConfig = requestConfigBuilder.build();\n\nHttpClientBuilder retBuilder = builder.setDefaultRequestConfig(requestConfig);\n ",
            "author": "Varun Thacker",
            "id": "comment-16464557"
        },
        {
            "date": "2018-05-05T01:53:40+0000",
            "content": "if we don't set the timeout on the HttpPost request by setting a request config\n\nCool - the tricky part is, if only one of the properties of the two is overridden on the client itself on the fly, we still want to pick up the default for the other one. ",
            "author": "Mark Miller",
            "id": "comment-16464563"
        },
        {
            "date": "2018-05-08T04:09:07+0000",
            "content": "I updated the CR with a new patch. Added a test for minRf, but this is more deeply tested in ReplicationFactorTest (that test now takes longer because of the retries. I'm thinking in either making the wait time configurable or modify it for test purposes only). ReplicationFactorTest is marked as @BadApple pointing to SOLR-6944, this retry logic will probably fix that one. I haven't seen failures of that test so far.\nThere is one nocommit in the code, I'm wondering if we want to keep the retries for DBQs. I'm thinking in setting the retry count for DBQs to 0, since those are not versioned AFAIK.\nAnother thing I noticed is that we sleep after each error retried (so if we need to retry two requests to two hosts, we sleep before the first request, and sleep before the second one). This seems odd, I think we want to sleep before retrying a batch of errors. I won't be changing this here though, I'll create a new Jira for that.\nI'll be running some tests with the current patch, feel free to review and let me know if you have any thoughts ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16466833"
        },
        {
            "date": "2018-05-08T04:25:28+0000",
            "content": "Yeah, I'd be hesitant to add retries to DBQ without input from Yonik Seeley. ",
            "author": "Mark Miller",
            "id": "comment-16466841"
        },
        {
            "date": "2018-05-08T04:46:34+0000",
            "content": "Patch looks nice Tom\u00e1s Fern\u00e1ndez L\u00f6bbe - very clean. ",
            "author": "Mark Miller",
            "id": "comment-16466856"
        },
        {
            "date": "2018-05-11T19:08:08+0000",
            "content": "I was looking into DBQ a bit more. It is actually versioned (which I didn\u2019t know) so in theory we could retry too.  But there is another issue with them. In the doDeleteByQuery method we have this comment:\n\n    // NONE: we are the first to receive this deleteByQuery\n    //       - it must be forwarded to the leader of every shard\n    // TO:   we are a leader receiving a forwarded deleteByQuery... we must:\n    //       - block all updates (use VersionInfo)\n    //       - flush *all* updates going to our replicas\n    //       - forward the DBQ to our replicas and wait for the response\n    //       - log + execute the local DBQ\n    // FROM: we are a replica receiving a DBQ from our leader\n    //       - log + execute the local DBQ\n\n\nHowever, that\u2019s not what the code is doing now, not in the leader at least. We block, run locally (like we do with other operations), unblock, then we send the DBQ to followers by calling cmdDistrib.distribDelete(cmd, replicas, params, false, rollupReplicationTracker, leaderReplicationTracker);, and then call the cmdDistrib.blockAndDoRetries();.  The problem with that is that inside the cmdDistrib things can be reordered (and even more now since we are adding retries to updates), the DBQ needs to be the last request to be executed otherwise it can miss docs.\n\nI think that call to cmdDistrib.distribDelete needs to be synchronous=true, that way we\u2019ll flush (and retry) all updates before sending the DBQ, then send the DBQ and flush, and then continue. I\u2019ll try to work on a test for that, but some feedback would be great. Yonik Seeley ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16472495"
        },
        {
            "date": "2018-05-11T19:15:33+0000",
            "content": "Yeah, it's versioned,\u00a0that is being counted on in\u00a0SOLR-12305 as well. My bigger concern with retires would be stuff we didn't think - I know it's much tricker than the other distributed commands in terms of what ramifications changes have.\n\n\u00a0bq.\u00a0and then call the\u00a0cmdDistrib.blockAndDoRetries();\n\nWhy don't we call that first? Isn't this just the same case as a commit? Commit has to blockAndDoRetries first, to make sure it applies to all previous updates. ",
            "author": "Mark Miller",
            "id": "comment-16472501"
        },
        {
            "date": "2018-08-04T17:22:05+0000",
            "content": "Uploaded a new patch. In the latest patch I'm calling blockAndDoRetries before distributing the DBQ. I Also reduced the number of retries on standard requests from 5 to 3 (I did some experimentation I saw that the majority of requests either succeed or fail after the first couple requests). I'll do another check at the ChaosMonkey tests and commit this if I see no errors. ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16569247"
        },
        {
            "date": "2018-08-04T22:34:01+0000",
            "content": "\n\n\n  +1 overall \n\n\n\n\n\n\n\n\n\n Vote \n Subsystem \n Runtime \n Comment \n\n\n\u00a0\n\u00a0\n\u00a0\n  Prechecks  \n\n\n +1 \n  test4tests  \n   0m  0s \n  The patch appears to include 3 new or modified test files.  \n\n\n\u00a0\n\u00a0\n\u00a0\n  master Compile Tests  \n\n\n +1 \n  compile  \n   1m  9s \n  master passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Patch Compile Tests  \n\n\n +1 \n  compile  \n   1m  2s \n  the patch passed  \n\n\n +1 \n  javac  \n   1m  2s \n  the patch passed  \n\n\n +1 \n  Release audit (RAT)  \n   1m  2s \n  the patch passed  \n\n\n +1 \n  Check forbidden APIs  \n   1m  2s \n  the patch passed  \n\n\n +1 \n  Validate source patterns  \n   1m  2s \n  the patch passed  \n\n\n\u00a0\n\u00a0\n\u00a0\n  Other Tests  \n\n\n +1 \n  unit  \n  37m 51s \n  core in the patch passed.  \n\n\n  \n   \n  41m 59s \n   \n\n\n\n\n\n\n\n\n\n Subsystem \n Report/Notes \n\n\n JIRA Issue \n SOLR-11881 \n\n\n JIRA Patch URL \n https://issues.apache.org/jira/secure/attachment/12934388/SOLR-11881.patch \n\n\n Optional Tests \n  compile  javac  unit  ratsources  checkforbiddenapis  validatesourcepatterns  \n\n\n uname \n Linux lucene1-us-west 4.4.0-130-generic #156~14.04.1-Ubuntu SMP Thu Jun 14 13:51:47 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux \n\n\n Build tool \n ant \n\n\n Personality \n /home/jenkins/jenkins-slave/workspace/PreCommit-SOLR-Build/sourcedir/dev-tools/test-patch/lucene-solr-yetus-personality.sh \n\n\n git revision \n master / b33df4e \n\n\n ant \n version: Apache Ant(TM) version 1.9.3 compiled on July 24 2018 \n\n\n Default Java \n 1.8.0_172 \n\n\n  Test Results \n https://builds.apache.org/job/PreCommit-SOLR-Build/159/testReport/ \n\n\n modules \n C: solr/core U: solr/core \n\n\n Console output \n https://builds.apache.org/job/PreCommit-SOLR-Build/159/console \n\n\n Powered by \n Apache Yetus 0.7.0   http://yetus.apache.org \n\n\n\n\n\n\nThis message was automatically generated.\n ",
            "author": "Lucene/Solr QA",
            "id": "comment-16569309"
        },
        {
            "date": "2018-08-06T16:17:18+0000",
            "content": "I've been running the ChaosMonkeyNothingIsSafeTest over the weekend and saw no failures (after applying the patch to SOLR-12626). I'll commit this later today ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16570421"
        },
        {
            "date": "2018-08-06T22:57:24+0000",
            "content": "Commit c338cf61e7baba4908c31e02beda47ae3e201752 in lucene-solr's branch refs/heads/master from Tomas Fernandez Lobbe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c338cf6 ]\n\nSOLR-11881: Retry update requests from leaders to followers ",
            "author": "ASF subversion and git services",
            "id": "comment-16570893"
        },
        {
            "date": "2018-08-06T22:58:59+0000",
            "content": "Commit 7519e172ce3a3d44dd5b55ce7c74bdf74614f0a8 in lucene-solr's branch refs/heads/branch_7x from Tomas Fernandez Lobbe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=7519e17 ]\n\nSOLR-11881: Retry update requests from leaders to followers ",
            "author": "ASF subversion and git services",
            "id": "comment-16570894"
        },
        {
            "date": "2018-08-09T21:39:13+0000",
            "content": "Closing out this issue ",
            "author": "Varun Thacker",
            "id": "comment-16575418"
        },
        {
            "date": "2018-08-10T09:14:00+0000",
            "content": "Commit 9f554f6de0420e39128ce366e6e8e028c2ff66e7 in lucene-solr's branch refs/heads/jira/http2 from Cao Manh Dat\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=9f554f6 ]\n\nResolved merge conflict caused by SOLR-11881 ",
            "author": "ASF subversion and git services",
            "id": "comment-16576003"
        },
        {
            "date": "2018-08-10T09:54:13+0000",
            "content": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe It will be a great help if you can take a look at jira/http2 branch and validate my merge. ",
            "author": "Cao Manh Dat",
            "id": "comment-16576037"
        },
        {
            "date": "2018-08-10T22:26:49+0000",
            "content": "LGTM Cao Manh Dat ",
            "author": "Tom\u00e1s Fern\u00e1ndez L\u00f6bbe",
            "id": "comment-16576928"
        }
    ]
}