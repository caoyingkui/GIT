{
    "id": "LUCENE-1072",
    "title": "NullPointerException during indexing in DocumentsWriter$ThreadState$FieldData.addPosition",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Bug",
        "fix_versions": [
            "2.3"
        ],
        "affect_versions": "2.3",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "In my case during indexing sometimes appear documents with unusually large \"words\" - text-encoded images in fact.\nAttempt to add document that contains field with such token produces java.lang.IllegalArgumentException:\njava.lang.IllegalArgumentException: term length 37944 exceeds max term length 16383\n        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.addPosition(DocumentsWriter.java:1492)\n        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.invertField(DocumentsWriter.java:1321)\n        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.processField(DocumentsWriter.java:1247)\n        at org.apache.lucene.index.DocumentsWriter$ThreadState.processDocument(DocumentsWriter.java:972)\n        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:2202)\n        at org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:2186)\n        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1432)\n        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1411)\n\nThis is expected, exception is caught and ignored. The problem is that after this IndexWriter becomes somewhat corrupted and subsequent attempts to add documents to the index fail as well, this time with NPE:\njava.lang.NullPointerException\n        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.addPosition(DocumentsWriter.java:1497)\n        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.invertField(DocumentsWriter.java:1321)\n        at org.apache.lucene.index.DocumentsWriter$ThreadState$FieldData.processField(DocumentsWriter.java:1247)\n        at org.apache.lucene.index.DocumentsWriter$ThreadState.processDocument(DocumentsWriter.java:972)\n        at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:2202)\n        at org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:2186)\n        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1432)\n        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1411)\n\nThis is 100% reproducible.",
    "attachments": {
        "LUCENE-1072.take2.patch": "https://issues.apache.org/jira/secure/attachment/12370935/LUCENE-1072.take2.patch",
        "LUCENE-1072.patch": "https://issues.apache.org/jira/secure/attachment/12370634/LUCENE-1072.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2007-11-29T23:16:59+0000",
            "content": "I think this is related: https://issues.apache.org/jira/browse/LUCENE-1052\n\nSee http://www.gossamer-threads.com/lists/lucene/java-dev/54371 ",
            "author": "Grant Ingersoll",
            "id": "comment-12546936"
        },
        {
            "date": "2007-11-30T09:58:10+0000",
            "content": "Attached patch.  I plan to commit in a day or two.\n\nI added a unit test showing that indeed DocumentsWriter becomes\nunusable once it's hit a \"too long term\", then fixed the issue so the\nunit test passes.\n\nNow, if we encounter too-long terms in the doc we skip those terms but\ncontinue indexing the other acceptable terms from the doc, then throw\nthe IllegalArgumentException at the end after processing the full\ndocument.  So it's now \"ok\" to catch & ignore this exception though\nclearly in general you should address its root cause so you don't\naccidentally pollute your term dictionary (see LUCENE-1052, as Grant\nsuggested, once that happens!). ",
            "author": "Michael McCandless",
            "id": "comment-12547059"
        },
        {
            "date": "2007-12-03T10:10:10+0000",
            "content": "I just committed this.  Thanks for reporting it Alexei! ",
            "author": "Michael McCandless",
            "id": "comment-12547778"
        },
        {
            "date": "2007-12-04T06:56:46+0000",
            "content": "I'm seeing a similar issue when TokenStream.next() throws an\nIOException (or a RuntimeException). The DocumentsWriter is\nthereafter not usable anymore, i. e. subsequent calls of \naddDocument()  fail with a NullPointerException.\n\nI added this test to TestIndexWriter which shows the problem:\n\n  public void testExceptionFromTokenStream() throws IOException {\n    RAMDirectory dir = new RAMDirectory();\n    IndexWriter writer = new IndexWriter(dir, new Analyzer() {\n\n      public TokenStream tokenStream(String fieldName, Reader reader) {\n        return new TokenFilter(new StandardTokenizer(reader)) {\n          private int count = 0;\n\n          public Token next() throws IOException {\n            if (count++ == 5) {\n              throw new IOException();\n            }\n            return input.next();\n          }\n        };\n      }\n\n    }, true);\n\n    Document doc = new Document();\n    String contents = \"aa bb cc dd ee ff gg hh ii jj kk\";\n    doc.add(new Field(\"content\", contents, Field.Store.NO,\n        Field.Index.TOKENIZED));\n    try {\n      writer.addDocument(doc);\n      fail(\"did not hit expected exception\");\n    } catch (Exception e) {\n    }\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"aa bb cc dd\", Field.Store.NO,\n        Field.Index.TOKENIZED));\n    writer.addDocument(doc);\n\n    // Make sure we can add another normal document\n    doc = new Document();\n    doc.add(new Field(\"content\", \"aa bb cc dd\", Field.Store.NO,\n        Field.Index.TOKENIZED));\n    writer.addDocument(doc);\n\n    writer.close();\n  }\n\n\n ",
            "author": "Michael Busch",
            "id": "comment-12548152"
        },
        {
            "date": "2007-12-04T10:32:07+0000",
            "content": "OK, I added that as a test case (to TestIndexWriter), and then fixed\nit.  Attached patch.  I plan to commit in 1 or 2 days.  Thanks\nMichael!\n\nThis was happening during DW.abort(), which was being called on an\nunhandled exception to clear all documents added since the last flush.\nIt was incorrectly recycling a null Posting instance.\n\nI've also tightened when abort() is called to only those places that\nactually require it.  A failure in the tokenization of one document\nshould not discard previously indexed documents but not-yet-flushed\ndocuments.  So I added asserts to the test case to verify that. ",
            "author": "Michael McCandless",
            "id": "comment-12548206"
        },
        {
            "date": "2007-12-04T22:13:11+0000",
            "content": "Thanks for the quick fix, Mike. All unit tests, incl. the new one, pass.\n\nI also added this patch to the Lucene version in our app and it works\nfine now. So even after the TokenStream throws a RuntimeException\nthe DocsWriter is still usable for subsequent docs.\n\n+1 for committing this soon!! ",
            "author": "Michael Busch",
            "id": "comment-12548435"
        },
        {
            "date": "2007-12-04T22:23:36+0000",
            "content": "OK, thanks for testing it!  I will commit shortly... ",
            "author": "Michael McCandless",
            "id": "comment-12548440"
        },
        {
            "date": "2007-12-04T22:30:30+0000",
            "content": "OK I just committed this! ",
            "author": "Michael McCandless",
            "id": "comment-12548443"
        }
    ]
}