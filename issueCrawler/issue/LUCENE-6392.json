{
    "id": "LUCENE-6392",
    "title": "Add offset limit to Highlighter's TokenStreamFromTermVector",
    "details": {
        "resolution": "Fixed",
        "affect_versions": "None",
        "components": [
            "modules/highlighter"
        ],
        "labels": "",
        "fix_versions": [
            "5.2"
        ],
        "priority": "Major",
        "status": "Closed",
        "type": "Improvement"
    },
    "description": "The Highlighter's TokenStreamFromTermVector utility, typically accessed via TokenSources, should have the ability to filter out tokens beyond a configured offset. There is a TODO there already, and this issue addresses it.  New methods in TokenSources now propagate a limit.\n\nThis patch also includes some memory saving optimizations, to be described shortly.",
    "attachments": {
        "LUCENE-6392_highlight_term_vector_maxStartOffset.patch": "https://issues.apache.org/jira/secure/attachment/12709274/LUCENE-6392_highlight_term_vector_maxStartOffset.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-14394747",
            "author": "David Smiley",
            "date": "2015-04-03T17:38:13+0000",
            "content": "(Patch attached).\nElaborating on the description:\n\nThis patch includes a tweak to the TokenLL[] array size initialization to consider this new limit when guessing a good size.\n\nThis patch includes memory saving optimizations to the information it accumulates.  Before the patch, each TokenLL had a char[], so there were a total of 2 objects per token (including the token itself).  Now I use a shared CharsRefBuilder with a pointer & length into it, so there's just 1 object now, plus byte savings by avoiding a char[] header.  I also reduced the bytes needed for a TokenLL instance from 40 to 32.  It does assume that the char offset delta (endOffset - startOffset) can fit within a short, which seems like a reasonable assumption to me. For safety I guard against overflow and substitute Short.MAX_VALUE.\n\nFinally, to encourage users to supply a limit (even if \"-1\" to mean no limit), I decided to deprecate many of the methods in TokenSources for new ones that include a limit parameter.  But for those methods that fall-back to a provided Analyzer, I have to wonder now if it makes sense for these methods to filter the analyzers.  I think it does \u2013 if you want to limit the tokens then it shouldn't matter where you got them from \u2013 you want to limit them.  I haven't added that but I'm looking for feedback first. "
        },
        {
            "id": "comment-14394749",
            "author": "David Smiley",
            "date": "2015-04-03T17:40:21+0000",
            "content": "Oh and before I forget, obviously Solr's DefaultSolrHighlighter should propagate the offset. It's not in this patch to avoid interference with my other WIP. "
        },
        {
            "id": "comment-14504307",
            "author": "David Smiley",
            "date": "2015-04-21T04:34:43+0000",
            "content": "I created LUCENE-6445 to for a broader refactor/simplification of TokenSources.java.  I don't think this issue here should bother with modifications to that class; it can be limited to TokenStreamFromTermVector.  I plan to commit this issue in ~24 hours without any TokenSources modifications. "
        },
        {
            "id": "comment-14507955",
            "author": "ASF subversion and git services",
            "date": "2015-04-22T21:35:17+0000",
            "content": "Commit 1675504 from David Smiley in branch 'dev/trunk'\n[ https://svn.apache.org/r1675504 ]\n\nLUCENE-6392 Highlighter: add maxStartOffset (and other memory saving efficiencies) to TokenStreamFromTermVector.\nDelaying TokenSources changes for LUCENE-6445. "
        },
        {
            "id": "comment-14507984",
            "author": "ASF subversion and git services",
            "date": "2015-04-22T21:48:13+0000",
            "content": "Commit 1675505 from David Smiley in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1675505 ]\n\nLUCENE-6392 Highlighter: add maxStartOffset (and other memory saving efficiencies) to TokenStreamFromTermVector.\nDelaying TokenSources changes for LUCENE-6445. "
        },
        {
            "id": "comment-14586823",
            "author": "Anshum Gupta",
            "date": "2015-06-15T21:43:01+0000",
            "content": "Bulk close for 5.2.0. "
        }
    ]
}