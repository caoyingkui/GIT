{
    "id": "SOLR-2347",
    "title": "Use InputStream and not Reader for XML parsing",
    "details": {
        "affect_versions": "None",
        "status": "Open",
        "fix_versions": [
            "4.9",
            "6.0"
        ],
        "components": [
            "contrib - DataImportHandler"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Unresolved"
    },
    "description": "Followup to SOLR-96:\n\nSolr mostly uses java.io.Reader and passes this Reader to the XML parser. According to XML spec, a XML file should be initially seen as a binary stream with a default charset of UTF-8 or another charset given by the network protocol (like Content-Type header in HTTP). But very important, this default charset is only a \"hint\" to the parser - mandatory is the charset from the XML header processing inctruction. Because of this, the parser must be able to change the charset when reading the XML headers (possibly also when seeing BOM markers). This is not possible if the XML parser gets a java.io.Reader instead of java.io.InputStreams. SOLR-96 already fixed this for the XmlUpdateRequestHandler and the DocumentAnalysisRequestHandler. This issue should fix the rest to be conforming to XML-spec (open schema.xml and config.xml as InputStream not Reader and others).\n\nThis change would not break anything in Solr (perhaps only backwards compatibility in the API), as the default used by XML parsers is UTF-8.",
    "attachments": {
        "SOLR-2347.patch": "https://issues.apache.org/jira/secure/attachment/12563169/SOLR-2347.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Uwe Schindler",
            "id": "comment-12990132",
            "date": "2011-02-03T15:32:49+0000",
            "content": "To again post the most important info from XML spec how to handle charset detection:\n\n\nXML parsers by definition only take a byte stream and a charset hint and use the XML 1.0 spec to determince the charset: http://www.w3.org/TR/REC-xml/#charencoding and http://www.w3.org/TR/REC-xml/#sec-guessing "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12990505",
            "date": "2011-02-04T10:23:18+0000",
            "content": "The same problem applies to DIH, this may be the problem behind SOLR-2346!\n\nDataSource in DIH always is generified to <Reader>, so all input from files or URLs are read using a charset. When you want to stream XML data using DIH, the XML parser is also unable to use the encoding as declared in the XML file itsself. In my opinion, all DataSources should simply be generified to DataSource<ContentStream>, which makes also a lot of code easier and the consumer can choose between binary or chars. "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-12990521",
            "date": "2011-02-04T10:48:06+0000",
            "content": "I checked Schema and SolrConfig loading: It's fine, no changes needed. It uses InputStream to load. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-12990564",
            "date": "2011-02-04T14:15:37+0000",
            "content": "Thanks for looking into this Uwe! "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13006285",
            "date": "2011-03-13T22:20:59+0000",
            "content": "This now only affects DIH anymore.\n\nDIH have to be changed that all the base classes that open files, read from network don't use Readers but InputStreams. This is easy to do, but breaks backwards (which is no problem as DIH is contrib and experimental). "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13006286",
            "date": "2011-03-13T22:44:22+0000",
            "content": "which is no problem as DIH is contrib and experimental\n\n+1 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13043802",
            "date": "2011-06-03T16:47:09+0000",
            "content": "Bulk move 3.2 -> 3.3 "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13106493",
            "date": "2011-09-16T14:51:19+0000",
            "content": "3.4 -> 3.5 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13235078",
            "date": "2012-03-21T21:57:23+0000",
            "content": "Won't fix for 3.6, in 4.0 we should move DIH from Reader to InputStream "
        },
        {
            "author": "Hoss Man",
            "id": "comment-13412144",
            "date": "2012-07-11T22:26:08+0000",
            "content": "bulk fixing the version info for 4.0-ALPHA and 4.0 all affected issues have \"hoss20120711-bulk-40-change\" in comment "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13429750",
            "date": "2012-08-07T03:42:42+0000",
            "content": "rmuir20120906-bulk-40-change "
        },
        {
            "author": "Robert Muir",
            "id": "comment-13452186",
            "date": "2012-09-10T17:41:57+0000",
            "content": "moving all 4.0 issues not touched in a month to 4.1 "
        },
        {
            "author": "James Dyer",
            "id": "comment-13504989",
            "date": "2012-11-27T21:58:40+0000",
            "content": "Uwe,  Does your concern here entirely have to do with when DIH is indexing XML files? "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13505056",
            "date": "2012-11-27T23:24:18+0000",
            "content": "It is not only XML files. In general, the encoding information of textual content should be determined by the parser. E.g. if you write a DIH instance reading from a network stream, the encoding might be defined by the headers (e.g. HTTP). In the case of XML it is defined by both headers and the data itsself (<?xml> header). Data import handler should in this case work with InputStreams, so the encoding could be determined later (e.g. when reading unknown text files, e.g. ICU4J could autodetect the encoding from language, etc.). This would also fit DIH better with TIKA processing.\n\nMy proposal is to let DIH take InputStreams and let the encoding be determined in a later stage of processing. "
        },
        {
            "author": "James Dyer",
            "id": "comment-13543340",
            "date": "2013-01-03T22:08:29+0000",
            "content": "Here is an attempt to fix DIH.  But we cannot make DataSource always deal in InputStreams.  JDBCDataSource, for instance, is entirely different and returns Maps. \n\nOne difficult thing here is FieldReaderDataSource which can take a java.sql.Clob and pass it down to an XML or other text processor.  Clob#getCharacterStream() returns a Reader, not an Inputstream.  So I ended up having XpathEntityProcessor take a DataSource<?> and checking instanceof for Reader or InputStream.\n\nAll of this makes me wonder if having DataSource separate from EntityProcessor is really good design here.  The EntityProcessors are very much married to their DataSources and you really can't mix-n-match very much as the conceptualization would lend one to believe... "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13717255",
            "date": "2013-07-23T18:47:38+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13971278",
            "date": "2014-04-16T12:57:38+0000",
            "content": "Move issue to Solr 4.9. "
        }
    ]
}