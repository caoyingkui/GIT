{
    "id": "SOLR-8034",
    "title": "If minRF is not satisfied, leader should not put replicas in recovery",
    "details": {
        "components": [
            "SolrCloud"
        ],
        "type": "Improvement",
        "labels": "",
        "fix_versions": [
            "5.4",
            "6.0"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "If the minimum replication factor parameter (minRf) in a solr update request is not satisfied \u2013 i.e. if the update was not successfully applied on at least n replicas where n >= minRf \u2013 the shard leader should not put the failed replicas in \"leader initiated recovery\" and the client should retry the update instead.\n\nThis is so that in the scenario were minRf is not satisfied, the failed replicas can still be eligible to become a leader in case of leader failure, since in the client's perspective this update did not succeed.\n\nThis came up from a network partition scenario where the leader becomes sectioned off from its two followers, but they all could still talk to zookeeper. The partitioned leader set its two followers as in leader initiated recovery, so we couldn't just kill off the partitioned node and have a follower take over leadership. For a minRf=1 case, this is the correct behavior because the partitioned leader would have accepted updates that the followers don't have, and therefore we can't switch leadership or we'd lose those updates. However, in the case of minRf=2, solr never accepted any update in the client's point of view, so in fact the partitioned leader doesn't have any accepted update that the followers don't have, and therefore the followers should be eligible to become leaders. Thus, I'm proposing modifying the leader initiated recovery logic to not put the followers in recovery if the minRf parameter is present and is not satisfied.",
    "attachments": {
        "SOLR-8034.patch": "https://issues.apache.org/jira/secure/attachment/12755208/SOLR-8034.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2015-09-11T21:06:48+0000",
            "author": "Jessica Cheng Mallet",
            "content": "Tim Potter This is what we discussed a while ago. Will you please give it a look? Thanks! ",
            "id": "comment-14741536"
        },
        {
            "date": "2015-09-11T21:36:57+0000",
            "author": "Anshum Gupta",
            "content": "I'm kind of split on this as the replica here would be out of sync from the leader and would never know about it, increasing the odds of inconsistency when the client doesn't handle it the right way i.e. it kind of self-heals at this point, and that would stop happening.\n\nAt the same time, I kind of like the idea as it allows for failover in the case you've mentioned above.\n\nFor the patch, LGTM. Can you also test that the replica is down, right after the first network partition, but before you send the document? That would ensure that the replication factor is 2, because of the partition and not due to another variable.\n\nAlso, can you fix the comment for assertion failure below? You're expecting 2 non-leader replicas and not 2 replicas :\n\n+    List<Replica> notLeaders =\n+        ensureAllReplicasAreActive(testCollectionName, \"shard1\", 1, 3, maxWaitSecsToSeeAllActive);\n+    assertTrue(\"Expected 2 replicas for collection \" + testCollectionName\n+            + \" but found \" + notLeaders.size() + \"; clusterState: \"\n+            + printClusterStateInfo(testCollectionName),\n+        notLeaders.size() == 2);\n\n ",
            "id": "comment-14741577"
        },
        {
            "date": "2015-09-15T06:39:09+0000",
            "author": "Tim Potter",
            "content": "Hi Jessica Cheng Mallet.  I don't recall talking with you - perhaps you meant someone else? ",
            "id": "comment-14744936"
        },
        {
            "date": "2015-09-15T16:44:58+0000",
            "author": "Jessica Cheng Mallet",
            "content": "Anshum Gupta, I fixed the comment for the assertion, but I didn't add the test that the replica is down after the first network partition, because the point is that the replica will not realize it's down on its own since the partition is between the leader and the replica, not between the replica and zookeeper \u2013 so it won't be set to down until the leader tries to forward the document to it and fails, and then set it in leader-initiated-recovery.\n\nTim Potter, we discussed this in ticket 4072. ",
            "id": "comment-14745706"
        },
        {
            "date": "2015-09-15T16:53:18+0000",
            "author": "Jessica Cheng Mallet",
            "content": "Ah, and regarding\n\n\nI'm kind of split on this as the replica here would be out of sync from the leader and would never know about it, increasing the odds of inconsistency when the client doesn't handle it the right way i.e. it kind of self-heals at this point, and that would stop happening.\nI'd hope that if the user is explicitly using minRf that they handle it the right way (i.e. retry if minRf isn't achieved). The contract would be if the request fails, it needs to be retried or we can possibly see inconsistent state. I think this is true currently in a normal update if the forwarded parallel update to the followers succeeds but somehow it fails on the leader--a failure would be returned to the user but the update could be present on the followers.  ",
            "id": "comment-14745722"
        },
        {
            "date": "2015-09-15T17:00:26+0000",
            "author": "Timothy Potter",
            "content": "Tim Potter oops! Jessica was actually pinging me thelabdude (same name, different handle)\n\nJessica Cheng Mallet this looks good to me ... nice test case! Also, I agree that if the client is using minRf then it is their responsibility to handle the response correctly. Previously, we talked about throwing an exception instead of just returning to value for the client to interpret and maybe that makes it more explicit that clients MUST handle minRf not being achieved.  We should handle that in another ticket though. ",
            "id": "comment-14745735"
        },
        {
            "date": "2015-09-15T17:01:10+0000",
            "author": "Anshum Gupta",
            "content": "Thanks for fixing the assert.\n\nreplica will not realize it's down on its own since the partition is between the leader and the replica, not between the replica and zookeeper \u2013 so it won't be set to down until the leader tries to forward the document to it and fails\n\nRight, should've realized that.\n\nAlso, about my opinion being split, I wasn't in on this, but I thought more and it makes more sense to go with this.\n\nThanks Jessica Cheng Mallet . LGTM overall, I'll commit this. ",
            "id": "comment-14745737"
        },
        {
            "date": "2015-09-15T17:06:23+0000",
            "author": "Jessica Cheng Mallet",
            "content": "Oops, sorry! Didn't know there's another Tim Potter.  ",
            "id": "comment-14745746"
        },
        {
            "date": "2015-09-15T20:39:15+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1703289 from Anshum Gupta in branch 'dev/trunk'\n[ https://svn.apache.org/r1703289 ]\n\nSOLR-8034: Leader no longer puts replicas in recovery in case of a failed update, when minRF isn't achieved. ",
            "id": "comment-14746127"
        },
        {
            "date": "2015-09-15T22:06:29+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1703302 from Anshum Gupta in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1703302 ]\n\nSOLR-8034: Leader no longer puts replicas in recovery in case of a failed update, when minRF isn't achieved. (merge from trunk) ",
            "id": "comment-14746353"
        },
        {
            "date": "2015-09-16T14:05:04+0000",
            "author": "Mark Miller",
            "content": "Previously, we talked about throwing an exception instead of just returning to value for the client to interpret and maybe that makes it more explicit that clients MUST handle minRf not being achieved. \n\nI still think that is how this should work. ",
            "id": "comment-14768979"
        },
        {
            "date": "2015-09-16T14:10:19+0000",
            "author": "Timothy Potter",
            "content": "Cool - I created SOLR-8062 to handle that change. ",
            "id": "comment-14768990"
        },
        {
            "date": "2015-09-16T15:18:53+0000",
            "author": "Anshum Gupta",
            "content": "Thanks Jessica Cheng Mallet. ",
            "id": "comment-14790539"
        },
        {
            "date": "2018-10-03T16:46:35+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 46f753d7c6df52c06d970a13d3b742310276f2ca in lucene-solr's branch refs/heads/master from Tomas Fernandez Lobbe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=46f753d ]\n\nSOLR-12767: Always include the achieved rf in the response\n\nThis commit deprecates the min_rf parameter. Solr now always includes the achieved replication\nfactor in the update requests (as if min_rf was always specified). Also, reverts the changes\nintroduced in SOLR-8034, replicas that don't ack an update will have to recover to prevent\ninconsistent shards. ",
            "id": "comment-16637181"
        },
        {
            "date": "2018-10-03T16:47:17+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 1854bb1ff8377390c1117dcfb8a35c6480977c21 in lucene-solr's branch refs/heads/branch_7x from Tomas Fernandez Lobbe\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=1854bb1 ]\n\nSOLR-12767: Always include the achieved rf in the response\n\nThis commit deprecates the min_rf parameter. Solr now always includes the achieved replication\nfactor in the update requests (as if min_rf was always specified). Also, reverts the changes\nintroduced in SOLR-8034, replicas that don't ack an update will have to recover to prevent\ninconsistent shards. ",
            "id": "comment-16637183"
        }
    ]
}