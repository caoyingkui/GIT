{
    "id": "LUCENE-2229",
    "title": "SimpleSpanFragmenter fails to start a new fragment",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/highlighter"
        ],
        "type": "Bug",
        "fix_versions": [
            "5.4.1",
            "5.5"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "SimpleSpanFragmenter fails to identify a new fragment when there is more than one stop word after a span is detected. This problem can be observed when the Query contains a PhraseQuery.\n\nThe problem is that the span extends toward the end of the TokenGroup. This is because waitForProps = positionSpans.get.end + 1; and position += posIncAtt.getPositionIncrement(); this generates a value of position greater than the value of waitForProps and (waitForPos == position) never matches.\n\nSimpleSpanFragmenter.java\n  public boolean isNewFragment() {\n    position += posIncAtt.getPositionIncrement();\n\n    if (waitForPos == position) {\n      waitForPos = -1;\n    } else if (waitForPos != -1) {\n      return false;\n    }\n\n    WeightedSpanTerm wSpanTerm = queryScorer.getWeightedSpanTerm(termAtt.term());\n\n    if (wSpanTerm != null) {\n      List<PositionSpan> positionSpans = wSpanTerm.getPositionSpans();\n\n      for (int i = 0; i < positionSpans.size(); i++) {\n        if (positionSpans.get(i).start == position) {\n          waitForPos = positionSpans.get(i).end + 1;\n          break;\n        }\n      }\n    }\n   ...\n\n\n\nAn example is provided in the test case for the following Document and the query \"all tokens\" followed by the words of a.\n\nDocument\n\"Attribute instances are reused for all tokens of a document. Thus, a TokenStream/-Filter needs to update the appropriate Attribute(s) in incrementToken(). The consumer, commonly the Lucene indexer, consumes the data in the Attributes and then calls incrementToken() again until it retuns false, which indicates that the end of the stream was reached. This means that in each call of incrementToken() a TokenStream/-Filter can safely overwrite the data in the Attribute instances.\"\n\n\nHighlighterTest.java\n public void testSimpleSpanFragmenter() throws Exception {\n\n    ...\n\n    doSearching(\"\\\"all tokens\\\"\");\n\n    maxNumFragmentsRequired = 2;\n    \n    scorer = new QueryScorer(query, FIELD_NAME);\n    highlighter = new Highlighter(this, scorer);\n\n    for (int i = 0; i < hits.totalHits; i++) {\n      String text = searcher.doc(hits.scoreDocs[i].doc).get(FIELD_NAME);\n      TokenStream tokenStream = analyzer.tokenStream(FIELD_NAME, new StringReader(text));\n\n      highlighter.setTextFragmenter(new SimpleSpanFragmenter(scorer, 20));\n\n      String result = highlighter.getBestFragments(tokenStream, text,\n          maxNumFragmentsRequired, \"...\");\n      System.out.println(\"\\t\" + result);\n\n    }\n  }\n\n\n\n\nResult\nare reused for <B>all</B> <B>tokens</B> of a document. Thus, a TokenStream/-Filter needs to update the appropriate Attribute(s) in incrementToken(). The consumer, commonly the Lucene indexer, consumes the data in the Attributes and then calls incrementToken() again until it retuns false, which indicates that the end of the stream was reached. This means that in each call of incrementToken() a TokenStream/-Filter can safely overwrite the data in the Attribute instances.\n\n\nExpected Result\nfor <B>all</B> <B>tokens</B> of a document",
    "attachments": {
        "LUCENE-2229.patch": "https://issues.apache.org/jira/secure/attachment/12451843/LUCENE-2229.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-01-21T14:44:23+0000",
            "content": "Test case to reproduce this error. ",
            "author": "Elmer Garduno",
            "id": "comment-12803306"
        },
        {
            "date": "2010-08-12T00:12:01+0000",
            "content": "This patch fixes the problem, and adds a test case. ",
            "author": "Elmer Garduno",
            "id": "comment-12897522"
        },
        {
            "date": "2015-12-28T20:40:32+0000",
            "content": "This is still a problem and the patch works (so says a user \"lukhnos\" on IRC) ",
            "author": "David Smiley",
            "id": "comment-15073132"
        },
        {
            "date": "2015-12-29T00:25:41+0000",
            "content": "Updated patch that works against the current trunk. ",
            "author": "Lukhnos Liu",
            "id": "comment-15073296"
        },
        {
            "date": "2015-12-29T00:31:29+0000",
            "content": "Chatted with David Smiley on #lucene-dev about this bug. This bug still exists in Lucene 5.x as well as the current trunk.\n\nI've updated Elmer Garduno's patch and am attaching it here. I tested it with the current trunk (r1721808) and the patch will fix the issue. I've been using the original patch in my day job (on Lucene 5.3.0), and the patch should also work in the 5.x branch.\n\nThe bug is a boundary condition that doesn't manifest itself all the time. For example, if you loop through the test case in the patch with different fragment lengths (say from 30-200), some of the short lengths don't show the issue (because the fragment may end at the first highlighted term when the requested length is short), but most of the lengths do. If you print out the isNewFragment() method in question, you'll find it looks like this when the bug shows up:\n\n\nterm: instances, position: 0, waitForPos: -1\nterm: reused, position: 2, waitForPos: -1\nterm: all, position: 4, waitForPos: -1\nterm: tokens, position: 5, waitForPos: -1\nterm: document, position: 8, waitForPos: 7  # uh oh\nterm: thus, position: 9, waitForPos: 7  # and all the following tokens are included in the fragment\nterm: tokenstream, position: 11, waitForPos: 7\nterm: filter, position: 12, waitForPos: 7\nterm: needs, position: 13, waitForPos: 7\nterm: update, position: 15, waitForPos: 7\n...\n\n\n\nRecall that the example document reads Attribute instances are reused for all tokens of a document. ... and \"of\" and \"a\" are stop words, and so there is a offset-by-2 gap between the positions of \"tokens\" and \"document\".\n\nFrom the printout, position is way over waitForPos from the token \"document\" on and therefore isNewFragment() never gets to start a new one.\n\nAfter the fix, the progression changes:\n\n\nterm: instances, position: 0, waitForPos: -1\nterm: reused, position: 2, waitForPos: -1\nterm: all, position: 4, waitForPos: -1\nterm: tokens, position: 5, waitForPos: -1\nterm: document, position: 8, waitForPos: 7  # reset\nterm: thus, position: 9, waitForPos: -1\nterm: tokenstream, position: 11, waitForPos: -1\nterm: filter, position: 12, waitForPos: -1\nterm: needs, position: 13, waitForPos: -1\nterm: update, position: 15, waitForPos: -1\n...\n\n\n\nSo it now gets to reset waitForPos and fixes the issue.\n\nThe updated test can be run with ant test -Dtests.class=org.apache.lucene.search.highlight.HighlighterTest\n\n ",
            "author": "Lukhnos Liu",
            "id": "comment-15073306"
        },
        {
            "date": "2015-12-29T05:29:43+0000",
            "content": "Looks good to me Lukhnos.  Thanks for the detailed explanation.  I plan to commit this tomorrow.  \n\nI'll also adjust the mistaken javadoc comment claiming fragmentSize is measured in bytes \u2013 it's actually characters.  And I'll update the for loop to use Java 5 style. ",
            "author": "David Smiley",
            "id": "comment-15073512"
        },
        {
            "date": "2015-12-29T06:52:54+0000",
            "content": "Updated with the svn-formatted patch (the previous one was git-formatted) ",
            "author": "Lukhnos Liu",
            "id": "comment-15073555"
        },
        {
            "date": "2015-12-29T22:00:55+0000",
            "content": "Commit 1722241 from David Smiley in branch 'dev/trunk'\n[ https://svn.apache.org/r1722241 ]\n\nLUCENE-2229: Fix SimpleSpanFragmenter bug with adjacent stop-words ",
            "author": "ASF subversion and git services",
            "id": "comment-15074345"
        },
        {
            "date": "2015-12-29T22:02:54+0000",
            "content": "Commit 1722242 from David Smiley in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1722242 ]\n\nLUCENE-2229: Fix SimpleSpanFragmenter bug with adjacent stop-words ",
            "author": "ASF subversion and git services",
            "id": "comment-15074347"
        },
        {
            "date": "2015-12-29T22:03:49+0000",
            "content": "Thanks Elmer & Lukhnos! ",
            "author": "David Smiley",
            "id": "comment-15074348"
        },
        {
            "date": "2016-01-11T17:36:55+0000",
            "content": "Commit 1724095 from Adrien Grand in branch 'dev/trunk'\n[ https://svn.apache.org/r1724095 ]\n\nLUCENE-2229: Move CHANGES entry to 5.4.1. ",
            "author": "ASF subversion and git services",
            "id": "comment-15092347"
        },
        {
            "date": "2016-01-11T17:38:12+0000",
            "content": "Commit 1724096 from Adrien Grand in branch 'dev/branches/branch_5x'\n[ https://svn.apache.org/r1724096 ]\n\nLUCENE-2229: Move CHANGES entry to 5.4.1. ",
            "author": "ASF subversion and git services",
            "id": "comment-15092349"
        },
        {
            "date": "2016-01-11T17:38:20+0000",
            "content": "Commit 1724097 from Adrien Grand in branch 'dev/branches/lucene_solr_5_4'\n[ https://svn.apache.org/r1724097 ]\n\nLUCENE-2229: Fix SimpleSpanFragmenter bug with adjacent stop-words ",
            "author": "ASF subversion and git services",
            "id": "comment-15092350"
        },
        {
            "date": "2016-01-12T08:26:57+0000",
            "content": "Commit 1724175 from Adrien Grand in branch 'dev/branches/lucene_solr_5_4'\n[ https://svn.apache.org/r1724175 ]\n\nLUCENE-2229: Add missing CHANGES entry. ",
            "author": "ASF subversion and git services",
            "id": "comment-15093527"
        }
    ]
}