{
    "id": "SOLR-9310",
    "title": "PeerSync fails on a node restart due to IndexFingerPrint mismatch",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [
            "5.5.3",
            "6.3",
            "7.0"
        ],
        "affect_versions": "None",
        "status": "Closed",
        "resolution": "Fixed",
        "priority": "Major"
    },
    "description": "I found that Peer Sync fails if a node restarts and documents were indexed while node was down. IndexFingerPrint check fails after recovering node applies updates. \n\nThis happens only when node restarts and not if node just misses updates due reason other than it being down.\n\nPlease check attached patch for the test.",
    "attachments": {
        "PeerSync_Experiment.patch": "https://issues.apache.org/jira/secure/attachment/12820975/PeerSync_Experiment.patch",
        "SOLR-9310_5x.patch": "https://issues.apache.org/jira/secure/attachment/12825007/SOLR-9310_5x.patch",
        "SOLR-9310_3ReplicaTest.patch": "https://issues.apache.org/jira/secure/attachment/12823537/SOLR-9310_3ReplicaTest.patch",
        "PeerSync_3Node_Setup.jpg": "https://issues.apache.org/jira/secure/attachment/12823536/PeerSync_3Node_Setup.jpg",
        "SOLR-9310.patch": "https://issues.apache.org/jira/secure/attachment/12819602/SOLR-9310.patch",
        "SOLR-9310_final.patch": "https://issues.apache.org/jira/secure/attachment/12824570/SOLR-9310_final.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2016-07-15T19:41:33+0000",
            "author": "Pushkar Raste",
            "content": "Yonik Seeley and Mark Miller, in SOLR-8690 you mentioned that fingerprint check could have performance cost. Was performance cost you mentioned could be due to the fact that PeerSync was failing on node restart and hence Solr was falling back to do full replication ? \n\nIs PeerSync failing on node restart expected behavior\n ",
            "id": "comment-15379987"
        },
        {
            "date": "2016-07-15T19:42:37+0000",
            "author": "Pushkar Raste",
            "content": "Adding Keith Laban in the loop ",
            "id": "comment-15379989"
        },
        {
            "date": "2016-07-17T20:02:51+0000",
            "author": "Pushkar Raste",
            "content": "I was able to figure a few more things about what going own. The existing PeerSyncTest does not change core's state from ACTIVE to recovering and hence the condition following condition (block) in DistributedUpdateLogProcessor.versionAdd() does not get executed \n\nif (ulog.getState() != UpdateLog.State.ACTIVE && (cmd.getFlags() & UpdateCommand.REPLAY) == 0) {\n              // we're not in an active state, and this update isn't from a replay, so buffer it.\n              cmd.setFlags(cmd.getFlags() | UpdateCommand.BUFFERING);\n              ulog.add(cmd);\n              return true;\n            }\n\n\n\nHowever, the test I have attached certainly goes through above mentioned code which results in updates came from log replay to be buffered. As a result of this compareFingerPrint() check at the end of PeerSync.handleUpdates() fails,whenever a PeerSync was triggered and core was in not ACTIVE state. I am not entirely sure why a core would in ACTIVE state if PeerSync was triggered (which happens in PeerSyncTest). \n\nI think compareFingerPrint check should be moved out of PeerSync class to RecoveryStrategy after buffered updates are applied. It might also be a good idea to move the commit after log replay as current commit seems to be resulting in a NOOP. ",
            "id": "comment-15381495"
        },
        {
            "date": "2016-07-17T20:03:19+0000",
            "author": "Pushkar Raste",
            "content": "Adding Ramkumar Aiyengar in the loop ",
            "id": "comment-15381496"
        },
        {
            "date": "2016-07-17T23:26:08+0000",
            "author": "Pushkar Raste",
            "content": "On second thought, moving fingerprint check after log replay would not work as it can contain updates other than the node received from the leader.\n\nOnly two options I could think of is \n\n\tChange the check in DistributedUpdateProcessor.versionAdd() to handle PEER_SYNC flag (i.e. not to buffer updates from PEER_SYNC). I am not entirely sure what other issues it might cause.\n\tNot to do the fingerprint check for after updates are applied in PeerSync if core is not in active state.\n\n ",
            "id": "comment-15381572"
        },
        {
            "date": "2016-07-18T23:21:12+0000",
            "author": "Pushkar Raste",
            "content": "Attached patch to fix the issue. Fix is not to buffer updates with UpdateCommand.PEER_SYNC in DistributedUpdateProcessor.addVersion() ",
            "id": "comment-15383277"
        },
        {
            "date": "2016-07-18T23:24:17+0000",
            "author": "Pushkar Raste",
            "content": "If fix itself looks good, I will upate my Test to include all the scenarios in exiting PerrSyncTest test ",
            "id": "comment-15383282"
        },
        {
            "date": "2016-07-20T03:37:48+0000",
            "author": "Pushkar Raste",
            "content": "Although my patch would work, if there was no active indexing going on during PeerSync, fingerprint check may fail if there was any active indexing was going on. There are too many race conditions here. \n\nIn my opinion people who are continuously indexing data, should disable fingerprint check. ",
            "id": "comment-15385265"
        },
        {
            "date": "2016-07-20T03:52:48+0000",
            "author": "Pushkar Raste",
            "content": "There is another problem in RealTimeGetComponent.processGetVersions(), since asking for a fingerprint causes a new RealTime Searcher to open, we should first get the fingerprint and then get versions from ulog ",
            "id": "comment-15385278"
        },
        {
            "date": "2016-07-20T04:01:05+0000",
            "author": "Pushkar Raste",
            "content": "Updated patch and tests for scenarios I have described ",
            "id": "comment-15385283"
        },
        {
            "date": "2016-07-22T10:41:13+0000",
            "author": "Noble Paul",
            "content": "is there a test in the patch? I don't see it ",
            "id": "comment-15389298"
        },
        {
            "date": "2016-07-22T12:17:42+0000",
            "author": "Pushkar Raste",
            "content": "Noble Paul - Thanks for pointing out. Check updated patch with the test  ",
            "id": "comment-15389384"
        },
        {
            "date": "2016-07-22T12:28:14+0000",
            "author": "Noble Paul",
            "content": "When you post a patch please post the whole patch, not pieces. This contains the test only. We can't know in which order to apply your patches. No need to delete old patches.  ",
            "id": "comment-15389402"
        },
        {
            "date": "2016-07-22T12:48:04+0000",
            "author": "Pushkar Raste",
            "content": "Damn the first lapse was 4:00 am mystery thing, this second on is on me.\nRecomputing and attaching patch.  ",
            "id": "comment-15389428"
        },
        {
            "date": "2016-07-23T14:17:59+0000",
            "author": "Noble Paul",
            "content": "I'm assuming that this one is not related to this ticket. Should it not be a new ticket? ",
            "id": "comment-15390702"
        },
        {
            "date": "2016-07-23T16:25:14+0000",
            "author": "Pushkar Raste",
            "content": "I think it is related to this ticket. It is just another scenario in which PeerSync would fail on restart \"if you index documents while a node was down, but did not issue a commit (or not autoCommit / autoSoftCommit was triggered). \n\n\nDo you think there needs to be a different ticket for this scenario. If needed, I will create a sperate ticket, update my patch for this ticket not to cover that scenario and send a patch for this scenario on the new ticket ",
            "id": "comment-15390758"
        },
        {
            "date": "2016-07-25T16:31:34+0000",
            "author": "Noble Paul",
            "content": "I guess we should compare the fingerprint before the updates are applied. I'm testing another approach. Will report back ",
            "id": "comment-15392232"
        },
        {
            "date": "2016-07-25T16:46:29+0000",
            "author": "Pushkar Raste",
            "content": "That won't work as leader may have already diverged when compared to node that is just coming up ",
            "id": "comment-15392262"
        },
        {
            "date": "2016-07-25T17:04:48+0000",
            "author": "Noble Paul",
            "content": "Well, There is a way to compute a fingerprint upto a point . So, if replica has updates  till version 'x' and if  fingerprint of the leader matches till 'x' wouldn't it be good enough?  ",
            "id": "comment-15392301"
        },
        {
            "date": "2016-07-25T17:13:59+0000",
            "author": "Yonik Seeley",
            "content": "The variable name is confusing. maxInHash is not a hash \n\nmax-in-hash... t's the maximum value included in the hash.  That name does not seem to imply that it is a hash. ",
            "id": "comment-15392314"
        },
        {
            "date": "2016-07-25T18:11:04+0000",
            "author": "Pushkar Raste",
            "content": "Another question. \n\nWhy update commands with flag REPLAY are not buffered in DistributedUpdateProcessor.versionAdd() but update commands with flag PEER_SYNC are. \n\nAlso even you ask for IndexFingerPrint of a specific version, you will end up sending Long.MAX_VALUE while asking for versions to RealTimeGetComponent. \n\nGiven all different race conditions, I am not sure how much we really gain out of fingerprint check ",
            "id": "comment-15392412"
        },
        {
            "date": "2016-07-26T14:54:13+0000",
            "author": "Noble Paul",
            "content": "Pushkar Raste I have tried a few approaches and only your approach (as given in the patch) seems to yield results. I'll do some more testing anyway. Your test has some commented out stuff, parallel indexing. Does it work too? ",
            "id": "comment-15393901"
        },
        {
            "date": "2016-07-26T15:38:37+0000",
            "author": "Pushkar Raste",
            "content": "Parallel indexing unfortunately does not work, it is a race condition where things could diverge on the leader during call to get index fingerprint and get versions. I think fingerprint check is mostly likely to fail if documents are being indexed while node is recovering and if a commits (or softCommits) get issued/triggered. ",
            "id": "comment-15393961"
        },
        {
            "date": "2016-07-27T12:27:33+0000",
            "author": "Noble Paul",
            "content": "Another approach. Compute the fingerprint with the latest version in the current node and compare it with the same version in the remote node ",
            "id": "comment-15395571"
        },
        {
            "date": "2016-07-27T12:37:19+0000",
            "author": "Pushkar Raste",
            "content": "Looks like you uploaded wrong patch. It doesn't look any different than one I attached. ",
            "id": "comment-15395592"
        },
        {
            "date": "2016-07-27T12:56:32+0000",
            "author": "Noble Paul",
            "content": "sorry , wrong file ",
            "id": "comment-15395622"
        },
        {
            "date": "2016-07-27T14:54:08+0000",
            "author": "Pushkar Raste",
            "content": "Thanks Sarah noble, I will take a look at it tonight and run my origin test that exposed the issue and will let you know if it works as expected. \n\nDoes changing logic in the {[DistributedUpdateProcessor}} has any downside to it? I am wondering why REPLAY flag is treated differently than PEER_SYNC ",
            "id": "comment-15395796"
        },
        {
            "date": "2016-07-27T21:40:01+0000",
            "author": "Pushkar Raste",
            "content": "Noble Paul -  Patch looks good. Can you please provide info about my question for PEER_SYNC vs REPLAY flag. \n\nare there any downsides of the way I was doing it?\n\nOnly problem I could think of your approach is we are requesting updates twice, which in my case is asking for tens of thousands of updates, which could be lot of chatter of the wire. ",
            "id": "comment-15396439"
        },
        {
            "date": "2016-07-28T02:31:44+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "am wondering why REPLAY flag is treated differently than PEER_SYNC\n\nREPLAY updates are not written to update log because that would be redundant. The update log itself is the source of the updates being replayed. PEER_SYNC updates come from a different node and they certainly weren't present in the local update log already (or else we would not have requested them). This is why they must be written to the tlog. ",
            "id": "comment-15396804"
        },
        {
            "date": "2016-07-28T16:42:09+0000",
            "author": "Pushkar Raste",
            "content": "Noble Paul - Seems like with your patch we are matching fingerprint upto the version before node went down, whereas intent is to compare fingerprint after apply updates from the leader.  I modified PeerSync.handleUpdates() to not apply updates at all and fingerprint check still passed. Here is excerpt of change I am talking about \n\n  private boolean handleUpdates(ShardResponse srsp) {\n    // we retrieved the last N updates from the replica\n    boolean test = true;\n    \n    List<Object> updates = (List<Object>)srsp.getSolrResponse().getResponse().get(\"updates\");\n\n    SyncShardRequest sreq = (SyncShardRequest) srsp.getShardRequest();\n    \n    if(test) {\n      return compareFingerprint(sreq);\n    }\n   ...\n\n\n\nI am not sure if there is a way to write a test for my observation about fingerprint check happening on state before updates were applied and not after were applied.\n\nThis probably defies intent of comparing fingerprint after applying updates. Why not check it before asking for updates in the first place then.  \n ",
            "id": "comment-15397793"
        },
        {
            "date": "2016-07-28T17:21:05+0000",
            "author": "Anshum Gupta",
            "content": "Noble Paul just wanted to check on the current state here. Do you suggest holding back the 5.5.3 release for this ?\n\nPlease consider the complexity as well as how close we are to the solution at this point. ",
            "id": "comment-15397850"
        },
        {
            "date": "2016-07-29T06:32:21+0000",
            "author": "Noble Paul",
            "content": "it depends. if there is no urgency, let's hold back 5.5.3. Else there can always be a 5.5.4 ",
            "id": "comment-15398806"
        },
        {
            "date": "2016-07-29T06:37:58+0000",
            "author": "Noble Paul",
            "content": "This probably defies intent of comparing fingerprint after applying updates. Why not check it before asking for updates in the first place then.\n\nIsn't it enough if we compare the fingerprint up to the point when it was down? After that, we are going to apply the updates from the  other replica (so fingerprint of the delta will be same anyway. If we can get a consensus on this  approach, I guess we should implement this solution \n ",
            "id": "comment-15398816"
        },
        {
            "date": "2016-07-29T11:51:29+0000",
            "author": "Pushkar Raste",
            "content": "What was the original reason for adding fingerprint check.  Unless that is clear we are not solving the right problem here. I couldn't find a test case that led to adding fingerprint checl.\n\n\n\tIf intent is to check state before node down, for clarity. check should be made before applying updates.\n\tLet's also make read and sort update log of recovering node only once.\n\tI would also uncomment parallel indexing logic in the PeerSyncReplicationTest, as this won't break fingerprint check, with your patch.\n\n ",
            "id": "comment-15399194"
        },
        {
            "date": "2016-07-29T12:07:59+0000",
            "author": "Noble Paul",
            "content": "What was the original reason for adding fingerprint check?\n\nPreviously we were just comparing the latest versions. We had no way to know if some intermediate versions were missing or not. In most cases, that would be correct. but if there was an out of order update, then we would assume we have everything and go ahead with downloading versions after our latest update. Fingerprints compute the hash of all versions. So , we will be able to avoid such errors\n\nLet's also make read and sort update log of recovering node only once.\nSure. I'm aware of this optimization. It was a rudimentary patch to ensure that the approach is valid  ",
            "id": "comment-15399217"
        },
        {
            "date": "2016-07-29T12:56:48+0000",
            "author": "Noble Paul",
            "content": "Here is excerpt of change I am talking about\n\nSomehow I could not make that change to work. Can u post your patch ",
            "id": "comment-15399290"
        },
        {
            "date": "2016-07-29T14:44:49+0000",
            "author": "Pushkar Raste",
            "content": "This an experimental patch, just to test what happens if PeerSync falls on its face. \n\nI could not think of an elegant way to test it, but it just proves that we if compare index fingerprint only before applying updates, it would not validate if PeerSync itself is successful. \n\nIMHO we should know that after recovery nodes are in sync irrespective of whether they were in sync at some point before failure or not.  ",
            "id": "comment-15399429"
        },
        {
            "date": "2016-07-29T14:56:51+0000",
            "author": "Pushkar Raste",
            "content": "Also looking at Yonik's comments \nhttps://issues.apache.org/jira/browse/SOLR-8586?focusedCommentId=15122263&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15122263\n\nhttps://issues.apache.org/jira/browse/SOLR-8586?focusedCommentId=15126352&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15126352\n\nIt seems to me that we should make sure that nodes are in sync after applying updates. \n\nI had tagged Yonik and Mark in my comments but haven't heard anything from them yet. May be we should reach out to them to ask what's the intention of fingerprint check. ",
            "id": "comment-15399450"
        },
        {
            "date": "2016-08-01T05:54:02+0000",
            "author": "Noble Paul",
            "content": "It's hard to have the fingerprint match after applying the updates. The system as exists today does not work in case of  restarting node or LIR. So , it's already broken. I shall wait for some time for Yonik Seeley to revert on this.  ",
            "id": "comment-15401563"
        },
        {
            "date": "2016-08-03T05:34:47+0000",
            "author": "Mark Miller",
            "content": "Yeah, it was better before. Except for the fact that it didn't work.  ",
            "id": "comment-15405342"
        },
        {
            "date": "2016-08-04T17:07:03+0000",
            "author": "Noble Paul",
            "content": "I Plan to commit this soon if there are no other comments ",
            "id": "comment-15408141"
        },
        {
            "date": "2016-08-04T17:14:41+0000",
            "author": "Pushkar Raste",
            "content": "Can this wait for Yonik's comment. Je fact his implementation used MAX\nVERSION and that check was added at the end of handleUpdates makes me think\nintention was to do the check later and not before starting PeerSync\n ",
            "id": "comment-15408155"
        },
        {
            "date": "2016-08-11T15:56:07+0000",
            "author": "Yonik Seeley",
            "content": "intention was to do the check later and not before starting PeerSync\n\nYes, that was the intention.  The approach was deliberately conservative : in cases where peersync formerly thought it had gotten into sync, verify that with a fingerprint.\n\nOn a quick skim through, it's not clear what people think the bug is... but I'll try a more thorough read-through next. ",
            "id": "comment-15417480"
        },
        {
            "date": "2016-08-11T16:00:09+0000",
            "author": "Noble Paul",
            "content": "The testcase fails without the fix. That can help you see the bug easily ",
            "id": "comment-15417489"
        },
        {
            "date": "2016-08-11T16:41:00+0000",
            "author": "Yonik Seeley",
            "content": "You were ready to commit a fix though... so what was exactly is the bug, what caused it, and how does the patch fix it? ",
            "id": "comment-15417545"
        },
        {
            "date": "2016-08-11T16:45:26+0000",
            "author": "Pushkar Raste",
            "content": "Here is short description of bug\n1. A node goes down in solr cloud \n2. More documents and added (and may be a commit issued)\n3. Node that was down comes up. \n4. Node gets fingerprint from the leader and version too \n5. Node calculates diff for missing versions and  requests updates for the same  \n6. Node applies updates and then checks it's fingerprint against the leader's fingerprint\n7. Check in #6 always fail, fingerprint of recovering node does not reflect updates applied during PeerSync\n\n\nThere are two proposed fixes\n\n\tMy fix is not to buffer updates commands that have PEER_SYNC flag on it.\nI think, hesitation about my patch, we don't know what other side effect it may have. (All test cases are passing, but we might not have a test case where my fix would break things)\n\n\n\n\n\tNoble's fix to check fingerprint before we start applying updates.\nIn my opinion this no really fixing original issue, what really matters is if fingerprint matches after applying updates during PeerSync.\n\n\n\nand we don't know which approach in right. May be there 3rd better approach ",
            "id": "comment-15417558"
        },
        {
            "date": "2016-08-11T20:45:47+0000",
            "author": "Yonik Seeley",
            "content": "My fix is not to buffer updates commands that have PEER_SYNC flag on it.\n\nAh, I see... I had no idea that PEER_SYNC updates were buffered (I don't recall that being intentional at least).\nThat would prevent updates obtained from the leader and applied to the replica from being included in the fingerprint on the replica after the peersync.\n ",
            "id": "comment-15417909"
        },
        {
            "date": "2016-08-12T02:50:16+0000",
            "author": "Noble Paul",
            "content": "I think, hesitation about my patch, we don't know what other side effect it may have\n\nWe know the side effect. If this replica becomes the leader and some other node tries to peersync from this replica, that node will not get those updates. That breaks the functionality ",
            "id": "comment-15418279"
        },
        {
            "date": "2016-08-12T03:17:03+0000",
            "author": "Pushkar Raste",
            "content": "Noble Paul -  I will write a test for the scenario you mentioned ",
            "id": "comment-15418297"
        },
        {
            "date": "2016-08-12T11:42:41+0000",
            "author": "Pushkar Raste",
            "content": "Hesitation about your patch is it checks index fingerprint before applying\nupdates. Which Yonik agrees is not right.\n ",
            "id": "comment-15418720"
        },
        {
            "date": "2016-08-12T13:43:48+0000",
            "author": "Noble Paul",
            "content": "If there is a way to get the fingerprint after applying the updates, it will be good.  ",
            "id": "comment-15418843"
        },
        {
            "date": "2016-08-12T14:31:45+0000",
            "author": "Noble Paul",
            "content": "Do you see a potential problem if fingerprint is compared before applying updates ",
            "id": "comment-15418915"
        },
        {
            "date": "2016-08-12T14:34:03+0000",
            "author": "Pushkar Raste",
            "content": "If something goes wrong while applying updates, how can you assure that replica is in sync? ",
            "id": "comment-15418920"
        },
        {
            "date": "2016-08-12T15:15:28+0000",
            "author": "Noble Paul",
            "content": "Of something goes wrong peersync fails ",
            "id": "comment-15418979"
        },
        {
            "date": "2016-08-12T21:03:48+0000",
            "author": "Pushkar Raste",
            "content": "Here is my patch with update test for scenario Noble is concerned about and a small diagram depicting what is going on in the test.\n\nTest for my patch still passes.\n\nIf it looks good, I will take stab at fixing PeerSync failure (due index fingerprint) during active indexing, however, due to race conditions, it might not work well. \n\nCan we setup a call to discuss this ? ",
            "id": "comment-15419478"
        },
        {
            "date": "2016-08-19T14:16:52+0000",
            "author": "Pushkar Raste",
            "content": "Final patch. Here are highlights about the changes\n\n\tDon't buffer updates with `PEER_SYNC` flag on it, otherwise those would not be included in the fingerprint and PeerSync would always fail on fingerprint check\n\n\n\n\n\tPeerSync should care for fingerprint only for the updates it is applying. Otherwise if documents are being indexed while a node is PeerSync. Node recovering would have some of the documents in a buffer already. Node recovering would ask for updates for those documents. Leader's fingerprint will however reflect all the documents but recovering node's fingerprint would not consider documents in the buffer.\n\n\n\nThis also applies for missed updates \n\n\n\tCache fingerprint in `SolrIndexSearcher` judiciously\n\n ",
            "id": "comment-15428248"
        },
        {
            "date": "2016-08-21T05:39:25+0000",
            "author": "Noble Paul",
            "content": "Use the maxVersion in fingerprint before returning versions ",
            "id": "comment-15429597"
        },
        {
            "date": "2016-08-21T15:24:59+0000",
            "author": "Pushkar Raste",
            "content": "Since any race condition would be handled by limiting fingerprint upto the maxVersion will get in `getUpdates`, I could not think of compelling reason to uset in `getVersions()`. \n\n It probably won't hurt to use `maxVersion` before returning versions ",
            "id": "comment-15429773"
        },
        {
            "date": "2016-08-22T06:42:44+0000",
            "author": "Noble Paul",
            "content": "It probably won't hurt to use `maxVersion` before returning versions\n\nYou are right. It won't hurt peersync because getUpdates take care of it. But , for correctness of the API the the following should be true \n\nfingerPrint.maxEncounteredVersion== max(versions) ",
            "id": "comment-15430139"
        },
        {
            "date": "2016-08-22T07:02:28+0000",
            "author": "ASF subversion and git services",
            "content": "Commit c2e769450fac21a8f98e818b4783d7dca14cffb8 in lucene-solr's branch refs/heads/master from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c2e7694 ]\n\nSOLR-9310: PeerSync fails on a node restart due to IndexFingerPrint mismatch ",
            "id": "comment-15430210"
        },
        {
            "date": "2016-08-22T08:28:15+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 80f916780798162a5c68875fed10ef1ff132c8f7 in lucene-solr's branch refs/heads/master from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=80f9167 ]\n\nSOLR-9310: PeerSync fails on a node restart due to IndexFingerPrint mismatch, precommit errors fixed ",
            "id": "comment-15430294"
        },
        {
            "date": "2016-08-22T08:35:25+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 2f73129edae7541d5cf45c2085d9ca40ff048b9b in lucene-solr's branch refs/heads/branch_6x from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=2f73129 ]\n\nSOLR-9310: PeerSync fails on a node restart due to IndexFingerPrint mismatch ",
            "id": "comment-15430303"
        },
        {
            "date": "2016-08-22T08:35:28+0000",
            "author": "ASF subversion and git services",
            "content": "Commit c37c22dbb0cd6de5804b1d72c4e2e86c1bba7ef2 in lucene-solr's branch refs/heads/branch_6x from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c37c22d ]\n\nSOLR-9310: PeerSync fails on a node restart due to IndexFingerPrint mismatch ",
            "id": "comment-15430304"
        },
        {
            "date": "2016-08-22T14:38:05+0000",
            "author": "Yonik Seeley",
            "content": "Reopening... the cache that was added in SolrIndexSearcher is not thread safe and is checked outside synchronization.\n\nAlso, about this comment:\n\n    // TODO what happens if updates came out of order, would cached fingerprint still be valid?\n    // May be caching fingerprint may lead more problems\n\n\n\nThe index fingerprint only depends on what documents are in the index, not on their order in the index.  And since the cache is on SolrIndexSearcher (which has a static view of the index), it will be impossible for fingerprint to change for a given max version.  The comment should probably just be removed to avoid confusion.\n\nIn changes on UpdateLog:\n\n+          if(ptr.version > maxVersion) continue;\n\n\n\nversions can be negative for deletes, so we should really be checking against the absolute value of ptr.version ",
            "id": "comment-15430880"
        },
        {
            "date": "2016-08-22T14:45:43+0000",
            "author": "Keith Laban",
            "content": "What is the need for the cache? I seems like there would only ever be a cache hit if if there is no active indexing. It seems like the added complexity is not worth the potential small performance boost.   ",
            "id": "comment-15430898"
        },
        {
            "date": "2016-08-22T14:56:22+0000",
            "author": "Yonik Seeley",
            "content": "What is the need for the cache?\n\nMultiple replicas syncing against each other (or all replicas syncing against a new leader...)  I think it's part of the protocol to elect a new leader (because when one leader goes down, we don't know which replicas may have received the last update(s) and which replicas did not...)  In such a scenario, there is no active indexing because no new leader yet.\nSome people run with large numbers of replicas (10 or 20), and hence the difference could well be large. ",
            "id": "comment-15430922"
        },
        {
            "date": "2016-08-22T15:34:03+0000",
            "author": "Noble Paul",
            "content": "Addressing the thread safety issue and compare absolute values of versions in UpdateLog ",
            "id": "comment-15431013"
        },
        {
            "date": "2016-08-22T17:37:58+0000",
            "author": "Yonik Seeley",
            "content": "minor: since there is a cache now, we could sync on that and get rid of fingerprintLock ",
            "id": "comment-15431250"
        },
        {
            "date": "2016-08-22T17:40:47+0000",
            "author": "Noble Paul",
            "content": "\ud83d\udc4d ",
            "id": "comment-15431255"
        },
        {
            "date": "2016-08-22T17:48:23+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 37ae065591772172dbd44cde4c952d7b56fc8803 in lucene-solr's branch refs/heads/master from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=37ae065 ]\n\nSOLR-9310: fixing concurrency issue and taking care of negative versions ",
            "id": "comment-15431280"
        },
        {
            "date": "2016-08-22T18:00:19+0000",
            "author": "David Smiley",
            "content": "I see the double-check locking pattern there... I believe ConcurrentHashMap.computeIfAbsent would be perfect in this case? ",
            "id": "comment-15431306"
        },
        {
            "date": "2016-08-22T18:05:19+0000",
            "author": "Pushkar Raste",
            "content": "I tried that, but since `IndexFingerprint.getFingerprint()` can throw an exception, implementation using `computeIfAbsent` looked too ugly to use it. ",
            "id": "comment-15431312"
        },
        {
            "date": "2016-08-22T18:36:34+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 0bdbbbfd52a95e83ce3827161852dbccdd618f5b in lucene-solr's branch refs/heads/branch_6x from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=0bdbbbf ]\n\nSOLR-9310: fixing concurrency issue and taking care of negative versions ",
            "id": "comment-15431393"
        },
        {
            "date": "2016-08-23T07:58:52+0000",
            "author": "Noble Paul",
            "content": "ported to 5.5.x ",
            "id": "comment-15432331"
        },
        {
            "date": "2016-08-23T17:30:45+0000",
            "author": "ASF subversion and git services",
            "content": "Commit c6c3166bf5d28922bb3639ac9da3912aab85f520 in lucene-solr's branch refs/heads/master from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=c6c3166 ]\n\nSOLR-9310: addressing the test failures in jenkins ",
            "id": "comment-15433253"
        },
        {
            "date": "2016-08-23T17:39:26+0000",
            "author": "ASF subversion and git services",
            "content": "Commit d9c4c5282a46bc5d3d1a6e4b1586083dc8970837 in lucene-solr's branch refs/heads/branch_6x from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=d9c4c52 ]\n\nSOLR-9310: addressing the test failures in jenkins ",
            "id": "comment-15433267"
        },
        {
            "date": "2016-08-25T16:57:18+0000",
            "author": "Yonik Seeley",
            "content": "Ah, a piece of the puzzle: we didn't always buffer before peersync... SOLR-8407 ",
            "id": "comment-15437214"
        },
        {
            "date": "2016-08-25T18:26:33+0000",
            "author": "Pushkar Raste",
            "content": "Thanks Yonik. I don't know context for SOLR-8407. Should we have not buffered updates to fix SOLR-8407, or patch for this (SOLR-9310) issue would break SOLR-8407  ",
            "id": "comment-15437399"
        },
        {
            "date": "2016-08-25T18:49:11+0000",
            "author": "Yonik Seeley",
            "content": "Mark pointed me to SOLR-8085 for where/when we started buffering before peersync.\nI think it probably had to do with the following scenario:\n1) replica that is behind comes up and starts peersync\n2) replica receives a bunch of updates forwarded from leader (and indexes them and adds them to tlog as normal)\n3) replica goes down\n4) replica comes up, looks at last 100 versions in it's transaction log, does peersync and concludes that it's up-to-date\n\nPutting the udpate log in buffering mode adds a FLAG_GAP to all the records int he log, which signals that there is a gap somewhere and hence one can't conclude that if the last 100 updates are good that everything else is good.  This is all before fingerprinting of course. ",
            "id": "comment-15437446"
        },
        {
            "date": "2016-08-31T15:09:42+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 8655b97b27d8da470c8235683af11a8b85a2b10f in lucene-solr's branch refs/heads/branch_5_5 from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=8655b97 ]\n\nSOLR-9310: java 7 compile errors ",
            "id": "comment-15452488"
        },
        {
            "date": "2016-08-31T15:38:15+0000",
            "author": "ASF subversion and git services",
            "content": "Commit 4f6e2546739e5352738f786aaddfb6f08b1549aa in lucene-solr's branch refs/heads/branch_5x from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=4f6e254 ]\n\nSOLR-9310: PeerSync fails on a node restart due to IndexFingerPrint mismatch ",
            "id": "comment-15452558"
        },
        {
            "date": "2016-08-31T15:41:07+0000",
            "author": "ASF subversion and git services",
            "content": "Commit afcc8c05dd2c13a0cb0165674931a99decf98373 in lucene-solr's branch refs/heads/branch_5x from Noble Paul\n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=afcc8c0 ]\n\nSOLR-9310: PeerSync fails on a node restart due to IndexFingerPrint mismatch ",
            "id": "comment-15452566"
        },
        {
            "date": "2016-08-31T17:19:42+0000",
            "author": "Anshum Gupta",
            "content": "I've manually checked that these changes are in branch_5_5, but there's no log/comment here in the JIRA, other than the single entry about fixing Java7 compile errors. Am I missing something here? ",
            "id": "comment-15452802"
        },
        {
            "date": "2016-08-31T17:22:48+0000",
            "author": "Noble Paul",
            "content": "Actually, the commit did not make it to JIRA. May be the bot was down or something. ",
            "id": "comment-15452810"
        },
        {
            "date": "2016-09-16T18:36:06+0000",
            "author": "Yonik Seeley",
            "content": "I looped HdfsChaosMoneyNothingIsSafeTest after this change, and I occasionally started getting some shards \"non consistent\" failures again. ",
            "id": "comment-15497044"
        },
        {
            "date": "2016-09-16T18:46:22+0000",
            "author": "Pushkar Raste",
            "content": "Is this happening when active indxing is going on when node is in recovery. That is only scenario I could think of fingerprint might mismatch (but I don't really doubt that), as we compare fingerprint only upto the max version that we request during peersync. \n\nAlso, earlier, the only reason nodes were able to sync was nodes were doing snap pull every single time. \n\nDoes this fail only with HDFS or in non-hdfs scenario as well. If this breaks only with HDFS, may be we can add option to recover using replication only until the fingerprint implementation stabilizes.  ",
            "id": "comment-15497075"
        },
        {
            "date": "2016-09-16T19:39:27+0000",
            "author": "Yonik Seeley",
            "content": "I only tested with the HDFS variant quickly... it tends to fail a little more often because of greater timing variability I think.\nWe should also test the \"safe-leader\" test... this may be just an issue when leaders are killed.\n ",
            "id": "comment-15497183"
        },
        {
            "date": "2016-09-20T20:45:29+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "This wasn't backported to 6.2.1 so removing the fix version 6.2.1 ",
            "id": "comment-15507745"
        },
        {
            "date": "2016-09-25T03:27:22+0000",
            "author": "Yonik Seeley",
            "content": "I just happened to notice that PeerSyncReplicationTest failed in jenkins:\nhttps://jenkins.thetaphi.de/job/Lucene-Solr-6.x-Linux/1796/\nNot sure how often it's happened though. ",
            "id": "comment-15520083"
        },
        {
            "date": "2016-09-25T12:25:44+0000",
            "author": "Pushkar Raste",
            "content": "I went through logs at https://jenkins.thetaphi.de/job/Lucene-Solr-6.x-MacOSX/429/consoleFull \nIf PeerSync was unsuccessful I would expect to see a line like \no.a.s.u.PeerSync Fingerprint comparison: -1 \n\nHowever, I don't see such line. I could think of two scenarios that could break the test \n\n\tdata directory could get deleted while a node is brought down, since data directory is created in temp. Upon restart replica would have no frame of reference and will have to fall back on replication.\n\twe need a better check than relying number of requests made to ReplicationHandler\n\n ",
            "id": "comment-15520747"
        },
        {
            "date": "2016-11-09T08:36:53+0000",
            "author": "Shalin Shekhar Mangar",
            "content": "Closing after 6.3.0 release. ",
            "id": "comment-15650197"
        }
    ]
}