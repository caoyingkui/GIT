{
    "id": "SOLR-5436",
    "title": "Eliminate the 1500ms wait in overseer loop",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "4.7",
            "6.0"
        ],
        "components": [],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "The Overseer thread waits 1500 ms before it polls for new events. The wait should be eliminated and it should just wait for new events till they come the way it is done in OverseerCollectionProcessor",
    "attachments": {
        "SOLR-5436.patch": "https://issues.apache.org/jira/secure/attachment/12614275/SOLR-5436.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Shalin Shekhar Mangar",
            "id": "comment-13820082",
            "date": "2013-11-12T13:29:06+0000",
            "content": "As I understand it, the reason behind the wait are to group the processing of multiple events together so that the cluster state change is not broadcasted to all watching nodes too many times. But this can work along with splitting the cluster state in SOLR-5381. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13820228",
            "date": "2013-11-12T16:33:44+0000",
            "content": "Shalin Shekhar Mangar Originally we did not have a blocking lookup of the queue. that was also one of the reasons.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13824686",
            "date": "2013-11-17T01:07:25+0000",
            "content": "Originally we did not have a blocking lookup of the queue\n\nRight - the history is that Sami wrote the Overseer loop first, later I wrote the OverseerCollectionProcessor loop - I didn't like the polling and used the blocking lookup of the queue and just never got to catching the Overseer loop impl up. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13824732",
            "date": "2013-11-17T04:37:14+0000",
            "content": "What if there are state change events happening in quick succession (like a rolling restart) will it not lead to too many updates getting propagated to all the nodes? Is it a real problem? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13824735",
            "date": "2013-11-17T04:41:32+0000",
            "content": "Shalin's original comment? Yeah, I suppose it does make sense. How much of a problem it is probably depends on the scale. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13824778",
            "date": "2013-11-17T08:48:58+0000",
            "content": "In my patch it would ensure that any event would get processed within the STATE_UPDATE_DELAY or all normal events would get processed within 100ms "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13825219",
            "date": "2013-11-18T09:26:41+0000",
            "content": "added a new method to peek(waitTime) to DistributedQueue "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13825450",
            "date": "2013-11-18T16:36:01+0000",
            "content": "This time w/ all the tests passing\n\nMark Miller please do a review . If it is fine I can check it in soon "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13825482",
            "date": "2013-11-18T17:05:46+0000",
            "content": "Yeah, I'll take a look today. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13826188",
            "date": "2013-11-19T04:27:55+0000",
            "content": "I've looked it over, but I have to look closer at a couple things and I got tied up in other issues. Let me take a closer look tomorrow. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13826235",
            "date": "2013-11-19T06:07:42+0000",
            "content": "optimized the DistributedQueue code "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13827834",
            "date": "2013-11-20T16:47:59+0000",
            "content": "I'm reviewing the latest patch. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13827906",
            "date": "2013-11-20T17:56:17+0000",
            "content": "Looks good.\n\nHere is the same patch but with some minor cleanup and that eliminates the 5 second poll of the main queue. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13827924",
            "date": "2013-11-20T18:10:14+0000",
            "content": "I kept the 5sec peek() on purpose. Because if the OverSeer changes and no state event happens there is no way for the node to know it. So let's keep it "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13827935",
            "date": "2013-11-20T18:26:25+0000",
            "content": "Because if the OverSeer changes and no state event happens there is no way for the node to know it. \n\nThere is no reason the Overseer needs to pull every 5 seconds to avoid this. We know when a new Overseer is elected. \n\nIf we keep the 5 second poll of the queue, this issue gains very little IMO. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13827968",
            "date": "2013-11-20T18:50:48+0000",
            "content": "If for some odd reason the Overseer stops being the Overseer without losing it's connection to ZooKeeper, this patch will stop the current Overseer from running when it's alerted the Overseer has changed. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13827981",
            "date": "2013-11-20T19:00:53+0000",
            "content": "Keep in mind, that latch watcher will fire on zk connection events as well as when items are added to the queue. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13827997",
            "date": "2013-11-20T19:11:34+0000",
            "content": "Yeah the Latch Watcher gets a callback , but it the peek(true) would not return unless there is some entry is added to the queue. What happens when the OverSeer changed and no entry is added to the queue ? "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13828021",
            "date": "2013-11-20T19:31:28+0000",
            "content": "Ah, right - good point. We don't catch the watcher connection event and have peek fail.\n\nThe latest change should handle this case though. Either the connection to ZK is gone and the peek call will fail when it retries after getting the connection watcher event. Or the connection might have failed and come back before the retry tries to talk to zk - in this case, the code change I just made should kill the current Overseer threads - either when we are alerted the Overseer has changed or when we start a new overseer because the zk connection was expired. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13828039",
            "date": "2013-11-20T19:50:20+0000",
            "content": "New patch - I was a little off there.\n\nFor the case where the queue loop does not fail on a down zookeeper because it bounced quickly or something, I said that when we start a new overseer, that will close the old threads - but that only happens if we are elected the overseer. This patch stops any local overseer threads on joining the overseer election. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13828400",
            "date": "2013-11-21T02:13:06+0000",
            "content": "does this patch take care of this scenario?\n\nThe Overseer went into a GC pause or something and others nodes assume it is down and re-elected another OverSeer. And there is no event in the queue and this OverSeer is still waiting on the queue\n\nI don't see the exit condition from the loop in the peek() method "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13828404",
            "date": "2013-11-21T02:23:32+0000",
            "content": "The Overseer went into a GC pause or something and others nodes assume it is down and re-elected another OverSeer.\n\nThis will only happen if our ephemeral overseer leader node goes down - which means we lost our connection to zookeeper. If we lose our connection to zookeeper, the queue thread will exit trying to talk to zk. If we reconnect to zookeeper before the queue loop has a chance to fail (some kind of wicked quick flap), we will stop the overseer threads on getting in line to be the Overseer again. Also, if zk tells us the overseer leader went down, we stop any overseer threads we might have.\n\nI think its about as strong as the poll. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13828416",
            "date": "2013-11-21T02:30:03+0000",
            "content": "I don't see the exit condition from the loop in the peek() method\n\nYou shouldn't need one. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13828421",
            "date": "2013-11-21T02:34:34+0000",
            "content": "You could of course explicitly add one - catch the connection change notification from the watcher and then cause the loop to fail on the next run. I don't think it's strictly necessary, but it wouldn't hurt. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13828423",
            "date": "2013-11-21T02:41:12+0000",
            "content": "Updated patch to trunk changes. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13828443",
            "date": "2013-11-21T03:10:26+0000",
            "content": "I think that exception handling is better in peek() method better than expecting the consumer of the API to take care of that\n\neither way it works "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13828451",
            "date": "2013-11-21T03:16:25+0000",
            "content": "better than expecting the consumer of the API to take care of that\n\nIt's really an Overseer specific thing though - I don't think it's a problem for the consumer of the DistributedQueue API. We are dealing with special Overseer considerations, and I don't really see the strategy for that as a consumer.\n\nA standard consumer of the queue API does not necessary need to detect this kind of zk connection flap - It's kind of Overseer specific I think. \n\nAdding may actually hurt the distributedqueue consumer experience - you expect to keep working the queue in the face of zk connection/disconnection without any special handling.  "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13828467",
            "date": "2013-11-21T03:56:24+0000",
            "content": "This patch is even better. It ensures we stop any Overseer threads before we reconnect to ZooKeeper on an expiration. This should be even stronger than the 5 second poll. "
        },
        {
            "author": "Mark Miller",
            "id": "comment-13828470",
            "date": "2013-11-21T04:06:34+0000",
            "content": "One more patch attached - fixes silly NPE bug when BeforeReconnect was null. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13829095",
            "date": "2013-11-21T17:02:10+0000",
            "content": "Is there anything more we need to do before committing this? "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13829116",
            "date": "2013-11-21T17:28:38+0000",
            "content": "Commit 1544255 from Mark Miller in branch 'dev/trunk'\n[ https://svn.apache.org/r1544255 ]\n\nSOLR-5436: Eliminate the 1500ms wait in overseer loop as well as polling the ZK distributed queue. "
        },
        {
            "author": "ASF subversion and git services",
            "id": "comment-13829121",
            "date": "2013-11-21T17:35:33+0000",
            "content": "Commit 1544257 from Mark Miller in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1544257 ]\n\nSOLR-5436: Eliminate the 1500ms wait in overseer loop as well as polling the ZK distributed queue. "
        },
        {
            "author": "Noble Paul",
            "id": "comment-13829908",
            "date": "2013-11-22T12:02:47+0000",
            "content": "Why is it not yet resolved? "
        }
    ]
}