{
    "id": "LUCENE-2946",
    "title": "change file format documentation from \"bit-for-bit\" to highlevel",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "general/website"
        ],
        "type": "Task",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "affect_versions": "None",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "While reviewing website docs in LUCENE-2924,\nI noticed the the existing fileformats is going to be pretty hopeless for 4.0.\n\nBefore it described the format \"bit-for-bit\", but with flexible indexing this is \nsomewhat silly (and who really wants a bit-for-bit explanation of some of the new formats!)\n\nI think it would be much better to give a high-level overview, perhaps linking to javadocs or\neven source code for the low-level details. \n\nWe probably should delay this until 4.0 is really close in sight (since things are changing so fast) but we can go ahead and think about it some now.\n\nFor example:\n\n\thigh level explanation of what a codec is, and the various subsystems one is usually composed of (terms index, terms data, skiplist impl, postings impl, etc). We can reiterate that you can make your own, and hopefully this kind of documentation will actually encourage that.\n\thigh level explanation of what StandardCodec is \"composed of\". For example assume its Variable Terms Index, Block Terms Reader, PForDelta docs and freqs and Simple64 positions. I think really this is the only codec we should try to \"diagram\" in any way.\n\thigh level explanation (probably with links) of some of the components. For example we could explain what the purpose of a Terms Index is, and that this implementation uses a finite state transducer to find the terms block for a given term. In this case maybe we have an image now that Dawid made the toDot useful.\n\thigh level explanation (probably with links) of some of the compression algorithms. For example, we could explain the basics of the available algorithms we have (vbyte/simple/for/pfor/...) and what their advantages and disadvantages are.\n\n\n\nSome of the things i mentioned here are probably optional, for instance I think its \"enough\" to give a high-level overview of StandardCodec, but I can't help but think that explaining some of the architecture will be useful for new developers.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2011-03-02T22:02:34+0000",
            "content": "+1 - this all sounds great.\n\nSome of the things i mentioned here are probably optional, for instance I think its \"enough\" to give a high-level overview of StandardCodec, but I can't help but think that explaining some of the architecture will be useful for new developers.\n\n+1 to still going into great detail for StandardCodec. I think doing this for one codec will be supremely useful, as I have found the files format page in the past. ",
            "author": "Mark Miller",
            "id": "comment-13001703"
        },
        {
            "date": "2012-04-25T21:04:32+0000",
            "content": "+1 for the high level overview.\n\n+1 for to still going into detail for StandardCodec.  Going into details of one Codec (especially the default one) will help those of us who have some trouble reading source code and understanding how the specific implementation details fit into the big picture.  I have certainly found both the high level and detailed level information in the existing file formats documentation helpful in understanding the trade-offs in addressing our issues with slow phrase queries and with billions of unique terms. ",
            "author": "Tom Burton-West",
            "id": "comment-13262086"
        },
        {
            "date": "2012-04-25T22:05:51+0000",
            "content": "we can go into detail, but we can't do bit-for-bit with even StandardCodec... its simply not feasible.\n\nFor the simple metadata files, and even stored fields and postings its fine (for now), but\ne.g. going bit-for-bit with packed integer compression of docvalues isnt very realistic,\nnor is try to explain how FSTs for blocktree are serialized.\n\nHell at my current pace I'll just be happy if we can even document all the different docvalues types \ngive some general idea how they are encoded, or give a high-level explanation of the terms dictionary.\n\nEven the existing \"simple\" metadata files are a pretty serious effort because most of the existing\ndocs are wildly out of date.\n\nI figure all of this is ok (i'm heavy committing) since we essentially have nothing today: just out\nof date useless docs  ",
            "author": "Robert Muir",
            "id": "comment-13262179"
        },
        {
            "date": "2012-05-07T04:16:48+0000",
            "content": "fileformats is updated for 4.0 ",
            "author": "Robert Muir",
            "id": "comment-13269354"
        },
        {
            "date": "2012-05-07T10:16:47+0000",
            "content": "Thanks Robert! ",
            "author": "Michael McCandless",
            "id": "comment-13269491"
        }
    ]
}