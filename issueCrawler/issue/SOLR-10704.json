{
    "id": "SOLR-10704",
    "title": "REPLACENODE can make the collection lost data which replicaFactor is 1",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "SolrCloud"
        ],
        "type": "Bug",
        "fix_versions": [
            "6.7",
            "7.0"
        ],
        "affect_versions": "6.2",
        "resolution": "Fixed",
        "status": "Resolved"
    },
    "description": "When some replicas which the relative collection's replicaFactor is 1, it will lost data after executing the REPLACENODE cmd. \n\nIt may be the new replica on the target node does not complete revovering, but the old replica on the source node  was already be deleted.\n\nAt last the target revocery failed for the following exception:\n2017-05-18 17:08:48,587 | ERROR | recoveryExecutor-3-thread-2-processing-n:192.168.229.137:21103_solr x:replace-hdfs-coll1_shard1_replica2 s:shard1 c:replace-hdfs-coll1 r:core_node3 | Error while trying to recover. core=replace-hdfs-coll1_shard1_replica2:java.lang.NullPointerException\n        at org.apache.solr.update.PeerSync.alreadyInSync(PeerSync.java:339)",
    "attachments": {
        "219.log": "https://issues.apache.org/jira/secure/attachment/12868736/219.log",
        "SOLR-10704.patch": "https://issues.apache.org/jira/secure/attachment/12872768/SOLR-10704.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2017-05-20T08:35:04+0000",
            "content": "The details: \n1. Collections' replicas distribution: \nreplace-hdfs-coll1 has two shards, each shard has one replica and the index files was stored on hdfs. \nreplace-hdfs-coll1_shard1_replica1  on node 192.168.229.219\nreplace-hdfs-coll1_shard2_replica1  on node 192.168.228.193\n\nreplace-hdfs-coll2 has two shards, each shard has two replica and the index files was stored on hdfs.\nreplace-hdfs-coll2_shard1_replica1  on node 192.168.229.219\nreplace-hdfs-coll2_shard1_replica2  on node 192.168.229.193\n\nreplace-hdfs-coll2_shard2_replica1  on node 192.168.228.193\nreplace-hdfs-coll2_shard2_replica2  on node 192.168.229.219\n\nreplace-local-coll1 has two shards, each shard has one replica and the index files was stored on disk.\nreplace-local-coll1_shard1_replica1  on node 192.168.228.193\nreplace-local-coll1_shard2_replica1  on node 192.168.229.219\n\nreplace-local-coll2 has two shards, each shard has two replica and the index files was stored on disk.\nreplace-local-coll2_shard1_replica1  on node 192.168.229.193\nreplace-local-coll2_shard1_replica2 on node 192.168.229.219\n\nreplace-local-coll2_shard2_replica1  on node 192.168.228.193\nreplace-local-coll2_shard2_replica2  on node 192.168.229.219\n\n2. Execute REPLACENODE to replace node 192.168.229.219 with node 192.168.229.137\n\n3. The REPLACENODE request was executed successfully\n\n4. The target replace-hdfs-coll1_shard1_replica2 does not complete revovering, but the source replace-hdfs-coll1_shard1_replica1 was already be deleted. At last the target revocery failed for the following exception:\n2017-05-18 17:08:48,587 | ERROR | recoveryExecutor-3-thread-2-processing-n:192.168.229.137:21103_solr x:replace-hdfs-coll1_shard1_replica2 s:shard1 c:replace-hdfs-coll1 r:core_node3 | Error while trying to recover. core=replace-hdfs-coll1_shard1_replica2:java.lang.NullPointerException\n        at org.apache.solr.update.PeerSync.alreadyInSync(PeerSync.java:339)\n\n5. The main process log messages\nlog in 193.log, the node 192.168.228.193 is overseer role.\nstep 1. node 192.168.229.193 recevied the REPLACENODE request\n2017-05-18 17:08:32,717 | INFO  | http-nio-21100-exec-6 | Invoked Collection Action :replacenode with params action=REPLACENODE&source=192.168.229.219:21100_solr&wt=json&target=192.168.229.137:21103_solr and sendToOCPQueue=true | org.apache.solr.handler.admin.CollectionsHandler.handleRequestBody(CollectionsHandler.java:203)\n\nstep 2. OverseerCollectionConfigSetProcessor get the task msg and process REPLACENODE\n\nstep 3.  add replica\n2017-05-18 17:08:36,592 | INFO  | OverseerStateUpdate-1225069473835599708-192.168.228.193:21100_solr-n_0000000063 | processMessage: queueSize: 1, message = \n{\n  \"core\":\"replace-hdfs-coll1_shard1_replica2\",\n  \"roles\":null,\n  \"base_url\":\"http://192.168.229.137:21103/solr\",\n  \"node_name\":\"192.168.229.137:21103_solr\",\n  \"state\":\"down\",\n  \"shard\":\"shard1\",\n  \"collection\":\"replace-hdfs-coll1\",\n  \"operation\":\"state\"}\n current state version: 42 | org.apache.solr.cloud.Overseer$ClusterStateUpdater.run(Overseer.java:221)\n\n 2017-05-18 17:08:40,540 | INFO  | OverseerStateUpdate-1225069473835599708-192.168.228.193:21100_solr-n_0000000063 | processMessage: queueSize: 1, message = \n{\n  \"core\":\"replace-hdfs-coll1_shard1_replica2\",\n  \"core_node_name\":\"core_node3\",\n  \"dataDir\":\"hdfs://hacluster//user/solr//SolrServer1/replace-hdfs-coll1/core_node3/data/\",\n  \"roles\":null,\n  \"base_url\":\"http://192.168.229.137:21103/solr\",\n  \"node_name\":\"192.168.229.137:21103_solr\",\n  \"state\":\"recovering\",\n  \"shard\":\"shard1\",\n  \"collection\":\"replace-hdfs-coll1\",\n  \"operation\":\"state\",\n  \"ulogDir\":\"hdfs://hacluster/user/solr/SolrServer1/replace-hdfs-coll1/core_node3/data/tlog\"}\n current state version: 42 | org.apache.solr.cloud.Overseer$ClusterStateUpdater.run(Overseer.java:221)\n step 4.  deletecore\n2017-05-18 17:08:47,552 | INFO  | OverseerStateUpdate-1225069473835599708-192.168.228.193:21100_solr-n_0000000063 | processMessage: queueSize: 1, message = \n{\n  \"operation\":\"deletecore\",\n  \"core\":\"replace-hdfs-coll1_shard1_replica1\",\n  \"node_name\":\"192.168.229.219:21100_solr\",\n  \"collection\":\"replace-hdfs-coll1\",\n  \"core_node_name\":\"core_node2\"}\n current state version: 42 | org.apache.solr.cloud.Overseer$ClusterStateUpdater.run(Overseer.java:221)\n\n 192.168.229.219 is the source node.\n  2017-05-18 17:08:47,484 | INFO  | http-nio-21100-exec-6 | Removing directory before core close: hdfs://hacluster//user/solr//SolrServerAdmin/replace-hdfs-coll1/core_node2/data/index | org.apache.solr.core.CachingDirectoryFactory.closeCacheValue(CachingDirectoryFactory.java:271)\n2017-05-18 17:08:47,515 | INFO  | http-nio-21100-exec-6 | Removing directory after core close: hdfs://hacluster//user/solr//SolrServerAdmin/replace-hdfs-coll1/core_node2/data | org.apache.solr.core.CachingDirectoryFactory.close(CachingDirectoryFactory.java:204)\n\n 192.168.229.137 is the target node, but  replace-hdfs-coll1_shard1_replica2 recovering is not finished\n 2017-05-18 17:08:48,547 | INFO  | recoveryExecutor-3-thread-2-processing-n:192.168.229.137:21103_solr x:replace-hdfs-coll1_shard1_replica2 s:shard1 c:replace-hdfs-coll1 r:core_node3 | Attempting to PeerSync from http://192.168.229.219:21100/solr/replace-hdfs-coll1_shard1_replica1/ - recoveringAfterStartup=[true] | org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:370)\n2017-05-18 17:08:48,547 | INFO  | recoveryExecutor-3-thread-2-processing-n:192.168.229.137:21103_solr x:replace-hdfs-coll1_shard1_replica2 s:shard1 c:replace-hdfs-coll1 r:core_node3 | PeerSync: core=replace-hdfs-coll1_shard1_replica2 url=http://192.168.229.137:21103/solr START replicas=http://192.168.229.219:21100/solr/replace-hdfs-coll1_shard1_replica1/ nUpdates=100 | org.apache.solr.update.PeerSync.sync(PeerSync.java:214)\n2017-05-18 17:08:48,587 | ERROR | recoveryExecutor-3-thread-2-processing-n:192.168.229.137:21103_solr x:replace-hdfs-coll1_shard1_replica2 s:shard1 c:replace-hdfs-coll1 r:core_node3 | Error while trying to recover. core=replace-hdfs-coll1_shard1_replica2:java.lang.NullPointerException\n        at org.apache.solr.update.PeerSync.alreadyInSync(PeerSync.java:339)\n        at org.apache.solr.update.PeerSync.sync(PeerSync.java:222)\n        at org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:376)\n        at org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:221)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor.lambda$execute$0(ExecutorUtil.java:229)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\n\n\n org.apache.solr.common.SolrException.log(SolrException.java:159)\n\n\n\n\n\n2017-05-18 17:08:48,587 | INFO  | recoveryExecutor-3-thread-2-processing-n:192.168.229.137:21103_solr x:replace-hdfs-coll1_shard1_replica2 s:shard1 c:replace-hdfs-coll1 r:core_node3 | Replay not started, or was not successful... still buffering updates. | org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:441)\n2017-05-18 17:08:48,587 | ERROR | recoveryExecutor-3-thread-2-processing-n:192.168.229.137:21103_solr x:replace-hdfs-coll1_shard1_replica2 s:shard1 c:replace-hdfs-coll1 r:core_node3 | Recovery failed - trying again... (0) | org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:478)\n ",
            "author": "Daisy.Yuan",
            "id": "comment-16018369"
        },
        {
            "date": "2017-06-08T12:25:44+0000",
            "content": "I can reproduce this also using a 2 node setup, using HDFS collection with 2 shards and 1 replica. It appears that the REPLACENODE deletes the original replica (which is also the replica leader) while the new replica is starting the recovery - at which point the recovery fails. Then it tries to find a shard leader, which no longer exists...\n\n\n2017-06-08 12:11:49.760 INFO  (recoveryExecutor-3-thread-1-processing-n:192.168.0.202:8983_solr x:gettingstarted_shard1_replica2 s:shard1 c:gettingstarted r:core_node3) [c:gettingstarted s:shard1 r:core_node3 x:gettingstarted_shard1_replica2] o.a.s.c.RecoveryStrategy Attempting to PeerSync from [http://192.168.0.201:8983/solr/gettingstarted_shard1_replica1/] - recoveringAfterStartup=[true]\n2017-06-08 12:11:49.789 INFO  (recoveryExecutor-3-thread-1-processing-n:192.168.0.202:8983_solr x:gettingstarted_shard1_replica2 s:shard1 c:gettingstarted r:core_node3) [c:gettingstarted s:shard1 r:core_node3 x:gettingstarted_shard1_replica2] o.a.s.u.PeerSync PeerSync: core=gettingstarted_shard1_replica2 url=http://192.168.0.202:8983/solr START replicas=[http://192.168.0.201:8983/solr/gettingstarted_shard1_replica1/] nUpdates=100\n2017-06-08 12:11:49.856 ERROR (recoveryExecutor-3-thread-1-processing-n:192.168.0.202:8983_solr x:gettingstarted_shard1_replica2 s:shard1 c:gettingstarted r:core_node3) [c:gettingstarted s:shard1 r:core_node3 x:gettingstarted_shard1_replica2] o.a.s.c.RecoveryStrategy Error while trying to recover. core=gettingstarted_shard1_replica2:java.lang.NullPointerException\n        at org.apache.solr.update.PeerSync.alreadyInSync(PeerSync.java:340)\n        at org.apache.solr.update.PeerSync.sync(PeerSync.java:223)\n        at org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:376)\n        at org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:221)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor.lambda$execute$0(ExecutorUtil.java:229)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\n...\n\n2017-06-08 12:12:03.826 INFO  (zkCallback-4-thread-2-processing-n:192.168.0.202:8983_solr) [c:gettingstarted s:shard2 r:core_node4 x:gettingstarted_shard2_replica2] o.a.s.c.ActionThrottle The last leader attempt started 34ms ago.\n2017-06-08 12:12:03.827 INFO  (zkCallback-4-thread-2-processing-n:192.168.0.202:8983_solr) [c:gettingstarted s:shard2 r:core_node4 x:gettingstarted_shard2_replica2] o.a.s.c.ActionThrottle Throttling leader attempts - waiting for 4965ms\n2017-06-08 12:12:03.873 ERROR (recoveryExecutor-3-thread-1-processing-n:192.168.0.202:8983_solr x:gettingstarted_shard1_replica2 s:shard1 c:gettingstarted r:core_node3) [c:gettingstarted s:shard1 r:core_node3 x:gettingstarted_shard1_replica2] o.a.s.c.RecoveryStrategy Error while trying to recover. core=gettingstarted_shard1_replica2:org.apache.solr.common.SolrException: No registered leader was found after waiting for 4000ms , collection: gettingstarted slice: shard1\n        at org.apache.solr.common.cloud.ZkStateReader.getLeaderRetry(ZkStateReader.java:747)\n        at org.apache.solr.common.cloud.ZkStateReader.getLeaderRetry(ZkStateReader.java:733)\n        at org.apache.solr.cloud.RecoveryStrategy.doRecovery(RecoveryStrategy.java:305)\n        at org.apache.solr.cloud.RecoveryStrategy.run(RecoveryStrategy.java:221)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor.lambda$execute$0(ExecutorUtil.java:229)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n\n\n\nThe main problem here is that the original replica is deleted before the new replica fully recovers - the code should wait until this happens when there's only one active replica left in the cluster. This should also consider a scenario when there are several replicas of the same shard on the same node, and again the code has to wait with deleting them before at least one new replica has fully recovered. ",
            "author": "Andrzej Bialecki",
            "id": "comment-16042621"
        },
        {
            "date": "2017-06-12T20:11:39+0000",
            "content": "This patch waits for all replicas that we're moving that were leaders (which covers also the case of replicationFactor=1) until they are recovered and only then proceeds to deleting the old replicas. ",
            "author": "Andrzej Bialecki",
            "id": "comment-16047009"
        },
        {
            "date": "2017-06-13T08:26:56+0000",
            "content": "Thanks Andrzej.\n\nA few comments:\n\n\tThe key for the watcher should be collection_name + coreNodeName \u2013 that is necessary and sufficient to be unique across the cluster\n\tInstead of if (replica.getState().equals(Replica.State.ACTIVE)), you should use replica.isActive(liveNodes) to check if replica is active \u2013 another gotcha of SolrCloud that we really should fix at some point\n\tThe RecoveryWatcher's latch is counted down even if there exists at least one replica other than the one being moved \u2013 which is completely fine but a bit confusing reading the code \u2013 perhaps a code comment is pertinent.\n\tThe RecoveryWatcher should have additional checks for replica types e.g. there must be at least 1 active NRT or TLOG replicas somewhere otherwise the slice will be left leaderless as PULL type replicas cannot become leaders.\n\tUnrelated to these changes \u2013 it looks like if anyOneFailed is true, then we delete all newly created replicas from the target AND continue to delete the source node as well?\n\n\n\nOverall, this kind of operation is hard to guarantee in the current state of SolrCloud because at any time, the leader can put another replica in LIR. If that happens after we checked for the replica to be active, then deleting the leader will make that slice leader-less as replicas in LIR cannot become leaders without recoverying first. However, at this point, this is the best we can do. ",
            "author": "Shalin Shekhar Mangar",
            "id": "comment-16047556"
        },
        {
            "date": "2017-06-13T11:07:41+0000",
            "content": "Updated patch that addresses the issues from Shalin's review. If there are no objections I'll commit this shortly. ",
            "author": "Andrzej Bialecki",
            "id": "comment-16047735"
        },
        {
            "date": "2017-06-13T11:44:14+0000",
            "content": "The replicaName in your patch is actually the coreNodeName. The replica ZkNodeProps does not have a property for ZkStateReader.CORE_NODE_NAME_PROP. Just use collectionName + \"_\" + replicaName. Rest looks good. ",
            "author": "Shalin Shekhar Mangar",
            "id": "comment-16047765"
        },
        {
            "date": "2017-06-13T15:40:04+0000",
            "content": "Commit 232eff0893bccb93d01042f26a00e50870be2f29 in lucene-solr's branch refs/heads/master from Andrzej Bialecki \n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=232eff0 ]\n\nSOLR-10704 Wait until all leader replicas are recovered before deleting\nthe originals. ",
            "author": "ASF subversion and git services",
            "id": "comment-16048044"
        },
        {
            "date": "2017-06-13T15:56:33+0000",
            "content": "Commit ccd1f45b3ba3fbb862cae5d0ab0ce821b966026a in lucene-solr's branch refs/heads/branch_6x from Andrzej Bialecki \n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=ccd1f45 ]\n\nSOLR-10704 Wait until all leader replicas are recovered before deleting\nthe originals. ",
            "author": "ASF subversion and git services",
            "id": "comment-16048057"
        },
        {
            "date": "2017-06-13T15:59:47+0000",
            "content": "Fix it so that we wait until the new replica completes recovery in case of leader replicas. ",
            "author": "Andrzej Bialecki",
            "id": "comment-16048059"
        },
        {
            "date": "2017-06-13T16:53:51+0000",
            "content": "Commit cdccbfb92f30cc70a3fee4c1b3655b1aaf1acb7a in lucene-solr's branch refs/heads/master from Andrzej Bialecki \n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=cdccbfb ]\n\nSOLR-10704 Update the ref guide. ",
            "author": "ASF subversion and git services",
            "id": "comment-16048105"
        },
        {
            "date": "2017-06-13T17:09:09+0000",
            "content": "Commit 9612e318568e48ea187700accc1f6e88b1afea43 in lucene-solr's branch refs/heads/branch_6x from Andrzej Bialecki \n[ https://git-wip-us.apache.org/repos/asf?p=lucene-solr.git;h=9612e31 ]\n\nSOLR-10704 Update the ref guide. ",
            "author": "ASF subversion and git services",
            "id": "comment-16048114"
        }
    ]
}