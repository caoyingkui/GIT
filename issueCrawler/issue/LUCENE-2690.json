{
    "id": "LUCENE-2690",
    "title": "Do MultiTermQuery boolean rewrites per segment",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [],
        "type": "Improvement",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "affect_versions": "4.0-ALPHA",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "MultiTermQuery currently rewrites FuzzyQuery (using TopTermsBooleanQueryRewrite), the auto constant rewrite method and the ScoringBQ rewrite methods using a MultiFields wrapper on the top-level reader. This is inefficient.\n\nThis patch changes the rewrite modes to do the rewrites per segment and uses some additional datastructures (hashed sets/maps) to exclude duplicate terms. All tests currently pass, but FuzzyQuery's tests should not, because it depends for the minimum score handling, that the terms are collected in order..\n\nRobert will fix FuzzyQuery in this issue, too. This patch is just a start.",
    "attachments": {
        "LUCENE-2690-attributes.patch": "https://issues.apache.org/jira/secure/attachment/12457150/LUCENE-2690-attributes.patch",
        "LUCENE-2690.patch": "https://issues.apache.org/jira/secure/attachment/12456665/LUCENE-2690.patch",
        "LUCENE-2690-hack.patch": "https://issues.apache.org/jira/secure/attachment/12456801/LUCENE-2690-hack.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2010-10-08T20:52:17+0000",
            "content": "Updated patch, that also checks for duplicate terms in the fuzzy rewrite. This should be fine now, but we need to fix the FuzzyQuery tests to checks for multiple segments with the same terms that should fail with this patch.\n\nMaybe we need a separate MTQ tests that creates two IndexWriters which add documents with an overlapping term set to both indexes. Queries are then ran using MzultiReader, so we can control merging and make sure the term appears really in two \"segments\". I will work on a test for that. ",
            "author": "Uwe Schindler",
            "id": "comment-12919375"
        },
        {
            "date": "2010-10-09T14:43:01+0000",
            "content": "It'd be nice somehow to have MTQ.getTotalNumberOfTerms return the unique term count instead of the total number of terms visited across all segments... ",
            "author": "Michael McCandless",
            "id": "comment-12919496"
        },
        {
            "date": "2010-10-09T18:43:12+0000",
            "content": "We have to sort the terms coming out of the BytesRefHash, else we get bad seek performance because the within-block seek opto will otherwise often fail to apply...\n\nSo I used a TreeMap instead of HashMap.\n\nThen ran a quick perf test on 10 M Wikipedia index:\n\n\n\n\nQuery\nQPS clean\nQPS mtqseg\nPct diff\n\n\nunit*\n11.83\n11.80\n-0.3%\n\n\nun*d\n13.64\n16.95\n24.3%\n\n\nu*d\n2.67\n3.77\n41.1%\n\n\nun*ed\n34.85\n74.94\n115.0%\n\n\nuni*ed\n183.37\n437.13\n138.4%\n\n\n\n\n\nSo these are good gains!  I can't run FuzzyQuery until we fix the tie-break problem...\n\nI'm really not sure why the prefix query sees no gain yet the others do (I would have actually expected the reverse, because PrefixTermsEnum's accept method is so simple). ",
            "author": "Michael McCandless",
            "id": "comment-12919528"
        },
        {
            "date": "2010-10-10T01:36:48+0000",
            "content": "we fixed some bugs in the patch, it is not ready for committing, but the tests now pass. ",
            "author": "Robert Muir",
            "id": "comment-12919551"
        },
        {
            "date": "2010-10-10T03:02:00+0000",
            "content": "Another iteration... same perf as before but more failures! ",
            "author": "Michael McCandless",
            "id": "comment-12919556"
        },
        {
            "date": "2010-10-10T03:28:01+0000",
            "content": "Same as Mike's patch, only with some nocommits removed (max clause count increased) and added the missing FloatUtils file. ",
            "author": "Uwe Schindler",
            "id": "comment-12919560"
        },
        {
            "date": "2010-10-10T03:31:04+0000",
            "content": "by the way: no more failures - only speed improvements (mostly)! Seems that TestFuzzyQuery2 failed because of the incorrectly increased max clause count!\n\nThe last thing to do is fixing the attribute stuff to separate the two attribute parts. ",
            "author": "Uwe Schindler",
            "id": "comment-12919561"
        },
        {
            "date": "2010-10-10T05:53:10+0000",
            "content": "Guys, awesome improvements!! Here are some comments...\n\n\n\tIn CutOffTermCollector:\n\n final BytesRefHash pendingTerms = new BytesRefHash(new ByteBlockPool(new RecyclingByteBlockAllocator()));\n\nSice we do not reuse the allocator we don't need to use the synced one here. There is no reset call anywhere to free the allocated blocks too. We should just use new BytesRefHash() here.\n\n\n\n\n\n\tBooleanQueryRewrite#rewrite uses a HashMap to keep track of BytesRef and TermFreqBoost. I wonder if we should make use of the ParallelArray technique we us in the indexing chain together with a BytesRefHash which could safe us lots of object creation and GC cost would be lower to once MTQ gets under load. Those MTQ can create a very large amount of objects though and this seems to be a hot spot. I currently have use-cases for direct support of something like a ParallelArray base class in LUCENE-2186 and it seems we can use it here too.\n\n\n\n\n\tIn FloatsUtil#nextAfter I wonder if we need the following lines:  \n\nreturn new Float(direction)\n...\nreturn Double.valueOf(direction).floatValue();\n\n\n since those methods do nothing else than a (float) direction case really.\n\n ",
            "author": "Simon Willnauer",
            "id": "comment-12919573"
        },
        {
            "date": "2010-10-10T11:21:36+0000",
            "content": "\nI attached a hacked patch... nowhere near committable, various tests\nfail, etc... yet I think once we clean it up, the approach is viable.\n\nI started from the patch like 2 iterations ago, and then fixed how the\nMTQ BQ rewrite works so that instead of the two passes (first to\ngather matching terms, second to create weight/scorers & run the BQ),\nit now makes a single pass.\n\nIn that single pass it records which terms matched which segments, and\ncreates TermScorer for each.\n\nAfter the single pass, once we've summed up the top level docFreq for\nall terms, I go back and reset the weights for all the TermScorers,\nsumSQ them, normalize, etc., and then create a FakeQuery object whose\nonly purpose is to remember the per-segment scorers and provide them\nonce .scorer(...) is called on each segment.\n\nThe big gain with this approach is you don't waste effort trying to\nseek to non-existent terms in the sub readers.  Normally the terms\ncache would save you here, but, we never cache a miss and so when we\ntry to look that up again it's always a real (costly) seek.\n\nWith this approach we can disable using the terms cache entirely from\nMTQ.rewrite, which is great.\n\nI believe the patch works correctly, at least for this test, because\non my 10M wikipedia index it gets identical top N results as clean\ntrunk.  Here're the perf gains:\n\n\n\n\nQuery\nQPS clean\nQPS mtqseg\nPct diff\n\n\nstate\n37.49\n37.40\n-0.2%\n\n\nunit*\n11.86\n20.23\n70.5%\n\n\nun*d\n13.58\n30.85\n127.2%\n\n\nuni*ed\n173.22\n535.27\n209.0%\n\n\nu*d\n2.61\n9.05\n247.3%\n\n\nun*ed\n33.59\n120.32\n258.1%\n\n\n\n\n\nNote that these gains already include the sizable gains from the\noriginal patch, but the single pass approach makes further great\ngains, especially eg on the prefix query.\n\nI don't think we should couple this new patch w/ this issue... this\nissue already has awesome gains with a fairly minor change...\nI'll open a new issue. ",
            "author": "Michael McCandless",
            "id": "comment-12919588"
        },
        {
            "date": "2010-10-10T11:23:52+0000",
            "content": "OK I opened LUCENE-2694. ",
            "author": "Michael McCandless",
            "id": "comment-12919590"
        },
        {
            "date": "2010-10-10T11:56:09+0000",
            "content": "\nThe big gain with this approach is you don't waste effort trying to\nseek to non-existent terms in the sub readers. Normally the terms\ncache would save you here, but, we never cache a miss and so when we\ntry to look that up again it's always a real (costly) seek.\n\nWith this approach we can disable using the terms cache entirely from\nMTQ.rewrite, which is great.\n\nThis is the way to go because its horrible for the MTQ to touch the terms cache at all,\nand depending on it for good performance is even worse.\n\nI think if you somehow changed the benchmark to use multiple threads and had different\nqueries executing at the same time, you would see these guys fighting each other\nover huge amounts of terms with df=1 and slowing each other down... but we wouldnt\nhave this problem with them rewriting to FakeQuery\n ",
            "author": "Robert Muir",
            "id": "comment-12919594"
        },
        {
            "date": "2010-10-10T12:02:44+0000",
            "content": "In FloatsUtil#nextAfter I wonder if we need the following lines:\n\nSimon, this is a good point. I poached this method from harmony's StrictMath.nextAfter\nIts interesting to take a look also at their Math.nextAfter\n\nThe difference is this Double promotion, I don't understand if this affects us at all or what it would change.\nIn both cases I do not understand why the boxing is necessary! ",
            "author": "Robert Muir",
            "id": "comment-12919597"
        },
        {
            "date": "2010-10-13T12:19:24+0000",
            "content": "Simon:\n\nSice we do not reuse the allocator we don't need to use the synced one here. There is no reset call anywhere to free the allocated blocks too. We should just use new BytesRefHash() here.\n\nThere is no such ctor in trunk. The only available allocator is the used one. ",
            "author": "Uwe Schindler",
            "id": "comment-12920511"
        },
        {
            "date": "2010-10-13T14:56:15+0000",
            "content": "There is no such ctor in trunk. The only available allocator is the used one.\ngood point there is one in mine  - I'm going to upload a patch for this later / tomorrow... ",
            "author": "Simon Willnauer",
            "id": "comment-12920589"
        },
        {
            "date": "2010-10-13T20:52:50+0000",
            "content": "Current patch makes use of a DirectAllocator without recycling etc. I remove the unnecessary boxing in FlaotsUtil and replaced the terms HashMap with a BytesRefHash. I skipped the latest patch since mike marked it as a hack and opened a new issue for that. This one is based on uwes latest one. All tests pass for me though. ",
            "author": "Simon Willnauer",
            "id": "comment-12920762"
        },
        {
            "date": "2010-10-14T09:23:14+0000",
            "content": "Thanks for the improvements, some comments and changes I did locally:\n\n\n\tThe code in BooleanQueryRewrite uses += for the boost and docFreq in the case of (>=0, no entry in BytesRefHash), but this should only be an assignment. The update and comparison in the assert should be done only when an entry is already in the hash. Boosts should never be sumed up.\n\tThe parts for update with LUCENE-2702 are marked, they wrap currently with new BytesRef(#get) and should be replaced with code like it was before using PagedBytes\n\tThe work for creating the BytesStartArray is much to do, maybe we can unfinal the DirectBytesStartArray and reuse the code. This would make it easier to extend it and simply add more parallel arrays. Client code should not need to replcate the code (this is maybe another issue).\n\tBut there is also a problem with the current code in TermFreqBoostByteStart: The arrays may not use the exact same size as expected (depending how oversize/grow works). As they are parallel arrays, all should be equal size, so we should only use grow/oversize only for the base array and resize the others to same size. Do we have an ArrayUtil method for that? Currently it (may) be broken. Any comments?\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-12920896"
        },
        {
            "date": "2010-10-14T09:57:25+0000",
            "content": "Here a patch with the allocation problems resolved. Also the DirectBytesStartArray is public. ",
            "author": "Uwe Schindler",
            "id": "comment-12920906"
        },
        {
            "date": "2010-10-14T10:01:42+0000",
            "content": "The code in BooleanQueryRewrite uses += for the boost and docFreq in the case of (>=0, no entry in BytesRefHash), but this should only be an assignment. The update and comparison in the assert should be done only when an entry is already in the hash. Boosts should never be sumed up.\nah yeah - true for sure! it did not break since that only happens once when it is initially added. but you are right for sure that this should only be an assignment\n\n\nBut there is also a problem with the current code in TermFreqBoostByteStart: The arrays may not use the exact same size as expected (depending how oversize/grow works). As they are parallel arrays, all should be equal size, so we should only use grow/oversize only for the base array and resize the others to same size. Do we have an ArrayUtil method for that? Currently it (may) be broken. Any comments?\n\ngood catch man! this won't happen here but its cleaner to use the exact same size. The bigger problem is that I missed to add the right constant to the grow method though. I can fix in a minute ",
            "author": "Simon Willnauer",
            "id": "comment-12920909"
        },
        {
            "date": "2010-10-14T10:26:48+0000",
            "content": "Updated patch after committing LUCENE-2707\n\nI also fixed the constant in the grow(float[]) method.\n ",
            "author": "Simon Willnauer",
            "id": "comment-12920915"
        },
        {
            "date": "2010-10-14T10:55:22+0000",
            "content": "Merged patch. I had some addition asserts and some spelling probs in comments. I will now remove the attributes hell.\n\nSo this patch is just for review, before the big changes come  ",
            "author": "Uwe Schindler",
            "id": "comment-12920922"
        },
        {
            "date": "2010-10-14T11:51:23+0000",
            "content": "This is the attributes hell patch (not yet finally done on the FuzzyTermsEnum side, Robert can you review?).\n\nThe change is:\n\n\tBoostAttribute is only added to the TermsEnum, because the TermsEnum produces the boost, the MTQ rewrite consumes.\n\tMaxNonCompetitiveBoostAttribute is owned by the rewrite mode as it is the producer. The TermsEnum consunmes this attribute\n\n\n\nFixing needs the hackish attributes() method in the Fuzzy rewrite.\n\nTODO: Contrib/Solr is not yet reviewed for the API change in MTQ.getTermsEnum()! ",
            "author": "Uwe Schindler",
            "id": "comment-12920930"
        },
        {
            "date": "2010-10-14T12:20:14+0000",
            "content": "small improvements. Also added missing bottomChanged() to Fuzzy ctor. ",
            "author": "Uwe Schindler",
            "id": "comment-12920937"
        },
        {
            "date": "2010-10-14T12:27:54+0000",
            "content": "I will play with the latest patch some, and hopefully upload a new one.\n\nThe real solution to this \"tie-break\" case really is the fact that the priority queue comparison is \"compare by boost, then term text\".\n\nWith the MultiTermsEnum this was no problem, because we look at all terms in order, so we made MaxNonCompetitiveBoostAttribut just a float.\n\nWith per-segment rewrite, then we can look at terms out-of-order.\n\nSo I think if we add the optional term text of the pq's bottom for the previous segment to the MaxNonCompetitiveBoostAttribute itself, then the enum itself can implement the tie break, cleaner, and more efficiently. The rewrite method should or consumer should only be setting the values of this attribute and not dealing with this case. ",
            "author": "Robert Muir",
            "id": "comment-12920940"
        },
        {
            "date": "2010-10-14T13:35:16+0000",
            "content": "Here the patch for Robert, which fails the tie break test. I think you can fix the tie brea  case using the competitiveTerm and we are done  ",
            "author": "Uwe Schindler",
            "id": "comment-12920959"
        },
        {
            "date": "2010-10-14T14:13:48+0000",
            "content": "here is my patch, but the test still fails... I think it is a bug in the rewrite method's priority queue.\n\nit has nothing to do with maxBoostAttribute, because i can add a \"if (true) return;\" to FuzzyTermsEnum.bottomChanged() and the test will still always fail. ",
            "author": "Robert Muir",
            "id": "comment-12920966"
        },
        {
            "date": "2010-10-14T15:50:21+0000",
            "content": "Here the final patch. There is only one special case:\nOur boolean clauses sorting only works for TermQuery. But The BoostOnly fuzzy rewrite creates ConstScoreQueries for each clause, so no reordering.\n\nYou can see this in TestMultiTermQueryRewrites with tests.verbose=true. ",
            "author": "Uwe Schindler",
            "id": "comment-12920991"
        },
        {
            "date": "2010-10-14T16:28:54+0000",
            "content": "This patch contains a better BooleanClause comparator that also reorders ConstantScores that contain TermQuery.\n\nMaybe the ideal case would be that every query gets a method that returns the \"primary\" term or null. The default would be null, but TermQuery and all delegating wrappers should implement that. ",
            "author": "Uwe Schindler",
            "id": "comment-12921006"
        },
        {
            "date": "2010-10-14T16:49:49+0000",
            "content": "Just as a first result here are the results I see on my workstation with a 10 M Wikipedia index (5 segments):\n\n\n\n\nQuery\nQPS trunk\nQPS LUCENE-2690\nPct diff\n\n\nunit state\n3.74\n3.81\n1.8%\n\n\nunited~0.6\n10.07\n10.26\n1.9%\n\n\nunit*\n11.89\n12.65\n6.5%\n\n\nunited~0.7\n39.29\n45.52\n15.9%\n\n\nun*d\n15.17\n27.86\n83.7%\n\n\n\n\n\n\nusing the latest patch.\n\nthose are run with Xmx2G on  an intel core2 3ghz ",
            "author": "Simon Willnauer",
            "id": "comment-12921015"
        },
        {
            "date": "2010-10-14T17:30:36+0000",
            "content": "Simon: Just for comparing with Mike's results: How many segments? ",
            "author": "Uwe Schindler",
            "id": "comment-12921035"
        },
        {
            "date": "2010-10-14T17:50:51+0000",
            "content": "Updated patch with optimization in ctor of FuzzyTermsEnum ",
            "author": "Uwe Schindler",
            "id": "comment-12921048"
        },
        {
            "date": "2010-10-14T18:11:47+0000",
            "content": "Revision of last patch (was buggy).\n\nAbout the \"chicken and egg problem\": Maybe AutomatonTermsEnum should throw Ex, if termComparator is not the exspected one. This would prevent people from trying automaton with other indexes? ",
            "author": "Uwe Schindler",
            "id": "comment-12921056"
        },
        {
            "date": "2010-10-14T18:12:10+0000",
            "content": "Hmmm, it looks like this changes BQ.rewrite() to always rewrite/clone?  Do we need that extra overhead? ",
            "author": "Yonik Seeley",
            "id": "comment-12921057"
        },
        {
            "date": "2010-10-14T18:14:27+0000",
            "content": "Hmmm, it looks like this changes BQ.rewrite() to always rewrite/clone? Do we need that extra overhead? \n\n...until we find a better solution, how we reorder clauses. This has a big speed degradion for lots of MTQs if we don't reorder clauses intelligent. ",
            "author": "Uwe Schindler",
            "id": "comment-12921058"
        },
        {
            "date": "2010-10-14T18:24:58+0000",
            "content": "This has a big speed degradion for lots of MTQs if we don't reorder clauses intelligent.\n\nSeems like the right place for sorting is in the MTQ rewrite to a BQ.\nThe current patch makes BQ rewrite quite a bit more expensive... a clone is always made, and equals is always called on the clone after.\n\nFor normal boolean queries (caused by someone typing in a few words), it seems like a real-world speedup is unlikely (since the terms would need to be in the same tii block).  People generating very large boolean queries should also be able to pre-sort them and not have the overhead imposed every time. ",
            "author": "Yonik Seeley",
            "id": "comment-12921066"
        },
        {
            "date": "2010-10-14T18:51:15+0000",
            "content": "Yes, Mr. No-inlining-Policeman \n\nWe are still working on this patch, its marked as TODO, so we will investigate further. For random queries it had a huge positive impact on query perf. The BQ cloning/reordering was not measureable. We did this after you left Mike's house, so it was just a quick idea. ",
            "author": "Uwe Schindler",
            "id": "comment-12921077"
        },
        {
            "date": "2010-10-14T19:11:39+0000",
            "content": "For random queries it had a huge positive impact on query perf. \n\nIf the clauses were just term queries, that would make me really suspect the test.\nIf it was MTQ queries, then MTQ should sort, not BQ.\n\nThe BQ cloning/reordering was not measureable.\n\nRight - I would expect that for typical queries and typical uses.\nI guess I'm worried about the atypical cases since I've seen so many of them - people putting together single boolean queries with 10K clauses, people doing complex nested queries with thousands of terms, or people executing thousands of queries per request (or per document added, via memory index) where this overhead suddenly becomes significant.\n\nWe are still working on this patch, its marked as TODO, so we will investigate further.\n\nCool  ",
            "author": "Yonik Seeley",
            "id": "comment-12921081"
        },
        {
            "date": "2010-10-14T20:00:54+0000",
            "content": "Attached is a new patch with two changes:\n\n\n\tmoved the BQ reordering to MTQ for now. A general reordering of BooleanQueries should be done in a separate issue (with more performant rewrite). Currently this uses the same comparator like BQ before. You may wonder: why not simply use a sorted map? - the idea is that sorting at the end is faster than using a TreeMap where all terms are compared against (even those falling out of queue). I sort the BQ clauses directly like BQ, to not create an additional array to hold all terms again. Maybe its still faster by copying all BytesRefs to an array before and then build BQ? For now this should be enough. To improve we need SorterTemplate again (for the BytesRefHash case) \n\tfixed an issue with the PQ in TopTermsRewrite: The bottom information was previously only set when the PQ was overflowing. In the past and now its set once the queue is full. This was an optimization bug, its now as it was always. Maybe this explains Mike's score changes on wikipedia index?\n\n\n\nMike: can you test? ",
            "author": "Uwe Schindler",
            "id": "comment-12921101"
        },
        {
            "date": "2010-10-14T23:58:09+0000",
            "content": "Patch with BytesRefHash parallel array sorting instead of sorting the BQ. This should improve all cases. This patch also contains a test that this resorting works.\n\nIt also has an assert that the docFreq is correct. This only slows down tests, but is more secure!\n\nNow we only need to fix contrib and Mike can check the performance (Mike: you have to update your current trunk checkout, too - so scores will compare correct). ",
            "author": "Uwe Schindler",
            "id": "comment-12921179"
        },
        {
            "date": "2010-10-15T00:04:18+0000",
            "content": "Test results on 10M Wiki index:\n\nSingle seg:\n\n\n\n\nQuery\nQPS clean\nQPS mtqseg3\nPct diff\n\n\nunited~0.6\n26.01\n25.48\n-2.0%\n\n\nun*ed\n260.88\n258.61\n-0.9%\n\n\nun*d\n91.52\n90.99\n-0.6%\n\n\nunited~0.7\n98.01\n97.99\n-0.0%\n\n\nstate\n39.95\n39.94\n-0.0%\n\n\nunit*\n33.60\n33.73\n0.4%\n\n\nu*d\n29.87\n30.01\n0.5%\n\n\nuni*ed\n1825.14\n1859.49\n1.9%\n\n\n\n\n\nMulti seg (22 segments):\n\n\n\n\nQuery\nQPS clean\nQPS mtqseg3\nPct diff\n\n\nunit*\n34.68\n34.56\n-0.3%\n\n\nstate\n40.43\n40.30\n-0.3%\n\n\nunited~0.6\n3.18\n3.20\n0.6%\n\n\nu*d\n16.81\n19.55\n16.3%\n\n\nunited~0.7\n11.01\n13.85\n25.8%\n\n\nun*d\n52.51\n66.21\n26.1%\n\n\nun*ed\n42.88\n92.95\n116.8%\n\n\nuni*ed\n175.06\n543.64\n210.5%\n\n\n\n\n\nAnd, the test did not barf so the hits (docID & scores) are identical! ",
            "author": "Michael McCandless",
            "id": "comment-12921182"
        },
        {
            "date": "2010-10-15T10:16:41+0000",
            "content": "Patch:\n\n\tcontrib changed and tests pass (fixed also bug in MemoryIndex TermsEnum)\n\tImproved test in core for dumplicate terms, boosts, sorting\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-12921298"
        },
        {
            "date": "2010-10-15T14:24:31+0000",
            "content": "Final patch, will commit this soon:\n\n\tadded javadocs, changes and migration instructions\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-12921379"
        },
        {
            "date": "2010-10-15T14:26:49+0000",
            "content": "Committed revision: 1022934 ",
            "author": "Uwe Schindler",
            "id": "comment-12921380"
        }
    ]
}