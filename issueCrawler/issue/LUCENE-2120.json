{
    "id": "LUCENE-2120",
    "title": "Possible file handle leak in near real-time reader",
    "details": {
        "labels": "",
        "priority": "Major",
        "components": [
            "core/index"
        ],
        "type": "Bug",
        "fix_versions": [
            "4.0-ALPHA"
        ],
        "affect_versions": "3.1",
        "resolution": "Cannot Reproduce",
        "status": "Closed"
    },
    "description": "Spinoff of LUCENE-1526: Jake/John hit file descriptor exhaustion when testing NRT.\n\nI've tried to repro this, stress testing NRT, saturating reopens, indexing, searching, but haven't found any issue.\n\nLet's try to get to the bottom of it, here...",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2009-12-06T10:03:30+0000",
            "content": "Zoie code that's invoking NRT is here: http://code.google.com/p/zoie/source/browse/branches/BR_DELETE_OPT/java/proj/zoie/impl/indexing/luceneNRT/ThrottledLuceneNRTDataConsumer.java ",
            "author": "Michael McCandless",
            "id": "comment-12786593"
        },
        {
            "date": "2009-12-06T11:03:06+0000",
            "content": "Why does Zoie even retain 3 readers?  Why not keep only the current one? ",
            "author": "Michael McCandless",
            "id": "comment-12786600"
        },
        {
            "date": "2009-12-14T17:51:08+0000",
            "content": "Jake/John, how can I run the Zoie test that hits the file descriptor / memory / incorrect deletions with Lucene's NRT?  Is it some variant of this?\n\n  http://code.google.com/p/zoie/wiki/Running_Performance_Test\n\nIt looks like the test uses both Wikipedia & Medline for document sources?  Do I really need both? ",
            "author": "Michael McCandless",
            "id": "comment-12790260"
        },
        {
            "date": "2009-12-15T01:11:12+0000",
            "content": "Hi Michael:\n\nbq: Why does Zoie even retain 3 readers? Why not keep only the current one?\n\n1 mem reader for when the disk batch, 1 mem reader for the time disk reader indexes, 1 disk reader\n\nbq: It looks like the test uses both Wikipedia & Medline for document sources? Do I really need both?\n\nBy default, it only runs with Medline data. You don't need both. perf/settings/index.properties->data.type dictates which to use, file->medline, wiki->wikipedia\n\nAlso, \n\nYou should use the branch: BR_DELETE_OPT\n\nIt has the optimization you suggested on handling deleted docs, e.g. should not check for each hit candidate with IntSetAccelerator.\nAlso, I have added a DataConsumer to handle delayed reopen for NRT case. You see the file handle leakage quickly with it: see perf/conf/zoie.properties to turn on ThrottledLuceneNRTDataConsumer.\n\nOn my mac, I use lsof to see the file handle count.\n\n-John ",
            "author": "John Wang",
            "id": "comment-12790506"
        },
        {
            "date": "2009-12-15T11:14:54+0000",
            "content": "Thanks for the details John!\n\n\nbq: Why does Zoie even retain 3 readers? Why not keep only the current one?\n\n1 mem reader for when the disk batch, 1 mem reader for the time disk reader indexes, 1 disk reader\n\nHmm \u2013 is this what the private static int MAX_READER_GENERATION = 3; in ThrottledLuceneNRTDataConsumer (link above) is doing? From that code, it looks like it just always retains the last 3 reopened readers... it sounds like the logic to keep the 2 mem readers & 1 disk reader is elsewhere (in addition) in Zoie?  Or maybe this logic is what's doing that?  (Not yet familiar enough w/ Zoie...).\n\nBy default, it only runs with Medline data. You don't need both. perf/settings/index.properties->data.type dictates which to use, file->medline, wiki->wikipedia\n\nOK I'll try to use only Wikipedia \u2013 I've already slurped that down.  It'd be great to get a ContentSource in contrib/benchmark that can produce docs from medline...\n\n\nYou should use the branch: BR_DELETE_OPT\n\nIt has the optimization you suggested on handling deleted docs, e.g. should not check for each hit candidate with IntSetAccelerator.\nAlso, I have added a DataConsumer to handle delayed reopen for NRT case. You see the file handle leakage quickly with it: see perf/conf/zoie.properties to turn on ThrottledLuceneNRTDataConsumer.\n\nOn my mac, I use lsof to see the file handle count.\n\nOK, will do.  So (on this branch) you resolve the deleted docs in the BG, so that, once finished, the reader no longer double-checks hits for deletions?  That sounds like a good improvement...    \n\nIt's sort of a \"warm in the background\" tradeoff, ie, give me my reader very quickly, even if the first searches against it must run a bit slower since they double check deletions, until the warming is done.... vs Lucene which forcefully \"warms\" (making reopen time longer) before returning the reader to you. ",
            "author": "Michael McCandless",
            "id": "comment-12790673"
        },
        {
            "date": "2009-12-15T22:18:43+0000",
            "content": "is this what the private static int MAX_READER_GENERATION = 3\n\nNo, I misunderstood you. This is just a number I think it is safe to keep for make sure you don't close reader while it is being searched.\n\nyou resolve the deleted docs in the BG\n\nThat is not really true. It is stored in a DocIdSet, and it does the skipping in a special TermDocs for avoid deletes.\n\nIt's sort of a \"warm in the background\" tradeoff, ie, give me my reader very quickly, even if the first searches against it must run a bit slower since they double check deletions, until the warming is done.... vs Lucene which forcefully \"warms\" (making reopen time longer) before returning the reader to you.\n\nI am not sure I understand what you mean by \"double check deletions\". Warming is done in the background, in the foreground you do search along with ram indexes that hold transient indexing updates. So one guarantee is that the any time search is happening the disk index is warm, so you don't have to pay the cost related to warming.\n\n\nBTW, i have merged BR_DELETE_OPT down to trunk and did a release. Feel free to take from trunk.\n ",
            "author": "John Wang",
            "id": "comment-12790985"
        },
        {
            "date": "2009-12-16T00:06:06+0000",
            "content": "\nis this what the private static int MAX_READER_GENERATION = 3\n\nNo, I misunderstood you. This is just a number I think it is safe to keep for make sure you don't close reader while it is being searched.\n\nAhh I see.... you can incRef/decRef the reader to handle that.\n\n\nyou resolve the deleted docs in the BG\n\nThat is not really true. It is stored in a DocIdSet, and it does the skipping in a special TermDocs for avoid deletes.\n\nI am not sure I understand what you mean by \"double check deletions\". Warming is done in the background, in the foreground you do search along with ram indexes that hold transient indexing updates. So one guarantee is that the any time search is happening the disk index is warm, so you don't have to pay the cost related to warming.\n\nSorry, maybe I'm misunderstanding what was done on the BR_DELETE_OPT branch \u2013 I thought the idea was to avoid having to always double-check deletes by resolving them to docIDs in the BG (described at https://issues.apache.org/jira/browse/LUCENE-1526?focusedCommentId=12776994&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12776994).  I'll try to diff the branch...\n\nBTW, i have merged BR_DELETE_OPT down to trunk and did a release. Feel free to take from trunk.\n\nOK thanks. ",
            "author": "Michael McCandless",
            "id": "comment-12791065"
        },
        {
            "date": "2009-12-22T20:36:00+0000",
            "content": "I briefly diff'd the BR_DELETE_OPT branch in Zoie \u2013 it looks like you switched (via custom TermDocs/Positions) to enforcing deleted docs using an iterator instead of double-check random-access, right?  I had tried the same thing with Lucene (a while back now), under LUCENE-1476/LUCENE-1536, and found that the simple BitVector gave much better performance than an iterator (which then led to applying filters random-access as well).  Have you tested performance of this switch?  Maybe it works out to be faster, net/net, than doing the double-deletions check? ",
            "author": "Michael McCandless",
            "id": "comment-12793775"
        },
        {
            "date": "2009-12-22T21:50:01+0000",
            "content": "Yes we have done perf tests.\nWe see no indexing throughput improvement, query throughput improved by 40%. ",
            "author": "John Wang",
            "id": "comment-12793803"
        },
        {
            "date": "2009-12-26T13:45:32+0000",
            "content": "Michael:\n\n    I wrote a little test to measure and to understand the perf:\n\nI compare the two methods:\nstatic long testBits(BitVector bv,int numhits) throws Exception{\n    \tlong start = System.nanoTime();\n    \tfor (int i=0;i<numhits;++i){\n    \t\tif (bv.get){\n    \t\t}\n    \t}\n\n\t\tlong end = System.nanoTime();\n\t\treturn (end-start);\n    }\n\n\tstatic long testSkipIter(ArrayDocIdSet delSet,int numhits) throws Exception{\n\t\tDocIdSetIterator delIter = delSet.iterator();\n\t\tint nextDelDoc = delIter.nextDoc();\n\t\tlong start = System.nanoTime();\n\t\tfor (int i=0;i<numhits;++i){\n\t\t\tif (i>=nextDelDoc){\n\t\t\t  if (i==nextDelDoc){\n\t\t\t  }\n\t\t\t  nextDelDoc = delIter.advance;\n\t\t\t}\n\t\t}\n\t\tlong end = System.nanoTime();\n\t\treturn (end-start);\n\t}\n\n\nI removed everything to the barebones to understand the perf implications.\n\nHere are the results on my macbook pro, with numHits and del count:\n\n5M 500: \nbits: 42417850\nskip: 15234650\n\n5M 100:\nbits: 43053650\nskip: 15268850\n\n5M 10k:\nbits: 41694350\nskip: 17504900\n\n5M 100k:\nbits: 41737350\nskip: 42966000\n\n1M 1000:\nbits: 8722700\nskip: 3249100\n\n1M 10k:\nbits: 8210650\nskip: 6119700\n\n1M 25k:\nbits: 8558150\nskip: 9477850\n\nYou see BitVector starts to win with numDeletes's density at about 2%, and it is pretty consistent between diff numHits parameter.\n\nSo in real life scenario, we see that the numDeletes to be very small. However, it would be a great improvement if we can case it out depending on result set. ",
            "author": "John Wang",
            "id": "comment-12794617"
        },
        {
            "date": "2009-12-26T14:22:43+0000",
            "content": "I realized in my ArrayDocIdSet.skip, i am doing binary search always between index 0 and array.length-1, after optimizing this, the percentage is at 4% instead of 2%.\n\nOne important distinction is the memory consumption, the skip algorithm is much compact when the deleted set is sparse compare to the corpus. ",
            "author": "John Wang",
            "id": "comment-12794618"
        },
        {
            "date": "2009-12-28T14:47:19+0000",
            "content": "Thanks for running that test John.  You should probably add something\ninside the if bodies, eg increment a count, to make sure the compiler\ndoesn't optimize it away.  (And print the count in the end to make\nsure it's identical for the two methods).\n\nI had done a similar test in LUCENE-1476, where I hacked\nSegmentTermDocs to represent deleted docs as list-of-int-docIDs, and\nthen \"iterate\" through them, but found only a tiny perf. gain at <= 1%\ndeleted docs.  Not sure why I didn't see a bigger gain... I would\nexpect at low rates to see gains.\n\nAnd the perf. loss at even moderate deletion rates is what inspired\nopening LUCENE-1536, to explore applying filters the same way we apply\ndeleted docs (ie, switch from iterator to random access). ",
            "author": "Michael McCandless",
            "id": "comment-12794819"
        },
        {
            "date": "2009-12-29T02:17:15+0000",
            "content": "Hi Michael:\n\n     You are abs. right! By adding an integer increment in the loop made a difference. From my laptop, I see the break even point at 1.5%.\n\n     For what we are using it for in Zoie, the trade-off is worth-while. Because we rely on Lucene to do the delete check, and use this iteration for skipping over transient deletes, e.g. the ones that have not been made to the index. Normally they are << corpus size, e.g. 100 - 1k out of 5M docs. The memory cost for this is also very small in comparison due to the sparsity of del docset.\n\n-John  ",
            "author": "John Wang",
            "id": "comment-12794998"
        }
    ]
}