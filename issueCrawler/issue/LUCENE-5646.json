{
    "id": "LUCENE-5646",
    "title": "stored fields bulk merging doesn't quite work right",
    "details": {
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Fixed",
        "components": [],
        "affect_versions": "None",
        "status": "Resolved",
        "fix_versions": [
            "4.9",
            "6.0"
        ]
    },
    "description": "from doing some profiling of merging:\n\nCompressingStoredFieldsWriter has 3 codepaths (as i see it):\n1. optimized bulk copy (no deletions in chunk). In this case compressed data is copied over.\n2. semi-optimized copy: in this case its optimized for an existing storedfieldswriter, and it decompresses and recompresses doc-at-a-time around any deleted docs in the chunk.\n3. ordinary merging\n\nIn my dataset, i only see #2 happening, never #1. The logic for determining if we can do #1 seems to be:\n\nonChunkBoundary && chunkSmallEnough && chunkLargeEnough && noDeletions\n\n\n\nI think the logic for \"chunkLargeEnough\" is out of sync with the MAX_DOCS_PER_CHUNK limit? e.g. instead of:\n\nstartOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough\n\n\n\nit should be something like:\n\n(it.chunkDocs >= MAX_DOCUMENTS_PER_CHUNK || startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize) // chunk is large enough\n\n\n\nBut this only works \"at first\" then falls out of sync in my tests. Once this happens, it never reverts back to #1 algorithm and sticks with #2. So its still not quite right.\n\nMaybe Adrien Grand knows off the top of his head...",
    "attachments": {
        "LUCENE-5646.patch": "https://issues.apache.org/jira/secure/attachment/12643485/LUCENE-5646.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-13990259",
            "author": "Robert Muir",
            "content": "perhaps the reason i fall \"out of sync\" is because the first segment ended on a non-chunk boundary (i have no deletions). \n\nSo when it moves to the next segment, it falls out of sync and never \"recovers\". I'm not sure what we can do here: but it seems unless you have very large docs, you aren't gonna get a \"pure-bulk copy\" even with my fix, because the chances of everything aligning is quite rare.\n\nMaybe there is a way we could (temporarily for that marge) force flush() at the end of segment transitions to avoid this, so that the optimization would continue, if we could then recombine them in the next merge to eventually recover? ",
            "date": "2014-05-06T04:05:31+0000"
        },
        {
            "id": "comment-13990262",
            "author": "Robert Muir",
            "content": "another option would be to drop this case #1 entirely, and just focus on optimizing case #2 to be fast.\n\nForget about the performance perspective, its quite a bit scary how the stars have to align just perfectly for case #1 to happen. What if we have bugs that aren't being found because its just so rare? Unfortunately, builds.apache.org just went down as I tried to inspect the clover report to see how many times tests even hit the #1 case, so I don't yet know. ",
            "date": "2014-05-06T04:22:22+0000"
        },
        {
            "id": "comment-13990275",
            "author": "Robert Muir",
            "content": "I investigated clover, the case #1 is hit a few times but only by all the wrong tests:\n\n\norg.apache.solr.search.TestRealTimeGet.testStressGetRealtime\norg.apache.solr.spelling.IndexBasedSpellCheckerTest.testAlternateLocation \norg.apache.solr.morphlines.cell.SolrCellMorphlineTest.testSolrCellDocumentTypes2\n\n\n\nhonestly this is a little concerning. I think we need to do something here. ",
            "date": "2014-05-06T04:58:35+0000"
        },
        {
            "id": "comment-13990289",
            "author": "Robert Muir",
            "content": "Patch disabling the \"one in a billion\" optimization. IMO Its too rare you ever get this, and doesn't see hardly any test coverage.\n\nIn order to fix bulk merge to really bulk-copy over compressed data, it would have to be more sophisticated I think: e.g. allowing/tracking \"padding\" for final chunks in segments and at some point, determining it should GC the padding by forcing decompression/recompression. Honestly I'm not sure that kind of stuff belongs in bulk merge.\n\nNOTE: I didnt not do any similar inspection yet of term vectors. But IIRC that one has less fancy stuff in bulk merge. ",
            "date": "2014-05-06T05:12:15+0000"
        },
        {
            "id": "comment-13990300",
            "author": "Robert Muir",
            "content": "I have not thought about what we shoudl do here 4.8.1-wise.\n\nI cannot prove that this optimization corrupts data yet, but i feel we should either remove it, or fix it and test the crap out of it. \n\nIf there is objection to removing it, I will try to think about a test that really hammers it (possibly would be quite slow). I have the feeling something bad might happen... ",
            "date": "2014-05-06T05:19:21+0000"
        },
        {
            "id": "comment-13990352",
            "author": "Adrien Grand",
            "content": "Indeed, code path #1 almost only happens on the first segment because it is unlikely that segments end on a chunk boundary. I'm +1 to removing it.\n\nNOTE: I didnt not do any similar inspection yet of term vectors. But IIRC that one has less fancy stuff in bulk merge.\n\nActually, they are worse: term vectors only have #1 and #3. So I think we should just use the default merge routine (which is what has been happening in practice in most cases anyway). ",
            "date": "2014-05-06T06:45:43+0000"
        },
        {
            "id": "comment-13990573",
            "author": "Robert Muir",
            "content": "Thanks Adrien, Ill take care of this one first and move on to vectors: ill open an issue ",
            "date": "2014-05-06T12:07:23+0000"
        },
        {
            "id": "comment-13990579",
            "author": "ASF subversion and git services",
            "content": "Commit 1592731 from Robert Muir in branch 'dev/trunk'\n[ https://svn.apache.org/r1592731 ]\n\nLUCENE-5646: Remove rare/undertested bulk merge algorithm in CompressingStoredFieldsWriter ",
            "date": "2014-05-06T12:21:50+0000"
        },
        {
            "id": "comment-13990592",
            "author": "ASF subversion and git services",
            "content": "Commit 1592734 from Robert Muir in branch 'dev/branches/branch_4x'\n[ https://svn.apache.org/r1592734 ]\n\nLUCENE-5646: Remove rare/undertested bulk merge algorithm in CompressingStoredFieldsWriter ",
            "date": "2014-05-06T12:35:02+0000"
        },
        {
            "id": "comment-13990595",
            "author": "Robert Muir",
            "content": "Thanks Adrien! ",
            "date": "2014-05-06T12:37:11+0000"
        }
    ]
}