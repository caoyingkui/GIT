{
    "id": "LUCENE-8516",
    "title": "Make WordDelimiterGraphFilter a Tokenizer",
    "details": {
        "components": [],
        "status": "Open",
        "resolution": "Unresolved",
        "fix_versions": [],
        "affect_versions": "None",
        "labels": "",
        "priority": "Major",
        "type": "Task"
    },
    "description": "Being able to split tokens up at arbitrary points in a filter chain, in effect adding a second round of tokenization, can cause any number of problems when trying to keep tokenstreams to contract.  The most common offender here is the WordDelimiterGraphFilter, which can produce broken offsets in a wide range of situations.\n\nWe should make WDGF a Tokenizer in its own right, which should preserve all the functionality we need, but make reasoning about the resulting tokenstream much simpler.",
    "attachments": {
        "LUCENE-8516.patch": "https://issues.apache.org/jira/secure/attachment/12941667/LUCENE-8516.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16631638",
            "author": "Alan Woodward",
            "content": "Here's a first stab at a patch, which largely just copies existing WDGF functionality.  WordDelimiterTokenizer takes a root tokenizer (so you could base it on standard, keyword or whitespace and still get the extra level of tokenization you need) and then applies its extra tokenization on top.\n\n\tI've removed the 'english possessive' option as we have an existing filter that will do that\n\tI've kept configuration flags, but this may be an opportunity to make the API easier to use - for example, we could make WordDelimiterIterator an abstract class with an overridable isBreak(int previous, int current) method\n\n ",
            "date": "2018-09-28T09:52:56+0000"
        },
        {
            "id": "comment-16631648",
            "author": "Robert Muir",
            "content": "\nWordDelimiterTokenizer takes a root tokenizer (so you could base it on standard, keyword or whitespace and still get the extra level of tokenization you need) and then applies its extra tokenization on top.\n\nThis seems unnecessary. Its already over-configurable as far as how to break itself. ",
            "date": "2018-09-28T10:10:39+0000"
        },
        {
            "id": "comment-16631705",
            "author": "Alan Woodward",
            "content": "It's needed at the moment for the concatenation parameters, in that if you're stringing terms back together again then you need to know where to stop.  Then again, that's an argument to get rid of concatenization.\n\nIME I've seen WDGF used for two purposes: searching for hyphenated or apostrophised words, and searching for IDs or manufacturing part numbers.  Concentrating on the second, we could make this tokenizer something like CharTokenizer only instead of only breaking on specific characters, you can also break on transitions.  For the first, a simple filter that indexes all subparts of a word without changing offsets (more like a synonym filter) might be the way forward? ",
            "date": "2018-09-28T11:05:17+0000"
        },
        {
            "id": "comment-16631756",
            "author": "Robert Muir",
            "content": "It just doesnt seem like it will really improve anything if it takes that tokenizer parameter. Depending on what the tokenizer does, it may still be buggy just like before. still a tokenfilter in disguise. That is clear from the api being totally wrong (tokenizer taking tokenizer as input). ",
            "date": "2018-09-28T12:10:57+0000"
        },
        {
            "id": "comment-16633406",
            "author": "Alan Woodward",
            "content": "Another solution would be for WordDelimiterGraphFilter to no longer amend offsets.  So all token parts would be stored with the offsets of the original undelimited token. ",
            "date": "2018-09-30T15:31:04+0000"
        },
        {
            "id": "comment-16633677",
            "author": "Alan Woodward",
            "content": "Comment from Mike Sokolov:\n\nMy current usage of this filter requires it to be a filter, since I need to precede it with other filters. I think the idea of not touching offsets preserves more flexibility, and since the offsets are already unreliable, we wouldn't be losing much. ",
            "date": "2018-10-01T07:46:15+0000"
        },
        {
            "id": "comment-16634218",
            "author": "Mike Sokolov",
            "content": "Thanks for copy/paste, Alan Woodward, I meant to reply-all, but fumble fingered on the phone. If we can get TokenFilters to stop messing with offsets, then maybe we can kill OffsetAttribute altogether. I feel like that is the vision we are groping towards? Either that or support Offsets in a first-class way in TokenFilters, but nobody seems to want to do that, Do we agree those are the choices? In the end, highlighting the entire original token (even when your query only really matches a piece of it) doesn't seem so terrible. I would advocate for fixing the API problems first by tightening the API around offsets, and then later if we want to make it possible to do more precise offsets / multiple passes of token splitting, we can maybe find a way to do that, but the \"highlight a subtoken doesn't work\" seems like a relatively minor problem, not really deserving of major efforts to support it. ",
            "date": "2018-10-01T15:50:43+0000"
        },
        {
            "id": "comment-16636467",
            "author": "David Smiley",
            "content": "or support Offsets in a first-class way in TokenFilters, but nobody seems to want to do that,\n\nCan you elaborate?  This rings a bell but I forget.  \n\nHonestly I don't see the big deal in a token filter doing tokenization.  I see it has certain challenges but don't think it's fundamentally wrong. ",
            "date": "2018-10-03T05:18:57+0000"
        },
        {
            "id": "comment-16638156",
            "author": "Mike Sokolov",
            "content": "Can you elaborate? This rings a bell but I forget.\u00a0\nLUCENE-8450 has the discussion. The basic idea there was to add some methods to TokenStream that would be analogous to CharFilter.correctOffset so TokenFilters could also apply offsets correctly. ",
            "date": "2018-10-04T12:33:02+0000"
        }
    ]
}