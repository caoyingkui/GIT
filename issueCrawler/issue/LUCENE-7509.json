{
    "id": "LUCENE-7509",
    "title": "[smartcn] Some chinese text is not tokenized correctly with Chinese punctuation marks appended",
    "details": {
        "resolution": "Unresolved",
        "affect_versions": "6.2.1",
        "components": [
            "modules/analysis"
        ],
        "labels": "",
        "fix_versions": [],
        "priority": "Major",
        "status": "Open",
        "type": "Bug"
    },
    "description": "Some chinese text is not tokenized correctly with Chinese punctuation marks appended.\n\ne.g.\n\u78a7\u7eff\u7684\u773c\u73e0 is tokenized as \u78a7\u7eff|\u7684|\u773c\u73e0. Which is correct.\n\nBut \n\u78a7\u7eff\u7684\u773c\u73e0\uff0c\uff08with a Chinese punctuation appended )is tokenized as \u78a7\u7eff|\u7684|\u773c|\u73e0\uff0c\n\nThe similar case happens when text with numbers appended.\n\ne.g.\n\u751f\u6d3b\u62a58\u67084\u53f7 -->\u751f\u6d3b|\u62a5|8|\u6708|4|\u53f7\n\u751f\u6d3b\u62a5-->\u751f\u6d3b\u62a5\n\nTest Sample:\npublic static void main(String[] args) throws IOException\n{\n    Analyzer analyzer = new SmartChineseAnalyzer(); /* will load stopwords */\n    System.out.println(\"Sample1=======\");\n    String sentence = \"\u751f\u6d3b\u62a58\u67084\u53f7\";\n    printTokens(analyzer, sentence);\n    sentence = \"\u751f\u6d3b\u62a5\";\n    printTokens(analyzer, sentence);\n    System.out.println(\"Sample2=======\");\n    \n    sentence = \"\u78a7\u7eff\u7684\u773c\u73e0\uff0c\";\n    printTokens(analyzer, sentence);\n    sentence = \"\u78a7\u7eff\u7684\u773c\u73e0\";\n    printTokens(analyzer, sentence);\n    \n    analyzer.close();\n\n  }\n\n  private static void printTokens(Analyzer analyzer, String sentence) throws IOException{\n    System.out.println(\"sentence:\" + sentence);\n    TokenStream tokens = analyzer.tokenStream(\"dummyfield\", sentence);\n    tokens.reset();\n    CharTermAttribute termAttr = (CharTermAttribute) tokens.getAttribute(CharTermAttribute.class);\n    while (tokens.incrementToken()) \n{\n      System.out.println(termAttr.toString());\n    }\n    tokens.close();\n  }\n\nOutput:\nSample1=======\nsentence:\u751f\u6d3b\u62a58\u67084\u53f7\n\u751f\u6d3b\n\u62a5\n8\n\u6708\n4\n\u53f7\nsentence:\u751f\u6d3b\u62a5\n\u751f\u6d3b\u62a5\nSample2=======\nsentence:\u78a7\u7eff\u7684\u773c\u73e0\uff0c\n\u78a7\u7eff\n\u7684\n\u773c\n\u73e0\nsentence:\u78a7\u7eff\u7684\u773c\u73e0\n\u78a7\u7eff\n\u7684\n\u773c\u73e0",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-15595364",
            "author": "Michael McCandless",
            "date": "2016-10-21T15:09:06+0000",
            "content": "Hi peina, could you please turn your test fragments into a test that fails?  See e.g. https://wiki.apache.org/lucene-java/HowToContribute\n\nDo you know how to fix this?  Is there a Unicode API we should be using to more generally check for punctuation, so that Chinese punctuation is included? "
        },
        {
            "id": "comment-15714052",
            "author": "Chang KaiShin",
            "date": "2016-12-02T05:10:49+0000",
            "content": "This is not a bug. The underlying Viterbi algorithm segmenting Chinese sentences is based on the probability of the occurrences of the Chinese Characters. Take sentence \"\u751f\u6d3b\u62a58\u67084\u53f7\" as an example. The \"\u62a5\" here is meant 2 meanings. If it is placed in the end of the sentence. It means daily newspaper. However, if placed with conjunctions with other Chinese Characters. It is meant to report something. So the algorithm segments \"\u62a5\" as independent word to mean reporting. On the Contrary,  \"\u751f\u6d3b\u62a5\" is assumed to have higher chance to mean daily newspaper. You need to add some words to the dictionary to let the algorithms to learn, so that you get the correct result you wanted. \n\nThe same induction applies to the case \"\u78a7\u7eff\u7684\u773c\u73e0\uff0c\". It was segmented into \u78a7\u7eff|\u7684|\u773c| \u73e0\uff0c\nThe punctuation \"\uff0c\" is a stopword, so the result is \u78a7\u7eff|\u7684|\u773c| \u73e0. I suggest put the word \"\u773c\u73e0\" into the dictionary , the problem should be solved. "
        },
        {
            "id": "comment-15721489",
            "author": "peina",
            "date": "2016-12-05T07:36:30+0000",
            "content": "Thanks. Make sense to me. "
        },
        {
            "id": "comment-15721497",
            "author": "peina",
            "date": "2016-12-05T07:39:50+0000",
            "content": "BTW, is there any chance that https://issues.apache.org/jira/browse/LUCENE-7508 will be fixed? "
        }
    ]
}