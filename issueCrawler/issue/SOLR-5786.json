{
    "id": "SOLR-5786",
    "title": "MapReduceIndexerTool --help output is missing large parts of the help text",
    "details": {
        "affect_versions": "4.7",
        "status": "Closed",
        "fix_versions": [
            "4.8"
        ],
        "components": [
            "contrib - MapReduce"
        ],
        "type": "Bug",
        "priority": "Major",
        "labels": "",
        "resolution": "Duplicate"
    },
    "description": "As already mentioned repeatedly and at length, this is a regression introduced by the fix in https://issues.apache.org/jira/browse/SOLR-5605\n\nHere is the diff of --help output before SOLR-5605 vs after SOLR-5605:\n\n\n130,235c130\n<                          lucene  segments  left  in   this  index.  Merging\n<                          segments involves reading  and  rewriting all data\n<                          in all these  segment  files, potentially multiple\n<                          times,  which  is  very  I/O  intensive  and  time\n<                          consuming. However, an  index  with fewer segments\n<                          can later be merged  faster,  and  it can later be\n<                          queried  faster  once  deployed  to  a  live  Solr\n<                          serving shard. Set  maxSegments  to  1 to optimize\n<                          the index for low query  latency. In a nutshell, a\n<                          small maxSegments  value  trades  indexing latency\n<                          for subsequently improved query  latency. This can\n<                          be  a  reasonable  trade-off  for  batch  indexing\n<                          systems. (default: 1)\n<   --fair-scheduler-pool STRING\n<                          Optional tuning knob  that  indicates  the name of\n<                          the fair scheduler  pool  to  submit  jobs to. The\n<                          Fair Scheduler is a  pluggable MapReduce scheduler\n<                          that provides a way to  share large clusters. Fair\n<                          scheduling is a method  of  assigning resources to\n<                          jobs such that all jobs  get, on average, an equal\n<                          share of resources  over  time.  When  there  is a\n<                          single job  running,  that  job  uses  the  entire\n<                          cluster. When  other  jobs  are  submitted,  tasks\n<                          slots that free up are  assigned  to the new jobs,\n<                          so that each job gets  roughly  the same amount of\n<                          CPU time.  Unlike  the  default  Hadoop scheduler,\n<                          which forms a queue of  jobs, this lets short jobs\n<                          finish in reasonable time  while not starving long\n<                          jobs. It is also an  easy  way  to share a cluster\n<                          between multiple of users.  Fair  sharing can also\n<                          work with  job  priorities  -  the  priorities are\n<                          used as  weights  to  determine  the  fraction  of\n<                          total compute time that each job gets.\n<   --dry-run              Run in local mode  and  print  documents to stdout\n<                          instead of loading them  into  Solr. This executes\n<                          the  morphline  in  the  client  process  (without\n<                          submitting a job  to  MR)  for  quicker turnaround\n<                          during early  trial  &  debug  sessions. (default:\n<                          false)\n<   --log4j FILE           Relative or absolute  path  to  a log4j.properties\n<                          config file on the  local  file  system. This file\n<                          will  be  uploaded  to   each  MR  task.  Example:\n<                          /path/to/log4j.properties\n<   --verbose, -v          Turn on verbose output. (default: false)\n<   --show-non-solr-cloud  Also show options for  Non-SolrCloud  mode as part\n<                          of --help. (default: false)\n< \n< Required arguments:\n<   --output-dir HDFS_URI  HDFS directory to  write  Solr  indexes to. Inside\n<                          there one  output  directory  per  shard  will  be\n<                          generated.    Example:     hdfs://c2202.mycompany.\n<                          com/user/$USER/test\n<   --morphline-file FILE  Relative or absolute path  to  a local config file\n<                          that contains one  or  more  morphlines.  The file\n<                          must     be      UTF-8      encoded.      Example:\n<                          /path/to/morphline.conf\n< \n< Cluster arguments:\n<   Arguments that provide information about your Solr cluster. \n< \n<   --zk-host STRING       The address of a ZooKeeper  ensemble being used by\n<                          a SolrCloud cluster. This  ZooKeeper ensemble will\n<                          be examined  to  determine  the  number  of output\n<                          shards to create  as  well  as  the  Solr  URLs to\n<                          merge the output shards into  when using the --go-\n<                          live option. Requires that  you  also  pass the --\n<                          collection to merge the shards into.\n<                          \n<                          The   --zk-host   option   implements   the   same\n<                          partitioning semantics as  the  standard SolrCloud\n<                          Near-Real-Time (NRT)  API.  This  enables  to  mix\n<                          batch  updates  from   MapReduce   ingestion  with\n<                          updates from standard  Solr  NRT  ingestion on the\n<                          same SolrCloud  cluster,  using  identical  unique\n<                          document keys.\n<                          \n<                          Format is: a  list  of  comma  separated host:port\n<                          pairs,  each  corresponding   to   a   zk  server.\n<                          Example: '127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:\n<                          2183' If the optional  chroot  suffix  is used the\n<                          example  would  look  like:  '127.0.0.1:2181/solr,\n<                          127.0.0.1:2182/solr,127.0.0.1:2183/solr'     where\n<                          the client would  be  rooted  at  '/solr'  and all\n<                          paths would  be  relative  to  this  root  -  i.e.\n<                          getting/setting/etc... '/foo/bar' would  result in\n<                          operations being run on  '/solr/foo/bar' (from the\n<                          server perspective).\n<                          \n< \n< Go live arguments:\n<   Arguments for  merging  the  shards  that  are  built  into  a  live Solr\n<   cluster. Also see the Cluster arguments.\n< \n<   --go-live              Allows you to  optionally  merge  the  final index\n<                          shards into a  live  Solr  cluster  after they are\n<                          built. You can pass the  ZooKeeper address with --\n<                          zk-host and the relevant  cluster information will\n<                          be auto detected.  (default: false)\n<   --collection STRING    The SolrCloud  collection  to  merge  shards  into\n<                          when  using  --go-live   and  --zk-host.  Example:\n<                          collection1\n<   --go-live-threads INTEGER\n<                          Tuning knob that indicates  the  maximum number of\n<                          live merges  to  run  in  parallel  at  one  time.\n<                          (default: 1000)\n< \n---\n>       \n\n\n\nAs already mentioned repeatedly and at length, this bug is because there's a change related to buffer flushing in argparse4 >= 0.4.2. \n\nThe fix is to apply CDH-16434 to MapReduceIndexerTool.java as follows:\n\n\n-            parser.printHelp(new PrintWriter(System.out));  \n+            parser.printHelp();",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "author": "Uwe Schindler",
            "id": "comment-13982539",
            "date": "2014-04-27T23:25:40+0000",
            "content": "Close issue after release of 4.8.0 "
        }
    ]
}