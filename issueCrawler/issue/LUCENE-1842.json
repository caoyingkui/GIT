{
    "id": "LUCENE-1842",
    "title": "Add reset(AttributeSource) method to AttributeSource",
    "details": {
        "labels": "",
        "priority": "Minor",
        "components": [
            "modules/analysis"
        ],
        "type": "Wish",
        "fix_versions": [],
        "affect_versions": "None",
        "resolution": "Won't Fix",
        "status": "Resolved"
    },
    "description": "Originally proposed in LUCENE-1826\n\nProposing the addition of the following method to AttributeSource\n\npublic void reset(AttributeSource input) {\n    if (input == null) {\n      throw new IllegalArgumentException(\"input AttributeSource must not be null\");\n    }\n    this.attributes = input.attributes;\n    this.attributeImpls = input.attributeImpls;\n    this.factory = input.factory;\n}\n\n\n\nImpacts:\n\n\trequires all TokenStreams/TokenFIlters/etc to call addAttribute() in their reset() method, not in their constructor\n\trequires making AttributeSource.attributes and AttributeSource.attributesImpl non-final\n\n\n\nAdvantages:\nAllows creating only a single actual AttributeSource per thread that can then be used for indexing with a multitude of TokenStream/Tokenizer combinations (allowing utmost reuse of TokenStream/Tokenizer instances)\nthis results in only a single \"attributes\"/\"attributesImpl\" map being required per thread\naddAttribute() calls will almost always return right away (will only be \"initialized\" once per thread)",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2009-08-21T23:48:23+0000",
            "content": "\nAllows creating only a single actual AttributeSource per thread that can then be used for indexing with a multitude of TokenStream/Tokenizer combinations (allowing utmost reuse of TokenStream/Tokenizer instances)\n\nI don't understand why this would be better. Now if reusable TokenStreams are used then they contain already all necessary attributes after the first use and reset only needs to reset the default values.\n\nWith your proposed change an AttributeSource would be used across different TokenStreams. So for each document you would fill the maps multiple times with different attributes. Even if the reflection costs are low, you still pay the price for filling the two hashmaps and the cache lookups. \n\nWhy is that better than having per-thread reusable Tokenstreams with their own AttributeSources and not-changing attributes?\n\nOr maybe I'm missing something in your proposal? ",
            "author": "Michael Busch",
            "id": "comment-12746369"
        },
        {
            "date": "2009-08-22T00:30:37+0000",
            "content": "still pay the price for filling the two hashmaps and the cache lookups. \nthis would only ever be incurred once per thread (if the same root AttributeSource was always used)\nthe cache lookups would still need to be done at TokenStream.reset() time, however they would pretty much always get a hit\n\nthe main use case this proposal supports is as follows:\n\ni have a TokenStream that merges multiple sub token streams (i call this out in LUCENE-1826)\nin order to do this really efficiently, all sub token streams need to share the same AttributeSource\nthen, the \"merging\" TokenStream can just iterate through its sub streams, calling incrementToken() to consume all tokens from each stream\n\nwithout the ability to reset the \"sub\" streams AttributeSource to the same AttributeSource used by this merging TokenStream, the you have to copy the attributes from the sub streams as you iterate \nfurthermore, the \"sub\" TokenStreams could potentially be any TokenStream (or chain of TokenStreams rooted with a Tokenizer)\nwithout the reset(AttributeSource) method, i would have to create the TokenStream chain anew for every \"merging\" TokenStream (or do the attribute copying approach) ",
            "author": "Tim Smith",
            "id": "comment-12746380"
        },
        {
            "date": "2009-08-22T08:10:03+0000",
            "content": "I still do not understand your proposal. You can always create all tokenizer chains at the beginning with exactly one AttributeSource (after LUCENE-1826). You are then free to call incrementToken() on all sub-tokenstreams and all these calls will put the tokenized values in the same attributes.\n\nAdding a reset(AttributeSource) method would not help really, as you would have to do this for the whole Tokenizer chain. If you do it in the wrong way, there may be some tokenfilters in the chain that use a different attributesource and so on. Because of all these problem and the complexity, we do not want to have setters for AttributeSources or changes of AttributeFactory and so on (and because of this the design is to make the fields \"final\" and we have this extra warning). During the lifetime of one TokenStream, there is in my opinion no real use-case for changing its attribute maps that rectify the added complexity and risk for errors. \n\nThe cost of adding Attributes is very low if you reuse TokenStreams, what you could even do with your concenatting TokenStream. ",
            "author": "Uwe Schindler",
            "id": "comment-12746428"
        },
        {
            "date": "2009-08-22T08:39:11+0000",
            "content": "Tim, why can't you achieve the same by implementing an Analyzer with reusableTokenStream(). You could have a class holding all streams/tokenizers that you put into your own ThreadLocal. Then of course you need an instance of each tokenizer/stream/filter per thread - is that what you're trying to avoid? You create them all once per thread, so their AttributeSource never has to change. You can still change for every document (field) which input streams your merging tokenizer uses, as long as it takes them from your ThreadLocal.\n\nOne other question: does you merging tokenizer know before it can inspect the tokens returned from its inputs which one it will itself return? The answer must be yes, otherwise you can't avoid copying in any case, right?\n\nIf this comment doesn't make any sense to you then I still didn't understand what you're trying to achieve.  ",
            "author": "Michael Busch",
            "id": "comment-12746429"
        },
        {
            "date": "2009-08-22T12:47:22+0000",
            "content": "Here's some pseudo code to hopefully fully show this use case:\n\n\n// These guys are initialized once\nAnalyzer analyzer1 = new SimpleAnalyzer();\nAnalyzer analyzer2 = new StandardAnalyzer();\nAnalyzer analyzer3 = new LowerCaseAnalyzer();\n\n// This is done on a per Field basis\nReader source1 = new StringReader(\"some text\");\nReader source2 = new StringReader(\"some more text\");\nReader source3 = new stringReader(\"final text\");\n\nTokenStream stream1 = analyzer1.reusableTokenStream(source1);\nTokenStream stream2 = analyzer2.reusableTokenStream(source2);\nTokenStream stream3 = analyzer3.reusableTokenStream(source3);\n\n// Create the container for the shared attributes map\nAttributeSource attrs = new AttributeSource();\n\n// Have all streams share the same attributes map\nstream1.reset(attrs);\nstream2.reset(attrs);\nstream3.reset(attrs);\n\n// Create my merging TokenStream (have it use attrs as its attribute source)\nTokenStream merger = new MergeTokenStreams(attrs, new TokenStream[] { stream1, stream2, stream3 });\n\n/// Add a filter that will put a token prior to the source token stream, and after the source token stream is exhausted\nTokenStream finalStream = new WrapFilter(merger, \"anchor token\");\n\n// finalStream will now be passed to the indexer\n\n\n\nHopefully this makes this use case more clear\nIn order to use reusableTokenStreams from the Analyzers, the MergeTokenStreams must be able to share its attributes map with the underlaying TokenStreams its merging\notherwise, MergeTokenStreams has to do something like this in its incrementToken:\n\npublic boolean incrementToken() {\n if (currentStream.incrementToken()) {\n    copy currentStream.termAttr into my local termAttr\n    copy currentStream.offsetsAttr into my local termAttr\n    return true;\n  } else {\n    advance currentStream to be the next stream in line\n  } \n}\n\n\n\nas opposed to:\n\npublic boolean incrementToken() {\n  if (currentStream.incrementToken()) {\n    // don't need to do anything (because underlying tokenstreams share the same attributes map as me)\n    return true;\n  } else {\n    advance currentStream to be the next stream in line\n  }\n}\n\n\n\nHopefully this makes my use case clear ",
            "author": "Tim Smith",
            "id": "comment-12746450"
        },
        {
            "date": "2009-08-22T12:59:55+0000",
            "content": "As I said, this can be solved the following way:\nWhen you create the TokenStreams, tell them to use one AttributeSource (using the new ctors from LUCENE-1826). The problem in your example is, that you use reuseableTokenStreams from an analyzer which has no possibility to use a different factory/attributesource. To solve this here, either create the streams directly and cache the complete merger (use a MergeAnalyzer that caches the tokenstreams to merge) or we have to add a factory/attributesource also to Analyzer's ctor. ",
            "author": "Uwe Schindler",
            "id": "comment-12746451"
        },
        {
            "date": "2009-08-22T13:07:32+0000",
            "content": "Yes, i know that creating the Tokenizer/TokenStream fully each time will do the trick as well, but i was hoping for some way to take advantage of the \"reusableTokenStream\" concepts (esecially in the case of Tokenizers that take a long time to construct (load resources/etc))\n\nwhat i guess i really want is this method added to Analyzer:\n\npublic TokenStream tokenStream(AttributeSource attrs, Reader reader);\n\n\n\nbut i assume this would either have to reconstruct the full TokenStream chain every time (could be costly), or it would require AttributeSource.reset(AttributeSource) method in order to reuse saved streams\n\n ",
            "author": "Tim Smith",
            "id": "comment-12746452"
        },
        {
            "date": "2009-08-22T13:08:48+0000",
            "content": "Or simply create a MergeAnalyzer that creates the whole chain one time and reuses it (this is what I suggest). ",
            "author": "Uwe Schindler",
            "id": "comment-12746453"
        },
        {
            "date": "2009-08-22T13:10:21+0000",
            "content": "but i assume this would either have to reconstruct the full TokenStream chain every time (could be costly), or it would require AttributeSource.reset(AttributeSource) method in order to reuse saved streams\n\nIn this case, I said: Add a AttributeSource/AttributeFactory to the Analyzer's ctor. EDIT: this would break the multi-threaded reusableility of analyzers \u2013 ignore this.\n\nThe only possibility you have is to create the whole chain incl. mergers one time and reuse this merged tokenstrea, This would conform to the usage of Analyzers. ",
            "author": "Uwe Schindler",
            "id": "comment-12746454"
        },
        {
            "date": "2009-08-22T13:14:51+0000",
            "content": "The problem with the \"MergeAnalyzer\" is that it requiers multiple Readers as input, but i think idea does put me on another (potentially better) track for handling sharing the same underlaying AttributeSource for all the merged tokenstreams (as well as sharing reusable TokenStreams)\n\nI'll try to put this to the test on monday when i get back to work ",
            "author": "Tim Smith",
            "id": "comment-12746455"
        },
        {
            "date": "2009-08-22T13:19:54+0000",
            "content": "I wanted to still warn you about merging tokenstreams this way: It completely breaks with offsets! Highlighter would completely fail to highlight.\n\nIn your case, I would simply let the IndexWriter merge the fields by adding the field mltiple times. ",
            "author": "Uwe Schindler",
            "id": "comment-12746456"
        },
        {
            "date": "2009-08-22T13:27:47+0000",
            "content": "I would never use the Merging TokenStream when doing highlighting anyway, \nalso, i'm sure i can get the Merging TokenStream to update the offsets to be appropriate (based on the merge) \u2013 i never use offsets for anything right now anyway (although i may in the future)\n\nand i can't let the indexer do the merging because i want to add additional analytics on top of the merge (which can't be done on the sub streams in piecemeal fashion)\n\nalso, Merging may not be a straight \"cat\", more complex merges may merge sorted streams into a final sorted token stream, interleave tokens from sub streams in round robin fashion, and so on (the only use i have for it right now is the straight \"cat\", however this concept could be applied to more nasty stuff)\n ",
            "author": "Tim Smith",
            "id": "comment-12746457"
        },
        {
            "date": "2009-08-26T13:56:44+0000",
            "content": "I'm happy with just having the Tokenizer's provide the AttributeSource in the constructor (this provides the ability for me to do my merging token stream stuff pretty well after all)\n\nreset(AttributeSource) not required (and would cause some issues if misused i agree) ",
            "author": "Tim Smith",
            "id": "comment-12747947"
        }
    ]
}