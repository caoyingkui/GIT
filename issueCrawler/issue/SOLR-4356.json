{
    "id": "SOLR-4356",
    "title": "SOLR 4.1 Out Of Memory error After commit of a few thousand Solr Docs",
    "details": {
        "affect_versions": "4.1",
        "status": "Closed",
        "fix_versions": [
            "4.8"
        ],
        "components": [
            "clients - java",
            "Schema and Analysis",
            "Tests"
        ],
        "type": "Improvement",
        "priority": "Major",
        "labels": "",
        "resolution": "Invalid"
    },
    "description": "we are testing solr 4.1 running inside tomcat 7 and java 7 with  following options\n\nJAVA_OPTS=\"-Xms256m -Xmx2048m -XX:MaxPermSize=1024m -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode -XX:+ParallelRefProcEnabled -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/home/ubuntu/OOM_HeapDump\"\n\nour source code looks like following:\n/**** START *****/\nint noOfSolrDocumentsInBatch = 0;\nfor(int i=0 ; i<5000 ; i++) {\n    SolrInputDocument solrInputDocument = getNextSolrInputDocument();\n    server.add(solrInputDocument);\n    noOfSolrDocumentsInBatch += 1;\n    if(noOfSolrDocumentsInBatch == 10) \n{\n        server.commit();\n        noOfSolrDocumentsInBatch = 0;\n    }\n}\n/**** END *****/\n\nthe method \"getNextSolrInputDocument()\" generates a solr document with 100 fields (average). Around 50 of the fields are of \"text_general\" type.\nSome of the \"test_general\" fields consist of approx 1000 words rest consists of few words. Ouf of total fields there are around 35-40 multivalued fields (not of type \"text_general\").\nWe are indexing all the fields but storing only 8 fields. Out of these 8 fields two are string type, five are long and one is boolean. So our index size is only 394 MB. But the RAM occupied at time of OOM is around 2.5 GB. Why the memory is so high even though the index size is small?\nWhat is being stored in the memory? Our understanding is that after every commit documents are flushed to the disk.So nothing should remain in RAM after commit.\n\nWe are using the following settings:\n\nserver.commit() set waitForSearcher=true and waitForFlush=true\nsolrConfig.xml has following properties set:\ndirectoryFactory = solr.MMapDirectoryFactory\nmaxWarmingSearchers = 1\ntext_general data type is being used as supplied in the schema.xml with the solr setup.\nmaxIndexingThreads = 8(default)\n<autoCommit><maxTime>15000</maxTime><openSearcher>false</openSearcher></autoCommit>\n\nWe get Java heap Out Of Memory Error after commiting around 3990 solr documents.Some of the snapshots of memory dump from profiler are attached.\n\ncan somebody please suggest what should we do to minimize/optimize the memory consumption in our case with the reasons?\nalso suggest what should be optimal values and reason for following parameters of solrConfig.xml \nuseColdSearcher - true/false?\n    maxwarmingsearchers- number\n    spellcheck-on/off?\n    omitNorms=true/false?\n    omitTermFreqAndPositions?\n    mergefactor? we are using default value 10\n    java garbage collection tuning parameters ?",
    "attachments": {
        "memorydump2.png": "https://issues.apache.org/jira/secure/attachment/12566496/memorydump2.png",
        "memorydump1.png": "https://issues.apache.org/jira/secure/attachment/12566495/memorydump1.png"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Mark Miller",
            "id": "comment-13562734",
            "date": "2013-01-25T14:42:20+0000",
            "content": "Please send questions like this to the user list - then open a JIRA issue if you determine the issue is a bug. "
        },
        {
            "author": "Steve Rowe",
            "id": "comment-13717207",
            "date": "2013-07-23T18:47:28+0000",
            "content": "Bulk move 4.4 issues to 4.5 and 5.0 "
        },
        {
            "author": "Uwe Schindler",
            "id": "comment-13982535",
            "date": "2014-04-27T23:25:39+0000",
            "content": "Close issue after release of 4.8.0 "
        }
    ]
}