{
    "id": "LUCENE-8526",
    "title": "StandardTokenizer doesn't separate hangul characters from other non-CJK chars",
    "details": {
        "components": [],
        "status": "Resolved",
        "resolution": "Not A Bug",
        "fix_versions": [],
        "affect_versions": "None",
        "labels": "",
        "priority": "Minor",
        "type": "Improvement"
    },
    "description": "It was first reported here https://github.com/elastic/elasticsearch/issues/34285.\nI don't know if it's the expected behavior but the StandardTokenizer does not split words\nwhich are composed of a mixed of non-CJK characters and hangul syllabs. For instance \"\ud55c\uad6d2018\" or \"\ud55c\uad6dabc\" is kept as is by this tokenizer and mark as an alpha-numeric group. This breaks the CJKBigram token filter which will not build bigrams on such groups. The other CJK characters are correctly splitted when they are mixed with other alphabet so I'd expect the same for hangul.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "id": "comment-16640228",
            "author": "Steve Rowe",
            "content": "Hangul syllables' UAX#29\u00a0word-break category is ALetter (see e.g. the properties for U+AC00).  The word-break rules in UAX#29 don't have any special handling for Hangul syllables.\n\nThe other CJK characters are correctly splitted when they are mixed with other alphabet so I'd expect the same for hangul.\n\nUAX#29 word break rules include no provisions for breaking at script boundaries.  By contrast, ICUTokenizer does include this functionality.  From https://lucene.apache.org/core/7_5_0/analyzers-icu/org/apache/lucene/analysis/icu/segmentation/ICUTokenizer.html:\n\nWords are broken across script boundaries, then segmented according to the BreakIterator and typing provided by the ICUTokenizerConfig\n\nThe current StandardTokenizer implements Unicode 6.3, which is pretty old.  Lucene should update to the recently released JFlex 1.7, which supports Unicode 9.0. (I'll go make an issue.)  But I checked, and Unicode 11.0 still does not include any script-boundary splitting. ",
            "date": "2018-10-05T19:06:32+0000"
        },
        {
            "id": "comment-16640244",
            "author": "Steve Rowe",
            "content": "Lucene should update to the recently released JFlex 1.7, which supports Unicode 9.0. (I'll go make an issue.)\n\nSee LUCENE-8527 ",
            "date": "2018-10-05T19:21:24+0000"
        },
        {
            "id": "comment-16640245",
            "author": "Jim Ferenczi",
            "content": "Ok thanks for explaining Steve Rowe. I thought that script boundary break was part of the UAX#29 and that the ICUTokenizer and StandardTokenizer should behave the same regarding CJK splits. We can maybe add a note in the CJKBigram filter regarding this behavior when the StandardTokenizer is used ? ",
            "date": "2018-10-05T19:22:51+0000"
        },
        {
            "id": "comment-16640279",
            "author": "Steve Rowe",
            "content": "We can maybe add a note in the CJKBigram filter regarding this behavior when the StandardTokenizer is used ?\n\n+1\n\nHow's this, to be added to the CJKBigramFilter class javadoc:\n\n\n * <p>\n * Unlike ICUTokenizer, StandardTokenizer does not split at script boundaries.\n * Korean Hangul characters are treated the same as many other scripts'\n * letters, and as a result, StandardTokenizer can produce tokens that mix\n * Hangul and non-Hangul characters, e.g. \"\ud55c\uad6dabc\".  Such mixed-script tokens\n * are typed as <code>&lt;ALPHANUM&gt;</code> rather than\n * <code>&lt;HANGUL&gt;</code>, and as a result, will not be converted to \n * bigrams by CJKBigramFilter. \n\n ",
            "date": "2018-10-05T19:51:28+0000"
        },
        {
            "id": "comment-16640282",
            "author": "Jim Ferenczi",
            "content": "Sounds great Steve Rowe. I'll prepare a patch. ",
            "date": "2018-10-05T19:55:54+0000"
        },
        {
            "id": "comment-16646398",
            "author": "Jim Ferenczi",
            "content": "I pushed the javadocs addition in master and 7x, thanks Steve Rowe ",
            "date": "2018-10-11T13:03:50+0000"
        }
    ]
}