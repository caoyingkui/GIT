{
    "id": "SOLR-234",
    "title": "TrimFilter should update the start and end offsets",
    "details": {
        "affect_versions": "None",
        "status": "Closed",
        "fix_versions": [
            "1.2"
        ],
        "components": [],
        "type": "Improvement",
        "priority": "Minor",
        "labels": "",
        "resolution": "Fixed"
    },
    "description": "As implemented, the TrimFilter only trims the text.  It does not update the the startOffset and endOffset\n\nsee:\nhttp://www.nabble.com/TrimFilter----t.startOffset%28%29%2C-t.endOffset%28%29-tf3728875.html",
    "attachments": {
        "SOLR-234-TrimFilterOffsets.patch": "https://issues.apache.org/jira/secure/attachment/12357130/SOLR-234-TrimFilterOffsets.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "author": "Ryan McKinley",
            "id": "comment-12495141",
            "date": "2007-05-11T19:31:40+0000",
            "content": "This patch moves the start and end offests by how many spaces where eaten on either site.\n\nThis patch also extracts many of the generally useful token testing bits from TestSynonymFilter (and a few others) into a BaseTokenTestCase, and uses this base class rather then duplicating the helper functions.\n "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12495146",
            "date": "2007-05-11T20:07:02+0000",
            "content": "Chris points out:\n\n\"in Lucene Token offset information is suppose to reflect exactly where in\nthe orriginal stream of date the source of the token was found ... if hte\ntoken is modified in some way (ie: stemmed, trimmed, etc..)  the offsets\nare suppose to remain the same becuase regardless of the token text\nmunging, the orriginal location hsa not actually changed.\"\n\n\nI'll move the test refactoring to another issue as that is still useful "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12495147",
            "date": "2007-05-11T20:13:17+0000",
            "content": "Updating the offsets does seem like the right thing to do.\n\nI imagine using toCharArray() will be slower than using charAt() given that it will allocate a new array, and the number of charAt() calls will be low in the average case because there will only be a small amount of whitespace.\n\nIsn't it annoying that Java never seems to let you do things as efficiently as the class lib itself...\n\nAnother issue here is that the position increment isn't maintained.\nAnd let another future issue is that any payloads aren't maintained (that's in a newer version of Lucene).\nI'll bring up the latter issue on the lucene list since I think it's a bit of a design flaw. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12495153",
            "date": "2007-05-11T20:31:06+0000",
            "content": "> \n> Updating the offsets does seem like the right thing to do.\n> \n\nMy real use case is adding the the trim filter to the pattern tokenizer.  the 'correct' answer in my case it to update the offsets.\n\nThe case i can imagine leading to something like SOLR-42 is if a token is replaced with something that has leading or trailing spaces.  \n\nPerhaps we could add a parameter to the factory:\n\n <filter class=\"solr.TrimFilterFactory\" updateOffests=\"true\" />\n\nTo limit SOLR-42 style errors, the default could be false.\n\n\n> \n> Isn't it annoying that Java never seems to let you do things as efficiently as the class lib itself...\n> \n\nespecially for strings! "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12495158",
            "date": "2007-05-11T20:43:44+0000",
            "content": "updated to take a n \"updateOffsets\" argument.\n\nthis uses charAt() rather then toCharArray()\n\nWhat should happen with the position increment?\n\n\n      if( start > 0 || end < txt.length() ) \n{\n        int incr = t.getPositionIncrement();\n        t = new Token( t.termText().substring( start, end ),\n             t.startOffset()+start,\n             t.endOffset()-endOff,\n             t.type() );\n        \n        t.setPositionIncrement( incr ); //+ start ); TODO? what should happen with the offset\n      } "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12495161",
            "date": "2007-05-11T20:56:18+0000",
            "content": "> What should happen with the position increment? \nIt should remain the same as the original.\n\n> The case i can imagine leading to something like SOLR-42 is if a token is replaced with something\n> that has leading or trailing spaces. \n\nReally whacky, but possible I guess.  I don't know of any token filters that would do that, unless someone explicitly used a synonym with spaces at the end.  It doesn't make any sense.\nI'd think that updating the offsets is almost always the right thing to do (and should be the default?), given that spaces will almost always come from the field value itself.\n\n-Yonik "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12495202",
            "date": "2007-05-11T23:20:52+0000",
            "content": "> I'd think that updating the offsets is almost always the right thing to do (and should be the default?), given that spaces will \n> almost always come from the field value itself.\n\ni don't follow your reasoning ... the offsets are suppose to denote where in the original text the Token came from ... a Filter can't make any assumptions about source of the tokens except the token itself, so i don't' see why a Filter would by default assume it can muck with the offsets.\n\nIn Ryan's use case he may want his highlighter-esque code to be able to know how many characters were trimmed off of each end \u2013 and i buy that it makes sense for TrimFilter to have an option to relay that info by modifying the offset \u2013 but joe random user should be able to expect that by default the offsets of the Tokens his tokenizer produces won't be modified ... i would personally think it's a bug to get the behavior ryan describes out of a highlighter if i knew that my tokenizer was only spliting on punctuation. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12495209",
            "date": "2007-05-12T00:12:49+0000",
            "content": "\n> \n> .. a Filter can't make any assumptions about source of the tokens except the token itself ...\n\nI get the basic pattern now:  Tokenizers determin the start/end offsets and Filters just transform the text along the way.  \n\n\n> In Ryan's use case he may want his highlighter-esque code to be able to know ...\n> \n\nI am fine with either:\n\n1. leave the TrimFilter as is and do the highlighter-esque code on the highlighting side.  \n\n2. Add an optional updateOffsets=\"true\" param, with the default set to \"false\" "
        },
        {
            "author": "Yonik Seeley",
            "id": "comment-12495213",
            "date": "2007-05-12T00:54:45+0000",
            "content": "offsets point back to the original field value for a particular token... and to me, it's a semantic contract (point to what makes sense in the source). It's not limited to the offsets generated by the Tokenizer... Analyzers don't have to use Tokenizers and TokenFilters at all.\n\nAs an example, WordDelimiterFilter modifies offsets when it splits words, and that makese sense to me.\n\nAnother way to think about it is that there is more than one way to solve a problem (construct an analyzer).\nWhat matters is the tokens that come out the end... not if I did\na) a tokenizer that split on something followed by a filter that trimmed\nvs\nb) a tokenizer that managed to split on something including discarding the whitespace\n\nFor this specific case, I think it comes down to the likely usecases for the filter, and an argument could be made either way.  I'm fine with either as this is a very minor issue. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12495224",
            "date": "2007-05-12T03:48:43+0000",
            "content": "yeah ... i'm not saying there aren't good usecases both ways \u2013 it definitely makes sense to have an option \u2013 i'm just saying that as a general rule TokenFilters shouldn't be munging offsets ... i don't see a big difference between TrimFilter and StemmingFilter (where the the stem of \"foo   \" and \"foo      \" is \"foo\").  so the option should default to off. "
        },
        {
            "author": "Ryan McKinley",
            "id": "comment-12495233",
            "date": "2007-05-12T06:40:11+0000",
            "content": "Unless there are objections, I'd like to add \"updateOffsets\" as an option to the TrimFilter.\n\nBy default it will not modify the offsets.\n\nDepending on how the Tokenizer+Analyzer stream is configured it may or may not make sense, so the option seems reasonable. "
        },
        {
            "author": "Hoss Man",
            "id": "comment-12589290",
            "date": "2008-04-15T23:44:42+0000",
            "content": "This bug was modified as part of a bulk update using the criteria...\n\n\n\tMarked (\"Resolved\" or \"Closed\") and \"Fixed\"\n\tHad no \"Fix Version\" versions\n\tWas listed in the CHANGES.txt for 1.2\n\n\n\nThe Fix Version for all 39 issues found was set to 1.2, email notification\nwas suppressed to prevent excessive email.\n\nFor a list of all the issues modified, search jira comments for this\n(hopefully) unique string: 20080415hossman2 "
        }
    ]
}