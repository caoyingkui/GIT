{
    "id": "LUCENE-4766",
    "title": "Pattern token filter which emits a token for every capturing group",
    "details": {
        "components": [
            "modules/analysis"
        ],
        "fix_versions": [
            "4.4",
            "6.0"
        ],
        "affect_versions": "4.1",
        "priority": "Minor",
        "labels": "",
        "type": "New Feature",
        "resolution": "Fixed",
        "status": "Closed"
    },
    "description": "The PatternTokenizer either functions by splitting on matches, or allows you to specify a single capture group.  This is insufficient for my needs. Quite often I want to capture multiple overlapping tokens in the same position.\n\nI've written a pattern token filter which accepts multiple patterns and emits tokens for every capturing group that is matched in any pattern.\nPatterns are not anchored to the beginning and end of the string, so each pattern can produce multiple matches.\n\nFor instance a pattern like :\n\n\n    \"(([a-z]+)(\\d*))\"\n\n\n\nwhen matched against: \n\n\n    \"abc123def456\"\n\n\n\nwould produce the tokens:\n\n\n    abc123, abc, 123, def456, def, 456\n\n\n\nMultiple patterns can be applied, eg these patterns could be used for camelCase analysis:\n\n\n    \"([A-Z]{2,})\",\n    \"(?<![A-Z])([A-Z][a-z]+)\",\n    \"(?:^|\\\\b|(?<=[0-9_])|(?<=[A-Z]{2}))([a-z]+)\",\n    \"([0-9]+)\"\n\n\n\nWhen matched against the string \"letsPartyLIKEits1999_dude\", they would produce the tokens:\n\n\n    lets, Party, LIKE, its, 1999, dude\n\n\n\nIf no token is emitted, the original token is preserved. \nIf the preserveOriginal flag is true, it will output the full original token (ie \"letsPartyLIKEits1999_dude\") in addition to any matching tokens (but in this case, if a matching token is identical to the original, it will only emit one copy of the full token).\n\nMultiple patterns are required to allow overlapping captures, but also means that patterns are less dense and easier to understand.\n\nThis is my first Java code, so apologies if I'm doing something stupid.",
    "attachments": {
        "LUCENE-4766.patch": "https://issues.apache.org/jira/secure/attachment/12568746/LUCENE-4766.patch"
    },
    "issue_links": {},
    "comments": [
        {
            "date": "2013-02-10T12:35:40+0000",
            "content": "Patch implementing org.apache.lucene.analysis.pattern.PatternCaptureGroupTokenFilter and org.apache.lucene.analysis.pattern.TestPatternCaptureGroupTokenFilter ",
            "author": "Clinton Gormley",
            "id": "comment-13575427"
        },
        {
            "date": "2013-02-11T10:01:18+0000",
            "content": "Hey Clinton, this looks very interesting and given this is your first java experience pretty impressive too. I am not sure how expensive this filter in practice will be but given that you can do stuff you can't do with any of the other filters I think folks just have to pay the price here. I like that all patterns operate on the same CharSequence and that you are setting offsets right. Cool stuff!  ",
            "author": "Simon Willnauer",
            "id": "comment-13575714"
        },
        {
            "date": "2013-02-11T10:21:44+0000",
            "content": "I like that all patterns operate on the same CharSequence and that you are setting offsets right.\n\nDoes this filter need to set offsets? I'm worried that under certain circumstances filters that modify offsets might create inconsistent offset graphs (because they don't know what filters have been applied before, there is an exclusion list for filters that modify offsets in TestRandomChains). ",
            "author": "Adrien Grand",
            "id": "comment-13575718"
        },
        {
            "date": "2013-02-11T11:13:49+0000",
            "content": "Does this filter need to set offsets? I'm worried that under certain circumstances filters that modify offsets might create inconsistent offset graphs (because they don't know what filters have been applied before, there is an exclusion list for filters that modify offsets in TestRandomChains).\n\nyeah I agree offsets are tricky here. I just wonder if we really should restrict our TF to not fix offsets? Kind of an odd thing though. What should a tokenfilter like this do instead? ",
            "author": "Simon Willnauer",
            "id": "comment-13575731"
        },
        {
            "date": "2013-02-11T11:15:43+0000",
            "content": "I updated the patch and added a FilterFactory etc. to make tests pass. it still contains the offset fix and I personally don't really know what TF should do in those cases ",
            "author": "Simon Willnauer",
            "id": "comment-13575733"
        },
        {
            "date": "2013-02-11T11:29:03+0000",
            "content": "I just wonder if we really should restrict our TF to not fix offsets? Kind of an odd thing though. What should a tokenfilter like this do instead?\n\nI think that for some examples, it makes sense not to fix offsets? In the case of the URL example ((https?://([a-zA-Z\\-_0-9.]+))), I think it makes sense to highlight the whole URL (including the leading http(s)://) even if the query term is just www.mysite.com. On the other hand, it could be weird if the goal was to split a long CamelCase token (letsPartyLIKEits1999_dude), but maybe this should be done by a Tokenizer rather than a TokenFilter?\n\n(No strong feeling here, I'd just like to see if we can find a way to commit this patch without having to grow our TokenFilter exclusion list.) ",
            "author": "Adrien Grand",
            "id": "comment-13575739"
        },
        {
            "date": "2013-02-11T12:33:36+0000",
            "content": "\n(No strong feeling here, I'd just like to see if we can find a way to commit this patch without having to grow our TokenFilter exclusion list.)\n\nI dont think tokenfilters should change offsets in general. This is not possible to do correctly. In general if you are splitting and creating like this... its a tokenizer not a tokenfilter. And only a tokenizer can set offsets, because its the only one that has access to the charfilter correction data.\n\nbesides, trying to be a token 'creator' over an incoming tokenstream graph is really hard to get right.\n\nSo I would prefer if this filter either became a tokenizer or did not change offsets at all. Then we can probably get it committed without hassle.\n\nwe cannot allow this exclusion list to grow. Its not an exclusion list that says 'its ok to add more broken filters'. Its a list of filters that will get deleted from lucene soon unless someone fixes them, because we have to stop indexwriter from writing invalid data into the term vectors here.\n\nAlso the test should call checkRandomData()  ",
            "author": "Robert Muir",
            "id": "comment-13575760"
        },
        {
            "date": "2013-02-11T13:11:13+0000",
            "content": "Is it OK for a tokenizer to create multiple tokens in the same positions but with different offsets? eg \n\n\n\"foobar\" -> [\"foobar\",\"foo\",\"bar\"] with positions [1,1,1] and startOffsets [0,0,3]?\n\n\n\nHaving a look at the wordDelimiter token filter, with preserveOriginal set to true, it increments the position for each new offset, eg: \n\n\n\"fooBar\" -> [\"fooBar\",\"foo\",\"Bar\"] with positions [1,1,2] and startOffsets [0,0,3].\n\n\n\nI'm asking because I'm not sure exactly how positions and offsets get used elsewhere, and so what the correct behaviour is. From my naive understanding, the wordDelimiter filter can produce spurious results with phrase searches, eg matching \"fooBar Bar\" ",
            "author": "Clinton Gormley",
            "id": "comment-13575774"
        },
        {
            "date": "2013-02-11T13:24:19+0000",
            "content": "The positions are used for searching, offsets for highlighting.\n\nSo you can (unfortunately) set the offsets to whatever you want, it wont affect searches. Instead it will only cause problems for highlighting. An example of this is: https://issues.apache.org/jira/browse/SOLR-4137\n\nFor a tokenfilter, it doesnt make sense to change offsets, because a tokenizer already broke the document into words and mapped them back to their original location in the document.\n\nIf a tokenfilter REALLY needs to change offsets, then its a sign its subclassing the wrong analysis type and should be a tokenizer: because its trying to break the document into words, not just alter existing tokenization  ",
            "author": "Robert Muir",
            "id": "comment-13575776"
        },
        {
            "date": "2013-02-11T13:35:25+0000",
            "content": "Is it OK for a tokenizer to create multiple tokens in the same positions but with different offsets?\n\nAlthough it's not common, it is perfectly fine for a Tokenizer to generate multiple tokens in the same position.\n\nHowever, I think the correct way to tokenize your example would be:\n\ntokens: foo, foobar, bar\npositions: 1, 1, 2\nposition lengths: 1, 2, 1\nstart offsets: 0, 0, 3\nend offsets: 3, 6, 6\n\n\n\nI'm not sure WordDelimiterFilter is the best example to look at. I'm not familiar with it at all, but it's currently in the exclusion list for both positions and offsets (and is the culprit for SOLR-4137). ",
            "author": "Adrien Grand",
            "id": "comment-13575786"
        },
        {
            "date": "2013-02-11T13:53:32+0000",
            "content": "WDF is one of those \"bad\" examples. WDF should be included in a custom Tokenizer. WDF is always used together with WhitespaceTokenizer, so it should be included into some  \"WordDelimiterWhitespaceTokenizer\". ",
            "author": "Uwe Schindler",
            "id": "comment-13575793"
        },
        {
            "date": "2013-02-11T14:33:02+0000",
            "content": "OK, so I should redo this as a tokenizer, and set positionLengths correctly.\n\nOne issue is that, because there are multiple patterns, the emitted tokens can overlap, eg:\n\n\n   \"foobarbaz\" -> foo, foobar, oba, bar, baz\n\n\n\nin which case I think I would need to emit:\n\n\n    positions:         1, 1, 2, 3, 5\n    position lengths:  2, 4, 2, 2, 1\n    start offsets:     0, 0, 0, 0, 0\n    end offsets:       3, 6, 3, 3, 3\n\n\n\nIs this correct? It's starting to look quite complex... ",
            "author": "Clinton Gormley",
            "id": "comment-13575811"
        },
        {
            "date": "2013-02-11T14:39:12+0000",
            "content": "I tend to disagree that this should be a tokenizer. IMO a tokenizer should only split tokens in a stream fashion and should not emit tokens on the same position really. This is what token filters should do. It also clashes with reuseability since you can't really reuse tokenizers you have to decide which one you want. At some point you need to know what you are doing here really. I don't have a definite answer but there is currently no clean way to do what clinton wants to do IMO. ",
            "author": "Simon Willnauer",
            "id": "comment-13575813"
        },
        {
            "date": "2013-02-11T16:47:48+0000",
            "content": "I use WDF with ICUTokenizer.  ICUTokenizer is customized with an RBBI file for latin1 that only breaks on whitespace. ",
            "author": "Shawn Heisey",
            "id": "comment-13575889"
        },
        {
            "date": "2013-02-13T10:23:04+0000",
            "content": "I tend to disagree that this should be a tokenizer.\n\nMaybe another option is to change this filter so that it doesn't change offsets? Let's imagine this filter is used to break TokenFilter into Token, TokenFilter and Filter, I think it's acceptable to highlight TokenFilter as a whole when searching for \"Token\" or \"Filter\"? ",
            "author": "Adrien Grand",
            "id": "comment-13577469"
        },
        {
            "date": "2013-02-13T11:59:36+0000",
            "content": "New patch which preserves the offsets of the original token. Includes Simons patch to create the filter factory ",
            "author": "Clinton Gormley",
            "id": "comment-13577505"
        },
        {
            "date": "2013-02-13T12:08:36+0000",
            "content": "The patch should add a test that uses checkRandomData.\n\nit would find bugs like this:\n\ncharOffsetStart = offsetAttr.startOffset();\ncharOffsetEnd = charOffsetStart + spare.length;\n\n\n\ncharOffsetEnd should be offsetAttr.endOffset(). If there is a charfilter, the current calculation will be incorrect. ",
            "author": "Robert Muir",
            "id": "comment-13577512"
        },
        {
            "date": "2013-02-13T12:15:22+0000",
            "content": "Clinton, I think you can trash the offset attribute reference in there entirely just don't mess with them at all. Can you also call BaseTokenStreamTest#assertAnalyzesToReuse and BTST#checkRandomData in your tests please ",
            "author": "Simon Willnauer",
            "id": "comment-13577514"
        },
        {
            "date": "2013-02-13T13:31:39+0000",
            "content": "The charOffsetEnd now uses the correct offsetAttr.endOffset() and, added a test using checkRandomData() ",
            "author": "Clinton Gormley",
            "id": "comment-13577555"
        },
        {
            "date": "2013-02-13T13:31:43+0000",
            "content": "Clinton, I think you can trash the offset attribute reference in there entirely just don't mess with them at all.\n\nThat's part of a bigger problem in the current code. The idea of this filter is to make from one input Token multiple output Tokens. To make this work correct, the new output tokens must be produced based on the original token (means the filter must reset the new produced token to a clean state, otherwise it might happen that unrelated and unknown attributes stay alive with wrong values - especiall if later TokenFilter change attributes, e.g. a Synonymfilter is inserting more synonyms). The problem Clinton had was that he had to re-set the offset attribute (although he does not change it); but he missed possible other attributes on the stream he does not know about.\n\nIf you look at other filters doing similar things like Synonymfilter, WDF, the way it has to work is like that:\n\n\tThe first token emmitted is the \"original one, maybe modified\n\tAll \"inserted Tokens\" are cloned from the original (first) token, use captureState/restoreState to do that. This will initialize the attribute source to the exact same token like the original (unmodified one). After you called restoreState, you can modify the attribute (like term text) and setPositionIncrement(0). You can then leave the the offset (and other unknown attributes that may be on the token stream) unchanged - don't reference them at all.\n\n ",
            "author": "Uwe Schindler",
            "id": "comment-13577556"
        },
        {
            "date": "2013-04-24T09:02:27+0000",
            "content": "here is a new patch, updated to trunk and moved over to use capture&restore state. I think this one is ready, I will commit this today if nobody objects. ",
            "author": "Simon Willnauer",
            "id": "comment-13640260"
        },
        {
            "date": "2013-04-24T09:05:11+0000",
            "content": "argh... I missed svn add... here is the right patch ",
            "author": "Simon Willnauer",
            "id": "comment-13640262"
        },
        {
            "date": "2013-04-24T09:36:21+0000",
            "content": "+1 ",
            "author": "Adrien Grand",
            "id": "comment-13640292"
        },
        {
            "date": "2013-04-24T10:21:06+0000",
            "content": "[trunk commit] simonw\nhttp://svn.apache.org/viewvc?view=revision&revision=1471347\n\nLUCENE-4766: Added a PatternCaptureGroupTokenFilter that uses Java regexes to emit multiple tokens one for each capture group in one or more patterns ",
            "author": "Commit Tag Bot",
            "id": "comment-13640314"
        },
        {
            "date": "2013-04-24T10:32:11+0000",
            "content": "[branch_4x commit] simonw\nhttp://svn.apache.org/viewvc?view=revision&revision=1471352\n\nLUCENE-4766: Added a PatternCaptureGroupTokenFilter that uses Java regexes to emit multiple tokens one for each capture group in one or more patterns ",
            "author": "Commit Tag Bot",
            "id": "comment-13640318"
        },
        {
            "date": "2013-04-24T10:33:21+0000",
            "content": "committed, thanks clinton ",
            "author": "Simon Willnauer",
            "id": "comment-13640320"
        },
        {
            "date": "2013-07-16T04:14:04+0000",
            "content": "I just happened to notice that the underlying token filter accepts a list of patterns, but the factory only accepts a single pattern.\n\nWas this intentional or an oversight?\n\nIn fact, the main example for the filter requires multiple patterns, which the factory does not support. ",
            "author": "Jack Krupansky",
            "id": "comment-13709450"
        },
        {
            "date": "2013-07-23T18:37:11+0000",
            "content": "Bulk close resolved 4.4 issues ",
            "author": "Steve Rowe",
            "id": "comment-13716774"
        }
    ]
}