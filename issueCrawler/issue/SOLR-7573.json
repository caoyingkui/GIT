{
    "id": "SOLR-7573",
    "title": "Inconsistent numbers of docs between leader and replica",
    "details": {
        "components": [],
        "type": "Bug",
        "labels": "",
        "fix_versions": [],
        "affect_versions": "4.10.3",
        "status": "Resolved",
        "resolution": "Cannot Reproduce",
        "priority": "Major"
    },
    "description": "Once again assigning to myself to keep track. And once again not reproducible at will and possible related to firehosing updates to Solr.\n\nSaw a situation where things seemed to be indexed normally, but the number of docs on a leader and follower were not the same. The leader had, as I remember, a 4.5G index and the follower a 1.9G index. No errors in the logs, no recovery initiated, etc. All nodes green.\n\nThe very curious thing was that when the follower was bounced, it did a full index replication from the leader. How that could be happening without the follower ever going into a recovery state I have no idea.\n\nAgain, if I can get this to reproduce locally I can put more diagnostics into the process and see what I can see. I also have some logs to further explore.",
    "attachments": {},
    "issue_links": {},
    "comments": [
        {
            "date": "2015-05-19T19:50:49+0000",
            "author": "Erick Erickson",
            "content": "Not entirely sure whether all there are related to firehosing updates at Solr or not, but they were all seen in the same environment. ",
            "id": "comment-14551098"
        },
        {
            "date": "2015-06-04T22:36:22+0000",
            "author": "Erick Erickson",
            "content": "Additional data. I have a test harness that I can cause \"things to go wrong\" with fairly regularly, but not on demand. It works like this:\n\nFor (some number of configurable cycles)\n    Spawn a bunch of threads that create really simple documents and send them to the collection\n    wait for all the threads to terminate\n    commit(true, true)\n    for each shard\n       check that q=: returns the same number of docs found.\n          If there is a discrepancy, report and exit\n\n\nThe interesting thing here is that I saw this error, but by the time I could investigate via the admin UI, the counts were identical. However, the replica that had a smaller count was also forced into leader-initated recovery which is a symptom I saw onsite. So the working hypothesis is that the node was in LIR for some period but managed to respond to a query. After LIR was over it had re-synched and was OK. I'm not clear at all how the replica managed to respond, I'll add more logging to see what I can see. I am using HttpSolrClient to do the verification with distrib=false so I'm not sure whether the active state in ZK matters at all. When I was onsite, the replica didn't recover, but we didn't wait very long and restarted it, at which point it did a full sync from the leader so it's consistent with what I just saw.\n\nThis seems like correct (eventual consistency) behavior, the problem is that the replica goes into LIR in the first place. And that it manages to respond to a direct ping via HttpSolrClient.\n\nThis gives me some hope that if we do SOLR-7571 and have the client(s) keep from overwhelming Solr we have a mechanism to at least avoid the situation arising in the first place. And if I incorporate that into this test harness and the problem goes away it'll give me confidence that we're getting to root causes.. ",
            "id": "comment-14573708"
        },
        {
            "date": "2015-06-19T20:31:41+0000",
            "author": "Erick Erickson",
            "content": "On the theory that the replica being much busier than it needed to be was leading to \"bad stuff happening\", I'm linking in SOLR-7333. Combined with SOLR-7344 this may affect all three of these JIRAs. ",
            "id": "comment-14593924"
        },
        {
            "date": "2016-01-20T22:02:45+0000",
            "author": "Erick Erickson",
            "content": "I'm suspect that the fact that things eventually settled out simply means that the commits were returning before actually being completed and/or autowarming was kicking in. We can re-open if we see this again or create a new JIRA. ",
            "id": "comment-15109569"
        }
    ]
}